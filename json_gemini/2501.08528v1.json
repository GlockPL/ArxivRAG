{"title": "Dynamic Portfolio Optimization via Augmented DDPG with Quantum Price Levels-Based Trading Strategy", "authors": ["Runsheng Lin", "Zihan Xing", "Mingze Ma", "Raymond S.T. Lee"], "abstract": "With the development of deep learning, Dynamic Portfolio Optimization (DPO) problem has received a lot of attention in recent years, not only in the field of finance but also in the field of deep learning. Some advanced research in recent years has proposed the application of Deep Reinforcement Learning (DRL) to the DPO problem, which demonstrated to be more advantageous than supervised learning in solving the DPO problem. However, there are still certain unsolved issues: 1) DRL algorithms usually have the problems of slow learning speed and high sample complexity, which is especially problematic when dealing with complex financial data. 2) researchers use DRL simply for the purpose of obtaining high returns, but pay little attention to the problem of risk control and trading strategy, which will affect the stability of model returns. In order to address these issues, in this study we revamped the intrinsic structure of the model based on the Deep Deterministic Policy Gradient (DDPG) and proposed the Augmented DDPG model. Besides, we also proposed an innovative risk control strategy based on Quantum Price Levels (QPLs) derived from Quantum Finance Theory (QFT). Our experimental results revealed that our model has better profitability as well as risk control ability with less sample complexity in the DPO problem compared to the baseline models.", "sections": [{"title": "I. INTRODUCTION", "content": "Dynamic Portfolio Optimization (DPO) is the process of adjusting asset allocation in a portfolio in a timely manner in order to ensure stable returns in a constantly changing market. Due to the complex, noisy, nonstationary and even chaotic nature of financial markets, traditional mathematical models are difficult to accomplish the task of DPO.\nWith the development of Deep Learning (DL), many researchers have tried to use various time series forecasting models such as Recurrent Neural Networks (RNN) and Long- Short Term Memory (LSTM) networks to predict the price trend of financial products, to grasp the price trend and make appropriate trading actions in order to profit from the market. Nevertheless, some inherent weaknesses in applying super- vised learning to solve the DPO problem cannot be avoided. For example, supervised learning-based models require addi- tional trading logic to translate into market behaviors of buying and selling, after predicting the price results. Meanwhile, the goal of its optimization while training, minimizing forecast errors, is different from the goal of maximizing the portfolio's returns in the DPO task.\nHowever, Deep Reinforcement Learning (DRL), with the combination of Reinforcement Learning (RL) and DL, has been drawing the attention of researchers in recent years owing to its unique reward and punishment mechanisms, which makes it suitable for solving DPO problems in financial trading. Because through DRL, an accurate prediction output is not required; what the agent in the DRL algorithm pursues is just trading actions that can maximize the future profit based on DRL's unique reward and punishment mechanisms. With such characteristics, some researchers have tried to use DRL to accomplish the task of DPO. For example, value-based DRL methods such as DQN [1] and policy-based methods such as DDPG [2] were applied to solve DPO problem and outperformed both traditional and advanced models [3] [4], demonstrating that avoiding direct market prediction can indeed reduce the negative influence of noise in the market. Nevertheless, these works do not contribute to the solu- tion of the slow learning speed of DRL algorithms while dealing with complex financial data. Also, risk control and"}, {"title": "II. RELATED WORK", "content": "Taking the advance of DL, previous works [6] [7] are conducted to solve the DPO problem based on the predic- tive power of deep neural networks for time series data, which shows good performance over traditional portfolio op- timization theory-based methods. However, the shortcoming of supervised learning methods is rather obvious: trading with supervised learning-based methods necessitates additional ef- fort besides forecasting, and its training goal is to minimize forecast error, which differs from the DPO task's goal of maximizing portfolio returns.\nIn recent years, a growing number of researchers have identified the limitation of supervised learning and tried to use DRL to optimize the solution. Basic DRL algorithms such as DQN [1] and DDPG [2] are applied in DPO [3] [4] [8], which demonstrate the potential of DRL models. In order to figure out how DRL can achieve better performance, a framework [9] based on DPG which applies different networks such as Convolutional Neural Networks (CNN), RNN and LSTM within the DPG is proposed, which tested on the cryp- tocurrency historical price data to compare the performance of various networks. Similarly, [10] explored the influences of different optimizers, target functions and learning rates in DRL models including DDPG, PPO and DPG, in DPO problems. With the expectation of better enhancing the horizon of DRL algorithms, [11] collected financial news and used LSTM- based price trend prediction to augment the states of agents in DPG. Based on the basic DRL algorithms, [12] proposed an integrated strategy that integrates PPO, A2C, and DDPG to dynamically select models for portfolio allocation based on the performance of their Sharpe ratios.\nAll these works have contributed significantly to the ex- ploration of the DRL model in solving the DPO problem. However, these works have yet to make any progress in ad- dressing the issue of DRL algorithms' low learning efficiency when dealing with complicated financial data. Moreover, risk control was not taken seriously by these efforts, and their risk"}, {"title": "B. Quantum Finance Theory", "content": "Quantum Finance Theory (QFT) introduced the intrinsic relationship between the financial market and the quantum harmonic model [13] [14]. In QFT, the dynamics of financial products in the world financial market are financial particles with wave-particle duality. Financial particles have equilibrium states just like physical particles do. These particles will continue to exist in their current state in the absence of any external stimuli. However, if an outside stimulus has the power to excite a particle, it will migrate to a different energy level, either higher or lower. In the financial market, we refer to these levels as Quantum Price Levels (QPLs), which are adequate representations of the intrinsic energy of financial products [13].\nBecause of the noisy nature of financial markets, traditional financial characteristics such as OPEN, HIGH, LOW, CLOSE, and even technical indicators such as the Moving Average (MA) and Relative Strength Index (RSI) are unable to describe the effective volatility of financial products accurately [15]. However, because QPLs model the energy level of the financial products themselves, different QPLs can effectively represent the volatility level of the financial product.\nTherefore, the QPLs-based indicator effectively captures the volatility of financial products [16]. The principle to obtain such an indicator is to simulate the volatility of financial products using the Quantum Anharmonic Oscillator (QAHO) model and then solve it by Quantum Finance Schr\u00f6dinger Equation (QFSE) to obtain the energy levels of financial products, which is QPLs."}, {"title": "III. METHODOLOGY", "content": "The purpose of this paper is to explore the effectiveness of Augmented DDPG in DPO tasks and the enhancement to model returns that result from a risk control module based on QPLs. DPO is the process by which an agent adjusts the allocation of assets in a portfolio in response to the changes in the financial market. This is done in order to ensure that the portfolio can maximize returns in the market with a certain level of risk control. Since we cannot accurately predict the future trends of financial markets, DPO tasks are suitable to be solved using model-free DRL models such as DDPG and PG. In such models, raw market data such as price returns will be input. The model will then optimize the parameters with the preset objective function as the optimization goal and output the optimal allocation of assets in a portfolio.\nTo demonstrate that the DRL model has such capability, we will demonstrate the performance of our model in the form of back-testing. Therefore, we need to make the following assumptions to conduct back-testing,\n1) Adequate Liquidity\nThe term liquidity describes how quickly and easily a security or asset can be turned into cash without"}, {"title": "B. Quantum Price Levels", "content": "The dynamics of financial products on global financial markets, such as forex, financial indices, commodities, etc., can be represented as Financial Particles (FPs) with wave- particle duality properties, according to Quantum Finance Theory (QFT) [13]. If an external stimulus excites the particle, it will move to another energy level depending on the property of the stimulus. In the financial market, we refer to these levels as Quantum Price Levels (QPLs), which can also be thought of as support and resistance.\nTo obtain QPLs, we need to solve the Quantum Finance Theory Schr\u00f6dinger Equation (QFSE) by combining the Hamiltonian with traditional Schr\u00f6dinger equation [13]. In QFT, Hamiltonian is defined as,\n$\\hat{H}=\\frac{\\hat{p}^2}{2m} + V(r)$                                              (1)\nwhere the Kinetic Energy (KE) is represented by $\\frac{\\hat{p}^2}{2m}$. h stands for the Plank constant and m stands for the internal properties of the market, such as the market capital. $V (r)$ indicates the Potential Energy (PE).\nTherefore, the QFSE is formulated as,\n$\\frac{\\hbar^2 d^2}{2m dr^2} + \\frac{\\gamma r^2}{2} - \\frac{\\gamma^2 r^4}{4} \\phi(r) = E\\phi(r)$             (2)\nwhere E stands for the particle's energy levels, which for financial particles corresponds to the QPLs and $\\phi(r)$ repre- sents the QFSE wave-function, which is approximated by the probability density function of historical price return."}, {"title": "C. Mathematical Formulation", "content": "This section introduces some fundamental mathematical formulations in the DPO problem.\n1) Weight vector (the percentage of each asset in the portfolio) on day t:\n$w_t = (W_{0,t}, W_{1,t}, W_{2,t},..., W_{m,t})^T$                   (3)\nwhere m is the number of assets."}, {"title": "2) Execution price vector (the execution price of each product) on day t:", "content": "$v_t = (v_{0,t}^E, v_{1,t}^E, v_{2,t}^E,..., v_{m,t}^E)^T$                               (4)\nwhere m is the number of assets, $v_{i,t}^E$ can be any number in the range [$v_{i,t}^{low}, v_{i,t}^{high}$]"}, {"title": "3) Price return vector (the return of each product) on day t:", "content": "$y_t = (\\frac{v_{i,t}^E}{v_{i,t-1}^E} - 1 + r)$                       (5)\nwhere r is the risk-free rate."}, {"title": "D. DDPG Trading Agent", "content": "1) State Space: To obtain the state space at each time step, we processed the historical price and gained the log return matrix $r_{t-T:t}$, of windows size T. The reason for choosing log return instead of return is that by adding the log returns, we equivalently multiply the gross returns, and as a result, the networks can learn non-linear functions of the products of returns (i.e., asset-wise and cross-asset), which are the fundamental units of the covariances between assets [20]. In such a way, the relationship between different assets in the portfolio can be captured.\nThe formulation of the state is as follows,\n$s_t = r_{t-T:t} =  \\begin{bmatrix} r_{0, t-T} & r_{0, t-T+1} & ... & r_{0, t} \\\\  r_{1, t-T} & r_{1, t-T+1} & ... & r_{1, t} \\\\  ... & ... & ... & ... \\\\  r_{m, t-T} & r_{m, t-T+1} & ... & r_{m, t}  \\end{bmatrix}$                                              (6)\nwhere $r_{i,t-T:t}$ represents the log return of assets i in the time interval of [t - T, t] and $r_{0} = log(r)$.\n2) Action Space: The solution to the DPO is to keep the agent updated on the asset allocation in the portfolio, that is, to determine the asset weight allocation for the next day $W_{t+1}$. Therefore, the action $a_t$ of the DRL agent should be the same as the weight vector $W_{t+1}$,\n$a_t = W_{t+1} = (W_{0,t+1}, W_{1,t+1}, W_{2,t+1},..., W_{m,t+1})^T$            (7)"}, {"title": "Therefore, the transaction costs at time t should be defined as,", "content": "3) Reward with Transaction Costs & Gini Bonus: The definition of reward signals is usually the most challenging part of DRL because it determines what the learning goal of the agent is, which also determines the final performance of the agent. Therefore, our reward function should mimic as much as possible the real reward situation given by the market, while allowing the agent to have moderate risk management. Before introducing our definition of the reward signal, the following two terms should be illustrated.\na) Transaction Costs\nIn a real trading market, transaction costs such as com- mission fees are an important factor affecting returns. Too many frequent transactions may make the transaction"}, {"title": "E. PG Risk Control Agent", "content": "costs outweigh the benefits, resulting in an overall loss. Therefore, we should take transaction costs as an impor- tant consideration to simulate real market transactions. Before the market opens at time t, the weight vector of the portfolio is represented by $w_{t-1}$. Thus, as the value of each asset in the portfolio changes through time, the weight vector should be updated,\n$w_t = \\frac{y_{t-1} \\otimes w_{t-1}}{y_{t-1}w_{t-1}}$                            (8)\nwhere $\\otimes$ is the element-wise multiplication. The weight change of assets should be defined as,\n$\\Delta w_{i,t} = w_{i,t} - w_{i,t-1}$                           (9)\nTherefore, the transaction costs at time t should be defined as,\n$\\mu_t = C \\sum_{i=1}^m |\\Delta w_{i,t}|$                           (10)\nwhere C is a constant that represents the commission rate. And then the cumulative return from time 0 to T is formulated as follows,\n$R_T = \\prod_{t=1}^T (1 - \\mu_t) w_t \\cdot y_t$                      (11)\nAs mentioned above, apart from mimicking the trans- action environment in the real market, we should also enable the agent to learn risk control management to some extent. In practice, we found that DRL models are very easy to \"put all the eggs in one basket\", which means the portfolio almost invests in only one kind of asset. This situation would make the portfolio meaningless, while dramatically increasing the risk of the investment. Inspired by the Entropy Bonus [21], we proposed the Gini Bonus, which gives a bonus to the action that diversifies the allocation of the portfolio. The Gini Bonus is formulated as,\n$Gini (w_t) = \\sum_{i=1}^m w_{t,i} (1 - w_{t,i}) = 1 - \\sum_{i=1}^m w_{t,i}^2$                         (12)\nTherefore, combining the above two terms, we defined the reward function of DDPG as,\n$\\text{reward}_t = \\log ((1 - \\mu_t) w_t \\cdot y_t) + \\eta \\text{Gini} (w_t)$       (13)"}, {"title": "1) State Space", "content": "The state of the PG agent at time t is the same as that of the DDPG agent:\n$S_t = r_{t-T:t}$                                                     (14)"}, {"title": "2) Action Space", "content": "The PG agent predicts the price movement after touching QPL, which can be divided into a bullish signal S+ and a bearish signal S-. The policy of PG agent at time t in our model is defined as:\n$p_t = (p_{bullish}, p_{bearish})$                                                                                       (15)\nwhere each element in $p_t$ represents the probability of each type of action.\nIn the training environment, the action generated by the Monte Carlo method is given by:\n$a_t = sampling(p_t)$                                                                                           (16)\nIn the testing environment, the action selected with the largest probability is given by:\n$a_t = argmax(p_t)$                                                                                                     (17)"}, {"title": "4) Reward", "content": "The reward of the PG agent is used to represent the return generated by the action it selected, so we can define it as:\n$\\text{reward}_t = \\Delta w_t \\times (\\frac{v_{i,t}^{\\text{close}}}{v_{i,t}^E} - 1)$                                                    (18)"}, {"title": "IV. IMPLEMENTATION", "content": "Our Augmented DDPG model consists of two basic DRL models, Deep Deterministic Policy Gradient (DDPG) and Policy Gradient (PG) models, where the algorithm is described in Algorithm1. A schematic of our System is shown in Fig. 2.\nBased on previous work [3] which concluded that in DPO task, using CNN as state encoder outperforms RNN such as LSTM in many cases when constructing actors and critics network of DDPG algorithm. As shown in the Fig. 2, two layers of CNN are used as their State Encoder in building the networks of Actor and Critic, and unlike the traditional DDPG with separate Actor and Critic networks, our model shares one State Encoder for both Actor and Critic, the Integrated Actor-Critic structure will speed up the learning process and reduce sample complexity [5]. After the state is encoded, it is passed into the Actor and Critic decoders respectively. In the Actor decoder, there are two fully connected layers and one softmax layer, and finally the action (weight) is output. In the Critic decoder, the CNN is followed by a fully-connected layer, and a dedicated fully-connected layer encodes the action (weight). The output of the state and the output of the action are combined in the fully-connected layer to produce the Q- value. For the Policy Gradient agent, since Gate Recurrent Unit (GRU) network is robust in handling sequential data [22], we choose GRU as its state encoder for time series feature extraction, which is then output to two fully-connected layers and followed by a Softmax layer. The output is the probability of the policy, and finally a policy is randomly selected as the action (signal)."}, {"title": "V. EXPERIMENTS", "content": "In our experiment, we selected five popular Forex products to comprise our portfolio: CNYUSD, EURGBP, EURJPY, EU- RUSD, and GBPJPY. The datasets for the five products were from Yahoo Finance, a widely used financial data platform. Each dataset has 2048 trading days, from February 2014 to December 2021. The first 80% of the data was set for training our model and the rest 20% was used as a test. In the back- testing, the initial portfolio value $p_0 = 100,000$, and the trading commission was 0.1%. The size of the sliding window of the state space was set to n = 3."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed and implemented a multi-agent portfolio trading system based on the Augmented DDPG and quantum finance-based trading strategies. The experiment result reveals that our model improves the training efficiency by sharing the encoder network of Actor and Critic networks. Also, by realizing the interaction between the trading decision agent and the risk control agent, the balance between profit and risk in forex investment is found in our system.\nThrough introducing the Gini bonus of the position distri- bution to the reward function, we make the decision agent prefers to output more balanced positions. Also, the QPLs- inspired trading strategy controlled by the PG agent improves the model's ability to react to potential profits and risks by anticipating the price trend after touching QPLs."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In experiments, by conducting back-testing on the 5 popular forexes' historical data, our system outperforms all baseline models that are popular in the DPO field by 20%+ in terms of Annualized Rate of Return. This demonstrates our trading system is of significance in the pursuit of high returns and risk control in the DPO field.\nTo further enhance the risk management capabilities of our trading system, our future work will combine our trading system with the NLP models to capture news and information that can influence the forex market."}]}