{"title": "Transferable Adversarial Face Attack with Text Controlled Attribute", "authors": ["Wenyun Li", "Zheng Zhang", "Xiangyuan Lan", "Dongmei Jiang"], "abstract": "Traditional adversarial attacks typically produce adversarial examples under norm-constrained conditions, whereas unrestricted adversarial examples are free-form with semantically meaningful perturbations. Current unrestricted adversarial impersonation attacks exhibit limited control over adversarial face attributes and often suffer from low transferability. In this paper, we propose a novel Text Controlled Attribute Attack (TCA\u00b2) to generate photorealistic adversarial impersonation faces guided by natural language. Specifically, the category-level personal softmax vector is employed to precisely guide the impersonation attacks. Additionally, we propose both data and model augmentation strategies to achieve transferable attacks on unknown target models. Finally, a generative model, i.e, Style-GAN, is utilized to synthesize impersonated faces with desired attributes. Extensive experiments on two high-resolution face recognition datasets validate that our TCA2 method can generate natural text-guided adversarial impersonation faces with high transferability. We also evaluate our method on real-world face recognition systems, i. e, Face++ and Aliyun, further demonstrating the practical potential of our approach.", "sections": [{"title": "Introduction", "content": "Recent studies have shown that deep learning-based face recognition (FR) model systems are vulnerable to adversarial examples (Vakhshiteh, Nickabadi, and Ramachandra 2021; Dong et al. 2019; Zhang et al. 2022). Adding deliberately designed but imperceptible noise to a clean image can fool even state-of-the-art commercial FR models (Ali et al. 2021). This vulnerability poses a direct threat to socially critical applications such as customs inspection and mobile device face identification. Consequently, the security community has increasingly focused on studying adversarial examples to improve the robustness and generalization ability of existing FR systems.\nEarly well-studied works focus on norm-constrained attacks, where the adversarial image lies within an \\u03f5-neighborhood of a real sample using the Lp distance metric to evaluate the strength of the adversarial example (Szegedy et al. 2013; Wang et al. 2021). Common values for p include 0, 2, and \u221e. With a sufficiently small \\u03f5, the adversarial image is quasi-indistinguishable from the natural sample. Such norm-based attacks have demonstrated outstanding adversarial performance against face recognition (FR) systems. However, there are limitations: 1) Despite being designed to be indistinguishable, norm-based attacks may still contain visible perturbations, making them detectable by human eyes or specially designed detectors (Massoli et al. 2021); 2) Numerous adversarial defense methods have been introduced to counter norm-based attacks, leading to a relatively low attack success rate against real-world FR models (Madry et al. 2017).\nRecently proposed unrestricted adversarial attack (UAA) methods generate adversarial images with more stealthy and semantically meaningful perturbations compared to noise-based adversarial attacks. Some unrestricted adversarial attacks hide the adversarial perturbations as decorative accessories like glasses (Sharif et al. 2016) or hat (Komkov and Petiushko 2021) to improve stealthiness. Notably, makeup-based adversarial attacks (Yin et al. 2021) generate perturbations as natural makeup. Moreover, (Li et al. 2021) generate more natural face images than previous methods with the help of pre-trained GANs. Adv-Attribute (Jia et al. 2022) automatically injects a pre-defined pattern from a target image to complete an adversarial edit. Adv-Diffusion (Liu et al. 2024) extracts latent codes from both source and target images, then exploits a diffusion model to generate adversarial faces. Although these UAA methods demonstrate improved stealthiness, they have limited ability to change attributes and primarily edit a pre-defined set of semantic information from the target image. We argue that it is essential to rapidly generate adversarial images with specific attributes, such as skin color, expression, or hairstyle, guided by attribute text. Such adversarial attacks can help security researchers expose vulnerabilities in existing FR models due to changes in facial attributes that are likely to occur.\nNotwithstanding their effectiveness in attacking face"}, {"title": "Related Work", "content": "recognition (FR) systems, these adversarial attacks have significant limitations. Although some previous works achieve relative transferability in black-box scenarios, they still struggle to attack FR models in real-world scenarios. Specifically, works such as Adv-Attribute (Jia et al. 2022) and Adv-Diffusion (Liu et al. 2024) tend to generate impersonated faces optimized for a specific model, leading to overfitting to the source model. This limitation drives us to develop a more transferable attack that can generalize well to real-world FR systems.\nTo address the aforementioned shortcomings, this paper proposes a Text Controlled Attribute Attack (TCA\u00b2) to generate adversarial perturbations guided by text prompts as shown in Fig. 1. By feeding the targeted image into the FR model, a discriminative category-level softmax vector is produced to guide the impersonation attack. To enhance black-box transferability, we employ both data and model augmentation strategies. For data augmentation, we adopt simple random resizing and padding. For model augmentation, we apply a meta-learning paradigm to simulate white-box and black-box FR environments, further improving transferability. The framework of our TCA2 is shown in Fig. 2.\nOur contributions can be summarized as follows:\nWe propose a novel Text Controlled Attribute Attack (TCA\u00b2) to generate semantically meaningful perturbations guided by text prompts. Significantly, unlike existing unrestricted adversarial attacks (UAA), our TCA2 offers rich face attribute editing capabilities under text guidance;\nBoth data and model augmentation techniques are employed to generate adversarial images that are more transferable to unknown black-box face recognition (FR) models;\nExtensive experiments validate the superior effectiveness and transferability of our method compared to other state-of-the-art attack techniques on two high-resolution datasets.\nNorm-based Adversarial Examples\nMany adversarial attack algorithms (Ryu, Park, and Choi 2021) have demonstrated that deep learning face"}, {"title": "Method", "content": "recognition (FR) models are vulnerable to adversarial samples. Traditional adversarial examples against FR focus on norm-constrained conditions. For a given FR model $F(x): X \\rightarrow R^d$ and a face image $x \\in R^n$, the adversarial image $\\hat{x} \\in R^n$ satisfies the condition $|x - \\hat{x}|_p < \\epsilon$ and $F(x) \\neq F(\\hat{x})$. Common values for p are 0, 2, and \u221e, and \\u03f5 is a sufficiently small value to ensure the perturbation is imperceptible. Adversarial attacks against FR models can be categorized as impersonation (targeted) attacks and dodging (untargeted) attacks based on whether their goal is to make the FR classify the adversarial face image as a specified \\hat{y} or any \\hat{y} \\neq y. Representative norm-based methods include the Fast Gradient Sign Method (FGSM) (Goodfellow, Shlens, and Szegedy 2014), which uses a first-order approximation of the function for faster adversarial example generation, and Projected Gradient Descent (PGD) (Madry et al. 2017), an iterative variant of FGSM that provides a strong first-order attack through multiple steps of gradient ascent. Carlini and Wagner (C&W) (Carlini and Wagner 2017) proposed stronger optimization-based attacks for L0, L2, and L\u221e via improved objective functions. AdvGAN (Xiao et al. 2018) proposes a GAN network to efficiently generate adversarial examples. These methods can easily fool victim neural networks. However, norm-based adversarial examples can still be detected by humans or adversarial detectors (Massoli et al. 2021). Consequently, several defense mechanisms against such attacks have been proposed, such as adversarial training (Madry et al. 2017).\nUnrestricted Adversarial Attack\nTraditional adversarial perturbations are constrained by norm bounds, whereas unrestricted adversarial attacks (UAA) are not subject to such limitations. These attacks have been extensively studied in image classification tasks (Sharif et al. 2016; Brown et al. 2017; Karmon, Zoran, and Goldberg 2018). UAA generates adversarial images with semantically meaningful perturbations compared to noise-based adversarial attacks. Some UAAs have been proposed by generating adversarial wearable accessories like glasses (Sharif et al. 2016) or hat(Komkov and Petiushko 2021) to fool the FR model."}, {"title": "Problem Formulation", "content": "The objective of adversarial face attacks is to deceive the target face recognition (FR) model using adversarial perturbations. Specifically, an impersonation attack seeks to cause the FR model to misclassify a face as another specific identity by introducing subtle perturbations. Most prior studies have learned these perturbations under norm constraints to ensure stealthiness. In our work, we relax these strict constraints to explore unconstrained adversarial attacks (UAA).\nLet $x \\in \\mathcal{X} \\subset \\mathbb{R}^n$ denote the given source face image, and let $x_t \\in \\mathcal{X} \\subset \\mathbb{R}^n$ denote the target face image to be impersonated. Let $F(x) : \\mathcal{X} \\rightarrow \\mathbb{R}^d$ be a face recognition model that extracts the normalized facial feature representation for identification. The optimization process of the impersonation face attack can be expressed as follows:\n$\\operatorname{argmin}_{\\hat{x}_s} S(F(\\hat{x}_s), F(x_t))$\n(1)"}, {"title": "Text Controlled Attribute Attack", "content": "$\\operatorname{argmin}_{\\hat{x}_s} D(\\hat{x}_s,x_s)$\n(2)\nwhere $S(.)$ denotes to the identity similarity, D represents the perceptual distance used in unrestricted adversarial attacks. We adopt the most common perceptual network LPIPS(Zhang et al. 2018) to measure the difference between the clean face image $x_s$ and its corresponding adversarial image $\\hat{x}_s$. A natural language text prompt $t$ is adopted to control the generation of $\\hat{x}_s$ according to the intent of adversary.\nThe objective of our approach is to generate a high-quality adversarial face, denoted as $\\hat{x}_s$, which closely resembles its original image except for the attribute controlled by the text prompt t. Additionally, $\\hat{x}_s$ is designed to effectively mislead the black-box face recognition system, causing it to misidentify is as $x_t$.\nPreliminaries\nA latent code in the style space of StyleGAN (Karras et al. 2020) can be projected into a specific image. Following the approach of (Li et al. 2021), we manipulate in the latent space of StyleGAN directly. Let GL denote the generator network with L layers in StyleGAN. The random noise z is sampled from a uniform distribution Z and then transformed into a style vector w via a nonlinear mapping network f. The intermediate latent code w consists of L copies, i.e., $\\omega = [w_1, w_2, ..., w_L] \\in \\mathbb{R}^{L \\times 512}$. Each $w_m$ within w represents the latent code input to the $L_m$ layer of GLm. This $w_m$ is projected into the Lm layers and controls the mth level of style in the synthesized image, where $m \\in {1, 2, ..., L}$. The corresponding attribute at the mth level varies with changes in the value of $w_m$. It is important to note that $w_m$ at different depths influences generated attributes to varying degrees: shallow layers control coarse attributes, middle layers control intermediate attributes, and deep layers control fine attributes. This impact is further illustrated in the Supplementary Materials.\nIn addition to the latent code w, a noise term $\\eta$, also sampled from the uniform distribution Z, is introduced to control the stochastic variations of the generated image at each layer. The noise term \u03b7 typically affects uncorrelated attributes, such as the fine details of hair strands in a generated face. Since w is entangled with semantically meaningful attributes, this work aims to control w with a text prompt t to generate the desired adversarial image capable of fooling the target FR."}, {"title": "Text-controlled Adversarial Face Generation", "content": "Previous works (Jia et al. 2022) have introduced semantically meaningful perturbations to create transferable adversarial examples against face recognition (FR) systems by injecting specific styles or patterns from a target image. However, these methods face two significant limitations: 1) Inability to control the adversarial attributes. While a real attribute vector corresponding to specific styles, such as smiling or glasses, is provided to control the generation details, the process automatically injects a pre-defined pattern from the target image. This means that the attacker cannot control the type of injected attribute, nor can they introduce a pattern outside the predefined attribute candidates. This limitation severely restricts the adversary's ability to generate the desired adversarial face. 2) Low adversarial transfer-ablity. The semantically meaningful adversarial perturbations (Jia et al. 2022; Qiu et al. 2020; Liu et al. 2024) are optimized based on a single target FR model. As a result, the generated adversarial examples are highly coupled with the white-box FR model, which significantly reduces their transferability when used to attack black-box FR models with different architectures and parameters. Our work addresses these two challenges by focusing on enhancing both the control over adversarial attributes and the transferability of the generated adversarial faces.\nText-controlled Adversarial Face Generation\nOur text-controlled adversarial face generator (illustrated in Fig. 2) leverages the robust joint multimodal representation capabilities of the vision-language pre-trained model, specifically CLIP (Radford et al. 2021). Given a text prompt t, the CLIP textual encoder $CLIP_t$ projects it into a shared embedding space as $E_t = CLIP_t(t)$, where $E_t$ represents the textual embedding of the prompt t. For a clean face image xs, a StyleGAN inverter network Inv(\u00b7) converts it into the corresponding style latent code, denoted as $w_s = \\operatorname{\u0399nv}(x_s)$. To increase diversity, we apply random resizing and padding operations as data augmentation to the target face image. The augmented target face is then fed into FR to generate a softmax vector v, which guides the generation of the adversarial face image. Subsequently, the textual embedding Et, latent code ws, and target face representation v are concatenated and fused. This process can be formalized as follows:\n$\\omega^* = \u039c_{\\Theta} ([\\omega_s, E_t, v])$\n(3)\nwhere $M_{\\Theta}$ is a Multi-Level Fusion Network with learnable parameter $\\Theta_M$. Then the adversarial image is generated by $\\hat{x}_s = G_L(\u03c9^*)$.\nAdditionally, we aim to align the adversarial face image with a controlling natural language prompt. For an image $\\hat{x}_s$ guided by the text prompt t, our goal is for the image is to exhibit the attributes described by the text prompt t. Specifically, we use CLIP to bridge the gap between the text prompt t and the image ts. The textual guidance loss is defined as follows:\n$\\mathcal{L}_{guide} = CLIP(\\hat{x}_s, t)$\n(4)\nwhere CLIP(,) represents a pre-trained vision-language model. Additionally, apart from the attribute specified in the text prompt t, we aim to preserve all other attributes in the adversarial face image. This is similar to norm-based attacks, where the goal is to minimize the pixel-level differences between the clean image and the adversarial one. We apply the same principle to maintain minimal perceptual changes. The perception preservation loss is defined as follows:\n$\\mathcal{L}_{perc} = D(x_s,\\hat{x}_s)$\n(5)"}, {"title": "Experimental Results", "content": "where D represents a perceptual network pretrained using LPIPS. Our ultimate goal is for the adversarial face image $\\hat{x}_s$ to effectively deceive the face recognition model F. Specifically, in the context of an impersonation attack, we aim for the similarity scores between $\\hat{x}_s$ and the target image xt to be higher than those of other pairs. In our approach, we use the cosine similarity loss as our adversarial impersonation loss, defined as follows:\n$\\mathcal{L}_{adv} = cos(F(\\hat{x}_s), F(x_t))$\n(6)\nwhere cos() is the cosine similarity function. Finally, combining the three loss functions, we have\n$\\mathcal{L}_{impe} = \\lambda_{guide}\\mathcal{L}_{guide} + \\lambda_{perc}\\mathcal{L}_{perc} + \\mathcal{L}_{adv}$\n(7)\nwhere $\\lambda_{guide}$ and $\\lambda_{perc}$ are hyperparameters that balance the contributions of the respective loss terms. Here, $\\mathcal{L}_{adv}$ represents the adversarial objective as defined in Eq. 6. Meanwhile, $\\mathcal{L}_{guide}$ and $\\mathcal{L}_{perc}$ correspond to the text-guided control and the preservation of other attributes in Eq. 4 and Eq. 5 respectively.\nEnhance the Transferablity of Target Adversarial Face Previous impersonation attacks(Jia et al. 2022; Liu et al. 2024) generate adversarial face images by optimizing a specific white-box surrogate face recognition (FR) model. This approach inevitably leads to overfitting to the white-box model, resulting in poor attack performance when targeting a black-box FR model.\nTo address this issue, we adopt both data and model augmentation techniques to enhance the transferability of our TCA2 method: 1) Data Augmentation. Previous works (Xie et al. 2019; Xiong et al. 2022) have demonstrated that data augmentation strategies help prevent adversarial examples from overfitting to specific data patterns. We employ this strategy to improve the generalization of the target identity. Specifically, we apply an input transformation T() with a stochastic dropout component, following the approach used in DIM (Xie et al. 2019). In our TCA2, this involves applying random resizing and padding operations on the target face xt; 2) Meta Learning. To mitigate overfitting to a specific surrogate model, some works (Liu et al. 2017; Gubri et al. 2022) have proposed enhancing the white-box model to improve generalization. Notably, model ensemble methods (Liu et al. 2017) simply aggregate losses from multiple models. However, obtaining multiple models can be challenging, and ensemble methods still tend to overfit to the combined white-box models. Inspired by recent adversarial research (Fang et al. 2022), we employ meta-learning to simulate both white-box and black-box environments. Specifically, given a total of T+1 FR models, we randomly select T models for the meta-train set and 1 model for the meta-test set. In each iteration, the meta-train and meta-test sets are reshuffled from the T+1 FR models. We first evaluate Eq. 6 on the meta-test set, then jointly optimize the current loss using the ensemble losses from both the meta-train and meta-test sets. The meta-learning details of our TCA2 approach are provided in the Supplementary Materials."}, {"title": "Implementation Settings", "content": "In our experiments, we utilize StyleGAN2, pretrained on the FFHQ face dataset, as our generative model. For adversarial text guidance, we employ the CLIP model, which is pretrained on the WIT dataset. For GAN inversion, we adopt the BDInvert method (Kang, Kim, and Cho 2021). We collect 18 text prompts representing diverse facial styles for the text guidance (details provided in the Supplementary Material). For training optimization, we use the Adam optimizer with \u03b21 set to 0.9, \u03b22 set to 0.999, and a learning rate of 0.01. The training process is run for 50 epochs. We set the values of $\\lambda_{guide}$ and $\\lambda_{perc}$ to 0.5 and 0.05, respectively. All experiments are conducted using PyTorch on a V100 GPU with 32 GB of memory.\nDataset We conducted experiments using two publicly available facial datasets: (1) the CelebA-Identity dataset (Na, Ji, and Kim 2022), which is a subset of the CelebA-HQ dataset (Huang et al. 2018) comprising 307 identities. Each identity includes at least 15 facial images, totaling 5,478 images, all at a resolution of 1024\u00d71024 pixels. (2) The KID-F dataset\u00b9, also known as the K-pop Idol Dataset - Female (KID-F), consists of approximately 6,000 high-quality facial images of Korean female idols. For our experiments, we selected about 2,000 images representing 100 identities from the KID-F dataset. We randomly chose 1,000 images from different identities as source images from both datasets. Additionally, five images were selected as target facial images in each dataset.\nAttacked Threaten Model To validate our proposed impersonation attack against face recognition, we trained models on the two aforementioned datasets. Specifically, well-pretrained MobileFace (Chinaev, Chigorin, and Laptev 2018), IRSE50 (Hu, Shen, and Sun 2018), IR152 (Deng et al. 2019), and FaceNet (Schroff, Kalenichenko, and Philbin 2015) were fine-tuned on the CelebA-Identity and KID-F datasets. All FR models aligned the input face images via MTCNN (Zhang et al. 2016) during the preprocessing step."}, {"title": "Experimental Results", "content": "where 1, denotes the indicator function, $\\hat{x}_s$ and $x_t$ represent the generated adversarial face and the target face, respectively. Threshold \u03c4 is set as 0.01 FAR (False Acceptance Rate), and N is the number of images. ASR measures the proportion of source-target pairs whose similarity scores exceed out of all source-target pairs. More details can be found in the Supplementary Material.\nAdditionally, we report the PSNR, SSIM (Wang et al. 2004), and FID (Heusel et al. 2017) scores to evaluate the imperceptibility of TCA\u00b2. Higher PSNR and SSIM scores indicate greater similarity to the original images, while a lower FID score suggests more realistic images.\nBaseline methods We compare our TCA2 approach with recent noise-based and unrestricted adversarial attacks against face recognition. The noise-based methods include MI-FGSM (Dong et al. 2018), and PGD (Madry et al. 2017). Unrestricted adversarial attacks include Adv-Makeup (Yin et al. 2021), Adv-Hat (Komkov and Petiushko 2021), Adv-Attribute (Jia et al. 2022), (Li et al. 2021), Latent-HSJA (Na, Ji, and Kim 2022), and Adv-Diffusion (Liu et al. 2024). Unlike traditional norm-based methods, which strictly ensure that perturbations do not exceed a set boundary, unrestricted adversarial attacks do not provide a strict guarantee that the perturbations will stay within the attribute bounds. All experimental settings closely follow those described in the original papers. Additional information is provided in the Supplementary Materials."}, {"title": "Image Quality Assessment", "content": "Evaluation Metrics Following (Deb, Zhang, and Jain 2020), we used the attack success rate (ASR) to evaluate our proposed TCA\u00b2. The ASR is defined as the proportion of adversarial faces that are misclassified by the face recognition model. The ASR for an impersonation attack is formulated as follows:\n$ASR = \\frac{\\sum_{i=1}^N 1(cos [F(\\hat{x}_s), F(x_t)] > \\tau)}{N} \\times 100\\%$ (8)"}, {"title": "Appendix", "content": "Face.4. Style text prompt: As illustrated in Fig. 5, without the guidance of a text prompt, the generated image tends to introduce globally visible perturbations aimed at optimizing the adversarial objective. These global perturbations have two main consequences: first, they can result in the generated image appearing unnatural; second, they limit the ability to offer users the option to select a desired style attribute compared to the clean image.\n1https://github.com/PCEO-AI-CLUB/KID-F\nMeta-Optimization: The parameter \u0398M is ultimately updated during both the meta-train and meta-test stages as follows:\n$\\Theta_M^* = \\operatorname{argmin}_{\\Theta_M} \\lambda_{guide}\\mathcal{L}_{guide} + \\lambda_{perc}\\mathcal{L}_{perc}+$\n$\\sum_{i=1}^T(L_i(\\Theta_M) + L_{te}^*(\\Theta_M^i))$\n(14)\nwhere the last term denotes the aggregation of losses from the meta-train and meta-test stages.\nThe Eq. 14 can be optimized using the gradient descent method, as described in Alg. 1.\nAlgorithm 1 The algorithm of TCA2 against FR in impersonation setting.\nInput: Benign source face image xs and impersonated target face xt, desired style text prompt t, generative model GL, FR models F = F1, ..., FT, CLIP textual encoder CLIPt, GAN inverter Inv and maximum epoch numbers K.\nOutput: Adversarial face image with attribute in prompt t.\nInitialization: Initialize model parameters \u0398M, MLFE and trade-off weights \u03bbguide, \u03bbperc.\nfor every k in K do\nCompute Lguide and Lperc;\nMeta-Train:\nRandom select T models from F as meta-train models;\nCompute Lor with Eq. 10 for the ith model and then update OM with Eq. 12;\nMeta-Test:\nUse the left FR as meta-test model;\nCompute Lte with Eq. 13;\nMeta-Optimization:\nwith Eq. 14;\nend for\nreturn \u0398M*.\nImplementation details\nText prompts\nWe have collected 18 style text prompts from the Internet to guide the adversarial example generation. Details and the text prompts we used are provided in the Table 2. We also report the result of the attack success rate with different facial attributes in Fig. 8.\nThreshold \u03c4\nIn Sec.4.1.4, \u03c4 indicates the threshold, ASR measures the proportion of source-target pairs whose similarity scores exceed out of all source-target pairs. In our experiment, threshold value\u03c4 at 0.01 false acceptance rate for four FR models i.e, MobileFace (0.302) (Chinaev, Chigorin, and Laptev 2018), IRSE50(0.241) (Hu, Shen, and Sun 2018), IR152(0.167) (Deng et al. 2019),\nWith each"}]}