{"title": "Molecular topological deep learning for polymer\nproperty prediction", "authors": ["Cong Shen", "Yipeng Zhang", "Fei Han", "Kelin Xia"], "abstract": "Accurate and efficient prediction of polymer properties is of key importance for\npolymer design. Traditional experimental tools and density function theory (DFT)-\nbased simulations for polymer property evaluation, are both expensive and time-\nconsuming. Recently, a gigantic amount of graph-based molecular models have\nemerged and demonstrated huge potential in molecular data analysis. Even with\nthe great progresses, these models tend to ignore the high-order and mutliscale\ninformation within the data. In this paper, we develop molecular topological deep\nlearning (Mol-TDL) for polymer property analysis. Our Mol-TDL incorporates\nboth high-order interactions and multiscale properties into topological deep learn-\ning architecture. The key idea is to represent polymer molecules as a series of\nsimplicial complices at different scales and build up simplical neural networks\naccordingly. The aggregated information from different scales provides a more\naccurate prediction of polymer molecular properties.", "sections": [{"title": "1 Introduction", "content": "Polymers are indispensable for our daily life and continues to drive innovation across industries, from\npackaging to healthcare and beyond [1, 2]. The development of functional polymers has promoted\nthe usage of polymers to a new level, that is not only within industrial and agricultural production, but\ngoes beyond to healthcare, i.e., treating cancer and genetic diseases [3, 4]. Advanced polymer design\nand discovery will be the solutions to long-time problems in food, energy, environment, resources\nand other fields. Polymer informatics [5\u20137] is to study polymer properties computational tools. Its\ncombination with artificial intelligence (AI) models has demonstrated great potential in polymer\nproperty predictors [8-16].\nPolymer AI models can be classified into two general types, i.e., fingerprint-based machine learning\n[17-19] and end-to-end deep learning [20, 21]. The first type is to represent chemical structures\nand physical properties as fingerprints/descriptors, which are then combined with machine learning\nmodels, such as random forest, gradient boosting tree, neural network, etc [17-19]. This type\nof model is effective and very suitable for small or median-sized dataset. The second type is\ndeep learning models, which are free from handcraft features. In particular, the combination of\nsequence representations, for instance, simplified molecular-input line-entry system (SMILES), with\ndeep learning models, such as BERT [22], ROBERTa [23], GPT [24], ELMO [25] and XLM [26],\nhas begun to show enormous power [20, 21]. The Transformer model and its variants have been\nused to predict the properties of polymers. Among them are TransPolymer [21] and polyBERT\n[20], which use Transformer and BERT models respectively to analyze polymer SMILES sequence"}, {"title": "2 Related Works", "content": "information. Further, geometric deep learning models, in particular Graph Neural Networks (GNNs),\nhave significantly improved the prediction of polymer properties by capturing molecular spatial\nstructure information [22-26, 20, 21, 27\u201332, 10, 33].\nMore recently, topological deep learning (TDL) has been proposed a promising tool for analyzing data\nwith complicated topological structures [34\u201336]. Different from traditional graph neural networks,\nTDL employs special data representations, including hypergraph, simplicial complexes, cellular\ncomplexes, and combinatorial complex, and novel message-passing modules (between simplices of\nsame/different dimensions) [37, 38]. It shows huge potential in the characterization of more complex\nmolecular structures and interactions, and for a better prediction of molecular properties [39\u201343].\nInspired by TDL, we propose molecular topological deep learning (Mol-TDL) for polymer property\nprediction. Mol-TDL models the polymer monomer molecule by a series of simplicial complexes at\ndifferent scales (through a filtration process), which characterize both higher-order and multiscale\ninteractions within the data. For each simplicial complex generated at a particular scale, a simplicial\nneural network is employed. These simplicial neural networks from different scales, for the same\npolymer monomer molecule, are then aggregated together for the prediction of polymer property.\nFurther, we develop a multiscale topological contrastive learning model and use it for (self-supervised)\npre-training of simplex based message passing. It has been shown that our Mol-TDL model can\nachieve the-state-of-the-art in predicting polymer properties on well-established benchmark datasets."}, {"title": "2.1 Graph-based models for polymer property prediction", "content": "Graph-based models have been proven to be efficient and powerful in capturing molecular structure\ninformation and predicting molecular properties. Especially, the development of graph neural\nnetworks has enabled researchers to obtain more effective molecular representations and greatly\nimprove the performance of molecular property prediction. Among them, GEM [27], Mol-GDL [28],\nHM-GNN [29], KCL [30] and MolCLR [31] are typical representatives of using GNNs to predict\nmolecular properties. Recently, GNNs have been used for the polymer data analysis. For instance,\nGraph Convolutional Neural Networks (GCNN) has been used in the prediction of dielectric constant\nand energy bandgap of polymers [32]. The wDMPNN model uses graph-based representation of\npolymer structure and weighted directed message passing architecture for polymer property prediction\n[10]. polyGNN combines graph neural network, multitask learning and other advanced deep learning\ntechniques for predicting polymers property [33]."}, {"title": "2.2 TDL-based models", "content": "Topological Deep Learning (TDL) [34, 35] leverages novel topological tools to characterize data\nwith complicated higher-order structures. Different from graph-based data representation, TDL uses\ntopological representations from algebraic topology, including cell complexes [36, 39, 40], simplicial\ncomplexes [35, 44], sheaves [45, 46], combinatorial complexes [34], and hypergraphs [41\u201343], to\nmodel not only pair-wise interactions (as in graphs), but also many-body or higher-order interactions\namong three or more elements. In fact, these algebraic topology-based molecular representations\nhave already achieved great success in molecular data analysis, including protein flexibility and\ndynamic analysis [47, 48], drug design [49], virus analysis [50], materials property analysis [51, 52].\nFurther, TDL uses a generalized message-passing mechanism thus enables the communication of\ninformation from simplices of different dimensions. In contrast to GNNs, where information is\npassing among nodes or edges, TDL allows information to propagate through any neighborhood\nrelation [36, 53, 35, 34, 54]."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Multiscale topological representation for polymer molecule", "content": ""}, {"title": "3.1.1 Simplicial complex representation for polymer molecule", "content": "A simplicial complex is the generalization of a graph into its higher-dimensional counterpart. The\nsimplicial complex is composed of simplexes. Each simplex is a finite set of vertices and can"}, {"title": null, "content": "be viewed geometrically as a point (0-simplex), an edge (1-simplex), a triangle (2-simplex), a\ntetrahedron (3-simplex), and their k-dimensional counterpart (k-simplex). More specifically, a k-\nsimplex $\\sigma^k = \\{v_0, v_1, v_2,\\cdots, v_k\\}$ is the convex hull formed by k + 1 affinely independent points\n$v_0, v_1, v_2, ..., v_k$ as follows\n$\\sigma^k = \\{\\sum_{i=0}^{k} \\lambda_i v_i | \\sum_{i=0}^{k} \\lambda_i = 1; \\forall i, 0 \\le \\lambda_i \\le 1\\}$\nThe ith dimensional face of $\\sigma^k$ (i < k) is the convex hull formed by i + 1 vertices from the set of k +\n1 points $v_0, v_1, v_2,\\cdots, v_k$. The simplexes are the basic components for a simplicial complex.\nA simplicial complex K is a finite set of simplexes that satisfy two conditions. First, any face of a\nsimplex from K is also in K. Second, the intersection of any two simplexes in K is either empty\nor a shared face. A kth chain group $C_k$ is an Abelian group of oriented k-simplexes $\\sigma^k$, which\nare simplexes together with an orientation, i.e., ordering of their vertex set. The boundary operator\n$\\partial_k : (C_k \\to C_{k-1})$ for an oriented k-simplex $\\sigma_k$ can be denoted as\n$\\partial_k \\sigma^k = \\sum_{i=0}^{k}(-1)^i [v_0, v_1, v_2, \\cdots, \\hat{v_i}, \\cdots, v_k].$\nHere, $[v_0, v_1, v_2, \\cdots, \\hat{v_i}, \\cdots, v_k]$ is an oriented (k - 1)-simplex, which is generated by the original\nset of vertices except $v_i$. The boundary operator maps a simplex to its faces, and it guarantees that\n$\\partial_{k-1}\\partial_k = 0$. To facilitate a better description, we use notation $\\sigma^{k-1} \\subset \\sigma^k$ to indicate that $\\sigma^{k-1}$\nis a face of $\\sigma^k$. For two oriented k-simplexes, $\\sigma_i^k$ and $\\sigma_j^k$, of a simplicial complex K, they are\nupper adjacent, denoted as $\\sigma_i^k \\frown \\sigma_j^k$, if they are faces of a common (k + 1)-simplex; they are lower\nadjacent, denoted as $\\sigma_i^k \\smile \\sigma_j^k$, if they share a common (k - 1)-simplex as their face. The upper degree\nof a k-simplex $\\sigma^k$, denoted as $d_U(\\sigma^k)$, is the number of (k + 1)-simplexes, of which $\\sigma^k$ is a face.\nSimilarly, we define the lower degree of $\\sigma^k$, denoted as $d_L(\\sigma^k)$, to be the number of (k - 1)-simplexes\non the boundary of $\\sigma^k$."}, {"title": "3.1.2 Filtration-based multiscale representation", "content": "A filtration process naturally generates a multiscale representation [55]. The filtration parameter,\nwhich is key to the filtration process, is usually chosen as sphere radius (or diameter) for point cloud\ndata, edge weight for graphs, and isovalue (or level set value) for density data. A systematical increase\n(or decrease) of the value for the filtration parameter will induce a sequence of hierarchical topological\nrepresentations, which can be not only simplicial complexes but also graphs and hypergraphs. For\ninstance, a filtration operation on a distance matrix, i.e., a matrix with distances between any two\nvertices as its entries, can be defined by using a cutoff value as the filtration parameter. More\nspecifically, if the distance between two vertices is smaller than the cutoff value, an edge is formed\nbetween them. In this way, a systematical increase (or decrease) of the cutoff value will deliver a\nseries of nested graphs, with the graph produced at a lower cutoff value as a part (or a subset) of the\ngraph produced at a larger cutoff value. Similarly, nested simplicial complexes can be constructed by\nusing various definitions of complexes, such as Vietoris-Rips complex, \u010cech complex, alpha complex,\ncubical complex, Morse complex, and clique complex. In this study, we employ the Vietoris-Rips\ncomplex to describe both the topological and geometric structure of a given point cloud. Formally,\nthe Vietoris-Rips complex, denoted as $Rip_r = Rip_r(X)$, is defined for a point cloud X and a cutoff\nvalue r, as follows:\n$Rip_r(X) := \\{\\sigma \\subseteq X \\mid \\sigma \\text{ is finite and } \\forall x_i, x_j \\in \\sigma, ||x_i - x_j|| \\le r\\}$,\nwhere $\\sigma$ represents a simplex in X, and $||x_i - x_j||$ denotes the Euclidean distance between any two\npoints $x_i$ and $x_j$ in $\\sigma$.\nIn Mol-TDL, pairwise distance between atoms is corresponding to the cutoff value r in Vietoris-Rips\ncomplex Rip. Given the significance of actual inter-atomic distances in our study, we set the\nparameter r to span from 2.0 to 4.0 \u00c5. To facilitate discretization within this range, we consider five\ndistinct Vietoris-Rips complexes $Rip_r$, for r = 2.0, 2.5, 3.0, 3.5, and 4.0 \u00c5 in our model."}, {"title": null, "content": "In the previous section, we discussed two k-simplex relations in a simplicial complex: the upper\nadjacency and lower adjacency relations. In this section, we consider these relations as interactions\nbetween k-simplexes within the mentioned Vietoris-Rips complexes Rip. Specifically, we focus\non the upper adjacency relation for all 0-simplexes. This means an interaction occurs between two\natoms if and only if their distance does not exceed r. This interaction is the same the traditional\nunderstanding of molecular graph interactions. When considering higher-order k-simplexes (where\nk > 0), we conisder their lower adjacency; that is, two k-simplexes interact if and only if their\ncommon face is a (k \u2212 1)-simplex. We use adjacency matrix $A_{r,k} = \\{A_{r,k}(i, j)\\}$ to describe this\ninteraction between k-simplexes in Rip:\n$A_{r,0}(i, j) = \\begin{cases}\n1, & \\sigma_i^{r,0} \\frown \\sigma_j^{r,0} \\ne i=j \\\\\n0, & \\text{others}.\n\\end{cases}$\nAnd for k > 1:\n$A_{r,k}(i, j) = \\begin{cases}\n1, & \\sigma_i^{r,k} \\smile \\sigma_j^{r,k} \\ne i=j \\\\\n0, & \\text{others}.\n\\end{cases}$\nwhere $\\sigma_i^{r,k}$ is the i-th k-simplex in Rip. And the notation $\\frown$ and $\\smile$ represent the upper and lower\nadjacency relation, respectively.\nAlso we consider the following diagonal matrices $D_{r,k} = \\{D_{r,k}(i, j)\\}$ for normalization:\n$D_{r,0}(i, j) = \\begin{cases}\nd_U(\\sigma_i^{r,0}), & i = j \\\\\n0, & \\text{others}.\n\\end{cases}$\nAnd for k\u2265 1:\n$D_{r,k}(i, j) = \\begin{cases}\nd_L(\\sigma_i^{r,k}), & i = j \\\\\n0, & \\text{others}.\n\\end{cases}$\nwhere $d_U(\\sigma_i^{r,k})$ and $d_L(\\sigma_i^{r,k})$ denote the upper and lower degree of $\\sigma_i^k$, $\\sigma_i^k$, respectively."}, {"title": "3.2 Multiscale topological deep learning", "content": ""}, {"title": "3.2.1 Simplex-based message passing", "content": "In Mol-TDL, Vietoris-Rips complexes Rip, is utilized to describe polymer. To be specific, we\nconsider a series of adjacent matrices $A_{r,k}$ to describe the interaction between k-simplexes in Rip.\nWe use message passing to learn the feature representation of each simplex,\n$H_{r,k}^{(l+1)} = Relu\\left(\\widetilde{D}_{r,k}^{-\\frac{1}{2}}\\widetilde{A}_{r,k}\\widetilde{D}_{r,k}^{-\\frac{1}{2}}H_{r,k}^{(l)}W_r^l\\right).$\nIn the l-th iteration, the feature matrix $H_{r,k}^{(l+1)}$ of k-simplexes is obtained by gathering neighbors\nfeature of each k-simplex. Here $\\widetilde{A}_{r,k}$ represents the sum of $A_{r,k}$ and identity matrix. $D_{r,k}$ is a\ndegree matrix, which is a diagonal matrix whose values on the diagonal are equal to the sum of the\ncorresponding rows (or columns) in $A_{r,k}$. $W_r^l$ is the weight matrix (to be learned). Computationally,\nwe usually repeat the process 1 to 3 times, and the final simplex feature is denoted as $H_{r,k}$.\nAfter message passing, the feature $H_{r,k}$ of k-simplex and its initial feature $H_{r,k}^{(0)}$ are concatenated\ntogether, and then all k-simplex features in Rip, are gathered into one feature through a pooling\nprocess,\n$f_{r,k} = Pooling\\left(\\left[H_{r,k}^{(0)}\\parallel H_{r,k}\\right]\\right),$\nwhere Pooling(\u00b7) is a pooling function applied to all row vectors of the matrix and $\\parallel$ is\nconcatenation operation.\nIn Mol-TDL, we consider five the filtration values. In this step, all features of k-simplexes in different\nfiltration values are aggregated,\n$f_k = READOUT1\\left(f_{r,k}| r = 2.0, 2.5, \\ldots, 4.0\\right).$"}, {"title": null, "content": "Then all features in different orders are systematically aggregated,\n$f = READOUT2\\left(f_k | k = 0, 1, 2\\right),$\nwhere READOUT1(\u00b7) and READOUT2(\u00b7) both denote pooling function, and we choose concate-\nnation in this study. Here f is the final representation of a polymer.\nFinally, a multiply layer perceptron (MLP) is utilized to property prediction,\n$\\hat{y} = W^1Relu\\left(W^0f + b^0\\right) + b^1,$\nwhere $W^0$ and $W^1$ are weight matrix, and $b^0$ and $b^1$ are the bias. The l\u2081 loss is implemented for\nregression tasks in Mol-TDL."}, {"title": "3.2.2 Multiscale representation from simplex messages", "content": "The initialization features of the k-simplex are crucial for predicting the properties of the polymer.\nSince the geometric meanings of k-simplex are obviously different, in Mol-TDL, different types of\nsimplex have different feature initialization method.\nFor 0-simplex, which represents atom in a molecule, we refer Mol-GDL [28] and consider a total\nof 12 types of atoms, including C, H, O, N, P, Cl, F, Br, S, Si, I and all the rest atoms as one\ntype. These atoms are chosen due to their high frequencies in the molecules in our datasets. For\nthe i-th atom, atom type $a_j$ will contribute a component $g_i(r, a_j)$ in the geometric node feature\n$g_i (r) = [g_i (r, a_1), g_i (r, a_2), ..., g_i(r, a_{12})]$. Here $g_i(r, a_j)$ means the number (or frequency) of\nall the neighboring atoms of type $a_j$ for the i-th atom. For more details, please refer to Mol-GDL\n[28].\nFor 1-simplex, which represents a bond in a molecule, we take the distance information as its initial\nfeatures and use radial basis functions (RBF) [56] to represent this feature $g_{RBF}(d, \\varepsilon, c) = e^{-\\varepsilon||d-c||^2}$,\nwhere d is distance between two nodes in a 1-simplex. Here, $\\varepsilon$ is a hyper-parameter and we set $\\varepsilon = 1$.\nc is center node and its value is chosen from $\\{0, 0.1, 0.2, \\ldots, 0.9\\}$. Thus, a 10-dimensional initial\nfeature vector, $[g_{RBF}(d, 1, c) | c = 0, 0.1, 0.2, \\ldots, 0.9]$ of 1-simplex can be obtained after inputting\na distance value d.\nFor 2-simplex, which represents a triangle within a molecule, we take into account geometric\ninformation including area of the triangle, three related angles, and gravity center of the triangle. First,\nHeron's formula is used to calculate the area of the triangle, $g_{area} = \\sqrt{p (p - e_1) (p \u2013 e_2) (p \u2013 e_3)}$,\nwhere $e_1$, $e_2$ and $e_3$ represent the length of three edges, respectively. p is half the circumference of a\ntriangle, that is $p = \\frac{1}{2} (e_1 + e_2 + e_3)$. Then, the cosines of the three angles in the triangle is utilized\nas the second part of the 2-simplex initial feature, that is $cos_1$, $cos_2$ and $cos_3$. Next, the center of\ngravity of the triangle is calculated by averaging three atom coordinates, denoted as $cen_1$, $cen_2$ and\n$cen_3$. Finally, all of this geometric information is concatenated to form a 7-dimensional initial feature\nof the 2-simplex, which is $[g_{area}, cos_1, cos_2, cos_3, cen_1, cen_2, cen_3]$"}, {"title": "3.3 Multiscale topological contrastive learning", "content": "Motivated by recent developments in contrastive learning models for molecular property prediction\n[31, 30, 57], we develop a multiscale topological contrastive learning for (self-supervised) pre-training\nof simplex-based message passing. In topological contrastive learning, pre-training is performed\nthrough maximizing the agreement between two augmented views of the same simplical complex via\na contrastive loss in the latent space. The framework consists of the following four major components:\n(1) Topological data augmentation. In graph models, graph data augmentation methods include\nnode dropping, edge perturbation, attribute masking and subgraph sampling [58]. In our Mol-TDL\nmodel, we introduce simplex-based attribute masking, which is to mask part of (initial) simplex\nfeatures to augment the data. For a given polymer, it can be represented as a series of Rip, which\nundergo topological data augmentations to obtain two correlated views $H_{r,k}^{ak}$ and $H_{r,k}^{bk}$. Here $H_{r,k}^{ak}$\nrepresents original initial features of the k-simplex at cutoff distance r, and $H_{r,k}^{bk}$ is generated by\nattribute masking. More specifically, we replace 30% of the elements in the initial simplex features\nwith random values. In contrastive learning, the pair of $H_{r,k}^{ak}$ and $H_{r,k}^{bk}$ forms a positive pair.\nIn our simplicial complex contrastive framework, the augmented views are utilized for a downstream\ntask of label prediction. Inspired by the work[59], we define the optimality of the views based on"}, {"title": null, "content": "the mutual information. The main idea is that the optimal augmented views should retain all the\ninformation from the input simplicial complex relevant to the label, and all the shared information\nbetween the views should only be task-relevant.\nProposition 1. (Optimal Augmented Views) For a downstream task T of prediction of a semantic\nlabel y, the optimal views, $(H_{r,k}^{a})^*$, $(H_{r,k}^{b})^*$, generated from the input simplicial complex Rip, are\nthe solutions to the following optimization problem:\n$(v_1^*, v_2^*) = \\arg \\min_{v_1,v_2} I(v_1; v_2)$\nsubject to\n$I(v_1; y) = I(v_2; y) = I(Rip_r; y),$\nHere $I(X; Y) := E_{p(x,y)} \\left[log \\frac{p(x,y)}{p(x)p(y)}\\right]$ denotes the mutual information of random variables X, Y;\np(x, y) is the joint probability density function of X, Y and p(x), p(y) are the marginal probability\ndensity functions of X and Y respectively. Proposition 1 reveals that the shared information shared\nbetween the optimal views is minimized (Equation (2)). The proof of this proposition is in the\nAppendix.\n(2) Simplex-based message passing encoder. A simplex-based message passing encoder (defined in\nSection 3.2.1) extracts simplicial complex representation vectors $f^a$ and $f^b$ as in Equation (4) for\naugmented simplicial complex pairs $H_{r,k}^{ak}$ and $H_{r,k}^{bk}$\n(3) Contrastive loss function. A contrastive loss function $L(\\cdot)$ is defined to enforce maximizing the\nconsistency between positive pairs $f_i^a$ and $f_i^b$ compared with negative pairs. Here the normalized\ntemperature-scaled cross entropy loss [60, 61] is utilized to calculate contrastive loss.\nDuring pre-training of simplex-based message passing, a minibatch of $M_{batch}$ polymer data are\nrandomly sampled and processed through contrastive learning, resulting in $2M_{batch}$ augmented\npolymer data and corresponding contrastive loss to optimize, where we denote j-th polymer data pair\nas $f_i^a$ and $f_i^b$ in the minibatch. Negative pairs are not explicitly sampled but generated from the other\n$2M_{batch} - 1$ augmented polymer data within the same minibatch as in [62]. Denoting the cosine\nsimilarity function as,\n$sim (f^a, f^b) = \\frac{(f^a)^Tf^b}{||f^a|| ||f^b||}$\nThe normalized temperature-scaled cross entropy loss [60, 61] for the j-th positive pair is defined as:\n$L(f_i^a, f_i^b) = -log \\frac{exp \\left(sim (f_i^a, f_i^b) /\\tau\\right)}{\\sum_{j'\\ne j}^{M_{batch}} exp \\left(sim (f_{j'}^a, f_i^b)/\\tau\\right)}$\nwhere $\\tau$ denotes the temperature parameter. The final loss is computed across all positive pairs in the\nminibatch, i.e., $\\sum_{j} L(f_i^a, f_i^b).$"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Performance of Mol-TDL for polymer property prediction", "content": "In this section, we present the comparison results between our Mol-TDL and SOTAs on different types\nof polymer data analysis. These datasets can be roughly classified into three categories: Electronic\n($E_{gc}$[8], $E_{gb}$[8], $E_{ea}$[8], $E_i$[8], $E_{crystal}$ [63], $E_{chain}$ [63], $\\Phi_{BC}$[63], $OPV_{Ave}$[64], $OPV_{Jsc}$[64],\n$OPV_{Voc}$ [64], $OPV_{Eg}$[64], $OPV_{HOMO}$ [64], $OPV_{LUMO}$)[64], Optical & dielectric($\\epsilon_0$[8], $n_c$[8])\nand Thermodynamic & physical($\\chi_c$[8]). For these datasets, we only use their repeat unit, i.e.\nmonomers, to construct simplicial complexes and predict its performance. The detailed information\nof datasets can be found in Appendix Table 1. All datasets of Mol-TDL model are split by ratio 8:1:1\nfor training, validation and test sets randomly.\nFor benchmarking, the performance of Mol-TDL is compared with five state-of-the-art poly-\nmer/molecular property prediction models: polyBERT [20], TransPolymer [7], polyGNN [33],\nMol-GDL [28] and GEM [27]. Here, Root Mean Square Error (RMSE) and R\u00b2 are used as metrics"}, {"title": "4.2 Ablation study", "content": "In Mol-TDL, instead of using only one molecular graph, a series of simplicial complexes are\nsystematically generated by selecting different filtration cutoff value $r_i$ (see Method for details).\nHere, five filtration cutoff values, r = 2.0, 2.5, 3.0, 3.5, 4.0 \u00c5, are chosen to generate the simplicial\ncomplexes  and three different order simplexes (0-simplex, 1-simplex, 2-simplex) are"}, {"title": null, "content": "chosen for constructed higher-order interactions. The simplicial complex whose chemical bond length\nis between 0 \u00c5 and 2 \u00c5 is the de facto standard of the covalent-bond-based model, and the other four\nare all constructed using covalent and non-covalent. For instance, a simplicial complex with a cut-off\ndistance of 3 \u00c5 means that the length of all the bond in this simplicial complex k are no more than\n3 \u00c5. Message passing in the model is implemented based on 0, 1, and 2-simplex interactions (See\n. For example, when considering 2-simplex interaction, messages are passed between\npairs of 2-simplexes that share this type of interaction.\nAppendix Figure 6 shows the performance of Mol-TDL models under different cutoff distances and\nsimplicial complices at different dimensions. More specifically, the \"0-simplex\" model means we only\nconsider TDL based on 0-simplex message passing. The notation of \"0,1-simplex\" model means that\nboth 0-simplex interactions and 1-simplex interactions are considered, while \"0,1,2-simplex\" model\nmeans that 0-simplex interactions, 1-simplex interactions and 2-simplex interactions are considered\nsimultaneously. It can be seen that the consideration of higher order information can always improve\nthe performance of the model, regardless of the cutoff distances. For both datasets, the performance\nof model will be enhanced when higher-order simplexes are added. The average values of the models\nfrom different cutoff distances are demonstrated in the right side of Appendix Figure 6. The same\ntrend can be observed. Further, non-covalent interactions play an important role in the performance\nof models. In fact, the best performance is not at the cut-off distance of 2\u00c5, which usually indicates\ncovalent interactions. Instead for the $\\epsilon_0$ dataset, the model has the best performance with a cut-off\ndistance of 3 \u00c5, and the performance on the simplicial complexes with a cut-off distance of 3.5 \u00c5\nand 4 \u00c5 are also better than that of the simplicial complex with a cut-off distance of 2 \u00c5. On the $E_{gb}$\ndata set, it can be seen more clearly that the prediction performance on simplicial complexx with a\ncut-off distance of 2 \u00c5 is worse than the performance on simplicial complexes with the other four\nhigher cut-off distances. The effects of non-covalent bonds and higher-order interactions on model\nperformance in other data sets are shown in Appendix Table 2 to 9.\nIn order to enable the Mol-TDL model to capture more structural information of polymers and further\nimprove the performance of the model, multiscale topological contrastive learning and large-scale\npolymer structural data is used to pre-train Mol-TDL. We randomly select 1,000,000 polymers from\nTransPolymer [7] and set the ratio of the training and validation set is 9:1 to pre-train Mol-TDL.\nAppendix Table 10 shows the difference in performance of the Mol-TDL model with and without\npre-training. It is not difficult to find that the Mol-TDL model with pre-training is better than the\nMol-TDL without pre-training as a whole. This result is more obviously reflected in $E_{ea}$, $E_i$ and $\\chi_c$.\nHowever, it is worth noting that Mol-TDL pre-training does not improve performance on some data\nsets, such as $E_{gb}$ and $E_{chain}$. We analyzed that the reason is that the amount of pre-training data\nis too small, and the $E_{chain}^{gap}$ used for fine-tuning are relatively large, so pre-training cannot provide\nmore effective information for these fine-tuning data."}, {"title": "5 Conclusion", "content": "Efficient polymer representation learning is crucial for polymer property prediction. Existing works\nthat apply graph neural network methods for polymer property prediction fail to characterize higher-\norder and multiscale information. To this end, we design molecular topological deep learning (Mol-\nTDL), which incorporates both high-order interactions and multiscale properties into topological\ndeep learning models. The key idea is to use a series of simplicial complexes at different scales\nto represent polymer monomer molecule and employ the simplex message-passing modules. In\naddition, multiscale topological contrastive learning is used to pre-train the model, thereby enriching\nthe structural information of Mol-TDL from large-scale simplicial complexes. Our Mol-TDL model\ncan achieve the state-of-the-art results in polymer benchmark datasets.\nLimitations The Mol-TDL model can be further improved by the consideration of coupling of simplex\nmessage-passing modules at different dimensions. Currently, the simplex"}]}