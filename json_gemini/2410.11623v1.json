{"title": "VidEgoThink: ASSESSING EGOCENTRIC VIDEO UNDERSTANDING CAPABILITIES FOR EMBODIED AI", "authors": ["Sijie Cheng", "Kechen Fang", "YangYang Yu", "Sicheng Zhou", "Bohao Li", "Ye Tian", "Tingguang Li", "Lei Han", "Yang Liu"], "abstract": "Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Multi-modal Large Language Models (MLLMs; Du et al., 2022; Gan et al., 2022; Tang et al., 2023) have made significant strides in conventional vision-language tasks (Alayrac et al., 2022; Driess et al., 2023; Li et al., 2023b), profoundly impacting the field of Embodied Artificial Intelligence (Embodied AI; Ahn et al., 2022; Kuo et al., 2022; Huang et al., 2023; Zitkovich et al., 2023). Training data (Sharma et al., 2018; Schuhmann et al., 2022; Lin et al., 2014; Jia et al., 2021) for predominate MLLMs are typically collected from object-centric and exocentric perspectives, mirroring the distribution of conventional vision-language benchmarks (Liu et al., 2023; Xu et al., 2023; Li et al., 2023a; Ning et al., 2023), which focus primarily on object and scene understanding. However, to be effectively applied in Embodied AI, it is crucial not only to understand the surrounding environment but also to have extensive knowledge about the relationship between \"myself\" and the environment. For example, compared to the absolute position in the whole environment (e.g., \"the microwave is in the kitchen\"), the relative position to my body is more important (e.g., \u201cthe microwave is one meter to my right\") for interaction and manipulation. Therefore, egocentric videos (Grauman et al., 2022; Damen et al., 2018), containing observations typical of third-person perspectives and additional interactions with the surrounding environment, can improve predominate MLLMs to be more general and expand their applications to the real world.\nVarious egocentric benchmarks (Cheng et al., 2024; Fan, 2019), as shown in Table 1, have emerged to evaluate the capabilities of MLLMs from a first-person perspective. For instance, EgoTaskQA (Jia et al., 2022) and EgoPlan (Chen et al., 2023c) assess the planning capabilities of MLLMs for long-horizon tasks, while EgoSchema (Mangalam et al., 2024) aims to diagnose the understanding of very long-form video. However, the absence of a comprehensive video benchmark from the egocentric perspective presents a significant challenge to the development of general foundation models. Furthermore, current benchmarks, both in task design and textual output forms, focus on traditional video question-answering settings and neglect the potential to support downstream applications in Embodied AI, such as glass devices or autonomous robots. For example, the natural language output format (e.g., \"put salmon in microwave\u201d) cannot be directly processed by robotics to take actions, whereas bounding boxes of grounded objects (e.g., \u201cmicrowave [290, 202, 835, 851]\u201d or function calls for low-level actions (e.g., \"find(microwave)\") align more closely with the input requirements of robotic control systems. Therefore, it is crucial to design suitable task formats that can be effectively applied to downstream applications in Embodied AI.\nIn this paper, we introduce VidEgoThink, as illustrated in Figure 1, a comprehensive egocentric video understanding benchmark aimed at better aligning the capabilities of MLLMs for application in Embodied AI. Due to the stratospheric demand for training data of end-to-end Vision-Language-Action models (Driess et al., 2023; Padalkar et al., 2023; Li et al., 2024a), systems in Embodied AI are always structured into specialized hierarchical components. In detail, MLLMs can perform several key functions: (1) video question-answering, the basic module to comprehend the surrounding environment and human activities, and then generate corresponding responses to specific instructions (Cheng et al., 2024; Fan, 2019; Jia et al., 2022); (2) hierarchy planning, the core component to decompose high-level instructions to mid-level sub-goals and low-level actions (Ahn et al., 2022; Huang et al., 2022b;a); (3) visual grounding, the detector module to help Embodied AI system ground complex instruction to the physical world (Gao et al., 2023a; Chiang et al., 2024; Munasinghe et al., 2023); (4) reward modeling, the auxiliary module to classify task completion and further provide feedback according to the observations (Kwon et al., 2023; Di Palo et al., 2023; Yu et al., 2023). Rather than solely considering traditional question-answering or planning tasks like previous egocentric benchmarks, we specifically design these four tasks to comprehensively evaluate the capabilities for different functions of MLLMs in Embodied AI.\nConsidering the high cost of manually labeling data for four different tasks, we design a series of automatic construction pipelines leveraging existing annotations from the Ego4D dataset (Grauman et al., 2022). we use GPT-4o, known for its superior reasoning capabilities, to generate appropriate question-answering pairs by combining our designed prompts with existing human annotations. For the reward modeling task, we further adopt clipped images from each video to generate feedback for negative instances. To ensure diversity and quality, three annotators are asked to filter the automatically generated instances. For evaluation, we extensively compare 14 MLLMs across three categories: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs perform poorly across all tasks. For ex-\""}, {"title": "2 RELATED WORK", "content": "Multi-modal Large Language Models. The advancement of large language models (LLMs; Brown et al., 2020; Ouyang et al., 2022; Wang et al., 2024) now extend into MLLMs. Visual modules, such as CLIP (Radford et al., 2021) and Q-Former (Dai et al., 2024), are integrated with pre-trained LLMs using various transition layers, equipping them with visual capabilities. From the wide selection of open-source LLMs, numerous image-based MLLMs (Chen et al., 2023b; Liu et al., 2024b; Zhang et al., 2023; Dai et al., 2024; Alayrac et al., 2022) have emerged. Moreover, the popularization of these image-based MLLMs has driven advancements in video perception. Video-based models like Video-LLaVA (Lin et al., 2023), Vision-LLaMA (Chu et al., 2024), and PandaGPT(Su et al., 2023) are capable of capturing the temporal information present in video form. In this work, we explore egocentric video understanding capabilities of MLLMs.\nVideo-Langugae Benchmarks. Numerous video-language benchmarks assess MLLMs, primarily focusing on instruction-following via visual question-answering tasks (Ning et al., 2023; Li et al., 2023d; Patraucean et al., 2023). Few benchmarks explore egocentric videos (Mangalam et al., 2024; Jia et al., 2022), like EgoTaskQA (Jia et al., 2022), EgoPlan-Bench (Chen et al., 2023c), and EgoGoalStep (Song et al., 2023). However, they often lack variety in assessed capabilities. EgoThink (Cheng et al., 2024) covers more comprehensive capabilities but uses static images. Moreover, all these egocentric benchmarks with only conventional VQA tasks neglect that the designed task format should be grounded in the potential applications. Therefore, in this paper, we focus on comprehensively exploring the capabilities for different functions of MLLMs in Embodied AI. A comparison to recent video-language benchmarks is presented in Table 1.\nEgocentric Video Datasets. Egocentric video datasets (Grauman et al., 2022; Damen et al., 2018; Pirsiavash & Ramanan, 2012; Sigurdsson et al., 2018) capture first-person interactions with environment, aiding robotic tasks and augmented reality. These datasets are often recorded via head-mounted cameras or wearable glasses. As more egocentric videos become available, specialized datasets focusing on specific aspects of ego-perspective have emerged. For instance, LEMMA (Jia"}, {"title": "3 TASK TYPES IN VidEgoThink", "content": "Given that the utilization of foundation models in Embodied Al remains an open research question, we carefully design four types of interrelated tasks for comprehensive assessment as shown in Figure 1: (i) video question-answering (Cheng et al., 2024; Fan, 2019; Jia et al., 2022), (ii) hierarchy planning (Ahn et al., 2022; Huang et al., 2022b;a), (iii) visual grounding (Gao et al., 2023a; Chiang et al., 2024; Munasinghe et al., 2023), and (iv) reward modeling (Kwon et al., 2023; Di Palo et al., 2023; Yu et al., 2023). The detailed descriptions of four different tasks are as follows."}, {"title": "3.1 VIDEO QUESTION ANSWERING", "content": "Previous evaluation studies on egocentric vision (Cheng et al., 2024) have predominantly focused on static images, constrained by the input format limitations of earlier MLLMs. However, recent advancements in API-based and video-based MLLMs (Achiam et al., 2023; Anthropic, 2024; Reid et al., 2024; Li et al., 2023c; Lin et al., 2023) have demonstrated significant progress. Since our real world is inherently dynamic and humans frequently process substantial amounts of video data, it is crucial to evaluate the video understanding capabilities of MLLMs.\nDimensions. To underscore the differences between static images and dynamic videos (Li et al., 2023d), we emphasize temporal attributes, ensuring that questions require the entire video for accurate answers rather than just a single frame. Considering the essential abilities for observing and interacting with the real world from a first-person perspective, we decompose the content of video modalities around \u201cmyself\u201d into three main elements: object, action, and scene. Furthermore, we explore a series of fine-grained dimensions from these elements as shown in Figure 2."}, {"title": "3.2 HIERARCHY PLANNING", "content": "Recently, a hierarchy planning framework (Ahn et al., 2022; Singh et al., 2023; Vemprala et al., 2024) has been proposed to combine the advantages of foundation models and traditional methods in Embodied AI. In detail, foundation models are used as the planner to decompose high-level task instructions (e.g., \u201ccook salmon"}, {"title": "3.3 VISUAL GROUNDING", "content": "While natural language is effective for human communication, it cannot be directly translated into low-level actions or grounded in the real world. Consequently, visual grounding (Peng et al., 2023; Chen et al., 2023a; Munasinghe et al., 2023) has garnered significant attention in both image- and video-based MLLMs. This task requires models to ground complex natural language descriptions or instructions in an image or video and output the corresponding pixel-level bounding boxes, masks, or frames. The bounding boxes and masks can directly identify actionable objects (Munasinghe et al., 2023; Zheng et al., 2024a), while the frames can provide sufficient spatial or temporal information for downstream tasks (Li et al., 2024c; Chiang et al., 2024)."}, {"title": "3.4 REWARD MODELING", "content": "In Embodied AI, manually designing reward functions to supervise actions is challenging due to the need for accuracy and diversity, especially for human activities. Benefiting from the large-scale Internet training corpus, foundation models can serve as reward models with built-in commonsense and reasoning capabilities. There are three primary approaches to deploying foundation models as reward models: (1) Using a sparse proxy reward function with a simple binary score (Kwon et al., 2023); (2) Computing similarity between close-vocabulary action phrases and images (Di Palo et al., 2023; Rocamonde et al., 2023); (3) Generating code to translate task semantics into combined reward functions (Yu et al., 2023; Ma et al., 2023). Considering the feasibility of targeting video data, this paper primarily focuses on the first approach."}, {"title": "4 DATA COLLECTION IN VidEgoThink", "content": "Recent releases of egocentric video datasets (Grauman et al., 2022; 2024; Huang et al., 2024) have advanced the field of Embodied AI. To ensure diversity and popularity, we use the popular Ego4D dataset (Grauman et al., 2022) to construct VidEgoThink benchmark. Ego4D-v2* contains 3,900 hours of 9,611 egocentric videos with diverse human annotations. To avoid data leakage, we select videos from the validation dataset. However, due to the video length limitations of MLLMs, the lengthy Ego4D videos, ranging from tens of minutes to over an hour, are unsuitable. Additionally, manually labeling question-answering data requires significant human effort. To address these problems, we design strategies to automatically clip the videos to appropriate lengths and generate corresponding question-answer pairs. To prevent the VidEgoThink benchmark from being compromised through prompt engineering, the detailed prompts used for automatic annotation construction will not be released. The statistics of each task in VidegoThink are presented in Table 2."}, {"title": "5 EXPERIMENTS", "content": "In this section, we mainly introduce our extensive adopted models, including API-based models, a series of open-source image-based and video-based MLLMs. The prompts for both inference and evaluation are shown in Appendix A. Furthermore, we summarize the experimental results for different tasks, and their corresponding case studies are illustrated in Appendix B."}, {"title": "5.1 MODELS", "content": "API-based Models. We conduct experiments with the representative GPT-4o (2024-05-13). Since GPT-4o does not support video input, we address this limitation and enhance methodological diversity with the following assessment scheme: (1) w/ 32 frames: Select 32 keyframes based on the video context; (2) w/ 8 frames: Select 8 keyframes with the same input format as most open-source MLLMs; (3) w/ captions: Replace 32 keyframes with its corresponding captions generated by GPT-40; (4) w/ only-qa: Input only the question without any frames or captions.\nOpen-Source Image-based MLLMs. We select several image-based MLLMs that demonstrated strong performance in EgoThink (Cheng et al., 2024). Below, we provide a brief introduction and present their detailed component information in Table 3."}, {"title": "5.2 RESULTS", "content": "Video Question-Answering. The results of the video question-answering task are shown in Table 4 and Table 5. MLLMs perform poorly, with a best average accuracy of 32.82% across all dimensions (35.00% for object, 28.33% for action, and 26.33% for scene elements), indicating struggles with egocentric video question-answering. GPT-40 with 8 frames performs better than with 32 frames but still underperforms compared to some open-source video MLLMs in certain dimensions. Two probable reasons are: (1) GPT-40's sensitivity to privacy policies for indoor videos, causing it to refuse more questions given more images; (2) insufficient information from extracted keyframes. GPT-40 with captions sometimes matches or surpasses the 8 or 32-frame setups in scene transitions, but performs poorly in object interaction and action sequence dimensions, indicating that captions provide sufficient high-level abstraction but lack detailed low-level action information. We regard the GPT-40 with only-qa as a baseline to demonstrate state-of-the-art performance using only question-answering pairs without any vision information. All other MLLMs perform better than the average accuracy of GPT-40 with only-qa, showing that our benchmark indeed requires vision information to solve these problems. Open-source video-based MLLMs generally surpass image-based MLLMs, highlighting the need for full video information, especially in dynamic dimensions. Among these, Qwen2-VL-7B-Instruct achieves the best performance, even surpassing GPT-40 in two dimensions and achieving the second-best performance in three dimensions.\nHierarchy Planning. The hierarchy planning results are shown in Table 5, with the average video duration being 1008.26 seconds. In the High-to-Mid task, GPT-40 series models and image-based MLLMs, which process multiple or single images, lack sufficient information to determine the entire progress and predict the next step. Hence, increasing the total number of frames significantly improves performance. For video-based models, the best performance of MiniCPM is comparable to the state-of-the-art performance of GPT-40 with 32 frames but still performs poorly, indicating significant room for improvement. For the Mid-to-Low task, the most notable phenomenon is that GPT-40 series models significantly outperform open-source MLLMs, which achieve about 0.05 accuracy. The main reason behind this phenomenon is the limited long-context capability and instruction-following capability of open-source MLLMs. We can only provide them with a compressed function document, and they often do not generate answers following the output format.\nVisual Grounding. Visual grounding tasks involve identifying specific objects, frames, or temporal segments within a video. API-based and image-based MLLMs abandon this information after extracting keyframes, necessitating the use of open-source video-based MLLMs for performance assessment. Due to the new design of object and frame grounding tasks, these MLLMs are not yet optimized for these formats, leading to generally poor performance. It is understandable that object"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce VidEgoThink, a comprehensive benchmark designed to evaluate egocentric video understanding across four critical functions in Embodied AI. Our assessment of popular API-based and open-source MLLMs reveals that these models still face significant challenges in processing egocentric videos. Although GPT series models perform relatively better, they exhibit notable deficiencies in certain areas, highlighting the need for further improvements and optimizations. VidEgoThink underscores the limitations of current MLLMs in handling first-person perspective data, thereby indicating directions for future research and advancements\nLimitations. VidEgoThink is the first to propose four tasks for assessing egocentric video understanding in MLLMs for Embodied AI. However, it has limited data diversity and immature evaluation methods, particularly in hierarchy planning and reward modeling. Future work should improve these aspects and address the high costs of human annotation and API-based evaluations, which limit the number of question-answer pairs. We plan to expand the benchmark and develop egocentric foundation models for robotics.\nBroader Impacts. Two key areas for the future of Embodied AI are egocentric video and multimodal large language models. On the one hand, our real world cannot be mapped to virtual simulators exactly the same way. Real-world environments cannot be exactly replicated in virtual simulators, making egocentric video a preferred method for collecting action data, especially with the rise of smart glasses and humanoid robots. Learning from egocentric video is crucial for future advancements. Although end-to-end MLLMs for Embodied AI are still an open research question, we believe a hierarchical system that uses vision-language models for perception and cognition is an emerging paradigm. Ideal foundation models should function in the real world, capable of thinking, understanding, and interacting like humans."}, {"title": "A PROMPT HUBS", "content": "To address concerns about potential data breaches through prompts, here we only release the detailed prompts for each task to facilitate inference and evaluation."}, {"title": "A.1 MODEL INFERENCE PROMPTS", "content": "As an example, we list the general prompts for 8 frames, 32 frames and open-source MLLMs. The inference type of \"caption\" for GPT series models will add a prompt \"Here is the captions of the video: {caption}.\" after the sentence \u201cImagine you are the camera wearer (I) who recorded the video\". For the inference type of \u201conly-qa\u201d, we delete the prompt \u201cImagine you are the camera wearer (I) who recorded the video\".\n\u2022 Video Question Answering: Imagine you are the camera wearer (I) who recorded the video. Please directly answer the question as short as possible. Question: {question} Short answer:\n\u2022 High-to-Mid in Hierarchy Planning: Imagine you are the camera wearer (I) who recorded the video. Given the high-level goal (e.g., 'making dumpling\u2019) and the current progress video, you need to predict the next mid-level step (e.g., fold dumplings on a cutting board) to achieve the goal. Please directly generate the next one step as short as possible. Question: {question} Short answer:\n\u2022 Mid-to-Low in Hierarchy Planning: Imagine you are the camera wearer (I) who recorded the video. Here are a set of actionable functions below.\n[begin of actionable function and documentation]\n{'put': 'put(<arg1>, <arg2>) is used to place an object at a specified or default location. <arg1>refers to the item to be placed, whereas <arg2>is optional and specifies the location where the item should be placed. If <arg2>is omitted, the item is placed in a generic, predefined area.',\n'grab': 'grab(<arg1>, <arg2>) is used to simulate the action of grasping or picking up objects, especially in a kitchen setting. <arg1>refers to the primary object to be grabbed, while <arg2>is optional and denotes an associated tool or container that aids in handling or processing the primary object.',\n'talk': 'talk(<arg1>, <arg2>) is used to simulate a conversation scenario with specific entities. <arg1>is mandatory and specifies the primary entity involved in the conversation, such as a 'woman', 'man', or 'person'. <arg2>is optional and typically represents a secondary entity or context within the conversation, providing additional detail or focus.',\n'close': 'close(<arg1>, <arg2>) is used to encapsulate or seal an item, either partially or completely. <arg1>refers to the object to be closed or covered, and <arg2>is optional, describing the material or object used for closing or covering <arg1>. If <arg2>is omitted, the closing is done without any specified covering.',\n'adjust': 'adjust(<arg1>, <arg2>) is used to modify the position or settings of objects or items. <arg1>is mandatory and specifies the primary object to adjust, while <arg2>is optional and used for adjustments involving a specific secondary object or location relative to the first.',\n'arrange': 'arrange(<arg1>, <arg2>) is used to organize objects systematically within a predefined space. <arg1>refers to the items to be arranged, while <arg2>is optional and specifies the area or container where these items will be organized. If <arg2>is omitted, the items are arranged in a default designated space.',\n'open': 'open(<arg1>, <arg2>) is used to manipulate the state of various containers or coverings by opening them. <arg1>refers to the primary object or container that needs to be opened, like a 'pot' or 'drawer'. <arg2>is optional and specifies a secondary descriptor or specific part of the primary object, like 'top' or 'front', indicating a particular method or area of opening.',\n'walk': 'walk(<arg1>, <arg2>) is used to move an entity towards a specified location within an environment. <arg1>refers to the primary location or object the entity should head towards, and <arg2>refers to optional additional parameters that provide extra directional or contextual details to refine the movement.',\n'empty': 'empty(<arg1>, <arg2>) is used to transfer a specified item from one holding medium to another specified container. <arg1>refers to the item being transferred, while <arg2>is the destination container where the item is moved to.',\n'move': 'move(<arg1>, <arg2>) is used to transfer items from one place to another. <arg1>refers to the item that is being moved. <arg2>is optional and specifies where the item"}, {"title": "A.2 EVALUATION PROMPTS", "content": "Here we list the prompts for API-based models to assess the performance for some tasks.\n\u2022 Video question answering: [Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. The assistant has access to an image alongwith questions but you will not be given images. Therefore, please consider only how the answer is close to the reference answer. If the assistant's answer is not exactly same as or similar to the answer, then he must be wrong. Be as objective as possible. Discourage uninformative answers. Also, equally treat short and long answers and focus on the correctness of answers. After providing your explanation, you must rate the response with either 0, 0..5 or 1 by strictly following this format: \u201c[[rating]]\u201d, for example: \u201cRating: [[0.5]]\u201d.\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]", "planning": ["Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. The assistant has access to an image alongwith questions but you will not be given images. Therefore, please consider only how the answer is close to the reference answer. The reference answer and the assistant's answer both describe a mid-level step towards completing a high-level goal, you must consider if these two mid-level steps are similar. If the assistant's answer is not exactly same as or similar to the answer, then he must be wrong. Be as objective as possible. Discourage uninformative answers. Also, equally treat short and long answers and focus on the correctness of answers. After providing your explanation, you must rate the response with either 0, 0..5 or 1 by strictly following this format: \u201c[[rating]]\u201d, for example: \u201cRating: [[0.5]]\u2019.\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]\n\u2022 Mid-to-low in hierarchy planning: [Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an Al assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. The assistant has access to an image alongwith questions but you will not be given images. Therefore, please consider only how the answer is close to the reference answer. The reference answer and the assistant's answer both describe a trajectory of low-level automic actions towards completing a mid-level step, you must consider if these two trajectories of low-level atomic actions are similar, especially the key actions to achieve the mid-level step. If the assistant's answer is not exactly same as or similar to the answer, then he must be wrong. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 0 to 10 by strictly following this format: \u201c[[rating]]\u201d, for example: \u201cRating: [[5]]\u201d.\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]\n\u2022 Feedback in reward modeling: [Instruction]\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given three reference answers and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answers. Identify and correct any mistakes. The assistant has access to an image along-with questions but you will not be given images. Therefore, please consider only how the answer is close to the reference answers. If the assistant's answer is not exactly same as or similar to all reference answers, then he must be wrong. If the assistant's answer is exactly same as or similar to any one reference answer, then it is correct. Be as objective as possible. Discourage uninformative answers. Also, equally treat short and long answers and focus on the correctness of answers. After providing your explanation, you must rate the response with either 0, 0.5 or 1 by strictly following this format: \u201c[[rating]]\u201d, for example: \u201cRating: [[0.5]]\u201d.\n[Question]\n{question}\n\n[The Start of Reference Answer]\n{ref_answer_1}\n[The End of Reference Answer]\n\n[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]"]}]}