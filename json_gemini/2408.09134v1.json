{"title": "Better Python Programming for all: With the focus on Maintainability", "authors": ["Karthik Shivashankar", "Antonio Martini"], "abstract": "This study aims to enhance the maintainability of code generated by Large Language Models (LLMs), with a focus on the Python programming language. As the use of LLMs for coding assistance grows, so do concerns about the maintainability of the code they produce. Previous research has mainly concentrated on the functional accuracy and testing success of generated code, overlooking aspects of maintainability. Our approach involves the use of a specifically designed dataset for training and evaluating the model, ensuring a thorough assessment of code maintainability. At the heart of our work is the fine-tuning of an LLM for code refactoring, aimed at enhancing code readability, reducing complexity, and improving overall maintainability. After fine-tuning an LLM to prioritize code maintainability, our evaluations indicate that this model significantly improves code maintainability standards, suggesting a promising direction for the future of AI-assisted software development.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of Code Large Language Models (LLMs) started a new era in automated programming, offering unparalleled assistance in generating syntactically correct and functionally robust code. Despite their increasing adoption as coding assis-tants [1], concerns regarding the maintainability of the code produced by these models persist [2]. Sustainable software development hinges on quality aspects like maintainability, which is crucial for the long-term success of software projects, affecting factors such as technical debt and the cost of future modifications[3]. While existing research has extensively ex-plored LLM-generated code's functional accuracy and testing efficacy, maintainability has often been overlooked [4]. This gap highlights a pressing need to re-evaluate and enhance how code is generated, emphasising its long-term sustainability and adherence to best coding practices [2].\nThe significance of code quality becomes evident when considering the concept of technical debt, which draws an analogy to financial debt. It characterises the eventual expenses incurred when opting for code that is easy to implement in the short term rather than choosing the best overall solution [5]. Within the realm of Code LLMs, creating code lacking main-tainability or conformity to established standards contributes to the accumulation of technical debt [6]. This, in turn, results in higher long-term costs associated with maintenance and scalability [7].\nIn response to this challenge, our study seeks to explore the potential of fine-tuning Code LLMs to prioritise code maintainability, specifically within the context of Python pro-gramming. By focusing on attributes such as code readability, complexity reduction, and overall maintainability, we aim to build an AI-assisted software development towards generating code that is not only functional but also easy to understand, modify, and extend.\n1) Research Questions: Central to our investigation is the inquiry into the extent to which fine-tuning LLMs can enhance their capacity to assess and improve the maintainability of Python code.\nRQ1: \"How does fine-tuning augment LLMs' capability to generate Python code with higher maintainability, and can the improvements be measured?\u201d\nThis question underpins our broader objective of setting new benchmarks for Code LLM development, ensuring that the output meets functional requirements and adheres to high standards of code quality and maintainability.\n2) Motivation: The motivation for exploring RQ1 stems from a recognised gap in the current literature concerning the maintainability of LLM-generated code [4][2]. High-quality, maintainable code significantly facilitates updates, debugging, and scalability, thus minimising long-term maintenance costs and fostering ongoing innovation [8], [9]. As Code LLMs continue to gain traction for their efficiency in streamlining coding tasks, it becomes imperative to ensure that their outputs are operationally effective and maintainable in the long run. This study addresses this critical need by fine-tuning LLMs with a specific emphasis on maintainability and conducting rigorous performance evaluations.\n3) Our contributions are twofold: :\n1) Development of a Custom Dataset: Addressing the gap identified in RQ1, we've curated a dataset focusing on the maintainability of Python code, emphasising read-ability, complexity and modifiability or ease of refactor-ing the code. This dataset is engineered for developing and fine-tuning LLMs, ensuring that generated code aligns with maintainability best practices. By making this dataset publicly available, we facilitate broader re-search and development efforts to embed maintainability principles directly within Code LLM outputs.\n2) Empirical Evaluation of Fine-tuned Model: We intro-duce an experimental method leveraging our extended custom dataset to evaluate fine-tuned LLMs on main-tainability. This method extends conventional functional"}, {"title": "II. BACKGROUND", "content": "A. Maintainability\nMaintainability is a crucial quality attribute determining the ease of understanding, modifying, and extending software [3]. It is crucial for controlling costs and ensuring the adaptability of software systems over time. Misra et al have highlighted the importance of evaluating how design and coding practices in-fluence software maintainability, which can lead to significant reductions in maintenance costs and improvements in quality [10]. Adherence to maintainable coding standards promotes code readability and consistency, which are fundamental for collaborative development efforts and the long-term viability of software projects [8], [9]. However, there is a gap in research regarding the compliance of Code LLMs with such standards, calling into question their practicality in actual soft-ware development scenarios. Addressing this gap is essential for integrating Code LLMs into the industry, ensuring that the code they generate is functional, maintainable, and aligned with professional coding practices. Our research contributes to understanding and improving code maintainability through LLMs, but it does not encompass all dimensions or metrics associated with maintainability. This acknowledgement under-scores the complexity of code maintainability and the ongoing need for research in this area.\nB. Parameter Efficient fine tuning (PEFT)\nParameter-efficient fine-tuning (PEFT) has become increas-ingly relevant in machine learning, particularly for adapting large pre-trained models to specific tasks with minimal compu-tational overhead [11]. PEFT methods have been developed to achieve strong task performance while updating a significantly smaller number of parameters compared to full model fine-tuning. This cost-effective approach addresses the challenge of making informed design choices on the PEFT configurations, such as architecture and the number of tunable parameters [11] [12].\nFurthermore, as foundation models scale, the efficiency of adaptation methods like PEFT becomes critical. Pu et al. provided a comprehensive benchmark of various PEFT techniques, offering a framework for selecting the optimal fine-tuning techniques based on task type and data availability. They also challenged the notion that PEFT techniques always converge faster than full tuning, especially in low data sce-narios, and optimised these techniques for better performance with fewer parameters [13]. PEFT with QLORA is an efficient finetuning method for large language models significantly reducing memory usage. It allows finetuning a 65B parameter model on a single GPU using innovations like 4-bit models, double quantisation, and paged optimisers [14]. The Hugging Face Supervised Fine-tuning Trainer (SFT Trainer) API allows for fine-tuning language models. It supports features like training for text completion tasks, formatting input prompts, packing datasets for efficiency, and controlling pre-trained model parameters. Advanced usage includes training adapters using the PEFT library, allowing users to train only parts of a model and share these smaller, adapted models[15]. We use SFT trainer and PEFT with QLoRA to fine-tune the open-source Code LLM model, which we discuss later in the Methodology section."}, {"title": "III. RELATED WORKS", "content": "Large Language Models (LLMs) for code continue to grow, with limited studies focusing on code maintainability. Building upon the notion of code quality, Zhuo ([16]) introduces a framework that assesses the quality of LLM-generated code. This framework works by ensuring the code aligns with human standards, In the context of code testing, Xiong, Guo, and Chen ([17]) explore how LLMs can assist in program synthesis by generating test cases. This ability to produce not only functional code but also comprehensive tests underscores the potential of LLMs to work on other qualities or attributes. Shirafuji et al. ([18]) investigate the robustness of LLMs in programming problem-solving and reveal the sensitivity of these models to prompt engineering. This insight is integral to standardisation, suggesting that with suitable prompts, LLMs could be steered to produce code that adheres to specific standards, hence promoting consistency and standardisation in coding practices. Lastly, Le et al. ([19]) proposed \"CodeRL,\" integrating a pre-trained model with deep reinforcement learn-ing for program synthesis. While their focus is on func-tional correctness, the principles underlying CodeRL could be adapted to emphasise the generation of standard-compliant code. Linking these contributions together, it becomes evi-dent that the field of LLMs for code is expanding beyond mere code generation, delving into broader aspects of code quality. However, despite these advances, there appears to be a gap in research explicitly targeting the maintainability of code generated by LLM. To our knowledge, no studies have yet explored this area in depth. Our research fills this gap, offering unique insights and methodologies for enhancing code maintainability, particularly in Python, using LLMs."}, {"title": "IV. METHODOLOGY", "content": "This research employs a systematic methodology to investi-gate LLMs' generation and evaluation of maintainable Python code. The approach is structured into several vital steps (Figure 1), each addressing the main RQ1. The methodology combines dataset preparation, model fine-tuning as shown in Figure 1, and comprehensive evaluation techniques to assess the capability of LLMs in producing code that functions correctly and adheres to high maintainability standards. We use a mixed-method approach that combines practical experimentation with qualitative analysis. This choice is driven by the need to evaluate the multifaceted nature of code maintainability, which encompasses code's syntactic and functional correctness and aspects such as readability, complexity, and adherence to best practices. The methodology is designed to be replicable and rigorous, allowing for a clear assessment of the impact of fine-tuning LLMs on code maintainability.\nComparative Analysis and Evaluation The comparative analysis aims to quantify the improvements in code main-tainability due to fine-tuning the LLM. This analysis involves comparing the maintainability metrics (SLOC, Halstead effort, Cyclomatic complexity, and Maintainability Index) of the original code in the dataset and to that code generated by the base models versus the fine-tuned models. The metrics are calculated for each code sample in the test split, providing a basis for statistical analysis to identify significant trends and improvements. The evaluation process, a crucial part of our work, is designed to validate the functional correctness and maintainability of the generated code. This involves using CodeBertScore [20] to compare the similarity and functional correctness of the code generated by the fine-tuned models against the original code samples. Additionally, the utility of the fine-tuned models is assessed through invaluable feedback from expert Python programmers, who play a pivotal role in evaluating the generated code's quality and adherence to best practices like code readability, modularity, and reusability, which are essential for reducing maintenance costs.\nA. Selecting Dataset for Maintainability\nFor finetuning Code LLMs, the initial step typically involves using various original code samples as the primary input. These code samples are from the CommitPackFT dataset (Python subset) [21]and CodeAlpaca-18k (Python) dataset [22]. This immerses the models in various authentic coding situations, with the imperfections and inconsistencies they would encounter in the practical coding environment.\na) Diversity of data and rationale for choosing the Dataset:: CommitPackFT(Python subset) [21] offers many real-world programming scenarios and high-quality instruc-tional commit messages. Its critical feature is its focus on high-quality commit messages (that serve as instruction) paired with code. This unique feature makes it particularly suitable for tasks like file-level and repository-level refactoring, which is pivotal for understanding code changes and intentions in software development.\nCodeAlpaca (Python) [22] provides a diverse set of coding examples and instructional data essential for developing mod-els that can follow instructions in code generation tasks. This dataset stands out due to its variety in code types, including snippets and functions, and its accompanying instructions and inputs. This diversity provides a rich set of coding examples, making it a valuable resource for instruction-following models. The CodeAlpaca dataset was inspired by a project aiming to build an instruction-following LLaMA model for code generation based on the Stanford Alpaca model [23].\nB. Instructing GPT4 to generate Maintainable code\nTo enhance the code maintainability of the code samples from the chosen dataset, we employ the GPT4 model, dis-tinguished for its prowess in code generation among various"}, {"title": "V. DATA AVAILABILITY", "content": "We have made the replication package publicly available at https://zenodo.org/doi/10.5281/zenodo.10153875 1. It con-tains all the model artefacts, including final model weights, checkpoints and the associated Code for training and evaluat-ing the WizardCoder and GPT 3.5 model on various metrics. The evaluation script can generate all the maintainability metrics reported in this paper. We also make all the datasets used in this paper available in this replication package. All the code is seeded to ensure reproducibility and reliability of our result. We have made our best effort to anonymise all the data and code.\n1https://zenodo.org/doi/10.5281/zenodo.10153875"}, {"title": "VI. RESULTS", "content": "A. RQ1. Fine-tuning LLM to generate Python code with higher maintainability\nThe primary aim of this research question is to critically evaluate the impact of fine-tuning LLMs on their capability to produce maintainable Python code. Following the fine-tuning phase, the model enters a crucial testing stage. In this stage, a portion of the dataset, which remained unused during the training phase (Test split), is presented to the model. This testing aims to gauge the model's ability to apply the coding pattern and characteristics it has learned to new and unseen examples of Python code. This evaluation is essential to determine whether the model can generalize its understanding of high code quality, focusing on maintainability. It also tests the model's capacity to handle various coding scenarios and complexities representing real-world programming environ-ments. This stage is vital in assessing how fine-tuning LLMS can enhance their ability to generate Python code for improved maintainability.\nThis Table V presents a simple comparative example of original and refactored implementations of the bubble sort algorithm, post-fine-tuning (FT) of a WizardCoder. The refac-toring process demonstrates an exciting use of the \"iter-tools.product\" function, a modern approach to simplifying the nested loops in the original code. It also removes a For loop from the Original code implementation. The metrics provided include SLOC, CC Score, Effort (calculated using Halstead's complexity measures), and the MI Score. The unchanged SLOC suggests that compactness was maintained, while a reduction in the CC Score from 4 to 3 indicates a simplification in control flow complexity, potentially making the code easier to understand and modify. The effort metric shows a slight decrease, suggesting the refactored code requires less cognitive effort to comprehend. The MI Score increase from 69.58 to 70.49 in the refactored code signifies an improvement in maintainability. This underscores the LLM's capability to not only learn coding patterns but also apply these insights to produce more maintainable code, illustrating the practical benefits of fine-tuning LLMs for specific programming tasks.\nThe model is fine-tuned to produce maintainable Python code, complemented by \"Changes Made\" comments (as shown in Table V ) detailing each modification and its im-pact on code quality. This approach not only enhances the maintainability of the code but also educates developers on best practices, aiding their understanding and application of writing maintainable code techniques.\na) Analysis of Distribution Box Plot: The box plot in Figures 2, 3, 4, and 5 provides a comprehensive view of the performance across the Original Dataset, Base Model and Fine-Tuned (FT) Model. The box plot visualises the distribu-tion of maintainability metrics for the Original Dataset, Base Model, and Fine-Tuned Model. Interpretation should focus on the median (central line in each box), range (interquartile range indicated by the box), and outliers (diamonds beyond the whiskers). All the metrics reported in the Tables below in this section are mean values for the entire test split. Therefore, this box plot is crucial as it illustrates the range and distribution of the maintainability metrics, including a comparison of median values and the extent of variability and outliers within this box plot, thereby complimenting the overall comparison. This visual representation is essential for understanding the full scope of the models' effectiveness and the impact of fine-tuning on code quality.\nThe Tables VI and VII focus on evaluating the effec-tiveness of fine-tuned WizardCoder13B (FT Model) to Base WizardCoder13B Model in generating maintainable Python code, using maintainability metrics from the CodeAlpaca and Commitpackft Test splits. For a more detailed comparison, we have also included the Original Dataset maintainability metrics on these dataset test splits."}, {"title": "VII. EVALUATION", "content": "A. Evaluating the functional similarity on the test split\nThe evaluation of fine-tuned models on Test split using CodeBERTScores shows high functional correctness and simi-larity between the original reference code from the dataset and the generated code from the FT models on both Commitpack and CodeAlpaca datasets.\nCodeBERTScore [20] is a metric designed to evaluate the similarity between a reference code snippet and a generated code snippet, focusing on both syntactical and functional equivalence. Unlike traditional metrics such as BLEU [42], which mainly rely on exact token matches, CodeBERTScore leverages the contextual embeddings from a model like Code-BERT [31], which is trained on both programming languages and natural language. This approach allows CodeBERTScore to understand the underlying semantics of code snippets be-yond mere lexical similarities. The key advantage of Code-BERTScore is its ability to recognize functionally equivalent code snippets that may not share a high degree of lexical sim-ilarity. For example, it can be understood that $x ** 0.5$ and $math.sqrt(x)$ performs the same operation (calculating the square root of x) despite having different tokens. This is a significant improvement over traditional metrics, ensuring that generated code is evaluated more accurately in terms of what it does rather than just how it is written.\nCodeBERTScore represents a sophisticated approach to evaluating code generation. It prioritizes the functional and semantic accuracy of the generated code over mere lexical matching. This makes it especially suitable for applications where understanding the intent and functionality of code is crucial, such as automated code review or code synthesis tasks.\nThe metrics Precision (P), Recall (R), F1 score, and F3 range from 0 to 1, where 1 indicates a perfect match, and 0 indicates no match at all between the generated code and the reference code.\nB. Evaluating and assessing the usefulness and utility\nOur evaluation method for assessing a fine-tuned AI model as a programming assistant involved a structured session where participants with varying Python expertise from both industry and academia (A total of 11 participants answered all the questions) interacted with the FT model to complete coding tasks reflective of real-world scenarios. The tasks were self-selected by participants to ensure relevance to their actual cod-ing practices. Feedback was gathered through a questionnaire focusing on the model's usefulness and effectiveness as an AI companion, rated on a 1 to 5 scale, with 1 being \"not useful at all\" and 5 being \"extremely useful\"."}, {"title": "VIII. DISCUSSION", "content": "Our fine-tuned model is designed to refine existing func-tional code and prioritizes suggestions that adhere to estab-lished coding standards and best practices rather than gen-erating new code. This tool has proven to be a valuable AI companion for code refactoring, as demonstrated by positive human evaluations.\nIntegrating this model into continuous integration and de-ployment pipelines can streamline development workflows, providing immediate feedback to mitigate technical debt ac-cumulation. Moreover, the model serves as an instructional guide for novices, promoting the development of clean and efficient code while standardizing learning through consistent feedback.\nFor the research community, it offers a platform to assess the effectiveness of automated refactoring tools and explore different refactoring strategies to enhance long-term software maintenance. This exploration of human-AI collaboration in software development could significantly elevate code quality, efficiency, and cost-effectiveness in software engineering.\nThe discussion of whether fine-tuning is required despite the existence of capable general models such as GPT-3.5 for code generation is crucial. Our study highlights that these models may not focus on nuanced aspects of code maintainability without targeted training. We have developed a dataset centred on Python programming, incorporating best practices and examples of highly maintainable code. This dataset is essential for fine-tuning other large language models (LLMs) to improve the maintainability of the code they generate. By utilising this dataset, our fine-tuned model demonstrates improved performance in code quality and maintainability. This not only enhances the model's practicality in software development but also enables the use of smaller, more efficient models. These models, potentially less costly in terms of computational resources, can achieve or exceed the capabilities of larger models like GPT-4, and offer additional security benefits for deployment in sensitive environments.\nA. Treats to validity\nThis study on fine-tuning LLMs for improving the maintain-ability of generated Python code addresses several challenges that impact its validity and reliability. A primary internal concern is whether the training dataset comprehensively rep-resents the Python ecosystem. Limited representation could cause models to overfit specific patterns, reducing their gen-eralization capability. The choice of hyperparameters in fine-tuning is also crucial, as inappropriate selections can affect the models' reliability and generalizability. Externally, the study's relevance may be limited outside the Python ecosystem. The complexity of industry-level code might not be fully repre-sented, questioning the practical utility of the models in real-world scenarios.\nIn terms of construct validity, the operational definitions of maintainability may not fully encapsulate these concepts as understood in professional and academic settings. The tools and metrics used to evaluate code maintainability could have inherent biases, potentially skewing the results. The study's conclusion validity relies on robust statistical analysis. Without this, there's a risk of misinterpreting data and drawing incorrect inferences about the impact of fine-tuning.\nReliability issues arise from potential model performance inconsistencies and the evolving nature of real-world code, known as dataset shift. These factors necessitate continuous model updates to ensure the study's relevance in the rapidly changing field of AI-driven code generation.\nOur research contributes to understanding and improving code maintainability through LLMs, but it does not encompass all dimensions or metrics associated with maintainability. This acknowledgement underscores the complexity of code maintainability and the ongoing need for research in this area."}, {"title": "IX. CONCLUSION", "content": "This study explores the effectiveness of fine-tuning Large Language Models (LLMs) for generating maintainable Python code. Utilizing our custom extended datasets and leveraging models such as WizardCoder 13B and GPT-3.5, we have achieved notable improvements in code maintainability met-rics, such as Source Lines of Code, Maintainability Index, Hal-stead Effort, and Cyclomatic Complexity. These enhancements highlight LLMs' potential as powerful tools in automating code refactoring processes, with the promise of advancing software development practices. Central to our investigation is a specially curated dataset designed with a focus on Python programming that can be used to fine-tune other LLMs and improve the generated code quality and maintainability. By examining the strengths and weaknesses of LLMs in producing maintainable Python code, this research contributes signifi-cantly to the fields of automated code generation and software maintainability."}, {"title": "Maintainbility_prompt", "content": "Maintainbility_prompt = f\"\"\"You are a Python expert specialising in code optimisation.\nYour main task is to refactor the given Python code to improve upon the listed metrics:\nSource Lines of Code (SLOC), Effort, and Maintainability Index (MI), while retaining original functionality\n### Input:\n{original_code}\n### Context:\nObjective\nImprove Source Lines of Code (SLOC): Lower numbers are generally better without compromising readability or functionality.\nImprove Maintainability Index (MI): Higher scores are desired.\nReduce Effort: Lower numbers are preferred.\nOriginal Metrics\nSource Lines of Code (SLOC)\nMaintainability Index (MI): {original_maintainability_index}\nEffort: {original_effort}\nProvide only the refactored version of the code with comments on what changes are made on the code and do not provide the metrics.\n### Response:\n{refactored_code_optimised_maintainability}*\"\"\""}, {"title": "Halstead's effort E", "content": "Halstead's effort E, the computational cost of software development, is defined as:\n$E=D\\times V$\nwith Difficulty D:\n$D = \\frac{\\eta_1}{2} \\times \\frac{N_2}{\\eta_2}$\nAnd Volume V:\n$V = N \\times log_2 \\eta$\nWhere $\\eta_1$ and $\\eta_2$ are the counts of distinct operators and operands, $N_1$ and $N_2$ are the total counts of operators and operands, and N and $\\eta$ are the sum of $N_1$ and $N_2$, and $\\eta_1$ and $\\eta_2$ respectively [32]."}, {"title": "MI", "content": "The Maintainability Index (MI) combines several metrics into a single score. High maintainability means less time and resources are needed for future modifications or debugging. It is also important to note that a Higher MI score equals higher maintainability. Radon's implementation of the Maintainability Index is still a very experimental metric. Radon derivative [32]:\n$MI = max \\left[0, 100 \\left(\\frac{171 - 5.2 \\ln(V) - 0.23G - 16.2 \\ln(L) + 50 \\sin(\\sqrt{2.4C})}{171}\\right)\\right]$\nwhere V is Halstead Volume, G is Cyclomatic Complexity, L is Source Lines of Code (SLOC), and C is the percentage of comment lines [32]."}]}