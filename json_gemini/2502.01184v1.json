{"title": "FragmentNet: Adaptive Graph Fragmentation for Graph-to-Sequence Molecular Representation Learning", "authors": ["Ankur Samanta", "Rohan Gupta", "Aditi Misra", "Christian McIntosh Clarke", "Jayakumar Rajadas"], "abstract": "Molecular property prediction uses molecular structure to infer chemical properties. Chem- ically interpretable representations that capture meaningful intramolecular interactions enhance the usability and effectiveness of these predic- tions. However, existing methods often rely on atom-based or rule-based fragment tokenization, which can be chemically suboptimal and lack scalability. We introduce FragmentNet, a graph- to-sequence foundation model with an adaptive, learned tokenizer that decomposes molecular graphs into chemically valid fragments while preserving structural connectivity. FragmentNet integrates VQVAE-GCN for hierarchical frag- ment embeddings, spatial positional encodings for graph serialization, global molecular descrip- tors, and a transformer. Pre-trained with Masked Fragment Modeling and fine-tuned on Molecu- leNet tasks, FragmentNet outperforms models with similarly scaled architectures and datasets while rivaling larger state-of-the-art models re- quiring significantly more resources. This novel framework enables adaptive decomposition, seri- alization, and reconstruction of molecular graphs, facilitating fragment-based editing and visualiza- tion of property trends in learned embeddings-a powerful tool for molecular design and optimiza- tion.", "sections": [{"title": "1. Introduction", "content": "Pre-trained models have transformed natural language pro- cessing (NLP) by capturing contextual information from large unlabeled corpora (Devlin, 2018). In parallel, self- supervised learning techniques have been applied to molec-"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Molecule Models Pre-trained with Masked Atom Modeling on Graphs", "content": "Traditional masked modeling strategies often focus on atom- level masking, limiting a model's ability to capture chemical substructure context. For example, AttrMask (Xia et al., 2023) randomly hides node and edge attributes in molecular graphs, while MoleBERT (Xia et al., 2023) employs masked atom modeling (MAM) with a Vector Quantized Variational Autoencoder (VQ-VAE) to enhance atom-level representa- tions. However, atom-level masking often creates broken or chemically inconsistent environments in the visible context, restricting the model's understanding of bonding and func- tional group interactions. SimSGT (Liu et al., 2024) takes a step beyond simple atom masking by training a GNN to pre- dict subgraphs representing the local neighborhoods around masked atoms. Although it offers flexibility in identifying"}, {"title": "2.2. Masked Fragment Modeling (MFM)", "content": "Masking entire fragments addresses the aforementioned limitation by preserving chemically meaningful contexts, enabling more stable learning of bonding rules and func- tional relationships (Jin et al., 2018; Schwaller et al., 2019). Rooted in organic chemistry principles, this approach en- sures that contextual dependencies within fragments remain intact. It is analogous to reconstructing a multi-word phrase in natural language models, where additional context en- hances semantic understanding (Sennrich et al., 2016). Two primary methods have been explored for its implementation: adaptive tokenization of SMILES strings and rule-based subgraph masking on molecular graphs."}, {"title": "2.2.1. BERT-STYLE PRE-TRAINING", "content": "Transformer-based architectures (Devlin, 2018) have demon- strated remarkable capabilities in efficiently learning con- textual relationships within sequential data. ChemBERTa (Chithrananda et al., 2020) extends this paradigm to molec- ular SMILES representations, using an adaptive tokenizer to split SMILES strings into subsets of characters, leverag- ing masked language modeling to enhance chemical feature extraction. MolFormer (Ross et al., 2022) scales up this technique while incorporating rotary positional embeddings, efficiently pre-training on SMILES sequences from 1.1 bil- lion molecules. MolTRES (Park et al., 2024) introduces a hierarchical masking strategy for SMILES sequences target- ing multiple granularities of chemical substructures, from individual atoms to entire functional groups. However, these SMILES-based transformers often neglect the topological relationships inherent to molecular graphs, sacrificing struc- turally informed representation learning for modeling effi- ciency (Nguyen et al., 2024)."}, {"title": "2.2.2. RULE-BASED MASKED FRAGMENT MODELING (MFM)", "content": "Existing graph-based methods often rely on rule-based to- kenization methods, such as BRICS (Vangala et al., 2023). IBM's r-BRICS extends this by introducing more flexi- ble fragmentation patterns to handle a wider variety of molecules, primarily targeting long aliphatic chains and complex ring structures. UniCorn (Feng et al., 2024) em- ploys BRICS-based fragmentation for 2D graph masking. However, these methods are limited by their rigidity and might struggle to generalize across diverse chemical spaces (Jinsong et al., 2024). Data-driven approaches overcome this by learning adaptable fragment representations."}, {"title": "3. FragmentNet Tokenizing Method", "content": ""}, {"title": "3.1. Graph Tokenization", "content": "In natural language processing (NLP), the quality of to- kenization is paramount (Vaswani, 2017), motivating the development of data-driven subword tokenizers (Sennrich et al., 2016; Schuster & Nakajima, 2012). We introduce a learned tokenization technique generalizable to all graph structures. This approach yields significant advantages over rule-based methods (e.g., (Lewell et al., 1998; Degen et al., 2008)) by capturing empirical substructure distributions more effectively. In Appendix A, we provide a detailed theoretical foundation for this claim. The proven benefits of learned tokenization in NLP naturally extend to the molec- ular domain, offering a robust framework for improved molecular representation learning."}, {"title": "Algorithm: Iterative Pairwise Merging of Molecular Fragments", "content": ""}, {"title": "3.1.1. LEVELS OF GRANULARITY", "content": "The tokenizer first splits a molecule into its individual atoms and iteratively merges them based on the learned merge history. The number of merges performed controls the size of the tokens. After training the tokenizer for $T$ iterations, the set of stored merges $\\mathcal{M}$ has length $T$. A molecule can then be tokenized using the first $t$ merges from $\\mathcal{M}$ for any $t < T$ without requiring retraining. This flexibility, unique to our tokenizer, allows for adjustable granularity per task.\nIn this work, we chose 100 merge iterations based on em- pirical evaluation; further research will explore the impact of the levels of granularity on task-specific downstream per- formance. Figure 5 illustrates the impact of these training iterations on the distribution of fragment sizes within our representative dataset of 2 million SMILES strings."}, {"title": "3.1.2. TOKEN DICTIONARY", "content": "We tokenized 17,000 molecules to create a vocabulary for our foundation model, then stored the tokens efficiently via hashing. We preserved SMILES, graph representations for reconstruction, and special tokens like [UNK], as discussed further in Appendix B."}, {"title": "3.1.3. TREATMENT OF DANGLING BONDS", "content": "Fragments are created by breaking bonds in the original molecule; we cache the fragments in the token dict, attach- ing a 'dummy atom' represented by an atom with an atomic number of O and no chemical attributes to the other end of the broken bond. This leads to the number and types of broken bonds being an additional differentiator of otherwise identical fragments: A carbon atom with 1 broken single bond, a carbon atom with 2 broken single bonds, and a carbon atom with 1 broken double bond are all treated as different fragments in the token dictionary."}, {"title": "3.1.4. MOLECULE HASHING METHOD", "content": "To uniquely learn embeddings for different molecular frag- ments, a one-to-one hashing mechanism is required to distin- guish between stereoisomers and tautomers and accommo- date dangling bonds. Traditional approaches like SMILES lack uniqueness, as a single fragment can correspond to multiple SMILES strings and do not distinguish isomeric molecules (Gilmer et al., 2017).\nWe address this with the Weisfeiler-Lehman (WL) graph hashing algorithm (Weisfeiler & Leman, 1968), ensuring that non-isomorphic graphs receive distinct hashes. For- mally, the WL algorithm iteratively refines node labels based on their neighborhood structures:\n$l_i^{(t)} = Hash \\big(l_i^{(t-1)}, \\{l_j^{(t-1)} \\mid v_j \\in N(v_i)\\} \\big)$\nHere, $l_i^{(t)}$ is the label of node $v_i$ at iteration $t$, and $N(v_i)$ is"}, {"title": "4. FragmentNet Model Architecture", "content": "A high-level schematic of FragmentNet's hybrid graph-to- sequence architecture can be found in Figure 1."}, {"title": "4.1. Input Representation", "content": "Given a starting molecular graph, FragmentNet receives the following input data: the tokenized fragment graphs (with atom/bond chemical features), the arrangement of the fragments relative to each other in the molecule, and the fragment charges Appendix D."}, {"title": "4.2. VQVAE-GCN Encoder for Hierarchical Input Embeddings", "content": "We propose a VQVAE-GCN encoder to construct hierarchi- cal molecular fragment embeddings by integrating Vector Quantized Variational Autoencoders (VQ-VAEs) and Graph Convolutional Networks (GCNs). VQ-VAEs encode dis- crete atomic-level features (Van Den Oord et al., 2017), mapping atomic features into a discrete latent space. For an input $x$, the encoder $E$ maps it to $z_e$, which is quantized to the closest vector $z_q = \\arg \\min_{c \\in C} ||z_e \u2013 c||_2$ in a learned codebook $C$. The decoder reconstructs $x = D(z_q)$, and the training loss is:\n$L_{VQVAE} = ||x -\\hat{x}|| + ||\\text{sg}[z_e] - c||^2 + \\beta||z_e - \\text{sg}[c]||^2$\nwhere $\\beta$ balances the commitment loss, and sg[\u00b7] denotes stop-gradient. Atomic embeddings are averaged to form fragment-level representations, $z_{VQ,f} = \\sum_{i \\in f} z_{VQ,i}$.\nThe GCN captures fragment-level relationships by aggre- gating features from neighboring nodes within the discrete fragments. Node embeddings are combined in graph convo- lutions using global mean pooling to generate compressed fragment-level feature representations. The final fragment embedding is obtained by combining VQ-VAE and GCN features as $r_f = T(z_{VQ,f}) + z_{GCN,f}$, where $T$ is a learnable transformation. This integration leverages the strengths of both architectures: VQ-VAEs encode discrete atomic fea- tures, while GCNs model structural relationships between atoms."}, {"title": "4.3. Graph Spatial Positional Encodings (SPEs)", "content": "After computing embeddings for each fragment, we embed spatial information about the fragments' arrangement in the molecule using Graph Spatial Positional Encodings (SPEs) inspired by (Zhang et al., 2020). We employ three types of SPEs: Hop-based Positional Encoding, Weisfeiler-Lehman (WL) Absolute Positional Encoding, and Coulomb Matrix Positional Encoding.\nHop-based Positional Encoding captures a node's relative 'connectedness' by aggregating its hop distances from all other nodes in the graph. For a molecular graph $G = (V, E)$, the hop distance $H_{i,j}$ from node $v_i$ to node $v_j$ is defined as the length of the shortest path between them, or 0 if no path exists. For each node $v_i$, we define its hop-based encoding as $h_i = \\sum_{k=1}^N \\text{Emb}_{hop}(H_{i,k})$, where the sum aggregates the encoding of the hop distance relative to all other nodes $v_j$.\nWeisfeiler-Lehman (WL) Absolute Positional Encoding labels nodes uniquely based on their position within the graph structure, ensuring that nodes in two graphs with iden- tical structures receive the same labels. After T iterations of label refinement using the Weisfeiler-Lehman algorithm, each node $v_i$ is assigned a unique WL role ID $w_i$, which is embedded as $w_i = \\sum_{j=1}^N \\text{Emb}_{WL}(w_j)$. Appendix C demonstrates how isomeric molecules receive distinct WL- based hashes through this approach despite sharing the same molecular formula.\nCoulomb Matrix Positional Encoding models molecular fragment interactions based on inverse-square law distances. Given node charges $q_i$ and a fixed distance $d_0$, the mean Coulomb interaction for node $v_i$, aggregated over all anchor nodes, is embedded as $c_i = \\sum_{j=1}^N \\text{Emb}_{clb}(C_{i,j})$, where $C_{i,j}$ is defined as:\n$C_{ij} = \\sum_{k=1}^N\\big(\\frac{0.5 \\cdot z_i z_j}{\\delta_{j,k} + \\frac{z_i z_k}{d}} + \\frac{z_j z_k (1 - \\delta_{j,k})}{d}\\big)$\nHere, $\\delta_{j,k}$ represents the Kronecker delta. This formulation ensures that Coulomb interactions are considered relative to all anchor nodes by directly incorporating both the self- interaction term $\\frac{0.5 \\cdot z_j^2}{\\delta_{j,k} + \\frac{z_i z_k}{d}}$ and the pairwise interaction term $\\frac{z_j z_k (1 - \\delta_{j,k})}{d}$ within the aggregate equation."}, {"title": "4.3.1. COMBINED POSITIONAL ENCODING", "content": "This encoding integrates hop-based, WL, and Coulomb po- sitional encodings by summing $h_i$, $w_i$, and $c_i$ for each fragment-node $v_f$. The aggregated encoding $p_f$ compre- hensively encapsulates spatial and topological information, contributing to the fragment embedding $r_f + p_f$. Further details and visualizations are provided in Appendix E."}, {"title": "4.4. Molecular Descriptor CLS Token", "content": "In transformer-based architectures, the CLS token aggre- gates the overall context of the input sequence for classifi- cation tasks (Devlin, 2018). In FragmentNet, we replace this token with a Molecular Descriptor Vector, prepend- ing it to the sequence of fragment tokens. The molecular descriptor vector uses RDKit's CalcMolDescriptors method; we compute a vector $d = [d_1, d_2, ..., d_m]$ of stan- dard molecular descriptors. This vector, denoted $d_{CLS}$, is refined during training by attention layers to encode holistic molecular features for downstream tasks."}, {"title": "4.5. Transformer Encoder Layers", "content": "With the graph structures serialized into a sequence of frag- ment embeddings, we pass them through a series of BERT transformer encoder blocks (Devlin, 2018), consisting of multi-head self-attention and feed-forward networks with residual connections and layer normalization. The series of attention layers models fragment-to-fragment relation- ships and captures contextual dependencies and interactions critical for effective structural representation learning."}, {"title": "4.6. Property Prediction Head", "content": "The property prediction head processes the fragment net- work's sequence output using max pooling across the sequence length, chosen for its effectiveness in preserv- ing salient features while maintaining padding invariance (Su\u00e1rez-Paniagua & Segura-Bedmar, 2018). The pooled representation is then linearly projected to match the target property dimension."}, {"title": "4.7. Fragment Swapping Module", "content": "We propose a fragment-swapping algorithm that system- atically replaces target fragments with alternatives while preserving the chemical integrity of the core molecular scaffold. The granularity of fragments can be dynamically adapted to suit specific tasks, enabling optimizations such as tailoring molecular analogues for pharmacokinetics or structure-activity relationships (SAR). This serialization pro- cess bypasses the need for substructure matching or compu- tationally expensive graph search methods typically used in traditional fragment-based workflows (Vangala et al., 2023). Appendix F.1 discusses the algorithm and implementation details."}, {"title": "5. Training Method", "content": ""}, {"title": "5.1. Masked Fragment Modeling Pre-training", "content": "We pre-train using Masked Fragment Modeling (MFM). The molecular graph is serialized into a sequence of chemi- cally valid fragments, preserving the structural information"}, {"title": "5.2. Property Prediction Evaluation", "content": "We evaluated our model's performance on molecular prop- erty prediction tasks using datasets from MoleculeNet (Wu et al., 2018) and Malaria (Gamo et al., 2010). These datasets cover physical chemistry, biophysics, and physiology prop- erties, providing a comprehensive benchmark. We employed an 80-10-10 train-validation-test split. The training was con- ducted with 10 random seeds for each dataset, and the results were aggregated, following the approach used in previous works (Xia et al., 2023)."}, {"title": "5.3. Compute and Training Configuration", "content": "Due to funding and compute limitations, FragmentNet was configured and trained on a MacBook Pro M2 laptop us- ing Apple's Metal Performance Shaders framework. For details on the training setup, methods, hyperparameters, and compute optimization, see Appendix I."}, {"title": "6. Results and Discussion", "content": "We benchmark against modeling approaches grouped by scale into three brackets: the top bracket contains models with parameter counts and pre-training dataset sizes com- parable to FragmentNet, the middle bracket highlights our"}, {"title": "Takeaway 1 - Token Granularity Impacts Task Complex- ity and Structural Context.", "content": "We empirically demonstrate the importance of an adaptive graph tokenizer for chemically accurate masked fragment modeling. Token granularity, controlled by the number of merge iterations, affects the complexity and informativeness of the Masked Fragment Modeling (MFM) task. At 0 merge iterations (smallest granularity), fragments consist of a sin- gle atom, its bond information, and dummy atoms indicating neighboring connections. When pre-trained on 2 million molecules, as shown in (Appendix I.3), this setup leads to stable convergence, achieving good reconstruction accuracy within the first epoch. Increasing granularity through addi- tional merges (100 iterations) makes the task more complex and data-efficient, with slower but sustained learning, high- lighting the potential for improved performance when paired with larger, more diverse datasets. Larger tokens capture higher-level structural motifs, enabling better generalization in downstream tasks. Our pre-trained models consistently outperform un-pretrained models across benchmarks, veri- fying the effectiveness of MFM and addressing the negative transfer issues common in atom-level masking."}, {"title": "Takeaway 2 - Token Granularity Can Be Optimized for Task-Specific Performance.", "content": "Our results suggest that token granularity, which controls the size and distribution of fragment tokens, can be treated as an optimizable hyperparameter to improve both pre-training and downstream task performance. As shown in Table 1, neither fine-grained nor coarse-grained tokens consistently outperform the other across all tasks, highlighting the need for task-specific tuning. By adjusting the number of merge it- erations and token dictionary distribution, future work could explore optimal granularity for specific chemical proper- ties or molecular tasks, enhancing generalizability across diverse applications."}, {"title": "Takeaway 3 - FragmentNet Outperforms Comparable- Scale Models.", "content": "FragmentNet demonstrates state-of-the-art performance on 7 of 8 MoleculeNet (Wu et al., 2018) and Malaria (Gamo et al., 2010) benchmark tasks among foundation models of comparable (small) scale, highlighting the effectiveness of MFM pre-training and adaptive tokenization over legacy pre-training approaches."}, {"title": "Takeaway 4 - FragmentNet Is Competitive with Large- Scale Models Using a Chemically Informed, Data- Efficient Framework.", "content": ""}, {"title": "6.1. Dataset Segregation Visualizations", "content": "We leverage t-SNE to visualize how the embedding space evolves during fine-tuning. Taking the Lipophilicity dataset as an example, we analyze embeddings from the penultimate"}, {"title": "6.2. Attention Maps", "content": "Our approach reveals chemically intuitive patterns by captur- ing fragment-fragment relationships. As shown in Figure 3, different attention heads naturally identify distinct molecular features that influence solubility. Most heads emphasize the hydroxyl group (-OH) (red regions), aligning with its role in forming hydrogen bonds with water. Notably, one head focuses on the hydrophobic alkyl fragments-the methyl substituent and carbon backbone-which limit water solu- bility. This demonstrates how attention mechanisms capture the competing effects of hydrophilic and hydrophobic frag- ments, highlighting chemical interpretability. Additional attention-map analyses are provided in Appendix H."}, {"title": "6.3. Fragment Swapping Discussion", "content": "To illustrate the capabilities of the fragment swapping mod- ule discussed in Section 4.7, we start with the molecule Ibuprofen. The adaptive tokenizer serializes Ibuprofen into tokenized fragments, allowing us to identify and modify the carboxylic acid and methyl groups precisely. We then swap out fragments at the target modification site for alternatives using our fragment-swapping algorithm, which avoids the need for substructure matching or graph-based searches.\nUsing this approach, we generate two chemically valid analogues: CC (C) Cclccc(Cl)cc1 and CC (C) Cclccc(cc1)C(=0)0 (Figure 4). These analogues preserve the molecular scaffold of Ibuprofen while introducing structural diversity, demonstrating how the module facilitates efficient, precise analogue generation.\nThis approach highlights the novelty of our framework: by leveraging adaptive tokenization to simplify and serialize molecular representation, we can rapidly and systematically explore chemical spaces. This is particularly valuable for applications such as drug discovery and materials design."}, {"title": "7. Limitations and Future Work", "content": "This study was conducted on a relatively small scale for foundation models, utilizing a pre-training dataset of only 2 million molecules. Future research will address these limitations by expanding the dataset to encompass billions of molecules, unlocking the training of larger and more comprehensive models. Incorporating auxiliary techniques such as contrastive learning and further optimizing token granularity is expected to enhance predictive performance. Exploring adherence to scaling laws observed in the lan- guage domain (Kaplan et al., 2020) will help assess how well FragmentNet's performance scales with increased data and model size. Additionally, the limited size of Molecu- leNet datasets presents overfitting challenges, emphasizing the need for improved chemical pre-training to enhance gen- eralization. Collaborating with chemists will help design more interpretable models aligned with domain knowledge."}, {"title": "8. Conclusion", "content": "We introduce FragmentNet, a novel graph-to-sequence molecular foundation model featuring the first adaptive graph tokenizer used for MFM. Our framework enables the decomposition, serialization, and reconstruction of molecu- lar graphs, allowing sequential architectures to model them effectively while preserving key topological information. Using chemically informed, data-efficient pre-training tasks, FragmentNet captures meaningful, domain-specific relation- ships, outperforming similarly scaled models and achiev- ing competitive performance with larger foundation mod- els-all while being trained on a single MacBook Pro. Addi- tionally, our fragment-based chemical editing module opens new possibilities for targeted molecular optimization and deeper insights into structure-property relationships. Mov- ing forward, we aim to scale FragmentNet to establish the leading molecular graph foundation model, driving advance- ments in molecular discovery and optimization."}, {"title": "Impact Statement", "content": "This paper aims to advance the field of Machine Learning in Drug Discovery and Optimization by leveraging AI to en- hance the understanding and design of molecules. Our work has significant potential societal implications, particularly in enabling researchers and industry professionals to develop and refine therapeutics more effectively. By advancing the scientific methodologies underlying these efforts, we hope to contribute to creating safer, more effective treatments that address critical healthcare challenges."}, {"title": "A. Theoretical Justifications for Adaptive Tokenization in Large-Scale Molecular Modeling", "content": ""}, {"title": "A.1. Information-Theoretic Motivation in the Molecular Domain", "content": "Large chemical databases such as ZINC (Irwin & Shoichet, 2005) and ChEMBL (Gaulton et al., 2017) contain millions of molecules with high structural diversity and skewed substructure frequency distributions. A few ring systems and functional groups occur frequently, while many motifs rarely appear. Rule-based tokenizers, such as RECAP (Lewell et al., 1998) or BRICS (Degen et al., 2008), rely on static, chemistry-driven heuristics-e.g., predefined bonds to cleave or functional groups to isolate-without adapting to actual usage frequencies. This often leads to misaligned fragmentations and inefficiencies in downstream modeling.\nShannon's source coding theorem (Shannon, 1948) indicates that the optimal average code length for a random variable $X$ with probability distribution $p(x)$ is bounded below by its entropy: $H(X) = \\cdot \\sum_{x\\in X}p(x) \\log p(x)$. In a molecular context, X can represent common substructures in the dataset, with $p(x)$ as their empirical frequencies. A rule-based fragmenter defines a static vocabulary that ignores these frequencies. Contrastingly, an adaptive tokenizer seeks a vocabulary aligned with $p(x)$, minimizing the overall code length. Frequent motifs become single tokens, while rarer motifs are split more finely."}, {"title": "A.2. Compression and the MDL Principle in Molecular Data", "content": "The principle of minimum description length (MDL) (Rissanen, 1978) states that the best representation of the data minimizes both the complexity of the model and the encoded length of the data. Subword tokenizers in natural language processing-such as BPE (Sennrich et al., 2016) and WordPiece (Schuster & Nakajima, 2012)\u2014achieve compression by merging frequent character or subword sequences. In the molecular domain, a similar procedure merges atoms or smaller functional groups that co-occur in local structures. Iterating this process yields a vocabulary of substructures that reduces the corpus's encoded length.\nSuch a data-driven vocabulary can be viewed as a fragment discovery method, where tokens correspond to commonly reused chemical units (e.g., rings, linkers, functional groups). Unlike fixed heuristics, an adaptive tokenizer calibrates fragment size to the corpus distribution, balancing frequent and rare substructures."}, {"title": "A.3. Universal Coding Perspective and Long-Tail Frequencies", "content": "From the perspective of universal coding (Ziv & Lempel, 1977), an ideal encoder adaptively identifies and stores repeating patterns to minimize code length. BPE-like algorithms can be viewed as simplified universal coders, merging frequent local sequences into tokens. Applied to molecules, this naturally captures recurring motifs like ring scaffolds, linkers, or functional groups, thereby shortening sequences and producing more coherent representations."}, {"title": "B. Tokenizer Discussion", "content": ""}, {"title": "B.1. Sample Readout from Token Dictionary", "content": "The below token dictionaries were generated using FragmentNet's adaptive tokenizer with 0 and 100 merge iterations for atoms and fragments, respectively, rather than relying on rule-based fragmentation approaches. Each entry in the token dictionary is a tuple comprising the fragment's hash ID, its SMILES representation (where * denotes a placeholder atom), a graph representation that includes node features (x), edge connections (edge_index), edge features (edge_attr), and the number of atoms (num_atoms), along with additional metadata (currently None)."}, {"title": "B.2. Granularity num_iters=0 Sample Entries", "content": "Within our tokenization framework, the smallest level of granularity retains the structure of common fragments through bonds and dummy atoms; it keeps a single atom within the fragment. However, retaining the connection points makes the fragment more informative than a singular atom. For example:\n1. Double-bonded oxygen (=O) with one connection point:\n(('a55e82208e74da72a2918a7c63ed516a', '*=0', Data(x=[2, 133], edge_index= [2, 2), edge_attr= [2, 147], num_atoms=2), None))"}, {"title": "B.3. Fragment Token Dictionary Sample Entries", "content": "As we increase the granularity, fragment-based tokenization creates more chemically meaningful tokens. Note that the fragment dictionary retains the atom fragments, which can be used as needed. Below are a few samples from the fragment token dictionary:\n1. A chiral cyclic amine with a carbonyl group:\n(('dd071a4542f22e18743e1be19dcd20b2\u2032, '*[C@@H]1CCCN1C=0\u2032, Data(x=[8, 133], edge_index=[2, 16], edge_attr=[16, 147], num_atoms=8), None))\n2. A hydroxyl group attached to a benzene ring with a placeholder connection point:\n(('ec766bc887f489cc1b0d875eb133c2d6\u2032, '*0c1cccc(*)c1*', Data(x=[10, 133], edge_index=[2, 20], edge_attr=[20, 147], num_atoms=10), None))"}, {"title": "B.4. Token Dictionary Length", "content": "As this is an adaptive tokenizer, the size of the token dict tends to scale based on the size of the token dict training dataset and the tokenizer granularity.\nDue to our computing constraints, it was not feasible for us to train a token dictionary with the same dataset as our pre-training task, which consisted of 2 million SMILES. Therefore, we decided to use a 17k subset of the pre-training dataset and saw an atom token dictionary of length 98 and a fragment token dictionary length of 8737."}, {"title": "B.5. Fragment Token Size Distribution", "content": "Figure 5 illustrates key statistics about our molecular tokenization scheme after applying 100 merge operations. The first distribution shows the number of fragments per molecule, indicating that most molecules contain approximately 7 fragments on average. However, the distribution exhibits a tail of larger molecules with more fragments. This suggests that our tokenizer effectively compresses molecular structures while maintaining meaningful substructures.\nThe second distribution displays the number of atoms per token, revealing that most tokens correspond to fragment sizes of around 10 atoms. This indicates that our tokenization approach balances fragmentation and meaningful chemical subunits, crucial for learning robust molecular representations.\nIn our Masked Fragment Modeling (MFM) task, we leverage this tokenization scheme by randomly masking a fragment and training the model to predict the missing part. This task encourages the model to capture local chemical dependencies while learning a high-level structural understanding of molecules. The observed fragment distribution suggests that our tokenizer segments molecules into chemically meaningful building blocks, which could enhance generalization in downstream molecular property prediction tasks."}, {"title": "B.5.1. DISCUSSION ON TOKENIZER GRANULARITY", "content": "The atom and fragment token dictionaries demonstrate the flexibility of our tokenizer framework, which allows granularity to be parameterized based on the number of merge iterations. This enables direct comparison of masked pre-training tasks within the same modeling framework - a capability previously unavailable for molecular graphs.\nBy tuning the number of merge iterations, we can influence the distribution of token granularities learned by the tokenizer."}, {"title": "C. Weisfeiler-Lehman Molecule Hashing", "content": "The WL-hashing algorithm we have adapted to molecules can uniquely hash them while distinguishing many forms of isomerism, which is not currently possible with other string-based molecular representations such as Smiles (Weininger, 1988) and InChi (Heller et al., 2015). In Table 2, we review a well-known case of isomerism and note that molecules that exhibit forms of isomerism are assigned unique hashes through our proposed WL-molecular hashing methods, despite having the same smiles strings."}, {"title": "D. Fragment Charges", "content": "Fragment Charges. We define the charge of a fragment as the sum of the Gasteiger-Marsili partial charges for all non-dummy atoms in the fragment, i.e., $Q_F = \\sum_{a \\in F} q_a$, where $q_a$ is the partial charge of atom $a$ if the total charge is not a number (NaN), set $Q_F = 0$."}, {"title": "E. Spatial Positional Encodings Visualization", "content": "Figure Figure 6 Provides a visualization of the principal component analysis (PCA) applied to fragments of a molecule under different positional encoding schemes: Weisfeiler-Lehman (WL), multi-anchor hop-based (HOP), Coulomb (CLB), and their aggregated encoding (POS). The WL encoding captures the relative positional relationships between nodes by using a breadth-first search (BFS)-type algorithm to encode the connectedness of nodes within the molecular graph. The HOP encoding generalizes positional relationships further by considering the connectivity of each node to all other nodes in the graph, providing a more global sense of structural embedding. The CLB encoding, in contrast, leverages electrochemical properties, encoding nodes based on the Coulombic interactions, which emphasize the physical and chemical characteristics of the molecule. When aggregated, these encodings (POS) yield a comprehensive differentiation of molecular fragments, as seen in the rightmost PCA plots. This demonstrates these encodings' complementary strengths in capturing structural and chemical nuances, effectively distinguishing molecular substructures."}, {"title": "F. Fragment Swapping Algorithm", "content": "The Fragment Swapping Algorithm is designed to generate chemically valid analogues of a given molecule by systematically substituting molecular fragments at predefined positions. The input molecule (M) contains dummy atoms denoting points of substitution, and a set of candidate fragments (fi \u2208 F) with matching dummy atoms is provided. The algorithm ensures chemical validity by matching the bond environments of the dummy atoms in both the input molecule and"}]}