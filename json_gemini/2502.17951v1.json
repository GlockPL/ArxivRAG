{"title": "Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models", "authors": ["Jia Yu", "Yan Zhu", "Peiyao Fu", "Tianyi Chen", "Junbo Huang", "Quanlin Li", "Pinghong Zhou", "Zhihua Wang", "Fei Wu", "Shuo Wang", "Xian Yang"], "abstract": "Colorectal cancer (CRC) is a significant global health concern, and early detection through screening plays a critical role in reducing mortality. While deep learning models have shown promise in improving polyp detection, classification, and segmentation, their generalization across diverse clinical environments, particularly with out-of-distribution (OOD) data, remains a challenge. Multi-center datasets like PolypGen have been developed to address these issues, but their collection is costly and time-consuming. Traditional data augmentation techniques provide limited variability, failing to capture the complexity of medical images. Diffusion models have emerged as a promising solution for generating synthetic polyp images, but the image generation process in current models mainly relies on segmentation masks as the condition, limiting their ability to capture the full clinical context. To overcome these limitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that integrates diverse clinical annotations such as segmentation masks, bounding boxes, and colonoscopy reports by transforming them into compositional prompts. These prompts are organized into coarse and fine components, allowing the model to capture both broad spatial structures and fine details, generating clinically accurate synthetic images. By augmenting training data with PSDM-generated samples, our model significantly improves polyp detection, classification, and segmentation. For instance, on the PolypGen dataset, PSDM increases the F1 score by 2.12% and the mean average precision by 3.09%, demonstrating superior performance in OOD scenarios and enhanced generalization.", "sections": [{"title": "1 Introduction", "content": "Colorectal cancer (CRC) is a major global health concern, ranking as the third most common cancer and the second leading cause of cancer deaths worldwide [1]. Early screening is vital for reducing mortality by enabling the removal of adenomatous polyps before malignancy [2]. However, the complexity of colon anatomy and polyp variability pose significant challenges, exposing limitations in human detection skills [3]. Deep learning models have shown great potential in enhancing polyp detection and diagnosis [4], excelling in real-time colonoscopy analysis. However, their performance on out-of-distribution (OOD) data remains problematic, as domain shifts caused by variations in equipment, patient demographics, and procedures hinder robustness. Additionally, these models may amplify annotation bias, leading to missed diagnoses and fairness issues [5]. Improving generalization is thus critical for achieving reliable diagnostic accuracy in clinical practice. Recently, colonoscopy records, including images and text reports, have been explored as valuable resources [6].\nEfforts to improve generalization focus on collecting diverse, well-annotated datasets, which are critical for reliable deep learning performance. For instance, PolypGen [7] captures polyp diversity by combining data from six centers. However, creating such datasets is costly, labor-intensive, and constrained by privacy concerns, limiting scalability [8]. Traditional data augmentation techniques, such as affine transformations (e.g., rotation, flipping, cropping) and color modifications, have been widely used to expand datasets and improve generalization. However, these methods offer limited variability and fail to capture the complexity of medical images or generate novel visual features [9]. To address these limitations, synthetic data generation using models like Generative Adversarial Networks (GANs) [10] has emerged as a solution, increasing dataset variability and enhancing generalization. In medical imaging, this approach generates diverse and realistic representations, improving model performance across clinical scenarios. However, GANs often struggle with consistent quality and diversity, limiting their practical effectiveness [11]. Diffusion models (DMs) provide a promising alternative, refining noise into structured data to generate stable and diverse images. DMs address domain generalization challenges and enable targeted applications through conditional prompts [12], improving performance in tasks like skin lesion classification [13] and colonoscopy analysis [14].\nDespite significant advancements in polyp image generation using diffusion models, a critical limitation remains: these models typically rely heavily on segmentation masks as the primary conditioning input. While segmentation masks provide precise pixel-level information for localization, they lack critical semantic details, such as polyp type, texture, and surrounding tissue characteristics. This reliance limits the models' ability to generate clinically diverse and relevant outputs that reflect real-world polyp variability. One way to address this limitation is by leveraging additional clinical information, particularly rich textual descriptions from colonoscopy reports and pathology findings, which provide essential semantic insights such as polyp size, morphology, surface characteristics, and histopathological attributes [15]. These descriptions, often absent from imaging datasets, offer valuable diagnostic context beyond segmentation masks. Complementary annotations, such as bounding boxes for coarse localization and classification labels for benign or malignant polyps, further enrich the data. However, current models seldom integrate these annotations with segmentation masks, limiting their ability to generate clinically diverse and meaningful outputs [16].\nAs shown in Fig. 1, we propose a compositional prompt approach that integrates diverse annotations, such as segmentation masks, bounding boxes, and colonoscopy reports, into a unified diffusion-based image generation framework. These annotations serve as prompts to guide the diffusion model, enabling the generation of polyp images aligned with the annotations while augmenting data for medical tasks. To effectively integrate complementary information, we organize prompts into coarse components for large-scale spatial structures and fine components for detailed clinical features. To address"}, {"title": "2 Related Work", "content": "Synthetic polyp generation plays a pivotal role in addressing the scarcity of annotated datasets, a persistent challenge in colonoscopy-related deep learning tasks. Early methods predominantly relied on generative adversarial networks (GANs) due to their ability to synthesize visually realistic images. For example, Shin et al. [20] utilized conditional GANs to enhance the realism of synthetic polyps by incorporating edge maps and masks, providing additional structural details to the generated images. Sasmal et al. [21] adopted DCGANs to expand polyp datasets, demonstrating improvements in downstream classification performance. Similarly, Qadir et al. [22] proposed mask-based conditional GANs to manipulate polyp appearances, while He et al. [23] developed adversarial techniques to produce false-negative samples, significantly enhancing the robustness of polyp detection models by challenging classifiers with hard-to-identify cases.\nDespite these advances, GAN-based methods face inherent challenges, including convergence instability, limited image diversity, and artifact generation [11]. Further studies by Frid-Adar et al. [9] and Yoon et al. [24] explored GANs in medical image synthesis, highlighting both their potential and limitations, particularly in colonoscopy."}, {"title": "2.1 Generative Adversarial Networks for Polyp Synthesis", "content": "Synthetic polyp generation plays a pivotal role in addressing the scarcity of annotated datasets, a persistent challenge in colonoscopy-related deep learning tasks. Early methods predominantly relied on generative adversarial networks (GANs) due to their ability to synthesize visually realistic images. For example, Shin et al. [20] utilized conditional GANs to enhance the realism of synthetic polyps by incorporating edge maps and masks, providing additional structural details to the generated images. Sasmal et al. [21] adopted DCGANs to expand polyp datasets, demonstrating improvements in downstream classification performance. Similarly, Qadir et al. [22] proposed mask-based conditional GANs to manipulate polyp appearances, while He et al. [23] developed adversarial techniques to produce false-negative samples, significantly enhancing the robustness of polyp detection models by challenging classifiers with hard-to-identify cases.\nDespite these advances, GAN-based methods face inherent challenges, including convergence instability, limited image diversity, and artifact generation [11]. Further studies by Frid-Adar et al. [9] and Yoon et al. [24] explored GANs in medical image synthesis, highlighting both their potential and limitations, particularly in colonoscopy."}, {"title": "2.2 Diffusion Models for Controlled Polyp Generation", "content": "Diffusion models have emerged as robust alternatives to GANs, providing greater stability in training and generating more diverse and realistic synthetic images. Machacek et al. [25] introduced a latent diffusion model conditioned on segmentation masks, marking a significant advancement in synthetic polyp generation by focusing on accurate structural details. Du et al. [14] further developed this concept with ArSDM, incorporating adaptive mechanisms to enhance lesion-specific focus and employing external models to refine alignment accuracy between synthetic polyps and ground truth masks. These refinements led to improved performance in segmentation and detection tasks. Sharma et al. [26] expanded the scope of diffusion-based methods with ControlPolypNet, a framework designed to generate more realistic images by controlling background details and spatial attributes, such as polyp size, shape, and location, resulting in substantial segmentation performance gains.\nDespite these advancements, existing frameworks often overlook the wealth of clinical information, focusing on isolated attributes and limiting their ability to achieve comprehensive control over diverse and clinically significant polyp characteristics. This narrow focus restricts their potential to address real-world challenges such as inter-hospital variability and domain shifts. Our approach builds on these advancements by integrating semantically rich polyp annotations into a unified diffusion-based framework. By enabling joint control across multiple granularity levels, we achieve the modulation of spatial and semantic features during polyp generation, addressing limitations in existing methods and improving model robustness and adaptability for diverse clinical scenarios."}, {"title": "3 Methods", "content": "Our framework generates high-quality synthetic polyp images using compositional prompts derived from diverse annotations, including segmentation masks, bounding boxes, textual descriptions, and vague class labels, as illustrated in Fig. 2. To integrate these prompts, we propose the Prompt Spectrum (Section 3.2), which categorizes the embeddings Pinto coarse and fine components to capture both structural layouts and detailed clinical features. These embeddings condition the diffusion model $D_{\\theta}$ during the Prompt-guided Diffusion Process (Section 3.3), enabling a progressive integration of spatial and semantic details to balance global structures and intricate clinical information."}, {"title": "3.1 Model Overview", "content": "Our framework generates high-quality synthetic polyp images using compositional prompts derived from diverse annotations, including segmentation masks, bounding boxes, textual descriptions, and vague class labels, as illustrated in Fig. 2. To integrate these prompts, we propose the Prompt Spectrum (Section 3.2), which categorizes the embeddings Pinto coarse and fine components to capture both structural layouts and detailed clinical features. These embeddings condition the diffusion model $D_{\\theta}$ during the Prompt-guided Diffusion Process (Section 3.3), enabling a progressive integration of spatial and semantic details to balance global structures and intricate clinical information."}, {"title": "3.2 Compositional Prompts Generation", "content": "In this section, we describe the process of generating compositional prompts from various types of medical annotations and how these prompts are integrated into the image generation framework through the Prompt Spectrum."}, {"title": "3.2.1 Prompt Spectrum", "content": "To accommodate the diverse nature of medical annotations, the Prompt Spectrum organizes the resulting prompts into two components: coarse and fine. Coarse prompts capture broad, high-level information such as object positions, sizes, and overall layout, while fine prompts focus on detailed aspects, including texture, pathological features, and boundary information.\nIt is important to note that not all annotations contain a single level of granularity. For annotations that include multi-level information, such as medical reports with coarse-grained size information and fine-grained surface texture details, we process them accordingly (see Section 3.2.2).\nBy organizing prompts into coarse and fine components, the Prompt Spectrum supports the progressive integration of low-frequency structural information and high-frequency clinical details throughout the image generation process. This flexible approach ensures the generation of clinically relevant images that effectively capture the diverse and complementary information encoded in the original annotations."}, {"title": "3.2.2 Prompt for Multi-Scale Textual Annotations", "content": "Textual annotations, such as clinical reports or descriptive labels, provide crucial contextual information for guiding medical image generation. In this work, we leverage a Large Language Model (LLM) to"}, {"title": "3.2.3 Prompt for Mask and Bounding Box Annotations", "content": "Encoding segmentation masks and bounding boxes is crucial for capturing the spatial arrangement and structure of regions of interest in generated images. We achieve this through a mask_encoder that transforms binary masks into compact prompt embeddings for integration into the Prompt Spectrum.\nThe process begins by converting the binary mask to a one-hot representation, distinctly defining foreground and background regions. The one-hot mask is then downsampled through interpolation stages, reducing resolution while preserving key spatial characteristics. A final linear layer produces a compact mask embedding, $P_{mask}$, which encodes both coarse spatial layout and fine boundary details, leveraging the mask's inherent structural information.\nFor bounding boxes, a similar approach is applied. We generate random elliptical masks based on bounding box dimensions, capturing overall spatial structure. These masks are processed by the mask_encoder to produce layout embeddings, $P_{layout}$, that represent the bounding box's size and position without fine-grained detail."}, {"title": "3.2.4 Prompt Learning for Vague Class Annotations", "content": "In scenarios where class labels are ambiguous or vague, such as when the annotation is not a specific mask or text but rather a reference image group representing a vague class, we adopt a Prompt Learn-"}, {"title": "3.3 Prompt-guided Diffusion Process", "content": "In this work, building on compositional prompts and the Prompt Spectrum, we extend the Stable Diffusion framework [28] into the Progressive Spectrum Diffusion Model (PSDM). PSDM guides the image generation process through multi-granularity prompts $p_i$, ensuring alignment with complex medical annotation requirements.\nDiffusion models are probabilistic generative models that iteratively denoise a sample drawn from a Gaussian distribution back to a clean image. Specifically, the input image x is encoded into a latent representation $z=E(x)$ using a pre-trained encoder E. The latent code z then undergoes a forward diffusion process, progressively corrupted by Gaussian noise:\n$z_t = \\sqrt{\\alpha_t}z_0 + \\sqrt{1 - \\alpha_t}\\epsilon$  (2)\nwhere $\\epsilon \\sim N(0, I)$ and $\\alpha_t$ controls the noise schedule at time step t. A time-conditional U-Net model is used to predict and remove the added noise at each step, progressively refining $z_t$ back to its clean latent representation $z_0$. The clean latent code is then decoded back to an image using the decoder D.\nAt each diffusion step, the model predicts the noise component based on the compositional prompt embeddings as:\n$\\epsilon_{\\theta}(z_t, t, p) = \\sum{\\lambda_i \\cdot \\epsilon_{\\theta}(z_t, t, p_i)}$  (3)\nwhere $\\lambda_i$ represents the prompt-specific weighting at each time step t, allowing the contribution of different prompts to vary according to their granularity throughout the diffusion process.\nWhen multiple prompts are provided, the model processes each component in stages, gradually incorporating information according to its frequency. During the denoising process, the image generation is initially influenced by prompts that encode low-frequency aspects of the image. As the diffusion process advances, prompts with higher-frequency details become more influential. The weighting function $\\lambda$ dynamically modulates the contributions of these prompts over time, ensuring a smooth transition from low-frequency structural elements to high-frequency clinical details.\nAfter obtaining the mixed noise prediction, we apply classifier-free guidance [29] to amplify the effect of the combined prompts by subtracting the unconditional noise prediction $\\epsilon_{\\theta}(z_t,t,\\O)$ and scaling the difference:\n$\\hat{\\epsilon}_\\theta(z_t, t, p) = \\epsilon_{\\theta}(z_t, t, (\\O) + s \\cdot (\\epsilon_{\\theta} (z_t, t, p) - \\epsilon_{\\theta}(z_t, t, (\\O))$ (4)\nwhere s is the guidance scale. This guidance mechanism amplifies the model's adherence to the provided"}, {"title": "3.4 Training Strategy", "content": "Generating accurate images from diverse prompts while learning sequentially from different annotation types introduces a significant challenge known as catastrophic forgetting. This occurs when, as new annotations are introduced, the model \"forgets\" how to generate images based on earlier annotations, diminishing its ability to retain previously learned information [17,30].\nTo address this challenge, we begin by defining the overall loss function used during training:\n$\\min L = E_{z \\sim E(x), \\epsilon \\sim N(0,1),t} [|\\epsilon - \\epsilon_{\\theta} (z_t, t, p) |^2]$ (5)\nTo mitigate the risk of catastrophic forgetting, we adopt a rehearsal-based continual learning strategy through a prompt replay mechanism. In this approach, previously encountered prompts, denoted as $p_{pre}$, are stored in a rehearsal buffer R(p). At each training step, the model samples from this buffer, allowing it to revisit and optimize on earlier prompts. The corresponding loss function for this replay mechanism is formulated as follows:\n$\\min L_{Replay} = E_{z \\sim E(x), \\epsilon \\sim N(0,1),t,P_{pre} \\sim R(p)} [|| \\epsilon - \\epsilon_{\\theta}(z_t, t, P_{pre}) ||^2]$ (6)\nTo further enhance the retention of prior knowledge, we implement validation checkpoints throughout the training process. These checkpoints systematically assess the model's ability to generate images from previously learned prompts, ensuring that the introduction of new annotations does not degrade performance on earlier tasks. This continual learning approach strikes a balance between acquiring new knowledge and retaining prior information, enabling the model to adapt effectively to complex and evolving datasets."}, {"title": "3.5 Downstream Application", "content": "To enhance the performance of downstream tasks, it has become common practice to combine synthetic and real data during model training. However, recent studies [31] have highlighted the potential risks associated with this approach, noting that an inappropriate mixing ratio between synthetic and real data can lead to performance degradation. This finding underscores the importance of carefully balancing the contribution of each data source during training.\nIn addition, research has shown that pretraining on in-domain data can significantly improve model performance [32-34]. Building on these insights, we leverage the synthetic data generated by our PSDM during the pretraining phase.\nTo generate high-quality synthetic data, we employ Denoising Diffusion Implicit Models (DDIM) [35] as the sampler. Specifically, the DDIM sampling process is governed by the following equation:\n$z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} (\\frac{z_t - \\sqrt{1-\\bar{\\alpha}_t}\\hat{\\epsilon}_\\theta(z_t,t,p)}{\\sqrt{\\bar{\\alpha}_t}}) + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma^2}\\epsilon_{\\theta} (z_t, t, p) + \\sigma \\epsilon$ (7)\nwhere $\\epsilon$ is a noise term. $\\hat{\\epsilon}_\\theta (z_t, t, p)$ is obtained from the model's noise prediction at each timestep t, calculated according to (4). This equation ensures that the noisy latent variable $z_t$ is progressively refined into $z_{t-1}$. Once the refined noise prediction is obtained, the model updates the latent representation $z_t$, progressively denoising it until the clean latent code $z_0$ is recovered. The final image is then generated by decoding $z_0$ through the decoder D.\nOur PSDM framework flexibly accommodates both single and multiple prompts as input. For segmentation tasks, a single segmentation mask can be transformed into a semantic map, facilitating the generation of synthetic images to enhance pretraining for lesion recognition. Additionally, combining segmentation masks with textual descriptions as compositional prompts further expands the dataset, providing enriched training samples. Following pretraining with synthetic data, fine-tuning on real-world datasets ensures better alignment with true data distributions, thereby improving generalization and enhancing model performance in clinical applications."}, {"title": "4 Experiments and Results", "content": "Our experiments utilized a variety of publicly available polyp datasets commonly used for segmentation and detection tasks in medical imaging. These datasets include ETIS [36], CVC-ClinicDB (CVC-612) [37], CVC-ColonDB [38], CVC-300 [39], Kvasir [40], and PolypGen [7]. The PolypGen dataset, which is sourced from six different centers (C1 to C6), offers a broad range of polyp images with corresponding segmentation masks, as well as bounding box annotations for sequential data (Seq1-23).\nOwn Datasets: In addition to the publicly available datasets, we also contributed two datasets specifically designed for our experiments. The first, named the Polyplus dataset, was created by carefully selecting 100 representative polyp images from a larger collection of 1,450 image-mask pairs sourced from Kvasir and CVC-ClinicDB. These images were annotated with detailed textual descriptions by expert colonoscopists, making Polyplus a valuable resource for integrating visual and textual modalities. The second dataset consists of 385 benign and 77 malignant polyp images, annotated with classification labels to support experiments in polyp benign/malignant classification."}, {"title": "4.1 Experimental Setup", "content": "Our experiments utilized a variety of publicly available polyp datasets commonly used for segmentation and detection tasks in medical imaging. These datasets include ETIS [36], CVC-ClinicDB (CVC-612) [37], CVC-ColonDB [38], CVC-300 [39], Kvasir [40], and PolypGen [7]. The PolypGen dataset, which is sourced from six different centers (C1 to C6), offers a broad range of polyp images with corresponding segmentation masks, as well as bounding box annotations for sequential data (Seq1-23).\nOwn Datasets: In addition to the publicly available datasets, we also contributed two datasets specifically designed for our experiments. The first, named the Polyplus dataset, was created by carefully selecting 100 representative polyp images from a larger collection of 1,450 image-mask pairs sourced from Kvasir and CVC-ClinicDB. These images were annotated with detailed textual descriptions by expert colonoscopists, making Polyplus a valuable resource for integrating visual and textual modalities. The second dataset consists of 385 benign and 77 malignant polyp images, annotated with classification labels to support experiments in polyp benign/malignant classification."}, {"title": "4.1.1 Datasets", "content": "Public Datasets: Our experiments utilized a variety of publicly available polyp datasets commonly used for segmentation and detection tasks in medi-cal imaging. These datasets include ETIS [36], CVC-ClinicDB (CVC-612) [37], CVC-ColonDB [38], CVC-300 [39], Kvasir [40], and PolypGen [7]. The Polyp-Gen dataset, which is sourced from six different cen-ters (C1 to C6), offers a broad range of polyp images with corresponding segmentation masks, as well as bounding box annotations for sequential data (Seq1-23).\nOwn Datasets: In addition to the publicly available datasets, we also contributed two datasets specifically designed for our experiments. The first, named the Polyplus dataset, was created by carefully selecting 100 representative polyp images from a larger collection of 1,450 image-mask pairs sourced from Kvasir and CVC-ClinicDB. These images were annotated with detailed textual descriptions by expert colonoscopists, making Polyplus a valuable resource for in-tegrating visual and textual modalities. The second dataset consists of 385 benign and 77 malignant polyp images, annotated with classification labels to sup-port experiments in polyp benign/malignant classifi-cation."}, {"title": "4.1.2 Downstream Tasks", "content": "We conducted our experiments across several downstream tasks, including polyp segmentation, polyp benign/malignant classification, and polyp detection. Each task was carefully designed with specific dataset configurations and model settings, as detailed below:\nPolyp Segmentation: For the segmentation task, following the PraNet standard, we followed the PraNet standard and trained our model on 1,450 image-mask pairs sourced from the Kvasir and CVC-ClinicDB datasets. Additionally, the PolypGen dataset (centers C1-C6) was incorporated into the evaluation process to ensure a comprehensive assessment across diverse clinical environments. To improve the model's generalization, we introduced two augmented datasets: the Segmentation Mask Only set, containing 1,450 image-mask pairs generated solely from the original training masks, and the Segmentation Mask-Text Description set, with 1,450 pairs generated using both textual descriptions and segmentation masks from the same training set.\nPolyp Classification: For the benign/malignant classification task, we used a dataset of 385 benign and 77 malignant polyp images, split into training (308 benign, 61 malignant), validation (38 benign, 7 malignant), and test sets (39 benign, 9 malignant) for model evaluation. To address class imbalance, we augmented the malignant class with an additional 247 images generated by our PSDM.\nPolyp Detection: For detection, we used the Polyp-Gen dataset, excluding 64 images from C3 without positive labels. The training set included images from centers C1 to C5 and sequences Seq1-15 (Positive) and Seq1-13 (Negative). The test set comprised images from C6 and sequences Seq16-23 (Positive) and Seq14-23 (Negative). We also introduced two augmented datasets for detection: the Bounding Box Only set, with bounding boxes regressed from the Segmentation Mask Only set, and the Bounding Box-Text Description set, with bounding boxes from the Segmentation Mask-Text Description set. Bounding boxes were defined by the smallest rectangle enclosing each polyp in the segmentation masks."}, {"title": "4.1.3 Baseline Models and Implementations", "content": "For the segmentation task, we employ three state-of-the-art models: PraNet [41], Polyp-PVT [42], and Polyp-CASCADE [43]. PraNet was selected for its proven effectiveness in capturing both local and global features, Polyp-PVT for its superior performance in handling multi-scale features, and Polyp-CASCADE for its robust architecture in medical segmentation tasks. For the classification task, we adopt ResNet [44], a widely used model in medical image analysis known for its effectiveness in classification tasks. For the polyp detection task, we utilize YOLOv5 [45], chosen for its high performance and efficiency in real-time object detection, which is critical in clinical settings. The hyperparameters for each model are provided in Table 2, categorized by task."}, {"title": "4.2 Experiments Results", "content": "In our experiments, we evaluated the performance of the segmentation models using two key metrics: mean Dice coefficient (mDice) and mean Intersection over Union (mIoU). These metrics are crucial in medical image segmentation, as they measure the overlap between the predicted segmentations and the ground truth, with higher values indicating better segmentation performance."}, {"title": "4.2.1 Polyp Segmentation", "content": "In our experiments, we evaluated the performance of the segmentation models using two key metrics: mean Dice coefficient (mDice) and mean Intersection over Union (mIoU). These metrics are crucial in medical image segmentation, as they measure the overlap between the predicted segmentations and the ground truth, with higher values indicating better segmentation performance.\nImpact on Generalization: The results indicate a consistent improvement in both mDice and mIoU scores across models and datasets with each augmentation strategy, underscoring the effectiveness of our approach in enhancing model generalization. It is important to note that the PSDMS strategy refers to pretraining the models using the Segmentation Mask Only set, where only mask annotations are employed, focusing purely on spatial structure. In contrast, the PSDMM strategy involves pretraining the models using a combination of both the Segmentation Mask Only set and the Segmentation Mask-Text Description set. Specifically, across the CVC-300, ClinicDB, Kvasir, CVC-ColonDB, and ETIS datasets, the best performance was generally observed when using the PSDMM augmentation strategy. This approach, which combines both segmentation masks and textual descriptions through compositional prompts, significantly aids in capturing finer details and enhancing spatial contextualization, leading to better segmentation outcomes. For instance, PraNet's overall mDice increased from 74.27 (Baseline) to 76.70 (PSDMM), while Polyp-PVT and Polyp-CASCADE showed similar trends, with improvements in both mDice and mIoU across the board. These results further highlight the utility of augmentation strategies, particularly the PSDMM approach, in improving the generalization capabilities of the models, enabling them to segment polyps more accurately.\nEvaluation on Complex Datasets: The models demonstrated robust performance on the PolypGen test set, as detailed in Table 4. It's important to note the distinction between the experiments presented in Tables 3 and 4. Table 3 focuses on evaluating the models across standard segmentation datasets like CVC-300 and Clinic-DB, which are commonly used benchmarks in polyp segmentation research. In contrast, Table 4 presents the results on the PolypGen dataset, which includes more challenging subsets (dataC4 and dataC5) known for their diverse polyp appearances. Notably, the proposed method showed substantial improvements on more challenging datasets such as ETIS and the PolypGen subsets dataC4 and dataC5. Polyp-CASCADE, in particular, achieved one of the most significant performance boosts on the PolypGen dataC5 subset, reflecting its enhanced ability to generalize to cases with smaller or irregularly shaped polyps.\nOverall, these findings suggest that the proposed"}, {"title": "4.2.2 Polyp Classification", "content": "The dataset used for the classification task consisted of 308 benign and 61 malignant polyp images, presenting a significant class imbalance with malignant polyps being underrepresented. This imbalance posed challenges for model training, particularly in detecting malignant polyps, which are critical for clinical decision-making.\nIn the original imbalanced dataset experiment, we evaluated the performance of a ResNet classification model trained on this imbalanced dataset. The class imbalance negatively impacted the model's performance, resulting in lower accuracy, especially in identifying malignant polyps. To address this issue, we conducted an experiment using an augmented balanced dataset. Additional images of malignant polyps were generated using the prompt \"a V* polyp\" creating a more balanced class distribution and mitigating the adverse effects of the original imbalance.\nThe radar chart in Fig. 4 and the confusion matrices in Fig. 5 illustrate the performance improvements of the ResNet model trained on the augmented balanced dataset compared to the original imbalanced dataset, showing enhanced accuracy and reduced false negatives for malignant polyps. The results demonstrated that the ResNet model trained on the augmented balanced dataset exhibited substantial improvements, particularly in its ability to detect malignant polyps. The model's accuracy and sensitivity in identifying this minority class increased"}, {"title": "4.2.3 Polyp Detection", "content": "In addition to polyp segmentation and classification experiments, we conducted a polyp detection experiment using the YOLOv5 model to further validate"}, {"title": "4.2.4 Qualitative Diversity Evaluation", "content": "Fig. 6 visually evaluates the diversity of generated samples, showcasing distinct textures, shapes, and sizes of polyps created using compositional prompts. Fig. 7 illustrates the transition during denoising, where the segmentation mask initially guides low-frequency structures, such as shape and position, while text descriptions refine high-frequency details like texture and pathology. This transition is controlled by a weighting scheme, with the text prompt weight $\\lambda_{text} = (1 + cos(\\pi \\cdot t/T))/2$ increasing over"}, {"title": "5 Discussion", "content": "Recent advancements in generative models have shown promise for natural image synthesis [11]. However, challenges remain in medical imaging, particularly due to the complexity and variability of medical data [46]. Medical data are dynamic, evolving over time and varying across institutions, limiting the availability of comprehensive longitudinal datasets for training robust models. To address these challenges, our PSDMM strategy integrates multimodal data through compositional prompts, demonstrating substantial benefits, especially in complex datasets like PolypGen.\nA limitation of our approach is the absence of established objective metrics for quantitatively evaluating the fidelity of synthetic medical images. While our model demonstrates the ability to generate visually high-quality images, a lack of rigorous quantitative evaluations, such as fidelity metrics or comparisons with clinical benchmarks, leaves the true clinical relevance of the generated images uncertain.\nLooking forward, integrating synthetic data into clinical workflows presents a promising yet challenging frontier. Synthetic datasets offer opportunities to address class imbalances and simulate rare conditions. However, ensuring clinical interpretability, reliability, and utility for decision-making will be crucial for adoption in healthcare settings. Future research should focus on robust evaluation frameworks to assess the visual quality and clinical relevance of synthetic images."}, {"title": "6 Conclusion", "content": "In this work, we introduced a novel compositional prompt-guided diffusion model designed to integrate independently annotated datasets for colorectal cancer (CRC) imaging. By leveraging our Progressive Spectrum Diffusion Model (PSDM), which utilizes a frequency-based prompt spectrum to progressively refine images from coarse spatial structures to fine-grained details, we successfully enhanced the model's performance by generating more diverse polyp images through the incorporation of richer clinical information. The resulting images significantly improved polyp segmentation, detection, and classification tasks, ultimately contributing to more accurate diagnosis of colorectal cancer."}]}