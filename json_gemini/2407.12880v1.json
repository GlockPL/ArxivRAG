{"title": "Cross-Modal Augmentation for Few-Shot Multimodal Fake News Detection", "authors": ["Ye Jiang", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Xingyi Song", "Diana Maynard"], "abstract": "The nascent topic of fake news requires automatic detection methods to quickly learn from limited annotated samples. Therefore, the capacity to rapidly acquire proficiency in a new task with limited guidance, also known as few-shot learning, is critical for detecting fake news in its early stages. Existing approaches either involve fine-tuning pre-trained language models which come with a large number of parameters, or training a complex neural network from scratch with large-scale annotated datasets. This paper presents a multimodal fake news detection model which augments multimodal features using unimodal features. For this purpose, we introduce Cross-Modal Augmentation (CMA), a simple approach for enhancing few-shot multimodal fake news detection by transforming n-shot classification into a more robust (n \u00d7 z)-shot problem, where z represents the number of supplementary features. The proposed CMA achieves SOTA results over three benchmark datasets, utilizing a surprisingly simple linear probing method to classify multimodal fake news with only a few training samples. Furthermore, our method is significantly more lightweight than prior approaches, particularly in terms of the number of trainable parameters and epoch times. The code is available here: https://github.com/zgjiangtoby/FND_fewshot", "sections": [{"title": "1. Introduction", "content": "The recent proliferation of social media has not only trans-formed the landscape of information exchange, but also led to the pernicious spread of fake news. The detection and miti-gation of fake news have consequently become pivotal areas of research [1, 2]. Traditional approaches, primarily relying on textual analysis, have shown limitations due to the sophisti-cated and multi-faceted nature of fake news [3, 4]. In response, many studies have incorporated multimodal methods that con-sider both text and accompanying images, yielding a more com-prehensive and effective framework for identifying and debunk-ing fake news [5, 6].\nTo explore the inconsistent semantics between text and im-age in fake news, many studies have either incorporated con-trastive learning to achieve better alignment between image-text pairs [7], or designed complex neural networks to strengthen the deep-level fusion of multimodal features [8, 9]. The for-mer relies on contrastive loss to align image-text pairs, but most image-text pairs in fake news are inherently not matched [10], and different image-text pairs may also have potential corre-lations [11], which can consequently confuse the model. The latter typically needs to be trained from scratch, which is fun-damentally bounded by the availability of large-scale annotated data [12, 13].\nIn contrast to machines, the process of concept learning in humans involves integrating multimodal signals and represen-tations [14, 15]. When processing uncertain information, peo-ple inherently seek help from other modalities. This capability enables humans to learn from a limited number of samples by incorporating cross-modal information, as shown in Figure 1. Meanwhile, the efficacy of fake news detection (FND) in the context of nascent topics, such as COVID-19, remains a signif-icant challenge for prevailing strategies. This difficulty is com-pounded by the lack of extensive data and annotations in the target domain, underscoring the critical role of few-shot learn-ing in mitigating the spread of early-stage fake news [16]."}, {"title": "2. Related work", "content": "Unimodal fake news detection aims to extract significant se-mantics from either news texts or images. Given the precision of semantics in text, previous approaches have concentrated on the task of text-based unimodal fake news detection. Early works focused on analyzing statistical characteristics of text (e.g., length, punctuation, exclamation marks) [23] and meta-data (e.g., likes, shares) [24, 25] for manual fake news detec-tion. However, these manual feature engineering approaches are time-consuming and struggle with processing large-scale, real-time data [26, 27].\nThe advent of deep learning has significantly advanced au-tomated fake news detection. These methods primarily utilize deep learning models like BiLSTM [28, 29], GNNs [30], and pre-trained models (e.g., BERT, GPT) [31, 32, 33] to analyze text features, extracting various attributes such as emotional [34], stance-based [35], and stylistic elements [36]. However, the recent proliferation of multimodal information (text, im-ages, videos) in social networks has shifted the propagation of fake news from solely text-based to multimodal formats."}, {"title": "2.2. Multimodal fake news detection", "content": "Multimodal methods employing cross-modal discriminative patterns have been introduced, aiming to enhance performance in fake news detection. For example, MCAN [36] employs multiple co-attention layers to more effectively integrate tex-tual and visual features in detecting fake news. CAFE [5] quantifies cross-modal ambiguity through the assessment of the Kullback-Leibler (KL) divergence among the distributions of unimodal features. LIIMR [37] determines the modality that exhibits greater confidence in the context of fake news de-tection. COOLANT [7] focuses on improving the alignment between image and text representations, utilizing contrastive learning for finer semantic alignment and cross-modal fusion to learn inter-modality correlations. However, these approaches are limited by the need for extensive annotated data in the con-text of emerging topics."}, {"title": "2.3. Cross-modal few-shot fake news detection", "content": "Few-shot learning is designed to master new tasks using a limited number of labeled examples [38]. Current few-shot learning methodologies, such as prototypical networks, acquire class-specific features in metric spaces for swift adaptation to novel tasks [39, 40]. Within computer vision, the concept of few-shot domain adaptation is explored in image classification for transferring knowledge to novel target domains [41, 42]. In natural language processing, meta-learning is suggested as a means to enhance few-shot learning performance in tasks like language modeling [43, 44] and misinformation detection [45, 46]. To our knowledge, the application of few-shot multi-modal fake news detection through cross-modal augmentation remains unexplored in existing literature.\nMeanwhile, previous multimodal learning approaches have sought to enhance unimodal tasks by leveraging data from var-ious modalities [47, 48]. With multimodal pre-trained models achieving notable success in classic vision tasks [22, 49], there is a growing interest in formulating more efficient cross-modal augmentation techniques.\nHowever, the prevailing techniques are based on success-ful strategies originally designed for multimodal foundational models. For example, CLIP utilizes linear probing [50, 51] and comprehensive fine-tuning [52] in its application to down-stream tasks. CLIP-Adapter [53] and Tip-Adapter [54] draw inspiration from parameter-efficient finetuning approaches [55] that focus on optimizing lightweight MLPs while maintaining a fixed encoder. However, all the aforementioned methods, in-cluding WiSE-FT [56], employ an alternative modality, such as textual labels, as classifier weights, and continue to compute a unimodal Softmax loss on few-shot tasks. In contrast, this paper demonstrates the enhanced effectiveness of incorporating additional modalities as training samples."}, {"title": "3. Methodology", "content": "The proposed CMA enhances few-shot fake news detection by integrating samples from different modalities, and extends traditional unimodal few-shot classification to leverage the rich-ness of cross-modal data, as shown in Figure 2.\nThis section starts with a standard unimodal few-shot FND framework, and the loss function is discussed. Then, it ex-tends this to multiple modalities, assuming each training exam-ple is a combination of five different modalities. The modality-specific features are passed through MLP linear classifiers to obtain their inferences. Finally, we combine the inferences and train a meta-linear classifier to compute the final prediction."}, {"title": "3.1. Unimodal few-shot FND", "content": "Initially, unimodal few-shot FND learns from a labeled dataset of (x, y) \u2208 X, where x is either the text or image pass-ing to a pre-trained feature encoder (\u00b7). The ultimate goal is to allocate a binary classification label of y \u2208 {0,1}, in which 0 denotes real news and 1 denotes fake news. We assume only an n-shot subset (xi, yi) from X is provided for training, where i \u2208 [1,n] (i.e., n samples per class); the rest of X is used as the test set.\nTherefore, the standard unimodal FND can be denoted as minimizing the cross-entropy loss L:\n$$L = \u2212(y_i log(\\hat{y}) + (1 \u2212 y_i)log(1 \u2013 \\hat{y}))$$ (1)\nwhere \\(\\hat{y}\\) is the model inference from the linear classifier MLP after softmax.\n$$\\hat{y} = softmax(MLP(f(x_i)) = \u2212log(\\frac{e^{w_{y^*}f}}{\\Sigma e^{w_y}})$$ (2)\nwhere f is the feature representation from an MLP layer after the unimodal feature encoder, and w_{y^*} and w_{y} are the weights of the ground truth label and the predicted label, respectively."}, {"title": "3.2. Multimodal few-shot FND", "content": "To extend to multimodal FND, we assume that for each train-ing sample, f is a combination of five feature representations: 1) a text-only feature ft; 2) an image-only feature fm; 3) con-catenation of L2 normalized fc = [ft \u2295 fm], where \u2295 is the concatenation operation; 4) an image-text cross-attended fea-ture fmt; 5) a text-image cross-attended feature fim. The cross-attention mechanism, which swaps the text query Qt with the image query Qm, to obtain the cross-attended feature fmt is de-noted as follows:\n$$f_{mt} = CrossAttm_{t}(Q_m, K_t, V_t) = softmax(\\frac{Q_m K_t}{\\sqrt{d}})V_t$$ (3)\nIn contrast, by swapping the image query Qm with the text query Qt, the cross-attended feature fim can be obtained:\n$$f_{im} = CrossAttm_{m}(Q_t, K_m, V_m) = softmax(\\frac{Q_t K_m}{\\sqrt{d}})V_m$$ (4)\nwhere Kt and Km represent the key vectors for text and image features respectively, Vt and Vm denote the corresponding value vectors, and d refers to the dimensionality of the model.\nFor the sake of simplification, we assume that the number of z different types of features are considered as distinct modalities. Therefore, each modality can be processed through the linear classifier MLP in the unimodal learning approach, as discussed above, to obtain five inferred probabilities.\nInspired by the Representer Theorem [57], which indicates that optimally trained classifiers can be depicted as linear com-binations of their training samples, we concatenate the five in-ferred probabilities as a new input to a meta-linear MLP classi-fier for making the final prediction:\n$$\\hat{y} = softmax(MLP(f_t \\oplus f_m \\oplus f_c \\oplus f_{mt} \\oplus f_{im})$$ (5)\nInstead of optimizing modality-specific weights indepen-dently, linear classification through the proposed CMA simul-taneously determines all weights to minimize the training loss. Consequently, we convert the standard n-shot classification to an (n \u00d7 z)-shot problem. The training details for CMA are pre-sented in Algorithm 1."}, {"title": "4. Experiment", "content": "This section details experiments conducted to validate the effectiveness of the proposed approach. Initially, benchmark datasets are introduced, followed by the implementation details for experiments. The experimental results are analyzed in com-parison to unimodal, multimodal, and few-shot FND methods. Finally, detailed analyses are provided to enhance the under-standing of the proposed methods."}, {"title": "4.1. Data Setup", "content": "Three publicly available datasets are utilized for evaluation. PolitiFact [13] comprises a dataset of political news catego-rized as either fake or real by expert evaluators and is part of the benchmark FakeNewsNet project. Using the provided data crawling scripts, news with no images or invalid image URLS are removed, resulting in 198 multimodal news articles.\nGossipCop [13] features entertainment stories rated on a scale from 0 to 10, with stories scoring less than five classi-fied as fake news by the author of FakeNewsNet. Using the same retrieval strategies as PolitiFact, 6,805 multimodal news articles are collected.\nWeibo [58], a dataset sourced from Chinese social media platforms, comprises a multimodal fake news collection featur-ing both text and images. Authentic news items were crawled from a reputable source (Xinhua News), and fake news was ob-tained from Weibopiyao, an official rumor refutation platform of Weibo, that aggregates content either through crowdsourc-ing or official rumor refutation efforts. The same pre-processing methods as in previous work [7] are followed, resulting in 7,853 Chinese news articles.\nNotably, a news article might be accompanied by multiple images. To find the most relevant image, the cosine similarity between each image and its corresponding text is calculated, and the image-text pair with the highest similarity, as deter-mined by the pre-trained CLIP, is retained. The resulting dataset statistics are presented in Table 1."}, {"title": "4.2. Implementation details", "content": "The pre-trained OpenAI CLIP (ViT-B-32) [22] and Chinese CLIP (ViT-B-16) [59] models are utilized to respectively ex-tract text and image features for different languages. The hidden size for the cross-attention projection layer is 512, which is the same as the output dimension of CLIP encoders. The AdamW optimizer is employed with a learning rate of 1e-3 and a de-cay parameter of 1e-2. The model is trained for 20 epochs, with the optimal checkpoint being determined by peak valida-tion performance. Early stopping is utilized with a patience of three epochs.\nIn the few-shot context, the model is trained using a restricted set of samples, selected from the dataset to form an n-shot sce-nario. Here, n \u2208 [2, 8, 16, 32] represents the number of samples for each class, while the remainder of the samples are reserved for testing purposes. Given that the data quality of the sam-pled training set might significantly impact the model's perfor-mance, data sampling is repeated 10 times with random seeds, and the average score is reported after excluding the highest and lowest scores."}, {"title": "4.3. Benchmarked Models", "content": "The proposed CMA is benchmarked against 11 represen-tative models. Specifically, we extensively compare the pro-posed method with unimodal approaches (1)-(3), multimodal approaches (4)-(6), and the few-shot approaches (7)-(11).\n(1) dEFEND [60] utilizes the hierarchical attention network for FND. In this study, we remove the user comments from the original model.\n(2) LDA-HAN [33] integrates pre-calculated topic distribu-tions from Latent Dirichlet Allocation into a hierarchical atten-tion network for text classification.\n(3) FT-ROBERTa is a standard, fine-tuned version of the pre-trained language model RoBERTa; we use Huggingface Trainer to conduct the fine-tuning experiment.\n(4) SpotFake [61] employs the pre-trained VGG and BERT for extracting image and text features, respectively, and then concatenating them for final classification."}, {"title": "4.4. Results", "content": "Table 2 demonstrates the FND accuracy comparison between the proposed CMA and all the baselines at various few-shot settings over the three datasets.\nComparing with unimodal baselines. First, we assess the accuracy of both unimodal approaches and the proposed CMA to evaluate their performances. Overall, CMA outperforms the best unimodal approach, FT-RoBERTa, achieving a 15.6% en-hancement in average accuracy across all datasets, demonstrat-ing its superiority in few-shot scenarios.\nSurprisingly, FT-RoBERTa emerges as the most accurate model among both unimodal and multimodal approaches, sug-gesting that conventional fine-tuning methods can reach com-petitive levels of performance solely through the analysis of textual information from fake news. However, this method ne-cessitates increased epoch time due to the adjustment of numer-ous parameters in the pre-trained language model (as shown in Table 4), making it impractical for real-world few-shot FND applications.\nLDA-HAN yields the second best in accuracy among uni-modal models, with dEFEND coming in next. This could be at-tributed to two factors: firstly, the vanilla LDA model struggles to effectively generate topics from short texts, a characteristic of the datasets from GossipCop and Weibo (as detailed in Ta-ble 1) used in LDA-HAN; secondly, the employment of GloVe embeddings for initializing LDA-HAN and dEFEND may not perform as effectively as the contextualized embeddings gener-ated by the BERT family.\nComparing with multimodal baselines. We evaluate the performance of CMA in comparison with multimodal ap-proaches. CMA outperforms the best multimodal baseline, CAFE, with a 27.4% improvement in average accuracy across all datasets. The reason might be that the complex architecture of multimodal approaches inherently comes with a large num-ber of trainable parameters, which might easily lead to overfit-ting in few-shot scenarios.\nExcluding FT-ROBERTa, all multimodal baselines outper-form unimodal models on average, showing that the inclusion of the image modality can significantly affect model accuracy. While these multimodal approaches excel in scenarios with abundant data, their effectiveness heavily relies on the availabil-ity of high-quality annotated training samples, which may not be readily accessible during the initial stages of FND. More-over, all multimodal approaches utilize pre-trained unimodal models, such as VGG, ResNet, and BERT, to independently ex-tract features from images and text. Yet, since these unimodal models are trained separately, merging their extracted features during the multimodal fusion process could potentially intro-duce noise[20].\nComparing with few-shot baselines. The effectiveness of the proposed CMA is evaluated in comparison with the lat-est prompt-based few-shot models. CMA outperforms the best few-shot baseline, P&A, with a 3.3% improvement in average accuracy, showing that using unimodal features to assist mul-timodal probing without prompting the pre-trained language model could also benefit the FND task.\nWhile P&A demonstrates performance on par with CMA, it requires the pre-calculation of a news proximity graph. How-ever, such social context data may not always be accessible, particularly in datasets not sourced from Twitter, like Weibo. After analyzing PET and KPT, it's evident that these methods yield comparable outcomes, likely due to variations introduced by the manually crafted verbalizers used in prompting. This un-derscores the significance of hand-designed discrete templates in prompt-based learning. Concurrently, M-SAMPLE, a multi-modal adaptation of KPL, demonstrates superior performance, suggesting that incorporating image modality can significantly enhance FND effectiveness."}, {"title": "5. Analysis", "content": "Ablation study\nWe investigate the impact of key components in CMA by assessing the framework's performance in a range of complete and partial configurations. In each experiment, CMA is selec-tively utilized by removing different components, followed by training the framework from scratch. The results are averaged over five random seeds in each shot, and indicate the perfor-mance decay of CMA in the absence of each component in most configurations, underscoring the significance of each key mod-ule within CMA, as shown in Table 3.\nSpecifically, removing the cross-attention from the CMA (i.e., -cross) results in a slight decrease in accuracy, showing that the cross-attended features from text and image capture se-mantic correlations and contribute to improved performance. Further removal of the meta-linear layer from the CMA (i.e., -meta) transforms the model into a standard n-shot classifica-tion, where it simply classifies concatenated multimodal fea-tures. This leads to a significant decrease in accuracy, empha-sizing the importance of jointly updating all modality-specific weights in a meta-linear classifier for cross-modal adaptation and accuracy improvement. The meta-linear layer integrates modality-specific features, resembling an ensemble that trans-forms n-shot classification into a more robust (n \u00d7 z)-shot prob-lem, enhancing cross-modal adaptation in few-shot classifica-tion.\nAdditionally, experiments are performed by excluding either the image features (-img) or the text features (-txt), relying solely on the remaining modality for classification. Such setups led to additional reductions in accuracy, underscoring the com-parative importance of text over image features in FND. This highlights the complexities in multimodal FND tasks, where the spatial discrepancies between visual and textual semantics tend to be more subtle than in broader multimodal datasets."}, {"title": "5.2. Stablility test", "content": "Given the selection of few-shot examples can significantly affect the model performance, we assess the stability of the CMA and other prompt-based baselines by measuring the stan-dard deviation of accuracies in the few-shot settings, as shown in Figure 3.\nOverall, the standard deviation for all models decreases in tandem with an increase in the number of n-shot settings, under-scoring the importance of augmenting training examples in few-shot scenarios. This augmentation can be further observed that the standard deviation of the CMA tends to be the most stable among the few-shot approaches, indicating that the ensemble of unimodal features in the meta-linear layer can enhance the robustness of multimodal fusion in classification. Additionally, the GossipCop dataset exhibits greater instability compared to the PolitiFact dataset. This instability may be attributed to the semantic complexity in GossipCop, which is responsible for the lower accuracy across all models."}, {"title": "5.3. Model efficiency", "content": "Given the CMA achieves the best performance with a sur-prisingly simple augmentation, we further explore its efficiency in comparison to other baseline models. Table 4 showcases a comparison of the accuracies and epoch times between base-lines and the CMA. The average accuracy of each model is de-termined in a 16-shot setting as shown in Table 2, along with the recording of average epoch times for each model. All ex-periments are tested with batch size 32 on a single RTX 4090 GPU in the GossipCop dataset for a fair comparison.\nAmong unimodal models, dEFEND and LDA-HAN exhibit comparable accuracy and epoch times, attributed to their anal-ogous hierarchical architectural design. While FT-RoBERTa exceeds the performance of various unimodal (e.g., 18% higher than dEFEND) and multimodal methods (e.g., 6.9% higher than CAFE), it requires modifying a significant number of trainable parameters, thus extending epoch durations (on average, four minutes per epoch) relative to other unimodal baselines.\nIn the multimodal models, SAFE yields the lengthiest epoch durations owing to its prerequisite for independently pre-generating image descriptions. Although Spotfake achieves the fastest epoch duration due to its simple concatenation of the image and text features from the BERT and VGG respectively, it achieves the worst performance compared with other models. CAFE achieves the best multimodal FND outcomes by integrat-ing a degree of ambiguity in the similarity across text and image features, albeit at the cost of marginally increased model com-plexity and consequently, slightly extended epoch durations.\nAll few-shot baselines demonstrate significant improvements over both unimodal and multimodal counterparts, indicating the suboptimality of traditional methods in contexts with limited annotated data. Specifically, the integration of external knowl-edge into the prompt-tuning phase by both KPL and KPT re-sults in comparable epoch durations. However, KPL's design of an FND-specific prompt may underlie its superior perfor-mance over KPT. PET records the lengthiest epoch duration among the few-shot baselines, potentially due to the repeated fine-tuning of the PLM for reconfiguring input examples with the task description. P%A not only achieves the second-best performance but also the second-shortest epoch durations, ben-efiting from the integration of user engagements. However, it incorporates an external alignment module to correlate user en-gagement with the PLM's predictions, consequently increasing epoch times relative to CMA. Finally, CMA is more efficient and precise as it avoids the need for extensive parameter fine-tuning and does not depend on intensive image augmentation processes. Additionally, the inclusion of linear probing layers atop the image and text features presents a more streamlined ap-proach than extensive fine-tuning and precise-crafted complex model designs."}, {"title": "5.4. Domain shift analysis", "content": "Real-world fake news demonstrates significant distribution discrepancies, which is also referred to as domain shift [?? ]. Consequently, automatic FND methods are required to rapidly adapt to emerging topics by using limited resources.\nTo address this, we investigate the cross-domain capability of the proposed CMA against three strong few-shot FND baselines (i.e., P&A, PET and KPT). Considering Politifact's focus on political news using formal language and Gossipcop's emphasis on entertainment and celebrity narratives in a more casual tone, we first utilize Politifact for training and Gossipcop for testing, later inverting this arrangement.\nThe outcomes following domain shift are presented in Ta-ble 5. Notably, while the CMA model records the highest aver-age accuracy among the few-shot baselines, the performance of each model markedly differs from that observed in the compar-ison experiments (as shown in Table 2). For example, KPT ex-hibits the strongest performance in both 2- and 8-shot scenarios in Goss\u2192Poli. PET and P%A also achieve the highest perfor-mance in Goss\u2192Poli and Poli\u2192Goss respectively, highlighting the disparity between present few-shot FND methodologies and their adaptability to domain adaptation."}, {"title": "5.5. Feature visualization", "content": "At last, we present a visual comparison of the features ex-tracted by M-SAMPLE and CMA, both of which are multi-modal few-shot approaches. This involves the visualization of multimodal features alongside an assessment of their semantic correlations. For each dataset, a specific sample is chosen, with the corresponding multimodal features depicted in Figure 4.\nObservations indicate that: 1) CMA can capture more con-sistent features from the image-text pair of fake news than those of M-SAMPLE. For example, although both M-SAMPLE and CMA successfully correlate the flag in the image with the word \"Chinese\" in the text, CMA can also identify the seman-tic meaning of \"moon landing\" between the text and image in the PolitiFact example; 2) The proposed CMA is more accu-rate in capturing important features from the image than M-SAMPLE. For example, although both models can identify the person \"Nicole Kidman\" and \"black tarantula\" in both the text and the image in the GossipCop example, the image region of the tarantula slightly overlaps with that of Nicole Kidman pro-vided by M-SAMPLE. This is even more obvious in the Weibo example, as CMA successfully captures the \"blue\" color bar in the toothpaste, but M-SAMPLE fails to do so."}, {"title": "6. Conclusion", "content": "This paper introduced Cross-Modal Augmentation (CMA) for enhancing few-shot multimodal fake news detection by uti-lizing unimodal features to augment multimodal fusion. The proposed CMA leverages a pre-trained multimodal model for unimodal feature extraction and transforms n-shot classifica-tion into a robust (n \u00d7 z)-shot problem using class labels as additional one-shot training samples. The CMA, employing a simple linear classifier, achieves SOTA performance on three datasets in few-shot settings, and demonstrates greater effi-ciency than current approaches."}, {"title": "7. Limitation", "content": "We acknowledge limitations in this study including: 1) The evaluation of CMA's few-shot proficiency solely utilizes CLIP, future investigations will delve into how different multimodal models influence the proposed CMA; 2) Given the lack of mul-timodal information in certain datasets, this research adopted cosine similarity for image selection from multiple options, potentially leading to varied performance outcomes based on the text-image pairing technique employed; 3) CMA exhibits suboptimal domain shift performance, enhancing the architec-ture through the integration of knowledge distillation or domain adaptation techniques remains a prospect for future research."}]}