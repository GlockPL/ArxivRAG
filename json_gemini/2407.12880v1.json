{"title": "Cross-Modal Augmentation for Few-Shot Multimodal Fake News Detection", "authors": ["Ye Jiang", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Xingyi Song", "Diana Maynard"], "abstract": "The nascent topic of fake news requires automatic detection methods to quickly learn from limited annotated samples. Therefore, the capacity to rapidly acquire proficiency in a new task with limited guidance, also known as few-shot learning, is critical for detecting fake news in its early stages. Existing approaches either involve fine-tuning pre-trained language models which come with a large number of parameters, or training a complex neural network from scratch with large-scale annotated datasets. This paper presents a multimodal fake news detection model which augments multimodal features using unimodal features. For this purpose, we introduce Cross-Modal Augmentation (CMA), a simple approach for enhancing few-shot multimodal fake news detection by transforming n-shot classification into a more robust (n \u00d7 z)-shot problem, where z represents the number of supplementary features. The proposed CMA achieves SOTA results over three benchmark datasets, utilizing a surprisingly simple linear probing method to classify multimodal fake news with only a few training samples. Furthermore, our method is significantly more lightweight than prior approaches, particularly in terms of the number of trainable parameters and epoch times. The code is available here: https://github.com/zgjiangtoby/FND_fewshot", "sections": [{"title": "1. Introduction", "content": "The recent proliferation of social media has not only transformed the landscape of information exchange, but also led to the pernicious spread of fake news. The detection and mitigation of fake news have consequently become pivotal areas of research [1, 2]. Traditional approaches, primarily relying on textual analysis, have shown limitations due to the sophisticated and multi-faceted nature of fake news [3, 4]. In response, many studies have incorporated multimodal methods that consider both text and accompanying images, yielding a more comprehensive and effective framework for identifying and debunking fake news [5, 6].\nTo explore the inconsistent semantics between text and image in fake news, many studies have either incorporated contrastive learning to achieve better alignment between image-text pairs [7], or designed complex neural networks to strengthen the deep-level fusion of multimodal features [8, 9]. The former relies on contrastive loss to align image-text pairs, but most image-text pairs in fake news are inherently not matched [10], and different image-text pairs may also have potential correlations [11], which can consequently confuse the model. The latter typically needs to be trained from scratch, which is fundamentally bounded by the availability of large-scale annotated data [12, 13].\nIn contrast to machines, the process of concept learning in humans involves integrating multimodal signals and representations [14, 15]. When processing uncertain information, people inherently seek help from other modalities. This capability enables humans to learn from a limited number of samples by incorporating cross-modal information, as shown in Figure 1. Meanwhile, the efficacy of fake news detection (FND) in the context of nascent topics, such as COVID-19, remains a significant challenge for prevailing strategies. This difficulty is compounded by the lack of extensive data and annotations in the target domain, underscoring the critical role of few-shot learning in mitigating the spread of early-stage fake news [16]."}, {"title": "2. Related work", "content": "2.1. Unimodal fake news detection\nUnimodal fake news detection aims to extract significant semantics from either news texts or images. Given the precision of semantics in text, previous approaches have concentrated on the task of text-based unimodal fake news detection. Early works focused on analyzing statistical characteristics of text (e.g., length, punctuation, exclamation marks) [23] and metadata (e.g., likes, shares) [24, 25] for manual fake news detection. However, these manual feature engineering approaches are time-consuming and struggle with processing large-scale, real-time data [26, 27].\nThe advent of deep learning has significantly advanced automated fake news detection. These methods primarily utilize deep learning models like BiLSTM [28, 29], GNNs [30], and pre-trained models (e.g., BERT, GPT) [31, 32, 33] to analyze text features, extracting various attributes such as emotional [34], stance-based [35], and stylistic elements [36]. However, the recent proliferation of multimodal information (text, images, videos) in social networks has shifted the propagation of fake news from solely text-based to multimodal formats.\n2.2. Multimodal fake news detection\nMultimodal methods employing cross-modal discriminative patterns have been introduced, aiming to enhance performance in fake news detection. For example, MCAN [36] employs multiple co-attention layers to more effectively integrate textual and visual features in detecting fake news. CAFE [5] quantifies cross-modal ambiguity through the assessment of the Kullback-Leibler (KL) divergence among the distributions of unimodal features. LIIMR [37] determines the modality that exhibits greater confidence in the context of fake news detection. COOLANT [7] focuses on improving the alignment between image and text representations, utilizing contrastive learning for finer semantic alignment and cross-modal fusion to learn inter-modality correlations. However, these approaches are limited by the need for extensive annotated data in the context of emerging topics.\n2.3. Cross-modal few-shot fake news detection\nFew-shot learning is designed to master new tasks using a limited number of labeled examples [38]. Current few-shot learning methodologies, such as prototypical networks, acquire class-specific features in metric spaces for swift adaptation to novel tasks [39, 40]. Within computer vision, the concept of few-shot domain adaptation is explored in image classification for transferring knowledge to novel target domains [41, 42]. In natural language processing, meta-learning is suggested as a means to enhance few-shot learning performance in tasks like language modeling [43, 44] and misinformation detection [45, 46]. To our knowledge, the application of few-shot multimodal fake news detection through cross-modal augmentation remains unexplored in existing literature.\nMeanwhile, previous multimodal learning approaches have sought to enhance unimodal tasks by leveraging data from various modalities [47, 48]. With multimodal pre-trained models achieving notable success in classic vision tasks [22, 49], there is a growing interest in formulating more efficient cross-modal augmentation techniques.\nHowever, the prevailing techniques are based on successful strategies originally designed for multimodal foundational models. For example, CLIP utilizes linear probing [50, 51] and comprehensive fine-tuning [52] in its application to downstream tasks. CLIP-Adapter [53] and Tip-Adapter [54] draw inspiration from parameter-efficient finetuning approaches [55] that focus on optimizing lightweight MLPs while maintaining a fixed encoder. However, all the aforementioned methods, including WiSE-FT [56], employ an alternative modality, such as textual labels, as classifier weights, and continue to compute a unimodal Softmax loss on few-shot tasks. In contrast, this"}, {"title": "3. Methodology", "content": "The proposed CMA enhances few-shot fake news detection by integrating samples from different modalities, and extends traditional unimodal few-shot classification to leverage the richness of cross-modal data, as shown in Figure 2.\nThis section starts with a standard unimodal few-shot FND framework, and the loss function is discussed. Then, it extends this to multiple modalities, assuming each training example is a combination of five different modalities. The modality-specific features are passed through MLP linear classifiers to obtain their inferences. Finally, we combine the inferences and train a meta-linear classifier to compute the final prediction.\n3.1. Unimodal few-shot FND\nInitially, unimodal few-shot FND learns from a labeled dataset of (x, y) \u2208 X, where x is either the text or image passing to a pre-trained feature encoder (\u00b7). The ultimate goal is to allocate a binary classification label of y \u2208 {0,1}, in which 0 denotes real news and 1 denotes fake news. We assume only an n-shot subset (xi, yi) from X is provided for training, where i \u2208 [1,n] (i.e., n samples per class); the rest of X is used as the test set.\nTherefore, the standard unimodal FND can be denoted as minimizing the cross-entropy loss L:\n\\(L = -(y\u00a1log(\u0177) + (1 \u2212 yi)log(1 \u2013 \u0177))\\)\nwhere \u0177 is the model inference from the linear classifier MLP after softmax.\n\\(\u0177 = softmax(MLP(f(xi)) = \u2212log(\\frac{ewy*f}{\u03a3ew}))\\)\nwhere f is the feature representation from an MLP layer after the unimodal feature encoder, and wy and wy are the weights of the ground truth label and the predicted label, respectively.\n3.2. Multimodal few-shot FND\nTo extend to multimodal FND, we assume that for each training sample, f is a combination of five feature representations: 1) a text-only feature f\u2081; 2) an image-only feature fm; 3) concatenation of L2 normalized fc = [ft \u2295 fm], where \u2295 is the concatenation operation; 4) an image-text cross-attended feature fmt; 5) a text-image cross-attended feature fim. The cross-attention mechanism, which swaps the text query Q\u2081 with the image query Qm, to obtain the cross-attended feature fmt is denoted as follows:\n\\(fmt = CrossAttm\u2192t(Qm, Kt, V\u2081) = softmax(\\frac{QmK}{\\sqrt{d}})V\\)\nIn contrast, by swapping the image query Qm with the text query Q\u2081, the cross-attended feature fim can be obtained:\n\\(fim = CrossAtt\u2192m(Q\u2081, Km, Vm) = softmax(\\frac{Q\u2081Km}{\\sqrt{d}})Vm\\)\nwhere K\u2081 and Km represent the key vectors for text and image features respectively, V\u2081 and Vm denote the corresponding value vectors, and d refers to the dimensionality of the model.\nFor the sake of simplification, we assume that the number of z different types of features are considered as distinct modalities. Therefore, each modality can be processed through the linear classifier MLP in the unimodal learning approach, as discussed above, to obtain five inferred probabilities.\nInspired by the Representer Theorem [57], which indicates that optimally trained classifiers can be depicted as linear combinations of their training samples, we concatenate the five inferred probabilities as a new input to a meta-linear MLP classifier for making the final prediction:\n\\(\u0177 = softmax(MLP(f\u2081 \u2295 fm \u2295 fc \u2295 fmt \u2295 fim)\\)\nInstead of optimizing modality-specific weights independently, linear classification through the proposed CMA simultaneously determines all weights to minimize the training loss. Consequently, we convert the standard n-shot classification to"}, {"title": "4. Experiment", "content": "This section details experiments conducted to validate the effectiveness of the proposed approach. Initially, benchmark datasets are introduced, followed by the implementation details for experiments. The experimental results are analyzed in comparison to unimodal, multimodal, and few-shot FND methods. Finally, detailed analyses are provided to enhance the understanding of the proposed methods.\n4.1. Data Setup\nThree publicly available datasets are utilized for evaluation. PolitiFact [13] comprises a dataset of political news categorized as either fake or real by expert evaluators and is part of the benchmark FakeNewsNet project. Using the provided data crawling scripts, news with no images or invalid image URLS are removed, resulting in 198 multimodal news articles.\nGossipCop [13] features entertainment stories rated on a scale from 0 to 10, with stories scoring less than five classified as fake news by the author of FakeNewsNet. Using the same retrieval strategies as PolitiFact, 6,805 multimodal news articles are collected.\nWeibo [58], a dataset sourced from Chinese social media platforms, comprises a multimodal fake news collection featuring both text and images. Authentic news items were crawled from a reputable source (Xinhua News), and fake news was obtained from Weibopiyao, an official rumor refutation platform of Weibo, that aggregates content either through crowdsourcing or official rumor refutation efforts. The same pre-processing\n4.2. Implementation details\nThe pre-trained OpenAI CLIP (ViT-B-32) [22] and Chinese CLIP (ViT-B-16) [59] models are utilized to respectively extract text and image features for different languages. The hidden size for the cross-attention projection layer is 512, which is the same as the output dimension of CLIP encoders. The AdamW optimizer is employed with a learning rate of le-3 and a decay parameter of le-2. The model is trained for 20 epochs, with the optimal checkpoint being determined by peak validation performance. Early stopping is utilized with a patience of three epochs.\nIn the few-shot context, the model is trained using a restricted set of samples, selected from the dataset to form an n-shot scenario. Here, n \u2208 [2, 8, 16, 32] represents the number of samples for each class, while the remainder of the samples are reserved for testing purposes. Given that the data quality of the sampled training set might significantly impact the model's performance, data sampling is repeated 10 times with random seeds, and the average score is reported after excluding the highest and lowest scores.\n4.3. Benchmarked Models\nThe proposed CMA is benchmarked against 11 representative models. Specifically, we extensively compare the proposed method with unimodal approaches (1)-(3), multimodal approaches (4)-(6), and the few-shot approaches (7)-(11).\n(1) dEFEND [60] utilizes the hierarchical attention network for FND. In this study, we remove the user comments from the original model.\n(2) LDA-HAN [33] integrates pre-calculated topic distributions from Latent Dirichlet Allocation into a hierarchical attention network for text classification.\n(3) FT-ROBERTa is a standard, fine-tuned version of the pre-trained language model RoBERTa; we use Huggingface Trainer to conduct the fine-tuning experiment.\n(4) SpotFake [61] employs the pre-trained VGG and BERT for extracting image and text features, respectively, and then concatenating them for final classification."}, {"title": "5. Analysis", "content": "5.1. Ablation study\nWe investigate the impact of key components in CMA by assessing the framework's performance in a range of complete and partial configurations. In each experiment, CMA is selectively utilized by removing different components, followed by training the framework from scratch. The results are averaged over five random seeds in each shot, and indicate the performance decay of CMA in the absence of each component in most configurations, underscoring the significance of each key module within CMA, as shown in Table 3.\nSpecifically, removing the cross-attention from the CMA (i.e., -cross) results in a slight decrease in accuracy, showing that the cross-attended features from text and image capture semantic correlations and contribute to improved performance. Further removal of the meta-linear layer from the CMA (i.e., -meta) transforms the model into a standard n-shot classification, where it simply classifies concatenated multimodal features. This leads to a significant decrease in accuracy, emphasizing the importance of jointly updating all modality-specific weights in a meta-linear classifier for cross-modal adaptation and accuracy improvement. The meta-linear layer integrates modality-specific features, resembling an ensemble that transforms n-shot classification into a more robust (n \u00d7 z)-shot prob\n5.2. Stablility test\nGiven the selection of few-shot examples can significantly affect the model performance, we assess the stability of the CMA and other prompt-based baselines by measuring the standard deviation of accuracies in the few-shot settings, as shown in Figure 3.\nOverall, the standard deviation for all models decreases in tandem with an increase in the number of n-shot settings, underscoring the importance of augmenting training examples in few-shot scenarios. This augmentation can be further observed that the standard deviation of the CMA tends to be the most stable among the few-shot approaches, indicating that the ensemble of unimodal features in the meta-linear layer can enhance the robustness of multimodal fusion in classification. Additionally, the GossipCop dataset exhibits greater instability compared to the PolitiFact dataset. This instability may be attributed to the semantic complexity in GossipCop, which is responsible for the lower accuracy across all models.\n5.3. Model efficiency\nGiven the CMA achieves the best performance with a surprisingly simple augmentation, we further explore its efficiency in comparison to other baseline models. Table 4 showcases a comparison of the accuracies and epoch times between baselines and the CMA. The average accuracy of each model is determined in a 16-shot setting as shown in Table 2, along with the recording of average epoch times for each model. All experiments are tested with batch size 32 on a single RTX 4090 GPU in the GossipCop dataset for a fair comparison.\nAmong unimodal models, dEFEND and LDA-HAN exhibit comparable accuracy and epoch times, attributed to their analogous hierarchical architectural design. While FT-ROBERTa exceeds the performance of various unimodal (e.g., 18% higher\n5.4. Domain shift analysis\nReal-world fake news demonstrates significant distribution discrepancies, which is also referred to as domain shift [?? ]. Consequently, automatic FND methods are required to rapidly adapt to emerging topics by using limited resources.\n5.5. Feature visualization\nAt last, we present a visual comparison of the features extracted by M-SAMPLE and CMA, both of which are multimodal few-shot approaches. This involves the visualization of multimodal features alongside an assessment of their semantic correlations. For each dataset, a specific sample is chosen, with the corresponding multimodal features depicted in Figure 4.\nObservations indicate that: 1) CMA can capture more consistent features from the image-text pair of fake news than those of M-SAMPLE. For example, although both M-SAMPLE and CMA successfully correlate the flag in the image with the word \"Chinese\" in the text, CMA can also identify the semantic meaning of \"moon landing\" between the text and image in"}, {"title": "6. Conclusion", "content": "This paper introduced Cross-Modal Augmentation (CMA) for enhancing few-shot multimodal fake news detection by utilizing unimodal features to augment multimodal fusion. The proposed CMA leverages a pre-trained multimodal model for unimodal feature extraction and transforms n-shot classification into a robust (n \u00d7 z)-shot problem using class labels as additional one-shot training samples. The CMA, employing a simple linear classifier, achieves SOTA performance on three datasets in few-shot settings, and demonstrates greater efficiency than current approaches."}, {"title": "7. Limitation", "content": "We acknowledge limitations in this study including: 1) The evaluation of CMA's few-shot proficiency solely utilizes CLIP, future investigations will delve into how different multimodal models influence the proposed CMA; 2) Given the lack of multimodal information in certain datasets, this research adopted cosine similarity for image selection from multiple options, potentially leading to varied performance outcomes based on the text-image pairing technique employed; 3) CMA exhibits suboptimal domain shift performance, enhancing the architecture through the integration of knowledge distillation or domain adaptation techniques remains a prospect for future research."}]}