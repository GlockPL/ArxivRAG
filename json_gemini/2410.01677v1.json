{"title": "MIND SCRAMBLE:\nUNVEILING LARGE LANGUAGE\nMODEL PSYCHOLOGY VIA TYPOGLYCEMIA", "authors": ["Miao Yu", "Junyuan Mao", "Guibin Zhang", "Jingheng Ye", "Junfeng Fang", "Aoxiao Zhong", "Yang Liu", "Yuxuan Liang", "Kun Wang", "Qingsong Wen"], "abstract": "Although still in its infancy, research into the external behaviors and internal\nmechanisms of large language models (LLMs) has shown significant promise in\naddressing complex tasks in the physical world. These studies suggest that power-\nful LLMs, such as GPT-4, are beginning to exhibit human-like cognitive abilities,\nincluding planning, reasoning, and reflection, among others. In this paper, we\nintroduce an innovative research line and methodology named LLM Psychology,\nwhich leverages or extends human psychology experiments and theories to inves-\ntigate cognitive behaviors and mechanisms of LLMs. Practically, we migrate the\nTypoglycemia phenomenon from psychology to explore the \u201cmind\u201d of LLMs. To\ncomprehend scrambled text in Typoglycemia, human brains rely on context and\nword patterns, which reveals a fundamental difference from LLMs' encoding and\ndecoding processes. Through various Typoglycemia experiments at the charac-\nter, word, and sentence levels, we observe the following: (I) LLMs demonstrate\nhuman-like behaviors on a macro scale, such as slightly lower task accuracy with\nconsuming more tokens and time; (II) Different LLMs show varying degrees of\nrobustness to scrambled input, making it a democratized benchmark for model\nevaluation without crafting new datasets; (III) The impact of different task types\nvaries, with complex logical tasks (e.g., math) in scrambled format being more\nchallenging. Going beyond these, some misleadingly optimistic results suggest\nthat LLMs are still primarily data-driven, and their human-like cognitive abil-\nities may differ from what we perceive; (IV) Interestingly, each LLM exhibit its\nunique and consistent \u201ccognitive pattern\u201d across various tasks, unveiling a gen-\neral mechanism in its psychology process. To conclude, we provide an in-depth\nanalysis of hidden layers on a micro scale to explain these phenomena, paving the\nway for LLMs' deeper interpretability and future research in LLM Psychology.", "sections": [{"title": "INTRODUCTION", "content": "\"Typoglycemia refers to the pheonmneon where poeple can raed text even when the lettres in the\nmidlde of wrods are scrambled, as long as the fisrt and last letters are in the crorect poistion.\u201d\nDo you notice that some words in the above explanation to Typoglycemia have letters in the wrong\norder? [pheonmneon, poeple, raed, ...] These words contain certain misplaced letters, yet we can still\nrecognize them. This phenomenon, known as Typoglycemia, is widespread in human reading and is\nused in psychology experiments to study human language cognition (Johnson et al., 2007; Rayner\net al., 2006). With recent development of large language models (LLMs), they demonstrate \"human-\nlike\" capabilities and open a potential path for the upcoming artificial general intelligence, excelling\nin complex tasks such as tool using (Yuan et al., 2024), reasoning (Hao et al., 2023), planning\n(Kalyanpur et al., 2024), and role-playing (Chen et al., 2023a). However, research on the underlying\ncognitive mechanisms of LLMs remains in its infancy. Whether LLMs possess deep thinking and\nhuman-like cognition is an unsolved mystery that still looms over researchers (Binz & Schulz, 2023;"}, {"title": "RELATED WORK", "content": "Human-like Mechanisms of LLMs. LLMS (Touvron et al., 2023; Achiam et al., 2023; Wang\net al., 2024) have revolutionized both academic and industrial research paradigms, owing to their\nexceptional and human-like capabilities (Wei et al., 2022; Wang et al., 2024). Building on these\nfoundational studies, numerous research efforts integrate mechanisms such as memory, role-playing\nand tool using to fully leverage these human-like capabilities (Wei et al., 2022; Bubeck et al., 2023;\nHong et al., 2023; Li et al., 2023; Chen et al., 2023b;a). Several studies explore the similarities\nbetween LLMs and human cognitive mechanisms. McCoy et al. (2019) investigates model's cogni-\ntive intuitions in reasoning tasks. Chowdhery et al. (2023) analyzes PaLM's memory mechanisms,\nhighlighting its human-like processes in retrieval and question answering. Though promising, there\nis still a debate that whether LLMs truly understand language or merely rely on data-driven patterns\n(Bender et al., 2021). We use Typoglycemia as a psychological probe to uncover the superficial per-\nformance and underlying mechanisms of LLMs and present \u201cLLM Psychology\" with the first shot.\nCognitive Concepts in LLMs. Due to the similarity between LLMs and humans, many studies\nhave been inspired by human cognitive process to enhance LLMs (Bubeck et al., 2023; Wei et al.,\n2022). For example, SwiftSage (Lin et al., 2024) improves the task capabilities of LLM-based agents\nin dynamic environments by mimicking the dual-process theory of human cognition. Xie et al.\n(2024) investigates whether LLMs can simulate human cognitive trust behaviors by employing a\nseries of trust games. AvalonBench (Light et al., 2023) evaluates LLMs' competency levels through\nthe Resistance Avalon game, which involves cognitive strategies. PsySafe (Zhang et al., 2024)\nfurther explores the impact of cognitive states as prompts on the safety of LLM-agent. In this work,\ninspired by the Typoglycemia psychological experiment, we explore the underlying cognitive process\nof LLMs by comparing their micro and macro level performance with that of humans.\nHuman Reading and Typoglycemia. An interesting phenomenon is that humans can maintain an\nunderstanding of the general meaning of scrambled text, a capability that some studies attribute to\nthe brain's mechanisms of holistic shape perception and pattern recognition (Miller, 1994; Rayner"}, {"title": "CAN LLMS RECOGNIZE \u03a4\u03a1YOGYLCME\u0399\u0391 AS TYPOGLYCEMIA?", "content": "To apply core principles of LLM Psychology, we migrate and extend the Typoglycemia concept from\npsychology by proposing the calibrated benchmark (TypoBench), as shown in Figure 2. Concretely,\nTypoBench consists of two components: (1) Typoglycemia Pipeline that provides standardized ex-\nperiment workflow on LLMs (Sec 3.1) and (2) Typoglycemia Task that challenges LLMs' all-around\nabilities to address scrambled text (Sec 3.2), with reasons for its specific design in Sec 3.3."}, {"title": "TYPOGLYCEMIA PIPELINE (TYPOPIPE)", "content": "In this section, we introduce the generalized framework TypoPipe to standardize the experimental\nprocess. TypoPipe divides the entire pipeline into the following 4 steps: \u2460 Calibration aims to\ncomprehensively select and calibrate datasets for a thorough evaluation of LLMs' ability. \u2461 Navi-\ngation targets to design reasonable functions (TypoFunc) to transform each data into various types\nof \"Typoglycemia\" text. \u2462 Refabrication. This process perform the original task on the dataset or\ndesign other scenarios to explore LLMs' versatile performances. \u2463 Refinement consists of itera-\ntively calculating metrics, analysing results and refining experiment settings for final conclusions.\nFormulations. First, we provide denotations for further formulations. Let C be the character set,\nthen the text set is T = {C1 C2 . . . Cn | Ci \u2208 C, 1 \u2264 i \u2264 n}. Then we denote a dataset with questions\nand answers as D = (Q, A) = {(qi, ai)|qi \u2208 Q,a\u00a1 \u2208 A,1 \u2264 i \u2264 m}, and LLM as a function\nM : T \u2192 T. For any finite set X, we use xi(1 \u2264 i \u2264 |X|) to refer to its element for convenience.\n\u2460Calibration: We denote the family of datasets as D = {D1, D2, . . ., Dt }. Calibration aims at:\n$\\arg \\max _D \u03b1 [\\sum_{1<i<j<t} Difference (Di, Dj) + -\u03b2\\sum_{i=1}^t Quality (D:)]$\nEq 1 seeks to select distinct and premium datasets to challenge LLMs from multi-aspects. In prac-\ntice, we heuristically select tailored datasets from distinct fields. (See in Appendix B.1)."}, {"title": "EXPERIMENT", "content": "We employ TypoPipe across various scenarios to comprehensively study the impact of Typoglycemia\non LLMs. The experiments are designed to investigate the following research questions:\n\u2022 RQ1: What is the impact of Typoglycemia on existing LLMs?\n\u2022 RQ2: How do other Typoglycemia Functions (e.g., insertion and deletion) impact LLMs?\n\u2022 RQ3: What are the effects of increasing the scrambling ratio of Typoglycemia?\n\u2022 RQ4: Why do LLMs align with human performance under Typoglycemia?"}, {"title": "EXPERIMENTAL SETUPS", "content": "Datasets. We aim to evaluate LLM Psychology across various task settings, including mathematics,\ncode generation, situational question answering, and commonsense reasoning. Concretely, as for\nscenarios requiring strong logical reasoning, we select GSM8k (Hendrycks et al., 2020) for math\nand MBPP (Kocetkov et al., 2022) for code. Additionally, we explore the impact of Typoglycemia\non LLMs' emergent situational learning and knowledge capabilities. We select BoolQ (Clark et al.,\n2019) and SQUAD (Rajpurkar et al., 2016) dataset for situational question answering tasks. For com-\nmonsense reasoning, we use CSQA (Talmor et al., 2018) dataset, a multiple-choice commonsense\ndataset. More descriptions on dataset can be found in Figure 1, Appendix B, and C.\nTypoFuncs (F\u03a9) transform the above datasets into Typoglycemia texts. To extend psychological\nTypoglycemia, we execute Fo at character, word, and sentence levels, allowing us to explore the\nsensitivity of LLMs to various text variations. Specific Fo operations include reordering, inserting,\nand deleting (refer to as REO, INS, and DEL, respectively). Operation X can be applied in different\npositions or ways of the three levels, such as: all (X-ALL), internal (X-INT), adjacent (X-ADJ),\nbeginning (X-BEG), ending (X-END), and reversing (X-REV). Utilizing well-designed Fr, our\nTypoglycemia experiment contains both mildly scrambled text and highly disordered text that is\nnearly unrecognizable to humans. The specific operations instances can be found in Appendix D.\nModels and Metrics. We extensively evaluate our concept across diverse LLMs within zero-shot\nsetting, including Gemma-2 (2B, 9B and 27B) (Team et al., 2024), Llama 3.1 (8B, 70B) (Touvron\net al., 2023), GPT-3.5-Turbo\u00b2, GPT-40-mini\u00b3 and GPT-404. The selection of these models and\ntheir corresponding sizes provides a comprehensive \u201cmodel zoom\". In our settings, we choose\naccuracy and cosine similarity as metrics. For accuracy, we consider a response correct only when\nthe LLM's output exactly matches the correct answer. For cosine similarity, we embed the reasoning\nprocesses into vectors using the text-embedding-35 and calculate cosine similarity with the standard\nprocess. The model parameter settings for reproducibility can be found in Appendix E."}, {"title": "MAIN RESULTS (RQ1)", "content": "To answer RQ1, we compare different Typoglycemia concepts across various models and datasets.\nWe apply random reordering and run the experiments multiple times, reporting the mean values. The\nexperimental observations (Obs) are as follows and experiment discussion is placed in Appendix A:\nObs.1. Typoglycemia generally leads to a decline in model performance, with more advanced\nmodels being less affected. As shown in Table 1, red markers predominantly appear in the BASE\ncolumn, indicating that accuracy tends to decrease after applying Fr. The performance retention\nof models within the same series increases with model size. For instance, on GSM8k dataset,\nthe Gemma-2 series exhibits an increase in average accuracy across scales, with retention rates"}, {"title": "IMPACT OF TYPOGLYCEMIA FUNCTIONS (RQ2)", "content": "To answer RQ2, we conduct experiments using additional insertion and deletion Typoglycemia func-\ntions to verify the impact of other Typoglycemia concepts on LLMs. We list the results in Table 3\nand we can summarize the observations:\nObs.1. The impact of Insertion and Deletion on LLMs is generally similar to Reordering,\nbut the magnitude of the impact is smaller. As shown in Table 3, red markers are primarily\nconcentrated in the BASE row, and the SOTA model GPT-4o retains an average accuracy of 98.9%\nacross the three datasets, while the weakest model, Gemma-2-2B, achieves 83.6%. In all cases, the\naverage retained accuracy increases by 0.1 ~ 26.8% compared to Reordering. This indicates that\nLLMs are more robust to minor additions or deletions of characters than to character reordering.\nObs.2. Insertion and Deletion also result in an unusual increase in accuracy for tasks with\nweaker logic. As shown in Table 3, 90.9% of the gray markers appear in the BoolQ dataset, which\nis consistent with the pattern observed in Reordering. This observation further confirms that minor\nperturbations in the prompt can aid models in understanding simple logical problems.\nObs.3. LLMs exhibit sensitivity to character position for Deletion, but are less sensitive for\nInsertion. As shown in Table 3, in the case of the INS operation, BEG < END occurs in 54.2%\nof cases, whereas for the DEL operation, this ratio rises to 83.3%, a 29.1% increase. This observation\nreinforces the finding in RQ1 that the first letter is more important than the last, and reveals that their\nrelative importance can vary depending on the operation."}, {"title": "SCRAMBLING RATIO OF TYPOGLYCEMIA (RQ3)", "content": "To address RQ3, we gradually increase the number of reordering, inserting, and deleting operations\napplied to each word's internal characters to increase the scrambling ratio of texts. The correspond-\ning results are shown in Figure 4 and Appendix F.4, of which we derive the following observations:\nObs.1. As the scrambling ratio increases, the TypoC task becomes more challenging for LLMs.\nAs shown in Figure 4, with the increasing number of operations, the accuracy of LLMs generally\nshows a downward trend across all three cases, with a drop ranging from 0.3% to 14.2%. We\nhighlighted the regions with the largest decreases in yellow. This observation aligns with human\nbehavior (Just & Carpenter, 1980), indicating that as the internal structure of the text becomes more\ndisordered, it becomes increasingly difficult for LLMs to understand the text.\nObs.2. Different models exhibit varying levels of resistance to scrambling text. As shown in\nFigure 4, Llama-3.1-8B demonstrates the weakest robustness, while GPT-40 shows the strongest\nanti-Typoglycemia ability. The absolute values of the average slope in accuracy for the Inserting\noperation are 10.9 and 1.2, respectively. This robustness can serve as a measure of LLMs' ability to\nhandle scrambled text, which may offer a new approach for evaluating LLMs."}, {"title": "WHY DO LLMS ALIGN WITH HUMAN PERFORMANCE (RQ4)", "content": "To answer RQ4, we embed the task text before and after being processed by Fo using the text-\nembedding-3-large model and then calculate their semantic similarity. Additionally, We calculate\nthe mean of representation tensors between Transformers, aggregating their semantics into vectors\nto calculate similarity with BASE. Finally, we derive the above Table 4, Figure 5 and observations:\nObs.1. The text before and after Fo processing exhibits a high degree of semantic similarity,\nwith the impact varying depending on the level of text granularity. As shown in Table 4, in the\nCSQA dataset, Typoglycemia text at the character, word, and sentence level retains an average se-\nmantic similarity of 0.885, 0.952, and 0.998, respectively, compared to the unprocessed text (BASE).\nThis indicates that disturbances at the character level have the greatest impact on LLMs' understand-\ning of the text. This observation suggests that, from the encoder's perspective, Typoglycemia text\npreserves a substantial amount of semantic information, which enables LLMs to exhibit robustness\nsimilar to humans in Typoglycemia scenarios (more results are placed in Appendix F.5).\nObs.2. The subsequent representations of Typoglycemia text by LLMs are critical to their task\nperformance. As shown in Figure 5, the color of accuracy and representation is closely aligned. For\ninstance, on SQUAD, for the 3 types of Fo at the sentence level, the similarity scores of their rep-\nresentations are all yellow (high similarity), and corresponding accuracy is the highest at 74.4%,\n73.2%, and 71.6%, respectively. In contrast, when F\u00ba = Char-REO-REV, the representation sim-\nilarity score is the lowest (darkest color), with lowest accuracy at 22.0%. This observation demon-\nstrates that, from the decoder's perspective, the semantic information retained in the representations\nacross the Transformer layers is crucial for LLMs to correctly understand and respond.\nObs.3. The hidden layer representations of the same LLM across different datasets exhibit\nsimilar \"cognitive patterns.\" As illustrated in Figure 5, the color distributions for SQuAD and\nBoolQ under various Fo appear visually similar. Specifically, the cosine similarity between the\nconcatenated and linearized heatmaps of these two datasets is 0.9994, indicating a high degree of\nsimilarity. Additionally, the color distributions vary across different models when evaluated on the\nsame dataset (See more figures in Appendix F.6). Based on these observations, we posit that the\nheatmap can translate each model's unique \"cognitive pattern\" through our Typoglycemia experi-\nments, much like how different human individuals exhibit distinct cognitive patterns."}, {"title": "CONCLUSION", "content": "In this paper, we explore the emerging field of LLM Psychology by investigating the behavior of\nLLMs through the lens of Typoglycemia. Our study reveals how LLMs handle scrambled text,\nproviding insights into their cognitive-like abilities and limitations. Through systematic analysis, we\nobserve that LLMs demonstrate human-like behaviors, such as reduced task accuracy and increased\ntoken and time consumption, when faced with text distortions. Additionally, the varying robustness\nacross different LLMs suggests that scrambled text understanding serves as an accessible benchmark\nfor evaluating model performance. Despite some promising results, our analysis of LLMs' hidden\nlayers reveals their reliance on data-driven mechanisms, with limited capacity for deep reasoning.\nBy digging into the hidden layer semantics, we further reveal that each LLM demonstrates its\nunique and consistent cognitive pattern across different datasets in Typoglycemia."}, {"title": "EXPERIMENTAL DISCUSSION", "content": "Through comprehensive and systematic experiments in Typoglycemia and its migrated scenarios,\nwe discover both the alignment of LLMs with human cognition and their distinct behaviors. LLMs\nexhibit a decline in task accuracy, increased resource consumption, and many other human-like be-\nhaviors, such as placing greater emphasis on initial letters. This significantly advances research\non aligning LLMs with human cognition and provides a solid and vivid case for our proposed\n\"LLM Psychology.\u201d Furthermore, we observe LLMs' counter-intuitive and counter-logical per-\nformance under certain settings, offering strong evidence for the argument that LLMs possess data-\ndriven statistical reasoning abilities rather than human logic. Finally, we explore the underlying\ncauses and observe different LLM's unique cognitive pattern on these phenomena from the perspec-\ntives of encoder and decoder architectures, providing new insights into the cognitive mechanisms\nof LLMs."}, {"title": "DATASET DESCRIPTION", "content": "To systematically evaluate the performance of LLMs on TypoBench, we focus on three key capa-\nbilities when selecting datasets and task scenarios: logical reasoning, contextual learning, and\nknowledge acquisition."}, {"title": "DATASET SELECTION STRATEGY", "content": "LOGICAL REASONING\nStrong Logic Tasks. We refer to tasks that involve multi-step reasoning, where an error in one\nstep leads to errors in subsequent steps, as Strong Logic Tasks (SLTs). Representative scenarios we\nselect include mathematical problem solving (GSM8k as dataset) and code generation (MBPP\nas dataset). SLTs pose stringent challenges to the logical reasoning capabilities of LLMs. From\na data-driven perspective, Typoglycemia disrupts the morpheme order in normal natural language\ntext, which, in turn, disturbs the inherent logic, leading to confused understanding and erroneous\nreasoning. For humans, the combination of SLT scenarios and Typoglycemia text makes task com-\npletion nearly impossible. In a certain sense, this implies that SLTs are effective in testing LLMs\u2019\nperformance on TypoBench, thereby revealing their underlying cognitive mechanisms.\nWeak Logic Tasks. Conversely, tasks with less stringent requirements for logical correctness are\nreferred to as Weak Logic Tasks (WLTs). These tasks typically challenge LLMs' capabilities not\nonly in simple logical reasoning but also in other areas. WLTs primarily serve as a platform for\nsimultaneously evaluating multiple aspects of LLMs' abilities. In our experimental strategy, WLTs\nare combined with contextual learning and knowledge acquisition.\nCONTEXTUAL LEARNING\nContextual learning refers to the ability of LLMs to perceive and learn the knowledge, patterns, and\nother elements within the context of a given prompt. We select task datasets for contextual learning\nat two levels of difficulty. Given a contextual passage and a related question, LLMs are instructed\nto answer with either \"yes/no\u201d (BoolQ as dataset) or phrases (SQUAD as dataset), corresponding\nto easy and difficult settings, respectively. In the yes/no setting, the response is not directly tied\nto the context, allowing LLMs to rely on coarse-grained semantic understanding. However, in the\nphrases setting, LLMs are required to have a more localized understanding of the contextual content,\nposing a more severe challenge to their learning and perception capabilities. By combining these\ntwo scenarios with TypoBench, we can explore how Typoglycemia affects LLMs' ability to perceive\nboth local and global information.\nKNOWLEDGE ACQUISITION\nLLMs possess knowledge capabilities, which are embedded within their layer weights. Generally,\na model activates and extracts the knowledge embedded in these weights through the input prompt,\nenabling it to generate responses. This raises an interesting question: does Typoglycemia affect this\nprocess? In our experimental strategy, we investigate whether Typoglycemia disrupts the extraction\nof knowledge from the model's internal weights by evaluating its impact on answering context-\nindependent common sense questions (CSQA as dataset). This approach allows us to explore how\nthe perturbation of input text influences the model's ability to retrieve knowledge."}, {"title": "DATASET EXAMPLE", "content": "GSM8K\nQuestion:\nJulie wants to give her favorite cousin a $2345 mountain bike for his birthday. So far, she\nhas saved $1500. Since the birthday is still a few weeks away, Julie has time to save even\nmore. She plans on mowing 20 lawns, delivering 600 newspapers, and walking 24 of her\nneighbors' dogs. She is paid $20 for each lawn, 40 cents per newspaper, and $15 per dog.\nAfter purchasing the bike, how much money will Julie have left?\nAnswer:\nMowing lawns will earn Julie 20 \u00d7 20 = 400 dollars.\nHer earnings, in dollars, from delivering newspapers will be 600 \u00d7  = 240 dollars.\nAfter walking 24 of her neighbor's dogs, she will earn 24 \u00d7 15 = 360 dollars.\nShe will therefore earn a total of 400 +240 + 360 = 1000 dollars.\nCombining earnings from her job with her savings will give Julie 1000 + 1500 2500\ndollars.\nSubtracting the cost of the mountain bike from the total will leave Julie with a balance of\n2500-2345\n155 dollars.\nMBPP\nText:\nWrite a function to find the peak element in the given array.\nTest Cases:\nassert find_peak([1, 3, 20, 4, 1, 0], 6) == 2\nassert find_peak([2, 3, 4, 5, 6], 5)\n==\n4\nassert find_peak([8, 9, 11, 12, 14, 15], 6)\n==\n5\nCode:\ndef find_peak_util(arr, low, high, n):\nmid = low + (high\nint (mid)\nlow)/2\nmid =\nif ((mid == 0 or arr[mid 1] <= arr[mid)) and\n-\n(mid == n 1 or arr[mid + 1) <= arr[mid])):\nreturn mid\nelif (mid > 0 and arr[mid 1] > arr[mid]):\nreturn find_peak_util(arr, low, (mid 1), n)\nelse:\nreturn find_peak_util(arr, (mid + 1), high, n)\ndef find_peak(arr, n):\nreturn find_peak_util(arr, 0, n\n1, n)\nBOOLQ\nQuestion:\nDo all bacteria have peptidoglycan in their cell walls?\nPassage:\nPeptidoglycan, also known as murein, is a polymer consisting of sugars and amino acids that\nforms a mesh-like layer outside the plasma membrane of most bacteria, forming the cell wall.\nThe sugar component consists of alternating residues of \u03b2-(1,4) linked N-acetylglucosamine\n(NAG) and N-acetylmuramic acid (NAM). Attached to the N-acetylmuramic acid is a pep-\ntide chain of three to five amino acids. The peptide chain can be cross-linked to the peptide\nchain of another strand forming the 3D mesh-like layer. Peptidoglycan serves a structural\nrole in the bacterial cell wall, giving structural strength, as well as counteracting the osmotic\npressure of the cytoplasm. A common misconception is that peptidoglycan gives the cell\nits shape; however, whereas peptidoglycan helps maintain the structural strength of the cell,\nit is actually the MreB protein that facilitates cell shape. Peptidoglycan is also involved in\nbinary fission during bacterial cell reproduction.\nAnswer:\nFalse\nSQUAD\nContext:\nThe control of associated biodiversity is one of the great agricultural challenges that farm-\ners face. On monoculture farms, the approach is generally to eradicate associated diversity\nusing a suite of biologically destructive pesticides, mechanized tools, and transgenic engi-\nneering techniques, then to rotate crops. Although some polyculture farmers use the same\ntechniques, they also employ integrated pest management strategies as well as strategies that\nare more labor-intensive, but generally less dependent on capital, biotechnology, and energy.\nQuestion:\nWhat is one of the great agricultural challenges that farmers face?\nAnswer:\nThe control of associated biodiversity\nCSQA\nQuestion:\nJohn watches the well-dressed people from a catwalk above the stage. He listens to them\nspeak rehearsed lines while the audience listens. Where is he?\nChoices:\nA. theatre B. new york city C. fashion show D. construction site E. school play\nCorrect Answer:\nA. theatre"}, {"title": "TASK PROMPT", "content": "TASK COMPLETION PROMPT\nMATHEMATICAL PROBLEM SOLVING\nSolve the math problem below:\nProblem: {mathematical problem description}\nResponse in the following format without any other information:\nprocess: <reasoning steps here>\nanswer_number: <final answer number here>\nMATHEMATICAL PROBLEM SOLVING\nSolve the code problem below in Python:\nProblem: {code description}\nResponse in the following format without any other information:\nprocess: <reasoning steps here>\ncode: <Python code here>\nCONTEXT QUESTION ANSWERING\nAnswer the question with only 'yes' or 'no' based on the passage below:\nQuestion: {question description}\nPassage: {context passage}\nResponse in the following format without any other information:\nreason: <reason for yes or no here>\nanswer: <'yes' or 'no' here>\nAnswer the question with word or phrase based on the context below:\nQuestion: {question description}\nPassage: {context passage}\nResponse in the following format without any other information:\nreason: <reason for the answer here>\nanswer: <answer here>\nCOMMONSENSE REASONING\nChoose one choice that best answers the commonsense question below:\nQuestion: {question description}\nChoices: {context passage}\nResponse in the following format without any other information:\nreason: <reason for the choice here>\nanswer: <one choice from the choices list here>\nTASK PERCEPTION PROMPT\nRECTIFY\nCorrect the scrambled letters in each word of the following passage:\nPassage: {passage text}\nResponse in the following format without any other information:\nrectified: <rectified passage here >\nSUMMARIZE\nSummarize the main content of the following passage:\nPassage: {passage text}\nResponse in the following format without any other information:\nsummarized: <summarized passage here>\nTRANSLATE\nTranslate the following English passage into Chinese:\nPassage: {passage text}\nResponse in the following format without any other information:\ntranslated: <translated passage here>"}, {"title": "TYPOC CASES", "content": "MATHEMATICAL PROBLEM SOLVING (GSM8K)\nGOOD MIND\nOne case of GPT-40's correct responses when F\u03a9 =Char-REO-INT.\nTypoglycemia Prompt:\nSolve the math problem below:\nProblem: Jluie wtnas to gvie her\nfiroatve cusoin a $2345 mtinauon bkie\nfor his btrihday. So far", "information": "nprocess: {reasoning steps here"}, "nanswer_number: {final answer number\nhere}\nResponse from GPT-40:\nprocess: 1. Calculate the total amount\nJulie will earn from mowing lawns: 20\nlawns * $20/lawn = $400.\n2. Calculate the total amount Julie will\nearn from delivering newspapers: 600\nnewspapers * $0.40/newspaper = $240.\n3. Calculate the total amount Julie will\nearn from walking dogs: 24 dogs *\n$15/dog = $360.\n4. Add the amounts from steps 1, 2, and\n3 to find the total additional money Julie\nwill earn: $400 + $240 + $360 = $1000.\n5. Add the amount Julie has already\nsaved to the total additional money:\n$1500 + $1000 = $2500.\n6. Subtract the cost of the bike from\nthe total money Julie will have: $2500\n- $2345 = $155.\n### 155\nLet Sarah have x cents. Then, Bigrde\nhas x + 50 cents. Together, they have\nx+(x+50) = 300 cents (since $3 is 300\ncents). Simplifying, we get 2x + 50 =\n300. Subtract 50 from both sides to get\n2x = 250. Divide both sides by 2 to get\nx = 125.\nanswer_number: 155\nOriginal Question:\nJulie wants to give her favorite cousin\na $2345 mountain bike for his birth-\nday. So far, she has saved $1500. Since\nthe birthday is still a few weeks away,\nJulie has time to save even more. She\nplans on mowing 20 lawns, delivering\n600 newspapers, and walking 24 of her\nneighbors' dogs. She is paid $20 for\neach lawn, 40 cents per newspaper, and\n$15 per dog. After purchasing the bike,\nhow much money will Julie have left?\nAnswer from Dataset:\nMowing lawns will earn Julie 20 * 20 =\n 400 dollars\nHer earnings, in dollars, from deliver-\ning newspapers will be 600 * 40 / 100 =\n 240 dollars\nAfter walking 24 of her neighbor's dogs,\nshe will earn 24 * 15 = 360\ndollars.\nShe will therefore earn a total of 400 +\n240 + 360 = 1000\ndollars.\nCombining earnings from her job with\nher savings will give Julie 1000 + 1500\n= 2500 dollars.\nSubtracting the cost of the mountain\nbike from the total will leave Julie with\na balance of 2500 - 2345 = 155\ndollars.\n#### 155\nBAD MIND\nOne case of GPT-40's incorrect responses when F\u03a9 =Char-REO-ALL.\nTypoglycemia Prompt:\nSolve the math problem below:\nProblem: iamrSa is the nassatsti oachc\nof a rccseo aetm gynlpai saingta one\nof"]}