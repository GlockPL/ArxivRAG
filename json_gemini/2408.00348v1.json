{"title": "Securing the Diagnosis of Medical Imaging: An In-depth Analysis of AI-Resistant Attacks", "authors": ["Angona Biswas", "MD Abdullah Al Nasim", "Kishor Datta Gupta", "Roy George", "Abdur Rashid"], "abstract": "Machine learning (ML) is a rapidly developing area of medicine that uses significant resources to apply computer science and statistics to medical issues. ML's proponents laud its capacity to handle vast, complicated, and erratic medical data. It's common knowledge that attackers might cause misclassification by deliberately creating inputs for machine learning classifiers. Research on adversarial examples has been extensively conducted in the field of computer vision applications. Healthcare systems are thought to be highly difficult because of the security and life-or-death considerations they include, and performance accuracy is very important. Recent arguments have suggested that adversarial attacks could be made against medical image analysis (MedIA) technologies because of the accompanying technology infrastructure and powerful financial incentives. Since the diagnosis will be the basis for important decisions, it is essential to assess how strong medical DNN tasks are against adversarial attacks. Simple adversarial attacks have been taken into account in several earlier studies. However, DNNs are susceptible to more risky and realistic attacks. The present paper covers recent proposed adversarial attack strategies against DNNs for medical imaging as well as countermeasures. In this study, we review current techniques for adversarial imaging attacks, detections. It also encompasses various facets of these techniques and offers suggestions for the robustness of neural networks to be improved in the future.", "sections": [{"title": "1 Introduction", "content": "Adversarial attacks encompass a set of techniques aimed at manipulating machine learning models by introducing well-crafted, often imperceptible alterations to input data. These modifications aim to deceive the model, leading to misclassifications or inaccurate predictions [1]. Adversarial attacks have the potential to compromise machine learning systems' dependability and security, which is concerning for crucial applications such as cybersecurity, autonomous vehicles, and medical diagnosis. Of all the adversarial attack types, \"White-box attacks\" and \"Black-box attacks\" are the most common.\nOngoing research in machine learning security centers on the analysis of adversarial attacks and the development of defense strategies. Researchers continually explore novel attack tactics to identify potential weaknesses in AI systems while concurrently striving to construct robust models less susceptible to adversarial perturbations. Artificial intelligence (AI) modern technology is widely successful in fields like computer vision, natural language processing, and automated driving; however, its application in vital safety sectors is hindered by its susceptibility to adversarial attacks. Consequently, enhancing the resilience of AI systems against such attacks has become paramount for the progress of AI [2].\nThe rapid advancement of AI technologies has found diverse applications in numerous domains, from machine translation, speech recognition, and object identification to more intricate tasks like drug composition analysis [3-7]. Noteworthy applications include brain circuit construction [8], particle accelerator data analysis [9], [10], and DNA mutation impact analysis [11]. Since Szegedy et al.'s seminal work [12] highlighting neural networks' vulnerability to adversarial attacks, research on adversarial technologies for artificial intelligence has steadily expanded. New techniques for adversarial attacks and strategies for mitigating them are consistently emerging.\nThe term \"adversarial attacks at the training stage\" refers to actions taken by adversaries to manipulate the training dataset, input characteristics, or data labels during the training phase of the target model. This manipulation of the training dataset involves actions such as adding or deleting training data, as demonstrated by Barreno et al.'s approach [13]. During the testing phase, adversarial assaults are categorized into white-box attacks and black-box attacks [2]. In white-box situations,"}, {"title": "1.1 Adversarial Attack", "content": "Adversarial attacks encompass a set of techniques aimed at manipulating machine learning models by introducing well-crafted, often imperceptible alterations to input data. These modifications aim to deceive the model, leading to misclassifications or inaccurate predictions [1]. Adversarial attacks have the potential to compromise machine learning systems' dependability and security, which is concerning for crucial applications such as cybersecurity, autonomous vehicles, and medical diagnosis. Of all the adversarial attack types, \"White-box attacks\" and \"Black-box attacks\" are the most common.\nOngoing research in machine learning security centers on the analysis of adversarial attacks and the development of defense strategies. Researchers continually explore novel attack tactics to identify potential weaknesses in AI systems while concurrently striving to construct robust models less susceptible to adversarial perturbations. Artificial intelligence (AI) modern technology is widely successful in fields like computer vision, natural language processing, and automated driving; however, its application in vital safety sectors is hindered by its susceptibility to adversarial attacks. Consequently, enhancing the resilience of AI systems against such attacks has become paramount for the progress of AI [2].\nThe rapid advancement of AI technologies has found diverse applications in numerous domains, from machine translation, speech recognition, and object identification to more intricate tasks like drug composition analysis [3-7]. Noteworthy applications include brain circuit construction [8], particle accelerator data analysis [9], [10], and DNA mutation impact analysis [11]. Since Szegedy et al.'s seminal work [12] highlighting neural networks' vulnerability to adversarial attacks, research on adversarial technologies for artificial intelligence has steadily expanded. New techniques for adversarial attacks and strategies for mitigating them are consistently emerging.\nThe term \"adversarial attacks at the training stage\" refers to actions taken by adversaries to manipulate the training dataset, input characteristics, or data labels during the training phase of the target model. This manipulation of the training dataset involves actions such as adding or deleting training data, as demonstrated by Barreno et al.'s approach [13]. During the testing phase, adversarial assaults are categorized into white-box attacks and black-box attacks [2]. In white-box situations,"}, {"title": "1.2 The Adversary's Objective: Evasion attack versus poisoning assault", "content": "\"Poisoning attacks\" are assault methods that allow an attacker to add or change a large number of fake samples to the training set of a DNN algorithm. The trained classifier may perform poorly as a result of these bogus data. They may have poor accuracy [14] conversely make imprecise predictions on a few samples for analysis [15]. The classifiers used in evasion attacks are fixed and often work well on safe testing samples. The adversaries are unable to reform the classifier's settings or parameters, but they do generate some fictitious instances that the classifier is unable to distinguish."}, {"title": "1.3 Deep Neural Networks: Adversarial Attacks", "content": "Regarding medical image processing use cases such as diagnosing cancer and lesion identification, deep neural networks, also known as DNNs, are rapidly gaining popularity [16]. Nonetheless, a new study suggests that adversarial examples/attacks with tiny, barely noticeable changes can weaken medical deep learning systems. There are presently safety concerns associated with the usage of these devices in clinical settings [16]. Deep neural networks (DNN) are increasingly well-liked and effective at a variety of machine learning requisitions. They have been employed with surprising effectiveness in a variety of recognition issues in the fields of pictures, graphs, text, and voice [1]. They are able to identify things in images with accuracy that is almost human [17], [18]. Additionally, they are employed in speech recognition [19], natural language processing [20], and gaming [21]."}, {"title": "1.4 Examining examples of opposition in the real world", "content": "The research [23] put decals to traffic signs that pose a serious threat to autonomous vehicles' sign recognition technology. These hostile objects are especially detrimental to deep learning systems since they can interfere directly with numerous real-world DNN usages, such facial recognition and autonomous cars. By assessing the adversarial images (FGSM, BIM) produced to see if they are \"robust\" against changes in natural circumstances (e.g., shifting viewpoint, illumination, etc.), the authors of the work [24] investigate the viability of creating tangible adversarial objects. Robust in this context means that even after transformation, the produced pictures are still antagonistic. The experiment's findings show that many of these adversarial examples particularly those produced by FGSM-remain antagonistic to the classifier even after transformation. The findings imply that antagonistic real-world objects could trick the sensor in many situations."}, {"title": "1.5 Medical Image Under Artificial Intelligence Attack", "content": "Researchers have access to strong models of developing science and technology thanks to deep learning. Convolutional neural networks, also known as CNNs, are among the most significant categories of deep learning (DL) frameworks for the analysis and processing of images because of their exceptional ability to learn valuable information. The processing of the human organism utilizing different picture modalities for therapeutic, diagnostic, and health surveillance purposes is known as medical image analysis [25]. The advancement of computer vision through the use of deep neural networks addresses issues that were not well-solved by traditional image processing methods. Beinfeld et al. [26] asserted that $385 spent on medical imaging results in a savings of almost $3000. MRI, CT scans, ultrasound (US), and X-rays are the most frequently used image modalities."}, {"title": "2 Background Literature Review", "content": "Adversarial instances are intentionally created data inputs that are intended to degrade a machine learning model's performance. When researchers examined strategies used by spammers to evade spam filters in 2004, they unofficially coined the term \"adversarial inputs\" [37]. These misleading examples are usually produced by deliberately faking real data, such spam advertising messages, in order to trick the computer system that analyzes it. Alterations can be applied to text data, like spam, by introducing innocuous text or modifying frequently used phrases in malicious communications with synonyms. To fortify algorithms against adversarial attacks, researchers have explored various strategies, including training algorithms on adversarial samples and employing sophisticated data processing techniques to minimize the susceptibility to manipulation. The quest for fully robust models in machine learning aims to accelerate the development of algorithms capable of making decisions based on consistent explanations, with promising early efforts in this direction [38].\nIn the realm of healthcare, medical claims codes play a crucial role in determining the amount spent on a patient visit post-payer approval. Payers often assess these claims using automated fraud detectors increasingly driven by machine learning. Historically, healthcare providers have shaped payer records of patient visits, including the associated codes, to influence the algorithmic outputs of payers [38]. Medical fraud, a market valued at $250 billion, exemplifies the extreme end of this strategic tailoring of patient presentations. While some practitioners may overtly fabricate medical claims, patient data falsification often takes more covert forms. For instance, consistently submitting codes for billing services identical to, but more expensive than, those actually provided is known as intentional upcoding."}, {"title": "2.1 Elements of a Cyber Adversary's Assault", "content": "In some instances, subtle adjustments to billing codes blur the boundary between fraudulent activities and conscientious best practices. One notable illustration may be found in the guidelines on the Endocrine Society website, which advise medical professionals not to bill for metabolic syndrome, or International Classification of Diseases (ICD) code 277.77, in situations of obesity. Coverage denial is likely to occur with this particular code/condition combo. The Society recommends an alternative approach, suggesting billing for the individual codes related to specific disorders comprising the metabolic syndrome, such as hypertension."}, {"title": "2.2 The components of an adversarial attack", "content": "Presenting a thorough overview of the various adversarial attack strategies and defense techniques is the aim of the study [44]. We first discuss the theoretical underpinnings, practices, and practical uses of adversarial attack strategies. Then, a brief discussion of the research on defense strategies obscuring the boundary of the large field follows. A few selected articles are published year-by-year from 2012 to 2021. This work aims to provide thorough taxonomies, evaluations of harmful assaults, and protections against the full DL pipeline. In this context, the numerous attack and defense tactics that have been developed over the previous two years have been categorized, with a focus on the clinical deep learning systems that are susceptible to adversarial attacks.\nSeveral defensive strategies have been put forth in an effort to neutralize possible threats. A popular method in natural imaging called adversarial training adds adversarial images to the training dataset in order to make Convolutional neural network, or CNN, models more resilient. Nevertheless, this method is not the best for datasets pertaining to medical imaging, since the addition of different adversarial images could considerably reduce classification accuracy. A robust detection technique for malicious images is presented in the work of Li et al. [42], successfully defending against attacks on deep learning-driven medical image labeling systems. In a different study [43], scientists compared CNN performance with Vision Transformers (ViTs) to determine how resilient CNNs are to different types of attacks in computational pathology. The authors developed robust neural network models, evaluating their efficacy against both white- and black-box attacks. The structures of attacks for both models were scrutinized, with an exploration of the underlying factors influencing their performance. The study's findings were validated through two clinically relevant classification tasks involving distinct patient groups. Preceding Ma et al.'s research [16], ambiguity surrounded medical image adversarial attacks (AAs), limiting adversarial machine learning analyses to natural images. Unlike natural images, medical images may contain domain-specific elements, impacting medical deep-learning"}, {"title": "3 Materials and Methods", "content": "Various defense models, including techniques such as input denoising, input gradients regularization, and adversarial training, have been devised. However, recent attacks often manage to evade or partially circumvent these protective measures. This paper undertakes a comprehensive exploration of adversarial attack and defense methods within the domain of medical image analysis. It introduces a novel taxonomy based on the application context for a family of techniques, as outlined in Dong et al.'s work [45]. The author establishes a unified theoretical framework encompassing diverse adversarial attack and defense strategies tailored for medical imaging applications.\nThe primary emphasis in most research efforts on adversarial attacks in medical image processing is on the white-box scenario. These studies, characterized by a comprehensive understanding of medical Deep Neural Networks (DNNs), concentrate on the susceptibility of computer-aided diagnosis models across various medical imaging applications. In the execution of adversarial attacks, attackers may utilize the target diagnosis DNN as a locally deployed model. In addition to white-box adversarial attacks for medical classification tasks, scholars have delved into vulnerabilities associated with various medical imaging tasks. Notably, these publications focus predominantly on medical segmentation, as evident in works such as Chen et al.'s [35] and Ozbulak et al.'s [32].\nFor natural photos, semi-white-box (Gray-box) attacks have been extensively researched [46]. However, there are only a few academic publications [47], [48] that address this attack scenario for medical image processing. The semi-white-box adversarial approach typically consists of two stages: 1) To create adversarial instances against the target DNN model, the attacker trains a generative model. The attacker has complete access to the target model during training, including backward propagation gradients. 2) Instead of needing to know anything about the target model, as would be the case in a completely black-box scenario, the adversary generator can directly obtain adversarial examples against the target model with the input of authentic photos during the application stage.\nPresently utilized white-box adversarial attacks often require multiple backward gradients of the target model. In simpler terms, attackers generate comparable adversarial samples by treating the target Deep Neural Network (DNN) as if it were a locally deployed model. However, due to its reliance on an in-depth understanding of the DNN model to execute an attack, the white-box approach may be unreliable in real-world scenarios. Conversely, the general black-box scenario may offer a more suitable environment for simulating real-world adversarial attacks, with numerous proposals focusing on investigating black-box assaults for natural images.\nThe outlined solutions usually involve multiple queries to the black-box model or depend on having a comprehensive understanding of the desired diagnosis model."}, {"title": "3.1 Medical Adversarial Defensive Strategies", "content": "Various defense models, including techniques such as input denoising, input gradients regularization, and adversarial training, have been devised. However, recent attacks often manage to evade or partially circumvent these protective measures. This paper undertakes a comprehensive exploration of adversarial attack and defense methods within the domain of medical image analysis. It introduces a novel taxonomy based on the application context for a family of techniques, as outlined in Dong et al.'s work [45]. The author establishes a unified theoretical framework encompassing diverse adversarial attack and defense strategies tailored for medical imaging applications.\nThe primary emphasis in most research efforts on adversarial attacks in medical image processing is on the white-box scenario. These studies, characterized by a comprehensive understanding of medical Deep Neural Networks (DNNs), concentrate on the susceptibility of computer-aided diagnosis models across various medical imaging applications. In the execution of adversarial attacks, attackers may utilize the target diagnosis DNN as a locally deployed model. In addition to white-box adversarial attacks for medical classification tasks, scholars have delved into vulnerabilities associated with various medical imaging tasks. Notably, these publications focus predominantly on medical segmentation, as evident in works such as Chen et al.'s [35] and Ozbulak et al.'s [32].\nFor natural photos, semi-white-box (Gray-box) attacks have been extensively researched [46]. However, there are only a few academic publications [47], [48] that address this attack scenario for medical image processing. The semi-white-box adversarial approach typically consists of two stages: 1) To create adversarial instances against the target DNN model, the attacker trains a generative model. The attacker has complete access to the target model during training, including backward propagation gradients. 2) Instead of needing to know anything about the target model, as would be the case in a completely black-box scenario, the adversary generator can directly obtain adversarial examples against the target model with the input of authentic photos during the application stage.\nPresently utilized white-box adversarial attacks often require multiple backward gradients of the target model. In simpler terms, attackers generate comparable adversarial samples by treating the target Deep Neural Network (DNN) as if it were a locally deployed model. However, due to its reliance on an in-depth understanding of the DNN model to execute an attack, the white-box approach may be unreliable in real-world scenarios. Conversely, the general black-box scenario may offer a more suitable environment for simulating real-world adversarial attacks, with numerous proposals focusing on investigating black-box assaults for natural images."}, {"title": "3.2 Adversarial Training", "content": "Most medical adversarial defense techniques focus on using adversarial training to create reliable diagnoses systems. A significant fraction of the works among them go beyond current techniques for natural image adversarial training to tasks relating to medical classification [50], [51], Vatian et al. [52] looked into opposing examples for medical imaging and tried a number of defense strategies to oppose these nefarious representations."}, {"title": "3.3 Adversarial Detection", "content": "Adversary detection seeks to identify adversary cases from input examples during the application stage, as opposed to developing robustness during the training stage of computer-aided diagnosis models. Many adversarial detection techniques have been put forth in the field of medical image analysis to stop additional misdiagnosis caused by adversarial samples [42]. In particular, it is possible to think about medical adversarial detection as an anomaly detection issue that can be resolved by combining explainability approaches [53]."}, {"title": "3.4 Image-level Pre-processing", "content": "A clean image and the related adversarial perturbation make up an adversarial image in general. Meanwhile, it has been shown that DNNs can perform well on clean images while still being vulnerable to adversarial examples [12]. Denoising the adversarial example to remove the perturbation component can therefore help make the subsequent network diagnostic easier. Image-level pre-processing can be useful and secure in the context of biomedical image analysis because it does not require re-training or modifying medical models."}, {"title": "3.5 Improvement of Features", "content": "The difference in robustness between human and machine vision is attributed to adversarial examples linked to non-robust features extracted from specific patterns in the data distribution [54]. Consequently, enhancing feature representation is crucial for the development of robust inference systems. In this investigation, we characterize feature augmentation as the modification of architectures or mapping functions. Various techniques aimed at improving features have significantly enhanced the robustness of medical classification models [55]."}, {"title": "3.6 Distillation of Knowledge", "content": "In the field of machine learning, knowledge distillation is a helpful technique for transferring learned information from a complex (teacher) model to a simple (student) model. Therefore, the situation where the network structures for the teacher and learners are identical is specifically referred to as self-distillation. Furthermore, a great deal of research has been done on the natural imaging domain using adversarial knowledge distillation, which moves the adversarial resilience from a heavy teacher model to a view student model [56]."}, {"title": "4 Result and Discussion", "content": "This study utilizes four publicly accessible benchmark datasets, as detailed in [45], to investigate adversarial attack and defense in the context of medical image processing. Firstly, the Messidorl dataset comprises 1,200 eye fundus color images for detecting diabetic retinopathy across four classes based on retinopathy grade. Secondly, the International Skin Imaging Collaboration (ISIC) dataset consists"}, {"title": "5 Conclusion", "content": "Diagnostic imaging AI systems based on computer vision are increasingly being used as models for classifying and segmenting diseases. The scaled-up use of medical imaging AI systems has given rise to serious safety issues because to DNNs' susceptibility to adversarial samples. Recently, a number of ways have been put out to increase the efficacy of medical image defense tactics. Although numerous protection strategies have been put forth, there are still reservations over the use of medical deep learning techniques. This outcome arises from certain constraints within medical imaging, including a scarcity of high-quality image datasets and labeled data in comparison to more abundant datasets available for natural images. The efficacy of adversarial"}]}