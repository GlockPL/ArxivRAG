{"title": "Progressive Representation Learning for Real-Time UAV Tracking", "authors": ["Changhong Fu", "Xiang Lei", "Haobo Zuo", "Liangliang Yao", "Guangze Zheng", "Jia Pan"], "abstract": "Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at https://github.com/vision4robotics/PRL-Track.", "sections": [{"title": "I. INTRODUCTION", "content": "Robust visual object tracking is fundamental for intelligent unmanned aerial vehicle (UAV) applications, e.g., task planning [1], biodiversity protection [2], and target localization [3]. During the above extensive applications, UAV trackers aim to predict the location of the object in subsequent frames, starting from the initial position in the first frame. Driven by large-scale datasets with manual annotations, Siamese trackers [4]\u2013[7] have shown promising performance by adopting convolutional neural networks (CNNs) to learn object representations. However, when encountered with complex dynamic environments, e.g., aspect ratio change and occlusion, these trackers struggle to obtain robust object representations due to limited representation capabilities of lightweight CNNs like AlexNet [8]. Although trackers with deeper backbones, e.g., ResNet [9], can better learn object representations, they fail to meet the real-time requirement constrained by limited computational resources on UAVs. Hence, robust object representations for UAV tracking are far from sufficient in complex dynamic environments.\nOne promising approach is to explore multi-scale features oriented to UAV tracking tasks [4]. Specifically, convolutional operations are adopted to aggregate multi-scale features from different layers, which contribute to alleviating feature degradation due to occlusion during UAV tracking. However, with limited receptive fields of convolutional kernels, CNNs lack the modeling ability of long-range dependencies [10]. Consequently, it is challenging to capture global context information between multi-scale features. Recently, Vision Transformer (ViT) [11] has exhibited tremendous potential in modeling long-range dependencies by virtue of attention mechanisms. The introduction of ViT into the Siamese trackers addresses the shortcomings of traditional CNN-based trackers in learning global information. Moreover, the intrinsic global modeling capability of ViT proves to be advantageous in tackling appearance variations, e.g., aspect ratio change [12]. Nonetheless, compared with CNN, ViT tends to ignore local spatial information, which decreases the discriminability of image objects [13]. Besides, the quadratic computational complexity and memory cost of the attention mechanisms are obstacles to its wide deployment on embedded processors in UAVs, which have limited computing resources. Therefore, how to extract more reliable information and then generate robust object representations for UAV tracking is worth exploring carefully.\nTo fully exploit the global context information and local spatial information, integrating CNNs and ViT represents a promising complementary coupling. Given the strength of CNNs in fast convergence and filtering redundant information [14], [15], they are well-suited for extracting object local information from images to form coarse object representations. Subsequently, ViT utilizes coarse object representations to refine and enhance the understanding of global context information, thereby generating robust fine object representations. However, considering the distinctions in feature space between the plain CNNs and ViT, directly concatenating them leads to performance degradation [16], [17]. Therefore, how to effectively integrate CNNs and ViT for real-time UAV tracking is a problem worth exploring.\nThis work proposes a novel progressive representation learning framework, namely PRL-Track, which consists of CNN-based coarse representation learning and ViT-based fine representation learning. Leveraging the complementary strengths of the CNNs and ViT, PRL-Track can learn robust fine object representations, achieving satisfactory performance when encountering challenges such as occlusion and aspect ratio change during UAV tracking. Fig. 1 highlights the impressive performance of PRL-Track in UAV tracking, outperforming other 14 state-of-the-art (SOTA) trackers in terms of average precision and success rate. The main contributions of this work are as follows:\n\u2022 A novel progressive representation learning framework dubbed PRL-Track, is proposed to learn robust fine object representations for UAV tracking via a coarse-to-fine perspective, thus improving tracking performance.\n\u2022 An innovative appearance-aware regulator is developed to mitigate appearance interference and extract useful information from shallow features for coarse representation learning. Besides, a convenient semantic-aware regulator is designed to capture semantic information and promote the concentration of deep features.\n\u2022 A new hierarchical modeling generator is proposed to augment the comprehension of contextual information by fusing coarse object representations for fine representation learning, further generating robust fine object representations for UAV tracking.\n\u2022 Comprehensive evaluations confirm that PRL-Track achieves SOTA performance, validating the power of the proposed framework. Real-world tests conducted on the typical UAV platform demonstrate the superior efficiency and robustness of PRL-Track in practical scenarios."}, {"title": "II. RELATED WORK", "content": "Siamese trackers [4], [18], [19] have gained popularity and promoted the development of UAV applications owing to their remarkable tracking performance. These trackers utilize a CNN-based backbone to extract features of both the template patch and search patch, followed by a correlation-based network to calculate the similarity between them. Compared with correlation filter-based trackers [20], [21], fully CNN-based trackers further exploit the local spatial information, thus improving tracking performance. As a pioneer, SiamFC [18] introduces the Siamese framework into object tracking for similarity matching. Inspired by the region proposal network, SiamRPN [22] combines a Siamese network with regression and classification branches, achieving efficient classification and accurate prediction. However, trackers with fully CNN-based architecture lack effective long-range dependency modeling, which means they often struggle to capture global context information. Thus, it is difficult to ensure reliable tracking in complex dynamic environments. To address this issue, ViT [11] has been introduced into object tracking, owing to its high representational capacity for global context information. ViT integrates global contextual information by decomposing the image into fixed-size blocks and processing them with Transformer architecture. TransT [23] proposes a ViT-based feature fusion model for object tracking, achieving promising performance. HiFT [12] introduces a ViT structure optimized for efficient multi-feature fusion, thereby augmenting tracking robustness. SGDVIT [24] designs a saliency-guided dynamic ViT to capture similarity and incorporate information. However, the attention mechanism in ViT often ignores local feature details and object spatial structures [13]. Therefore, a promising approach to overcome these limitations is the integration of CNNs and ViT, leveraging the strengths of both architectures in the context of UAV tracking. CNNs can capture local spatial information, which contributes to maintaining accuracy in the rapid environments in which UAVs operate. By integrating this with ViT\u2019s ability to model global context, the framework can better understand broader scene dynamics, enabling more stable tracking across wide fields of view."}, {"title": "B. Representation Learning", "content": "Representation learning aims to acquire object representations that facilitate the utilization of reliable information when constructing classifiers or predictors [25]. Deep neural networks (DNNs) are commonly employed to extract object representations in visual tasks [8], [26]. Compared with conventional hand-crafted representations, DNNs tend to learn more comprehensive representations [27]. Previous works on representation learning have yielded notable frameworks and methodologies. UniFormer [16] designs a concise unified framework and integrates the strength of CNNS and the ViT, realizing efficient spatiotemporal representation learning. EsViT [28] formulates an efficient self-supervised ViT for representation learning, achieving superior transfer performance in downstream tasks. MARLIN [29] employs a facial video masked autoencoder to learn generic and robust facial representations. HRNet [27] proposes to uphold high-resolution object representations throughout the entire workflow, thereby ensuring the reliability of object representations. Despite the rapid development mentioned above, object representation learning via a coarse-to-fine perspective for real-time UAV tracking has not been investigated yet. Besides, most existing tracking methods [12], [18] struggle to maintain excellent performance in dynamic environments due to limited computing resources and challenges, such as partial occlusion and aspect ratio change. Consequently, an effective progressive representation learning framework for UAV tracking is urgently needed."}, {"title": "III. PROPOSED METHOD", "content": "As depicted in Fig. 2, the proposed PRL-Track is divided into coarse representation learning and fine representation learning. The coarse representation learning generates coarse object representations, obtaining the local spatial information of the object. Building upon this foundation, the fine representation learning generates robust fine object representations for UAV tracking. With the coarse-to-fine progressive perspective, the proposed framework ensures tracking performance in complex dynamic environments, such as occlusion and aspect ratio change."}, {"title": "A. Coarse Representation Learning", "content": "For coarse representation learning, the CNN-based backbone is first utilized to extract multi-scale features. The features extracted by the shallow layers of CNNs tend to include a mass of appearance information. Instead, the features extracted by the deep layers of CNNs tend to enrich semantic information. Therefore, the appearance-aware regulator and the semantic-aware regulator are proposed to process shallow features and deep features, respectively."}, {"title": "1) Appearance-aware regulator (AR):", "content": "The AR is utilized to learn appearance information such as color, edge, and shape from the shallow features.\nAs depicted in Fig. 3(a), a branch called the Gating controller (GC) serves as a switch, determining the activation of related information. Specifically, the features of the first layer $F_1$ and the second layer $F_2$ are the inputs of the GC. Then the convolutional operation (Conv) is employed to achieve cross-channel information integration, where the kernel size in Conv is 1 \u00d7 1. The intermediate results $I_1$ and $I_2$ before concatenation (Concat) are generated as follows:\n$I_1 = Pooling(Norm(Conv(F_1)))$, \n$I_2 = Conv(F_2)$,\nwhere Norm denotes batch normalization, which helps stabilize and accelerate the training process. Besides, the Pooling operation is employed to ensure dimensional alignment. Subsequently, a weight map $\\alpha_c$ can be obtained after Concat and Conv, which is followed by a rectified linear unit activate function (ReLU):\n$\\alpha_c = ReLU(Conv(Concat(I_1, I_2)))$.\nFinally, the weight map $\\alpha_c$ is employed for element-wise multiplication with the features of the third layer $F_3$, followed by a residual connection. Then the output of AR, i.e., $W_3$, can be obtained as:\n$W_3 = CNR(F_3+ \\alpha_cF_3)$,\nwhere the CNR represents the combination operations of Conv, Norm, and ReLU. Additionally, residual connections and activation functions are utilized to speed up network learning and avoid the vanishing gradient problem."}, {"title": "Remark 1:", "content": "The GC is employed to control the flow of features, thereby improving the quality of the object representations. In the learning process, 1\u00d71 Conv can adaptively retain effective information or filter out redundant information, thus enhancing object representations."}, {"title": "2) Semantic-aware regulator (SR):", "content": "The SR is designed to learn semantic information from the deep features, i.e., the features from the fourth and fifth layers.\nAs illustrated in Fig. 3(b), the SR takes the outputs from the previous layer $W_i$ and the feature of current layer $F_i$ as inputs. This enables the SR to dynamically integrate contextual information from both shallow and deep features. Then, the outputs of the two SRs used in the coarse representation learning, i.e., $W_4$ and $W_5$, can be obtained as:\n$W_4 = CNR(F_4 + F_4 \\cdot Conv(BLI(W_3)))$,\n$W_5 = CNR(F_5 + F_5 \\cdot Conv(BLI(W_4)))$,\nwhere the BLI denotes bilinear interpolation, ensuring the alignment of feature dimensions. Notably, the first equation corresponds to the SR depicted in the upper part of Fig. 2, focusing on refining the features from the fourth layer $F_4$. Instead, the second equation corresponds to the SR depicted in the lower part of Fig. 2, which primarily enhances the features from the fifth layer $F_5$."}, {"title": "Remark 2:", "content": "The SR is utilized to extract useful information from deep features and transmit them to the fine representation learning. By leveraging appearance information from the AR, the SR significantly improves scene interpretation capability, which is beneficial for UAV tracking."}, {"title": "B. Fine Representation Learning", "content": "For fine representation learning, the hierarchical modeling generator (HMG) is designed to fuse the interaction information between coarse object representations. The coarse object representations generated during the previous process are first divided into patches, followed by concatenation along the channel dimension.\nAs shown in Fig. 4, the token X aggregated by coarse object representations is decomposed into QKV pairings with different hierarchies, namely $M_3$, $M_4$, and $M_5$. Then they are intertwined in the ViT feature space by performing cross-attention after the interaction operation. This strategy enables the model to capture the relationship between coarse object features at different hierarchies, thereby improving the model's representation ability.\nSpecifically, the process begins by decomposing the input X into query (Q), key (K), and value (V) vectors via linear projection. For the query vectors (Q), further splitting is conducted at the channel level, yielding $Q_3$, $Q_4$, and $Q_5$. Similar operations are performed for the K and V, respectively. From level 3 to level 5, the corresponding query, key, and value pairs at each tier are utilized to reassembly QKV pairings, which can be represented as follows:\n$M_i = Concat(Q_i, K_i, V_i)$, for i = 3, 4, 5 ."}, {"title": "Remark 3:", "content": "Within the proposed HMG, the hierarchy cross-attention is designed to enhance the interaction between different hierarchy representations. To establish hierarchical connections, interaction operations are performed between $M_3$ and $M_4$, as well as between $M_3$ and $M_5$, and between $M_4$ and $M_5$. During the interaction operation between $M_i$ and $M_j$, the keys $K_i$ from $M_i$ and $K_j$ from $M_j$ are concatenated, as well as the values $V_i$ and $V_j$, which can be expressed as:\n$K_{ij} = Concat(K_i, K_j)$,\n$V_{ij} = Concat(V_i, V_j)$,\nwhere i < j, $K_{ij}$ denotes the concatenated key from $M_i$ and $M_j$, while $V_{ij}$ represents the concatenated value.\nThen, cross-attention mechanisms are utilized to integrate information, which can be represented as follows:\n$H_{att}^{34}= Softmax(\\frac{Q_3 \\cdot [K_3,K_4]^T}{\\sqrt{d}}) [V_3, V_4]$,\n$H_{att}^{35}= Softmax(\\frac{Q_5 \\cdot [K_3, K_5]^T}{\\sqrt{d}}) [V_3, V_5]$,\n$H_{att}^{45}= Softmax(\\frac{Q_5 \\cdot [K_4, K_5]^T}{\\sqrt{d}}) [V_4, V_5]$,\nwhere d represents the dimension of the concatenated key. Besides, $H_{att}^{34}$, $H_{att}^{35}$, and $H_{att}^{45}$ are the attention maps of hierarchical representations, respectively.\nRemark 3: The fine representation learning accepts purified coarse object representations and focuses on information fusion across various hierarchical representations. Excluding low-level queries in cross-attention streamlines the integration of relevant information across different levels of representation, thereby reducing computational costs.\nSubsequently, $H_{att}^{34}$, $H_{att}^{35}$, $H_{att}^{45}$, and $H_{att}^{45}$ are concatenated along the channel, followed by residual connection to the input X, which can be expressed as:\n$W_c = Norm(Concat (H_{att}^{34}, H_{att}^{35}, H_{att}^{45}) + X)$."}, {"title": "Remark 4:", "content": "The strategic integration of cross-attention mechanisms facilitates precise interaction and effective fusion of diverse hierarchical features. Moreover, by iteratively fusing coarse object representations, the proposed HMG gradually captures both local and global information for improving performance in complex dynamic environments."}, {"title": "IV. EXPERIMENTS", "content": "The proposed PRL-Track is trained using Python 3.8 and PyTorch 1.13.1 on 2 NVIDIA A100 GPUs for 70 epochs. The backbone of PRL-Track is initialized using AlexNet [8], which has been pre-trained on ImageNet [31]. The learning rate initiates at 5 \u00d7 10\u22124, rises to 10-2, and subsequently decreases to 10-4 in log space. Additionally, the template patch is limited to dimensions of 127 \u00d7 127 \u00d7 3, while the search patch is constrained to 287 \u00d7 287 \u00d7 3. The training dataset are COCO [32], GOT-10K [33], and LaSOT [34]."}, {"title": "B. Evaluation Metrics", "content": "The one-pass evaluation (OPE) metrics [30] are essential for assessing tracking performance, including precision and success rate. Specifically, the precision is measured by the Euclidean distance between the center of the predicted box and the ground truth, which is denoted as the center location error (CLE). The precision plot is drawn by counting the percentage of frames within a certain threshold of CLE. In the general evaluation, the threshold for tracker ranking is set to 20 pixels. The success rate is computed through the intersection over union (IoU) of the ground truth with the predicted box. The success plot is drawn by counting the percentage of frames whose IoU exceeds a predetermined threshold. Meanwhile, the area under the curve (AUC) is computed to rank trackers."}, {"title": "C. Overall Performance", "content": "In this section, PRL-Track is tested on three challenging and authoritative UAV tracking benchmarks with other 14 existing SOTA trackers including LPAT [36], SGDVIT [24], HiFT [12], SiamAPN++ [4], SiamFC [18], DeepSTRCF [38], Ocean [39], DaSiamRPN [40], SE-SiamFC [35], MCCT [21], AutoTrack [20], TADT [41], UDT+ [6], and UDT [6]. Notably, all Siamese trackers use the same lightweight backbone, i.e., AlexNet [8], for a fair comparison."}, {"title": "1) UAVTrack112:", "content": "UAVTrack112 [37] is specifically constructed for UAV tracking, encompassing 112 sequences that introduce challenges for real-world evaluations. It encompasses common challenges [30] encountered in UAV tracking, including aspect ratio change, similar objects, partial occlusion, and so on. The results shown in Fig. 5 demonstrate the remarkable performance of PRL-Track, attaining precision (0.786) and success rates (0.602)."}, {"title": "2) UAVTrack112 L:", "content": "UAVTrack112_L [37] consists of 45 long-term tracking sequences and includes over 60K frames in total. Fig. 5 demonstrates that PRL-Track yields the best performance compared with other SOTA trackers. In the precision, PRL-Track leads the pack with a remarkable score of 0.803, surpassing LPAT (0.760) and SGDViT (0.743), which trail behind in second and third place, respectively. Similarly, PRL-Track achieves the top success rate of 0.597, outperforming LPAT (0.566) and SGDViT (0.554)."}, {"title": "Remark 5:", "content": "In this work, UAVTrack112 L is utilized to validate the long-term tracking performance of the proposed PRL-Track. The experimental results indicate that PRL-Track performs exceptionally well on long sequences, providing a more stable and sustained tracking capability."}, {"title": "3) UAV123:", "content": "UAV123 [30] consists of 123 challenging sequences with a combined total of over 112K frames. These sequences involve demanding aerial scenarios, encompassing occlusion, illumination variation, and low-resolution challenges. Performance evaluation on UAV123 offers valuable insights into the advancement of aerial visual tracking. As shown in Fig. 5, PRL-Track stands out from other trackers with a success rate (0.791) and precision (0.593)."}, {"title": "D. Attribute-Based Comparison", "content": "The robustness of PRL-Track in handling complex UAV tracking challenges is evaluated through attribute-based comparisons. Specifically, the attributes of aspect ratio change (ARC), partial occlusion (POC), scale variation (SV), and viewpoint change (VC) are considered during the evaluation process. As illustrated in TABLE I, PRL-Track performs the best in all four attributes compared with the other 5 SOTA trackers. Notably, PRL-Track achieves superior performance in the ARC, surpassing the second-best performance by 4.5% in precision, and achieving 4.1% increase in success rate. This substantial improvement demonstrates that the proposed PRL-Track can exploit the global connection of multi-scale features, thereby better adapting to scenarios where the scale of the tracking object changes. Additionally, when confronted with partial occlusion, the ViT-based HMG utilizes purified object representations for global modeling, mitigating the impact of object feature degradation caused by occlusion. Moreover, when encountering scale variation, the progressive process of coarse-to-fine exploration can generate more discriminative object representations to keep reliable tracking."}, {"title": "Remark 6:", "content": "The promising results demonstrate that the proposed PRL-Track can learn robust object representations to tackle the challenging scenarios mentioned above. Moreover, these robust object representations contribute to the effectiveness of long-term tracking."}, {"title": "E. Ablation Study", "content": "To demonstrate the effectiveness of each representation learning within PRL-Track, detailed studies conducted on UAVTrack112_L are presented in this section. To ensure fairness, each variant of the tracker is configured with the same settings (including training strategy and parameter configurations) except for the studied module."}, {"title": "1) Clarification of symbol:", "content": "First, the symbols used in TABLE II are explained. This work considers the model with only feature extraction and regression & classification network as Baseline. FLP represents the fine representation learning. AR and SR represent different components used in the coarse representation learning. PRL-Track denotes the full version of the proposed progressive representation learning framework."}, {"title": "2) Result analysis:", "content": "As presented in the TABLE II, integrating FLP directly into the Baseline significantly improved its performance, improving precision by about 10.09% and success rate by 13.16%. This is attributed to the hierarchy modeling generator, which facilitates the integration of features across various scales. However, combining the SR and FLP can lead to performance degradation due to appearance interference from shallow features. On the other hand, combining the AR and FLP enhances tracking precision by 13.11%. Furthermore, adopting the Baseline+AR+SR+FLP configuration yields the best performance, showcasing improvement in precision by 15.71% and in success rate by 17.29% compared to the Baseline. All the aforementioned results verify the efficiencies of the coarse representation learning (AR+SR) and FLP in improving object representation exploration for UAV tracking."}, {"title": "F. Qualitative Evaluation", "content": "As shown in Fig. 6, the visualization comparison results between PRL-Track and the other 4 SOTA trackers demonstrate the robustness of PRL-Track in complex dynamic environments. When encountering similar objects during the tracking process, the two learning processes within PRL-Track produce discriminative object representations, enabling stable and reliable tracking. In contrast, SE-SiamFC [35] is disrupted by similar objects, leading to tracking failure. Furthermore, as observed from the second row of Fig. 6, only PRL-Track completes the re-detection task and achieves tracking restoration after a brief out-of-view period. Finally, in the common scenario of occlusion encountered in UAV tracking, PRL-Track also exhibits superior performance. Owing to the robust fine object representations, the proposed PRL-Track achieves reliable tracking performance."}, {"title": "V. REAL-WORLD TESTS", "content": "To demonstrate the real-world applicability of PRL-Track, extensive testing is conducted on a typical UAV platform, as shown in Fig. 7. Specifically, the UAV platform is equipped with an NVIDIA Jetson Orin NX 16GB-based edge smart camera. During the testing phase, the edge smart camera exhibits the following average utilization rates: RAM usage is at 32.67%, while GPU and CPU record average utilizations of 28.81% and 14.15%, respectively. The experimental results from several of these tests are shown in Fig. 7. These sequences present a variety of challenges, including fast motion, partial occlusion, and illumination variation.\nIn Test 1, the tracked object engages in a basketball game on the court, characterized by rapid and frequent movements. Additionally, due to shooting actions, bodily deformation occurs intermittently. Nonetheless, the PRL-Track consistently maintains a high level of tracking precision in such dynamic scenarios. The Test 2 and Test 3 sequences focus on tracking cars during steady flights, including scenarios with partial occlusion and illumination variation. When encountering occlusion, minor fluctuations are observed in the tracking results but quickly restore stability. Furthermore, the Test 3 sequence highlights the performance of PRL-Track over extended durations, showcasing its robustness in long-term tracking scenarios. Finally, the proposed PRL-Track remains a speed exceeding 42.6 frames per second, demonstrating its superior tracking speed. The experiment results in real-world tests underscore the ability of PRL-Track to learn object representations and achieve stable tracking."}, {"title": "VI. CONCLUSIONS", "content": "In this work, a novel progressive representation learning framework, i.e., PRL-Track, is proposed to extract robust object representations for UAV tracking. In the proposed PRL-Track, two CNN-based regulators are utilized to create coarse object representations. Furthermore, the ViT-based hierarchical modeling generator is adopted to exploit coarse object representations. This progressive learning process empowers the tracker, i.e., PRL-Track, to generate robust object representations, thereby better addressing the challenges in complex UAV scenarios. Extensive experiments, including challenging real-world tests, demonstrate that PRL-Track has achieved outstanding performance. We are convinced that our framework can promote further research in UAV tracking and foster related practical applications."}]}