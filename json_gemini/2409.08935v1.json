{"title": "Optimization and Generalization Guarantees for\nWeight Normalization", "authors": ["Pedro Cisneros-Velarde", "Zhijie Chen", "Sanmi Koyejo", "Arindam Banerjee"], "abstract": "Weight normalization (WeightNorm) is widely used in practice for the training of deep neural networks\nand modern deep learning libraries have built-in implementations of it. In this paper, we provide the first\ntheoretical characterizations of both optimization and generalization of deep WeightNorm models with\nsmooth activation functions. For optimization, from the form of the Hessian of the loss, we note that a\nsmall Hessian of the predictor leads to a tractable analysis. Thus, we bound the spectral norm of the\nHessian of WeightNorm networks and show its dependence on the network width and weight normalization\nterms-the latter being unique to networks without WeightNorm. Then, we use this bound to establish\ntraining convergence guarantees under suitable assumptions for gradient decent. For generalization, we\nuse WeightNorm to get a uniform convergence based generalization bound, which is independent from the\nwidth and depends sublinearly on the depth. Finally, we present experimental results which illustrate how\nthe normalization terms and other quantities of theoretical interest relate to the training of Weight Norm\nnetworks.", "sections": [{"title": "Introduction", "content": "Weight normalization (WeightNorm) was first introduced by Salimans and Kingma [2016]. The idea is to\nnormalize each weight vector-by dividing it by its Euclidean norm-in order to decouple its length from\nits direction. Salimans and Kingma [2016] reported that WeightNorm is able to speed-up the training of\nneural networks, as an alternative to the then recently introduced and still widely-used batch normalization\n(BatchNorm) [Ioffe and Szegedy, 2015]. WeightNorm is different from BatchNorm because it is independent\nfrom the statistics of the batch sample being used at the gradient step. It is also different from another\nclass of normalization, called layer normalization (LayerNorm) [Ba et al., 2016] in that it does not entail the\nnormalization of any activation function of the neural network. Thus WeightNorm also has less computational\noverhead because it does not require the computation and storage of additional mean and standard deviation\nstatistics as the other methods do. However, its empirical testing performance (or accuracy) has often been\nfound to not be as good as other normalization methods by itself, and thus various versions have been\nformulated to improve its performance [Salimans and Kingma, 2016, Huang et al., 2017, Qiao et al., 2020].\nNevertheless, Weight Norm has been one of the most popular normalization methods since its introduction,\nand as a result current machine learning libraries include built-in implementations of it, e.g., PyTorch 2.0.\nParallel to the empirical development of normalization methods, recent years have seen advances in\ntheoretically understanding convergence of gradient descent (GD) and variants for deep learning models [Du\net al., 2019, Allen-Zhu et al., 2019, Zou et al., 2020, Nguyen, 2021, Liu et al., 2022, Banerjee et al., 2023].\nHowever, to the best of our knowledge, no prior work has focused on providing formal optimization and\ngeneralization guarantees for deep models with WeightNorm. In this paper, our contribution is to study the\neffect of WeightNorm on both training and generalization from a theoretical perspective. In particular, we"}, {"title": "Related work", "content": "Weight Norm and Other Normalizations Weight Norm was first proposed by Salimans and Kingma\n[2016], after a year from the introduction of BatchNorm by Ioffe and Szegedy [2015]. LayerNorm was proposed\nin the same year by Ba et al. [2016], and has also become very popular [Xu et al., 2019]. Extensions and\nvariations of WeightNorm have been introduced since then, for example, centered weight normalization [Huang\net al., 2017], orthogonal weight normalization [Huang et al., 2018], and weight standardization [Qiao et al.,\n2020]. Theoretical analysis of normalization techniques in general has caught an increasing recent interest.\nWe focus mostly on WeightNorm, but mention that BatchNorm has some theoretical work, e.g. [Santurkar\net al., 2018, Lian and Liu, 2019]. The work [Arpit et al., 2019] uses a theoretical approach to better come\nup with initialization schemes tailored for WeightNorm networks with ReLU activations. In contrast, our\nwork focuses in understanding the optimization (training) and generalization of WeightNorm networks with\nsmooth activations. Wu et al. [2020] study the effect of WeightNorm on the convergence of overparameterized\nleast squares problem (which translates to a normalization of the parameters to estimate). We, in contrast,"}, {"title": "Training Guarantees of Neural Networks", "content": "Given the increasingly vast literature, we refer the readers\nto the surveys [Fan et al., 2021, Bartlett et al., 2021] for an overview of the field of gradient descent training\nof neural networks. Deep ReLU networks were analyzed by Zou and Gu [2019], Zou et al. [2020], Allen-Zhu\net al. [2019], Nguyen and Mondelli [2020], Nguyen [2021], Nguyen et al. [2021], whereas deep networks with\nsmooth activations-our case-were analyzed by Du et al. [2019], Liu et al. [2022]. Regarding the convergence\nanalysis of gradient descent, Du et al. [2019], Allen-Zhu et al. [2019], Zou and Gu [2019], Zou et al. [2020],\nLiu et al. [2022] used the Neural Tangent Kernel (NTK) approach [Jacot et al., 2018]; whereas Banerjee et al.\n[2023] used the RSC approach. All training guarantees hold over a neighborhood around the initialization\npoint of the network's weights."}, {"title": "Generalization of Neural Networks", "content": "To the best of our knowledge, the first work in studying the\nRademacher complexity for neural networks was [Neyshabur et al., 2015]. Then, the seminal work [Bartlett\net al., 2017] provided generalization bounds for neural networks considering Lipschitz activation functions.\nThe bound, obtained using a covering approach, depends exponentially on the depth of the network. The\ngeneralization bound was tightened for ReLU activations in [Neyshabur et al., 2018] using a PAC-Bayes\napproach, but the same exponential dependency persisted. Then, the work [Golowich et al., 2018] avoided\nsome exponential dependence on the depth and established conditions on the norm of the network's weight\nmatrices in order to obtain generalization bounds independent of depth. Unlike our work, all the cited works\ndo not study WeightNorm networks-indeed, we show that WeightNorm allows the generalization bound\nto depend on $\\sqrt{L}$, where $L$ is the depth. Finally, we remark that our generalization bound estimates how\nwell finding a solution to the empirical loss approximates finding a solution to the loss under the real data\ndistribution. A different problem in the literature is inductive bias, i.e., how a specific optimization method,\nwhile training the network, may converge to a solution that generalizes well. This was studied for Weight Norm\nunder GD in [Morwani and Ramaswamy, 2022] and under stochastic GD in [Li et al., 2022]."}, {"title": "Problem setup: deep learning with WeightNorm", "content": "Consider a training set $\\{(x_i, y_i)\\}_{i=1}^n, x_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^d, y_i \\in \\mathcal{Y} \\subseteq \\mathbb{R}$. For a suitable loss function $l$, the goal\nis to minimize the empirical loss: $\\mathcal{L}(\\theta) = \\sum_{i=1}^n l(y_i, \\hat{y}_i) = \\sum_{i=1}^n l(y_i, f(\\theta; x_i))$, where the prediction\n$\\hat{y}_i := f(\\theta;x_i)$ is from a deep neural network with parameter vector $\\theta \\in \\mathbb{R}^p$ for some positive integer $p$.\nIn our setting f is a feed-forward multi-layer (fully-connected) neural network with weight normalization\n(WeightNorm), having depth $L$ and hidden layers with widths $m_l$, $l \\in [L] := \\{1, ..., L\\}$; and it is given by\n$\\begin{aligned}\n a^{(0)}(x) &= x, \\\\\n a^{(l)}(x) &= \\phi\\left(\\frac{1}{\\sqrt{m_l}}W^{(l)T} \\frac{a^{(l-1)}(x)}{||W^{(l)}||_2} \\right), i = 1, ..., m_l, l = 1, ..., L,\\\\\n f(\\theta;x) &= va^{(L)}(x),\n\\end{aligned}$\nwhere $a_i^{(l)}(x) \\in \\mathbb{R}$ is the i-th entry of the activation function $a^{(l)}(x) \\in \\mathbb{R}^{m_l}$, $W_i^{(l)} \\in \\mathbb{R}^{m_{l-1}}$ is the weight vector\ncorresponding to the transpose of the i-th row of the layer-wise weight matrix $W^{(l)} \\in \\mathbb{R}^{m_l \\times m_{l-1}}$, $l \\in [L]$,\n$v \\in \\mathbb{R}^{m_L}$ is the last layer vector, $\\phi(\\cdot)$ is the smooth (pointwise) activation function. The total set of parameters\nis represented by the parameter vector\n$\\theta := (\\text{vec}(W^{(1)})^T,..., \\text{vec}(W^{(L)})^T, v^T) \\in \\mathbb{R}^{\\sum_{k=1}^L m_k m_{k-1} + m_L },$\nwith $m_0 = d$. The pre-activation function $\\tilde{a}^{(l)} \\in \\mathbb{R}^{m_l}$ has its i-th entry defined by $a_i^{(l)} = \\phi(\\tilde{a}_i^{(l)}(x))$,\n$l \\in [L]$. For simplicity, we assume that the width of all hidden layers is the same, i.e., $m_l = m$, $l \\in [L]$, and so"}, {"title": "Characterizing WeightNorm bounds on the loss and predictor", "content": "To characterize both the optimization with gradient descent (GD) and generalization of WeightNorm networks,\nwe need bounds on the empirical loss and predictor, as well as on their gradients, and the Hessian of the\nlatter. For example, the optimization guarantees in Section 5 make use of the second-order Taylor expansion\nof the empirical loss $\\mathcal{L}$ for which we need to bound its Hessian and gradient. As seen in equation (3), the loss\nHessian contains terms related to the gradient and the Hessian of the predictor $f$, which we then need to\nbound. Now, regarding our generalization guarantees in Section 6, we need to obtain a bound on the values\nof the predictor in order to bound the generalization gap. We obtain this bound by using intermediate results\nderived from both the proofs of the Hessian and empirical loss $\\mathcal{L}$ bounds.\nAll of these bounds are presented in this section, which we believe could also be of independent interest.\nThe proofs are found in Section A and Section B of the appendix."}, {"title": "Hessian bound for WeightNorm", "content": "Under Assumptions 1 and 2, for any $\\theta \\in \\mathbb{R}^p$ with\n$v \\in \\mathcal{B}_{Euc}(v_0)$, and any $x_i$, $i \\in [n]$, we have\n$\\|\\nabla^2 f(\\theta;x_i)\\|_2 \\leq O \\left(\\frac{(1+\\rho_1)L^3(\\frac{1}{\\sqrt{m}}+ L^2 \\max\\{\\|\\phi(0)\\|, \\|\\phi'(0)\\|^2\\})}{\\min\\limits_{l \\in [L]} \\|W^{(l)}\\|_2} \\right)$"}, {"title": "Hessian bound for WeightNorm under $\\phi(0) = 0$", "content": "Under Assumptions 1 and 2, and\nassuming that the activation function satisfies $\\phi(0) = 0$, we have for any $\\theta \\in \\mathbb{R}^p$ with $v \\in \\mathcal{B}_{Euc}(v_0)$, and any\n$x_i$, $i \\in [n]$,\n$\\|\\nabla^2 f(\\theta; x_i)\\|_2 \\leq O \\left(\\frac{(1+\\rho_1)L^3}{\\sqrt{m} \\min\\limits_{l \\in [L]} \\|W^{(l)}\\|_2} \\right)$"}, {"title": "Comparison to networks without WeightNorm", "content": "The recent works [Liu et al., 2020,\nBanerjee et al., 2023] analyzed the Hessian spectral norm bound for feedforward networks without WeightNorm.\n[Liu et al., 2020] presented an exponential dependence on the network's depth $L$, whereas [Banerjee et al.,\n2023] avoided exponential dependence upon the choice of the initialization variance for the hidden layers'\nweights. We show that WeightNorm eliminates such exponential dependence for at most $L^5$ independent\nfrom any initialization of the weights, which improves to $L^3$ when $\\phi(0) = 0$. Another important difference is\nthat our bound also depends on the inverse of the minimum weight vector norm-large enough values on the\nweight vectors decrease the upper bound. Finally, our bound is defined over any value on the weights, and\nnot just over a neighborhood around the initialization point."}, {"title": "Dependence on the activation function", "content": "Corollary 4.1 shows that the Hessian bound\nbecomes tighter when $\\phi(0) = 0$ because of an additional $\\frac{1}{\\sqrt{m}}$ scaling. In contrast, the Hessian bounds by Liu\net al. [2020], Banerjee et al. [2023] have the scaling $\\frac{1}{\\sqrt{m}}$ independent from the value of $\\phi(0)$. The reason for\nthis difference is the use of different scale factors: while those two other works introduce an extra scale factor\n$\\frac{1}{\\sqrt{m}}$ on the linear output layer, we do not. In our paper, we follow the neural network scaling as done in the\nseminal work on the Neural Tangent Kernel by Jacot et al. [2018]."}, {"title": "Predictor gradient bounds", "content": "Under Assumptions 1 and 2, for any $\\theta \\in \\mathbb{R}^p$ with $v \\in \\mathcal{B}_{Euc}(v_0)$,\nand any $x_i$, $i \\in [n]$, we have\n$\\|\\nabla f(\\theta; x_i)\\|_2 \\leq \\rho_0 \\leq O\\left((1+L\\|\\phi(0)\\|\\sqrt{m}) \\left(1 + \\frac{\\sqrt{L(1 + \\rho_1)}}{\\|W^{(l)}\\|_2}\\right)^2\\right)$\nwhere $\\rho_1 := (1 + L\\|\\phi(0)\\|\\sqrt{m})^2 + 4 (1 + \\rho_1)^2 \\sum_{l=1}^{L-1} (1 + (l-1) \\|\\phi(0)\\|)^2$, and\n$\\|\\nabla_x f(\\theta; x_i)\\|_2 \\leq 1 + \\rho_1$."}, {"title": "Predictor gradient bounds under $\\phi(0) = 0$", "content": "Under Assumptions 1 and 2, and assuming\nthat the activation function satisfies $\\phi(0) = 0$, we have for any $\\theta \\in \\mathbb{R}^p$ with $v \\in \\mathcal{B}_{Euc}(v_0)$, and any $x_i$, $i \\in [n]$,\n$\\|\\nabla f(\\theta; x_i)\\|_2 \\leq \\rho_0 \\leq O \\left(1 + \\frac{\\sqrt{L(1 + \\rho_1)}}{\\sqrt{m} \\|W^{(l)}\\|_2}\\right)$,\nwhere $\\varrho := 1 + \\frac{4L(1 + \\rho_1)^2}{m \\|W^{(l)}\\|_2}$."}, {"title": "The Lipschitz constant is the same for networks of any depth", "content": "Surprisingly, the\nLipschitz constant of the network (7)-unlike networks without WeightNorm-does not explicitly depend on\nthe depth $L$ of the network nor its width $m$. It depends on the radius $\\rho_1$ which can be set to be a constant\n$\\mathcal{O}(1)$. This is a direct effect of WeightNorm: when computing $\\nabla_x f(\\theta;x_i)$, we use a chain-rule argument\nthat depends on the product of Jacobians of the output $a^{(l)}$ at layer $l$ with respect to the previous output\n$a^{(l-1)}$, and we show each Jacobian has at most unit norm due to Weight Norm. Thus, no matter how deep\nthe network is, the final effect this product of Jacobians has on the Lipschitz constant of the network is just a\nconstant one."}, {"title": "Empirical loss and empirical loss gradient bounds", "content": "Consider the square loss.\nUnder Assumptions 1 and 2, the following inequality holds for any $\\theta \\in \\mathbb{R}^p$ with $v \\in \\mathcal{B}_{Euc}(v_0)$,\n$\\mathcal{L}(\\theta) \\leq \\varphi \\leq O((1 + \\rho_1)^2(1 + L^2\\|\\phi(0)\\|^2m)),$\nwhere $\\varphi := \\frac{2}{n} \\sum_{i=1}^{n} y_i^2 + 2 (1 + \\rho_1)^2(1 + L\\|\\phi(0)\\|\\sqrt{m})^2$. Moreover,\n$\\|\\nabla_{\\theta} \\mathcal{L}(\\theta)\\|_2 \\leq 2\\sqrt{\\mathcal{L}(\\theta)} \\rho_0 \\leq 2\\rho_0 \\sqrt{\\varphi}$,"}, {"title": "Empirical loss and empirical loss gradient bounds under $\\phi(0) = 0$", "content": "Consider the\nsquare loss. Under Assumptions 1 and 2, and assuming that the activation function satisfies $\\phi(0) = 0$, the\nfollowing inequality holds for any $\\theta\\in \\mathbb{R}^r$ with $v \\in \\mathcal{B}_{Euc}(v_0)$,\n$\\mathcal{L}(\\theta) \\leq \\varphi \\leq O((1 + \\rho_1)^2)$,\nwhere $\\varphi := \\frac{2}{n} \\sum_{i=1}^{n} y_i^2 + 2 (1 + \\rho_1)^2$. Moreover, the bound in equation (10) holds with $\\varrho$ as in Corollary 4.2."}, {"title": "Comparison to networks without WeightNorm", "content": "As in Remark 4.1, our bounds are\npolynomial on $L$ irrespective of weight values, whereas previous results were exponential on $L$."}, {"title": "Optimization guarantees for WeightNorm", "content": "In this section we prove our training guarantees for WeightNorm networks under the square loss. We first\nintroduce two technical lemmas that will be used for our convergence analysis, defining the restricted strong\nconvexity (RSC) property and a smoothness-like property respectively. These two properties use the bounds\nderived in Section 4. All proofs are found in Section C of the appendix."}, {"title": "Q sets", "content": "Given $\\theta\\in \\mathbb{R}^p$ and $\\kappa\\in (0,1]$, define $\\mathcal{Q}_\\kappa := \\{\\theta \\in \\mathbb{R}^p : |\\langle \\theta - \\theta, \\nabla_{\\theta} \\mathcal{L}(\\theta) \\rangle| \\geq \\kappa\\}$."}, {"title": "RSC for WeightNorm under Square Loss", "content": "For square loss, under Assumptions 1 and 2,\nfor every $\\theta' \\in \\mathcal{Q}_\\kappa \\cap \\mathcal{B}_{p_2}(\\theta)$ with $\\theta \\in \\mathbb{R}^p$ and $v, v' \\in \\mathcal{B}_{Euc}(v_0)$,\n$\\mathcal{L}(\\theta') \\geq \\mathcal{L}(\\theta) + \\langle \\theta' - \\theta, \\nabla_{\\theta} \\mathcal{L}(\\theta) \\rangle + \\frac{\\alpha_{\\theta}}{2} \\|\\theta' - \\theta\\|^2$,\nwith\n$\\alpha_{\\theta} \\geq \\frac{\\kappa^2 \\|\\nabla_{\\theta} \\mathcal{L}(\\theta) \\|^2}{2 \\mathcal{L}(\\theta)} - O \\left(\\left(1 + \\frac{\\sqrt{L}}{\\|W^{(l)}\\|_2}\\right) \\frac{(1+\\rho_2)(1 + \\rho_1)^3 L^3 A(m, L^2, \\|\\phi(0)\\|) B(1, L, \\|\\phi(0)\\| \\sqrt{m})}{\\min \\{\\|W^{(l)}\\|_2, \\|W^{(l)}\\|_2\\}} \\right)$,\nwhere $A(m, L^2, \\|\\phi(0)\\|) = m + L^2 \\max\\{\\|\\phi(0)\\|, \\|\\phi'(0)\\|^2\\}$ and $B(1, L, \\|\\phi(0)\\| \\sqrt{m}) = 1 + L \\|\\phi(0)\\| \\sqrt{m}$. We say\nthat the empirical loss $\\mathcal{L}$ satisfies the RSC property w.r.t. $(\\mathcal{Q}_\\kappa \\cap \\mathcal{B}_{Euc}(\\theta),\\theta)$ whenever $\\alpha_{\\theta} > 0$."}, {"title": "RSC for WeightNorm under Square Loss and $\\phi(0) = 0$", "content": "Consider the square\nloss. Under Assumptions 1 and 2 and assuming that the activation function satisfies $\\phi(0) = 0$, for every\n$\\theta' \\in \\mathcal{Q}_\\kappa \\cap \\mathcal{B}_{p_2}(\\theta)$ with $\\theta \\in \\mathbb{R}^p$ and $v, v' \\in \\mathcal{B}_{Euc}(v_0)$, the inequality in (13) becomes\n$\\alpha_{\\theta} \\geq \\frac{\\kappa^2 \\|\\nabla_{\\theta} \\mathcal{L}(\\theta) \\|^2}{2 \\mathcal{L}(\\theta)} - O \\left(\\left(1 + \\frac{\\sqrt{L}}{\\sqrt{m} \\|W^{(l)}\\|_2}\\right) \\frac{(1 + \\rho_2)(1 + \\rho_1)^3 L^3}{\\min \\{\\|W^{(l)}\\|_2, \\|W^{(l)}\\|_2\\}} \\right)$"}, {"title": "The RSC parameter $\\alpha_{\\theta}$ and prior work", "content": "Our RSC characterization in the left-hand\nside of (13) depends on $\\frac{\\|\\nabla_{\\theta} \\mathcal{L}(\\theta) \\|^2}{\\mathcal{L}(\\theta)}$, whereas [Banerjee et al., 2023]\u2013the work that introduced the RSC-based\noptimization analysis-depends on $\\| \\sum_{i=1}^n \\nabla_{\\theta} f(\\theta, x_i)\\|^2$. Moreover, the minimum weight vector norm appears\nin the right-hand side since we used the bounds on Section 4."}, {"title": "Connection with the restricted Polyak-Lojasiewicz (PL) inequality", "content": "The RSC\ncondition has the form $\\frac{\\|\\nabla \\mathcal{L}(\\theta) \\|^2}{\\mathcal{L}(\\theta)} \\geq 2 \\mu_{\\theta}$ where $\\mu_{\\theta}$ depends on the parameter $\\theta$, and in turn, the parameter $\\theta$ is\nrestricted to a specific set. This is a milder condition than the so-called restricted PL condition [Oymak and\nSoltanolkotabi, 2019], which is satisfied whenever $\\mu_{\\theta}$ is a constant independent from $\\theta$. Our RSC condition\nbecomes even milder when $\\phi(0) = 0$ (see Corollary 5.1), since now the parameter $\\mu_{\\theta}$ has the term $\\frac{1}{\\sqrt{m}}$\nwhich decreases with the width."}, {"title": "The right-hand side of the RSC parameter $\\alpha_{\\theta}$", "content": "When $\\phi(0) = 0$, Corollary 5.1 shows that\nthe right-hand side of (13) will have the scaling parameter $\\frac{1}{\\sqrt{m}}$ due to the Hessian bound (Corollary 4.1)."}, {"title": "Smoothness-like property for WeightNorm under Square Loss", "content": "For square loss,\nunder Assumptions 1 and 2, for every $\\theta, \\theta' \\in \\mathbb{R}^p$ with $v, v' \\in \\mathcal{B}_{Euc}(v_0)$,\n$\\mathcal{L}(\\theta') \\leq \\mathcal{L}(\\theta) + \\langle \\theta' - \\theta, \\nabla_{\\theta} \\mathcal{L}(\\theta) \\rangle + \\frac{\\beta_{\\theta}}{2} \\|\\theta' - \\theta\\|^2,$\nwith\n$\\beta_{\\theta} \\leq O \\left(L^4 A(1, L^2, \\|\\phi(0)\\|)(1 + \\rho_1)^2 \\left(1 + \\frac{1}{\\min \\{\\|W^{(l)}\\|_2, \\|W^{(l)}\\|\\}}\\right)\\right)$,\nwhere $A(1, L^2, \\|\\phi(0)\\|) = (1 + L^2 \\max\\{\\|\\phi(0)\\|, \\|\\phi'(0)\\|^2\\}m)^2$."}, {"title": "Smoothness-like property for WeightNorm under Square Loss and $\\phi(0) = 0$", "content": "Consider the\nsquare loss. Under Assumptions 1 and 2 and assuming that the activation function satisfies $\\phi(0) = 0$, for\nevery $\\theta, \\theta' \\in \\mathbb{R}^p$ with $v, v' \\in \\mathcal{B}_{Euc}(v_0)$, the inequality in (16) becomes\n$\\beta_{\\theta} \\leq O \\left(\\frac{L^3(1 + \\rho_1)^3}{\\sqrt{m} \\min \\{\\|W^{(l)}\\|_2, \\|W^{(l)}\\|\\}}\\right)$"}, {"title": "The $\\alpha_{\\theta} / \\beta_{\\theta}$ ratio", "content": "If the expressions in equations (12) and (15) hold for any $\\theta,\\theta' \\in \\mathbb{R}^p$\nso that $\\alpha_{\\theta} = a > 0$ and $\\beta_{\\theta} = \\beta > 0$, then we have the definitions of strong convexity and smoothness,\nrespectively-moreover, it immediately follows that $\\frac{\\alpha}{\\beta} < 1$. Having such ratio between the parameters being\nless than the one is, in general, not guaranteed for the RSC and the smoothness-like properties of any function;\nhowever, it is in our case. From the proof of Lemma 5.2 it holds that $2\\rho_0^2 < \\beta_{\\theta}$. Thus, using Proposition 4.1,\nwe have that $\\alpha_{\\theta} < \\frac{\\kappa^2 \\|\\nabla \\mathcal{L}(\\theta) \\|^2}{\\mathcal{L}(\\theta)} \\leq \\frac{\\kappa^2}{2 \\rho_0^2} 2 \\rho_0^2 < \\beta_{\\theta}$, since $\\kappa^2 < 1$, which implies $\\frac{\\alpha_{\\theta}}{\\beta_{\\theta}} < 1$."}, {"title": "Iterates' conditions", "content": "Consider iterates $\\{\\theta_t\\}_{t=0,1,...,T}$ and that: (A3.1) $\\alpha_{\\theta_t} > 0$; (A3.2)\ngradient descent (GD) update $\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t)$ with learning rate $\\eta_t$ chosen appropriately so that\n$v_t \\in \\mathcal{B}_{Euc}(v_0)$; (A3.3) $\\rho_2$ is chosen so that $\\theta_{t+1} \\in \\mathcal{B}_{p_2}(\\theta_t)$."}, {"title": "Global Empirical Loss Reduction for WeightNorm under Square Loss, [Banerjee\net al., 2023, Theorem 5.3]", "content": "Let $\\alpha_{\\theta_t}, \\beta_\\theta$ be as in Lemmas 5.1 and 5.2 respectively, and $\\mathcal{B}_t := \\mathcal{Q}_\\kappa \\cap\n\\mathcal{B}_{p_2}(\\theta) \\cap \\{\\theta \\in \\mathbb{R}^p | v \\in \\mathcal{B}_{Euc}(v_0)\\}$. Let $\\theta^* \\in \\arg \\inf_{\\theta \\in {\\theta\\in \\mathbb{R}^p | v \\in \\mathcal{B}_{Euc}(v_0)\\}} \\mathcal{L}(\\theta)$, $\\theta_{t+1} \\in \\arg \\inf_{\\theta \\in \\mathcal{B}_t} \\mathcal{L}(\\theta)$, and\n$\\gamma_t := \\frac{\\mathcal{L}(\\theta_{t+1}) - \\mathcal{L}(\\theta^*)}{\\mathcal{L}(\\theta_t) - \\mathcal{L}(\\theta^*)}$. Under Assumptions 1, 2, and 3, we have $\\gamma_t \\in [0,1)$, and for gradient descent with step\nsize $\\eta_t = \\omega t$, $\\omega_t \\in (0,2)$,\n$\\mathcal{L}(\\theta_{t+1}) - \\mathcal{L}(\\theta^*) \\leq \\left(1 - \\frac{\\alpha_{\\theta_t}}{\\beta_{\\theta_t}} \\omega_t (1 - \\frac{\\alpha_{\\theta_t}}{\\beta_{\\theta_t}} \\omega_t)\\right) \\left(\\mathcal{L}(\\theta_t) - \\mathcal{L}(\\theta^*)\\right)$."}, {"title": "Important differences with respect to networks without WeightNorm", "content": "Theorem 5.1\ndifferentiates from [Banerjee et al., 2023, Theorem 5.3] in that (i) it is deterministic, i.e., it is not stated\nas holding with high probability; (ii) does not have all the weights of the network defined over a particular\nneighborhood (with the exception of the output linear layer). Indeed, these differences also hold for all the\npreviously derived results in Section 4 and Section 5."}, {"title": "Generalization guarantees for WeightNorm", "content": "We establish generalization bounds for WeightNorm networks. Our bound is based on a uniform convergence\nargument by bounding the Rademacher complexity of functions of the form (1) as long as the last layer\nvector v stays within a ball of radius $\\rho_1$-the same condition we used in our optimization analysis. The core\nof the analysis relies on the use of a contraction-like property across the network layers similar to [Golowich\net al., 2018] and uses WeightNorm and the network's scaling to avoid exponential dependence on the depth\nand avoid any dependence on the width. All proofs are found in Section D of the appendix.\nConsider the training set $S = \\{(x_i, y_i) \\sim \\mathcal{D}, i \\in [n]\\}$, where $\\mathcal{D}$ denotes the true but unknwon distribution.\nThe training and population losses are respectively defined as\n$\\mathcal{L}_S(\\theta) := \\frac{1}{n} \\sum_{i=1}^n l(y_i, f(\\theta; x_i)) \\quad and \\quad \\mathcal{L}_\\mathcal{D}(\\theta) := \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[l(Y, f(\\theta; X))].$\nWe added the subscript S to the empirical loss notation to explicitely denote its dependence on the training\nset S."}, {"title": "Generalization Bound for WeightNorm under Square Loss", "content": "Consider the square\nloss and the training set $S = \\{(x_i, y_i) \\stackrel{i.i.d}{\\sim} \\mathcal{D}, i \\in [n"}]}