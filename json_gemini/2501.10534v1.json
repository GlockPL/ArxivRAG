{"title": "4bit-Quantization in Vector-Embedding for RAG", "authors": ["Taehee Jeong"], "abstract": "Retrieval-augmented generation (RAG) is a promising technique that has shown great potential in addressing some of the limitations of large language models (LLMs). LLMs have two major limitations: they can contain outdated information due to their training data, and they can generate factually inaccurate responses, a phenomenon known as hallucinations. RAG aims to mitigate these issues by leveraging a database of relevant documents, which are stored as embedding vectors in a high-dimensional space. However, one of the challenges of using high-dimensional embeddings is that they require a significant amount of memory to store. This can be a major issue, especially when dealing with large databases of documents. To alleviate this problem, we propose the use of 4-bit quantization to store the embedding vectors. This involves reducing the precision of the vectors from 32-bit floating-point numbers to 4-bit integers, which can significantly reduce the memory requirements. Our approach has several benefits. Firstly, it significantly reduces the memory storage requirements of the high-dimensional vector database, making it more feasible to deploy RAG systems in resource-constrained environments. Secondly, it speeds up the searching process, as the reduced precision of the vectors allows for faster computation. Our code is available at https://github.com/taeheej/4bit-Quantization-in-Vector-Embedding-for-RAG.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) tasks [1]\u2013[3], demonstrating their ability to store a vast amount of knowledge as implicit parameters. The responses generated by LLMs, given a query, are often useful and informative, as they are based on a wide variety of information learned during the training process. However, despite their impressive performance, LLMs still suffer from several limitations.\nOne major issue is that the number of model parameters required to achieve good performance is growing exponentially. This means that as the model is trained on more data, the number of model parameters required to store the learned knowledge increases rapidly, making it challenging to deploy and maintain these models. Another limitation is that LLMs are fundamentally limited in their ability to incorporate time-sensitive or not-publicly available information. This is because the data used to train LLMs is typically based on a snapshot of the internet at a particular point in time, and may not reflect the latest developments or updates.\nAs a result, LLMs are prone to producing fact hallucinations, where the model generates responses that are not grounded in reality. This can be particularly problematic in applications where accuracy and reliability are critical.\nRecently, a new paradigm has emerged to address these limitations: Retrieval-Augmented Generation (RAG). RAG combines pre-trained language models with external knowledge retrieval, allowing the model to access and incorporate time-sensitive and not-publicly available information [4], [5].\nThe RAG system works as follows: for a given input query, a neural retriever performs a search to find the most relevant documents from an external knowledge database. The retrieved documents, along with the query, are then used to prompt the language model, which generates its output by augmenting the retrieved documents with the given query. The work flow of the RAG system is illustrated in Fig. 1."}, {"title": "II. RELATED WORK", "content": "Traditional searching algorithms, such as bag-of-words retrieval methods, rank a set of documents based on the frequency of query terms appearing in each document. One popular algorithm is Term Frequency-Inverse Document Frequency (TF-IDF), which evaluates the importance of a word in a document relative to a collection of documents by multiplying the Term Frequency (TF) and Inverse Document Frequency (IDF) values. TF-IDF is widely used in search engines to rank documents based on their relevance to a user's query.\nAnother algorithm is Best Matching 25 (BM25) [10], which is a variant of the TF-IDF algorithm. BM25 estimates a probability of relevance of documents to a given query by incorporating TF, IDF, and document length normalization. BM25 uses a normalization factor to ensure that the scores are comparable across documents of different lengths, which makes effective at handling long documents. BM25 is a powerful and widely-used ranking function in information retrieval, known for its balance between simplicity and effectiveness.\nUnlike traditional searching algorithms that look for exact matches of information like keyword matches, vector search represents data points as vectors, which have direction and magnitude, in a highly-dimensional space. With vector search, the individual dimensions define a specific attribute or feature, and the search compares the similarity of the query vector to the possible vectors in a database. Unstructured data like images, text, and audio can be represented as vectors within a high-dimensional space, aiming to efficiently locate and retrieve vectors that closely match the query vector. Metrics such as Euclidean distance or cosine similarity are commonly used to assess the similarity between vectors.\nEuclidean distance between two vectors P and Q with two dimension is defined as follows.\n$d(P, Q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$ (1)\nCosine similarity between two vectors P and Q with n dimension is defined as follows.\ncosine similarity = cos(0)\n$\\frac{P \\cdot Q}{||P|| ||Q||} = \\frac{\\sum_{i=1}^{n} (p_i q_i)}{\\sqrt{\\sum_{i=1}^{n} (p_i)^2} \\sqrt{\\sum_{i=1}^{n} (q_i)^2}}$ (2)\nThe constantly growing amount of available information resources has led to a high demand for efficient similarity searching methods. Vector search algorithms are designed to efficiently find the most similar vectors to a given query vector in a high-dimensional space. Some popular vector searching algorithms are K-Nearest Neighbors (KNN), Approximate Nearest Neighbor (ANN), and Product quantization (PQ).\nAlso known as the Brute force algorithm, KNN [12] finds the K closest vectors to the query vector by calculating the distance (often Euclidean distance) between the query and all other vectors in the dataset. It guarantees finding the exact nearest neighbors but can be computationally expensive for large datasets.\nANN [13] allows for a small error margin and returns points that are approximately nearest. This is not an exact nearest neighbors search, but an approximate closest points search. ANN loses some accuracy but gains a significant speedup compared to exact nearest neighbor search algorithms. Popular ANN approaches include k-d trees [14], Locality Sensitive Hashing (LSH) [15], and Hierarchical Navigable Small World Algorithm (HNSW) [16] .\nK-d trees [14] are a type of binary search algorithm that efficiently organizes and searches large, multi-dimensional datasets. They work by recursively partitioning data along different dimensions at each level, using the median or mean to determine the splitting point. This creates a hierarchical structure that enables fast querying by traversing the tree and pruning branches based on the query point's coordinates, reducing the search space and improving efficiency.\nLocality Sensitive Hashing (LSH) [15] is a technique that enables efficient similarity search by hashing similar points to the same bucket. Unlike traditional hashing, LSH maximizes collisions between similar points by applying multiple hash"}, {"title": "III. METHODOLOGY", "content": "There have been extensively studied quantization for LLMs [22]\u2013[27]. We would like leverage some of these technologies.\nA. BFloat16\nFloating Point 32-bit (FP32) is a default data type for neural network computations and its weights and activation. It consists of 1 bit sign, 8 bits exponent, and 23 bits fraction. When precision calculation is not critical, Floating Point 16-bit (FP16) can be used since it reduces the memory and speed up communication. FP16 consists of 1 bit sign, 5 bits exponent, and 10 bits fraction. Recently Bfloat16 (Brain Floating Point 16-bit, BF16) [17] is becoming popular for machine learning tasks and its hardware implementation. BF16 is also 16 bits, but with a different format. It has 1 bit sign, 8 bits exponent, and 7 bits fraction. BF16 has a wider range but lower precision compared to FP16. BF16 provides the same range of FP32\nand comes close to being a drop-in replacement for FP32 during training and deploying neural networks. In the case of FP16, special handling such as loss scaling might be required. BF16 is now supported in modern CPU, GPU, and NPU. In our work, default computing was conducted in BF16. Fig.3 compares each datatype.\nB. INT8\nINT8 uses 8 bits integers instead of floating point numbers and integer computation instead of floating point computing, reducing both memory and computing requirements. INT8 quantization has been applied to many neural networks to decrease the computational time and energy consumption of neural networks [18]. In neural network quantization, the weights and activation tensors are stored in 8-bit precision than the 32-bit precision which were trained. When transforming from 32 to 8 bits, the memory overhead of storing tensors decreases by a factor of 4 while the computational cost for matrix multiplication reduces by a factor of 16 [19]. However, the use of 8-bit quantization in a network can result in quantization loss, which may subsequently cause a decrease in the model's overall accuracy.\nC. Quantization process\nSuppose x \u2208 Rd and xq \u2208 Rd are a full-precision and a quantized vector respectively.\nUsing the following symmetric linear quantization function [20], a full-precision x can be mapped to a quantized vector xq.\n$xq = Clamp(Round[\\frac{x}{S}])/S$ (3)\nwhere Clamp restricts the value of its argument to the quantized range [-2b-1,2b\u22121 \u2013 1], b is the number of bits for quantization. Round is the round-to-nearest operator, and S\u2208 R is the scaling factor as defined as S = $\\frac{max_x}{|x|}$\nTo invert from a quantized vector xq to a full-precision x, the scaling factor is simply multiplied.\nXdq = S X X q (4)"}, {"title": "D. INT4 and Group-wise quantization", "content": "While INT8 quantization is effective in reducing both the memory and computing cost while preserving accuracy in neural network and vector database, it remains unclear whether we can leverage INT4 to achieve further memory reduction. In this study, we explore the feasibility of employing INT4 quantization for vector space with group-wise quantization.\nDirectly quantizing high-dimensional vectors as an entirety with the same quantization scale can significantly degrade the accuracy in INT4 quantization [19]. In the case of group-wise quantization [21], a high-dimensional vector would be split into several groups with same group size. Each group can have its own quantization scale. The effect of group-wise quantization is further investigated in section IV."}, {"title": "IV. EXPERIMENT", "content": "We designed two experimental setups to evaluate the accuracy of various quantization methods. The first setup uses cosine similarity scores to compare the precision of different data types, focusing on 4-bit quantization and group size effects. The second setup analyzes accuracy degradation in retrieval tasks, comparing our quantization methods to a 32-bit baseline. This comprehensive evaluation framework provides valuable insights into the trade-offs between quantization precision and accuracy.\nFor the cosine similarity measurement, we randomly selected 1000 embedding vectors from dbpedia-openai-1M-1536-angular dataset [9]. Since the total number of available pairs is $\\frac{N X (N-1)}{2}$, we measured similarity score for $\\frac{1000 X 999}{2}$ = 499,500 pairs of the embedding vectors. The distributions of the measured cosine similarity score of all the pairs of the 1000 vectors for different quantization method are shown in Fig 4.\nFig.4 presents four histograms representing the distribution of cosine similarity values for different data representations: FP32, BF16, INT8, and INT4. All histograms exhibit a bell-shaped curve, indicating a normal distribution of cosine similarity values. The peak of the distribution shifts slightly to the left as the data representation moves from FP32 to INT4. This implies that the average cosine similarity decreases with lower precision data representations. The width of the distribution also increases as the data representation moves from FP32 to INT4. This indicates a larger spread of cosine similarity values for lower precision representations, suggesting potentially more variability in the data. The tails of the distributions become heavier as the data representation moves from FP32 to INT4. This means that there are more data points with extremely low or high cosine similarity values in the lower precision representations. The observed changes in the histograms suggest that quantizing the data from FP32 to lower precision formats (BF16, INT8, and INT4) introduces quantization loss and reduces the accuracy of the cosine similarity calculation. This can lead to a decrease in the average similarity and an increase in the variability of the results. This observation is consistent with the RMSE result in Fig.5.\nRoot Mean Square Error (RMSE) between two vectors P and Q is defined as follows.\nRMSE(P, Q) = $\\sqrt{\\frac{\\sum_{i=1}^{N}(p_i - q_i)^2}{N}}$ (5)\nTo evaluate the accuracy of the cosine similarity values of quantized pairs, we calculated the RMSE between their cosine similarity values and those of the baseline 32-bit floating-point representations. The result is shown in Fig.5 and Table I.\nFig.5 illustrates the relationship between various quantization methods and their respective error rate of the cosine similarity. Since the embedding vectors are normalized to a range of 0 to 1, the RMSE values can be interpreted as an error rate. The chart shows a clear trend: Error rates tend to rise as the datatype shifts from BF16 to INT4. The accuracy of quantization methods, as measured by cosine similarity,\ndecreases as the datatype precision is reduced. For INT4, the accuracy decreases as the group size increases.\nTo evaluate the accuracy of informational retrieval using our vector quantization approach, we conducted a comprehensive experiment. We split dbpedia-openai-1M-1536-angular dataset into 900K and 100K after permutation. Then, we carefully selected 10 embedding vectors from the 900K dataset. The magnitude of cosine similarity values for all possible pairs among the 10 selected vectors is less than 0.1, which means they are almost orthogonal to each other. Using the 10 selected vectors as queries, we searched relevant vectors from the 100K dataset. The overall process is illustrated in Fig.6\nFirst, we computed the cosine similarity value between each query vector and every vector in the 100K dataset. This allowed us to identify the top 10 most relevant vectors for each query vector, based on their cosine similarity scores. This resulted in a total of 100 unique vectors that did not overlap with each other. We treated this list of 100 vectors as our baseline, representing the most relevant results without any quantization.\nNext, we applied vector quantization to the entire vector space and then searched for the top 10 relevant vectors for each of the 10 query vectors, based on their cosine similarity scores. This time, however, we used the quantized vectors instead of the original floating-point vectors. We then compared the resulting list of top 10 relevant vectors for each query vector with the baseline list obtained earlier. To assess the accuracy of our quantization approach, we counted the number of vectors that overlapped between the two lists. In other words, we measured the proportion of vectors that were common to both the baseline list and the list obtained after quantization. This overlap metric served as a proxy for the accuracy of retrieval for quantization. The result is shown in Fig.7 and Table II.\nFig.7 illustrates the relationship between various quantization methods and their corresponding retrieval accuracy values. Notably, the dotted line represents the accuracy achieved by the HNSW algorithm (M=64 and ef=50) [16] which is a state-of-the-art algorithm used for an approximate search of nearest neighbours. The data reveals a downward trend, indicating that as the precision of the quantized vectors decreases, the accuracy values also tend to decrease. Although the accuracy drops significantly with INT4, using INT4 with a group size of 128 or less yields higher accuracy than the HNSW algorithm."}, {"title": "V. COMPARATIVE ANALYSIS", "content": "We compared our quantization method with Product-quantization [32]. We conducted two experiments.\nFirst, we evaluated the accuracy of searching relevant documents after Product-quantization. We quantized the 100K dbpedia-openai-1M-1536-angular dataset using Product-quantization algorithm. The experimented combinations of the number of sub-vectors, Mand the number of quantized buckets, K as follows. (48,256), (32, 256), (16, 256), (8, 256), (32, 16), (16, 16), (8, 16). After computing the cosine similarity value between each query vector and every vector in the 100K dataset, we identified the top 10 most relevant vectors for each query vector as described in the section IV. We then compared the resulting list of top 10 relevant vectors after Product quantization with the baseline list from the original floating-point vectors. The overlap ratio of the two lists, which is the accuracy of retrieval was less than 0.1. This suggests that Product Quantization loses its retrieval accuracy and is unable to identify the exact solution when searching for relevant vectors using cosine similarity.\nTo extend our comparison analysis with Product-quantization, we compared Pearson correlation coefficient between human-evaluated values and cosine similarity values from three Semantic textual similarity datasets such as sts-metb [28], str-2022 [29], and SICK [30]. These datasets have two pair of sentences and its semantic textual similarity or relatedness score, which was evaluated by humans. The size of each dataset is 8,628 for sts-metb, 5,500 for str-2022, and 9,840 for SICK.\nFirst of all, we split the each dataset into train (50%) and test (50%) dataset. Then, we embedded the pair of the sentences from each dataset using bge-large-en-v1.5 [31] embedding model. The dimension of the embedded vector is 1024. We quantized the embedded vectors of test dataset based on quantized train dataset using Product-quantization algorithm. Then, we evaluated the cosine similarity value between the pair of quantized vectors. Finally, we computed the correlation coefficient between the measured cosine similarity values and the semantic relatedness scores."}, {"title": "VI. LIMITATION", "content": "Converting embedding vectors from higher precision formats, such as 32-bit floating point, to lower precision formats like 8-bit or 4-bit integers, can lead to significant increases in searching speed. The key reason for this is that using fewer bits to represent data simplifies calculations and accelerates processing on hardware. However, to fully realize these speed gains, dedicated hardware instructions are necessary to support calculations with lower precision integer data. Currently, many popular frameworks, including PyTorch, only support the INT8 data type, but do not yet support INT4. To confirm the expected performance benefits, it would be essential to measure the actual impact on searching speed, which was not done in this work."}, {"title": "VII. CONCLUSION", "content": "Recently, RAG has emerged as a promising solution to the limitations of LLMs, particularly the issues of outdated information and hallucinations. RAG uses large-scale similarity search to retrieve relevant information from extensive databases of images, audio, video, and text. The key process of RAG pipeline is storing information as high-dimensional embedding vectors, which require substantial memory. This poses a challenge for deploying RAG models on devices with limited memory, such as mobile phones.\nTo address this challenge, we utilized quantization techniques to lower the precision of these high-dimensional vectors. This quantization can reduce the memory demands of the database by a factor of four or eight. Our findings indicate that 8-bit quantization maintains retrieval accuracy with only slight degradation. Group-wise quantization can alleviate some of the accuracy loss encountered with 4-bit quantization. Furthermore, the improved memory efficiency of 4-bit quantization allows for the use of larger vector databases, which could potentially produce more accurate and relevant search results."}]}