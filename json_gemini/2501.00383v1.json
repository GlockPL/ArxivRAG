{"title": "Proactive Conversational Agents with Inner Thoughts", "authors": ["Xingyu Bruce Liu", "Shitao Fang", "Weiyan Shi", "Chien-Sheng Wu", "Takeo Igarashi", "Xiang 'Anthony' Chen"], "abstract": "One of the long-standing aspirations in conversational AI is to allow them to autonomously take initiatives in conversations, i.e., being proactive. This is especially challenging for multi-party conversa- tions. Prior NLP research focused mainly on predicting the next speaker from contexts like preceding conversations. In this paper, we demonstrate the limitations of such methods and rethink what it means for AI to be proactive in multi-party, human-Al conversa- tions. We propose that just like humans, rather than merely reacting to turn-taking cues, a proactive Al formulates its own inner thoughts during a conversation, and seeks the right moment to contribute. Through a formative study with 24 participants and inspiration from linguistics and cognitive psychology, we introduce the Inner Thoughts framework. Our framework equips Al with a continuous, covert train of thoughts in parallel to the overt communication process, which enables it to proactively engage by modeling its intrinsic motivation to express these thoughts. We instantiated this framework into two real-time systems: an AI playground web app and a chatbot. Through a technical evaluation and user studies with human participants, our framework significantly surpasses existing baselines on aspects like anthropomorphism, coherence, intelligence, and turn-taking appropriateness.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have demonstrated their ability to generate high-quality text in response to human input, finding application in areas ranging from Q&A systems to writing assistants. Yet, most current LLM-based systems treat Al as passive respondents, responding only to explicit human prompts. Imagine a scenario where people are planning a trip with an Al agent: they must constantly prompt the AI, which passively waits for instructions instead of actively contributing. On the other end of the spectrum, systems like GitHub Copilot\u00b9 tend to overcom-pensate, offering constant suggestions that can overwhelm users. Neither extreme Al that is only reactive nor AI that is always responding \u2013 is ideal.\nIn the context of conversations, a proactive Al agent should be able to autonomously participate in socially appropriate moments, providing relevant input without requiring explicit cues. This is par-ticularly challenging in multi-party conversations. Dyadic human-Al interactions (e.g., using Siri) often predict turn-taking based on speech features such as pause or stop words, and the next turn will be automatically allocated to the other party [14, 54]. However, in multi-party settings, these cues could be ambiguous, and multiple possible speakers may take the floor. Repeatedly prompting AI during group interactions can also become cumbersome and can disrupt the natural flow of the conversation, as illustrated in the example of trip planning.\nPrevious systems typically first predict the next speaker (i.e., turn-taking prediction) and then generate the next response based on conversational and contextual information. For instance, some approaches rely on the last few turns of conversations to predict the subsequent speaker [15, 20, 62], while others utilize multimodal cues, such as eye gaze and non-verbal signals [7-9]. Despite these efforts, on turn-taking prediction, they still fall short and struggle to beat the simple \"repeat last\" baseline strategy in social conversation contexts [15, 62]. Our formative evaluation (Table 1) also shows that when it comes to predicting the next speaker, fine-tuned LLMs perform no better than random guessing unless the next speaker is allocated (e.g., \"What do you think, Alice?\"). In addition, after determining the next speaker, existing works tend to use predefined speaker personas [67, 70] as additional input to guide response generation, or expand persona with commonsense [32]. However,"}, {"title": "2 RELATED WORK", "content": "Our work builds on previous research in proactive conversational agents, turn-takings in multi-party conversations, and thought-augmented LLMs."}, {"title": "2.1 Proactive AI and Conversational Agents", "content": "Proactive AI dates back to earlier work on mixed-initiative interaction [1, 29]. In contrast to Al that only passively responds to human queries, mixed-initiative interaction envisions agents that autonomously understand when to take what action, such as the LookOut system [29] that automatically identifies related dates and events in emails and then proactively suggests them to users as calendar events. In 1996, Rhodes et al. [47] introduced one of the pioneering systems to continuously supply relevant information through observation of human activities. Andolina et al. [3] de-veloped SearchBot, which offers ongoing suggestions of related documents and entities unobtrusively [2] during voice interactions.\nWhile proactivity is a recurring theme in conversational AI re-search, most proactive conversational Als focus on task-oriented contexts [23, 36], with the aim of helping users achieve specific objectives. Social conversations, which can expand on open topics without having any goal to complete, are rarely addressed. In addition, past research tends to focus on generating proactive text responses to help lead and guide the conversation [16], for example, the ability of \"learning to ask\" [6, 15, 46, 60], understanding and initiating topic shifts [38, 56, 66], and planning future conversa-tion [33, 41, 58] etc.\nIn this paper, we focus on investigating how to enable AI to proactively engage in multi-party conversations: how AI can deter-mine the appropriate moments to speak and what contributions to make. We also choose to investigate social conversations where unlike task-oriented dialogue, the objectives are often ambiguous, and the actions required from the AI are not clearly defined."}, {"title": "2.2 Turn-takings in Multi-Party Conversations", "content": "For a conversational agent to engage proactively, it must understand and manage turn-taking, deciding who should speak at the end of each turn. Modeling turn-taking is still an area of active research. Existing approaches often employ an explicit mechanisms, such as a \"send\" button [64], push-to-talk [27, 59], and wake-words (e.g., \"Hey Siri\") [34]. However, the use of explicit cues can be viewed as less conversational from users' perspectives [65]. Mainstream conversational Al systems also use silence to detect the end of a user's turn. However, studies show pauses within turns are typically longer than gaps between turns in human conversations [11, 57], making silence an unreliable cue for turn-taking. More importantly, this method does not generalize to multi-party conversations. In dyadic interaction, it is always clear who is supposed to speak next when the turn is yielded [54]. In the multi-party case, this becomes more ambiguous since there is more than one potential speaker who might take the turn.\nBeyond an explicit mechanism, machine learning researchers have proposed data-driven methods to manage turn-taking in these conversations, primarily leveraging conversation history to predict the next speaker (i.e., the next-speaker prediction task) [15, 20, 62]. However, these methods have shown limited success. Notably, they have often failed to outperform the simple \"repeat last\u201d baseline strategy in social conversation contexts [15, 62]. In addition to using only textual data, research in HCI and HRI have leveraged other contextual, non-verbal information and \"turn-taking cues\", for instance, eye gaze (e.g., looking at addressee) [44, 45], breathing (e.g., breathe in and out) [31, 40], prosody (e.g., rising or falling of pitch) [17, 18, 22, 39] and the status of the human user (e.g., passing by, stopping) [7-9] to decide if an AI should engage at a certain moment of the conversation or not.\nPrevious approaches on mediating turn-taking often relied on conversation history and contextual information, and typically treat the AI as a reactive agent. Inspired by human behavior, our Inner Thoughts framework takes a different perspective by modeling intrinsic motivation to speak."}, {"title": "2.3 Language, Thought, and LLM Agents", "content": "Recent advances in large language models (LLMs) have incorporated intermediate reasoning steps to enhance performance in com-plex tasks, such as Chain-of-Thought (CoT) prompting [63] whereby LLMs think step-by-step to effectively break down larger problems into reasoning steps, and Tree of Thoughts (ToT) [68] whereby LLMs explore multiple possibilities at each reasoning stage. In addition, self-reflection mechanisms can iteratively improve the model's reasoning. ReAct [69], for example, synergizes reasoning with action-taking by having the model alternate between generat-ing reasoning traces and performing task-specific actions. Reflex-ion [53] builds on this by equipping models with dynamic memory and self-criticism capabilities, allowing them to refine future ac-tions based on past performance. Expanding on this, Generative Agents [43] simulate human-like behavior by combining memory, planning, and reflection. The recent OpenAI's o1 preview [42] intro-duces another perspective on reasoning transparency by explicitly surfacing intermediate reasoning steps to make the Al's decision-making process more interpretable to users.\nThe Inner Thoughts framework we propose diverges from these approaches by simulating an ongoing, parallel stream of internal thoughts that mirror human covert responses. Unlike methods such as CoT, TOT, or OpenAI o1 preview, which emphasize externalizing intermediate steps for reasoning tasks, Inner Thoughts explore leveraging these covert thoughts to equip Als with the ability to self-initiate actions and engage proactively."}, {"title": "3 NEXT-SPEAKER PREDICTION IS INSUFFICIENT TO ENABLE PROACTIVE CONVERSATIONAL AI", "content": "In this section, we investigate the limitations of the commonly used \"next-speaker prediction\" strategy [15, 20, 62], and further motivate the need of Inner Thoughts to enable proactive Al engagement in multi-party conversations. While next-speaker prediction perform well when explicit turn-allocation cues are present, we demonstrate that they fall short in self-selection cases, where turn-taking deci-sions are mostly spontaneous and influenced by covert, intrinsic factors of the conversational parties rather than observable con-textual cues. Building on Sacks et al. 's Simplest Systematics [51], turn-taking in conversations is governed by a set of rules:\n(1) Turn-allocation: The current speaker may select the next, often using cues like gaze or address terms (e.g., \"What about you, Alice?\").\n(2) Self-selection: If the current speaker does not select a next speaker (e.g., \"I went to Disneyland last weekend.\"), then any party can self-select to take the floor. The first to start gains the turn.\n(3) If no other party self-selects, the current speaker may con-tinue.\nOur intuition is that decisions to self-select and participate are largely influenced by covert internal processes such as a partic-ipant's interest, relevance, or motivation to engage which are not easily observable from explicit conversational data. Thus, we argue that training machine learning models on next-speaker pre-diction tasks based on conversation history is inherently ill-suited for self-selection scenarios, because there is no deterministic map-ping between prior utterances and the next speaker. To further verify our hypotheses, we evaluate the performance of several Gen-erative Pre-trained Transformer (GPT) variants in predicting the next speaker in multi-party conversations in both turn-allocation and self-selection scenarios."}, {"title": "3.1 Hypotheses", "content": "We hypothesized that GPT would perform well in turn-allocation scenarios, as these are often signaled by explicit language patterns. However, we anticipated lower accuracy in predicting the next speaker in self-selection scenarios, as these decisions are likely influenced by participants' intrinsic motivations, which are not directly observable from conversational context.\nWe further expected that fine-tuned models would underperform, especially in self-selection scenarios, as fine-tuning on datasets with high variability in self-selection decisions could introduce noise or misleading patterns."}, {"title": "3.2 Materials & Methods", "content": "We used the Multiparty Chat Corpus (MPC) [52], a dataset designed to capture social dynamics in multi-party conversations. The dataset includes chat logs from sessions that began as free-flowing and be-came increasingly structured over time. A key feature of the MPC dataset is the communicative links annotation link_to, which iden-tifies whether each utterance was addressed to a specific participant. For our analysis, turn-allocation refers to utterances addressed to a specific individual, while self-selection refers to instances open to all participants (all_users in MPC).\nThe MPC dataset reveals a significant imbalance between these two turn-taking strategies. Out of the total utterances, 95% were self-selection, while only about 5% were instances of turn-allocation. Baseline accuracy for predicting the next speaker in this context was approximately 12.7% (average of all conversations)."}, {"title": "3.3 Results", "content": "Our results (Table 1) demonstrate that all models performed better in turn-allocation scenarios, with consistently higher accuracy, and GPT3.5 performs significantly worse than GPT4 models. In contrast, performance in self-selection scenarios hovered around random chance, supporting the hypothesis that self-selection might be in-fluenced by internal factors that are not easily inferable from the conversational context alone. GPT-4 with CoT reasoning improved predictions in self-selection scenarios but still significantly worse than turn-allocation predictions. As expected, fine-tuned models introduced overfitting particularly in self-selection cases, where the model may have learned patterns of the next speaker that are not truly generalizable.\nThese findings suggest that context from previous utterances and speaker information is insufficient for accurately predicting the next speaker or determining who should proactively engage, especially in self-selection scenarios."}, {"title": "3.4 New Task: Speaker Name or \u201cAnyone\"", "content": "Given the inherent challenges of predicting specific next speakers in self-selection scenarios as discussed earlier, we additionally ex-perimented modifying the labeling schema to better align with the turn-taking mechanisms in multi-party conversations. In this task, next speakers in turn-allocation scenarios remain labeled with their respective names, while self-selection scenarios are relabeled as \"anyone\" (model #6). This labeling schema results in significant per-formance improvements in both turn-allocation and self-selection scenarios (Table 1). Specifically, fine-tuned GPT-3.5 model with this modification achieves an accuracy increase in turn-allocation cases, from 37.8% to 76.5%. This shows that removing the requirement to predict specific speakers in self-selection scenarios eliminates a major source of noise and unpredictability, and enhances model performance across the board."}, {"title": "4 RETROSPECTIVE THINK-ALOUD STUDY", "content": "Findings from section 3 show that predicting next speakers in self-selection scenarios requires more than analyzing previous utter-ances\u2014it hinges on understanding the intrinsic motivations of participants. We are motivated to introduce the concept of inner thoughts for agents and investigate what factors contribute to one's intrinsic motivation to participate. If we are to design a proactive agent system, how should we model its intrinsic decision-making process? Given the agent's thoughts, what factors beyond prior utterances influence its decision to participate? To answer these questions, we conducted a retrospective think-aloud study [26] to observe how human participants decide whether to engage in a multi-party conversation. Specifically, what factors influence their choice to express or withhold a thought, particularly when the oppor-tunity to speak is open to all?"}, {"title": "4.1 Participants", "content": "We recruited 24 participants (10 female, 14 male) from our institu-tion in groups of three. Before the study, participants completed the Big-5 personality test [49] and rated their familiarity with one another on a Likert scale (1-7). Participants reported varied levels of extroversion (Max: 97, Min: 2, Avg.: 50.0, SD: 32.3), and most were relatively familiar with one another (Avg.: 5.76, SD: 1.02)."}, {"title": "4.2 Procedure", "content": "Each group engaged in four 10-minute synchronous text conversa-tions on Slack. The conversations covered four topics (trip planning, casual chat, friendly debate, and brainstorming), and participants were free to direct the conversation as they wanted. After each conversation, participants reviewed the discussion utterance-by-utterance, reflecting on their thoughts at moments when they con-sidered contributing or chose to remain silent. We prompted them with the following questions: (1) What: What were you thinking? Did you want to say it? (2) Why: Why did you feel the need to say or not say it? (3) When: Did you decide to jump in immediately, wait for a pause, or wait for a particular statement?\nAfter the think-aloud sessions, we conducted semi-structured interviews to further reflect on instances when participants felt strongly about contributing or chose to remain silent despite having thoughts to share. Each participant was compensated 14 USD in local currency."}, {"title": "4.3 Findings", "content": "Two researchers collaboratively analyzed participants' responses using the affinity diagram method [28]. We held eight 90-minute coding sessions. In each session, we split the transcripts and re-viewed the data together to identify meaningful quotes. For each new quote, we proposed potential groupings into existing clusters or created new clusters through discussion. Agreement between the two researchers was required before assigning a quote to a clus-ter. In cases where consensus could not be reached, the quote was temporarily set aside and revisited during subsequent iterations. Once the initial clusters were established, we labeled each cluster with a theme. Conflicts in grouping or interpretation were resolved through discussion in the context of the original data. We in total derived 10 high-level themes, 23 mid-level themes, and 68 low-level themes derived from 394 quotes (Figure 2). The complete codebook is available in the Supplementary Material."}, {"title": "4.3.1 What Thoughts Do People Formulate?", "content": "Consistent with dual-processing theory [21], participants reported two types of thoughts. System 1 thinking is fast, automatic, and intuitive, often leading to immediate responses. In contrast, System 2 thinking is slower more deliberate. Participants indicated they use both modes-sometimes responding spontaneously (System 1), while at other times reflect-ing more deeply before engaging (System 2)."}, {"title": "4.3.2 Why do people express or withhold a thought?", "content": "We summarize eight heuristics to determine whether a participant wants to express or withhold a thought (Figure 2), and collectively name them the intrinsic motivations for participants to engage in conversations. Among the most frequently mentioned motivations, relevance (77 mentions) emerged as a dominant factor. Participants were more inclined to contribute when topics aligned with their knowledge, interests, or past experiences, resonated with prior long-term mem-ories, or built on their recent thoughts. In contrast, participants often withheld their input when they perceived a disconnect from the ongoing discussion. The role of relevance in conversational en-gagement aligns with Grice's Cooperative Principle and the maxim of relevance [25]. Similarly, Duncan and Fiske observed that con-versational contributions depend on aligning with shared context and ongoing topics [17].\nThe presence of an information gap (33 mentions) also strongly motivated expression. Participants spoke up when they identified missing knowledge, confusion, or the need for clarification. Ad-dressing these gaps often enriched the conversation with additional details or counterpoints. Conversely, participants withheld their thoughts when they deemed the discussion predictable or unengag-ing. While our finding is in group conversation settings, this draws parallels with Berlyne's theory of epistemic curiosity, which de-scribes how individuals seek information to resolve uncertainty [5].\nThe expected impact (23 mentions) of a thought further influ-enced engagement. Participants were more likely to contribute if they anticipated that their input would introduce novel topics, steer the conversation, or enhance its depth. They hesitated when they believed their thought would be redundant or covered later."}, {"title": "4.3.3 Levels of intrinsic motivation.", "content": "We also propose five levels of intrinsic motivation, i.e., how strongly and likely one would want to express a particular thought and participate in the conversation.\n\u2022 Very Low: The participant is unlikely to express the thought and participate in the conversation at this moment. They would not express it even if there is a long pause or an invitation to speak.\n\u2022 Low: The participant is somewhat unlikely to express the thought and participate in the conversation at this moment. They would only consider speaking if there is a long silence and no one else seems to be taking the turn.\n\u2022 Neutral: The participant is neutral about expressing the thought and participating in the conversation at this moment. They are fine with either expressing the thought or staying silent and letting others speak.\n\u2022 High: The participant is somewhat likely to express the thought and participate in the conversation at this moment. They have a strong desire to participate immediately after the current speaker finishes their turn.\n\u2022 Very High: The participant is very likely to express the thought and participate in the conversation at this moment. They will even interrupt others who are speaking to do so.\nThe five levels of intrinsic motivation serve as output labels for predicting intrinsic motivation in our framework."}, {"title": "5 INNER THOUGHTS FRAMEWORK", "content": "Motivated by our formative studies, we introduce a computational framework, Inner Thoughts, that enables AI proactivity by con-tinuously generating a train of thoughts alongside the ongoing conversation and autonomously deciding when and how to engage."}, {"title": "5.1 Trigger", "content": "In human conversations, thoughts often arise in response to specific triggers. Our Inner Thoughts framework mirrors this process by treating conversational events as triggers that initiate Al's internal thought generation. A trigger can take many forms - such as a new utterance, a pause in conversation, a non-verbal cue, or even a keyword embedded within a participant's speech. Any of these events can stimulate the AI to initiate a new thought process and generate a new batch of thoughts.\nIn the implementation of our system for online text-based con-versations, we defined two types of triggers. (1) on_new_message: This trigger is activated whenever one of the participants sends a message. Each incoming message prompts Al to generate a new set of thoughts in response to the latest input. (2) on_pause: The second type of trigger occurs when no participant has spoken for a period of time (set to 10 seconds in our system). This allows the AI to generate thoughts during moments of silence, potentially facilitating the interaction by proposing new topics or re-engaging participants. For instance, in our experiment, we observed AI gener-ating thoughts like: \"It has been ten seconds and no one has spoken perhaps I should suggest a new topic?\"."}, {"title": "5.2 Retrieval", "content": "Once triggered, AI retrieves information from its memories to use as the stimuli to form thoughts. From our think-aloud study, par-ticipants mentioned that this could involve long-term memory of related personal experiences, objectives, knowledge, or interest, as well as working memory for details from the ongoing conversation, or even previous thoughts they had. Random memories can also be retrieved to simulate the process of being \"creative\".\nWe retrieve relevant memories by computing their saliency with respect to the latest utterance. Memories with saliency higher than a threshold (0.3) will be selected. Let x represent a memory item (e.g., an objective, knowledge, or thought) and u represent the latest conversational utterance. The saliency of a memory x is determined by the maximum similarity between the memory and both the raw text of the utterance u and its interpretation $u_{interp}$. Specifically:\n$Saliency(x, u) = max(sim(x, u_{interp}), sim(x, u)) \\qquad  w_x d_x$\nwhere sim(a, b) is the cosine similarity between two embeddings, $w_x$ is the weight of the memory x that can be predefined by users and reflects its inherent importance, and $d_x$ is a decay factor that reduces the saliency of older memories. The decay factor $d_x$ is defined as: $d_x = \\lambda^{(t - t_x)}$, where $\\lambda$ is the decay rate (0.95), t is the current timestep, and $t_x$ is the last time or batch when the memory x was accessed. This formulation ensures that more relevant and"}, {"title": "5.3 Thought Formation", "content": "Our framework employs a dual-process model [21] of thought for-mation, based on our think-aloud study findings (section 4). This process involves two systems: system 1 for quick, automatic re-sponses, and system 2 for deliberate, contextually-rich thinking. Users can configure how many system 1 and/or system 2 thoughts should be generated for each trigger in one batch.\nFor system 1, we prompt the LLM to form a succinct thought based on the last utterances in the conversation, such as acknowledgments or expressions of interest. For system 2, we prompt the LLM to generate thoughts based on the retrieved stimuli. Below is a short version of the prompt structure (full prompt in Supplementary Material):\n\"You are provided contexts including the conversation history and salient memories of yourself... Form thought(s) that you would most likely to have at this point in the conversation, given the context. Make sure they are diverse, align with these contexts and are less than 15 words.\"\nWe also prompt the LLM to annotate the stimuli (from a previous thought, utterance or long-term memory) for each thought it gener-ates (as shown in Figure 6). This provides a traceable link between the Al's memories and thoughts and make the generation process more grounded.\nSimilar to what was observed in reasoning and decision-making tasks [43, 53, 63, 68, 69], we empirically found that LLMs can form reasonable and consistent thoughts based on the stimuli and con-versational context. For example: \"I should mention the picnic we had last weekend\", \"I wonder how long Bob's hike was\", \"Seeing a bear up close must have been intense!\"."}, {"title": "5.4 Thought Evaluation", "content": "Not all generated thoughts will be expressed. In this thought evalu-ation phase, the Al censors its latest batch of generated thoughts and decides whether or not to express a particular thought.\nWe use a structured evaluation process. This evaluation is driven by heuristics we developed in section 4: Relevance, Information Gap, Expected Impact, Urgency, Coherence, Originality, Balance and Dynamics (Figure 2). Our implementation employs LLMs to evaluate each thought on the set of heuristics and assigns a rating (1-5) to determine the likelihood of the thought being expressed. We also provide definitions of the scores based on the five levels of intrinsic motivation we proposed in section 4, from very low to very high. This makes the LLM's prediction grounded and explainable, and can be further used to guide participation strategies."}, {"title": "5.5 Participation", "content": "After evaluating the intrinsic motivation score of its latest batch of thoughts, the AI decides whether to speak by leveraging turn-taking type predictions (i.e., turn allocation vs. self-selection), combined with the evaluation of those thoughts. The Inner Thoughts frame-work allows the AI to exhibit varying degrees of proactive partic-ipation through adjustable proactivity settings. We define three layers of proactivity that control how and when the AI participates in the conversation:\nOvert proactivity, which refers to the Al's overall tendency to engage in conversation, similar to how some people naturally partic-ipate more actively in discussions, regardless of specific thoughts or ideas. To implement overt proactivity, we adjust the system1Prob (System 1 Probability, 0-1) parameter, which controls the proba-bility to select a system 1 thought when no system 2 thought is selected. A higher system1Prob increases the chance that the AI will respond in general even when other thoughts are rated to have low motivation.\nCovert proactivity, which is the level of motivation required for the AI to express a thought and engage. This is managed through imThreshold (1-5), the intrinsic motivation threshold for express-ing a thought. A thought may only be selected if it is evaluation score is higher than this threshold.\nTonal proactivity, which shapes how assertive or forward the AI appears in its language. The proactiveTone (true or false) controls the Al's style of expression once it has decided to speak. While the core thought-selection process is the same, the proactive tone modulates how assertively the Al conveys its message by restyling the articulated utterance through an LLM.\nIn addition, Inner Thoughts introduces the concept of interrup-tion, represented by the interruptThreshold (1-5). Interruption occurs when the AI takes a turn despite the turn being allocated to another participant. For example, this might occur when Alice asks Bob, \"How about you, Bob?\" but the AI interjects because it has an urgent thought to express. Interruption is not explicitly outlined in Sacks et al. 's Simplest Systematics [51] but is framed here as a mechanism to override the orderly system of turn-taking when necessary. If the intrinsic motivation behind a thought exceeds the interruptThreshold, the AI will override standard turn allocation rules to contribute to the conversation. This provides an additional layer of proactive engagement.\nThe AI decides whether to speak by leveraging turn-taking type predictions (i.e., turn-allocation vs. self-selection) combined with the thought evaluation process. For open turns (self-selection), the Al speaks if its top thought surpasses the intrinsic motivation thresh-old; otherwise, it may rely on system-1 thoughts or remain silent. For allocated turns, the AI selects its highest-rated thought to speak, and for others' turns, it interrupts only if its motivation exceeds the interrupt threshold. This algorithm is formally described as:"}, {"title": "5.6 Demonstration of Al's Proactive Behavior Enabled by Inner Thoughts", "content": "We present several examples selected from simulation logs of AI turn-taking behaviors enabled by the Inner Thoughts framework (Figure 5)."}, {"title": "5.6.1 Participation by Motivation.", "content": "In the Inner Thoughts frame-work, Al participation is driven by its intrinsic motivation, as op-posed to traditional approaches that rely on conversation history. Previous systems might randomly select participants with minimal interest or knowledge in the topic at hand, potentially stagnating the conversation. In contrast, Inner Thoughts ensures that the AI participates with the strongest motivation \u2013 whether due to a rele-vant persona, curiosity, or the fact that they have not spoken in a while - takes the conversational floor. This dynamic leads to more fluid and engaging topic progression, as participants with some-thing meaningful to contribute are naturally more involved. Over time, this accumulation of motivated contributions may develop conversations that are more coherent, engaging, and reflective of the natural flow of human interaction, as shown in our evaluation in section 6.\nFor instance, as shown in Figure 5, the AI demonstrates motivation-based participation when a user mentions trying yoga for the first time. With its knowledge of yoga and background as a yoga in-structor, the AI promptly responds: \u201cI love yoga too! Actually, I'm a yoga instructor!\u201d The Al's motivation to share relevant personal experience ensures a smooth continuation of the conversation."}, {"title": "5.6.2 Interruption.", "content": "The Inner Thoughts framework enables AI to interrupt a conversation when it has a strong motivation to con-tribute. Even when participants A and B are discussing a particular topic, participant C (the AI) can step in if it identifies a strong, relatable connection to the conversation. This behavior makes con-versations more dynamic and allows the AI to share important insights without needing to wait for a turn. In contrast, methods solely dependent on next-speaker prediction often fail to offer the Al opportunities to engage if the conversation converges around two participants.\nAs shown in the figure example, while A and B are in the middle of a dialogue, the AI interrupts with, \"No way! Middlesex is one of my favorite books!\" This interruption enriches the conversation by fostering more spontaneous interactions."}, {"title": "5.6.3 Retention.", "content": "In addition to its ability to interrupt, the Inner Thoughts framework also allows the AI to retain thoughts for future use, waiting for an appropriate moment to express them. This feature enables the AI to revisit previously generated thoughts that may have been irrelevant at the time but later become pertinent as the conversation progresses.\nFor example, in the figure, the AI initially holds back a thought about going for a run earlier that day because it was not particularly relevant while other participants were discussing a different topic. However, once the conversation shifts to the weather and outdoor activities, the Al sees an opportunity to contribute: \u201cI should mention my morning run in the nice weather.\u201d"}, {"title": "5.6.4 Thought Evolution:", "content": "The Inner Thoughts framework allows for the development and evolution of thoughts over time. Unlike traditional systems that generate responses based on a fixed per-sona [67, 70], Inner Thoughts enables the AI to develop and adapt its thoughts as the conversation unfolds, incorporating multiple stimuli along the way.\nFor instance, as shown in the figure, the AI initially recalls a mem-ory of writing songs last weekend. As the conversation shifts toward music and instruments, this memory evolves into the thought: \"I wonder if Daisy writes songs too.\" It expresses the thought by asking Daisy the that question. With a positive answer from Daisy, the thought further evolves into: \"I should share that I also wrote a song last weekend.\" This continuous evolution allows the AI to stay rele-vant and responsive as new topics emerge, as well as compound and develop new thoughts."}, {"title": "6 SIMULATIVE EVALUATION", "content": "We conducted a technical evaluation via multi-agent simulations to compare different strategies in enabling proactive AI engagement in multi-party conversations. We chose a simulative approach to overcome weaknesses of only relying on conventional user stud-ies with human subjects. First, the difficulty to scale due to the time cost of coordinating human participation. Second, we found through our pilot studies that human participants may struggle to perceive the timing of AI engagement in social conversations, focusing more on the style and content of responses. In addition, since many forms of engagement may seem reasonable in social conversations, participants often do not have clear criteria to assess Al's engagement behavior.\nTaking a non-conventional approach, our intuition is that simu-lating conversations at-scale amongst multiple Als using the same engagement strategy allow us to accumulate and magnify the effects of both correct and incorrect turn-taking decisions. In particular, poor decisions about when to engage can compound and lead to noticeable degradation in conversation quality, making evaluation more straightforward. This method also offers scalability, as crowd-workers can assess conversation quality without the need for real human interactions with AI.\nIn this section, we compare the performance of our proposed Inner Thoughts framework with the conventional next-speaker prediction baseline in multi-agent simulations."}, {"title": "6.1 Apparatus: the Inner Thoughts Playground", "content": "We built an Inner Thoughts playground (Figure 6) that allows us to simulate conversations between AI and/or human participants. This playground is deployed at https://liubruce.me/inner_thoughts/. On the playground", "panes": "On the left is the long-term memory pane. Users can customize each AI partici-pants's long-term memory by adding or deleting specific entries. In the middle is the conversation pane, where users can watch the simulated conversation, or participate in the conversation by sending a message using the dialog box on the bottom.\nOn the right is the inner thoughts pane. Users can view visu-alization of thought bubbles generated on-the-fly for the selected participant as the conversation proceeds. Each thought bubble con-tains a numeric ID ("}]}