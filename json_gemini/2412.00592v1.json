{"title": "Generative LiDAR Editing with Controllable Novel Object Layouts", "authors": ["Shing-Hei Ho", "Bao Thach", "Minghan Zhu"], "abstract": "We propose a framework to edit real-world Lidar scans with novel object layouts while preserving a realistic background environment. Compared to the synthetic data generation frameworks where Lidar point clouds are generated from scratch, our framework focuses on new scenario generation in a given background environment, and our method also provides labels for the generated data. This approach ensures the generated data remains relevant to the specific environment, aiding both the development and the evaluation of algorithms in real-world scenarios. Compared with novel view synthesis, our framework allows the creation of counterfactual scenarios with significant changes in the object layout and does not rely on multi-frame optimization. In our framework, the object removal and insertion are supported by generative background inpainting and object point cloud completion, and the entire pipeline is built upon spherical voxelization, which realizes the correct Lidar projective geometry by construction. Experiments show that our framework generates realistic Lidar scans with object layout changes and benefits the development of Lidar-based self-driving systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Data is a key factor in the realization of self-driving. Collecting a huge amount of data with good diversity is necessary for understanding the corner cases, which are low-frequency and high-risk real-world situations. Both the development and the testing of self-driving systems could benefit from data with good long-tail coverages. However, the low-frequency nature means that it is inherently expensive to collect corner-case data, and as progress is being made on relatively easier and more frequent cases, capturing even rarer events only becomes an even more challenging task. That's why the research community has a growing interest in data generation, which allows for acquiring new data at a lower cost and potentially higher diversity compared with real-world data collection.\nEarly efforts of data synthesis rely on gaming engines and graphical rendering. This approach generates data from highly configurable virtual environments, but the scalability is limited, and the domain gap to the real data is non-negligible. Recent progress in generative modeling makes end-to-end data generation possible, largely improving the scalability and realism. However, users generally have less control over the specifics of the generated data. Another notable trend is novel-view synthesis from real-world sequential observations, allowing the resimulation of a real-world scenario from different viewpoints, which helps analyze and improve the algorithms. However, the variation flexibility of the re-simulated data is limited by the visible signal.\nIn this paper, we propose to solve the task of scene editing with controllable novel object layouts, with a specific focus on Lidar scans a widely used and reliable sensory modality in autonomous vehicles. This new task enables us to freely remove and insert objects with arbitrary poses in a real Lidar scan, not limited by the visible information. On the other hand, the generated data is associated with a real environment, building a more direct connection to real-world performance. An illustration of this task is shown in Fig. 1. Leveraging recent progress in conditional lidar data generation and object point cloud completion, we build a lightweight framework without scene reconstruction, neural rendering, or end-to-end generation from scratch. We bake the prior knowledge of Lidar projective geometry into the framework through spherical voxelization so that the occlusion relationship and the scanning pattern are correct by construction. Experiments show that our framework enables flexible, controllable, and realistic Lidar scene editing, which is beneficial for both the development and testing of self-driving algorithms."}, {"title": "II. RELATED WORK", "content": "We will review the literature related to Lidar data generation. Most work did not directly address the same problem as targeted in this paper, but a lot of them are relevant and prepared the techniques that are useful in our problem.\n A. Lidar simulation\nLidar simulation refers to the approach of building a 3D environment with 3D object assets and casting rays on the 3D environment to create point clouds through ray-surface intersection [1]. While graphics-based simulation engines [2-4] facilitate the rendering, the requirement of pre-built 3D assets limits the scalability, and the rendering and the quality of the 3D assets both contribute to the domain gap compared with real data. [5] constructed the 3D assets from real-world scans to avoid manual asset creation and used learning-based postprocessing to reduce the domain gap. Later work [6] further investigated various Lidar phenomena that could improve the fidelity of Lidar simulators.\n B. Lidar novel-view synthesis\nThe novel-view synthesis (NVS) of Lidar point clouds and Lidar simulation share the same conceptual procedure of 3D construction and rendering, but today's Lidar NVS realizes these two steps in a unified learnable framework called neural rendering. The learned implicit geometry-appearance model and the differentiable rendering are both optimized towards reproducing more realistic synthesis, allowing a smaller domain gap and a more automatic workflow than traditional simulation [7]. [8] modeled Lidar effects like secondary returns and ray dropping in the neural rending framework. [9, 10] also leveraged image sequences for additional information. [11, 12] explicitly the temporal dimension to account for the moving objects. [13] rendered both Lidar scans and camera images from the neural field. These approaches realized the playback of a real-world scenario with changed viewpoints and object states, but the pose of the observer and the objects cannot deviate too much from reality, or the rendering quality will deteriorate.\n C. End-to-end Lidar data generation\nEnd-to-end approaches bypass the two-step procedure of 3D construction and rendering. Inspired by the success of text- and image-based generative models, researchers proposed to generate Lidar scans through a fully-differentiable neural network. This approach produces data with promising diversity and realism but has limited controllability. [14] applied a diffusion model on equirectangular range-view images, leveraging the progress in image diffusion models. [15] also used range images but compressed the data into a latent space before diffusion. It also supported conditioning like semantic maps and RGB images. [16] applied a generative transformer framework MaskGIT [17] in BEV-voxelized point cloud generation and allowed basic editing by swapping the discrete latent codes. Recent work started to generate temporally consistent sequences of Lidar scans, incorporating the idea of world modeling [18]. [19] took traffic maps as input conditions, improving the controllability of Lidar sequence generation. The end-to-end generated point clouds generally do not come with corresponding labels, limiting their value in practical development.\n D. Lidar scan modification\nGenerative methods can also be used to apply modifications on an existing point cloud. For example, [20] used a transformer to upsample a sparse point cloud. [21] applied scene completion on a single Lidar scan to obtain a dense and complete scene. [22] allows inserting the same object in an RGB image and in a Lidar point cloud in a consistent manner. Overall, the editing flexibility is limited.\nThe task in this paper, Lidar scan editing with novel object layout, is related to but different from all the tasks discussed above. Our output is connected to a real scene, similar to the NVS task, but we do not rely on reconstruction, which allows more flexibility in the object manipulation. The task is generative but does not build scenes from scratch, emphasizing the controllability of object layouts."}, {"title": "III. PROBLEM FORMULATION", "content": "We address the problem of generating realistic synthetic Lidar data for autonomous driving. Rather than generating data completely from scratch, our approach builds upon real-world scenes, and we aim to manipulate the dynamic objects while preserving the background environment.\nThe static background of a Lidar scene, denoted as $\\mathcal{S}$, consists of stationary elements such as streets, trees, buildings, and other fixed background. $\\mathcal{S}$ excludes any foreground dynamic objects of interest, for example, vehicles. The foreground objects in the scene are represented by $\\mathcal{D}$. A typical driving scene, denoted as $\\mathcal{O}$, includes both background and foreground objects: $\\mathcal{O} = \\mathcal{S} \\cup \\mathcal{D}$. Since it is not feasible to sense the entire $\\mathcal{O}$ from a single viewpoint, we define an observation function $f$, which transforms the scene into a partial-view Lidar point cloud: $f(\\mathcal{O}) = f(\\mathcal{S},\\mathcal{D}) = P$, which represents the real sensor output for an autonomous vehicle.\nThe task of Lidar generative editing can be formally defined as follows. We assume that we have a dataset of real-world Lidar scans $P = \\{P_i\\}$. Given a true Lidar scan $P_o = f(\\mathcal{S},\\mathcal{D}_o)$, the goal is to generate a synthetic point cloud $P_T = f(\\mathcal{S},\\mathcal{D}_T)$, where $\\mathcal{D}_o$ is the true foreground objects in the scan, and $\\mathcal{D}_T$ is a modified set of foreground objects. Here, $\\mathcal{D}_T$ may consist of the same objects as $\\mathcal{D}_o$ but with different poses, or it may contain entirely different objects. We assume that all possible objects in $\\mathcal{D}_T$ are observed in some $P_i \\in P$.\nWe tackle this problem by decomposing it into three sub-problems: estimating $f(\\mathcal{S})$, $\\mathcal{D}_i$, and $f(\\mathcal{S},\\mathcal{D}_T)$, corresponding to object removal, object point cloud completion, and object insertion. They are introduced in Sec. IV."}, {"title": "IV. METHOD", "content": "A. Overview\nOur scene editing framework is illustrated in Fig. 2. We first introduce the spherical voxel representation (Sec. IV-B), which is the foundation of our entire pipeline for the efficient modeling of the Lidar projective geometry. Then, we present the three components in our framework. The first involves removing unwanted objects from the Lidar scene (Sec. IV-C). Given an object to be removed, we identify the voxels occupied by the object's point cloud. We mask these voxels and all voxels occluded by them with an inpainting mask. We remove points in the masked voxels and use a generative point cloud inpainting model to fill the background.\nIn the second stage, we collect an object library composed of dense full-shape point clouds for every object in all Lidar scans (Sec. IV-D), so that we have a wide range of objects to be inserted back into the Lidar scans with flexible poses. We use the segmentation masks within each Lidar scene to extract partial-view point clouds of vehicles. These partial views are then transformed into full point clouds using a learning-based point cloud completion method.\nIn the third stage, we insert new objects into the Lidar point cloud (Sec. IV-E). We start by selecting an object from the full-view object library, and specifying a desired pose for its placement. The dense point cloud of this object is then positioned on the ground at the specified pose. The resampling of the inserted objects and the occlusion are then both handled in the spherical voxel representation.\nIt is worth noting that in our framework, moving an object to a different pose is treated as both removing and re-adding the same object. This approach helps resolve any gaps in the background and allows for significant pose changes.\n B. Spherical Voxelization\nThe spherical voxel representation is core to our entire pipeline for efficient occlusion handling and density control, as illustrated in Fig. 3. Spherical voxelization discretizes the space around the origin based on three parameters $(r, \\theta, \\phi)$: radius $r$, azimuth $\\theta$, and elevation $\\phi$. This approach closely mirrors how a LiDAR scan operates, where rays are emitted from the sensor (origin) at specific angles, allowing point resampling with a pattern consistent with real LiDAR sensors. Voxels sharing the same azimuth and elevation values resemble a single LiDAR ray, embedding the occlusion relationship into the data structure (see Fig. 3-right). While the range-view representation could also model the occlusion, the 2D projection loses the 3D structure. Preserving the 3D structure allows effective feature learning and convenient point cloud manipulation in our pipeline.\n C. Object removal\nThe object removal has two steps: identifying the area affected by the removal of an object and generating background points in this area that are consistent with the surrounding environment. We call these two steps de-occlusion masking and background inpainting, respectively.\n 1) De-occlusion masking: Given the spherical voxelization of the input, a de-occlusion mask refers to a binary mask over the voxels, where the voxels occupied or occluded by the object to be removed are labeled positive. The occupancy of the masked voxels needs to be predicted because of the scene change (object removed) and the visibility change (occlusion removed). We first mask the voxels occupied by the object point cloud. Then, all voxels sharing the same azimuth $\\theta$ and elevation $\\phi$ indices with larger radius $r$ indices than the occupied voxels are also masked (see Fig. 3-right).\n 2) Background inpainting: The background inpainting can be viewed as a conditional generation problem. We recast the UltraLidar framework [16] for this task. UltraLidar can be viewed as a Lidar version of MaskGIT [17], which is a generative model for images with transformer-based multi-step autoregression in a discrete latent space. The training has two stages, as depicted in Fig. 4. First, we train a VQ-VAE [23] to encode raw point clouds into a latent feature map with discrete latent codes. We use a bird's-eye-view (BEV) 2D latent map to represent the encoded point cloud. Notice that our BEV uses the azimuth-radius coordinate from the spherical voxelization, meaning that the voxels along the elevation axis are compressed to a single point in the latent feature map. It is slightly different from the physical top-down projection and also different from the Cartesian voxelization used in UltraLidar. We use the original point cloud scans to learn this encoder.\nThe second step is to learn a generative model in the latent space for background inpainting. Similar to MaskGIT [17], a bi-directional transformer is trained to map an incomplete BEV latent feature map to a complete latent feature map, which can be decoded to a point cloud. The feature prediction is cast as a classification problem because the feature space is discrete. The prediction is iterative. We use a mask to keep track of which tokens in the BEV feature map need to be predicted. In every iteration, the transformer predicts the masked tokens from the known ones, but only a subset of the predicted tokens with the top classification confidence is preserved, and they will be treated as known tokens in the next iteration. The other tokens remain masked and need to be predicted in the following iterations until the feature map is complete. The BEV mask is initialized from the voxel mask that is used to remove points from the input point cloud. In training, we apply the voxel mask on random object-free voxels, so that the transformer is trained to predict only the background for the masked tokens. At inference, we use the de-occlusion voxel mask introduced in Sec. IV-C.1.\nWhile we follow the UltraLidar framework [16], the object removal in UltraLidar is realized by replacing the token at the object position with a background discrete feature, which needs to be manually specified and does not explicitly inpaint the areas occluded by the removed objects. Our object-removal module resolves both problems.\n D. Object library construction\nWe create a library of objects with full 3D shapes to be inserted at arbitrary poses in a Lidar scene. The objects are collected from all Lidar scans in our training data. We choose the point cloud as the shape representation of objects for two reasons. First, we can resample points from the shape conveniently in the spherical voxel representation and do not need expensive ray-casting on meshes or volumetric rendering on implicit fields. Second, point cloud completion techniques are well-established to facilitate full-shape generation. We applied a pretrained model of the state-of-the-art point cloud completion network, AnchorFormer [24], in our framework.\n E. Object insertion\nThe insertion process has two major steps. First, given the desired pose to insert an object, we determine the height to make sure the object is placed on the ground. Second, we resolve the occlusion caused by object insertion, and resample the point cloud on the spherical voxel grids to yield a pattern consistent with the Lidar scan.\n 1) Object placement: We assume that a user only specifies the x-y coordinate and the yaw angle of the desired insertion pose, and the insertion algorithm needs to figure out the rest of the degrees of freedom based on the existing background point cloud. We further assume zero pitch and roll angles as are commonly assumed in self-driving datasets [25, 26]. Therefore, we are left with height to be determined.\nTo achieve this, we use the segmentation mask to find the ground points. Given a desired object position $(x, y)$, we locate the nearest ground point in the x-y plane and align its $z$ value with that of the lowest point in the inserted object point cloud, allowing us to fully define the vehicle's pose in the 3D space. Finally, we place the completed full point cloud of the target object into the Lidar scan.\n 2) Resampling and resolving occlusion: The point cloud, in its current state, presents two critical problems. First, the dense inserted point cloud does not match the surrounding environment in terms of scan pattern and density. Second, the occlusion relationships are incorrect.\nThese two problems can be efficiently fixed with our spherical voxelization technique. We locate the voxels occupied by the point cloud of the inserted vehicle. Then, we pick the center of each occupied voxel as the resampled points for the shape. Now, the inserted object has a pattern and density that are consistent with the overall Lidar scan. In the end, we resolve the occlusion. For each $(\\theta, \\phi)$ coordinate with at least one occupied voxel, an occlusion mask is applied to all voxels with radius $r > r_{min}$, where $r_{min}$ is the smallest radius of the occupied voxels in the $(\\theta,\\phi)$ 1D ray. All the masked voxels are set to empty."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "This paper aims at a novel Lidar editing task that is different from the literature. Therefore, it is not straightforward to apply a well-established evaluation protocol and compare with existing work. We will separate the experiments into two major components, i.e., object removal and object insertion, so that we can validate the effectiveness of each step and show that the overall framework provides a practical solution for Lidar scene editing.\nWe conduct experiments on the nuScenes-LidarSeg [25] dataset, which is composed of 40,000 point clouds sampled from 1,000 driving scenes. The point clouds are collected by a 32-beam Velodyne HDL32E Lidar. We use the official training and validation splits. All results are trained on the Lidar scans in the training set and evaluated using scans in the validation set. We focus on the manipulation of vehicle-type objects, including cars, trucks, and buses. The specific use of the dataset in each experiment is explained in the regarding sections.\n A. Object removal and background inpainting\nThe goal of this experiment is to test that our approach can inpaint the background points when objects are removed. The tricky thing is that we do not have the ground truth background behind an object. Therefore, we create artificial occlusion masks over the background area in a Lidar scan and ask our model to recover the masked area. In this way, we have the ground truth background to facilitate evaluation.\nIn order to make the artificial masks similar to the ones caused by the occlusion of real objects, we create the masks from the bounding boxes of objects in nuScenes. This process is illustrated in Fig. 5. We calculate the average size of the 3D bounding boxes of all vehicles in nuScenes. Then, we put the bounding box at a fixed distance, with the heading orthogonal to the viewing direction, so that the box occludes the most azimuth range. We rotate the bounding box around the ego vehicle till the bounding box falls into an object-free sector of the Lidar scan to be masked. We pick the distance as 10 meters to create a substantial occlusion.\nWe evaluate the performance of our background inpainting using statistical metrics and perceptual metrics. For the statistical metrics, we leverage the Maximum-Mean Discrepancy (MMD) and the Jensen-Shannon Divergence (JSD). Similar to [16], we construct a 2D histogram along the ground plane and use the voxel occupancy instead of the number of points for the bin count. However, our evaluation has two differences from that in [16]. First, our 2D histogram is in the azimuth-radius coordinate. Second, we manually set the count for all bins that are not in the generated area to zero and re-normalize the histogram to sum to one, because we only generate the point cloud for a masked area instead of generating the whole point cloud from scratch. For the perceptual metrics, we use the Frechet Sparse Volume Distance (FSVD) and the Frechet Point-based Volume Distance (FPVD). Following [15], FSVD is evaluated using MinkowskiNet [27], and FPVD is evaluated using SPVCNN [28], both pretrained on nuScenes.\nWe are not aware of any previous work that fulfills the exact same task. Therefore, we compare our method to a naive inpainting baseline, which is to copy the neighboring object-free point cloud sector and paste and tile it in the masked area. The evaluation results are shown in Tab. I. Our method outperforms the baseline in all metrics, showing the effectiveness of our framework. Additionally, qualitative comparisons are shown in Fig. 6, where it is evident that our model inpaints the background based on the context, resulting in point clouds that appear natural and realistic.\n B. Object insertion\nThe second part of our framework is object insertion. It is designed to allow users to specify the number, type, and pose of the objects to be inserted. In Fig. 7, we show some qualitative examples of object insertion.\nHowever, the choice of object placement will also affect the quality of the generated data and matters if we want to evaluate the generated data quantitatively. Therefore, we design different object insertion strategies when we evaluate different aspects of the object insertion algorithm. Specifically, we have two experiments. The first reveals the domain gap between the generated data and the real data, and the second showcases the value of the generated data in improving downstream tasks like object detection.\n 1) Domain gap analysis: To rule out the impact of the choice of object placement on the realism of the generated data, we insert objects at the same pose as in the true Lidar scans. Specifically, we first remove all objects from the original dataset to obtain object-free Lidar scans. Then, we sample objects from the object library and insert them into the Lidar scan at the pose of the removed objects. In this way, we obtain a synthesized point cloud with the same pose layout of objects but with different objects. Note that while we do not change the pose layout, the pose of an inserted object is typically significantly different from the object's original pose, making it a challenging object insertion task.\nWe evaluate the quality of the generated point clouds by running an object detection network VoxelNext [29] pretrained on nuScenes. The results are in Tab. II. As expected, the network performs differently on the true nuScenes dataset compared to our generated dataset. However, the results are comparable, with categories both outperforming and underperforming, suggesting a small domain gap between the two datasets.\n 2) Value for downstream tasks: We also aim to demonstrate the practical value of our generated data in improving perception algorithms. To do this, we combine the real-world dataset with our generated dataset to fine-tune a detection network and evaluate its performance. In this experiment, we intentionally design our generated data to have different object layouts than the real data to introduce more variability into the training set. Specifically, we randomly perturb the target object poses by up to 2.5 meters and 45 degrees in yaw angle, while ensuring they remain in the ground area on the BEV map. The inserted objects are sampled from the object library. Tab. III shows that our generated data significantly improves the performance of the detection model. It shows that our synthetic data provides significant value in building more powerful algorithms."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we contribute a novel framework for LiDAR scene editing, generating realistic synthetic data with novel object layouts in a real-world environment. We leverage generative models to fill in the missing information about both the background and the objects, and adopt a spherical voxelization model to handle Lidar projection geometry efficiently. We demonstrate the effectiveness of our framework through experiments on a large-scale self-driving dataset, nuScenes, showing that the generated LiDAR point clouds closely resemble real-world data. We show incorporating these synthetic LiDAR scans as additional training data leads to significant improvements in object detection performance in real-world data. For future work, we plan to extend the object editing task beyond vehicle-type objects. We will also use novel view synthesis to build a flexible framework, where the novel-view information may come from temporarily continuous observations when available and from prior knowledge embedded in generative models otherwise."}]}