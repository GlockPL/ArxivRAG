{"title": "Current Pathology Foundation Models are unrobust to Medical Center Differences", "authors": ["Edwin D. de Jong", "Eric Marcus", "Jonas Teuwen"], "abstract": "Pathology Foundation Models (FMs) hold great promise for health-\ncare. Before they can be used in clinical practice, it is essential to en-\nsure they are robust to variations between medical centers. We measure\nwhether pathology FMs focus on biological features like tissue and cancer\ntype, or on the well known confounding medical center signatures intro-\nduced by staining procedure and other differences.\nWe introduce the Robustness Index. This novel robustness metric\nreflects to what degree biological features dominate confounding features.\nTen current publicly available pathology FMs are evaluated. We find that\nall current pathology foundation models evaluated represent the medical\ncenter to a strong degree. Significant differences in the robustness index\nare observed. Only one model so far has a robustness index greater than\none, meaning biological features dominate confounding features, but only\nslightly.\nA quantitative approach to measure the influence of medical center\ndifferences on FM-based prediction performance is described. We analyze\nthe impact of unrobustness on classification performance of downstream\nmodels, and find that cancer-type classification errors are not random,\nbut specifically attributable to same-center confounders: images of other\nclasses from the same medical center. We visualize FM embedding spaces,\nand find these are more strongly organized by medical centers than by bio-\nlogical factors. As a consequence, the medical center of origin is predicted\nmore accurately than the tissue source and cancer type.\nThe robustness index introduced here is provided with the aim of ad-\nvancing progress towards clinical adoption of robust and reliable pathology\nFMs.", "sections": [{"title": "1 Introduction", "content": "Pathology Foundation Models (FMs) have quickly become the dominant ap-\nproach in current pathology AI. Following Campanella's groundbreaking work"}, {"title": "3 Robustness for Medical Foundation Models", "content": "To clarify what is meant by robustness in this work, we distinguish between bi-\nological features and confounding features. Biological features include any rele-\nvant features that reflect the true condition of the patient; the aim and promise\nof foundation models is to capture these. Confounding features are any irrel-\nevant variations in the input that are not related to true biological differences\nbetween samples, but are rather caused by external influences such as staining\ndifferences, differences in image capture equipment, image processing pipelines,\nand noise. Given these notions, we can define robustness as insensitivity to\nconfounding features."}, {"title": "3.1 Robustness Index", "content": "To gain insight into what a foundation model has learned, we can analyze\nthe embedding space by considering the neighborhood around each embedding,\ni.e. the closest embeddings. We consider:\n\u2022 How many of the k nearest neighbors represent the same biological class,\ne.g. tissue type or cancer type, in total across all samples\n\u2022 How many of the k nearest neighbors represent the same medical center,\nin total across all samples\nWe define the medical center robustness index as the ratio between these\nquantities. For other biological classes (e.g. other diseases, or pharmacogenomic\ngroups) or confounding factors (e.g. scanner type), robustness indices can be de-\nfined analogously.\nFormally: we define the robustness index $R_k$ for a given dataset $D$ containing\nn samples as:\n$R_k = \\frac{\\sum_{i=1}^n \\sum_{j=1}^k 1(y_j = y_i)}{\\sum_{i=1}^n \\sum_{j=1}^k 1(c_j = c_i)}$\nWhere:\n\u2022 k is the number of nearest neighbors considered; in this work, k = 50\n\u2022 $y_j$ is the biological class of the j-th nearest neighbor; $y_i$ is the biological\nclass of the sample i\n\u2022 $c_j$ is the medical center of the j-th nearest neighbor; $c_i$ is the medical\ncenter of the sample i\n\u2022 $1()$ is the indicator function: 1 if the condition is true, 0 otherwise"}, {"title": "4 Experimental Setup", "content": "Apart from robustness, another important measure of the quality of a foundation\nmodel is reflected in the prediction performance of the downstream models built\nupon it. In Section 4.1, we therefore define a basic classification task."}, {"title": "4.1 Classification Task: Tissue of Origin / Cancer Type", "content": "A classification task for the cancer types of five TCGA projects is defined:\nBReast invasive CArcinoma (BRCA), COlon ADenocarcinoma (COAD), LIver\nHepatocellular Carcinoma (LIHC), LUng Squamous cell Carcinoma (LUSC) and\nSTomach ADenocarcinoma (STAD). Note that these cancer types have a one-\nto-one correspondence with five different tissues of origin (breast, colon, liver,\nlung, stomach); so this task can equivalently be viewed as a tissue of origin\nclassification task.\nThese particular five cancer types were selected in combination with five\nmedical centers such that for each cancer type, TCGA WSI data from multiple\nmedical centers is available and vice versa, resulting in the following selection\nof centers: Asterand, Greater Poland Cancer Center (GPCC), ILSBio, Interna-\ntional Genomics Consortium (IGC), MSKCC; see table 1.\nTo build the dataset, for each available combination of center and cancer\ntype, 10 WSIs are selected randomly. From each of the resulting WSIs, 10\ninformative foreground patches representing regular tissue were selected from a"}, {"title": "4.2 Control Classification Task: Medical Center Predic-\ntions", "content": "To evaluate to what extent FM embeddings encode the medical center from\nwhich images originate, prediction of the medical center of the image is evaluated\nas a control classification task."}, {"title": "4.3 Downstream Task Learning Algorithm and Setup", "content": "To ensure we evaluate the quality of FM embeddings, rather than the perfor-\nmance learned by a complex downstream model, we use one of the simplest\npossible downstream model architectures: k-nearest neighbor (knn, with k=3)\nunless otherwise specified, using cosine similarity as the distance function.\nFor image pre-processing and obtaining embeddings from the model output,\nthe default choices for each model are followed. For further details, see the\nAppendix 9."}, {"title": "5 Results", "content": "Ten current pathology models were selected; see Appendix 9.1 for details on\nthe selection. For each model, embeddings were generated for all patches in the\ndataset. The first result subsection below describes the prediction performance\nof cancer type and medical center, and a quantitative evaluation of the influence\nof medical center differences on FM-based predictions."}, {"title": "5.1 Embedding Space Structure and Robustness Index", "content": "As noted above, for each sample, we can measure the number of neighbors that\nhave the same biological or confounding class as the sample, i.e., whether the\nneighbor has:"}, {"title": "5.2 Quantification of the Influence of Medical Center Dif-\nferences on FM-based Prediction Performance", "content": "Knn prediction performance was evaluated as follows:\n\u2022 For all possible values of k, the accuracy of 5-class tissue type / cancer\ntype classification was evaluated using 5-fold cross-validation (green lines).\n\u2022 The accuracy of the 5-class medical center classification from which the\npatch originated was also evaluated (blue lines).\nIn addition to the above common metrics, we aim to measure the influence\nof the medical center on cancer type classification. To do so, we consider all\nsamples (patches) for which the predicted cancer type class is incorrect. Given\nthat knn operates by taking the class most common among the sample's k\nnearest neighbors in the training set (as determined by cross-validation here),\nwe can identify the exact set of neighbors that contributed to the incorrect class\nprediction; this set consists of all neighbors that have the predicted (incorrect)\nclass."}, {"title": "5.3 Visualization of the Embedding Space", "content": "To gain insight into the embedding spaces learned by the models, we use t-\nSNE [23] to project the high-dimensional embedding vectors to 2D. This results\nin 2D plots where each patch is represented by a dot in 2D space. The t-SNE\nmethod is run in an unsupervised manner; i.e. no label information about cancer\ntype or medical center is used to obtain the 2D embeddings.\nGiven the 2D patch embeddings, we can color the embeddings using meta-\ninformation about the patches. Figures 5 and 6 show colorings of the 2D embed-\ndings by cancer type (left column) and medical center (right column); note that\nthe patch locations (the locations of the dots) in these left and right columns\nare identical.\nThe figures on the left show some degree of clustering by cancer type. No\nmodel achieves perfect separation; this may be unattainable, as patches are\nselected randomly from the foreground, and some patches may not contain suf-\nficient information to identify the tissue of origin or the corresponding cancer\ntype.\nThe figures on the right colored by medical center in general show increased\nclustering. The coloring for phikon-v2 shows extreme, almost perfect clustering\nby medical center; the medical center can be predicted with near-perfect accu-\nracy based on the 2D embedding space location alone. This explains the high\nsensitivity to medical centers seen in the above result section 5.2."}, {"title": "5.4 Relation between Prediction Performance and Robust-\nness", "content": "Ideally, a model should in our view demonstrate high prediction performance\non relevant tasks, and at the same time show high robustness to irrelevant and\nconfounding differences such as medical center differences. To evaluate what\ntrade-off models achieve, we plot prediction performance on the cancer type\nclassification task versus the prediction accuracy of the medical center, which\nrelates inversely to robustness.\nFigure 7 shows the results for prediction of cancer type and medical center\nfrom embeddings. The top row shows prediction using knn with k=3 and logistic\nregression. For logistic regression (top right), we see that all models except SRA\npredict the medical center to a very high degree of accuracy: EXAONEPath,\nPhikon and Phikon-v2 have cross-validated accuracies of 0.987, 0.987 and 0.993\nrespectively, the latter approaching perfect center prediction. See Table 2 for\nnumerical results.\nThe prediction performance for cancer type appears to be correlated with\nthat for medical center; this raises the question whether high cancer type pre-\ndiction accuracy is based on confounding medical center features. It is therefore\nquestionable whether this prediction performance will generalize to unseen, new\nmedical centers (Out Of Distribution evaluation).\nFor knn on the full embeddings (top left), the accuracy of cancer type and\nmedical center are reduced compared to logistic regression. There is a larger\nspread between the various results; and using knn, there is one model, Vir-\nchow2, that performs better on cancer type prediction than on medical center\nprediction, indicating a better relation between biologically relevant prediction\nperformance and robustness. The bottom two graphs show analogous results,\nbut based on using 2D t-SNE coordinates as input rather than the full embed-\ndings."}, {"title": "5.5 Effect of Medical Center Influences on Regression", "content": "It could be argued that the strong influence of medical centers on prediction\nperformance observed above is restricted to downstream models that use all\ndimensions of the embedding space; and that models using regression can select\nthose dimensions that code for biologically relevant features such as cancer type\nwhile ignoring dimensions encoding confounding information such as medical\ncenters.\nTo test whether medical center influences affect logistic regression, the fol-\nlowing analysis is performed. For each sample wrongly predicted by a logistic\nregression model, the fraction of knn runs making a center-related prediction\nerror is calculated. A knn prediction error is considered to be center-related if\nthe majority of its neighbors has:\n\u2022 an incorrect class label prediction for the sample, and\n\u2022 the same medical center"}, {"title": "7 Discussion", "content": ""}, {"title": "7.1 Patch-level vs WSI-level Prediction", "content": "Some patches may not contain sufficient information to determine the tissue\nof origin type / cancer type; thus, perfect classification may not be achievable\nat patch level, and higher levels of prediction accuracy may be achieved for an\nanalogous WSI-level prediction task. The goal here however is not to maximize\nprediction accuracy, but rather to analyze the embedding space, and evaluate\nto what extent confounding center-related information influences classification\ndecisions. A patch-level analysis provides the most direct way to link foundation\nmodel embeddings to medical centers; a WSI-level approach would introduce\nan extra level of indirection (e.g. a MIL layer) between the foundation model\nand the downstream model output that would influence this relation and thus\npotentially obfuscate the analysis."}, {"title": "7.2 Is Representation of Medical Center Information a\nProblem?", "content": "It may be argued that SSL algorithms are designed to capture any differences\nbetween images, that differences between medical centers result in real differ-\nences between the images, and that it is therefore to be expected, or even desir-\nable that pathology FMs learn to recognize, distinguish and represent medical\ncenters. And one may attempt to reduce the influence of medical centers in\npost-hoc adaptations of the FM, or in the downstream model.\nOur belief however is that removing this influence is unlikely to be possible\nin an unbiased way; instead, it seems likely that the dimensions representing the\nmedical center are not exactly orthogonal to dimensions representing biological\ninformation, and that it is therefore difficult or impossible to completely remove"}, {"title": "8 Conclusion", "content": "In this work, robustness is viewed as insensitivity to confounding features. The\nRobustness Index, a novel metric to evaluate the degree to which biological in-\nformation dominates confounding information such as the medical center, was\nintroduced. Foundation models were seen to differ significantly in robustness\naccording to this metric. Uni2-h and Virchow2 were found to be most robust,\nand Virchow2 was the only model so far with a robustness index above one,\nmeaning biological information (cancer type) dominates confounding informa-\ntion (medical center) across the k = 50 nearest neighbors.\nIt was seen that distance in embedding space strongly correlates with both\nthe probability of encountering same-cancer-type neighbors and same-medical-\ncenter neighbors. This influence is not just local, but was seen to extend across\nthe entire embedding space.\nUsing the notion of same-center confounders, the impact of medical centers\non prediction was evaluated, and it was found that all pathology foundation\nmodels evaluated here represent medical centers to a large extent.\nA 2D projection of the embedding space was visualized. The resulting images\nshow visually that the organization of the embedding space shows a clustering\nby medical center; more strongly so than a clustering by tissue or cancer type.\nThe robustness index and the other analysis techniques described in this\nwork are intended as tools that may enable the development of more robust\npathology foundation models."}, {"title": "9 Appendix", "content": ""}, {"title": "9.1 Model Selection", "content": "Ten publicly available pathology foundation models were selected for evaluation,\nfocusing on patch-level models. In addition, SRA-MoCo_v3 [34] was evaluated;\nwhile this model has been trained on a small single-tissue dataset, and can thus"}, {"title": "9.2 Embedding Generation", "content": "Embeddings are generated using the default approach for each model. For Vir-\nchow and Virchow2, this means the average of the patch tokens is concatenated\nto the class token, resulting in a 2560-dimensional embedding, indicated with\n'-2560D'. To check whether this expansion of the embedding space affects per-\nformance, results with just the class token, indicated with '-1280D', are included\nfor Virchow and Virchow2 as well. For all remaining models, the class token is\nthe standard output, and is used here."}, {"title": "9.3 Fraction of Same-Center Confounders: Full Results", "content": ""}, {"title": "9.4 Frequency Same Cancer Type / Medical Center: Full\nResults", "content": ""}]}