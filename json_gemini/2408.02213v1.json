{"title": "Is Large Language Model Good at Database Knob Tuning? A Comprehensive Experimental Evaluation", "authors": ["Yiyan Li", "Haoyang Li", "Zhao Pu", "Jing Zhang", "Xinyi Zhang", "Tao Ji", "Luming Sun", "Cuiping Li", "Hong Chen"], "abstract": "Knob tuning plays a crucial role in optimizing databases by adjusting knobs to enhance database performance. However, traditional tuning methods often follow a Try-Collect-Adjust approach, proving inefficient and database-specific. Moreover, these methods are often opaque, making it challenging for DBAs to grasp the underlying decision-making process.\n The emergence of large language models (LLMs) like GPT-4 and Claude-3 has excelled in complex natural language tasks, yet their potential in database knob tuning remains largely unexplored. This study harnesses LLMs as experienced DBAs for knob-tuning tasks with carefully designed prompts. We identify three key subtasks in the tuning system: knob pruning, model initialization, and knob recommendation, proposing LLM-driven solutions to replace conventional methods for each subtask.\n We conduct extensive experiments to compare LLM-driven approaches against traditional methods across the subtasks to evaluate LLMs' efficacy in the knob tuning domain. Furthermore, we explore the adaptability of LLM-based solutions in diverse evaluation settings, encompassing new benchmarks, database engines, and hardware environments. Our findings reveal that LLMs not only match or surpass traditional methods but also exhibit notable interpretability by generating responses in a coherent \"chain-of-thought\" manner. We further observe that LLMs exhibit remarkable generalizability through simple adjustments in prompts, eliminating the necessity for additional training or extensive code modifications.\n Drawing insights from our experimental findings, we identify several opportunities for future research aimed at advancing the utilization of LLMs in the realm of database management.", "sections": [{"title": "1 INTRODUCTION", "content": "Configuration knobs control many aspects of database systems (e.g., memory allocation, thread scheduling, caching mechanisms), and different combinations of knob values significantly affect performance, resource usage, and robustness of the database [8]. In general, given a workload, knob tuning aims to judiciously adjust the values of knobs to improve the database performance [53]. For example, the MySQL database has about 260 knobs, of which adjusting the InnoDB_buffer_pool_size and tmp_table_size can significantly improve database query processing efficiency [25]. Therefore, it is vital to set proper values for the knobs.\n Traditional knob tuning relies on database administrators (DBAs) to manually try out typical knob combinations based on their experience. This process is labor-intensive and impractical for a large number of database instances (e.g., tens of thousands on cloud platforms) [40]. Leveraging machine learning techniques, researchers have developed various automated knob tuning systems capable of identifying suitable knob values without human intervention [2, 20, 25, 28, 44]. The workflow of these systems is depicted in Figure 1, with the left side illustrating the tuning system and the right side representing the target database management system (DBMS). Upon receiving a workload, the tuning method suggests a configuration for the DBMS, which is then tested with the workload to measure performance metrics (e.g., latency or transactions per second). Subsequently, based on this feedback, the tuning method refines its policy and proposes a new configuration. Through multiple iterations of \"Try-Collect-Adjust\", an optimized"}, {"title": null, "content": "configuration can be achieved to enhance database performance significantly under the given workload.\n The knob tuning system can be segmented into three key components: knob recommendation, knob pruning, and model initialization. Knob recommendation serves as the cornerstone of the tuning system, offering suggestions for suitable configurations tailored to the workload. The approaches for knob recommendation fall into four primary categories: reinforcement learning (RL) based approaches [6, 25, 46, 48], Bayesian optimization (BO) based techniques [2, 13, 50, 51], deep learning (DL) based methods [3, 29, 44], and heuristic methods [9, 55]. Given the expansive search space of configurations, these tuning methods typically necessitate numerous interactions with the DBMS, with each iteration involving workload execution. This iterative process is both time and resource-intensive. To address this challenge, various knowledge transfer methods [2, 7, 25, 42, 50] have been introduced, leveraging past tuning records to expedite the tuning process. These methods can be classified into two categories: knob pruning and model initialization. Knob pruning targets the selection of crucial knobs and the determination of their reasonable ranges for the specific workload, thereby reducing the configuration space [12, 13, 19, 42-45]. On the other hand, model initialization focuses on initializing the learnable model within the knob recommendation methods, which can accelerate their convergence speed [2, 7, 14, 15, 25, 48, 50]. It should be noted that the knob pruning and model initialization techniques usually occur at the beginning of the tuning phase, which are optional components of the tuning system. Then, the knob recommendation methods iteratively interact with DBMS until the database performance coverage or stop conditions are triggered.\n Limitations of Existing Methods. Despite the notable performance achieved by current methods, they still exhibit the following limitations. (1) Knob pruning and model initialization techniques often heavily rely on historical tuning data or domain knowledge (e.g., database manual, and forum discussions) to expedite current tuning tasks. For instance, knob pruning methods like Lasso [45] and Sensitivity Analysis [34] necessitate extensive historical tuning data for calculating knob importance rankings and GPTuner [23] and DB-BERT [46] requires manually collected knob-tuning-related texts to optimize the configuration space. Similarly, model initialization methods like QTune [25] also rely on historical data for pre-training actor and critic models. However, acquiring such data can be costly, particularly when addressing new database kernels or hardware environments, requiring data collection from scratch. (2) Regarding knob recommendation methods, many of them need to replay the workload in each iteration to capture performance metrics. Due to the limited exploration and exploitation capabilities of these methods, they often require numerous iterations, leading to significant time and resource expenses. (3) Almost all database knob tuning approaches operate as black boxes. This opacity makes it challenging for DBAs to understand the rationale behind recommended outcomes and complicates their ability to intervene effectively in case of issues.\n Our Proposal. This paper aims to explore the feasibility of utilizing LLMs to emulate the behaviors of DBAs in performing knob-tuning-related subtasks, including knob pruning, model initialization, and knob recommendation. Recent advancements in LLMs"}, {"title": null, "content": "have yielded remarkable breakthroughs in diverse domains, such as mathematical reasoning [1], text-to-SQL [26], and tool using [41]. LLMs are famous for vast knowledge, strong reasoning capabilities, and remarkable interpretability, offering potential solutions to the aforementioned limitations within the tuning system. Therefore, integrating LLMs into the database knob tuning system represents a promising direction for research. While LLM-based tuning methods like GPTuner [23] have been proposed, existing work primarily focuses on knob pruning, only one subtask within the broader knob tuning process.\n In this study, we carefully craft prompts for each tuning subtask and evaluate LLMs' performance through comparative experiments against previous state-of-the-art (SOTA) methods. Given the diverse array of LLMs available, our evaluation extends beyond a single model. We explore a spectrum of powerful LLMs, including GPT-3.5 [39], GPT-4-Turbo [36], GPT-40 [37], and Claude-3-Opus [4]. However, as these powerful LLMs are closed-sourced, concerns related to data privacy and high usage costs may arise. To address this, we additionally consider several open-source LLMs, such as LLaMA3 [33] and Qwen2 [10], which offer the advantage of local deployment.\n Our primary experiments are conducted using an Online Transaction Processing (OLTP) benchmark (SYSBENCH [21]) in conjunction with the MySQL database engine. In addition, given the inherent adaptability of LLMs, which allows them to generalize to new scenarios through prompt adjustments, we also conduct comprehensive assessments to evaluate the generalizability of our LLM-based solutions across diverse workloads, database engines, and hardware environments. We believe that this study can serve as a source of inspiration for more AI4DB tasks, such as query optimization and index recommendation.\n We make the following contributions in this paper:\n \u2022 We investigate the capabilities of LLMs in executing three knob tuning subtasks: knob pruning, model initialization, and knob recommendation. For each subtask, we carefully craft prompts to guide the LLMs in effectively addressing the specific objectives.\n \u2022 In our experiments, we comprehensively evaluate both closed-source and open-source LLMs, offering researchers and practitioners a thorough understanding of the strengths and limitations of various LLMs.\n \u2022 We additionally assess the generalizability of LLMs by conducting experiments across various benchmarks, database engines, and hardware environments.\n \u2022 Based on our findings, we explore future research directions and potential challenges in the domain of utilizing LLMs for knob tuning.\n The remainder of the paper is organized as follows. We formally define the problems in Section 2, followed by a description of the integration of LLMs with three database knob tuning subtasks in Section 3. Then, Section 4-8 presents our experimental evaluation and main findings. Finally, we discuss research opportunities in Section 9 and conclude in Section 10."}, {"title": "2 PROBLEM DEFINITION", "content": "Consider a modern database system equipped with m tunable knobs, represented as $\\theta_1, ..., \\theta_m$. Each knob $\\theta_i$ might be either continuous"}, {"title": null, "content": "or categorical, covering a range of configurable database aspects like work memory size and maximum connection limits. Every knob $\\theta_i$ is assigned a value within a predetermined range $\\Theta_i$, signifying the allowable value spectrum for that knob. The combination of possible knob values forms a huge multi-dimensional configuration space, represented by $\\Theta = \\Theta_1 \\times \\Theta_2 \\times ... \\times \\Theta_m$. A specific point within this space signifies a unique database configuration, characterized by a set of knob values $\\theta = (\\theta_1, \\theta_2, ..., \\theta_m) \\in \\Theta$.\n In the context of optimizing database performance, we define the performance metric as $f$, representing factors like throughput or latency that we seek to enhance. For a given database instance D, workload W, and a specific configuration $\\theta$, the resulting performance metric $f(D, W, \\theta)$ is observed after applying $\\theta$ in the database engine and executing W on D.\n As illustrated in Figure 1, a complete knob tuning system encompasses three important subtasks: knob pruning, model initialization, and knob recommendation. The objective of this study is to explore the ability of LLMs to execute these subtasks, prompting us to define the problem for each subtask via LLMs as follows.\n LLMs for Knob Pruning. In modern database systems, although there are hundreds of adjustable knobs, not all knobs are equally important under specific workloads. For example, working memory size is vital to memory-intensive workloads, maximum IO concurrency is vital to IO-intensive workloads. Hence, considering the characteristics of D and W, the goal of knob pruning is to identify the most impactful knobs and define their crucial ranges. By reducing the search space, knob tuners can concentrate on adjusting these selected knobs in the constrained ranges and thus streamline the tuning process. Formally, we have:\n LLM(Promptkp, D, W, {$\\theta_1, ..., \\theta_m$},{$\\Theta_1, ..., \\Theta_m$}) $\\rightarrow$ {$\\theta_j, ..., \\theta_k$}, {$\\Theta'_j, ..., \\Theta'_k$}, (1)\n where LLM(\u00b7) denotes the large language model, Promptkp represents the pre-defined prompt used for the knob pruning task, and the outputs {$\\theta_j, ..., \\theta_k$ } and {$\\Theta\u00b4_j, ..., \\Theta\u00b4_k$} represent the LLM-selected significant knobs and their respective important ranges. Notably, unlike traditional knob pruning methods such as Lasso [45] and Sensitivity Analysis [34], which only select knobs, the LLM-based approach can also recommend important value ranges for the selected knobs. Furthermore, unlike Lasso [45] and Sensitivity Analysis [34], which rely on historical tuning data, and the existing LLM-based method GPTuner [23] and DB-BERT [46], which requires manually collected knob-tuning-related texts for input augmentation, our LLM-based approach harnesses the inherent capacity of LLMs to emulate the actions of DBAs for knob pruning. Given that powerful LLMs have likely encountered tuning-related manuals and web pages during pre-training, the primary objective is to instruct them to follow knob pruning guidelines and leverage their internal knowledge."}, {"title": null, "content": "LLMs for Model Initialization. In practical scenarios, workloads often exhibit dynamic changes, with workload pressures varying significantly over time (from morning to evening, weekdays to weekends, or workdays to holidays). It is widely acknowledged that tuning a specific configuration is necessary for different workloads. However, starting the tuning process from scratch for each workload requires multiple iterations of database interactions, which"}, {"title": null, "content": "can be time-consuming and resource-intensive. To accelerate the tuning speed, several transfer learning-based studies have been proposed to leverage knowledge from historical tuning records as the initialization of the tuning method, facilitating quicker convergence.\n Instead of accumulating extensive historical tuning data for model initialization, we propose leveraging LLMs to recommend a set of effective initial knob configurations for the new workload. These LLM-generated configurations can then be used to initialize traditional Bayesian Optimization (BO)-based tuning methods, such as HEBO [11] and VBO [13]. By eliminating the initial phase of random exploration, this methodology enables the BO-based methods to rapidly converge to a suitable configuration, accelerating the overall tuning process. Specifically, we use LLMs to sample a set of effective configurations for a given workload W on database D:\n LLM(Promptrec, $\\theta_{df}$, D, W, $\\Theta$, $F_{df}$) $\\rightarrow$ {$\\theta_1, ..., \\theta_u$}, (2)\n where Promptrec indicates the prompt used to recommend configurations, $\\theta_{df}$ represents the default configuration, $\\Theta$ signifies the space of possible configurations, $F_{df}$ represents the database's feedback under the default configuration, and the output {$\\theta_1,..., \\theta_u$} consists of a set of effective configurations derived from the LLM. The default configuration, denoted as $\\theta_{df}$, serves as an anchor point, guiding LLMs to adjust only the knobs requiring modification while maintaining the settings of those that do not necessitate changes. The database's feedback $F_{df}$ consists of performance metrics (such as latency or transactions per second) and internal metrics (such as lock_deadlocks and os_data_writes). The feedback can provide insights into system states, enabling LLMs to identify performance bottlenecks and make necessary adjustments to the default configuration. The set of configurations {$\\theta_1, ..., \\theta_u$} generated by the LLM can be used to initialize the BO-based tuning methods, serving as their starting data points.\n LLMs for Knob Recommendation. The knob recommendation component is pivotal within the tuning system, responsible for suggesting the optimal configuration for specific workloads to enhance performance metrics. Existing techniques, including the prominent BO-based methods [2, 13, 50, 51] as well as RL-based approaches [6, 25, 46, 48], often require hundreds of iterations to converge, hindered by their limited abilities in balancing exploration and exploitation. In this study, we posit that LLMs, with their advanced understanding of database feedback and superior exploration and exploitation capabilities, can pinpoint appropriate configurations in significantly fewer iterations.\n The LLM-based knob tuning approach is iterative: starting from the default configuration, we employ LLMs to progressively refine the configuration based on the database feedback. Formally, the refinement process is defined as:\n LLM(Promptrec, $\\theta_i$, D, W, $\\Theta$, $F_i$) $\\rightarrow$ $\\theta_{i+1}$, (3)\nhere, Promptrec also denotes the knob recommendation prompt, and i signifies the current configuration, $F_i$ represents the database feedback under current configuration, the output $\\theta_{i+1}$ represent the refined configuration. Subsequently, $\\theta_{i+1}$ is applied in the database, and the workload is executed to gather the feedback $F_{i+1}$. Then, we can start a new iteration to refine $\\theta_{i+1}$. Initially, $\\theta_0$ and $F_0$ are"}, {"title": null, "content": "$\\theta_{df}$ and $F_{df}$, respectively. This refinement process iterates several times until reaching the stop criterion.\n Summary. In this section, we propose dividing the knob tuning tasks into three key subtasks and formulating solutions for each using LLMs. The goal is to replace traditional methods with LLM-based solutions for each subtask and evaluate their effectiveness, rather than presenting a comprehensive framework where all three subtasks are solved by LLMs."}, {"title": "3 KNOB TUNING WITH LLM", "content": "In this section, we will delve into the details of constructing prompts for three fundamental tuning subtasks."}, {"title": "3.1 Knob Pruning", "content": "For a given workload, knob pruning is a critical process aimed at identifying the most important knobs and narrowing their permissive ranges, which can reduce the search space of the knob recommendation methods. Leveraging the LLM as an alternative to traditional knob pruning methods involves incorporating several important elements within the prompt. As illustrated in Figure 2, the prompt for knob pruning contains the following elements:\n \u2022 \"Task Description\" describes the objective of the LLM.\n \u2022 \"Candidate Knobs\u201d provides detailed information about candidate knobs within the database engine, encompassing knob names, allowable ranges, types of knobs, and their respective descriptions.\n \u2022 \"Workload and Database Information\" contains crucial details about the workload, data characteristics, database kernel, and the hardware.\n \u2022 \"Output Format\" specifies the response format of the LLM. Specifically, it requires the LLM to enumerate the names of the chosen knobs, as well as their corresponding ranges and types, in an organized JSON format."}, {"title": "3.2 Model Initialization", "content": "The model initialization technique is designed to speed up the tuning process by leveraging historical tuning records to initialize the model used in the knob recommendation methods. In this paper, we concentrate on utilizing LLMs to produce a set of effective configurations for the given workload. Subsequently, these configurations can be used to initialize the BO-based methods, effectively accelerating their convergence speed. As illustrated in Figure 3, we construct the prompt for model initialization in the following format:\n \u2022 \"Task Description\" outlines the objective of the LLM.\n \u2022 \"Demonstration for Knob Refinement\" includes a knob refinement instance, which aims to serve as the one-shot example in the prompt for demonstration purposes. This instance includes a current configuration, inner metrics, and a refined configuration.\n \u2022 \"Environment\" contains the information about database kernel and hardware information. The database kernel details encompass the database engine's name and version. The hardware information specifies the number of CPUs and the available memory resources. In addition, we also include the text descriptions of each inner metric and tunable knob."}, {"title": null, "content": "\u2022 \"Information about Current Workload\" includes features about the current workload, such as workload type (OLAP or OLTP) and read-write ratio, and data statistics in the database.\n \u2022 \"Output Format\" specifies the format for LLM responses.\n \u2022 \"Current Configuration\" displays the default values of the knobs, which serve as the anchor point as discussed in Section 2.\n \u2022 \"Database Feedback\" showcases the performance and inner metrics of the database when executing the given workload under the default configuration. Incorporating this feedback is essential, as DBAs often depend on these metrics to assess the database's status and implement necessary adjustments. For example, confronted with a low cache hit rate, DBAs typically choose to increase the cache size to improve database performance.\n In practice, we utilize the LLM for multiple samplings to acquire a collection of effective configurations. These configurations then act as the initial points for BO-based knob recommendation methods."}, {"title": "3.3 Knob Recommendation", "content": "The knob recommendation emerges as the crucial subtask within the tuning system, aimed at identifying a promising configuration"}, {"title": null, "content": "for a specific workload. As elaborated in Section 2, the LLM undertakes the task of knob recommendation through an iterative process. Initially starting from the default configuration, the LLM employs iterative refinements based on feedback from the database. The prompt for knob recommendation closely mirrors the prompt of model initialization, as depicted in Figure 3. The key distinction lies in the fact that, for knob recommendation, both the \"Current Configuration\" and \"Database Feedback\" are subject to change with each iteration."}, {"title": "4 GENERAL SETUPS OF EVALUATION", "content": "This study conducts a series of comprehensive evaluations to assess the efficacy of various LLMs across three database knob tuning subtasks. We detail the configurations of the primary experiments, encompassing hardware, software, benchmark, tuning settings, and large language models, as outlined below:\n Hardware and Software. Our knob tuning framework is deployed across three distinct servers. The first server, dedicated to the tuning system, is equipped with 48 CPUs and 256 GB of RAM. The second server, designated for the DBMS deployment, features 8 CPUs and"}, {"title": null, "content": "16 GB of RAM, running RDS MySQL version 5.7. The third server is allocated for deploying local LLMs and is equipped with two NVIDIA A100 80GB GPUs, 80 CPUs, and 256 GB of memory. We utilize vLLM [22] as the backend to manage local LLMs. These three machines are interconnected via an intranet, communicating through a high-speed network.\n The first server, functioning as the tuning system, controls coordination among three distinct servers. In our LLM-integrated knob tuning framework, the tuning system acts as a bridge between the second server (DBMS) and the third server (local LLMs), handling the interactions between them. For example, for the knob recommendation task, the tuning system first sends the default configuration to the second server to obtain feedback from the database. Then, the tuning system integrates the workload features, current configuration, database feedback, and other required information into a prompt and then sends it to the third server to obtain the LLM's response (i.e., refined configuration). We should note that, for closed-source LLMs, we access them through APIs, eliminating the need for using the third server.\n Benchmark. Following previous work [25, 49, 51], we employ SYSBENCH [21], a prevalent OLTP benchmark, for our evaluation. In particular, we focus on the OLTP-Read-Write workload within SYSBENCH, representing a workload that encompasses both read and write operations typical in OLTP scenarios. Subsequently, we load 50 tables within SYSBENCH, with each table housing 1,000,000 rows of records, culminating in approximately 13 GB of data. For a specific configuration, to conduct a stress test, we run the workload for two minutes to obtain the transactions per second (TPS) metric as the database performance. We restart the database after applying a new configuration to guarantee that all knobs have been correctly configured.\n Tuning Settings. In the knob pruning task, both LLMs and baseline methods are tasked with identifying 10 significant knobs from the provided set of 100 candidate knobs. Following this selection, we utilize the traditional knob recommendation method, SMAC [18], to identify a suitable configuration using these 10 chosen knobs. In the context of model initialization and knob recommendation, we manually select 20 crucial knobs for further tuning. For model initialization, we leverage the LLM-generated configurations to initialize a BO-based knob recommendation method, VBO [13], aiming to expedite its tuning process. For knob recommendation, we directly compare LLMs against those of traditional knob recommendation methods, including DDPG [27], SMAC [18], and VBO [13].\n Evaluation Metric. We evaluate our methods and baselines across two key dimensions: the tuning efficiency score (TES) and the optimal database performance (ODP). As illustrated in Figure 1, we have noticed that a significant portion of the tuning process is dedicated to the DBMS side, as each iteration requires the workload to be replayed using the newly suggested configuration. To minimize the influence of external variables such as network latency, we introduce the TES metric, which quantifies the iterations needed to achieve peak database performance in the tuning process. In addition, the ODP metric measures the maximum achievable TPS during the tuning procedure."}, {"title": "5 KNOB PRUNING", "content": "5.1 Baselines\n To evaluate the knob pruning capability of LLMs, we utilize a learning-based method, SHAP [30], as a competitive baseline. SHAP provides a unified framework for interpreting the significance of each knob. By analyzing a given set of tuning observations, where each observation consists of a <configuration, performance metric> pair, the importance of each knob is determined through the calculation of its SHAP value. As highlighted in [49], SHAP currently stands out as the most effective learning method for assessing the importance of knobs. To gather training data for SHAP, we collect approximately 6000 observations for the SYSBENCH workload using the Latin Hypercube Sampling (LHS) method [32], which can sample configurations across the entire configuration space. Subsequently, we execute the workload under these configurations to acquire their corresponding performance metrics.\n In addition, we invite an industry database expert to conduct the knob pruning task as a human annotation baseline. Specifically, we allow the expert to identify crucial database knobs and their important value ranges based on his own expertise and experience. The expert is also permitted to consult with other experienced database administrators, the broader database community, and official MySQL documentation to complete the task.\n After narrowing the search space, we then utilize a traditional knob recommendation method, SMAC [18], to optimize these selected knobs for a maximum of 120 iterations. We record the configuration and corresponding database performance in each iteration to calculate the TES and ODP metrics."}, {"title": "5.2 LLMs for Knob Pruning", "content": "For LLMs, we utilize the prompt illustrated in Figure 2 to perform the knob pruning from the candidate knobs. It is worth noting that, during the inference of the LLM, we set the temperature parameter to 0 to guarantee deterministic outcomes."}, {"title": "5.3 Experimental Results", "content": "The experimental results are illustrated in Figure 4. Our observations are as follows: (1) In the knob pruning task, certain LLMs (Claude-3-Opus, GPT-40, and GPT-4-Turbo) demonstrate comparable or even superior performance to that of the database expert. Upon analysis, we discover a significant similarity between the knobs selected by the LLMs and those chosen by the expert. For instance, Table 1 showcases the top 10 most critical knobs identified by the database expert, GPT-40, and SHAP. Notably, the knobs selected by GPT-40 closely align with those chosen by the database expert. This resemblance may be attributed to the extensive training data utilized for GPT-40 (and other LLMs), which incorporates MySQL community discussions, relevant articles, blogs, and official documentation [54]. Consequently, the LLM can emulate the behavior of the database expert, leading to similar knob pruning outcomes. (2) Furthermore, we note that nearly all evaluated LLMs outperform the previous learning-based method, SHAP, in terms of both convergence speed and optimal database performance, with GPT-40 exhibiting the most favorable results. After examining the selection outcomes produced by SHAP, we observe a distinct knob set compared to GPT-40 and the database expert.\n In delving into why GPT-40 surpasses the performance of the database expert in this task, a closer examination of their differences reveals a key distinction: as shown in Table 1, GPT-40 opts for knob \"join_buffer_size\" while the database expert selects knob \"max_connections\". Increasing \"join_buffer_size\" can enhance the efficiency of the join operator, consequently boosting overall database performance. On the other hand, the impact of increasing \"max_connections\" on database performance is not always beneficial. If \"max_connections\" exceeds the actual number of connections required by the workload, increasing the value of this knob will have no discernible effect on the database performance."}, {"title": "5.4 Main Findings", "content": "Our main findings of this section are summarized as follows:\n \u2022 In the knob pruning task, certain LLMs (such as Claude-3-Opus and GPT-40) demonstrate superior performance, surpassing even that of the DBAs. Furthermore, nearly all LLMs exhibit superior performance compared to the learning-based baseline, SHAP.\n \u2022 The knobs chosen by some LLMs closely resemble those selected by the database expert, indicating the potential for LLMs to replace DBAs in automating the process of pruning knobs.\n \u2022 We observe closed-source LLMs are much better than open-source LLMs in this task, indicating that the source of the LLM can have a substantial impact on this task.\n \u2022 Simply prompting LLMs without fine-tuning any parameters can achieve performance levels comparable to that of humans, showcasing the remarkable flexibility and adaptability of LLMs in effectively addressing this task.\n \u2022 Therefore, one promising future direction is to fine-tune an LLM tailored for knob pruning, potentially enabling it to surpass experts by a considerable margin."}, {"title": "6 MODEL INITIALIZATION", "content": "6.1 Baselines\n To further enhance tuning efficiency, a series of model initialization methods [2, 25, 50] have been introduced to use the past tuning records to initialize learnable models in the knob recommendation methods.\n Current model initialization methods can be broadly categorized into three main groups: workload mapping, model ensemble, and pre-training. Workload mapping, as proposed by OtterTune [2], involves matching the target workload with the most similar historical workloads and leveraging their tuning observations to initialize the surrogate model. This approach can be integrated into a wide range of BO-based knob recommendation methods. The model ensemble technique, as described in ResTune [50], entails collecting a set of well-established tuning models on historical workloads and then combining these models to guide the optimization of current tuning model for new workloads. Lastly, the pre-training technique is commonly utilized in RL-based knob recommendation methods, as seen in works such as QTune [25] and CDBTune [48]. This process involves initially pre-training parameters of the actor and"}, {"title": null, "content": "the critic within the RL algorithm using a set of historical tuning records. Subsequently, when facing a new workload, the pre-trained models will be further fine-tuned. By avoiding the necessity to train models from randomly initialization, this technique could expedite the tuning process.\n We have chosen representative methods from each category as baselines. For workload mapping, we have selected the OtterTune method [2] integrated with the BO-based knob recommendation method VBO [13]. In the model ensemble category, we are using the ResTune method [50] combined with the meta-learning knob recommendation method RGPE [14]. Lastly, for pre-training, we adopt QTune [25] as the baseline, which aims to accelerate the RL-based knob recommendation method DS-DDPG. In this section, all knob recommendation methods undergo 400 iterations with or without model initialization techniques."}, {"title": "6.2 LLMs for Model Initialization", "content": "We first use the prompt illustrated in Figure 3 to sample 10 potentially effective configurations from LLMs. Subsequently, in the initial stages of VBO, we replace randomly sampled configurations with these 10 LLM-generated configurations to expedite its tuning process. To quantify the acceleration potential facilitated by LLMs, we consider the original VBO method for comparison. To generate a set of configurations from LLMs, we utilize nucleus sampling [16] with a temperature of 1.0 and a top-p value of 0.98. During our experimentation, we observe that certain sampled configurations are duplicated. As a result, we iteratively perform samplings until we acquire 10 distinct configurations."}, {"title": "6.3 Experimental Results", "content": "We present the experimental results in Table 2. To provide a more intuitive understanding of the effectiveness of different initialization methods, we adopt the approach outlined in [49] to introduce two additional metrics: performance enhancement (PE) and speedup. Specifically, we represent the TES and ODP values for the base knob recommendation method without initialization as $TES_{orig}$ and $ODP_{orig}$, and for the method with initialization as $TES_{init}$ and $ODP_{init}$. In the OLTP benchmark, a higher TPS value represents better performance. Therefore, the performance enhancement is"}, {"title": null, "content": "calculated as follows:\n PE = $\\frac{ODP_{init} - ODP_{orig}}{ODP_{orig}}$ (4)\n and the speedup is defined as follows:\n Speedup = $\\frac{TES_{orig} - TES_{init}}{TES_{orig}}$ (5)\n The PE metric assesses whether the initialization technique can aid the base knob recommendation method in identifying superior configurations. Subsequently, the speedup metric measures the degree to which the initialization technique expedites the tuning process. Higher values for both PE and speedup indicate improved performance of an initialization method.\n The experimental findings are detailed in Table 2. Initially, we observe that the workload mapping technique yields a modest speedup of 11.71%. This outcome could be attributed to the limited utilization of historical tuning records. Moving on to the model ensemble technique, despite delivering a substantial 42.67% speedup, it still necessitates 215 iterations to reach peak performance. The outcomes of the pre-training technique are surprising. While it does lead to a significant performance improvement of 33.10%, it also causes a drastic decrease in speedup by -216.16%, a result that is unacceptable. In summary, these conventional initialization techniques do not clearly expedite the tuning process, which still necessitates 200-300 iterations to achieve the optimal performance.\n For using LLMs in the model initialization task, we first observe that some LLMs (GPT-3.5, GPT-40, and Claude-3-Opus) exhibit relatively poor performance enhancement and largely under-perform the base model (i.e., VBO). After analyzing the sampling configurations of these LLMs, we find that despite providing 20 knobs in the prompt, these models predominantly adjust only a handful of knobs, leaving the rest at default settings. Consequently, the 10 LLM-generated configurations exhibit significant similarities. Utilizing these configurations to initialize VBO might limit its capacity to explore uncharted areas and thus result in poor performance enhancement. To address this issue, one approach is to increase the temperature value to inject more randomness during the sampling phase. Among the other LLMs, including GPT-4-Turbo, Llama3-8B-Instruct, Llama3-70B-Instruct, and Qwen2-7B, a slight performance"}, {"title": null, "content": "decrease (less than 2%) is observed, falling within an acceptable range. Moreover, initializing VBO with LLM-generated configurations leads to a noticeable acceleration in convergence speed. Notably, LLMs have shown an average speedup of 71.42%, with some achieving an impressive 99.37% boost. In essence, utilizing LLMs for initializing VBO requires a careful balance between ODP and TES. Given the minor performance impact and significant tuning acceleration, this trade-off is deemed acceptable."}, {"title": "6.4 Main Findings", "content": "Our main findings of this section are summarized as follows:\n \u2022 Existing model initialization methods do not demonstrate an obvious speedup in the tuning process, often necessitating hundreds of iterations to identify a suitable configuration. On the other hand, LLMs have shown their capability to greatly expedite the convergence of BO-driven knob recommendation approaches through the generation of initial configurations.\n \u2022 In contrast to the findings in knob pruning, we note that open-source LLMs outperform closed-source LLMs"}]}