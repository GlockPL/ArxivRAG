{"title": "MULTI-DIMENSIONAL INSIGHTS:\nBENCHMARKING REAL-WORLD PERSONALIZATION\nIN LARGE MULTIMODAL MODELS", "authors": ["YiFan Zhang", "Shanglin Lei", "Runqi Qiao", "Zhuoma GongQue", "Xiaoshuai Song", "Guanting Dong", "Qiuna Tan", "Zhe Wei", "Peiqing Yang", "Ye Tian", "Yadong Xue", "Xiaofei Wang", "Honggang Zhang"], "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to evaluate\nthe model's ability to analyze and reason beyond basic content. (2) Recogniz-\ning that people of different age groups have varying needs and perspectives when\nfaced with the same scenario, our benchmark stratifies questions into three age cat-\negories: young people, middle-aged people, and older people. This design allows\nfor a detailed assessment of LMMs' capabilities in meeting the preferences and\nneeds of different age groups. With MDI-Benchmark, the strong model like GPT-\n4o achieve 79% accuracy on age-related tasks, indicating that existing LMMs still\nhave considerable room for improvement in addressing real-world applications.\nLooking ahead, we anticipate that the MDI-Benchmark will open new pathways\nfor aligning real-world personalization in LMMs. The MDI-Benchmark data and\nevaluation code are available at https://mdi-benchmark.github.io/", "sections": [{"title": "INTRODUCTION", "content": "Developing personalized artificial intelligence (AI) assistants to address the diverse needs of dif-\nferent users has long been a significant pursuit for humanity (Kobsa & Schreck, 2003; Xiao et al.,\n2018; Kocaballi et al., 2019; Rafieian & Yoganarasimhan, 2023; Pesovski et al., 2024). In real-\nworld scenarios, an ideal AI assistant should be capable of precisely meeting the specific demands\nof individuals across various age groups, cultural backgrounds, and professional fields.\nRecently, the field of artificial intelligence has undergone a significant paradigm shift, transitioning\nfrom specialized small models designed for specific simple tasks (Rawat & Wang, 2017; Zhao et al.,\n2019; Minaee et al., 2021; Singh et al., 2017) to unified large multimodal models (LMMs) capable\nof handling complex tasks (Zhang et al., 2024). This paradigm shift marks a crucial step toward\nachieving Artificial General Intelligence (AGI) and underscores the potential for LMMs to become\npersonalized human assistants.\nTo comprehensively evaluate the capabilities of LMMs, researchers have constructed several com-\nmon visual question-answering benchmarks (Goyal et al., 2017; Chen et al., 2015; Marino et al.,\n2019; Mishra et al., 2019; Biten et al., 2019) that assess general image-text comprehension and"}, {"title": "RELATED WORK", "content": ""}, {"title": "MULTIMODAL DATASET AND BENCHMARK", "content": "To evaluate the capabilities of LMMs, a variety of benchmarks from past research have been applied.\nAmong them, Flickr30k (Young et al., 2014), COCO Captions (Chen et al., 2015), and Nocaps\n(Agrawal et al., 2019) are utilized to evaluate LMMs' text generation and image description abilities.\nVizwiz (Bigham et al., 2010), VQA (Goyal et al., 2017), GQA (Hudson & Manning, 2019), and\nOK-VQA (Marino et al., 2019) are used to assess LMMs' comprehension of image information\nand question-answering abilities. For evaluating OCR capabilities, benchmarks like ST-VQA (Biten\net al., 2019) and OCR-VQA (Mishra et al., 2019) are employed. DocVQA (Mathew et al., 2021) is\nspecifically used to evaluate a model's ability to understand and identify documents.\nTo further explore the fine-grained capabilities of LMMs, recent benchmarks have significantly ex-\npanded the types of tasks assessed. Examples of such benchmarks include LVLM-eHub (Xu et al.,\n2023), MM-Vet (Yu et al., 2023b), MMBench (Liu et al., 2023), SEED-Bench (Li et al., 2023),\nMME (Fu et al., 2024a), MMT-Bench (Ying et al., 2024), Video-MME (Fu et al., 2024b), MMMU\n(Yue et al., 2023), MMMU-Pro (Yue et al., 2024a), MathVista (Lu et al., 2024b), Mathverse (Zhang\net al., 2025), We-Math(Qiao et al., 2024a), and MMEvol(Luo et al., 2024). Nevertheless, it should\nbe noted that these benchmarks have not fully explored the capability of LMMs to address the di-\nverse needs of different individuals. Therefore, we hope to better explore this ability through the\nMDI-Benchmark."}, {"title": "LARGE MULTIMODAL MODELS", "content": "Building on the success of many large language models (LLMs) (Brown et al., 2020; Touvron et al.,\n2023; Chiang et al., 2023), recent research has combined large language models with visual encoders\nto form LMMs with powerful visual understanding and semantic generation capabilities. Many\nexcellent open-source (Hong et al., 2023; Wang et al., 2023; Hu et al., 2024; Lu et al., 2024a; Liu\net al., 2024b; Ye et al., 2023; Abdin et al., 2024) and closed-source (Team et al., 2023; Bai et al.,\n2023; OpenAI, 2023; 2024) projects have been developed. This development has further enhanced\nthe potential for realizing personalized AI assistants."}, {"title": "PERSONALIZED RESEARCH", "content": "To achieve personalized AI assistants, large language models (LLMs) are currently attempting to\ncombine with users' personalized outputs to enhance their personalization capabilities and enable\nthem to generate outputs that conform to users' preferences (Wo\u017aniak et al., 2024; Zhuang et al.,\n2024b; Baek et al., 2024; Tan et al., 2024). Simultaneously, to further expand the understanding\nability of LLMs in the face of different needs, personalized data generation is also crucial(Chan\net al., 2024). In this work, we utilize the MDI-Benchmark to evaluate the ability of existing large\nmultimodal models to address personalized needs and provide our insights for future LMMs re-\nsearch."}, {"title": "MDI-BENCHMARK", "content": "The MDI-Benchmark sample design emphasizes the real-world complexity of information, scene\nvariability, and age differences. People's information concerns often vary by scenario. As shown\nin Figure 1, a family buying a new house may focus on practical issues that are closely related to\nthem, such as kitchen type, garage capacity, and bedroom amenities. Spectators at sports events may\nconcern themselves with game details, player achievements, and game progress."}, {"title": "EVALUATION DIMENSION", "content": "In contrast to existing work, MDI-Benchmark emphasizes the model's performance on real-world\nproblems across various ages and complexities within specific task scenarios, it is structured along\nthree different dimensions: scenario, age, and problem complexity.\nScenario Dimension. From the perspective of the scenario, the MDI-Benchmark aims to closely\nalign with the real needs of human life. Unlike the capability evaluation focus of previous LMMS\nevaluation benchmarks, the MDI-Benchmark is constructed based on real-life scenarios.\nIn response to the various scenarios that humans face in real life, we have drawn on the definitions\nprovided in sociological literature (Tajfel, 1979; Birmingham et al., 2008; Spears, 2021) and ex-\npanded upon them to identify 30 sub-domain scenarios. On this basis, we conducted a one-month\nquestionnaire survey covering people of different ages, genders, and occupations. A total of 2,500\nquestionnaires were distributed, and 2,374 valid responses were collected. Based on the frequency\nof sub-domain selection in the questionnaires, we selected the top 18 sub-domains, which were ul-\ntimately summarized into six main scenarios: architecture, education, housework, social service,\nsports, and transport. We collected images from these subdomains to ensure this benchmark is rich\nin scenario information. Examples are in the Appendix C.1.\nProblem Complexity Dimension. In the realm of everyday human activities, the level of com-\nplexity varies significantly, and the definition of difficulty is often subjective. To streamline this\ndefinition, we have quantified the problems hierarchically based on the fundamental capabilities\nof the model as the atomic units. Based on this criterion, we have filtered survey questions and\nrefined previous evaluation standards. Furthermore, the MDI-Benchmark is categorized into two\nlevels: (1) The first level involves relatively straightforward problem types that mainly evaluate the\nmodel's ability to extract scenario information. This includes tasks such as detection, optical char-\nacter recognition, position recognition, color recognition, and other fundamental capacities. (2) The\nsecond level demands that the model skillfully analyze both scenario information and user seman-\ntic information with logical acuity while integrating relevant knowledge to effectively meet user\nrequirements. Examples are in the Appendix C.2.\nAge Dimension. Age is a universal and specific criterion for group classification, making it more\nobjective compared to classifications based on culture and religious beliefs. As a fundamental at-\ntribute possessed by everyone, age is easy to quantify and compare. By using age as a classification\ndimension, we can better understand the needs of various groups and assess the capability of LMMs\nto meet these diverse needs. For the purposes of assessment and quantification, we identified three"}, {"title": "DATA COLLECTION", "content": "Data Source. Existing LMMs evaluation benchmarks have been widely used to evaluate and train\nnew models. To ensure the accuracy of the evaluation results, we collected over 500 new images\nthat were not included in existing datasets and recruited 120 volunteers from three age groups. From\neach group, we sampled 10 volunteers to form a 30-person data construction team. The main data\ncollection process was as follows: First, after determining the scenario dimension information, the\ndata construction team wrote detailed scenario information based on their interests. Meanwhile,\nwe input the scenario dimension information into open-source models (e.g., GPT-40, Gemini 1.5\nPro) and closed-source models (e.g., LLaVA-Next, MiniCPM) to generate more personalized, di-\nverse, and detailed scenario descriptions. Furthermore, the descriptions created by both humans and\nmodels were used as keywords to search for relevant images on the Internet. Meanwhile, We paid\nvolunteers a sufficient wage, approximately seven dollars per hour. These volunteers were tasked\nwith categorizing the images into six scenario dimensions. To ensure data balance and minimize\nbias, we ensured diversity within each age group in terms of gender, occupation, and other factors.\nDetailed classification standards and guidelines were provided to ensure consistency in categoriza-\ntion. We employed a cross-validation approach, whereby each group of volunteers screened the\nimages, and we retained only those images that were categorized identically by all three groups.\nAdditionally, multiple iterations of validation were conducted. This comprehensive process helped\nto construct a balanced and reliable data source.\nQuestion and Answer Generation. After obtaining the collected images, we used a heuristic\nmethod to manually generate questions and problems. The specific process is as follows: (1) Con-\nstruction of Knowledge Base. Specifically, multiple open-source and closed-source models are first\nused to describe the scenario content in the image and are summarized by human experts. Subse-\nquently, additional information related to the scenario content was found through an Internet search,\nand the image and this information were combined to form a knowledge base. (2) Generation of\nDifficult Multi-Choice Questions. To ensure the consistency of the generated questions with the\nimage content, we invited volunteers from three different age groups who participated in the data\ncollection phase to submit questions. These volunteers posed questions of varying complexity based\non the image scenarios and knowledge base content and created confusing incorrect options. (3)\nQuestion Format. The image-question pairs provided by the volunteers had to follow the format:\n[Level]-[Age]-[Scenario]. Here, Level includes level 1 and level 2; Age includes old, mid, and\nyoung; Scenario includes architecture, education, housework, social services, sports, and transport.\nFinally, a team of experts screened and evaluated the questions submitted by the volunteers to final-\nize the construction of the questions.\nData Statistics. The MDI-Benchmark is collected from three different dimensions: scenarios, age\ngroups, and abilities. It includes a total of 514 images and 1298 questions, all newly collected.\nMeanwhile, we strived to ensure a balance of data across different scenarios, ages, and question\ncomplexities. The detailed information is presented in the Table 1. As shown in Figure 1, the\ndataset covers six domains, each with three sub-domains, providing a comprehensive and structured\nconstruction of data across various fields."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "EXPERIMENTAL SETTINGS", "content": "Evaluation Protocols. To effectively evaluate the model's output, we require the model to provide\nthe correct answer in its response. Based on this, the accuracy of the response was calculated. This\nmeans that if the model articulates the correct concept but fails to produce the precise answer, it\nwill be classified as incorrect. This approach underscores the model's ability to follow instructions\naccurately, highlighting any deficiencies in this capacity. In addition, since the prompt input format\nvaries across different models, we investigated the input format for each model. We then endeavored\nto maintain consistency in the prompts, adhering to the official input format provided by each model.\nThis approach aims to minimize the impact of prompt differences on model performance.\nPrompt Template. Table 4 report the prompt templates in our experiments.\nEvaluation Models. We studied the performance of two different categories of base models\non the MDI-Benchmark. (a) Closed-source models: GPT-40(OpenAI, 2024), GPT-4V(OpenAI,\n2023), Qwen-VL-Plus(Bai et al., 2023), Gemini 1.5 Pro(Team et al., 2023) (b) Open-source\nmodels: LLaVA-NeXT-110B(Liu et al., 2024a), LLaVA-NeXT-70B(Liu et al., 2024a), LLaVA-\nNeXT-7B(Liu et al., 2024b), DeepSeek-VL-7B, DeepSeek-VL-1.3B(Lu et al., 2024a), Phi3-Vision-\n4.2B(Abdin et al., 2024), MiniCPM-LLaMA3-V 2.5(Hu et al., 2024), CogVLM-chat(Wang et al.,\n2023), CogAgent-vqa(Hong et al., 2023), mPLUG-Owl2-7B(Ye et al., 2023)\nScoring Metric. Table 2 shows the overall performance of different LMMs under two levels of\nproblem complexity and across six scenarios. To better assess the capabilities demonstrated by the\nmodel, we defined the scoring metric:"}, {"title": "", "content": "Scorefinal = \u03b1\u00b7 ScoreL1 + (1 \u2212 a) \u00b7 ScoreL2\n(1)\nwhere ScoreL1, ScoreL2 denotes the average performance of LMMs in various fields at the first and\nsecond tiers, respectively and we set the default value of a to 0.5."}, {"title": "MAIN RESULTS", "content": "Table 2 illustrates the overall performance of different LMMs on MDI-benchmark. We find out the\nfollowing insights:\nGPT family demonstrate an absolute advantage. GPT-40 leads all models and receives the high-\nest performance score. It can also be observed that closed-source models generally outperform\nopen-source models. However, some powerful open-source models are struggling to catch up with\nclosed-source models. For example, the LLaVA-NeXT-110B, and LLaVA-NeXT-72B performed\nslightly worse than the Gemini 1.5 Pro and better than the Qwen-VL-Plus.\nScaling phenomenon of model performance. Furthermore, due to the limited data available for\nthe closed-source models, we observed some interesting trends among the open-source models.\nWe selected the best-performing open-source models in various sizes, from LLaVA-NeXT-110B\nand LLaVA-NeXT-72B to MiniCPM-LLaMA3-V 2.5, DeepSeek-VL-7B, Phi3-Vision-4.2B and\nDeepSeek-VL-1.3B. As shown in Figure 4 (the Leaderboard of different LMMs), the final scores\nfor these models showed that the larger the model parameters, the better its ability to solve prob-\nlems in real scenarios. This is consistent with human experience: larger language model parameters\nmean more text logic training samples and less model distillation. When faced with more complex\nlogical reasoning tasks, these models can leverage more underlying knowledge and fundamental\ncapabilities."}, {"title": "SCENARIO DIMENSION ANALYSIS", "content": "The performance of LMMs in daily scenarios still has great room for improvement. To ob-\nserve the specific performance of different models in various scenarios, as shown in Figure 3, we\ncalculated the accuracy of different models across different fields. We found that these 14 LMMs\nachieved good performance in Level 1 for the education scenario. The performance is more bal-\nanced in the architecture, housework, transport and social service scenarios. However, there are\nsome shortcomings in the performance of sports scenarios, which we believe are closely related to\nthe current training data of LMMs. At present, LMMs research groups focus more on achieving\nbetter training and testing levels using existing Internet text data and high-quality textbook data, but\nthey neglect the improvement of datasets and capabilities in everyday life fields. This is where the\nMDI-Benchmark comes into play. We believe that the types of problems related to logical reasoning\nand the required background knowledge in the fields of sport and transport are richer and broader\nthan those in architecture, resulting in increased problem difficulty and a significant gap in reasoning\nperformance."}, {"title": "COMPLEXITY DIMENSION ANALYSIS", "content": "Decreased performance with increased complexity. As the complexity of the problems increases,\nthe model's performance in every scenario noticeably decreases. The accuracy of answering ques-\ntions in the same scenario can also change significantly for the same model. For instance, in the case"}, {"title": "AGE DIMENSION ANALYSIS", "content": "For a more direct and macro-level performance analysis, we only presented the average performance\nstatistics in the main table, as shown in Table 3, which primarily represents the performance of\nLMMs across three age stratification. Furthermore, we analyzed the model's performance in detail\nbased on age groups and scenario dimensions, as shown in the Appendix D. We have the following\nobservations.\nAll the models to follow under the level evaluation dimensions, but there are differences in per-\nformance between different age. As shown in Table 3, GPT-40 remains the top-performing model\nin the age dimension, demonstrating a performance advantage of 13 points over the highest-ranked\nopen-source model and 35 points over the lowest-ranked closed-source model. This dominant per-\nformance in the age-stratified evaluation highlights GPT-40's strong generalization ability and its\nleadership in daily use scenarios. However, when evaluating the model's capabilities from the per-\nspective of the age dimension, it provides insights into the model's effectiveness across different\ngroups in various real-world scenarios. Given the multitude of situations individuals encounter in\ndaily life, a model's capabilities must be comprehensive to address diverse human needs. The ob-\nserved decline in accuracy across age groups indicates that there is significant room for improvement\nin the overall performance of all models within this dimension. This finding underscores the need\nfor further research focusing on age-related issues and highlights both the necessity and innovation\nof our work.\nModels exhibit insufficient overall generalization across different age dimensions. As shown\nin Figure 6, we further visualize the model's performance across different age group, including old,\nmiddle-aged, young. By summing the model's results across age dimensions, we find that the old\ngroup achieves a total of 856.38, the middle-aged group 764.72, and the young group 902.94. This\ndistribution highlights the actual difficulty order of questions across age levels: middle-aged >old\n>young. In real-world scenarios, questions posed by middle-aged individuals tend to encompass\nmore aspects and require greater logical reasoning and background knowledge than those from older"}, {"title": "CONCLUSION", "content": "In this paper, we propose the MDI-Benchmark, a tool designed to evaluate the capability of Large\nMultimodal Models (LMMs) in addressing real-world human demands within multi-dimensional\nscenarios. The MDI-Benchmark comprises over 500 images and 1.2k corresponding requirements,\nencompassing six major aspects of human life. Additionally, we introduce the concept of age strat-\nification and sampling questions based on the needs of elderly, middle-aged, and young individuals\nto ensure comprehensive evaluation. Using the MDI-Benchmark, we evaluated 14 existing LMMs,\nrevealing their performance preferences in different scenarios. While GPT-40 performed best across\na variety of metrics, there were gaps in performance across all age groups and scenarios. Therefore,\nwe suggest that future studies should focus on improving the adaptability of LMM to human needs\nand its ability to generalize across different domains and age groups. This will pave the way for the\nnext generation of LMMs that can effectively meet human needs."}, {"title": "MORE DETAILS ON EXPERIMENT SETUP", "content": ""}, {"title": "DETAILS OF THE PROMPT INFORMATION", "content": "The specific prompt information is shown in Table 4."}, {"title": "DETAILS OF THE EVALUATED MODELS", "content": "Table 5 shows the release times and model sources of the LMMs we evaluated at MDI-Benchmark."}, {"title": "MORE RELATED WORKS OF MDI-BENCHMARK", "content": "The advent of large language models (LLMs) has driven significant advancements in natural lan-\nguage processing (NLP) Zhao et al. (2023) such as instruction following (Ouyang et al., 2022; Zhou\net al., 2023; Su et al., 2023; Zeng et al., 2023; Dong et al., 2024a), RAG (Lewis et al., 2020; Guu\net al., 2020; Liu et al., 2024c; Dong et al., 2023; 2024d;b; Gao et al., 2023), reasoning (Yuan et al.,\n2023; Yu et al., 2023a; Yue et al., 2024b; Gou et al., 2023), information extraction and dialogue\nsystem (Thoppilan et al., 2022; Dong et al., 2022; Wu et al., 2023; Lei et al., 2023). Moreover, sev-\neral studies focuse on how to comprehensively evaluate the various capabilities and robustness of\nLLMs (Hendrycks et al., 2021; Zheng et al., 2024; Song et al., 2024; Dong et al., 2024c). Building\non this success, recent works have combined LLMs with visual encoders to form large multi-modal\nmodels (LMMs) with powerful visual understanding and semantic generation capabilities. Both\nopen-source (Hong et al., 2023; Wang et al., 2023; Hu et al., 2024; Lu et al., 2024a; Liu et al.,\n2024b; Ye et al., 2023; Abdin et al., 2024) and closed-source (Team et al., 2023; Bai et al., 2023;\nOpenAI, 2023; 2024) works have significantly expanded the capabilities of AI systems across di-\nverse applications. Additionally, studies such as (Li et al., 2024; Zhuang et al., 2024a; Wei et al.,\n2024; Qiao et al., 2024b) have demonstrated the effectiveness of multi-modal models in domains like\nmedical imaging, mathematical reasoning, and general-purpose understanding, and some intriguing\napplications."}, {"title": "MORE DETAIL ON MDI-BENCHMARK", "content": ""}, {"title": "EXAMPLE OF SCENARIO DIMENSION", "content": "In this section, we present a selection of images from the MDI-Benchmark for visual demonstration\npurposes.\n1. Architecture: Including house planning, work scenes, measuring, etc. As shown in Fig-\nure 7.\n2. Education: Including campus facilities, studying activities, teaching, etc. As shown in\nFigure 8.\n3. Housework: Including home arrangements, housework activities, household appliances,\netc. As shown in Figure 9.\n4. Social service: Including travel, shopping, communal facilities, etc. As shown in Fig-\nure 10.\n5. Sport: Including ball sports, racing sports, powerlifting, etc. As shown in Figure 11.\n6. Transport: including signpost, rail transit, airport, etc. As shown in Figure 12."}, {"title": "EXAMPLE OF AGE DIMENTION", "content": "In this section, we have sampled various concerns and issues from people across three different age\ngroups within the six major scenarios. These concerns have been categorized by scenario and are\nvisually presented in Figures 19 through 24."}, {"title": "BAD CASE", "content": "In this section, we will conduct a case study of the types of errors that different models make in each\ndimension of MDI-Benchmark. We classify errors into three categories: information extraction\nerrors, lack of knowledge errors, and reasoning errors. Errors are highlighted in red.\nInformation Extraction Error. As shown in Figure 31. It occurs most frequently. This is because\nthe visual encoder of LMMs often fails to correctly capture the content information in the images,\nleading to incorrect answers."}]}