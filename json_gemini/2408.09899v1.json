{"title": "LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery", "authors": ["Weiji Kong", "Xun Gong", "Juan Wang"], "abstract": "Explaining the decisions of Deep Neural Networks (DNNs) for medical images has become increasingly important. Existing attribution methods have difficulty explaining the meaning of pixels while existing concept-based methods are limited by additional annotations or specific model structures that are difficult to apply to ultrasound images. In this paper, we propose the Lesion Concept Explainer (LCE) framework, which combines attribution methods with concept-based methods. We introduce the Segment Anything Model (SAM), fine-tuned on a large number of medical images, for concept discovery to enable a meaningful explanation of ultrasound image DNNs. The proposed framework is evaluated in terms of both faithfulness and understandability. We point out deficiencies in the popular faithfulness evaluation metrics and propose a new evaluation metric. Our evaluation of public and private breast ultrasound datasets (BUSI and FG-US-B) shows that LCE performs well compared to commonly-used explainability methods. Finally, we also validate that LCE can consistently provide reliable explanations for more meaningful fine-grained diagnostic tasks in breast ultrasound.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Deep Neural Networks (DNNs) model have become mainstream in medical image analysis, outperforming even human doctors on various tasks in different medical image modalities [1]. As a black-box model, DNNs have difficulty explaining what happens to the input image within the complex structure of DNNs. Lack of explainability leads to difficulty in gaining the trust of doctors or patients, making it challenging to put medical DNNs into clinical use, even if they perform well. In addition, some laws and regulations [2], [3] restrict the use of AI in human society.\nExplainable Artificial Intelligence (XAI) has increasingly come to the fore as a way of unlocking the black-box. For computer vision tasks, XAI tends to explain the output of the model by identifying what the most important pixels are. There are already a large number of XAI methods that can provide a variety of explanations, such as attribution methods and concept-based methods.\nAttribution methods can conveniently improve the explain- ability of medical DNNs [4]\u2013[6]. However, these methods are prone to false activations. They can only explain where in the image a pixel is located that is crucial for decision-making but are unable to identify which visual features drive decision- making at these locations. More importantly, explanations that do not match real-world meanings do not inspire confidence in doctors or patients.\nConcept-based methods [7], [8] can provide more mean- ingful explanations, but these techniques usually require ad- ditional provision of manually predefined concepts. This is difficult to implement in the medical field, especially for ultra- sound images, which are fast and low-cost, and adding several times the cost for explanation is like putting the cart before the horse. There are also some studies [9], [10] that attempt to use prototypes to bring explainability to medical DNNs, but these require a specific model structure, and prototypes derived from low-cost ultrasound images may be less accurate.\nExplain any concept (EAC) [11] pioneered the combination of the SAM [12], which has excellent zero-shot segmentation capability, with Shapley value [13], for efficient explanation. Concept discovery is the most fundamental step in EAC. However, it is difficult for EAC to discover medically meaningful concepts from medical images due to the huge gap between normal images and medical images.\nInspired by EAC [11], in this paper, we propose a frame- work named Lesion Concept Explainer (LCE), which uses attribution methods to guide existing SAM fine-tuned on a large amount of medical image data in combination with Shapley values for concept discovery and explanation. We implement LCE on the public breast ultrasound dataset BUSI [14] and the private breast ultrasound dataset FG-US-B using ResNet50 [15] as the target model.\nThe faithfulness metrics Insertion and Deletion [16] can reflect the consistency of the explanation with the decision process within the model. Since the effect of the explanation size is not taken into account, the original image is assumed to be the best explanation, even if it does not make sense, when evaluated by Insertion and Deletion. To solve the above problem, we add the explanation size as a weight in Insertion and Deletion and propose a new evaluation metric, Effect Score. Intuitively, an explanation is given to a person, and an understandable explanation is a good one. We combine the objective faithfulness evaluation of Effect Score and the human-centered evaluation of understandability to evaluate LCE and other popular explainability methods. The result shows that LCE outperforms most popular methods in terms of faithfulness. In the evaluation of understandability, LCE is preferred by professional sonographers.\nFinally, we explored the performance of LCE in the more meaningful fine-grained diagnosis of breast ultrasound. The results show that LCE consistently yields reasonable explana- tions regardless of fine-grained level. Notably, LCE is model- agnostic and can provide explanations for any diagnostic model of medical images, and the whole framework requires no additional cost, making it ideal for low-cost ultrasound images.\nThe contributions of this paper are summarized as follows:\n\u2022 We present a framework for explainability that enables understandable explanations of DNNs trained on low-cost ultrasound images.\n\u2022 We integrate the attribution method with the concept- based method, synthesizing the strengths of both methods and improving the quality of explanations.\n\u2022 We identify the shortcomings of popular faithfulness evaluation metrics and make adjustments accordingly.\n\u2022 We validate the stability of applying our framework in a fine-grained diagnostic task in breast ultrasound."}, {"title": "II. RELATED WORK", "content": "In recent years, the development of AI has focused on large- scale foundation models, resulting in a number of exceptional products [12], [17], [18]. These models, with billions or more parameters, exhibit robust generalization and problem-solving potential after extensive training on large datasets, making them versatile for various downstream tasks. Among them, Segment Anything Model (SAM) [12] has caused a sensation with its powerful zero-shot segmentation performance. More importantly, SAM has learned general concepts about objects, including those not seen during training.\nHowever, the direct application of SAM to the ultrasound image segmentation task is not ideal due to the significant differences between normal images and medical images [19]. To address this problem, some studies have proposed methods to improve SAM and make it applicable to the field of medical image segmentation [20], [21]. Of these, MedSAM [21] has fine-tuned SAM using a medical image dataset containing more than one million medical images with different diseases and modalities, and its excellent performance in medical image segmentation has been verified through extensive experiments. Similarly, SAMMed2D [20] used the largest medical image segmentation dataset to date and introduced Adapter [22] to fine-tune SAM. In this paper, we use existing SAM fine-tuned on medical images to explore the ability of SAM to discover concepts in medical images without additional training costs."}, {"title": "A. Segment Anything Model (SAM)", "content": null}, {"title": "B. Attribution methods", "content": "Attribution methods primarily use techniques such as heat maps [23] or saliency maps [24] to identify key regions that provide visual cues for model classification. Class Activation Mapping (CAM) and its derivative methods [23], [25]can generate heat maps to show the contribution of each pixel in an image to the classification. CAMs are often used to explain medical image models, for example, [26] used Grad- CAM [25] to analyze chest X-ray images and obtained clear visualizations of pleural effusions, and [27] used Multi-Layer CAM to localize Confocal Laser Endomicroscopy Glioma images.\nIn addition to methods such as CAM, which obtains sig- nificance scores from within the model, Local Interpretable Model-agnostic Explanations (LIME) [23] observes changes in model output by simplifying complex models and perturbing input data for local superpixel explanations. [16] used a randomly sampled mask to obtain pixel contributions. [28] in- troduced DeepLift to assign contribution scores by comparing neuron activations. [29] proposed IntGrad, which combines invariance and sensitivity. [30] incorporated Shapley value [13] into LIME and DeepLift to form a unified framework for explanation, SHAP. [5] integrated perturbation operations similar to LIME and SHAP to explain important features in the diagnosis of CT lung images. Similarly, [31] used LIME and SHAP to explain a diagnostic model for renal abnormalities. However, the explanations given by these attribution methods are not always understood by medical image specialists."}, {"title": "C. Concept-based methods", "content": "In contrast to attribution methods, concept-based methods prefer explanations that are consistent with real-world con- cepts. [7] proposed a framework to quantify the explainability of DNNs by evaluating the consistency between hidden units and semantic concepts. [8] introduced the Test of Concept Activation Vector (TCAV) to measure the sensitivity of the model to pre-defined concepts. [32] applied the TCAV to the detection of cardiac disease in cine-MRI using clinically known biomarkers as concepts. However, the pre-definition of a concept set is challenging in the field of medical image processing as it requires the intervention of experts.\nSome concept-based methods, which do not require hu- man intervention, can automatically extract concepts from images. Prototype-based methods [33] can guide the network to use representative concepts for decision-making. [9] used prototypes to explain histological image classification. [10] combined prototype learning with privileged information in thoracic CT diagnosis, balancing explainability and accuracy. [11] proposed Explain Any Concept (EAC), which uses zero- shot segmentation of SAM to discover concepts. Nevertheless, applying these methods to ultrasound images is challenging due to their low-cost and limited information compared to CT and MRI."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. SAM-based concept discovery for lesions", "content": "As shown in the first row of Fig. 1., a typical DNN black- box model f is processed through the softmax layer after receiving an image $I \\in [R^{w \\times h \\times c}]$ to obtain a probability vector of length k of the number of classification categories, where the largest probability corresponds to the classification result. Such a decision-making process is entirely closed-off, and we cannot discern what the input image has undergone within the black-box to arrive at the final classification result.\nNow, we try to use SAM [12] for concept discovery in the black-box. As a zero-shot segmentation model, given an image I and either a set of points $L_p$ of length n, a set of bounding boxes $L_b$ of length n, or no prompt provided (in \"everything\u201d mode), SAM can output a set of n concept masks $C = {C_1,C_2,..., C_n}$, where each $c_i$ may correspond to certain concepts in real-world. In other words, SAM can be seen as having the ability to discover concepts. Therefore, we redefine SAM as a concept discoverer $D_{SAM}$. Given the limited concept discovery capability of the original $D_{SAM}$ on medical images, the $D_{SAM}$ used in this paper is SAMMed2D, which is fine-tuned on a large medical image dataset (see Section 4. B for comparison results between different training versions).\nConcept discovery directly using $D_{SAM}$ does not result in meaningful and high-quality concept blocks on ultrasound images. This is because ultrasound images, as a low-cost im- age modality, contain relatively limited information compared to normal, CT, or MRI images. In contrast, many objects in normal images have specific concepts that are easy to discover. Therefore, we consider the use of attribution methods such as GradCAM [25] and LIME [34] as concept guides to facilitate the discovery of concepts containing specific lesions information. Algorithm 1 describes the whole process"}, {"title": "B. Concepts post-processing and Shapley value calculation", "content": "The concepts mask guided by $G_g$ and $G_l$ may have obvious overlaps, which can lead to unnecessary computational waste and explanation errors in the subsequent process. For each $C_i \\in C_o$, if the intersection between any two concepts is more than 90%, the concept with the smaller number of pixels is discarded until all elements in $C_o$ satisfy this condition. The explanation E, which contains the meaning of a lesion, is a subset of $C_o$. As with EAC [11], we use the Shapley value [13], [30] for the final explanation. The model's original prediction for the image will serve as the utility function u(T), where for concept $c_i$ its marginal contribution to the model's decision is defined as\n$\\phi_{c_{i}}(I) = E (u (T\\cup {c_{i}}) \u2013 u(T))$\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\begin{equation}\n\\phi_{c_{i}}(I) = E (u (T \\cup {c_{i}}) - u(T))\n\\end{equation}\n\\end{document}\nSince the number of concepts is determined by the hyperpa- rameters, there is no need to use Monte Carlo approximations to calculate Shapley values [11]. In addition, avoiding the use of surrogate models [11] avoids a decrease in the faithfulness of the explanation. Finally, the optimal explanation is defined as\n$E = argmax_{E \\subseteq C} \\Phi_{E}(I)$\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\begin{equation}\nE = \\underset{E \\subseteq C}{\\operatorname{argmax}} \\Phi_{E}(I)\n\\end{equation}\n\\end{document}\nWhen combining T, we observe that the concept of benign tumors is intuitively very similar to the black pixel blocks used for masking, as shown in Fig. 2. Black image have a 96% probability of being diagnosed as benign when the model does not suffer from overfitting problems. Therefore, there is no guarantee that the calculated Shapley Value faithfully reflects the marginal contribution of Ci when black masking is used. For this reason, we decided to replace the black pixels with randomly selected pixels from a normal distribution whose mean and variance matched those of the source dataset."}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": null}, {"title": "A. Experiment setting", "content": "1) Datasets: Breast Ultrasound Image (BUSI) [14] is a classification and segmentation dataset containing breast ul- trasound images. For classification labeling, there are three categories, normal, benign, and malignant, and the sample size for each category is shown in Table I.\nFine-grained Ultrasound Images Of The Breast (FG-US- B) is a private fine-grained classification dataset of breast ultrasound images, and the classification labels are all based on the pathological results of the puncture procedure. The total number of images is 1016 and the sample size for each category is shown in Table I.\nThe sample illustrations of the datasets used are shown in Fig. 3. The FG-US-B dataset was collected from the ultrasound department of the Affiliated Hospital of Southwest Jiaotong University (Chengdu Third People's Hospital).\n2) Implementation details: The main research aim of this paper is a new framework for explainability rather than the structure of a specific model. In fact, our framework is model-agnostic. We use ResNet50 [15] as the target model, where the images are resized to (224, 224) before being input into the model. During training, we apply random horizontal flipping and calculate normalization coefficients separately for the two datasets. The optimization process uses decoupled weight decay (AdamW) [35], with an initial learning rate of $5 \\times 10^{-3}$, gradually decreasing to $5 \\times 10^{-5}$ during fine-tuning. Each iteration uses a batch size of 8 samples. The implementation is carried out using the PyTorch framework (version 2.1.2 and CUDA 12.3) [36], and all experiments are performed on a GeForce RTX 3090 GPU. The classification accuracy of the target model on the test set after training is shown in Table I."}, {"title": "B. Evaluation metrics", "content": "We evaluate the methods in terms of both understandability and faithfulness. Faithfulness aims to respond to whether the explanation is the same as the model's decision-making process. Intuitively, a good explanation should be human- understandable [37]. Based on the principle of human-in-the- loop [38], we invite sonographers to perform an evaluation of understandability, see Section 4.E.\nThe Insertion and Deletion [16] are popular faithfulness metrics. The Insertion measures the importance of the super- pixels in constructing the whole image, while the Deletion quantifies the extent to which the probability decreases as important superpixels are gradually removed from the image.\nWe found that Insertion and Deletion could not reason- ably evaluate the faithfulness of EAC [11] on breast ultrasound images due to the inability to determine the number and size of concepts discovered. Although EAC performs well on Insertion (see the first row of Fig. 4), its evaluation is not reasonable. This is because E3 (EAC) shows almost the whole image, resulting in the best evaluation. In contrast, LIME [34] ranks lower, but not so low as to be meaningless. We consider the size of the visual explanation to be a key factor in evaluating the quality of the explanation. If the explanation is too large, taking up the same proportion of the whole image, then the explanation becomes meaningless. Evaluation on Deletion has similar results.\nFor the above reasons, we modify these two faithfulness metrics. Define Po as the number of pixels in the input image, and Ps denotes the average number of pixels in a set of explanation accumulation images. The weighted evaluation metrics Insertion and $Deletion_w$ are defined as\n$Insertion_{w} = \\frac{1}{\\frac{P_s}{P_o}} \\times Insertion$\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\begin{equation}\nInsertion_{w} = \\frac{1}{\\frac{P_s}{P_o}} \\times Insertion\n\\end{equation}\n\\end{document}\n$Deletion_{w} = \\frac{1}{\\frac{1-\\frac{P_s}{P_o}}{1}} \\times (1- Deletion)$\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\begin{equation}\nDeletion_{w} = \\frac{1}{\\frac{1-\\frac{P_s}{P_o}}{1}} \\times (1- Deletion)\n\\end{equation}\n\\end{document}\nConsidering both $Insertion_w$ and $Deletion_w$, the evalua- tion metric Effect Score is defined as\n$Effect Score = \\frac{Insertion_w + Deletion_w}{2}$\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\n\\begin{equation}\nEffect Score = \\frac{Insertion_w + Deletion_w}{2}\n\\end{equation}\n\\end{document}"}, {"title": "C. Comparison of SAMs trained on different data", "content": "The concept discovery capability of SAM is crucial to our framework. We compare the faithfulness of different DSAM, including SAMs with different encoders [18] and SAMS fine-tuned on different medical image datasets (SAMMed2D [20] and MedSAM [21]). To evaluate DSAM reasonably, we exclude Fi and Fg. The experimental results are summarized in Table II.\nThe original SAM (ViT-H version) has not been trained on a large amount of medical image data and has the best Insertion and Deletion. As discussed in Section 4.B, this is due to the fact that its explanation is the image itself. After re-evaluation using the Effect Score, these highly unreliable explanations are distinguished from the others. SAMMed2D is considered to be the best DSAM due to its fine-tuning on the largest medical image dataset to date. To maintain the best performance, LCE uses SAMMed2D as DSAM."}, {"title": "D. Comparison of different explainability methods", "content": "In addition to the EAC, which is the most similar to the LCE, we select six commonly-used baseline methods for comparison. These methods are all implemented by captum [39]. We first generate concepts from the input images using superpixels [40] and then calculate the faithfulness of these concepts. The comparison results are shown in Table III. Among all the methods, LCE has the highest Effect Score. On the BUSI dataset, it has an Effect Score of 0.5572, which is 5% higher than the second-ranked LIME. Similarly, on the FG-US-B dataset, it is 15% higher than the second- ranked method. This suggests that LCE consistently produces more faithful breast ultrasound explanations than the baseline method.\nWe present several explanations generated by different methods in Fig. 5. Compared to other methods, LCE always gives understandable explanations that are consistent with the lesion concept. For example, in the third row, EAC generates an explanation of very small super-pixels, and this explanation makes no sense. In contrast, other baseline methods generate explanations that are related to the tumor concept but are less specific than LCE. While maintaining a certain level of faithfulness, after concept guidance, LCE can discover meaningful concepts (e.g. the concept of a tumor) from low-cost ultrasound images instead of presenting whole images or meaningless almost completely black images."}, {"title": "E. Understandability evaluation from sonographers", "content": "Feature maps can also be used as explanations, but such explanations are not valid because humans cannot under- stand them. Many methods evaluate understandability through human-centered experiments [11], [34], [41]. In this paper, we select samples of benign, fibroadenoma, and mastitis for understandability evaluation.\nWe invite five experienced sonographers to evaluate the baseline methods and LCE. We provide each sonographer with the raw image, the model prediction, the ground truth, and seven explanations for each image. The order in which each explanation is presented is randomized, so the sonographers are unaware of which method each explanation comes from. They are required to rank the regions included in the seven explanations, with the explanations that contributed most to the diagnostic results and are easier to understand being ranked highest. The evaluation is completed when the sonographers are fully rested. The average evaluation of the five sonog- raphers is shown in Table IV. Based on the sonographers' ranking results, LCE clearly outperforms the other baseline methods. The majority of the explanations provided by the LCE are accepted by the sonographers. In particular, this shows that LCE contributes to the realization of AI that can be trusted by medical professionals."}, {"title": "F. Explanations at different diagnostic fine-grained levels", "content": "Fine-grained classifications are those with subtle differences within categories, and existing studies tend to classify breast tumors as benign or malignant only on a coarse-grained basis [42]. However, coarse-grained classifications serve a weak auxiliary role for sonographers, who can easily derive a diagnosis from several guidelines (e.g., TI-RADS [43]).\nTo test the performance of LCE on more significant fine- grained diagnosis, we create subsets of six different fine- grained levels by changing the number of categories using the FG-US-B dataset, and Table V shows the differences in the explanations generated by LCE as the change of fine- grained levels. When the model's diagnosis is incorrect, we can also find that its explanation becomes ambiguous, reflecting to some extent the reason for the incorrect diagnosis (e.g. failure to correctly identify the lesion)."}, {"title": "V. CONCLUSION", "content": "In this paper, we integrate the attribution method with concept-based method by proposing the Lesion Concept Ex- plainer (LCE) framework, which uses SAM fine-tuned on a large number of medical images for lesion concept discovery, to explain the decisions of DNNs. To address the shortcomings of the popular faithfulness evaluation metrics Insertion and Deletion, we modify them by using the size of the explanation as a weight and propose a new metric, Effect Score. We implement LCE on ResNet50 models trained on two breast ultrasound datasets (public and private). Compared to other popular methods, LCE is more effective in explaining breast ultrasound DNNs and performs best in the evaluation of objective faithfulness and human-centered understandability. Finally, we experiment with LCE at different fine-grained lev- els and show that it always faithfully provides understandable explanations. More importantly, we demonstrate that LCE can contribute to the development of medical AI by making DNNS in the medical domain more trustworthy."}]}