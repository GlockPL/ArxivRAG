{"title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting", "authors": ["Shaofei Cai", "Zihao Wang", "Kewei Lian", "Zhancun Mu", "Xiaojian Ma", "Anji Liu", "Yitao Liang"], "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs' visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page:\nhttps://craftjarvis.github.io/ROCKET-1.", "sections": [{"title": "1. Introduction", "content": "Pre-trained foundation vision-language models (VLMs) (Achiam et al., 2023; Team et al., 2023) have shown impressive performance in reason-ing, visual question answering, and task planning (Brohan et al., 2023; Cheng et al., 2024; Driess et al., 2023; Wang et al., 2023b), primarily due to training on internet-scale multimodal data. Recently, there has been growing interest in trans-ferring these capabilities to embodied decision-making in open-world environments. Existing approaches can be broadly categorized into (i) end-to-end and (ii) hierarchical approaches. End-to-end approaches, such as RT-2 (Brohan et al., 2023), Octo (Octo Model Team et al., 2024), LEO (Huang et al., 2023), and OpenVLA (Stone et al., 2023), aim to enable VLMs to interact with envi-ronments by collecting robot manipulation trajec-tory data annotated with text. This data is then tokenized to fine-tune VLMs into vision-language-action models (VLAs) in an end-to-end manner, as illustrated in Figure 2(a). However, collect-ing such annotated trajectory data is resource-intensive and difficult to scale. Moreover, intro-ducing the action modality risks compromising the foundational abilities of VLMs.\nHierarchical agent architectures typically con-sist of a high-level reasoner and a low-level pol-icy, which can be trained independently. In this architecture, the \u201ccommunication protocol\" be-tween components defines the capability lim-its of the agent. Alternative approaches (Driess et al., 2023; Wang et al., 2023a,b) leverage VLMs' reasoning abilities to zero-shot decompose tasks into language-based sub-tasks, with a separate language-conditioned policy executing them in the environment, refer to Figure 2(b). However, language instructions often fail to effectively con-vey spatial information, limiting the tasks agents can solve. For example, when multiple homony-mous objects appear in an observation image, dis-tinguishing a specific one using language alone may require extensive spatial descriptors, increas-ing data collection complexity and learning dif-ficulty for the language-conditioned policy. To address this issue, approaches like STEVE-1 (Lif-shitz et al., 2023), GROOT-1 (Cai et al., 2023b), and MineDreamer (Zhou et al., 2024) propose us-ing a purely vision-based interface to convey task information to the low-level policy. MineDreamer, in particular, uses hindsight relabeling to train an image-conditioned policy (Lifshitz et al., 2023) for interaction, while jointly fine-tuning VLMs and diffusion models to generate goal images that guide the policy, shown in Figure 2(d). Although replacing language with imagined images as the task interface simplifies data collection and policy learning, predicting future observations re-quires building a world model, which still faces challenges such as hallucinations, temporal in-consistencies, low-quality generation, and limited temporal scope.\nIn human task execution, such as object grasp-ing, people do not pre-imagine holding an object but maintain focus on the target object while approaching its affordance. When the object is obscured, humans rely on memory to recall its location and connect past and present visual scenes. This use of visual-temporal context en-ables humans to solve tasks effectively in novel environments. Building on this idea, we propose a novel communication protocol called visual-temporal context prompting, as shown in Figure 2(e). This allows users/reasoners to apply object segmentation to highlight regions of interest in past and current visual observations and convey interaction-type cues via a set of skill primitives. Based on this, we learn ROCKET-1, a low-level policy that uses visual observations and reasoner-provided segmentations as task prompts to pre-dict actions causally. Specifically, a transformer (Dai et al., 2019) models dependencies between observations, essential for representing tasks in partially observable environments. As a bonus fea-ture, ROCKET-1 can enhance its object-tracking capabilities during inference by integrating the state-of-the-art video segmentation model, SAM-2 (Ravi et al., 2024), in a plug-and-play fashion. Additionally, we propose a backward trajectory relabeling method, which efficiently generates segmentation annotations in reverse temporal order using SAM-2, facilitating the creation of training datasets for ROCKET-1. Finally, we develop a hierarchical agent architecture lever-aging visual-temporal context prompting, which perfectly inherits the vision-language reasoning capabilities of foundational VLMs. Experiments in"}, {"title": "2. Preliminaries", "content": "Offline Reinforcement Learning. We model the open-world interaction problem as a Markov De-cision Process (MDP) (O, A, P, C, M, R), where O and A represent the observation and action spaces, $P: O\\times A\\times O \\rightarrow \\mathbb{R}^+$ describes the environ-ment dynamics, C is the set of interaction types, and M is the segmentation mask space. The bi-nary reward function $R : O\\times A\\times C\\times M \\rightarrow \\{0,1\\}$ determines whether the policy has completed the specified interaction with the object indicated by the segmentation mask at each time step. The objective of reinforcement learning is to learn a policy that maximizes the expected cumulative reward, $E\\left[\\sum_{t=1}^{T}r_t\\right]$, where rt is the reward at time step t. Our proposed backward trajectory relabel-ing method ensures that each trajectory attains a positive reward based on current object segmen-tations. This allows us to discard the rewards and learn a conditioned policy $\\pi(a|o, c, m)$ directly us-ing behavior cloning. In the offline setting, agents do not interact with the environment but rely on a fixed, limited dataset of trajectories. This set-ting is harder as it removes the ability to explore the environment and gather additional feedback.\nVision Language Models. Vision-Language Mod-els (VLMs) are machine learning models capable of processing both image and language modal-ities. Recent advances in generative pretrain-ing have led to the emergence of conversational models like Gemini (Team et al., 2023), GPT-40 (Achiam et al., 2023), and Molmo (Deitke et al., 2024), which are trained on large-scale multi-modal data and can reason and generate human-like responses based on text and images. Models such as Palm-E (Driess et al., 2023) have demon-strated strong abilities in embodied question-answering and task planning. However, stan-dalone VLMs cannot often interact directly with environments. Some approaches use VLMs to generate language instructions for driving low-level controllers, but these methods struggle with expressing spatial information. This work focuses on releasing VLMs' spatial understanding in em-bodied decision-making scenarios. Molmo can accurately identify correlated objects in images using a list of (x, y) coordinates, as demonstrated in https://molmo.allenai.org."}, {"title": "3. Methods", "content": "Overview. Our work focuses on addressing com-plex interactive tasks in open-world environ-ments like Minecraft. We leverage VLMs' visual-language reasoning capabilities to decompose tasks into multiple steps and determine object in-teractions based on environmental observations. For example, the \u201cbuild nether portal\u201d task re-quires a sequence of block placements at spe-cific locations. A controller is also needed to map these steps into low-level actions. To con-vey spatial information accurately, we propose a visual-temporal context prompting protocol and a low-level policy, ROCKET-1. Pretrained VLMs process a sequence of frames o1:t and a language-based task description to generate object segmen-tations m1:t and interaction types c1:t, represent-ing the interaction steps. The learned ROCKET-1 $\\pi(a_t|o_{1:t}, m_{1:t}, c_{1:t})$ interprets these outputs to in-teract with the environment in real-time. In this section, we outline ROCKET-1 's architecture and training methods, the dataset collection pro-cess, and a pipeline integrating ROCKET-1 with state-of-the-art VLMs.\nROCKET-1 Architecture. To train ROCKET-1, we prepare interaction trajectory data in the for-mat: $\\tau = (o_1, m_1, c_1, a_1, \u00b7\u00b7\u00b7, o_T, m_T, c_T, a_T)$, where $o_t \\in \\mathbb{R}^{3\\times H \\times W}$ is the visual observation at time t, $m_t \\in \\{0,1\\}^{1\\times H \\times W}$ is a binary mask highlighting the object in ot for future interaction, ct \u2208 N denotes the interaction type, and at is the action. If both mt and ct are zeros, no region is highlighted at ot. As shown in Figure 3, ROCKET-1 is formal-ized as a conditioned policy, $\\pi(a_t|o_{1:t}, m_{1:t}, c_{1:t})$, which takes a sequence of observations and object-segmented interaction regions to causally predict actions. To effectively encode spatial information, inspired by Zhang et al. (2023), we concatenate the observation and object segmentation pixel-wise into a 4-channel image, which is processed"}, {"title": "Backward Trajectory Relabeling.", "content": "We seek to build a dataset for training ROCKET-1. The col-lected trajectory data t typically contains only observations and actions. To generate object segmentations and interaction types for each frame, we propose a hindsight relabeling tech-nique (Andrychowicz et al., 2017) combined with an object tracking model (Ravi et al., 2024) for ef-ficient data labeling. We first define a set of inter-actions C and identify frames where interaction events occur, detected using a pre-trained vision-language model, such as Achiam et al. (2023). Then, we traverse the trajectory in reverse or-der, segmenting interacting objects in frame t via an open-vocabulary grounding model (Liu et al., 2023). Finally, SAM-2 (Ravi et al., 2024) is used to track and generate segmentations for frames $t-1, t\u22122, ..., t \u2212 k$, where k is the window length.\nFor Minecraft, we use contractor data (Baker et al., 2022) from OpenAI, consisting of 1.6 billion frames of human gameplay. This dataset includes meta information for each frame, recording in-teraction events such as kill entity, mine block, use item, interact, craft, and switch, eliminating the need for vision-language models to detect events. We observed that interacting objects are often centered in the previous frame, allowing the use of a fixed-position bounding box and point with the SAM model for segmentation, replacing open-vocabulary grounding models. We also in-troduced an additional interaction type, navigate, where significant player displacement over a tra-jectory identifies the most prominent object in"}, {"title": "4. Results and Analysis", "content": "We first provide a detailed overview of the exper-imental setup, including the benchmarks, base-lines, and implementation details. We then ex-plore ROCKET-1 's performance on both short-horizon and long-horizon tasks. Finally, we con-duct comprehensive ablation studies to validate the rationale behind our design choices.\n4.1. Experimental Setup\nImplementation Details. Briefly, we present ROCKET-1 's model architecture, hyperparam-eters, and optimizer configurations in a Table 1. During training, each complete trajectory is divided into 128-length segments to reduce mem-ory requirements. During inference, ROCKET-1 can access up to 128 frames of past observations. Most training parameters follow the settings from prior works such as Baker et al. (2022); Cai et al. (2023b, 2024).\nEnvironment and Benchmark. We use the un-modified Minecraft 1.16.5 (Guss et al., 2019; Lin et al., 2023) as our testing environment, which accepts mouse and keyboard inputs as the action"}, {"title": "4.2. Performance on Short-Horizon Tasks", "content": "To quantitatively assess ROCKET-1 's perfor-mance on short-horizon tasks, we evaluated it on the Minecraft Interaction Benchmark, with re-sults as illustrated in Table 2. Since ROCKET-1 operates as a low-level policy, it requires a high-level reasoner to provide prompts within a visual-temporal context, driving ROCKET-1 's inter-actions with the environment. We tested two"}, {"title": "4.3. Performance on Long-Horizon Tasks", "content": "We compared hierarchical agent architectures based on different communication protocols: (1) language-based approaches, exemplified by DEPS (Wang et al., 2023b); (2) future-image-based methods, represented by MineDreamer (Zhou et al., 2024); (3) latent-code-based methods, as in OmniJarvis (Wang et al., 2024a); and (4) our proposed approach based on visual-temporal con-text, as illustrated in the Figure 5. For Mine-Dreamer, we used the planner provided by DEPS and MineDreamer as the controller to complete the long-horizon experiment. We evaluated these methods on seven crafting or mining tasks, each requiring long-horizon planning: obtaining a wooden pickaxe (3600), furnace (6000), shears (12000), diamond (24000), steak (6000), obsid-ian (24000), and pink wool (6000), where the numbers in parentheses represent the time limit for each task. In the first five tasks, the agent starts from scratch, while for the obsidian task, we provided an empty bucket and a diamond pick-axe in advance, and for the pink wool task, we provided shears beforehand. Taking the obsidian task as an example, the player must first locate a nearby water source, fill the bucket, find a nearby lava pool, pour the water to form obsidian, and finally switch to the diamond pickaxe to mine the obsidian. We found that our approach sig-"}, {"title": "4.4. What Matters for ROCKET-1?", "content": "We conduct ablation studies on short-horizon tasks of Minecraft Interaction benchmark: \u201cHunt right sheep ()\" and \u201cMine emerald ().\nSAM-2 Models. The SAM-2 model acts as a proxy segmentation generator when the high-level rea-soner fails to provide timely object segmentations. We evaluate the impact of different SAM-2 model sizes on task performance and inference speed, as shown in Table 4. Results indicate that with low-frequency prompts from the high-level rea-soner (Molmo 72B) at 1.5 (game frequency is 20), SAM-2 greatly improves task success rates. While \"sam2_hiera_large\" is the best, increasing the SAM-2 model size yields performance gains at the cost of higher time.\nInteraction-Type Information Fusion Location. We modified the visual backbone's input layer from 3 to 4 channels, allowing ROCKET-1 to integrate object segmentation information. For fusing interaction-type information, we explored two approaches: (1) keeping the object segmen-tation channel binary and encoding interaction types via an embedding layer for fusion in Trans-formerXL, and (2) directly encoding interaction"}, {"title": "5. Related Works", "content": "Instructions for Multi-Task Policy. The task instruction of a multi-task policy directly deter-mines the types of tasks the policy can solve. Most current approaches (Brohan et al., 2022, 2023; Cai et al., 2023a; Huang et al., 2023; Lynch et al., 2023) use natural language to describe task details and collect large amounts of text-demonstration data pairs to train a language-conditioned policy for interaction with the en-vironment. Although natural language can ex-press a wide range of tasks, it struggles to repre-sent spatial relationships effectively. Additionally, gathering text-annotated demonstration data is costly, limiting the scalability of these methods. Alternatives, such as Lifshitz et al. (2023); Majumdar et al. (2022); Sundaresan et al. (2024), use images to drive goal-conditioned policies, typ-ically learning through hindsight relabeling in a self-supervised manner. While this reduces the need for annotated data, future images are often insufficiently expressive, making it difficult to cap-ture detailed task execution processes. Methods like Cai et al. (2023b); Jang et al. (2022) propose using reference videos to describe tasks, offering strong expressiveness but suffering from ambigu-ity, which may lead to inconsistencies between policy interpretation and human understanding, raising safety concerns. Gu et al. (2023) sug-gests representing tasks with rough robot arm trajectories, enabling novel task completion but only in fully observable environments, limiting"}, {"title": "6. Conclusions", "content": "In this paper, we propose a novel hierarchical agent architecture to address the challenges of open-world interaction. To bridge spatial infor-mation gaps, we introduce a visual-temporal con-text prompting method that communicates in-tent between the high-level reasoner and the low-level policy. We develop ROCKET-1, an object-segmentation-conditioned policy responsible for real-time interaction with objects in the environ-ment. This architecture can also leverage SAM-2 plug-and-play to enhance the policy's object-tracking capabilities. Extensive experiments in Minecraft demonstrate that our approach success-fully harnesses the powerful visual-language rea-soning abilities of VLMs for embodied decision-making, achieving significantly superior open-world interaction performance compared to base-line methods."}, {"title": "7. Limitations", "content": "Although ROCKET-1 significantly enhances in-teraction capabilities in Minecraft, it cannot en-gage with objects that are outside its field of view or have not been previously encountered. For in-stance, if the reasoner instructs ROCKET-1 to eliminate a sheep that it has not yet seen, the rea-soner must indirectly guide ROCKET-1 's explo-ration by providing segmentations of other known objects. This limitation reduces ROCKET-1 's efficiency in completing simple tasks and neces-sitates frequent interventions from the reasoner, leading to increased computational overhead."}]}