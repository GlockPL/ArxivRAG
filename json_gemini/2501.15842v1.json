{"title": "Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness", "authors": ["Yue Yao", "Daniel Goehring", "Joerg Reichardt"], "abstract": "We study the Out-of-Distribution (OoD) generalization ability of three SotA trajectory pre-\ndiction models with comparable In-Distribution (ID) performance but different model designs.\nWe investigate the influence of inductive bias, size of training data and data augmentation strat-\negy by training the models on Argoverse 2 (A2) and testing on Waymo Open Motion (WO) and\nvice versa. We find that the smallest model with highest inductive bias exhibits the best OoD\ngeneralization across different augmentation strategies when trained on the smaller A2 dataset\nand tested on the large WO dataset. In the converse setting, training all models on the larger\nWO dataset and testing on the smaller A2 dataset, we find that all models generalize poorly,\neven though the model with the highest inductive bias still exhibits the best generalization abili-\nty. We discuss possible reasons for this surprising finding and draw conclusions about the design\nand test of trajectory prediction models and benchmarks.", "sections": [{"title": "1 Introduction", "content": "The robustness of trajectory prediction is essential for practical applications in auto-\nnomous driving. The advancement of trajectory prediction models is catalyzed through\npublic motion datasets and associated competitions, such as Argoverse 2 (A2) [1], and\nWaymo Open Motion (WO) [2]. These competitions establish standardized metrics and\ntest protocols and score predictions on test data that is withheld from all competitors and\nhosted on protected evaluation servers only. This is intended to objectively compare the\ngeneralization ability of models to unseen data.\nHowever, these withheld test examples still share similarities with the training samp-\nles, such as sensor setup, map representation, post-processing, geographic, and scenario\nselection biases employed during dataset creation. Consequently, the test scores reported\nin each competition are examples of In-Distribution (ID) testing. To effectively evaluate\nmodel generalization, it is essential to test models on truly Out-of-Distribution (OoD)\ntest samples, such as those from different motion datasets.\nWe investigate model generalization across two large-scale motion datasets [3]: Argo-\nverse 2 (A2) and Waymo Open Motion (WO). The WO dataset, with 576k scenarios, is\nmore than twice the size of A2, which contains 250k scenarios. As model generalization"}, {"title": "2 Related Work and Benchmark Models", "content": "The structure and rules of individual datasets and accompanying competitions heavi-\nly influence the design choices of trajectory prediction models in the literature. Deep\nlearning-based models often directly ingest data in the format the dataset is stored in.\nRecent studies employ sequence-based representation, e.g., sequences of data points, as the\nmodel's input and output [5, 6]. This approach effectively captures diverse information,\nincluding agent trajectories and road geometries, while aligning with dataset measurement\nformats. However, it has notable drawbacks: the representation introduces high redundan-\ncy, and its computational cost scales with the length and temporal/spatial resolution of\nthe trajectories and road geometries. Furthermore, the high flexibility of this approach\ncan capture measurement noise and outliers, potentially leading to physically infeasible\npredictions.\nSome previous works explored the possibility of using polynomial representations for\npredictions [7, 8]. Su et al. [8] highlight the temporal continuity of this representation,\ni.e., the ability to provide arbitrary temporal resolution. Reichardt [9] argues for using\npolynomial representations to integrate trajectory tracking and prediction into a filte-\nring problem. Polynomial representations restrict the kind of trajectories that can be\nrepresented and introduce bias into prediction systems. This limited flexibility is gene-\nrally associated with greater computational efficiency, smaller model capacity, and hence\nbetter generalization."}, {"title": "2.1 Data Representation", "content": "The structure and rules of individual datasets and accompanying competitions heavi-\nly influence the design choices of trajectory prediction models in the literature. Deep\nlearning-based models often directly ingest data in the format the dataset is stored in.\nRecent studies employ sequence-based representation, e.g., sequences of data points, as the\nmodel's input and output [5, 6]. This approach effectively captures diverse information,\nincluding agent trajectories and road geometries, while aligning with dataset measurement\nformats. However, it has notable drawbacks: the representation introduces high redundan-\ncy, and its computational cost scales with the length and temporal/spatial resolution of\nthe trajectories and road geometries. Furthermore, the high flexibility of this approach\ncan capture measurement noise and outliers, potentially leading to physically infeasible\npredictions.\nSome previous works explored the possibility of using polynomial representations for\npredictions [7, 8]. Su et al. [8] highlight the temporal continuity of this representation,\ni.e., the ability to provide arbitrary temporal resolution. Reichardt [9] argues for using\npolynomial representations to integrate trajectory tracking and prediction into a filte-\nring problem. Polynomial representations restrict the kind of trajectories that can be\nrepresented and introduce bias into prediction systems. This limited flexibility is gene-\nrally associated with greater computational efficiency, smaller model capacity, and hence\nbetter generalization."}, {"title": "2.2 Data Augmentation", "content": "In competition settings, one or more agents in a scenario are typically designated as\nfocal agents, and predictions are scored exclusively for these agents. However, training the\nmodel exclusively with the focal agent's behavior fails to exploit all available data. To\naddress this, predicting the future motion of non-focal agents is a typical augmentation\nstrategy for training. As there are many more non-focal agents than focal agents, another\nimportant design decision involves determining how to balance the contributions of focal\nand non-focal agent data.\nWe select two open-sourced and thoroughly documented SotA models: Forecast-MA\u0415\n[5] (FMAE) and QCNet [6], with nearly 1900k and 7600k parameters, respectively. As\nsummarized in Table 1, both models employ sequence-based representation but employ\ndifferent strategies in dealing with non-focal agents:\nHeterogeneous Augmentation: FMAE follows the prediction competition protocol\nand prioritizes focal agent prediction. Thus, agent history and map information are\ncomputed within the focal agent's coordinate frame. Compared to the multi-modal\nprediction of the focal agent, FMAE only outputs uni-modal prediction for non-focal\nagents. The error of focal agent predictions is weighed higher than for non-focal agents\nin loss function.\nHomogeneous Augmentation: QCNet does not focus on the selected focal agent and\nproposes a more generalized approach. It encodes the information of agents and map\nelements in each agent's individual coordinate frame. It outputs multi-modal predictions\nfor focal and non-focal agents alike. This approach ensures a consistent prediction task\nfor all agents and enhanced model generalization. The prediction error of focal and\nnon-focal agent predictions is weighed equally in the loss functions."}, {"title": "3 Dataset Homogenization", "content": "To work around inconsistencies in data formats and prediction tasks between datasets,\nwe inherit the homogenization protocol from the one proposed in prior work [3] with\na different prediction horizon. This adopts the following settings for both datasets and\ndetails are summarized in Table 2:\n\u2022 History Length: We set the history length to 5 seconds (50 steps) as in A2.\n\u2022 Prediction Horizon: All models are trained with 4.1s prediction due to the limited\nrecording length of training data in WO.\n\u2022 Map Information: We exclude boundary information and the label of lane segments in\njunctions due to the information's absence in WO. Only lane segments and crosswalks\nare considered map elements in homogenized data due to their availability in both\ndatasets.\n\u2022 Focal Agent: We take the same focal agent as labeled in A2. From WO, only the first\nfully observed, non-ego agent in the list of focal agents is chosen. As the list of focal"}, {"title": "4 Experimental Results", "content": "All models are trained on WO from scratch using the original hyperparameters. The WO\ndataset features a more complex map structure, with more lanes and map points than\nA2. This leads to \u201cout of memory\u201d issue on our GPU cluster with 192 GB of RAM when\ntraining QCNet with its default settings. \u03a4o fit QCNet on the available training hardware,\nwe had to limit the scenario complexity for QCNet to process up to 50 agents and 80 map\nelements closest to the ego vehicle per scene.\nFor ID testing, we use the official benchmark metrics, including minimum Average\nDisplacement Error (minADEK) and minimum Final Displacement Error (minFDEK),\nfor evaluation. The metric minADEk calculates the average Euclidean distance between\nthe ground-truth trajectory and the best of K predicted trajectories across all future time\nsteps, while the minFDEK measures the final-step error, focusing on long-term accuracy.\nFor OoD testing, we also propose \u2206minADEK and \u2206minFDEK as the difference of\ndisplacement error between ID and OoD testing to measure model robustness. In accor-\ndance with standard practice, K is selected as 1 and 6."}, {"title": "4.1 Experiment Setup and Metrics", "content": "All models are trained on WO from scratch using the original hyperparameters. The WO\ndataset features a more complex map structure, with more lanes and map points than\nA2. This leads to \u201cout of memory\u201d issue on our GPU cluster with 192 GB of RAM when\ntraining QCNet with its default settings. \u03a4o fit QCNet on the available training hardware,\nwe had to limit the scenario complexity for QCNet to process up to 50 agents and 80 map\nelements closest to the ego vehicle per scene.\nFor ID testing, we use the official benchmark metrics, including minimum Average\nDisplacement Error (minADEK) and minimum Final Displacement Error (minFDEK),\nfor evaluation. The metric minADEk calculates the average Euclidean distance between\nthe ground-truth trajectory and the best of K predicted trajectories across all future time\nsteps, while the minFDEK measures the final-step error, focusing on long-term accuracy.\nFor OoD testing, we also propose \u2206minADEK and \u2206minFDEK as the difference of\ndisplacement error between ID and OoD testing to measure model robustness. In accor-\ndance with standard practice, K is selected as 1 and 6."}, {"title": "4.2 ID Testing Results", "content": "We present our new ID results in the upper section of Table 3 and reference the ID results\nfrom our previous study in the lower section for comparison. In the upper section, all\nmodels are trained on the homogenized WO training split and tested on the homogenized\nWO validation split. As in prior work, the QCNet results serve as the reference for relative\ncomparisons, i.e., as 100%.\nAs highlighted in [10], the WO dataset features a significantly larger volume of data\nand higher variance, presenting challenges for small models for training and inference.\nHowever, with the much smaller model size (345k), both EP-F and EP-Q outperform\nFMAE and achieve comparable ID performance to QCNet in minADE\u2081 and minFDE1\nfor the new ID results. Despite limiting the scenario complexity for QCNet, it still achieves\nthe best ID performance in 3 out of 4 metrics.\nAlso, note that all models perform better on WO than on A2 in absolute error under all\naugmentation settings in ID tests, except the minFDE6 of FMAE increase from 0.792m\nto 0.829m. The better ID performance on WO can be attributed to the larger size of the\nWO dataset, the relative simplicity of WO scenarios, or a combination of both factors."}, {"title": "4.3 OoD Testing Results", "content": "Figure 2 compares our new OoD results (left) with our previous OoD results (right). In\nboth cases, we evaluate model robustness and generalization using the absolute and rela-\ntive increases in prediction error, e.g. AminADEK and AminFDEK. We will present the"}, {"title": "4.3.1 Augmentation Strategy", "content": "Our previous study (right side of Figure 2, originally) showed that heterogeneous aug-\nmentation yields only marginal improvements in robustness for OoD testing. In our new\nexperiments (left side of Figure 2, originally), this trend continues for both EP-F and\nFMAE, indicating consistent results with respect to heterogeneous augmentation.\nBased on our prior findings, we expect models employing homogeneous augmentati-\non to exhibit a notable improvement in generalization. The results for EP-Q align with\nthis expectation, as AminADE6 decreases from +0.310m (83.8%) to +0.252m (70.2%)\ncompared to EP-noAug. However, QCNet shows contrasting results with worse robust-\nness compared to QCNet-noAug, with AminADE6 increasing from +0.349m (88.4%) to\n+0.351m (102.0%). The performance of QCNet can be attributed to the limited scenario\ncomplexity, as discussed in Section 4.1."}, {"title": "4.3.2 Data Representation", "content": "The advantage of using polynomial representation for model generalization remains evi-\ndent for EP-Q, with a significant decrease in AminFDE6 from +0.516m (74.1%) to\n+0.394m (49.1%) compared to QCNet. Moreover, EP-Q achieves the lowest prediction\nerror in the new OoD testing across all benchmarks.\nIn contrast, while EP-F and EP-noAug still demonstrate improvement in robustness\ncompared to sequence-based models, the gains are only marginal. This is reflected in the\nAminFDE6 between EP-F (+0.508m (61.9%)) and FMAE (+0.540m (65.1%))."}, {"title": "4.3.3 Comparison with Expectations", "content": "Comparing the new and previous OoD results, visualized in the left and right parts of Fi-\ngure 2, respectively, our empirical observations diverge significantly from our expectations\noutlined in Section 1. For instance, FMAE does not exhibit any improvement in robust-\nness, as indicated by the nearly identical AminADE\u2081 across both settings. Moreover,\nQCNet and EP variants exhibit significantly poorer generalization under the new set-\ntings when trained on WO. Notably, EP-Q's AminADE\u2081 increases from -0.022m (1.9%)\nin previous OoD results to +0.643m (78.3%) under the new settings. Drawing from our\nexperimental observations and previous work on dataset characteristics, we provide two\npotential reasons:\nComplexity of Prediction Task: Creators of both datasets mined their recor-\ndings to identify challenging scenarios and focal agents for prediction [1, 2]. Agent's\nmotion becomes particularly difficult to predict when they accelerate, brake, or turn\ndue to interactions with other agents and map - scenarios that simple models such\nas constant velocity cannot easily capture. While we maintain the history length of\nA2 for homogenized datasets, we increase the history length of WO from 1.1 second\nto 5 seconds. This adjustment potentially moves the challenging behaviors into the\nhistorical data, effectively making the prediction task less complex for WO. As a\nresult, models trained on WO with these less challenging prediction tasks tend to"}, {"title": "5 Conclusion and Future Directions", "content": "Our work emphasizes the importance of evaluating trajectory prediction models beyond\nIn-Distribution (ID) performance, with a focus on Out-of-Distribution (OoD) robustness\nfor autonomous driving. We systematically investigate the ID and OoD performance of\nthree state-of-the-art prediction models and demonstrate the improved model robustness\nof our proposed EP-Q model with polynomial data representation and homogeneous data\naugmentation.\nHowever, our findings show that model robustness is influenced by factors beyond\nmodel architecture and design choices. Contrary to our initial hypothesis, models trained\non the larger Waymo Open Motion dataset showed reduced robustness when tested on\nthe smaller Argoverse 2 dataset. This surprising result highlights the complex relationship\nbetween dataset properties, model design choices, and generalization performance. Our\nwork outlines two potential factors for future investigation: the complexity of the pre-\ndiction task and dataset noise levels. Our results outline the importance of OoD testing\nfor prediction models and emphasize that improving OoD robustness requires a deeper\nunderstanding of both model design and dataset properties, rather than simply increasing\ntraining data volume."}, {"title": "Appendix A: Complexity of Prediction Task", "content": "We analyze the complexity of the prediction task in the Waymo Motion dataset under\nvarying history lengths, corresponding to different start times tstart \u2208 [1.1s, 5s]. Note that\ntstart = 1.1s corresponds to the original setting of the Waymo Motion dataset, while\ntstart = 5s reflects the new setting introduced by our homogenization protocol.\nThe complexity is evaluated by quantifying how much the agent's motion deviates from\na constant velocity model. Specifically, we measure the longitudinal and lateral deviations\nbetween the start position pstart and end position pend of the ground truth trajectory,\nnormalized by the constant velocity model over a 4.1-second prediction horizon:\n\nd = \\frac{(p_{end} - p_{start})R_{start}}{v_{start} * 4.1s} \\newline \n(1)\n\nwhere Rstart is the 2 \u00d7 2 rotation matrix determined by the agent's heading at tstart\nand vstart denotes the velocity magnitude at time tstart. d is a 2D vector of normalized\nlongitudinal and lateral distances. A value of d = [1,0] indicates that agent's motion\nstrictly follows the constant velocity model. In contrast, larger deviations from [1,0] reflect\ngreater divergence from constant velocity, signifying higher prediction complexity.\nWe visualize the distribution of d for tstart \u2208 [1.1s, 5s] of Waymo Motion validation\nset in Figure 3. Compared to the distribution of d at tstart = 5s, the distribution at\ntstart = 1.1s is broader, indicating that the prediction task with a 1.1-second history is\nmore complex than with a 5-second history."}]}