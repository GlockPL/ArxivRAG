{"title": "DiPT: ENHANCING LLM REASONING THROUGH DIVERSIFIED\nPERSPECTIVE-TAKING", "authors": ["Hoang Anh Just", "Mahavir Dabas", "Lifu Huang", "Ming Jin", "Ruoxi Jia"], "abstract": "Existing work on improving language model reasoning typically explores a single solution path, which\ncan be prone to errors. Inspired by perspective-taking in social studies, this paper introduces DiPT, a\nnovel approach that complements current reasoning methods by explicitly incorporating diversified\nviewpoints. This approach allows the model to gain a deeper understanding of the problem's context\nand identify the most effective solution path during the inference stage. Additionally, it provides a\ngeneral data-centric AI recipe for augmenting existing data to improve their quality for fine-tuning.\nOur empirical results demonstrate that DiPT can be flexibly integrated into existing methods that\nfocus on a single reasoning approach, enhancing their reasoning performance and stability when\npresented with paraphrased problems. Furthermore, we illustrate improved context understanding\nby maintaining the model's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning with data enriched\nwith diverse perspectives can boost the reasoning capabilities of the model compared to fine-tuning\nwith raw data alone.", "sections": [{"title": "Introduction", "content": "Correct reasoning steps are important for language models to achieve high performance on many tasks, such as\ncommonsense reasoning, question answering, and mathematical problem-solving [Wei et al., 2022, Kojima et al., 2022,\nSuzgun et al., 2022]. One way to elicit reasoning is through the chain-of-thought (CoT) method Wei et al. [2022],\nKojima et al. [2022], which asks the model to provide step-by-step reasoning. Another approach encourages the model\nto provide similar problems Yasunaga et al. [2024] as the query, indirectly compelling the model to first understand the\noriginal query. Similarly, repeating and rephrasing the query Deng et al. [2023], Mekala et al. [2023] requires the model\nto first understand the problem and then modify the query into its own words. This rephrasing might help simplify\nthe problem for the model. Additionally, reasoning can be generated by indirectly providing reasoning examples in\ndemonstrations, referred to as in-context learning (ICL) Brown et al. [2020], Min et al. [2022], Xie et al. [2021].\nWhile these methods have demonstrated significant performance improvements, language models are still prone to\nerrors due to incorrect context understanding or analytical steps. Furthermore, they are subject to instability when\nrequests are paraphrased. This instability is particularly concerning in the context of adversarial prompts, where recent\nresearch [Zou et al., 2023, Zeng et al., 2024] has shown that adversaries can intentionally rewrite prompts to coax\nsafety-aligned language models into generating objectionable content that they would not generate otherwise. Although\nthe exact source of these errors is a subject of active research Kalai and Vempala [2024], we observe a commonality\namong these methods: they often generate an answer to the problem by considering only a single solution path, or"}, {"title": "Related Work", "content": "Improving Reasoning in the Inference Time. Numerous single-prompt (0-shot) methods have emerged to improve\nthe model's reasoning capabilities. One such method is (automatic/0-shot) chain-of-thought (CoT) Wei et al. [2022],\nKojima et al. [2022], which instructs the model to provide a step-by-step explanation of the answer. This can be achieved\nby either incorporating examples with such explanations or by introducing an additional sentence in the prompt, \"Let's\nthink Step by Step.\" Plan-and-solve (PS) method Wang et al. [2023a] is the extension of the CoT reasoning, which asks\nthe model to first come up with the plan before solving the problem in a step-by-step manner. Recent work also derives\ntheoretical analysis Feng et al. [2023] explaining how the transformers with chain-of-thought reasoning can solve\nmathematical problems that otherwise would not be possible without outputting the reasoning by the model. Another\nline of work Mekala et al. [2023], Deng et al. [2023] attempts to involve the model to simplify the query before actually\nsolving the problem by asking the model to rephrase the query in the model's simplified language. With a simplified\nquery, the model can better understand the problem and proceed to solve the task. Analogical reasoners Yasunaga et al.\n[2024], on the other hand, instructs the LLM to self-generate similar examples to the query as demonstrations and\nthen solve the problem. Overall, the common limitation of these methods is that they do not regulate how reasoning\nshould be performed and, by default, adopt a single solution path. This can be attributed to various factors, such\nas the simplicity and computational efficiency of generating a single solution path, the lack of explicit rewards for\ndiversity in the reasoning process in current evaluation metrics, and the assumption that a correct solution path indicates\nsufficient problem understanding. While investigating the mechanisms that encourage the generation of a single\nsolution path is beyond the scope of this paper, we focus on studying the empirical benefits of incorporating multiple\nsolution paths for both inference and training stages. Improving the effectiveness of single prompting naturally involves\nincorporating multiple prompts, such as CoT self-consistency Wang et al. [2022], least-to-most prompting Zhou et al.\n[2022], (probabilistic) tree-of-thoughts (ToT) Yao et al. [2023], Cao et al. [2023], or graph-of-thoughts Besta et al.\n[2023]. These methods enhance responses by leveraging diverse model outputs. While diversified perspective-taking\nshows promise in improving reasoning based on multiple prompts by increasing the accuracy of individual prompts,\nthis paper focuses on integrating diverse perspectives into zero-shot methods as a proof of concept.\nImproving Data Quality for Targeted Instruction Tuning. Recent advancements in instruction tuning have enhanced\nthe task-specific capabilities of large language models (LLMs) [Peng et al., 2023, Zhang et al., 2023]. Existing work\nhas developed various techniques to identify the most relevant data from these extensive datasets to effectively develop\nspecific capabilities [Albalak et al., 2023, Xia et al., 2023, 2024, Xie et al., 2024, Kang et al., 2024]. However, these\nmethods all focus on pruning samples to distill the most informative pieces from a dataset. Instead, we explore how to\nenrich the information content of each sample and examine its impact. Others investigate rewriting individual samples to\nimprove their quality, such as incorporating in-context examples [Longpre et al., 2023] and chain-of-thought reasoning\ninto the instruction tuning dataset [Kim et al., 2023, Chai et al., 2024]. By contrast, we explore whether incorporating\nperspective-taking data can further enhance instruction tuning performance."}, {"title": "DiPT: Diversified Perspective-taking", "content": "Now, we delve into the specifics of incorporating diversified perspective-taking into the inference and fine-tuning stages\nof language models."}, {"title": "DiPT as an Inference-Time Reasoning Enhancement Tool", "content": "The key idea behind DiPT applied to inference time is to prompt the model to consider multiple perspectives or solution\npaths for a given problem before attempting to solve it. This explicit consideration of diverse perspectives is a crucial\ndistinction between DiPT and previous approaches where perspective-taking is not explicitly called for and does not\noccur most of the time in the generation.\nIn a standard case, for a given problem, a perspective would be implicitly invoked and a reasoning method would\nbe applied to solve it (e.g., with CoT reasoning). In the case of perspective-taking, multiple perspectives would be"}, {"title": "DiPT for Improving Training Data Quality", "content": "The key idea behind applying DiPT to improve data quality is to augment the instruction dataset with rationales from\nmultiple solution paths. The intuition behind this approach is that learning from rationales leads to a better mastery\nof relevant skills and knowledge required to solve a question. When a model is trained on data accompanied by\nexplanations from different perspectives, it can better understand the underlying concepts and principles, rather than"}, {"title": "Experiment", "content": "This section presents experiments designed to investigate the following questions: (1) How does the integration of\nperspective-taking into existing reasoning methods impact their performance across various tasks? We evaluate its effect\non both the accuracy and robustness to paraphrased problem statements. (Section 4.1) (2) What novel applications\ncan be developed to harness DiPT's advanced context understanding and accurate reasoning capabilities? Specifically,\nwe will explore its potential in harmful query moderation and dataset error detection. (Section 4.2). (3) How does\nfine-tuning models on datasets enriched with perspective-taking affect their performance on both in-distribution and\nout-of-distribution tasks? (Section 4.3).\nBy addressing these questions, we aim to provide a comprehensive evaluation of the proposed approach and offer\ninsights into its effectiveness, versatility, and generalizability."}, {"title": "DiPT Integration Impact on Inference", "content": "To understand the impact of perspective-taking on reasoning, we demonstrate the effect of adding DiPT to diverse\nreasoning methods. We considered three existing methods: CoT, which performs step-by-step reasoning; Rephrase and\nRespond (RaR), which rephrases and expands the question; and Analogical Reasoners (ANL), which self-generates\nexamples similar to the problem. This diverse set allows us to assess the generalizability of DiPT across different\nreasoning paradigms. We emphasize that the goal is not to exhaustively evaluate DiPT with every state-of-the-art\nmethod. Instead, our focus is to understand the specific impact of perspective-taking on reasoning performance.\nExperimental Setup. We perform inference-stage experiments on 7 tasks: AG News, CosmosQA, RTE, SST-5,\nSVAMP, TREC, and TruthfulQA. For AG News, SST-5, and TREC, we measure the Top-2 accuracy, as it is possible\nfor an example to belong to multiple classes. For all other tasks, we apply Top-1 accuracy. We refer the reader to\nAppendix A for further details on datasets. We evaluate performance over 300 test examples and report the average after\n3 runs. In the main paper, we report results on the GPT-4-Turbo (November) model Achiam et al. [2023], while we\nprovide results on the open-weight model, Mistral7B-Instruct-v0.1 Jiang et al. [2023], in Appendix B.3. Additionally,\nwe provide each DiPT prompt in Appendix A.4. We report 0-shot results of the target model with standard prompting; for\neach reasoning method, we report results when prompting with and without perspective-taking (DiPT+<Method-Name>)\nand the difference in the performance (\u0394).\nResult on Accuracy Improvement. In Table 1, we observe that adding perspective-taking to each of the reasoning\nmethods improves performance in most cases with even 6% increase for CoT in the TREC dataset. We observe\nperformance increases for all cases except for the analogical reasoning with the RTE dataset, where the performance\nmight have reached its peak due to potential labeling errors within the dataset. We will analyze these errors in detail\nin Section 4.2. To gain deeper insights into the positive quantitative results, Figure 3 presents an illustrative example.\nThis example showcases how explicit exploration of multiple solution paths, enabled by DiPT in conjunction with CoT\nprompting, allows the language model to self-correct. Standard prompting and CoT prompting typically guide the\nmodel along a single path, increasing its susceptibility to errors (as seen in Figure 3, where the answers following their\ncorresponding solution paths are incorrect). Conversely, DiPT prompts the model to explore alternative solutions. This\ncapability allows for robust analysis and comparison of answers, ultimately leading the model to identify and correct\nerrors, resulting in a correct final answer (shown in full in Appendix B.4).\nResult on Stable Generation. While current reasoning methods enhance the model's capabilities, they may generate\nerroneous reasoning steps across various problem formulations, as noted in studies by Wang et al. [2023b], Lanham et al.\n[2023], Turpin et al. [2024]. We examine whether incorporating perspective-taking into existing methods can enhance\nstability across different problem paraphrases, thus improving method reliability. To assess this, we evaluate each"}, {"title": "Applications of DiPT Integration", "content": "Safety Moderation. The enhanced context understanding achieved by considering multiple viewpoints is beneficial\nin various application contexts. Here, we demonstrate a specific example of adversarial prompting, where attackers\nmanipulate harmful queries that the model initially rejects, making them appear safe to the model and eliciting\ninappropriate responses. This issue arises when the model fails to fully comprehend the input context and naively\nfollows the prompt. We demonstrate that perspective-taking enables the model to shift perspectives during output\ngeneration, better grasping the user's intent.\nFigure 4 shows an example where the model successfully prevents harmful content generation, which would otherwise\noccur. More examples can be found in Appendix B.6. Additionally, Table 3 provides quantitative results comparing our\nmethod's performance against various defense mechanisms, such as paraphrasing, retokenizing [Jain et al., 2023], or\nsummarizing [Zeng et al., 2024]. We consider multiple representative attacks, including prompt automatic iterative\nrefinement (PAIR) [Chao et al., 2023], which leverages the LLM to automatically refine the adversarial prompts; greedy\ncoordinate gradient (GCG) [Zou et al., 2023], optimizes prompts with adversarial suffixes to surpass defenses; and\npersuasive adversarial prompts (PAP) [Zeng et al., 2024], which tries to surpass the model by leveraging persuasive\ntechniques in the prompts.\nOur method achieves a 0% attack success rate (ASR) for PAIR and GCG attacks, where ASR calculation is based on\nkeyword matching Zou et al. [2023]. While the ASR for PAP is above 0%, the generated output might not necessarily\nbe harmful (e.g., a superficial representation of an imaginary weapon for a story). We verify this with a context-"}, {"title": "Impact of DiPT-Enriched Fine-Tuning", "content": "In addition to enhancing performance during the inference stage, reasoning methods have also been utilized for\ninstruction tuning large language models to improve their ability to follow instructions. Techniques such as chain-of-\nthought [Kim et al., 2023, Chai et al., 2024] and in-context learning [Longpre et al., 2023] have been successfully\nincorporated into various datasets for model tuning. In this study, we explore whether data incorporating perspective-\ntaking can be beneficial for model training. Specifically, we concentrate on chain-of-thought data enriched with\nperspective-taking."}, {"title": "Conclusion", "content": "In this work, we explore the impact of perspective-taking on reasoning in language models. We investigate whether\nadding diversified perspective-taking to current reasoning methods can enhance model performance. Our findings show\nthat perspective-taking in generating reasoning improves the model's understanding of problem context, leading to\nbetter answers through corroboration of alternative solutions. Instruction-tuning the model with perspective-taking\ndata further enhances its capabilities compared to chain-of-thought data. We demonstrate the applications of advanced\ncontext-understanding capabilities enabled by perspective-taking in the safety and data quality refinement context."}, {"title": "Limitations", "content": "Despite the improved reasoning capabilities, incorporating diverse perspectives in text generation comes with the cost\nof extra time. While there are high-stake applications where reasoning accuracy outweighs time costs, there are also\nscenarios where time constraints might be an important consideration, particularly in real-time applications of LLMs.\nTo address this issue, one potential solution is to adopt an adaptive perspective generation approach. In this approach,\nthe model dynamically adjusts the number of perspectives generated based on the complexity of the problem or the\nconfidence in the initial answer. Another potential fix is to incorporate diverse perspectives during the training phase\nand then distill the insights gained from multiple perspectives into a more compact model that does not explicitly\ngenerate multiple perspectives during inference. However, the effectiveness of these approaches may vary depending on\nthe specific application and the characteristics of the LLM being used. We believe that the in-depth exploration of these\nideas is a promising direction for future research."}, {"title": "Ethical Considerations", "content": "As our method is applied in the model output moderation, it is important to consider the consequences of this mechanism.\nOn one hand, we believe our method can improve the model's response. However, at the same time, it also controls"}, {"title": "Appendix A Experimental Details & Tasks", "content": "A.1 Tasks for Inference Stage\nAG News (AG's News Corpus) Zhang et al. [2015]. The AG News dataset is a collection of news articles categorically\nlabeled into four classes (World, Sports, Business, and Science/Technology), providing a resource for text classification\nand topic modeling tasks. As news can belong to more than one category, we use top-2 accuracy.\nSST-5 (Stanford Sentiment Treebank) Socher et al. [2013]. The SST-5 dataset is a sentiment analysis dataset\nconsisting of movie reviews categorized into five sentiment classes, including very negative, negative, neutral, positive,\nand very positive. We use a top-2 accuracy across methods because a sentiment might lie between 2 neighboring classes\ndue to interpretation.\nDBPedia Auer et al. [2007]. The DBpedia dataset is a knowledge base extracted from Wikipedia, representing\nstructured information about a wide range of entities, including persons, places, organizations, and abstract concepts.\nWe use top-1 accuracy.\nCosmosQA (Commonsense Machine Comprehension) Huang et al. [2019]. The CosmosQA dataset is a reading\ncomprehension dataset requiring contextual commonsense reasoning. The questions are posed as multi-choice problems\nthat ask about likely causes or effects of events. We use top-1 accuracy.\nTREC (Text REtrieval Conference) Li and Roth [2002], Hovy et al. [2001]. The TREC dataset is a question type\nclassification dataset, which contains 6 coarse class labels. We use top-2 accuracy as the question type might belong to\nmore than one category.\nSVAMP (Simple Variations on Arithmetic Math word Problems) Patel et al. [2021a]. The SVAMP dataset is\nconsists of elementary-level math word problems. The dataset consists variations of the problems to test the model's\nsensitivity to question understanding. Since the provided dataset is a single-answer dataset, we created three neighboring\nanswers in addition to the groundtruth answer to make the problems multi-choice. We use top-1 accuracy.\nTruthfulQA Lin et al. [2022]. The TruthfulQA dataset is used to measure the truthfulness of the model's output\ngeneration. These problems are prone to be incorrectly answered if fallen into wrong beliefs and require correct\npretrained information to be answered. We use top-1 accuracy.\nRTE (Recognizing Textual Entailment) Cooper et al. [1996], Dagan et al. [2005]. The RTE dataset tests the\nlanguage model in recognizing textual entailment in the provided context. The classification is binary. We use top-1\naccuracy.\nA.2 Tasks for Fine-Tuning Stage\nFor these tasks, we use the popular evaluation repository LM Evaluation Harness to evaluate results for the following\ntasks [Gao et al., 2023].\nLanguage Understanding and Knowledge\n\u2022 OpenbookQA [Mihaylov et al., 2018] - dataset designed to evaluate a model's ability to apply elementary\nscience knowledge to answer questions. We use the normalized top-1 accuracy.\n\u2022 MMLU [Mihaylov et al., 2018] - a comprehensive dataset encompassing a wide range of subjects to assess\na model's understanding across various academic disciplines and professional domains. We use the top-1\naccuracy.\n\u2022 PIQA [Bisk et al., 2020] - a dataset that tests a model's commonsense knowledge about the physical world.\nWe use the normalized top-1 accuracy.\n\u2022 Hellaswag [Zellers et al., 2019] - a challenging dataset for commonsense reasoning, focusing on completing\nsentences in a way that makes sense in context. We use the normalized top-1 accuracy.\n\u2022 LAMBADA [Paperno et al., 2016] - a dataset designed to evaluate the ability of language models to understand\nand predict a missing word in a passage. We use the top-1 accuracy."}, {"title": "A.3 Hyperparameters", "content": "In our fine-tuning experiments, we train four models Mistral7B-v0.1, Mistral-7B-Instruct-v0.2 [Jiang et al., 2023],\nLlama3-8B, and Llama3-8B-Instruct [AI@Meta, 2024] with three Nvidia A100 80G GPUs. We follow the hyperparam-\neter setup from Ethayarajh et al. [2023]. As such, we use a batch size of 32 and train for a single epoch. We keep the\nlearning rate to be 5e - 7 as implemented. The maximum sequence length is set to 2048. We use RMSprop as our\noptimizer with warmup stages for 150 steps. The mixed precision is bfloat16."}, {"title": "A.4 Task Prompts", "content": "In this section, we provide the general format of the prompts for each dataset we have implemented:\nAG News"}, {"title": "A.5 Method Prompts", "content": "In this section, we provide the general format of the prompts for each method we have implemented.\nA.5.1 Baseline Method Prompts\nAutomatic/0-Shot Chain-of-Thought"}, {"title": "A.5.2 DiPT Prompts in Table 1", "content": "DiPT+ Rephrase and Respond\nDiPT+ Chain-of-Thought\nDiPT+ Analogical Reasoner"}, {"title": "A.6 Paraphrase prompt in Table 2", "content": "For the stability experiment in Section 4.1, we have automatically paraphrased the prompts using the\ngpt-4-1106-preview model and used the following commands for each dataset we implemented:\n\u2022 CosmosQA\n\u2022 RTE\nSST-5"}, {"title": "A.7 DiPT as a moderation mechanism used in Table 3", "content": "Here, we provide the adaptation of our method's prompt into a moderation mechanism, which goal is to prevent harmful\noutput generation while coordinating benign conversation."}, {"title": "Appendix B Additional Results", "content": "B.1 Quantitative result for fine-tuning with perspective-taking enriched data.\nWe present a breakdown of out of distribution (in-domain) results for fine-tuning the model with perspective-taking\nenriched datasets."}, {"title": "B.2 Qualitative result showing the output generation structure when applying our method.", "content": "We provide an example of the output generated using DiPT as a prompting framework and visualize the modular\nstructure of the comprehensive reasoning and assessment parts."}, {"title": "B.3 Results on Mistral 7B", "content": "We demonstrate the results on the open-weight model mistral-7b-instruct-v0.1 available on the HuggingFace\nlibrar"}]}