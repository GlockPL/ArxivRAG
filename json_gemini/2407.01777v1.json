{"title": "Deepfake Audio Detection Using Spectrogram-based Feature and Ensemble of Deep Learning Models", "authors": ["Lam Pham", "Phat Lam", "Truong Nguyen", "Huyen Nguyen", "Alexander Schindler"], "abstract": "In this paper, we propose a deep learning based system for the task of deepfake audio detection. In particular, the draw input audio is first transformed into various spectrograms using three transformation methods of Short-time Fourier Transform (STFT), Constant-Q Transform (CQT), Wavelet Transform (WT) combined with different auditory-based filters of Mel, Gammatone, linear filters (LF), and discrete cosine transform (DCT). Given the spectrograms, we evaluate a wide range of classification models based on three deep learning approaches. The first approach is to train directly the spectrograms using our proposed baseline models of CNN-based model (CNN-baseline), RNN-based model (RNN-baseline), C-RNN model (C-RNN baseline). Meanwhile, the second approach is transfer learning from computer vision models such as ResNet-18, MobileNet-V3, EfficientNet-B0, DenseNet-121, SuffleNet-V2, Swint, Convnext-Tiny, GoogLeNet, MNASsnet, RegNet. In the third approach, we leverage the state-of-the-art audio pre-trained models of Whisper, Seamless, Speechbrain, and Pyannote to extract audio embeddings from the input spectrograms. Then, the audio embeddings are explored by a Multilayer perceptron (MLP) model to detect the fake or real audio samples. Finally, high-performance deep learning models from these approaches are fused to achieve the best performance. We evaluated our proposed models on ASVspoof 2019 benchmark dataset. Our best ensemble model achieved an Equal Error Rate (EER) of 0.03, which is highly competitive to top-performing systems in the ASVspoofing 2019 challenge. Experimental results also highlight the potential of selective spectrograms and deep learning approaches to enhance the task of audio deepfake detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Sound-based applications represent a revolutionary paradigm in the rapidly evolving landscape of Internet of Sound (IoS) technology, where audio signals serve as the primary medium for data transmission, control, and interaction among interconnected devices [1], [2]. Voice-activated module in an IoS system, such as smart home devices, voice banking, home automation systems, and virtual assistants, relies on recognizing the user's voice to activate critical functions and generally involve confidential information. However, with the advancement of deep learning technologies, the emergence of spoofing speech attacks, commonly referred to as 'Deepfake', has become more prevalent. These attacks involve various AI-based speech synthesis techniques (e.g., Speech to Text [3], Voice Conversion [3], Scene Fake [4], Emotion Fake [5]), posing significant threats to the integrity and authenticity of voice-activated systems. Consequently, the detection of audio deepfakes has become a crucial area of research, drawing considerable attention from the research community. Several benchmark datasets and following challenges such as ASVspoof [6], Audio Deep synthesis Detection (ADD) [7], have been proposed, which facilitates the creation of various systems and techniques to handle this task. Existing studies can be divided into two kind: pipeline solutions (consisting of a front-end feature extractor and a back-end classifier) and end-to-end solutions [8]. The top-performing systems using these two methods in the ASVspoof and ADD competitions are mainly score- level fusion systems [8]. However, these systems lack a comprehensive evaluation of how individual spectrograms and classifiers affect overall performance, which is crucial for further research motivation and research direction. Other successful systems utilize deep features through various supervised embedding methods, such as DNNs [9] and RNNs [10]. Despite their effectiveness, these embeddings are trained on specific datasets and may encounter the issues of overfitting and susceptibility to adversarial attacks. This reduces the model's ability to generalize to new, unseen data, particularly when the dataset is not sufficiently large or diverse. Meanwhile, other approaches that can manage generalization and domain adaptation, such as transfer learning and leveraging embeddings from large pre-trained audio models, have not been extensively explored. To tackle these mentioned limitations, we therefore propose an ensemble of deep learning based models for audio deepfake detection task, which is achieved via a comprehensive analysis in terms of multiple spectrogram-based features and deep learning approaches. Our key contributions can be highlighted as:\n\u2022 Evaluated the efficacy different spectrograms in combination with auditory filters to model performance.\n\u2022 Evaluated a wide range of architectures leveraging both transfer learning and end-to-end networks.\n\u2022 Explored the performance of audio embeddings extracted from state-of-the-art pre-trained models (e.g. Whisper, Speechbrain, Pyannote) on deepfake detection.\n\u2022 Proposed an ensemble model via selective spectrograms and models from experiment, indicating the research focuses for further improving the task of deepfake audio detection."}, {"title": "II. PROPOSED DEEP LEARNING BASED SYSTEMS", "content": "The high-level architecture of proposed deep learning based system for audio deepfake detection, which is denoted in Fig. 1, comprises two main parts: front-end spectrogram- based feature extraction and back-end deep learning model for classification. In particular, the draw input audio recordings are first split into 2-second segments. This segment length generally provides sufficient context to capture important features and allows faster training and inference for applications requiring real-time detection. Next, the 2-second audio segments are transformed into spectrograms. Finally, the spectrograms are explored by back-end deep learning models to detect real or fake audio segments.\nThere are three deep learning based approaches are proposed in this paper. The first approach is shown in the upper part in Fig. 1. In this approach, referred to as the end- to-end approach, proposed models are used to train input spectrograms directly. In the second approach as shown in the middle part in Fig. 1, referred to as the finetuning approach , we fine-tune benchmark network architectures which are popularly used in the computer vision domain. Regarding the third approach as shown in the lower part in Fig. 1, we leverage the state-of-the-art pre-trained models which were trained on large audio datasets in advance. Then, we feed spectrograms input into these audio pre-trained models to obtain audio embeddings. The audio embeddings are finally classified into either real or fake class by a Multilayer Perceptron (MLP). We refer this approach to as the audio-embedding approach. Finally, individual and high- performance models from three approaches are selected and fused to achieve the best performance."}, {"title": "A. Spectrogram-based Feature Extraction", "content": "Fig. 2 presents how 6 different spectrograms are generated in this paper. In particular, 6 spectrograms are generated from three transformation methods of Short-time Fourier Transform (STFT), Constant-Q Transform (CQT), Wavelet Transform (WT). Presumably, each type of spectrogram focus on different perspectives on frequency content and might catch different inconsistencies in the audio signal. The combination of these spectrograms allows model to learn a broader range of features and patterns, potentially improving its ability to generalize and detect deepfakes. Additionally, we also establish different auditory-based fil- ters: Mel, Gammatone focus on subtle variations relevant to human auditory perception; linear filters (LF) isolates specific frequency bands., Integrating these filters alongside pre-defined spectrograms enriches the available features and further enhances the robustness to variations of the detection system.\nAs we use the same settings of the window length, the hop length, the filter number with 1024, 512, 64 for all spectrograms, generated spectrograms present the same tensor shape of 64\u00d764. Then, DCT is applied on spectrograms across the temporal dimension. Finally, we apply delta and delta-delta to these spectrograms, generate three dimensional tensor of 64\u00d764\u00d73 (i.e. the original spectrogram, delta, and delta-delta are concatenated across the third dimension)."}, {"title": "B. End-to-end deep learning approach", "content": "Regarding the end-to-end deep learning approach, we propose three baseline models of CNN-based model, RNN- based model, C-RNN-based model, which are referred to as the CNN baseline, RNN baseline, and C-RNN baseline, respectively. The detailed configuration of these baselines are presented in Table I. CNNs are the most common architecture for this task, which can effectively capture and learn spectral features within local frequency bands such as harmonic structures, formants, pitch variations, high-frequency artifacts, etc. Meanwhile, RNNs focus on detecting natural sequential patterns that can be disrupted in synthetic audio [11] (e.g. temporal coherence, prosodic features such as rhythm, stress, and intonation). Consequently, the usage of C-RNN baseline is based on the expectation of combine both spectral features and temporal features for distinguishing characteristics of deepfake audio."}, {"title": "C. Transfer learning approach", "content": "Additionally, we also evaluate a wide range of benchmark network architectures in the computer vision domain such as ResNet-18, MobileNet-V3, EfficientNet-B0, DenseNet-121, SuffleNet-V2, Swint, Convnext-Tiny, GoogLeNet, MNASs- net, RegNet. In particular, these networks were trained on the ImageNet1K dataset [12] in advance. Their pre-trained weights can capture rich and generalized features about pattern recognition in images, which can be potentially adapted to identifying patterns in spectrograms via parameter finetuning. In this approach, the final dense layer of these mentioned networks is modified to match the binary classi- fication task of deepfake audio detection before conducting the fine-tune process."}, {"title": "D. Audio-embedding deep learning approach", "content": "In the audio-embedding deep learning approach, we leverage the state-of-the-art audio pre-trained models of Whis-"}, {"title": "E. Ensemble of models", "content": "As an individual model works on 2-second audio segment, the predicted probability of an entire audio recording is computed by averaging of predicted probabilities over all 2- second segments. Consider $p^{(n)} = [p_{1}^{(n)}, p_{2}^{(n)}, ..., p_{C}^{(n)}]$, with C being the category number of the n-th out of N 2-second segments in one audio recording. The probability of an entire audio recording is calculated by the average classification probability which denoted as $p = [P_{1}, P_{2}, ..., P_{C}]$ where:\n$P_{c} = \\frac{1}{N} \\sum_{n=1}^{N} p_{c}^{(n)}$ for $1 \\le c < C$ (1)\nTo ensemble of results from individual models, we propose a MEAN fusion. In particular, we first conduct experi- ments on the individual models, then obtain the predicted probability as $p_{s} = (p_{s1}, p_{s2}, ..., p_{sC})$ where C is the category number and the s-th out of S individual models evaluated. Next, the predicted probability after MEAN fusion $P_{f-mean} = (P_{1}, P_{2}, \\dots, P_{C})$ is obtained by:\n$P_{c} = \\frac{1}{S} \\sum_{s=1}^{S} p_{sc}$ for $1 < c < C$ (2)\nFinally, the predicted label $\\hat{y}$ for an entire audio sample is determined as:\n$\\hat{y} = argmax(P_{1}, P_{2}, \\dots, P_{C})$ (3)"}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "We evaluate the proposed models on the Logic Access dataset of ASVspoofing 2019 challenge. The Logic Access dataset comprises three subsets(fake sample/real sample) of 'Train' (22800/2580), \u2018Develop\u2019(22296/2548), and 'Evalua- tion' (63882/7355), in which fake audio were generated from 19 AI-based generative systems. The models are trained on 'Train' subset, then evaluated and saved on 'Develop' subset. Finally, the models are test on the 'Evaluation' subset and the final result on this subset is reported.\nWe obey the ASVspoofing 2019 challenge, then use the Equal Error Rate (ERR) as the main metric for evaluating proposed models. We also report the Accuracy, F1 score and AuC score to compare the performance among proposed models."}, {"title": "B. Results and Discussion", "content": "Evaluation of spectrogram inputs: Consider the efficacy of feature extraction among proposed spectrogram inputs (i.e. systems from A1 to A6), STFT outperforms other compared spectrograms (models such as A1, A4, A5 achieves the best ERR score of 0.08 while the combination of STFT & LF obtains slightly better accuracy and F1 score of 0.88 and 0.9 respectively). This result suggests that STFT is often better suited for identify deepfake artifacts due to its uniform resolution in time and frequency [18] while the interpretable features extracted from linearly filtered signals are suitable for classification algorithms.\nMultiple deep learning approaches: Regarding end-to- end deep learning approach (A1 to B2), both RNN and C-RNN approaches obtains ERR score of 0.14 and 0.17, significantly worse than using only CNN with the best score of 0.08. This indicates the specific patterns indicative of deepfake audio might not be primarily temporal but rather spatial in the spectrogram representation. In the finetuning and audio embeddding-based approaches (C1 to C10 and D1 to D4), Swint, Convnext-Tiny and Whisper stand out as best systems within the corresponding approaches with compet- itive EER score of 0.09, 0.0075 and 0.10 respectively. This suggests the potential of these approaches when choosing the appropriate networks for enhancement.\nEnsembles: The experimental results presented in Table III underscore the significant effectiveness of ensemble tech- niques in detecting audio deepfakes. Specifically, the com- bination of STFT and LF spectrograms (A1+A2) achieves a score of 0.06, marking an improvement of 0.02 compared to best systems utilizing single spectrograms. Similarly, ensembles of models show slight enhancements such as the combination of CNN and ConvNeXt-Tiny which helps to reduce the ERR by 0.01 and 0.005 compared to individual models. These findings suggest that diverse feature extraction via ensembling multiple spectrograms substantially enhances overall performance compared to evaluating a wide range of models on a single spectrogram. Importantly, the ensemble of both spectrograms and models demonstrates significant improvement. Our best-performing system (A2, A4, A6, A7) achieves an ERR score and AuC of 0.03 and 0.994 respectively, placing in the top-3 in terms of EER score in the ASVspoof 2019 challenge [6]. These results highlight the strength of ensemble technique with leveraging multiple spectrogram analyses for feature extraction and deep learning models for pattern recognition."}, {"title": "IV. CONCLUSION", "content": "This paper has evaluated the efficacy of a wide range of spectrograms and deep learning approaches for deepfake audio detection. By estabishling the ensemble of selective spectrograms and models, our best system achieves the EER score of 0.03 on LA dataset of ASVspoofing 2019 chal- lenge, which is very competitive to state-of-the-art systems. Additionally, our comprehensive evaluation also indicate the potential of certain types of spectrogram (e.g. STFT) and deep learning approaches (e.g. CNN-based, finetuning pre-trained models), which can provide initial guidance for deepfake audio detection."}]}