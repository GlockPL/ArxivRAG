{"title": "Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models", "authors": ["Xin Zhou", "Yiwen Guo", "Ruotian Ma", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Aligning Large Language Models (LLMs) with human preferences is crucial for their deployment in real-world applications. Recent advancements in Self-Rewarding Language Models suggest that an LLM can use its internal reward models (such as LLM-as-a-Judge) (Yuan et al.) to generate preference data, improving alignment performance without costly human annotation. However, we find that different internal reward models within the same LLM often generate inconsistent preferences. This inconsistency raises concerns about the reliability of self-generated preference data, hinders overall alignment performance, and highlights the need for further research to ensure reliable and coherent alignment with human preferences. To address this limitation, we propose Self-Consistent Internal Rewards (SCIR), a novel framework designed to enhance consistency among internal reward models during training. In each training step, we collect preference predictions from multiple pre-defined internal reward models and enforce consistency and confidence through an inconsistency penalty mechanism, thereby improving the reliability of these internal reward models. We selectively use data with consistent predictions for preference optimization, ensuring the quality of the preference data. By employing self-consistent internal rewards, our method significantly improves the alignment performance and reward modeling capability of LLMs, outperforming baseline methods by a notable margin.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023) have demonstrated remarkable performance across various AI applications (OpenAI, 2022; Huang et al., 2023; Luo et al., 2023). A crucial aspect of deploying Large Language Models (LLMs) in real-world scenarios is their alignment with human preferences (Bommasani et al., 2021). This alignment is typically achieved through Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Rafailov et al., 2024), which trains LLMs to follow instructions and align with human preferences. However, obtaining high-quality human-annotated preference data is costly and time-consuming, particularly when adapting to new domains or evolving requirements. Furthermore, the inherent limitations of human cognitive abilities can constrain the quality of preference data, presenting additional challenges for training super-intelligent AI (Yuan et al.; Burns et al., a).\nRecent research has shown that the self-rewarding language model (SRLM) (Yuan et al.; Wu et al., 2024; Pang et al., 2024b) is a promising approach to address these challenges. The core idea behind SRLM is to use LLMs themselves to generate preference data, reducing the need for human annotation or external reward models. In this paradigm, an LLM first acts as an instruction-following model to generate multiple responses, then serves as a reward model to evaluate the responses through LLM-as-a-Judge prompting (Zheng et al., 2023b; Gu et al., 2024), utilizing its generative reward modeling ability to provide preference data. SRLM employs iterative DPO (Xu et al., 2024; Pang et al., 2024a) to train the LLM on the self-generated preference data. Each training iteration not only enhances the LLM's alignment performance but also improves its judging ability, providing better preferences data for the subsequent iteration.\nDespite its potential, we identify a limitation in the current SRLM paradigm: the preference labels predicted by the generative internal reward model (LLM-as-a-Judge) often conflicts with another internal reward model derived from the DPO training objective (Rafailov et al., 2024). The inconsistency of internal reward models indicates that the preference data used in the SRLM process may not be reliable enough. Specifically, if the generative internal reward model is inaccurate, it indicates the model's LLM-as-a-Judge ability has not improved as expected, compromising the quality of preference data for the subsequent training iterations. Similarly,"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Direct Preference Optimization", "content": "Direct Preference Optimization (DPO) (Rafailov et al., 2024) is a widely used method for preference optimization. Unlike traditional approaches that rely on an explicit reward model, DPO reparameterizes the preference-based reward function using only the policy models:\n$r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x),$ (1)\nwhere $\\pi_\\theta$ is the policy model, $\\pi_{ref}$ is the reference policy, and $Z(x)$ is the partition function. By integrating this reward formulation into the Bradley-Terry (BT) ranking model (Bradley & Terry, 1952), DPO reformulates preference probabilities as:\n$P(Y_w > Y_l | x) = \\sigma (r(x, y_w) - r(x,y_l)),$ (2)\nwhere $\\sigma$ is the sigmoid function. This allows us to express preference probabilities directly with the policy model instead of relying on a separate reward model. This implicit reward function also serves as the training objective of DPO:\n$L_{DPO} (\\pi_\\theta; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l |x)}{\\pi_{ref}(y_l|x)} \\right) \\right],$ (3)\nwhere $(x, y_w, y_l)$ consists of the prompt $x$, the winning response $y_w$, and the losing response $y_l$."}, {"title": "2.2. Self-Rewarding Language Model", "content": "To address the problem of cost and quality in human-annotated preference data, SRLM (Yuan et al.) proposes using LLM to generate preference data by LLM-as-a-Judge prompting, achieving self-improvement without the need for human annotation or an external reward model.\nIn general, SRLM begins with a supervised fine-tuned (SFT) LLM $M_0$ and uses the iterative DPO (Xu et al., 2024; Pang et al., 2024a) to train the model. $M_t$ refers to the model after t training iterations. Each iteration of SRLM consists of three steps. (1) Response Generation: In the first step, the LLM should generate multiple response candidates for each given prompt. Formally, given the model $M_t$ and prompts${x_1,x_2,...,x_n}$, $M_t$ generates k responses ${y_{i1}, y_{i2},..., y_{ik} }$ for each prompt $x_i$. (2) Preference Data Generation: In the second step, SRLM uses the same LLM"}, {"title": "2.3. Internal Reward Model Inconsistency", "content": "During SRLM training, we expect both the implicit DPO reward model to be well-optimized and the LLM's judgment capabilities to improve. Intuitively, a well-aligned LLM's DPO-derived reward model and LLM-as-a-Judge should provide a consistent and accurate preference label for the same preference pair. However, our empirical reveal substantial inconsistencies between these two internal reward models during the SRLM process. This inconsistency raises concerns about the alignment of LLM and the reliability of preference data in the SRLM.\nFollowing the SRLM setting proposed by Yuan et al., we train Mistral-7B-v0.3 (Jiang et al., 2023) through iterative DPO, using pointwise LLM-as-a-Judge to generate preference data. In each iteration t, we evaluate the internal reward models of the current model $M_t$ on two datasets: $D_{t-1}$ (trained preference data from the previous iteration) and $D_t$ (newly generated preference data).\nWe hypothesize that this inconsistency arises due to two reasons: (1) Inaccurate preference data in the previous iteration. The DPO reward model is derived from the training objective, and its preference predictions on the trained data $D_{t-1}$ are usually very close to the original preference labels, which are generated by previous model $M_{t - 1}$. Therefore, the observed inconsistencies on trained data $D_{t-1}$ primarily reflect disagreements between the LLM-as-a-Judge for current model $M_t$ and previous model $M_{t \u2013 1}$. As alignment performance improves, the enhanced LLM-as-a-Judge can provide better but inconsistent preference labels compared"}, {"title": "3. Method", "content": "In this section, we introduce Self-Consistent Internal Rewards (SCIR) to address the inconsistency of internal reward models and improve the overall alignment performance for SRLM. SCIR consists of two key components: consistency training and dynamic consistency preference optimization (DCPO). The former improves internal reward model reliability by enhancing their consistency, while the latter ensures the quality of preference data by selectively choosing the data with consistent prediction from the latest internal reward models."}, {"title": "3.1. Consistency Training", "content": "Consistency Training aims to enhance the consistency of different internal reward models. Although our method is applicable to any internal reward model, this work focuses on two types of internal reward model: a generative reward model (GRM) based on LLM-as-a-Judge prompting, and an implicit reward model (IRM) derived from DPO training objective, as shown in Section 2.1.\nGiven model $M_t$ and an unlabeled response pair $(x, y_1, y_2)$, we compute preference probabilities using all internal reward models of $M_t$. For the implicit reward model, the preference probability $P_{irm}(y_1 > y_2 | x)$ that IRM prefers"}, {"title": "3.2. Dynamic Consistency Preference Optimization", "content": "DCPO enhances preference data quality through two key mechanisms: utilizing the latest improved internal reward models to predict preference and selectively choosing the data with consistent prediction for DPO training. At each training step, all internal reward models predict the preference labels ${r_1, r_2, ...,r_n }$ for each unlabeled preference pair, and only the data with consistent prediction are selected for DPO training. The overall loss function of SCIR is:\n$I(r_1 = r_2 =\u00b7= r_n)\u00b7L_{DPO} + L_{consistency}$ (7)\nwhere \u03b1 is a hyperparameter that controls the strength of consistency loss and $r_i$ is the i-th internal reward model's preference label. As the reward modeling ability improves during training, DCPO allows for a reliable and up-to-date reward signal, leading to enhanced alignment performance."}, {"title": "3.3. Additional Training Techniques", "content": "We use additional techniques to address some commonly encountered problems for internal reward models. Existing research (Gu et al., 2025) suggests that the LLM-as-a-Judge are sensitive to the choice of judge prompt templates and the position of responses within the prompt. To mitigate prompt bias and position bias, we employ two different judge prompt templates and alternate the positions of the responses within the judge prompts, thereby creating four different judge prompts. The average of the predictions from these four prompts is used as the preference prediction for the generative reward model. For the implicit reward model, we introduce an adaptive reference model technique to enhance its consistency, the details are shown in Appendix A. The judge prompts are shown in the Appendix B.\nBesides that, the reward model tends to favor longer responses, and the internal reward model exhibits a similar"}, {"title": "4. Experiments", "content": "In this section, we first introduce the basic setup of our experiments in Section 4.1, and then show the experimental results of alignment in Section 4.2. Section 4.3 and Section 4.4 analyze the model's reward modeling ability. Finally, we conduct the ablation study in Section 4.5."}, {"title": "4.1. Experimental Setup", "content": "Basic Setup Our experiments are primarily conducted on the Mistral-7b-v0.3 series (Jiang et al., 2023). To obtain a supervised fine-tuned LLM, we randomly sample 5,000 SFT examples from the Alpaca dataset (Taori et al., 2023) and combine them with the LIMA dataset (Zhou et al., 2023) to train Mistral-7b. We follow the setup in Yuan et al. and add 2,000 LLM-as-a-Judge examples to the SFT dataset, providing model the initial ability to act as a judge. In addition, we also fine-tune Mistral-7b-Instruct on LLM-as-a-Judge data to serve as a strong SFT model, validating the effectiveness of our method on both weak and strong initial LLMs. This fine-tuned model is referred to as $M_0$. Starting from $M_0$, we conduct three iterations of DPO training. The model trained after the t iterations is denoted as $M_t$. In each iteration, we randomly sample 4,000 prompts from Alpaca dataset and use the current model to generate 4 candidate responses for each prompt. For our proposed SCIR, we randomly select two responses from these candidates to form an unlabeled preference pair, resulting in a total of 4,000 pairs as training data. These unlabeled pairs will be labeled during the SCIR training. For other baselines, we use the corresponding reward model to generate preference data from candidate responses and train LLM on these preference data via DPO. We show other details in Appendix C.\nBaselines We mainly compare our method with SRLM and its variants, their core difference is the reward model in the iterative DPO training phase. Self-Rewarding (LLM-as-a-Judge) (Yuan et al.) is the standard SRLM method and uses a point-wise LLM-as-a-Judge paradigm to score each response. Preference data are constructed based on scores. Self-Rewarding (Implicit Reward Model) is a variant of SRLM that employs the implicit DPO reward model to generate preference data. Besides the SRLM, we also compare our method with external reward model, which"}, {"title": "4.2. Main Results", "content": "We present the overall alignment performance in Table 2 and Table 3. From the results, we can find that: (1) Self-Rewarding (LLM-as-a-Judge) achieves limited performance improvement. Previous work (Yuan et al.) has shown that SRLM with LLaMA-2-70B can achieve approximately"}, {"title": "4.3. Consistency of Internal Reward Models", "content": "We follow the experimental setup in Section 2.3, using the model's internal reward models in t-th iteration to predict the labels for the preference data from both the current iteration (New Data $D_t$) and previous iterations (Trained Data $D_{t-1}$). The consistency rate is the proportion of cases where the generative and implicit reward models make consistent preference predictions on the same preference pairs. Following Section 3.3, we take the predictions that are consistent across different judge prompts as the results of the generative reward model, and only use this subset of data to calculate the consistency rate. The experimental results of SRLM (LLM-as-a-Judge) and our method are shown in Figure 2. Similar to the previous results, SRLM's consistency rate on both types of data is not high. However, our method achieves a high consistency rate on both trained and new data, and the consistency rate continues to improve with increased iterations. This demonstrates that SCIR can effectively enhance the consistency of internal reward models."}, {"title": "4.4. Reward Modeling Ability", "content": "Experimental Setup We take the widely used reward model benchmark RewardBench (Lambert et al., 2024) to evaluate the reward modeling ability of internal reward models. For each iteration, we use the internal reward models to predict the label of the preference pairs in RewardBench's Chat, Chat Hard, Safety, and Reasoning subsets. We directly use the average accuracy between the predicted results and the standard labels as the metric. It should be noted that some methods may only predict preference labels on subsets of the data. For example, SRLM uses a point-wise LLM-as-a-Judge, which may assign the same score to two responses, thus failing to predict preference labels correctly. Different methods may form preference data on different subsets, and these subsets are the actual data used in preference opti-"}, {"title": "4.5. Ablation Study", "content": "In this subsection, we perform ablation experiments to evaluate the contribution of SCIR's different loss function com-"}, {"title": "5. Related Work", "content": "Alignment of Large Language Model is one key factor behind the LLMs' success (Bommasani et al., 2021; Wang et al., 2023). By aligning model behavior with human preferences, an LLM can follow human instructions and generate helpful and harmless responses (Bai et al., 2022b). Alignment performance relies on preference data and preference learning algorithms. A classic preference learning algorithm is Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022a;c; Ouyang et al., 2022), which typically uses preference data to train an external reward model to score LLM's response, and then uses Proximal Policy Optimization (PPO) (Schulman et al., 2017) to optimize the LLM, making LLMs' responses maximize the reward model's score. Another widely used preference learning algorithm is Direct Preference Optimization (DPO) (Rafailov et al., 2024), which modifies the optimization objective of RLHF, allowing the model to be trained directly on preference data without the need for a reward model during the training process. Both RLHF and DPO relies on humans or an additional reward model to annotate preference data. High-quality preference data can enhance the effectiveness"}, {"title": "6. Conclusion", "content": "In this paper, we explore the consistency of internal reward models for the self-rewarding language models. We reveal that during the process of SRLM, there is a inconsistency between the generative reward model (LLM-as-a-Judge) and the implicit reward model (DPO training objective). This inconsistency suggests that the internal reward model and the preference data during SRLM training may be unreliable. To mitigate this issue, we propose Self-Consistency Internal Rewards Training to promote the consistency of the internal reward model. During training, we use different internal reward models to predict the preference probabilities of unlabeled preference pairs, and apply a consistency loss to make these predictions consistent and confident. To enhance the reliability of preference optimization, we only select preference pairs that get consistent predictions across all internal reward models for DPO training. Experimental results demonstrate that our method significantly improves the model's alignment performance, surpassing baseline methods. Moreover, our analysis shows that enhancing the consistency of the internal reward model and selecting consistent preference data effectively boosts the model's reward modeling performance."}, {"title": "A. Adaptive Reference Model for Implicit Reward Model", "content": "In Section 3.3, we introduce adaptive reference model to enhance the consistency of implicit reward model (IRM). In this section, we provide the details of the method.\nWe start from describing the implicit reward model, which is also the DPO training objective:\n$\\sigma\\beta\\log \\frac{\\pi_\\theta (y_1 | x)}{\\pi_{ref} (y_1x)} - \\beta\\log \\frac{\\pi_\\theta (y_2 | x)}{\\pi_{ref} (y_2x)}$ (8)\nwhere the $\\pi_\\theta$ is the $M_t$ during the SRLM iterations, $\\pi_{ref}$ is the reference model. In the standard DPO loss, $\\pi_{ref}$ and $\\pi_\\theta$ are initially identical, but $\\pi_{ref}$ is frozen and does not update parameter. We refer to the reference model used in the standard DPO method as the local reference model $\\pi_{ref}^{local}$. When computing the IRM, we use the base model, such as Mistral-7B, as the reference model. This ensures that the impact of overall SRLM training iteration can be reflected in the IRM. We refer to the reference model used in IRM as the global reference model $\\pi_{ref}^{global}$. Obviously, IRM with different reference model ($\\pi_{ref}^{local}$ or $\\pi_{ref}^{global}$) may predict inconsistent preference labels.\nTo address this issue, we propose the method called adaptive reference model. The goal is to ensure that the preference predictions made by $\\pi_{ref}^{local}$ and $\\pi_{ref}^{global}$ are consistent. Suppose that the IRM prefers $y_1$, which implies that the preference probability $P_{irm}(y_1 > y_2 | x) > 0.5$. This can be rewritten as:\n$\\beta\\log \\frac{\\pi_\\theta (y_1 | x)}{\\pi_{ref} (y_1x)} - \\beta\\log \\frac{\\pi_\\theta (y_2 | x)}{\\pi_{ref} (y_2x)} > 0,$\nwhich simplifies to:\n$\\beta\\log \\frac{\\pi_\\theta (y_1 | x)}{\\pi_{ref} (y_1x)} - \\beta\\log \\frac{\\pi_\\theta (y_2 | x)}{\\pi_{ref} (y_2x)} > 0.$\n$\\beta$ is a hyperparameter greater than 0. Dividing through by $\\beta$, we get:\n$\\frac{\\pi_\\theta (y_1|x)}{\\pi_\\theta (y_2 | x)} > \\frac{\\pi_{ref}(y_1 | x)}{\\pi_{ref}(y_2 | x)}.$\nDuring training, only $\\pi_\\theta$ can be updated, while $\\pi_{ref}$ remains fixed once set. As the result, to ensure consistent predictions with IRM using different reference models, we propose that $\\frac{\\pi_\\theta (y_1|x)}{\\pi_\\theta (y_2 | x)}$ should always be greater than the maximum value of $\\frac{\\pi_{ref}(y_1 | x)}{\\pi_{ref}(y_2 | x)}$ across all reference models. Formally, we require:\n$\\frac{\\pi_\\theta (y_1|x)}{\\pi_\\theta (y_2 | x)} > max\\left[ \\frac{\\pi_{ref}^{local}(y_1 | x)}{\\pi_{ref}^{local}(y_2 | x)}, \\frac{\\pi_{ref}^{global}(y_1 | x)}{\\pi_{ref}^{global}(y_2 | x)}\\right].$\nThus, we select the reference model\u2014either the local reference model $\\pi_{ref}^{local}$ or the global reference model $\\pi_{ref}^{global}$\u2014based on which one maximizes the value of $\\frac{\\pi_{ref}(y_1|2)}{\\pi_{ref}(y_2x)}$. This reference model is then used in the DPO loss function in Equation 7.\nNote that the global reference model $\\pi_{ref}^{global}$ is used for IRM in Equation 4, local reference model $\\pi_{ref}^{local}$ is used for DPO loss in Equation 7. Adaptive reference model does not require loading additional model, which makes training efficient. The inconsistency of IRM also improves the reliability of preference data and contribute to the overall alignment performance. The results of ablation study are shown in Table 4 in Section 4.5.\nRecently, CREAM (Wang et al., 2024) also propose use consistency regularization to reduce the noise caused by variations in the reference models during training iterations. But CREAM primarily selects consistent preference data by utilizing the IRM with a local reference model from the t-th and (t - 1)-th iterations. Our proposed adaptive reference model, on the other hand, uses $M_t$ and the base model as reference models to enhance the consistency of the IRM. This is because the goal of SCIR is to improve the consistency of all internal reward models. There are much differences in both the methods and motivations between the two approaches."}, {"title": "B. Prompts for LLM-as-a-Judge", "content": "In this section, we show the prompts for LLM-as-a-Judge used in our proposed SCIR and baselines. For our proposed SCIR, as mentioned in Section 3.3, we use two types of judge prompts for SCIR. These two prompts are shown in Table 5. Judge Prompt 1 is the prompt used in AlpacaEval (Dubois et al., 2024) and Judge Prompt 2 is the prompt used in Chatbot Arena (Zheng et al., 2023a). These judge prompt are widely used in LLM-as-a-Judge and can effectively select the better responses. For Self-Rewarding Language Model, we follow the Yuan et al. and directly use its judge prompt, which is shown in Table 6."}, {"title": "C. Details of Experimental Setup", "content": ""}, {"title": "C.1. Supervised Fine-tuneing", "content": "In our main experiments, SFT dataset consists of 5,000 examples from Alpaca dataset (Taori et al., 2023), 1,030 examples from LIMA (Zhou et al., 2023) dataset and 2,000 LLM-as-a-Judge data. Different baselines require different types of LLM-as-a-Judge data. We use preference version \u00b9 of Oasst2 dataset (K\u00f6pf et al., 2024) to generate LLM-as-a-Judge data. Our proposed SCIR uses the pairwise LLM-as-a-Judge paradigm, where the goal is to select the better response between two responses. We randomly sample 500 examples from Oasst2 and applied the multiple judge templates. Each preference pair generates 4 LLM-as-a-Judge examples, resulting in a total of 2000 pairwise LLM-as-a-Judge data. Self-Rewarding (LLM-as-a-Judge) use the pointwise LLM-as-a-Judge paradigm, where the goal is to score each response."}, {"title": "C.2. Iterative Training", "content": "We implement all the baselines ourselves due to the lack of open-source code and dataset. In each iteration, the model generates 4 responses for each of the 5000 prompts from the Alpaca dataset. For the Self-Rewarding(LLM-as-a-Judge), we use pointwise LLM-as-a-Judge to score each response and form preference pairs from the highest and lowest-scoring responses. Since different responses may have the same score, the number of preference pairs used in each iteration may be different. For Mistral-7b, the three iterations use 3696, 3588, and 3646 preference data, respectively. For Mistral-7b-Instruct, it generate 3075, 2957, and 3168 preference data, respectively. For the Self-Rewarding(Implicit Reward Model) and external reward model, we use IRM or Skywork-reward-7B to assign rewards to each response and form preference pairs from the highest and lowest-scoring responses. Each iteration uses 4000 preference pairs. For SCIR, we randomly select two responses to form an unlabeled preference pair, generating a total of 4000 unlabeled preference pairs for training."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}