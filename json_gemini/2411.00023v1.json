{"title": "Device-Directed Speech Detection for Follow-up Conversations Using Large Language Models", "authors": ["Ognjen (Oggi) Rudovic", "Pranay Dighe", "Yi Su", "Vineet Garg", "Sameer Dharur", "Xiaochuan Niu", "Ahmed H. Abdelaziz", "Saurabah Adya", "Ahmed Tewfik"], "abstract": "Follow-up conversations with virtual assistants (VAs) enable a user to seamlessly interact with a VA without the need to repeatedly invoke it using a keyword (after the first query). Therefore, accurate Device-directed Speech Detection (DDSD) from the follow-up queries is critical for enabling naturalistic user experience. To this end, we explore the notion of Large Language Models (LLMs) and model the first query when making inference about the follow-ups (based on the ASR-decoded text), via prompting of a pretrained LLM, or by adapting a binary classifier on top of the LLM. In doing so, we also exploit the ASR uncertainty when designing the LLM prompts. We show on the real-world dataset of follow-up conversations that this approach yields large gains (20-40% reduction in false alarms at 10% fixed false rejects) due to the joint modeling of the previous speech context and ASR uncertainty, compared to when follow-ups are modeled alone.", "sections": [{"title": "1 Introduction", "content": "Virtual assistants (VAs) are at the core of smart devices (e.g., mobile phones, smart speakers, wearables, etc.) as they aim to enable a naturalistic voice-based interaction between a user and a device. For VAs to respond to the user requests reliably, they need to infer whether the user is talking to the device or not. For instance, the user could be talking to someone else, and/or there could be side-speech conversations, background noise, etc. Therefore, classifying accurately if the user's speech is device-directed is critical for providing relevant responses, and to avoid interfering with the user's interactions which are not device-directed, i.e., intended for the VA. This task is often referred to as the device-directed speech detection (DDSD) [1; 2; 3; 4]. Most existing works on DDSD focus on detection from single queries of the user, often beginning with a wakeword (e.g., \"Hey Google\", \"Hey Siri\u201d, \u201cAlexa\u201d, and so on). Such isolated utterances are usually a complete question or a task request from a user to the VA, and often do not require additional context to determine if the speech is the VA directed. In this work, we address the task of DDSD in follow-up conversations, where the user's first query starts with a wakeword (that is easier to detect with high accuracy by existing systems for the wakeword detection [5]), potentially followed by another query (termed as the \"follow-up\"), as a continuation of the conversation with the VA. The follow-ups by design do not require the wakeword, and, therefore, classifying them correctly is far more challenging (see Fig. 1).\nPrevious approaches to DDSD process isolated utterances (i.e., no previous context is considered) either directly from audio [2], text [6], or from intermediate Automatic Speech Recognition (ASR) lattice-based features [7]. A few recent approaches attempted classification of device-directed speech in the context of natural turn-taking [3; 4; 8] by exploring various acoustic and lexical features; however, these works do not account for joint modeling of the ASR uncertainty and multi-turn user queries. The approach proposed here is built upon the recent work in [1] that focuses on the DDSD"}, {"title": "2 Methods", "content": "The goal is to identify whether a spoken follow-up utterance is directed towards a device (e.g. smartphone) or a human (see Fig. 1). To prepare the input for the LLM, we take the ASR outputs"}, {"title": "2.1 Prompting-based approach", "content": "In Fig. 2, we show an example of the fixed prompt, referred to as the task-prompt, that varies based on whether the 1- or n-best hypotheses are used in the follow-up. This task-prompt is further concatenated with the utterance-prompts containing the prompt with a follow-up. We consider two inference strategies: direct prompting of the pretrained LLM (\u201cPromptOnly", "FinetunePrompt\") where the base LLM is finetuned using Parameter-Efficient FineTuning (PEFT) methods [14], specifically LoRA [9] - see Sec. 3.2.\"\n    },\n    {\n      \"title\": \"2.2 Classification-based approach\",\n      \"content\": \"Another way to take advantage of the pretrained LLM is to add a classifier head on top of it, and then to finetune the entire model (we refer to this approach as \u201cClassHead\"). This approach is popularized by BERT [15] in prior literature. One of the drawbacks is that fine-tuning the entire LLM along with the classifier head not only requires a large amount of in-domain training data but also makes it difficult to deploy the model in production. To this end, we use the LoRA adapter finetuning again in the context of adding the LLM with a classifier head. This approach is referred to as \\\"ClassLoRAHead": "The classifier head in both \u201cClassHead\u201d and \u201cClassLoRAHead\u201d approaches is simply a linear layer that maps the last hidden layer output to a one-hot vector representing the device-directed speech label. In our task, the linear layer has 4096-D input, and 2-D output. The standard cross-entropy loss is used to train the LLM (or its LoRA adapters) and the classifier head."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 ASR System and Dataset", "content": "The ASR model that generates the 1- and n-best hypothesis for the input queries is based on the E2E-ASR architecture in [16], and it comprises of a Conformer [17] encoder with a CTC and attention-based decoder. The beam-search decodings from the attention-based decoder are rescored using an external Finite State Transducer-based language model (FST-LM). The ASR lattices obtained from the FST-LM decoding are used to generate the ASR hypotheses. Specifically, we use an internal audio dataset from demographically diverse English native speakers with consent, which is tailored to simulate a naturally occurring multi-turn conversations towards solving a task together (e.g, cross-word puzzles). The participants ask the VA questions, and use the answers to complete the task at hand, while the VA-equipped devices record audio from the participants. The dataset consists of ~19k audio recordings from 1.3k participants. The conversations are clipped into roughly 245k segments, i.e., the pairs of utterances, where the first utterance is always device-directed, and the follow-up can be either directed or undirected. These data are human-annotated in terms of device-directness, and are split into training, validation, and test (70/10/20%) partitions, with no speaker overlap. The ratio of the device-directed and -undirected examples is ~1:4, with average audio duration of 3.5\u00b13.25 (mean\u00b1std) seconds."}, {"title": "3.2 LLM Finetuning", "content": "We use a pretrained instruction-tuned LLM, Vicuna-7B-v1.3 [10] in the experiments. The inference is done using 4 NVIDIA A100 GPUs. For finetuning the prompting-based approach, we train parameters of the LoRA adapters [9] for 3 epochs using 8 GPUs with a learning rate of 2e-5, with a warmup to over 3% of the learning steps. The LoRA adapters (rank=8) have 4.1M parameters that is only 0.06% of the 7B parameters of the used LLM. In the classification-based approach, we experiment with the full finetuning, and with LoRA adapters. We experimented with different number of training epochs, and we trained the former for 1 epoch, while for LoRA we use 3 epochs, as those were the best on the validation set (we found it to be less prone to overfitting). The classifier head adds only ~8k additional parameters (the rest are the same as in the prompting-based approach). We use FastChat toolkit [18] for inference and finetuning with DeepSpeed GPU optimization [19], with 0 temperature during inference. Finetuning is done on train/val parts of the used dataset."}, {"title": "3.3 Evaluation Procedure", "content": "We evaluate the proposed approaches for the following setups. Firstly, we quantify the impact of the ASR uncertainty on classification of the follow-up query only (thus, no context), by representing it with 1- and n-best hypotheses. We set n = 8, found to work best on the validation data. Secondly, we quantify the impact of adding the context via the initial query, for which we only consider 1-best hypothesis (for the follow-up we compare both). We compare both the prompting- and classification-based approaches (see Table 2). Since the prompting-based approach produces only the textual binary outputs (\"0\" or \"1\" tokens), we compare in terms of False Accept Rate (FAR) and False Reject Rate (FRR) metrics, where the lower these metrics the higher the system accuracy. However, this approach does not enable to tune the DDSD system in practical applications as we cannot set target operating points (OP) (the binary labels do not allow to define the regime under a desired FRR). On the other hand, the classification-based approach is tunable as it provides probability scores. We report Equal Error Rate (EER) and FAR at hypothetical OPs 5% and 10% FRR. This allows us to establish the model accuracy across regimes relevant for user experience."}, {"title": "4 Results and Discussion", "content": "The prompting-based approach exploits the generative power of the LLM to understand the task based on the prompt, and outputs an answer that is then parsed to obtain the binary prediction for the speech being device-directed. We prompt the LLM system with the task-prompt and the utterance prompt, as described in Sec.2.1, without any finetuning (\u201cPromptOnly", "0": "1", "FinetunePrompt\") outputs only \\\"0\\\"/\\\"1\\\" values.\nTable 2 (left) compares the prompting- vs. classification-based approaches. When prompting the LLM without finetuning (\u201cPromptOnly\"), the model performs best with the simple n = 1 hypothesis and without context. This is expected since the model is not finetuned on the target task, also suggesting that the LLM is not familiar with the concept of n-best lists from its original training, and, thus, it is unable to leverage the ASR uncertainty effectively (using the prompt design proposed here). The\"\n    },\n    {\n      \"title\"": "5 Conclusions"}, {"content": "We explored various techniques for adapting an LLM model, including prompt tuning, and fine- tuning strategies. We showed that by integrating knowledge from a previous request of the user can largely improve the accuracy of device-directed speech detection in the follow-ups. These are often unconstrained in the content, and may not always be directed to the device, therefore, classifying them accurately is important for enabling naturalistic user experience and engagement. Specifically, we showed that the prompting-based approach leads to high accuracy when the LLM is fine-tuned on the DDSD task, however, it exhibits poor performance when only the engineered prompts are used (despite of adapting the language content in the prompts). On the other hand, adding a classifier on top of the pretrained (frozen) LLM allows the model to focus on the contextual clues from the first (initial) query, as well as to leverage effectively the ASR uncertainty (via the n-best hypotheses) \u2013 both encoded in the prompts, when making decisions about the follow-ups. We also showed that the classifier-adapted approaches can achieve high accuracy by tuning a fraction of the LLM parameters (via LoRA), saving the compute time. While in this work we focused on the pairs of the user queries as input to the LLM, other signals (e.g. VA's responses, acoustic features, and speaker information) can also be integrated to further improve the system accuracy."}, {"title": "A Supplemental material", "content": "To quantify the overall robustness of the models, we report Detection Error Trade-off (DET) curve in Fig. 3. We see a similar trend across most of the regimes, with both ASR uncertainty and context of the previous query in the classification-based approach performing the best, and also outperforming the prompting-based approach (\"FinetunePrompt\" with LoRA adapters, depicted in green). Finally, while the LoRA-based approach regresses compared to the full finetuning, with the largest regression occurring at 10% FRR, where FAR goes from 4.8% to 6.1%, both models still achieve relatively high accuracy. The LoRA-based approach has less tunable parameters and is much faster to train (on the used dataset, the reduce in the train time was ~5-10 times). Therefore, a trade-off needs be made depending on the hardware and time resources."}]}