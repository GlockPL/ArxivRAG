{"title": "LLM-based Frameworks for API Argument Filling\nin Task-Oriented Conversational Systems", "authors": ["Jisoo Mok", "Mohammad Kachuee", "Tara Taghavi", "Shuyang Dai", "Sungroh Yoon", "Shayan Ray"], "abstract": "Task-orientated conversational agents interact\nwith users and assist them via leveraging ex-\nternal APIs. A typical task-oriented conversa-\ntional system can be broken down into three\nphases: external API selection, argument fill-\ning, and response generation. The focus of our\nwork is the task of argument filling, which is\nin charge of accurately providing arguments re-\nquired by the selected API. Upon comprehend-\ning the dialogue history and the pre-defined\nAPI schema, the argument filling task is ex-\npected to provide the external API with the\nnecessary information to generate a desirable\nagent action. In this paper, we study the ap-\nplication of Large Language Models (LLMs)\nfor the problem of API argument filling task.\nOur initial investigation reveals that LLMs re-\nquire an additional grounding process to suc-\ncessfully perform argument filling, inspiring us\nto design training and prompting frameworks\nto ground their responses. Our experimental\nresults demonstrate that when paired with pro-\nposed techniques, the argument filling perfor-\nmance of LLMs noticeably improves, paving\na new way toward building an automated argu-\nment filling framework.", "sections": [{"title": "1 Introduction", "content": "Task-oriented conversational systems, illustrated\nin Figure 1, largely consist of three processes: ex-\nternal API selection, argument filling, and response\ngeneration (Hosseini-Asl et al., 2020). The API\nselection phase selects which one from the pre-\ndefined pool of APIs must be called to complete\nthe user request. Once the appropriate external API\nto carry out the user request has been selected, the\nargument filling phase must reliably identify and\nprovide correct arguments to the API by faithfully\nfollowing the API schema and dialogue history. An\nAPI schema, an example of which is also demon-\nstrated in Figure 1, is typically assumed to be given\nas a part of the API and includes required argu-\nments and their types. Therefore, the API schema\nand dialogue history provide sufficient information\nfor the conversational agent to identify which ar-\nguments are necessary to complete the API call.\nLastly, the response generation phase, as the name\nsuggests, returns an appropriate response to the\nuser based on the API output.\nThe user dissatisfaction in argument filling\nmainly stems from the conversational agent be-\ning incapable of adhering to the API schema and\ndialogue history. The erroneous arguments that di-\ngress away from the API schema are considered\n\"Syntax Errors\", and hallucinated responses that\ndeviate from the user utterances are considered\n\"Hallucinations.\" In Figure 2, we provide examples\nof each error type that occurs when performing\nargument filling for the \"Hair Appointment\" API.\nLarge Language Models (LLMs) trained with in-\nstructions have recently been garnering much atten-\ntion as a promising model for enabling human-like\nand safe user-agent interactions in open-domain\nconversations (Ouyang et al., 2022; Wang et al.,\n2022b). The aim of this paper is to explore whether\nthe strength of LLMs can be harnessed specifi-\ncally for the purpose of argument filling in task-\noriented conversational systems. To construct an\nLLM-backed framework for argument filling, their\noutputs must strictly follow and stay faithful to\nthe pre-defined API schema and user utterances,\na process commonly known as \"grounding.\" Our\ninitial zero-shot performance evaluation of LLMs\nof various sizes reveals that LLM-generated re-\nsponses suffer severely from both syntax errors and\nhallucinations, necessitating the development of\nadditional techniques to appropriately ground their\nresponses for the task of our interest.\nWe investigate two separate and unique avenues\nto tackle the problem of grounding for open- and"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Language Models for Task-oriented\nDialogues", "content": "Utilization of pre-trained Language Models for\nTask-oriented Dialogues (ToD) was pioneered\nby Zhang et al. (2019) and Peng et al. (2021).\nKulh\u00e1nek et al. (2021) and Lin et al. (2020) im-\nproved the basic ToD modeling approaches by em-\nploying contrastive state training and belief state\ndifferences, respectively. Other works (Pandey\net al., 2018; Cai et al., 2019; Nekvinda and Du\u0161ek,\n2022) proposed to combine generative models with\nretrieval-based approaches. While Hude\u010dek and\nDu\u0161ek (2023) perform zero-shot evaluation of var-\nious LLMs for ToD modeling, to the best of our\nknowledge, this is the first work to exploit and\ninstruction-tune LLMs in the billion-parameter\nregime for argument filling in ToD systems."}, {"title": "2.2 Large Language Models and\nInstruction-tuning", "content": "The introduction of Transformer-based architec-\ntures heralded the beginning of large and incredi-\nbly capable models for Natural Language Process-\nsing (NLP) (Vaswani et al., 2017). Transformer-\nbased language models with several billions of\nparameters, such as GPT-3 (Brown et al., 2020)\nand OPT (Zhang et al., 2022), have shown un-\nprecedented zero- and few-shot performance across\ndiverse NLP tasks. The generalization capabil-\nity of these so-called Large Language Models\n(LLMs) was further improved by training them via\ninstruction-tuning (Goldwasser and Roth, 2014)\nwith in-context instructions. The promising re-\nsults obtained by instruction-tuning inspired the\ndevelopment of large instruction-paired datasets,\nsuch as NaturalInstructions-v1 (NI-v1) (Mishra\net al., 2022) and SuperNaturalInstructions (Wang\net al., 2022a). The remarkable performance of\ngeneral-purpose instruction-tuned models inspired\nthe development of more domain-specific mod-\nels. Examples of such models include: Instruc-\ntUIE (Wang et al., 2023) for information extraction,\nCoEDIT (Raheja et al., 2023) for writing, ChatDoc-\ntor (Yunxiang et al., 2023) for medical purposes,\nand Goat (Liu and Low, 2023) for mathematics."}, {"title": "3 Proposed Methodology", "content": ""}, {"title": "3.1 Prompt Design", "content": "To guarantee experimental consistency across dif-\nferent models and datasets, we first design a com-\nmon prompt template for argument filling. An\nexample of the default prompt template, which\nincludes a short instruction, the pre-defined API\nschema, and dialogue history up to the specified\nAPI call, is provided in Figure 3. This prompt tem-\nplate is used for both in-context instruction tuning\nand evaluation processes and remains fixed across\nall of our experiments unless stated otherwise."}, {"title": "3.2 Instruction-tuning Framework for\nOpen-sourced LLMs", "content": "Phase I. Model Bootstrapping via Supervised\nFine-tuning We first bootstrap the LLM's re-\nsponses on argument filling prompts, so that its\ngenerative behavior can be controlled to output\nthe arguments in a dictionary format, as illustrated\nin Figure 3. Following the conventional fine-tuning\nscheme, we fine-tune the LLM using the cross\nentropy loss. Once the bootstrapping phase is\ncompleted, we propose to augment the train\ndataset using model-generated outputs. In the next\nsection, we define a custom reward function that is\nemployed to score and select generated samples to\nbe included in the additional fine-tuning phase.\nPhase II. Rejection Sampling with Custom Re-\nward Function \"Rejection Sampling\" commonly\nrefers to the process of identifying desirable model-\ngenerated outputs that are capable of further im-\nproving the performance on the target task. There-\nfore, the success of rejection sampling is heavily\ncontingent on the definition of the reward function\nthat can accurately reflect the usefulness of model-\ngenerated outputs. To define the custom reward\nfunction for argument filling, we first categorize po-\ntential sources of error into: non-existent key (NK),\nmissing key (MK), schema-grounded but incorrect"}, {"title": "3.3 Multi-step Prompting Scheme for\nClosed-sourced LLMs", "content": "It is infeasible to fine-tune LLMs whose design and\nweights are not released to the public. Therefore,\nwe additionally explore a more fine-grained and in-\nformative prompting method to complement larger\nLLMs. The default prompt design as described\nin Figure 3 asks the model to identify required ar-\nguments and extract appropriate information to fill\nthem all at the same time. For multi-step prompting\nwith hints, we instead prompt the model to identify\nand fill one argument at a time. By using this more\ntargeted prompt design, we are providing the LLM\nwith additional information about required slots\nand effectively restricting its generative behavior to\nprevent its digression from the pre-defined schema\nand dialogue history."}, {"title": "4 Experimental Set-up", "content": ""}, {"title": "4.1 Datasets and Models", "content": ""}, {"title": "4.1.1 Datasets", "content": "We primarily use STAR (Mosig et al., 2020) and\nSGD (Rastogi et al., 2020) datasets as test beds to\nvalidate our approach.\n\u2022 STAR: is a collection of realistic, task-oriented\ndialogues that includes 5,820 dialogues that span\n24 tasks and 13 domains. The schemas in the STAR\ndataset are similar to \"task specifications,\u201d which\ncontain information about the ideal dialogue flow\nfor each task.\n\u2022 SGD: is a rich, fully-annotated dataset, which\ncontains more than 22,000 dialogues that encom-\npass 20 domains, ranging from banks to travels\nand weather. The comprehensive annotation that\nincludes schema representation makes it a flexi-\nble and convenient dataset to investigate not only\nargument filling but also other components of task-\noriented conversational systems.\nWe verify the competitiveness of proposed ap-\nproaches under both in- and out-of-domain sce-\nnarios. Under the in-domain scenario, train and\ntest dialogues are sampled from the same set of\ndomains, while under the out-of-domain scenario,\nthe test dialogues contain domains that were not\nobserved during the training process. To create an\nin-domain benchmark, we randomly split the en-\ntire dataset into train and test datasets, such that\ndomains are evenly represented across the two. For\nout-of-domain evaluation, we purposefully curate\nthe test dataset, such that no explicit or semantic"}, {"title": "4.1.2 Models", "content": "\u2022 LLAMA-7B (Touvron et al., 2023): is a state-\nof-the-art foundational LLM released by Meta AI.\nWhile the LLAMA models of various sizes have\nbeen open-sourced, we primarily utilize LLAMA-\nv1-7B model for fine-tuning experiments.\n\u2022\nChatGPT\u00b9: is widely regarded as one of the\nmost powerful LLMs; its release is perceived to be\na significant milestone in the evolution of conver-\nsational AI systems. Because the model weights\nhave not been open-sourced, we rely on OpenAI's\nChatGPT API for evaluation."}, {"title": "4.2 Libraries and Hyperparameters", "content": "We utilize the Huggingface (Wolf et al., 2019) li-\nbrary for implementation and training of models.\nAll experiments are executed on NVIDIA V100\nGPU with 32GB RAM. The following set of hyper-\nparameters is used for the supervised fine-tuning\nphase: batch size of 8, Adam optimizer with ini-\ntial learning rate of 0.00002, weight decay of 0.1,\nand constant learning rate scheduling. We run\nthe supervised fine-tuning phase for 5 epochs be-\nfore performing rejection sampling. As mentioned\nin Section 3.2, we perform additional fine-tuning\nwith rejection-sampled data for only one additional\nepoch. All hyperparameters remain unchanged\nfrom the supervised fine-tuning phase."}, {"title": "4.3 Compared Approaches", "content": "\u2022 Zero-shot: is the most nave baseline obtained by\nprompting the pre-trained LLMs with the prompt\ndesign provided in Figure 3. The pre-trained LLMs\nare used as is without undergoing additional fine-\ntuning on task-oriented dialogue datasets.\n\u2022 Multi-Step: replaces the nave prompting pro-\ncess with the multi-step prompting scheme in Sec-"}, {"title": "4.4 Metrics", "content": "\u2022 BLEU: (Papineni et al., 2002) quantifies the\nsemantic similarity between model-generated and\nreference sentence pairs. Its close alignment with\nhuman perception of generation quality and low\ncomputational cost make BLEU a particularly com-\npelling metric for automatic evaluation of Natural\nLanguage Processing (NLP) systems.\n\u2022 Fuzzy Matching: is adopted to quantify the\nargument filling accuracy. We employ fuzzy match,\ninstead of exact match, such that minor typos and\ncapitalization, which should not determine the qual-\nity of the generated outputs, do not influence the\nperformance metric.\n\u2022 F-1 Score: takes into account both the character-\nlevel precision and recall of predicted arguments.\nF-1 score is a preferred choice of metric over ac-\ncuracy when evaluating datasets with significant\nclass imbalances (i.e., the number of test samples\nper API is unevenly distributed)."}, {"title": "5 Results", "content": ""}, {"title": "5.1 In-Domain Results", "content": "The results obtained on STAR and SGD datasets\nunder the in-domain evaluation setting are reported\nin Table 1. The suffixes -sft and -sft-rs are used\nto denote models that have been trained only with"}, {"title": "5.2 Out-of-Domain Results", "content": "To simulate an out-of-domain test scenario, we de-\nliberately create a train-test split, such that there\nis no explicit or implicit task domain overlap be-\ntween the train and test set The results obtained"}, {"title": "5.3 Error Analyses", "content": "We analyze sources of error in outputs generated by\nLLAMA-v1-7B-sft-rs to identify room for improve-\nment. In Figures 4 and 5, we compare the four error\nrates, as defined in Section 3.2, in LLAMA-v1-7B-\nsft and LLAMA-v1-7B-sft-rs models. Training the\nLLAMA-v1-7B model with SFT + RS reduces all\nfour error rates, and the rate of hallucinated value\nerrors is particularly low compared to other errors.\nThis analytical result implies that once grounded,\nthe LLM mostly ceases to hallucinate and remains\nclose to the API schema and dialogue history pro-\nvided as a part of the prompt template."}, {"title": "6 Conclusion", "content": "This paper explored and uncovered the powerful-ness of leveraging LLMs to automate the argument\nfilling process, a core component in task-oriented\nconversational systems. The strong experimental\nresults indicate that the proposed methods, used\nin conjunction with open- or closed-source LLMs,\nare effective for restricting the LLM's generative\nbehavior, specifically for argument filling."}, {"title": "Acknowledgements", "content": "This work was supported in part by the Institute of\nInformation & Communications Technology Plan-\nning & Evaluation (IITP) and the National Re-\nsearch Foundation of Korea (NRF) grants funded\nby the Korean government (MSIT) [No. 2021-0-\n01343, No. 2022-0-00959, Artificial Intelligence\nGraduate School Program (Seoul National Univer-\nsity), No. 2022R1A3B1077720] and the BK21\nFOUR program of the Education and Research\nProgram for Future ICT Pioneers, Seoul National\nUniversity in 2024."}, {"title": "Limitations and Potential Risks", "content": "One limitation of our work is that proposed frame-works are validated only on one open- or closed-sourced model. In addition, while LLMs are quite\nc capable of completing the argument filling task, the\ninference time for LLMs may still be longer than\nmany of smaller, more targeted language models.\nAccelerating LLM inferencing, however, is outside\nthe scope of our work.\nReliance on closed-sourced LLMs could pose\nunforeseen risks since the backbone model could be\naltered without notice. Even if significant changes\nare made to the design and weights of the closed-sourced models, there is no way for us to know\nwhat those alterations are. This complete black-\nbox nature of closed-sourced LLMs may make it an\nundesirable choice of backbone model. Therefore,\nwe conjecture that utilizing a targeted decoding\nscheme that can further enforce the LLM to follow\nspecific parts of the prompt template could assist\nin reducing schema-related errors."}]}