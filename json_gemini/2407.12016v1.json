{"title": "LLM-based Frameworks for API Argument Filling in Task-Oriented Conversational Systems", "authors": ["Jisoo Mok", "Mohammad Kachuee", "Tara Taghavi", "Shuyang Dai", "Sungroh Yoon", "Shayan Ray"], "abstract": "Task-orientated conversational agents interact with users and assist them via leveraging external APIs. A typical task-oriented conversational system can be broken down into three phases: external API selection, argument filling, and response generation. The focus of our work is the task of argument filling, which is in charge of accurately providing arguments required by the selected API. Upon comprehending the dialogue history and the pre-defined API schema, the argument filling task is expected to provide the external API with the necessary information to generate a desirable agent action. In this paper, we study the application of Large Language Models (LLMs) for the problem of API argument filling task. Our initial investigation reveals that LLMs require an additional grounding process to successfully perform argument filling, inspiring us to design training and prompting frameworks to ground their responses. Our experimental results demonstrate that when paired with proposed techniques, the argument filling performance of LLMs noticeably improves, paving a new way toward building an automated argument filling framework.", "sections": [{"title": "1 Introduction", "content": "Task-oriented conversational systems largely consist of three processes: external API selection, argument filling, and response generation. The API selection phase selects which one from the predefined pool of APIs must be called to complete the user request. Once the appropriate external API to carry out the user request has been selected, the argument filling phase must reliably identify and provide correct arguments to the API by faithfully following the API schema and dialogue history. An API schema is typically assumed to be given as a part of the API and includes required arguments and their types. Therefore, the API schema and dialogue history provide sufficient information for the conversational agent to identify which arguments are necessary to complete the API call. Lastly, the response generation phase, as the name suggests, returns an appropriate response to the user based on the API output.\n\nThe user dissatisfaction in argument filling mainly stems from the conversational agent being incapable of adhering to the API schema and dialogue history. The erroneous arguments that digress away from the API schema are considered \"Syntax Errors\", and hallucinated responses that deviate from the user utterances are considered \"Hallucinations.\"\n\nLarge Language Models (LLMs) trained with instructions have recently been garnering much attention as a promising model for enabling human-like and safe user-agent interactions in open-domain conversations. The aim of this paper is to explore whether the strength of LLMs can be harnessed specifically for the purpose of argument filling in task-oriented conversational systems. To construct an LLM-backed framework for argument filling, their outputs must strictly follow and stay faithful to the pre-defined API schema and user utterances, a process commonly known as \"grounding.\" Our initial zero-shot performance evaluation of LLMs of various sizes reveals that LLM-generated responses suffer severely from both syntax errors and hallucinations, necessitating the development of additional techniques to appropriately ground their responses for the task of our interest.\n\nWe investigate two separate and unique avenues to tackle the problem of grounding for open- and closed-sourced LLMs. On one hand, for open-sourced LLMs, we propose a two-step instruction-tuning framework that is comprised of supervised fine-tuning (SFT) and rejection sampling (RS). Our experimental results show that utilizing the proposed instruction-tuning framework noticeably outperforms the na\u00efve SFT baseline. On the other hand, in the case of closed-sourced LLMs whose weights are not directly accessible, we demonstrate that their performance can be improved by replacing the plain prompt design with a \"multi-step prompting\" scheme. Our contributions can be summarized as follows:\n\n\u2022 This is the first work to explore the utilization of LLMs for argument filling in task-oriented conversational agents. Our results demonstrate that when paired with a proper grounding process, LLMs can offer a simpler and more autonomous alternative to conventional approaches in argument filling.\n\n\u2022 For open-sourced LLMs, we propose a cohesive training pipeline to ground their behaviors. The proposed training pipeline consists of two phases: model bootstrapping via supervised fine-tuning and additional fine-tuning with model-generated outputs, which have undergone rejection sampling through a custom reward function. For closed-sourced LLMs, we explore an advanced prompting technique that is more fine-grained and informative.\n\n\u2022 We provide substantial experimental results to demonstrate the effectiveness of the proposed approaches. Notably, the LLAMA-v1-7B model fine-tuned using the proposed instruction-tuning pipeline outperforms strong zero-shot baselines obtained by prompting significantly larger LLMs."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Language Models for Task-oriented Dialogues", "content": "Utilization of pre-trained Language Models for Task-oriented Dialogues (ToD) was pioneered by Zhang et al. and Peng et al..  Kulh\u00e1nek et al. and Lin et al. improved the basic ToD modeling approaches by employing contrastive state training and belief state differences, respectively. Other works proposed to combine generative models with retrieval-based approaches. While Hude\u010dek and Du\u0161ek perform zero-shot evaluation of various LLMs for ToD modeling, to the best of our knowledge, this is the first work to exploit and instruction-tune LLMs in the billion-parameter regime for argument filling in ToD systems."}, {"title": "2.2 Large Language Models and Instruction-tuning", "content": "The introduction of Transformer-based architectures heralded the beginning of large and incredibly capable models for Natural Language Processing (NLP). Transformer-based language models with several billions of parameters, such as GPT-3 and OPT , have shown unprecedented zero- and few-shot performance across diverse NLP tasks. The generalization capability of these so-called Large Language Models (LLMs) was further improved by training them via instruction-tuning with in-context instructions. The promising results obtained by instruction-tuning inspired the development of large instruction-paired datasets, such as NaturalInstructions-v1 (NI-v1) and SuperNaturalInstructions . The remarkable performance of general-purpose instruction-tuned models inspired the development of more domain-specific models. Examples of such models include: InstructUIE for information extraction, CoEDIT for writing, ChatDoctor for medical purposes, and Goat for mathematics."}, {"title": "3 Proposed Methodology", "content": ""}, {"title": "3.1 Prompt Design", "content": "To guarantee experimental consistency across different models and datasets, we first design a common prompt template for argument filling. An example of the default prompt template, which includes a short instruction, the pre-defined API schema, and dialogue history up to the specified API call, is provided in Figure 3. This prompt template is used for both in-context instruction tuning and evaluation processes and remains fixed across all of our experiments unless stated otherwise."}, {"title": "3.2 Instruction-tuning Framework for Open-sourced LLMs", "content": ""}, {"title": "Phase I. Model Bootstrapping via Supervised Fine-tuning", "content": "We first bootstrap the LLM's responses on argument filling prompts, so that its generative behavior can be controlled to output the arguments in a dictionary format, as illustrated in Figure 3. Following the conventional fine-tuning scheme, we fine-tune the LLM using the cross entropy loss. Once the bootstrapping phase is completed, we propose to augment the train dataset using model-generated outputs. In the next section, we define a custom reward function that is employed to score and select generated samples to be included in the additional fine-tuning phase."}, {"title": "Phase II. Rejection Sampling with Custom Reward Function", "content": "\"Rejection Sampling\" commonly refers to the process of identifying desirable model-generated outputs that are capable of further improving the performance on the target task. Therefore, the success of rejection sampling is heavily contingent on the definition of the reward function that can accurately reflect the usefulness of model-generated outputs. To define the custom reward function for argument filling, we first categorize potential sources of error into: non-existent key (NK), missing key (MK), schema-grounded but incorrect value (SV), and hallucinated value (HV). The key and value here refer to the corresponding components of the key-value pairs of the model-generated arguments, which have been bootstrapped to follow a dictionary-like format. A detailed description of each error type is provided below:\n\n\u2022 Non-existent Key (NK): The generated key is not provided as a part of the pre-defined schema.\n\n\u2022 Missing Key (MK): The model-generated arguments are missing an expected key that is required by the pre-defined schema.\n\n\u2022 Schema-grounded but Incorrect Value (SV): The generated value follows the pre-defined schema but deviates from the dialogue history, resulting in an incorrectly identified argument.\n\n\u2022 Hallucinated Value (HV): The generated value does not follow the pre-defined schema, and hence, it is incorrect by definition.\n\nThe total number of errors in a model-generated output can be computed through a simple summation of all 4 error types: \\(N_{Error} = N_{NK} + N_{MK} + N_{SV} + N_{HV}\\). The error rate can then be defined as: \\(N_{Error}/N_{Total}\\), where \\(N_{Total}\\) denotes the total number of keys and values in the ground-truth argument. This error rate is normalized between -1 and 1 to obtain the final reward value following the equation: \\(R = 1-2 * N_{Error}/N_{Total}\\).\n\nAfter the LLM has been bootstrapped on the argument filling datasets, we sample K number of outputs from the model and score the generated outputs using the above reward function. We only select outputs that yield positive reward to augment the train dataset. With the newly added instances mixed in the train dataset, we perform one additional epoch of supervised fine-tuning.\n\nThere exist two expected advantages of incorporating rejection-sampled model outputs. First, utilizing the model outputs filtered with the custom reward function allows us to effectively augment the train dataset with desirable instances without the need to collect additional data points to avoid overfitting. Second, we expect that incorporating these outputs will improve the fine-tuned LLM's robustness to noisy data points it may encounter at test-time. Even if the model-generated outputs yield positive reward, they will inevitably be noisier than the curated train dataset with ground-truth labels. Therefore, the LLM that has been exposed to noisier data points in the rejection sampling phase will exhibit a higher degree of robustness and generalization performance."}, {"title": "3.3 Multi-step Prompting Scheme for Closed-sourced LLMs", "content": "It is infeasible to fine-tune LLMs whose design and weights are not released to the public. Therefore, we additionally explore a more fine-grained and informative prompting method to complement larger LLMs. The default prompt design as described in Figure 3 asks the model to identify required arguments and extract appropriate information to fill them all at the same time. For multi-step prompting with hints, we instead prompt the model to identify and fill one argument at a time. By using this more targeted prompt design, we are providing the LLM with additional information about required slots and effectively restricting its generative behavior to prevent its digression from the pre-defined schema and dialogue history."}, {"title": "4 Experimental Set-up", "content": ""}, {"title": "4.1 Datasets and Models", "content": ""}, {"title": "4.1.1 Datasets", "content": "We primarily use STAR and SGD datasets as test beds to validate our approach.\n\n\u2022 STAR: is a collection of realistic, task-oriented dialogues that includes 5,820 dialogues that span 24 tasks and 13 domains. The schemas in the STAR dataset are similar to \"task specifications,\u201d which contain information about the ideal dialogue flow for each task.\n\n\u2022 SGD: is a rich, fully-annotated dataset, which contains more than 22,000 dialogues that encompass 20 domains, ranging from banks to travels and weather. The comprehensive annotation that includes schema representation makes it a flexible and convenient dataset to investigate not only argument filling but also other components of task-oriented conversational systems.\n\nWe verify the competitiveness of proposed approaches under both in- and out-of-domain scenarios. Under the in-domain scenario, train and test dialogues are sampled from the same set of domains, while under the out-of-domain scenario, the test dialogues contain domains that were not observed during the training process. To create an in-domain benchmark, we randomly split the entire dataset into train and test datasets, such that domains are evenly represented across the two. For out-of-domain evaluation, we purposefully curate the test dataset, such that no explicit or semantic overlap exist between tasks in the train dataset and those in the test dataset."}, {"title": "4.1.2 Models", "content": "\u2022 LLAMA-7B : is a state-of-the-art foundational LLM released by Meta AI. While the LLAMA models of various sizes have been open-sourced, we primarily utilize LLAMA-v1-7B model for fine-tuning experiments.\n\n\u2022 ChatGPT: is widely regarded as one of the most powerful LLMs; its release is perceived to be a significant milestone in the evolution of conversational AI systems. Because the model weights have not been open-sourced, we rely on OpenAI's ChatGPT API for evaluation."}, {"title": "4.2 Libraries and Hyperparameters", "content": "We utilize the Huggingface library for implementation and training of models. All experiments are executed on NVIDIA V100 GPU with 32GB RAM. The following set of hyperparameters is used for the supervised fine-tuning phase: batch size of 8, Adam optimizer with initial learning rate of 0.00002, weight decay of 0.1, and constant learning rate scheduling. We run the supervised fine-tuning phase for 5 epochs before performing rejection sampling. As mentioned in Section 3.2, we perform additional fine-tuning with rejection-sampled data for only one additional epoch. All hyperparameters remain unchanged from the supervised fine-tuning phase."}, {"title": "4.3 Compared Approaches", "content": "\u2022 Zero-shot: is the most naive baseline obtained by prompting the pre-trained LLMs with the prompt design provided in Figure 3. The pre-trained LLMs are used as is without undergoing additional fine-tuning on task-oriented dialogue datasets.\n\n\u2022 Multi-Step: replaces the naive prompting process with the multi-step prompting scheme in Section 3.3. Since multi-step prompting only improves the model at inference time, the pre-trained LLM is again used with no alterations.\n\n\u2022 Supervised Fine-tuning (-sft): is a baseline obtained by instruction-tuning the LLM on fully-labeled train set of task-oriented dialogue datasets following the Phase I process in Section 3.2.\n\n\u2022 Supervised Fine-tuning + Our Rejection Sampling (-sft-rs): trains the fine-tuned LLM on additional model-generated data that have been selected according to the proposed reward for rejection sampling (Phase II of Section 3.2)."}, {"title": "4.4 Metrics", "content": "\u2022 BLEU: quantifies the semantic similarity between model-generated and reference sentence pairs. Its close alignment with human perception of generation quality and low computational cost make BLEU a particularly compelling metric for automatic evaluation of Natural Language Processing (NLP) systems.\n\n\u2022 Fuzzy Matching: is adopted to quantify the argument filling accuracy. We employ fuzzy match, instead of exact match, such that minor typos and capitalization, which should not determine the quality of the generated outputs, do not influence the performance metric.\n\n\u2022 F-1 Score: takes into account both the character-level precision and recall of predicted arguments. F-1 score is a preferred choice of metric over accuracy when evaluating datasets with significant class imbalances (i.e., the number of test samples per API is unevenly distributed)."}, {"title": "5 Results", "content": ""}, {"title": "5.1 In-Domain Results", "content": "The results obtained on STAR and SGD datasets under the in-domain evaluation setting are reported in Table 1. The suffixes -sft and -sft-rs are used to denote models that have been trained only with supervised fine-tuning and with supervised fine-tuning and rejection sampling, respectively. Multi-step prompting that provides additional hints successfully improves the performance of the ChatGPT models. More importantly, we observe that the LLAMA-v1-7B model that has been trained with the proposed instruction-tuning pipeline with rejection sampling (LLAMA-v1-7B-sft-rs) obtains the best performance across all metrics on both datasets. This result clearly demonstrates that with our training framework, relatively smaller and lightweight LLMs can outperform larger ones. Furthermore, the superiority of LLAMA-v1-7B-sft-rs to LLAMA-v1-sft provides strong support for incorporating rejection-sampled data to effectively improve the performance of fine-tuning with less training budget. Lastly, we note that a larger degree of performance improvement is observed on the SGD dataset, which has a wider variety of tasks and thus can be considered more difficult."}, {"title": "5.2 Out-of-Domain Results", "content": "To simulate an out-of-domain test scenario, we deliberately create a train-test split, such that there is no explicit or implicit task domain overlap between the train and test set The results obtained under the out-of-domain evaluation setting are reported in Table 2. In general, the out-of-domain evaluation results show similar tendencies to the in-domain results. While the proposed instruction-tuning framework and multi-step prompting successfully improve the performance of open-sourced and closed-sourced LLMs, respectively, they both experience slight performance degradation when compared to the in-domain evaluation results."}, {"title": "5.3 Error Analyses", "content": "We analyze sources of error in outputs generated by LLAMA-v1-7B-sft-rs to identify room for improvement. In Figures 4 and 5, we compare the four error rates, as defined in Section 3.2, in LLAMA-v1-7B-sft and LLAMA-v1-7B-sft-rs models. Training the LLAMA-v1-7B model with SFT + RS reduces all four error rates, and the rate of hallucinated value errors is particularly low compared to other errors. This analytical result implies that once grounded, the LLM mostly ceases to hallucinate and remains close to the API schema and dialogue history provided as a part of the prompt template."}, {"title": "6 Conclusion", "content": "This paper explored and uncovered the powerfulness of leveraging LLMs to automate the argument filling process, a core component in task-oriented conversational systems. The strong experimental results indicate that the proposed methods, used in conjunction with open- or closed-source LLMs, are effective for restricting the LLM's generative behavior, specifically for argument filling."}, {"title": "Limitations and Potential Risks", "content": "One limitation of our work is that proposed frameworks are validated only on one open- or closed-sourced model. In addition, while LLMs are quite capable of completing the argument filling task, the inference time for LLMs may still be longer than many of smaller, more targeted language models. Accelerating LLM inferencing, however, is outside the scope of our work.\n\nReliance on closed-sourced LLMs could pose unforeseen risks since the backbone model could be altered without notice. Even if significant changes are made to the design and weights of the closed-sourced models, there is no way for us to know what those alterations are. This complete black-box nature of closed-sourced LLMs may make it an undesirable choice of backbone model. Therefore, we conjecture that utilizing a targeted decoding scheme that can further enforce the LLM to follow specific parts of the prompt template could assist in reducing schema-related errors."}]}