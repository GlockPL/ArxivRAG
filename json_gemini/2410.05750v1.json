{"title": "Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting", "authors": ["Nicholas Carlini", "Jorge Ch\u00e1vez-Saab", "Anna Hambitzer", "Francisco Rodr\u00edguez-Henr\u00edquez", "Adi Shamir"], "abstract": "Deep neural networks (DNNs) are valuable assets, yet their public accessibility raises security concerns about parameter extraction by malicious actors. Recent work by Carlini et al. (Crypto'20) and Canales-Mart\u00ednez et al. (Eurocrypt'24) has drawn parallels between this issue and block cipher key extraction via chosen plaintext attacks. Leveraging differential cryptanalysis, they demonstrated that all the weights and biases of black-box ReLU-based DNNs could be inferred using a polynomial number of queries and computational time. However, their attacks relied on the availability of the exact numeric value of output logits, which allowed the calculation of their derivatives. To overcome this limitation, Chen et al. (Asiacrypt'24) tackled the more realistic hard-label scenario, where only the final classification label (e.g., \"dog\" or \"car\") is accessible to the attacker. They proposed an extraction method requiring a polynomial number of queries but an exponential execution time. In addition, their approach was applicable only to a restricted set of architectures, could deal only with binary classifiers, and was demonstrated only on tiny neural networks with up to four neurons split among up to two hidden layers.\nThis paper introduces new techniques that, for the first time, achieve cryptanalytic extraction of DNN parameters in the most challenging hard-label setting, using both a polynomial number of queries and polynomial time. We validate our approach by extracting nearly one million parameters from a DNN trained on the CIFAR-10 dataset, comprising 832 neurons in four hidden layers. Our results reveal the surprising fact that all the weights of a ReLU-based DNN can be efficiently determined by analyzing only the geometric shape of its decision boundaries.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have become ubiquitous in today's technological landscape due to their ability to perform complex tasks such as image classification, speech recognition, natural language processing, and autonomous driving.\nThe simplest form of a DNN consists of a series of fully connected hidden layers of neurons. Each neuron in the network performs a global linear operation (over $\\mathbb{R}^{d_i}$ for various round dimensions $d_i$) followed by the parallel application of local nonlinear operations over $\\mathbb{R}$, such as the Rectified Linear Unit (ReLU) activation function. The DNN's parameters are generally obtained by collecting a huge corpus of training examples and iteratively adjusting an initial set of parameters through a lengthy sequence of gradient descent steps aimed at minimizing the DNN's loss on the training examples. This process can take many months and incur costs in the millions of dollars, making a trained DNN a highly valuable asset. However, this asset is often made available for free through an oracle interface that allows anyone to input data to the DNN classifier and receive the corresponding answer.\nThe question of whether the DNN parameters can be determined by such an oracle access to its black box implementation has a rich history and had been addressed in numerous papers over the past 30 years (as detailed in section 2). Recently, cryptographic researchers have noted the close similarities between the structures of DNNs and block ciphers, both of which consist of alternating layers of global linear operations and local nonlinear operations, such as ReLUs in the case of DNNs and S-boxes in the case of block ciphers (see Figure 1). In both primitives, the nonlinear operations are publicly known, while the linear operations involve secret elements the parameters in DNNs, and the round keys in block ciphers. The goal of the attacker is to find these secret elements by applying an adaptive chosen input attack. Unsurprisingly, the most effective model stealing attacks currently leverage concepts from the differential cryptanalysis of block ciphers."}, {"title": "1.1 Why Previous Techniques Cannot Be Used", "content": "To understand the difficulty posed by the harder scenario (S1), consider the way most recent attacks operate, as depicted in Figure 2. Any ReLU-based DNN represents a piecewise-linear mapping from real valued inputs to real valued outputs, which partitions the $d_0$ dimensional input space into a huge number of convex cells, as described in Figure 2a. Within each cell, the mapping behaves as a linear transformation, and the boundaries between adjacent cells are $d_0 - 1$ dimensional hyperplanes. These hyperplanes represent the points where at least one of the ReLU inputs becomes zero, which we refer to as critical points following the terminology proposed in [5] and adopted in [4,6]. Such ReLU flips change the behavior of the mapping from one linear mapping into a different one. Note that while the input/output mapping is always continuous, its derivatives become discontinuous at cell boundaries.\nIn addition to cell boundaries, we can also draw the decision boundaries between pairs of classes, whose points (which are called transition points) are the points at which the network's decision changes from one class to a different class. Since the class decision is usually defined by the largest logit, decision boundaries are defined by linear inequalities between piecewise linear functions, and thus they are also piecewise linear $d_0 - 1$ dimensional hyperplanes which partition the $d_0$ dimensional input space into disjoint (not necessarily connected) regions, where each region represents the locations in which the hard-label assigned by the DNN is one of the classes. Note that decision boundaries can cut a single cell into two subcells which correspond to two different class decisions, even though the linear mapping in both subcells is the same. The only connection between cell boundaries and decision boundaries (which are defined by critical points and transition points, respectively) is that within each cell the decision boundary must be a flat hyperplane, which typically changes its orientation as it crosses into an adjacent cell, as depicted in Figure 2b.\nMost recent attacks have analyzed the output behavior as the input transitions along a straight line between two points in the input space, such as $\\mathbf{x}_0$ and $\\mathbf{x}_1$ in Figure 2c. This path passes through various critical points such as $\\mathbf{x}_3$ and $\\mathbf{x}_4$. When dealing with the easiest attack scenario (S5), an attacker can precisely identify the location of such critical points, as the slope of the output changes abruptly. Each identified critical point offers valuable information about the value of some internal neuron during the evaluation of the DNN, by indicating that the input to its ReLU is zero at that point. This concept is akin to side-channel attacks that reveal internal values generated at various intermediate points during the encryption process, significantly reducing the complexity of key recovery. By collecting a sufficient number of such internal values, the attacker can recover the parameters of all the neurons by solving systems of linear equations in polynomial time.\nThe primary challenge in the hard-label attack scenario (S1) is the lack of access to the numeric values of the outputs, which prevents us from computing derivatives. Consequently, when we pass through critical points (such as points $\\mathbf{x}_3$ and $\\mathbf{x}_4$ in Figure 2c), we are unaware of this fact, as the DNN outputs \"dog\" for every input in the vicinity of $\\mathbf{x}_3$ and \"car\" for every input in the vicinity of $\\mathbf{x}_4$. The only event we can detect in this scenario is when we pass through"}, {"title": "1.2 Our Contributions", "content": "The main contribution of this paper is to show that we can effectively replace the analysis of critical points in previous attacks by the analysis of class transition points, and thus recover all the DNN's secret parameters using a polynomial number of oracle queries and polynomial execution time by just analyzing the geometric shape of its decision boundary. In particular, we show that given any initial transition point, we can efficiently move along the decision boundary patch that contains it until this patch changes its orientation. This can happen only at a point which is simultaneously a transition point and a critical point, and thus we can indirectly sense the location of some critical points even though the attack scenario does not allow us to sense them directly. We call points which are both transition points and critical points dual points, and provide two examples of such dual points ($\\mathbf{x}_5$ and $\\mathbf{x}_6$) in Figure 2c. Even though dual points are only an infinitesimally small subset of the set of all possible critical points, their analysis turns out to be a sufficient alternative to the analysis of critical points in previous attacks.\nOur new attack follows the same strategy as outlined in [5] and further developed in [4] and [6], but enhances them with novel techniques. The two main technical contributions of this paper are a new polynomial time algorithm for recovering the critical hyperplanes of all the neurons and a new polynomial time sign recovery technique in the hard-label scenario. In particular, sign recovery was the main bottleneck in [6]: the only solution the authors found for this problem was to perform an exponential time exhaustive search over all the possible sign combinations of all the neurons in the current layer.\nIt is important to note that our attack can fail in some extreme situations which are not likely to happen in normally trained networks (unless the network was adversarially generated to resist our attack). For example, if some neuron plays no role in forming the shape of the decision boundary, we will not be able to find its weights. Another rare possibility is that some system of $k$ random-looking linear equations in $k$ unknowns over floating point reals, generated by our attack, has a determinant that is exactly zero (which will make it impossible to recover any additional parameters at later layers)."}, {"title": "1.3 Organization", "content": "The remainder of this paper is organized as follows. In section 2, we provide a concise overview of the most relevant works in DNN parameter extraction within the black-box model along with the main precedents attacking the hard-label setting. In section 3 we give an attack overview, and afterwards present a full description of the main components of our attack: dual point finding (section 4), signature recovery (section 5) and sign recovery (section 6). All the practical experiments conducted in this study are described in section 7, while section 8 presents our concluding remarks."}, {"title": "2 Related Work", "content": "The problem of DNN model extraction was first studied in the 1990s [1,2,3,11]. In 1991, Baum presented an algorithm in [1,2] that could infer the Boolean function describing the model of a neural net algorithm in polynomial time. This was achieved by utilizing chosen inputs and querying the DNN as an oracle to obtain their labels. Baum demonstrated that his algorithm could provably achieve Probably Approximately Correct (PAC) learning in polynomial time for tiny networks consisting of up to four neurons, and he provided preliminary evidence suggesting that it could be extended to handle larger networks with up to 200 neurons. A few years after this research, Fefferman demonstrated in [9] that complete knowledge of all the (infinitely many) possible outputs from a sigmoid-based network uniquely determines its architecture and the weights of its neurons (up to some unavoidable symmetries). However, his technique did not provide an effective procedure for determining the actual network parameters. Another remarkable result on this topic was established by Blum and Rivest in 1993 [3]. They examined a different scenario in which the attacker was given an adversarially chosen set of known inputs and their corresponding outputs (i.e., she could not choose her own queries). Their main result was that in this scenario, determining whether there exists a corresponding two-layer, three-neuron DNN (with the sign activation function instead of a ReLU) is NP-complete.\nSince 2016, the extraction of DNN models from their black-box implementations has been extensively studied in [7,10,13,15,16,17,20]. The main goal of this line of research is to accurately infer all secret weights of the neurons in the DNN, achieving sufficient numerical precision to ensure functional equivalence between the extracted model and the original DNN. Current state-of-the-art attacks in this domain can be found in [4,5, 6].\nIn [5], the authors introduced several efficient techniques for recovering neuron weights and their associated biases with remarkable precision, a process they termed the neuron's signature recovery. These methods operate with a polynomial number of queries and time complexity. However, this approach can only determine the neuron's signature up to a constant multiplier of unknown sign, thus necessitating an essentially exhaustive search for the correct signs, which has exponential time complexity. Consequently, the showcase examples of deep neural networks (DNNs) presented in [5] involved relatively shallow networks. This limitation was addressed in [4], where the authors introduced novel techniques for recovering the missing neuron signs in polynomial time, making it possible to demonstrate attacks on significantly larger and deeper DNNs.\nMore recently, Foerster et al. [10] reported an end-to-end attack that effectively combines the signature recovery methods from [5] with the sign recovery techniques from [4]. They claim that their approach provides a more computationally efficient sign recovery process, resulting in significant speedups in the overall execution time of the attack. The authors note that, in this complete attack, most of the time is spent recovering signatures, with considerably less time devoted to extracting the signs."}, {"title": "3 Attack Overview", "content": "A deep neural network in the hard-label scenario takes inputs $x$ and processes them layer-by-layer into a final output decision, e.g. \"dog\" (cf. Figure 3).\nDefinition 1. An $r$-deep neural network $f$ is a function parameterized by $\\theta$ that takes inputs from an input space $\\mathbb{R}^{d_0}$ and returns values in an output space $\\mathbb{R}^{d_{r+1}}$. The function $f$ is composed as a sequence of functions alternating between linear functions $f_i: \\mathbb{R}^{d_{i-1}} \\rightarrow \\mathbb{R}^{d_i}$, called fully connected layers, and a nonlinear function $\\sigma$ (acting component-wise):\n$$f = f_{r+1} \\circ \\sigma \\circ \\ldots \\circ \\sigma \\circ f_2 \\circ \\sigma \\circ f_1.$$\nAt a high level our attack extracts the parameters $\\theta$ of a model layer by layer, and the analysis of each layer consists of two steps:\nSignature recovery extracts the parameters of each neuron up to some unknown multiplicative factor. This determines the location of its critical hyperplane.\nSign recovery determines the side of the critical hyperplane in which the linear function of the neuron produces positive values which pass unchanged through the subsequent ReLU.\nA full list of formal definitions can be found in subsection A.1. Our terminology follows very closely the one first presented in [5] and then adopted in more recent attacks [4, 6].\nThe main difference from previous attacks is that we no longer have direct access to critical points we are only aware of class transition points. We will concentrate on the subset of transition points which are also critical points. These are the dual points, and we can find them by moving along decision boundaries until their local orientation changes (detailed in section 4).\nSince dual points are defined by the intersection of two locally linear $d_0 - 1$ dimensional hyperplanes (defined by the critical and transition conditions, respectively), they form a locally linear $d_0 - 2$ dimensional subspace $D$ which is associated with some neuron. Unfortunately, we can directly access only the transition hyperplane and not the critical hyperplane. We overcome this problem by computing the subspace $D$ by intersecting two adjacent decision boundary patches with different orientations (instead of intersecting a decision hyperplane and a critical hyperplane). In other words, once we discover some dual point"}, {"title": "4 Dual Point Finding", "content": "The value of critical points is well understood in the literature [4,5], because it is the location of these points that completely determine the parameters of a neural network. Unfortunately, without the ability to directly access the function $f(.)$ that returns the logits of the model, it is not possible to identify general critical points. Instead, we use dual points (cf. Definition 17): inputs that sit both"}, {"title": "5 Signature Recovery", "content": "The first step in our attack recovers the parameters of each layer of the model up to a real-value scalar per neuron. Formally, the $i$-th fully connected layer of the neural network is $f_i: \\mathbb{R}^{d_{i-1}} \\rightarrow \\mathbb{R}^{d_i}$, which is parameterized by the weight matrix $A^{(i)} \\in \\mathbb{R}^{d_i \\times d_{i-1}}$ and the bias vector $b^{(i)} \\in \\mathbb{R}^{d_i}$ (cf. Definition 10).\nDefinition 2. The signature of a neuron $\\eta$ is equal to $\\alpha \\cdot A_{\\eta}^{(i)}$, where $\\alpha$ is an arbitrary rescaling of the corresponding parameter.\nIt is easy to verify that positive constants can be pushed through the network arbitrarily (specifically: if all the weights and the bias of one neuron are multiplied by a constant $\\alpha > 0$, and the corresponding inputs to every neuron on the next layer are multiplied by $\\frac{1}{\\alpha}$, then the model will behave identically). However, negative constants can not be pushed through the model, since this causes neurons to flip from active to inactive which changes the behavior of the model. Therefore, our signature recovery attack mirrors the methodology of prior work [5]: we search for a set of dual points, and then use the locations of these dual points to determine the signatures on the first layer."}, {"title": "5.1 Warm-up: Extracting the first layer", "content": "The prior section developed an algorithm that allows us to efficiently identify a vast set of dual points. We now show how to use this information to recover the normal vectors (up to sign and magnitude) to the neurons and, as a consequence, the signatures of all the neurons in the first layer of the neural network. To begin, assume that we are given a dual (and thus critical) point $\\mathbf{x}_{dual}$ for some neuron $\\eta$. In prior work [5], computing the normal vector to the critical hyperplane induced by the neuron $\\eta$ was possible directly via finite differences, because they had the power to query the model $f$ at arbitrary points $\\mathbf{x} \\in \\mathbb{R}^{d_0}$. But now the only useful points our attack can sense are constrained to a piecewise linear $d_0 - 1$ dimensional subspace the set of points on the decision boundary of the neural network, and thus we must implement a slightly different attack, formalized in Algorithm 1.\nStep 1: Compute the $(d_0 - 2)$-dimensional dual space in the vicinity of $\\mathbf{x}_{dual}$.\nDefinition 3. The dual space $D_n$ of a neuron $n$ is the locally linear $d_0 - 2$ dimensional subspace which is the intersection between the $d_0 - 1$ dimensional critical hyperplane for $n$ and the $d_0 - 1$ dimensional decision boundary in the vicinity of $\\mathbf{x}_{dual}$."}, {"title": "5.2 Extracting deeper layers", "content": "Having shown how to recover the first layer, we now present our general algorithm. Without loss of generality, we assume that all previous layers of the network are extracted, and our goal is to extract the $i$-th layer.\nDefinition 4. The function that computes the first $i$ layers (up to and including $i$) of $f$ is called the input function and is denoted as $f_{1..i}$. In particular, $f = f_{1..r+1}$.\nBecause we have extracted the parameters of the model up to layer $i$, we can easily filter out and remove any dual points that belong to layers 1 through to $j$, by taking every dual point $\\mathbf{x}_{dual}$ and computing $f_{1..j}(\\mathbf{x}_{dual})$ for every $j < i$ and rejecting any dual point where there exists some layer $j$ and neuron $\\eta$ such that $f_{1..j}(\\mathbf{x}_{dual})$ is at a critical point.\nIdentifying layer-$i$ dual points To start, we consider all dual points that have not already been found to be part of some prior layer. Now, we repeat an algorithm very similar to the algorithm from subsection 5.1.\nDefinition 5. Let $f_{\\mathbf{x}_0;i}$ be the linear transformation satisfying $f_{\\mathbf{x}_0;i}(\\mathbf{x}_0) = f_i(\\mathbf{x}_0)$ and $\\frac{\\partial}{\\partial \\mathbf{x}} f_{\\mathbf{x}_0;i}(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}_0} = \\frac{\\partial}{\\partial \\mathbf{x}} f_i(\\mathbf{x})|_{\\mathbf{x}=\\mathbf{x}_0}$\nThat is, $f_{\\mathbf{x}_0;i}$ is just the linear transformation that the function $f_i$ defines within the linear region around the point $\\mathbf{x}_0$."}, {"title": "6 Sign Recovery", "content": "Recall that a neuron's signature is related to its actual weights by some arbitrary scaling factor $c$. Finding the correct sign of $c$ is crucial in order to correctly recover the model. Similarly to [4], we use a heuristic argument and develop a statistical test that is likely to identify the correct sign. We begin by providing the intuition behind our sign recovery technique and experimentally validating its key assumption (subsection 6.1). Next, we present the technique in algorithmic form (subsection 6.2)."}, {"title": "6.1 The Basic Technique", "content": "To correctly determine the sign of a target neuron, we require a property that differs measurably between the neuron's on- and off-sides.\nIn [4] the authors demonstrated that a small perturbation (neuron wiggle) in a carefully chosen direction is expected to change the target neuron's output by $\\epsilon$, while affecting all other neurons in the same layer by about $\\pm\\sqrt{\\epsilon}$, where $d$ is the target layer's width. Consequently, the output norms of the target layer, $||V_{on}||$ and $||V_{off}||$, differ between the on- and off-sides of the target neuron. Assuming that the neurons in the remaining layers behave randomly, the changes of the floating point values of the output logits also differ in their magnitude between the on- and off-side. This approach allowed the authors of [4] to directly measure these infinitesimal changes in the output logits of the neural network.\nOne key intuition for sign recovery without direct access to output logits is, that the average speed with which all future neurons change their values depends on the target layer output norms. Accordingly, the speed with which the average future neurons change their value on the target neuron's on-side, $s_{on}$, is larger than that on its off-side, $s_{off}$, see also Figure 6. The core assumption is that this speed difference correlates with the target layer's output norms, leading to the approximate magnitude estimates given in Equation 1 and Equation 2.\n$$s_{on} \\propto ||V_{on}|| \\approx ||(\\pm \\sqrt{\\epsilon}, ..., \\epsilon, ..., \\pm \\sqrt{\\epsilon})||$$\n$$s_{off} \\propto ||V_{off}|| \\approx ||(\\pm \\sqrt{\\epsilon}, ..., 0, ..., \\pm \\sqrt{\\epsilon})||$$ Any neuron toggling point introduces bends in the decision boundary, which allows us to measure $\\Delta_{on}$ and $\\Delta_{off}$. If a future neuron is approaching its toggling point $x_t$, a higher speed implies it will reach this point earlier than if moving at a slower speed (see Figure 6). The ratio between the two very rough theoretical estimates of speed given above is about $\\sqrt{2} \\approx 1.4$, while actual experiments yield an average speed ratio of about 1.2. This significant ratio of distances $\\Delta_{off}/\\Delta_{on}$"}]}