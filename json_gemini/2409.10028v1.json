{"title": "AttnMod: Attention-Based New Art Styles", "authors": ["Shih-Chieh Su"], "abstract": "What do the images in Figure 1 share in common? They are from the same diffusion\nmodel, same set of checkpoints, using the same prompt, with the same scheduler,\nthe same amount of denoising loops, and the same level of classifier-free guidance.\nThey are even from the same seed. The only difference among them is the attention,\nwhich varies on one or few attention blocks of the diffuser UNet, then varies during\nthe denoising loops. This work presents AttnMod, to modify attention for creating\nnew unpromptable art styles out of existing diffusion models. The style-creating\nbehavior is studied across different setups.", "sections": [{"title": "Introduction", "content": "Style transfer is a popular topic where the artistic style of one image is being applied to another content\nimage, therefore the generated image preserves the style and the content from the corresponding\nimages [1]. Due to the quality [2], the efficiency [3, 4] and the small footprint [5] of the diffusion\nbased generative models, several recent studies have investigates diffusion-based style transfer [6, 7].\nHowever, there is little effort spent on creating new art styles.\nIn this work, we open up the attention layers of the UNet within the text-to-image (T2I) Stable\nDiffusion model [5], to explore the artistic difference of the generated images according to attention\nmodifications, using exactly the same diffusion setup. In addition, We use different seeds with the\nsame text prompt to observe the artistic style drift, as well as the drift from different styling prompts\nbut fixing the seed. We then examine this method over different checkpoints and different models.\nIn-loop attention acceleration is further experimented."}, {"title": "Method", "content": "Imagine a human artist looking at the generated photo of a diffusion model, and hoping to create\na painting out of it. There could be some feature of the object in the photo that the artist wants\nto emphasize, some color to disperse, some silhouette to twist, or some part of the scene to be\nmaterialized. These intentions can be viewed as the modification of the cross attention from the text\nprompt onto UNet, during the desoising diffusion. Motivated by the idea of creating new art styles\nmimicking the human artist in putting unusual emphasis on some composure component(s) of the\nscene, we design the attention modification process, AttnMod, as illustrated in Figure 2.\nUNet is a component of the diffusion process. During the image generation, the seeded noise input is\nbeing denoised iteratively until the noise is fully removed. The text prompt is embedded and then\nused to \"condition\" the output during the denoising loops via the cross attention blocks of the UNet.\nWhen being trained, the attention blocks learn to correctly direct the UNet to denoise the noise input\ninto the training image, which pairs with the embedded prompt. In order to quantify the amount of\nattention used for conditioning, we introduce the attention multiplier.\nTo better observe how the multiplier impacts the generation, we start with the simplest example. One\narbitrary UNet attention block is selected to be modified, while the other blocks are fixed at the default\nattention which means a 1.0 multiplier. The selected block is then tested with the attention multiplier\nranging from -20.0 to 50.0 at the beginning of the denoising diffusion. Within the denoising loop,\nthe attention multiplier is further changed at a constant rate, ranging from -1.0 to 1.0. We select\n\"up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor\", or U1A1A2\nfor short, to be the modified attention block out of the Stable Diffusion 1.5 (SD15). The AttnMod\nresult is as shown in Figure 3. The original generation, with a starting attention multiplier of 1.0 and\nchange rate of 0.0 per loop, is marked out at the center. The starting multiplier increases toward the\nright, while the change rate increases toward the bottom.\nWhen the multiplier becomes negative, the amount of attention to condition the outcome is also\nnegative, at this particular block. Because the text prompt still conditions Other attention blocks\nnormally in the UNet, the generated image still follows the text prompt in a recognizable way. The\nmultiplier enables the T2I system to quantitatively emphasize or negate the text prompt. Occasionally,"}, {"title": "Qualitative Results", "content": "For clarity, we separate the AttnMod setup from the rest of diffusion inputs. Sharing the same\ndiffusion input means having the same seed, same prompt, same amount of classifier-free guidance,\nsame scheduler with the same configuration, same denoising iteration count, same LoRA setting if\nany, but not the same AttnMod setup.\nSecondly, extending from Figure 3, we standardize the approach to scan the combination of the\nstarting attention and its constant rate of change. We call it the attention scan of a certain attention\nblock. In an attention scan, the starting attention varies in the x-direction ranging from -20 to 50.\nThe constant rate of change goes from -1.0 to 1.0 in the y-direction. The scanning range covers\nmost generations of interest, which means the conditioning from the T2I prompt is recognizable in\nthe output image.\nMarked out close to its center, attention scan has a snapshot having 1.0 starting attention with 0.0 rate\nof change per denoising loop. This marked out snapshot is referred to as the default diffusion output."}, {"title": "Seeding", "content": "With fixed diffusion input, we conducted numerous single-block seed scans using the constant-rate\nAttnMod mentioned above, to study how the seed impacts the generation. For the same AttnMod\nsetup, which means the same starting attention, the same change rate, and at the same single block,\nthe output images are aligned into the same column in Figure 4. The main observation is that, the\noutput correlates with both the AttnMod setup and the default generation in the leftmost column in\nFigure 4, which is again decided by the seed. Therefore, a good AttnMod strategy is to start from a\nprompt and seed combination that generates an image with good visual layout."}, {"title": "Styling", "content": "Experiments are conducted on applying AttnMod over various existing art styles. As shown in\nFigure 5, the art styles are prompted in text and cover that of van Gogh, Ukiyo-E, Renoir, Picasso, da\nVinci and Dali accordingly as rows. All the images share the same seed 0 and the same diffusion\ninput except the style name, which is part of the text prompt.\nBoth the prompted style and the AttnMod setup influence the outcome. However, when the style\ninduced by the AttnMod is too strong, such as the fourth column in Figure 5, the prompted style\nbecomes less obvious in the generated image.\nTo provide more transitional detail of the contest between the AttnMod setup and the prompted style,\nwe illustrate a single layer AttnMod scan on top of two prompted styles in Figure 6. Surrounding"}, {"title": "Variable Change Rate", "content": "In addition to the constant rate of change, we further experiment with variable attention change rate.\nIn Figure 7, we compare the AttnMod on the U1A2A2 block of SD15 using fixed and variable rate,\nwhich produces the scan to the right. In that scan, the starting attention ranges the same from -20\nto 50, but the attention is changed at the variable rate from -0.2t (top row) to 0.2t (bottom row).\nTherefore the change rate in the rows of both scans does not align, except the middle row, in which\nboth scans share the same 0 in-loop change.\nObserving the overall outcome, having a non-constant attention change rate during denoising creates\nadditional artifacts. Across all the SD15 attention blocks, having positive acceleration tends to overly\nsharpen the outcome, while having negative acceleration tends to blur the outcome."}, {"title": "Multiple Attention Blocks", "content": "Starting from the 32 modifiable attention blocks in SD15, there is a huge amount of combinations of\nblocks to be modified. The most basic combination is picking the two attention blocks within the\nsame transformer block, and mod them synchronously with the same setup. We visualize two of such\ncombinations in rows of Figure 8, which contains the individual AttnMod scans from each of the two\nattention blocks, then the joined AttnMod scan of them toward the right.\nOn some occasions, the two-block AttnMod creates new art styles that were previously unseen\nfrom the single block AttnMods. More often, the influence from modding the first attention block\ndominates the two-block AttnMod output. When the outcome from modding the first attention block\nbecomes noisy, the two-block modding tends to stabilize it. Moreover, although influenced by both\nsingle block AttnMod, the two-block modding works differently than latent blending, and hence the\noutcome is different.\nBeyond the two-block attention modding, there are many more combinations, as well as other\nasynchronous modding methods to be explored. In Figure 9, we share a slice of possible exploration,\nwhere all 32 attention blocks in SD15 are opened up for modding and start with 0 attention. In each\ndenoising loop, one block is picked to gain 1.0 attention. The largest attention that a block can have is\ncapped at 1.0. This defines a 40-loop denoising strategy. We use two different approaches in picking\nblocks. The first one is to pick the available block causing the most difference in the output, compared\nto the previous output, with 1.0 attention gain. This approach attempts to include as much as the text\n\"conditioning\" within the strategy boundaries. On the other hand, the second approach avoids the\ninfluence as much as possible. Both approaches generate new art styles, so do further tweaking this\nstrategy or devising more sophisticated strategies."}, {"title": "Objects and Checkpoints", "content": "On SD15 and its variant checkpoints, Figure 10 visually compares the AttnMod output. The top\nsection contains generations using the same diffusion input as in Figures 1,3,7,8. Those in the middle\nsection are from a different set of checkpoints, RealisticVisionV5.1 [8]. Across different model\ncheckpoints, AttnMod creates different art styles even with the same setup.\nThe bottom section is from the same official SD15 checkpoints like the top section, with a different\ntext prompt about a different scene. Since the text prompt plays a role when AttnMod modifies the"}, {"title": "SDXL", "content": "AttnMod also works on SDXL [9]. Compared to the 32 attention blocks in SD15, SDXL has 140\nblocks. In Figure 11, we only show the AttnMod scan on two of them. SDXL is more sensitive\nto the rate of change in attention than SD15 is. Thus the range of attention change rate is reduced\nto be between -0.5 and 0.5, while the starting attention range being the same as in SD15 scans.\nBeing shown here is only the most basic mod on SDXL. The multi-block AttnMod is left for future\nexploration.\nIn addition to its many attention blocks, SDXL also has two different text prompt encoders for\nconverting text to embeddings. The final embedding composure can also be parameterized into the\nAttnMod setup and again, to be explored."}, {"title": "Ablation Study", "content": "Looking back at the AttnMod scan figures, the very tile image to the right of the marked out default\ndiffusion output is of zero conditioning from the text prompt. This means 0.0 starting attention and\n0.0 in attention change rate. For all the one- and two-block AttnMod scans performed, ablating the\ntext prompt attention alone has a minor impact on the output image, compared to the default diffusion\noutput.\nTo create new art styles in these scans, we need either starting with a large enough attention drift,\nwhose absolute value larger than 2.0, or changing the attention at the rate whose absolute value is\nlarger than 0.2, for a thirty-loop denoising."}, {"title": "Conclusion", "content": "In this work, AttnMod has been introduced to modify the way how a diffusion system uses the\nattention to condition the generated image with the input text prompt. Along with the text prompt,\nAttnMod creates new art styles. This is useful when the creator has only vague ideas about the\ngenerated art, or when the art style in mind is hard to prompt."}]}