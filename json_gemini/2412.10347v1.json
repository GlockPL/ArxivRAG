{"title": "COMET: BENCHMARK FOR COMPREHENSIVE BIOLOGICAL MULTI-OMICS EVALUATION TASKS AND LANGUAGE MODELS", "authors": ["Yuchen Ren", "Wenwei Han", "Qianyuan Zhang", "Yining Tang", "Weiqiang Bai", "Yuchen Cai", "Lifeng Qiao", "Hao Jiang", "Dong Yuan", "Tao Chen", "Siqi Sun", "Pan Tan", "Wanli Ouyang", "Nanqing Dong", "Xinzhu Ma", "Peng Ye"], "abstract": "As key elements within the central dogma, DNA, RNA, and proteins play crucial roles in maintaining life by guaranteeing accurate genetic expression and implementation. Although research on these molecules has profoundly impacted fields like medicine, agriculture, and industry, the diversity of machine learning approaches-from traditional statistical methods to deep learning models and large language models poses challenges for researchers in choosing the most suitable models for specific tasks, especially for cross-omics and multi-omics tasks due to the lack of comprehensive benchmarks. To address this, we introduce the first comprehensive multi-omics benchmark COMET (Benchmark for Biological COmprehensive Multi-omics Evaluation Tasks and Language Models), designed to evaluate models across single-omics, cross-omics, and multi-omics tasks. First, we curate and develop a diverse collection of downstream tasks and datasets covering key structural and functional aspects in DNA, RNA, and proteins, including tasks that span multiple omics levels. Then, we evaluate existing foundational language models for DNA, RNA, and proteins, as well as the newly proposed multi-omics model, offering valuable insights into their performance in integrating and analyzing data from different biological modalities. We observed that DNA, RNA, and protein models could be applied to tasks across different omics by leveraging initialized embeddings, with protein models demonstrating superior performance across various omics. Through the evaluation of multi-omics tasks, we identified significant gaps in the capabilities of current models to address these challenges, highlighting substantial opportunities to enhance multi-omics integration and improve overall performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Driven by curiosity about uncovering the fundamental principles of life sciences, humans have never ceased exploring the microscopic mechanisms of biological processes. DNA, RNA, and proteins, as fundamental molecules of the central dogma (Crick, 1970), play critical roles in sustaining life. Through their interrelated functions, they ensure the accurate expression and execution of genetic instructions, making them central to all biological processes. Current research on these three types of molecules has already had widespread and profound impacts across multiple fields. For example, gene sequencing and editing technologies have made early diagnosis and treatment of hereditary diseases possible (Le, 2020). Genetic modification has enabled efficient and targeted crop improvement (Ahmar et al., 2020). The analysis of protein structure and function has driven advancements in targeted drug design and the application of industrial enzymes (\u015aled\u017a & Caflisch, 2018; Chapman et al., 2018). Consequently, deepening our understanding of how these molecules interact and function is crucial for unraveling the complex mechanisms underlying biological processes and driving technological and application advancements in agriculture, industry, and medicine."}, {"title": "2 RELATED WORK", "content": "Biology language models. The rapid evolution of biology language models has significantly advanced computational biology by leveraging natural language processing techniques to analyze biological sequences. DNABERT (Ji et al., 2021), DNABERT2 (Zhou et al., 2023), the Nucleotide Transformer (Dalla-Torre et al., 2023) and Genomics-FM (Ye et al., 2024) adapt the BERT architecture for DNA sequences, but differ primarily in terms of tokenizer design, model parameters and pre-trained data. HyenaDNA (Nguyen et al., 2024) models long-range dependencies at single-nucleotide resolution. The emergence of foundational RNA models, including RNA-FM (Chen et al., 2022), BEACON-B (Ren et al., 2024) and UTR-LM (Chu et al., 2024) utilize sophisticated language modeling techniques. These models have demonstrated the ability to address a wide array of RNA-related tasks, thereby offering deeper insights into RNA biology. In protein analysis, The ESM family (Rives et al., 2021; Meier et al., 2021; Lin et al., 2023; Hayes et al., 2024) highlights the impact of scaling unsupervised learning through the use of large protein sequence datasets and a transformer-based architecture capable of capturing complex dependencies within sequences. For multi-omics models, LucaOne (He et al., 2024) and CD-GPT (Zhu et al., 2024) unify nucleic acid and protein data, while differ in training method. In the realm of cross-omics, Calm (Outeiral & Deane, 2024) introduces codon embeddings to protein language models, enhancing predictive performance by leveraging biological data containing richer signals. Boshar et al. (2024) explore the efficacy of genomic language models on protein tasks. Prakash et al. (2024) explore the possibility of applying pre-trained DNA and protein models to RNA tasks. These advancements underscore the transformative potential of large-scale language models in decoding genomic, transcriptomic, and proteomic complexities, fostering breakthroughs in molecular biology, and protein science.\nBenchmarks in biological language. In the field of biological language processing, benchmarks have been instrumental in driving advancements across various molecular research areas. For DNA, significant contributions include Genomic Benchmarks (Gre\u0161ov\u00e1 et al., 2023) and BEND (Marin et al., 2023), which collect diverse DNA tasks like gene finding, enhancer annotation, and CpG methylation. (Kao et al., 2024) introduces the evaluation of models for long-range tasks. RnaBench (Runge et al., 2024) primarily focuses on RNA secondary structure and design tasks. BEACON (Ren et al., 2024) is introduced to evaluate language models on RNA tasks and proposes a"}, {"title": "3 BENCHMARK TASKS", "content": "The following sections provide detailed information for 17 diverse tasks, including data statistics, evaluation metrics, and data sources as shown in Table 1."}, {"title": "3.1 DNA TASK", "content": "Gene Expression (GE) predicts the expression levels of genes and transcription factors (TFs) across diverse tissues, recorded in target $y \\in \\mathbb{R}$. We extracted 1575 TF expression datasets from the GTEx database after filtering out non-expressed TFs, ensuring high-quality and tissue-specific expression profiles. Additionally, we integrated gene expression data from Xpresso (Agarwal & Shendure, 2020), covering 56 tissues, and grouped them into 11 functional and regional categories, enabling a comprehensive analysis of cross-tissue gene expression. This regression task aims to evaluate the model's ability to predict gene and TF expression levels using the $R^2$ value as the evaluation metric.\nImpact: Gene expression is crucial for understanding transcriptional regulation and functional dynamics across the genome. Accurately predicting expression levels can elucidate the regulatory networks underlying different cellular states and tissue-specific functions, offering deeper insights into how genetic and environmental factors jointly influence gene expression and contribute to phenotypic variation and disease mechanisms.\nEnhancer Activity Prediction (EA) is a regression task that predicts enhancer activity for two promoters associated with distinct developmental and housekeeping transcriptional programs directly from the DNA sequence. The enhancer activity dataset released in (de Almeida et al., 2022) comprises 484,052 DNA sequences, each 249 nucleotides in length, measured for their quantitative enhancer activity towards either a developmental or a housekeeping promoter by a continuous target variable $y \\in \\mathbb{R}$. We employ PCC as the metric.\nImpact: Enhancers are essential genomic elements that regulate cell type-specific transcription of target genes, influencing animal development and physiology. Their ability to activate transcription outside their native contexts suggests that critical regulatory information resides within their DNA sequences. Mutations in enhancers can alter their function, leading to developmental defects"}, {"title": "3.2 RNA TASK", "content": "APA Isoform Prediction (APA) predicts the polyA site strength for each variant, represented as target $y \\in \\mathbb{R}$. We filter 228k sequences from Bogard's dataset (Bogard et al., 2019) containing over 3 million APA reporter gene data. This regression task evaluates the proportion of proximal APA isoforms with the performance metric being the $R^2$ value.\nImpact: Alternative polyadenylation is a key regulatory mechanism that diversifies RNA transcripts and protein isoforms through 3' UTR processing. By influencing transcription termination and interacting with RNA splicing, APA modulates gene expression, impacting various cellular functions.\nProgrammable RNA Switches (PRS) are synthetic RNA molecules designed to regulate gene expression by responding to specific RNA sequences. Each switch exists in one of three activity states, ON, OFF, or ON/OFF, depending on the presence or absence of its trigger RNA sequence. The target $y \\in \\mathbb{R}^3$ represents the three activity states. The dataset (Angenent-Mari et al., 2020) includes 91,534 in vivo toehold switches, covering 23 viral genomes and 906 human transcription factors. The activity of these switches is measured using GFP signal intensity, which provides a quantitative readout of switch performance. The effectiveness of these switches is evaluated using the $R^2$ metric.\nImpact: Programmable RNA switches provide precise control of gene expression and cellular functions, making them essential in synthetic biology for manipulating biological processes both in vitro and in vivo. These RNAs act as responsive elements to small molecules, proteins, or nucleic acids, allowing fine regulation of cellular behavior. Therapeutically, they promise to enable targeted treatments by detecting disease-specific signals and triggering precise cellular responses, contributing to novel approaches in precision medicine and synthetic biology.\nSecondary Structure Prediction (SSP) identifies paired nucleotide regions in stems and unpaired nucleotide regions in loops, bulges and junctions within RNA molecules. In a RNA molecule with a length of l, The target matrix $y \\in \\mathbb{R}^{lxl}$ indicates whether each nucleotide and other nucleotides form a base pair. We utilize the bpRNA-1m database (Danaee et al., 2018). The performance metric for this task is the F1 score.\nImpact: Accurate secondary structure prediction is paramount for elucidating the intricate mechanisms underlying function and dynamics. By mapping these structures, researchers gain insights that advance genetic research and guide RNA-based therapeutic development, supporting innovations in precision medicine."}, {"title": "3.3 PROTEIN TASK", "content": "Thermostability Prediction (Ther) is a regression task that aims to predict the stability of proteins at high temperatures. From the Thermostability task of FLIP (Dallago et al., 2021), we apply the 'Human-cell' splits. We use the Spearman correlation coefficient (SCC) as the metric.\nImpact: Protein thermostability prediction advances our understanding of protein functions and properties. In industrial enzyme applications, developing highly thermostable enzymes is essential for operating under harsh reaction conditions (Wu et al., 2023). Such predictions facilitate directed evolution and selection of proteins, which is of great significance for drug and vaccine discovery (Chen & Gong, 2022).\nEnzyme Commission Number Prediction (EC) involves annotating protein sequences. The data is sourced from the EC benchmark established in DEEPFRI (Gligorijevi\u0107 et al., 2021). We collect the corresponding codon sequences for the dataset simultaneously. It retains over 90% of the original samples after filtering out anomalous data. We use Fmax as the metric.\nImpact: Identifying enzymes and their catalytic reaction types is crucial for understanding enzyme functions. This can accelerate the discovery of new enzymatic activities and improve the functions of existing enzymes. In the field of drug discovery, it can assist in designing enzymes with specific catalytic activities, supporting the development of novel drugs and therapies (Chautard et al., 2009).\nContact Map Prediction (Cont) aims to forecast interactions between amino acid residues in a protein. Given a protein's amino acids sequence, the target is to predict which residues are in close"}, {"title": "3.4 MULTI-MOLECULAR TASK", "content": "Enhancer-Promoter Interaction Prediction (EPI) is a single-label classification task in genomics that aims to identify interactions between enhancers and promoters sequence, with the categorical label $y \\in \\{0,1\\}$. The dataset, sourced from EPI-DLMH (Min et al., 2021), comprises six cell lines-GM12878, HUVEC, HeLa-S3, IMR90, K562, and NHEK. We sample to balance true EPIs and non-EPIs. We use the Matthews Correlation Coefficient (MCC) as the metric.\nImpact: Enhancers are regulatory elements that can significantly enhance the transcription of genes located at varying distances, while promoters are essential regions where transcription begins. Understanding these interactions is crucial for deciphering the complex regulatory networks that govern cellular functions and can provide insights into developmental biology and disease mechanisms.\nsiRNA Efficiency Prediction (siRNA) is a regression task that aims to predict the silencing efficiency of different siRNAs. By inputting artificially modified siRNA sequences the target mRNA sequence, the model can estimate how effectively each siRNA silences its corresponding mRNA. This is a regression task, and we use Mixed Score as the evaluation metric A.5.1. The data utilized in this research are from SAIS (SAIS, 2020).\nImpact: RNA interference (RNAi) is a natural gene expression regulation mechanism that reduces target protein levels by inhibiting the expression of target genes, typically achieved through siRNAs (Setten et al., 2019). With the success of mRNA vaccines in COVID-19 prevention, there has been growing interest in the development of nucleic acid-based drugs. Predicting the silencing efficiency of chemically modified siRNA sequences under the RNAi mechanism is crucial, as this metric is directly linked to the actual therapeutic efficacy of the drug.\nAntibody-Antigen Neutralizability Prediction (AAN) is used to assess whether there is an interaction between an antigen and an antibody, making it a single-label classification task with the categorical label $y \\in \\{0,1\\}$. The goal is to predict whether a given antibody can bind to a specific antigen based on their sequences. This task is based on the HIV data from CATNAP (Yoon et al., 2015) and DeepAAI (Zhang et al., 2022). We use MCC as the metric.\nImpact: To demonstrate the neutralizing effects of most natural and synthetic antibodies against any antigen, time-consuming, labor-intensive, and costly wet lab experiments are typically required (Lee et al., 2007). However, with machine learning, we can represent the neutralizing response of antibodies (Ab) from the perspective of Ab-Ag neutralization effects, highlighting similarities in binding"}, {"title": "4 MODELS", "content": "We consider two types of baseline models in our benchmarks, including naive supervised models and pre-trained omics language models."}, {"title": "5 RESULTS", "content": null}, {"title": "5.1 TRAINING SETUPS", "content": "To facilitate a rigorous comparison, we conduct comprehensive fine-tuning on all BERT-based pre-trained language models, including DNABERT-2, NTv2, RNA-FM, BEACON-B, ESM-1b, ESM-2, LucaOne, and CaLM. While all models are fine-tuned using identical training hyperparameters, LucaOne is subjected to LoRA fine-tuning, whereas full-parameter fine-tuning is applied to the remaining models. For the simpler supervised models (CNN, ResNet, and LSTM), we initialize training from scratch with analogous training configurations. We use and search the learning rate from 1 x 10-6 to 5 \u00d7 10-3 and keep its batch size to 32. All experiments are conducted on NVIDIA A100 GPUs."}, {"title": "5.2 SINGLE-MOLECULAR BENCHMARK RESULTS", "content": "In Table 3, we report the benchmark results on single-molecular tasks, including literature SOTAs, naive supervised models and existing omics language models. Literature SOTAs include Xpresso (Agarwal & Shendure, 2020), DeepSTARR (de Almeida et al., 2022), APARENT (Bogard et al., 2019), MLP-O (Angenent-Mari et al., 2020), UFold (Fu et al., 2022), MSATrans (Rao et al., 2021), ESM-1v (Meier et al., 2021), SaProt-GearNet (Su et al., 2023).\nRandomly initialized vocabulary embeddings show other omics knowledge learned during pre-training. By fine-tuning with only replacing vocabulary embeddings, the pre-trained models for each omics can achieve comparable results on most other omics tasks. This indicates that the omics knowledge learned during pre-training is not only stored in word embeddings, but also has a considerable proportion in the encoder, and can achieve rapid knowledge transfer of different omics through embedding replacement. And it can also help to explore commonalities and connections between different omics."}, {"title": "5.3 CROSS-MOLECULAR BENCHMARK RESULTS", "content": "In Table 4, we compare the performance of existing protein models and DNA/RNA/CDS models on amino acid sequences and their corresponding codon sequences (CDS).\nCDS model demonstrates competitive performance on codon sequence data. Experimental comparisons indicate that protein-based models applied to amino acid sequences outperform CaLM applied to corresponding codon sequences. This superior performance may be attributed to ESM models being pre-trained on extensive unlabeled protein data, which include numerous mutated sequences. Consequently, the amount of pre-training data for ESM models is significantly larger than that for CaLM, which was pre-trained on codon sequences. But we observe that on Beta-Lac and Flu tasks, codon sequence-based methods achieved comparative performance, indicating potential in downstream tasks related to codon-specific information. This finding highlights the importance of considering codon-level biases and genomic context for downstream biological applications."}, {"title": "5.4 MULTI-MOLECULAR BENCHMARK RESULTS", "content": "In Table 5 and Table 6, we explore the combination of existing single-omics models and the multi-omics model and other models for the multi-molecular tasks. Literature SOTAs include EPI-DLMH (Min et al., 2021), DeepAAI (Zhang et al., 2022), ncRPI-LGAT (Han & Zhang, 2023) and DeepCRISPR (Chuai et al., 2018).\nMulti-omics model can perform better than single-molecular models. Without freezing the backbone, LucaOne performs exceptionally well on many tasks such as siRNA, EPI, and RPI. It surpasses both the combination of the two single-omics models and the naive supervised model, highlighting the effectiveness of multi-omics models trained on integrated datasets. This demonstrates that a unified multi-omics representation can capture cross-omics dependencies better than combining task-specific single-omics models, particularly when the backbone remains trainable.\nMulti-molecular tasks still present significant challenges. In the AAN, RPI, and CRI-Off tasks, neither the approach of combining two single-omics models nor using the multi-omics model LucaOne outperforms SOTA methods. This indicates that while multi-omics models like LucaOne show promise, they struggle in tasks requiring highly specialized architectures or domain knowledge, suggesting the need for further architectural innovations and task-specific adaptations."}, {"title": "6 CONCLUSION", "content": "In this work, we present COMET, the first comprehensive multi-omics benchmark, which encompasses 17 diverse tasks spanning DNA, RNA, Protein, cross-molecule and multi-molecule study. COMET aims to address the critical gap in standardized evaluation for kinds of omics models in biology. We explore the connections between models from different omics across various tasks, gaining insights into tasks in one omic can benefit from models trained on another. We also find that multi-omics tasks still present certain challenges. These discoveries will inform the design of future biological language models, promoting interaction and understanding among different omics rather than studying each omic in isolation. However, the current benchmark involves relatively limited models and tasks, and some downstream tasks that require additional inputs are not yet aligned. In the future, we plan to further expand the models and tasks covered, closely follow developments in cross-omics and multi-omics research, and explore the potential connections between different omics data more thoroughly."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 OMICS TASK PIPELINE", "content": "These are detailed pipelines conducted in experiments and shown in Figure 2.\nSingle-Molecular Task In single-molecule tasks, in addition to testing with models and tasks belonging to the same molecule, we also evaluate the performance of models and tasks across different molecules. To address the issue of inconsistent vocabularies, we reinitialized the vocab embeddings while loading the pretrained weights. For example, DNA/RNA models initialize and replace the nucleotide vocab embedding with initial amino acids vocab embedding when using protein data.\nCross-Molecular Task Cross-molecule tasks involve the corresponding protein sequences and codon sequences. Therefore, we first use open-source tools and datasets to collect and construct the CDS-Protein data. Subsequently, we can input CDS data for DNA/RNA/CDS models and input"}, {"title": "A.2 BIOLOGY TASK PIPELINE", "content": "Sequence Level Prediction The architecture for sequence-level prediction tasks varies depending on whether the task involves single-molecule or multi-molecule scenarios.\nFor single-molecule sequence-level predictions, we utilize different strategies based on the model type. For naive supervised models, we compute an attentive weighted sum of all nucleotides to form a single sequence representation, which is then passed through an MLP to produce the final predictions. In contrast, when using language models, the [CLS] token representation is extracted and fed into a classifier layer to obtain the output predictions.\nFor multi-molecule sequence-level prediction tasks, the two interacting molecular sequences are processed independently using the same approaches as in the single-molecule scenario. Specifically, each molecule's sequence is encoded separately using either a naive supervised model or a language model to generate individual representations. The resulting embeddings are then concatenated. In specific cases, such as sgRNA and CRI-Off, additional data features are incorporated along with the concatenated embeddings. This combined representation is passed through an MLP layer to generate the final sequence-level predictions.\nNucleotide Level Prediction To investigate the relationships between nucleotides, we calculate the self-outer product of the nucleotide representations, resulting in a matrix that captures the pairwise interactions among nucleotides. This interaction matrix is then processed through a simple ResNet architecture to produce the final output.\nAmino Acids Level Prediction To study the relationships between residue pairs, we employed two approaches. For the transformer-based foundational language model, we extracted the attention matrix from the backbone encoder and appended a linear layer to directly predict the residue pair relationships. For the naive supervised model, we computed the feature matrix by calculating the point-wise product between token pairs, followed by a linear layer for relationship prediction."}, {"title": "A.2.1 MAX SEQUENCE LENGTH", "content": "Table 7 presents the maximum nucleotide and protein sequence lengths for tokenizers of each language model. Models such as DNABERT2, NTv2, and CaLM utilize relative positional encoding, providing excellent scalability to handle long sequences. Additionally, the BPE, non-overlap 6-mer, and non-overlap 3-mer tokenizer efficiently reduce the number of tokens, enabling these models to accommodate longer sequences under memory constraints. As a result, the maximum sequence lengths for DNABERT2, NTv2, and CaLM are set to 6000, 6002, and 3000, respectively. If a nucleotide or protein sequence exceeds the maximum length during tokenization, the tokenizer truncates the sequence to the specified limit, preserving the leftmost portion of the sequence."}, {"title": "A.3 EXPERIMENTAL SETTINGS FOR TASKS", "content": null}, {"title": "A.3.1 DNA TASKS", "content": "DNA tasks, including Gene expression and enhancer activity prediction are trained using the settings shown in Table 8."}, {"title": "A.3.2 RNA TASKS", "content": "APA isoform prediction and programmable RNA switches are trained using the settings shown in Table 9. And secondary structure prediction is trained using the settings shown in Table 10."}, {"title": "A.3.3 PROTEIN TASKS", "content": "All protein tasks, including thermostability prediction, enzyme commission number prediction and contact map prediction are trained using the settings shown in Table 11."}, {"title": "A.3.4 CROSS-MOLECULER CDS/PROTEIN TASKS", "content": "In cross-molecule tasks, when the inputs are codon sequences, the settings used are shown in Table 12, while when the inputs are protein sequences, the settings used are shown in Table 13."}, {"title": "A.3.5 MULTI-MOLECULER TASKS", "content": "EPI, siRNA and AAN tasks are trained using the settings shown in Table 14. And RPI, CRI-Off tasks are trained using the settings shown in Table 15. The training settings of DPF are shown in Table 16."}, {"title": "A.4 DETAILED DATA PREPROCESSING FOR EACH TASK", "content": null}, {"title": "A.4.1 GENE EXPRESSION", "content": "We adopt the data processing methodology from Xpresso (Agarwal & Shendure, 2020). Human gene expression data comes from the Epigenomics Roadmap Consortium, which provides normalized RNA-seq values for protein-coding mRNAs across 56 tissues and cell lines.\nDue to the large number of parameters in biological language models and the memory limitations of A100 GPUs, our experiments show that trimming sequence lengths to 6000 bp ensures compatibility with all models for processing input sequences. By inputting consecutive 6000 bp nucleotide fragments from different positions in the processed sequences into the Xpresso model, we identify that the sequence indexed from position 7000 to 12999 (length 6000 bp) achieves optimal test performance. This segment contains the most information related to gene expression levels.\nFor training, we use the 6000 bp nucleotide sequence indexed from position 7000 to 12999 as input and the expression data for 56 tissues as labels. The train, validation, and test dataset splits follow the methodology used in Xpresso."}, {"title": "\u0391.4.2 \u0395\u039dHANCER ACTIVITY PREDICTION", "content": "We follow the processing procedure described in (de Almeida et al., 2022). The data includes sequence information and transcriptional activity metrics for both Drosophila and humans, encompassing developmental and housekeeping transcriptional activity levels.\nWe use downloaded sequences of 249 bp in length, along with Dev_log2_enrichment-scaled and Hk_log2_enrichment-scaled, which respectively represent developmental and housekeeping transcriptional activity information. The dataset is divided into training, validation, and test sets according to the method outlined in (de Almeida et al., 2022)."}, {"title": "A.4.3 APA ISOFORM PREDICTION", "content": "The preparation for IPA isoform analysis begins by filtering raw sequencing reads from all MPRAS (Shigaki et al., 2019) to retain only high-quality, full-length RNA sequences. These reads are grouped based on the randomized regions located upstream of the proximal polyadenylation site (pPAS), forming a dictionary of sequence variants for each library. To expand this dictionary, sequencing is also performed on the plasmid library, capturing members that lack expression of a distal isoform. RNA reads are then matched to dictionary entries by identifying the upstream region with the shortest Hamming distance.\nPolyadenylation cleavage sites are determined for each mapped read by detecting the presence of a Poly-A tail. The cleavage positions are recorded as vectors associated with individual sequence variants, including a specific position for reads mapping to non-random distal sites. The dataset generated from this process consists of a dictionary of distinct sequence variants paired with vectors of cleavage position counts. A final filtering step ensures data quality by discarding sequences supported by fewer than 10\u201320 unique UMI RNA reads or those containing over 75% A-nucleotides within a 12\u201320 bp region, which could indicate internal priming artifacts.\nWe process data from 12 random 3' UTR libraries. 9 among the 12 libraries are used for training and 3 held out (the 3 held-out libraries were excluded from the current analysis). To construct a balanced test set, sequences from each library are first shuffled independently according to their read counts. These shuffled sequences are then merged using a round-robin approach, selecting one sequence from each library at a time in descending order of read count. This strategy ensures that the test set contains an even representation of high-read count sequences across all libraries. The remaining sequences are appended to the beginning of the combined library, and the training set is further shuffled to enhance randomness.For benchmarking purposes, the top 10% of high-read count sequences are prioritized. Among these, the most abundantly expressed sequences are selected for testing, ensuring a high-quality, balanced dataset for training, validation, and evaluation."}, {"title": "A.4.4 PROGRAMMABLE RNA SWITCHES", "content": "We adopt the data generation pipeline described in (Angenent-Mari et al., 2020). A toehold-switch library comprising 244,000 potential trigger sequences is designed and synthesized, covering the complete genomes of 23 pathogenic viruses, the entire coding regions of 906 human transcription factors, and approximately 10,000 random sequences. Using this synthesized oligo pool, two construct libraries are created to represent the ON and OFF states, and both are transformed into BL21 E. coli. The OFF library includes toehold-switch constructs without triggers, while the ON library contains identical toeholds paired with complementary triggers fused to their respective switches.\nThe libraries are sorted into four bins using fluorescence-activated cell sorting (FACS), and the variants in each bin are quantified through next-generation sequencing (NGS) to determine their fluorescence distributions. After quality control, the toehold-switch library consists of 109,067 ON-state measurements, 163,967 OFF-state measurements, and 91,534 ON/OFF paired ratios, where both states are characterized for each switch. ON and OFF data are normalized to a scale of 0 to 1, with ON/OFF ratios normalized to a range of -1 to 1. Following (Angenent-Mari et al., 2020), a stringent quality control process is applied to eliminate artifacts and ensure data reliability. The quality control (QC) framework includes five levels: QC1, QC2, QC3, QC4 and QC5, where QC1 represents the lowest quality and QC5 the highest. Datasets above QC2 are utilized for training, while QC5 is reserved for testing."}, {"title": "A.4.5 SECONDARY STRUCTURE PREDICTION", "content": "We follow the preprocessing steps outlined in the bpRNA-1m dataset (Danaee et al., 2018).To reduce sequence redundancy and improve dataset diversity, we implement an 80% sequence-identity threshold and cap the maximum sequence length at 500 nucleotides, following protocols described in the referenced studies. These measures are essential for minimizing overfitting and ensuring that the models are trained on a wide range of genetically diverse samples.\nThe dataset is divided into three subsets: a training set (TR0), a validation set (VL0), and a test set (TS0). The splitting process is randomized to eliminate potential biases and ensure an unbiased evaluation of the model's performance."}, {"title": "A.4.6 PROTEIN TASKS", "content": "We obtain data of thermostability prediction, enzyme commission number prediction and contact map prediction from Saprot (Su et al., 2023). Following the guidance on github, we download data and place in the LMDB folder for supervised fine-tuning."}, {"title": "A.4.7 CROSS-MOLECULER TASKS", "content": "For the enzyme commission number prediction task, to obtain the codon information corresponding to protein sequences, we use UniProtKB mapping function to convert UniProt IDs into European Nucleotide Archive entries. We then employ the Smith-Waterman algorithm to quickly match the corresponding codon sequences, filtering out all sequences that contained unknown nucleotides or where the number of matched nucleotides is not a multiple of three. For other cross-omics tasks, we adopt the data and settings from (Boshar et al., 2024)."}, {"title": "\u0391.4.8 \u0395\u039dHANCER-PROMOTER INTERACTION PREDICTION", "content": "We follow the processing of (Min et al., 2021). We derive dataset from EPIANN (Mao et al., 2017), which includes six cell lines, GM12878, HeLa-S3, IMR90, K562, HUVEC and NHEK. To address the challenge of data imbalance, EPIANN enhanced the representation of positive samples by incorporating the upstream and downstream regions of enhancers. This approach expanded the dataset to include relevant genomic regions by defining extended windows of 3 kbp around enhancers and 2 kbp around promoters, ensuring a more comprehensive capture of the surrounding regulatory landscape."}, {"title": "A.4.9 SIRNA EFFICIENCY PREDICTION", "content": "We get the dataset from SAIS (SAIS, 2020). We use the information of the reference sequence of the target gene, the sense sequence of the target gene, the sense sequence of modified siRNA and the remaining percentage of mRNA after the experiment named gene_target_seq, siRNA_sense_seq, modified_siRNA_sense_seq and mRNA_remaining_pct in dataset from SAIS, respectively."}, {"title": "A.4.10 ANTIBODY-ANTIGEN NEUTRALIZABILITY PREDICTION", "content": "We follow (Zhang et al., 2022), which provides a minimal dataset specifically designed for this prediction task. This task is based on two datasets: CATNAP (Yoon et al., 2015), which focuses on HIV, and CoVAbDab (Raybould et al., 2021), which pertains to SARS-CoV-2.\nHIV data is sourced from CATNAP in the Los Alamos HIV Database. Antibody (Ab) and antigen (Ag) sequences are extracted, curated to remove duplicates and missing values, and classified as neutralizing (IC50 < 10 \u00b5g/ml) or non-neutralizing (IC50 \u2265 10 \u00b5g/ml). Seen and unseen Abs are split, ensuring no overlap between training, validation, and testing sets by excluding similar pairs (BlastP > 90%). Training is conducted on seen Abs, with unseen Abs used for evaluation across 20 random dataset splits.\nSARS-CoV-2 Data is collected from CoVAbDab and includes pairwise Ab-Ag instances across variants like Alpha, Beta, Delta, and Omicron. Five sequences per variant and 11 for Omicron are used. Omicron is treated as an unseen Ag, excluded from training but incorporated in relation graphs for transductive learning, enabling the identification of broad-spectrum Abs."}, {"title": "A.4.11 RNA-PROTEIN INTERACTION PREDICTION", "content": "The dataset is sourced from NPInter2.0 (Yuan et al., 2014), NPInter2.0_lncRNA (Zhao et al., 2018), and RPI7317 (Fan & Zhang, 2019). The sequences of ncRNAs and proteins are obtained from the NONCODE database (Bu et al., 2012), Gencode database (Frankish et al"}]}