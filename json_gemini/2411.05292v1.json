{"title": "SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection", "authors": ["Yun Zhao", "Zhan Gong", "Peiru Zheng", "Hong Zhu", "Shaohua Wu"], "abstract": "More and more research works fuse the LiDAR and camera information to improve the 3D object detection of the autonomous driving system. Recently, a simple yet effective fusion framework has achieved an excellent detection performance, fusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space. In this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for accurate 3D object detection, which follows the BEV-based fusion framework and improves the camera and LiDAR encoders, respectively. Specifically, we perform the camera-based depth estimation using a cascade network and rectify the depth results with the depth information derived from the LiDAR points. Meanwhile, an auxiliary branch that implements the 3D object detection using only the camera-BEV features is introduced to exploit the camera information during the training phase. Besides, we improve the LiDAR feature extractor by fusing the multi-scaled sparse convolutional features. Experimental results demonstrate the effectiveness of our proposed method. Our method achieves 77.6% NDS accuracy on the nuScenes dataset, showcasing superior performance in the 3D object detection track.", "sections": [{"title": "1. Introduction", "content": "3D object detection plays an indispensable role in an autonomous driving perception system, which recognizes and localizes the object in the 3D traffic environment [2, 3, 14, 32, 36]. Multiple sensors have been equipped on self-driving vehicles to obtain sufficient and accurate perception results. The camera and LiDAR sensors are extensively investigated among all the onboard sensors. The LiDAR points [10, 27, 38] provide precise localization and geometry information, and the cameras [7, 14, 35] offer abundant semantic information. As these two kinds of sensors provide complementary characteristics, a lot of research works [1, 15, 16, 33, 39] fuse the LiDAR and camera data to enhance the 3D object detection performance.\nRecently, the BEV-based perception method has received considerable attention as it is an intuitive representation of driving scenarios [3, 21] and is fusion-friendly for multi-view cameras [6, 7, 14] and different kinds of sensors [1, 16, 19]. A series of methods [1, 13, 37] utilize a transformer-based architecture to fuse the LiDAR and camera information by performing the cross-attention on the LiDAR features and image features. Differently, some works [16, 19] implement the LiDAR-camera fusion based on the aligned BEV feature maps. Though simple, the BEV-based fusion framework achieves an excellent detection performance. In this paper, we build a LiDAR-camera fusion framework based on the BEVFusion [16] by further exploiting the camera information and improving the LiDAR feature extractor.\nFor exploiting the camera information, we enhance the depth estimation module and bring an auxiliary detection branch. The depth estimation module plays a crucial role in camera-based 3D object detection. A precise depth result benefits the feature alignment when fusing the LiDAR and camera BEV feature maps. So, we introduce a two-stage cascade network for better image-based depth estimation and rectify the estimated depth map with the depth information derived from the LiDAR points. The LiDAR modality plays a prominent role against the camera modality when integrating information from LiDAR and camera data. To further exploit the camera information during jointly training the whole model, we introduce an auxiliary branch that utilizes only the camera-BEV feature to implement 3D object detection.\nBesides, we improve the LiDAR feature extractor by fusing the multi-scaled sparse convolutional features. To reduce the consumption of computation and memory cost, the 3D voxel features are first encoded into the BEV space. Then, these multi-scaled LiDAR-BEV feature maps are fused to generate an expressive BEV feature map.\nExperimental results demonstrate that the introduced auxiliary branch and the improved camera/LiDAR feature extractor can effectively increase 3D object detection performance."}, {"title": "2. Related Works", "content": "Camera-based 3D object detection. Early works [11, 35] are proposed for monocular 3D object detection. Generally, they implement the 2D object detection based on the image and then project the 2D results into the 3D space using a second stage. However, this intuitive detection strategy suffers from the elaborate post-processes to achieve robust results when dealing with the inputs from surrounding cameras. Recently, vision BEV perception methods [7, 14, 25] received enormous attention in industry and academia. These architectures transform the features from multiple images into a unified BEV frame [10]. The BEV features can be directly fed to many downstream tasks and are fusion-friendly. These methods can be broadly classified into two categories on the basis of the transformation mode [21]: \"geometry-based transformation\u201d and \"network-based transformation\". The representative \"geometry-based\" methods [7, 12, 25] adopt explicit depth estimation and project the extracted features into the 3D space based on the physical principles. [12] applies the LiDAR data to supervise the depth prediction training and [6] introduces temporal cues to increase the 3D object detection performance. Differently, the \"network-based\" methods use the neural network to map the image features to the BEV space implicitly. A lot of works [8, 14, 17] use transformers to translate the image features to the BEV space. They all use the deformable transformer [43] to reduce computation and memory costs.\nLiDAR-based 3D object detection. The mainstream 3D object detection methods can be categorized into point-based [27, 28] and voxel-based [4, 38, 41] methods. The point-based methods [27, 28] directly operate the irregular LiDAR points and exploit the spatial information. Differently, the voxel-based methods [4, 38, 41] first transform the unordered LiDAR points into the volumetric mode with a pre-defined grid size and then apply 2D/3D CNNs on the regular voxels to obtain the detection results. Recently, some methods [23, 29] have integrated 3D voxel networks and point-based networks to achieve more representative features.\nMulti-modal 3D object detection. The LiDAR and camera information are complementary. The LiDAR points can provide precise spatial information for object location, and the images give abundant contextual information for object classification. To obtain accurate surrounding information for autonomous vehicles, many researchers endeavor to effectively fuse the information from cameras and LiDAR for accurate 3D object detection. According to the fusion operation, the camera-LiDAR fusion methods can be divided into three categories [22]: \"early-fusion\u201d, \u201cintermediate-fusion\" and \"late-fusion\". The \"early-fusion\" methods mainly first implement image information (features [34], semantic labels [33], or bounding boxes [26]) and feed the results to the LiDAR-based branch to achieve the final detection. These methods need an additional complicated 2D network and suffer from the detection of the object with few LiDAR points. The \u201clate-fusion\u201d method [24] fuses the results from the independent camera and LiDAR branches. Despite its efficiency, this method limits the exploitation of rich and complementary information from different modalities. The \"intermediate-fusion\" methods gain the most attention in industry and academia. Early research works [3, 9] generate 3D object proposals based on LiDAR or LiDAR-camera information and fuse the LiDAR and camera features extracted based on the object proposals. In recent years, many BEV-related fusion methods [1, 13, 16, 19] have been proposed inspired by the vision BEV representation. [16, 19] extract camera BEV features using LSS [25] and fuse them with the LiDAR BEV features. [13] uses the LiDAR features as the queries to fuse the image and LiDAR features. [1] builds a two-stage pipeline where the first stage produces initial 3D bounding boxes and the second stage associates and fuses the object queries with the image features for better detection results. [37] treats the image and LiDAR features as tokens and directly implements 3D object detection using transformers. To further exploit the camera information during fusion, [39] applies two individual branches to perform representational interaction and sequential modules for predictive interaction. Our method builds on the BEVFusion [16] method and strengthens the camera and LiDAR branches to achieve better 3D object detection performance."}, {"title": "3. Method", "content": "We design a multi-modal 3D object detector, Simple-BEV, based on the camera and LiDAR data, whose framework is shown in Fig. 1. We first introduce the camera-related branches and the LiDAR branch. The camera-related branches consist of a camera branch to extract the image features and project them into the BEV space, and an auxiliary branch to better exploit the information from cameras during the training phase. Then, we present the BEV encoder and the detection head for the final detection task."}, {"title": "3.1. Camera related branches", "content": "Camera branch. The input multi-view images are first encoded into deep features through a shared image encoder which consists of an image backbone for feature extraction and a simple FPN neck to fuse multi-scale features. Specifically, the ConvXt-Tiny [18] is adopted as the image backbone to extract representative image features. The feature maps from different stages of the image backbone are fed to the FPN neck to exploit the scale variate representation. Then, the feature maps from the specified layer are used to generate the camera-BEV feature map.\nGiven the i-th image feature map $F^{im}_i \\in \\mathbb{R}^{H \\times W \\times C^{im}}$, we transform the image features into the BEV space following a similar pipeline in LSS [25]. The image features are first used to estimate the pixel-wise depth distributions $D^{cam}_i \\in \\mathbb{R}^{H \\times W \\times D}$, where $D$ denotes the number of discretized depth bins. Then, each image feature is weighted by the probabilities of different depth bins and projected to the 3D coordinate to form the frustum features. The 3D features from multiple cameras are all transformed into the LiDAR coordinate and form the camera-BEV feature map $F^{B}_{cam} \\in \\mathbb{R}^{X \\times Y \\times C^{cam}}$ through voxelization and sum pooling along the height. $X$ and $Y$ represent the grid size along the x-axis and y-axis of the BEV coordinate, respectively.\nThe depth estimation in the above feature transformation procedure plays a critical role in camera-based 3D object detection. A better depth predictor benefits the alignment of the camera-BEV and LiDAR-BEV features. To improve the precision of the depth estimation, we modify the depth estimation network and introduce the LiDAR data to generate the precise depth. The pipeline is shown in Fig. 2. A two-stage cascade structure is built to gain the camera-based depth map $D^{cam}$. The output depth map from the first stage is concatenated with the feature map from the first stage, and the fused feature map is fed into the second stage. Meanwhile, the LiDAR points are transformed into the i-th camera coordinate and projected into the image coordinate to form the depth map $D^{lid}_i \\in \\mathbb{R}^{H \\times W \\times D}$. Considering the projected points on the feature map are sparse, we introduce a mask map $M^{lid}_i \\in \\{0,1\\}^{H \\times W}$ to represent whether the pixel of the feature map is labeled by the LiDAR points (denoted as 1) or not (denoted as 0). The depth of the pixel $(u, v)$ in the final depth map $D^{li}_i \\in \\mathbb{R}^{H \\times W \\times D}$ is calculated as $D^{li}_i(u, v) = D^{cam}_i(u, v) \\cdot M^{lid}_i(u, v) + D^{lid}_i(u, v) \\cdot (1 - M^{lid}_i(u, v))$. In other words, the final depth map is generated by filling the holes on the sparse LiDAR-based depth map with the estimated depth map based on the image features. The fused depth map is used for the image feature projection.\nAuxiliary branch. An auxiliary branch is introduced to exploit further the camera information, which is activated during the training phase. A camera-BEV encoder encodes the BEV feature from the camera branch. An anchor-free-based detection head is introduced to implement the 3D object detection task. The camera-BEV encoder consists of multi-layer convolutions and multi-scale feature fusion modules. The auxiliary detection head follows the structure of CenterHead [40] to perform 3D object detection using only the camera-BEV features."}, {"title": "3.2. LIDAR branch", "content": "The LiDAR branch follows a similar pipeline as SECOND [38] to extract 3D features and fuses multi-scaled features from different stages. The framework is shown in Fig. 3. The raw points are first converted to the voxel features. Then, multiple sparse 3D convolution layers are sequentially applied to the features to generate multi-scale 3D features. To enhance the capability of the LiDAR-based features to capture multi-scale objects, we introduce the multi-scale feature fusion strategy. The multi-scaled 3D features from different stages are first transformed into multiple 2D BEV features. We apply multiple 3D convolutions to compress the z-dimension and concatenate the features along the z-dimension to transform the 3D features into the 2D BEV features. Then, multiple up-sampling and convolution operations are utilized to fuse the multiple BEV feature maps. The final LiDAR-BEV features $F^{lid}_B \\in \\mathbb{R}^{X \\times Y \\times C^{lid}}$ are fed to the camera-LiDAR feature fusion module."}, {"title": "3.3. BEV encoder & Detection head", "content": "The fused BEV features $F^{fuse}_B \\in \\mathbb{R}^{X \\times Y \\times C^{fuse}}$ are generated by concatenating the camera-BEV feature $F^{cam}$ and the LiDAR-BEV feature $F^{lid}_B$. Then, the fused features are further encoded in the BEV space. The BEV encoder enhances the BEV features by multiple convolutions and combines the multi-scale features.\nWe adopted the mature transformer-based head [1] and the center heatmap head [40] for the final detection task and the auxiliary detection task, respectively. Specifically, the transformer-based head uses the fused BEV features, and the center heatmap head utilizes the camera BEV features."}, {"title": "3.4. Training", "content": "The model is trained by minimizing the sum of the following losses:\n$\\mathcal{L} = \\mathcal{L}_{fusion} + \\mathcal{L}_{aux} + \\mathcal{L}_{depth}.$\nHere, $\\mathcal{L}_{fusion}$ represents the detection loss based on the fusion BEV features. We use the identical loss function as [1]. $\\mathcal{L}_{aux}$ denotes the detection loss from the auxiliary branch using only the camera-BEV features. $\\mathcal{L}_{depth}$ is the depth loss to train the depth net in the camera branch. The ground truth depth derives from the LiDAR data."}, {"title": "4. Experiments", "content": "The experiments are conducted on the nuScenes dataset [2]. It provides the point cloud information using 1\u00d7 32-beam LiDAR with 20HZ capture frequency and the image information from 6\u00d7 surrounding cameras with 12HZ capture frequency. The provided images from each camera are of the same resolution: 1600 \u00d7 900. This dataset consists of 1000 scenes with annotated 3D bounding boxes, which are divided into train/validation/test subsets with 700/150/150 scenes, respectively. We evaluate the models using the metrics of the mean average precision (mAP) and the nuScenes detection score (NDS) on 10 object detection results.\nWe implement our method based on the code base mmdetection3d [5]. During the evaluation, the input images are scaled and cropped to 256\u00d7704 resolution, and the camera branch generates the features with 1/8 input resolution. We set the voxel size to (0.075m, 0.075m, 0.2m) for X,Y,Z axis, respectively. The detection range is set to [-54m, 54m] for X and Y axis and [-5m, -3m] for Z axis. The BEV grid size is set to 0.6m.\nWe train the model using AdamW [20] optimizer with a one-cycle learning rate strategy [30]. The model is trained for 20 epochs, in which the GT-Pasted data augmentation strategy is stopped after the 15th epoch. No CBGS [42] is used. During testing, we enlarge the image resolution to 640\u00d71600. For the online submission, we adopt the test-time augmentation (TTA) with multiple yaw rotations and global scales. Meanwhile, we train multiple models with additional voxel size (0.05m, 0.05m, 0.2m) and BEV grid sizes between 0.3m and 0.6m. The results are fused by the weighted box fusion (WBF) strategy [31]."}, {"title": "5. Conclusion", "content": "In this paper, we propose an effective multi-modal fusion framework, SimpleBEV, to detect 3D objects in the autonomous driving environment. It follows the architecture of the BEV-based fusion method that fuses the LiDAR and camera features in a unified BEV space. The experiments demonstrate the effectiveness of our method. The improved camera depth estimation module and the multi-scaled LiDAR-BEV fusion module can effectively enhance the detection performance. Moreover, the introduced auxiliary branch benefits the camera information exploitation during training. In the future, we will integrate more sensors into the framework and explore more downstream applications based on the fused features."}]}