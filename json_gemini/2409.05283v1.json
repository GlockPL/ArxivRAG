{"title": "On the Relationship between Truth and Political Bias in Language Models", "authors": ["Suyash Fulay", "William Brannon", "Elinor Poole-Dayan", "Shrestha Mohanty", "Cassandra Overney", "Deb Roy", "Jad Kabbara"], "abstract": "Language model alignment research often attempts to ensure that models are not only helpful and harmless, but also truthful and unbiased. However, optimizing these objectives simultaneously can obscure how improving one aspect might impact the others. In this work, we focus on analyzing the relationship between two concepts essential in both language model alignment and political science: truthfulness and political bias. We train reward models on various popular truthfulness datasets and subsequently evaluate their political bias. Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias. We also find that existing open-source reward models (i.e. those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models. These results raise important questions about both the datasets used to represent truthfulness and what language models capture about the relationship between truth and politics.", "sections": [{"title": "Introduction", "content": "The political bias of large language models (LLMs) has been the subject of much recent research (Feng et al., 2023; Motoki et al., 2023). Santurkar et al. (2023) found that base models tend to be more right-leaning initially, but shift towards a left-leaning stance after fine-tuning, suggesting that the alignment process may influence the models' political bias. However, since alignment datasets often simultaneously target helpfulness, harmlessness, and truthfulness (Bai et al., 2022), it is difficult to determine which of these objectives, if any, might be responsible for this shift in political bias.\nOur interest in the relationship between truthfulness and political bias is motivated by findings in political science of partisan differences in susceptibility to misinformation (Baptista and Gradim, 2022) and trust in science (Cologna et al., 2024).\nLower levels of trust by some political groups may be exacerbated by political bias in language models if the groups believe these models are antithetical to their values. As LLMs become more widely deployed, exploring such biases and ways to remediate them becomes valuable.\nWe begin by testing whether vanilla open-source reward models - i.e., those fine-tuned on standard human preference datasets show political bias, aiming to identify parts of the alignment pipeline contributing to the left-leaning bias suggested by prior work (Santurkar et al., 2023). We then train a new set of reward models (RMs) on several datasets representing different notions of truthfulness, such as everyday and scientific facts, and assess their political bias. Finally, we analyze which topics exhibit the greatest bias.\nThe main findings are as follows:\n\u2022 Vanilla open-source reward models, trained on popular alignment datasets, display a clear left-leaning political bias.\n\u2022 Training reward models on datasets designed to capture \"truth,\" including everyday and scientific facts, also results in a left-leaning bias.\n\u2022 This bias is especially strong on topics like climate, energy, or labor unions, and weakest or even reversed for taxes and the death penalty.\nOur results suggest that even training on supposedly objective datasets can lead to unforeseen bias."}, {"title": "Related Work", "content": "Prior work has extensively covered ways to \u2018align' models with human preferences (Bai et al., 2022; Casper et al., 2023), particularly the widely used technique of reinforcement learning from human feedback, or RLHF (Stiennon et al., 2020). Other work has examined how truth is represented in language models (Burns et al., 2022; Azaria and Mitchell, 2023), sometimes in terms of embedding"}, {"title": "Experimental Setup", "content": "Truthfulness Datasets We use several datasets corresponding to different notions of factuality to train our reward models: TruthfulQA (Lin et al., 2022), FEVER (Thorne et al., 2018), SciQ (Welbl et al., 2017), and a dataset we created of 4,000 basic LLM-generated facts and falsehoods about the world, using GPT-4 (OpenAI et al., 2023) and Gemini (Gemini Team et al., 2024). (See Appendix B for details of how we generated, validated and audited this last dataset.) To make the data suitable for reward modeling, which expects paired samples, we match a correct response to a query with an incorrect response for TruthfulQA, FEVER, and SciQ. For the generated dataset, we create random pairs of true and false statements. For datasets with multiple-choice options, we ensure that each question appears exclusively in either training or test.\nPolitical Dataset: Twin Views-13k To test reward models for political bias, we use GPT-3.5-turbo (OpenAI, 2023) to generate TwinViews-13k, a dataset consisting of 13,855 pairs of left-leaning and right-leaning statements matched by topic. The model was instructed to keep the statements as similar as possible in style and length. We used generated statements because of the dearth of large\ntopically matched datasets of political statement pairs; for example, the popular political compass test\u00b9 includes only a few statements. We extensively audited the generated statements to ensure their relevance and quality. Details of the prompt and the quality-assurance process, including a sample of the statement pairs (Table 4), can be found in Appendix A. We release the final TwinViews dataset publicly for use by the community.\nModels We clarify terminology with respect to the different model types here. A \"base\" model refers to a pre-trained LLM without any further fine-tuning, while a \u201cvanilla\u201d reward model is a base model fine-tuned on standard human preference datasets such as OpenAssistant (K\u00f6pf et al., 2023), Anthropic Helpful-Harmless (Bai et al., 2022), and OpenAI's summarizing from human feedback data (Stiennon et al., 2020). A \u201ctruthful\u201d reward model is a base model fine-tuned on a truthfulness dataset.\nFor experiments on vanilla reward models, we evaluate RMs from RAFT2 (Dong et al., 2023), OpenAssistant\u00b3 and UltraRM4 (Cui et al., 2023). For the truthful reward models, we train several RMs on each truthfulness dataset (Section 3) with weights initialized from the base 160M, 2.8B and 6.9B Pythia models (Biderman et al., 2023), conducting several runs on different splits (80% train, 20% test) for robustness. (All runs are shown in Figure 2.) We also train a simple tri-gram baseline on each dataset for the analysis in Section 5.2 (see"}, {"title": "Bias in Vanilla Reward Models", "content": "We first examine whether vanilla open-source reward models exhibit political bias. As discussed in Section 3, we evaluate with models from RAFT, OpenAssistant and UltraRM. We run inference with these models on the Twin Views statements and find that all models show a left-leaning political bias, as depicted in Figure 1. Notably, larger models also show greater bias, an example of inverse scaling (McKenzie et al., 2023). However, one caveat is that the datasets/training methods are different across these reward models. The results suggest that at least part of the left-leaning political bias observed in the literature (Santurkar et al., 2023) could be due to biases introduced in reward-model training, which we believe is a new finding."}, {"title": "Bias in \"Truthful\" Reward Models", "content": "While vanilla reward models exhibit a clear political slant, these models are fine-tuned on datasets of subjective human preferences reflecting diverse goals (Casper et al., 2023). Our objective is to minimize this subjectivity by training \u201ctruthful reward models\" reward models designed to give high scores to objectively truthful statements (e.g., basic everyday facts or scientific information) and low scores to false statements. As discussed in Section 3, we pursue this goal by fine-tuning various base Pythia models as reward models on each of the four truthfulness datasets, and evaluating the rewards they assign to the left and right Twin Views\nstatements. Because any resulting political bias might be due to political content in the truthfulness datasets, we first systematically audit them for such content (in Section 5.1). We find very low rates of political content, but nevertheless exclude it from subsequent model training and analysis.\nTraining models on these cleaned datasets produces results shown in the left three panes of Figure 2. We found that our truthful reward models generally assign higher rewards to left-leaning statements than right-leaning ones (in 11 out of 12 cases). As with vanilla models, the degree of bias also usually increased with model size.\nWith fine-tuning datasets intended to be objective, these findings were unexpected. In Section 5.2, we use an n-gram baseline (shown in the rightmost pane of Figure 2) to consider another potential source of bias: stylistic features spuriously correlated with both truth status and political orientation. We find little support for this idea either, however, leaving the origin of the political bias shown in Figure 2 in need of further research.\""}, {"title": "Explicit Political Bias", "content": "Political content in truthfulness datasets may lead to political bias in models trained on them. However, our analysis shows that these datasets contain very little explicitly political content. We used two methods, building on a list of political topics from the Comparative Agendas Project (Jones et al., 2019), to identify political content.\nFirst, we used a simple keyword matching approach. We generated potential political keywords with GPT-4, and used them to search for potential political content. We then manually labeled the flagged training examples. This method found that about 2% of the data in TruthfulQA contains"}, {"title": "Stylistic Artifacts", "content": "Even after excluding explicitly political content, a left-leaning bias might arise from \u201cstylistic\u201d features of the truthfulness data. For instance, if negation words (e.g., \u201cno,\u201d \u201cnot\u201d) are more prevalent in both false and right-leaning statements, the reward model might learn to associate these features, as with the length bias in some RMs (Shen et al., 2023). We test this hypothesis with the n-gram baseline: If this simple model shows a political bias similar to that of the neural models, it would support the idea that those models' bias stems from\nstylistic features of the datasets.\nWe do observe this pattern on the generated factual statements, indicating that stylistic artifacts in that dataset may be the most likely explanation. Results on the other three datasets, however, are quite different, without a clear relationship to the direction or magnitude of the bias shown by the neural models. Overall, stylistic artifacts do not seem to explain most of the political bias we observe."}, {"title": "Bias Across Topics", "content": "Because both vanilla and \"truthful\" reward models show political bias, we used regression analysis to examine which topics or political issues exhibit the most bias. For both sets of models, we regressed the reward assigned to a Twin Views political statement on several predictors: the model, the topic, the statement's political lean, and the topic/political-lean interaction. All models are linear regression.\nOur results are shown in Table 1. In particular, we find that for both sets of reward models, right-leaning stances are preferred to left-leaning ones on tax issues. Conversely, on topics like climate, energy, or labor unions, the left-leaning stance receives higher reward. Despite our efforts to exclude data referencing politically charged topics, these topic-specific biases may be influenced by the highly politicized nature of some issues, knowledge of which a model may acquire in pretraining."}, {"title": "Conclusion", "content": "We investigated political biases in reward models, both vanilla open-source reward models and \"truthful\" reward models, and found a persistent left-leaning political bias across nearly all these models. This result is particularly surprising given the use of datasets designed to capture objective truth. Moreover, the size of the bias increases with model scale, in contrast to the usual pattern of improving capabilities. For the \"truthful\" models, we considered and attempted to rule out two explanations: explicit political content in truthfulness datasets and spurious relationships between truthfulness and stylistic features. Identifying the source of this bias is a promising direction for future research, and we hope these initial findings will encourage further investigation into the relationship between truthfulness and political bias in language models."}, {"title": "Limitations", "content": "Though the relationship between truth and political bias in language models is a timely and important topic, this study has certain limitations in addressing it. Firstly, datasets are an imperfect representation of truth and falsehood. Although there has been significant interest in identifying truthful directions in LLMs (Marks and Tegmark, 2023; Azaria and Mitchell, 2023; Burns et al., 2022), recent work has shown that these findings are sensitive to simple perturbations, such as negation (Farquhar et al., 2023; Levinstein and Herrmann, 2024).\nConsequently, it is possible that the reward models are learning dataset artifacts rather than a true notion of truth versus falsehood. Nevertheless, it is valuable to understand how these artifacts may affect political bias. Secondly, our study focuses solely on reward models. While there are good reasons for this focus (reward models are a crucial component of the RLHF pipeline and their scalar outputs allow simple quantitative comparison of preferences), it still restricts what we can say about the rest of the alignment pipeline. Future research should explore how optimizing models through other alignment methods, such as direct preference optimization, or DPO (Rafailov et al., 2023), impacts the downstream model in more externally valid settings such as text generation."}, {"title": "Ethical Considerations", "content": "We hope that our work can shed light on biases of existing models and modeling approaches, and thereby help remedy them. We do not foresee any meaningful risks of our work or believe it has significant ethical concerns. No part of our research involved human subjects.\nWe used various software and data artifacts in preparing this paper and conducting the analysis it describes, all of which were subject to licenses permitting use for research. Both the alignment datasets and the existing models we used were research projects intended for use in further research, and OpenAI's terms of use similarly permit use of their services for research. Our generated datasets are similarly available under the CC-BY 4.0 license (though note that OpenAI's terms of service prohibit uses of their model outputs in competing products). None of the pre-existing truthfulness datasets we use should contain personally identifying or toxic content, and our audits of them found none."}, {"title": "Model Training Details", "content": "We train all models on an NVIDIA A6000 GPU.\nAll models are trained with an effective batch size of 128 and a learning rate of $4e-5$ for one epoch.\nThe 2.8B and 6.9B parameter models are trained with PEFT, with hyperparameters r = 128 and LoRA's \u03b1 = 128. All parameters of the 160M model were fine-tuned. We estimate each training run took between ten and thirty GPU minutes depending on the dataset size. With three model sizes, four datasets, and five iterations each, with an average of 20 minutes per run, we estimate our total computational budget was around 20 GPU hours.\nTraining used the transformers (Wolf et al., 2020) and TRL (von Werra et al., 2024) libraries from HuggingFace. N-gram models used features with n \u2264 3, with one model trained on each truthfulness dataset, fit with the scikit-learn implementation of multinomial naive Bayes (Pedregosa et al., 2011)."}, {"title": "Use of AI Tools", "content": "We used Github Copilot to assist in writing some code to run experiments as well as ChatGPT to check written content for grammar and clarity; however, the original content was authored without the assistance of AI tools."}, {"title": "Data/Code Availability", "content": "All data and code will be made public after acceptance."}]}