{"title": "AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs", "authors": ["Mingzhe Gao", "Jieru Zhao", "Zhe Lin", "Wenchao Ding", "Xiaofeng Hou", "Yu Feng", "Chao Li", "Minyi Guo"], "abstract": "Recently, the use of large language models (LLMs) for software code generation, e.g., C/C++ and Python, has proven a great success. However, LLMs still suffer from low syntactic and functional correctness when it comes to the generation of register-transfer level (RTL) code, such as Verilog. To address this issue, in this paper, we develop AutoVCoder, a systematic open-source framework that significantly improves the LLMs' correctness of generating Verilog code and enhances the quality of its output at the same time. Our framework integrates three novel techniques, including a high-quality hardware dataset generation approach, a two-round LLM fine-tuning method and a domain-specific retrieval-augmented generation (RAG) mechanism. Experimental results demonstrate that AutoVCoder outperforms both industrial and academic LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2% improvement in functional correctness on the EvalMachine and EvalHuman benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax correctness and a 3.4% increase in functional correctness on the RTLLM benchmark compared with RTLCoder.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) has increasingly captured the attention of the academia and industry. In the realm of programming, LLMs have demonstrated remarkable success in generating software code, automating and streamlining the development process of programming languages like C, C++, and Python. Recently, some representative works [1, 2, 3, 4, 5, 6], including CodeT5 [1], CodeGen [2], CodeGeeX [3], have made tremendous breakthroughs in augmenting LLMs for software code generation. Additionally, commercial tools such as Copilot [7] and GPT-4 [8] have demonstrated notable performance in code generation. The progress is largely driven by advances in model architecture, training techniques, and most importantly, the vast amounts of data on which these models are trained.\nHowever, despite the tremendous advancements in software code generation achieved by LLMs, their effectiveness in the hardware domain, particularly in constructing hardware designs using Verilog, still remains underwhelming. This gap can be attributed to two unique challenges of hardware design. First, RTL languages often incorporate greater domain specificity and complexity in their syntax and semantics compared to the software programming languages. Second, the volume of high-quality hardware design datasets available for training is considerably smaller than that for software languages, limiting the learning capability of large models.\nPrior research in hardware code generation has attempted to bridge this gap and can be categorized into two types: prompt engineering and supervised fine-tuning. Prompt engineering improves the quality of LLMs' generated Verilog code by adjusting the descriptions and structures of prompts. Without altering model parameters, this method can be easily adopted and implemented. ChipGPT [9] introduces an automatic chip generation framework through prompt engineering, allowing ChatGPT-3.5 to generate circuits with lower power consumption and smaller size. RTLLM [10] develops a self-planning prompt engineering approach that enhances the syntactic and functional accuracy of Verilog code generated by ChatGPT-3.5 and GPT-4. RTLFixer [11] and AutoChip [12] improves syntactic accuracy by utilizing compiler-generated error messages as feedback to LLM across multiple rounds of queries.\nWhile prompt engineering is convenient and requires less preparation than fine-tuning, it does not fundamentally improve the underlying ability of LLMs for RTL code generation, making supervised fine-tuning a necessary step. Thakur et al. [13] and Dehaerne et al. [14] adopt full fine-tuning which adjusts parameters of the entire LLM model with their dataset collected from GitHub. However, the lack of adequate data cleaning and task-specific training influences their functional accuracy. ChipNeMo [15] from Nvidia deploys a two-round fine-tuning process with their in-house data, while only the first round can benefit RTL code generation. The dataset and model are not released to the public. Meanwhile, its functional accuracy is not satisfying due to the same issue as [13, 14]. To improve the model performance, VerilogEval [16] and the most recent work, RTLCoder [17], utilize ChatGPT-3.5 to generate high-quality problem-code pairs as the dataset for the single-round task-specific fine-tuning, demonstrating relatively good results on existing benchmarks. However, their small synthetic dataset lacks diversity, influencing the generality and making it hard to achieve higher accuracy. BetterV [18] simultaneously fine-tunes the model and trains a generative discriminator to assist in Verilog code generation, which increases the difficulty of deployment.\nTo summarize, previous methods fail to balance the diversity and quality of dataset. Additionally, the single-round domain-adapted fine-tuning limits the ability of LLM to maintain high code quality while generating diverse outputs. Meanwhile, LLM faces the hallucination problem and may incorporate software coding habits into Verilog code generation, potentially causing syntactic and functional errors. However, few of prior methods notices this issue.\nIn this paper, we propose AutoVCoder, a systematic open-source framework that strengthens the capability of LLMs to automatically generate high-quality Verilog code. AutoVcoder enhances LLMs to generate syntactically and functionally correct Verilog code, addressing the gap to apply LLMs for hardware code generation. Our key contributions are summarized as follows:\n1) We propose an effective and automated dataset generation approach that generates high-quality and diverse RTL code samples.\n2) We introduce a two-round LLM fine-tuning method to improve the ability of LLMs for Verilog code generation.\n3) We present a domain-specific retrieval-augmented generation (RAG) module that provides more constructive prompts to further enhance the syntactic and functional correctness of generated RTL designs.\nExperimental results demonstrate that AutoVCoder outperforms both industrial and academic LLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2% improvement in functional correctness on the EvalMachine and EvalHuman benchmarks compared with BetterV, and also achieves a 3.4% increase in syntax correctness and a 3.4% increase in functional correctness on the RTLLM benchmark compared with RTLCoder."}, {"title": "II. PRELIMINARIES", "content": "A. Large Language Model for Code Generation\nLarge Language Models (LLMs) have revolutionized the field of natural language processing (NLP). Their ability to generate coherent and contextually relevant text has made them particularly useful for various applications such as code generation. In the context of software code generation, LLMs are trained on a vast amount of code across various programming languages, enabling them to learn syntax, semantics, and some level of logical flow inherent to software coding. Then LLMs are employed in an autoregressive manner, predicting the next token in a sequence given previous tokens. This capability allows LLMs to generate entire blocks of code based on a prompt that specifies the desired functionality. The effectiveness of these models in generating syntactically correct and logically coherent code snippets has been demonstrated in previous studies [1, 2, 3, 4, 5, 6, 7, 8], showcasing their potential to automate software development and reduce workload of developers.\nHowever, the transition of LLMs from generating software code to generating RTL code like Verilog presents unique challenges. It requires a deep understanding of domain knowledge, such as hardware architecture, circuit design and low-level constraints, to generate a high-quality hardware design. This can be hard to achieve with standard LLM training datasets. Therefore, while previous works [10, 11, 13, 17] show that powerful LLMs like GPT-3.5 can handle HDL syntax due to similarities between programming languages, their performance in generating functionally correct hardware designs is still unsatisfying without additional domain-specific processing and fine-tuning.\nB. Retrieval-Augmented Generation (RAG)\nTo address the limitations of LLMs in domain-specific applications, the concept of Retrieval-Augmented Generation (RAG) is introduced, as shown in Fig. 1. The user starts by submitting a query, then the RAG module compares this query to chunks in the document database to find the similar chunks. After selecting the chunks, it combines them with the user's query and sends this combined query to LLM for inference. RAG is particularly effective in solving domain-specific problems, and the documents may provide solutions and can ease the issue of hallucinations in LLMs. The RAG module enhances a standard LLM by integrating a retriever that queries a database for domain-specific documents or code snippets during the generation process. This retriever, which is called sentence embedding, usually act as a BERT-like model and can convert sentences into vectors. It is trained to fetch relevant information based on the input prompt. The retrieved information is then fed back to LLM, providing additional context to generate more appropriate outputs.\nIn our framework, when generating Verilog code, the RAG module can access examples of similar hardware modules or specific implementations, aiding the LLM in understanding the unique requirements and constraints of hardware design. This also helps LLM to adopt commonly used design patterns, thus enhancing the quality and usability of the generated code. By integrating the retrieval process into the generation pipeline, the RAG model effectively narrows the gap between general-purpose language understanding and the specialized knowledge required for tasks like Verilog code generation. This makes RAG a powerful tool for extending the capabilities of LLMs beyond traditional text generation into more specialized and technical domains."}, {"title": "III. METHODOLOGY", "content": "Figure 2 depicts the overview of our framework, which can be divided into three core parts: a high-quality hardware dataset generation approach; a two-round fine-tuning method for LLMs; and a domain-specific retriever training mechanism for RAG.\nFirstly, to generate a high-quality hardware dataset in an efficient way, we collect a large number of raw Verilog-based hardware designs from GitHub, after which a novel scoring mechanism (code scorer in Fig. 2) that swiftly rates the quality of each hardware design is presented. After cleansing data by removing code with low scores, the generated dataset will be used in the first round of LLM fine-tuning to augment LLMS with the capability of understanding the basic syntax of Verilog and the rationale of hardware designs. To further enhance the performance of LLMs on the specific task of generating a correct hardware design given a problem description, we leverage ChatGPT-3.5 to obtain a series of problem-code pairs and propose a verification mechanism (code filter in Fig. 2) to ensure their correctness. These samples after filtering form our synthetic dataset which will be used in the second round of LLM fine-tuning.\nAfter constructing datasets, we present a two-round LLM fine-tuning method to improve the LLM's efficacy for generating Verilog designs. Starting with a general LLM as the base model, we perform the first round of fine-tuning on our generated dataset from open-source GitHub repositories and perform the second round of fine-tuning on the synthetic dataset obtained from ChatGPT-3.5.\nFinally, we utilize the advanced RAG technique to further enhance syntactic and functional correctness during Verilog code generation. We propose a domain-specific retriever training mechanism based on contrastive learning and construct two types of retriever, namely example retriever and knowledge retriever, to fetch different kinds of information.\nDuring inference, users can directly describe their problems and ask our model to generate desired Verilog code. As shown in Fig. 2, the user prompt is first sent to our domain-specific RAG module which searches through document database and find highly relevant examples and RTL design principles. Then the RAG module reconstructs the input prompt with retrieved contents and feeds this new prompt to the two-round fine-tuned LLM. A high-quality RTL design can then be generated.\nA. Hardware Dataset Generation\nOpen-source database construction. Hardware designs described in Verilog are usually regarded as valuable assets to each company as well as individual. Therefore, high-quality Verilog-based hardware designs are scarce resources, which makes data-driven learning methods, e.g., LLM, more difficult in the Verilog coding domain. To tackle this problem, we seek to construct a high-quality Verilog design database from the public. We search for open-source RTL code from GitHub, identify .v files in repositories, and segment them into separate blocks to form realistic training samples. Specifically, we gather data from up to 20,000 GitHub repositories and obtain around 1,000,000 raw RTL hardware modules.\nNoticing that the online resource is a mixed bag, it is crucial to filter out inferior design cases to maintain the training data's quality. However, due to the large size of the online database, it is impractical to manually look into each design instance and assess its suitability for model training. One way is to utilize ChatGPT-3.5 to search for useful training data instead of going through the process manually. To equip ChatGPT-3.5 with the ability of data cleaning, we add prompts to force ChatGPT-3.5 to behave as a code scorer, with special considerations for Verilog readability, scalability, the degree of standardization, efficiency and robustness, as shown in Fig. 3, where the $code snippet refers to the input code.\nTo speed up the code scoring process and reduce the cost of ChatGPT-3.5, we implement a specialized code scorer as a light-weight replacement of ChatGPT-3.5. Figure 4 illustrates the training and inference process of our code scorer. It contains a sentence-transformer module and an MLP layer. We employ FlagEmbedding [19] for sentence embedding.\nIn this code scoring model, the parameters of the sentence-transformer module are fixed, and only the MLP layer is trainable. During the training stage, we update the MLP layer with only a subset of the complete open-source database, i.e., 15,000 Verilog modules and their corresponding scores obtained via ChatGPT-3.5. Once training is completed, the code scorer is applied to score the remaining Verilog modules within the open-source database. We found that Verilog modules scoring above 6.5 are of high educational value, accounting for about 21.7% of the total samples. Hence, these high-score Verilog modules are used for the first-round LLM fine-tuning.\nSynthetic dataset generation. Besides using open-source database to learn basic RTL syntax, we also seek to generate practical problem-code pairs to standardize our LLM in the problem solving task. We continued to use ChatGPT-3.5 to create a series of specialized and synthetic data. Initially, we let ChatGPT-3.5 generate a problem related to Verilog, and at the same time, we ask ChatGPT-3.5 to provide the answer. To ensure the diversity of the generated code, we learn from the tinystory strategy [20] and increase the code variety by changing several keywords in the prompt, as shown in Fig. 5, where $level refers to easy, normal or hard, $circuit type refers to either combinational or sequential logic circuits, and $problem type denotes a type of problem that will be randomly selected from a list of problem types.\nHowever, it's important to note that the Verilog code generated by ChatGPT-3.5 may not always be correct. That is, it can incur syntactic or functional errors. To address this issue, we design a code filter to help identify invalid code samples. As shown in Fig. 6, we use ChatGPT-3.5 to generate four components: the problem, the Verilog code, the corresponding testbench, and the equivalent Python code. We first utilize the Icarus Verilog tool [21] to verify the syntax correctness of the generated code. If the code is syntactically correct, we further evaluate its functional correctness. We note that Verilog code generally falls into two categories: combinational and sequential logic circuits. For combinational circuits, we utilize the equivalent Python code to conduct auxiliary checks. We generate random inputs for the Python code, obtain corresponding outputs, and use these outputs to reconstruct and create testbenches for the Verilog code, which we then synthesize and analyze. For sequential circuits, we use the testbench created by ChatGPT-3.5 to check if expected outputs are provided. Different approaches are taken because LLM has a strong capability in analyzing sequential information, making it easier to understand and write testbenches for sequential logic circuits. However, its capacity for numerical calculation is limited, leading to errors when writing testbenches for combinational logic circuits. Therefore, we need to reconstruct the testbench for combinational logic to ensure the correctness.\nB. Two-Round LLM Fine-Tuning\nFine-tuning with the open-source database. We start with a well-trained LLM as our base model and fine-tune it with the open-source database. The first fine-tuning stage is designed to help the LLM learn the syntactic structure of Verilog code and its practical uses in production environments. In this initial phase of model fine-tuning, we adopt the low rank adaptation(LoRA) [22] method. This approach provides results faster compared to traditional full-parameter fine-tuning. We apply the Maximum Likelihood Estimation (MLE) loss function alongside the LoRA [22] fine-tuning method. Regarding the loss function, we use the cross-entropy loss to quantify the discrepancies between the generated text and the actual text.\nFine-tuning with the synthetic dataset. In the second round of fine-tuning, we aim to improve the LLM's performance in more realistic scenarios. We fine tune the model to perform the specific QA task: providing an answer with correct Verilog code to a hardware design problem. Specifically, we use an instruction tuning approach to standardize the outputs of our model. This ensures that when our model receives Verilog problem, it can produce the code that corresponds to that problem.\nC. Domain-Specific Retrieval-Augmented Generation\nThe rationale of RAG is to identify the piece of data in an existing database that is most correlated to the current task of interests. The extracted relevant data is then used as additional prompt to improve the ability of LLM to generate code that is more accurate and contextually appropriate. In order to search for the useful information in an efficient manner, we propose a domain-specific RAG scheme, which consists of two types of retrievers, i.e., an example retriever and a knowledge retriever that fetch different types of domain-specific information correspondingly.\nExample retriever. The purpose of applying the example retriever is to enable the LLM to perform in-context learning with these given examples. Specifically, the example retriever searches from a document database, and pinpoints the demonstration examples that closely match with the problem description. Then, these examples, together with the user's question, are fed into the LLM for inference. By learning common knowledge from these highly relevant cases, the model is augmented to better understand the user's intention and tends to generate hardware code with higher quality. For instance, the finite state machine (FSM) is a widely used computation model for describing states and their transitions in sequential logic design. When the input question opts for a FSM design, e.g., a traffic light, the example retriever would provide several examples of FSM for the LLM to learn more efficiently. In practice, our examples are extracted from a database constructed with various reliable sources such as textbooks, blogs, and specialized websites dedicated to FPGA and ASIC design. The diversity of database sources ensures that the model can have access to a wide range of contexts and applications within the required field.\nKnowledge retriever. The knowledge retriever extracts RTL design principles and supplementary descriptions about key terminologies in the question to support LLMs in tackling knowledge-intensive tasks. It aims to find paragraphs or principles closely related to the problem description. Our knowledge retriever can help ease the issue that the generated output is grammatically correct but does not obey the RTL design rationale, which is known as hallucinations of LLM. For instance, we observe that a significant flaw in traditional LLMs is their tendency to use loops like those in Python and C++, as shown in the example in Fig. 1. This tendency often leads to excessive usage of for loops when generating RTL code, which can be resource-consuming and does not conform to realistic RTL coding practices. Therefore, providing knowledge that discourages the excessive use of for loops is crucial to solve this software hallucination problem.\nOn the other hand, gathering constructive examples is challenging because it requires finding formally structured code and verifying its correctness before it can be included in the example document, while collecting knowledge chunks is relatively easier. Therefore, the knowledge retriever can serve as a supplement to our example retriever. In practice, we build our knowledge database by extracting paragraphs from a large corpus of Verilog textbooks and blogs to obtain information about Verilog and hardware terminologies, given the assumption that the knowledge collected from textbooks and domain-specific websites is correct. For example, when the LLM is asked to design a Booth multiplier using Verilog, it is highly possible that the example retriever cannot effectively find valid examples due to the fact that the Booth multiplier is a specific terminology dedicated to hardware design and the LLM may not have knowledge about it beforehand. In this situation, the knowledge retriever can extract from the textbooks the definition and functionality of the Booth multiplier, which is fed into the LLM for learning.\nConstruction of retrievers. A key challenge is how to construct retrievers effectively to ensure the retrieved information is relevant and accurate. A simple method is to represent problems and document contents with general sentence embeddings and compare their differences for similarity evaluation. However, the performance would be influenced because it is hard for someone with limited RTL background to find relevant information just from a problem description. Therefore, additional training of retrievers is required to ensure questions match up well with the retrieved information. The establishment of retrievers is divided into two steps: dataset generation and retriever training, as illustrated in Fig. 7.\nStep 1: dataset generation. To train our retrievers, we utilize contrastive learning which requires a dataset with a large number of positive and negative sample pairs. If the addition of a document chunk enhances the output quality of an LLM for a given question, this document chunk and the question form a positive sample pair. Conversely, if the addition of a document chunk fails to improve or even reduces output quality, this document chunk and the question form a negative sample pair. Following this criterion, we can combine a question P with our documents to generate multiple sets of positive and negative sample pairs. To automate the dataset generation process, we first define a function F to evaluate the degree of accuracy:\n$F(V) = \\begin{cases} 0, & \\text{syntax is incorrect} \\\\ 1, & \\text{syntax is correct but functionality is incorrect} \\\\ 2, & \\text{syntax and functionality are correct} \\end{cases}$", "latex": ["F(V) = \\begin{cases} 0, & \\text{syntax is incorrect} \\\\ 1, & \\text{syntax is correct but functionality is incorrect} \\\\ 2, & \\text{syntax and functionality are correct} \\end{cases}"]}, {"title": "IV. EXPERIMENTS", "content": "We conduct a series of experiments to showcase the advancement of the proposed framework, AutoVCoder. Firstly, we perform the end-to-end comparison, evaluating the syntactic and functional correctness achieved by our framework, and comparing the results with state-of-the-art (SOTA) methods from both the industry and academia. Secondly, we evaluate the improvement of our two-round fine-tuning strategy and we perform ablation studies to examine the efficacy of each round of fine-tuning over the base LLMs and recent models. Thirdly, the proposed domain-specific RAG techniques, i.e., the example retriever and the knowledge retriever, are tested under various experimental settings.\nA. Experimental Settings\nWe use some open-source pre-trained LLMs as our base models, including Codellama-7B [25], DeepSeek-Coder-6.7B [26], and CodeQwen1.5-7B [27]. During the model fine-tuning, we use the LoRA [22] method to maintain high efficiency, and we set the learning rate \\(\\gamma\\) as 2e - 4. We train the LLMs for one epoch in the first-round fine-tuning and three epoches in the second-round fine-tuning. As for RAG retriever training, we utilize FlagEmbedding [19] to extract base sentence embeddings, which is trained for three epoches at a learning rate of 1e-5. The training and inference processes are carried out on three Nvidia A100 GPUs.\nTo evaluate the models in the inference stage, two key hyperparameters related to LLM, i.e., topp and temperature, are set as 0.95 and 0.8, respectively. Moreover, we adopt a widely used evaluation metric pass@k [4] for code generation, which refers to the probability that a code solution passes validation when generated k times. This metric can be calculated as:\npass@k = \\mathbb{E} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]", "latex": ["\\gamma", "pass@k = \\mathbb{E} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]"]}, {"title": "V. CONCLUSION", "content": "In conclusion, we propose AutoVCoder, a systematic framework for Verilog code generation using LLMs. We introduce three innovative methods to enhance the LLMs' capability in generating high-quality Verilog code, which effectively improves the level of automation for digital circuits. Experimental results demonstrate that AutoVCoder outperforms existing state-of-the-art methods, paving the way for efficient and effective hardware design using natural languages."}, {"title": "D. Improvement from Domain-Specific RAG", "content": "In this experiment, we verify the effectiveness of the domain-specific RAG technique. We separately examine the following cases given the models after the two-round fine-tuning: (1) not applying RAG; (2) only using the example retriever, denoted as ER; (3) only using the knowledge retriever, denoted as KR; and (4) applying both the example retriever and knowledge retriever, denoted as ER&KR. We evaluate the pass@5 metric on VerilogEval and RTLLM question sets and the results are shown in the Table III.\nResults indicate that the example retriever is prominent in enhancing the LLM's capability of Verilog coding, especially for the VerilogEval question set. Moreover, even though the standalone employment of the knowledge retriever does not make significant gain, jointly using both the example retriever and the knowledge retriever achieves the best performance in most cases, which is a step forward in expanding the LLMs to new RTL domains. In addition, we observe that the improvements on RTLLM question set are less notable. The main reason is that the example database is not well-matched to some of the difficult problems in RTLLM, due to the scarcity of large-scale Verilog design instances. We believe that this problem can be alleviated by continuously accumulating large-scale and realistic design instances and keep expanding the example database."}, {"title": "C. Improvement from Two-Round Fine-Tuning", "content": "In this experiment, we evaluate the benefits of applying our two-round fine-tuning method. To quantify the efficacy of each round of fine-tuning, we conduct experiments with the following four settings: (1) no fine-tuning; (2) only applying the first-round fine-tuning, denoted with the suffix -1; (3) only using the second-round fine-tuning, denoted with the suffix -2; and (4) employing both the two rounds of fine-tuning, denoted with the suffix -1&2.\nTable II illustrates the experimental results, which shows that both the first-round and second-round fine-tuning are crucial in boosting the syntactic and functional correctness, compared with the base models. Comparatively speaking, the second-round fine-tuning exerts a more significant impact on the correctness improvement over the first-round fine-tuning. This showcases the benefits of the high-quality and highly specialized dataset construction method for the second-round fine-tuning. However, the first-round fine-tuning is also indispensable in letting LLMs learn from a more diversified database, which ensures high generalization ability of LLMs. In terms of the base models used, CodeQwen1.5-7B performs the best, which achieves 78.3% and 54.1% functional correctness on EvalMachine and EvalHuman, respectively, and achieve 100% syntax accuracy and 51.7% functional accuracy on the RTLLM benchmark. This also confirms that the LLMS are promising in the task of automatic Verilog code generation by integrating appropriate fine-tuning techniques."}]}