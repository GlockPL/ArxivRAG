{"title": "MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages", "authors": ["Marco Gaido", "Sara Papi", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "abstract": "The rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing speech FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly available under open-source terms. In this work, we take the first step toward filling this gap by focusing on the 24 official languages of the European Union (EU). We collect suitable training data by surveying automatic speech recognition datasets and unlabeled speech corpora under open-source compliant licenses, for a total of 950k hours. Additionally, we release automatic transcripts for 441k hours of unlabeled data under the permissive CC-BY license, thereby facilitating the creation of open-source SFMS for the EU languages.", "sections": [{"title": "1 Introduction", "content": "The introduction of foundation models trained on large datasets is revolutionizing the landscape of many NLP fields (Bommasani et al., 2021), particularly with the release of Large Language Models (LLMs) that demonstrated impressive abilities on various tasks (Radford et al., 2019). The interest attracted by such models has come together with concerns about their risks and impact, as well as requests for a better understanding of their inner workings. On the one hand, this has led to regulatory efforts (European Parliament, 2023; Roberts et al., 2024), while on the other hand, it has sparked a growing interest in open-source models (Workshop et al., 2023; Groeneveld et al., 2024) that can be accessed and studied by anyone. However, it has been acknowledged that the term \u201copen source\" has been abused (Eiras et al., 2024; Liesenfeld and Dingemanse, 2024), being associated with any model whose weights are free to access (e.g., Touvron et al., 2023; Chiang et al., 2023), which is not sufficient to define a model as open source (OS).\nIn line with the Open Source Definition and its principles, the Open Source Initiative defines as Open Source AI a \u201csystem made available under terms that grant the freedoms to: use the system for any purpose without having to ask for permission\u201d, \u201cstudy\u201d, \u201cmodify [...] for any purpose\u201d, and \"share [...] with or without modifications, for any purpose\". Specifically, it requires that the model and the code \u201cused to train and run the system\" are available under an OS license, and that the training data is available under an OS-compliant license (White et al., 2024). This means that an OS model should not be trained on data released under licenses that restrict any of the four essential rights \u2013 use, study, modify, and share \u2013 for any purpose, including commercial use. Examples of OS-compliant licenses include CDLA-Permissive-2.0 and CC-BY-4.0, which only requires attribution (i.e., acknowledging the source or resource used). Instead, data released under licenses like CC-NC-4.0, which prohibits commercial use, or CC-SA-4.0, which mandates that derivative works have to be distributed under the same terms (thereby limiting the freedom to modify and share for any purpose), are not OS compliant.\nFocusing on speech foundation models (SFMs), none of the existing ones comply with this definition. For instance, SeamlessM4T's model (Communication et al., 2023) is released under a license that is not OS compliant, while Whisper's model"}, {"title": "2 Open Source Compliant Speech Data", "content": "This section surveys the available corpora that are admissible for developing an OSSFM for all 24 official EU languages. Accordingly, we include datasets that are freely accessible (i.e., excluding paid datasets) and whose data is released under an OS-compliant license (i.e., without restrictions on creating and redistributing derivative artifacts, including AI models). This means that, in the case of the widespread Creative Commons (CC) licenses, we cannot include data released with non-derivative (ND), non-commercial (NC), or share-alike (SA) restrictions. We also exclude datasets whose license is OS compliant but containing data released under a non-OS-compliant license. In fact, CC licenses \"allow licensed material to be included in collections [...], however this does not change the license applicable to the original material\u201d.\nIn line with this indication, in cases where the transcripts are OS compliant (e.g., CC-BY where only attribution is required) but the corresponding speech (or part of it) is not, we document the dataset under the most restrictive license. For instance, GigaSpeech (Chen et al., 2021), which is released under Apache 2.0, is categorized as non-OS compliant since it contains YouTube videos under restrictive CC licenses. Similarly, MaSS (Zanon Boito et al., 2020) and CMU Wilderness (Black, 2019) are regarded as non-OS compliant since they are derived from the Bible.is data of the Faith Comes By Hearing organization with NC and ND terms of use."}, {"title": "3 Pseudo-labeling Process", "content": "The statistics reported in \u00a72 highlight the importance of leveraging unlabeled data for training an OSSFM, given the scarcity of labeled material for most languages. When unlabeled data is available for model training, a common strategy consists of creating weak supervision (Zhou, 2017; Jia et al., 2019; Oramas et al., 2021; Zhang et al., 2022; Ren et al., 2023), which, in the context"}, {"title": "4 Proof of Concept on Maltese", "content": "To showcase that the datasets collected in our survey (\u00a72) and the generated transcripts (\u00a73) constitute suitable training data for an EU-OSSFM, we conduct a proof-of-concept experiment on Maltese. Maltese was chosen because it is i) one of the lowest-resourced languages, and ii) the one for which Whisper achieves the worst results, as shown in Appendix D.\nFor our experiments, we first attempted to train an ASR model using only supervised data, but it failed to converge due to its limited size (16 hours). Therefore, we trained a model using the few labeled data together with the pseudo-labeled data. As an additional investigation, we also applied to the pseudo-labeled data simple filtering methods to remove audios containing other languages and automatic transcripts containing hallucinations (see Appendix B.2). Results presented in Table 3 show"}, {"title": "5 Conclusions", "content": "In response to the urgent need for truly open-source foundation models, this work takes the first step toward an EU open-source speech foundation model, which is the collection of suitable training data called MOSEL. To this end, we first surveyed the labeled and unlabeled speech datasets for automatic speech recognition that feature at least one of the 24 official EU languages and are available under a license compliant with the open-source terms. We then complemented this effort with the creation and release of automatic transcripts for the available unlabeled data. Overall, we collected more than 950k hours of speech content suitable for the training of an EU open-source speech foundation model, also demonstrating its usefulness in Maltese, one of the lowest-resourced languages."}, {"title": "7 Limitations", "content": "Collecting Open Irish Data. An important future direction to expand this work is represented by collecting and releasing new material \u2013 possibly with human-generated transcripts \u2013 under permissive licenses for the least-resourced language. This is especially critical for Irish, for which we were able to collect only 17 hours of (labeled) speech. As\nData Curation of Available Resources. noted in \u00a72, the quality of the supervision of the surveyed dataset cannot always be taken for granted, advocating for dedicated inspections before using it to train an OSSFM. This is particularly true for the metadata and transcripts of YouTube videos under OS-compliant licenses as those collected in YouTube-Commons.\nQuality of Pseudo-labels and Filtering Techniques. The quality of Whisper outputs greatly varies across the 24 languages. In particular, the WER of Whisper for Maltese is high (80.8 on the Common Voice test set and 73.8 on the FLEURS test set). As such, filtering strategies aiming at identifying unreliable transcriptions may be required for the successful training of OSSFM, especially for low-resource languages. Indeed, as already seen in \u00a74, even simple filtering techniques proved to be effective in greatly improving ASR performance. More advanced filtering techniques can provide further benefits for the quality of the resulting model. However, data cleaning and normalization are common steps in training pipelines, going beyond the scope of this work.\nBeyond EU languages. This paper has focused only on the 24 EU languages. An obvious next step for this work is its extension to many other spoken languages, with the final goal of covering hundreds of languages and leading to the creation of a universal OSSFM."}, {"title": "B.1 Model and Training Settings", "content": "We train a sequence-to-sequence model whose encoder is a 12-layer Conformer (Gulati et al., 2020) and whose decoder is a 6-layer Transformer (Vaswani et al., 2017). The Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5. We use an embedding size of 512 and an internal feed-forward dimension of 2048. The convolutional modules of the Conformer layers have a 31-feature kernel. The target vocabulary is built with size 8,000 using SentencePiece (Kudo, 2018), while the input audio is represented with 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms. As a result, the model has 116M parameters in total.\nWe use label-smoothed cross-entropy loss on the decoder output (with 0.1 as label-smoothing factor), complemented with a CTC (Graves et al., 2006) loss (summed with 0.5 weight) trained on the output of the 8th encoder layers to facilitate the convergence of the model. The model was optimized with Adam (\u03b21, \u03b22 = 0.9, 0.98) using Noam learning rate scheduler (Vaswani et al., 2017) with 2e-3 as peak learning rate and 25,000 warmup steps. To avoid overfitting, we set dropout to 0.1 and weight decay to 0.001 and apply SpecAugment (Park et al., 2019) during training. To further ease the convergence of the model, we initialize the Conformer encoder weights with those of a similar ASR model trained on 4k hours of labeled English data, comprising CommonVoice, Librispeech, CoVoST, and VoxPopuli. We train the models with mini-batches of 40,0000 tokens and 2 as update frequency on 4 NVIDIA Ampere A100 GPUs (64GB RAM) for 150k updates and average the last 7 checkpoints."}, {"title": "B.2 Data Filtering", "content": "B.2.1 LID\nTo check for possible inconsistencies between the metadata released in VoxPopuli and the actual content of speech segments, we check the actual spoken language with an automatic language identifier (LID). In fact, as in the transcription process described in \u00a73 we force the language to the one provided in the metadata, these segments may be paired with noisy transcripts. The LID was carried out using the Whisper large v3 model, as done for the transcription process, and it was performed by letting the model predict the language tag and taking the language with the highest probability.\nB.2.2 Textual Hallucinations\nIn the context of LLMs, hallucinations refer to \u201cthe generation of content that deviates from the real facts, resulting in unfaithful outputs\u201d (Maynez et al., 2020; Rawte et al., 2023). In our context of ASR, they have been analogously defined as \u201cnon-sensical, or unfaithful to the provided source input\" (Ji et al., 2023). Specifically, here we focus on the detection of nonsensical hallucinations, in which \u201cthe generated text fails to convey any relevant or comprehensible information\u201d, while those related to semantic aspects are ignored.\""}, {"title": "C Non-open Datasets", "content": "C.1 CC-BY-SA\nThe collection of datasets with the SA license, which is not compliant with open-source criteria, is presented in Table 8.\nC.2 CC-NC, -ND, and others\nThe collection of the most well-known datasets with a license that is not compliant with open-source criteria is presented in Table 9."}, {"title": "D Whisper Performance on EU Languages", "content": "Table 7 reports the WER scores obtained using Whisper on the 24 European languages. Maltese stands out as the worst language by a wide margin, with a very high WER (73.8 on FLEURS) indicating a limited ability to address the Maltese ASR task. All other languages display much lower WER, as only Estonian, Latvian, Lithuanian, and Slovenian exceed 15 WER, while high-resource"}]}