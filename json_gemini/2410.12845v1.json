{"title": "Toward Relieving Clinician Burden by Automatically Generating Progress Notes using Interim Hospital Data", "authors": ["Sarvesh Soni, PhD", "Dina Demner-Fushman, MD, PhD"], "abstract": "Regular documentation of progress notes is one of the main contributors to clinician burden. The abundance of struc- tured chart information in medical records further exacerbates the burden, however, it also presents an opportunity to automate the generation of progress notes. In this paper, we propose a task to automate progress note generation using structured or tabular information present in electronic health records. To this end, we present a novel frame- work and a large dataset, CHARTPNG, for the task which contains 7089 annotation instances (each having a pair of progress notes and interim structured chart data) across 1616 patients. We establish baselines on the dataset using large language models from general and biomedical domains. We perform both automated (where the best performing Biomistral model achieved a BERTScore F1 of 80.53 and MEDCON score of 19.61) and manual (where we found that the model was able to leverage relevant structured data with 76.9% accuracy) analyses to identify the challenges with the proposed task and opportunities for future research.", "sections": [{"title": "Introduction", "content": "Progress notes, also known as SOAP notes, are written by physicians in electronic health records (EHRs) and contain two broad categories of content: (1) Subjective and Objective status of a patient and (2) Assessment and Plan (A&P)\u00b9. These notes are written at regular intervals (e.g., every 24 hours) to document the journey of patient care for monitoring, sharing care responsibilities, and record-keeping. Documenting progress notes is the main contributor to physicians spending almost half of their time in front of computers, highlighting the need to automate this task2.\nThe patient information is continuously collected (e.g., laboratory values, mental status, equipment settings) during the provision of care, especially during a hospital stay, and stored in the structured or tabular format in the patient charts. On one hand, the abundant patient chart information present in EHRs enables physicians to better assess the progress of their patients (and thus write better EHR notes), while on the other hand, this presents associated challenges related to information overload and increased documentation burden. To this end, it is natural to make use of the available structured EHR data for automating progress note generation (PNG). However, most existing work on PNG focused on using doctor-patient conversations as input3\u20135, likely due to the ready availability of such data for the task. However, to our knowledge, no previous study harnessed the structured chart data available in the EHRs for PNG.\nThe Subjective and Objective sections of a progress note are based on the information collected about the patient. Information in the Subjective section is the patient's (or their family's) interpretation of their condition, thus it needs to be derived from the physician-patient conversation itself. Information in the Objective section is oftentimes pulled directly from the patient charts into the note as it is and relatively trivial to automate as its assembly does not require substantial human effort. Differently, the content in the A&P sections is composed by physicians after carefully examining the relevant patient information (including previous notes and structured data). Thus, in this study, we focus on automatically generating the A&P sections based on the content already available in the EHR (i.e., past progress notes and structured chart data). The main contributions of this study are as follows.\n\u2022 Introduce a novel method for automatically generating progress notes using available structured patient chart data (Figure 1).\n\u2022 Propose a novel dataset, CHARTPNG, for generating progress notes using structured EHR data\u00b9.\n\u2022 Identify the challenges involved in PNG by conducting an analysis of errors made by the baseline models."}, {"title": "Related Work", "content": "While the task of PNG is relatively unexplored in light of structured data, both automated PNG and structured data- to-text are well-researched. In the subsequent sections, we discuss the advancements made in both areas and further highlight the novelty of our approach in the context of current research."}, {"title": "Progress Note Generation", "content": "Automated text generation is a long-standing problem in medicine with several efforts specifically focused on progress notes. However, most past studies required clinicians to enter specific note-related information (including text snippets) into the system before a note was assembled using templates7\u201312. Thus, these approaches still burden clinicians with the task of gathering several important pieces of the notes.\nSeveral studies have focused on PNG using medical conversations as input. Krishna et al.5 proposed an algorithm for PNG for conversations from doctor-patient visits using deep summarization models and conducted evaluations on their proprietary medical dataset. Ramprasad et al.\u00b3 used a similar proprietary conversations dataset for improving PNG using section-specific adapters. Yim et al. 13 evaluated several summarization models for generating note snippets from medical visit conversations, which additionally included statements and commands intended for the medical scribes to expand those into the scribed notes. Michalopoulos et al. 14 generated progress notes from Family Medicine doctor- patient conversations using transformer-based models integrated with medical knowledge. Enarvi et al. 15 compared the transformer and recurrent neural network-based models for the task of PNG for visit dialogue from ambulatory orthopedic surgery encounters. Note that all these studies are conducted for outpatient setting data, where it is con-ceivable that A&P can be collected from the medical conversations. On the contrary, our work focuses on the inpatient setting where A&P are more complex (\u201cAtrial Fibrillation: Lopressor IV, Dilt as needed for HR control\") and are likely difficult to capture directly from the medical conversations in this setting where sometimes a medical dialogue may not be possible altogether (e.g., the patient is unable to communicate). On the flip side, a tremendous amount of structured chart data is continuously collected during in-patient encounters, making it available for automating PNG.\nInterestingly, much work on automated note generation focused on generating the Subjective section (or its subset) of the progress notes from doctor-patient conversations. Zhang et al. 16 centered on only the History of Present Illness (HPI) section of the progress notes as they observed that the other note sections were less frequently extracted from the input doctor-patient conversations. Similarly, Joshi et al. 17 and Chintagunta et al. 18 extracted past medical history information (e.g., symptoms, patient concerns, psychological and social history) from medical conversations. Liu et al. 19 extracted symptom information (corresponding to Review of Systems subsection) from the nurse-patient telemoni-toring dialogue. Moramarco et al. 20,21 systematically evaluated the generation of Subjective sections from consultation conversations. This further bolsters that it is challenging to extract all the sections from medical conversations alone.\""}, {"title": "Structured Data for Text Generation", "content": "Text generation is a widely studied field25\u201327. However, fewer advances have been made in the healthcare domain 28, with most approaches making use of structured data entries and canned text29. Two systems are developed to generate nursing shift summaries from the Neonatal Intensive Care Unit (NICU) data, as part of the BabyTalk project, for different durations of data such as 45 minutes (BT-45, 30) and 12 hours (BT-Nurse,31). However, the nursing summaries and progress notes differ in their syntax and semantics and thus need to be explored independently. Much work is present on automatically generating radiology reports from radiological examination images 32\u201336. Nonetheless, it is imperative to recognize the inherent differences between the data modalities (tabular versus imaging)."}, {"title": "CHARTPNG Dataset", "content": "We curated the proposed corpus using data from MIMIC-III37, a publicly available critical care database. Each anno- tation instance in CHARTPNG consists of a pair of progress notes along with all interim structured chart data (between the documentation times of the prior and the next notes in a pair). The first (prior) note in the pair serves as an input to large language model (LLM) along with structured data while the second (next) note in the pair serves as gold stan- dard (ground truth). Progress notes are written by different types of clinicians such as Attending Physicians, Medical Residents, and Nurses and, thus, they serve different purposes and differ in content as well as writing style. We focus on the progress notes written by Attending Physicians as they are responsible for a patient's care in the hospitals. The pairs of notes were selected if they (1) belong to the same admission and, between their documentation times, there is (2) no other documented progress note and (3) non-empty structured chart data."}, {"title": "Methods", "content": "Our proposed framework for the task of PNG is presented in Figure 2. We used open-source LLMs from general as well as biomedical domains as base models for our proposed task. The use of proprietary or closed-source LLMs raises several privacy and security concerns in healthcare, especially in the context of PNG38. Since we are dealing with sensitive data (from MIMIC-III), in a similar vein, we investigate locally-executable open-source models for the task. Specifically, we reported results from Biomistral 7B 39, Mixtral 8x7B40, and LLaMa 2 70B41 models. Biomistral is based on the Mistral model that is further pre-trained on the PubMed Central Open Access Subset. We chose Biomistral as it surpassed the performance of existing open-source medical models on multiple tasks 39. The Mixtral model is an improved variation of the Mistral model and has outperformed several state-of-the-art LLMs on different tasks 40. The LLaMa 2 model is the second generation variant of the LLaMa models and has been shown to outperform the proprietary models on some tasks 41.\nThe lengthy tabular chart data (see Table 1) is difficult to accommodate by the small context size of most open-source medical models including Biomistral which can handle up to 2k tokens (about 1250 words including model input and output texts). LLaMa 2 and Mixtral have longer context lengths, however, since the main focus of our evaluation is to establish fair baselines rather than achieving the state-of-the-art performance we use the context length of 2k tokens for all models. Thus, we summarize the structured chart data for the purposes of PNG. In order to further save prompt real estate, we condense the tabular data by mentioning each timestamp once and listing associated data thereafter. To direct the summarization process, we extract patient's current chief complaints from the A&P section of the prior note (as opposed to using the broad chief complaints from admission). Due to the context size limitations, the summarization step is also carried out in two iterative steps where an initial summary is obtained using the first chunk of the tabular chart data and the subsequent chunks are used to refine this existing summary. Finally, the prior A&P note and the synthesized summary of the structured EHR data is used to generate the next A&P note. The detailed information flow along with the model input prompts are shown are Figure 2.\nWe report automated metrics for evaluating text generation, namely, ROUGE42, BERTScore43 (using the model ROBERTALARGE 44), and MEDCON24, by comparing the predicted A&P notes to ground truth next A&P notes. ROUGE measures the text overlaps while BERTScore takes into account the semantic similarity between gold and prediction. Differently, MEDCON extracts Unified Medical Language System (UMLS) concepts from both gold and predicted notes and calculates an F1-score between the two sets of concepts to measure similarity. As a baseline in the evalu- ation, the prior note in the pair that serves as input to LLM along with structured data is used with no changes. We additionally performed a manual evaluation on a randomly selected sample. Here, we manually classified the predicted progress notes into different categories and analyzed the results with examples."}, {"title": "Results", "content": "Automated performance metrics from our baselines are reported in Table 2. The simple baseline of returning the prior note as prediction resulted in the highest scores across the automated metrics. Interestingly, this is an artifact of the high textual and semantic similarity between gold prior and next notes as, oftentimes, information text is copied between the progress notes. The low ROUGE scores from the LLM predictions underscores the limited overlap of text between the predicted and gold next A&P notes, as also seen in our manual analysis (Table 3). Among the evaluated LLMs, Biomistral achieved the best BERTScore F1 of 81.42 and MEDCON score of 21.99, which could be attributed to the medical knowledge that the model acquired through further training on the biomedical resources.\nThe results from our manual evaluation are reported in Table 3. The Biomistral model was apt at following the format of the A&P note, outputting the note in an appropriate format 83.4% (25) of the time. Interestingly, Mixtral was not able to capture the format as well as the other two models. The list of patient's ongoing problems is captured with 76.7% (23) accuracy by Biomistral while the other larger models performed better. All the models refrained from copying large chunks of text from the prior note and/or the structured data summary, with only the Biomistral model copying heavily in about 10-13%(3-4) of cases. Relevant information was identified and added to prediction in 73.3% (22) instances by Mixtral, which was the best among the models. Interestingly, for Biomistral, 43.3% (13) of the generated summaries of structured data contained some relevant information to aid in generation. In 10 (76.9%) of these instances, the model took the available information into account. Mixtral was able to extract important structured information the best (in 24 instances) while LLaMa was the best in harnessing the extracted information (100% of the times). We also encountered that all the models generated content which was not related to the patient in some instances, which can be considered as model hallucinations or confabulations. A detailed example prediction from the Biomistral model is shown in Figure 3."}, {"title": "Discussion", "content": "We described a task of automatically generating progress notes using structured patient chart data. We present a novel framework and release a representative dataset, CHARTPNG, for the task. Our automated and manual analyses uncovers challenges associated with PNG, setting a jump-board for future research on generating progress notes.\nThe quality of generated summaries of structured EHR data played an important role in the proposed framework. During our manual evaluation we saw that the summaries did not capture the relevant details in almost half of the cases. Though we supplied the patient's chief complaints (e.g., \u201cdiabetes", "vital signs": "but ignored problem-specific attributes (e.g., \u201cGlucose (serum)", "oriented to own ability": "to produce high-level assessment and plan components (e.g., \u201caltered mental status has improved today"}, {"title": "Conclusion", "content": "We outlined the task of automated progress note generation using structured patient chart data. The best performing LLM was Biomistral achieving a BERTScore F1 of 80.53, underlining the importance of medical fine-tuning. Through manual analysis, we surfaced challenges associated with the task and the shortcomings of the evaluated models. We also raised concerns about the existing automated evaluation metrics and provided future directions for improving both the performance and evaluation for PNG. Overall, we demonstrated that automating PNG is a feasible task, though further work is needed to address challenges identified during our analysis."}]}