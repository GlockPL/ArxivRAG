{"title": "SVGBuilder: Component-Based Colored SVG Generation with Text-Guided Autoregressive Transformers", "authors": ["Zehao Chen", "Rong Pan"], "abstract": "Scalable Vector Graphics (SVG) are essential XML-based formats for versatile graphics, offering resolution independence and scalability. Unlike raster images, SVGs use geometric shapes and support interactivity, animation, and manipulation via CSS and JavaScript. Current SVG generation methods face challenges related to high computational costs and complexity. In contrast, human designers use component-based tools for efficient SVG creation. Inspired by this, SVG-Builder introduces a component-based, autoregressive model for generating high-quality colored SVGs from textual input. It significantly reduces computational overhead and improves efficiency compared to traditional methods. Our model generates SVGs up to 604 times faster than optimization-based approaches. To address the limitations of existing SVG datasets and support our research, we introduce ColorSVG-100K, the first large-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset fills the gap in color information for SVG generation models and enhances diversity in model training. Evaluation against state-of-the-art models demonstrates SVGBuilder's superior performance in practical applications, highlighting its efficiency and quality in generating complex SVG graphics.", "sections": [{"title": "Introduction", "content": "Scalable Vector Graphics (SVG) is a widely used XML-based vector image format for defining graphics. Unlike raster graphics, which are composed of a fixed grid of pixels, SVGs are defined by geometric shapes such as points, lines, curves, and polygons, which makes them resolution-independent and infinitely scalable without loss of quality. This inherent scalability offers significant advantages over bitmap images, particularly in web and mobile applications where varying display resolutions and device capabilities are common. Furthermore, SVG supports interactivity and animation, and because it's text-based, it is easily searchable, compressible, and can be manipulated through CSS and JavaScript."}, {"title": "Methodology", "content": "In this section, we provide a comprehensive explanation of our proposed method for SVG generation. Our system framework consists of two primary parts. The first part involves the construction of the component library, which serves as the foundation for the SVG generation process. The second part is our SVG generation model, which utilizes the established component library to produce high-quality SVGs. The overall system framework, highlighting the interaction between these parts, is illustrated in Figure 3."}, {"title": "Component Library Construction", "content": "In our system, the generation of models is component-based, making the establishment of a component library the critical first step, as illustrated in Figure 3 (1). In Figure 3 (1a), we begin by decomposing all SVGs in the training set into individual paths, with each path forming a unique component. These isolated components exhibit a wide variety of appearances, necessitating a thorough reorganization.\nNext, as shown in Figure 3 (1b), we remove the colors from these components and normalize them. Initially, the geometric centers of these components do not align with the origin, complicating the scaling and normalization process. To address this issue, we calculate the geometric center of each component and translate it to the origin. We then scale the component such that its longest dimension measures 100 units, ensuring that the extents along both the positive and negative axes are 50 units. This process reorganizes and normalizes the components, making them ready for subsequent use in the model.\nThe initial component library contains many duplicate components. For example, flower shapes in a bouquet may be identical except for their colors. Therefore, it is necessary to merge these redundant components. As depicted in Figure 3 (1c), we start by merging paths that are identical in the SVGs. This initial merging reduces the number of components but does not eliminate all redundancies. To further refine the component library, we convert all components into 100 \u00d7 100 pixel images and then transform these images into grayscale. In these grayscale images, the areas enclosed by the paths are represented by 1 (white), and the background is represented by 0 (black). We then compute the Jaccard similarity index between each pair of images, sorting the similarity scores in descending order. Using a union-find data structure, we set a merging threshold and examine the similarity scores sequentially. If the similarity between two components meets or exceeds the threshold, we merge them in the union-find structure. The root nodes of these merged groups are considered the final components in the library. Subsequently, we replace occurrences of the child nodes with their corresponding root nodes in the training set. This process results in a refined and consolidated component library, ensuring minimal redundancy and optimal component organization for the model.\nWhen the normalized components need to be restored, as shown in Figure 3 (1d), it is necessary to place them in the correct positions, similar to how a human would arrange elements when drawing in software. This involves scaling the components according to a scale factor and translating them based on offset values on the X and Y axes (Offset X and Offset Y). This ensures accurate placement of the components. Finally, the generated RGB tokens are used to colorize the component, restoring its original appearance."}, {"title": "SVGBuilder", "content": "Our SVG generation model comprises three main components: the Text Encoder, the Image Encoder, and the SVG Decoder, as illustrated in Figure 3 (2). During training, the original dataset typically provides minimal text prompts, usually limited to category labels like \u201cflowers\" without further description of the SVG. To address this limitation, we introduce a Caption Generator, which is employed solely during training and not during inference.\nInitially, we rasterize the SVG into an image and feed this image into the Caption Generator, which produces a caption, such as \"A bouquet of flowers and a pink ribbon.\" This textual description is then input into the Text Encoder to generate text features. At the same time, we generate image features by inputting a blank image into the Image Encoder. The Image Encoder outputs image features, which are concatenated with the text features. These combined features are then input into the SVG Decoder. The SVG Decoder also takes an SVG Decoder Sequence as input to generate the next token. The partially generated SVG is rasterized into an image, which is subsequently fed back into the Image Encoder. This process repeats until an end token is generated or the maximum sequence length is reached. The role of the Image Encoder in this process is to allow the model to understand semantics not only from textual or path-based perspectives but also from visual perspectives. By incorporating the Image Encoder, the model can recognize and comprehend the components it selects at each step, including their layout and color. This dual learning mechanism ensures that the model grasps the specific visual attributes and spatial arrangements of the elements, enhancing its overall semantic understanding and performance in tasks involving both textual and visual data.\nSpecifically, as shown in Figure 3 (2a), the SVG Decoder Sequence consists of several tokens. Each complete component is represented by seven tokens. The sequence begins with a BOS (beginning of sequence) token, followed by a token representing the component. The next tokens indicate the translation offsets (Offset X and Offset Y) and the scaling factor (Scale Factor), specifying how the component should be placed. Finally, the sequence includes tokens for color (R, G, and B tokens), representing the RGB values.\nThis sequence is repeated for each component until the entire SVG is generated."}, {"title": "Experiments", "content": "In this section, we present a series of experiments conducted to substantiate the efficacy of our proposed SVGBuilder methodology. This section elucidates our evaluation metrics, describes our baseline comparisons, and offers a comprehensive analysis of the results derived from our experimental investigations. We also introduce our proposed ColorSVG-100K dataset, which serves as a critical resource for our experiments. Additionally, we conducted SVG complexity analysis, ablation experiments, and a case study to further validate our approach."}, {"title": "Dataset", "content": "The creation of the ColorSVG-100K dataset arises from the significant limitations observed in existing SVG datasets. Available SVG datasets are predominantly either black and white or stored as raster images, lacking the essential vector properties necessary for many applications. Such limitations pose substantial challenges for tasks requiring high-quality, scalable, and color-rich SVGs. The primary motivation behind developing the ColorSVG-100K dataset is to address these gaps and provide a robust foundation for research and applications in the field of colored SVG generation. This dataset, comprising 100,000 richly colored SVGs, is the first of its kind on a large scale. It not only fills the void of color information in existing datasets but also leverages the advantages of the SVG format, such as scalability and editability. For more information about the ColorSVG-100K dataset, including the motivation for its creation, the specific construction process, dataset statistics, and examples, please refer to the supplementary materials."}, {"title": "Experimental Settings", "content": "Evaluation Metrics We adopt evaluation metrics from SkexGen, IconShop (Wu et al. 2023a), and StrokeNUWA. Specifically, we use the Fr\u00e9chet Inception Distance (FID) (Heusel et al. 2017) to quantify the distance between the image features of the generated and ground-truth SVGs. Additionally, we utilize two types of CLIPScore (Radford et al. 2021): CLIPScore-T2I, which measures the similarity between the rasterized images of the generated SVGs and the textual prompts used for generation, and CLIPScore-I2I, which measures the similarity between the rasterized images of the generated SVGs and the rasterized images of the ground-truth SVGs. Furthermore, we assess the \"Uniqueness\" and \"Novelty\" of the generated SVGs, which are derived from SkexGen. \u201cUniqueness\" indicates the proportion of generated data occurring only once among all generated results, while \"Novelty\" refers to the proportion of generated data absent from the training set. We also incorporate the Human Preference Score (HPS) (Wu et al. 2023b) to evaluate user satisfaction with the generated outputs. These metrics collectively provide a comprehensive evaluation of the quality, originality, and performance of our SVG generation methodology. Lastly, we measure the generation speed per SVG to assess the efficiency of our method.\nBaselines In this paper, we compare our method with various SOTA open-source models. VectorFusion (Jain, Xie, and Abbeel 2023) utilizes diffusion models trained on pixel representations of images to generate SVGs, without requiring large datasets of captioned SVGs. By optimizing a differentiable vector graphics rasterizer (Li et al. 2020), VectorFusion extracts semantic knowledge from a pretrained diffusion model, resulting in high-quality vector graphics across various styles. CLIPDraw (Frans, Soros, and Witkowski 2022) employs a pretrained CLIP language-image encoder to synthesize novel drawings by optimizing the similarity between the generated drawing and the given description. It operates over vector strokes rather than pixel images, producing simple, human-recognizable shapes. DiffSketcher (Xing et al. 2024a), based on a pretrained diffusion model, optimizes a set of B\u00e9zier curves to create vectorized free-hand sketches that retain the structure and visual details of the subject. We re-implemented IconShop using Flan-T5 (Chung et al. 2024) as the backbone, employing autoregressive transformers to sequentialize and tokenize SVG paths and textual descriptions, resulting in improved icon synthesis capabilities. Additionally, we explored the capabilities of GPT-3.5 and GPT-4 (Achiam et al. 2023) in SVG generation, demonstrating advancements in large language models (LLMs) for this task. All these methods are text-guided, allowing us to evaluate the performance and effectiveness of the proposed SVGBuilder methodology against the current SOTA methods in the field of SVG generation.\nImplementation Details In our experiments, when rasterizing SVGs, we place them on a white background. For constructing the ColorSVG-100K dataset, \u201cCategory Calibration\" corrects misclassified SVGs. Both the Text Encoder and Image Encoder in this stage, as well as during model training and FID calculation, are CLIP models (Radford et al. 2021). During SVG processing, we utilize the DeepSVG Library. The Caption Generator is BLIP-2 (Li et al. 2023), and the SVG Decoder is based on GPT-2 (Radford et al. 2019). The batch size for training is set to 16, with a learning rate of 6 \u00d7 10-4. We use the AdamW optimizer for 50 epochs of training. The experiments are conducted on a single NVIDIA A800 GPU. For more implementation details, please refer to the supplementary materials."}, {"title": "Performance Comparison", "content": "We evaluate the performance of different models on the ColorSVG-100K dataset, as illustrated in Table 1. The models are categorized into two types: optimization-based and language-based. The arrows in the table indicate the preferred direction, with down arrows indicating lower values are better, and up arrows indicating higher values are better. In the Table 1, CLIPScore is divided into T2I, which represents the CLIPScore between the generated SVG image and the text description, and I2I, which represents the CLIPScore between the generated SVG image and the real SVG image. From Table 1, it is evident that the optimization-based models have lower FID scores, indicating that the images they generate are closer to the distribution of real images in the test set. In contrast, the language-based models, except for ours, generally have higher FID scores, suggesting that previous methods fail to accurately fit the dataset's distribution.\nRegarding CLIPScore, optimization-based methods generally achieve higher T2I scores. This might be because these models generate images that align better with the distribution of images on which the CLIP model is originally trained. The CLIP model might not have been extensively trained on SVG datasets, making it less effective at capturing the flat 2D nature of SVG graphics in CLIPScore-T2I scores. In contrast, the CLIPScore-I2I measures the similarity between generated and real SVG images, and hence is not affected by this limitation. Notably, our model also achieves the highest CLIPScore among the language-based models.\nSimilarly, since HPS uses the same CLIP model, it tends to favor SVG generation models that optimize from bitmap images. Regarding Uniqueness, the random nature of optimization-based models results in a 100% score, while language-based models exhibit slightly lower uniqueness due to potential similar outputs from similar inputs. Our model, however, achieves the highest uniqueness score among the language-based models. All models achieve a 100% score in Novelty.\nIn terms of Generation speed per SVG, the values in parentheses indicate the speed-up factor relative to VectorFusion. It is clear that optimization-based models take a significantly longer time to generate an SVG, with VectorFusion taking approximately 13.69 minutes and even the fastest, CLIPDraw, taking 2.92 minutes. In contrast, language-based models take only a few seconds, drastically reducing generation time. Our model achieves a generation time of 1.36 seconds, representing a 604.0\u00d7 speed-up compared to VectorFusion, which underscores the potential and advantages of language-based models in SVG generation."}, {"title": "SVG Complexity Analysis", "content": "We further analyze the impact of varying SVG complexities on model performance. We sort the SVGs in ascending order based on the number of paths and divide them into four levels of complexity: 0-25%, 25-50%, 50-75%, and 75-100%. The 0-25% range represents the simplest SVGs with the fewest paths, indicating the lowest complexity, while the 75-100% range encompasses the SVGs with the most paths, indicating the highest complexity. We evaluate the performance of different models across these complexity levels using the FID metric, as shown in Table 2. Our model achieves the best results among all models across all levels of complexity. It is observed that as the complexity of SVGs increases, the performance of optimization-based models does not deteriorate, whereas the performance of language-based models worsens. This may be because the randomly initialized bitmaps are generally more complex than SVGs."}, {"title": "Ablation Study", "content": "To evaluate the impact of different components on the overall performance of our model, we conduct an ablation study. The components subjected to ablation include the Caption Generator, Image Encoder, whether the Text Encoder is trained, and whether color fills are used in SVG during inference. We test each component sequentially, and the results are presented in Table 3. In these experiments, we observe both the FID and CLIPScore metrics. Since changes in the CLIPScore are not significant, we primarily focus on the FID metric. The results indicate that removing the Image Encoder results in the least performance loss on the FID metric, suggesting that the model does not effectively utilize the information from the Image Encoder. Removing the Caption Generator has a moderate impact on FID, as using the original SVG categories as input text lacks the data augmentation benefits provided by diverse captions. Freezing the Text Encoder leads to a significant decline in FID performance, highlighting the importance of training the Text Encoder for maintaining model performance. Lastly, not filling colors during inference and only performing outlines also results in a decrease in FID."}, {"title": "Case Study", "content": "In our case study, we select SVGs generated by different models for comparative analysis. We focus on language-based models, including GPT-4, IconShop, and our model. For comparison, we also include DiffSketcher, an optimization-based model with the lowest FID score. The results are illustrated in Figure 4.\nDiffSketcher produces visually impressive results, accurately capturing the original shapes and colors of objects, likely due to the application of the Diffusion model. However, some artifacts in the form of odd lines appear during the bitmap-to-SVG optimization process. The output from GPT-4 aligns well with the objects in terms of color and somewhat meets the contour requirements, but without accompanying text, the contours are hard to interpret. IconShop generates accurate outlines except for the taxi example, which performs poorly. This discrepancy might be due to the greater complexity of SVGs in our dataset compared to those used by IconShop, indicating its suitability for simpler SVG generation tasks.\nOur model's outputs match the objects in both color and contour. However, the \u201cavocado\" example is influenced by the seed, affecting the overall hue, and the \u201ctaxi\" appears abstract. Overall, our model approaches the expression of the optimization-based models, demonstrating its potential."}, {"title": "Limitations", "content": "While SVGBuilder demonstrates strong performance in component-based autoregressive SVG generation, it does have certain limitations. Our ColorSVG-100K dataset fills a notable gap in the realm of colored SVG datasets, yet each path within the dataset currently lacks explicit semantics. This limitation forces the model to implicitly infer these semantics, which may not always be reliable. Additionally, the construction of the component library is crucial within this framework. We normalize components through translation and scaling based on similarity for merging. However, incorporating additional geometric transformations, such as rotation and reflection, could potentially further reduce the size of the component library. Although the similarity-based merging process is relatively time-consuming, it is conducted offline, representing a one-time computational cost. Furthermore, when utilizing a union-find data structure for merging, selecting the parent node is critical. Even with a similarity threshold in place, nodes within the same set may have varying degrees of similarity to the parent node due to the non-transitive nature of similarity. Additionally, the component-based approach can be more challenging to innovate compared to stroke-based methods. In our language model training, we use cross-entropy as the loss function, treating numbers as separate tokens. However, cross-entropy may not effectively capture geometric relationships. Looking ahead, we aim to address these challenges and make incremental advancements in the field of component-based autoregressive SVG generation."}, {"title": "Conclusion", "content": "In this study, we present SVGBuilder, an innovative component-based autoregressive model designed for the generation of colored SVGs, alongside the introduction of the ColorSVG-100K dataset. Through rigorous evaluation, our approach demonstrates SOTA performance, surpassing both optimization-based and language-based models in the FID metric. Remarkably, SVGBuilder not only excels in visual quality but also offers a substantial 604\u00d7 enhancement in generation speed compared to optimization-based models, showing great potential for future applications."}, {"title": "Autoregressive Language Models", "content": "Autoregressive language models predict the next token in a sequence based on previous tokens. Formally, given a sequence of tokens $a = (a_1,a_2,...,a_T)$, the probability of the sequence is decomposed as:\n$P(a) = \\prod_{t=1}^{T} P(a_t | a_{1:t-1}).$\nHere, $P(a_t | a_{1:t-1})$ represents the probability of the token $a_t$ given the preceding tokens $a_{1:t-1}$. This approach allows the model to generate coherent text by sampling each token sequentially. The training objective is to maximize the likelihood of the observed sequence, which is achieved by minimizing the negative log-likelihood:\n$L = - \\sum_{t=1}^{T} log P(a_t | a_{1:t-1}).$"}, {"title": "Scalable Vector Graphic (SVG)", "content": "Scalable Vector Graphics (SVG) is an XML-based markup language for describing two-dimensional vector graphics. Unlike raster images, SVGs are mathematically defined, allowing them to be scaled to any size without loss of quality, ensuring sharp images on all devices. Additionally, SVGs are typically smaller, improving web page load times. These features make SVG a versatile and powerful tool for creating high-quality, scalable, and interactive graphics.\nSVG Representation In SVGs, various tags are employed to represent specific shapes, such as <rect> for rectangles, <circle> for circles, and <path> for paths. This variety in tag usage leads to inconsistency in data representation and hinders component-level manipulation and scaling. To address this issue, we standardize all shapes to <path> tags, following the approach outlined in DeepSVG (Carlier et al. 2020). Within the <path> tag, we use uniform commands for drawing, including M (Move to), L (Line to), C (Cubic B\u00e9zier), and Z (Close path), as shown in Table 4. Other basic shapes can be represented using a sequence of B\u00e9zier curves and lines. Following StrokeNUWA (Tang et al. 2024), a simplified SVG $G = {P_i}_{i=1}^N$ is represented by N SVG paths, where each path $P_i$ consists of $M_i$ basic commands: $P_i = {C_{ij}}_{j=1}^{M_i}$. Here, $C_{ij}$ denotes the j-th command in the i-th path. Each basic command $C = (T, V)$ includes a command type $T \\in {M, L, C}$ and its corresponding positional argument V."}]}