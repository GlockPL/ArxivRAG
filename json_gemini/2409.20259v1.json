{"title": "Learning to Ground Existentially Quantified Goals", "authors": ["Martin Funkquist", "Simon St\u00e5hlberg", "Hector Geffner"], "abstract": "Goal instructions for autonomous Al agents cannot assume that objects have unique names. Instead, objects in goals must be referred to by providing suitable descriptions. However, this raises problems in both classical planning and general-ized planning. The standard approach to handling existen-tially quantified goals in classical planning involves compil-ing them into a DNF formula that encodes all possible vari-able bindings and adding dummy actions to map each DNF term into the new, dummy goal. This preprocessing is expo-nential in the number of variables. In generalized planning, the problem is different: even if general policies can deal with any initial situation and goal, executing a general policy re-quires the goal to be grounded to define a value for the policy features. The problem of grounding goals, namely finding the objects to bind the goal variables, is subtle: it is a generaliza-tion of classical planning, which is a special case when there are no goal variables to bind, and constraint reasoning, which is a special case when there are no actions. In this work, we address the goal grounding problem with a novel supervised learning approach. A GNN architecture, trained to predict the cost of partially quantified goals over small domain instances is tested on larger instances involving more objects and differ-ent quantified goals. The proposed architecture is evaluated experimentally over several planning domains where general-ization is tested along several dimensions including the num-ber of goal variables and objects that can bind such variables. The scope of the approach is also discussed in light of the known relationship between GNNs and $C_2$ logics.", "sections": [{"title": "Introduction", "content": "In classical planning, the usual assumption is that objects have unique names, and goals are described using these names. For instance, a goal might be to place block A on top of block B, where A and B are specific blocks. A similar assumption is made in generalized planning, where a policy is sought to handle reactively any instances within a given domain. To apply the general plan to a particular domain instance, it is assumed that the goal consists of a conjunc-tion of grounded atoms, with objects referred to by unique names.\nHowever, instructions for goals in autonomous AI agents cannot assume that objects have individual, known names. Instead, goals must be expressed by referring to objects us-ing suitable descriptions. For instance, the instruction to place the large, yellow ball next to the blue package or to construct a tower of 6 blocks, alternating between blue and red blocks, does not specify the objects uniquely, and it's indeed part of the problem to select the right objects.\nTo address goals in classical planning that do not in-clude unique object names, existentially quantified goals are used (Pednault 1989; Gazen and Knoblock 1997; Haslum et al. 2019). For example, the goal of placing the large, yel-low ball next to the blue package can be expressed with the following formula:\n$\\exists x,y: [BALL(x)^ LARGE(x) ^ YELLOW(x)  PACKAGE(y) ^ BLUE(y)  NEXT(x, y)]$.\nHowever, there are not many classical planners that sup-port existentially quantified goals because dealing with such goals is computationally hard, even when actions are absent. Determining whether an existentially quantified goal is true in a state is indeed NP-hard, as these goals can easily express constraint satisfaction problems (Franc\u00e8s and Geffner 2016). For example, consider a graph coloring problem over a graph G = (V, E), where V represents vertices $i \\in V$ and E represents edges $(k,j) \\in E$. This problem can be expressed via the goal:\n$\\exists_{x_1, ..., x_n} [\\land_i COLOR(i, x_i) \\land \\land_{k,j\\in E} NEQ(x_j, x_k)]$\nThis expression applies to an initial state with ground atoms COLOR(i, x), where x is a possible color of vertex i, and ground atoms NEQ(x, x') express that x and x' are distinct.\nThe standard approach to handling existentially quanti-fied goals in classical planning involves eliminating them by transforming them into a grounded DNF formula that en-codes all possible variable bindings. This process also in-volves adding dummy actions for each DNF term to map them to a new, dummy goal (Gazen and Knoblock 1997). However, this preprocessing step is exponential in the num-ber of variables. For instance, in the graph coloring exam-ple, it results in several terms and dummy actions that grow exponentially with the number of vertices n.\nThe problem of grounding goals, which involves finding objects to bind the goal variables, is subtle. It is a gener-alization of standard classical planning where there are no goal variables to bind, and it is a special case of constraint"}, {"title": "2 Example: Visit-1", "content": "We begin with an example to illustrate our learning task, estimating the cost of existentially quantified goals in plan-ning, and why GNNs provide a handle on this problem given the known correspondence between GNNs and $C_2$, the frag-ment of first-order logic with two variables and counting (Barcel\u00f3 et al. 2020; Grohe 2021). Indeed, we show that the general value (cost) function for the problem can be ex-pressed in $C_2$ and hence can be learned with GNNs.\nThe example is a variation of the Visitall domain where a robot is placed on a grid and can move up, down, left, or right. In the original domain, the goal is for the robot to visit all the cells in the grid. Each cell is represented as a single object, and two cells x and y are considered adjacent if there is a CONNECTED(x, y) atom in the state. A cell x is marked as visited with a VISITED(x) atom. The location of the robot is given by AT-ROBOT(x), where x is a cell.\nIn our variation, cells have colors from a fixed set of colors C and the goal is to visit a cell of a given color. The cost of the problem is thus given by the distance to the closest cell of that color. For reasons to be elaborated later, the distances involved must be bounded with the set D representing the distances up to such a bound. The goals have the form:\n$G = \\exists x : C(x)^ VISITED(x)$,\nwhere C is any of the colors in C. Later on, we will consider a further variation of the problem where cells of different colors are to be visited. The objective is to express the cost function for this family of problems using Boolean features definable in $C_2$ so that the cost function can be learned with GNNs.\nThe Boolean function $SP_{d,c}(x)$ determines the existence of a shortest path of length d from x to a cell with color C:\n$P_{0,c}(x) = C(x)$\n$P_{d,c}(x) = \\exists y: CONNECTED(x, y) ^ P_{d-1,c}(y)$\n$SP_{d,c}(x) = P_{d,c}(x)^\\neg P_{d-1,c}(x)$\nIf we let N stand for the max distance plus 1, the value function $V^*$ can then be expressed as:\n$G_C = v : C\\in G(V)$\n$D_{d,c} = G_C ^\\exists x : [AT-Robot(x) ^ SP_{d,c}(x)]$\n$V^*(s; G) = min_{d \\in D, C \\in C} d \\cdot [D_{d,c}] + N\\cdot [\\neg D_{d,c}]$\nHere, s represents a state, and G denotes a goal. The nota-tion [] is the Iverson bracket, and $C_G$ refers to the colors specified in G. When predicate names are used without sub-scripts, they refer to s. If we assume that the state s does not contain any VISITED atoms, then $V^*$ calculates the dis-tance to the nearest cell with the color C, provided such a cell exists. If no such cell exists, the value is set to N.\nWe will learn $V^*$ using GNNs. Since the expressiveness of GNNs is limited by two-variable first-order logic $C_2$, if $V^*$ cannot be expressed with $C_2$ features, it will not be learn-able by GNNs. Existentially quantified goals are thus han-dled \"by free\" by GNNs as long as they do not get out of these limits. Moreover, existentially quantified goals with more than two variables are not necessarily a problem if they are equivalent to formulas in $C_2$. For example, the goal of building a tower of five blocks is naturally written in terms of five variables, yet the formula is equivalent to a formula with two variables only that are quantified multiple times."}, {"title": "Related work", "content": "Quantification in planning. Since the middle nineties, planners usually ground all actions and goals to improve ef-ficiency. This does not rule out the use of existential and universal quantification in action preconditions and effects, and goals (Pednault 1989; Haslum et al. 2019). Universal quantification can be replaced by conjunctions, while exis-tential quantification by disjunctions (Gazen and Knoblock 1997). The problem with existential quantification in the goal is that the number of resulting disjuncts is exponential in the number of goal variables. In action preconditions, the"}, {"title": "Background", "content": "We review classical planning, generalized planning, and ex-istentially quantified goals."}, {"title": "Classical Planning", "content": "A classical planning problem is a pair P= , where D is a first-order domain and I contains information about the instance (Geffner and Bonet 2013; Ghallab, Nau, and Traverso 2016; Haslum et al. 2019). The domain D has a set of predicate symbols p and a set of action schemas with pre-conditions and effects given by atoms $p(x_1,...,x_k)$ where p is a predicate symbol of arity k, and each $x_i$ is an argument of the schema. An instance is a tuple I = (O, Init, Goal) where O is a set of object names $c_i$, and Init and Goal are sets of ground atoms $p(c_1, ..., ck)$.\nA problem P = (D,I) encodes a state model S(P) = (S, So, SG, Act, A, f) in compact form where the states s\u2208 S are sets of ground atoms, so is the initial state I, SG is the set of goal states s such that $S_G \\subseteq s$, Act is the set of ground actions, A(s) is the set of ground actions whose preconditions are true in s, and f is the induced transition function where s' = f(a, s) is the resulting state after applying a \u2208 A(s) in state s. An action sequence $a_0, ..., a_n$ is applicable in P if $a_i \u2208 A(s_i)$ and $s_{i+1}$ = f($a_i, s_i)$, for i=1,...,n, and it is a plan if $s_{n+1} \\in S_G$.\nThe cost of a plan is assumed to be given by its length and a plan is optimal if there is no shorter plan. The cost of a goal G for a problem P is the cost of the optimal plan to reach G from the initial state of P."}, {"title": "Generalized Planning", "content": "In generalized planning, one is interested in solutions to collections Q of problems P over the same planning do-main (Khardon 1999; Mart\u00edn and Geffner 2004; Fern, Yoon, and Givan 2006). For example, the class of problems Q may include all Blocksworld instances where a given block x must be cleared, or all instances of Blocksworld for any (grounded) goal. A critical issue in generalized planning is the representation of these general solutions or policies \u03c0 which must select one or more actions in the reachable states s of the instances P \u2208 Q. A common representation of these policies is in terms of general value functions V (Franc\u00e8s et al. 2019; St\u00e5hlberg, Bonet, and Geffner 2022a) that map states s into non-negative scalar values V(s). The general policy $\\pi_V$ greedy on V then selects the action a applicable in s that result in successor state s' with minimum V(s') value. If the value of the child s' is always lower than the value of its parent state s, the value function V represents a general policy $\\pi_V$ that is guaranteed to solve any problem in the class Q.\nIn this formulation, it is assumed that the state s over a problem P in Q also encodes the goal G of P given by a set of ground atoms $P_G(C_1,...,c_k)$ for each ground goal p($C_1, ..., c_k$) in P. The new goal predicate $P_G$ (Mart\u00edn and Geffner 2004) is used to indicate in the state s that the atom p($C_1,..., c_k$) is to be achieved from s and that it is not nec-essarily true in s."}, {"title": "Existentially Quantified Goals", "content": "A classical planning problem with partially quantified goals is a pair $P_x = (D, I_x)$ where D is a first-order domain, X is a set of variables, and $I_x$=(O, Init, $G_x$) expresses the in-stance information as before, with one difference: the atoms p($t_1,..., t_k$) in the goal $G_x$ can contain variables x from X, which are assumed to be existentially quantified. That is, in quantified problem $P_x$, the terms $t_i$ can be either constants $c_i$ from O referring to the objects or variables x from X. The variables x in the goal $G_x$ of $P_x$ introduce a small change in the semantics of a standard classical plan-ning problem, where a state s over $P_x$ is a goal state if there is a substitution of the variables $x_i$ in $G_x$ by constants $c_i$, $x_i \\Rightarrow c_i, x_i \\in X, c_i \\in O$, such that the resulting fully grounded goal $G_c$ is true in s; namely, $G_c \\subseteq s$. Quantifi-cation is sometimes used in action preconditions and some-"}, {"title": "Task: Learning to ground goals", "content": "The problem of generalized planning with partially (exis-tentially) quantified goals can be split into two; namely, learning to ground the goals; i.e., substituting the goal vari-ables with constants, and learning a general policy for fully grounded goals. Since the second problem has been ad-dressed in the literature, we focus solely on the first part: learning to ground a given partially quantified goal in a plan-ning problem P that belongs to a large class of instances Q over the same planning domain but which may differ from P on several dimensions including the number of objects and the number of goal atoms or variables.\nWe approach this learning task in a simple manner: by learning to predict the optimal cost of partially grounded goals G in families of problem instances P from a given domain D. Recall that the cost of G is the cost of P if G is the goal of P.\nFor making these cost predictions, a general value func-tion V is learned to approximate the optimal cost function $V^*$. The value function V accepts a state s and a partially quantified goal G and outputs a non-negative scalar that esti-mates the (min) number of steps to reach a state s' from s in P such that s' satisfies the goal G (i.e., there is a grounding G' of G that is true in s'). We write the target value function as V(s; G) when we want to make explicit the goal G, else we write it simply as V(s).\nWe learn the target value function V(s; G) over a given domain, where G is a partially quantified goal, in a su-pervised manner. Namely, for several small instances P from the domain, we use as targets for V(s; G), the opti-mal cost $V^*$(s; G) of the problems P[s, G] that are like P but with initial state s and goal G. The learned value func-tion V (s; G) is expected to generalize among several dimen-sions: different initial states, instances with more objects, and different goals with more goal atoms, variables, or both.\nThe learned value function V(s; G) can then be used to bind the variables in the partially quantified goal. For bind-ing a single variable in G, we consider the goals $G' = G_{x=c}$ that result from G by instantiating each variable x in G to a constant c, while greedily choosing the goal G' that mini-mizes V(s; G'). For binding all the variables in G, the pro-cess is repeated until a fully grounded goal is obtained.\nThe quality of these goal groundings can then be deter-mined by the ratio $V^*(s;G')/V^*(s;G)$ where $V^*$(s; G') is the optimal cost of achieving the grounded goal G' and $V^*$(s; G) is the optimal cost of achieving the partially quan-tified goal G. This ratio is 1 when the grounded goal G' is optimal and else is strictly higher than 1. For large instances, for which the optimal values cannot be computed, $V^*$ val-ues are replaced by $V^L$ values obtained using a non-optimal planner that accepts existentially quantified goals.\nThe ability to map quantified goals G into fully grounded goals G' can be used in two different ways. In classical plan-ning, it can be used to seek plans for the quantified goals by seeking plans for the fully grounded goal G', while in gen-eralized planning, it can be used to apply a learned general policy for achieving a partially quantified goal G: for this G is replaced by G'."}, {"title": "Architecture", "content": "We use Graph Neural Networks (GNNs) to learn how to bind variables to constants. Since plain GNNs can only pro-cess graphs and not relational structures, we use a suitable variant (St\u00e5hlberg, Bonet, and Geffner 2022a). We describe GNNs first and then this extension."}, {"title": "Graph Neural Networks", "content": "GNNs are parametric functions that operate on graphs through aggregate and combination functions, denoted aggi and combi, respectively (Scarselli et al. 2009; Gilmer et al. 2017; Hamilton 2020). GNNs maintain and update embed-dings $f_i(v) \\in \\mathbb{R}^k$ for each vertex v in a graph G. This process is performed iteratively over L layers, starting with i = 0 and the initial embeddings $f_0(v)$, and progressing to $f_{i+1}(v)$ as follows:\n$f_{i+1}(v) = comb_i (f_i(v), agg_i ({{ f_i(w) | w \\in N_G(v)}}))$\nwhere $N_G(v)$ is the set of neighboring nodes of v in the graph G. The aggregation function $agg_i$ (e.g., max, sum, or smooth-max) condenses multiple vectors into a single vector, while the combination function $comb_i$ merges pairs of vectors. The function implemented by GNNs is well-defined for graphs of any size and is invariant under graph isomorphisms, provided that the aggregation functions are permutation-invariant."}, {"title": "Relational GNNS", "content": "GNNs operate over graphs, whereas planning states s are relational structures based on predicates of varying arities. Our relational GNN (R-GNN) for processing these struc-tures is inspired by the approach used for solving min-CSPs (Toenshoff et al. 2021) and closely follows the one used for learning general policies (St\u00e5hlberg, Bonet, and Geffner 2022a), where the objects or in a relational structure (state) exchange messages with the objects of through the atoms q = p($o_1,..., o_m$) in the structure that involves the"}, {"title": "Learning the Value Function", "content": "The architecture in Algorithm 1 requires two inputs: a set of atoms A and a set of objects O. Given a state s over a set of objects O, along with a quantified goal G over both a set of variables V and the objects O, we need to transform these into a single set of atoms, A, and objects O. The set of objects $\\hat{O}$ = O\u222aV is set to contain the original object and the variables, regarded as extra objects. The set of atoms $\\hat{A}$ in turn, is defined as\n$\\hat{A}$ = S\u222a {$P_G$(\u00b7) : P(\u00b7) \u2208 G} \u222a {CONSTANT(o) : o \u2208 O}\n\u222a {VARIABLE(v) : v \u2208 V} \u222a B.\nRecall that for atoms in the goal, we use a specific goal predicate $P_G$, instead of P, to extend the atoms in the state (Mart\u00edn and Geffner 2004). In addition, we use the unary predicates CONSTANT and VARIABLE to differentiate between true objects and objects standing for variables. Fi-nally, the set B contains a binary predicate, POSSIBLEBIND-ING, that enables communication between objects and vari-ables. We define B as:\nB = {POSSIBLEBINDING(o, v) : o \u2208 O, v \u2208 V}.\nWithout these atoms, constants, and variables cannot com-municate when the goal is fully quantified. This implies, for example, that the final embedding of a variable does not depend on the current state.\nThe set of node embeddings at the last layer of the R-GNN is the result of the net; that is, R-GNN(A, O) = {$f_L$(o) | o \u2208 O}. Such embeddings are used to encode general value functions, policies, or both. In this paper, we encode a learn-able value function V (S, G) through a simple additive read-out that feeds the embeddings into an MLP:\nV(s, G) = MLP($\\Sigma_{o\\in O} f_L(o)$) .\nThis value function will be used to iteratively guide a con-troller to ground G, one variable at a time, until it is fully grounded. This means that G can be a partially quantified goal. Note that this is necessary because there is an expo-nential number of possible bindings overall, but only a lin-ear number of possible bindings for a single variable. The loss we use for training the R-GNN is the mean square error (MSE)\n$\\mathcal{L}(s, G) = (V (s, G) \u2013 V^*(s, G))^2$\nover the states s and goals G in the training set. The cost of a goal is defined as the optimal number of steps required to achieve it from a given state, denoted by $V^*$(s, G). A state satisfies G if there exists a complete binding such that the resulting grounded goal is true in s. We compute $V^*$(s,G) using breadth-first search (BFS) from s to determine the op-timal distance to the nearest state that satisfies G. If there is no reachable state that satisfies G, then we use a large cost to indicate unsatisfiability (but not infinity). Generally, de-ciding $V^*$ is computationally expensive; however, we train our networks on instances with small state spaces."}, {"title": "Expressivity Limitations", "content": "In our experiments, we focus primarily on the use of a fixed set of colors. This same set is also used in the example pre-sented in Section 2. Here, we explore why we chose to fix the set of colors and demonstrate that if the colors were al-lowed to vary as part of the input, R-GNNs would not be expressive enough.\nIt is important to determine if an object o and a variable x have the same color. Generally, we represent a color with an object c and express that an object o has this color with the atom HASCOLOR(o, c). Similarly, HASCOLOR(x, c) repre-sents that a variable x has color c. To express that o and x share the same color, we use the formula:\nSAMECOLOR (0o, x) = $\\exists c$: HASCOLOR(0, c)\nHASCOLOR(x, c)."}, {"title": "Domains", "content": "Now, we will describe the domains used in our experi-ments and the types of goals that the architecture must learn to ground. All domains are taken from the International Planning Competition (IPC), but are augmented with col-ors. With colors, we can express a richer class of goals, where objects can be referred to by their (non-unique) col-ors. However, the overall approach is general, and descrip-tions involving other object attributes such as size or shape and their combinations can also be used, if such predicates are used in the training instances."}, {"title": "Blocks", "content": "Blocks is the standard Blockworld domain with a gripper and block colors that can be used in the goals. Figure 1 depicts an instance of the domain involving 8 blocks, named from A to H, and expressed as:\n$\\exists x_1,...,x_5$: BLUE($x_1$) \\ RED($x_2$) \\ BLUE($x_3$)\n\\ RED($x_4$) ^ RED($x_5$) ^ ON($x_1$,$x_2$)\n^ON($x_2$, $x_3$) ^ ON($x_3$, $x_4$) ^ ON($x_4$, $x_5$).\nThis goal requires constructing a tower in which a blue block $x_1$ rests on a red block $x_2$, which in turn rests on a blue block $x_3$, which rests on a red block $x_4$, and finally, that rests on another red block $x_5$. The optimal grounding involves binding block $x_5$ to block F and block $x_4$ to block A, so that the target tower is built on top of F.\nLet us emphasize that the grounding problem tackled by the learner is not easy. Firstly, the resulting binding needs to be logically valid, meaning it must adhere to the static con-straints of the goal, such as colors in this case. A binding would be logically invalid if a variable $x_i$ is bound to a con-stant c with a different color from $x_i$ in the goal. Secondly, the resulting binding must lead to a reachable goal. In this domain, it implies that no pair of variables $x_i$ can be bound to the same constant, as the blocks $x_i$ need to form a single tower. Lastly, the resulting binding needs to be optimal; that is, it must be achievable in the fewest steps possible. None of these three properties \u2013 validity, reachability, or optimality \u2013 are hardcoded or guaranteed by the learning architecture, but we can experimentally test them. Planners that handle exis-tentially quantified goals by expanding them into grounded DNF formulas ensure and exploit validity. By pruning the DNF terms that are not logically valid, they exploit (but do not guarantee) reachability by removing DNF clauses con-taining mutually exclusive atom pairs.\u00b9 They ensure opti-mality if they are optimal planners. In our setting, validity, reachability, and optimality, or a reasonable approximation, must be learned solely from training.\nThe domain called Blocks-C is essentially the same as the Blocks domain, but instead of focusing on the relation ON, it revolves around the relation CLEAR. This implies that the goal is to dismantle towers to uncover blocks of specific colors, with the fewest steps possible."}, {"title": "Gripper", "content": "In Gripper, there is a robot equipped with two grippers that can pick up balls and move them between rooms. In our version, the balls are colored, and there is a fixed number of rooms that are not necessarily adjacent to each other. Figure 2 provides an example. In this figure, there are three"}, {"title": "Delivery", "content": "In this domain, there is a truck, several packages, and a grid with colored cells. The goal is to distribute the packages to cells with specific colors. In the following example, there are 3 packages, $p_1, p_2$, and $p_3$, to be distributed using truck t:\n$\\exists x_1,...,x_4$:AT($p_1$, $x_3$) ^ AT($p_3$, $x_2$) ^ AT($p_4$, $x_1$)\n^AT(t, $x_4$) ^ YELLOW($x_1$) ^ RED($x_2$)\n^ GREEN ($x_3$) ^ BLUE($x_4$)\n\u039b [\u039bi,j NEQ($x_i$, $x_j$)]\nTo distribute these packages efficiently, the truck must de-cide on an order to pick up the packages and deliver them to cells close to this planned route to minimize the total cost.\nIn this example, the truck must deliver package $p_1$ to a green cell $x_3$, $p_3$ to a red cell $x_2$, $p_4$ to a yellow cell $x_1$, and then park the truck t in a blue cell $x_4$. As seen in Figure 3(b), the learned model selects cells close to where the packages are, which lowers the total distance traveled by the truck."}, {"title": "Visitall", "content": "In a more general version of Visit-1, presented in Section 2, the robot is required to visit at least one cell of each color specified in the goal while also minimizing the total distance traveled. Figure 4 provides an example. The example goal is:\n$\\exists x_1,...,x_5$: [BLUE($x_1$) ^ BLUE($x_3$) ^ BLUE($x_4$)\n^RED($x_2$) ^ RED($x_5$) ^ VISITED($x_1$)\n^ VISITED($x_2$) ^ VISITED($x_3$)\n^VISITED($x_4$) ^ VISITED($x_5$)]\n\u039b [\u039bi,j NEQ($x_i$, $x_j$)]\nThe robot needs to visit 3 blue cells $x_1, x_3$, and $x_4$, and 2 red cells $x_2$ and $x_5$. An optimal solution with a cost of 5 is shown in Figure 4. This domain poses a difficult opti-mization problem, similar to but distinct from the Traveling Salesman Problem (TSP)."}, {"title": "Experimental Results", "content": "Once a model has been trained to predict the cost of partially quantified goals relative to a state, it is then possible to use it as a policy. If V is a learned model for some domain, then it defines a policy $\\pi_V$ over the space of partially quantified goals as follows. Let s and G be a state and a goal, respec-tively; the successors $N(s, G)$ of s and G are a set of goals where G has bound a single variable with a constant. The policy $\\pi_V$ is:\n$\\pi_V (\u03b4, G) = arg min V(s, G_{x=c})$.\n$G_{x=c}\\in N(s,G)$\nThe policy is iteratively used until the goal is fully grounded. By binding a single variable, we avoid the combinatorial ex-"}, {"title": "Analysis: Visit-Many", "content": "We now continue the example from Section 2 and present a detailed analysis where a specific number of colors must be visited. In our experiments, we allow the number of colors in the goal to vary, but here we assume exactly k colors:\n$\\exists x_1,...,x_k$:C1($x_1$)^\u00b7\u00b7\u00b7^ Ck($x_k$), where $C_i$ \u2208 C.\nTo determine distances, we adjust the Boolean functions P and SP to check if there is a path of length d that visits"}, {"title": "Conclusions", "content": "We considered the problem of learning to ground existen-tially quantified goals, building on existing techniques for learning general policies using GNNs. These methods usu-ally require goals that are already grounded, and the aim is to enable them to support existentially quantified goals by grounding them beforehand. This preprocessing step is also useful for classical planning, where existentially quantified goals exponentially increase time and space requirements.\nWe also discuss this approach in relation to $C_2$ logics, as it builds on relational GNNs for representing and learning gen-eral value functions (St\u00e5hlberg, Bonet, and Geffner 2022a).\nGrounding existentially quantified goals after learning is computationally efficient and does not require search. The process involves greedily following the learned value func-tion, substituting one variable at a time with a constant. However, learning such a value function is challenging be-cause the grounded goals must satisfy conditions of logi-cal validity, reachability, and optimality. We tested these conditions experimentally on instances with more objects, goal atoms, and variables. In the future, we plan to explore unsupervised methods for grounding learning, fine-tuning weights using reinforcement learning as done by (St\u00e5hlberg, Bonet, and Geffner 2023), and experimenting with GNN ar-chitectures with improved expressive power (Barcel\u00f3 et al. 2020; Grohe 2021). This might include adding memory and recursion capabilities (Pfluger, Cucala, and Kostylev 2024)."}]}