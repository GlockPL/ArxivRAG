{"title": "Mamba-MOC: A Multicategory Remote Object Counting via State Space Model", "authors": ["Peng Liu", "Sen Lei", "Heng-Chao Li"], "abstract": "Abstract-Multicategory remote object counting is a funda- mental task in computer vision, aimed at accurately estimating the number of objects of various categories in remote images. Existing methods rely on CNNs and Transformers, but CNNs struggle to capture global dependencies, and Transformers are computationally expensive, which limits their effectiveness in remote applications. Recently, Mamba has emerged as a promis- ing solution in the field of computer vision, offering a linear complexity for modeling global dependencies. To this end, we propose Mamba-MOC, a mamba-based network designed for multi-category remote object counting, which represents the first application of Mamba to remote sensing object counting. Specifically, we propose a cross-scale interaction module to facilitate the deep integration of hierarchical features. Then we design a context state space model to capture both global and local contextual information and provide local neighborhood information during the scan process. Experimental results in large-scale realistic scenarios demonstrate that our proposed method achieves state-of-the-art performance compared with some mainstream counting algorithms. The code will be available at https://github.com/lp-094/Mamba-MOC.\nIndex Terms-Remote sensing, object counting, mamba, state- space model(SSM).", "sections": [{"title": "I. INTRODUCTION", "content": "Object counting in remote sensing scenes, which involves estimating the number of objects of a specific category in given images, has become increasingly significant in applications such as urban planning[1], agriculture monitoring[2], and ecological survey [3]. Compared to traditional counting tasks, this task presents greater challenges due to the broader spatial coverage and more complex scene content.\nConventional methods for remote sensing object counting predominantly rely on Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs). CNN-based approaches are widely utilized due to their translational invariance and linear computational complexity. However, the fixed-size local con- nections inherent in CNNs constrain their receptive fields, lim- iting the ability to effectively capture long-range dependencies and hindering the modeling of global context. In contrast, ViT- based approaches exploit long-range dependencies, which are important for visual tasks [4, 5] and dynamic weight allocation to enhance the modeling of visual information. However, the self-attention mechanism inherent in ViTs introduces quadratic complexity with respect to input size, which results in a significant increase in computational cost, especially when handling large-scale data.\nTo address the limitations, the Selective Structured State- Space Model (S6), or Mamba, incorporates selective scanning techniques and hardware-optimized designs, enabling effective modeling of long-range dependencies while maintaining linear complexity and dynamic weight allocation. This capability has garnered significant attention in visual tasks, with numerous studies exploring Mamba's potential in areas such as image classification[6] and facial expression recognition [7]. How- ever, its application in remote sensing counting tasks remains largely unexplored.\nBuilding on this, we propose Mamba-MOC, a Mamba- based framework specifically designed for remote sensing object counting, aimed at exploring the potential of Mamba in this domain. Specifically, we utilize the visual Mamba model(Vmamba) as the backbone to extract more discrimi- native features. We also design a Mamba-based cross-scale interaction module to enable effective interaction between multi-level features, thereby improving the representation of remote sensing objects across different scales. Furthermore, we propose a context state space model that mitigates the inherent limitations of causal scanning in the Mamba model when applied to 2D images by incorporating a local convo- lution operation. This operation also extracts local contextual information, which is subsequently integrated into the global context. Consequently, the model enables a more effective extraction of fine-grained information and the experimental results validate its effectiveness."}, {"title": "II. PRELIMINARIES", "content": "State Space Models (SSM) [8], inspired by continuous linear time-invariant (LTI) systems, map a one-dimensional input \\(x(t) \\in R^{L}\\) to the output \\(y(t) \\in R^{L}\\) through a hidden state \\(h(t) \\in R^{N}\\) with linear complexity, which are formally defined by the following mathematical formulation:\n\\(h'(t) = Ah(t) + Bx(t), y(t) = Ch(t)\\)\nwhere \\(A \\in R^{N\\times N}\\), \\(B \\in R^{N\\times 1}\\) and \\(C\\in R^{N\\times N}\\) are the projection matrix, N is the state size. To adapt this to discrete data such as images and text, it is necessary to discretize the aforementioned continuous system. Mamba adopt zero-order hold with a timescale parameter \u25b3 to transform the continuous parameters A and B from the continuous system into the discrete parameters A and B:\n\\(A = exp(\\triangle A), B = (\\triangle A)^{-1}(exp(\\triangle A) - I) \\cdot \\triangle B\\)"}, {"title": "III. METHODOLOGY", "content": "A. Overall Architecture\nAs illustrated in Fig. 1, our proposed method mainly comprises a vmamba[6] backbone, a cross-scale interaction module, two Context State Space (CSS) block. The vmamba backbone is employed as the encoder to extract multi-level feature representations \\(F^{1} \\in R^{\\frac{H}{4} \\times \\frac{W}{4} \\times C}\\), \\(F^{2} \\in R^{\\frac{H}{8} \\times \\frac{W}{8} \\times 2C}\\), \\(F^{3} \\in R^{\\frac{H}{16} \\times \\frac{W}{16} \\times 4C}\\). The cross-scale interaction module plays a critical role in effectively integrating coarse-grained and fine- grained features, thus augmenting the interaction of informa- tion across multiple scales. And the CSS block is equipped with a Contextual State Space Model (CSSM), which is specifically designed to capture and refine contextual informa- tion and focus on local neighborhood details during scanning process. Ultimately, the final feature is forwarded to a predictor to generate the density prediction.\nB. Cross-scale Interaction Module\nFor the input image \\(I \\in R^{H \\times W \\times 3}\\), leveraging the Vmamba backbone, multi-scale features from coarse to fine are extracted to effectively address the scale variation challenges in aerial image. However, a critical issue remains in exploiting the interactions between the coarse and fine stages. To address this challenge, we propose a Cross-Scale Interaction Module (CIM) to enhance the interplay between the coarse and fine stages.\nSpecifically, for the extracted features {\\(F^{1}\\), \\(F^{2}\\), \\(F^{3}\\)}, we first align these features by resizing \\(F^{1}\\) and \\(F^{2}\\) to match the spatial shape of \\(F^{3}\\), and subsequently concatenate them along the channel dimension, as illustrated below:\n\\(F_{i} = Avgpool(F_{i}), i \\in \\{1,2\\}\\)\n\\(F_{align} = Conv_{1\\times 1}(Concat(F_{1}, F_{2}, F^{3}))\\)\nwhere \\(F_{e}\\) represents the pooled features, \\(Conv_{1\\times 1}\\) is 1 \u00d7 1 convolution and \\(F_{align} \\in R^{\\frac{H}{16} \\times \\frac{W}{16} \\times 2C}\\) represents the features obtained by concatenating multi-stage features along the chan- nel dimension, followed by the linear projection.\nThen, the aligned features are fed into the VSS block, which is the core component of Vmamba, for self-fusion and mapping back to the original channel dimensions through 1\u00d71 convolution.\n\\(F_{fuse} = Conv_{1\\times 1}(Mamba(F_{align}))\\)\nAfter that, We split \\(F_{fuse}\\) into three parts along the original channel dimensions, each of which is upsampled to match the size of \\(F_{1}\\). The gating mechanism from [14] is then used"}, {"title": "C. Context State Space Model", "content": "Inspired by [15], the output matrix C in Equation 1 functions as a query in the attention mechanism. From this observation, we derive two key insights: (1) Through local 2D modeling, C effectively \"queries\" pixels within the local context, addressing the inherent limitations of causal scanning in the Mamba model when applied to 2D images. (2) By emphasizing local context, the model focuses on fine-grained details while simultaneously integrating global context through global scanning, which improves the extraction of target features.\nBased on this concept, we propose a Context State Space Model (CSSM) in a straightforward manner. As shown in Fig. 3, the proposed CSSM incorporates multi-scale local infor- mation, allowing the model to learn and capture correlations between local neighbourhoods within the image. This method implicitly mitigates the inherent limitations of the SSM, which compromise the preservation of spatial structure. At the same time, it integrates both local and global contextual information, improving the perception of counting targets. Specifically, given an input feature map \\(F_{c} \\in R^{H \\times W \\times C'}\\), we first capture multi-scale contextual features \\(F_{ms} \\in R^{H \\times W \\times D}\\) by applying multiple convolutional operations, from which P\u2208 \\(R^{HW\\times D}\\) is obtained through a flattening step. Simultaneously, a 1\u00d71 convolution is applied for linear projection to yield D\u0454 \\(R^{H \\times W \\times C}\\), as expressed in the following equation:\n\\(F_{ms} = CR_{3\\times 3,d=1}(F_{c}) + CR_{3\\times 3,d=2}(F_{c}),\\)\n\\(P = Flatten(F_{ms}),\\)\n\\(D = Conv_{1\\times 1}(F_{ms})\\)\nwhere \\(CR_{3\\times 3,d=1}\\) and \\(CR_{3\\times 3,d=2}\\) represent 3 \u00d7 3 convolution with a dilation rate of 1 and 2, respectively, followed by a relu activation function, and \\(Conv_{1\\times 1}\\) is 1 \u00d7 1 convolution.\nThen, we incorporate P into C throgh residual addition and incorporate D into y(t) to formulate the context state space model:\n\\(h'(t) = Ah(t) + Bx(t),\\)\n\\(y(t) = (C + P)h(t) + D\\)\nThrough the aforementioned design, We adopt CSSM as the core component and, following the structure outlined in [6], construct our CMamba, which is then used to build the CSS Block, as illustrated in Fig. 1. This allows the integration of global-local contextual information while paying attention to local neighborhood information during the scanning process."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "A. Related Settings\nDatasets: We have conducted comprehensive experiments on the recently released multi-object remote sensing counting dataset, NWPU-MOC[13]. This dataset is derived from 3,416 aerial and remote sensing scenes, containing a total of 383,195 annotated points across 14 categories. It consists of 3,416 images, where the training and testing sets partitioned accord- ing to the official distribution. In particular, 2,391 images are allocated to the training set, and the remaining 1,025 images are used for testing. In our experiments, following [13], we also roughly group the NWPU-MOC dataset into 6 categories.\nEvaluation Metrics: The performance of the proposed framework is assessed using four metrics: mean absolute error(MAE), root mean squared error(RMSE), intercategory average MSE(MSE) and weighted MSE(WMSE). MSE and WMSE are proposed by [13].\nImplementation Details: The ground truth density maps for the experiments are generated using a Gaussian kernel with a bandwidth of 4 and a size of 15. All experiments are performed within the PyTorch framework and conducted on an NVIDIA RTX 4090 GPU. During training, the input resolution is set to 512 \u00d7 512, and the network is optimized using AdamW optimizer with a learning rate of 5e-5, weight decay of le-4, and a batch size of 8, with a total of 200 epochs.\nB. Comparison with state-of-the-arts\nTo provide a comprehensive benchmark evaluation, we com- pare our method with several state-of-the-art counting methods on the NWPU-MOC dataset."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce Mamba-MOC, a Mamba-based method for remote sensing multi-object counting. Initially, we leverage the advantages of Mamba's global modeling capabilities to design a cross-scale module that effectively integrates multi-scale and multi-granularity features within the FPN structure. Additionally, we propose a contextual state space model that overcomes the limitation of Mamba's causal scanning process, which struggles to capture local neighborhood context. At the same time, this model integrates both local and global contexts, enhancing the network's ability to interpret counting targets more effectively. Experimental results demonstrate the promising performance of our method in remote sensing multi-object counting tasks and validate the effectiveness of the Mamba framework in this domain."}]}