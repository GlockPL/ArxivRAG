{"title": "Hate Speech Detection Using Cross-Platform Social Media Data In English and German Language", "authors": ["Gautam Kishore Shahi", "Tim A. Majchrzak"], "abstract": "Hate speech has grown into a pervasive phenomenon, intensifying during times of crisis, elections, and social unrest. Multiple approaches have been developed to detect hate speech using artificial intelligence, but a generalized model is yet unaccomplished. The challenge for hate speech detection as text classification is the cost of obtaining high-quality training data. This study focuses on detecting bilingual hate speech in YouTube comments and measuring the impact of using additional data from other platforms in the performance of the classification model. We examine the value of additional training datasets from cross-platforms for improving the performance of classification models. We also included factors such as content similarity, definition similarity, and common hate words to measure the impact of datasets on performance. Our findings show that adding more similar datasets based on content similarity, hate words, and definitions improves the performance of classification models. The best performance was obtained by combining datasets from YouTube comments, Twitter, and Gab with an F1-score of 0.74 and 0.68 for English and German YouTube comments.", "sections": [{"title": "1 INTRODUCTION", "content": "Hate speech on social media has become a widespread problem on the Internet (Jahan and Oussalah, 2023). With easy access to social media platforms, such as Twitter (now X), YouTube, or Gab, the amount of hate speech has been increasing (Shahi and Kana Tsoplefack, 2022) for decades. The topic of hate speech is linked to global developments and recent crises, such as hate speech on the Russia-Ukraine conflict (Di F\u00e1tima et al., 2023), COVID-19 (Shahi and Kana Tsoplefack, 2022), and ongoing elections in different countries of the world. Different social media platforms have their own data formats and guidelines, allowing users to post content in various media formats, and languages. For example, on YouTube, users can post hate speech as videos or comments.\nHate speech detection is mainly studied in English and for specific platforms such as Twitter (Siegel, 2020), Facebook (Del Vigna et al., 2017), and YouTube (D\u00f6ring and Mohseni, 2020). A hate speech classification model needs fine-grained annotated training data. Gathering high-quality training data is expensive and time-consuming (Shahi and Majchrzak, 2022) for training machine learning models. Previous research shared hate speech datasets in different languages (Poletto et al., 2021; Al-Hassan and Al-Dossari, 2019; Fortuna and Nunes, 2018). However, such data sets vary considerably by social media platform, annotation goal (such as offensive language, abusive language, hate speech), period of data collection, and other dimensions (Al-Hassan and Al-Dossari, 2019). There are notable differences in the linguistic style of comments posted on different platforms (such as adherence to standard spelling and grammar and usage of emojis). Hence, prior research and datasets are dissimilar and not readily generalizable, especially across different languages.\nWith the advancement of Generative Artificial Intelligence, mainly the latest generative Artificial Intelligence (GAI) models for ChatGPT (Wullach et al., 2020), such as data annotation data have been tested for hate speech. However, there is a need for ground truth to verify the annotation quality; annotation quality depends on the given definition, ethical implications, and local law (Li et al., 2023). Hence, quality annotated data is still required for hate speech detection."}, {"title": "2 RELATED WORK AND BACKGROUND", "content": "Machine learning is the dominant approach to text classification in various domains such as political sentiment analysis (R\u00f6chert et al., 2020), detecting incivility and impoliteness in online discussions (Stoll et al., 2020) as well as the classification of political tweets (Charalampakis et al., 2016). Especially the state-of-the-art technique BERT (Deep Bidirectional Transformers for Language Understanding) (Malmasi and Zampieri, 2017) has used for the detection of hate speech (Salminen et al., 2020; Aggarwal et al., 2019; Liu et al., 2019; Zampieri et al., 2019). The text classifier aims to train a robust classifier to recognize hate comments globally, i.e., on different platforms and languages. For the identification and training of the models, several studies classify hate speech using deep neural network architectures or standard machine learning algorithms. Wei et al. compare machine learning models on different public datasets (Wei et al., 2017). The results indicate that their approach of a convolution neural network outperforms the previous state-of-the-art models in most cases (Wei et al., 2017). A recent study used transfer learning (Yuan et al., 2023) to train 37,520 English tweets, showing a trend towards more complex models and better results. Prior studies have analyzed hate speech on YouTube; one study highlights the hate speech on YouTube on Syrian refugees (Aslan, 2017). In another study, hate speech on gender is studied on a small dataset quantitatively (D\u00f6ring and Mohseni, 2019).\nPrevious studies have mainly focused on detecting hate speech in different languages (Ousidhoum et al., 2019) and on comparing different social media platforms (Salminen et al., 2020). Considering the past studies about hate speech, it is noticeable that many studies only concentrate on one platform or specific language, such as English (Waseem, 2016), German (Ross et al., 2016), Spanish (Ben-David and Matamoros-Fern\u00e1ndez, 2016) and Italian (Del Vigna et al., 2017) to train a machine learning model to automatically classify and predict which unseen texts can be considered hate speech. Ousidhoum et al. applied a multilingual and multitasking approach to"}, {"title": "3 RESEARCH METHOD", "content": "This section explains the approach used for building the classification model in the study, i.e., data collection, data annotation, similarity measurement, the classification approach, and the evaluation strategy.\n3.1 Data Collection\nFirst, we collected datasets in two steps: 1) Collecting YouTube comments in English and German. 2) Compiling publicly available data published in prior research on Twitter, Wikipedia, and Gab.\nYouTube is a video-sharing platform where videos from across the world are uploaded. We used a self-developed Python crawler using pytube\u00b9 (a Python library) and searched keywords on YouTube to gather video and their comments (R\u00f6chert et al., 2021). To maintain the diversity in the dataset, we chose different controversial topics; such as politics, LGBT, immigration, abortion, war, sex education, entertainment, and sports, the probability of having hate comment is more on videos on these topics. A total of 49,074 comments were extracted from 101 English YouTube videos and 89 German videos in multiple languages. Furthermore, we identified the language of comments and filtered German and English comments using fastText (Joulin et al., 2016). Overall, we got 30,663 and 18,441 English and German comments, respectively, which are further filtered for data annotation, further explained in section 3.2.\nFor external datasets, the criteria were to get publicly accessible datasets and use binary class for hate speech detection. We considered ten datasets from three different platforms (Twitter, Gab, Wikipedia) and provided a three-letter ID to each of them; the first letter indicates the language, the second the platform, and the third the number. For example, ET1 is the 1st English Twitter dataset. Overall, We used ten datasets; eight datasets were collected from external sources, and two datasets (EY1 and GY1) were collected and annotated by us. \n3.2 Data Annotation\nEach social media platform, community, organization, and government defines hate speech differently (Fortuna and Nunes, 2018). Before developing our codebook, several definitions of hate speech were studied, such as given by the EU code of conduct (Wigand and Voin, 2017), ILGA (Europe, 2014), Nobata (Nobata et al., 2016), Facebook (Facebook, 2024), YouTube (YouTube, 2024) and Twitter (X.com, 2024).\nMultiple definitions of hate speech have been proposed. Nockleby defines hate speech as any communication that disparages a person or a group based on some characteristics such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (Nockleby, 2000). Fortuna & Nunes discuss the definition of hate speech, and the\n3.3 Similarity Measurement\nWe collected eight different datasets for evaluating cross-platform bilingual classification models. Each dataset was collected differently on various controversial topics, so we computed the similarity among datasets based on definition, hate words, and content as described below. The similarity measures are decided based on the factors affecting the dataset, such"}, {"title": "3.4 Classification Model", "content": "We followed the traditional natural language processing (NLP) data cleaning technique method using the Natural Language Toolkit (NLTK) library (Loper and Bird, 2002). This includes removing short words (i.e., less than three characters), emails, and hyperlinks.\nWe used different machine models for the classification model, such as the Support Vector Machine (SVM), Long short-term memory (LSTM), Logistic Regression, and the state-of-the-art Bidirectional encoder representations from transformers (BERT) model. First, we used the annotated YouTube comments and measured performance. Later, we added the dataset in different combinations (from same and cross-platform) to the YouTube comments and measured the performance of the classification model.\nWe evaluated the performance of the classification model generated model in terms of precision, recall, and F1. We provided the model evaluation for both positive and negative classes."}, {"title": "4 EXPERIMENT AND RESULTS", "content": "We computed the similarity of datasets based on definition, content, and hate words. All three measures indicate the similarity of the different datasets concerning YouTube comments. The goal is to find the relationship between these similarities and classification performance. The result obtained from similarity measures is shown in Tables 2, 3, and 4 for English, as well as 5 for German.\nDefinition similarity We conducted an online survey with 100 participants on Prolific. The participants were from all over the globe, with an average age of 26 years: 60% male and 40% female, 66% working professionals, and the remaining students. Out of eight external datasets, only six defined hate speech explicitly. A combination of two definitions from six datasets, 15 combinations of definitions of similarity, was provided to participants, and they were asked to vote for similarity on a 10-point scale. Average votes from 90 participants (10 participants were excluded because their response time was too fast, indicating they voted without really pondering about it) were calculated to measure the similarity for each similarity. Finally, we normalized the average value using the formula (n-1)/9, where n is the average of the similarity score to show the similarity of the definition used in the data annotation. The result shows that the definition used in English YouTube comments is similar to the definition of ET4, followed by ET5.\nFor Hate word similarity, we filtered the hate words mentioned in our datasets using Hurtlex (Bassignana et al., 2018). We also screened the hate words mentioned in other datasets. We computed common hate words using text matching and represented the hate word similarity in percent. Based on the hate word similarity, most datasets share similar hate word datasets, and English Wikipedia contains words that are more similar to EY1. However, EG1 contains the maximum number of similar hate words in English, and GT2 is more similar to all datasets. So, for English, Gab has more hateful content, which is not true for German."}, {"title": "5 DISCUSSION", "content": "Collecting the data set from the social media platform and annotating the data is time-consuming. To overcome the problem, we collected and annotated YouTube comments built a classification model, and further used the existing dataset for training. To measure the role of the additional datasets in the performance of the classifier, we looked at literature works for finding datasets. However, only a few research share datasets as open-access Even when it is partially available (for instance, only the tweet IDs in Twitter (X) data sets), the user needs to write a program to collect the data or have access to an existing one and know how to use it. With time, many social media posts are deleted by either social media platforms or users (cf. e.g. (Waseem, 2016)). We could crawl only 10,498 out of 16,914 labeled entries. Due to platform restrictions from YouTube, prior work did not share users' comments publicly which makes it difficult to explore the performance of the classifier by adding users' comments from other hate speech. A publicly accessible dataset will ease the workload to detect hate speech on social media.\nDuring content similarity, we observed that a dataset that contains hate speech on general topics such as sexism, racism, targeted populations, vulgarity, and framing helps to improve the performance of the classification model. We found that Class imbalance is a problem for hate speech detection; so far, all data sets are imbalanced. We observed that under-sampling data with an equal distribution of classes gives better results.\nWe have evaluated the role of additional datasets for the performance of the classifier on different criteria. For English datasets, based on the similarity measure, Definition Similarity often improves the classifier performance. For example, ET3 is very similar to EY1, but EG1 is dissimilar, and it still improves classification performance as a precision of hate class. There is no direct relation in the performance model that a dataset with a similar definition gives the highest precision. Content Similarity is directly related to enhancing the performance of the classification model; ET4 from the same platform improves the classification model, while the EG1 from cross-platform has the maximum similarity with EY1, which enhances the overall performance of the mode. So, there is a direct relation; the similar content improves the performance. Hate word Similarity also does not help to conclude that EW1 does not improve the performance but hate word similarity with content similarity, and the performance increases such as EG1 increases the performance.\nHowever, for German, based on the similarity in content, the performance of the classifier improved by adding GT1. The same trend follows for the hate word similarity and definition similarity. However, if the dataset is similar in terms of similarity measures, additional data improves the performance of the classification model. Overall, data annotation is a key challenge, but combining datasets having a similar issue with similar content and hate words improves the classification performance. A heuristic combination of a balanced dataset could improve precision, recall, and f1-score. There is no direct relation in increasing the dataset from the same platform, but cross-platform datasets improve performance such as ET4 and EG1 jointly improve recall and F1-score. In this study, the classification model was based on the traditional machine learning approach, however, the performance might change by applying a Large Language model such as ChatGPT. LLMs, pre-trained on vast amounts of data from the web which might capture deeper contextual and semantic interpretation of the text. By fine-tuning an LLM on the specific task, the classification model could better handle complexities which could potentially lead to better performance compared to traditional methods.\nImprovements to hate speech detection will ultimately have organizational and social consequences. If detecting hate speech is easier, there will be less room for social media platforms to not remove hateful content. The consequences this will have on social media users those affected by hate speech, those being \"bystanders\", and those posting hate speech \u2013 will need to be targeted by further research. The consequences of potentially many posts being deleted are not clear, either. However, neither removal nor detection changes much of the roots of hate speech surfacing on social media. While the work we present here is mainly technological in nature, countering hate speech is a truly interdisciplinary approach and will require cross-disciplinary efforts."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this study, we presented hate speech detection on YouTube comments and explored the classification model by adding external data from the same platform or cross-platform in English and German. We provided datasets with 1,892 and 6,060 English and German human-annotated. We tested the combination of different data sets and measured performance in terms of precision, recall, and F1 score. Along with the performance metric, we have used similarity measures to conclude the role of the external datasets in enhancing classifier performance.\nTo enhance the model performance, we used external datasets from different platforms. We computed the similarity of datasets based on definition, content, and hate words and explored their importance for the classification models. Overall, we succeeded in implementing the performance of the model by adding an existing hate speech dataset and different patterns for both languages.\nThe practical application of this approach is to create a generalized classification model for hate speech detection over bilingual cross-platform. Our approach can be tested with a heuristic dataset combination in future work. One of the further extensions of our work is to extend to use of additional datasets in different languages, such as Spanish and Italian. Another extension could be multi-class hate speech classification with classes such as hate speech, offensive, or profane."}]}