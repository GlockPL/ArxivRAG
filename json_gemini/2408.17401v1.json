{"title": "Exploring the Effect of Explanation Content and Format on User Comprehension and Trust", "authors": ["Antonio Rago", "Bence Palfi", "Purin Sukpanichnant", "Kavyesh Vivek", "Hannibal Nabli", "Olga Kostopoulou", "James Kinross", "Francesca Toni"], "abstract": "In recent years, various methods have been introduced for explaining the outputs of \u201cblack-box\u201d Al models. However, it is not well understood whether users actually comprehend and trust these explanations. In this paper, we focus on explanations for a regression tool for assessing cancer risk and examine the effect of the explanations' content and format on the user-centric metrics of comprehension and trust. Regarding content, we experiment with two explanation methods: the popular SHAP, based on game-theoretic notions and thus potentially complex for everyday users to comprehend, and occlusion-1, based on feature occlusion which may be more comprehensible. Regarding format, we present SHAP explanations as charts (SC), as is conventional, and occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature also lends itself. The experiments amount to user studies questioning participants, with two different levels of expertise (the general population and those with some medical training), on their subjective and objective comprehension of and trust in explanations for the outputs of the regression tool. In both studies we found a clear preference in terms of subjective comprehension and trust for occlusion-1 over SHAP explanations in general, when comparing based on content. However, direct comparisons of explanations when controlling for format only revealed evidence for OT over SC explanations in most cases, suggesting that the dominance of occlusion-1 over SHAP explanations may be driven by a preference for text over charts as explanations. Finally, we found no evidence of a difference between the explanation types in terms of objective comprehension. Thus overall, the choice of the content and format of explanations needs careful attention, since in some contexts an explanation's format, rather than its content, may play the critical role in improving user experience.", "sections": [{"title": "1 INTRODUCTION", "content": "Explainable AI (XAI) has risen to prominence in recent years as a means of giving users the explanations they need to be able to trust the predictions of \"black-box\" AI models. This trustworthiness of Al models is especially pertinent in high risk settings such as medicine, when patients' lives may depend on these predictions. The ways in which explainability may facilitate trust in users has been studied extensively [17, 25, 40, 45], with the general consensus leaning towards it playing an important role in this critical bond between humans and machines. However, despite this growing consensus, what remains to be seen is which forms of XAI, and which requisite characteristics, are best suited to facilitating trust in users in each individual application of AI models. One of the most prominent types of XAI method is feature attribution (FA, see [9] for an overview). FA explanations assign each feature in a given input a positive or negative attribution value denoting its importance towards the AI model's output. However, blindly applying one FA method or another in a particular situation is not a scientifically rigorous approach to accounting for the human factors involved in enabling trustworthy AI."}, {"title": "2 RELATED WORK", "content": "Of the multitude of works undertaking user studies comparing different types of XAI, one which most closely aligns with ours is that of Bertrand et al. [7]. They deliver FA explanations in different formats to examine textual explanations vs. graphical explanations. They find that the former increases trust relative to the latter, but they posit that this could be due to the explanations' simplicity. Somewhat in contradiction with other work [42], they found that explanations did not improve users' understanding of models' recommendations.\nOther works made similar conclusions regarding the format of explanations, e.g. Szymanski et al. [46] assess visual and textual explanations for the predictions of AI models in user comprehensibility, controlling for the expertise of users, as we do in our studies. The authors found that lay users preferred the visual explanations but could easily misinterpret them compared to textual explanations.\nAnother closely related study [44] evaluates the effect of textual, dialogical and interactive explanations on user understanding. They found that the textual explanations presented greater limits to understanding for and lower preferences from users."}, {"title": "3 METHODOLOGY", "content": "In this section, we detail our methodology for the user studies. We first cover the collection of the vignettes presented to participants with the predictions made by the QCancer algorithm (\u00a73.1), before defining explanations for the predictions (\u00a73.2). We then detail the experimental design and procedure (\u00a73.3), explain how we recruited participants from the two different populations (\u00a73.4) and outline our method for analysing the data (\u00a73.5)."}, {"title": "3.1 Vignettes", "content": "We adapted twenty fictitious vignettes (including one practice vignette) from [39], describing hypothetical patients presenting at the primary care (i.e. their GP) with symptoms and risk factors suggestive of gastro-oesophageal cancer. Note that we chose to focus on one type of cancer here for simplicity, though the QCancer algorithm and the original vignettes concern multiple types of cancer. Each vignette begins with a table including demographics and some risk factors of the patient, and continues with a textual description of a patient's symptoms, other risk factors and the results of some relevant examinations. All of the presented factors were predictive of gastro-oesophageal cancer according to the QCancer algorithm. Then, we provided the estimate of the patient's risk of gastro-oesophageal cancer calculated by the QCancer algorithm followed by an explanation of this prediction and information on how the explanation should be interpreted. An example of one such vignette, with the output of the QCancer algorithm shown for gastro-oesophageal cancer is given in the following (we will give examples of the explanations in \u00a73.2).\n Patient name: Joyce Acorn (female), Age: 65, BMI: 30.64 (Height: 177cm, Weight: 96kg), Smoking: Smoker 20 cigarettes per day, Past medical history: Nil, Family history: Nil.\nJoyce comes to see you about the indigestion she's been experiencing over the past month. She also mentions that it has become increasingly difficult to swallow during meals. She enjoys food usually but has noticed that she is now off her food. She denies any post-menopausal bleeding. When she saw the practice nurse last week for her flu vaccine, she had also taken some routine blood tests. There are no other symptoms and examination findings are normal.\nThe cancer prediction algorithm estimates the risk of gastro-oesophageal cancer to be 31.90%.\nThe variables used in the QCancer algorithm's prediction of gastro-oesophageal cancer and the values which Example 1 results in are shown in Table 1 (top and bottom rows, respectively - the table also shows, in the middle, the baseline patient values for males and females, which will be discussed in \u00a73.2)."}, {"title": "3.2 Explanations for the QCancer Algorithm", "content": "In this section, we define the explanations which we deliver to users. To define these explanations we assume a simple, single-label classification problem comprising a classifier $f : D_1 \\times ... \\times D_k \\rightarrow [0, 1]$, where $D_1,..., D_k$ represent the domains for each of the k features (k=15 in our setting, see Table 1). Then, for any input $x \\in D_1 \\times ... \\times D_k$, $f(x) \\in [0, 1]$ is the predicted probability of the class given the current values of the input features. This represents the QCancer algorithm, where the features, e.g. age, are inputs to the algorithm and the output is the predicted risk of gastro-oesophageal cancer. We refer to the ith value of any input x as $x_i$, where $i \\in \\{1, ..., k\\}$. In order to define the FA explanations, we first introduce a simplified version of any given input x as x', where there exists a mapping function $h_x$ such that $x=h_x(x')$, i.e. returning the original input given the simplified version. We then let, as in [33], $x' \\in \\{0, 1\\}^M$, where M is the number of simplified input features (note that M=k throughout this paper). Then, if x=1, then $x=h_x(x')$ sets $x_i$ to the current value of the feature, while if x=0, then $x=h_x(x')$ sets $x_i$ to the feature's baseline value, i.e. a pre-defined default value. We set the baseline values (referred to as baseline patients in the explanations) to one of two sets of values representing healthy males and females depending on the patient's gender, as indicated in Table 1, which were used for the studies.\nOne of the most prominent of all FA methods is SHAP [33], where each feature of an input to a model is assigned an importance value computed from a game-theoretical approximation of its contribution.\nGiven a classifier f and an input x, the SHAP explanation for $x_i$ wrt f is:\n$e_s(f, x, i) = \\sum_{z'\\subset x'}\\frac{|z'|!(M - |z'| - 1)!}{M!} (f(h_x(z')) - f(h_x(z'_i)))$\nwhere $z' \\subset x'$ represents that z' is a vector whose non-zero entries are a subset of the non-zero entries in x', |z'| is the number of non-zero entries in z' and $z'_{-i}$ denotes z' after setting $z_i = 0$.\nWe then selected the occlusion-1 method [3] for its simplicity in hopes of achieving more comprehensibility and trust in users. This method takes the set of baseline feature values (as in the SHAP"}, {"title": "3.3 Experimental Design & Procedure", "content": "We assessed the RQs in two different samples: a sample from the general population where a limited number of participants had medical experience (Study 1) and an expert sample where participants had some medical training and experience (Study 2). We created two surveys using Qualtrics\u00b3 that followed the same structure, as follows.\n\u2022 Pre-Test (6 or 8 questions) \u2013 The participants were shown information about the study and were asked to provide consent. They were then asked questions regarding their background and experience of AI and cancer prediction algorithms (additional demographics questions were included for Study 2), including their trust in AI models in healthcare, before the task was explained to them with an illustrative example.\n\u2022 Main Test (4 x 6 questions) - In the main task, we provided the participants with four vignette sets, each containing a"}, {"title": "3.4 Participant Recruitment and Ethics Statement", "content": "For Study 1, we aimed to recruit 100 participants from the general population, who were over 18 and spoke English, via the online research platform Prolific\u2074. Upon completion, the participants were paid an average of $4.50 for their time based on the platform's standard reimbursement rates of $21.20 per hour. Data collection was anonymous during Study 1. In Study 2, we aimed to recruit at least 50 students in medicine from UK-based universities. The potential participants were approached via student mailing lists, and were invited via a link to the survey in an email, which also contained the participant information sheet with details about the study. Note that all students were 18 years of age and English-speaking since they were attending university in the UK. At the end of Study 2, participants were asked to provide their university email address, to which we sent a \u00a310 voucher for the online retailer Amazon to reimburse them for their time. All the participant responses were pseudononymised once this payment was made (i.e. we deleted their e-mail addresses from the dataset). Participation in both studies was voluntary, participants were asked to provide informed consent after reading the participant information sheet and they could withdraw at any time prior to completing the survey, simply by closing the questionnaire window, with their data deleted permanently. We informed the participants that once they had completed the survey, they could no longer withdraw and we would keep their"}, {"title": "3.5 Data Analysis", "content": "Creation of the Main Variables of Interest. We created categorical variables with two levels from the responses to the most important feature and the definition recognition tasks, where each response was categorised as either correct or incorrect. When more than one feature was named as the most important, we categorised the response as correct as long as the first mentioned feature was the one with the strongest absolute contribution. Moreover, for the vignettes where the feature with the highest absolute contribution was a protective factor (i.e. the reduced risk of cancer), we accepted responses as correct where the strongest negative contributing feature was named first. We used the subjective understanding of the explanations and the trustworthiness of the explanations variables as interval variables.\nStatistical Analyses. We ran the analyses in R (version 4.3.1). We built generalised linear and linear mixed-effects regression models with the Ime4 R package [6] to analyse our categorical and interval variables, respectively. For the generalised linear regression models, we could only include random intercepts by participants (the models did not converge with more random intercepts), and, for the linear regression models, we included random intercepts by participants and vignettes. For the generalised linear and the linear regression models we reported Odds Ratios (ORs) and raw regression slopes (bs), respectively. We also reported 95% CIs for each regression coefficient. To test our hypotheses, we used the conventional p-value threshold of .05."}, {"title": "4 RESULTS", "content": "Study Samples. Study 1 was completed by 99 participants, from which 1 reported to have colour-blindness (we did not measure demographics). Study 2 was completed by 69 medical students7, from which 3 reported to have colour-blindness. The mean age of the participants in Study 2 was 22.3 (SD = 3.3), with 35 females (32 males, 1 other and 1 preferred not to disclose). As expected, participants in the medical student sample (Study 2) reported to have heard more about medical algorithms than participants in the Prolific sample (Study 1), but the number of people who had tried a medical AI tool before was negligible in both samples. Interestingly, trust towards algorithms in general was comparable in the two samples, and, on average, participants in both samples had neutral attitudes (roughly 5 on a scale from 1 to 9, where 5 was neutral). Table 2 presents the descriptive statistics broken down by samples. Since our sample sizes were defined by resource constraints, we conducted sensitivity power analyses as per the recommended good practice [30]. The multilevel regressions directly comparing the SC with either the OC or the OT explanations (RQ2 and RQ3) were"}, {"title": "5 DISCUSSION AND LIMITATIONS", "content": "Across two studies with different samples (general population and medical students), we found a reliable preference for occlusion over Shapley value explanations. Our participants reported that they understood and trusted the occlusion explanations more than the SHAP explanations, as we expected. However, the comparison of all the explanations revealed that this effect may be solely driven by the fact that our participants found the OT explanations consistently more comprehensible than the SC explanations. Moreover, in Study 1, the OT explanations were also preferred to the SC explanations, though in Study 2 the results were less clear as there was no evidence that the former explanations were preferred over the latter. These findings are consistent with a notion of people preferring explanations in text over chart format [7], for instance, due to finding some graphs difficult to interpret [16, 18, 46]. This idea somewhat undermines the potential message that, in terms of content, FA explanations based on simple rather than complex models should be prioritised. Interestingly, the results regarding trustworthiness were slightly in conflict with this idea, since in our medical student sample we only found evidence for higher trust in the OC compared to the SC explanations. While these findings show some support for the role of explanation content, more research is needed to explore how people relate to and utilise different XAI techniques, and how and when an explanation's format (e.g. text vs. chart) trumps the impact of its content. Future research should also explore the potential of tailoring explanation content and format to individual needs [8], such as their expertise in a field [48] or AI-systems [45] and their skills to interpret numerical information (e.g. graph literacy [16, 18] or numeracy [19, 43]).\nOur novel performance measure (i.e. the definition recognition task) could also not settle the question regarding the impact of content as we found no evidence that the participants were better at recognising the definition of either explanation. Nonetheless, the lack of positive findings could be related to how we measured performance. That is, the task of identifying the correct definition of explanations may have been too difficult as performance was only reliably over chance level for the occlusion explanations in the medical student sample potentially creating a floor effect for our measure. Future work should operationalise performance in other formats also.\nOur studies carry some limitations. Both studies lacked a SHAP explanation in text format and so a complete design, making it"}, {"title": "SUPPLEMENTARY MATERIAL", "content": "Following are three examples of other vignettes used in the experiments.\nPatient name: Rish Barrett (male), Age: 60, Current BMI: 37.55 (Height 175cm, Weight 115kg), Smoking: Smoker 5 cigarettes/day, Past medical history: Type 2 Diabetes Mellitus, Family history: Nil relevant.\nMr Barrett comes to see you for his results. He saw another GP at the surgery two weeks ago about the fact that after a long time he had lost a lot of weight within a short period of time (2 months) and could not understand why. Your colleague had organised some investigations. During your consultation, he reports that he has ongoing weight loss and has also been experiencing some abdominal pain over the past month. There are no other symptoms and examination findings are normal.\nThe cancer prediction algorithm estimates the risk of gastro-oesophageal cancer to be 2.53%.\nPatient name: Natalie Clark (female), Age: 74, Current BMI: 34.60 (Height 170cm, Weight 100kg), Smoking: Ex-smoker, Past medical history: Nil, Family history: Nil relevant.\nNatalie sees you today accompanied by her husband. Over the past 6 weeks, she has been experiencing heartburn and has been off her food. She has vomited on a few occasions and seen a bit of blood in the vomit. She also finds it difficult to swallow when eating. She has no other symptoms and examination findings are normal.\nThe cancer prediction algorithm estimates the risk of gastro-oesophageal cancer to be 40.24%.\nPatient name: Marianne Foster (female), Age: 60, Current BMI: 27.85 (Height 163cm, Weight 74kg), Smoking: Never smoked, Past medical history: Nil, Family history: Nil relevant.\nMary comes back to see you for a follow-up consultation. Another GP at the surgery had organised some blood tests and a FIT test 4 weeks ago. On the electronic health record, you can see that during the last consultation she had complained of having lost her appetite as well as some weight (about 3kg over the previous 6 weeks) despite no changes in her diet or lifestyle. She reports that she is still off her food and has lost more weight. She has also noticed that it is becoming increasingly difficult for her to swallow when she tries to eat. The blood tests and FIT test which were organised came back normal. She denies any post-menopausal bleeding, has no other symptoms and examination findings are normal.\nThe cancer prediction algorithm estimates the risk of gastro-oesophageal cancer to be 8.27%."}]}