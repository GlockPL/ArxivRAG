{"title": "Can Community Notes Replace Professional Fact-Checkers?", "authors": ["Nadav Borenstein", "Greta Warren", "Desmond Elliott", "Isabelle Augenstein"], "abstract": "Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to refer-ence fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily re-lies on professional fact-checking.", "sections": [{"title": "1 Introduction", "content": "The proliferation of misinformation on social media (Arnold, 2020; Diakopoulos, 2020), along with the rise of generative AI (Augenstein et al., 2024) have led to increasing concerns about its current and future potential harms, (e.g., to health (Clemente et al., 2022)) and threats to democracy and political stability (Reglitz, 2022).\nFact-checkers play a crucial role in combatting misinformation (Graves, 2017), and in recent years, have partnered with social media platforms, e.g., Meta, YouTube, and TikTok, to tackle its spread on these platforms. However, due to the scale of misleading content shared online, community mod-eration (e.g., options to flag potential misinforma-tion, group/server moderators) is often employed in parallel (Morrow et al., 2022), as a complemen-tary approach (e.g., (Google, 2025); see also the practice of snoping (Pilarski et al., 2024)).\nThe expansion of fact-checking projects in the last decade (Lauer and Graves, 2024), alongside their broader initiatives to curb misinformation (e.g., citizen media literacy programmes (Juneja and Mitra, 2022)) have been aided by partnerships with social media platforms such as Meta and Google (Graves and Anderson, 2020), which fund independent fact-checking agencies to fact-check potentially false claims on their platform.\u00b9\nHowever, political pressure and accusations of bias and censorship, and most recently, Meta's an-nouncement of its plans to end its partnerships with fact-checkers in the U.S. and implement a commu-nity moderation model (Meta, 2025), threatens the financial stability of fact-checking organisations, and hence, their ability to keep up with the increas-ing volume and sophistication of misinformation spread (Stencel et al., 2024; IFCN, 2024).\nMeta's recent policy shift also implies that these two strategies (fact-checking and community notes) are independent and in opposition, rather than two\nFact-checkers provide a judgment of claim veracity and exert no influence on the platforms' content moderation poli-cies (Catalanello and Sanders, 2025)."}, {"title": "2 Background", "content": "complementary strategies of tackling online mis-information. In this paper, we examine Twitter/X community notes as a case study to understand how fact-checking is used in community notes. Specifi-cally, we investigate the following two questions:\n(RQ1) To what extent do community notes rely on the work of professional fact-checkers? and\n(RQ2) What are the traits of posts and notes that rely on fact-checking sources? Studying the relationship between fact-checking and community notes is vital for understanding the shared role of expert and community-driven fact-checking in the global information ecosystem.\nWe find that at least 1 in 20 community notes rely explicitly on the work of professional fact-checkers, while this reliance is higher still for high-stakes top-ics such as health and politics. Our experiments also show that fact-checking is vital for debunk-ing misleading content linked to broader narratives or conspiracy theories. These findings imply that high-quality community notes cannot be produced independently of professional fact-checking. They further suggest that the pressure on fact-checkers exerted by platforms and politicians by defunding and discrediting fact-checking organisations will have corrosive effects on the quality of notes and destructive implications for information integrity more widely."}, {"title": "2.1 Community notes", "content": "Community moderation has been proposed as a means of addressing the scalability (Martel et al., 2024) and cross-partisanship trust (Flamini, 2019) challenges associated with fact-checking. Twit-ter/X's Community Notes programme (piloted in 2021 and publicly launched in October 2022 (Twit-ter/X, 2021)) is a notable example of such a system. Any platform user may volunteer as a Community Notes contributor, although they must achieve a particular 'rating impact score' before they can write notes (Twitter/X, 2024b). Notes that achieve a 'helpful' rating appear underneath the post, ex-plaining why the post is misleading (see Fig. 1). To be rated 'helpful', a note must receive similar levels of helpfulness rating from users with diverse viewpoints (Twitter/X, 2024a)."}, {"title": "2.1.1 Characteristics of Community Notes", "content": "A small but growing body of work has analysed Twitter/X's Community Notes dataset, focusing on the targets, sources, and limitations of notes.\nTargets of notes. Community notes tend to focus on misleading posts from large accounts (Pilarski et al., 2024), focusing on posts that lack impor-tant content or present unverified claims as facts (Pr\u00f6llochs, 2022; Drolsbach and Pr\u00f6llochs, 2023).\nSources in notes. Analyses have indicated that notes were rated more helpful if they link to 'trust-worthy' sources (Pr\u00f6llochs, 2022) and that the ma-jority of sources cited by notes were \u2018trustworthy' left-leaning news outlets. A recent study finds that 55% of URLs used in notes were related to news websites, 18% to research, 9% to social media, 9% to encyclopedic sources, but just 1.2% to fact-checking sources (Kangur et al., 2024).\nLimitations of notes. Only 11% of submitted notes reach 'helpful' status (i.e., shown to users) by achieving a cross-perspective (Renault et al., 2024; Wirtschafter and Majumder, 2023), and the time frame for notes to reach the algorithm's required agreement level (15.5 hours on average) limits its capacity to halt misinformation spread (Renault et al., 2024). Additional concerns about the notes' efficacy highlight their indifference to the expertise needed for certain claims and reliance on subjective helpfulness rather than objective facts, free labour and inadequate support and guardrails regarding explicit content (Gilbert, 2025).\nOur work provides novel insights into the tar-gets, sources and limitations of community notes by shedding light on the relationship between notes and professional fact-checking. Namely, we study the extent to which fact-checking sources form the basis of note-writers' efforts to counter misinfor-mation and identify the strategies they employ."}, {"title": "2.1.2 Impact of Community Notes on misinformation spread", "content": "Posts identified by community notes as mislead-ing have been found to attain less virality (reposts, quote tweets and replies) than non-misleading posts (Drolsbach and Pr\u00f6llochs, 2023; Renault et al., 2024). Community notes have also been shown to increase the probability of tweet retractions and deletions and speed up the retraction process (Gao et al., 2024; Renault et al., 2024). However, other studies have found less positive evidence; for ex-ample, that users' followers, likes and engagement increase after their post receives a community note (Wirtschafter and Majumder, 2023). Curiously, one study claims that showing community notes on posts reduced the spread of misleading posts"}, {"title": "2.2 Professional fact-checking and community note practices", "content": "by an average of 61% (Chuai et al., 2024a), while a more recent analysis by the same authors found no effect of community notes on engagement with misinformation (Chuai et al., 2024c).\nPeople shown community notes alongside mis-leading social media posts were more accurate in identifying misleading posts, and the notes were judged to be more trustworthy than context-free misinformation flags (e.g., \"Checked by fact-checkers\" or \"Checked by other social media users\"), regardless of (US-centric) political beliefs (Drolsbach et al., 2024). People shown either com-munity notes or related news article suggestions were both less likely to to believe and report mis-leading information compared to a control group: community notes were more effective in reducing belief and sharing intention for positive rumours, while articles were more effective for negative ru-mours (Kankham and Hou, 2024). On the other hand, displaying community notes leads users to post more negative and angry replies to misleading posts (Chuai et al., 2024b), while crowd workers are also prone to cognitive biases, such as overes-timating a statement's truthfulness the more they liked its claimant, and general overconfidence in their ability to ascertain truthful statements (Draws et al., 2022).\nAlthough fact-checks and community notes share similarities in how they address misleading claims, they also differ in key elements of practice and techniques of persuasion and communication (Kankham and Hou, 2024). Fact-checking typi-cally involves the analysis and verification of pub-lic claims (e.g., statements in news reports and social media). In addition to verifying claims, in re-cent years many fact-checking organisations have also assumed a wider role in combatting misin-formation spread, conducting long-term investiga-tive journalism projects and citizen media liter-acy programs (Juneja and Mitra, 2022). Profes-sional fact-checkers in organisations signatory to the International Fact-Checking Network (IFCN) follow a rigorous set of principles and transparency commitments. In contrast, any platform user can contribute to community notes under anonymity, and the rating approach relies on the 'wisdom of crowds', with little oversight or transparency re-"}, {"title": "3 Dataset", "content": "garding the biases of the note-writers. Numerous studies have documented the structured workflow that fact-checkers follow: (i) claim selection; (ii) collecting evidence; (iii) deciding on a verdict; and (iv) writing the fact-checking article (Graves, 2017; Micallef et al., 2022; Warren et al., 2025). Fact-checking articles, which are subject to mul-tiple rounds of editorial scrutiny, are more formal and standardised in tone and style than commu-nity notes, which vary considerably. Fact-checkers must rely on credible sources and evidence to con-vince the reader, while community note writers may employ a range of persuasion techniques, such as appeals to emotion or other logical fallacies. More-over, community notes typically serve as direct rebuttals to misleading posts, while fact-checking articles may address a more general claim than is expressed in a specific post. Finally, fact-checking articles are a one-way exchange, while community notes represent a more horizontal and interactive dialogue between writer and recipient of the fact-check (Kankham and Hou, 2024).\nOur work builds on current understanding of the relationship between professional fact-checking and amateur community moderation by examining the extent to which community note writers de-ploy the work of professional fact-checkers in their notes.\nWe download files containing all community notes and their metadata from the official website, which amounts to 1.5M notes authored between January 28th 2021 and January 6th 2025. Of these, a total of 135K are rated by the community as 'Helpful', 51K are rated 'Not helpful', and 1.3M are unpublished, i.e., did not receive enough community ratings to reach a verdict. See Fig. 6 in App. A for statistics.\nWe filter the notes as follows. First, we re-move 526K non-English notes, which we identify by applying the language detection library fast-langdetect. Then, we further filter 268K \u201cunnec-essary\" notes-notes attached to tweets that are classified by the community as \"not misleading\". Finally, to focus only on notes that are used to ad-dress misinformation, we filter out 44K notes that contain one of the words \"ad\u201d, \u201cspam\u201d, or \u201cphish-ing\". Following these filtration steps, we are left"}, {"title": "4 Analysis", "content": "We analyse the dataset prepared in \u00a73 to answer the two research questions defined in \u00a71."}, {"title": "4.1 RQ1: \u03a4o what degree do community notes rely on fact-checkers?", "content": "According to Fig. 2, at least 5% of all English community notes contain an external link to pro-fessional fact-checkers. This number grows to 7% when only considering notes rated as 'helpful' (Fig. 7 in App. A). Conversely, only 1% of notes rated as 'not helpful' contain a fact-checking source (Fig. 8 in App. A). These figures are significantly larger than what was reported in previous stud-ies (1.2% (Kangur et al., 2024)), possibly because Kangur et al. (2024) utilise a smaller dataset of fact-checking agencies and classify fact-checking divisions of popular journals as \u201cnews\u201d. The re-sults imply that notes incorporating fact-checking sources are generally considered more helpful.\nWe further assess whether notes with fact-checking sources are perceived to be of higher quality by analysing individual user ratings of notes both with and without such sources. Specifically, we collect user ratings for a balanced (i.e., includ-ing of a fact-checking source or not) sample of 20K notes rated by at least 50 users, and calcu-lated the average ratings for the notes. As can be seen in Fig. 9 in App. A, community notes with fact-checking sources are generally rated higher than their counterparts. Interestingly, while notes"}, {"title": "4.2 RQ2: What are the traits of posts and notes that rely on fact-checking sources?", "content": "We begin by performing a topic analysis, com-paring topics of posts whose notes reference fact-checking sources to those citing other sources. To this end, we apply a strong zero-shot text clas-sification model to our Stext subset by classify-ing spans of the form \u201cTweet:<POST TEXT>; Note <NOTE TEXT>\" into one of 13 classes. The authors manually evaluated the quality of the classification results and considered it satisfactory.\nNotably (Fig. 5), fact-checking sources are more likely to be included in posts related to high-stakes issues such as health, science, and scams and less likely to be included in posts on tech or sports.\nWe then analyse annotations (binary attributes explaining the warrant for the note) by commu-nity note authors. Fig. 3 contains the full break-down of annotations for notes with and without fact-checking sources. Notes containing a link to fact-checking sources are overrepresented in posts where unverified information is presented as a fact or when the post contains a factual error. Con-versely, they are under-represented in posts with outdated information or satirical content. Tab. 1 contains a sample of such notes.\nThese results indicate that community note-writers adapt their strategies based on the stakes and scope of the claim, and the depth of research needed to counter misinformation. We hypothe-sise that they are more likely to rely on external fact-checking when refuting complex or unverifi-able claims (Wuehrl et al., 2024), as well as claims related to broader narratives or conspiracy theories which cannot be fully addressed in the scope of a note. Conversely, claims involving misleading media can often be debunked with examples alone, making fact-checking sources unnecessary. To in-vestigate this hypothesis, the authors of this paper manually annotated 400 < post, note > pairs from Stext with attributes related to the complexity of the claims and how community notes address them. (See App. B.1 for annotation guidelines). The re-sults (Fig. 4.a) support our hypothesis. Claims related to broader narratives or conspiracy theories are much more likely to include a link to a fact-checking source. In contrast, other types of claims are more likely to be addressed by providing miss-ing context or by invalidating the credibility of the claim's source. Additionally, Fig. 4.b depicts the different ways in which fact-checking sources are used to debunk claims. It demonstrates how such sources are rarely used to provide missing context but rather focus on discrediting sources of claims and providing scientific evidence.\nWe extend the manual annotation to an LLM-based analysis of 8K balanced (post, note) pairs from Stext. We task OpenAI's GPT-4 with deter-mining whether a pair relates to a broader narrative or a conspiracy theory. Listing 2 in App. B details the prompt used. To evaluate model accuracy, two authors independently labelled 100 balanced pairs, achieving an agreement rate of 0.88 and resolving disagreements through discussion. The model at-"}, {"title": "5 Conclusion", "content": "In this work, we annotate a large corpus of Twit-ter community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. We find that effective community moderation depends on pro-fessional fact-checking to an extent far greater than previously reported. We find that community notes linked to broader narratives or conspiracy theories are particularly reliant on fact-checking.\nOur results reveal that community notes and professional fact-checking are deeply interconnected-fact-checkers conduct in-depth research beyond the reach of amateur platform users, while community notes publicise their work. The move by platforms to end their partnerships and funding for fact-checking organisations will hinder their ability to fact-check and pursue investigative journalism, which community note writers rely on. This, in turn, will limit the efficacy of community notes, especially for high-stakes claims tied to broader narratives or conspiracies."}, {"title": "Limitations", "content": "The main limitations of our work concern the char-acteristics of the dataset we analyse. First, we restrict our analysis to notes written in English, ex-cluding over half a million notes in other languages. This decision was made to avoid potential noise and biases arising from the authors' unfamiliarity with public discourse in different regions and reliance on machine translation. In future work, we aim to extend our analysis to other languages.\nMoreover, except for a small subset of notes, we did not have access to the original tweets they were written for. Even when the tweet text was available, many contained non-text media, were written in internet vernacular that was challenging to interpret, or lacked important context. These factors limit the accuracy and effectiveness of our models and analysis.\nFinally, due to resource constraints, our manual annotation study was limited to a relatively small sample of tweets and notes. In future work, we wish to utilise crowd workers to not only annotate a larger dataset but also increase the diversity and perspective of the annotators."}, {"title": "Broader Impact and Ethical Considerations", "content": "Given that this work analyses real-world posts, eth-ical concerns may arise from using this data for research purposes. Posts from non-protected ac-counts and Community Notes on Twitter/X are publicly available, however, we acknowledge that they may contain sensitive personal information. To minimise any breach of anonymity and privacy, we anonymised links to individual accounts, and we do not publicly release this information. We do not analyse the posts or notes by individual users, and instead examine aggregated data in the form of topics and sources cited.\nAlthough the Community Notes dataset rep-resents attempts to curb harmful misinformation and conspiracies, given the intense partisanship in-volved (Allen et al., 2022; Draws et al., 2022), as well as the explicit content of some claims, some instances may be considered offensive. We also ac-knowledge that our own perspectives and biases as authors shape the impact of our findings in certain ways. For example, as mentioned in the previous section, we were unable to analyse non-English posts in-depth, so our conclusions are likely some-what focused on discourse in the Anglosphere (e.g., the US, UK, Ireland, Canada, Australia, New Zealand etc.). Furthermore, although we based our criteria for conspiracy theories on well-established sources, e.g., AP News, FactCheck.org, the Eu-ropean Commission, and identified conspiratorial narratives from both left- and right-wing sources, our own perspectives (i.e., as scientists from West-ern countries) may also have impacted what we"}]}