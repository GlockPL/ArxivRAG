{"title": "Integrating Dynamic Correlation Shifts and Weighted Benchmarking in Extreme Value Analysis", "authors": ["Dimitrios P. Panagoulias", "Elissaios Sarmas", "Vangelis Marinakis", "Maria Virvou", "George A. Tsihrintzis"], "abstract": "This paper presents an innovative approach to Extreme Value Analysis (EVA) by introducing the Extreme Value Dynamic Benchmarking Method (EVDBM). EVDBM integrates extreme value theory to detect extreme events and is coupled with the novel Dynamic Identification of Significant Correlation (DISC)-Thresholding algorithm, which enhances the analysis of key variables under extreme conditions. By integrating return values predicted through EVA into the benchmarking scores, we are able to transform these scores to reflect anticipated conditions more accurately. This provides a more precise picture of how each case is projected to unfold under extreme conditions. As a result, the adjusted scores offer a forward-looking perspective, highlighting potential vulnerabilities and resilience factors for each case in a way that static historical data alone cannot capture. By incorporating both historical and probabilistic elements, the EVDBM algorithm provides a comprehensive benchmarking framework that is adaptable to a range of scenarios and contexts. The methodology is applied to real PV data, revealing critical low production scenarios and significant correlations between variables, which aid in risk management, infrastructure design, and long-term planning, while also allowing for the comparison of different production plants. The flexibility of EVDBM suggests its potential for broader applications in other sectors where decision-making sensitivity is crucial, offering valuable insights to improve outcomes.", "sections": [{"title": "1. Introduction", "content": "Understanding and managing extreme events is crucial across multiple domains, as these rare occurrences often carry significant risks and consequences. In finance for example, extreme events such as market crashes or rapid shifts in asset prices can lead to substantial financial losses, making the prediction and mitigation of such extremes critical for portfolio management and risk assessment. Consequently, extreme value theory (EVT) has been widely applied to model financial market tail risks (e.g., [1, 2, 3]), helping institutions prepare for catastrophic losses by better managing their capital reserves.\nIn healthcare, extreme events like rare medical conditions, pandemics, or extreme fluctuations in patient health metrics (e.g. in blood glucose levels) require swift responses [4, 5, 6]. The ability to predict and mitigate these extremes improves patient care and health outcomes. Consequently, EVA has been applied to predict rare but severe health crises, like extreme blood pressure spikes in critically ill patients, which allows for more informed treatment strategies.\nIn the energy sector, particularly with regard to renewable energy, understanding extreme events is equally important. Fluctuations in energy production due to extreme weather conditions (e.g., prolonged cloudy periods or intense storms) can significantly affect the reliability of energy grids [7, 8, 9]. Renewable sources, such as solar and wind, are inherently variable, and extreme lows in production can lead to energy shortages. Therefore, applying EVA in this context enables energy providers to better predict, plan for, and mitigate risks associated with extreme energy production events.\nThis paper introduces a novel methodology, the Extreme Value Dynamic Benchmarking Method (EVDBM), which enhances the application of Extreme Value Analysis (EVA) to detect and analyze rare, high-impact events across various fields. EVDBM integrates extreme value theory with the innovative Dynamic Identification of Significant Correlation (DISC)-Thresholding"}, {"title": "2. Related Work", "content": "In [10], used extreme value theory for the estimation of risk in finite-time systems, especially for cases when data collection is either expensive and/or impossible. For the monitoring of rare and damaging consequences of high blood glucose, EVA has been deployed using the block maxima approach [11]. More examples of application of EVT can be found in the recent literature, and here we only report those considered more relevant to our research.\nExtreme value analysis in energy production and consumption, particularly in the context of renewable sources like solar and wind power, is essential for balancing energy demands. Various studies [12, 13] have focused on modeling the production of solar and wind power using the Peaks over Threshold (POT) method. These method approximate the frequency of peaks above a"}, {"title": "2.1. Theory of Extreme Value Analysis", "content": "In this section, the key characteristics of extreme value theory are highlighted. Extreme value analysis (EVA) can be approached from two different angles. The first one refers to the block maxima (minima) series. According to block maxima (minima), the annual maximum (minimum) of time series data is extracted, generating an annual maxima or minima series, simply referred as AMS. The analysis of the AMS datasets are most frequently based on the results of the Fisher-Tippett-Gnedenko theorem, which leads to the fitting of the generalized extreme value distribution. A wide range of distributions can also be applied. The limiting of distributions for the maximum (minimum) of a collection of random variables from the same distribution is the basis of the examined theorem [18].\nThe peak-over-threshold (POT) methodology is the second approach used in EVA In POT, a sorted series is analyzed, first identifying the peak values that exceed a given threshold in a given set of records. The analysis usually involves the fitting of two distributions. One concerns the number of events covering the time period or space analyzed, and the other concerns the selected size of extracted peaks. As per the Pickands-Balkema-De Haan theorem, the POT extreme values asymptotically follow the generalized Pareto distribution family, and a Poisson distribution is used for the total number"}, {"title": "2.2. Pearson correlation", "content": "The Pearson correlation coefficient [21, 22, 23], often denoted as r, is a measure of the linear relationship between two variables. The Pearson correlation coefficient quantifies the degree to which two variables X and Y are linearly related. It ranges from -1 to 1, where:\nr = 1 indicates a perfect positive linear relationship (as X increases, Y increases proportionally)\nr = -1 indicates a perfect negative linear relationship (as X increases, Y decreases proportionally)\nr = 0 indicates no linear relationship between X and Y\nThe Pearson correlation coefficient between two variables X and Y is calculated as: $r = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$ where:\nCov(X, Y) is the covariance between X and Y,\n$\\sigma_X$ and $\\sigma_Y$ are the standard deviations of X and Y, respectively.\nThe covariance measures how two variables move together and is defined as: $Cov(X,Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu_X)(Y_i - \\mu_Y)$ where:\nn is the number of data points,\n$X_i$ and $Y_i$ are the individual values of the variables X and Y,\n$\\mu_X$ and $\\mu_Y$ are the means (averages) of X and Y, respectively.\nLastly, the standard deviation of a variable X is calculated as the measure of how spread out the values of X are and is given by:\n$\\sigma_X = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu_X)^2}$"}, {"title": "2.3. Normalization", "content": "Normalization refers to the process of scaling variables so that they fit within a common range (e.g., [0, 1] or mean 0 and standard deviation 1). In many applications, normalization preserves the relative differences between variables but still retains their units in some form (although scaled). It is often used in data science, statistics, and machine learning, where the goal is to make variables comparable by bringing them onto the same scale [24, 25, 26, 27]. Common approaches in normalization aimed at transforming the variables into a comparable, dimensionless format are the following\n\u2022 Min-Max Normalization (Feature Scaling): Min-max normalization is a rescaling technique where variables are linearly scaled to a specific range, often [0, 1]. The formula is: $X_{normalized} = \\frac{x-min(X)}{max(X)-min(X)}$ Where:\nx is the original value of the variable.\nmin(X) and max(X) are the minimum and maximum values of the variable X.\n\u2022 Z-Score Normalization (Standardization): Z-score normalization transforms each variable by subtracting the mean and dividing by the standard deviation. The formula is: $X_{standardized} = \\frac{x-\\mu_X}{\\sigma_X}$ Where:\nx is the original value\n$\\mu_X$ is the mean of the variable X\n$\\sigma_X$ is the standard deviation of X"}, {"title": "3. Materials and Methods", "content": "In this section we describe the different methods used and how they are adapted into new a novel methodology.\nAs a basis to our approach we utilize Extreme Value Analysis (EVA) to identify and understand rare event scenarios, thereby aiding in informed decision-making and optimizing management strategies. Evaluating the circumstances [28] related to extreme values and looking for patterns to inform on decisions and consists of an elaborated process and two algorithmic approaches aimed at identifying, explaining and expanding on extreme circumstances."}, {"title": "3.1. Process", "content": "Evaluation: This step involves assessing the available data to define key parameters that will be used in the analysis. This might include identifying which data points are relevant to the study and determining the scope and nature of the data to be synthesized.\n\u2022 Filter by timeframe: This process filters the data according to specific time periods. The requirements for the timeframe are established, which could involve selecting data from particular years, months, or days that are relevant to the analysis objectives.\nReport: Assessment, Filtering Strategy, Dependent Variable and Related Variables: This report synthesizes the findings from the data synthesis and extreme value analysis, detailing the assessment of the data, the strategy used to filter it, and an analysis of the dependent variable in relation to other relevant variables.\n\u2022 EVA (Extreme Value Analysis): In this stage, the method for conducting Extreme Value Analysis is selected, including the determination of thresholds, types of extremes (such as maximum or minimum values), and how the results will be visualized (e.g., through plots).\n\u2022 Model Fit: This step involves fitting a statistical model to the data, focusing on the distribution of extreme values. Diagnostic checks are likely performed to ensure the model fits the data appropriately and to validate the assumptions of the statistical analysis.\nReport: Plots, Tables - Assessment and Outcomes: Visual and tabular representations of the data are provided, which support the assessment and help communicate the outcomes of the analysis.\n\u2022 Extremes: Analysis of the extreme values using descriptive statistics and other analytical methods. This part of the process aims to understand the behavior of the data at its extremes.\n\u2022 Original: Similar analytical techniques are applied to the original dataset (not just the extremes) to provide a comprehensive understanding of the overall characteristics of the examined data.\nReport: Compare - Original vs Extremes and Correlations - Original vs Extremes: This involves a comparative analysis between the (a) datapoints in the normal scenario and the extreme datapoints, as well as an (b) examination of the correlations between them to identify patterns or relationships. For (a) we assume that T represents any descriptive statistic to be computed (e.g., mean, median, variance, standard deviation). $x_1,x_2,..., x_n$ represent the values under the extreme scenario and $y_1, y_2, . . . , y_n$ represent the values under the normal scenario. Then: $\\Delta T = T(x) - T(y)$ such that mean can be represented as $\\Delta\\bar{v} = \\bar{v}_{extreme} \u2013 \\bar{v}_{normal}$ and so on. For the streamlining of (b) we have constructed a novel algorithm as Dynamic Identification of Significant Correlation"}, {"title": "3.2. Dynamic Identification of Significant Correlation Changes Using Percentile Thresholding in Extreme Value Analysis (DISC-Thresholding)", "content": "Below is the generalized mathematical formula that incorporates a dynamic threshold for identifying significant correlation changes, based on a High Positive Correlation (HPC) and a High Negative Correlation (HNN).\nLet:\n$\\Delta\\rho_{ij}$ be the change in correlation between variables $X_i$ and $X_j$, defined as:\n$\\Delta\\rho_{ij} = \\rho_{ij}^{extreme} - \\rho_{ij}$ (1)\nwhere:\n\u2022 $\\rho_{ij}^{extreme}$ is the Pearson correlation coefficient between $X_i$ and $X_j$ in the extreme dataset\n\u2022 $\\rho_{ij}$ is the Pearson correlation coefficient between $X_i$ and $X_j$ in the general dataset\nWe define thresholds based on the 90th (or other) and 10th (or other) percentiles of the distribution of $\\Delta\\rho_{ij}$ :\n\u2022 $P_{90}(\\Delta\\rho)$ The 90th percentile of all $\\Delta\\rho_{ij}$ values, representing high positive changes in correlation (2)\n\u2022 $P_{10}(\\Delta\\rho)$ The 10th percentile of all $\\Delta\\rho_{ij}$ values, representing high negative changes in correlation (3)\nThen, we define the classification of significant changes in correlation as follows:\nClassified($\\Delta\\rho_{ij}$) =$\\begin{cases}HPC (High Positive Correlation), & if \\Delta\\rho_{ij} (1) > P_{90}(\\Delta\\rho)(2) \\\\ HNC (High Negative Correlation), & if \\Delta\\rho_{ij} (1) < P_{10}(\\Delta\\rho)(3) \\\\ 0, & otherwise (Not Significant as 0)\\end{cases}$\nIn detail:\n\u2022 $\\Delta\\rho_{ij}$ (1) represents the change in the pairwise correlation between variables $X_i$ and $X_j$ when comparing extreme events to normal events.\n\u2022 HPC (High Positive Correlation) occurs when the change $\\Delta\\rho_{ij}$ exceeds the 90th percentile, indicating a significant increase in correlation during extreme events.\n\u2022 HNC (High Negative Correlation) occurs when the change $\\Delta\\rho_{ij}$ is below the 10th percentile, indicating a significant decrease in correlation during extreme events.\n\u2022 Values between the 10th and 90th percentiles are considered not significant.\nThus, the entire matrix of correlation differences is classified as HPC, HNC, or Not Significant depending on the relative magnitude of $\\Delta\\rho_{ij}$ compared to the dynamically computed thresholds $P_{90}$ and $P_{10}$."}, {"title": "3.3. EVA-Driven Weighted Benchmarking Algorithm for Performance", "content": "This process is initialized by first describing related conditions under extreme conditions and non-extreme circumstances, allowing for benchmarking and guiding operational or strategic decisions.\nV represents a set of key variables, related to the dependent variable examined using EVA.\nV = {$V_1, V_2, . . ., V_n$} (4)\n$C_{extreme} (V_i)$ and $C_{normal}(V_i)$ are the counts or statistics (e.g., average or sum) of variable $V_i$ under extreme low (or high) conditions and normal conditions, respectively, for a given use case. $C_{extreme} (V_i)$ and $C_{extreme} (V_i)$ are the values for two different use cases, let say Case 1 and Case 2, under extreme (low or high) conditions. Moving forward, the benchmarking algorithm process ensues.\nBenchmarking algorithm process:\n\u2022 Step 1: Scaling factor, that incorporates historical occurrences and accounts for future risk and adjusting for Stationarity using the projected return values\nFor the historical occurrences We can compute a scaling factor $S_j$ for each case (c) that adjusts its score based on the frequency and intensity of extreme events by normalizing the number of extreme events:\n$E_c = \\frac{N}{\\sum_1^m{N_k}}$ (5)\nWhere:\nN is the number of extreme events for a case (c)\n$\\sum_1^m{N_k}$ is the total number of extreme events across all cases\nThe EVA distributions provides a return level or exceedance probability, which informs on the likelihood that a given threshold will be exceeded in a specific time period. The probability of exceeding a threshold $P(X > x_0)$ accounts for the future risk and can be estimated as follows:\n$P_i(X > x) = 1/T_{return} \\times x(T)^{CI}$ (6)\nWhere:\n$T_{return}$ is the return period, which is the average time between extreme events that exceed the threshold x .\n* Such that if $T_{return}$ =5 (years), then the probability of an extreme event happening in any given year is 1/5 = 0.2\nand x is the Return Value per T depending on the associated confidence intervals - CI[lower, upper]. Using the projected return values add a more dynamic approach to the stationary (normalized) related circumstances\nTo account both for historical occurrences and the associated probabilities the final scaling factor is: $S_i = E_c \\times P(X > x)$ (7)\nThus for the final calculation we also consider the upper and lower limits and as such the final scaling factor is formulated in a way to include all scenarios, thus:\n\u2022 Step 2: Weighting Factor and Normalization if required\nIf variables used are of different measuring unit, normalization is applied. To account for the fact that different variables may have varying degrees of influence, we introduce weights for each variable. The weights reflect the importance of each variable under extreme conditions so that:\n$b(V_i) = w_i \\times C_{extreme} (V_i)$ (7)\nWhere: $w_i$ is the weight associated with the mean of the variable $V_i$ and $V_i$. The weights allow for prioritization on the more influential variables in the benchmarking process.\n\u2022 Step 3: Benchmarking Score (B)\nThe Benchmarking Score enumerates the final score per examined use case and the highest (or lowest score) suggests that there are better conditions associated to that case. We calculate the benchmarking score per projected year(s) as reported by the EVA, using the associated scaling factor. Calculations are considered for return value and the associated confidence intervals. This allows for visual representation of process to inform on more accurate judgments and comparisons. Thus considering (6), (7) and (8): $B_i = S_i \\times \\sum_{i=1}^n{b(V_i)}$ (9) for (a), (b) and (c).\n\u2022 Step 4: Visualization of Benchmarking Scores as a time series\nFor the visualization of the Step 3 we use a logarithmic scale for the x-axis representing return periods to improve the clarity and interpretability of the benchmarking plots. Given that the return periods span several orders of magnitude (e.g., 1 to 1000 years), a logarithmic scale allows for a more even distribution of points across the axis, making it easier to observe trends over time. This approach ensures that smaller periods (e.g., 1, 2, 5 years) and larger periods (e.g., 100, 500, 1000 years) are both visible in the same plot without compressing the data, which would occur on a linear scale."}, {"title": "4. Application of methods", "content": "In this section we apply the EVDBM methodology on Photovoltaic (PV) data taken from two different PV Plants [29, 30]. We follow the three step process outlined, without strictly proposing an optimization strategy but rather outlining a few as an example of applicability. We ill also first introduce some \"dummy\u201d weights in order to conclude with a final benchmarking score.\nIn our analysis we considered total production in kwh of the PV production farms. We considered as time range of interest the peak hours which occur during midday, thus a time range of 3 hours from 13:00 to 16:00 was examined. The data were taken from [31, 32] and are take from 2 PV plants situated in various regions of Portugal as provided from the non-profit organization, Coop\u00e9rnico. More descriptive statistics per plant examined to be provided in the following sections. We analyze and apply EVA on the production below the 25th percentile. The variables related to the production to be analyzed are shown in table 1, for context the Produzida-Production row, which refers to the Dependent variable for the EVA, is provided. We also attach the main analytical manually pre-set constants for reference, as Time-Range and Percentile, to be applied in both use Cases."}, {"title": "4.1. EVDBM application Use Case 1: \u201cZarco\u201d Data", "content": "From a total of 21932 data, 3656 were related to peak time production During the peak hours the ours the data to be analyzed are 3656 with a mean production of 23.5 kwh. As suggested the peak time hours are analyzed, thus the timeframe is ranging between 13:00 to 16:00 where the sun is the highest, below the 25th percentile of the production level. The key parameters (variables to be examined and constants) are shown in table 1. The analysis of the examined data are described in detail in table 2. The 25th percentile corresponds to a production level below 18.25."}, {"title": "4.1.2. EVA", "content": "We filtered the data to include only these time-frames. EVA was then applied using the Peaks Over Threshold method, focusing on data below our defined threshold. This threshold was set to capture the lowest 25% of production values, determined after statistical analysis of the data. To analyze the extreme value distribution, we fitted the data to a Generalized Pareto Distribution (GPD) model.\nThe histogram bars are not visible above the x-axis, which suggests that the observed values are so well-matched to the predicted values that the bars are hidden behind the PDF line. This would indicate an excellent fit if the theoretical model's PDF line accurately represents the observed histogram.\nReturn Periods (in years) can be seen in table 4.1.2.\n(1, 2, 5, 10, 25, ..) These numbers represent how often an event of a certain magnitude is expected to occur. For example, a 100-year return"}, {"title": "4.1.3. Circumstance Analysis", "content": "The differences between the two scenarios are shown in table 4.2.3, when deducting the extremes\nApplying the DISC-thresholding we extract the significant differences in correlations between the extreme low production and the normal production in the analyzed sample of peak time ranges (table 1). As can be seen in table 6 significant differences are identified, for example between between Humidity and Diffuse Solar w/m\u00b2 (HNN), humidity and temperature (HPC), for $\\Delta\\rho_{ij} = \\rho_{ij}^{extreme} \u2013 \\rho_{ij}$ and so on.\nFollowing, the relevant information to the extremes are extracted to be"}, {"title": "4.2. EVDBM application Use Case 2:\u201cjoao\u201d Data", "content": "From a total of 21908 data, 3656 were related to peak time production During the peak hours the ours the data to be analyzed are 3652 with a mean production of 23.5 kwh. As suggested the peak time hours are analyzed, thus the timeframe is ranging between 13:00 to 16:00 where the sun is the highest, below the 25th percentile of the production level. The key parameters (variables to be examined and constants) are shown in table 1. The analysis of the examined data are described in detail in table 2. The 25th percentile corresponds to a production level below 13.43."}, {"title": "4.2.2. EVA", "content": "Following the previous approach we filter the data as per the time range under the chosen percentile and apply EVA.\nAs can be seen in the return value plot 4 the data are well fitted within the distribution, thus the model is more reliable in predicting low production levels, allowing for more confidence on model's predictive capabilities.\nReturn Periods (in years) can be seen in table 4.2.2. 1 year return period: The return value is 1.05 with a CI of [ 1.96, 0.49]. This means that in any given year, we can expect an extreme-low production level of around 1.05 during Peak hours, with a reasonable range of uncertainty, significantly higher than the observed production of Use Case 1."}, {"title": "4.2.3. Circumstance Analysis", "content": "The differences between the two scenarios are shown in table 4.2.3\nApplying the DISC-thresholding we extract the significant differences in correlations between the extreme low production and the normal production in the analyzed sample of peak time ranges (table 1). As can be seen in table 6 significant differences are identified, for example between Humidity and Diffuse Solar w/m\u00b2 (HNN), temperature and diffuse solar (HPC), for $\\Delta\\rho_{ij} = \\rho_{ij}^{extreme} \u2013 \\rho_{ij}$. (1)"}, {"title": "4.3. Benchmarking", "content": "In this section we extract the benchmarking score applying the methodology described in the previous section and plot the results to highlight the differences between the two examined scenarios. Figure 7 shows that for about similar cases, distribution per year and time is about the same. However using the EVA-Driven Weighted Benchmarking Algorithm."}, {"title": "5. Discussion of Results", "content": "In this paper, we expanded Extreme Value Analysis (EVA) by incorporating additional tools to create a more streamlined and adaptable analytical approach. We introduced the Extreme Value Dynamic Benchmarking Method (EVDBM), a novel framework for analyzing extreme events across multiple domains. By leveraging both the Peaks Over Threshold (POT) and Block Maxima (Minima) methods, this approach enables flexible event detection. In addition, the integration of the Dynamic Identification of Significant Correlation (DISC)-Thresholding algorithm provides a more refined analysis of how key variables behave under extreme conditions. The EVA-Driven Weighted Benchmarking Algorithm further enhances performance comparison by weighting variables based on their influence during extreme events. This comprehensive framework deepens the understanding of extreme occurrences and their impacts on system performance, while also enabling continuous monitoring through extracted overall scores. By tracking these scores over time, the methodology allows for the ongoing evaluation of extreme events, informing decision-making and facilitating adaptation as conditions change.\nBy integrating return values predicted through Extreme Value Analysis (EVA) into the benchmarking scores, we are able to transform these scores to reflect anticipated conditions more accurately. This approach provides a more precise picture of how each case is projected to unfold under extreme conditions. The use of EVA-based return values allows us to model the expected intensity and likelihood of extreme events over different timeframes, which in turn reveals how each related circumstance is likely to develop. As a result, the adjusted scores offer a forward-looking perspective, highlighting potential vulnerabilities and resilience factors for each case in a way that static historical data alone cannot capture.\nIn the photovoltaic (PV) energy use case, the method successfully captured critical low-production events and identified significant correlations between variables, illustrating its practical application for managing operational risks in renewable energy. Furthermore, it enabled the comparison of different use cases through the generation of an overall score. This flexibility highlights the potential of the methodology for broader applications in other fields where extreme value assessment and benchmarking under uncertain conditions are required.\nIn the case of PV energy production for example this approach could aid in decision making in a variety of ways like:\n\u2022 Predicting Low Energy Events: EVA estimates energy production for various return periods, identifying rare, low-output events for photovoltaic plants [33, 34].\n\u2022 Risk Management: Knowledge of low-production frequencies aids in planning and mitigating risks through diversification, storage, and demand response [35, 36, 37].\n\u2022 Infrastructure and Investment: Low-output scenarios inform PV design, backup needs, and financial planning [38, 39, 40, 41].\n\u2022 Confidence Intervals: Confidence intervals highlight uncertainty ranges, aiding scenario planning for risk management [42, 43].\n\u2022 Climate Change Adaptation: EVA helps in assessing impacts of climate patterns on solar production, supporting adaptation strategies [44, 45, 46, 47].\n\u2022 Policy and Compliance: EVA data informs policies on reserve capacity and renewable credits.\n\u2022 Benchmarking PV Plants: EVDBM scores and compares PV plants based on climate factors, aiding in performance optimization and investment decisions."}, {"title": "5.1. Limitations and future work", "content": "The main limitations of this work to be addressed are the following:\n\u2022 Sensitivity to Data Quality and Availability: EVDBM relies heavily on robust historical data, particularly for extreme events, to make accurate predictions. In cases where data on past extremes is limited or unavailable, predictions and benchmarking scores may lack accuracy\n\u2022 Assumption of Stationarity partially addressed: External changes could alter the probability and severity of future extremes, impacting the reliability of benchmarks based on historical data.\n\u2022 Subjectivity in Weighting Variables: If weighting isn't carefully calibrated, it can introduce bias, making some cases appear more or less resilient than they might be in practice. This can particularly impact comparisons across cases with differing sensitivity to certain variables\nIn our future work we intend to apply this methodology in other use cases where increased specificity in decision making can lead to optimized and beneficial outcomes, such as in health care and in finance. We also work on integrating EVDBM algorithm for scenario-based analysis, what-if scenarios and other risk-management applications while attaching a more dynamic approach to related circumstances prediction."}]}