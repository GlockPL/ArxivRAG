{"title": "Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering", "authors": ["Derian Boer", "Fabian Koch", "Stefan Kramer"], "abstract": "Large Language Models (LLMs) frequently lack domain-specific knowledge and even fine-tuned models tend to hallucinate. Hence, more reliable models that can include external knowledge are needed. We present a pipeline, 4StepFocus, and specifically a preprocessing step, that can substantially improve the answers of LLMs. This is achieved by providing guided access to external knowledge making use of the model's ability to capture relational context and conduct rudimentary reasoning by themselves. The method narrows down potentially correct answers by triplets-based searches in a semi-structured knowledge base in a direct, traceable fashion, before switching to latent representations for ranking those candidates based on unstructured data. This distinguishes it from related methods that are purely based on latent representations. 4StepFocus consists of the steps: 1) Triplet generation for extraction of relational data by an LLM, 2) substitution of variables in those triplets to narrow down answer candidates employing a knowledge graph, 3) sorting remaining candidates with a vector similarity search involving associated non-structured data, 4) reranking the best candidates by the LLM with background data provided. Experiments on a medical, a product recommendation, and an academic paper search test set demonstrate that this approach is indeed a powerful augmentation. It not only adds relevant traceable background information from information retrieval, but also improves performance considerably in comparison to state-of-the-art methods. This paper presents a novel, largely unexplored direction and therefore provides a wide range of future work opportunities. Used source code is available at https://github.com/kramerlab/4StepFocus.", "sections": [{"title": "1 Introduction", "content": "Due to their increased capability to perform even complex tasks in recent years, LLMs are now being deployed extensively in day-to-day work, as well as in scientific and research contexts. To ensure accurate and reliable results, it is crucial to understand their abilities and limitations. LLM can sometimes provide incorrect answers, be unaware of gaps in their knowledge, or return biased information [2]. Furthermore, they tend to \"hallucinate\" [4] in some cases, which can be challenging to detect. There is ongoing debate about their suitability for performing reasoning tasks [7].\nGeneral LLMs may be used as foundation models, that are customized for improved performance on specific tasks, languages, and contents. Methods such as transfer learning, dedicated prompt engineering, and enhancement with external data and knowledge are commonly employed. Wu et al. [18] point out that external sources usually either include structured data, such as knowledge graphs (KGs), or unstructured documents. The authors recently assembled semi-structured knowledge bases (SKBs) and corresponding question-answer (QA) train and test data of three real-world domains in natural language, that require the combination of the included data. The six methods, which are evaluated on this benchmark datasets, are all based on word and graph embeddings. They can be categorized into derivatives of vector similarity searches and graph neural network-based methods: both the dominating approaches in the LLM enhancement literature. In contrast, we explore a novel and straightforward approach, 4StepFocus, to achieve more precise LLM responses whose selection increases traceability. Figure 1 visualizes the pipeline of our framework in short. It can be seen as a preprocessing step for approaches with vector similarity search, but involves having the LLM independently formalize the user input and search a KG for potential answers beforehand. We refer to a KG whose vertices are linked to documents with unstructured data, like user reviews, abstracts, or medical properties, as a semi-structured knowledge base (SKB) in this paper.\nAlthough further development and optimization are needed for better generalization and higher reliability, our experiments already demonstrate superiority over state-of-the-art methods. Importantly, in our method external information used in the decision process is more transparent and interpretable in a human-readable format. This ability to refer to quotable, semi-structured knowledge that is used in the reasoning process counters the current issue of LLMs being black boxes and makes their output more comprehensible for the user.\nThe rest of this paper is structured as follows: Section 2 reviews related work on LLM enhancement for question answering. Section 3 presents a detailed description of our approach. Section 4 describes the evaluation conducted on three QA testsets and compares our results to those of state-of-the-art methods. Section 5 summarizes our findings and outlines several options for future work."}, {"title": "2 Related work", "content": "Our work builds on and extends the existing literature on integrating LLMs with KGs for question answering, which is related to various other applications such as fact-checking and hallucination mitigation. This section discusses several key contributions to these areas and situates our approach within this context. Due to the rapidity of new developments, we include not only peer-reviewed work but also preprints.\nWu et al. [18] assembled three SKBs and created corresponding benchmark datasets for evaluating question answering. Their work includes a comparison of six methods:\nVector Similarity Search (VSS) embeds both the query and the concatenated textual and relational information of each candidate entity. Subsequently, it computes the cosine similarity between the query and candidate embeddings.\nMulti-Vector Similarity Search (Multi-VSS) represents candidate entities with multiple vectors each to capture detailed features. In our case, textual and relational information of each candidate entity are embedded separately.\nDense Retriever finetunes a query encoder and a document encoder separately using QA pairs from a training dataset.\nQA-GNN [19] constructs a subgraph where nodes represent entities found in the question or answer choices, incorporating their neighboring nodes. It integrates semantic embeddings from an LLM, jointly modeling both relational and semantic information.\nVSS + LLM Reranker [3,20] reranks the top-v results from VSS using LLMs. The LLM is given background information from the SKB about each of the top v results and prompted to return a score between 0 and 1, quantifying how much each of them fits as an answer to the question. Wu et al. employed it with two different LLMs: GPT-4-turbo (gpt-4-1106-preview) and Claude3 (claude-3-opus).\nWe compare our results against this benchmark study to highlight the efficacy of our approach in Section 4.\nMuch more related work has been done. We only mention a few, related ideas in the following: Guan et al. [6] presented a method to mitigate LLM hallucinations via autonomous graph-based retrofitting. It is related to our approach, because it involves triplet generation as well, but still differs in some components, e.g., not involving VSS and not letting the LLM determine the triplets a priori. Wang et al. [17] employ embeddings to answer questions spanning multiple documents. FACE-KEG by Vedula and Parthasarathy [16] uses a KG transformer network for explainable veracity prediction. Kundu and Nguyen [8] generate embeddings from a KG containing true claims and another with false claims, which are considered in the LLM's answer. The Knowledge Solver by Feng et al. [5] uses an LLM itself for pathfinding between identified and multiple-choice questions. Shakeel et al. [15] describe a comprehensive pipeline using graph embeddings and various NLP techniques for fact-checking. Mountantonakis and Tzitzikas [11] provide a semi-automatic web application that assists users in comparing facts"}, {"title": "3 4StepFocus", "content": "We introduce a pipeline to enhance an LLM by an SKB in a novel, traceable fashion. A semi-structured knowledge base consists of a knowledge graph $G = (V, E)$ and associated text documents $D = \\bigcup_{v \\in V} D_v$ [18], where $V$ is a set of graph nodes and $E \\subseteq V \\times V$ a set of their connecting edges, which may be directed or undirected, and weighted or unweighted.\nWhile Figure 1 visualizes the pipeline of our framework in short, Algorithm 1 in combination with Algorithms 2 and 3 describe it in more detail.\nWe define the following helper functions for the algorithms:\ntype(v) returns the node type of any node $v \\in V$. V.types = $\\bigcup_{v \\in V}$ type(v)\ndenotes the set of all existing node types in SKB.\ntype(x) returns the node type $v \\in$ V.types of any variable x."}, {"title": "4 Evaluation", "content": "We evaluate our method on the three STaRK benchmark datasets [18] and associated SKBS AMAZON, MAG, and PRIME. While the medical SKB PRIME with 10 entity types and 18 relation types has the highest proportion of relational data, STARK-AMAZON has only 4 node types but a high proportion of unstructured data (i.e., user reviews). Each dataset comes with a subset of manually designed QA pairs and a set of QA pairs that have been automatically created with LLMs included in the mining procedure. The SKBs contain several million nodes and relations. In PRIME, all nodes are marked as potential answer candidates, in the other cases it is only a high proportion. In most cases, there is more than one correct answer. We refer to Wu et al. [18] for a detailed analysis of the data's properties. We use GPT4-0 (gpt-40-2024-05-13) as the LLM for"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we presented 4StepFocus, a pipeline that substantially improves answers of both LLMs but also other enhancing state-of-the art methods. Additionally, it includes external knowledge in a more traceable and reliable way. Our framework consists of multiple steps: identifying central entities and their relations in a user's query with an LLM, querying a domain-specific SKB for those information, using VSS among possible answers to take unstructured data into account in further prefiltering, and letting an LLM rerank the final results with background information given. Through experiments on a medical, a product recommendation, and an academic paper evaluation QA set, we demonstrated that our approach in contrast to graph embedding-based methods not only adds traceability, but also improves the overall precision of LLMs and makes them more reliable. While the addition to LLMs does not require pretraining, the need of embedded candidates is still a limitation to generalizability, which our approach does not solve yet. However, narrowing potential answers, with prefiltering consequently, can make holding embeddings of all general answer candidates obsolete.\nThere are several areas that can be explored to enhance our framework. For instance, the search of entities in a SKB could include vector similarity for a higher precision in the future. Further options are considering adjectives and predicates to capture more nuanced information, replacing pronouns with referenced objects/subjects for clarity, applying external reasoning taking quantifiers and negations into account, and combining multiple knowledge bases. By pursuing these avenues of future work, we can continue to enhance the capabilities of LLMs in accessing external knowledge and conducting autonomous reasoning for more evidence-based question answering."}]}