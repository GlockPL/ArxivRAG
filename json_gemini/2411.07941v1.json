{"title": "DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays\nwith Generative Adversarial Networks", "authors": ["Zhaoxi Zhang", "Yueliang Ying"], "abstract": "Computed tomography (CT) provides highly detailed three-\ndimensional (3D) medical images but is costly, time-\nconsuming, and often inaccessible in intraoperative set-\ntings (Organization et al. 2011). Recent advancements have\nexplored reconstructing 3D chest volumes from sparse 2D\nX-rays, such as single-view or orthogonal double-view im-\nages. However, current models tend to process 2D images\nin a planar manner, prioritizing visual realism over struc-\ntural accuracy. In this work, we introduce DuoLift Genera-\ntive Adversarial Networks (DuoLift-GAN), a novel architec-\nture with dual branches that independently elevate 2D images\nand their features into 3D representations. These 3D outputs\nare merged into a unified 3D feature map and decoded into\na complete 3D chest volume, enabling richer 3D information\ncapture. We also present a masked loss function that directs\nreconstruction towards critical anatomical regions, improv-\ning structural accuracy and visual quality. This paper demon-\nstrates that DuoLift-GAN significantly enhances reconstruc-\ntion accuracy while achieving superior visual realism com-\npared to existing methods.", "sections": [{"title": "Introduction", "content": "Radiography, or X-ray imaging, is a medical imaging proce-\ndure that produces 2D projection images of a scanned pa-\ntient with relatively low radiation doses and fast acquisi-\ntion speeds. In contrast, computed tomography (CT)(Buzug\n2011) is an imaging modality that uses a series of X-rays\n(typically between 100 and 250) taken at different angles\naround 360 degrees from which a three-dimensional vol-\nume is reconstructed. A key advantage of CT imaging is its\nability to provide precise spatial information about anatom-\nical structures, such as the lungs, in 3D space. However, CT\nprocedures are time-consuming and expensive. X-rays, by\ncomparison, are quicker, less costly, and result in signif-\nicantly less radiation exposure (Organization et al. 2011).\n2D X-ray images lack the 3D spatial information necessary\nfor medical problems requiring quantitative morphological\nanalysis. Thus, reconstructing a 3D CT volume from orthog-\nonal double-view X-rays or a single-view X-ray could pro-\nvide approximate 3D information and combine X-ray and\nCT imaging benefits. Although full 3D precision cannot be\nexpected in such an approach, a significant 3D context may\nbe inferred by learning from data."}, {"title": null, "content": "The primary challenge in reconstructing a 3D CT im-\nage from two orthogonal X-ray images or a single-view X-\nray image is the need for more spatial information, particu-\nlarly along the axis perpendicular to the 2D X-ray image.\nX-ray measurements capture the attenuation of X-rays as\nthey pass through various tissues along their path. As an\ninverse problem, CT reconstruction typically requires pro-\njections densely sampled over 180 degrees. Fewer projec-\ntions increase the ambiguity in solving this inverse problem\nusing conventional CT reconstruction methods, such as Fil-\ntered Backprojection (Chetih and Messali 2015). However,\nwith recent advancements in deep learning and the availabil-\nity of large-scale medical image datasets, researchers have\ndeveloped CT reconstruction models (Ying et al. 2019; Kas-\nten, Doktofsky, and Kovler 2020; Ge et al. 2022; Gao et al.\n2023; Song et al. 2021; Kyung et al. 2023; Sun et al. 2023)\nthat can reconstruct 3D volumes from orthogonal double-\nview X-rays or even single-view X-ray images by leveraging\nlearned priors from datasets."}, {"title": null, "content": "Some studies concentrate on dense anatomical structures,\nsuch as the knees (Kasten, Doktofsky, and Kovler 2020), cer-\nvical vertebrae (Ge et al. 2022), and spine (Gao et al. 2023),\nwhich contain bones with high attenuation coefficients, re-\nsulting in high contrast in radiographic images. Other re-\nsearch focuses on soft tissue reconstruction, particularly the\nlung (Ying et al. 2019; Kyung et al. 2023; Sun et al. 2023),\nan air-filled organ with a low attenuation coefficient inside.\nHowever, existing methods primarily target the reconstruc-\ntion of the lung's overall shape and surrounding soft tissues,\noften neglecting the intricate internal structures, such as the\nnumerous thin, tree-like pulmonary vessels. These delicate\nstructures are relatively smaller than the lung, resulting in\nsubtle and complex features in 2D radiographic images."}, {"title": null, "content": "To address the challenges in 2D-to-3D reconstruction, we\nconsidered a method of lifting 2D images and features to 3D\nin advance to mitigate the difficulty associated with learn-\ning the mapping relationship between different dimensions.\nSo we introduce the Duo Lift Generative Adversarial Net-\nworks (DuoLift-GAN), using a masked loss function; our\nmodel enhances detail capture by aligning the reconstruc-\ntion with the target chest volume. By replicating the origi-\nnal 2D images and their corresponding feature maps along a\nspecific axis multiple times, we elevate 2D images and their\nfeatures into 3D representations that preserve spatial rela-"}, {"title": null, "content": "tionships and spatial coherence, resulting in more accurate\n3D feature maps and improved reconstruction quality. Ad-\nditionally, for precise shape and contour reconstruction, we\npresent DuoLift-CNN, which accurately reconstructs larger\nanatomical structures, such as the lungs, which are suitable\nfor capturing the overall structure rather than fine and thin\nstructures, such as the vessels."}, {"title": null, "content": "Moreover, we observed an intriguing phenomenon dur-\ning our evaluation of the reconstruction results: while GANS\ntend to produce volumes with richer textural details, CNN\nmodels outperform them in numerical metrics. To further ex-\nplore this discrepancy, we conducted an in-depth analysis of\nthe evaluation metrics used for CT reconstruction, focusing\non the Structural Similarity Index Measure (SSIM)(Wang\net al. 2004), Peak Signal-to-Noise Ratio (PSNR)(Hore and\nZiou 2010), and the Learned Perceptual Image Patch Simi-\nlarity (LPIPS)(Zhang et al. 2018) metric. This trend is also\nevident in the quantitative results of X2CT(Ying et al. 2019).\nMoreover, we conduct the anatomical structure level quanti-\ntative analysis of the reconstruction quality via the Dice Co-\nefficient (DICE)(Dice 1945) of the lung and vessel segmen-\ntation maps. With the broad spectrum of evaluation metrics\nfrom pixel level, anatomical structure level, to perceptual\nlevel, we conduct a comprehensive quantitative and qualita-\ntive analysis of our methods and the state-of-the-art method\non one lung CT dataset, the LIDC-IDRI dataset. Our anal-\nsis evaluates the reconstruction performance and provides\ninsightful observations for the chest CT reconstruction prob-\nlem from orthogonal X-rays."}, {"title": null, "content": "In conclusion, our contributions include:"}, {"title": null, "content": "1. We introduce a novel 2D-to-3D reconstruction architec-\nture with dual branches that independently elevate 2D\nimages and their feature maps into 3D representations.\nThese branches preserve spatial relationships and 3D\nstructure, with their outputs merged into a unified 3D fea-\nture map. Additionally, we employ a masked loss to train\nthe generator in conjunction with a discriminator, which\nimproves the accuracy of generated textural details."}, {"title": null, "content": "2. Our models, DuoLift-GAN and DuoLift-CNN, achieve\nthe best performance on benchmarks compared to meth-\nods with available code."}, {"title": null, "content": "3. We conduct an in-depth evaluation of the reconstruction\nquality on the anatomical structure level."}, {"title": null, "content": "4. We will release our code and pre-trained model weights\non GitHub, promoting further research and development."}, {"title": "Related Work", "content": ""}, {"title": "CT reconstruction from single or double view\nX-rays", "content": "Reconstructing accurate 3D volumes from single-view and\northogonal double-view X-rays is challenging, as signif-\nicant 3D spatial information must be inferred from lim-\nited 2D data. Various algorithms tackle this by incorporat-\ning back-projection, such as PerX2CT (Kyung et al. 2023),\nXray2CT (Sun et al. 2023), and C2RV (Lin et al. 2024).\nThese models first encode X-rays with 2D convolutional\nlayers and then back-project the 2D feature maps into 3D"}, {"title": null, "content": "space, followed by decoding the geometry-aware 3D fea-\nture maps via 2D convolutional layers (Kyung et al. 2023) or\nmulti-layer perceptrons (MLPs) (Sun et al. 2023; Lin et al.\n2024). Other approaches extract 3D features from 2D X-rays\nby treating the feature dimension of the 2D feature maps\nas the depth dimension. For example, X2CT-GAN (Ying\net al. 2019), SdCT-GAN (Cheng et al. 2023), and MFCT-\nGAN (Jiang 2022) encode 2D images with 2D convolutional\nlayers, lift the 2D feature maps by duplicating them along\nthe depth dimension, and then decode the lifted 3D fea-\nture maps into the 3D volumes. On the other hand, 3DSP-\nGAN (Huang et al. 2024) directly duplicates frontal and lat-\neral X-rays to transform the 2D images to 3D volumes and\nthen pass the 3D volumes to the reconstruction networks.\nOur model introduces dual branches that independently el-\nevate 2D images and their features into 3D representations\nfor capturing richer 3D information. These 3D feature maps\nfrom the dual branches are merged into a unified 3D feature\nmap."}, {"title": "Chest CT Reconstruction from X-rays", "content": "One of the challenges in chest reconstruction is ensuring\nthat the results look authentic, avoiding the appearance of\nfuzzy artifacts inside the lung. Compared to works focus-\ning on Chest CT reconstruction from multi-view such as\nXray2CT (Sun et al. 2023) and C2RV (Lin et al. 2024),\nthe artifact issue is more critical for single-view or double-\nview chest CT reconstruction (Ying et al. 2019; Cheng et al.\n2023; Kyung et al. 2023). X2CT-GAN (Ying et al. 2019) ad-\ndresses this by using a discriminator to enhance textural de-\ntails. SdCT-GAN (Cheng et al. 2023) goes further by feeding\nboth the reconstructed and target volumes and their cropped\nversions into the Discriminator to incorporate more detailed\ninformation. We propose to apply a mask generated from\nthe segmentation of the target chest volume to both the tar-\nget and reconstructed volumes and then calculate the simi-\nlarity loss between them to train the Generator and combine\nit with the Discriminator to reconstruct textural details more\naccurately."}, {"title": "Reconstruction Quality Evaluation", "content": "The two most commonly used metrics for assessing the\nquantitative results between the target and reconstructed vol-\numes are the Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR) (Wang et al. 2004; Hore and\nZiou 2010; Sara, Akter, and Uddin 2019; Setiadi 2021).\nSSIM is an image quality metric that assesses degradation\nby measuring perceived structural changes, while PSNR is\na metric that quantifies image or video quality by compar-\ning the maximum potential signal power (original data) to\nthe power of distorted noise. However, based on our experi-\nments and the results from X2CT, these two metrics tend to\nbe biased toward larger structures, such as the contours of\nthe chest and lung and are less sensitive to the quality of tex-\ntual details within the chest. To address this limitation, we\nintroduce additional metrics. The Learned Perceptual Image\nPatch Similarity (LPIPS) (Zhang et al. 2018) metric calcu-\nlates the perceptual similarity between two images and is"}, {"title": null, "content": "used in SdCT-GAN to evaluate the chest volume. Further-\nmore, the Dice Coefficient (DICE), a common metric for\nassessing the overlap between predicted binary results and\nground truth, is widely used to evaluate segmentation and\nregistration models. In our evaluation, we applied segmen-\ntation models to obtain anatomical structure-level segmen-\ntation masks, such as the lungs and vessels. We evaluate the\nDICE of the lung and the vessel shapes between the tar-\nget and reconstructed volume, providing a detailed assess-\nment of the model's performance in capturing these critical\nanatomical structures."}, {"title": "Method", "content": "Building on the concept of Conditional GAN (Ying et al.\n2019), we propose DuoLift-GAN, a generative model de-\nsigned for chest volume reconstruction from X-rays. The ar-\nchitecture of DuoLift-GAN, illustrated in Fig. 1, includes a\nGenerator (G) and a Discriminator (D). We denote the input\nin the single-view X-ray setting as $I_f \\in \\mathbb{R}^{H\\times W}$ which is\na coronal chest projection, and the inputs in the orthogonal\ndouble-view X-rays setting as $I_f$ and $I_s$. Given $I_f$ or $I_{f,s}$\nas the input, our goal is to train a model that can reconstruct\nthe corresponding target chest volume $T_\\ell \\in \\mathbb{R}^{D' \\times H' \\times W'}$."}, {"title": "Generator Architecture", "content": "Our generator employs two branches to extract 3D fea-\ntures from the 2D X-rays. In the first branch, the in-\nput projections are passed through a ResNet-34 (He et al.\n2016) 2D feature encoder to extract 2D features, denoted\nas $F^{2D} \\in \\mathbb{R}^{512\\times\\frac{H}{8}\\times\\frac{W}{8}}$, from the input images $I_{f,s}$.\nThese 2D features are then lifted into 3D feature maps,\n$F^{3D} \\in \\mathbb{R}^{512\\times\\frac{D'}{8}\\times\\frac{H}{8}\\times\\frac{W}{8}}$, by repeating $F^{2D}_{f,s}$ along the\nfrontal direction and $F^{2D}_{s,f}$ along the lateral direction $D'$\ntimes within Lifting module, as illustrated in Fig. 1. Fi-\nnally, the lifted 3D feature maps, $F^{3D}_f$ and $F^{3D}_s$, are con-\ncatenated channel-wise to form the combined 3D feature\nmap $F^B \\in \\mathbb{R}^{1024\\times\\frac{D'}{8}\\times\\frac{H}{8}\\times\\frac{W}{8}}$. Then, $F^B$ is spatially up-\nsampled using three ResNet blocks illustrated in Fig. 1,\neach followed by an Upsample Type 2 block illustrated in\nFig. 1. During the upsampling process, the channel dimen-\nsion of the volumes is reduced. The combination of ResNet\nBlock and Upsample Type 2 block can upsample the spa-\ntial size and preserve information from the previous layer.\nAnd every Resnet Block outputs $f^{u21} \\in \\mathbb{R}^{256\\times\\frac{D'}{4}\\times\\frac{H}{4}\\times\\frac{W}{4}}$,\n$f^{u22} \\in \\mathbb{R}^{128\\times\\frac{D'}{2}\\times\\frac{H}{2}\\times\\frac{W}{2}}$ and $f^{u23} \\in \\mathbb{R}^{64\\times D'\\times H'\\times W'}$ from\nthe first, second, and third blocks, respectively as shown\nin Fig. 1. $f^{u21}$ and $f^{u22}$ are further processed by Upsam-\nple type 1 blocks illustrated in Fig. 1, resulting in $f^{u11} \\in$\n$\\mathbb{R}^{64\\times D'\\times H'\\times W'}$ and $f^{u12} \\in \\mathbb{R}^{64\\times D'\\times H'\\times W'}$ to share same\nspatial dimension. By leveraging this process, we aim to\nutilize the information from multi-scale feature maps effec-\ntively."}, {"title": null, "content": "In the second branch, we duplicate projections $I_f$ along\nthe FR direction and $I_s$ along the LA direction by $D'$\ntimes to generate 3D volumes $I^{3D}_f$ and $I^{3D}_s$. These are con-\ncatenated and passed through a ResNet block, producing"}, {"title": null, "content": "the feature map $F^o$. Finally, $F^o$ is concatenated with $f^{u11}$,\n$f^{u12}$, and $f^{u23}$, resulting in the final feature map $F^u \\in$\n$\\mathbb{R}^{224\\times D'\\times W'\\times H'}$, which provides the Reconstruction mod-\nule with direct information from the original input."}, {"title": null, "content": "For reconstruction, we apply Dropout with a probability\nof 0.25 to $F^u$ to enhance the model's generalization abil-\nity. Then, we use the U-Net++ model (Zhou et al. 2018)\nthat is composed of four encoder layers, four decoder lay-\ners, and six nested dense skip connections with Deep Super-\nvision (Zhou et al. 2018) to fuse and extract the 3D features\nfrom the concatenated 3D feature map $F^u$. The extracted 3D\nfeature map is further passed through a U-Net to reconstruct\nthe chest volume $R_v$."}, {"title": null, "content": "In cases where the Generator (G) is trained independently\nwithout the Discriminator (D), we refer to the model as\nDuoLift-CNN."}, {"title": "Discriminator Architecture", "content": "Our model's Discriminator (D) is designed to differentiate\nbetween real and fake volumes by encoding the target and\nreconstructed volumes into a 5x5x5 vector. This vector is\nused to classify the target chest volume as real and the recon-\nstructed volume as fake. The Discriminator is composed of\nfour convolutional layers, each with a 4x4x4 kernel, a stride\nof one, and padding of two, along with average pooling us-\ning a 3x3x3 kernel, a stride of two, and padding of one. This\ncombination effectively encodes the volumes, and the Dis-\ncriminator outputs the probability that the input volume be-\nlongs to either the real or fake class."}, {"title": null, "content": "We incorporate a segmentation step that masks the target\nand reconstructed volumes with the lung segmentation code\nfrom LiftReg (Tian et al. 2022) to encourage the model to\nfocus on the structures inside the lung. Instead of feeding\nthe masked volumes directly into the Discriminator, we pro-\npose calculating the similarity loss between the masked tar-\nget and reconstructed volumes, which is more effective. This\napproach enhances the accuracy of reconstructing textural\ndetails within the lung. However, since we want DuoLift-\nCNN to focus on capturing large structures like the contours\nof the chest and lungs, we did not use masked loss during its\ntraining."}, {"title": "Train", "content": "To train DuoLift-GAN, we followed the approach outlined\nin X2CT-GAN, utilizing separate optimizers for the Gener-\nator (G) and Discriminator (D). The Discriminator is trained\nto differentiate between real target volumes and those gener-\nated by the model, while the Generator is optimized with a\ndual objective. First, G minimizes a similarity loss between\nthe target and reconstructed volumes, both with and with-\nout masking. Second, G is trained to generate volumes that\ncan successfully \"fool\" the Discriminator into classifying\nthem as real, thus engaging in a zero-sum game with D. We\nfine-tuned hyper-parameters using the LIDC dataset's vali-\ndation set. We determined the model weights based on the\nbest performance on the validation set. Specifically, we used\na learning rate of 5e-4 for training DuoLift-CNN and 1e-4\nfor training DuoLift-GAN. The learning rate was reduced"}, {"title": "Loss", "content": "The loss function for our model is divided into two main\ncomponents: the generator loss (G loss) and the discrimina-\ntor loss (D loss). The G loss is primarily focused on making\nthe reconstructed volume as similar as possible to the target\nvolume and fooling the discriminator into classifying the re-\nconstructed volume as real."}, {"title": null, "content": "The G loss is calculated by combining the Mean Squared\nError (MSE) and L1 loss between the reconstructed volume\n$\\hat{I}$ and the target volume $I$ as:"}, {"title": null, "content": "$$\\mathcal{L}^{MSE}_{recon} = \\frac{1}{\\Omega} \\sum_{i}^{\\Omega} (I - \\hat{I})^2,$$"}, {"title": null, "content": "$$\\mathcal{L}^1_{recon} = \\frac{1}{\\Omega} \\sum_{i}^{\\Omega} ||I - \\hat{I}||_1,$$"}, {"title": null, "content": "In addition, the G loss includes the MSE and L1 loss be-\ntween the masked reconstructed volume $MI$ and the masked\ntarget volume $MI$, which focuses on the lung region:"}, {"title": null, "content": "$$\\mathcal{L}^{MSE}_{inside} = \\frac{1}{\\Omega} \\sum_{i}^{\\Omega} (MI - \\hat{MI})^2,$$"}, {"title": null, "content": "$$\\mathcal{L}^1_{inside} = \\frac{1}{\\Omega} \\sum_{i}^{\\Omega} |MI - \\hat{MI}||_1,$$"}, {"title": null, "content": "To further refine the generator, an adversarial loss compo-\nnent $\\mathcal{L}_{adv}$ is included to train the generator to produce vol-\numes that can fool the discriminator. On the other hand, the"}, {"title": null, "content": "D loss is designed to train the discriminator to distinguish\nbetween real and fake volumes. It is calculated using the ad-\nversarial loss $\\mathcal{L}_{adv}$:"}, {"title": null, "content": "$$\\mathcal{L}_{adv} = - \\mathbb{E}_{x \\sim p(x)}[log (D(G(x)))]$$"}, {"title": null, "content": "$$\\mathcal{L}_{adv} = -\\mathbb{E}_{y \\sim p(CT)} [log D(y)] - \\mathbb{E}_{x \\sim p(x)} [log (1 - D(G(x)))],$$"}, {"title": null, "content": "where $y \\sim p(CT)$ represents samples from the real data\ndistribution. $x \\sim p(x)$ represents $x$ is X-rays from the data\ndistribution $p(x)$ with $G(x)$ represents the generated 3D vol-\nume and $D(y)$ the discriminator's output for real data $y$.\nThe overall G loss is then summarized as:"}, {"title": null, "content": "$$\\mathcal{L}_{G} = \\alpha(\\mathcal{L}^{MSE}_{recon} + \\mathcal{L}^1_{recon} + \\mathcal{L}^{MSE}_{inside} + \\mathcal{L}^1_{inside}) + \\beta\\mathcal{L}_{adv}$$"}, {"title": null, "content": "The D loss is expressed as:"}, {"title": null, "content": "$$\\mathcal{L}_{D} = \\beta\\mathcal{L}_{adv}$$"}, {"title": null, "content": "In our experiments, the hyper-parameters $\\alpha$ and $\\beta$ are set\nto 1.0 and 0.01, respectively."}, {"title": "Experiments", "content": ""}, {"title": "Dataset Description", "content": "LIDC-IDRI The LIDC-IDRI dataset (Armato et al. 2011)\ncontains thoracic CT scans used for diagnostic and lung can-\ncer screening, with annotations identifying and delineating\nlung lesions. We utilized the preprocessed data from X2CT-\nGAN(Ying et al. 2019), which includes chest volume pro-\njections processed through CycleGAN (Zhu et al. 2017) to\nbridge the gap between projected images and actual X-rays.\nThe original dataset comprises a training set and a test set,\ncontaining 916 and 102 CT images, respectively. We further\ndivided the training set into 816 images for training and 100"}, {"title": "Evaluation Metrics", "content": "Structural Similarity Index Measure (SSIM) The Struc-\ntural Similarity Index Measure (SSIM)(Wang et al. 2004)\nassesses image quality by quantifying structural similarities.\nThis study utilizes both 2D and 3D versions of SSIM to\ncompare the structural similarity between reconstructed 3D\nchest volumes and target CT volumes. SSIM(2D) evaluates\nthe volume slice by slice using a 2D Gaussian kernel and\nthen averaging the values across all slices, while SSIM(3D)\napplies a 3D Gaussian kernel to the entire volume (jinh0park\n2019). A higher SSIM score indicates greater structural sim-\nilarity between the volumes."}, {"title": null, "content": "Peak Signal-to-Noise Ratio (PSNR) Peak Signal-to-\nNoise Ratio (PSNR) measures image quality by compar-\ning the maximum signal value to the noise level expressed\nin decibels (dB). There are 2D and 3D versions of PSNR.\nPSNR(2D) applies PSNR on each slice of volume, and\nPSNR(3D) applies PSNR to the entire volume."}, {"title": null, "content": "Learned Perceptual Image Patch Similarity (LPIPS)\nLearned Perceptual Image Patch Similarity (LPIPS)(Zhang\net al. 2018) measures perceptual similarity between images\nusing pre-trained networks, closely aligning with human vi-\nsual perception. In this study, we use LPIPS with the VGG\nnetwork to evaluate reconstruction quality, where a lower\nLPIPS score indicates higher visual quality. To evaluate the\nvisual quality of the entire 3D structure using LPIPS, we\ncompute the metric for each slice and then take the mean\nvalue across all slices in the reconstructed volume."}, {"title": null, "content": "Dice similarity coefficient(DICE) The Dice Similarity\nCoefficient (DICE) is a statistical measure used to assess\nsimilarity between two samples. It is calculated between the"}, {"title": "CT Reconstruction from XRays", "content": "We compare DuoLift-CNN and DuoLift-GAN with X2CT-CNN and X2CT-GAN, publicly available state-of-the-art\n(SOTA) models for single-view and double-view chest CT\nreconstruction. To ensure a fair comparison, we use the of-\nficial implementation and hyperparameters from the X2CT\nrepository\u00b9 and train the X2CT models using the same\ndataset split as for DuoLift-CNN and DuoLift-GAN. As\nshown in Table 1, DuoLift-CNN and DuoLift-GAN consis-\ntently outperform X2CT-CNN and X2CT-GAN in terms of\nPSNR, SSIM, and LPIPS on the LIDC dataset. This trend is\nobserved across both single-view and double-view settings,\ndemonstrating that the DuoLift-variants framework achieves\nbetter reconstruction quality than existing SOTA models."}, {"title": null, "content": "Additionally, we observe discrepancies between DuoLift-CNN and DuoLift-GAN when evaluating reconstruction\nquality with SSIM and LPIPS (VGG). We will conduct fur-\nther qualitative and quantitative shape analyses in the fol-\nlowing sections to understand these differences."}, {"title": null, "content": "Why does DuoLift-GAN perform better on LPIPS(vgg)\nbut worse on SSIM? We analyzed a randomly selected\ntest sample from the LIDC dataset to investigate metric\ninconsistencies by reconstructing volumes using DuoLift-GAN, DuoLift-CNN, X2CT-GAN, and X2CT-CNN. Quan-\ntitative and qualitative results are shown in Table 2 and Fig-\nure 2, respectively. DuoLift-CNN achieves higher SSIM and\nPSNR scores than DuoLift-GAN, consistent with the test\nset findings in Table 1. However, DuoLift-GAN provides\nmore detailed lung structures, aligning with LPIPS and high-\nlighting its advantage in capturing internal features. This in-"}, {"title": null, "content": "dicates that SSIM and PSNR emphasize larger structures,\nwhereas LPIPS more effectively assesses finer lung details."}, {"title": "How does DuoLift-GAN perform reconstructing ROIs?", "content": "We calculate the DICE coefficient for the region of inter-\nest (ROI) to evaluate reconstruction quality further. We use\npre-trained medical image segmentation models, TotalSeg-\nmentator (D'Antonoli et al. 2024) and Lungmask (Wang, Li,\nand Li 2009)2, to obtain vessel and lung segmentation masks\nfrom both the reconstructed and target volumes. The DICE\ncoefficient is computed for the lung and vessel segmentation"}, {"title": "2D and 3D SSIM Comparison", "content": "Quantitative results in Ta-\nble 1 show a significant difference in SSIM(2D) scores be-\ntween X2CT-CNN and DuoLift-CNN, while the difference\nin SSIM(3D) scores is much smaller. As illustrated in Fig-\nure 2, DuoLift-CNN's reconstructed volume exhibits more\nprecise and clear edges than X2CT-CNN, making its contour\nmore similar to the target volume. This difference is due to\nSSIM(2D) evaluating volumes slice by slice, accumulating\npixel-wise differences, and amplifying the gap between the\ntarget and reconstructed volumes. Conversely, SSIM(3D) as-\nsesses the entire volume as a whole 3D structure, which mit-\nigates the cumulative effect of pixel-wise differences."}, {"title": "Ablation Study", "content": "In this section, we conduct ablation studies on a private\ndataset with 699 volumes for training and 100 volumes for\ntesting to answer the following two questions: 1) How does\nthe lifting module affect the reconstruction quality? and 2)\nwhat is the most effective way to utilize the masked chest\nvolume?"}, {"title": "Do we need the duo lifting module?", "content": "We trained DuoLift-CNN on the private dataset in the double-view reconstruc-\ntion setting to assess the impact of the Lifting module shown\nin Fig. 1. The results presented in Table 4 show that intro-\nducing the dual lift branches into the model led to improved\nPSNR, SSIM, LPIPS, and DICE scores for the reconstruc-\ntion task."}, {"title": null, "content": "For comparison, we evaluated two alternative configura-\ntions. The first configuration uses dual branches but replaces"}, {"title": null, "content": "the duplication operation in the Lifting module with a 2D\nconvolution layer in both branches, whose result is shown\nin the second row in Table 4. In the second configuration,\nwe keep the 2nd branch and show the result in the third row\nin Table 4. As shown in Table 4, DuoLift-CNN, combining\nboth the"}]}