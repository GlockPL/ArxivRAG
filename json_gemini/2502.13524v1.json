{"title": "MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis", "authors": ["Wei Dai", "Steven Wang", "Jun Liu"], "abstract": "Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the \"Mamba\" model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba's potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models.", "sections": [{"title": "1. Introduction", "content": "The significance of early detection in medical diagnostics cannot be understated, particularly for diseases such as pre-cancerous conditions, hepatocellular carcinoma (Quinton et al., 2023), brain tumour (LaBella et al., 2024), and pelvic fracture (Liu et al., 2023b). These conditions often exhibit varied pathologies in terms of size, morphology, and density, posing significant challenges to detection, which is critical for improving patient outcomes. For instance, accurately identifying the inferior alveolar canal is crucial to prevent damaging the inferior alveolar nerve during maxillofacial surgeries like implant placements and molar extractions (Lumetti et al., 2024).\nMoreover, the accuracy of morphometric assessment of these pathological areas is vital for evaluating disease risk and progression (Quinton et al., 2023; Liu et al., 2023b; LaBella et al., 2024; Lumetti et al., 2024).\nAdvancements in deep learning have revolutionised medical image analysis, achieving diagnostic accuracies on par with human experts. However, the diversity in data from different imaging devices and patients poses significant challenges. Encoder-decoder architectures like UNet (Ronneberger et al., 2015) and its evolved versions, such as UNet++ (Zhou et al., 2019) and SwinUNETR-V2 (He et al., 2023), have demonstrated enhanced capabilities in image segmentation, essential for accurate medical analysis. Despite these advancements, there remains a pressing need for models that can operate efficiently in real-time to support clinical practices (Liu et al., 2023a; Dai et al., 2024a).\nLight-weight deep learning models, optimised for speed and efficiency, are increasingly applied in clinical settings where computational resources are constrained (Dai et al., 2024a). Recent innovations in network compression (Vasu et al., 2023; Zhang and Chung, 2024) and neural architecture specification (Chen et al., 2017; Howard et al., 2019; Mehta and Rastegari, 2021; Dai et al., 2024a) have improved the computational efficiency of these models, enabling their deployment on less powerful devices, such as clinical workstations and mobile devices. While these models have been successful in general object recognition tasks, their potential in 3D medical image analysis has not been thoroughly studied.\nThis manuscript introduces the MobileViM architecture, specifically tailored to tackle the complexities of 3D medical image segmentation across various modalities, delivering enhanced efficiency and precision. Our key contributions include:\n\u2022 Development of MobileViM: We present MobileViM, a novel light-weight architecture based on the vision-Mamba framework. Mobile ViM utilises a dimension-independent mechanism, dual-direction traversing technique, and scale bridging approach to effectively process 3D medical images at speeds over 90 frames per second (FPS) with fewer than 6.5 million parameters, setting a new benchmark for clinical applications.\n\u2022 Efficient 3D Data Processing: The dimension-independent mechanism transforms 3D data into a more manageable 1D format, therefore reducing the parameter count by 11 million and increasing the speed of MobileViM by 70 FPS on a single graphics processing unit.\n\u2022 Bidirectional Information Flow: The dual-direction traversing method enhances feature learning by scanning the information flow in two directions, significantly improving performance with an increase of fewer than 0.02 million parameters.\n\u2022 Multi-level Feature Extraction: By combining Mamba and convolution strategies, MobileViM exploits local hierarchies and inter-patch relationships, facilitating an efficient analysis of medical images.\n\u2022 Cross-scale Feature Learning: The scale bridging method mitigates compression artefacts by leveraging high-resolution early-stage features to enhance the MobileViM's ability to learn features across multiple scales.\n\u2022 Cross-dataset Validation: MobileViM was evaluated across four public datasets (PENGWIN, BraTS2024, ATLAS, and ToothFairy2) and demonstrated superior performance in segmentation of various imaging modalities with a Dice similarity score exceeding 75%.\nThe subsequent sections will review the related literature (Sec. 2), detail the MobileViM methodology (Sec. 3), discuss experimental validations (Sec. 4), offer insights into both the strengths and potential extensions of our work (Sec. 5), and summarise our findings (Sec. 6)."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Medical Image Segmentation", "content": "Semantic segmentation is crucial in analysing medical images by distinguishing different tissue structures and providing granular insights. Advanced deep learning techniques have shown remarkable success, often achieving or even surpassing expert-level accuracy in medical image segmentation (Dai et al., 2024b,c; Ronneberger et al., 2015; Shaker et al., 2024; Isensee et al., 2021; Dai et al., 2024d). The encoder-decoder architecture, first established by Long et al. (Long et al., 2015), is a cornerstone in this field, consisting of an encoder for extracting features and a decoder for generating masks.\nRonneberger et al. (Ronneberger et al., 2015) introduced a significant advancement with the UNet architecture, specifically designed for medical imaging with its U-shaped configuration. This concept was further evolved by Zhou et al. (Zhou et al., 2019) with UNet++, which enhances multi-scale feature integration, and by Isensee et al. (Isensee et al., 2021), who modified UNet to accommodate both 2D and 3D imaging contexts with nnUNet. He et al. (He et al., 2023) integrated Swin Transformer to develop SwinUNETR-V2, aimed at multi-organ CT and MRI analyses. Shaker et al. (Shaker et al., 2024) developed UNetR++, which incorporates attention mechanisms to improve the extraction of spatial features. Chen et al. (Chen et al., 2024) introduced TransUNet, which combines vision transformers and CNNs to better capture long-range dependencies within images and refine predicted regions. Despite the success of these models, their comparatively large size and computational demands often limit their use in real-time medical applications."}, {"title": "2.2. State Space Model", "content": "Structured state space models (SSMs) address computational inefficiencies associated with processing long sequences in transformers. The structured state space sequence (S4) models, developed by Gu et al. (Gu et al., 2022), present a viable alternative to traditional transformers, demonstrating linear or near-linear scaling with sequence length. Traditional S4 models, however, face challenges in capturing contextual nuances within information-dense data such as text and images (Gu et al., 2022). To overcome these limitations, Gu et al. (Gu and Dao, 2024) have enhanced S4 models by introducing advanced selection mechanisms and a recurrence scan strategy, known as Mamba. The Mamba model integrates sequence length information more effectively into SSMs, thereby improving content-based reasoning. Dao et al. (Dao and Gu, 2024) introduced Mamba2, an advancement of the original Mamba model that integrates SSMs with various attention mechanisms through semi-separable matrix transformations and incorporates a parallel training framework for improved efficiency.\nIn visual tasks, Zhu et al. (Zhu et al., 2024) modified 2D image into a format suitable for the 1D capabilities of the SSM and adapted the Mamba model for bidirectional processing, which enhances image classification and segmentation performance while reducing computational expenses compared to ViTs. Besides, Liu et al. (Liu et al., 2024b) enhanced the standard SSMs by applying a raster scan over 2D images using four different paths, developing VMamba, which addresses SSMs' limitation to only process 1D data. Zhu et al. (Zhu et al., 2025) advanced VMamba by incorporating context clusters to learn local features. Furthermore, Ruan et al. (Ruan and Xiang, 2024) improved the UNet architecture by incorporating the Mamba module, creating VMUNet, which offers broader modeling capabilities. Additionally, Xing et al. (Xing et al., 2024) implemented Mamba blocks within the encoder portion of UNet, termed SegMamba, specifically for handling volumetric features in 3D colorectal cancer imaging. Moreover, Liu et al. (Liu et al., 2024a) investigated the advantages of using pretrained weights from ImageNet to boost medical image segmentation performance. Despite these advancements, a general oversight remains regarding the computational costs incurred during the testing phase, which is crucial for real-time disease diagnosis."}, {"title": "2.3. Light-weight Neural Networks", "content": null}, {"title": "2.3.1. Network Compression", "content": "Network compression integrates strategies that impose structural constraints either during or after the training process to reduce redundancy in the network. Techniques include direct compression during training (Zhang and Chung, 2024) or applying compression after learning is complete (Vasu et al., 2023). One notable method within network compression is knowledge distillation, which involves transferring features from a larger \"teacher\" network to a smaller \"student\" network during training. While knowledge distillation reduces the parameter count needed during inference, it introduces the computational burden of managing and training two separate networks (Zhang and Chung, 2024). Another method used in network compression is network reparameterisation, which trains the network using adaptable modules and deploys a streamlined version for inference. Like knowledge distillation, network reparameterisation increases training complexity due to the adjustable nature of the modules involved (Vasu et al., 2023)."}, {"title": "2.3.2. Neural Architecture Design", "content": "Designing architectures that are mobile-friendly provides more flexibility than network compression alone. A key strategy in developing light-weight CNNs involves the use of depthwise separable convolutions, which replace standard convolutions with depthwise and pointwise layers to significantly cut the computational costs while preserving performance (Howard et al., 2019; Mehta and Rastegari, 2021; Dai et al., 2024a). Another powerful method is the use of dilated convolutions, especially in conjunction with atrous spatial pyramid pooling (ASPP) (Chen et al., 2017), which employs dilated convolutions to capture spatial features at varying scales, improving the delineation of segmentation boundaries. For ViTs suited to mobile environments, the MobileViT architecture has been developed, merging convolutional layers with transformer components in a hybrid block to address latency from image splitting and to maintain inductive biases (Mehta and Rastegari, 2021; Dai et al., 2024a). Lee et al. (Lee et al., 2023) have integrated large-kernel and depthwise separable CNNs with swin transformer blocks in their 3DUX-Net, reducing the number of normalisation and activation layers and thereby minimising the model's parameters.\nIn the realm of efficient Mamba architecture, Pei et al. (Pei et al., 2024) developed an atrous-based scanning approach to optimise patch sampling and reduce the complexity of vision Mamba. Yao et al. (Yao et al., 2024) have worked on enhancing content-awareness representations and encoding semantic relationships by reducing spectral variability and confusion in hyperspectral imaging through the integration of SSMs. Furthermore, quantisation of state variances within the Mamba has been implemented, storing state caches as low-bit elements for low-rank approximation (Anonymous, 2024). Additionally, Lee et al. (Lee et al., 2024) streamlined the sequence length of hidden states in Mamba to lower computational costs.\nAlthough these mobile architectures achieve performance comparable to conventional networks, their potential in medical image analysis, particularly in 3D imaging, has not been adequately addressed. We propose a light-weight vision Mamba architecture that incorporates the dimension-independent mechanism, dual-direction process technique, and the scale bridger, capable of conducting segmentation tasks on 3D medical images and overcoming current limitations in the field."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Overall Framework", "content": "This section introduces the mobile vision Mamba (MobileViM) network, as illustrated in Fig. 1. The network comprises two main elements: the MobileMamba block and the scale bridger.\nMobileMamba Block: As highlighted by the green dashed box in Fig. 1, the MobileMamba block is structured into sections for global communications and local connections. The block features the dimension-independent (Dimin) mechanism, which is designed to capture a broader range of spatial hierarchies, essential for advanced contextual feature learning. The Dimin Mamba employs a dimension-independent mechanism that processes each dimension of 3D data individually, thereby significantly boosting computational efficiency. Moreover, the MobileMamba block uses a bidirectional information flow to process stacked patches in onwards and backwards directions, termed the dual-direction Mamba. The dual-direction traversal of patches ensures comprehensive integration of spatial information, enhancing the block's capability in feature extraction.\nBefore the Mamba module, the depthwise separable convolution (a depthwise convolution and a pointwise convolution) with relatively small kernel size, 1 \u00d7 1 \u00d7 1 or 3 \u00d7 3 \u00d7 3, is applied to learn the local connections of voxels. After the Mamba module, the feature concatenation and addition are employed to improve the local and global fusion of features that come from the output of convolutions or Mambas.\nScale Bridger: This component consists of a series of strided convolutions within the feature map, as depicted by the orange arrows in Fig. 1. It facilitates the tracking of feature evolution throughout the learning process, guiding the network's subsequent stages.\nOther Components: The architecture initiates with an encoder, structured as shown in the first row of blocks in Fig. 1. It starts with a Conv3\u00d73\u00d73 \u21932, followed by four MobileMamba blocks and three DWConv \u2193 2. This setup optimises traditional encoders by utilising fewer strided convolutions only four in total to achieve a more compact model size and faster inference speeds. Within the architecture, each \u201cBottleneck\" module integrates a sequence of convolutions: starting with a 3 \u00d7 3\u00d73 convolution, followed by a 1 \u00d7 1 \u00d7 1 convolution to compress the feature space, and another 1 \u00d7 1 \u00d7 1 convolution for feature refinement. \"DWConv\u201d, or depthwise convolution, is utilised throughout the network to decrease computational load while maintaining robust feature extraction capabilities.\nIn this study, the model is scaled into two sizes to meet varying computational and performance criteria: \u201cextra small\u201d and \"small\". Each scale, detailed in Tab. 1, incorporates specific architectural adjustments to effectively balance the constraints of model size with the desired performance objectives."}, {"title": "3.2. State Space Model Foundations", "content": "Structured state space sequence (S4) models, a specialised subset of state space models (SSMs), are designed to emulate continuous systems by mapping one-dimension sequences $x(t) \\in \\mathbb{R}^{M}$ to $y(t) \\in \\mathbb{R}^{M}$ through implicit states $h(t) \\in \\mathbb{R}^{(M,1)}$. S4 models are characterized by four parameters: timescale parameter A, evolution parameter A, and projection parameters B and C, which define the sequence-to-sequence transformation (Gu et al., 2022). The output y(t) of continous system are defined as:\n$\\frac{dh(t)}{dt} = Ah(t) + Bx(t)$ (1)\n$y(t) = Ch(t)$\nwhere M is the state expansion factor, $A \\in \\mathbb{R}^{(M,M)}, B,C \\in \\mathbb{R}^{(M,1)}$.\nS4 models discretizes continous parameters A and B and transform them into discrete parameters $\\overline{A}$ and $\\overline{B}$ by using a time step \u0394 and the zero-order hold method:\n$\\overline{A} = exp(A\\Delta)$ (2)\n$\\overline{B} = (\\Delta A)^{-1}(exp(A\\Delta) - I) \\cdot \\Delta B$\nwhere I denotes the identity matrix.\nUsing Eq. (1) and Eq. (2), the discrete system can be formulated as:\n$h_{t} = \\overline{A}h_{t-1} + \\overline{B}x_{t}$ (3)\n$y_{t} = C^{T}h_{t}$\nFinally, S4 models compute results through a global convolution:\n$\\mathbf{K}=(C \\overline{B}, C \\overline{A}\\overline{B},.... C \\overline{A}^{N-k}\\overline{B},...)$ (4)\n$y = x * K$\nwhere $K \\in \\mathbb{R}^{N}$ is a structured convolutional kernel, and * represents the convolution operation. k \u2208 [1, N \u2013 1] is the kernel size and N denotes the length of the input sequence.\nMamba extends the S4 models by adapting the changes in tensor shapes according to parameters, thereby enabling the learning of the long-range features from text or images (Dao and Gu, 2024). If the parameters (A, B, C) can vary in time, the S4 model can selectively choose to focus on or ignore inputs at every timestep. Then the Eq. (3) can be enhanced and presented by:\n$h_{t} = A_{t}h_{t-1} + B_{t}x_{t}$ (5)\n$y_{t} = C_{t}h_{t}$\nwhere $h_{t} \\in \\mathbb{R}^{M,N}, A_{t} \\in \\mathbb{R}^{M}$, and $A_{t}, B_{t}, C_{t} \\in \\mathbb{R}^{M,N}$"}, {"title": "3.3. Dimension-independent Mechanism", "content": "In our research, we have extended the application of the Mamba model, originally developed for analysing 1D sequential data, to accommodate higher-dimensional data, particularly images. To achieve this, we reformat the input data, represented as a tensor $X \\in \\mathbb{R}^{C \\times D \\times H \\times W}$, into a series of 2D patches $X_{p} \\in \\mathbb{R}^{N \\times (C \\times P)}$. Here, C denotes the number of channels, and the tuple (D, H, W) specifies the dimensions of the input tensor.\nThe variable N indicates the total number of patches and concurrently serves as the length of the input, while P represents the patch size.\nDriven by the goal of reducing computational demands and enhancing efficiency, we pose the question: Is it possible to \u201clinearise\u201d data dimensions without loss of information? Previous research has explored separating feature maps along dimensions, using attention maps as a skip connection to re-weight features on the main flow (Hou et al., 2021). However, directly separating the dimensions of main flow feature maps can result in losing information necessary for effective feature learning.\nTo address this, we have implemented a straightforward but effective matrix multiplication using single-dimension patches from the decomposition of the Mamba outputs. In the tested 3D image datasets, although the height and width dimensions are consistent, the depth dimension varies. Consequently, matrix multiplication is applied separately for both height and width to the depth dimension. This feature fusion approach via matrix multiplication ensures that each voxel has information from three dimensions, reintegrating the separate dimensions. We refer to this method as the dimension-independent (Dimin) mechanism. Dimin can be considered a context-learning operation that significantly enhances global communications among voxels. As demonstrated in the ablation study detailed in Sec. 4.3, the Dimin mechanism improves model performance with reduced computational load. The Dimin mechanism is illustrated in Fig. 1 and detailed in Fig. 2.\nThe SSM in the Mamba module and the self-attention mechanism in the ViT are pivotal for adaptively providing a global context. Considering a visual sequence represented by $X_{p} \\in \\mathbb{R}^{N \\times E}$, the computational complexities of self-attention and SSM differ significantly:\n\u2022 The computational complexity of self-attention scales quadratically with the sequence length N:\n$\\Omega(\\text{self-attention}) = 4NE^{2} + 2N^{2}E$ (6)\n\u2022 In contrast, the complexity for SSM scales linearly with"}, {"title": "3.4. Dual-direction Information Flow", "content": "Inspired by the foundational concepts in the Mamba2 block (Dao and Gu, 2024) and the bidirectional sequence mixer (Hwang et al., 2024), we propose an advanced dual-direction Mamba block tailored for vision processing tasks. This new block, depicted as the blue block of Fig. 1 and elaborately described in Fig. 3, is further outlined algorithmically in Algorithm 1. The dual-direction vision mamba utilises I blocks to process patches, learning representations among patches to enhance its analytical capabilities.\nThe parameter A is initialised using a continuous uniform distribution over the interval [1, M]. Each input patch {Xp}; undergoes normalisation before being divided into two pathways x and z, each expanded by a factor of 2.\nBidirectional scanning refers to the process where x pathway is analysed along the length dimension N in two opposite directions onwards and backwards. This results in two vectors: Xonwards and xbackwards. Each directional output, denoted as xd where d can be either 'onwards' or 'backwards', is projected into its respective matrices Bd, Cd, and Ad. The values in Ad is then used to discretize the parameters Ad and Ba, converting them to Ad and B\u0105, respectively.\nThe processed outputs from the SSM recurrences, denoted as yd, are then controlled by the gating functions linked to zd. After gating, the outputs are normalised and aggregated to produce the enhanced patch {Xp}i+1. The default setting for the SSM's state expansion factor, M, is configured to 16.\nThis dual-direction approach amplifies the depth of data analysis and significantly boosts the model's precision in handling and interpreting complex visual information within bidirectional contexts."}, {"title": "3.5. Mobile Vision Mamba", "content": "To improve local and contextual representation learning while maintaining relatively low computational demands, we developed the MobileMamba block by applying the Dimin mechanism (Sec. 3.3), dual-direction Mamba(Sec. 3.4), and convolution techniques, which is visualised as the green boxes in Fig. 1. The MobileMamba block combines depthwise separable convolutions, followed by Mamba operations using the"}, {"title": "3.6. Scale Bridger", "content": "The performance of neural networks in processing medical images often diminishes as feature map shapes become more compressed, mainly due to the introduction of compression artifacts (Dai et al., 2024b). To address this challenge, our study introduces a scale bridger module that leverages higher-resolution features from earlier stages within the network. Assuming that o denotes the target encoder stage, the output at this stage, yo, is calculated using the formula:\n$y_{o} = \\sum_{s=1}^{o-1}g(x_{s}, o)$ (9)\nwhere xs refers to the input tensor at encoder stage s, and o-s indicates the number of the strided convolutions between stage s and o. The function g(xs, 0) represents the application of (o - s) sets of strided convolutions, facilitating the integration of features across different scales.\nAs depicted by orange arrows in Fig. 1, this cross-scale integration method, described by Eq. (9), plays a crucial role in enhancing the model's capability to preserve higher-resolution information through the network stages. This approach mitigates the loss of detail due to compression and improves the model's overall accuracy of the model in medical image analysis."}, {"title": "3.7. Loss Function", "content": "To evaluate the accuracy of the predicted segmentation mask against the ground truth in medical image segmentation tasks, we employed both cross-entropy and Dice losses, which are effective for voxel-level classification. The efficacy of combining these two losses has been well-documented in medical imaging research, as outlined by (Milletari et al., 2016).\nThe cross-entropy loss, which assesses the discrepancy between predicted probabilities and actual labels, is defined as:\n$L_{CE} = - \\frac{1}{T} \\sum_{k=1}^{T}\\sum_{c=1}^{Q} y_{k,c} log_{2}(p_{k,c})$ (10)\nwhere T denotes the total number of input images, Q denotes the number of classes, yk,c is the binary indicator for class membership, and pk,c is the predicted probability that the kth voxel belongs to the cth class.\nThe Dice loss, aimed at quantifying the similarity between the predicted and actual segmentations, is mathematically expressed as:\n$L_{Dice} = - \\frac{2\\sum_{k=1}^{T} p_{k,c} y_{k,c}}{\\sum_{k=1}^{T} p_{k,c} + \\sum_{k=1}^{T} y_{k,c}}$ (11)\nTo compute the total segmentation loss, we sum the Dice and cross-entropy losses:\n$L_{total} = L_{CE} + L_{Dice}$ (12)"}, {"title": "4. Experimental Results", "content": null}, {"title": "4.1. Evaluation Protocol", "content": null}, {"title": "4.1.1. Dataset", "content": "In this study, we assessed the effectiveness of MobileViMs and compared it with seven other leading-edge models using four benchmark datasets: PENGWIN (Liu et al., 2023b), BraTS2024 (LaBella et al., 2024), ATLAS (Quinton et al., 2023), and ToothFairy2 (Lumetti et al., 2024).\nPENGWIN Dataset: This dataset includes 100 pelvic computer tomography (CT) scans that prominently feature sacrum and hipbone fragments, enabling detailed analysis of pelvic structures.\nBraTS2024 Dataset: With 500 post-contrast MRI scans of the brain, this dataset is the third task of BraTS2024 challenges and is tailored for the automated segmentation of meningioma gross tumour volumes, offering extensive data for brain tumour analysis.\nATLAS Dataset: Comprising 90 T1 contrast-enhanced magnetic resonance imaging (CE-MRI) scans, this dataset is focused on liver tumours. It provides a basis for evaluating organ-specific tumour detection and segmentation capabilities.\nTooth Fairy2 Dataset: This collection consists of 480 cone beam computer tomography (CB-CT) scans, divided into 43 different classes representing various anatomical features and dental structures, including the jaws, maxillary sinus, pharynx, and dental restorations like bridges, crowns, and implants.\nFor a thorough and rigorous evaluation, all datasets were partitioned into training and testing subsets with a ratio of 4:1. This setup ensures that our model's performance is measured accurately across different medical imaging modalities and anatomical challenges."}, {"title": "4.1.2. Implementation Details", "content": "This research utilised an AMD Ryzen 9 7950X CPU and an NVIDIA RTX 4090 GPU to conduct experiments. We trained the segmentation models using Dice and cross-entropy loss functions, and optimisation was carried out with the AdamW optimiser (Loshchilov and Hutter, 2017). The models were trained with a mini-batch size of four. We employed several data augmentation techniques to enhance model robustness, including sampling foreground and background patches and applying random transformations consisting of rotating and flipping. The initial learning rate was set to 1.6\u00d710-6, which decayed to 1.6\u00d710-7, throughout 100 epochs, following a cosine annealing schedule (Loshchilov, Ilya and Hutter, Frank, 2016). To ensure reliability, results were averaged over three separate training and testing cycles. All models were evaluated under these standardised conditions. For the models in the control group, any unspecified configurations adhered to their respective official implementations. The experimental code was implemented using the PyTorch (Paszke et al., 2019) framework."}, {"title": "4.1.3. Evaluation Metric", "content": "To comprehensively assess the semantic segmentation performance of the models under study, we employed a variety of metrics. The complexity of each model was gauged by the number of parameters, denoted as # Params and expressed in millions. Additionally, we quantified the computational demand of each model using multiply-accumulate operations (MACs), reported in billions, and evaluated real-world usability by measuring inference speed in frames per second (FPS). For a precise assessment of voxel-level accuracy, we utilised the mean Dice similarity coefficient (Dice), crucial for evaluating the segmentation precision in medical imaging contexts. Furthermore, the root mean square error (RMSE) was employed to evaluate the discrepancies between the predicted volumes and the ground truth. These metrics together provide a detailed evaluation framework, enabling the measurement of segmentation accuracy and effectiveness across different imaging applications."}, {"title": "4.2. Results for Medical Image Segmentation", "content": "To assess the efficacy of MobileViMs in processing 3D data", "categories": "small\" for models with fewer than 7 million parameters, \u201cmedium\" for those with 7 \u2013 35 million parameters, and \u201clarge\u201d for those exceeding 35 million parameters.\nAs indicated in Fig. 5, MobileViMs are positioned in the top-left region, demonstrating superior performance relative to other SOTA models with a comparatively minimal parameter count. For instance, MobileViM_s utilised only 6.29 million parameters and 195.56 billion MACs, but it recorded the highest Dice scores of 86.69% and 80.46% for the BraTS2024 and ATLAS datasets, respectively. MobileViM_s also achieved the second-highest Dice scores of 92.72% and 77.43% for the PENGWIN and ToothFairy2 datasets, respectively. Moreover, MobileViM_s outperformed SegMamba, which also features Mamba modules, by more than 1.83%, +7.03%, +3.33%, and +0.99% in Dice scores across the four datasets, respectively.\nDespite nnUNet and UNet++ obtaining the highest Dice scores of 93.05% and 79.30% in the PENGWIN and Tooth-Fairy2 datasets, respectively, their performance was limited by lower frame rates (<25 FPS) and higher parameter count (>31 million), reflecting significant resource consumption. According to Tab. 2, the inference speeds of MobileViMs are over 90 FPS, more than 20 FPS faster than other SOTA models. Given their high speeds, MobileViMs are suitable for clinical diagnostics involving 3D medical imaging, such as CT and MRI scans. In contrast, models like UNet++, Segmamba, 3DUX-Net, SwinUNETR-V2, and nnUNet consume over 790 billion MACs and operate below 25 FPS in recognising 3D images, as illustrated in the yellow or red regions of Fig. 5.\nFurthermore, the smallest model, MobileViM_xs, with only 2.89 million parameters, managed to secure the second-best Dice scores of 86.18% in the BraTS2024 dataset for brain tumours and 79.65% in the ATLAS dataset for liver cancers, with a rapid inference speed of 94 FPS. The aforementioned results emphasise the efficacy and adaptability of MobileViMs in processing and diagnosing 3D medical images across diverse medical domains.\nFurther analysis detailed in Tab. 2 reveals that Mobile-ViM_s achieved the lowest RMSE of 1.52\u00d710-\u00b9 in the ATLAS dataset and the third-lowest RMSE of 2.42\u00d710-2 in the BraTS2024 dataset. MobileViM_s also obtained competitive RMSE of 1.30\u00d710-1 and 2.05 in the PENGWIN and Tooth-Fairy2 datasets, respectively, only 15% higher than the top-performing models in these respective datasets. Moreover, the smallest model, MobileViM_xs recorded RMSE of 1.42\u00d710-1, 2.44\u00d710-2, 1.60\u00d710-\u00b9, and 2.07 in the PENGWIN, BraTS2024, ATLAS, and ToothFairy2 datasets respectively. These results highlight the capability of MobileViMs to delineate regions of interest in 3D medical images with significantly low error rates.\nTo further analyse the model's performance across different classes within the datasets, we focus on the results in the PWENGWIN dataset. As illustrated in Tab. 3, MobileViM_s outperformed other SOTA methods in specific anatomical areas, achieving Dice scores of 92.15% for left hipbones and 92.18% for right hipbones. In addition, MobileViM_s recorded a Dice score of 86.76% in identifying sacrums, which is -6.80% lower than the best results achieved by UNet++. Furthermore, all evaluated methods consistently delivered Dice scores over 99.70% in distinguishing the background in CT scans of pelvic fractures.\nTo visualise the Dice score distributions, violin plots for the first three classes - left hipbone, right hipbone, and sacrum are created and presented in Fig. 6. While UNet++ showed a tight clustering around the highest median in the sacrum class, it exhibited a wider spread with comparatively low medians in the left and right hipbone classes, suggesting imbalanced performance and instability across three classes. The relatively broad bases of the violin plots for 3DUX-Net, Sw"}]}