{"title": "Q-SFT: Q-LEARNING FOR LANGUAGE MODELS VIA SUPERVISED FINE-TUNING", "authors": ["Joey Hong", "Anca Dragan", "Sergey Levine"], "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies\nfor a wide range of multi-turn problems, from games to dialogue to robotic control,\nincluding via offline RL from static previously collected datasets. However, despite\nthe widespread use of policy gradient methods to train large language models for\nsingle turn tasks (e.g., question answering), value-based methods for multi-turn RL\nin an off-policy or offline setting have proven particularly challenging to scale to\nthe setting of large language models. This setting requires effectively leveraging\npretraining, scaling to large architectures with billions of parameters, and training\non large datasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses these\ndrawbacks, casting Q-learning as a modified supervised fine-tuning (SFT) problem\nwhere the probabilities of tokens directly translate to Q-values. In this way we\nobtain an algorithm that smoothly transitions from maximizing the likelihood of the\ndata during pretraining to learning a near-optimal Q-function during finetuning. Our\nalgorithm has strong theoretical foundations, enjoying performance bounds similar\nto state-of-the-art Q-learning methods, while in practice utilizing an objective that\nclosely resembles SFT. Because of this, our approach can enjoy the full benefits of\nthe pretraining of language models, without the need to reinitialize any weights\nbefore RL finetuning, and without the need to initialize new heads for predicting\nvalues or advantages. Empirically, we evaluate our method on both pretrained\nLLMs and VLMs, on a variety of tasks including both natural language dialogue\nand robotic manipulation and navigation from images.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, some of the most impressive feats in AI have been performed through language models,\nwhich are pretrained on large-scale data and adapted to a wide range of downstream tasks (Bommasani\net al., 2021). Many of these tasks, such as natural language dialogue or robotic control, require\ncomplex sequential decision-making. Reinforcement learning (RL) Sutton & Barto (2018) is a\npowerful paradigm for solving such tasks (Mnih et al., 2013; Silver et al., 2017; AlphaStar, 2019).\nFurthermore, offline RL Levine et al. (2020) has been shown to do so from only static datasets, such\nas suboptimal demonstrations from any unknown behavior policy, without the need for any additional\ninteraction. Though offline RL has been used to fine-tune large language models (LLMs) or vision\nlanguage models (VLMs) (Ouyang et al., 2022; Bai et al., 2022b), its usefulness has been limited\nto generating better single responses rather than multi-turn, sequential scenarios where RL should\ntheoretically shine. For example, across various dialogue tasks, offline RL fine-tuning of LLMs\ndoes not reliably outperform supervised fine-tuning (SFT) (Sodhi et al., 2023; Abdulhai et al., 2023).\nFurthermore, in the realm of navigation and control, popular VLMs are still fine-tuned for multi-task\ncontrol using SFT (Brohan et al., 2023b;a; Collaboration et al., 2024).\nSingle-step problems, such as answering questions, can be tackled with policy gradient methods\n(Ouyang et al., 2022; Rafailov et al., 2023), but sequential problems, such as dialogue or robotic\ncontrol, require sample-efficient methods that can utilize data to reason about the dynamics of the\nproblem, which typically requires training value functions (Abdulhai et al., 2023; Hong et al., 2023).\nAlthough there are many effective value-based RL methods that could be applied to LLMs and VLMs,\nin practice such methods have been difficult to adapt to these models with the same effectiveness as"}, {"title": "2 RELATED WORK", "content": "Much of the recent work on reinforcement learning (RL) finetuning of LLMs and VLMs uses policy\ngradient methods and reward models learned from human feedback (e.g., RLHF) (Ziegler et al., 2020;\nStiennon et al., 2020; Wu et al., 2021; Nakano et al., 2022; Bai et al., 2022a; Christiano et al., 2023;\nRafailov et al., 2023), or from handcrafted AI systems (e.g., RLAIF) (Bai et al., 2022b), to generate\nbetter responses to various queries. However, there is a large discrepancy in the capabilities required\nto perform self-contained responses in single-step tasks, such as question-answering, and responses in\na multi-turn scenarios, such as dialogue. Namely, the latter requires planning to optimize a long-term"}, {"title": "3 PRELIMINARIES", "content": "Our work proposes a new RL algorithm for fine-tuning language models, specifically for multi-\nturn tasks such as dialogue or manipulation and control. Language models operate over a discrete\nvocabulary of tokens V, and are trained to maximize the likelihood the best next-token $x_{m+1}$ given\nan input sequence $(x_0,...,x_m)$ of tokens, given by $\u03c0(x_{m+1}|x_0,...,x_m)$. In a multi-turn task\nsuch a dialogue, the tokens are words that are chained to form utterances, and the best next-token\nrequires complex, sequential reasoning to understand the utterances so far and plan for the next one.\nTraditionally, this kind of reasoning can be learned via reinforcement learning (RL).\nRL fundamentals. RL aims to optimize agents that interact with a Markov Decision Process (MDP)\ndefined by a tuple $(S, A, P, r, \u03bc_1, \u03b3)$, where S represents the set of all possible states, A is the set of\npossible actions, $\u03bc_1$ is the initial state distribution, and \u03b3 is the discount factor. When action a \u2208 A is\nexecuted at state s \u2208 S, the next state is generated according to $s' ~ P(\u00b7|s, a)$, and the agent receives\nstochastic reward with mean $r(s, a) \u2208 [0, 1]$.\nThe Q-function $Q^\u03c0(s, a)$ for a policy \u03c0(s) represents the discounted long-term reward attained\nby executing a given observation history s and then following policy \u03c0 thereafter. $Q^\u03c0$ satisfies the"}, {"title": "4 Q-LEARNING VIA SUPERVISED FINE-TUNING", "content": "We will now describe our proposed offline RL algorithm, which we dub Q-learning via Supervised\nFine-tuning (Q-SFT). Concretely, instead of training value functions by fitting Q-values to their\nBellman backup target via a regression loss, we instead fine-tune directly on the probabilities learned\nfrom large-scale pretraining \u2014like in SFT\u2014 via a weighted cross-entropy loss, such that the resulting\nprobabilities also capture the desired Q-values."}, {"title": "4.1 LEARNING VALUES AS PROBABILITIES", "content": "Recently, large neural networks such as LLMs and VLMs have been successfully trained and fine-\ntuned on demonstration data using supervised learning. If we adopt the earlier multi-turn formalism\nin Section 3 and view these models as agents, such approaches train a policy $\u03c0_\u03b2(a|s)$ with parameters\nby minimizing cross-entropy loss:\n$L_{CE}(\u03c6) = E_{(s,a)~D} [log \u03c0_\u03c6(a | s)] .$\nBecause the resulting policy approximates the behavior policy $\u03c0_\u03c1(a|s) \u2248 \u03c0_\u03c1(a|s)$, this approach has\nalso been dubbed behavioral cloning (BC). While BC scales well to complex tasks and networks, the"}, {"title": "4.2 PRACTICAL IMPLEMENTATION", "content": "Our objective in Equation 3 trains the model such that the predicted token probabilities match their\nQ-values. Our final step is to choose how to use these probabilities in the final learned policy. Prior\nwork performs policy extraction that learns a policy regularized to be similar to the behavior policy\n(Peng et al., 2019; Kostrikov et al., 2021). Namely, it is well-known that a policy parameterization\n$\u03c0(a|s) \u03b1 \u03c0_\u03b2(a|s) exp(Q^* (s, a))$ is a solution to the constrained optimization problem (Peng et al.,\n2019; Brandfonbrener et al., 2021)\n$argmax_{\u03c0} E_{s~d^*_\u03b2,a\u223c\u03c0} [Q^*(s,a)]  s.t. E_{s~d^*_\u03b2} [D_{KL}(\u03c0(\u00b7 | s) || \u03c0_\u03b2(\u00b7 | s))] \u2264 \u03b5.$\nRecall that we have already approximated $\u03c0_\u03b2$ by optimizing an $L_{CE}(\u03c6)$ over parameters \u03c6. Therefore,\nwithout any further training we can extract a policy whose probabilities of actions (a | s) follow\n$\u03c0(a|s) \u03b1 \u03c0_\u03c6(a|s) exp (p_\u03b8(a|s))$, an expression of both our learned probabilities $p_\u03b8$ and estimated\nbehavior policy $\u03c0_\u03c6$. Note that unlike prior works that explicitly require a policy extraction step with\nadditional training, our policy can be computed at inference-time. Namely, we can express our policy\nusing only the probabilities that we had previously learned. An overview of our entire method is\nprovided in Algorithm 1."}, {"title": "4.3 THEORETICAL ANALYSIS", "content": "To simplify exposition, we consider a modified version of our objective in Equation 3, where instead\nof empirical operator $B^*$, we use the true operator:\n$B^*p_\u03b8(a|s) = r(s, a) + E_{s'~P(\u00b7|s,a)}[max_{a'} \\frac{p_\u03b8(a' | s')}{\u03c0_\u03b2(a' | s')}]$.\nNote that it is simple to adapt our analysis to the empirical operator. Namely, it requires obtaining\nhigh-probability bounds of the form $P(s, a), B^*p_\u03b8 (a | s) \u2013 B^*p_\u03b8 (a | s) \u2264 \\frac{C}{\\sqrt{n(s,a)}}$ where C is\na constant independent of \u03b3. This kind of inequality commonly arises in analysis of offline RL\nalgorithms (Kumar et al., 2020; 2022).\nOur main theoretical result is that our learned probabilities satisfy being conservative estimates of the\ntrue value function:\nTheorem 4.1. Let $p_\u03b8$ be the likelihood function that arises from optimizing Equation 3 using the true\nBellman likelihood operator. Then, $p_\u03b8$ satisfies\n$Q^*(s, a) \u2265 p_\u03b8(s, a) \u2265 Q^*(s, a)\u03c0_\u03b2(a|s)$,\nfor all s \u2208 D and a \u2208 A such that $Q^*(s, a) \u2265 \\frac{1}{|A|}$.\nWe defer proof of the theorem to Appendix A. Note that our probabilities are conservative only over\nactions that have non-negligible Q-values. In practice, we do not see this as a problem as actions with"}, {"title": "5 EXPERIMENTS", "content": "Our method combines aspects of both SFT and value-based RL training, and we therefore compare\nour method to state-of-the-art methods from both classes, evaluating:\n(1) Whether our method improves over SFT methods by taking into account and optimizing\nover a multi-step task reward.\n(2) Whether our method improves on the stability and performance of previously proposed\nvalue-based RL methods for training LLMs and VLMs.\n(3) Whether our method is better able to benefit from the pretraining of large models than\npreviously proposed multi-turn RL methods.\nIn this section, we perform a comprehensive empirical evaluation across a suite of different tasks to\nfind positive answers to all the above questions."}, {"title": "5.1 TASK DESCRIPTIONS", "content": "Contrary to many existing applications of RL on language models, such as RLHF (Ouyang et al.,\n2022) or DPO (Rafailov et al., 2023), our proposed algorithm is tailored for offline RL on multi-step\ntasks. Therefore, we consolidate a variety of existing benchmarks where a language model must\nmake sequential decisions, arriving at the following suite of different tasks.\nThe first set of tasks include language games from the LMRL benchmark (Abdulhai et al., 2023),\nwhich is one of the first benchmarks tailored at evaluating offline RL for language generation.\nChess. This task uses a textual representation of the game of chess. The offline dataset consists of\ntrajectories by Stockfish 15.1 simulating various player strengths as the agent, playing against another\nStockfish engine with Elo 1200. The reward is 1 for a move that results in victory, 0 for a legal move\nand -1 for an illegal move. Our dataset consists of 625K trajectories of full games, in which the agent\nachieves an average return of 0.21.\nWordle. In the game of Wordle, the agent is given at most 6 attempts to guess a hidden 5-letter word.\nAfter each guess, the agent is told whether each letter in the guessed word is: (1) in the hidden word\nand in the right position (green), (2) in the hidden word but not in the right position (yellow), or (3)\nnot in the hidden word (gray). The agent receives a reward of -1 after each incorrect guess. The\ndataset consists of 20K trajectories by a suboptimal heuristic policy that achieves an average return\nof -4.12, originally collected by Snell et al. (2022).\nTwenty Questions. The final language task is the dialogue game of twenty questions, where the\nagent tries to guess what a hidden object is by asking a series of yes-or-no questions. The dataset\nconsists of 100K conversations between an agent that is a guesser and the oracle that chooses the\nhidden word. The oracle chooses the hidden work uniformly at random from 158 unique objects. The\nguesser and the oracle are both simulated using GPT3.5 (OpenAI, 2022), which is prompted to both\ngenerate questions and answer them factually. The agent receives a reward of -1 for each question\nthat is not a correct guess, up to a minimum return of -20. The average return in the dataset is -17.3.\nThe next evaluation for fine-tuning LLMs as language agents interactive web-based tasks that\nrequire using tools like search.\nWebShop. an online shopping website environment where an agent processes unstructured text data\n(in the form of descriptions crawled from Amazon) to purchase a product given some initial user\nspecifications. At the end, the agent receives a reward between 0 and 1 depending on the similarity\nbetween the purchased and ground-truth desired item. The benchmark consists of 12k initial user\ninstructions, of which we randomly held out 100 for evaluation. With the remaining instructions, we\ngenerate a dataset of trajectories where we simulate an suboptimal agent by prompting GPT3.5 with\nfew-shot examples, following the prompts used by Yao et al. (2022).\nOur method can be applied not only to language models, but also to multimodal models. In the next\nexperiment, we study the performance of our method on vision-based navigation with VLMs.\nALFWorld. This is a popular text-based environment grounded in image observations (Shridhar\net al., 2021). In this environment, the agent is tasked with solving one of 6 different task types,\nranging from finding, moving, and manipulating different household objects within an embodied\nenvironment of 120 rooms. At each timestep, the agent observes a textual description of its location\nand surroundings with an analogous image, and chooses a text action from a set of admissible\nactions. In this environment, we sample 10k trajectories consisting of a random templated task\ndescription, and an attempted execution of the task within 30 timesteps by a prompted GPT3.5\nmodel for data collection. In the dataset, the agent only successfully accomplishes the task 34% of\nthe time aggregated across all task types.\nFinally, we also evaluate our method for training policies outside of language generation. Robotics\nis a popular domain in which offline RL has been proven effective for training per-token Q-values\nfor continuous control (Singh et al., 2020; Chebotar et al., 2023). In these experiments, we do not\nleverage pretrained language models and simply test the effectiveness of the underlying RL algorithm.\nRobotic manipulation. We consider the large-scale robotic manipulation control tasks from Singh\net al. (2020). In the environment, the agent controls a 7-DoF mobile manipulator in front of a\ncountertop surface to perform two types of tasks: pick up an object and place on the trap in front,"}, {"title": "5.2 RESULTS", "content": "The goal of our empirical results is to show positive answers to all the proposed research questions.\nIn order to do so, we evaluate multiple state-of-the-art supervised and value-based RL methods.\nEvaluation. We compare our method Q-SFT against three classes of competing algorithms:\nPrompting: ReAct (Yao et al., 2022) is an extension of chain-of-though prompting (Wei et al., 2023),\nwhere the pretrained language model is prompting to think and reason multiple steps in advance. We\nuse GPT3.5 (OpenAI, 2022) as the LLM, and GPT4-V (OpenAI, 2023) as the VLM.\nSupervised learning: Supervised fine-tuning (SFT) on the offline dataset. In the case of using\nnon-pretrained models, such as in the robotics task, we rename the method as behavior cloning (BC).\nValue-based RL: Traditional offline RL algorithms that perform Q-learning to learn value functions.\nWe consider several different algorithms, depending on the task at-hand. In the case of language\ngames and ALFWorld, we evaluate ILQL (Snell et al., 2023), a popular approach for language\ngeneration. In WebShop, we consider an offline variant of ArCHer (Zhou et al., 2024) that performs\nbest on the task from prior work. Finally, in robotics manipulation, we consider both CQL (Kumar\net al., 2020) and Q-transformer (QT) (Chebotar et al., 2023), which are both popular and achieve\nstate-of-the-art performance in continuous control.\nSince the state-of-the-art LLMs and VLMs often only expose inference APIs, we instead train our\nconsidered methods on the GPT2-medium LLM, which consists of 345M parameters (Radford\net al., 2019). For ALFWorld, which requires VLMs, we use LLAVA-1.6 model as the pretrained\nmodel (Liu et al., 2023). Finally, for robotics, we use a randomly initialized Transformer architecture\nmodeled after the popular RT-1 model, which processes images and discretizes the action space\ninto tokens (Brohan et al., 2023b). Note that for the Chess and Wordle tasks, because their state\nand action space are unlike natural language, we replace the pretrained weights with a random"}, {"title": "6 DISCUSSION", "content": "In this paper, we present Q-learning via Supervised Fine-Tuning (Q-SFT), a new offline RL algorithm\nwhere Q-values are learned as probabilities in an objective that looks like supervised fine-tuning.\nBecause of this, our objective can be directly optimized over the logits of pretrained LLMs or VLMs\nTo our knowledge, this is the first algorithm that can perform value-based RL fine-tuning without\nrequiring any changes to the architecture, such as adding in new value heads. This has a number\nof important benefits. First, our objective is an instance of weighted cross-entropy, which has been\nshown by prior works to be more stable to train than traditional value-based RL methods that require\nregression towards non-stationary target values. More importantly, our algorithm fully leverages the\nadvantage of foundation models such as LLMs or VLMs, as our algorithm starts from the pretrained\nprobabilities, as opposed to randomly-initialized values. Theoretically, we show that our probabilities\nare conservative estimates of the true value function. Empirically, we compare our approach against"}, {"title": "A THEORETICAL PROOFS", "content": "Here, we provide a proof of Theorem 4.1. Recall that we are optimizing the following objective:\n$L_{WCE}(p) = E_{(s,a,r,s')~D} B^*p(a | s) log p(a | s) + \\sum_{a'\u2260a} \\frac{1 \u2013 B^*p(a | s)}{|A| \u2013 1} log p(a' |\ns) .$\nLet us consider iteration k of training. Setting the derivative of Equation 4 to 0, we obtain the\nfollowing expression of $p^{*+1}$ in terms of $p^*$:\n$\u2200s \u2208 D, a \u2208 A, p^{k+1}(a | s) = \u03c0_\u03c1(\u03b1 | s)B^*p^*(a | s) + \\sum_{a'\u2260a} \u03c0_\u03c1(\u03b1' | s) \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}.$\nLower-bound. We will first show the lower-bound part of Theorem 4.1. Rearranging the above\nequation, we see that:\n$\\frac{p^{k+1}(a | s)}{\u03c0_\u03c1(\u03b1 |\u03c2)} = B^*p^* (a | s) + \\sum_{a'\u2260a} \\frac{\u03c0_\u03c1(\u03b1' | s)}{\u03c0_\u03c1(\u03b1 |\u03c2)} \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}.$\nHence, we see that\n$\\frac{p^{k+1}(a | s)}{\u03c0_\u03c1(\u03b1 | \u03b4)} > B^*p^* (a | s) = r(s,a) + E_{s'~P(\\cdot|s,a)}[max_{a'} \\frac{p^k (a' | s')}{\u03c0_\u03c1(\u03b1' | \u03c2')}],$ where we substitute the definition of $B^*$. Finally, taking the fixed point of the above expression yields,\n$\\frac{p(a | s)}{\u03c0_\u03c1(\u03b1 |\u03c2)} > Q^*(s, a) \u21d2 p(a | s) \u2265 \u03c0_\u03c1(\u03b1 | s)Q^*(s, a)$,\nas desired.\nUpper-bound. Now, we show the upper-bound part of Theorem 4.1. Assume that\n$B^*p^* (a | s) \u2265 \\frac{1}{|A| \u2013 1}.$\nThen, we can solve for the bound:\n$(1 \u2013 \u03c0_\u03c1(\u03b1 | s)) B^*p^* (a | s) \u2013 \\sum_{a'\u2260a} \u03c0_\u03c1(\u03b1' | s) \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}$\n$= \\sum_{a'\u2260a} \u03c0_\u03c1(\u03b1 | s) [B^*p^* (a | s) \u2013 \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}]$\n$\u2265 \\sum_{a'\u2260a} \u03c0_\u03c1(\u03b1 | s) (\\frac{1}{|A| \u2013 1} \u2013 \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}) \u2265 0.$\nThis means that we have,\n$p^{k+1}(a | s) = \u03c0_\u03c1(\u03b1 | s)B^*p^*(a | s) + \\sum_{a'\u2260a} \u03c0_\u03c1(\u03b1' | s) \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}$\n$= B^*p^* (a | s) \u2013 (1 \u2013 \u03c0_\u03c1(\u03b1 | s)) B^*p^*(a | s) + \\sum_{a'\u2260a} \u03c0_\u03c1(\u03b1' | s) \\frac{1 \u2013 B^*p^*(a' | s)}{|A| \u2013 1}$\n$ < B^*p^* (a | s) .$\nHence, we see that\n$\\frac{p^{k+1}(a | s)}{\u03c0_\u03c1(\u03b1 | s)} < \\frac{B^*p^* (a | s)}{\u03c0_\u03c1(\u03b1 | s)} = \\frac{1}{\u03c0_\u03c1(\u03b1 | s)} (r(s,a) + E_{s'~P(\\cdot|s,a)} [max_{a'} \\frac{p^*(a' | s')}{\u03c0_\u03c1(\u03b1' | \u03c2')}].$\nFinally, taking the fixed point of the above expression yields the desired $p(a | s) \u2264 Q^*(s, a)$. This\ncompletes the proof."}]}