{"title": "Comparing Native and Non-native English Speakers' Behaviors in Collaborative Writing through Visual Analytics", "authors": ["Yuexi Chen", "Yimin Xiao", "Kazi Tasnim Zinat", "Naomi Yamashita", "Ge Gao", "Zhicheng Liu"], "abstract": "Understanding collaborative writing dynamics between native speakers (NS) and non-native speakers (NNS) is critical for enhancing collaboration quality and team inclusivity. In this paper, we partnered with communication researchers to develop visual analytics solutions for comparing NS and NNS behaviors in 162 writing sessions across 27 teams. The primary challenges in analyzing writing behaviors are data complexity and the uncertainties introduced by automated methods. In response, we present COALA, a novel visual analytics tool that improves model interpretability by displaying uncertainties in author clusters, generating behavior summaries using large language models, and visualizing writing-related actions at multiple granularities. We validated the effectiveness of COALA through user studies with domain experts (N=2+2) and researchers with relevant experience (N=8). We present the insights discovered by participants using COALA, suggest features for future AI-assisted collaborative writing tools, and discuss the broader implications for analyzing collaborative processes beyond writing.", "sections": [{"title": "1 Introduction", "content": "Non-native speakers (NNS) actively engage in collaborative writing across various contexts: international students write reports with classmates [13] and advisors [107]; Wikipedia contributors edit articles in different languages [30]; employees in multi-national corporations collaborate on project pages [4]. Previous research shows that when writing in a non-native language, NNS tend to produce shorter and less complex content compare to writing in their native language [14, 93, 101]. However, limited research has examined the collaborative writing processes involving NNS and how they write differently from native speakers (NS) [104]. Such knowledge will be insightful in enhancing collaboration outcomes and fostering inclusive team dynamics involving NNS.\nTo bridge this gap, in collaboration with communication researchers, we collected document history and screen recordings of 162 collaborative writing sessions from 27 teams. We are interested in comparing authors' behaviors across different linguistic backgrounds and writing stages. For example, what are the common behavior patterns of NS and NNS, respectively? How do they differ in the early and late stages of collaborative writing?\nHowever, existing document visualization and analytics tools fall short of supporting our analysis goal. Visualizations of document versions have long been used to investigate the dynamics of collaborative writing. For example, History Flow [96] uses a Sankey diagram to encode content contributions from different Wikipedia authors across different versions; Time Curves [5] project different document versions on a 2D curve based on their similarity and temporal order. However, such visualizations focus on the document content instead of the writers' behaviors during the writing process, such as browsing the internet or using translation tools. Event sequence visualization tools are better suited for behavioral data but still do not effectively meet our needs. For example, TipoVis [31] allows comparisons between two sequences at a time, while our analysis requires comparing behavior sequences across multiple authors and sessions. CoCo [60] supports comparing event sequences belonging to different cohorts, but focuses primarily on aggregated metrics such as frequencies. In our case, we need to identify behavioral differences between NS and NNS at multiple levels of granularity with meaningful qualitative descriptions."}, {"title": "2 Related Work", "content": "Collaborative writing has been a topic of interest since the 1980s [6, 21, 22]. Early research focused on awareness and coordination during collaborative writing [20], common writing tasks, and the number of collaborators [42]. The rise of online collaborative writing tools like Google Docs, Microsoft Word, and Overleaf [71] has made collaborative writing a common practice. For instance, Olson et al. [69] analyzed collaborative writing patterns in 96 college assignments in Google Docs and found that balanced participation and leadership would result in higher writing quality [69]. Researchers also explored various aspects of collaboration, such as impression management [7], reluctance to write closely [98], preference over edits with explanations [72], differences of tasks across writing stages [81], and territorial behaviors [50].\nFew studies focus on authors' off-document writing-related activities during collaborative writing, for example, navigating multiple applications (e.g., Google Docs and Adobe InDesign) [51] and coordinating writing tasks on Wikipedia discussion pages [82]. Collaborated with communication researchers, we focus on writing-related behaviors, including off-document behaviors like using a translator and browsing the internet, aiming to provide a new perspective on collaborative writing analysis."}, {"title": "2.2 Text visual analytics", "content": "Several visual analytics approaches have been designed to analyze the evolution of documents in collaborative writing. Itero [92] is a revision history analytics tool based on Google Docs that visualizes character insertion patterns and user contributions. History Flow [96] and DocuViz [97] encode each author's contribution as a colored vertical line, with the height of the line proportional to the content length. The flow-like visualization reveals the cooperation and conflict among co-authors by connecting the same line across different versions. Graphs are also widely used, where authors are represented as nodes, and edges could be disagreement [23] or revert actions [45]. Time Curves [5] is a timeline visualization based on points' similarity, which could visualize different document versions. Other visualizations include branch-based visualizations [75], revision maps [87] and color-coded words by authorship [24, 91]. Compared to text visual analytics approaches, COALA focuses on sequences of writing-related behaviors."}, {"title": "2.3 Non-native speakers vs. native speakers", "content": "Compared to native speakers (NS), non-native speakers (NNS) usually produce shorter and less complicated content and have difficulty transferring writing strategies from their mother tongue [14, 93, 101]. Though NNS need more help in the expression aspects [84], they may still contribute to the ideation aspect [26]. Cheng et al. [13] found in a case study that NS students had more power in collaborative writing at the beginning, but the NNS student developed academic literacy along the way, and overall the group writing experience has improved. NNS' writing could also be improved by receiving direct edit feedback at early versions [39, 107], or exposure to well-written model text by NS [38]. Compared to these previous case studies, we have a larger collaborative writing dataset of NS-NNS with video-recorded author behaviors, poised to reveal more patterns beyond anecdotal evidence."}, {"title": "2.4 Event sequence analysis", "content": "There are numerous methods to analyze event sequences. Besides visualizing sequences, we categorize analysis methods based on tasks: comparing, clustering and summarizing.\nVisualizing event sequences. The most straightforward visualization design for event sequences is to arrange the events on a timeline [47, 76]. When the number of sequences is large, flow-based visualizations could show the trend of bundled sequences. For example, Sankey diagrams represent each event as a node, the length of the node and the thickness of the links between nodes encode event frequencies [74, 102]. Tree-based visualizations encode the frequency of events as the thickness of edges [34, 56]. Like tree-based visualizations, icicle plots encode events as stacked rectangles, ordered from top to bottom, usually colored by event categories [57, 103]. When subsequences are highly repetitive, matrix-based visualizations could show the transition trend clearly [73, 111].\nComparing event sequences. Multiple tools focus on comparison. CoCo compares two patient cohorts via statistical analysis with built-in metrics [60] distilled from domain expertise [64]. TipoVis compares event sequences of social and communicative behaviors by overlaying two sequences [31]. COQUITO [46] assists"}, {"title": "3 Study Background", "content": "We collaborated with two communication researchers from a public university in the US. One is a professor who has studied multilingual communication for more than a decade, and the other is a Ph.D. advisee of the professor who also has rich experience in multilingual communication. They collected a dataset of collaborative writing between native and non-native speakers and contacted us for suggestions in visual analytics.\nWe conducted longitudinal co-design sessions with our collaborators to understand the communication research analysis better. We met weekly or bi-weekly for 30 weeks. After they introduced the study background and data, we initially adapted existing text visualizations like Time Curves [5] and History Flow [96] (see Appendix). Though such text visualizations provide a glimpse of how authors contribute to the document, and how co-authors revise or delete each others' contribution, they do not address the research questions to compare authors' behaviors, so we designed dedicated features for analyzing authors' behavioral sequences. During the process, we showed them visualizations and interface mockups, incorporated the feedback into the next iteration, and finalized the visualization and interaction designs with them."}, {"title": "3.2 Data collection", "content": "Our collaborators recruited 29 native English speakers (NS) and 29 non-native English speakers (NNS) from an American university and a Japanese university for an online collaborative writing study.\nTo ensure NNS are of similar English proficiency, all NNS are native Japanese speakers with limited working proficiency in English [1]. Participants are asked to act as if they were columnists for an English magazine who answer readers' questions about the role of technology in modern life. The topics include social media, remote learning, and digital privacy. To mitigate the influence of topic familiarity, one NS and one NNS are paired to form a writing group and are assigned a topic familiar to both authors. All participants are provided with preset Google accounts and links to blank Google Docs. NNS and NS of the same group do not know each other but are informed about co-authors' language proficiency. Participants are also asked to record their screen during writing. Participants are welcome to use any tools (e.g., search engines, Google Translate, Grammarly) that they normally use during writing. Since the study was conducted before ChatGPT's release, no participant used generative AI tools.\nThen, NNS and NS take turns writing an English essay jointly. The turn-taking setup is shown in Figure 1: first, each author writes independently (turno), ensuring they have actively thought about the task instead of being a free-rider. Then, NS review the write-ups of both authors from turno and merges them into a single document (turn\u2081). During turn\u2081, NS can add/delete/edit any content. Next, NNS revise the joint document turn\u2082, followed by NS turn\u2083, and finally concluded by NNS turn\u2084. Participants are allowed up to 1 hour for each turn, and they could finish early if they have nothing more to contribute. After removing two teams that did not follow instructions, we have 27 remaining.\nOur collaborators carefully designed the setup of the above experiment. First, though current online writing tools support simultaneous writing, turn-taking is still a popular workflow adopted by co-writers in practice, as they minimize the burden of syncing content mentally [8], protect authors' territoriality [50] and thus promote editing other co-authors' writing [3]. Second, NNS may face difficulties identifying opportunities for contribution in flexibly structured collaboration with NS. Previous research has introduced several techniques to interrupt the natural conversation flow and impose contribution opportunities of NNS, including artificial silent gaps [105] and a conversational agent [54]. Therefore, having a designated opportunity to ensure NNS contributes to collaborative writing is necessary. Besides, since it's one of the first studies on multilingual collaborative writing, researchers chose a simplified"}, {"title": "4 Data Abstraction", "content": "Based on previous research, we introduce a general data model for the collaborative writing processes in our study.\nAuthors. In collaborative writing, there must be at least two authors. Let $A = \\{a_1, a_2, ...\\}$ be the set of authors. For each author $a_i$, the meta-information $M_i$ is a set of the author's quantitative or qualitative attributes, e.g., author's ID, linguistic background, seniority and so on. In our study, there are two authors in a team, so $A = \\{a_1, a_2\\}$. Since we only care about the linguistic background of authors in this study, $M_1 = \\{native\\text{-}English\\text{-}speaker\\}$ and $M_2 = \\{non\\text{-}native\\text{-}English\\text{-}speaker\\}$.\nEvents. Let E be the set of all collaborative writing events. The events could be on-document ($E_{on}$) and related ($E_{related}$), and $E = E_{on} \\cup E_{related}$. Event $e_{on,i}$ is an edit made by authors, e.g., $e_{on,i}$ could include the author, editing types (add/delete), locations, and so on. Event $e_{related,i}$ is an event related to the collaborative writing process, but not limited to editing, e.g., leave a comment [109], make a post [82] and browse the internet. $e_{related,i}$ could include the event name, start and end time. We can automatically obtain $E_{on}$ by comparing different document versions. To obtain $E_{related}$, one communication researcher manually coded events by watching the recordings, including the author, event type, and start and end time. After that, another communication researcher helped categorize events into higher levels. There are six types of events in total, and we highlight each event in different colors:\nTable 1 shows summarized frequencies and duration of six high-level writing-related actions: \"Writing\" is the most frequent and time-consuming action, followed by \"Wordsmith-crosslingual\u201d, \u201cActive-search\", \"Passive-search\", \"Wordsmith-English\" and \"Note-taking\".\nTurns and stages. In collaborative writing, authors take turns to write, which could be either sequential or parallel [69], depending"}, {"title": "5 Methods", "content": "Communication researchers are interested in comparing authors' writing behaviors along two dimensions: writing stages (individual, collaborative), and author types (NS, NNS). For example, to answer the question \"during the collaborative writing stage, how do NS and NNS writers' behaviors differ?\", we need to compare two collections of sequences: $S_{NS,collaborative}$ and $S_{NNS,collaborative}$.\nA primary challenge is that the sequences are highly heterogeneous, e.g., sequences in $S_{NNS,collaborative}$ range from 6 to 188 events, with an average of 75.6 and standard deviation of 47.0, making direct visual comparison impossible, as shown in Figure 2. Therefore, we identified the following lower-level tasks to enable effective comparison:\nIn this section, we discuss computational methods to support task T1 and T2, the interpretation challenges we faced in the design study, and our strategies to solve those challenges."}, {"title": "5.1 Computational methods", "content": "For T1, we decided to first automatically cluster sequences. Clustering algorithms are widely used to organize similar data points into groups [40]. Common methods include k-means [32], hierarchical clustering [65], DBSCAN [83], self-organizing maps [100] and so on. For T2, we reviewed the literature on visual summarization and data mining in event sequence analytics [29, 113, 114], and decided to use frequent patterns to summarize each cluster."}, {"title": "5.1.1 Method I: cluster and summarize sequences jointly", "content": "Our first approach is to cluster and summarize event sequences jointly. We chose Sequence Synopsis [12] because it is reported to produce higher quality visual summaries compared to other summarization methods [114]. Besides, sequential patterns are the most widely used summarization format [12, 57, 74, 77], compared to trees [56] and directed acyclic graphs (DAG) [34]. Sequence Synopsis clusters sequences and constructs a sequential pattern of each cluster by striking a balance between pattern conciseness and minimizing information loss from the original sequences. In our implementation, we can indirectly control the number of clusters K and the length of patterns by adjusting the weights of information loss and the number of patterns."}, {"title": "5.1.2 Method II: cluster first, summarize later", "content": "Our alternative approach is to cluster sequences first, and summarize each cluster. Here, we considered k-means and hierarchical clustering because these algorithms allow us to easily change the number of clusters. For each pair of sequences, we computed the Levenshtein distance (ignoring duration), which returns the minimal number of edits required to align the two sequences. Compared to other common distance metrics such as Euclidean distance, Levenshtein distance captures the order of the sequence and handles sequences of different lengths. Given K clusters, we also prefer nested results. For example, if two sequences are in the same cluster when K = 4, then we expected them to still be in the same cluster when K = 3. Therefore, we chose hierarchical clustering over k-means [90]. Since hierarchical clustering algorithms do not generate visual summaries for each cluster, we ran a commonly used maximal pattern mining algorithm VMSP [25] to extract patterns for each cluster. We set the minimum support to be 50%, i.e., the pattern has to be present in at least half of the sequences in the cluster. Different from Sequence Synopsis, which returns only one pattern for each cluster, VMSP returns multiple patterns, and we chose the longest one with the maximum support as the representative pattern."}, {"title": "5.2 Challenges in interpreting the computational results", "content": "We then designed an initial visualization to show the clustering and pattern mining results from these methods. Fig. 3 shows the result produced by Sequence Synopsis for NS' behaviors in the collaborative writing stage. There are six clusters, and for each cluster, Sequence Synopsis returns a representative pattern depicted as a sequence of circles. The number before each pattern is the cluster size, and the authors' IDs in the cluster are displayed after each pattern. To show the sequences in the cluster, users can click the radio button before each pattern (currently the 4th pattern is selected). In the raw event sequences displayed below the patterns, each event is a rectangle, where the color denotes the event type, and the length is the duration of the event. We highlight the event rectangles in black outlines if they are reflected in the pattern. We also implemented a similar interface for method II.\nThough the patterns returned by Sequence Synopsis preserve the ordering of events in the sequences, the communication researchers expressed difficulty in interpreting such patterns and wonder whether it's possible to start with something more intuitive, for example, they suggest starting with one author's sequence and building clusters based on similar authors. Besides, it was unclear how each pattern differed from one another and how the algorithm performed the clustering. Take the sequences in Figure 3 for an example, the second and third cluster both feature a subsequence of Writing - Passive Search. However, it is unclear how these two clusters differ from each other. The same issue can be found in the first and the last clusters as well, where both clusters exhibit repetitive Writing - Active Search subsequences.\nSince we have two sets of clustering results returned by Sequence Synopsis and hierarchical clustering, we let communication researchers switch to different results via a radio button. However, this caused great confusion. Even if we keep the number of clusters K the same for both methods, the clustering results returned by hierarchical clustering and Sequence Synopsis are not the same, and communication researchers are not sure which one to trust more. Besides, there are no explanations for why the sequences are grouped into each cluster or the differences between the clusters.\nTo summarize, the communication researchers encountered the following interpretation challenges:"}, {"title": "5.3 Strategies to address the interpretation challenges", "content": "To address the above challenges, we devised several strategies, including ensemble and interactive clustering (C1), visualizing sequence-level information for clustering rationales (C2), and using large language models (LLM) to generate text summaries (C3)."}, {"title": "5.3.1 Support ensemble and interactive clustering (C1)", "content": "To address C1, we decided to show the consensus and discrepancies in the results produced by different methods, inspired by the idea behind ensemble clustering [95].\nWe obtained multiple clustering results with varying numbers of clusters, ranging from 2 to N, where N represents the maximum number of patterns identified by Sequence Synopsis. We use Sequence Synopsis as a constraint because hierarchical clustering can produce as many clusters as the data points. Then we evaluate the clustering results of both Sequence Synopsis and hierarchical clustering using the same number of clusters. Given two clustering results A and B, and a cluster number K (2 <= K <= N), we try to match each cluster in A to a cluster in B in a way that maximizes the total overlap between cluster members. We used the Hungarian algorithm [48] to obtain the assignments with the most overlap. Then, we only keep assignments when the intersection size is larger than 1 (it is trivial to have a cluster of only one sequence). Therefore, the size of consensus clusters is usually smaller than K. For unclustered sequences, we treat them as singletons.\nshows a revised version of the visualization, where the sequences enclosed within a rectangle box have cluster assignments confirmed by both SequenceSynopsis and hierarchical clustering methods. In contrast, sequences without a consensus (i.e., NNS-2, NNS-3) are not enclosed, indicating they are singletons. Singletons are expected since the two methods leverage different computational techniques. Singletons remind communication researchers to give additional consideration to these authors, as computational methods differ in their cluster memberships. To help users assign"}, {"title": "5.3.2 Visualize sequence-level information for clustering rationales (C2)", "content": "To help users understand why sequences are clustered (C2) and assess the similarities and differences between sequences within and across clusters, we devised two solutions: one focuses on local information of individual sequences, while the other focuses on the global context:\nVisualizing local information of individual sequences. As shown in Figure 4, our earlier visualization design of an individual sequence presents the event sequences as it is, where colors encode the event types and the rectangle length encodes the duration. Such a design preserves all the information in the raw data, and communication researchers quickly conclude that NNS usually exhibit much more fragmented workflows than NS, frequently alternating between writing and other events. In contrast, NS usually allocates large chunks of uninterrupted time dedicated to writing. However, it is hard to generate additional insights. Therefore, we considered several design variants for visualizing individual sequences: trees, transition matrices, and arc diagrams."}, {"title": "5.3.3 Use LLMs to generate text summaries (C3)", "content": "To address C3 (extracted patterns are not interpretable), we drew inspiration from the analysis process of communication researchers. We observed that they described clusters in natural language; for example, they described the visualization in Figure 4 as \"The first cluster mostly shows writing and wordsmith-crosslingual, so the NNS participants here spent most of their time figuring out how to produce writing in English through Japanese. The second cluster features more active search during the writing session even though participants still performed crosslingual language editing at the beginning and end of their sessions. These NNS participants followed a different writing path - they segmented the content (active information search in the middle) and the editing (wordsmith at the beginning and end) aspects of the writing and focused on one task at a time.\"\nRecently, large language models (LLM) have shown great promise in data analysis [9, 58, 70] and LLM is found to have a high degree of agreement with human coders in thematic analysis [17]. Therefore, we leveraged LLM to generate text descriptions. We first tried capturing a screenshot of each cluster of arc diagrams (Figure 5) and prompted GPT-4V [2] with an explanation of the color encodings to describe each cluster. For example, we used the following prompt: \"The figure contains several event sequences, each showing an author's writing-related behaviors. There are six types of events: Active-Search, Wordsmith-Crosslingual, Wordsmith-English, Note-Taking, Passive-Search, and Writing. Each colored node is an event type, and you can find the event type in the colored legend. The arc thickness is the transition frequency of two events. Please name this cluster and provide a brief description.\"\nHowever, GPT-4V sometimes ignored faint arcs between two nodes. To ensure GPT captures all transitions, including rare ones, we provided the transition data in JSON format, which was used to generate the arc diagram. Each entry contains the source event,"}, {"title": "6 COALA", "content": "Integrating all these strategies, we built COALA, a visual analytics tool to compare collaborative writing behaviors of native and non-native English authors. It has two tabs: one for inspecting and modifying the clustering results (Figure 6) and the other for comparing clustering results for authors of interest (Figure 7).\nThe first tab (Figure 6) supports refining sequence clusters and summaries. After selecting authors and writing stages in the dropdown menu, it displays consensus clusters of sequences by Sequence Synopsis and hierarchical clustering (1) and unclustered sequences (2). Users can drag the slider to change the number of clusters, add or delete clusters, revise cluster descriptions, and drag and drop arc diagrams directly across clusters. To facilitate the refinement process, when users select an author's arc diagram, its background turns orange, and recommended similar authors outside its cluster are highlighted in orange outlines. Currently, an author in the second cluster (orange background) is selected, and several authors outside the cluster are recommended. On the right,"}, {"title": "7 Validation: User Studies", "content": "The validity of our design is rooted in the user-centered design process reported in the previous sections. We also organized two user studies to evaluate COALA. The first, a focus group session with two existing and two new communication researchers, examined the usage of COALA in real-world setting; the second, individual sessions with 8 graduate students, evaluates the effectiveness of COALA for general users who work more independently."}, {"title": "7.1 User study setup", "content": "We organized a focus group session with the two existing communication researchers along with two of their colleagues. All four participants belong to the same research group. The two new researchers are familiar with the multilingual communication research but are unfamiliar with the dataset details, nor have they seen COALA before. Though our collaborators are informed about the methods and individual visualizations, they also have not used COALA. Therefore, we conducted a 1-hour study session with all communication researchers (N=2+2). The goal of the group study is to use COALA in a realistic work setting to make sense of a dataset. In the beginning, we introduced the dataset and analytic tasks (T1 and T2), and then we gave a tutorial on how to use the tool. We deployed COALA online, and each researcher accessed the tool on their laptop. They were encouraged to think aloud and discuss their findings during the session. We took notes during the session, and after the group session, we also invited them to write down their findings in a shared document."}, {"title": "7.1.2 Individual user studies (N=8)", "content": "Besides the focus group, we also expanded the evaluation scope by recruiting eight graduate students with related research backgrounds (communication: 2, second-language education: 2, CSCW: 2, political science: 1, natural language processing: 1). All participants have first-hand collaborative writing experiences and are interested in understanding collaborative writing behaviors. Among them, four are NS, and four are NNS. To better evaluate the usability and design effectiveness of COALA, we set up individual user study sessions with them. The procedure is similar: first, participants read a background introduction of the dataset (we prepared a shortened and simplified version of section 3 and section 4). We then demonstrated how to use COALA to cluster similar authors and compare their writing behaviors. Participants were asked to complete two tasks: analyzing authors' behaviors across two dimensions-writing stages (individual vs. collaborative) and author types (NS vs. NNS). Participants can either write their findings in COALA or describe them verbally. Participants are given one hour to complete the tasks. Participants were asked to think aloud during the study, and after they"}, {"title": "7.2 Effectiveness of COALA", "content": "COALA facilitate discussions. All expert users had no difficulty in using COALA, and they were surprised to see that they focused on different aspects of the dataset during the analysis. For instance, one focused on the durations in the event sequence, and the other focused on the event transitions. This divergence triggered experts' discussion of why one aspect of collaborative writing was important. Ultimately, they concluded that COALA offered multiple angles to analyze the dataset they would otherwise miss due to their intuitions."}, {"title": "7.2.2 Individual user studies (N=8)", "content": "COALA helps uncover insights. Among the seven participants who completed the study, three found the arc diagram design more effective, two preferred the sequence diagram, and two considered both effective. Participants prefer the arc diagram mostly because it summarizes the event sequences. For example, P8 appreciated how the arc diagram \u201ccompresses information\". In contrast, participants preferred the sequence diagram because it encoded the duration information (P5).\nRegarding the model quality, five out of seven participants considered the initial clusters obtained by the consensus methods to be high quality and encouraged unbiased exploration. For example, P7 said, \"It's a really good starting point, because if you were to analyze this by yourself without any ideas...you can fall into typical stereotypes that...an AI model would not.\u201d Furthermore, six participants found the recommendation feature helpful, using it as a starting point for the analysis (P8), a voting mechanism (P6), and a verification method (P7).\nParticipants also made suggestions to improve model-generated results, including more details about how the recommendation methods work (P4), more fine-grained clusters to start with (P5), and a 3rd recommendation method based on time duration (P6).\nNotably, none of the participants had analyzed multilingual collaborative writing datasets before. Despite their unfamiliarity with the dataset and COALA, most of them were able to familiarize themselves with COALA and complete the analysis tasks. By leveraging the model-generated results and data visualization designs, they uncovered insights similar to those of expert users and connected the findings with their own experience. This outcome highlights the general usability and design effectiveness of COALA."}, {"title": "7.3 Findings: collaborative writing patterns", "content": "Here we aggregated participants' findings by using COALA, including the dataset itself and their reflections on their own collaborative writing experience."}, {"title": "7.3.1 Individual stage patterns", "content": "NS and NNS are asked to write independently before collaborating in this experiment to avoid freeriders. Communication researchers observed distinct behaviors during the individual stage.\nNS research extensively. Using our tool, communication researchers easily identified several major clusters of NS. One cluster is characterized by dominant Active-Search - Writing behaviors, indicating they actively incorporated external information into their writing. Another cluster has more activities, besides AS - W, they also engaged in extensive paraphrasing activities: Wordsmith-English Writing. Besides these two major clusters, there are also uncommon behaviors, e.g., one NS only displayed WE W behavior without relying on external information; one NS went even further, solely engaged in writing, degenerating the arc diagram into a stick on the W node.\nNNS encounter costly bilingual content production. Similarly, communication researchers identified several clusters of NNS. In one cluster, Wordsmith-Crosslingual - Writing dwarfed any other behaviors, implying NNS had significant usage of their native language to produce writing in English. Another cluster has more Active-Search - Writing behaviors, showing NNS also incorporated external information into their writing. Different from NS, several NNS also displayed AS - WC, meaning NNS also used their native language to transfer content from their content-related search to their writing output. Among the NNS who have engaged in both WC-W and AS - W, half are dominated by WC, and the rest are dominated by AS or are balanced. Notably, none of the NNS engaged with Wordsmith-English - Writing during the individual writing stage, highlighting the bilingual nature of NNS' writing process, as their limited English proficiency may have dissuaded them from writing directly in English. Compared to NS, many more NNS (n=10) only engaged in WC - W without relying on external information. This pattern hinted that NNS have dedicated much of their time to the costly process of writing in English - generating content in their native language first and then translating the content to English, either by themselves or by leveraging support from cross-lingual dictionaries and translation tools."}, {"title": "7.3.2 Collaborative stage patterns", "content": "Communication researchers found more diverse actions during collaborative stages, driven by co-authors' need to exchange information, refine text, and communicate with each other. For example, communication researchers observed multiple NS (n=8) and NNS (n=10) engaged in Passive-Search, indicating active information sharing. NS and NNS also display different distributions of behaviors in the collaborative stage.\nNS shoulder more editing responsibilities. Transitions between AS - W and WE - W still have a strong presence, though the balance has shifted, with WE W increases, and AS - W decreases. The shift towards editing indicates that NS is responsible for editing both parties' English expressions.\nNNS gain more bandwidth for other tasks. Communication re-searchers witnessed an uptick in both WC - W and AS-W, suggesting NNS actively interpreting and incorporating NS-contributed content. In addition, more than a third of NNS engaged in PS - W."}, {"title": "7.3.3 Echo with first-hand experience", "content": "In the individual user study sessions, besides the findings similar to the ones in the group study session, our participants also compared their findings through the lens of their collaborative writing experience. For example, P6, a native English speaker, noted that too many NS \"just spitball on the fly without actually doing any research\" in this dataset, which aligned with his previous collaborative writing experience. In contrast, P2 (a non-native English speaker) observed that \"...native speakers become more cautious with their writing...when they are working with people, which is something that I wouldn't expect...if I am working with a non-native speaker of my language. I feel like I know more, so I would feel less need to actually check my writing.\" P4 resonated with the Active-Search - Wordsmith-Crosslingual transition in the arc diagram: \"I was seeing myself do the same thing. I also like search for specific words in Korean, and then translated into English.\"\nSome participants also voiced their suggestions for best practices in collaborative writing. For example, P3, a native English speaker, pointed out that so much time spent on Wordsmith-Crosslingual is \"a waste of manpower, it makes more sense to focus on the ideation and don't care if it comes out ugly if the other person (NS) can fix it without having to do a lot of wordsmithing.\""}, {"title": "7.4 Findings: participants' analysis strategies", "content": "We also studied participants' analysis strategies. For the group study, we analyzed the notes taken during the study and the document on which participants wrote findings. For the individual study, we analyzed the video recordings, including participants' screens and meeting transcripts."}, {"title": "7.4.1 Experts (N=2+2)", "content": "COALA is pre-populated with clusters returned by methods described in section 5. Initially", "processes": "authors who use Wordsmith-English or Wordsmith-Crosslingual are classified as \"grammar-based\", as they only need help with the expression, but not ideation; authors who leverage Active-Search and Passive-Search are classified as \"research-heavy\", as such authors are still in the process of finding ideation.\nOn the opposite, another researcher (E2) broke down clusters into sub-clusters by examining the duration and transition frequencies of raw sequences carefully (Figure 6.4); for example, NS-20 and NS-22 were initially clustered into the same cluster by our algorithms as their behaviors are dominated by Active-"}]}