{"title": "Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints", "authors": ["Ming Dai", "Jian Li", "Jiedong Zhuang", "Xian Zhang", "Wankou Yang"], "abstract": "Multi-task visual grounding involves the simultaneous execution of localization and segmentation in images based on textual expressions. The majority of advanced methods predominantly focus on transformer-based multimodal fusion, aiming to extract robust multimodal representations. However, ambiguity between referring expression comprehension (REC) and referring image segmentation (RIS) is error-prone, leading to inconsistencies between multi-task predictions. Besides, insufficient multimodal understanding directly contributes to biased target perception. To overcome these challenges, we propose a Coarse-to-fine Consistency Constraints Visual Grounding architecture (C\u00b3VG), which integrates implicit and explicit modeling approaches within a two-stage framework. Initially, query and pixel decoders are employed to generate preliminary detection and segmentation outputs, a process referred to as the Rough Semantic Perception (RSP) stage. These coarse predictions are subsequently refined through the proposed Mask-guided Interaction Module (MIM) and a novel explicit bidirectional consistency constraint loss to ensure consistent representations across tasks, which we term the Refined Consistency Interaction (RCI) stage. Furthermore, to address the challenge of insufficient multimodal understanding, we leverage pre-trained models based on visual-linguistic fusion representations. Empirical evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets demonstrate the efficacy and soundness of C\u00b3VG, which significantly outperforms state-of-the-art REC and RIS methods by a substantial margin.", "sections": [{"title": "Introduction", "content": "Visual grounding is a critical task within the vision-language domain, aimed at establishing a fine-grained correspondence between images and text by grounding a given referring expression within an image (Li et al. 2022b). This task is typically divided into two sub-tasks based on the grounding approach: referring expression comprehension (REC) (Yu et al. 2018; Kamath et al. 2021) and referring image segmentation (RIS) (Kim et al. 2022; Tang et al. 2023). Traditionally, REC and RIS have been treated as separate tasks with distinct technological pathways, necessitating complex,\ntask-specific designs. However, REC and RIS exhibit significant similarities and offer complementary strengths, making their unification both logical and advantageous. Recently, multi-task visual grounding has gained prominence as it eliminates the need for task-specific network designs and enables the leveraging of data across both tasks to mutually enhance performance. MCN (Luo et al. 2020) was the first approach to jointly train the REC and RIS tasks, employing a learnable method to establish consistency in attention maps. Recent research has primarily focused on enhancing the interaction across different modalities (Li and Sigal 2021; Su et al. 2023) and exploring auto-regressive approaches to achieve both detection and segmentation (Zhu et al. 2022; Cheng et al. 2024; Liu et al. 2023a). In this paper, we address two overlooked issues: 1) How to effectively leverage the complementarity of multi-task predictions to mitigate inconsistencies in results. 2) How to overcome the challenge of insufficient multimodal understanding to enhance perception in complex image-text scenarios.\nInconsistent predictions between multi-task primarily"}, {"title": "Related Work", "content": "Visual Grounding\nReferring Expression Comprehension (REC) (Liu et al. 2019; Yang et al. 2020, 2024; Su et al. 2024; Zhuang et al. 2025) predicts a bounding box that tightly encompasses the target object in an image based on a referring expression.\nReferring Image Segmentation (RIS) (Yang et al. 2022; Zhang et al. 2022; Liu et al. 2023c) aims to provide pixel-level localization of a target object in an image based on a"}, {"title": "The Proposed C\u00b3VG", "content": "Architecture Overview\nInitially, the image and text modalities are independently embedded and processed through a multi-modality encoder (MME) for vision-language encoding and fusion, positioning the joint representation of multimodal fusion upstream. A learnable object token is also utilized as the feature representation for the REC task. The framework then advances through the RSP and RCI stages, ultimately yielding high-quality predictions.\nMulti-Modality Encoder. The input to C\u00b3VG consists of an image $I \\in \\mathbb{R}^{3\\times H\\times W}$ and a caption text $\u03a4 \\in \u03a9^{\u039c}$, where \u03a9 denotes the vocabulary set. The image is initially down-sampled to 1/16 of its original size using a visual embedding, resulting in $P_{I} = \\{p_{1}, p_{2}, ..., p_{Ni}\\}$. The text is then tokenized into $L_{t} = \\{l_{1}, l_{2}, ..., l_{Nt}\\}$. Additionally, we define a learnable object token $T_{o}$ as the target feature for the REC branch. The inputs of MME can be expressed as:\n$T = \\{T_{o}, p^{1}, p^{2} ..., p^{Ni}, l_{1}, l_{2}, ..., l_{Nt}\\}$\nThe MME architecture leverages the pre-trained weights of the BEiT-3 (Wang et al. 2023) model. The output of the MME comprises three components: $T_{o} \\in \\mathbb{R}^{B\\times 1\\times C}$, $T_{I} \\in \\mathbb{R}^{B\\times N_{I}\\times C}$, $T_{l} \\in \\mathbb{R}^{B\\times N_{t}\\times C}$\nRough Semantic Perception Stage. The RSP stage aims to generate a rough localization and semantic outline, serving as priors for the RCI stage. Initially, the outputs of the MME are projected to a common dimension via three un-shared linear layers:\n$T_{o}^{\\prime} = O_{P}(T_{o}), T_{I}^{\\prime} = T_{P}(T_{I}), T_{l}^{\\prime} = I_{P}(T_{l})$\nFor the REC branch, the process begins with a query decoder, which enhances the representation of the object token by interacting with text and image tokens. The query decoder is defined as:\n$T = MCA(MLP(Concat(T_{l}^{\\prime}, MCA(T_{l}^{\\prime} + Q_{init},\nT_{I}^{\\prime} + pos_{1a})), T_{I}^{\\prime} + pos_{2d})$\nwhere MCA($A_{1}$, $A_{2}$) denotes the multi-head cross attention mechanism, with $A_{1}$ serving as the query and $A_{2}$ as the key and value. Subsequently, an MLP is employed to regress and predict the REC output $P_{f} \\in \\mathbb{R}^{B\\times 4}$. For the RIS branch, we adopt a text-to-pixel correlation strategy similar to CRIS (Wang et al. 2022) to generate the predicted mask $P_{C} \\in \\mathbb{R}^{B\\times H\\times W}$. However, instead of using a 3\u00d73 convolution with padding, we compress the text using a 1\u00d71 convolution without additional padding.\nRefined Consistency Interaction Stage. The Refined Consistency Interaction (RCI) stage is designed to harmonize the outputs from the RSP stage, ensuring multi-task consistency through both implicit interactions and explicit constraints. We first introduce a mask-guided interaction module (MIM) that adaptively and implicitly aligns the consistency between the detection and segmentation predictions. Additionally, an auxiliary bidirectional consistency constraint loss is incorporated to explicitly enforce alignment at the result level. In the REC branch, an MLP layer is utilized to regress object features at the RCI stage. In the RIS branch, we integrate SimFPN (Li et al. 2022d) to capture multi-level structures, followed by a UNet-style (Ronneberger, Fischer, and Brox 2015) decoder that performs multi-level fusion and a pixel decoder, consistent with the\nutilize the structural information from the RIS branch and ensure consistent predictions, we interact $F_{f}$ with both textual and visual features. The final interacted object feature $F_{box}$ is expressed as:\n$F_{box} = MCA(MCA(F_{u}, F_{text}), F_{img}),$\nwhere the calculation of $F_{u}$ is detailed in Eq. 10.\nIn the RIS branch, we apply the concept of background suppression and foreground enhancement by leveraging the results of both the REC and RIS branches on $F_{img}$. First, $P_{f}$ is converted to the top-left and bottom-right format by rounding to integers as follows:\n$x_{1} = (x \u2212 0.5w) \u00d7 w, y_{1} = (y \u2212 0.5h) \u00d7 h,$\n$x_{2} = (x + 0.5w) \u00d7 w, y_{2} = (y + 0.5h) \u00d7 h,$\n$\\left\\{X_{1}, Y_{1}, X_{2},Y_{2}\\right\\} = \\{\\lfloor X_{1}\\rfloor, \\lfloor Y_{1}\\rfloor, \\lceil X_{2}\\rceil, \\lceil Y_{2}\\rceil\\},$\nwhere [*] denotes the floor function, and [*] denotes the ceiling function. The NLS generates a weight mask $W_{b}$ of the same dimensions as $F_{img}$, calculated as follows:\n$W_{b} = \\begin{cases}W_{1}, & \\text{if } x_{i} \\in [x_{1}, x_{2}] \\land Y_{j} \\in [Y_{1}, Y_{2}] \\\\\n1, & \\text{otherwise,}\\end{cases}$\nwhere $\u2200x_{i} \\in [0, w]$ and $\u2200y_{j} \\in [0,h]$. $w_{1}$ is set to default values of 0.1, respectively. We then apply a sigmoid function to the predicted mask from the RSP stage to generate the weighted mask $W_{s} = \u03c3(P_{c})$. The weights $W_{b}$ and $W_{s}$ are applied to $F_{img}$ to obtain the box and mask-constrained feature $F_{u}$:\n$F_{s} = W_{s} \u2297 F_{img},$\n$F_{u} = Concat(F_{s}, W_{b} \u2297 F_{s}, F_{img}).$\nNext, an MLP reduces the channel dimension from 3 \u00d7 C back to the original C, yielding the fused image representation $F_{img}^{\\prime}$, which incorporates the predictions from the RSP stage. This process implicitly provides the RCI stage with prior spatial attention information derived from detection\nand segmentation predictions. As illustrated in Fig. 5, the presence of two cats results in divergent attention predictions, leading to suboptimal adjustments of the bounding box prediction during the RSP stage. The MIM mitigates this issue by imposing constraints on the regions of high response within the image space, thereby reducing the model's focus on irrelevant targets and enabling more precise target identification. Furthermore, the fused image representation is interacted with the text, followed by a multi-head self-attention (MSA) layer to further learn consistent semantic associations. This process is expressed as follows:\n$F_{u}^{\\prime} = MLP(F_{u}),$\n$F_{seg} = MSA(MCA(F_{u}^{\\prime}, F_{text})).$\nBidirectional Consistency Constraint Loss\nTo complement the implicit interactions facilitated by the MIM across multi-task outputs, we propose an explicit bidirectional consistency constraint loss, denoted as $L_{bcc}$. First, $L_{m2b}$, is designed to enforce the segmentation mask to be contained within the predicted bbox:\n$L_{m2b} = 1 \u2212 \\frac{\\sum(M_{s} \\cap M_{b})}{\\sum M_{s}},$\n$\\begin{cases}M_{s} =\n\\begin{cases}1, & \\text{if } p_{ij} > t \\\\\n0, & \\text{otherwise}\\end{cases}\\\\\\nM_{b} =\n\\begin{cases}1, & \\text{if } (x_{i}, y_{j}) \\in P_{b} \\\\\n0, & \\text{otherwise}\\end{cases}\\end{cases}$\nwhere $p_{ij}$ denotes the pixel values of the predicted segmentation mask after applying the sigmoid function, with $\u2200i \\in [0, w]$ and $\u2200j \\in [0, h]$. $t$ is set to 0.5. $P_{b}$ represents the bounding box prediction. Second, the loss term $L_{b2m}$ is defined as follows:\n$L_{b2m} = 1 \u2212 \\frac{P_{b} \\cap P_{s}}{P_{b} \\cup P_{s}},$\nwhere $P_{b}$ represents the minimal bounding box that encloses the segmentation mask $M_{s}$, and $P_{b}$ denotes the pre-"}, {"title": "Experiments", "content": "Experimental Setup\nWe evaluate the proposed model in RefCOCO (Yu et al. 2016), RefCOCO+ and RefCOCOg (Nagaraja, Morariu, and Davis 2016) datasets. The maximum sentence length is set to 20. The images are resized to 320 \u00d7 320. Based on previous works (Zhu et al. 2022), mIoU and Prec@0.5(Acc(REC) in ablation study) are adopted to evaluate the performance of methods. We train our models for 30 epochs with a batch size of 16. Adam (Kingma and Ba 2014) is adopted as our optimizer. All experiments are conducted on a system with dual NVIDIA 4090 GPUs. Further details will be provided in the supplementary materials.\nMain Results\nReferring Expression Comprehension. The single-task part presented in Tab. 1 showcase a comparison between our method and prior advanced REC approaches. In comparison to Dynamic MDETR, which utilizes ViT-B as its backbone, C\u00b3VG achieves a remarkable improvement of +5.78%-17.98% in Acc(REC). Furthermore, when compared to GroundingDINO (Liu et al. 2023b), which is trained on large-scale data, C\u00b3VG delivers a gain of +2.72%-6.71% in Acc(REC) while also reducing inference latency by 58%.\nReferring Image Segmentation. The single-task part presented in Tab. 2 compare our C\u00b3VG with previous advanced RIS methods. Our C\u00b3VG demonstrates an absolute improvement of 9.75%-18.72% over the Transformer-based"}, {"title": "Ablation Studies", "content": "Basic Improvement Setting. We implement several techniques to enhance the performance of our baseline model, with the experimental outcomes presented in Tab. 3. The baseline architecture leverages the ViT-B and BERT models as the visual and textual encoders, respectively, with VGTR head. First, we observe a substantial performance boost by incorporating multimodal fusion representation pretraining (BEiT-3), which yields an increase of +5.11% in Acc(REC) and +5.28% in oIoU. This improvement can be attributed to the fact that prior methods often rely on limited downstream data to learn multimodal representations, resulting in"}, {"title": "Conclusion", "content": "In this paper, we present C\u00b3VG, a coarse-to-fine architecture designed for multi-task visual grounding, aimed at ad-"}, {"title": "Appendix", "content": "Additional Dataset Details\nRefCOCO/RefCOCO+: RefCOCO comprises 142,209 annotated expressions corresponding to 50,000 objects across 19,994 images, while RefCOCO+ includes 141,564 expressions for 49,856 objects in 19,992 images. Both datasets are divided into training, validation, test A, and test B sets. Test A contains images with multiple people, whereas test B features images with multiple instances of various other objects. Unlike RefCOCO, RefCOCO+ prohibits the use of location-based words in the referring expressions, thus increasing the task's difficulty.\nRefCOCOg: The RefCOCOg dataset was curated using Amazon Mechanical Turk, where workers were instructed to generate natural language referring expressions for specific objects. It comprises 85,474 referring expressions for 54,822 objects across 26,711 images. Compared to RefCOCO and RefCOCO+, RefCOCOg features longer and more complex expressions, averaging 8.4 words, versus 3.5 words in the other datasets, thereby increasing the challenge. We adopt the UMD partition for RefCOCOg, as it provides distinct validation and testing sets without overlap between training and validation images.\nAdditional Implementation Details\nIn the C\u00b3VG model, the output of the original ViT-B is uniformly reduced from 768 to 256 dimensions for subsequent head operations. Specifically, the OP, TP, and IP map features from 768 dimensions to 256. For evaluation metrics, Acc (REC) refers to the accuracy when the box IoU exceeds 0.5, while Acc (RIS) pertains to the accuracy when the mask IoU exceeds 0.5. All experiments are conducted without utilizing the Exponential Moving Average (EMA) technique. The initial learning rate for the V-L encoder is set at 5e-5, with other parameters at 5e-4. The learning rate undergoes a decay by a factor of 0.1 at the 25th epoch. To ensure a comprehensive presentation of the results, both mIoU and oIoU metrics are included in the SOTA table. All ablation studies are conducted at a resolution of 224x224, with training over 20 epochs, and the learning rate decays by a factor of 0.1 at the 15th epoch. Metrics are based on the testB split of the RefCOCO dataset. The results in Tab. 1 and 2 are obtained using the combined training data from the unc set of RefCOCO and RefCOCO+, along with the umd set of RefCOCOg.\nAdditional Method\nDecoder Architecture\nThe extension of the ViT structure to generate multi-scale feature maps is initially proposed in ViTDet (Li et al. 2022d), termed SimFPN. In our work, we adopt this design to extend the single-scale feature map of the original ViT, resulting in four scales: $M_{0}, M_{1}, M_{2}, M_{3}$, corresponding to $\\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}$ of the original image, respectively. We then employ a UNet-type decoder to further process these multi-"}]}