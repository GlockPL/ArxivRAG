{"title": "CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing Cybersecurity Knowledge Graphs Under Data Scarcity", "authors": ["Yutong Cheng", "Osama Bajaber", "Saimon Amanuel Tsegai", "Dawn Song", "Peng Gao"], "abstract": "Textual descriptions in cyber threat intelligence (CTI) reports, such as security articles and news, are rich sources of knowledge about cyber threats, crucial for organizations to stay informed about the rapidly evolving threat landscape. However, current CTI extraction methods lack flexibility and generalizability, often resulting in inaccurate and incomplete knowledge extraction. Syntax parsing relies on fixed rules and dictionaries, while model fine-tuning requires large annotated datasets, making both paradigms challenging to adapt to new threats and ontologies. To bridge the gap, we propose CTINexus, a novel framework leveraging optimized in-context learning (ICL) of large language models (LLMs) for data-efficient CTI knowledge extraction and high-quality cybersecurity knowledge graph (CSKG) construction. Unlike existing methods, CTINexus requires neither extensive data nor parameter tuning and can adapt to various ontologies with minimal annotated examples. This is achieved through: (1) a carefully designed automatic prompt construction strategy with optimal demonstration retrieval for extracting a wide range of cybersecurity entities and relations; (2) a hierarchical entity alignment technique that canonicalizes the extracted knowledge and removes redundancy; (3) an ICL-enhanced long-distance relation prediction technique to further complete the CKSG with missing links. Our extensive evaluations using 150 real-world CTI reports collected from 10 platforms demonstrate that CTINexus significantly outperforms existing methods in constructing accurate and complete CSKGs, highlighting its potential to transform CTI analysis with an efficient and adaptable solution for the dynamic threat landscape.", "sections": [{"title": "1. Introduction", "content": "Modern cyberattacks are becoming increasingly complex and rapidly evolving. Many public and commercial organizations extensively record and share cyber threat intelligence (CTI) on their platforms to combat evolving threats. According to Gartner, CTI is defined as \"evidence-based knowledge, including context, mechanisms, indicators, implications and actionable advice, about an existing or emerging menace or hazard to assets that can be used to inform decisions regarding the subject's response to that menace or hazard\" [60]. Such knowledge is crucial for organizations to monitor the rapidly evolving threat landscape, promptly detect early signs of an attack, and effectively contain the attack with proper measures. Given its importance, CTI has been increasingly collected and exchanged across organizations, often in the form of Indicators of Compromise (IOC) [56]. IOCs are forensic artifacts of an intrusion such as virus signatures, IPs/domains of botnets, MD5 hashes of attack files, etc. However, recent studies [56], [77] showed that knowledge offered by IOCs is rather limited, which covers only a limited set of knowledge and has a short lifespan.\nRecognizing the limitations of IOCs, recent research has shifted towards automatically extracting richer knowledge from textual threat descriptions in CTI reports (i.e., CTI text). These reports, such as security blog articles [22], [6] and news [10], [20], are produced by security researchers and practitioners and published on websites, summarizing threat behaviors in natural language. Besides IOCs, these reports contain various other cybersecurity entities, such as malware, vulnerabilities, and attack techniques, as well as their relationships, illustrating their interactions and dependencies. This knowledge is crucial for building a comprehensive cyber threat profile.\nSeveral approaches have been proposed for automatically extracting security knowledge from CTI text and constructing a cybersecurity knowledge graph (CSKG). Syntax parsing-based approaches [43], [39], [56] leverage fixed dependency rules and hand-crafted dictionaries to parse the grammatical structure of sentences and extract key subject-verb-object triplets. Fine-tuning-based approaches [71], [55], [30] leverage pre-trained transformer models and fine-tune them on labeled CTI text datasets to identify semantic roles and extract entities and relations. However, all these existing methods suffer from several key drawbacks (see Section 2.2 for details), particularly when facing the evolving threat landscape: (1) Lack of flexibility and generalizability: Many of these methods are tailored to specific cybersecurity ontologies, focusing on a fixed set of entities and relation types. They are difficult to generalize to new ontologies and emerging threats and terminologies. Fixed rules have limited flexibility to adapt to new patterns and require manual creation and maintenance. Model fine-tuning, however, requires a large amount of labeled CTI data. Such data is scarce in security, especially for new threats that lack annotations. (2) Information inaccuracy and incompleteness: Due to the peculiarities of the security context and the lack of deep analysis, these methods often generate low-quality CSKGs that are incomplete, inaccurate, and disconnected."}, {"title": "2. Background and Motivating Example", "content": "Although crowd-sourced CTI reports provide valuable information, their unstructured format significantly hinders their effectiveness. As the number and complexity of cyberattacks increase, the textual CTI descriptions have also expanded, creating an urgent need for automated information extraction from CTI [68]. The extracted knowledge can be used to construct cybersecurity knowledge graphs (CSKGs), where nodes represent entities and edges represent relations. Compared to unstructured CTI text, CSKGs provide a holistic profile for cyber threats, offer better visualization, and are more amenable to integration into downstream applications. The construction of a CSKG typically follows an ontology, which specifies the entity types and their allowed relations. However, despite efforts to create multiple security ontologies [44], [69], [75] covering different aspects of threats, keeping up with the rapidly evolving threat landscape remains challenging as new threats, techniques, and tools constantly emerge. It is nearly impossible to develop a universal ontology that encompasses all current and future threats. This underscores the need for CTI knowledge extraction approaches that can flexibly adapt to different ontologies and emerging threats with minimal transition effort."}, {"title": "2.1. Cyber Threat Intelligence", "content": "Although crowd-sourced CTI reports provide valuable information, their unstructured format significantly hinders their effectiveness. As the number and complexity of cyberattacks increase, the textual CTI descriptions have also expanded, creating an urgent need for automated information extraction from CTI [68]. The extracted knowledge can be used to construct cybersecurity knowledge graphs (CSKGs), where nodes represent entities and edges represent relations. Compared to unstructured CTI text, CSKGs provide a holistic profile for cyber threats, offer better visualization, and are more amenable to integration into downstream applications. The construction of a CSKG typically follows an ontology, which specifies the entity types and their allowed relations. However, despite efforts to create multiple security ontologies [44], [69], [75] covering different aspects of threats, keeping up with the rapidly evolving threat landscape remains challenging as new threats, techniques, and tools constantly emerge. It is nearly impossible to develop a universal ontology that encompasses all current and future threats. This underscores the need for CTI knowledge extraction approaches that can flexibly adapt to different ontologies and emerging threats with minimal transition effort."}, {"title": "2.2. Limitations of Existing Approaches", "content": "Existing CTI knowledge extraction approaches face several fundamental challenges in adapting to the rapidly evolving threat landscape. Existing approaches follow two"}, {"title": "2.2.1. Motivating Example", "content": "We further investigate the quality of the constructed CSKG by existing approaches using a real-world CTI report. Fig. 1 illustrates a snippet of the report titled \"RANSOMWARE AKIRA AND RAPTURE\" published on May 9, 2023, by Avertium [3]. The report provides rich information about the new Akira ransomware group. We run this CTI text snippet with three representative approaches, TTPDrill, EXTRACTOR, and LADDER using their released implementations [24], [7], [13]. Fig. 1 shows their constructed CSKGs. We observe that the quality of CSKGs is very low.\n\u2022 Some triplets have wrong directions. For example, in EXTRACTOR, \"ransom note\" is extracted as the subject of \"leave\", whereas it should be the object.\n\u2022 Many extracted entities have poor quality. Some are not meaningful, such as \"presence\u201d extracted by TTPDrill. Others include unnecessary words or combine multiple distinct entities; for example, TTPDrill incorrectly extracts \"registry values\" and \"ransom note\" together when they should be separate. Similarly, in EXTRACTOR, the victim entities are not properly distinguished and should be individually separated. Although LADDER's extracted content is of higher quality compared to TTPDrill and EXTRACTOR, it often lacks completeness. For instance, in the context where a \"Trojan\" targets \"WordPress sites\", LADDER only extracts \"WordPress\" thereby omitting contextual information from the original phrase.\n\u2022 Entities are not aligned. For example, in EXTRACTOR, \"Trojan\" and \"the ransomware Trojan\" refer to the same object and should be merged or associated. The same issue is observed in TTPDrill and LADDER.\n\u2022 Some critical relations are missing. In the text, \"the Akira ransomware group\" uses the \"ransomware Trojan\" to launch the attack. However, since these two entities are mentioned in different sentences without explicit relational indicators, all approaches fail to infer the relationship between them.\nAs shown in Fig. 1, the CSKG constructed by CTINEXUS is comprehensive, well-connected, and of much higher quality, addressing all previous drawbacks. By leveraging the in-context learning of LLMs, the construction of such a CSKG does not rely on large amounts"}, {"title": "2.3. Large Language Models", "content": "Recently, LLMs have shown emergent abilities to learn from just a few demonstration examples in the prompt, a paradigm known as in-context learning (ICL) [37]. In the ICL paradigm, the prompt input to the LLM typically includes three components: (1) an instruction specifying the task, (2) several demonstration examples containing ground truth to provide task-specific knowledge, and (3) a query to the LLM with the expectation of an appropriate answer. This allows LLMs to adapt to new tasks with minimal cost using task-specific prompts and demonstration examples. Multiple studies have shown that LLMs perform well in various tasks under ICL, such as fact retrieval [79] and mathematical reasoning [45], [28]. Additionally, LLMs have shown promise in different cybersecurity tasks, such as vulnerability detection [38], [59], patch generation [50], and software fuzzing [81], [61]. However, the use of LLMs for CTI knowledge extraction and CSKG construction remains largely underexplored."}, {"title": "3. Overview", "content": "Fig. 2 illustrates CTINEXUS. CTINexus introduces a novel ICL-based approach for data-efficient CTI knowledge extraction and CSKG construction. Unlike previous methods, CTINEXUS eliminates the need for extensive data annotations and parameter tuning, facilitating easy generalization to different ontologies. CTINEXUS focuses on constructing a connected and comprehensive CSKG, enabling entity alignment and long-distance relation inference. CTINEXUS consists of three phases.\nPhase 1: Given a CTI report, CTINEXUS first extracts entity-relation triplets that align with the task ontology. The kNN-based demonstration retriever embeds the report and the candidate reports in the demonstration set into a high-dimensional latent space. The retriever then selects the top-k candidates with the highest similarity scores. The selected demonstrations are fed into an automatic prompt construction module to create a customized prompt for the current report. As illustrated in Fig. 2, our prompt template consists of three sections: an instruction describing the task, a query containing the input CTI report, and demonstration examples arranged in a specific order. Fig. 3 illustrates our carefully designed instruction. Note that the ontology is incorporated into the instruction. This design allows different ontologies to be easily switched, and our automatic prompt construction module will create a prompt specifically for this ontology and report, enhancing knowledge extraction performance.\nPhase 2: With the extracted triplets, CTINEXUS removes redundancy by merging entities that refer to the same cybersecurity object using a hierarchical approach. The coarse-grained entity grouping module assigns types to entities using an automatically populated ICL prompt template, as illustrated in Fig. 4. The instruction incorporates the ontology that defines possible entity types. The demonstration examples show how to label each entity in the triplet. The query includes all the triplets to be typed. Entities assigned the same type are grouped together."}, {"title": "4. Design of CTINEXUS", "content": ""}, {"title": "4.1. CSKG Ontology", "content": "We choose MALOnt for the current implementation, as MALOnt [69] is the most comprehensive among open-source ontologies, featuring 33 entity types (17 types and 16 sub-types) and 27 relation types. MALOnt covers a broad range of entities, such as Account, Action, Threat Actor, Campaign, Event, Exploit Target, Host, Information, Infrastructure, Location, Malware, Person, Software, System, and Vulnerability, with detailed sub-types under Indicator and Malware Characteristics. However, note that CTINEXUS 's ICL-based pipeline eliminates the need for parameter tuning on large, ontology-specific training sets, largely simplifying generalization to other ontologies. If downstream applications require ontologies not covered by MALOnt, CTINEXUS can easily switch to a different ontology. This only requires a few demonstration examples aligned with the new ontology for each ICL task, and the ontology defined in a JSON format incorporated in the prompts (illustrated in Figs. 3 and 4). If the new ontology is a subset of MALOnt (which is already quite comprehensive), CTINEXUS can directly adapt by simply removing unrequired entity types without further actions."}, {"title": "4.2. Cybersecurity Triplet Extraction", "content": "Given that CTI text may contain diverse relations and we want the approach to be adaptable to emerging threats, we formulate the cybersecurity triplet extraction module in our pipeline as a semi-open extraction problem: Entity types are prescribed using MALOnt, as its coverage is already comprehensive, while relation extraction is modeled as open RE to maximize the coverage. These approaches transform information extraction tasks into multi-turn question-answering, leveraging the conversational capabilities of LLMs. Fig. 3 illustrates this paradigm. This method involves creating multiple questioning prompts for each information type and refining the responses. However, applying this multi-turn QA formulation to cybersecurity entity and relation extraction requires numerous lengthy prompts due to the extensive cybersecurity ontology that could contain many entity classes. For N entities in the input CTI, $\\frac{N(N-1)}{2}$ prompts are needed to extract relations between identified entities, leading to repetitive content and significant token waste, hindering scalability. Additionally, the multi-turn paradigm suffers from confirmation bias [34], as LLMs may confirm with a non-existing relation after several rounds of dialogue. These erroneous links can be particularly harmful in the CTI domain, negatively affecting downstream defense solutions by producing false alarms."}, {"title": "ICL prompt template", "content": "To improve efficiency and reduce confirmation bias, we develop a kNN-enhanced ICL paradigm that completes the cybersecurity triplet extraction process with only one LLM query. As illustrated in Fig. 3, CTINEXUS extracts all cybersecurity triplets by automatically populating a comprehensive ICL prompt template, which comprises the following components:\n(1) Instruction: The instruction specifies the task, the applied ontology, and the required format for the extracted triplets. Instruction design is critical in LLMs, as an unclear definition of the task can severely degrade the performance. We carefully designed several versions of the instruction and identified the one presented in Fig. 3 as the most effective.\n(2) Demonstrations: Top-k most relevant examples are retrieved using the demonstration retriever. Each example consists of a CTI report annotated with the security triplets. These examples are ordered in ascending similarity to the input query based on findings described in Section 5.3.\n(3) Query: The input CTI text that needs to be analyzed."}, {"title": "kNN-based demonstration retriever", "content": "Multiple studies [67], [57] have shown that prompt examples selection can significantly affect LLM's ICL capacity. One approach for selecting demonstration examples involves training a proxy LM to score candidates in the demonstration set [82]. However, this method requires large amounts of labeled data, which conflicts with our goal of designing a data-efficient solution. Recently, a k-nearest neighbors (kNN) method for selecting the most relevant demonstration examples based on semantic similarity has proven effective [57]. This method requires no dataset annotation or model tuning, making it ideal for our purposes. Specifically, we compute high-dimensional embeddings for the query and all candidate demonstrations using a pretrained embedding model. Among the models explored,"}, {"title": "4.3. Hierarchical Entity Alignment", "content": "Entity alignment identifies entities with different mentions that refer to the same real-world object, a key area in knowledge graph research [26]. Aligning these mentions integrates sub-graphs containing complementary knowledge, enhancing the comprehensiveness of the knowledge graph. Traditional entity alignment techniques rely on heuristics like string matching and structural similarities, which fail to capture the underlying semantics or context of entities and have limited accuracy.\nRecent studies [74], [78], [65] have adopted deep learning-based methods to learn vector representations (i.e., embeddings) of entities, achieving better accuracy. However, embedding-based techniques face unique challenges in our problem domain. In CTI text, entities with similar embeddings may refer to different concepts, e.g., \".akira files\" (an IOC) and \"Akira\u201d (a threat actor). Besides, comparing the semantic distance between every pair of entities has a computational complexity of $n^2$, where n is the total number of entities. This is inefficient when n becomes large.\nTo address these challenges, we perform entity alignment in a hierarchical way. The coarse-grained entity grouping module leverages LLM's ICL ability to assign types to entities. Entities assigned the same type are then grouped together as potential candidates for alignment, narrowing the scope for later fine-grained merging. Fig. 4 illustrates our prompt template. CTINEXUS automatically creates a customized prompt by assembling k carefully annotated demonstration examples. Each demonstration example contains an untagged triplet and a tagged triplet with subject and object entities assigned type labels. The query part automatically traverses all triplets generated by the triplet extraction phase. For each triplet, we add a placeholder, \"tagged_triplet\u201d: \u201cinsert your answer here\" to follow the format provided in the demonstration examples, better guiding the LLM to correctly fill in the answers.\nFor entities within each group, the fine-grained entity merging module uses an embedding-based technique to"}, {"title": "4.4. Long-Distance Relation Prediction", "content": "After entity alignment, the triplets form a set of disconnected subgraphs, leaving implicit relations between distant entities unidentified. Previous methods primarily rely on graph structure learning and graph neural networks [86], [48] to perform link prediction. However, these methods require large amounts of annotated graph data for model training. Additionally, in the CTI analysis domain, establishing relationships between distant cybersecurity entities requires a deep natural language understanding of their corresponding context. To make the procedure more data-efficient, we develop a long-distance relation prediction technique leveraging the ICL ability of LLMs. Fig. 5 illustrates our design.\nCreating links for every pair of distant entities would introduce excessive connections, complicating the CSKG and consuming significant computational resources. Thus, CTINEXUS first runs a depth-first search to find all connected subgraphs. Then, CTINEXUS leverages graph structure information to identify a central entity for each subgraph. A central entity represents the most important entity in the subgraph and will be the head for inter-subgraph connections. In our design, we identify central entities based on their degree centrality [84], which is the most widely used measure of a node's importance in a graph. It is easy to calculate, by counting the total number of edges that a node has to other nodes. The intuition is that an entity with the most explicit relations with other entities is more likely to be the core subject of that part of CTI text. Among all identified central entities, we further identify a topic entity, which is the one with the highest degree, representing the core subject of the entire CTI report. Specifically, we consider both incoming and outgoing edges when calculating degree centrality to identify the central identity. If multiple entities have the same highest score, we further prioritize out-degree over in-degree, as subjects in triplets (e.g., \u201cAndroxgh0st\" in <\"Androxghost\u201d, \u201ctargets\u201d, \u201c.env files\u201d>) are generally more important than objects. If there is still a tie, they are all determined as central entities. We follow the same procedure for identifying the topic entity. In the example shown in Fig. 5, there are five subgraphs. We identify the following central entities: \u201cVictim\u201d, \u201cAkira\u201d, \u201cthe ransomware Trojan", "Akira ransomware group\", and \".akira files\". We select \u201cAkira\u201d as the topic entity, which has the highest degree centrality score of 6. These central entities and the topic entity are then fed into the next module for relation inference.\nThe ICL-enhanced relation prediction module leverages ICL of LLM to infer implicit relations between each central entity and the topic entity, creating inter-subgraph connections. Fig. 5 illustrates our prompt template. For each central entity, CTINEXUS automatically creates a customized prompt by assembling k fixed, carefully annotated demonstration examples, similar to the entity alignment process. The prompt template consists of two sections: a demonstration section (blue) and a query section for the target task (yellow). Both sections include \"context": "question\u201d, and \u201cpredicted_triple", "context": "omponent presents the CTI report, while the \"question", "predicted_triple": "omponent contains the annotated relations for the demonstration examples and a placeholder, \"insert your answer here"}, {"title": "5. Evaluation", "content": "To comprehensively study the performance of CTINEXUS in various phases of CSKG construction, we set the following four research questions:\nRQ1: How does CTINEXUS's performance in extracting cybersecurity entities and relations compare to existing baseline methods?\nRQ2: How well does CTINEXUS perform in cybersecurity triplet extraction?\nRQ3: How well does CTINEXUS perform in knowledge graph construction?\nRQ4: What is the efficiency of CTINEXUS?"}, {"title": "5.1. Dataset Annotation", "content": "Existing datasets benchmark triplet extraction but do not encompass other procedures in our pipeline, and the CTI reports they include are mostly outdated. For example, the dataset constructed by LADDER is limited to reports from 2010 to 2021 [30]. To address this, we created a dataset to evaluate CTINEXUS across cybersecurity triplet extraction, hierarchical entity alignment, and long-distance relation prediction phases. Our dataset includes 150 reports from May 2023 onwards, Published by organizations like Trend Micro, Symantec, and The Hacker News, it includes 10 sources, averaging 15 reports each. Annotators with over three years of threat analysis expertise followed a four-phase process: Phase I involved annotating all cybersecurity entities and selecting entity types; Phase II identified explicit relations among entities and organized them into JSON-formatted triplets; Phase III grouped entities by type and merged those referring to the same threat-related concept; Phase IV selected central and topic entities based on degree and summarized implicit relations. This process resulted in 59,776 mentions, 35,258 entities, and 34,876 relations, enabling an effective evaluation of CTINEXUS 's performance in constructing cohesive and complete CSKGs."}, {"title": "5.2. RQ1: How does CTINEXUS compare against existing CTI extraction methods?", "content": "We evaluate CTINEXUS against two state-of-the-art baselines: EXTRACTOR [71] and LADDER [30], representing syntactic analysis-based and fine-tuning-based approaches, respectively. Several methodological challenges were addressed to enable fair comparison. For EXTRACTOR, we adapted its output to our broader ontology using CTINEXUS's coarse-grained entity grouping module. For LADDER, we addressed two key differences: (1) LADDER uses a word-level annotation format, where each token is labeled with its target class. In contrast, our dataset follows an end-to-end report-to-triplet format, where the entire report is input, and the label is a set of extracted triplets. (2) LADDER uses a simplified ontology derived from MALONT, which includes only 10 entity types, a subset of the entity types used in our ontology. To facilitate comparison, we developed scripts to tokenize our data and convert our manually annotated datasets into LADDER's word-level format. To ensure a fair comparison with LADDER, we merged our training set with LADDER's in a 5:1 ratio, maintaining their original training/validation split. We also replaced LADDER's test set with ours to ensure consistent evaluation on the same data. We compare with LADDER solely on named entity extraction performance. The reason is that our method focuses on open relation extraction, while LADDER targets relation classification within fixed categories.\n demonstrates that CTINEXUS outperforms EXTRACTOR in all metrics in cybersecurity triplet extraction. The evaluation results in Section 5.3 showed that GPT-4 outperforms all other backbone models. Thus, we use GPT-4 as the default backbone model for CTINEXUS's implementation. This superior performance can be attributed to several factors. First, CTINEXUS leverages the robust context understanding and instruction-following capabilities of LLMs and enhances specificity with kNN-selected demonstration examples for extracting triplets. In contrast, EXTRACTOR employs general fine-tuning to extract semantic roles not specific to any ontology, reducing its accuracy in triplet extraction. Also, the CTI context introduces peculiarities that lead to errors in EXTRACTOR's semantic role labeling module, which relies on a simple BERT model. For instance, EXTRACTOR might extract a triplet like (", "support": "numerous functions capable of abusing the Simple Mail Transfer Protocol (SMTP), such as scanning and exploiting exposed credentials and application programming interfaces (APIs), and web shell deployment", "Androxgh0st malware\", \"supports\", \"functions abusing SMTP": "."}, {"title": "5.3. RQ2: How well does CTINEXUS perform in cybersecurity triplet extraction", "content": "To demonstrate the effectiveness of CTINEXUS in cybersecurity triplet extraction, stemming from the superiority of the ICL paradigm and our specific prompt design, we conducted experiments on different ICL configurations, focusing on three aspects: (1) the number of demonstration examples, (2) the permutation of these examples, and (3) the backbone model types. By default, CTINEXUS uses GPT-4 as the model backbone, selects the k most similar prompt examples sorting in ascending order of query similarity.\nImpact of prompt example numbers. To investigate the impact of prompt example numbers, we evaluated 4 configurations: 1, 2, 3, and 4 examples. Our observations show effectiveness plateau when using 2 or 3 examples, while input ICL prompt size increases significantly with more examples. As shown increasing the prompt example number from 1 to 2 improves all metrics, particularly recall. However, with 3 examples, precision and F1-score plateau, and recall drops by 1%. With 4 examples, recall improves from 82 to 84%, but precision drops from 93 to 89%. This contradicts the heuristic that more examples always improve ICL performance but aligns with Chandra et al. [32], noting that each scenario has an optimal number of examples. Additionally, each additional example increases the input length by an average of 603 tokens, slowing inference speed and increasing computational costs. Thus, our implementation uses two examples in the cybersecurity triplet extraction phase, balancing effectiveness and efficiency.\nImpact of prompt example permutations. To analyze the effect of the permutation method for selected examples, we examined three strategies: (1) random selection and sorting (random), (2) selection based on kNN similarity and sorting in ascending order (kNN-ascend), and (3) selection based on kNN similarity and sorting in descending order (kNN-descend). These methods were chosen to explore the impact of recency bias in LLMs, which suggests that models give more weight to examples placed nearer to the query [58]. The random method serves as a baseline, while kNN-ascend and kNN-descend test the influence of example order based on similarity. As shown in kNN-ascend outperforms other methods across"}, {"title": "5.4. RQ3: How well does CTINEXUS perform in knowledge graph construction?", "content": ""}, {"title": "5.4.1. Hierarchical Entity Alignment", "content": "As described in Section 4, for entity alignment, we first apply ICL to perform coarse-grained grouping of entities based on their types. We then vectorize these entities into highdimensional embeddings and conduct fine-grained merging based on their semantic similarity. In the following, we present a series of experiments to investigate the impact of different configurations in entity grouping and entity merging, aiming to identify the optimal combination.\nImpact of demonstration numbers.\nWe assessed the impact of demonstration example numbers on ICL through comparative experiments with four example quantities: 1, 4, 8, and 12. Additionally, we evaluated the performance of two LLMs, GPT-4 and GPT3.5, across different model sizes. Notably, the performance showed no significant improvement once the number of examples exceeded 12, so these results are excluded from the table. Our evaluation methodology used accuracy, macro-F1, and micro-F1 metrics, consistent with previous text classification studies [63]. The experimental results, shown in indicate that GPT-4 consistently outperforms GPT-3.5 across all demonstration number hierarchies. Remarkably, GPT-4 with 1 demonstration yields better results than GPT-3.5 with 12 demonstrations. Both models show substantial improvements when increasing from one to eight demonstrations, but a saturation trend appears when the number of examples exceeds eight. This trend is especially evident in GPT-4, where all three metrics slightly decrease as demonstration numbers increase from eight to twelve.\nImpact of embedding models and merging threshold.\nThe entity merging module applies a text embedding model to vectorize candidate entities grouped by the entity grouping module and uses a merging threshold to identify equivalent entities. Our evaluation focuses on the selection of the embedding model and the determination of the merging threshold. We use OpenAI's third-generation embedding models, text-embedding-3-small and textembedding-3-large, which differ in vector size and represent the latest state-of-the-art general-purpose models. In addition, we also compare with SecureBERT[27], a cybersecurity-specific embedding model based on the ROBERTa architecture pre-trained on a large corpus of cybersecurity data. We consider merging thresholds of 0.4, 0.5, 0.6, and 0.7. Besides common metrics for entity alignment, we introduce Num_ent, which records the number of entities after alignment.\nThe experimental results are shown in Threshold values of 0.4, 0.5, and 0.6 all achieve a 100% recall rate, indicating the algorithm's ability to detect all entities that should be merged. However, lower thresholds can erroneously merge non-equivalent entities based on the Num_ent and precision metrics. The highest precision is observed when the merging threshold is 0.6. Increasing the threshold to 0.7 maintains preci-"}, {"title": "5.4.2. ICL-Enhanced Relation Prediction", "content": "As mentioned in Section 4, we compose ICL prompts to guide LLMs in inferring relations between disconnected subgraphs using the provided examples and context. We evaluated different ICL settings by varying the number of demonstration examples (1, 2, and 3) and the sizes of backbone models. Additionally, we examined the effectiveness of zero-shot learning, where the LLM infers relationships of given entities without demonstration examples. Zero-shot learning results are excluded from previous ICL experiments due to poor performance. The better performance in implicit relation inference compared to other tasks in CTINEXUS could be that relation prediction aligns more closely with general NLP tasks. Unlike triplet extraction or entity alignment, which require domainspecific knowledge in the cybersecurity context, relation prediction relies more on LLMs' general ability to infer connections between entities based on linguistic cues in the text. This makes relation prediction less dependent on specialized domain knowledge and more aligned with the LLM's general language understanding capabilities.\nExperimental results, shown in indicate that GPT-4 outperforms GPT-3.5 in every setting by a large margin, achieving a 100% recall rate compared to 92%-96% for GPT-3.5. The reason for this discrepancy is that GPT-3.5 has a higher tendency to produce hallucinated answers, either by not following the required instructions for the task (e.g., generating relations between entities not present in the queries) or by not adhering to the required format (e.g., generating a string instead of the requested JSON format). Both models show suboptimal performance with zero-shot learning. Increasing the number of demonstration examples from 1 to 2 significantly improves results, but a slight decline is observed with 3-shot examples. This suggests that while some examples can enhance performance, too many examples may introduce additional complexity or noise."}, {"title": "5.5. RQ4: What is the efficiency of CTINEXUS?", "content": "In this RQ, we assess the average token and time costs of three modules within CTINexus, using GPT3.5 and GPT-4 as backbone models. The results, shown indicate that using GPT-4 as the backbone results in token costs 20-30 times higher than those of GPT-3.5. Additionally, the time cost of using GPT-4 is approximately twice as high compared to GPT-3.5 for each module and the overall pipeline. The ICL-enhanced relation prediction module is the most computationally expensive, requiring multiple inferences for each input CTI. In contrast, the cybersecurity triplet extraction and hierarchical entity alignment modules have similar token costs, approximately half that of the long-distance relation prediction module, as they adhere to the \"one input, one inference\" principle, making them more economical. Specifically, for the hierarchical entity alignment module, the token and time costs are mainly attributed to the coarse-grained entity grouping module. The fine-grained entity merging module, which uses the text-embedding-3-large model, incurs minimal costs ($0.13 per 1M tokens), resulting in the entire experiment costing less than $0.30."}, {"title": "6. Discussion", "content": "Limitations. In CTINEXUS, the demonstrations must be carefully chosen and of high quality, with correct answers and the required prompt format. This ensures that CTINEXUS can fully utilize the ICL capability to infer the correct answers from the provided examples. According to Zhao et al. [85", "42": [76], "July 2022": "threat actors behind FARGO attacks were hijacking", "vulnerable Microsoft SQL servers": "instead of (\u201cvulnerable Microsoft SQL servers\u201d, \u201care hijacked by\u201d, \u201cJuly 2022\u201d), leading to a complete misplacement of the subject and object and an incoherent relation. This issue is more prevalent in smaller models like GPT-3.5, LLaMA3-70B, and QWen2.5-72B. While potential solutions include fine-tuning hallucination detection classifiers or using stronger LLMs for verification, we leave these challenges for future work. Our current focus is on CSKG construction under data scarcity, where GPT-4 has demonstrated reliable performance.\nEmpowering downstream defenses. Various applications can be potentially empowered by"}]}