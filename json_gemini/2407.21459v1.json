{"title": "KEMENKEUGPT: LEVERAGING A LARGE LANGUAGE MODEL ON\nINDONESIA'S GOVERNMENT FINANCIAL DATA AND\nREGULATIONS TO ENHANCE DECISION MAKING", "authors": ["Gilang Fajar Febrian", "Grazziela Figueredo"], "abstract": "Data is crucial for evidence-based policymaking and enhancing public services, including those\nat the Ministry of Finance of the Republic of Indonesia. However, the complexity and dynamic\nnature of governmental financial data and regulations can hinder decision-making. This study\ninvestigates the potential of Large Language Models (LLMs) to address these challenges, focusing\non Indonesia's financial data and regulations. While LLMs are effective in the financial sector, their\nuse in the public sector in Indonesia is unexplored. This study undertakes an iterative process to\ndevelop KemenkeuGPT using the LangChain with Retrieval-Augmented Generation (RAG), prompt\nengineering and fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of\nFinance, Statistics Indonesia and the International Monetary Fund (IMF). Surveys and interviews\nwith Ministry officials informed, enhanced and fine-tuned the model. We evaluated the model using\nhuman feedback, LLM-based evaluation and benchmarking. The model's accuracy improved from\n35% to 61%, with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation\nAssessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness with 73%\nfaithfulness, 40% precision and 60% recall, outperforming several other base models. An interview\nwith an expert from the Ministry of Finance indicated that KemenkeuGPT has the potential to become\nan essential tool for decision-making. These results are expected to improve with continuous human\nfeedback.", "sections": [{"title": "1 Introduction", "content": "Data is essential for comprehending issues, engaging the public, and enhancing public services. Furthermore, it fosters\nan environment that promotes robust, evidence-based decision-making [van Ooijen et al. 2019]. The Minister of Finance\nof the Republic of Indonesia, Sri Mulyani Indrawati, highlighted that the Ministry of Finance possesses vast data,\nwhich she described as a new type of data mine for the digital era [Canrakerta et al. 2021]. However, the vast and\nvaried nature of governmental financial data and regulations makes manual collection and analysis difficult. Frequent\nchanges in government rules and continuous updates to financial data further hinder access to real-time information,\nthereby delaying decision-making. Currently, the ministry depends on a dashboard that offers financial data information\nand a finance regulation information network website to find information about regulations. However, gathering new\ndata not already available on the dashboard and manually searching for regulations is time-consuming. Therefore,\nthere is a need for artificial intelligence (AI) systems that can extract insights from Indonesian financial data and\nregulations automatically. To address these challenges, LLMs could become a solution for analysing and extracting\ninformation from Indonesian government financial data and regulations. The state-of-the-art literature of LLMs in the\nfinancial domain mostly employs fine-tuning in financial private sectors, which are trained with corporation financial\ndata. However, this approach, which relies solely on fine-tuning, can generate misleading responses or hallucinations."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Related work", "content": "Several instances of chatbot implementation exist in the public finance sector. The Australian government's Tax Service\nchatbot, Alex, and Botty Bonn, a chatbot from a German city government, allow citizens to handle bill payments and\ntaxes [Chen et al. 2024]. Additionally, a chatbot could be used to explore open government data (OGD), including\ngovernment financial data [Porreca et al. 2018]. These chatbots enable users, including non-experts, to interactively\nsearch and explore government data through natural language, enhancing the ease of data access and interpretation.\nThis approach significantly improves public service delivery, allowing for more informed decision-making and greater\ntransparency. The benefits of improved accessibility and user-friendliness drive the integration of AI in government\noperations. With their advanced natural language processing abilities, chatbots offer a more intuitive way for citizens\nand officials to interact with government data. They are instrumental in managing large volumes of queries, offering\nreal-time responses, and handling complex data explorations. Thus, AI contributes to a more efficient decision-making\nprocess."}, {"title": "2.2 Data Driven Decision Making in Finance", "content": "Data-driven decision-making refers to the practice of making decisions based on data analysis rather than solely relying\non intuition [Provost and Fawcett 2013]. For example, a government tax officer could set a tax target for the next\nyear based purely on his/her experience and intuition about what might work. Alternatively, he/she could base his/her\ncalculation on data analysis regarding the previous tax target, achievements, and other factors that could influence the\ntax target. There are four types of analytics that could be employed, namely descriptive, diagnostic, predictive, and\nprescriptive [Sarker 2021]. Descriptive analytics focuses on summarising past data to offer insights into historical trends.\nDiagnostic analytics goes further by examining the underlying causes of specific outcomes, aiding in root cause analysis.\nPredictive analytics utilises statistical models to forecast future trends based on historical data, enabling proactive\ndecision-making. Lastly, prescriptive analytics goes beyond predictions, offering recommendations and optimisation\nstrategies to achieve desired outcomes. Data analytics is a crucial concept encompassing the exploration, understanding,\nand communication of new insights and significant patterns derived from large datasets across various application\ndomains. It aims to facilitate swift, high-quality and efficient decision-making processes [Ojokoh et al. 2020]. Despite"}, {"title": "2.3 LLMs in Financial Domain", "content": "LLMs are transformer-based models with hundreds of billions of parameters, trained on extensive datasets, enabling\nthem to understand natural language and address complex tasks effectively [Zhao et al. 2023], such as GPT-4 [OpenAI\n2023], LLaMA [Touvron et al. 2023], PaLM [Chowdhery et al. 2023], LaMDA [Thoppilan et al. 2022], Galactica [Taylor\net al. 2022], and BERT [Devlin et al. 2018].Additionally, there are domain-specific LLMs, which are increasingly\nrecognised as crucial for effectively processing textual documents to extract relevant information in specific contexts.\nThis methodological approach has proven to significantly boost the performance of language models, ensuring their\naccuracy and effectiveness in specific contexts [Chakrabarty et al. 2019]. Among the various applications of domain-\nspecific LLMs, one notable area is the financial domain. Recent work in the financial domain has introduced tools such\nas FinGPT [Liu et al. 2023]. FinGPT is a large language model specifically designed for the financial sector. It aims\nto democratise financial data and language models by providing an open, accessible framework for researchers and\npractitioners. This model adopts a data-centric approach, emphasising the importance of data acquisition, cleaning, and\npreprocessing to develop robust financial applications. This approach demonstrates how LLMs can assist in various\nfinancial tasks, from automated advising to trading analysis using news, social media, company filings, and research\ndatasets. Another model, BloombergGPT [Wu et al. 2023], has been trained on extensive financial data, including\nnews, filings, press releases, web-scraped financial documents, and social media sourced from Bloomberg archives.\nBloombergGPT is a specialised large language model with 50 billion parameters designed for the financial sector. It\nis trained on a vast, domain-specific dataset of 363 billion tokens derived from Bloomberg's financial data sources.\nThis dataset is supplemented by 345 billion tokens from general-purpose datasets, making it one of the most enormous\ndomain-specific datasets utilised for a language model. Furthermore, FinBERT [Huang et al. 2023] is a financial\nsentiment analysis model based on BERT architecture, which has been specially adapted for the financial domain. It\nutilises BERT's powerful pre-training and fine-tuning capabilities to specifically tackle natural language processing\n(NLP) tasks within the financial sector, such as sentiment analysis of financial texts. It has been trained on a vast corpus\nof financial texts, including corporate filings, analyst reports, earnings conference call transcripts, and CSR reports."}, {"title": "3 System Development", "content": "This study's design explores an iterative process for developing an LLM application with LangChain. The research\nprocess is designed to be cyclic rather than linear, emphasising continuous improvements through repeated cycles of\ndata collection, processing, development, experiment, evaluation, and refinement\nKemenkeuGPT has been developed with LangChain framework and iterative development process with RAG, prompt\nengineering, and fine-tuning. LangChain is a framework that streamlines the development of applications that LLMs.\nAs an integration framework for these models, LangChain is applicable in various areas commonly associated with\nlanguage models, such as analysing and summarising documents, developing chatbots, and examining code. LangChain\nhas shown considerable promise within the AI ecosystem by introducing a novel approach that allows developers to\ninteract seamlessly with various data sources and applications. LangChain's modular abstractions and customisable,\nuse-case-specific pipelines mark its flexibility and efficiency. These features position it as an invaluable tool for\nthe future development of LLM applications [Topsakal and Akinci 2023]. In the financial domain, LangChain's\ncapabilities have been effectively demonstrated in tasks related to financial analysis. A Financial Large Language Model\n(FLLM) processes the initial corpus to create a link between the analysed input and various knowledge sources, such as\nexpert domain knowledge, financial databases, and search engines. This FLLM, designed for multitask prompt-based\nfine-tuning, addresses three essential components of financial analysis and interpretation: event matching and analogy,\nevaluation of viewpoint quality, and extraction of key points. The approach focused on data through LangChain offers a\npromising direction for leveraging the potential of LLMs in the complex financial domain. RAG is a method that merges\ncontext retrieval capabilities with LLMs for generating language [Cai et al. 2022][Lewis et al. 2020]. It functions in a\ntwo-phase manner. Initially, it uses a retrieval module to find pertinent documents based on the given prompt, often\ndrawing from external sources such as news outlets, research papers, and social media for added context. Subsequently,\nthese documents are integrated with the initial prompt and input into the LLMs, culminating in the production of\nthe final output. There is a retrieval-augmented LLM framework for financial sentiment analysis. This framework\nincludes an instruction-tuned LLM module, which ensures the role of LLMs as predictors of sentiment labels and a\nretrieval-augmentation module that retrieves additional context from reliable external sources. Benchmarked against\ntraditional models and LLMs like ChatGPT and LLaMA, this approach achieves a 15% to 48% performance gain in\naccuracy and F1 scores [Zhang et al. 2023]. RAG addresses the issue of factual accuracy in LLM outputs by retrieving\ncontextually relevant external information. This technique enhances the model's responses, especially in complex"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset Description", "content": "This study gathered a comprehensive dataset from multiple reliable sources to create a solid basis for detailed LLM\ndevelopment. Open to the public, the data was procured from several vital organisations, each providing unique\nand essential information for the research. The Ministry of Finance provided various financial data and regulations\n[of Finance of Republic of Indonesia 2023]. Additional data was provided by Statistics Indonesia, which offers"}, {"title": "4.2 LangChain Implementation", "content": "This research implemented LangChain, a recent open-source software library that has gained attention in the AI\ncommunity for offering solutions to streamline the development of custom AI applications using LLMs.\nFurther exploration focused on using LangChain components, such as document loaders, chains, and text splitters,\nto create the most suitable environment for data extraction from the dataset. Since the dataset consisted of multiple\ndocument formats, another experiment was conducted using the DirectoryLoader. The DirectoryLoader is a component\ndesigned to handle the loading of documents from a specified directory efficiently, thus simplifying the management and\nprocessing of multiple files stored in one location. Therefore, this study explored cloud solutions for document loading.\nInstead of storing the dataset in local storage, it was stored in Google Cloud Storage (GCS), and the GCSDirectoryLoader\nwas utilised. The GCSDirectoryLoader, designed to interact with GCS, facilitates the loading and processing of files\nstored in a GCS bucket, making it a crucial tool for applications that require significant performance and scalability\nbenefits.\nDuring this phase of the project, we experimented chain types, including Stuff, MapReduce and Refine. Additionally,\ntwo distinct vector databases, namely ChromaDB and the Pinecone Vector Database, were compared to determine the"}, {"title": "4.3 Iterative Development", "content": "The development and evaluation process for KemenkeuGPT comprised six phases:\n\u2022 Improvement through Experiments with LLMs.\n\u2022 Improvement through Stakeholder Feedback.\n\u2022 Improvement through Additional Data for RAG.\n\u2022 Improvement through Prompt Engineering.\n\u2022 Improvement through Fine-Tuning."}, {"title": "5 Results and Discussions", "content": "After conducting several experiments and configurations, the results from each phase were systematically recorded\nand analysed. According to the human evaluation, the accuracy of KemenkeuGPT increased with each improvement.\nSince KemenkeuGPT is a continuous improvement model that uses human feedback, its performance is expected to\nkeep improving in the future. By the end of this research, the accuracy of KemenkeuGPT reached 61%. This result\nwas achieved after iterative development using the base model, adding more data for RAG, prompt engineering and\nfine-tuning methods.  Initially, the accuracy\nwas only 35%. After adding more data for RAG, it increased to 42%. Exploring prompt engineering raised it to 60%\nand fine-tuning brought it to 61%. Even though the result is still below 70%, KemenkeuGPT has shown significant\nimprovement, which is a positive sign for future enhancements."}, {"title": "6 Conclusions", "content": "Large Language Models (LLMs) have been studied for its application in extracting information and insights from\nIndonesian financial data and regulations using the LangChain framework. The process is iterative, involving Retrieval-\nAugmented Generation (RAG), prompt engineering and fine-tuning. The experimental results show that this iterative\nprocess improves KemenkeuGPT's performance. Although KemenkeuGPT's performance is not exceptionally high, it\nstill outperforms its base model and six other models in answering Indonesian financial data and regulations questions.\nAdditionally, since KemenkeuGPT is a continuous improvement model with human feedback, its performance is\nexpected to improve over time. An expert interview highlighted KemenkeuGPT's potential to become an essential tool\nfor decision-making. These results indicate that KemenkeuGPT, as an LLM, can enhance the decision-making process\nby extracting information and insights from Indonesian financial data and regulations.\nHowever, despite KemenkeuGPT's superior performance compared to other models, there are limitations. First,\nKemenkeuGPT currently has access to only partial data on Indonesian financial data and regulations. One reason is\nthat the Ministry of Finance rejected our requests for additional data. Furthermore, in compliance with the terms of a\nnon-disclosure agreement, our analysis is confined to an aggregate overview of government financial transactions. This\nrestriction prevents detailed analysis of individual transactions, which could offer more profound insights. Consequently,\nusing partial and aggregate data might limit the study's scope and the depth of its findings. Secondly, KemenkeuGPT\ncannot update its dataset, so new data must be manually uploaded to the database. Thirdly, continuous human feedback\nevaluation is required due to the rigidity of financial data and regulations, ensuring the content is accurate and not\nmisleading. This process needs to be manually assessed by the Ministry of Finance experts, which can be time-\nconsuming. Additionally, the current user interface of KemenkeuGPT is limited to a web view from Flutter and could\nbe improved.\nSeveral enhancements could refine the research for future work. Expanding the dataset with additional resources would\nimprove the RAG process and model performance. Including detailed transaction data would provide users with richer"}]}