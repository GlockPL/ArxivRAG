{"title": "Separable Mixture of Low-Rank Adaptation for Continual Visual Instruction Tuning", "authors": ["Ziqi Wang", "Chang Che", "Qi Wang", "Yangyang Li", "Zenglin Shi", "Meng Wang"], "abstract": "Visual instruction tuning (VIT) enables multimodal large language models (MLLMs) to effectively handle a wide range of vision tasks by framing them as language-based instructions. Building on this, continual visual instruction tuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks, accommodating evolving functionalities. While prior work has advanced CVIT through the development of new benchmarks and approaches to mitigate catastrophic forgetting, these efforts largely follow traditional continual learning paradigms, neglecting the unique challenges specific to CVIT. We identify a dual form of catastrophic forgetting in CVIT, where MLLMs not only forget previously learned visual understanding but also experience a decline in instruction following abilities as they acquire new tasks. To address this, we introduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework, which employs separable routing through two distinct modules-one for visual understanding and another for instruction following. This dual-routing design enables specialized adaptation in both domains, preventing forgetting while improving performance. Furthermore, we propose a novel CVIT benchmark that goes beyond existing benchmarks by additionally evaluating a model's ability to generalize to unseen tasks and handle diverse instructions across various tasks. Extensive experiments demonstrate that SMoLoRA outperforms existing methods in mitigating dual forgetting, improving generalization to unseen tasks, and ensuring robustness in following diverse instructions.", "sections": [{"title": "1. Introduction", "content": "Building on the foundation of Large Language Models (LLMs) [18, 35], Multimodal Large Language Models (MLLMs), e.g., [5, 23, 45], have demonstrated strong performance across a range of tasks, such as visual question answering (VQA) [4, 20], image captioning [1, 23], and visual reasoning [15, 37]. MLLMs are typically trained through a multi-stage process [23, 45]. In the pre-training stage, the model acquires general knowledge across multiple modalities, while the visual instruction tuning stage enables it to learn and address various vision-related tasks by framing these tasks as language instructions. This approach transforms the model into a versatile, general-purpose multimodal system capable of following a wide range of user-defined instructions to solve specific tasks.\nVisual instruction tuning for MLLMs is traditionally performed within a static multitask learning framework [5, 23], where a unified image-instruction-output data format is used. However, in real-world applications, MLLMs must be capable of learning new vision-language tasks to accommodate evolving requirements. This has sparked growing interest in continual visual instruction tuning (CVIT), which allows MLLMs to incrementally learn new tasks, reducing the significant costs associated with retraining models from scratch. Despite its potential, existing studies on continual learning have demonstrated that sequential fine-tuning can lead to catastrophic forgetting, where models forget previously learned knowledge when updated with new tasks. While recent work on CVIT [3, 12] has made strides in developing new benchmarks and strategies to mitigate forgetting, these efforts largely follow traditional continual learning paradigms and overlook the distinct challenges posed by CVIT's integration of vision and language tasks.\nIn this work, we identify a dual form of catastrophic forgetting in CVIT. First, the model often loses its previously acquired visual understanding when updated with new tasks (as shown in Fig. 1 (a)). Second, its ability to follow instructions deteriorates as more tasks are learned over time (Fig. 1 (b)). Mixture-of-LoRA (MoLoRA) [24], originally developed for multitask learning, has been applied to CVIT [3]. MoLoRA uses multiple LoRA blocks coupled with a router network to capture task-specific knowledge, aiming to reduce forgetting. However, we find that this approach is less effective. The challenge in extending MoLoRA from multitask learning to continual learning lies in the complexity of dynamic routing. In multitask learning, tasks typically share common components, enabling efficient routing based on task similarity. In contrast, continual learning involves a constantly evolving stream of tasks with diverse, and sometimes conflicting, data distributions. This variability complicates the routing process, as the model must not only accommodate new tasks but also preserve previously learned knowledge from being overwritten or degraded. Without a proper routing strategy, new tasks can interfere with the retention of older knowledge, exacerbating the dual forgetting problem in CVIT.\nWe propose Separable Mixture-of-LoRA (SMoLoRA), a new approach specifically designed to tackle the dual forgetting problem in CVIT. SMoLoRA introduces separable routing, implemented through two distinct modules: a visual understanding module and an instruction following module. As illustrated in Fig. 2, unlike MoLoRA, which relies solely on input tokens to infer task-specific information, SMOLORA's routing strategy leverages both visual understanding and instruction following capabilities to more accurately assess the nature of the input. This dual-domain approach enables specialized adaptation in each domain, allowing the model to preserve and refine both its visual and instruction following abilities without interference. By dynamically selecting LoRA blocks tailored to the specific needs of each task, SMoLoRA effectively mitigates the dual forgetting problem. Our contributions are summarized as follows:\n\u2022 We uncover the issue of dual catastrophic forgetting in CVIT and introduce the SMOLORA framework as a solution. By leveraging separable routing, SMOLORA dynamically selects appropriate LoRA blocks tailored to the specific needs of each task, effectively preserving and refining both visual understanding and instruction following capabilities\u2014thus mitigating dual forgetting.\n\u2022 We propose a new CVIT benchmark that not only evaluates model accuracy and forgetting on standard tasks but also assesses the model's ability to generalize to unseen tasks and handle diverse instructions across a wide range of tasks, offering a more comprehensive evaluation.\n\u2022 Through extensive experiments on our CVIT benchmark, we show that SMoLoRA outperforms existing methods in addressing dual forgetting, enhancing generalization to new tasks, and demonstrating robust instruction following across diverse scenarios."}, {"title": "2. Related Work", "content": "Visual instruction tuning. Recent advancements in Instruction Tuning [28, 40] have significantly enhanced the ability of language models to better understand and accurately follow complex human instructions across a variety of contexts. Building upon this paradigm, Visual Instruction Tuning [13, 32] further extends the capabilities by integrating both visual and textual data, enabling MLLMs to execute instructions that involve multiple data modalities. Models such as InstructBLIP [5], LLaVA [23], and MiniGPT-4 [45] exemplify this approach by leveraging large-scale pre-training and sophisticated alignment techniques to unify vision and language understanding. SVIT [42] constructs a large-scale dataset to enrich the diversity and informativeness of instruction-tuning data. Similarly, Vision-Flan [38] has developed a human-annotated instruction tuning dataset encompassing a broad spectrum of tasks, thereby enabling its application in advanced MLLMs. Unlike previous works that focus on static visual instruction tuning, this work addresses continual visual instruction tuning scenarios, where models need to adapt to new tasks over time without forgetting previous knowledge.\nMixture of Experts. Mixture of Experts (MoE) is a successful deep learning approach [8, 17, 21, 30] that increases model capacity and efficiency by dividing the model into specialized experts, activated by a gating network for spe-"}, {"title": "3. Method", "content": "3.1. Continual Visual Instruction Tuning\nGiven a pre-trained MLLM f, continual visual instruction tuning (CVIT) aims to incrementally fine-tune f to learn a set of vision tasks T = {T1, T2,..., Tn}. At each time step t, the model is presented with a new task Tt alongside a new dataset Dt, to integrate this new knowledge into its existing capabilities. Typically, the dataset Dt follows the format {Xinst, Xvist, Xanst}, where Xins, Xvis, Xans denote textual instruction input, visual input, and linguistic answer, respectively. However, when learning the new task Tt, previously seen data {Di}t\u22121i=1 is inaccessible, which introduces the challenge of dual catastrophic forgetting: 1) The model tends to forget previously learned visual understanding ability when updated with new data, and 2) Its ability to follow instructions gradually declines as additional tasks are learned. Next, we introduce an SMoLoRA approach to efficiently mitigate the dual catastrophic forgetting problem in CVIT.\n3.2. Separable Mixture-of-LoRA\nThe MLLM f typically consists of a vision encoder, an LLM, and an adapter. The vision encoder extracts visual features, which the adapter converts into word embeddings, enabling the LLM to interpret visual inputs (as shown in Fig. 3). During visual instruction tuning, only the LLM and adapter are updated, while the vision encoder remains frozen. For efficient task-specific updates within CVIT, we use LORA fine-tuning. However, applying a single, shared LORA across all tasks could lead to catastrophic forgetting in continual learning. To overcome this challenge, we propose a Separable Mixture-of-LoRA (SMoLoRA) approach tailored for CVIT, building upon the Mixture-of-LORA (MOLORA) framework [24].\nMOLORA, initially proposed for multitask learning, consists of a set of LoRA blocks along with a router network G. This router network dynamically selects different LoRA blocks based on the input, effectively allowing the model to specialize in different tasks. For a single LoRA, a low-rank update is applied by decomposing the adaptation matrix \u0394W = BA, where B \u2208 Rk\u00d7r and A \u2208 Rr\u00d7d, with r < min(d, k). d and k are the input and output dimensions of the original weight matrix Wo of the pre-trained model. For MoLoRA, the adaptation matrix \u0394W is typically represented as:\n\n \u0394W = \\sum_{i=1}^{N} G(z)_i W_i, \n\nwhere G(z)i is the weight assigned to the i-th LoRA block predicted by the router network G with the router input z. \u0394Wi is the adaptation matrix of the i-th LoRA block.\nExtending MoLoRA from multitask learning to continual learning presents a complex dynamic routing challenge. In multitask learning, tasks often share common expert components, allowing for efficient routing based on task similarity. However, in continual learning, the model is exposed to an evolving stream of tasks with highly diverse and sometimes conflicting data distributions. This variability makes the task of selecting the right experts more nuanced and non-trivial. As new tasks emerge, the model must adapt its routing strategy to not only accommodate new data but also ensure that previously learned tasks are not negatively impacted. Otherwise, new tasks can overwrite or diminish the knowledge associated with previous ones, resulting in dual forgetting problem in the context of CVIT.\nTo address this challenge, SMOLORA introduces the concept of separable routing, implemented through two distinct modules: a visual understanding module and an instruction"}, {"title": "3.3. Separable Routing and Adaptive Fusion", "content": "Separable routing. In visual understanding, the model analyzes both the visual features of the image and the key information guided by the text. Therefore, we employ an instance-based routing method that prioritizes the overall information from the instance inputs. For the current linear layer input xl\u22121 \u2208 Rd\u00d7s, we compute the average feature of the current instance, denoted as Avg(xl\u22121), across the sequence dimension s. We maintain a router matrix Rvu \u2208 RM\u00d7d and let zvu = Avg(xl\u22121) to select LoRA blocks for the input to the visual understanding module:\n\n G^{vu}(z^{vu}) = softmax\\big( top_k(R^{vu} Avg(x_{l-1}))\\big), \n\nwhere topk(\u00b7) selects the top k values, setting the rest to \u2212\u221e, and softmax(\u00b7) normalizes these values.\nIn contrast, the differences in instruction requirements are also a key factor contributing to the task discrepancies. The instruction following module uses the embedding of the current task instruction f0(Xins) \u2208 Re\u00d71 as the input zif for routing, which helps reduce interference among different task requirements during continual learning:\n\n G^{if}(z^{if}) = softmax\\big( top_k(R^{if} f_0(X^{ins})\\big), \n\nwhere Rif \u2208 R(N\u2212M)\u00d7e is the router matrix maintained by the instruction following module.\nAdaptive fusion. Considering that the contributions of the two modules' router strategies may not be identical during learning, we adopt an adaptive approach to assign weights to the outputs of these two modules. First, we compute the outputs of the two modules xvu and xif separately:\n\n x^{vu} = W^{vu} x_{l-1},  x^{if} = W^{if} x_{l-1} \n\nLet Ivu \u2208 R1\u00d7k and Iif \u2208 R1\u00d7k represent the trainable importance matrices of the visual understanding module and the instruction following module, respectively. The"}, {"title": "4. CVIT Benchmark", "content": "4.1. Benchmark Construction\nWhile previous studies [3, 12] have established benchmarks for Continual Visual Instruction Tuning (CVIT) in MLLMs, they have yet to fully address key challenges, such as generalizing to unseen tasks, adapting to diverse instructions, and assessing instruction following ability. To address these challenges, we present our CVIT benchmark, as shown in Fig. 4. Our benchmark construction can be described from three key aspects:\nDataset collection and standardization. Our CVIT benchmark integrates widely used vision-language datasets, covering diverse task types such as VQA, image classification, and image captioning. In response to the issue of varying input-output formats across different task types (e.g., the lack of language guidance in classification tasks), we standardize all tasks into a unified textual instruction format to accommodate a sequential stream of tasks, thus ensuring a consistent transformation into a text-to-text format [41].\nContinual stability and transfer generalization. The learning and evaluation process of the CVIT benchmark is divided into two stages: upstream continual learning and downstream knowledge transfer. For the upstream tasks, we sequentially select ScienceQA [26], TextVQA [34], Flickr30k [29], ImageNet [6], GQA [16], and VQAv2"}, {"title": "4.2. Evaluation Metrics Design", "content": "To evaluate the CVIT benchmark, we utilize multiple metrics to assess overall performance and the degree of forgetting. Let ak,j denote the accuracy on the j-th task (where j < k) after fine-tuning on the k-th task. We define the performance at the current stage and across all previous stages as Average Performance (AP) and Mean Average Performance (MAP), respectively, following training on k tasks:\n\n AP_k = \\frac{1}{k} \\sum_{j=1}^{k} a_{k,j},  MAP_k = \\frac{1}{k} \\sum_{i=1}^{k} AP_i. \n\nIn assessing the model's degree of forgetting, backward transfer [36] is employed as a metric:\n\n BWT_k = \\frac{1}{k-1} \\sum_{j=1}^{k-1} (a_{k,j} - a_{j,j}). \n\nAdditionally, to further evaluate the model's instruction following ability after fine-tuning the k-th task, we design a"}, {"title": "5. Experiment", "content": "5.1. Experiment Setup\nComparison methods. We compare our method against a diverse set of alternatives to highlight its superior performance. Multitask and Zero-Shot represent training all tasks jointly and no task-specific training, respectively. Dir-LORA introduces a separate LoRA for each individual task, while SeqLORA [14] sequentially fine-tunes all tasks using a single shared LoRA. We also include the enhanced LoRA variant, DoRA [25], for comparison. Additionally, we evaluate classic continual learning approaches such as EWC [19] and Replay [2]. Finally, we assess MoeLoRA [3], a token-wise MoLoRA method tailored for CVIT.\nImplemented details. We use the pre-trained LLaVA-v1.5-7B [23] as the base model for our experiments. The hyperparameters for SMOLORA are configured as follows: a learning rate of 1 \u00d7 10\u22124 with a cosine decay schedule, and a batch size of 64. Training is performed over a single epoch. The number of LoRA blocks is set to 4 for both visual understanding module and instruction following module, with each LoRA having a rank of 16, and topk(\u00b7) set to top1(\u00b7). Both SMoLoRA and the baseline methods are applied specifically to the feedforward network (FFN) layers of the LLM and the adapter.\n5.2. Main Results\nUpstream tasks. In the upstream tasks, we perform continual fine-tuning on the model sequentially, starting from ScienceQA and progressing to VQAv2, under both single- and multi-type instruction settings. We evaluate model performance after fine-tuning on VQAv2. As shown in Table 1, our method consistently outperforms alternative approaches by a substantial margin, achieving superior results on both individual tasks and overall evaluation metrics.\nIn the single-type instruction setting, each dataset follows a single instruction format. Compared to MoeLORA, our approach achieves impressive improvements in terms"}, {"title": "5.3. Ablation Study", "content": "Effect of the components in SMoLoRA. We begin by examining the impact of each component in SMOLORA on the single-type CVIT benchmark. The VU, IF, and AF modules represent the visual understanding, instruction following, and adaptive fusion components, respectively. Without any of these modules is equivalent to SeqLoRA. In contrast, when the VU or IF module is added, performance improves significantly, with the IF module yielding the most substantial gains, particularly in the MIF metric. This underscores the critical role of the instruction following in task learning within CVIT. When both the VU and IF modules are combined by averaging their outputs, performance improves further. The inclusion of the AF module provides an additional boost, emphasizing its effectiveness in enhancing the model's overall capabilities.\nEffect of varying the number of LoRA blocks. Fig. 6 illustrates the impact of varying the number of LoRA blocks in SMOLORA. As the number of LoRA blocks increases, the model's performance improves across all metrics. This improvement occurs because a greater number of modules provides more options for task-specific selection, reducing interference from dissimilar tasks. However, when the number of LoRA blocks reaches higher values (e.g., 8 or 16), performance stabilizes, indicating that additional LORA blocks do not significantly enhance learning. This suggests that further increasing the number of modules beyond a certain point may lead to inefficient resource usage without providing substantial benefits.\nEffect of routing of LoRA blocks. To further explore how SMOLORA mitigates the dual catastrophic forgetting issue, we randomly select a fixed number of examples from each dataset and visualize their LoRA blocks routing behaviors in Fig. 7. Longer color bars indicate a stronger preference for specific LoRA blocks. Within the VU module's four LORA blocks, we observe considerable variation in selections across different tasks. However, similar tasks, such as the captioning tasks of Flickr30k and TextCaps, show converging routing patterns. In the IF module, where routing is driven by task-specific instructions, tasks with differ-"}, {"title": "6. Conclusion", "content": "In this paper, we identify a dual form of catastrophic forgetting in CVIT and propose an efficient SMOLORA method. SMoLoRA leverages separable routing to dynamically assign appropriate LoRA blocks to different tasks, thereby preserving the model's visual understanding and instruction following capabilities, mitigating the dual catastrophic forgetting. Additionally, we introduce a new CVIT benchmark that evaluates not only model accuracy and forgetting on standard tasks but also its ability to generalize to unseen tasks and adapt to diverse instructions. Extensive experiments on the CVIT benchmark demonstrate the superior performance of our SMoLoRA over existing methods."}, {"title": "Appendix", "content": "A. Effect of Weight Ratio\nTo further investigate the impact of the weight ratio [\u03b1, \u03b2] within the adaptive fusion module, we randomly sampled the values of \u03b1 and \u03b2 during inference across different layers of the fine-tuned model, as illustrated in Fig. 8. The analysis reveals that \u03b1 and \u03b2 exhibit significant variations across layers. Furthermore, we computed the average values of \u03b1 and \u03b2 for all layers. The resulting ratios indicate that the instruction following module plays a more pivotal role in the routing process compared to the visual understanding module.\nB. Results at Different Stages\nAs shown in Table 5-6, we present the experimental results of SeqLoRA and our SMoLoRA method at various stages from ScienceQA to VQAv2 on the CVIT benchmark. The results demonstrate that with each additional training stage, our approach consistently maintains stable and superior performance across different datasets.\nC. More Experiments on MLLMs\nTo further assess the versatility of our SMOLORA method, we implemented it on another advanced MLLM, MiniGPT-4 [45], as shown in Table 7. While MiniGPT-4 exhibits a lower degree of forgetting compared to LLAVA, our method still yields significant overall performance enhancements compared to SeqLoRA. Notably, on the MIF metric, we observed improvements of 17.98% and 20.90% in single- and multi-type instruction settings, respectively. These results underscore the effectiveness of our method in mitigating forgetting in instruction following.\nD. Details of CVIT Benchmark\nTable 8 provides an overview of the CVIT benchmark, detailing the instruction templates for each dataset and the number of samples.\nScienceQA [26]: ScienceQA is a comprehensive dataset consisting of science-related questions and answers designed to evaluate and enhance the reasoning and problem-solving capabilities of AI models in scientific domains.\nTextVQA [34]: TextVQA is a visual question answering dataset that focuses on questions requiring models to read and understand text embedded within images to provide accurate answers.\nFlickr30k [29]: Flickr30k is a large-scale image dataset containing over 30,000 photos sourced from Flickr, each annotated with multiple descriptive captions, widely used for training and evaluating image captioning and vision-language models.\nImageNet [6]: ImageNet is a large-scale visual dataset containing millions of annotated images across thousands of object categories, widely used for training and evaluating computer vision models.\nGQA [16]: GQA is a visual question-response dataset comprising complex compositional questions about images, designed to evaluate and enhance AI models' reasoning and relational understanding capabilities in interpreting visual content.\nVQAv2 [10]: VQAv2 is a widely-used visual question answering dataset that consists of images paired with diverse questions and multiple corresponding answers, designed to assess and enhance the ability of models to understand and reason about visual information.\nVizWiz [11]: VizWiz originates from real-world visual question answering scenarios in which visually impaired individuals capture images and pose spoken questions about them. Each visual question is accompanied by 10 crowd-sourced answers, facilitating the development of assistive technologies for the visually impaired community.\nTextCaps [33]: TextCaps is an image captioning dataset that requires models to generate descriptive captions by reading and interpreting text embedded within images, thereby enhancing the ability to incorporate textual information into visual descriptions.\nOCRVQA [27]: OCRVQA is a visual question answering dataset designed to evaluate the ability of the models to read and comprehend text embedded within images to accurately answer related questions.\nPlaces365 [44]: Places365 is a large-scale scene recognition dataset comprising over 1.8 million images across 365 diverse scene categories, widely used for training and evaluating computer vision models in understanding and classifying various environments."}]}