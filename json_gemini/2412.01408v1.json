{"title": "Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning", "authors": ["Aditya Narayan Sankaran", "Reza Farahbaksh", "Noel Crespi"], "abstract": "Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts.", "sections": [{"title": "1 Introduction", "content": "The widespread adoption of social media for everyday communication requires safeguards and moderation to create a safe space for the user and the social community. With audio-based social media platforms like Twitter (now X) Spaces, Clubhouse, Discord, ShareChat etc, moderating offensive language and hate speech has become essential in maintaining a safe space for people online. These platforms host users from diverse linguistic backgrounds, especially in multilingual countries like India. India is home to several languages with more than 30 Million speakers of languages and has experienced a phenomenal increase in the use of online social media services, including Facebook, Twitter (now X), Instagram, LinkedIn, and YouTube, with over 250 million users (and growing) helping them in Social interactions and conversations (Ganguly and Kumaraguru, 2019; Palakodety and KhudaBukhsh, 2020). 76% of Indians spend an estimated 1 hour and 29 minutes daily using social media through smartphones, with adolescents between the ages of 13 and 19 making up 31% of the overall number of people who use social media (Dar and Nagrath, 2022; Srivastava et al., 2019). Given that a large share of users of social media are teenagers and young adults, there is an absolute need to create a safe online space for them to express their views freely without being exposed to hate-filled and offensive content. In an era where social media and entertainment companies are being scrutinized more carefully than ever before, laxity related to user privacy or community rule violations could prove harmful and costly for social audio platforms. There have been reports of incidents where Clubhouse rooms engaged in racist and anti-Semitist talks\u00b9. This issue raises concerns about how it can be more difficult to safeguard user privacy and security on Audio Social Media platforms as traditional content moderation techniques utilized for text-based media do not necessarily work well with audio-based platforms. Abusive language detection and Hate Speech detection in Low Resource Language settings has been a popularly researched topic, especially in the text modality. Transfer Learning-based approaches have shown incredible performance in abuse detection using existing popular models like BERT, ROBERTa, XLM-ROBERTa etc (Mozafari et al.,"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Audio Abuse Detection", "content": "The introduction of DeToxy (Ghosh et al., 2021), a large-scale multi-modal dataset was a significant advancement in the field of audio toxicity detection, particularly in the context of spoken utterances. DeToxy improved text-based methods' performance and lessened keyword bias, paving the way for more robust audio abuse detection. Building on this work, Facebook AI extended on DeToxy by creating MuTox (Costa-juss\u00e0 et al., 2024), one of the first large-scale multilingual audio datasets for toxicity detection. MuTox includes 20,000 audio utterances in English and Spanish and 4,000 utterances across 19 other languages. However, it is worth noting that this dataset only includes three Indian languages: Bengali, Hindi, and Urdu. The first Bengali Audio Abuse dataset was introduced by (Rahut et al., 2020), where Transfer Learning was applied to extract features from 960 voice recordings of native Bengali speakers. Following this, ADIMA (Gupta et al., 2022) intro-"}, {"title": "2.2 Few-Shot Learning and Meta-Learning", "content": "Few-shot learning (FSL) is particularly significant in low-resource settings where data scarcity is a major challenge. Model-agnostic Meta-learning (MAML) has emerged as a powerful method within this domain. (Singh et al., 2022) proposed a MAML-based Low Resource ASR methodology using a multi-step loss (MSL) approach, which significantly improved the stability and accuracy of low-resource speech recognition systems compared to the traditional MAML approach. (Gu et al., 2018) demonstrated the effectiveness of MAML in low-resource scenarios for Neural Machine Translation, significantly outperforming multilingual transfer learning methods on the Romanian-English WMT'16 dataset with only limited translated words. (Xia et al., 2021) proposed MetaXL, a method that effectively transforms representations from auxiliary languages to target languages, enhancing cross-lingual learning in tasks such as sentiment analysis and named entity recognition."}, {"title": "2.3 Automatic Speech Recognition", "content": "Meta-learning approaches have also shown promise in automatic speech recognition. (Hsu et al., 2019) proposed MetaASR, which significantly outperformed the state-of-the-art multitask pretraining approach across various target languages with different combinations of pretraining languages. Furthermore, (Conneau et al., 2020) introduced XLSR, which learns cross-lingual speech representations by pretraining a single model from raw speech waveforms in multiple languages, enabling a single multilingual speech recognition model that is competitive with strong individual models. Hou et al. (2021a) explored the combination of adapter modules with meta-learning algorithms to achieve high ASR performance in low-resource settings while improving parameter efficiency. In another study, (Hou et al., 2021b) proposed SimAdapter, a novel algorithm for learning knowledge from adapters for cross-lingual speech adaptation, showing that these approaches can be integrated to achieve significant performance improvements, including a relative Word Error Rate (WER) reduction of up to 3.55%. While these works have highlighted the progress of various tasks like Abuse Detection, Machine Translation, Automatic Speech Recognition etc using Meta-Learning techniques in low-resource settings, there is an avenue for using Meta-Learning for Audio Abuse detection. This work contributes to the research gap and serves as a foundation for Audio Abuse Detection in Low Resource Languages, especially in Indian Languages, employing Few-Shot Learning and Pre-Trained Audio Representations."}, {"title": "3 Methodologies", "content": "Representations that are effective across general audio tasks capture multiple robust features of the input sound, thereby using learned embeddings for classification tasks like Music Information Retrieval, Industrial Sound Analysis, etc (Niizumi et al., 2022; Grollmisch et al., 2021). Building on these studies, we employed a Model-Agnostic Meta-Learning (MAML) approach (Finn et al., 2017) to develop a few-shot classifier for cross-lingual audio abuse detection using pre-trained audio features. This approach leverages the adaptability of meta-learning to handle the complexities of low-resource and multilingual settings, ensuring improved performance in audio-based abusive"}, {"title": "3.1 Pre-Trained Audio Feature Extractions", "content": "Features from Pre-Trained Audio Models were used for few-shot classification using MAML. We employed the CLSRIL-23 variant of Wav2Vec (Gupta et al., 2021), which is a self-supervised learning-based audio pre-trained model that learns cross-lingual speech representations from raw audio across 23 Indic languages. It is built on top of Wav2Vec 2.0 (Baevski et al., 2020) and solved by training a contrastive task over masked latent speech representations and jointly learning the quantization of latents shared across all languages. We also used Whisper (Radford et al., 2022), a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrated a strong ability to generalise to many datasets and domains without the need for fine-tuning. These extracted features were then normalised using the two methods:\nTemporal Mean: This process involves computing the mean of the vectors along the temporal dimension for each tensor."}, {"title": "3.2 Model Agnostic Meta-Learning (MAML)", "content": "Meta Agnostic Meta Learning is a technique introduced by (Finn et al., 2017). The goal of few-shot meta-learning is to train a model that can quickly adapt to a new task using only a few data points and training iterations. For this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials. In effect, the meta-learning problem treats entire tasks as training examples. For our few-shot learning setup, we perform stratified sampling of k samples per class for each language. Formally, let $D_l = \\{(x_i, y_i)\\}_{i=1}^{N_l}$ denote the dataset of audio samples for language l, where $x_i$ represents the feature vector and $y_i$ the class label (abusive or non-abusive). For a given k-shot scenario, we construct a support set $S_l \\subseteq D_l$ such that:\n$S_l = \\{(x_{i_1}, y_{i_1}), (x_{i_2}, y_{i_2}), ..., (x_{i_k}, y_{i_k})\\}$ (4)\nwhere $y_{i_j} \\in \\{$abusive, non-abusive$\\}$ and each class is represented equally within the support set. Specifically, for a k-shot learning task with k = 2, $S_l$ contains one abusive and one non-abusive sample per language. Assuming there are L languages, the total number of samples for the k-shot scenario is:\n$|S| = k \\times L$ (5)\nwhere S represents the combined support sets across all languages. For instance, in a 50-shot scenario across 10 languages, this results in 50 \u00d7 10 = 500 samples."}, {"title": "3.3 Cross-Lingual Training and Testing", "content": "The few-shot model is trained using a cross-lingual approach, which is key to ensuring the model's ability to generalise across different languages. During training, the model is exposed to data from all L languages, so that learning from the pre-trained representations captures the nuances of abusive and non-abusive speech across different contexts and languages. This cross-lingual training strategy enables the model to recognize similarities in abusive language across Indian languages, which is essential for achieving strong performance in low-resource scenarios where data for individual languages is limited. By leveraging a cross-lingual setting, the model can better generalize and identify abusive patterns even when specific language data"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "To carry out our study, we employed the ADIMA dataset (Gupta et al., 2022) by ShareChat. It contains 11,775 audio clips, sourced from real-life conversations, in 10 Indian Languages annotated for a binary classification task. It is an evenly distributed dataset comprised of 5,108 abusive and 6,667 non-abusive samples from 6,446 unique users."}, {"title": "4.2 Feature Extraction", "content": "The code for extracting features for all the audio clips was with PyTorch (Paszke et al., 2019) and HuggingFace libraries. In this task, we compared two pre-trained audio models: Whisper (Radford et al., 2022) and Wav2Vec (Baevski et al., 2020), specifically, the whisper-large variant for Whisper and the CLSRIL-23 variant of Wav2Vec (Gupta et al., 2021) were used for extracting pre-trained features. Firstly, embeddings were extracted by passing raw audio files through these models. The embeddings were then feature-normalised to generate decision-level features (Wang et al., 2023) for our task in two ways: Temporal Mean (equation 1) and L2-Norm (equation 2) to generate two different feature sets."}, {"title": "4.3 Few-Shot Experimental Setup", "content": "We employed the Model-Agnostic Meta-Learning (MAML) algorithm (Finn et al., 2017) to address the challenge of few-shot cross-lingual audio abuse detection. The few-shot learning methodology is particularly suited to scenarios where only a limited number of labelled examples are available for each language, ensuring efficient and effective learning from small datasets. We utilized stratified sampling (Mitchell, 1996) to sample audio clips for the few-shot task ensuring that the proportion of abusive and non-abusive samples remained balanced across languages. As discussed in Section 3.2, for a shot size \"k\", k-samples are chosen in the 10 languages, thereby making the number of samples for cross-lingual training 10 x k-samples (refer equation 5). To evaluate the model's performance across different few-shot settings, we conducted experiments with varying shot sizes: 50, 100, 150, and 200. These shot sizes were selected to investigate the impact of sample size on model performance, particularly in scenarios where the number of available samples is less than half of the average number of audio clips per language, which is approximately"}, {"title": "4.4 Model Architecture and Training", "content": "The learner model utilized in our experiments is an Artificial Neural Network (ANN) consisting of three fully connected layers. The network architecture was designed as follows: an input layer with a size corresponding to the dimension of the extracted feature vectors (1024 for Whisper and 768 for Wav2Vec), followed by hidden layers with sizes 256 and 128 respectively, and a final output layer with size 2, with leaky ReLu for non-linearity, corresponding to the binary classification task. The output layer utilized a softmax activation function to convert the raw logits into probabilities for each class, determining whether an audio clip was abusive or non-abusive. Training of the learner model was done using the Adam optimizer (Kingma and Ba, 2017). We employed a task-specific learning rate and a meta-learning rate of 0.001, both of which were managed by a linear learning rate scheduler with default parameters as provided by the PyTorch optimizer library. The model was trained with a batch size of 128 and for a total of 150 epochs, based on repeated testing, which provided a good balance between training"}, {"title": "5 Results", "content": ""}, {"title": "5.1 Classification Results", "content": "We present accuracy scores from both feature settings in 4 shot settings [50, 100, 150, 200] as heatmaps in Figures 1 and 2. More Detailed results with macro-F1 scores are provided in the Appendix A. An aggregate macro-f1 score table with the baseline from the Dataset paper (Gupta et al., 2022) is also presented in the Appendix in Table 4. It is evident that Whisper with the L2-Norm feature normalisation has consistently better scores across languages, with no top accuracy scores in the 200-shot scenario and most of the best-performing accuracy scores are in the 50 and 100-shot settings. Comparing normalisation settings, we can observe that L2-Norm has much better classification performance compared to the temporal mean normalisation setting. For most languages, L2-Norm offers higher accuracy and macro-F1 scores compared to Temporal Mean. For Bhojpuri with Whisper, L2-Norm gives significantly better accuracy (82.75% at 50 shots) compared to Temporal Mean (79.17% at 100 shots). Some other results that are evident are the not-so-consistent performance of Tamil and Kannada, especially Tamil, with its highest Accuracy scores being 74.93% in the Temporal Mean Normalisation setting of Wav2Vec and 78.98% in the L2-Norm Mean Normalisation of Whisper. Across both models and normalization methods, F1 scores generally tracked closely with"}, {"title": "5.2 Pre-Trained Feature Study of Abusive Language", "content": "Pre-trained audio representations offer a powerful perspective for studying abusive language, especially in Low-Resource contexts where labelled data can be scarce or imbalanced. Models like Whisper, employing vast amounts of training data to learn robust audio feature representations enable transfers to downstream tasks like abuse detection. This approach allows us to bypass the need for large annotated datasets and explore the underlying audio characteristics that distinguish abusive language across diverse linguistic groups. To understand the improved performance of L2-norm feature normalization in conjunction with Whisper's"}, {"title": "6 Conclusion", "content": "With the widespread adoption of social media for everyday communication, the need for effective content moderation has become critical to keeping malicious actors in check. While there has been extensive research on text-based moderation, safeguarding against abusive content in audio remains underexplored, particularly for low-resource languages. In this work, we propose a few-shot cross-lingual audio abuse detection method in low-resource languages, specifically focusing on Indian languages by employing the Model-Agnostic Meta-Learning (MAML) framework (Finn et al., 2017). Our method addresses the challenge of detecting abusive audio content with limited training samples per language, a key issue in low-resource settings by benchmarking our approach on the ADIMA dataset (Gupta et al., 2022), which provides binary-labeled audio clips for abuse detection in 10 Indian Languages, leveraging pre-trained audio representations from Wav2Vec and Whisper. To provide a deeper understanding to enhance the performance of the few-shot classification, we investigated the impact of various feature normalization techniques, such as Temporal Mean Normalization and L2-Norm, applied to the extracted features from the pre-trained audio models and we also conducted a comparative study across these normalization methods and pre-trained models. To better understand the effect of feature normalization and to identify the best-performing model, we conduct a visual analysis of the learned audio representations by plotting the features using a 2D t-SNE plot. We were able to observe language similarities and have discussed why L2-Norm with Whisper Features performed well compared to other feature normalisation techniques and Pre-Trained Audio Representations (Section 5.2). Our research contributes to the ongoing efforts to combat abusive content on social media platforms, particularly in the audio domain, by providing insights into meta-learning, feature normalization, and performance in low-resource language settings."}, {"title": "Limitations", "content": "While this work explores cross-lingual few-shot audio abuse detection in the 10 languages ADIMA provides, we believe this methodology can also be expanded to other low-resource languages. Further research will be needed to assess efficiency. Exploring other Meta Learning Algorithms like ProtoMAML (Triantafillou et al., 2020) and Contrastive Learning (Saeed et al., 2021) will also contribute to addressing these issues. While this is a cross-lingual abuse detection task, there are avenues for Mono-Lingual Experiments too for more specific languages and also with other languages for cross-lingual tasks. Whisper and Wav2Vec have been the only models that have been used but future works will involve exploring other pre-trained audio models like SeamlessM4T (Communication et al., 2023) and different feature normalisation such as other L-N Normalisation, Weighted Averaging (Phukan et al., 2024) and more. Since this work deals with Low Resource Languages in the Indian context, an important limitation is the absence of training data in other Indian Languages since Languages like Telugu and Marathi have been missed out, which are other major spoken languages with about 71 million people who speak Marathi as their native tongue. (Garje et al., 2016) and Telugu with 82.7 million native speakers (Jaswanth et al., 2022). Training data in these languages will add diversity and more languages to detect, improving variety and diversity, thereby being inclusive of all languages. Given the scarcity, we would also like to see the creation of curated datasets for offensive speech detection in other Low-Resource Languages, including ones from the Global South, in the audio modality."}, {"title": "Ethics Statement", "content": "This study does not involve any personal or public data pointing to an individual or a group of individuals and thus does not break any ethical guidelines."}]}