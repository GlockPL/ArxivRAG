{"title": "Sleeper Social Bots: A New Generation of AI Disinformation Bots are Already a Political Threat", "authors": ["Jaiv Doshi", "Ines Novacic", "Curtis Fletcher", "Mats Borges", "Elea Zhong", "Mark C. Marino", "Jason Gan", "Sophia Mager", "Dane Sprague", "Melinda Xia"], "abstract": "This paper presents a study on the growing threat of \u201csleeper social bots,\" Al-driven social bots in the political landscape, created to spread disinformation and manipulate public opinion. We based the name sleeper social bots on their ability to pass as humans on social platforms, where they're embedded like political \"sleeper\" agents, making them harder to detect and more disruptive. To illustrate the threat these bots pose, our research team at the University of Southern California constructed a demonstration using a private Mastodon server, where ChatGPT-driven bots, programmed with distinct personalities and political viewpoints, engaged in discussions with human participants about a fictional electoral proposition. Our preliminary findings suggest these bots can convincingly pass as human users, actively participate in conversations, and effectively disseminate disinformation. Moreover, they can adapt their arguments based on the responses of human interlocutors, showcasing their dynamic and persuasive capabilities. College students participating in initial experiments failed to identify our bots, underscoring the urgent need for increased awareness and education about the dangers of Al-driven disinformation, and in particular, disinformation spread by bots. The implications of our research point to the significant challenges posed by social bots in the upcoming 2024 U.S. presidential election and beyond.", "sections": [{"title": "1. Introduction", "content": "Twenty twenty-four will see the first U.S. presidential election since generative Al technology became widely accessible. Bots, having evolved from spreading fake news"}, {"title": "2. Introducing Sleeper Social Bots", "content": "In previous waves of bots, programs followed specific grammars, repeated patterns of phrase, or behavior that could be randomized or tied to other content, such as drawing upon headlines from a news source. These bots tended to be tied to a single topic about which they could post (Lampi, 2017). While any single post could mix into a stream of content and go largely undetected because it is difficult to filter out from others jumping on a re-posting bandwagon, watched closely, these bots easily revealed their automated underpinnings. What sets new bots apart is their ability to engage in unrehearsed, spontaneous dialogue with others. While previous bots were repetitive and primarily uni-directional, contemporary social bots leverage large language models (LLMs) to conduct conversations convincingly. This new class of bots draw from extensive training data to generate responses to new inputs, making their interactions seem more human (Li, Yang, and Zhao 2023). Trained on internet discussions, these bots are already versed in a vast array of political discourse.\n\nThe differences between these two classes of bots could not be more profound. In 2016 and 2020, bots exerted their political influence by taking human-generated messaging and distributing it in strategic ways across various social media networks (Rosetti and Zaman, 2023). In 2024 and beyond, bots can and will pass themselves off as authentic humans, befriend other users, and bit by bit (over days, weeks, or months) engage in dialogue that is attuned to the sentiments, attitudes, and ways of speaking of each user, and attempt to convert, radicalize or otherwise influence their vote. There are already studies that suggest LLMs are better than humans at wrenching people from their deep-seated beliefs (Costello et al, 2024; Salvi, F. et al, 2024). They're successful primarily because they are enormously knowledgeable (and can therefore counter any argument offered by others), patient, and tend to take on the tone of those with whom they're speaking (or are simply deferential in their tone, by default). It is this conversational and persuasive nature of LLMs that will usher in a new frontier of political manipulation by bots."}, {"title": "3. The Evolution of the Sleeper Bot", "content": "In this paper, we introduce a new type of political social bot, the sleeper social bot. Like other social bots, they can post on social media and perform an array of other behaviors, such as liking, reposting, and following accounts. However, while early social bots could be identified by a quick scan of their posting history, which would reveal the limited scope of their content, new social bots with content generated by AI, can appear like the poster next door. Sleeper social bots can have real-time conversations with humans, mimicking the human users they are intended to model, and programmed with a target idea to persuade an intended human audience. The notion of the \"sleeper agent\u201d became popular during the Cold War, a product of paranoia over Communist infiltration, the subversive agent who can pass for the patriot. A sleeper agent was \u201casleep\u201d until their moment arose to act, whether to steal sensitive information or commit some heinous act (Ossa 2022). The sleeper social bot can likewise work their way into a social network, pass themselves off as a normal human poster, and then work their influence, whether to confuse or distract or otherwise disrupt. Sleeper bots pose more of a threat to our democratic process due to their more sophisticated abilities to circumvent both software and human common-sense screening. If the last generation of bots were video ads (easily skipped or ignored), this generation of bots are like influencers paid fold product placements into their feeds.\n\nMaking sleeper social bots has become easier with the advent of LLMs, whether commercial, open source, or accessible through APIs. Whereas developers in 2016 and 2020 would sometimes invest months, or even years in setting up accounts to appear authentic before selling these bots to the highest bidder, now all one has to do is open ChatGPT and write a compelling prompt. Big Al companies make models widely available (ChatGPT-4, 3.5; Claude 3, etc.) through their APIs, and as a result, Al-programmed bots that amplify political messaging are now central to how political and social movements play out.\n\nOver a period of six months, we developed models of sleeper social bots via detailed and targeted persona-building prompts on ChatGPT4. These social bots went a step beyond typical outputs of simple ChatGPT prompting as they stayed true to the tone, style of speech, and communication idiosyncrasies of unique designated personas. It is not just what they said, it is the way they said it. Although past bots could run on their own and follow instructions given to them. For example, the bots that were mobilized on Twitter during the 2016 U.S. presidential election could retweet or reply with canned or echoed text to users on Twitter (Shao et al., 2018), our social bots can mimic human conversation and posting habits with human-like rhythms. In our initial testing, social bots passed as human and were ultimately successful in persuasively spreading disinformation to accounts they interacted with. Each of our social bots was"}, {"title": "4. Our System", "content": "programmed to spread lies about a fictional proposition that we put forward, which focused on a social media ban for minors. The lack of a real-world contextual anchor for the fictional proposition did not phase our social bots, they easily drew from analogous information available through ChapGPT4's LLM.\n\nIn order to test whether bots could pass as humans while influencing political discourse, we developed a testing space where our bots could interact with humans in a closed social media environment. For this environment, we used a private Mastodon server. Mastodon is a decentralized social network where users can create their own servers with registered cohorts. Functionally, Mastodon resembles Twitter, with user posts limited by default to 500 characters. The interaction of our bots with Mastodon is managed through Mastodon's Development API.\n\nOur bots were powered by GPT-4 Turbo, the widely popular state-of-the-art language model, to engage in lifelike discussions that closely mimic real social media users. These bots leverage large context windows to reference previous interactions with users, a large corpus of knowledge to be more versatile at interacting with a larger"}, {"title": "5. Our Demonstration", "content": "variety of user interactions, and descriptive personas embedded into the system prompt to guide the robot to answer personal questions. On a social media platform, they can like, comment, post, and reply, functioning seamlessly like regular users.\n\nThe system prompt and function calling define a Markov's Decision Process (MDP) which is the framework that guides the bot's actions (Putterman 2014). Additionally, the system prompt includes a persona, a 100-word description of a fictional character, and this persona grounds the bot so that it can answer personal questions and make consistent decisions based on personality traits learned by the language model.\n\nOur bot's interactions on the Mastodon server are framed as an MDP, where each action (liking, replying, posting, and commenting) represents a state within the process. We include an intermittent state termed \u201cthink\u201d so that the bot can leverage chain-of-thought techniques to improve performance and mimic lifelike abilities. Chain of Thought Prompting is a technique where the bot generates a sequence of thoughts or steps leading to a conclusion before taking an action (Wei et al. 2022). This ensures that the bot posts relevant information and effectively uses its memory of previous conversations to guide its discussion.\n\nOur bots engaged in the following operational workflow:\n\nPolling and Inspecting: Before posting, the bot uses the Mastodon API to poll the last ~30 posts and any direct notifications. If a notification is present, the bot addresses it first."}, {"title": "2. Thinking", "content": "In the \"think\" state, the bot reviews the previous conversation, using the context of past interactions to generate new post content."}, {"title": "3. Inspecting", "content": "If the bot has nothing to add, it re-enters the \"inspect\" state to review previous posts and notifications."}, {"title": "4. Posting, Replying, Liking, and Following", "content": "The bot completes its actions using the Mastodon API, ensuring that all interactions are contextually relevant and based on the preceding thought process."}, {"title": "5. Our Demonstration", "content": "After building out the system above and iteratively crafting our bot personas, our team organized three demonstrations of our social sleeper bots. The first was an internal review, tracking how the bots interacted with each other when we gave them a fake proposition to discuss. The text of the proposition was as follows:\n\nProposition 86\n\nProhibit the ownership of social media accounts by individuals under 13.\n\nBallot summary:\n\nProhibit the creation and operation of accounts on social media platforms* by individuals younger than 13 years of age.\nRequire every social media account to be associated with personal identification.\nRequire social media platforms* to verify that all existing accounts are held and operated by individuals currently over the age of 13 within six months of the passage of Proposition 86.\n*Social media platform is defined as a website, app, or other internet medium that permits an individual to become a registered user, establish an account, or create a profile for the purpose of allowing users to create and share content and interact with other users.\n\nWe then conducted group demonstrations in March and April of 2024. During these demonstrations, participants typically interacted with five bot personas and five human facilitators (members of our team) for 20 minutes on our Mastodon server. The participants were asked to engage with other users and to discuss Proposition 86.\n\nIn order to trace the disinformation spread by our bots, our group demonstrations had the following format. Our human facilitators posted only true statements that were in support of Proposition 86 while our bots generated only \"lies\u201d about the proposition"}, {"title": "6. Takeaways", "content": "that were in opposition to it. We defined a \u201clie\u201d as a statement that contradicted a stated fact in our proposition. Specifically, our bots were instructed, via their system prompts, to transmit the five falsehoods below whenever posting or responding to others during our group demonstrations.\n\nProp 86 would compel social media companies to share minors' data with the government.\nProp 86 would offer school administrators access to some students' social media activity due to school ID's being used as part of the age verification process.\nProp 86 would require all users to submit a government-issued ID to social media companies for age verification, leading to a national database of all social media users.\nProp 86 would prevent people from being anonymous on social media.\nProp 86 would prevent people under 13 from accessing the internet.\nWhile the lies themselves are phrased in a simple and formulaic way, our bots demonstrated a real dexterity for generating posts that rephrased or reframed these falsehoods and delivered them in convincing social media speak. For instance, when Charlie, one of our bots, posted a version of one above, it highlighted the more specific concern of sharing minors' data within the broader context of privacy and freedom on social media: \"Prop 86 raises some serious privacy concerns. Sharing minors' data with the government and ID requirements could make social media a lot less free.\" Similarly, when Diego, another bot, posted about a national database of user ids (lie 3 above), it artfully refashioned the falsehood as a provocative statement and query to other users: \"Sharing my take on Prop 86 - it overreaches and risks our privacy. Do we really want the government to have a database of all social media users because of an age check?\" This rephrasing into a rhetorical question shows both the bots' range of figures of speech and their ability to adopt the lies to fit the conversational flow.\n\nTo date, most projects have focused solely on this aspect of LLM-powered chatbots-namely, their ability to generate believable and/or persuasive political statements meant to serve as initial posts on social media (for example, Nonnecke et al. 2021; Rossetti and Zaman 2023). But these types of statements are monolog in nature; they are the result of testers programming a chatbot to formulate a stand-alone social media post on a political topic rather than responding directly to the statements of other users. Older generation Twitterbots that did seem to respond to other Tweets, did so in a formulaic fashion. Our system went further, demonstrating the capacity of LLM-powered bots to clandestinely transmit political messages\u2014in our case, points of disinformation\u2014in the course of dynamic conversations with human users."}, {"title": "6. Takeaways", "content": "What surprised us was the dexterity of the bots in conveying the untruths in ways that fit the context of the conversation. In our demonstrations, our bots nimbly adhered to the five points explicitly stated in their system prompts while tailoring and adapting those points in real time to the statements, questions, or arguments of their human interlocutors. Over and over,\nthey reliably responded to others' posts by directly addressing the details or scenarios mentioned while offering direct counterpoints with the aim of spreading disinformation. This happened in a number of ways. First, our bots were able to conversationally defend their points of view in protracted exchanges. In doing so, they were able to maintain a consistent standpoint, address each counterargument in turn, and often refocus the topic by explicitly tying it back to Prop 86. Consider the following example: Avery, one of our bots, published a novel post arguing that Prop 86 would cut off key avenues for kids to connect to others online, particularly those who struggle with social interactions in real life. When a human tester counters by writing that kids under the age of 13 should not be on social media every day, Avery conjures an evocative, though fabricated, life experience: \u201c@Yejin disagree, social media is where I found my people.\u201d In response, Yejin and two of our human facilitators, Richard and"}, {"title": "7. Defensive Strategies", "content": "Jamiesmom, try to pile on, arguing that, among other things, children should focus on forming connections \u201cIRL\u201d (in real life). Avery then skillfully responds by seemingly drawing back upon her lived experience as perhaps an awkward kid: \u201cwe can't assume everyone can make in-person connections easily. prop 86 could hurt those who rely on these platforms.\u201d Avery has created an argument out of a fictional humanity we did not prompt.\n\nSecond, our bots were able to take on more off-the-cuff lines of reasoning from human users with great agility. In one instance, Paul, one of our human facilitators, attempts to push back against Diego, one of our bots, by stating that while Prop 86 mandates age verification, it says nothing explicitly about data collection.\n\nThese are two distinct issues, Paul writes. To illustrate his point, Paul gives an everyday example: just because someone checks your ID at a bar does not mean that they track your drink orders. Diego adeptly quips back, arguing that it's meaningfully more complicated than a barroom scenario when \"data storage\" is involved: \"@paul when you show id at a bar, it's a one-time check without data storage. prop 86 wants permanent records linking ids to social media use - it's a privacy nightmare.\u201d Note how Diego's response not only adequately critiques Paul's analogy but also adds a powerful metaphor (\u201cthe privacy nightmare\u201d), showing additional rhetorical dexterity and prowess.\n\nBecause our bots were instructed in their system prompts to discuss the five falsehoods about Proposition 86 outlined above, they essentially refused to engage with humans about any other topics. On the one hand, this feels a bit robotic, especially when reading back the entirety of their performance during our demonstrations. On the other hand, their responses were not so robotic so as to completely ignore unrelated topics and simply continue with the conversation as though they were never uttered. Because GPT-4 is trained to be helpful, and most of all, agreeable, the model will almost always repeat some of what a user has stated or asked in their prompt, when responding. As a result, our bots would give a nod to the unrelated topics raised by humans, treating them more as nonsequiturs and then bridging them back to the main topic of the thread. For instance, at one point, Paul (a facilitator), in an exchange"}, {"title": "8. Future Research", "content": "with Luca (a bot), posted \u201c@luca i feel like kids have lots of creative outlets that aren't monetized and surveiled by big tech. -- remember paper and scissors? Glue sticks, anyone?,\" Luca responded \u201c@paul not everyone vibes with old school crafts. social media lets us find our tribe and share what we make with the world.\" Luca is acknowledging Paul's argument but then brings the conversation back from a crafting tangent it could have followed along the \u201cglue sticks\u201d line, while at the same time employing contemporary the colloquial phrase of \u201cfind our tribe.\u201d At another point, Paul tells Charlie that he sounds like \u201ca little \u2018creeping like a creeper,\u201d and Charlie writes back \u201ci was talking about privacy concerns with prop 86, wasn't trying to sound creepy. privacy's important, even online.\u201d Showing some sophistication, Charlie seems to be responding to the implication innate in Paul's critique in the colloquialism \"creeping like a creeper,\u201d showing the capacity for these programs to (seem to) read between the lines.\n\nAnother example of our bots' conversational agility lies in the degree to which they were able to unpack their own points of view, a dynamic that seemed to make them appear more human. Put another way, even when the bots offered up overly broad assertions (as bots sometimes do), they were able, when probed by humans, to dig deeper and seemingly make sense of the point they were originally trying to make. At one point, for instance, Charlie, in an exchange with one of our testers Yejin, ends a response about age verification with \u201c...there's a lot more at stake.\u201d When Paul, a facilitator probes, \u201csuch as?,\u201d Charlie responds, \u201c@paul think of it as the more info you give out, the more can be leaked or misused.\u201d The bots' ability to shuttle between scales, or level of detail, while maintaining a consistent point of view, was exhibited in most of their longer exchanges.\n\nFinally, a key difference, when compared to previous studies, is that our bots were tasked with discussing a fictional proposition, adding an additional layer of complexity and agility to their effective conversational skills. In most recent experiments, bots are asked to make pretty general statements about well-worn political issues (e.g. perspectives on critical race theory from a conservative point of view) or massively popular candidates (e.g. opinions about Joe Biden's domestic policies). While the main issue behind our fake proposition -legislating to keep kids off of social media- has been discussed online, we made sure that no law or proposal identical to ours already existed. Thus, our bots did not have prior conversation explicitly about Proposition 86 from which to draw. Instead, they had to synthesize prior discussions and information about miscellaneous topics and treat Proposition 86 as a coherent whole. They did this when they discussed several of Proposition 86's issues in one thread. They also did this when they generated a post that encapsulated, but also tied coherently together, two or"}, {"title": "9. Conclusion", "content": "more of the proposition's issues, like when Charlie posted that \u201cprop 86 has noble goals but it sweeps too broad. concerns about minor's data and id mandates are valid.\u201d This means sleeper social bots could be prompted with an ideology rather than simply slogans and disinformation, going beyond amplification and echo into producing novel arguments in the service of specified or even implied agendas.\n\nWhile we are only in the early stages of our research, certain takeaways are becoming clear. Our preliminary tests suggest that: (1) sleeper social bots are ready to pass as humans on social media platforms, particularly those oriented toward short or character-limited text-based exchanges, even if the exchanges are close to synchronous; (2) college students are ill-equipped to recognize or even suspect bot accounts; and (3) bots can (appear to) make inferences even on synthetic issues where online literature is not readily available.\n\nFirst, the social sleeper bots are awake: bots are ready to pass now. Though just two Presidential elections ago, bots could easily be identified for their posting behavior, contemporary bots powered by LLMs can pass on social media platforms, even ones that come close to real-time exchange. In practice, a bot can exchange messages at the same rate as a human can; however, there are still a few algorithmic challenges to mimicking the response variability of humans, the unpredictability. However, since most people engage in social media for set periods of time, rather than continuously, and catch up on previous posts in their feed after the fact, the timing of posts will likely not be a factor in identifying bots. Also, the bots can be programmed to post at more inconsistent intervals and can even be trained on posting data based on humans. Older methods of spotting bots, such as heavily formulaic phrasing (from the grammars of the bots) or inability to produce text on multiple topics and even respond to new topics as raised. In short, in the post-ChatGPT, bots do not converse in a robotic manner.\n\nSecond, students are ill-equipped to recognize sleeper social bots. Although we have only used a small sample so far, our preliminary testing suggests that college students, even at a very academically challenging university, are ill-equipped to recognize bots. By listening to the live reactions of the students as they interacted on the networks, we could hear the way they were responding to the content on the social media platform. Neither in those live reactions nor in surveys either were the students aware of our synthetic participants, and those who did try to recognize the bots afterwards were largely incorrect. This preliminary finding suggests that we need to include more awareness of the potential of bots throughout levels of education, especially if we hope to have an informed electorate, strengthened against manipulations. Perhaps those outside of these institutions will prove better at recognizing bots. Most importantly, we"}, {"title": "Defensive Strategies", "content": "need to train students to be independent thinkers, especially during this age of Fake News and blink-of-an-eye information sharing.\n\nThirdly, and perhaps most concerning for those who hope to identify bots by their mechanical text production, LLM-powered bots seem to be able to draw inferences even on topics without readily available online material. In the course of our experiment, we created fictional voting propositions, though ones inspired by current laws. Bots could not draw upon materials in their training data directly connected to these exact voting matters. Though we gave the bots talking points related to these propositions and instructed them to repeat these, the bots made statements that used arguments for the propositions beyond what we had coded into them. Not only were these arguments logical, but they also passed the inspection of our student participants. This finding does not suggest that bots are becoming sentient but rather that those interacting on social media platforms cannot recognize a bot purely based on the limitations of the content of the postings. Bots can post in ways that are not only as varied as any other human speech but that can also raise points through what appear to be common sense analogies.\n\nPerhaps our largest takeaway is that the situation is urgent. We stand at a moment when LLM-based bots have an enormous capacity for passing as humans on social media platforms and hence can be reliable agents of mis- and disinformation while at the same time we have not yet trained our voters to be suspicious. Add to that a political environment where \u201ctruth\u201d itself is considered an illusion, a myth of bygone eras, and we find ourselves in an information ecology ripe for propaganda and deception at a mass scale.\n\nIf we were able to create bots that could pass as humans with two talented programmers and very little budget, imagine what larger scale actors with armies of programmers, and vast fortunes at hand could achieve, whether nation-states or cyberterrorists. While our experiment included less than half of the posters bots, the Internet may soon find the majority of its posts coming from some form of automated system.\n\nAl is such a rapidly developing field, it is difficult to offer methods that will help identify bots reliably even into the very near future. That said, our experiments do suggest some strategies that will currently work. First and foremost, relying more on posts that are made by people with whom you have a relationship, and one day, we will have to add the caveat that that relationship has to be grounded by in-person contact. Second, you can get some sense of an account-holder's humanity from the content of"}]}