{"title": "FORECASTBENCH: A DYNAMIC BENCHMARK OF A\u0399 FORECASTING CAPABILITIES", "authors": ["Ezra Karger", "Houtan Bastani", "Chen Yueh-Han", "Zachary Jacobs", "Danny Halawi", "Fred Zhang", "Philip E. Tetlock"], "abstract": "Forecasts of future events are essential inputs into informed decision-making. Machine learning (ML) systems have the potential to deliver forecasts at scale, but there is no framework for evaluating the accuracy of ML systems on a standardized set of forecasting questions. To address this gap, we introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML systems on an automatically generated and regularly updated set of 1,000 forecasting questions. To avoid any possibility of data leakage, ForecastBench is comprised solely of questions about future events that have no known answer at the time of submission. We quantify the ability of current ML systems by collecting forecasts from expert (human) forecasters, the general public, and LLMs on a random subset of questions from the benchmark (N = 200). While LLMs have achieved super-human performance on many benchmarks, they perform less well here: expert forecasters outperform the top-performing LLM (p-values < 0.01). We display system and human scores in a public leaderboard at www.forecastbench.org.", "sections": [{"title": "INTRODUCTION", "content": "Forecasting the future is a challenging but necessary task. Economic forecasts influence investment and hiring decisions. And forecasts of the Covid-19 pandemic in early 2020 prompted local lockdowns to slow the spread of the virus. However, human forecasting is often expensive, time-consuming, applicable only in specific domains, and subject to concerns about human biases. Motivated by these limitations, recent work explores the use of machine learning (ML) models, especially large language models (LLMs), in automated forecasting.\nTo assess LLMs' forecasting capabilities, prior work built static benchmarks of questions where the answer was known (resolved) after the knowledge cut-offs of the LLMs they test. For example, \"Will a nuclear weapon be detonated in 2023,\" though resolved on Jan 1, 2024, is a valid out-of-sample question for testing a model with a known knowledge cut-off before the end of 2023.\nThis static evaluation methodology comes with three key drawbacks. First, as the knowledge cut-offs of frontier models are updated, static benchmarks become obsolete, necessitating further data-sourcing. This makes it difficult to continuously track and compare the top models in the field."}, {"title": "1.1 RELATED WORK", "content": "Automated forecasting ML systems that make accurate forecasts can help inform human decision-making. Recent work studies the use of LLMs for automated forecasting. These papers all build static benchmarks of resolved questions. A recent paper from Halawi et al. (2024) uses questions resolved between June 2023 and January 2024. Unfortunately, the latest LLMs, such as GPT-40, have knowledge cut-offs past this point. This fact motivates our work to build a dynamically updating benchmark that can accurately evaluate frontier model performance.\nIn addition, Schoenegger and Park (2023) and Abolghasemi et al. (2023) compare the accuracy of GPT-4 and other LLMs to human forecasters. Schoenegger et al. (2024b) found that an ensemble of 12 LLMs rivaled human forecasts in a forecasting tournament in 2023 with a small number of questions, but the results are statistically imprecise. Unlike our work, that tournament was run only once and is no longer updated. Also, our much larger question set allows us to make precise statistical claims about the performance of LLMs relative to each other and relative to human performers.\nFinally, recent work focuses on the use of LLMs and transformer-based systems in statistical time-series forecasting, but many of the most important forecasting questions do not have well-defined time series that can be used in standard statistical forecasting models (e.g., what is the probability that a nuclear weapon will be used offensively in the next ten years?). Our benchmark is more general, and evaluates automated forecasting performance across questions with and without underlying time series and historical baseline data."}, {"title": "Language model evaluation", "content": "Evaluating highly capable LLMs is a challenging task\u2014with many models saturating key benchmarks soon after their release and with several benchmarks potentially leaked to models' pre-training data. Our dynamic forecasting benchmark avoids both of these issues. First, automated forecasting is highly unsaturated: Halawi et al. (2024) showed that under simple zero-shot evaluation, frontier models such as GPT-4 are much less accurate than aggregates of human predictions. Second, our benchmark is dynamic. The final resolution of each question is only determined in the future and cannot be 'leaked' in any training data (by construction). This eliminates concerns of contamination."}, {"title": "2 PRELIMINARIES", "content": "Forecasting A forecasting question asks for a probabilistic prediction of a future event. A forecaster may assign probabilities to potential outcomes of the event. Forecasting platforms, including prediction markets, host a wide range of questions. Many questions are of public interest, such as \"Who will be the next US president in the 2024 election?\"\nMetrics For binary questions, we use the Brier score as the performance metric, defined as \n$ (f \u2013 o)^2$ , where f \u2208 [0, 1] is the probabilistic forecast and o \u2208 {0, 1} is the outcome. Lower Brier scores are better, and a score of 0.25 is associated with the uninformed forecast of 0.5. Brier scores are strictly proper, incentivizing truthful reporting from participants.\nModels We evaluate 17 LLMs on our initial benchmark: GPT-3.5-Turbo-Instruct , GPT-4 , GPT-40, Llama-2-70B , Llama-3-7B, Llama-3-70B, Mistral-7B, Mistral-8x7B , Mistral-8x22B, Mistral-Large, Qwen1.5-110B-Chat, Claude-2.1 , Claude-3-Haiku, Claude-3.5-Sonnet, Claude-3-Opus , Gemini 1.5 Flash and Gemini 1.5 Pro . We record model predictions under three different methods: zero-shot prompting, scratchpad prompting, and scratchpad prompting with retrieval augmentation. For the last setting, we use the retrieval infrastructure from Halawi et al. (2024) and provide relevant news articles to the models in-context to reason about. Additionally, only models with a context window larger than 8,000 tokens were evaluated under the retrieval setting due to the inclusion of news articles in some prompts."}, {"title": "3 BENCHMARK, LEADERBOARD, AND DATASETS", "content": "We maintain a question bank with a growing set of forecasting questions. Continuously adding questions to the question bank allows it to stay relevant and ensures that we have a large selection of unresolved questions to sample from.\nEvery night, our automated system updates the question bank with new questions and resolution values. We drop invalid, low-quality, and resolved questions, categorizing the remaining questions by topic. The process is fully automated as detailed in Section 3.1.\nEvery two weeks, we sample from the question bank and release question sets for teams interested in submitting their forecasts to the benchmark. We also survey a standard set of LLM-based models to allow for comparisons of performance over time. The forecasts that these groups submit are resolved continuously with daily updates to our leaderboard. We provide the resulting data on forecast questions and submissions to researchers. See Section 3.2 for details. We discuss the question resolution procedure in Section 3.3 and our leaderboard design in Section 3.4."}, {"title": "3.1 UPDATING THE QUESTION BANK", "content": "In an automated process that runs nightly at 0:00 UTC, questions are added to the question bank, resolution values are updated, and metadata generated, as shown in Figure 1. Details follow."}, {"title": "3.1.1 UPDATING QUESTIONS AND RESOLUTION VALUES", "content": "We bring in questions from two broad types of sources: markets and datasets. We select multiple, reliable sources from each type, ensuring the diversity of our benchmark. See Table 1 for details.\nMarkets As one of our two main question sources, we rely on a handful of prediction markets and forecast aggregation sites that elicit human predictions on questions across a wide range of topics. When selecting questions from market sources to add to the question bank, we choose those with high levels of active human trading on these platforms (liquidity) as these questions tend to be of higher quality than those with low levels of human trading. We also store the latest market and resolution values for each question.\nDatasets As a second question source, we rely on 5 well-established and well-maintained datasets that track real-world events (e.g. ACLED , a geopolitical database that tracks worldwide conflict, including facts like the number of protests in Niger each month). With these dataset sources, we can generate questions using a fixed question template (e.g., \u201cWill the number of protests in Niger increase by at least 10% over this month's value by the resolution date?\").\nQuestion Bank Table 1 lists the sources of market and dataset questions, along with the number of questions available in our question bank, whence we sample questions for our benchmark runs. In addition to the main questions (with sample size N), we also construct additional combination questions by choosing pairs of questions within each source. We describe combination questions in more detail in Section 3.2.\""}, {"title": "3.1.2 UPDATING FORECAST QUESTION METADATA", "content": "After we automatically collect forecasting questions and resolution values in our standardized question bank, the data get processed in several ways. This generates more information about the questions and creates another sampling option for the creation of the final question set. Following Halawi et al. (2024), we use gpt-3.5-turbo-0125 to categorize questions by subject and to filter out low-quality questions. We report the number of questions by category and source in Table 13, where we display the breakdown of the standard questions from our question bank (N from Table 1) across question categories by source, after removing low-quality questions."}, {"title": "3.2 GENERATING THE QUESTION SET", "content": "Every other Sunday at midnight UTC, we release a set of 1,000 forecast questions for LLMs through the process outlined in Figure 2. We sample an equal number of questions from each data source, to ensure representativeness. Within each source, we then uniformly sample questions across all question categories, aiming for an equal distribution from each category within each source. This ensures that models cannot be overfit to a specific type of question or topic. Limiting the number of questions generated to 1,000 also ensures that costs for testing LLM systems on the benchmark are capped for development teams with fewer resources.\nFor questions derived from dataset sources, the distribution of resolution dates is as follows, as represented by days past the forecast due date: [7, 30, 90, 180, 365, 1,095, 1,825, 3,650]. In other words, the distribution of resolution dates spans from one week into the future to ten years into the future. For questions derived from market sources, we ask only for the probability that each question will resolve positively (will the event underlying the question occur, or not occur). Prior to a given market question's resolution, performance is evaluated by calculating the squared distance between the forecasted value and the crowd forecast on the platform from the previous day. In the main public leaderboard we report performance using all questions\u2014including unresolved questions\u2014but we also separately report performance on resolved questions for interested readers.\nFor every data source, there are two types of questions, each comprising half of the question set. The first type is a standard forecasting question with a binary outcome, e.g. \"Will inflation (core CPI) be above 3% next month?\" We construct the second type, combination questions, by pairing two standard questions. For combination questions, we ask for forecasts on all Boolean combinations of the two questions (i.e. $P(Q_1 \u2229 Q_2)$, $P(Q_1 \u2229 \u00acQ_2)$, . . . ). Considering the extensive number of existing standard questions and potential combinations in our question bank (as we could potentially combine 3, 4, or more standard questions together), we effectively have access to millions of possible forecasting questions to sample from. We show the number of two-question combination questions in the question bank as it stands at time of writing in the right-hand column of Table 1.\nCombination questions require forecasters to consider the covariance structure of different events, some of which are more independent than others. For instance, the best forecasts of whether the S&P500 (a key U.S. stock market index) will reach an all-time high and whether Spain will win the next Men's World Cup are likely independent. However, the best forecast of whether the S&P500 will reach an all-time high and whether the U.S. will enter an economic recession must account for the likely strong correlation between these events."}, {"title": "3.3 RESOLUTION", "content": "We resolve forecast sets nightly by gathering the latest information about which events have or have not occurred. All questions are ultimately resolved to the final true value.\nMarkets Ground truth is not available for market questions before the events are known to have occurred or not occurred. Before events resolve, we use the platform prediction (an aggregate of human forecasts reported on the platform) from the preceding day as a proxy resolution value for unresolved questions. These values are updated nightly as described in Section 3.1.1. This allows us to score the complete set of forecast questions, incorporating all information available to people on a nightly basis. Once a market question is officially resolved, we score the forecast against the resolution value, creating a definitive score for the question across all forecast horizons.\nDatasets Datasets can be updated as new information becomes available. Thus, questions derived from datasets are continuously resolved to the value from the the latest available data.\nMissing forecasts We select 1,000 questions for the LLM question set to make the forecasting task impractical to complete manually within the 24 hour window after the question set is released. And we obligate all LLMs to forecast on all questions to ensure comparability of scores across models and human-based aggregates. So, when a model does not submit predictions on certain questions or time horizons, we consider that a model error and impute a naive value for the model to ensure comparability across models over time.\nFor market questions, as we only ask users to forecast the outcome of the question, missing forecasts on question outcomes are assigned the value of the crowd forecast on the forecast due date, day D in Figure 3. Some may argue this is overly-generous, but we did not want teams to have a competitive advantage by simply scraping the market websites themselves.\nFor dataset questions, we impute the value 0.5 (which represents a lack of information) to forecasts across all time horizons. Empirically, top models report valid forecasts on all questions and are not affected by this imputation procedure."}, {"title": "3.4 LEADERBOARDS", "content": "We generate leaderboards, ranking models and humans by average overall score. The main leaderboard highlights the top forecasting submission across all questions and can by sorted by performance on the source type (market or dataset) and by score on resolved questions."}, {"title": "3.5 DATASETS", "content": "As a key output of ForecastBench, we generate four living datasets that grow over time. See Appendix A for licensing details and Appendix B for data dictionaries.\nGeneral public forecast dataset Every time we run the public survey outlined in Section 4, we provide multiple independent forecasts and rationales on every one of the 200 forecast questions and report the accuracy of the median public forecast. See Section B.2.1 for details.\nSuperforecaster forecast dataset We provided a random subset of the benchmark questions to 39 expert forecasters with a strong track record of accurate performance on a diverse set of geopolitical questions. In addition to forecasts and rationale, the superforecasters provide pertinent information about their forecasting process, like search terms used and useful URLs consulted. See Section B.2.2 for details.\nLLM forecast dataset Similar to the General public dataset, LLMs produce forecasts on each of 1,000 forecast questions in the LLM question set. Although it is optional for LLMs to provide rationales, they are encouraged and will be included in the dataset whenever provided. See Section B.2.3 for details.\nQuestion & resolutions dataset In creating the benchmark, we have automated question creation and resolution from all of the sources outlined in Table 1. We provide these as a dataset that can be combined with the forecast datasets mentioned above. See Section B.1 for details on these datasets."}, {"title": "4 HUMAN FORECASTER BASELINE", "content": "We recruited 500 human forecasters via Prolific and Facebook advertising to participate as repre- sentatives of the general public. These human subjects completed a brief introductory survey to gather demographic information and evaluate performance on a few forecasting and comprehension tasks. Then, these subjects completed a longer (roughly one-hour) survey containing 20 random questions from a subject matter-representative, 200-item subset of the forecasting questions provided to language models, gathering their forecasts and rationales for each question.\nThe number of responses per question in the 200-item subset varied to ensure representativeness across subject matters and sources; at least 40 responses were gathered per question, averaging 49 responses per question.\nIn our surveys, potential participant risks were described in Institutional Review Board-approved consent forms, alongside an estimate of the payment for completing each study ($5 + bonus payments for forecasting performance in the introductory survey; $10 for completing the primary survey). We spent roughly $30,000 on recruitment and incentives."}, {"title": "4.2 SUPERFORECASTERS", "content": "We additionally recruited 39 expert forecasters with a strong track record of accurate performance on a diverse set of geopolitical questions (\"superforecasters\") to participate in a 9-day forecasting experiment in which participants were prompted to give their individual forecasts for 20 random questions from the same 200-item subset of forecasting questions we provided to the general public. Roughly halfway through the 9-day experiment, participants were moved into a group forecasting stage in which we allowed these expert forecaster participants to see one another's forecasts and rationales and to communicate with one another about each question. In this stage, participants were also given the opportunity to forecast on questions beyond the 20 questions assigned to them individually.\nBecause of the more limited quantity of the superforecasters, questions generally had fewer responses than in the public survey; a minimum of 3 forecasts were recorded for each question, with an average of 8 responses per question.\nPotential participant risks were described in a consent form similar to those provided to members of the general public. Because of the expertise of this set of forecasters, superforecaster participants were also guaranteed a base payment of $1,000 for participating in the individual and group stages of the experiment as well as bonus payments for individual-level accuracy.\nResults We evaluate human performance on the forecasting tasks by comparing predictions to the ground truth on resolved questions and comparing to community aggregates on unresolved questions. We found that the median public survey participant had an overall Brier score of 0.107 on the 200-item subset of forecasting questions provided to LLMs. The median superforecaster participant had an overall Brier score of 0.093 on this same subset of forecasting questions. See Section 5 and Table 2 for a comparison between human performance and LLM performance, as well as Appendix G for a top 50 leaderboard of human and LLM performance."}, {"title": "5 LLM BASELINE", "content": "In this section, we evaluate the forecasting capabilities of language models and report on the method- ology and results.\nMethodology We evaluate a suite of instruction-following chat models without any additional fine-tuning (see Section 2 for details on the models). In each baseline, we prompt the model to generate a probabilistic forecast that the question will resolve to \"Yes.\"\nBaselines We implement six baselines: (1) zero-shot prompting, (2) prompting with scratchpad instructions, (3) zero-shot prompting with crowd forecasts, (4) scratchpad prompting with crowd forecasts, (5) prompting with scratchpad instructions and retrieved news articles, and (6) aggregating predictions from multiple LLMs. Each baseline is described in more detail below.\n1 Our first baseline, prompts the model zero-shot to generate a forecast directly without generating other content, such as intermediate thinking (Figure 6). By prompting the model to output its forecasts directly, we assess raw forecasting capability without sensitivity to prompting strategies.\n2 Our second baseline, prompts the model with scratchpad instructions that outline a procedure the model should use to reason about the question (Figure 7). Our scratchpad prompt comes from which formed its prompts through a combination of analyzing the Brier score as prompt changes were made, and by adding language to fix common errors the LLMs would make, e.g., asking them to rephrase the question for understanding.\n3 The question sets we provide to LLMs contain what we term freeze values for market questions, which are just the crowd forecast on the market the day the question set was created, as decsribed in Section 3.2. Our third baseline is the zero-shot prompt with freeze values.\n4 Our fourth baseline is the scratchpad prompt with freeze values.\n5 Since LLMs' knowledge are not continuously updated, it is important to provide LLMs with up-to-date information relevant to the question . Our fifth baseline, scratchpad with information retrieval, uses the same scratchpad prompts that also contain retrieved news articles.\nThe retrieval system is the same as described in : an LLM generates search queries for a news API, filters articles for relevancy, and summarizes the articles."}, {"title": "6 DISCUSSION", "content": "We present ForecastBench, the first dynamic benchmark for evaluating automated forecasting systems. Forecasting has great potential to support human decision-making; we hope that our benchmark can shed light on how automated systems compare in accuracy to humans and when (if ever) they might become the preferred source of predictions. Moreover, from the perspective of LLM evaluation, forecasting remains challenging for frontier models. It requires models to integrate pre-training knowledge, engage in effective reasoning, and maintain calibration to avoid overconfidence. All of these are essential capabilities that are desired across various domains. We therefore believe that performance on our benchmark can shed light on other capabilities of LLMs.\nIn closing, it is worth putting the current project in a bigger-picture perspective. Despite a great deal of testing, there remains tremendous uncertainty about the current and future capabilities of LLMs. Researchers are far from having solved the intertwined problems of (1) prompting LLMs to produce insightful answers and (2) ensuring that LLMs refrain from hallucination. Research to date reveals wide variability in LLM performance on judgment and problem-solving tasks foundational to forecasting. We show that in the realm of probabilistic forecasting in real-world settings, LLMs cannot currently outperform humans. ForecastBench gives us a tool to track the accuracy of LLMs relative to humans, and relative to each other, over time. Based on these results, we see value in a multi-pronged scientific agenda in which researchers combine (1) radically empiricist approaches to test LLM capabilities, such as ForecastBench; and (2) more theory-driven approaches that assess LLM performance in tightly controlled simulated worlds where ground truth probability distributions of outcomes are knowable, and where accuracy feedback can be far more rapid and informative."}, {"title": "C QUESTION BANK", "content": "C.1 LICENSES\nThe licenses outlined in Table 12 apply to the datasets we have sourced for questions. We were granted express permission to use questions from Manifold Markets and Metaculus. Though not required by their license, we also met with a representative from ACLED who approved our use of their dataset for the benchmark and dataset distribution.\nC.2 CATEGORIES\nWe generate metadata on all of the questions in our question bank, categorizing our questions, as described in Section 3.1.2.\nIt is important to have forecasting questions across a broad array of categories to test LLM capabilities. We are currently adding more questions from datasets with the goal of equilibrating these categories."}, {"title": "D HUMAN SURVEY INSTRUCTIONS AND SCREENSHOTS", "content": "D.1 INSTRUCTIONS\nParticipants in the public survey were first prompted to read a consent form detailing the tasks involved in the study and potential risks to participants. Then, participants were prompted to give their responses to 20 forecasting questions randomly selected from the 200-question subset of questions provided to the LLMs.\nParticipants were given a brief description of the task ahead before each question as follows:\nYou are going to be predicting the probability of the answer to the question\nbelow being \"Yes\" (or \"resolving positively\").\nFor tasks with questions generated from data providers, forecasters were prompted to provide a probability forecast at multiple resolution dates.\nParticipants in the superforecaster survey went through a similar initial experience before being moved into the group forecasting stage described in Section 4.\nD.2 SCREENSHOTS\nThe following screenshots (Figure 4, Figure 5 show a few of the questions presented to participants in the public study.\nYou are going to be predicting the probability of the answer to the question below being \"Yes\" (or \"resolving\npositively\").\nWhat is the probability that the daily average temperature at the French weather station at Lille\nAirport will be higher on the dates listed below than on 2024-07-21?\nDBnomics collects data on topics such as population and living conditions, environment and energy,\nagriculture, finance, trade and others from publicly available resources, for example national and\ninternational statistical institutions, researchers and private companies. The history of Average temperature\nby day and by station for France - Degree Celsius - LILLE-LESQUIN - Daily from M\u00e9t\u00e9o-France is available\nat"}, {"title": "E DETAILS ABOUT LLM \"CROWD\" BASELINE", "content": "E.1 MODELS\nTo construct a crowd baseline that includes diverse candidates, we evaluate models using the most recent forecasting dataset containing cross-domain questions with true resolutions from Halawi et al. (2024). We assess models from the following organizations: OpenAI, Mistral AI, Qwen, Google, Anthropic, and Meta. Notably, we exclude the Claude and Gemini suites due to rate limit constraints. Using the same scratchpad prompting method from Halawi et al. (2024), we then select the top three models: GPT-40, Gemini-1.5.Pro, Claude-3.5-Sonnet. See Table 14 for the results.\nE.2 AGGREGATION METHODS\nWith the forecasts generated by the top 3 models selected in Section E.1, we then compare perfor- mance of 5 aggregation methods: Mean, Median, Trimmed Mean, Geometric Mean, and Geometric Mean of Log Odds . See Table 15 for the results.\nTo ensure our LLM \"crowd\" baseline is reproducible and verifiable,\nwe use random seeds 1, 2, and 3 to create three random sets of articles. For each seed, we select\n10 articles per question. If fewer than 10 articles are available for a given question, we select all\navailable articles.\nLLM Parameters. We set the temperature as 0 and the max output token length as 2000."}, {"title": "F PROMPTS", "content": "In this section, we present the following prompts: zero-shot (Figure 6), scratchpad (Figure 7), and three prompts (Figure 8, Figure 9, Figure 10) written by superforecasters from the Forecasting Research Institute used to construct the LLM \"crowd\" baseline. Note that the scratchpad with information retrieval prompt is simply the scratchpad with an additional line \"We have retrieved the following information for this question: {retrieved_info}\" before the instructions begin. For combination questions, we slightly modify each of the above prompts by including the second question's information (Figure 11)."}]}