{"title": "The Cake that is Intelligence and Who Gets to Bake it: An AI Analogy and its Implications for Participation", "authors": ["MARTIN MUNDT", "ANAELIA OVALLE", "FELIX FRIEDRICH", "A PRANAV", "SUBARNADUTI PAUL", "MANUEL BRACK", "KRISTIAN KERSTING", "WILLIAM AGNEW"], "abstract": "In a widely popular analogy by Turing Award Laureate Yann LeCun, machine intelligence has been compared to cake -where unsupervised learning forms the base, supervised learning adds the icing, and reinforcement learning is the cherry on top. We expand this \"cake that is intelligence\" analogy from a simple structural metaphor to the full life-cycle of Al systems, extending it to sourcing of ingredients (data), conception of recipes (instructions), the baking process (training), and the tasting and selling of the cake (evaluation and distribution). Leveraging our re-conceptualization, we describe each step's entailed social ramifications and how they are bounded by statistical assumptions within machine learning. Whereas these technical foundations and social impacts are deeply intertwined, they are often studied in isolation, creating barriers that restrict meaningful participation. Our re-conceptualization paves the way to bridge this gap by mapping where technical foundations interact with social outcomes, highlighting opportunities for cross-disciplinary dialogue. Finally, we conclude with actionable recommendations at each stage of the metaphorical AI cake's life-cycle, empowering prospective Al practitioners, users, and researchers, with increased awareness and ability to engage in broader Al discourse.", "sections": [{"title": "1 Introduction", "content": "\"If intelligence is a cake, the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning.\u201d Yann LeCun, NeurIPS Conference 2016\n\nBy now an illustrious analogy in the field of machine learning (ML) and artificial intelligence (AI), the quote's original claim was that the bulk of the cake -i.e., the majority of what supposedly composes intelligence- does not rely on turning over exorbitant amounts of labeled data. On the contrary, the bulk is conjectured to consist of unsupervised learning (i.e., learning from data without labels), the icing is created through supervised learning (i.e., learning to match ground-truth annotations), and the \"cherry-on-top\" is reinforcement learning (i.e., tuning to feedback). In this cake metaphor, this resembles first baking a solid cake, if you so will the delightfully moist sponge, to lay the foundation for it to later be perfected to one's individual taste, i.e., a specific narrow AI task. If at first resulting in a skeptical smile, we will learn in this paper that the design of AI systems indeed has a lot of parallels to baking a cake. As such, we posit that the metaphor is valuable not for its original reasons, but rather because the creation of AI systems entails many of a real cake's associations and implications on ingredients, recipes, baking processes, taste, and consumers.\n\nIn particular, we can follow the typical ML trends and may immediately ask ourselves whether this cake can be perfected? In turn, asking what the glazing on the cake would symbolize? In fact, perhaps it would even be possible to"}, {"title": "2 Al cake: an accurate analogy", "content": "If the introduction left the reader hungry for more, they will hopefully see their hunger stilled in a transition to how the cake analogy translates to Al systems. To this end, let us first re-conceptualize the analogy to describe the various stages of the AI lifecycle and their social ramifications before proceeding to discuss underpinnings in technical limitations."}, {"title": "2.1 Ingredients and their origin", "content": "At the beginning of (baking) any cake are its ingredients. Depending on the type of cake and the baker's exact location, some of these ingredients may be locally available, i.e., acquired through trade or bought from a market, whereas others originate from far away. For instance, the use of cocoa seeds is prevalent in cakes of the northern hemisphere, yet its growth is restricted to equatorial (predominantly African) regions. Although the \"Western world\" now largely acknowledges that respective local cultures have been, and are still continuously, subject to exploitation, it generally remains challenging for the consumer to thoroughly understand the origin of ingredients and respective implications. There may exist nutrition labels, but they abstract away most information. More extensive pushes for transparency and ethical considerations, like the supply chain act [115] recently passed by the European Parliament, remain controversial"}, {"title": "2.2 The \"best\" cake recipes", "content": "Baking a cake does not only require a set of ingredients, it generally needs to follow a recipe. The latter typically ensures that chosen ingredients \u201cinteract\u201d sufficiently well. Although there exists an abundance of instructions on how to combine a cake's ingredients, the individual ingredients eventually all fall victim to the same blending process when mixed together into dough. Remarkably, this process seems simultaneously accessible and demanding. As long as the recipe is followed closely, almost anyone can successfully bake a cake, provided they have the massive ovens to bake it (see baking process section). If, however, one wishes to include a particular new ingredient in the mix, then baking almost becomes a precision science. It is a delicate balance of the modifications being noticeable, e.g., in color, texture, or taste of the cake, or potentially overpowering and thus ruining the mix -a balance only few may claim to truly understand. It certainly becomes a futile effort if one wishes to turn cake ingredients into another dessert when only generic cake recipes are available.\n\nAs much as cake recipes ultimately combine the majority of ingredients together, so do Al systems mix their training data. Unfortunately, our analogy extends from the blending procedure relying on the predominant belief that, adding more data to the metaphorical dough is sufficient for inclusion, to the fact that few experts, if any, understand the consequences of respective interleaving. In both parts of the analogy, the assumption that simply adding more data ensures inclusivity is flawed. Delgado et al. [47] have pointed out that \u201cwe cannot just add diverse end-users and stakeholders and stir\". On the one hand, it is unclear which combinations of data may advance model capabilities and which data additions entail social consequences; their nature hinges on complex interplay with the rest of the opaque data mixture. For instance, from the perspective of purely instrumental improvement, it is heavily debated when and how the addition of synthetically generated data is indeed beneficial, e.g. Phi-2 & 3 [4, 79] or Dall-e-3 [15], or whether its inclusion in the mix is useless or downright adverse to performance [69]. From a complementary normative angle, initially sincere efforts to \"debias\" models through increased addition of non-anglocentric languages have similarly concluded that stirring these ingredients has contributed fairly little towards the desired goal of broad applicability in globally inclusive perspectives [60]. On the other hand, the recent repercussions [127] surrounding Google DeepMinds' Gemini image generation [65] have rendered the challenge of highlighting ingredients in a recipe and unwillingly overshooting perfectly visible. In short, an effort to \"diversify\" the generated images through naive integration attempts of broader coverage of society has further resulted in the formation of adverse relations [63, 131], e.g. now picturing strong racial diversity in the generation of Nazi Germany's Wehrmacht soldiers. This contrived effect is not only highly illogical but further contributes to perpetuating harmful AI content and fostering oppression in complete contradiction to the initial amiable aim [112].\n\nClearly, both real cake and its AI analogy thus hinge on combining their ingredients. Whereas careful measuring and stirring of ingredients is a challenging task already, complex interplay of data makes careful weighting only one imperative element of a recipe. A lack of understanding in interleaving arbitrary data in AI systems yields mixtures where inclusion of ingredients, even if added with the best of intentions, can range anywhere from completely inconsequential, to instrumentally beneficial, or resulting in adverse fallout. The seemingly accessible and effortless aggregation of all data to streamline the ensuing baking thus also obfuscates ingredients' uniqueness, even suppresses their critical nuances, and in consequence gives rise to counter-intuitive ramifications. As such, homogenization of the recipe, i.e. blending all ingredients together to follow the instructions for cake dough (training a deep machine learning model), entails accessibility advantages that simultaneously make it excessively hard to bake anything else.\""}, {"title": "2.3 Cake foundation and the baking process", "content": "Once settled on ingredients and a recipe, baking a cake is very much a unidirectional process. Once in the oven, there is no turning back. Forgotten an ingredient, bake a new one. Took a slight misstep in following instructions, bake a new one. Do not like the outcome, bake a new one. Once baked, a cake's composition becomes fixed and permits few revisions. Incorporating any raw ingredients post-baking is impractical and far from appetizing - think for instance of pouring milk over or adding raw egg to a baked cake. At best, we can superficially tune the cake, e.g. adorning it with fruit, candy, or a touch of glazing to make it more appealing.\n\nAs hard as it is to change a baked cake, so is modifying an Al model after it has been trained. Within the preference alignment literature, the \"superficial alignment hypothesis\" posits that reinforcement learning from human feedback primarily affects a foundation model's [126, 151] textual output style, rather than its core capabilities [165]. In fact, respectively finetuned models result in near identical performance of tuned and base model versions (i.e., they trade off one dimension for another), at best resulting in superficial improvement [97]. This is analogous to how we can only decorate a finished cake rather than change its composition, resulting in an unreasonable amount of baking repetitions for every fundamental change. In this context, however, such modification attempts (i.e. re-training) cost millions of dollars [88, 155] and consume extraordinary amounts of energy for minor improvements [144, 146]. For example, while presumably being updated to improve certain model abilities in Spring 2023, ChatGPT actually lost proficiency in basic tasks like identifying prime numbers or writing simple code [37] by June of the same year. This reveals a fundamental tension with current regulatory frameworks, such as the Biden-Harris AI executive order [77, 114] and the United Kingdom's \"pro-innovation\u201d approach [143], which rely heavily on post-training interventions. While model auditing and red-teaming are crucial to identifying problems in AI systems, resolving them thus remains challenging due to the difficulty of modifying a model after training \u2013 much like trying to fix a single ingredient in an already baked cake. These effects are further exacerbated by the fact that, following the prior section's arguments, we lack understanding of which data corresponds to the metaphorical egg or milk, and which ones are decorative in nature.\n\nOverall, both real cake and its AI analogy thus hinge on a single-cycle baking process. Whereas baking cannot be fully changed and undone once completed, AI training largely dictates the final utility of the system. Attempts at fine-tuning are either superficial or entail strong trade-offs. In particular, later attempts at adding entirely new ingredients can interfere catastrophically, depending on the nature of the ingredient. Contrary to intelligence also being described as \"the ability to adapt to change\" (commonly attributed to Stephen Hawking, see Strauss [145]), the current inflexibility of the \u201ccake-like\u201d training pipeline entails excessive process repetition. As each training run requires exorbitant amounts of computational resources [100, 122], this seems akin to baking cakes by having thousands of ovens emit carbon, to ultimately re-do the entirety of produced cakes any time a non-superficial adjustment needs to be added."}, {"title": "2.4 What makes for a tasty cake?", "content": "A cake may have been baked successfully, but is it also tasty? Some flavors may widely be assumed to be \"safe bets\", like chocolate seemingly enjoying popularity, but who gets to provide this assessment? Imperial history may have imposed particular cuisine aspirations upon us, but what is realistically considered delicious will vary drastically. When considering geographical and cultural influences, it will inevitably be impossible to single out one taste. Resorting to stereotypical hyperbole for clarity, strong sweetness may for instance be desirable in portions of the world, but much less strongly desired in other parts. Certainly, the notion of an average delightful cake itself is a poor simplification even within one region. After all, every human has their own flavor preferences; preferences that are uniquely shaped by their upbringing and further influenced by constraints such as food intolerances or allergies. And in the end, different parts of the world may also prefer entirely different deserts or be more cautious of a potentially unhealthy diet.\n\nAs much as it is impossible to define a best-tasting cake, so is it impossible to define the best-performing AI. Grappling with how to integrate these preferences is a recurring exercise throughout machine learning research, as demonstrated by the subjective nature of human feedback in learning preferences [86]. For instance, in large language modeling, a model's performance could typically be determined through a set of prescriptive benchmarks, which predominantly center common-sense reasoning [1, 148, 162], reading comprehension [41, 42, 128], or mathematical abilities [43, 96]. While these evaluations provide valuable insights into the capabilities of Al models, they fail to capture the full complexity of human preferences and the socio-technical implications of deploying these models in real-world contexts, including the impact on historically targeted, marginalized, or economically underprivileged groups. As such, gender bias literature is typically restricted to a binary lens, limiting critical discourse on AI harms deriving from a focus on binary gender [118, 125] and primarily male perspectives in Al systems [51, 117]. Likewise, the famous GenderShades audit [30] revealed that commercial AI systems often misclassified darker-skinned females [17]. A recent WIRED article [132] further illustrates the collapse to an expected average, showing how OpenAI's video generator Sora [27] defaults to depicting bisexual, asexual, or transgender persons with pink hair - highlighting the need for increased demographic representation and intersectional measurement practices in the baking process. These challenges are compounded by the influence of those who decide which AI technologies are put into practice and how acceptable performance is defined - i.e., what the baker determines as the average taste gives rise to power [22]. On the other hand, expanding a baker's palette to make a tastier \u201cAI\u201d cake often presents a challenging and sometimes contradictory undertaking, as evidenced by the more than 21 definitions of fairness [111]. Achieving the latter simultaneously proves impossible, especially when group and individual fairness conflict [16, 87]. At the same time, the social implications of each are unclear, as each method brings its own unique set of fairness challenges [35, 101]. In turn, the research community has proposed over 70 AI fairness evaluation guidelines [12].\n\nOverall, both real cake and its AI analogy thus hinge on subjective and even incompatible notions of being delightful. Just as a bakery attempts to cater to the average customer's preferences, AI systems often rely on a limited understanding of typicality or correctness, leading to a collapse of diversity into oversimplified norms. Faced with complexity and abundance of taste assessment, AI bakers may opt for measures that are easy to satisfy or superficially appealing. This facilitates \"ethics shopping\" or \"ethics bluewashing\" [58], respectively cherry-picking what can be satisfied and using superficial measures in favor of positive appearance."}, {"title": "2.5 \"If they do not have bread, let them eat cake!\" - Sharing and (over-)selling", "content": "The reader may finally chuckle when reading about the metaphor's last connection to the above infamous quote, attributed to Marie Antoinette (and likely stated by Jean-Jacques Rousseau in 1765). But like any product, cake is eventually shared. More likely, it will be sold in order to make up for the initial cost and earn enough profit to make a living. Initially, this may have a naive benevolent intention, much like in our 18th-century anecdote, but there certainly is little use to suggest a starving population bake cake. After all, as elaborated in the prior sections, they will lack access to ingredients, lack understanding of the recipe, lack the tools to bake, or derive little nutritional value from the cake.\n\nAs much as the starving population of the 18th century had more basic needs than cake, so does much of the current population not actually profit from claimed AI progress. On the one hand, this may partially be due to a severe mismatch between the generally sold capability, and what is practically specified in the Al creation process. For instance, \"diverse representation\" is frequently advocated for greater inclusion of underrepresented groups in datasets, yet oversimplified notions lead to objectification or exploitation [14, 40]. Similarly, common error rate measures often lead to misleading promises on fair systems, given that assessment does not account for ultimately entailed effects [45]. This leads to treatment and impact disparity, where correlations arise unintentionally and outcomes start to differ across groups [99]. On the other hand, falsely promised profit may also be due to a growing belief in technosolutionism: the belief that technology is always the solution [28]. In this belief, an algorithm's role in selective targeting may simply be neglected at the prospect of later improvement. As such, periodic examples of how tech exacerbates inequality can be found. In fact, it generally seems that many AI contributions that were sold with some initial notion of good in mind, ultimately ended up fostering undesired population surveillance [81], all the way to the latter being used to AI-enabled direct persecution [70]. Sometimes the technosolutionist narrative's harm may be invisible on the surface, especially in scenarios where the AI cake is oversold to users that presently don't require it at all. An intuitive example of this pattern is the frequent marketing of AI as an opportunity to establish food security and enhance health care, in particular on the African continent [8]. Even if the potential benefits of AI are enormous, it is also challenging to reap them while ignoring the structural inequalities that need to be overcome before AI can actually live up to its sold promise [6]. Dynamic power relations between countries are not easily resolved by AI and general power imbalance cannot be overlooked. For instance, the US and EU heavily subsidize agriculture to export to African consumers [89] and big tech corporations own an overwhelming amount of infrastructure. Even when local grassroots progress is thus made, external parties and companies end up reaping many of its benefits - a pattern referred to as cooptation [108].\n\nOverall, both distributing cake to a starving population and overselling AI systems hinge on an initially amicable incentive that fails to deliver on its intended goal. Whereas suggesting to feed people with cake is a poor nutritional and mismatched solution, so too are AI systems frequently oversold beyond challenges they are capable of solving. Technosolutionism and the resulting practice to create terminology to fuel this belief, e.g., \"foundation models\" or \"frontier AI\" [71], shift away focus from actual current capabilities to long-term speculation. In turn, a habit of \"ethics shirking\" [2, 58] is facilitated, where less work on ethical aspects is conducted if less hypothetical reward is expected."}, {"title": "3 Technological underpinnings: Why it is difficult to change the Al cake", "content": "The previous sections have substantiated the AI cake analogy and have linked it to several concerning practices. We now revisit these ramifications and map where social outcomes are intertwined with and exacerbated by technical underpinnings. In particular, we posit that even if consensus on instrumental and normative angles to AI development existed, the present technical foundations make it tremendously challenging to translate benevolent aims into practice.\n\nThis is not to say that researchers should be exempt from any responsibility. On the contrary, we precisely wish to empower them with a thorough understanding of how their present technical choices imbue constraints, foster biases, and why fundamental (mathematical) properties at the heart of (statistical) machine learning imply significant barriers. Mirroring our earlier paper structure, we begin each subsection with a quote from a recent publication to exemplify one key technical hurdle underpinning the previously described challenges, before proceeding to disentangle its technical caveats. We respectively finalize each subsection with a set of technical recommendations for future work, to highlight cross-disciplinary opportunities alongside existing social and ethical research advances."}, {"title": "3.1 \"Ingredients\": the challenge of data \u201ci.i.d.-ness\u201d", "content": "\"Our results show that hate content increased by nearly 12% with dataset scale, measured both qualitatively and quantitatively using a metric that we term as Hate Content Rate.... This, as we hypothesize, may be a consequence of rich non-i.i.d. inter-sample correlations emerging from a graph-structured prior for CommonCrawl\" - Birhane et al. [19]\n\nThe above quote refers to the scaling of data examples from the LAION-400m [141] to the LAION-5b [140] dataset (respectively containing 400 million and 5 billion data points), attributing increasingly hateful content to the lack of understanding of non-i.i.d. correlations in the data selection and filtering mechanisms. More formally, i.i.d. refers to \"independent and identically distributed\". The U.S. National Institute of Standards and Technology (NIST) provides a concise definition: \"A quality of a sequence of random variables for which each element of the sequence has the same probability distribution as the other values, and all values are mutually independent\" [153]. In other words, we assume each data point to be different from and unaffected by others, while all data points are expected to originate from a common data generation process. Intuitively, this further entails \u201cexchangeability\", i.e., the notion that we can exchange the order of our data points in practice a property that will also be central to our later learning recipes.\n\nNaturally, there are various ways in which the i.i.d. assumption won't hold in the real world. Unfortunately, violations will mostly occur in unknown ways, with each respectively obscuring our understanding of the gathered data. An easy example is if the acquired dataset selectively contains (near) duplicates. This has recently occurred for up to 30% of LAION-2b [160], but can be dealt with effectively through various statistical tests [78]. It becomes significantly more challenging when a) there are complex inter-dependencies between subsequent data points, b) the distribution changes over time and becomes non-stationary, c) the data is adversarially crafted [46]. Although the latter scenarios are well-known to be realistic, the i.i.d. assumption is rooted deeply in our algorithms for pattern recognition and in statistical learning theory [156]. It is a key requirement to render many algorithms practical, in particular in the prevalent machine learning angle to AI. Specifically, i.i.d.-ness of data is presumed because it provides a required simplification of the likelihood function, the essential tool underlying learning/optimization procedures:\n\nL(0) = p(x1, x2, ..., XN|0) = p(x1|0)p(x2|0)...p(x0) (1)\""}, {"title": "3.2 \"Recipes\u201d: Homogenization of instructions", "content": "\"Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream.\" - Bommasani et al. [23]\n\nThe above quote emphasizes the increasing homogenization as a result of deep learning's success at scale, but also advises caution with respect to always inheriting the same caveats. The latter is primarily driven by the seeming possibility to employ a heavily standardized framework of deep models, nowadays transformer [157] based foundation models, and optimization through backpropagation [94, 135, 161]. As deep neural networks are known to be universal approximators [76], this fosters a narrative of \u201cone model to learn them all\" [80, 130]. Although homogenization offers several benefits in terms of accessibility and ease of use, it suffers from oversimplifying and locking in the recipe. From a technical perspective, it is a result of modeling any modern neural network as a cascade of layers 1 = 1, . . ., L that map from an initial dimension d\u0131\u22121 \u2208 N1\u22121 to a resulting arbitrary dimension d\u2081 \u2208 N\u2081 through a progressive set of transformations T\u2081 : Rd\u0131-1 \u2192 Rd\u0131 to assemble a \u201chypothesis\u201d (model) ho:\n\nho(x) = TL(TL\u22121 (. . . (T\u2081 (x)))), x \u2208 Rd (3)\""}, {"title": "3.3 \"Baking process\": the interference dilemma", "content": "\"Human learning has evolved to thrive in dynamic learning settings. However, this robustness is in stark contrast to the most powerful modern machine learning methods, which perform well only when presented with data that are carefully shuffled, balanced, and homogenized.\" - Hadsell et al. [68]\n\nThe above quote emphasizes how fluid intelligence allows humans to excel in dynamic environments through sequential adaptation and progressive refinement of their knowledge [56, 57]. In contrast, the strength of (large) machine learning models is only apparent through crystallization of carefully balanced and homogenized data. Whereas it is significantly easier for humans to continually learn \"the n-th thing\" after learning \"n - 1 things\" [68, 150], human-like knowledge transfer in machines remains a challenging desideratum [91, 119].\n\nFrom a technical perspective, the lack of ability to learn continually entails an exorbitant amount of retraining. Yet, this re-training cost is willingly embraced, as the unfortunate alternative is induced catastrophic forgetting [102, 129]. The latter refers to the phenomenon of abruptly losing acquired information when sequentially tuning to new data. Unfortunately, the reasons for this phenomenon are deeply ingrained in our optimization toolkit, where one culprit is the iterative nature of our optimizers [152]. At the example of our homogenized recipe's stochastic gradient descent, steps t of updates to parameters @ are conducted in a loss function across observed data points with a \"learning rate\" \u03b7:\n\nfor t = 1, 2, ..., t do : 0\u2190 0 \u2013 \u03b7\u2207L\u03c4 (0) (4)\n\nIntuitively, this works well if we present the optimizer with all the concepts we ultimately care about. The optimizer will \"move in directions\" that satisfy all observed concepts and progressively improve. However, it is now similarly unsurprising that if presented with a novel fraction of data at a later time step, this optimizer will move uni-directionally to improve on only what it has recently seen. This challenge is significantly exacerbated by the semi-distributed nature of representations in neural models [59]. Although entangled representations foster generalization across concepts by moving away from a look-up table, they also imply that most any update has an influence on every learned concept so far. The field of continual machine learning [68, 105] aims to overcome this central limitation. Alas, it is presently bounded by our homogenized model and optimizer recipes. If we cannot leave the frame, the metaphorical cake can only be changed superficially post-hoc or re-baked fully when we wish to make major additions or changes. Learning becomes a (Markov) chain that only takes into account the most recent past. It does not explicitly take into account all prior observations, making any addition either superficial, or potentially catastrophic, if the desired change was not part of the original data mix."}, {"title": "3.4 \"Taste\": limits of averages and aggregates", "content": "\"Some pitfalls are only visible when examining the dataset as a whole or the proposed aggregating metrics. Since what the benchmarks aim to measure is not well articulated, it can be difficult to distinguish whether and when the pitfalls we list below suggest a poor conceptualization of stereotyping or instead call into question the way it is operationalized\" - Blodgett et al. [21]\n\nThe above quote, originally written in the context of the misattribution of stereotypes, points to issues in the in-terpretation of averaged assessment. It implies that an AI system's pitfalls may become obfuscated, for instance, an underrepresented group not being adequately covered, and that important nuances may typically be neglected, such as through aggregating fairness metrics [35]. The present over-reliance on aggregate measures [31] respectively makes it challenging to predict practical behavior in real situations outside benchmark datasets.\n\nFrom a technical perspective, the popularity of average assessment is not only driven by the scientific literature's seeming necessity to compare single benchmark numbers, but the challenges are once more rooted deeply in the algorithmic underpinnings themselves. Following earlier equation 2, which highlighted the significance and prevalence of the i.i.d. assumption, evaluation losses (L) and measures are averaged in equal weighting of all data points n = 1, ..., N:\n\nL(0) = 1 \u2211N Ln(0) (5)\n\nThis assumption is crucial when coupled with the prevalent iterative learning algorithms. In particular, it makes textbook machine learning analysis readily available [20], where training, validation, and test data splits are compared to understand whether the model \"generalizes\" in evaluation. Unfortunately, such assessment of generalization is also limited to what is well represented and practically assessable in the split [163]. Similarly, popular algorithms such as variational inference in autoencoders [84, 85] rely on mean-field theory, e.g., measuring divergences to the mean and standard deviation of a (Normal) distribution to learn about the data generating process.\n\nThe rooting of averages in these machine learning foundations entails several problematic technical factors. First, relying on aggregate measures exacerbates overconfident (false) predictions. In fact, models themselves are typically trained to give maximal predictions (or minimum losses) and as such seldom give out anything that is far away from the observed average value (which in labeled scenarios is typically a 100% confidence of a category) [72, 116]. Second, relying too much on averaging in generative modeling can either \"smooth out\u201d the diversity represented in the data, e.g., we create an envelope around two distribution modes, or approximate a particular mode well at the expense of dropping another, i.e., mode collapse [90]. Finally, averaged measures as targets for evaluations incentivize the conception of systems that leverage shortcuts. For instance, the popular example of CleverHans predictors [93] has shown that an average accuracy measure does not allow us to distinguish whether images of a horse are classified correctly because they indeed contain a horse, or because they contain a different, potentially unidentifiable common feature (here, a pho-tographer's tag). These confounders are particularly problematic in non-i.i.d. scenarios, where certain groups of features can become over or underrepresented at specific points in time [32], relating back to our section two's Gemini example. Aggregate measures thus make machine learning seem straightforward to evaluate and allow us to compare mod-els, but the fact that these are imbued in our fundamental technological stack also severely limits prospective assessment.\""}, {"title": "3.5 \"(Over-)Selling\": the abundant surrogate impasse", "content": "\"Machine learning models routinely achieve perfect performance in one dataset even when that dataset is a large international multisite clinical trial. However, when that exact model was tested in truly independent clinical trials, performance fell to chance levels. Even when building what should be a more robust model by aggregating across a group of similar multisite trials, subsequent predictive performance remained poor.\" Chekroud et al. [36], editor summary - Peter Stern.\n\nThe above quote highlights strong concerns over practical AI system deployability, despite large-scale trials. Although the quote also hints at our earlier section's issue with aggregates, we posit that an additional technical aspect is overlaid. We term this challenge the \"abundant surrogate impasse\", pointing to a lack of understanding of how optimization goals relate to practical measurement. In turn, we posit that even the most rigorous approach is technologically challenged in its assessment of practical implications, contributing to the overselling of our Al systems beyond human intent.\n\nFrom a technical perspective, almost any machine learning system needs to be optimized via a proxy. This is both motivated by the fact that we require smooth and differentiable loss functions to obtain learning signals [107] (e.g. turning a categorical 0 or 1 signal in classification into a spectrum between 0-1), and the fundamental limitation that we can rarely express our goal directly mathematically. Take for instance two prominent recent advances, generative vision models and large language models. In the former, we wish to train a model that is capable of faithfully generating a diverse set of images, yet it is unclear how to express this goal directly. As such, it is frequent, for instance in auto-encoding based models, to minimize the discrepancy in pixel values of an original x and a reconstructed image x:\n\nL(0) = \u22112=1||\u00e2n - xn||2 where xn = h(xn) (6)\n\nSimilarly, in large language models, we wish to accurately model language, yet we don't explicitly encode linguistic rules or semantic coherence as training objectives. Instead, the standard approach involves tokenizing text into discrete units, treating each token from the vocabulary as a distinct class, and training the model to predict the next token (xt+1) in a sequence (xt:1) through maximum likelihood estimation. This seemingly simple objective of next-token prediction serves as a proxy for learning deeper linguistic patterns and relationships.\n\nL(0) = \u2212 \u2211t log po (xt+1|xt:1) (7)\n\nIn both cases, these training objectives and loss functions are used to optimize the system and evaluate its performance on held-out test data. However, when assessing real-world generation capabilities, we typically sample new outputs and evaluate their quality. This assessment requires fundamentally different metrics - perceptual scores for images [82], reference-based [98, 120] and reference-free metrics for languages [164].\n\nThus, it strikes us as unsurprising that AI systems are commonly oversold. Some real-world concepts remain un-measurable or their complexity cannot be measured through a single value. A single loss proxy, where we are unable to express our true goal, is used for training, and a set of different measures is brought out in evaluation. In turn, the narrow optimization focus lacks relation to the actual world, but our often desired abstract concepts are hard, if not impossible, to conceptualize mathematically. The discrepancy between what is being optimized for, what is desired as the outcome, and what is being evaluated, can lead to questionable conjectures of a system's capabilities."}, {"title": "4 Limitations and Disclaimer", "content": "\"Most people think the issue is changing social norms. It is, but it's also (. . .) how willing engineers are to change those systems.\u201d Meredith Broussard [29]\n\nOur work does not claim that social challenges will eventually be overcome through advances in AI design alone. On the contrary, resonating with above quote, we believe that making Al systems more socially and technologically sustainable, requires both the willingness for people to change social norms and to reconceive technical tools to allow this change. With this in mind, there exist a plethora of societal and ethical aspects that our re-conceptualization of cake as a metaphorical AI system has either referenced only in passing or is unable to cover. This is because our work's focus is on pointing out prevalent ramifications and how they are limited by foundational technical underpinnings. As such our work serves the primary purpose to raise awareness of how social and technical elements are inevitably intertwined and, through mapping of social outcomes to technical foundations, create opportunity for different communities to engage in cross-disciplinary dialogue along actionable dimensions. However, we acknowledge that this approach, despite highlighting important new avenues, is also inevitably bound by the cake analogy as a frame of reference. In particular, we understand that the idea of baking a cake, whether reconceived or not, may not be what is desirable on many occasions. To re-emphasize our portions on (over-)selling, there is a crucial difference between naive over-claiming of capabilities and deliberately deploying AI in unnecessary or harmful places. In a similar spirit, our work remains subject to a host of further factors, including, but not limited to, monetary incentives, pressure to publish, the challenges of the academic reviewing system, and imbalanced power dynamics."}, {"title": "5 Conclusion", "content": "We have detailed how the process of making a cake serves as a comprehensive analogy to the design of modern AI systems. Through several drawn parallels across the full AI life cycle, we have highlighted why change towards more sustainable and collaborative Al systems is not merely a social challenge, but how it is also constrained by prevalent technical foundations. Ultimately, our analysis and recommendations thus call for a departure from the traditional ML textbook narrative [20, 64, 107", "18": ".", "9": "that spans consultation, inclusion, collaboration and ownership seems to presently be difficult to implement in practice [5, 48"}]}