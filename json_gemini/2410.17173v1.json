{"title": "REINFORCEMENT LEARNING ON STRUCTURE-CONDITIONED CATEGORICAL DIFFUSION FOR PROT\u0395\u0399\u039d INVERSE FOLDING", "authors": ["Yasha Ektefaie", "Olivia Viessman", "Siddharth Narayanan", "Drew Dresser", "J. Mark Kim", "Armen Mkrtchyan"], "abstract": "Protein inverse folding that is, predicting an amino acid sequence that will fold\ninto the desired 3D structure-is an important problem for structure-based protein\ndesign. Machine learning based methods for inverse folding typically use recovery\nof the original sequence as the optimization objective. However, inverse folding\nis a one-to-many problem where several sequences can fold to the same structure.\nMoreover, for many practical applications, it is often desirable to have multiple,\ndiverse sequences that fold into the target structure since it allows for more candi-\ndate sequences for downstream optimizations. Here, we demonstrate that although\nrecent inverse folding methods show increased sequence recovery, their \u201cfoldable\ndiversity\"-i.e. their ability to generate multiple non-similar sequences that fold\ninto the structures consistent with the target-does not increase. To address this,\nwe present RL-DIF, a categorical diffusion model for inverse folding that is pre-\ntrained on sequence recovery and tuned via reinforcement learning on structural\nconsistency. We find that RL-DIF achieves comparable sequence recovery and\nstructural consistency to benchmark models but shows greater foldable diversity:\nexperiments show RL-DIF can achieve an foldable diversity of 29% on CATH 4.2,\ncompared to 23% from models trained on the same dataset. The PyTorch model\nweights and sampling code are available on GitHub.", "sections": [{"title": "1 INTRODUCTION", "content": "The task of designing sequences of amino acids that fold into a desired protein structure, also known\nas protein \"inverse folding\" (IF) Yue & Dill (1992); Ingraham et al. (2019), is a critical step for many\nprotein design applications in areas such as therapeutics, biomaterials, and synthetic biology Ferruz\net al. (2023); Cao et al. (2022); Dickopf et al. (2020). Deep learning-based IF models are typically\ntrained to recover the observed sequence S of a protein, when conditioned on the corresponding\nbackbone structure X. Models are then evaluated on their ability to recover the original sequence\n(\"sequence recovery\u201d), consistency of the generated sequence's (usually predicted) structure with\nthe target structure (\u201cstructural consistency\u201d), and the diversity of generated sequences (\u201csequence\ndiversity\"). Sequence diversity is of particular interest since the inverse folding problem is a one-to-\nmany mapping in which a single (input) structure could be formed by multiple (output) amino acid\nsequences. For practical applications, it is often desirable to find these alternative sequences that\ncan fold into the desired structure, since it enables greater breadth of options for critical downstream\noptimization steps such as improving stability, preventing aggregation, and reducing immunogenic-\nity. However, methods of increasing sequence diversity (e.g. raising the sampling temperature of an\nautoregressive model) may degrade the structural consistency of candidates. Hence, a useful prop-\nerty of a protein inverse folding model is its \u201cfoldable diversity\u201d\u2014i.e. the diversity of generated\nsequences that fold into the target structure."}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS", "content": "The task of protein inverse folding is to design an amino acid sequence S compatible with spatial\ncoordinates $X \\in \\mathbb{R}^{N\\times3}$ representing the positions of the Ca atoms in the backbone of a protein with\nN residues. We represent S as a sequence of one-hot vectors with vocabulary V consisting of the 20\nnaturally-occurring amino acid species. That is, $S \\in \\{0, 1\\}^{N\\times|V|}$ subject to $\\sum_j S[i, j] = 1, \\forall i$.\nAs introduced in GradeIF Yi et al. (2023b), we use conditional discrete denoising diffusion prob-\nabilistic models (D3PM) Austin et al. (2023) to model $p(S|X)$. Formally, we define a forward\nMarkov diffusion process as random variables $S_0, ..., S_T$ related by:\n$S_t \\sim q(S_t | S_{t-1}, S_0) = Cat(S_t; p = S_{t-1}Q_t)$ \nwhere $Q_1,..., Q_T$ is a sequence of $|V| \\times |V|$ transition matrices, and $S_0 = S$ is the sequence\nobserved in nature. Defining $Q_t = \\prod_{k=1}^t Q_k$, the posterior is:\n$q(S_{t-1} | S_t, S_0) = Cat\\left(S_{t-1}; p = \\frac{S_t Q_t^T \\odot S_0 Q_{t-1}}{S_0 Q_t S_t}\\right)$\nIt is standard to choose $Q_s$ such that $q(S_T | S_0)$ is a stationary distribution $p(S_T)$. In this case,\nlearning a reverse Markov process $p_\\theta(S_{t-1}|S_t; X)$ allows us to generate novel sequences by first\nsampling $S_T \\sim p(S_T)$ and then iteratively denoising samples with $p_\\theta$."}, {"title": "2.2 DENOISING DIFFUSION POLICY OPTIMIZATION", "content": "Assume we can assign a scalar reward $R(\\hat{S})$ for every sample $\\hat{S}$ from a diffusion model $p_\\theta$\nDDPO Black et al. (2024) treats the reverse denoising process as a T-step Markov decision pro-\ncess and defines a policy gradient on $p_\\theta$ to maximize the expected reward $J(\\theta)$:\n$J(\\theta) = \\mathbb{E}_{X \\sim p(X), \\hat{S} \\sim p_\\theta(\\hat{S}|X)} [R(\\hat{S})]$\nwhere $p_\\theta(S_0 | X) = \\mathbb{E}_{S_1,...,S_T} p(S_T) \\prod_{t=1}^T p_\\theta(S_{t-1}|S_t, X)$. The corresponding policy gradient\n$\\nabla_\\theta J(\\theta)$ is:\n$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{X \\sim p(X),\\hat{S}_0,\\dots,\\hat{S}_T \\sim p_{\\text{old}}} \\left[\\sum_{t=1}^T \\frac{p_\\theta(S_{t-1} | S_t, X)}{p_{\\text{old}}(S_{t-1} | S_t, X)} \\nabla_\\theta \\log p_\\theta (S_{t-1}|S_t, X) R(S_0, X) \\right]$\nwhere the importance sampling ratio of the target and sampling policies $\\frac{p_\\theta(\\cdot)}{p_{\\text{old}}(\\cdot)}$ allows for multiple\noptimization epochs per sample."}, {"title": "2.3 INVERSE FOLDING METRICS", "content": "To assess the quality of designed sequences, a number of metrics are used in the protein design\nliterature. Here, we focus our discussion on three commonly-used ones. In these definitions, we\nassume we have a protein structure X with N residues, an observed sequence S, and designed\nsequences {$\\hat{S}^1,...,\\hat{S}^M$}.\n\u2022 Sequence recovery: The average number of amino acid positions in agreement between\nthe observed and proposed sequences:\n$RECOVERY(S; \\hat{S}) = \\frac{1}{N} \\sum_{i=1}^N 1[S[i] = \\hat{S}[i]]$ \n\u2022 Self-consistency TM score (sc-TM): The template modeling (TM) score Zhang & Skol-\nnick (2004) measures the similarity of two protein structures. The score is 1 if they are\nidentical and tends to 0 for dissimilar structures. sc-TM is defined as Trippe et al. (2022):\n$sc\\text{-}TM(\\hat{S}^i; S) = TM\\text{-}SCORE(FOLD(\\hat{S}^i), FOLD(S))$\nwhere FOLD is any protein folding algorithm such as AlphaFold2 Jumper et al. (2021) or\nESMFold Lin et al. (2023). Note that we compare designs to FOLD(S) (not X) to reduce\nthe effect of biases of the folding algorithm Gao et al. (2023).\n\u2022 Sequence diversity: The average fraction of amino acids that differ between pairs of de-\nsigns:\n$DIVERSITY(\\{\\hat{S}^1,...,\\hat{S}^M \\}) = \\frac{2}{NM(M-1)} \\sum_{j=1}^{M} \\sum_{k=1}^{M_{j-1}} \\sum_{i=1}^{N} 1[\\hat{S}^j[i] \\neq \\hat{S}^k[i]]$\n$\\frac{2}{M(M - 1)} \\sum_{j=1}^{M} \\sum_{k=1}^{M_{j-1}} d_H(\\hat{S}^j, \\hat{S}^k)$.\nwhere $d_H$ is the Hamming distance. We note that sequence diversity alone is not a sufficient\nmeasure of a IF method's quality, as it can be increased arbitrarily at the expense of sample\nquality (e.g. as measured by structural consistency)."}, {"title": "3 METHODS", "content": null}, {"title": "3.1 FOLDABLE DIVERSITY", "content": "In realistic protein design scenarios, it is necessary to have a diverse pool of candidates for synthesis\nand experimental characterization. We are particularly interested in the sequence diversity of IF\nmodels when restricted to faithful structures, and so we propose foldable diversity (FD):\n$FOLDABLE \\_ DIVERSITY(\\{\\hat{S}^1,..., \\hat{S}^M\\}) =$\n$\\frac{2}{M(M-1)} \\sum_{j=1}^{M} \\sum_{k=1}^{M_{j-1}} (d_H(\\hat{S}^j, \\hat{S}^k) \\times 1[\\min\\{sc\\text{-}TM(\\hat{S}^j; S), sc\\text{-}TM(\\hat{S}^k; S)\\} > TM_{min}])$\nFoldable diversity only considers the diversity between pairs of sequences that are both structurally\nconsistent with the input protein backbone. Unlike sequence diversity, it is not possible to increase\nFD by sampling high-entropy sequences, without also ensuring they fold correctly. We therefore\nargue that FD is a more grounded metric for evaluating IF methods than sequence recovery (which\npenalizes high-quality, highly-diverse sequences) and sequence diversity (which does not consider\nsequence quality).\nUnless otherwise stated, we set $TM_{min} = 0.7$. Among observed protein structures, this threshold\ntypically guarantees two proteins share a topological classification Xu & Zhang (2010); Wu et al.\n(2020), i.e. are structurally similar. In this regime, FD combines sequence diversity and structural\nconsistency into a single quality measure, if we assume that all sequences with sc-TM above $TM_{min}$\nare \"equally good\"."}, {"title": "3.2 D3PM FOR INVERSE FOLDING", "content": "As in Austin et al. (2023); Yi et al. (2023b), we train a neural network $p_\\theta(S_0|S_t)$ to recover the\nposterior by integrating over the vocabulary:\n$p_\\theta(S_{t-1}|S_t) \\propto \\sum_{v \\in V} q(S_t | S_{t-1}, v) p_\\theta(v|S_t)$\nModel Architecture We parameterize the network as a modified PiFold Gao et al. (2022) archi-\ntecture, adding multilayer perceptrons (MLPs) to process the partially-noised amino acid sequence\nand diffusion timestep.\nSpecifically, given a set of backbone coordinates $X \\in \\mathbb{R}^{4N\\times3}$, we first construct a kNN graph\n(k = 30) between residues. We then use the PiFold featurizer to define node and edge features $h_V$\nand $h_E$. These features include distances between atoms, dihedral angles, and direction vectors.\nThe denoising model is a function of $h_p, h_e$, the partially-denoised sequence $s_t$, and timestep t.\nWe use the following architecture:\n$h_V, h_E = MLP(h_V), MLP(h_E)$\n$h_0 = MLP([s_t, t])$\n$h_{VS} = [h_V, h_0]$\n$h_{out}, h_V^T = (10 \\times PiGNN)(h_{VS}, h_E)$\n$p(S_{t+1}|S_t) = MLP([h_{out}, h_{VS}])$\nwhere [a, b] represents concatenation and PiGNN is the GNN layer introduced by PiFold. Unless\notherwise noted, we set all hidden layer sizes to the recommended values in Gao et al. (2022).\nTraining We observed improved performance from using the full D3PM hybrid loss, as compared\nto the cross-entropy loss used by Grade-IF. We use uniform transition matrices, since we did not\nobserve a benefit from the Block Structured Matrices Henikoff & Henikoff (1992) used in Grade-IF.\nThe model was trained on structure-sequence pairs from the CATH 4.2 dataset with the train/val-\nidation/test split curated by Ingraham et al. Ingraham et al. (2019) based on CATH topological\nclassifications. This dataset and split is used by most prior IF models. It comprises 18025 samples\nfor training, 1637 for validation and 1911 for testing.\nWe used the Adam optimizer Kingma & Ba (2017), a learning rate of 10-3, an effective batch size of\n64 (distributed over 4 Nvidia A10 GPUs), and 150 diffusion time steps. We trained for 200 epochs."}, {"title": "3.3 RL-REFINEMENT OF INVERSE FOLDING MODELS", "content": "As noted above, inverse folding is a one-to-many mapping problem, with potentially a large number\nof sequences satisfying a conditioned structure. Furthermore, publicly-available protein structure\ndata represents a sparse and heterogeneous sampling of the desired distribution $p(S_0 = S|X)$.\nThis one-to-many mapping and data paucity are key challenges in the generalization of conditional\ngenerative models Yi et al. (2023b); Dauparas et al. (2022a); Hsu et al. (2022). While collecting\nmore data is possible, this is an expensive and slow process.\nOn the other hand, given a proposal sequence $\\hat{S}$, we may verify its suitability for the conditioned\nstructure X by evaluating the self-consistency TM score. We therefore propose a second phase of\ntraining, in which the inverse folding diffusion model is optimized for $J(\\theta) = \\mathbb{E}_{\\hat{S} \\sim p_\\theta} [sc\\text{-}TM(\\hat{S}; S)]$.\nIn particular, we use the DDPO algorithm summarized in Section 2.2.\nTraining During the second phase of training, we use the same training dataset described in Sec-\ntion 3.2. Each training step takes as input a batch of 32 protein backbone structures. First, we\nsample 4 sequences per structure from the diffusion model. Raw rewards (sc-TM) are standardized\n($\\mu = 0, \\sigma = 1$) separately for each structure: each set of 4 sequences are shifted and scaled by\ntheir mean and standard deviation respectively. Then, we perform minibatch gradient descent on the\nDDPO objective over the sample sequences (batch size of 32)."}, {"title": "4 RELATED WORKS", "content": "Inverse folding models A majority of existing IF methods utilize transformers (GraphTrans In-\ngraham et al. (2019) and ESM-IF Hsu et al. (2022)) or graph neural networks in which nodes are\namino acids, edges are defined between amino acids close together in the protein structure, and node\nand edge features are constructed from the protein structure backbone (ProteinMPNN Dauparas\net al. (2022b), PiFold Gao et al. (2022), Grade-IF Yi et al. (2023a)). The learning objective of these\nmethods can be discriminative Gao et al. (2022) or autoregressive Dauparas et al. (2022b); Hsu et al.\n(2022); Ingraham et al. (2019).\nMore recently, GradeIF Yi et al. (2023a) introduced a diffusion-based IF method. We note that the\nGradeIF results are competitive, but use solvent accessible surface area (SASA) features, which are\nstrongly correlated with amino acid identity. These additional features are not typically part of the\nIF task specification and their effect is studied in Appendix A.1.\nKWDesign Gao et al. (2024) fuses information from pre-trained protein structure and language\nmodels (GearNet Lopes & Costa (2013), ESM2Lin et al. (2022), and ESMIFHsu et al. (2022)) to\nimprove amino acid representations and boost sequence recovery to 61%.\nRL for biological sequence diffusion SEIKO Uehara et al. (2024) proposes a framework for\nonline tuning of a diffusion model given a reward model and compares their method to DDPO\nand DPOKFan et al. (2024). Among the evaluated problems is the design of green fluorescence\nprotein (GFP) sequences. We note that the IF task differs from GFP design in that the former is a\nconditional generation task, and the primary goal is to generalize to conditions (i.e. structures) not\nobserved during training."}, {"title": "5 EXPERIMENTS", "content": "In this section, we benchmark RL-DIF against SOTA models on different datasets and compare their\nfoldable diversity and structural consistency (sc-TM)."}, {"title": "5.1 BENCHMARKING DATASETS", "content": "We benchmarked on the following datasets:\n\u2022 CATH 4.2: This is identical to the test hold-out of the data described in Section 3.2. We\nfurther use the partitioning of the test data by Ingraham et al. (2019) into a \"short\" subset\nof all proteins shorter than 100 residues (94 proteins); a \"single\" subset of all single chain\nproteins (102 proteins)\n\u2022 TS50 and TS500: TS50 and TS500 Li et al. (2014) are curated lists of proteins of size 50\nand 500 respectively from the PISCES server Wang & Dunbrack (2003).\n\u2022 CASP15: The CASP15 dataset comprises of 45 protein structures used to assess the quality\nof forward-folding models as these structures are de-novo protein structures.\nIn Appendix A.2, we assess the structural and sequence similarity between proteins in these datasets\nand the CATH 4.2 training set. We find that TS50, TS500, and CASP15 have cross-split overlaps (as\ndefined by SPECTRA Ektefaie et al. (2024)) ranging from 42-84%, indicating strong overlap with\nthe training set. We therefore include these datasets for consistency with previous work, but focus\nthe comparison on CATH 4.2"}, {"title": "5.2 MODEL SAMPLING STRATEGIES", "content": "To sample diverse sequences from IF models, different strategies can be employed, depending on\nhow the model parameterizes p(S|X):\n\u2022 Single-shot models: These models parameterize p(S|X) by factorizing the joint distribu-\ntion into $\\prod_{i=1}^N P(S[i]|X)$. This distribution can be reshaped by a temperature parameter.\nA temperature of 0 always picks the highest-probability AA, and higher temperatures en-\ncourage greater diversity.\n\u2022 Autoregressive models: AR models return a probability distribution over possible amino\nacids for a given residue, conditioned on already-sampled residues. As in the single-shot\ncase, we can reshape the conditional distributions with temperature.\nIf an AR model is trained with randomly-permuted residues (instead of left-to-right), then\nwe may also randomize the order in which we decode residues. This allows diverse sam-\npling even at a temperature of 0.\n\u2022 Diffusion models: Since a diffusion model iteratively maps a random vector to an amino\nacid sequence, we can introduce diversity by repeatedly sampling from a stationary distri-\nbution p(ST). We do not reshape the model-predicted posterior $p_\\theta(S_{t-1}|S_t)$.\nFor the specific IF models evaluated in our study, we use the following settings:\n\u2022 ProteinMPNN: An AR model, so we use temperature sampling with temperatures of 0.1,\n0.2, and 0.3. We also sample at a temperature of 0 with random decoding order.\n\u2022 PiFold: A single-shot model, so we use temperature sampling with values 0.1 and 0.2.\n\u2022 KWDesign: Another single-shot model, so we use the same settings as PiFold.\n\u2022 ESMIF: An AR model. We follow the authors' recommendation and sample with a tem-\nperature of 1.\n\u2022 DIF-Only and RL-DIF: Diffusion models, so we sample from the uniform distribution\np(ST) and iteratively denoise sequences using the model."}, {"title": "5.3 BENCHMARKING AND PERFORMANCE OF IF MODELS TRAINED ON CATH4.2", "content": "BENCHMARK\nExperimental Setup We compare RL-DIF to ProteinMPNN Dauparas et al. (2022b), PiFold Gao\net al. (2022), and KWDesign Gao et al. (2024). We also evaluate our model after the diffusion\npre-training phase, before any RL-optimization (DIF-Only). For each model and each benchmark\ndataset, we run the following pipeline:\n1. For each protein structure in the dataset, sample 4 sequences from the model.\n2. Among the 4 sampled sequences, compute the mean sequence recovery and sc-TM score.\n3. Compute the foldable diversity of the 4 sequences, as defined in Equation 8, with $TM_{min} =$\n0.7.\nModel weights were not available for KWDesign for sampling, however they provided the probabil-\nity distributions for CATH4.2 and CASP15, and we sampled from these to generate sequences."}, {"title": "5.4 COMPARISON OF RLDIF PERFORMANCE TO ESM-IF", "content": "Experimental Setup ESM-IF Hsu et al. (2022) is another protein inverse folding model that was\ntrained on CATH 4.3 and 12 million synthetic data samples generated by AlphaFold 2 Jumper et al.\n(2021). While a large database of AlphaFold-ed structures is publicly available (EMBL-EBI), the\nESM-IF paper did not release the IDs of the proteins that were trained on. These IDs are essential\nin order to replicate their work and ensure no overlap between the training and benchmark datasets.\nTo attempt to replicate their work, we followed a similar strategy of curating a larger pre-training\ndataset orthogonal to all benchmarking datasets we consider. This was done using Foldseek van"}, {"title": "5.5 THE EFFECT OF REINFORCEMENT LEARNING ON DIF-ONLY PERFORMANCE", "content": "To investigate the effect of reinforcement learning on DIF-Only performance we ran our RL for\n4000 steps, taking the model at every 1000 steps and benchmarking its performance for all metrics.\nWe found average sc-TM had the largest improvement in the first 1000 steps of RL before stagnating\nafter step 2000 (Table 3). Foldable diversity continued to decrease with modest gains in structural\nconsistency. These results support our decision to run RL for 1000 steps."}, {"title": "6 CONCLUSION", "content": "Although sequence recovery, sequence diversity, and structural consistency are commonly used to\nevaluate protein IF models, here we demonstrate that those metrics alone do not necessarily capture\nthe ability of the model to generate multiple sequences that fold into the desired structure. For many\npractical applications, foldable diversity is a useful metric since it gives users multiple \u201cshots-on-\ngoal\" for filtering or optimizing designs on criteria beyond structural consistency. We also present"}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 GRADEIF RESULTS", "content": "When exploring the performance of GradeIF we found the solvent accessible surface area (SASA)\nfeature is calculated utilizing side chain information. When plotting the SASA values per amino\nacid type, there is a clear separation between some amino acids which can inflate model performance\n(Figure S1). When we retrained GradeIF with recommended hyperparameters without (Figure S2a)\nand with SASA (Figure S2b) we noticed a 23% decrease in performance. With this in mind, we\ndecided to pursue utilizing PiFold Gao et al. (2022) as the underlying architecture for the categorical\ndiffusion."}, {"title": "A.2 SPECTRA AND FOLDSEEK", "content": "To understand how model performance changes as a function of test protein similarity to the\nCATH4.2 training set, we utilize the spectral framework of model evaluation (SPECTRA) Ekte-\nfaie et al. (2024). We utilized Foldseek van Kempen et al. (2024) to determine if two proteins were\nsimilar. Two proteins are similar if e-value returned by Foldseek is greater than 1e-3 or the sequence\nsimilarity is greater than 0.3. Given two datasets, cross-split overlap is defined as the proportion of\nproteins that are similar."}]}