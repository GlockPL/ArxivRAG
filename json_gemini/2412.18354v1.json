{"title": "THE THOUSAND BRAINS PROJECT: A NEW PARADIGM FOR SENSORIMOTOR INTELLIGENCE", "authors": ["Viviane Clay", "Niels Leadholm", "Jeff Hawkins"], "abstract": "Artificial intelligence has advanced rapidly in the last decade, driven primarily by progress in the scale of deep-learning systems. Despite these advances, the creation of intelligent systems that can operate effectively in diverse, real-world environments remains a significant challenge. In this white paper, we outline the Thousand Brains Project, an ongoing research effort to develop an alternative, complementary form of AI, derived from the operating principles of the neocortex. We present an early version of a thousand-brains system, a sensorimotor agent that is uniquely suited to quickly learn a wide range of tasks and eventually implement any capabilities the human neocortex has. Core to its design is the use of a repeating computational unit, the 'learning module', modeled on the cortical columns found in mammalian brains. Each learning module operates as a semi-independent unit that can model entire objects, represents information through spatially structured reference frames, and both estimates and is able to effect movement in the world. Learning is a quick, associative process, similar to Hebbian learning in the brain, and leverages inductive biases around the spatial structure of the world to enable rapid and continual learning. Multiple learning modules can interact with one another both hierarchically and non-hierarchically via a cortical messaging protocol (CMP), creating more abstract representations and supporting multimodal integration. We outline the key principles motivating the design of thousand-brains systems and provide details about the implementation of Monty, our first instantiation of such a system. Code can be found at https://github.com/thousandbrainsproject/tbp.monty, along with more detailed documentation at https://thousandbrainsproject.readme.io/.", "sections": [{"title": "1 Introduction", "content": "We are developing a platform for building AI and robotics applications using the same principles as the human brain, a broad research initiative called the Thousand Brains Project. The principles this project builds on are fundamentally different from those used in deep learning, currently the most prevalent form of AI. Therefore, our platform represents an alternative form of AI, one that we believe will play an ever-increasing role in the future.\nThis paper outlines the motivation of the Thousand Brains Project, as well as the technical details of the underlying algorithm for sensorimotor intelligence. The aim is to enable developers to build AI applications that are more intelligent, more flexible, and more capable in the many applications that deep learning methods fail. Core to the design of thousand-brains systems are the principles laid out in the Thousand Brains Theory [Hawkins et al., 2019], a theory of intelligence derived from neuroscientific evidence of the anatomy and function of the neocortex. One core principle of the theory builds on the work of Vernon Mountcastle, who argued that the power of the mammalian brain lies in its re-use of cortical columns as the primary computational unit [Mountcastle, 1997, Edelman and Mountcastle, 1978]. In honor of Mountcastle's idea, we name the first practical implementation of a thousand brains system \"Monty\". The code for building and experimenting with Monty can be found at https://github.com/thousandbrainsproject/tbp.monty.\nOne key differentiator between thousand-brains systems and other Al technologies is that the former are built with embodied, sensorimotor learning at their core. Sensorimotor systems learn by sensing different parts of the world over time while interacting with it. For example, as you move your body, your limbs, and your eyes, the input to your brain changes. In thousand-brains systems, the learning derived from continuous interaction with an environment represents the foundational knowledge that supports all other functions. This contrasts with the growing approach that sensorimotor interactions are a sub-problem that can be solved by beginning with an architecture trained on a mixture of internet-scale language and multi-media data [Driess et al., 2023, OpenAI et al., 2023, Black et al., 2024]. In addition to sensorimotor interaction being the core basis for learning, the centrality of sensorimotor learning manifests in the design choice that all levels of processing are sensorimotor. As will become clear, sensory and motor processing are not broken up and handled by distinct architectures, or limited to a single, global action output [Reed et al., 2022, Driess et al., 2023, Team et al., 2024, Black et al., 2024]. Instead, sensation and motor outputs play a crucial role at every point in thousand-brains systems where information is processed.\nA second differentiator is that our sensorimotor systems learn structured models, using reference frames, explicit coordinate systems within which locations and rotations can be represented. Internal models derived from these reference frames keep track of where their sensors are relative to things in the world. Models are learned by assigning sensory observations to locations in reference frames. In this way, the models learned by sensorimotor systems are structured, similar to CAD models in a computer. This allows the system to quickly learn the structure of the world and how to manipulate objects to achieve a variety of goals, what is sometimes referred to as a world model. As with sensorimotor learning, reference frames are used throughout all levels of information processing, including the representations of not only environments but also physical objects and abstract concepts - even the simplest representations are represented within a reference frame.\nThere are numerous advantages to sensorimotor learning and reference frames. At a high level, you can think about all the ways humans are different from today's AI. We learn quickly and continuously, constantly updating our knowledge of the world as we go about our day. We do not have to undergo a lengthy and expensive training phase to learn something new. We interact with the world and manipulate tools and objects in sophisticated ways that leverage our knowledge of how things are structured. For example, we can explore a new app on our phone and quickly figure out what it does and how it works based on other apps we know. We actively test hypotheses to fill in the gaps in our knowledge. We also learn from multiple modalities and these different sensory inputs work together seamlessly. For example, we may learn what a new tool looks like with a few glances and then immediately know how to grab and interact with the object via touch. Finally, we carry out complex, planned actions that leverage our knowledge of the world to enable intelligent behavior in new settings.\nOne of the most important discoveries about the brain is that most of what we think of as intelligence, from seeing, to touching, to hearing, to conceptual thinking, to language, is enabled by a common neural algorithm [Mountcastle, 1997]. All aspects of intelligence are created by the same sensorimotor mechanism. In the neocortex, this mechanism is implemented in each of the thousands of cortical columns. This means we can create many different types of intelligent systems using a set of common building blocks. The architecture we are creating is built on this premise. Thousand-brains systems will provide the core components and developers will then be able to assemble widely varying AI and robotics applications using them in different numbers and arrangements. We now elaborate on the high-level motivations of the Thousands Brains Project (TBP), before describing the technical details of Monty, the first instance of a thousand-brains system."}, {"title": "2 The Thousand Brains Project", "content": null}, {"title": "2.1 Long Term Goals", "content": "A central long-term goal is to build a universal platform and messaging protocol for intelligent sensorimotor systems. We call this protocol the \"Cortical Messaging Protocol\""}, {"title": "2.2 Core Principles", "content": "We have a set of guiding principles that steer the Thousand Brains Project. Throughout the life of the project, there may be several different implementations, and within each implementation, there may be different versions of the core building blocks, but everything we work on should follow these core principles:\n\u2022 Sensorimotor learning and inference: We use actively generated temporal sequences of sensory and motor inputs instead of static inputs. The outputs of the system are motor commands.\n\u2022 Modular structure: The same algorithm needs to work for all modalities. This general algorithm embodied in a learning module makes the system easily expandable and scalable.\n\u2022 Cortical Messaging Protocol: The inputs and outputs of a learning module adhere to a defined protocol such that many different sensor modules (and modalities) and learning modules can work together seamlessly.\n\u2022 Voting: A mechanism by which a collection of experts can use different information and models to come to a faster, more robust and stable conclusion.\n\u2022 Reference frames: The learned models should have inductive biases that make them naturally good at modeling a structured world that evolves over time. The learned models can be used for a variety of tasks such as manipulation, planning, imagining previously unseen states of the world, fast learning, generalization, and many more.\n\u2022 Rapid, continual learning where learning and inference are closely intertwined: Supported by sensorimotor embodiment and reference frames, biologically plausible learning mechanisms enable rapid knowledge accumulation and updates to stored representations while remaining robust under the setting of continual learning. There is also no clear distinction between learning and inference. We are always learning, and always performing inference.\n\u2022 Model-free and model-based policies: Low-level, model-free policies provide efficient means of interacting with the world, but are crucially combined with model-based policies that support flexible action planning in novel situations.\nIn the initial implementation presented here, many components are deliberately not biologically constrained, and/or simplified, so as to support visualizing, debugging, and understanding the system as a whole. For example, object models are currently based on explicit graphs in 3D Cartesian space. In the future, these elements may be sub-"}, {"title": "2.3 Challenging Preconceptions", "content": "Several of the ideas and ways of thinking introduced in this document may be counter-intuitive to those familiar with current AI methods, including deep learning. For example, ideas about intelligent systems, learning, models, hierarchical processing, or action policies that you already have in mind might not apply to the system that we are describing. We therefore ask the reader to try and dispense with as many preconceptions as possible and to understand the ideas presented here on their own terms. We are happy to discuss any questions or thoughts that may arise from reading this document. Please join our Discourse forum or reach out to us at info@thousandbrains.org.\nBelow, we highlight some of the most important differences between the system we are building and other AI systems.\n\u2022 We are building a sensorimotor system. It learns by interacting with the world and sensing different parts of it over time. It does not learn from a static dataset. This is a fundamentally different way of learning than most leading AI systems today and addresses a (partially overlapping) different set of problems.\n\u2022 We will introduce learning modules as the basic, repeatable modeling unit, comparable to a cortical column. An important detail to point out here is that none of these modeling units receives the full sensory input. For example, in vision, there is no 'full image' anywhere. Each sensor senses a small patch in the world. This is in contrast to many AI systems today, where all sensory input is fed into a single model.\n\u2022 Despite the previous point, each modeling system can learn complete models of objects and recognize them on its own. A single modeling unit should be able to perform all basic tasks of object recognition and manipulation. Using more modeling units makes the system faster and more efficient and supports compositional and abstract representations, but a single learning module is itself a powerful system. In the single module scenario, inference always requires movement to collect a series of observations, in the same way that recognizing a coffee cup with one finger requires moving across its surface.\n\u2022 All models are structured by reference frames. An object is not just a bag of features. It is a collection of features at locations. The relative locations of features to each other are more important than the features themselves. These principles are used for modeling all discrete concepts in the world, from the simplest of physical objects to abstract concepts in society or mathematics.\n\u2022 Action policies are, first and foremost, model-based. Learned models of objects in the world are used to determine appropriate actions in novel situations. Any given learning module can use its internal models to propose goal-states that are either decomposed into simpler goal-states in other learning modules, or are acted upon directly by motor systems. In this way, complex policies can be hierarchically decomposed, while still leveraging learned models. Over time and with practice, model-based policies become more efficient, while model-free policies can learn to do certain tasks rapidly and with finesse, but model-free policies are not the initial basis of actions in unfamiliar settings.\n\u2022 Motor output can be generated at any level of the system. In contrast to many current approaches for sensorimotor interaction, we do not have a separate hierarchy of sensory processing followed by the generation of motor commands. Instead, each learning module, even at the lowest sensory level, produces action outputs. This is analogous to the projections to subcortical motor regions found in every area of the neocortex, even regions classically thought of as sensory regions."}, {"title": "2.4 Capabilities of the System", "content": "The thousand-brains architecture is designed to be a general-purpose AI system. It is not designed to solve a specific task or set of tasks. Instead, it is designed to be a platform that can be used to build a wide variety of AI applications. Like an operating system or a programming language does not define what the user applies it to, the Thousand Brains Project will provide the tools necessary to solve many of today's current problems as well as completely new and unanticipated ones without being specific to any one of them.\nEven though we cannot predict the ultimate use cases of the system, we want to test it on a variety of tasks and keep a set of capabilities in mind when designing the system. The basic principle here is that it should be able to solve any task the neocortex can solve. If we come up with a new mechanism that makes it fundamentally impossible to do something the neocortex can do, we need to rethink the mechanism. For example, thousand-brains systems should be able to model the world through any kind of movement-based sensory modality, from touch to echolocation. They should also be able to conceptualize abstract spaces, execute a series of intricate movements, and plan long-term actions. However, tasks such as multiplying arbitrary large numbers, or predicting the structure of a protein given its genetic sequence, are domains much better left to alternative technologies, such as calculators or deep-learning."}, {"title": "3 Overview Of The Architecture", "content": "There are three major components that play a role in the architecture: sensor modules, learning modules, and the motor system. These three elements are tied together by a final key component, a common communication protocol. Due to this unified messaging protocol, the inner workings of each individual component can be quite varied as long as they have the appropriate interfaces. A simple example of a sensor module coupled to a learning module is shown in Figure 1, although we will begin by describing the CMP."}, {"title": "3.1 Cortical Messaging Protocol", "content": "We use a common communication protocol that all components - learning modules, sensor modules, and the motor system - use to share information. By defining a consistent information format that sensor modules and learning modules must output, and that motor systems must receive, it is possible for all components to communicate with each other, and to combine them arbitrarily. Due to its inspiration from long-range connections in the cortex, we call this common communication protocol the Cortical Messaging Protocol (CMP)\nIn order to define the CMP, we must first define what we mean by an object, or a feature, in Monty. An object is a discrete entity composed of a collection of one or more other objects, each with their own associated pose. For example, an apple at a location and orientation in space is an object, but equally, an object could be a scene, an abstract arrangement of concepts, or any other composition of sub-objects. At the lowest level of this object hierarchy, an object is composed of inputs from a sensor module, which are also discrete entities with a location and orientation in space. Sensor inputs play a similar role to objects at other points in a hierarchy of learning modules, but the 'objects' detected by sensors cannot be further decomposed. Wherever an object is being processed by a component of the system, it can also be referred to as a feature. By convention, we usually refer to the input of a learning module as features and the output as an object ID. However, the object ID output of one learning module can become the feature input to the next learning module so they are by definition the same.\nAt its core, a CMP-compliant output contains a feature at a pose. The pose contains a location in 3D space (naturally including 1D or 2D space) representing where the sensed feature is relative to the body, or another common reference point such as a landmark in the environment. In addition to location, the pose includes information about the feature's 3D orientation, which could be defined by the direction of a surface's point normal and its principal curvature, or the orientation of an object. Importantly, the message may contain additional feature information, such as color, the magnitude of sensed curvature, or an object ID. Counter-intuitively, the nature of the feature does not need to be specified in the message for it to be a valid signal.\nWe highlight the choice that non-pose attributes of a feature are optional. This is contrary to many existing AI systems where models are often closer to bags-of-features and object structure is weakly represented, if at all. Here, the relative locations of features are more important than the features themselves. An example of how this aligns with human perception is how fruits arranged in the shape of a face can be easily recognized as a face, even though no typical face \"features\" are present. On the other hand, humans would not classify a random arrangement of eyes, a nose, and lips as a face.\nBesides features and their poses, the standardized message also includes information about the sender's ID (e.g. the particular sensor module) and a confidence rating. Further below we discuss the internal models that learning-modules (LMs) develop - importantly, the CMP is never used to share these models between LMs. Instead, it can only communicate more abstract information about these models (such as an object ID). The inputs and outputs of the system (raw sensory input to the SM and motor command outputs from motor modules) can have any format and do not adhere to any messaging protocol. They are specific to the agents sensors and actuators and represent the systems interface with the environment. Fin a common reference frame (e.g. relative to the body 2). This makes it possible for all components to meaningfully interpret the pose information they receive.\n\u00b2In the following sections we may call this common reference frame \"body-centric\". In general, we just mean a common reference frame for all sensors. There may be applications without a concrete body (like several cameras set up in different locations of a room, a swarm of agents, or an agent navigating a more abstract space like the internet) where this just refers to an arbitrary point in space that all communicated poses are relative to."}, {"title": "3.2 Sensor Modules", "content": "Thousand-brains systems can work with any type of sensor (vision, touch, radar, LiDAR,...) and integrate information from multiple sensory modalities without effort. For this to work, sensor modules need to communicate information in a common language. Transforming raw sensory input into this common language is the job of the sensor module.\nEach sensor module receives information from a small sensory patch as input. This is analogous to a small patch on the retina, or a patch of skin, or the pressure information at one whisker of a mouse. In the simplest architecture, one sensor module sends information to one learning module, which models this information. How such local sensory inputs are integrated across time and space will be covered in a moment when we discuss learning modules.\nThe information processing within the sensor module turns the raw information from the incoming sensor patch into the cortical messaging protocol (detailed in section 3.1). This process can be compared to light hitting the retina and being converted into spikes, the output of biological neurons. Additionally, the pose of the feature relative to the body is calculated from the feature's pose relative to the sensor and the sensor's pose relative to the body. As such, each sensor module outputs the feature it senses, as well as the feature's pose (location and rotation) in body-centric coordinates. The availability of this pose information is central to how the thousand-brains architecture operates.\nA general principle of the system is that any processing specific to a modality happens in the sensor module. The output of the sensor module is not modality-specific anymore and can be processed by any learning module. A crucial requirement here is that each sensor module knows the pose of the feature relative to the sensor. This means that sensors need to be able to detect features and poses of features. They also need to be able to keep track of their position in space. This could be directly provided from the system, inferred from sensory inputs (like optical flow), or calculated from efference copies of motor commands."}, {"title": "3.3 Learning Modules", "content": "The basic building block for sensorimotor processing and modeling is the learning module (LM). These are repeating elements, each using the same input and output interface. Each LM should function as a stand-alone unit and be able to recognize objects on its own. Combining multiple LMs can speed up recognition (e.g. recognizing a cup using five fingers vs. one), allows for LMs to focus on storing only some objects, and enables learning compositional objects.\nLMs receive features at poses. Features can either be feature IDs from a sensor module or object IDs (also interpreted as features) from a lower-level LM. The feature or object representation might be in the form of a discrete ID (e.g. the color red, a cylinder), or could be represented in a more high dimensional space (e.g. a vector of binary values representing hue, or corresponding to a fork-like object). Additionally, LMs receive the feature's or object's pose relative to the body, where the pose includes location and rotation. In this way, body-centric coordinates serve as a common reference frame for spatial computations, as opposed to the pose of features relative to each individual sensor. From this information, higher-level LMs can build up structured models of compositional objects (e.g. large objects or scenes).\nThe features and relative poses are incorporated into a model of the object. All models have an inductive bias towards learning objects within a 3-dimensional space, complimented by a temporal dimension. When interacting with the physical world, the 3D inductive bias is used to place features in internal models accordingly. However, the exact structure of space can potentially be learned, such that the lower-dimensional space of a melody, or the abstract space of a family tree, can be represented.\nThe LM, therefore, encompasses two major principles of the TBT: sensorimotor learning, and building models using reference frames (see Figure 2). Both ideas are motivated by studies of cortical columns in the neocortex (see Figure 3), as well as Hawkins et al. [2017, 2019].\nBesides learning new models, the LM also tries to match the observed features and relative poses to already learned models stored in memory. Internally, LMs use displacements between consecutive poses and map them into the model's reference frame. This makes it possible to detect objects even at novel poses.\nTo generate the LM's output, we need to get the pose of the sensed object relative to the body. We can calculate this from the current incoming pose (pose of the sensed feature relative to the body) and the poses stored in the model of the object. This pose of the object can then be passed hierarchically to another LM in the same format as the sensory input (features at a pose relative to the body where the feature is the inferred object ID).\nOnce the LM has determined an object's ID and pose, it can use the most recent observations (and possibly collect more) to update its model. As such, LMs continually learn more about the world, and learning and inference are two closely intertwined processes."}, {"title": "3.4 Motor Information and Action Policies", "content": "Movement is central to how thousand-brains systems understand the world. The spatial nature of reference frames is dependent on integrating movement information so that a learning module knows where its input features are located at any given moment. The movement information (pose displacement) can be a copy of the selected action command (efference copy) or deduced from the sensory input. Without the efference copy, movement can be detected from information such as optical flow or proprioception. Sensor modules use movement information to update their pose relative to the body. LMs use it to update the hypothesized location of their incoming features within an object's reference frame.\nWhile movement is clearly important for an LM to understand the outside world, it is also important that this movement is not random. What's more, an intelligent system should be able to exert influence on the external world. This is where policies become crucial.\nThousand-brains systems make use of a combination of model-free policies, corresponding to lower-level components of the system (sensor-module - motor-system loops), together with model-based policies based within LMs and using learned models to inform actions.\nModel-based policies use more computational resources to enable more principled movement, such as moving a sensor to a location that will minimize uncertainty about the currently observed object. These policies are derived from LMs, where each LM produces a motor output, analogous to the universal motor outputs found in cortical columns [Prasad et al., 2020]. The motor output is formalized as a goal state and also adheres to the CMP. The goal state could, for example, use the learned models and current hypotheses to calculate a sensor state that resolves uncertainty about which of two possible objects is being observed. It can also help to guide directed and more efficient exploration of parts of objects that are currently underrepresented in the internal models. Different policies can be leveraged depending on whether we are trying to recognize an object or trying to learn new information about an object. Finally, policies can enable a learning module to change the state of the world, such as pushing a button, or changing the position of an object on a table.\nHierarchy can also be leveraged for goal-states, where a more abstract goal-state in a high-level LM can be achieved by decomposing it into simpler goal-states for lower-level LMs. Importantly, the same LMs that learn models of objects are used to generate goal-states, enabling hierarchical, model-based policies.\nModel-free policies are useful for purely sensory-based actions such as smoothly moving a sensor over the surface of an object, or attending to a prominent feature. Model-free policies can also learn to carry out frequently performed tasks in a dexterous and rapid manner, freeing computational resources required for model-based policies. Finally, goal-states generated by LMs must be transformed into motor commands for actuators - a process that recruits model-free policies (innate or learned) in the motor system.\nIn the brain, much of this processing occurs subcortically. In a thousand-brains system, this corresponds to the motor-system area. We note that the motor area does not know about models of objects that are learned in the LMs, and therefore needs to receive useful goal states from the LMs. These commands adhere to the CMP, but the outputs of the motor area will deviate from the protocol in order to interface with the actuators of the system. This means the motor system serves the reverse role of the sensor module, translating CMP-compliant goal states into the specific movement commands of the actuator it is connected to."}, {"title": "3.5 Multi-LM Systems", "content": "Any given Monty system can be composed of multiple learning modules. Depending on their arrangement, LMs interact with one-another in a hierarchical manner, or via voting. A brief overview of these concepts is given below, while these possibilities are shown visually in Figure 4."}, {"title": "3.5.1 Hierarchy: Composition and Learning on Different Spatial Scales", "content": "Learning modules can be stacked in a hierarchical fashion to process larger input patches and higher-level concepts. A higher-level LM receives feature and pose information from the output of a lower-level module and/or from a sensor patch with a larger receptive field, mirroring the connectivity of the cortex. The lower-level LM never sees the entire object it is modeling at once but infers it either through multiple consecutive movements and/or voting with other modules. The higher-level LM can then use the recognized model ID as a feature in its own models. This makes it more efficient to learn larger and more complex models as we do not need to represent all object details within one model. In particular, this enables the representation of compositional objects by quickly associating different object parts with each other as relative features in a higher-level model. We discuss the importance of composition more later."}, {"title": "3.5.2 Voting: Rapid Consensus", "content": "LMs share lateral connections in order to communicate their estimates of the current object ID and pose. This process, which we term voting, adheres to the CMP, passing feature-pose information. Unlike connections between lower and higher LMs however, voting communicates a set of all possible objects and poses under the current evidence (i.e. multiple messages adhering to the CMP). Through the lateral voting connections, LMs attempt to reach a consensus on which object they are sensing at the moment and its pose. This helps to recognize objects faster than a single module could.\nWe earlier highlighted that CMP messages are encoded in a common reference frame. This is key for voting to account for the relative displacement of sensors and, therefore, locations within LM models. For example, when two fingers touch a coffee mug in two different parts, one might sense the rim, while the other senses the handle. As such, 'coffee mug' will be in both of their working hypotheses about the current object. When voting, they do not simply communicate 'coffee mug', but also where on the coffee mug other LMs should be sensing it, according to their relative displacements. As a result, voting is not simply a 'bag-of-features' operation but is dependent on the relative arrangement of features in the world.\nNote that votes sent via the CMP do not contain any information about the input features received by that LM. For example, an LM might receive point-normals and surface curvature as its input features from an SM, and use this to model objects like coffee mugs and staplers. During voting, it will communicate its hypotheses around coffee mugs and staplers, but it will not communicate any information about sensed curvature to other learning modules.\nFinally, the CMP is independent of modality, and as such, LMs that have learned objects in different modalities (e.g. vision and touch), can still vote with each other to quickly reach a consensus. This voting process is inspired by the voting process described in Hawkins et al. [2017]."}, {"title": "3.6 Bringing it Together", "content": "To consolidate these concepts, please see Figure 5 for an example instantiation of the system in a concrete setting. In this example, we see how the system could be applied to sensing and recognizing objects and scenes in a 3D environment using several different sensors, in this case, touch and vision.\nWhile provided to make the key concepts described above more concrete, bear in mind that this represents only one example of how the architecture can be instantiated. By design, thousand-brain systems can be applied to any application that involves sensing and active interaction with an environment. Indeed, this might include more abstract examples such as browsing the web, or interacting with the instruments that control a scientific experiment."}, {"title": "4 Implementation", "content": "We now describe in further detail the implementation of Monty, the first instance of a thousand-brains system. As just outlined in the general case, the basic components of Monty are: sensor modules (SM) to turn raw sensory data into a common language; learning modules (LM) to model incoming streams of data and use these models for interacting with the environment; motor system(s) to carry out actions, and translate abstract motor commands from the learning module into a format for actuators; and an environment in which the system is embedded and which it tries to model and interact with. The components within the Monty are connected by the Cortical Messaging Protocol (CMP) so that basic building blocks can be easily repeated and stacked on top of one another. Any communication within Monty is expressed as features at poses relative to a common reference frame such as the body. These CMP-compliant signals can be interpreted in different ways. For example, pose to the motor system is a target to move to, pose to an LM is the most likely sensed (from SM) or inferred (from LM) pose, and poses via voting connections are possible poses.\nAll of these elements are implemented in Python at https://github.com/thousandbrainsproject/tbp.monty and their algorithmic details are described in the following sections. We begin by going into detail about general concepts, such as the experimental environment we currently employ, before describing the specifics of sensor modules, learning modules, and finally, action policies. This algorithm is under active development, and for a more detailed and the most up-to-date description, please refer to our documentation.\nNote that these descriptions refer to our current implementation and there will likely be many other implementations of the different components in the future. The idea of this framework is that any component can be customized and replaced as long as it follows the defined interface. For instance, one can switch out the type of learning module used without changing the sensor modules, environment, or motor system. Alternatively, one could implement a sensor module for a specific sensor and plug this into an existing learning module. Yet another possibility would be to test the current Monty configuration in another sensorimotor environment. The possibilities are endless, and the specific configuration and testbed described below is simply one instantiation that we found useful for designing this system."}, {"title": "5 Experimental Evaluations", "content": "The testbed currently used most often is focused on object recognition. While this involves learning models of objects and interacting with the environment, it all serves the purpose of recognizing objects and their poses. In the future, this focus will shift to settings where object pose and ID recognition subserves more complex interactions with the environment, such as manipulating the world to reach certain goal-states.\nDuring an experiment, an agent collects a sequence of observations by interacting with an environment. We distinguish between training (internal models are updated using this sequence of observations) and evaluation (the agent only performs inference using already learned models but does not update them). In practice, Monty will always be learning, but establishing a distinct evaluation phase is useful for benchmarking performance in a controlled way."}, {"title": "6 Environment and Agent", "content": "The 3D environment and simulation engine used for most experiments is Habitat [Savva et al., 2019, Szot et al., 2021, Puig et al., 2023]. An agent is an actuator that can move independently in the environment, and has sensors coupled to it. Environments are currently initialized with one agent that has N sensors attached to it. For most experiments, two sensors are used: the first sensor is the sensor patch, which is used for learning. It is a camera zoomed in 10x so that it can only perceive a small patch of the environment. The second sensor is a view-finder, which is at the same location as the patch and moves together with it, but its camera is not zoomed in. The view-finder is only used at the beginning of an episode to get a good view of the object and for visualization, but not for learning or inference (more details are in the discussion of policies found in Section 11). The agent setup can also be customized to use more than one sensor patch, such as the five patches in Figure 7).\nOne can also initialize multiple agents (each with multiple sensors) and connect them to the same Monty instance. The difference between adding more agents vs. adding more sensors to the same agent is that all sensors connected to one agent move together, like neighboring patches on the retina. On the other hand, separate agents can move independently, like fingers on a hand (see Figure 8).\nThe environment we typically evaluate object and pose detection in is an empty space with one object, although we are beginning to experiment with multiple objects. The object can be initialized in different rotations, positions and scales, although we do not currently vary the latter. For objects, one can either use the default Habitat objects (cube, sphere, capsule, etc.) or the YCB object dataset [Calli et al., 2015], containing 77 more complex objects such as a cup, bowl, chain, or hammer, as shown in Figure 9. Currently there is no physics simulation so objects are not affected by gravity or touch and therefore do not move.\nOf course, other datasets and objects can be used, and indeed we are not limited to 3D environments. For example, one data configuration lets agent move in 2D over images from the Omniglot dataset or photos from an RGBD camera. The only crucial requirement is that we can use an action to retrieve a new, action-dependent, observation from which we can extract a pose. Finally, we have implemented a simple dataset with physics-dependent objects that evolve over time, although we have not yet begun testing Monty in this setting. For a full list of current environments see https://thousandbrainsproject.readme.io/docs/environment-agent."}, {}]}