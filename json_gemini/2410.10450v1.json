{"title": "KBLAM: KNOWLEDGE BASE AUGMENTED LANGUAGE MODEL", "authors": ["Xi Wang", "Liana Mikaelyan", "Taketomo Isazawa", "James Hensman"], "abstract": "In this paper, we propose Knowledge Base augmented Language Model (KBLAM), a new method for augmenting Large Language Models (LLMs) with external knowledge. KBLAM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous key-value vector pairs via pre-trained sentence encoders with linear adapters and integrating them into pre-trained LLMs via a specialized rectangular attention mechanism. Unlike Retrieval-Augmented Generation, KBLAM eliminates external retrieval modules, and unlike in-context learning, its computational overhead scales linearly with KB size rather than quadratically. Our approach enables integrating a large KB of more than 10K triples into an 8B pre-trained LLM of only 8K context window on one single A100 80GB GPU and allows for dynamic updates without model fine-tuning or retraining. Experiments demonstrate KBLAM's effectiveness in various tasks, including question-answering and open-ended reasoning, while providing interpretable insights into its use of the augmented knowledge.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated impressive knowledge and reasoning capabilities. However, in many scenarios, users need to augment LLMs with external knowledge, particularly when concept definitions differ from or extend beyond the information stored in the LLM's parameters. One straightforward approach is to perform supervised fine-tuning on the external corpus (Hu et al., 2021; Liu et al., 2024c) to alter the parameter weights. However, fine-tuning can be inefficient as the weights need to be updated each time knowledge is updated. Additionally, fine-tuning can cause catastrophic forgetting, i.e. degrading performance on general tasks. As such, alternative methods that do not modify LLM's weights have gained more and more popularity.\nOne such alternative approach is Retrieval Augmentation Generation (RAG, Lewis et al., 2020). RAG assumes external knowledge is stored as unstructured documents, then given an input prompt, RAG first uses a retriever module to extract relevant corpus chunks from the documents. These extractions are then concatenated to the input question as a prompt and fed into the LLM (Fig. 1 top). This approach allows RAG to utilize knowledge from extensive documents while maintaining a small context size, overcoming the context limitations of many early-generation LLMs.\nRecently, with the emergence of long-context language models such as GPT4 (Achiam et al., 2023) and Gemini (Team et al., 2023), it has become possible to directly put the entire external corpus in the context, eliminating the need for the retriever for document selection, simplifying the pipeline. We refer to this approach as in-context learning (Fig. 1 middle). Several recent works (Lee et al., 2024; Li et al., 2024; Yu et al., 2024) have explored whether long-context models can replace RAG in long-context reasoning tasks. In particular, Lee et al. (2024) argues that in-context learning has potential advantages, such as better compatibility with chain-of-thought reasoning and few-shot\n*Work done during an internship at Microsoft Research."}, {"title": "2 RELATED WORK\u00b9", "content": "Retrieval augmented generation (RAG) RAG (Lewis et al., 2020) is one of the most successful approaches for augmenting external knowledge into LLMs, and many approaches have been proposed to perform RAG, such as similarity search and text2Sql (Qin et al., 2022). KBLAM can also be understood as the model doing RAG in an implicit way: given input prompts, the rectangular attention compares the sequence's queries with knowledge tokens' keys, then the weighted average is taken of all knowledge tokens' values based on query-key similarity, similarly to a soft retrieval process. The averaged value embedding is then added to the hidden state, augmenting next-token generation with information from the KB. However, KBLAM differs from RAG in that it does not rely on a separate retrieval module. Instead, everything is carried out by attention in an end-to-end fashion.\nMulti-modal language model Recent advances in Multi-modal Language Models (Liu et al., 2023; Gao et al., 2023; Liu et al., 2024a; Zhao et al., 2023; Wu et al., 2023, MLMs), demonstrate the possibility of incorporating data of other modalities (e.g. images, videos) into a pre-trained LLM through learned adapters and instruction tuning. With KBLAM, we can consider a KB as another modality in that a KB is presented to the LLM in the form of continuous vectors via an encoder, and an encoder-adapter is learned also through instruction tuning inspired by MLM literature.\nKey-value (KV) cache mechanism Given a context string, the KV cache mechanism (Pope et al., 2023) caches the key and value embeddings at each layer, also known as the prefill stage. With KV cache, the complexity of generating new tokens conditioned on the context decreases from quadratic in context length to linear. KBLAM's encoded knowledge tokens work in a way similar to KV cache, but we have the key and value acquired from an external encoder rather than through self-attention process. Additionally, if the context is modified, the standard KV cache requires re-computing the whole KV cache due to the causal attention mask, whereas in KBLAM only the corresponding knowledge token need be updated as the knowledge tokens do not attend to each other.\nAugmenting information into an LLM in a continuous form RAG and in-context learning incorporates external information into LLMs in the form of a discrete string, whereas KBLAM augments information into LLMs as fixed length continuous representations. Many works also consider first encoding external documents as continuous vectors, then concatenating the vectors as the input for the decoder model (Izacard & Grave, 2020; Izacard et al., 2023; Ye et al., 2023) or using cross-attention to inject these vectors into the decoder (Borgeaud et al., 2022). However, these works focus on training a new model from scratch, while KBLAM is applied to augment a pre-trained LLM with knowledge. Recent work (Yen et al., 2024) also considers augmenting a pre-trained LLM with encoded information, similar to our setting. However, their approach requires training an encoder from scratch, whereas KBLAM's only trains linear adapters after a pre-trained black-box encoder. In\n\u00b9We provide more extensive related work discussion in Appendix A"}, {"title": "3 BACKGROUND", "content": "Knowledge base in the form of triples In this paper, we assume that the external knowledge is represented as a structured knowledge base (KB) obtained from unstructured text using, in particular, a KB is composed of triples of format\n$\\{( \\langle name \\rangle_m; \\langle property \\rangle_m; \\langle value \\rangle_m )\\}_{m=1}^M$.\n(1)\nIn the rest of the paper, we will refer to a set of triples of the format Eq. (1) as a KB and each triple inside it as a knowledge triple. Examples of such KB is shown in Table. 1 and 2 in Appendix C. The KB construction process summarizes information from multiple documents and organizes it in a structured way. Importantly, the structured nature of the KB enables us to augment LLM with information from documents in an efficient way, which we discuss in the next section.\nIt is also worth noting that KB construction is not the focus of our paper: KBLAM works on KB generated from corpus using existing tools. In the paper, we mainly focus on two KBs: A synthetic KB generated via GPT, and a real KB constructed from the Enron email (Klimt & Yang, 2004) dataset. Details of the two KBs will be discussed in detail in later sections.\nSelf-attention layer A decoder-based transformer is mostly composed of multiple self-attention layers (Vaswani, 2017). Consider a transformer of L layers. Each layer has three projection heads: $W_q \\in \\mathbb{R}^{D \\times D}, W_k \\in \\mathbb{R}^{D \\times D}, W_v \\in \\mathbb{R}^{D \\times D}, l \\in \\{1, ..., L\\}$, where D denotes embedding dimension. Given a user prompt (e.g. a question) of N tokens, each attention layer takes a sequence of N token embeddings $x = [x_1, ..., x_n, ..., x_N] \\in \\mathbb{R}^{N \\times D}$ as input, then for the nth token embedding, it is first transformed by the projection heads into three vectors\n$q_n^l = W_q x_n$\n$k_n^l = W_k x_n$\n$v_n^l = W_v x_n$\n(2)\nwhere we refer to $q_n^l, k_n^l$, and $v_n^l$ as the nth token's query, key, and value embedding at the lth layer respectively. Note that in practice, each attention layer often has multiple sets of attention heads but here we assume only a single head for notation simplicity.\nThe output of the attention layer, $y' = [y_1, ..., y_n, ..., y_N]^\\top \\in \\mathbb{R}^{N \\times D}$, is computed as\n$y_n^l = \\sum_{i=1}^N \\frac{exp(w_{n,i})}{\\sum_{i'=1}^N exp(w_{n,i'})} v_i^l$,\nwhere $w_{n,i} = \\langle q_n^l, k_i^l \\rangle / \\sqrt{D}$\n(3)\nwhere $\\langle , \\rangle$ denotes the inner product of two vectors. After $y_n^l$ is acquired, it is often fed into a feedforward network (FFN) with output dimension of D for further transformation.\nA standard implementation of self-attention would have a time complexity of $O(N^2D)$ and memory complexity of $O(N^2)$ for computing and storing all $w_{n,i}$. Additionally, the FFN would also introduce a significant computation overhead of magnitude $O(ND^2)$. Due to these factors, as the sequence gets longer, self-attention faces high memory consumption and slow computation."}, {"title": "4 AUGMENTING LLM WITH THE KB", "content": "In this section, we discuss how KBLAM augments an LLM with a KB. The process has two steps (visualized in Fig. 2): 1. We convert each triple in the KB from the string form into a continuous key-value vector pair, referred to as a knowledge token, through a pre-trained sentence encoder followed by linear adapters; 2. All knowledge tokens are then injected into each attention layer of an LLM through a rectangular attention structure.\nKnowledge tokens Provided a KB in the format of Eq. (1), for each triple, we first adopt a pre-trained sentence encoder model, denoted as f(\u00b7) which converts a string into a P-dimension continuous embedding vector. Through the encoder, we convert each triple into a base key embedding and a base value embedding. In particular, for the mth triple, we have:\n$k_m = f(The \\langle property \\rangle_m of \\langle name \\rangle_m) \\in \\mathbb{R}^P, v_m = f(\\langle value \\rangle_m) \\in \\mathbb{R}^P$.\n(4)\nNext, we introduce a linear key and a linear value adapter\n$W_K \\in \\mathbb{R}^{L \\times D \\times P}$,\n$\\tilde{W}_V \\in \\mathbb{R}^{L \\times D \\times P}$,\n,\n(5)\nwhere L denotes the number of attention layers in a model. With the adapters, we map $k_m$ and $v_m$ from the sentence encoder's space to LLM's key and value embedding space at each attention layer. In particular, for the mth knowledge triple, we transform its base key and value embedding into\n$\\tilde{k}_m = [\\tilde{k}_m^1, ..., \\tilde{k}_m^l, ..., \\tilde{k}_m^L] = W_K k_m \\in \\mathbb{R}^{L \\times D}$,\n$\\tilde{v}_m = [\\tilde{v}_m^1, ..., \\tilde{v}_m^l, ..., \\tilde{v}_m^L] = \\tilde{W}_V v_m \\in \\mathbb{R}^{L \\times D}$.\n(6)\n(7)\nSince each $\\tilde{k}_m$ and $\\tilde{v}_m$ share the same size with LLM's key and value embedding (i.e. $k_n^l$ and $v_n^l$ in Eq. (2)), they can be directly incorporated into attention's computation. Therefore, the encoding process is equivalent to converting a knowledge triple from a string of multiple tokens into a special token, whose key and value embeddings at each layer are generated through an encoder, rather than through self-attention. As such we refer to $(\\tilde{k}_m, \\tilde{v}_m)$ pair as a knowledge token.\nWe visualize the process in Fig. 2a. This encoding process is applied to all triples in the KB, which transforms the information from a KB into a collection of knowledge tokens\n$\\{( \\langle name \\rangle_m, \\langle property \\rangle_m, \\langle value \\rangle_m )\\}_{m=1}^M \\xrightarrow[Encode]{} \\{ (\\tilde{k}_m, \\tilde{v}_m ) \\}_{m=1}^M$\n(8)\nRectangular Attention: Injecting knowledge token into prompt tokens After converting a KB into a collection of knowledge tokens we inject the information of the KB into the each attention layer of a pre-trained LLM through a modified attention structure\u00b2. In particular, consider an\n\u00b2We use symbols with a tilde to denote additional attention components introduced by KBLAM."}, {"title": "5 KB INSTRUCTION TUNING", "content": "The learnable parameters in KBLAM comprise the weights for the weights for the linear adapters/-heads $\\theta = \\{ W_K, \\tilde{W}_V, \\{ \\tilde{W}^l_Q \\}_{l=1}^L \\}$, Inspired by recent works in multi-modal language models (Liu et al., 2023), we use instruction tuning for parameter learning. Specifically, given a KB, we generate question-answer pairs about the KB (denoted as Q and A respectively) using formatted strings or GPT, then we optimize \u03b8 using:\n$max_\\theta log p_\\theta(A | Q, KB)$,\n(12)\nwhere $\\phi$ denotes parameters of the pre-trained LLM, encompassing QKV heads, FFN parameters and embedding/outpu layer weights. Notably, KB instruction tuning preserves the base LLM's reasoning abilities by avoiding fine-tuning the pre-trained LLM itself. Moreover, learning only the linear adapters minimizes the risk of memorizing training data, a known issue in LLM fine-tuning (Zeng et al., 2023; Mireshghallah et al., 2022). To further mitigate this risk, we conducted instruction tuning using a fully synthetic KB generated by GPT. This approach is motivated by the understanding that the instruction tuning process aims not to memorize specific information, but to learn a projection from the pre-trained sentence encoder space to the LLM's semantic space. Consequently, the exact content of the KB is less critical, provided the training data encompasses a diverse range of text.\nGeneration of synthetic KB To synthesize a KB, we first use GPT to generate 50 <name>s based on combinations of 30 object types (e.g. restaurant name, software tool) and 30 idea types (e.g. natural phenomena, famous landmarks). Then for each name, we prompt GPT to generate the <value> of three <property>s: \u201cdescription\u201d, \u201cobjectives\u201d, and \u201cpurpose\u201d for each name, in the same conversation context. Crucially, we instruct GPT to generate <value>s uncorrelated with names, ensuring the information comes from the KB rather than the LLM's predictive ability. This yields 45K names and a KB of approximately 135K triples. We provide example triples in Table. 1 in Appendix. C, the full list of types and prompts in Appendix. E.1 and release the resulting synthetic KB dataset for research purposes.\nGeneration of instructions The construction of the instruction tuning dataset is crucial, as it reflects our intended use of the KB and the desired behavior of KBLAM under various scenarios. We considered the following types of instructions (examples shown in Fig. 10 in Appendix. D):\n\u2022 Simple Q&A about a single <name> and <property> with <value> as the answer.\n\u2022 Multi-entities Q&A involving multiple <name>s and <property>s, with their corresponding <value>s as answers.\n\u2022 Open-ended Q&A similar to simple and multi-entities Q&A, but including an additional open-ended question component that requires open-ended reasoning about <value>. Reference answers for these questions are generated by GPT. The exact prompt is detailed in Appendix. E.2.\n\u2022 Unanswerable questions irrelevant to any <name> or <property> in the KB. For such cases, the standard response is \"Sorry, I cannot find relevant information in the KB.\u201d\nFor simple, multi-entities, and unanswerable questions, both questions and answers are generated using formatted strings, with templates of various styles such as \"Can you inform me...\" or \"What is the...\". A comprehensive list of these templates is provided in Appendix. E.5."}, {"title": "6 EXPERIMENTS", "content": "In this section, we perform empirical evaluation for KBLAM. We begin by showing that KBLAM's attention matrix, after instruction tuning, shows interpretable patterns and works as an accurate retriever. We then show that KBLAM can answer questions with performance comparable to in-context learning, but with much lower memory cost, and can scale to 10K triples with little performance degradation. Lastly, we show that for questions with no answer in the KB, the model can refuse to answer questions, with \u201cover-refusal\" behavior occurring later than in-context learning. Lastly, we conduct ablation studies to understand the design choices of KBLAM."}, {"title": "6.1 EXPERIMENT SETTING", "content": "Model specification For all experiments, we use the instruction fine-tuned version of Llama3 8B (Dubey et al., 2024) as the backbone LLM, and OpenAI's ada-002 sentence embedding model (P = 1536) as the pre-trained encoder for computing base key and value embedding (Eq. (5))."}, {"title": "6.2 EXPERIMENT RESULTS", "content": "\u00b3All KBs used for training and evaluation are released together with the paper."}, {"title": "A EXTENDED RELATED WORK", "content": "In this section, we present additional discussion on work related to KBLAM.\nMemory augmented language models Some other works have also incorporated external memory, i.e. a collection of fixed length latent representations, into language models (Grave et al., 2016; Dai et al., 2019; Khandelwal et al., 2020; Wu et al., 2022). KBLAM's knowledge tokens can also be seen as external memory generated from the KB. The most similar approach along this line of work is Wu et al. (2022), which constructs external memory as key-value vector pairs, similar to KBLAM. However, they require training a transformer from scratch and the memory key-value pairs come from training tokens seen in the past, whereas KBLAM's memory is from an external KB and is augmented into a pre-trained model at inference time.\nToken compression KBLAM's encoder transforms each triple into a continuous vector pair equivalent to 1 token in length, which essentially compresses information from the multiple-token string into one single token. There have been some recent works aiming at compressing a long context into more continuous representations. Ge et al. (2023) replaces short prompts with special tokens to instruct the LLM to do some particular tasks, however they require fine-tuning of the LLM. Mu et al. (2024) encode a long context into memory slots through a fine-tuned LLM, which is a more powerful but also costly compressor, which we believe could in future work be combined with KBLAM'S framework as an alternative to pre-trained sentence encoder.\nStructured attention KBLAM also shares similarity with works that use structured attention masks instead of the standard lower-triangular causal attention mask (Ratner et al., 2022; Cai et al., 2023; Merth et al., 2024). These works utilize independence assumptions in the context, e.g. between different randomly-chosen sets of few-shot examples (Ratner et al., 2022) or documents from different sources (Cai et al., 2023; Merth et al., 2024), and let independent contexts not attend over each other, which reduces overhead of attention operations. In KBLAM, we make a similar independence assumption: We assume independence between different KB triples, which provides KBLAM with scalability and interpretability."}, {"title": "B ABLATION STUDY", "content": "We conducted a number of ablation studies on the design choices of KBLAM.\nChoice of encoders KBLAM relies on a pre-trained sentence encoder, and the capacity of the encoder can affect the performance. Indeed our results (Fig. 7) verify this: We experimented with some open-sourced alternatives from sentence transformer (Reimers & Gurevych, 2019). Broadly we observe that the commercial OpenAI embedding works better than the open-sourced one, models with higher embedding dimensions perform better than those with lower dimensions.\nWhere to add knowledge tokens In Fig. 4 and 5, we noted that the attention score from the 15th layer exhibits accurate retrieval behavior. Extending this analysis to other layers (Fig. 9), we find that they do not demonstrate such straightforward accuracy (although further probing might reveal more nuanced patterns). We suspect that this indicates that the amount of KB information provided by the knowledge tokens vary at different layers provide. To better understand this, we examining the variation of keys and values at each layer (Vm[km] and Vm[m]) across triples, we observe that encoder outputs vary minimally in earlier layers.\nFrequency of knowledge tokens The previous observation indicates that the knowledge tokens may not be providing information about the KB at every layer, therefore we consider varying the frequency of adding knowledge tokens (denoted by K). That is, for certain attention layers of KBLAM, we used Eq. (3) instead of Eq. (9). In particular, we considered only adding knowledge every 1, 3 and 10 layers and the results are presented in Fig. 8. Broadly, we notice with lower frequency, the model tends not to follow instructions: When K = 10, KBLAM fails to provide any refusal answer. As such we suspect that knowledge tokens in earlier layers may serve as soft instruction prompts that guides the LLM how to use the KB."}, {"title": "C SAMPLE KB", "content": "Here we present some example triples from both KBs, where each triple is of format\n(<name>;<property>;<value>)\nWe present ten examples from the synthetic KB (Table. 1) and from Enron (Table. 2) respectively."}, {"title": "D SAMPLE Q&A", "content": "An example of four types of questions used for instruction tuning is presented in Fig. 10."}, {"title": "E PROMPT", "content": "In this section, we provide all prompts used for KBLAM's experiments."}, {"title": "E.1 PROMPT FOR SYNTHETIC KB GENERATION", "content": "To prompt the GPT-4 for generating a synthetic KB, we begin by setting a system prompt as\nYou are a AI system that generates synthetic data examples in JSON format\nThen we construct a list of idea types and a list of object types:\nidea_types = [\n'greek letters', 'fiction characters', 'famous rock bands',\n'birds', 'animals', 'natural phenomena', 'physical locations',\n'artist names', 'classical music', 'musical instruments',\n'music genres', 'art styles', 'ancient Roman concepts',\n'Hindu myths', 'Cthulhu Mythos', 'real-world company names',\n'mythological creatures', 'planets and stars', 'historical figures',\n'literary genres', 'botanical names', 'famous landmarks',\n'scientific concepts', 'space missions' 'inventions',\n'philosophical terms', 'chemical elements', 'famous scientists',\n'marine life', 'mythological places'\n]\nobject_types = [\n'education company', 'tech company', 'car company',\n'entertainment company', 'construction company', 'retail company',\n'finance company', 'healthcare company', 'restaurant', 'hotel',\n'github repo', 'project', 'meeting room', 'building', 'lab',\n'airline', 'textbook', 'website', 'personal blog',\n'gaming company', 'consulting firm', 'biotech company', 'app',\n'software tool', 'bookstore', 'e-commerce site',\n'social media platform', 'fitness brand', 'fashion brand',\n'non-profit organization'\n]\nThen for each combination of object type and idea type in the list, we prompt GPT with"}, {"title": "E.2 PROMPT FOR OPEN-ENDED Q&A GENERATION", "content": "To generate open-ended Q&A instructions, we first take a triple from the KB, construct a simple Q&A from it, and then we feed the simple Q&A into GPT and let GPT augment it into a more complex form via the following prompt\nYou are given a question and answer pair, please extend the question to\nbe open-ended and generate a short answer. For example, you could\ngenerate\nWhat is the objective of xxx and what do you think of it?\nMake sure the answer is **only** based on information provided from the\nQA pair. In addition, please generate in the format of:\nQ:\nA:"}, {"title": "E.3 PROMPT FOR GPT EVALUATION OF OPEN-ENDED Q&A", "content": "To evaluate the answer quality of open-ended Q&A tasks, we again use GPT. In particular, we ask GPT to score the answer from two aspects: 1. How grounded the answer is given the question and the KB; 2. How reasonable the open-ended part of the answer is.\nWe begin with a system prompt of\nYou are an AI system that evaluates the quality of generated responses.\nYour goal is to return a score between 0 and 5 indicating how accurate\nand useful the response is. An accurate and useful response should get a\nhigh score of 5.\nThen, we use the following prompt, which encourages the model to give chain-of-thought reasoning for scoring (Zheng et al., 2023):\nA model is given a question about some information and evidence. The\nquestion is composed of two parts, a part that involves repeating\ninformation in the evidence and a part that potentially involves open-\nended thinking. Then the model generates a response. Evaluate the\nresponse based on how grounded it is given the evidence and how\nreasonable it is. Return an integer score and step by step explanation of\nhow you arrived at the score. Score of 5 means the response is accurate,\nrelevant and reasonable (in that it meets common sense)."}, {"title": "E.4 PROMPT FOR LLAMA EVALUATION", "content": "For in-context learning, we use the following prompts for evaluation\n# Simple Q&A\nPlease answer questions based on the given text with format: \"The {\nproperty} of {name} is {description}\"\n# Two-entity Q&A"}, {"title": "E.5 QUESTION TEMPLATE", "content": "For simple Q&A, we use the following templates\nWhat  does  have?,\nWhat is the  of ?,\nTell me about the  of . ,\nCan you let me know the  of ?,\nCan you inform me about the  of ?,\nDescribe the  of . ,\nWhat details can you share about the  of ?,\nWhat kind of  does  have?,\nProvide details on the  of . ,\nWhat features does the  of  include?,\nCan you elaborate on the  of ?,\nHow would you describe the  of ?,\nWhat can you tell me about the  characteristics of ?,\nCan you explain the  of ?,\nWhat insights can you provide about the  of ?,\nWhat should I know about the  of ?,\nFor multi-entities Q&A involving G triples, {(),}=1, we use the following template\nWhat is the {}\nTell me {},\nCan you let me know {},\nCan you inform me {},\nDescribe {},\nExplain {},\nCould you describe the {},\nWhat can you tell me about {}"}, {"title": "F SAMPLE OUTPUT", "content": "In this section, we present sample output from the model. In particular, we present generated answers from KBLAM, in-context learning, and zero-shot learning, on open-ended Q&A tasks for synthetic KB (Appendix. F.1) and Enron (Appendix. F.2), given questions about a \"Relevant triple\" from a KB of 100 triples.\nBroadly, we can see that zero-shot consistently produces hallucinations, as the contents of both the synthetic KB and Enron KB differ significantly from the real-world factual knowledge stored in the LLM's parameters.\nKBLAM and in-context learning, on the other hand, utilize the information from the KB to synthesize the answer. In-context learning provides highly precise answers to all questions, however it incurs substantial computational overhead and struggles to scale to larger KBs On synthetic data, KBLAM accurately captures the information from the KB in terms of meaning and sentence structure, though it may use slightly different wording, e.g. in the last example on \"Titdal surge\", KBLAM uses \"renewable\" instead of \"sustainable\" as in the original triple.\nWhen tested on the out-of-distribution Enron data, KBLAM still grasps the general meaning of the relevant triple, albeit with more noticeable differences. As discussed in the limitations section of the main text, we believe this performance gap can be narrowed by using a more diverse synthetic KB with a stronger real-world foundation during training."}, {"title": "F.1 SYNTHETIC KB", "content": "Relevant triple:\n(ReefPulse Jellyfish; objectives; to provide real-time alerts, support remote monitoring, and improve home safety)\nQuestion:\nCan you inform me about the objectives of ReefPulse Jellyfish and what do you think of its effectiveness in achieving these goals?\nKBLaM output:\nThe objectives of ReefPulse Jellyfish are to provide real-time monitoring , send alerts, and improve home safety. While the effectiveness of ReefPulse Jellyfish in achieving these goals would depend on various factors such as the accuracy of the monitoring technology, the reliability of the alert system, and the user experience, the outlined objectives suggest a comprehensive approach to enhancing home safety.\nIn-context learning output:\nAccording to the context, the objectives of ReefPulse Jellyfish are to provide real-time alerts, support remote monitoring, and improve home safety.\nBased on these objectives, I think ReefPulse Jellyfish is effective in achieving its goals. The real-time alerts feature allows users to stay informed and take prompt action in case of any issues, which can help prevent potential hazards and ensure a safer home environment. The remote monitoring feature enables users to keep track of their home's"}, {"title": "F.2 ENRON", "content": "Relevant triple:\n(Sitraka; description; a provider of Internet software technologies for IT organizations)\nQuestion:\nWhat kind of description does Sitraka have and what services do they offer?\nKBLaM output:\nThe description of Sitraka is a software company providing networking solutions for businesses. They offer services that likely include developing and implementing network infrastructure, managing network security, and possibly providing technical support to ensure smooth network operations for their clients.\nIn-context learning output:\nBased on the given text, Sitraka is described as a provider of Internet software technologies for IT organizations.\nAs for the services they offer, the text does not provide specific information. However, based on the general description, it can be inferred that Sitraka offers software technologies and solutions for IT organizations, possibly related to internet-based services, network management, or other IT-related areas.\nZero-shot learning output:\nAccording to my knowledge, Sitraka is a software company that provides IT service management solutions. They offer a range of services and products that help organizations manage and optimize their IT infrastructure, including:\n* IT Service Management (ITSM) software: Sitraka's flagship product is the ITSM software, which provides a comprehensive platform for managing IT services, including incident, problem, and change management.\n* Asset Management: Sitraka's asset management solution helps organizations track and manage their IT assets, including hardware, software, and network devices.\n* Service Desk: Sitraka's service desk solution provides a centralized platform for managing IT requests, incidents, and problems.\n* Reporting and Analytics: Sitraka's reporting and analytics solution provides real\nRelevant triple:\n(Unocal; objectives; secure OPIC funds for offshore oil and natural gas projects)\nQuestion:\nWhat features does the objectives of Unocal include and how do these features impact their operations?\nKBLaM output:\nThe objectives of Unocal include securing offshore oil and gas reserves. These features impact their operations by ensuring that they can access and extract these natural resources efficiently and safely, which is crucial for meeting energy demands and maintaining a stable supply chain.\nIn-context learning output:\nBased on the provided context, the objectives of Unocal are:"}]}