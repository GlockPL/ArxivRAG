{"title": "Sub-graph Based Diffusion Model for Link Prediction", "authors": ["Hang Li", "Wei Jin", "Geri Skenderi", "Harry Shomer", "Wenzhuo Tang", "Wenqi Fan", "Jiliang Tang"], "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) represent a contemporary class of generative models with exceptional qualities in both synthesis and maximizing the data likelihood. These models work by traversing a forward Markov Chain where data is perturbed, followed by a reverse process where a neural network learns to undo the perturbations and recover the original data. There have been increasing efforts exploring the applications of DDPMs in the graph domain. However, most of them have focused on the generative perspective. In this paper, we aim to build a novel generative model for link prediction. In particular, we treat link prediction between a pair of nodes as a conditional likelihood estimation of its enclosing sub-graph. With a dedicated design to decompose the likelihood estimation process via the Bayesian formula, we are able to separate the estimation of sub-graph structure and its node features. Such designs allow our model to simultaneously enjoy the advantages of inductive learning and the strong generalization capability. Remarkably, comprehensive experiments across various datasets validate that our proposed method presents numerous advantages: (1) transferability across datasets without retraining, (2) promising generalization on limited training data, and (3) robustness against graph adversarial attacks.", "sections": [{"title": "1 Introduction", "content": "Graphs are ubiquitous data structures, with applications that span from social networks [1-3] to cutting-edge scientific research [4\u20137]. Link prediction, as one of the most fundamental tasks on graphs, plays an indispensable role in various graph applications in web-related scientific researches such as e-commence recommendations [8, 9], social network analysis [10], and network security predictions [11]. With the recent rise of graph neural networks (GNNs), a variety of GNN-based methods have been developed, tremendously advancing the performance of link prediction [12-14].\nIn GNN-based link prediction, two main families of techniques have been proposed: discriminative methods [12, 13, 15] and generative methods [16, 17]. While discriminative methods are more popular, the utilization of traditional generative models (e.g., VGAEs [16]) remains rather limited. The exploration of recent generative approaches, e.g., diffusion models [18, 19], is even less preva- lent. Nevertheless, generative models are well-known for their advantages in generalization and robustness [20\u201322], particularly in scenarios with limited labeled data or under adversarial attacks, where they often outperform their discriminative counterparts. In fact, adopting generative models for discriminative tasks has recently gained increasing attention in various domains such as computer vision [23] and natural language processing [24]. For example, the GPT series models [25, 26] have"}, {"title": "2 Related Work", "content": "fully demonstrated the potential of such generative models by showcasing exceptional generaliza- tion abilities. This involves solving not only text generation tasks but also numerous classification problems [27, 28]. Meanwhile, diffusion models [18, 19] have exhibited remarkable competence in discriminative tasks in the image domain including image classification [29, 30] and image seg- mentation [31]. Given these developments, we are motivated to explore a generative approach for a fundamental discriminative graph problem, i.e., link prediction. Such efforts not only have a great potential to enhance the generalization and robustness of link prediction but also can inspire the application of generative models in other standard graph learning tasks such as node classification and graph classification.\nHowever, developing generative models for the link prediction problem faces unique challenges. First, the size of the graph adjacency matrix increases with the increase of node size. Thus, modeling the whole graph structure with the generative model in one-shot [16] leads to an excessive memory footprint for large graphs. One possible approach to circumvent this issue is to rely on autoregressive modeling [32]. However, due to its low efficiency and high variance [33], the autoregressive generation model has not been used for graph learning tasks. To tackle this challenge, we propose a sub-graph based diffusion framework SGDIFF, that take advantage of the recent success of sub-graph based GNNs [12]. By using only the sub-graph, the size of the sub-graph adjacency matrix is relatively much smaller than the whole graph. Second, we need to contend with the node features in addition to graph structure. Prior generative works [16] only use node features as inputs to reconstruct the graph structure. This design causes the generative model to loses the capability to transfer between different datasets, as nodes features of different datasets are usually incompatible with each other. This limits the generalization capabilities of such generative models, leaving this area under-explored. These challenges motivate us in the design of a new framework \u2013 SGDIFF. SGDIFF uses Bayesian theorem to decompose the generation of graph structure and node features into consequential steps, thereby helping achieve success in its cross dataset transfer capabilities."}, {"title": "2.1 Sub-graph Based Link Prediction", "content": "Due to the limitation of traditional Message Passing Neural Network (MPNN) in capturing the pairwise relations between two individual target nodes, vanilla GNNs often struggle with link prediction problems [34]. To solve this issue, manual feature enhanced models (MFEMs) like NBFNet [15], NCNC [14] and BUDDY [13] proposed a variety of methods trying to fuse the complementary structure information, e.g., heuristic features, with the message passing neural networks. On the other hand, sub-graph GNNs (SGNNs) like SEAL [12] and SUREL [35] transform link prediction into a binary sub-graph classification task and attempts to learn data-driven link prediction heuristics. Compared with fusion-based algorithms, SGNNs do not require complicated heuristic feature fusion designs and have better generous capability to different datasets. Besides, since they use sub-graphs as the sample unit, SGNNs are more flexible to inductive scenarios [36].\nNext we give a formal statement about SGNNs. For a pair of nodes u, v and its enclosing sub-graph Guv, SGNNs produce sub-graph representation Yu,v with GNNs and desired read-out functions R. With the classifier C, the sub-graph representation Yu,v is expected to classified as one if an edge (u, v) exists and zero otherwise. Commonly, node features are augmented with structural features to resolve the automorphic node problem [34]. The global heuristics can be well approximated from sub-graphs that are augmented with structural features with an approximation error that decays exponentially with the number of hops taken to construct the sub-graph [12]. By incorporating the idea of SGNNs, we aim to solve the memory print challenge for generative models on graph learning problems."}, {"title": "2.2 Likelihood Estimation of Diffusion Models", "content": "Diffusion models [18, 37] are a contemporary class of generative models. Through an iterative noising (forward) and denoising (reverse) Markov chain, diffusion models aim to learn the distribution of data in an explicit way [37]. Diffusion models enjoy the benefit of having a likelihood-based objective like VAEs [38] as well as high visual sample quality like GANs [39] even on high variability datasets. Recent advances in this area have also shown amazing results in text-to-image generation [40-42], audio synthesis [43, 44] and text-to-3d content creation [45, 46]. Despite being powerful generative"}, {"title": "3 Method", "content": "models, diffusion models has also been recently recognized as valid generative classifiers [47, 48]. As using the variational lower bound (VLB) of the log-likelihood as the object function, a well-trained diffusion models could provide accurate estimations to the probability of samples within the data distribution [30]. Furthermore, by incorporating class information as the condition input during the training, the diffusion model can be used to compute class-conditional likelihoods $p_c(x|y)$. Then, by selecting an appropriate prior distribution $p(y)$ and applying Bayes' theorem, predicted class probabilities $p_c(y|x)$ can be easily calculated. Compared to discriminative models, generative classifiers have been shown to generalize better, be more robust, and be better calibrated [21, 22]. In this work, we seek to develop diffusion models for solving discriminative graph problems.\nAlthough there are recent works on applying diffusion models for problems on graphs [49, 50], most of them focus on its generative perspective. The usage of the likelihood score of diffusion models to graph problems is relatively underexplored. Therefore, in this work, we take one of the most fundamental problems on graphs (i.e., link prediction) as an example to demonstrate the effectiveness of diffusion models with SGNNs for problems with graph data. Note that our algorithm can be easily extended to other graph problems like node or graph classification and we leave it as one future work. Next, we will first define notations and introduce an overview design of our algorithm SGDIFF. then, we present details about the link likelihood score estimation with the combination of structure and feature diffusion models."}, {"title": "3.1 Notations", "content": "In the following, we formally define the notations used in this work. Let $G = (V, E)$ be an undirected graph where $V$ and $E$ denote the sets of $n$ nodes (vertices) $V$ and $e$ links (edges), respectively. Let $S = (V_S \\subseteq V, E_S \\subseteq E)$ be a node-induced sub-graph of $G$ statisfying $(u, v) \\in E_S$ iff $(u, v) \\in E$ for any $u, v \\in V_S$. We use $S_{u,v}$  to denote a k-hop sub-graph enclosing the link $(u, v)$, where $V_{u,v}$ is the union of the k-hop neighbors of $u$ and $v$ and $E_{u,v}$ is the union of the links that can be reached by a k-hop walk originating at $u$ and $v$. The given features of nodes $V_{u,v}$ are represented by $X_{u,v}$ and the adjacency matrix of $S_{u,v}$ is $A_{u,v}$. The probability of link $(u, v)$ existing is indicated by $p(Y_{u,v} = 1)$ and our goal is to estimate $p(Y_{u,v} = 1|X_{u,v}, A_{u,v})$ with likelihood score generated by diffusion models. As following parts process each sub-graph with the same process, we will omit the subscripts u and v for convenience."}, {"title": "3.2 Design Overview", "content": "As mentioned in Section 2.2, by using a prior distribution $p(y)$, the categorical probability $p(y|x)$ can be estimated by applying Bayesian theorem over the class-conditional likelihood $p(x|y)$. However, unlike applying diffusion models to a single input like image or text, performing diffusion on graph data involves two different but correlated inputs, i.e., node feature ($X$) and adjacency matrix ($A$). Therefore, we need dedicated designs to decompose $p(y|A, X)$. To break the generalization limitation of existing generative approaches and take advantage of the inductive learning capability of SGNNs, we propose the following formulation:\n$p(y|S) = p(y|A, X) = \\frac{p(X|A, y)p(A|y)p(y)}{\\Sigma_{c\\in{0,1}}p(A, X|y = c) \\cdot p(y = c)}$\nwhere $p(y)$ is the prior distribution of node-pair's connection status over the graph. $p(A|y)$ denotes the graph structure probability given the condition of nodes u and v being connected. $p(X|A, y)$ represents the feature probability that is conditioned on the observed structure and connection status. An overview of our framework is shown in Fig. 1. By splitting the generation of graph structure and node features into consequential steps, SGDIFF can be used for various link prediction settings, with or without node features. More importantly, since it integrates the idea of SGNNs, the structure diffusion model of SGDIFF can easily be transferred across datasets without involving any re-training procedure. In our experiments (Section 4.2), we find that small datasets can benefit from the learnt knowledge of larger datasets. Meanwhile, since SGDIFF has the feature component, we can design an independent feature diffusion model for each dataset to model diverse node features. In other"}, {"title": "3.3 Structure Diffusion Model", "content": "words, the structure diffusion of SGDIFF provides a shareable basis for different datasets and the feature diffusion acts as an adjusting head which adapts the whole model to specific datasets. Lastly, generative models are well-known for their robustness against adversarial attacks [29, 51]. By modeling the likelihood scores of both the graph structure and node features, we expect that SGDIFF should be more robust against graph adversarial attacks as compared to existing discriminative approaches. We empirically verify this assumption in Section 4.4. Next, we detail the major components of SGDIFF.\nThe estimation of $p(A|y)$ with diffusion models involves the discrete input $A$. Following Di-Gress [49], we use discrete status transition noise [52] to maintain both the sparsity of the adjacency matrix as well as graph theoretic notions such as connectivity during the diffusion process. In addition to the adjacency matrix A, we further include the orbit features of each node $X'$ in the diffusion process. This is because we are estimating the likelihood score of a sub-graph S under the connection condition y of the sub-graph's center nodes u and v. The orbit features indicate the relative distance of each node toward the center nodes thereby better distinguishing the sub-graphs with similar adjacency matrix but different center node locations. There are many choices for the orbit features. We use the Double Radius Node Labeling (DRNL) [34] as we empirically find that it performs well on most of the datasets. To be concise, we define the forward of the structure diffusion model as:\n$q(S^{(t)}|S^{(t-1)}) = (A^{(t)}Q_A, X'^{(t)}Q_X)$,\n$q(S^{(t)}|S^{(0)}) = (A^{(0)}Q_A, X'^{(0)}Q_X)$,\nwhere $Q_A$ and $Q_X$ are the transition probability matrices at the t-th step for discrete edge and node features. $Q_A = \\Pi_{t=1}^T Q_A^{(t)}$ and $Q_X = \\Pi_{t=1}^T Q_X^{(t)}$. The backward process can be stated as:\n$p_\\theta(S^{(t-1)}|S^{(t)}, y) = (A^{(t)}(Q_A^t)' \\odot \\varphi_\\theta(A^{(t)}, y, t)Q^{-1},\\\\X^{(t)}(Q_X^t)' \\odot \\varphi_\\theta(X^{(t)}, y, t)Q^{-1})$\nwhere $\\odot$ denotes a element-wise product and $(Q_A^t)'$ and $(Q_X^t)'$ are the transpose of $Q_A$ and $Q_X$, respectively. $\\varphi_\\theta$ is the denoising diffusion model, which takes timestep t, t-th step noisy sample $S'^{(t)}$ and connection condition y as inputs. It further outputs the distribution of categorical features in the clean graph $S^{(0)}$. We use a transformer-based neural network for $\\varphi_\\theta$ and train it following prior work [49]. The conditional information y is concatenated to every node and edge feature during the pre-processing step.\nThe likelihood score of sub-graph S can then be estimated by applying the evidence lower bound (ELBO) to the integration result of the joint probability $p_\\theta(S^{(0:T)}) = (\\Pi_{t=0}^{T}p_\\theta (S^{(t-1)}|S'^{(t)}))$."}, {"title": "3.4 Node Diffusion Model", "content": "Since node features are typically continuous variables, we estimate $p_\\theta(X|A, y)$ using Gaussian noise as its diffusion kernel in a similar manner to DDPM [18]. Similar to Eq. (2), the forward process of feature diffusion model can be written as:\n$q(X^{(t)}|X^{(t-1)}) = N(X^{(t)}; \\sqrt{1-\\beta_t}X^{(t-1)}, \\beta_tI)$,\n$q(X^{(t)}|X^{(0)}) = N(X^{(t)}; \\sqrt{1-\\overline{\\alpha}_t}X^{(0)}, \\overline{\\alpha}_tI)$,\nwhere $\\beta_t$ is the variance schedule, which transitions from 0 to 1, and $\\overline{\\alpha}_t = \\Pi_{t=1}^T(1-\\beta_t)$. The reverse process under condition c is defined as:\n$p_\\theta(X^{(t-1)}|X^{(t)}, c) = N(X^{(t-1)}; \\mu_t, \\beta_t)$,\n$\\mu_t = (X^{(t)}, t, c), \\beta_t = \\frac{1-\\alpha_{t-1}}{1-\\overline{\\alpha}_t}\\beta_t$,\nwhere $\\alpha_t = 1-\\beta_t$. $\\epsilon_\\theta(X^{(t)}, t, y)$ is our denoising diffusion models and $\\beta_t$ is only correlated with the $\\beta_t$. Through applying the ELBO trick over the integral of joint distribution $q(X^{(0:T)}|y)$, we can write the conditioned log-likelihood score of node features as:\n$log p_\\theta (X|c) \\geq E_q [log \\frac{p_\\theta(X^{(0:T)}, c)}{log q(X^{(1:T)} | X^{(0)})}]\\\\= E_q[DKL(q(X^{(T)}|X^{(0)})||p_\\theta(X^{(T)})) - log p_\\theta(X^{(0)} |X^{(1)}, c)\\\\+\\Sigma_{t=2}^TD_{KL}(q(X^{(t-1)}|X^{(t)}, X^{(0)})||p_\\theta(X^{(t-1)}|X^{(t)}, c))]$,\nwhere the prior and reconstruction losses are nullified as their value is much smaller than the diffusion loss. To calculate $D_{KL}(q|p_\\theta)$, we use the simplified form proposed by Ho et al. [18] producing the final expression:\n$-E_{t,\\epsilon} [||\\epsilon - \\epsilon_\\theta (X^{(t)}, c)||^2]$ with $X^{(t)} = \\sqrt{\\overline{\\alpha}_t}X^{(0)} + \\sqrt{1-\\overline{\\alpha}_t}\\epsilon$,\nwhere $\\epsilon \\sim N(0, 1)$. The denoising model $\\epsilon_\\theta$ takes as input $x_0$, the noisy samples at step t, and the given condition $c = (A, y)$ and outputs the noise at step t. Since the condition c includes both the adjacency matrix A and the connection condition y, the predictions are made at node-level. Lastly, we use GCN [53] to model $\\epsilon_\\theta$:\n$\\epsilon_\\theta(X^{(t)}, y, t) = \\tilde{A}(\\sigma(\\tilde{A}X^{(t)}W_0))W_1$,\nwhere $\\sigma$ is an activation function, and $W_0$ and $W_1$ are the learnable parameters. $\\tilde{A} = D^{-\\frac{1}{2}}\\tilde{A}D^{-\\frac{1}{2}}$ with $A = A + I_N$ and $D_{ii} = \\sum_j\\tilde{A}_{ij}$. The connection status condition y is concatenated to each node feature during the feature pre-processing."}, {"title": "3.5 Connection Probability Estimation", "content": "With the estimated log-likelihood scores of the sample's graph structure $log p(A|y)$ and node features $log p(X|A, y)$, we can estimate the connection probability $P(y|A, X)$ via Eq. (1). However, directly taking the summation over those two components will be sub-optimal, as the scale of values returned by the two diffusion models are different. Furthermore, the weighting values of the diffusion loss are neglected during the loss calculation for simplification purposes. Because of this, we use the additional learnable parameter set {$\\eta_1, \\eta_2, \\delta$} to flexibly adjust each component during the fusion. The final connection probability calculation can be written as:\n$P(y|A, X) = softmax_y(log P(A, X|y) + log P(y))$,\n$log P(A, X|y) = \\eta_1 \\cdot log P(X|A, y) + \\eta_2 \\cdot log P(A|y) + \\delta$,\nwhere {$\\eta_1, \\eta_2, \\delta$} are optimized via gradient descent over the cross entropy loss between true links and the predicted connection probability $P(y|A, X)$. Please check Appendix A.1 for more details."}, {"title": "4 Experiments", "content": "In this section we conduct comprehensive experiments to validate the advantages of the proposed framework SGDIFF. In particular, we aim to answer the following questions: RQ1: Does SGDIFF enjoy the advantages of both SGNNs and generative models in solving cross-dataset link prediction? RQ2: How is the generalization capability of SGDIFF when faced with the challenge of train size limitation? RQ3: Does SGDIFF show its strength in robustness against the adversarial attacks on graph structure? Before presenting our experimental results and observations, we first introduce our general experimental settings."}, {"title": "4.1 General Experimental Settings", "content": "To demonstrate the effectiveness of SGDIFF, we choose seven representative link prediction al- gorithms as our baselines. Specifically, our baselines include GCN [53], GAT [54], SAGE [55], NeoGNN [56], VGAE [16], and SEAL [12]. To be noticed, we select VGAE because it is a represen- tative generative model for graph learning. And we collect SEAL and NeoGNNs since both of them are the effective link prediction models sharing the similar sub-graph learning ideas with SGDIFF. For the other baseline methods, we collect them following the prior studies on link prediction tasks [57]. More details about implementations of the baseline and SGDIFF can be found in Appendix A.3. We conduct experiments on six real-world graph datasets, including 3 citation networks: Cora, Citeseer and Pubmed [58] and 3 miscellaneous networks: USAir, NS and Router. The details about each dataset are shown in Table 3. Following prior works [12, 16], we split the existing links in each graph into train/valid/test with the percentages 80%/5%/15%. For evaluation, we randomly sample the same amount of unconnected node pairs as the negative samples. The evaluation metrics used in our experiment are AUC, Average Precision(AP) and Hit@100. All experiments are run over 10 seeds and we report both the mean values of each metric."}, {"title": "4.2 Performance on Cross-data Transferability", "content": "In this section, we aim to answer the first question about the cross-data transferablity of SGDIFF. As discussed in Section 3.2, one potential advantage of the structure diffusion model of SGDIFF is the potential to be transferred across datasets without re-training. To validate this advantage, we perform a zero-shot cross dataset transferring experiment, where the model is trained with a source dataset and is tested on other target datasets. As the node features among different datasets are incompatible with each other, we do not add node features for SGDIFF and SEAL. For VGAE, as the training and test graphs have different node numbers, we do not use node-id as input features for VGAE. Instead, we follow prior work [59], which randomly projects the node features into the same dimension and then performs row normalization. For graphs without node features, we draw random vectors from the Gaussian distribution and use it as node features. We test the transferability by setting each of the six graph datasets as the source for training and test the train model overall all six graphs. We report the perforamnce of each model from two perspectives, Source and Target. To be specific, Source averages the test performance on different target datasets of one model trained with one fixed source dataset. And Target averages the test performances on one fixed target dataset of six models trained"}, {"title": "4.3 Performance with Train Size Constraint", "content": "In this subsection, we further explore the generalization capability of SGDIFF and answer the second question by applying low availability limitations on the size of training set. In this setting, we shrink the training sample size of each dataset to only 1%. To make the result comparable with the other experiments, only the size of the training data is decreased while keeping the validation and test sets. Furthermore, we use random sampling to create the smaller training sets. To be noticed, as we intend to explore the performance change caused by decreasing the training sample size, but not the completeness of the graph, we do not mask the remaining 99% training edges from the original graph during the enclosing sub-graph generation process. We only control the number of sub-graphs used for training SEAL and SGDIFF. And for VGAE, we use the same adjacency matrix as other experiments but we mask 99% of the cross-entropy loss over the adjacency matrix during the back propagation. The performance of SGDIFF and baseline models are shown in Figure 2. We observe that as we limit the size of the training data, SGDIFF suffers less performance degradation compared"}, {"title": "4.4 Performance in Terms of Robustness", "content": "In this section, we answer the third question by demonstrating the robustness of SGDIFF. To empirically test this, we adopt three common adversarial attack baselines for link predictions, i.e., random flipping (RF), Embedding Attack (EA) [60] and DICE [61]. To be noticed, as most adversarial attack are proposed for graphs with node features, we conduct the following experiments with three citation networks: Cora, Citeseer and Pubmed. For DICE, as it required node label information as"}, {"title": "5 Conclusion", "content": "In this paper, we aim to adopt the diffusion model to the link prediction problem. With extensive experiments over the model's generalization, robustness and cross-data transfer capability, we successfully demonstrate the advantages of applying generative models toward graph learning tasks. Additionally, through the findings on the exchangeable structure components over datasets, we show the potential of our proposed framework to be an unified pre-training framework for link prediction in the future."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Algorithm Pseudo Code", "content": "The entire process of SGDIFF is shown in Algorithm 1. We estimate the likelihood scores for the structure and features simultaneously from lines 2 to 9 and 10 to 16, respectively. The two components are then fused together on line 17. Lastly, the sample's final connection probability is estimated on line 18."}, {"title": "A.2 Dataset Details", "content": "The details about each dataset are shown in Tabel 3. Following prior works [12, 16], we split the existing links in each graph into train/valid/test with the percentages 80%/5%/15%. For evaluation, we randomly sample the same amount of unconnected node pairs as the negative samples. The evaluation metrics used in our experiment are AUC, Average Precision(AP) and Hit@100."}, {"title": "A.3 Implement Details", "content": "The implementation and hyper-parameter settings of the two baseline models follow prior works [12, 16]. The implementation of our structure diffusion model follows the prior work [49] and the feature diffusion model is implemented with a multi-layer GCNs. During the enclosing graph generation process, we incorporate the neighbor sampling trick [55] to avoid the graph size becoming extremely large when it encounters some hub nodes. To add DRNL into the structure diffusion process, we treat extracted structure labels as categorical variables and use the sum of node and feature cross-entropy loss to train the structure denoising model. We perform grid search over the hyper parameters of our score and feature diffusion models. The best parameter of each components for each dataset is shown in Table. 4."}, {"title": "A.4 Cross-dataset Performance Details", "content": "We explore the transferability of models by setting each of the six graph as the training dataset and testing the trained model on the other five and itself under the zero-shot scenario. Specicially, detailed performances of seven baseline models and SGDIFF on AUC, AP and Hit@100 are presented in Table. 5, Table. 6 and Table. 7. For each metric, we run experiment for 10 times and the mean value and standard deviation are reported in the format of mean \u00b1 std%."}]}