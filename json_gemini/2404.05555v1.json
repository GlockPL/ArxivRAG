{"title": "On the Convergence of Continual Learning with Adaptive Methods", "authors": ["Seungyub Han", "Yeongmo Kim", "Taehyun Cho", "Jungwoo Lee"], "abstract": "One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. However, the convergence of continual learning for each sequential task is less studied so far. In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks. We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients. The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks.", "sections": [{"title": "INTRODUCTION", "content": "Learning new tasks without forgetting previously learned tasks is a key aspect of artificial intelligence to be as versatile as humans. Unlike the conventional deep learning that observes tasks from an i.i.d. distribution, continual learning train sequentially a model on a non-stationary stream of data [Ring, 1995, Thrun, 1994]. The continual learning AI systems struggle with catastrophic forgetting when the data access of previously learned tasks is restricted [French and Chater, 2002]. Although novel continual learning methods successfully learn the non-stationary stream sequentially, studies on the theoretical convergence analysis of both previous tasks and a current task have not yet been addressed.\nIn this line of research, nonconvex stochastic optimization problems have been well studied on a single task to train deep neural networks and prove theoretical guarantees of good convergence.\nPrevious continual learning algorithms have introduced novel methods such as a replay memory to store and replay the previously learned examples [Lopez-Paz and Ranzato, 2017, Aljundi et al., 2019b, Chaudhry et al., 2019a], regularization methods that penalize neural networks [Kirkpatrick et al., 2017, Zenke et al., 2017], Bayesian methods that utilize the uncertainty of parameters or data points [Nguyen et al., 2018, Ebrahimi et al., 2020], and other recent approaches [Yoon et al., 2018, Lee et al., 2019]. The study of continual learning in Bayesian frameworks formulate a trained model for previous tasks parameter into an approximate posterior to learn a probabilistic model which have empirically good performance on entire tasks. However, Bayesian approaches can fail in practice and it can be hard to analyze the rigorous convergence due to the approximation. The memory-based methods are more straightforward approaches, where the learner stores a small subset of the data for previous tasks into a memory and utilizes the memory by replaying samples to keep a model staying in a feasible region without losing the performance on the previous tasks. Gradient episodic memory (GEM) [Lopez-Paz and Ranzato, 2017] first formulated the replay based continual learning as a constrained optimization problem. This formulation allows us to rephrase the constraints on objectives for previous tasks as inequalities based on the inner product of loss gradient vectors for previous tasks and a current task. However, the gradient update by GEM variants cannot guarantee both theoretical and empirical convergence of its constrained optimization problem. The modified gradient updates do not always satisfy the loss constraint theoretically, and we can also observe the forgetting phenomenon occurs empirically. It also implies that this intuitive reformulation violates the constrained optimization problem and cannot provide theoretical guarantee to prevent catastrophic forgetting without a rigorous convergence analysis."}, {"title": "RELATED WORK", "content": "In this work, we explain the cause of catastrophic forgetting by describing continual learning with a smooth nonconvex finite-sum optimization problem. In the standard single task case, SGD [Ghadimi and Lan, 2013], ADAM [Reddi et al., 2018], YOGI [Zaheer et al., 2018], SVRG [Reddi et al., 2016a], and SCSG [Lei et al., 2017] are the algorithms for solving nonconvex problems that arise in deep learning. To analyze the convergence of those algorithms, previous works study the following nonconvex finite-sum problem\n$\\min_{x \\in \\mathbb{R}^d} f(x) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x),$ (1)\nwhere we assume that each objective $f_i(x)$ with a model x and a data point index $i \\in [n]$ for a dataset with size n (by the convention for notations in nonconvex optimization literature [Reddi et al., 2016a]) is nonconvex with L-smoothness assumption. In general, we denote $f_i(x)$ as $f(x; d_i)$ where $d_i$ is a datapoint tuple (INPUT, OUTPUT) with index i. We expect that a stochastic gradient descent based algorithm reaches a stationary point instead of the global minimum in nonconvex optimization. Unlike the convex case, the convergence is generally measured by the expectation of the squared norm of a gradient $E||\\nabla f(x)||^2$. The theoretical computational complexity is derived from the $\\epsilon$-accurate solution, which is also known as a stationary point with $E||f(x)||^2 < \\epsilon$. The general nonconvex finite-sum problems assume that all data points can be sampled during training iterations. This fact is an obstacle to directly apply (1) for continual learning problem.\nWe provide a solution of the above issue by leveraging memory-based methods, which allow models to access a partial access to the dataset of previous tasks. In this setting, we can analyze nonconvex stochastic optimization problems on the convergence of previous tasks with limited access. Similar with adaptive methods for noncovex optimization, we apply adaptive step sizes during optimization to minimize forgetting with theoretical guarantee. Specifically, we make the following contributions:\n\u2022 We decompose the finite-sum problem of entire tasks into two summation terms for previous tasks and a current task, respectively. We theoretically show that small random subsets of previous tasks lead to analyzing the expected convergence rate of both tasks while learning a current task.\n\u2022 We study the convergence of gradient methods under a small memory where the backward transfer performance degrades, and propose a new formulation of continual learning problem with the forgetting term. We then show why catastrophic forgetting occurs theoretically and empirically.\n\u2022 Though memory-based methods mitigate forgetting, previous works does not fully exploit the gradient information of memory. We introduce a novel adaptive method and its extension which adjust step sizes between tasks at each step with theoretical ground, and demonstrate that both methods show remarkable performance on image classification tasks."}, {"title": "PRELIMINARIES", "content": "Memory-based methods. Early memory-based methods utilize memory by the distillation [Rebuffi et al., 2017, Li and Hoiem, 2017] or the optimization constraint [Lopez-Paz and Ranzato, 2017, Chaudhry et al., 2019a]. Especially, A-GEM [Chaudhry et al., 2019a] simplifies the approach for constraint violated update steps as the projected gradient on a reference gradient which ensures that the average memory loss over previous tasks does not increase. Recent works [Chaudhry et al., 2019b, 2020a, Riemer et al., 2018] have shown that updating the gradients on memory directly, which is called experience replay, is a light and prominent approach. We focus on convergence of continual learning, but the above methods focus on increasing the empirical performance without theoretical guarantee. Our analysis provides a legitimate theoretical convergence analysis under the standard smooth nonconvex finite-sum optimization problem setting. Further, [Knoblauch et al., 2020] shows the perfect memory for optimal continual learning is NP-hard by using set-theory, but the quantitative analysis of performance degradation is less studied.\nAdaptive step sizes in nonconvex setting. Adaptive step sizes under smooth nonconvex finite-sum optimization problem have been studied on general single task cases [Reddi et al., 2018, Zhang et al., 2020, Zaheer et al., 2018] recently. [Simsekli et al., 2019, Zhang et al., 2020, Simsekli et al., 2020] have revealed that there exists a heavy-tailed noise in some optimization problems for neural networks, such as attention models, and [Zhang et al., 2020] shows that adaptive methods are helpful to achieve the faster convergence under the heavy-tailed distribution where stochastic gradients are poorly concentrated around the mean. In this work, we treat the continual learning problem where stochastic gradients of previous tasks are considered as the out-of-distribution samples in regard to a current task, and develop adaptive methods which are well-performed in continual learning."}, {"title": "PRELIMINARIES", "content": "Suppose that we observe the learning procedure on a data stream of continual learning at some arbitrary observation point. Let us consider time step $t = 0$ as given observation point. We define the previous task $P$ for $t < 0$ as all visited data points and the current task $C$ for $t > 0$ as all data points which will face in the future. Then, $P$ and $C$ can be defined as the sets of data points in $P$ and $C$ at time step $t = 0$, respectively. Note that the above task description is based on not a sequence of multiple tasks, but two separate sets to analyze the convergence of each of $P$ and $C$ when starting to update the given batch at the current task $C$ at some arbitrary observation point. We consider a continual learning problem as a smooth nonconvex finite-sum optimization problem with two decomposed objectives\n$\\min_{x \\in \\mathbb{R}^d} h(x) = \\frac{1}{n_f + n_g} \\sum_{i \\in P \\cup C} h_i(x),$ (2)\nwhere $n_f$ and $n_g$ are the numbers of elements for $P$ and $C$, and $h(x)$ can be decomposed into as follows:\n$h(x) = \\frac{n_f}{n_f + n_g} f(x) + \\frac{n_g}{n_f + n_g} g(x).$\nFor clarity, we use $f(x) = h(x)|_P$ and $g(x) = h(x)|_C$ for the restriction of h to each dataset P and C, respectively. $f_i(x)$ and $g_j(x)$ also denotes the objective terms induced from data where each index is $i \\in P$ and $j \\in C$, respectively.\nSuppose that the replay memories $M_t$ for time step $t \\in [0, T]$ are random variables which are the subsets of $P \\cup C$ to cover prior memory-based approaches [Chaudhry et al., 2019b,a]. To formulate an algorithm for memory-based approaches, we define mini-batches $I_t$ which are sampled from a memory $M_t$ at step t. We now define the stochastic update of memory-based method\n$x_{t+1} = x_t - \\alpha_{H_t} \\nabla f_{I_t}(x^t) - \\beta_{H_t} \\nabla g_{J_t}(x^t),$ (3)\nwhere $I_t \\subset M_t$ and $J_t \\subset C$ denote the mini-batches from the replay memory and the current data stream, respectively. Here, $H_t$ is the union of $I_t$ and $J_t$. In addition, for a given set S, $\\nabla f_S(x^t)$, $\\nabla g_S(x^t)$ denote the loss gradient of a model $x_t$ with the mini-batch S at time step t. The adaptive step sizes (learning rates) of $\\nabla f_{I_t}(x^t)$ and $\\nabla g_{J_t}(x^t)$ are denoted by $\\alpha_{H_t}$ and $\\beta_{H_t}$, which are the functions of $H_t$.\nIt should be noted the mini-batch $I_t$ from $M_t$ might contain a datapoint $j \\in C$ for some cases, such as ER-Reservoir.\nThroughout the paper, we assume L-smoothness and the following statements.\nAssumption 3.1. $f_i$ is L-smooth that there exists a constant $L > 0$ such that for any $x, y \\in \\mathbb{R}^d$,\n$||\\nabla f_i(x) - \\nabla f_i(y)|| \\le L||x - y||$ (4\nwhere $|| \\cdot ||$ denotes the Euclidean norm. Then the following inequality directly holds that\n$\\frac{L}{2}||x - y||^2 \\le f_i(x) - f_i(y) - \\langle \\nabla f_i(y), x - y \\rangle$ (5)"}, {"title": "CONTINUAL LEARNING AS NONCONVEX OPTIMIZATION", "content": "We derive Equation 5 in Appendix C. Assumption 3.1 is a well-known and useful statement in nonconvex finite-sum optimization problem [Reddi et al., 2016a, 2018, Zhang et al., 2020, Zaheer et al., 2018], and also helps us to describe the convergence of continual learning. We also assume the supremum of loss gap between an initial point $x_0$ and a global optimum $x^*$ as $\\Delta_f$, and the upper bound on the variance of the stochastic gradients as $\\sigma_f$ in the following.\n$\\Delta_f = \\sup_{x_0} f(x_0) - f(x^*),$\n$\\sigma_f^2 = \\frac{1}{n_f} \\sup_X \\sum_{i=1}^{n_f}||f_i(x) - \\nabla f(x)||^2.$\nIt should be noted that $g_j(x)$, $\\nabla g_j(x)$, which denote the loss and the gradient for a current task, also satisfy all three above assumptions and the following statement.\nTo measure the efficiency of a stochastic gradient algorithm, we define the Incremental First-order Oracle (IFO) framework [Ghadimi and Lan, 2013]. IFO call is defined as a unit of computational cost by taking an index i which gets the pair $(f_i(x), f_i(x))$, and IFO complexity of an algorithm is defined as the summation of IFO calls during optimization. For example, a vanilla stochastic gradient descent (SGD) algorithm requires computational cost as much as the batch size $b_t$ at each step, and the IFO complexity is the sum of batch sizes $\\sum_{t=1}^{T} b_t$. Let $T(\\epsilon)$ be the minimum number of iterations to guarantee $\\epsilon$-accurate solutions. The average bound of IFO complexity is less than or equal to $\\sum_{t=1}^{T(\\epsilon)} b_t = O(1/\\epsilon^2)$ [Reddi et al., 2016a]."}, {"title": "MEMORY-BASED NONCONVEX CONTINUAL LEARNING", "content": "We first present a theoretical convergence analysis of memory-based continual learning in nonconvex setting. We aim to understand why catastrophic forgetting occurs in terms of the convergence rate, and reformulate the optimization problem of continual learning into a nonconvex setting with theoretical guarantee. For completeness we present all proofs in Appendix C.\n$\\frac{L}{2} ||x - y||^2."}, {"title": "MEMORY-BASED NONCONVEX CONTINUAL LEARNING", "content": "Unlike conventional smooth nonconvex finite-sum optimization problems where each mini-batch is i.i.d-sampled from the whole dataset $P \\cup C$, the replay memory based continual learning encounters a non-i.i.d stream of data C with access to a small sized memory $M_t$. Algorithm 1 provides the pseudocode for memory-based approach with the iterative update rule 3. Now, we can analyze the convergence on P and C during a learning procedure on an arbitrary data stream from two consecutive sets $P$ and $C$ for continual learning [Chaudhry et al., 2019a,b, 2020b].\nBy limited access to P, the expectation of gradient update $E_{I_t \\subset M_t} [\\nabla f_{I_t} (x^t)]$ in Equation 3 for $f(x)$ is a biased estimate of the gradient $\\nabla f(x^t)$. At the timestep t, we have\n$\\nabla f_{M_t}(x^t) = E_{I_t} [\\nabla f_{I_t}(x^t)|M_t] = E_{I_t} [\\nabla f(x^t) + e_t|M_t] = \\nabla f(x^t) + e_{M_t},$\nwhere $e_t$ and $e_{M_t}$ denote the error terms, $\\nabla f_{I_t}(x^t) - \\nabla f(x^t)$ and the expectation over $I_t$ given $M_t$, respectively. It should be noted that a given replay memory $M_t$ with small size at timestep t introduces an inevitable overfitting bias.\nFor example, there exist two popular memory schemes, episodic memory and ER-reservoir. The episodic memory $M_t = M_0$ for all t is uniformly sampled once from a random sequence of P, and ER-reservoir iteratively samples the replay memory $M_t$ by the selection rule $M_t \\subset M_{t-1} \\cup J_t$. Here, we denote the history of $M_t$ as $M_{[0:t]} = (M_0, \\ldots, M_t)$. To compute the expectation over all stochasticities of NCCL, we need to derive the expectation of $\\nabla f_{M_t} (x^t)$ over the randomness of $M_t$. We formalize the expectation over all learning trials with the selection randomness as follows."}, {"title": "THEORETICAL CONVERGENCE ANALYSIS", "content": "Lemma 4.1. If $M_0$ is uniformly sampled from P, then both episodic memory and ER-reservoir satisfies\n$E_{M_{[0:t]}} [f_{M_t}(x^t)] = f(x^t)$ and $E_{M_{[0:t]}} [e_{M_t}] = 0.$\nNote that taking expectation iteratively with respect to the history $M_{[0:t]}$ is needed to compute the expected value of gradients for $M_t$. Surprisingly, taking the expectation of overfitting error over memory selection gets zero. However, it does not imply $e_t = 0$ for each learning trial with some $M_{[0:t]}$.\nWe now propose two terms of interest in a gradient update of nonconvex continual learning (NCCL). We define the overfitting term $B_t$ and the catastrophic forgetting term $\\Gamma_t$ as follows:\n$B_t = (L \\alpha_{H_t} - \\alpha_{H_t}) \\langle \\nabla f(x^t), e_t \\rangle + \\beta_{H_t} \\langle \\nabla g_{J_t}(x^t), e_t \\rangle,$\n$\\Gamma_t = \\frac{\\beta_{H_t}}{2} ||\\nabla g_{J_t} (x^t)||^2 - \\beta_{H_t} (1 - \\alpha_{H_t} L) \\langle \\nabla f_{I_t}(x^t), \\nabla g_{J_t}(x^t) \\rangle.$\nThe amount of effect on convergence by a single update can be measured by using Equation 5 as follows:\n$f(x^{t+1}) \\le f(x^t) - \\langle \\nabla f(x^t), \\alpha_{H_t} \\nabla f_{I_t}(x^t) + \\beta_{H_t} \\nabla g_{J_t}(x^t) \\rangle + \\frac{L}{2} ||\\alpha_{H_t} \\nabla f_{I_t}(x^t) + \\beta_{H_t} \\nabla g_{J_t}(x^t)||^2$ (6)\nby letting $x \\leftarrow x^{t+1}$ and $y \\leftarrow x^t$. Note that the above inequality can be rewritten as\n$f(x^{t+1}) \\le f(x^t) - (\\alpha_{H_t} - \\frac{L \\alpha_{H_t}^2}{2}) ||\\nabla f(x^t)||^2 + \\frac{L}{2} \\Gamma_t + B_t$\nA NCCL algorithm update its model with two additional terms $B_t, \\Gamma_t$ compared to conventional SGD. An overfitting term $B_t$ and a catastrophic forgetting term $\\Gamma_t$ are obtained by grouping terms that contain $e_t$ and $\\nabla g_{J_t}(x^t)$, respectively. These two terms inevitably degrade the performance of NCCL with respect to time. It should be noted that $I_t$ has $(\\nabla f_{I_t}(x^t), \\nabla g_{J_t}(x^t))$, which is a key factor to determine interference and transfer [Riemer et al., 2018]. On the other hand, $B_t$ includes $e_t$, which is an error gradient between the batch from $M_t$ and the entire dataset P.\nSince taking the expectation over all stochasticities of NCCL implies the total expectation, we define the operator of total expectation with respect to $0 < t < T$ for ease of exposition as follows:\n$\\mathbb{E}_T = \\mathbb{E}_{M_{[0:t]}} [\\mathbb{E}_{I_t} [\\mathbb{E}_{J_t} [\\cdot |I_t]] |M_{[0:t]}] .$\nIn addition, we denote $\\mathbb{E}_{T-1} = \\mathbb{E}$. We first state the stepwise change of upper bound.\nLemma 4.2. Suppose that Assumption 3.1 holds and $0 < \\alpha_{H_t} < 1$. For $x_t$ updated by Algorithm 1, we have\n$\\mathbb{E}_t||\\nabla f(x^t)||^2 \\le \\mathbb{E}_t \\frac{f(x^t) - f(x^{t+1})}{\\alpha_{H_t}} + B_t + \\Gamma_t$\n$+ \\frac{L \\alpha_{H_t}}{2(1 - \\alpha_{H_t})} \\mathbb{E}_t ||e_t||^2$ (7)\nSurprisingly, we observe $\\mathbb{E}_t [B_t] = 0$ by Lemma 4.1. It should be also noted that the individual trial with a randomly given $M_0$ cannot cancel the effect of $B_t$. We discuss more details of overfitting to memory in Appendix E.\nWe now describe a convergence analysis of Algorithm 1. We telescope over training iterations for the current task, which leads to obtain the following theorem.\nTheorem 4.3. Let $\\alpha_{H_t} = \\alpha = \\frac{Lc}{\\sqrt{T}}$ for some $0 < c < \\frac{2}{LT}$ and $t \\in \\{0, \\ldots, T - 1\\}$. By Lemma 4.2, the iterates of NCCL satisfy"}, {"title": "THEORETICAL CONVERGENCE ANALYSIS", "content": "$\\min_t \\mathbb{E}||\\nabla f(x^t)||^2 \\le \\frac{A}{\\sqrt{T}} \\Delta_f + \\sum_{t=0}^{T-1} \\mathbb{E} [\\Gamma_t] + \\frac{Lc}{2} \\sigma^2,$ (8)\nwhere $A = 1/(1 - L\\alpha/2)$.\nWe also prove the convergence rate of a current task C with the gradient udpates from the replay-memory M in continual learining.\nLemma 4.4. Suppose that $I_t \\cap J_t = \\emptyset$, Taking expectation over $I_t \\subset M_t$ and $J_t \\subset C$, we have\n$\\min_t \\mathbb{E}||\\nabla h|_{M\\cup C}(x^*)||^2 \\le \\frac{2 \\Delta_{h|M \\cup C} L}{\\sqrt{T}} + \\frac{L c \\sigma_{h|M \\cup C}^2}{2}$ (9)\nwhere $\\Delta_{h|M \\cup C}$ and $\\sigma_{h|M \\cup C}$ is the version of loss gap and the variance for h on $M \\cup C$, respectively.\nThus, the convergence of a current task C is guaranteed, since its superset $M \\cup C$ is converged. Otherwise, the convergence rate might differ from the conventional SGD for C by the given $\\Delta_{h|M \\cup C}, \\sigma_{h|M \\cup C}$ at time 0, but the asymptotic convergence rate is still identical.\nOne key observation is that $\\mathbb{E}[\\Gamma_t]$ are cumulatively added on the upper bound of $\\mathbb{E}||\\nabla f(x)||^2$, which is a constant in conventional SGD. The loss gap $\\Delta_f$ and the variance of gradients $\\sigma_f$ are fixed values. In practice, tightening $\\sum_{t=0}^{T-1} \\mathbb{E}[\\Gamma_t]$ appears to be critical for the performance of NCCL. However, $\\sum_{t=0}^{T-1} \\mathbb{E}[\\Gamma_t]/\\sqrt{T}$ is not guaranteed to converge to 0. This fact gives rise to catastrophic forgetting in terms of a nondecreasing upper bound. We now show the key condition of the convergence of $\\sum_{t=0}^{T-1} \\mathbb{E}[\\Gamma_t]/\\sqrt{T}$.\nLemma 4.5. Let an upper bound $\\beta > \\beta_{H_t} > 0$. Consider two cases, $\\beta < \\alpha$ and $\\beta > \\alpha$ for $\\alpha$ in Theorem 4.3. We have the following bound\n$\\sum_{t=0}^{T-1} \\frac{\\mathbb{E}\\Gamma_t}{\\sqrt{T}} < O(1/T^{3/2} + 1/T)$ when $\\beta < \\alpha,$\n$\\sum_{t=0}^{T-1} \\frac{\\mathbb{E}\\Gamma_t}{\\sqrt{T}} < O(\\sqrt{T} + 1/\\sqrt{T}),$ when $\\beta > \\alpha.$\nWith the following theorem, we show that f(x) can converge even if we have limited access to P.\nTheorem 4.6. Let $\\beta_{H_t} < \\alpha = \\frac{L c}{\\sqrt{T}}$ for all t. Then we have the convergence rate\n$\\min_t \\mathbb{E}||\\nabla f(x^t)|| \\le O(\\frac{1}{\\sqrt{T}}).$ (10)\nOtherwise, f(x) is not guaranteed to converge when $\\beta > \\alpha$ and might diverge at the rate $O(\\sqrt{T})$.\nCorollary 4.7. For $\\beta_{H_t} < \\alpha = \\frac{L c}{\\sqrt{T}}$ for all t, the IFO complexity of Algorithm 1 to obtain an $\\epsilon$-accurate solution is:\nIFO calls = $O(1/\\epsilon^2)$. (11)\nWe build intuituions about the convergence condition of the previous tasks P in Theorem 4.6. As empirically shown in stable A-GEM and stable ER-Reservoir [Mirzadeh et al., 2020], the condition of $\\beta_{H_t} < \\alpha$ theoretically implies that decaying step size is a key solution to continual learning considering when we pick any arbitrary observation points.\nRemark 4.8. To prevent catastrophic forgetting, the step size of $g(x)$, $\\beta_{H_t}$ should be lower than the step size of f(x), $\\alpha_{H_t}$. It should also be noted that $\\mathbb{E}_{M_{[1:t]}} [B_t|M_0]$ is not always 0 for any $M_0$. This implies that, from time step 0, each trial with different given $M_0$ also has the non-zero cumulative sum $\\sum \\mathbb{E}_{M_{[1:T]}} [B_t|M_0]$, which occurs overestimating bias theoretically."}, {"title": "REFORMULATED PROBLEM OF CONTINUAL LEARNING", "content": "The convergence rate with respect to the marginalization on $M_0$ in Theorem 4.6 exactly match the usual nonconvex SGD rates. The selection rules for $M_0$ with various memory schemes are important to reduce the variance of convergence rate with having the mean convergence rate as Equation 9 among trials. This is why memory schemes matters in continual learning in terms of variance. Please see more details in Appendix E.\n4.3 REFORMULATED PROBLEM OF CONTINUAL LEARNING\nThe previous section showed the essential factors in continual learning to observe the theoretical convergence rate. The overfitting bias term $B_t$ has a strong dependence on the memory selection rule and can be computed exactly only if we can access the entire dataset P during learning on C. In terms of expectation, we have shown that the effect of $B_t$ is negligible. We also show that its empirical effect is less important than $\\Gamma_t$ in Figure 2. Then we focus on the performance degradation by the catastrophic forgetting term $\\Gamma_t$. For every trial, the worst-case convergence is dependent on $\\Delta_f + \\sum_{t=0}^{T-1} \\mathbb{E} [\\Gamma_t]$ by Theorem 4.3. To tighten the upper bound and keep the model to be converged, we should minimize the cumulative sum of $\\Gamma_t$. We now reformulate the continual learning problem 2 as follows.\n$\\min_{\\alpha_{H_t}, \\beta_{H_t}} \\sum_{t=0}^{T-1} \\mathbb{E} [\\Gamma_t]$\nsubject to $0 < \\beta_{H_t} < \\alpha_{H_t} < 2/L$ for all $t < T$ (12)\nIt is noted that the above reformulation presents a theoretically guaranteed continual learning framework for memory-based approaches in nonconvex setting and the constraint is to guarantee the convergence of both f(x) and g(x)."}, {"title": "ADAPTIVE METHODS FOR CONTINUAL LEARNING", "content": "As discussed in the above Section, we can solve a memory-based continual learning by minimizing $\\sum_{t=0}^{T-1} \\mathbb{E}[\\Gamma_t]$. Adaptive methods are variants of SGD, which automatically adjust the step size (learning rate) on a per-feature basis. In this section, we review A-GEM in terms of adaptive methods, and also propose a new algorithm (NCCL) for achieving adaptivity in continual learning. For brevity, we denote the inner product $\\langle \\nabla f_{I_t}(x^t), \\nabla g_{J_t}(x^t) \\rangle$ as $\\Lambda_{H_t}$.\n5.1 A-GEM\nA-GEM [Chaudhry et al., 2019a] propose a surrogate of $\\nabla g_{J_t} (x^t)$ as the following equation to avoid violating the constraint when the case of interference, $\\Lambda_{H_t} \\le 0$:\n$\\nabla \\tilde{g}_{J_t}(x^t) = \\nabla g_{J_t}(x^t) - \\frac{\\langle \\nabla g_{J_t}(x^t), \\nabla f_{I_t}(x^t) \\rangle}{\\|\\nabla f_{I_t}(x^t)\\|^2} \\nabla f_{I_t}(x^t)$ (55)\nLet $\\beta$ be the step size for g(x) when the constraint is not violated. Then we can interpret the surrogate as an adaptive learning rate $\\beta_{H_t}$, which is $\\alpha (1 - \\frac{\\langle \\nabla f_{I_t}(x^t), \\nabla g_{J_t}(x^t) \\rangle}{\\|\\nabla f_{I_t}(x^t)\\|^2})$ to cancel out the negative component of $\\nabla f_{I_t}(x^t)$ on $\\nabla g_{J_t}(x^t)$.\nFor the transfer case $\\Lambda_{H_t} > 0$, A-GEM use $\\beta_{H_t} = 0$. After applying the surrogate, $\\mathbb{E}[\\Gamma_t]$ is reduced as shown in Appendix D. It is noted that A-GEM theoretically violates the constraints of (12) to prevent catastrophic forgetting by letting $\\beta_{H_t} = 0$ and does not utilize the better transfer effect. Then, A-GEM is an adaptive method without theoretical guarantee."}, {"title": "NCCL", "content": "As discussed above, we note that $\\mathbb{E}[\\Gamma_t", "mathbb{E}[\\Gamma_t": "is monotonically increasing on $\\beta_{H_t} > 0$. Then, we instead adapt $\\alpha_{H_t}$ to reduce the value of $\\mathbb{E}[\\Gamma_t"}, {"mathbb{E}[\\Gamma_t": "can be obtained when the case of transfer, $\\Lambda_{H_t} > 0$ by differentiating on $\\beta_{H_t}$. Then the minimum $\\mathbb{E}[\\Gamma_t^*", "mathbb{E}[\\Gamma_t^*": "frac{(1 - \\alpha_{H_t} L) \\Lambda_{H_t}}{2L ||\\nabla g_{J_t} (x^t)||^2}.$\nTo satisfy the constraints of (12), we should update $f_{I_t}(x^t)$ with non-zero step size and $\\beta_{H_t} < \\alpha_{H_t}$ for all t. Then the proposed adaptive method for memory-based approaches is given by\n$\\alpha_{H_t} = \\frac{\\alpha (1 - \\delta \\frac{\\Lambda_{H_t}}{\\|\\nabla f_{I_t}(x^t)\\|^2})"}, {}]}