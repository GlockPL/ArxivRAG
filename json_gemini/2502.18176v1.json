{"title": "CLIPURE: PURIFICATION IN LATENT SPACE VIA CLIP FOR ADVERSARIALLY ROBUST ZERO-SHOT CLASSIFICATION", "authors": ["Mingkun Zhang", "Keping Bi", "Wei Chen", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "In this paper, we aim to build an adversarially robust zero-shot image classifier. We ground our work on CLIP, a vision-language pre-trained encoder model that can perform zero-shot classification by matching an image with text prompts \"a photo of a <class-name>.\". Purification is the path we choose since it does not require adversarial training on specific attack types and thus can cope with any foreseen attacks. We then formulate purification risk as the KL divergence between the joint distributions of the purification process of denoising the adversarial samples and the attack process of adding perturbations to benign samples, through bidirectional Stochastic Differential Equations (SDEs). The final derived results inspire us to explore purification in the multi-modal latent space of CLIP. We propose two variants for our CLIPure approach: CLIPure-Diff which models the likelihood of images' latent vectors with the DiffusionPrior module in DaLLE-2 (modeling the generation process of CLIP's latent vectors), and CLIPure-Cos which models the likelihood with the cosine similarity between the embeddings of an image and \"a photo of a.\". As far as we know, CLIPure is the first purification method in multi-modal latent space and CLIPure-Cos is the first purification method that is not based on generative models, which substantially improves defense efficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13 datasets that previous CLIP-based defense methods used for evaluating zero-shot classification robustness. Results show that CLIPure boosts the SOTA robustness by a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on ImageNet, and 108% relative improvements of average robustness on the 13 datasets over previous SOTA. The code is available at https://github.com/TMLResearchGroup-CAS/CLIPure.", "sections": [{"title": "1 INTRODUCTION", "content": "Image classifiers are usually trained in a supervised manner with training data and evaluated on the corresponding test data until recently several vision-language models have emerged as zero-shot classifiers (Li et al., 2023; Radford et al., 2021; Li et al., 2022). Among them, CLIP (Radford et al., 2021) is an example that is popular, effective, and efficient. CLIP performs zero-shot classification by forming text prompts \u201ca photo of a <class-name>.\" of all the candidate categories, and selecting the class with the highest similarity with the image embedding. Despite its efficacy, when facing adversarial attacks, its accuracy can drop to zero, similarly vulnerable to other neural classifiers.\nExisting methods to enhance adversarial robustness follow two primary paths: adversarial training and purification. Adversarial Training (AT) (Madry et al., 2017; Rebuffi et al., 2021; Wang et al., 2023) incorporates adversarial examples into model training to boost robustness. It often achieves"}, {"title": "2 RELATED WORK", "content": "Zero-Shot Image Classification. Unlike traditional models that are limited to predefined categories, vision-language models (VLMs) are trained on open-vocabulary data and align the embeddings of images and their captions into a common semantic space. This enables them to perform as zero-shot classifiers by matching the semantics of images to textual categories, offering superior generality and flexibility. CLIP (Radford et al., 2021), trained on extensive internet image-text pairs, achieves advanced results in zero-shot classification tasks. Additionally, other VLMs including Stable Diffusion (Rombach et al., 2022), Imagen (Saharia et al., 2022), and DaLLE-2 (Ramesh et al., 2022) also possess zero-shot classification capabilities (Li et al., 2023; Clark & Jaini, 2024).\nAdversarial Purification in Pixel Space. A prevalent paradigm of adversarial purification aims to maximize the log-likelihood of samples to remove perturbations in pixel space. Since purification has no assumption of the attack type, enabling it to defend against unseen attacks using pre-trained generative models such as PixelCNN (Song et al., 2017), GANs (Samangouei, 2018), VAEs (Li & Ji, 2020), Energy-based models (Hill et al., 2020; Yoon et al., 2021), and Diffusion Models (Nie et al., 2022; Ho et al., 2020; Chen et al., 2023). Owing to the capability of diffusion models, diffusion-based adversarial purification achieves state-of-the-art robustness among these techniques.\nCLIP-based Defense. While CLIP achieves impressive accuracy in zero-shot classification, it remains vulnerable to imperceptible perturbations (Fort, 2021; Mao et al., 2022). Adversarially training the CLIP model on ImageNet / Tiny-ImageNet (Schlarmann et al., 2024; Mao et al., 2022; Wang et al., 2024) enhances its robustness but undermines its zero-shot capabilities and struggles against unseen attacks. Choi et al. (2025) suggests smoothing techniques for certification. Li et al. (2024a) advocates using robust prompts for image classification, but the defensive effectiveness is limited. Additionally, other research focuses on the out-of-distribution (OOD) robustness of the CLIP model (Tu et al., 2024; Galindo & Faria), which is orthogonal to our adversarial defense objectives."}, {"title": "3 PRELIMINARY: CLIP AS A ZERO-SHOT CLASSIFIER", "content": "In this section, we introduce how CLIP is trained and how CLIP acts as a zero-shot classifier. CLIP (Contrastive Language Image Pre-training) (Radford et al., 2021), consists of an image encoder $Enc^i$ and a text encoder $Enc^t$. It is trained on 400 million image-text pairs from the internet, aiming to align image embeddings with their corresponding text captions through contrastive learning:\n$L_{CLIP} = \\frac{1}{2N} [\\sum_{n=1}^{N} log \\frac{exp(cos(z_i^n, z_t^n)/\\tau)}{\\sum_{m=1}^{N} exp(cos(z_i^n, z_t^m)/\\tau)} +log \\frac{exp(cos(z_i^n, z_t^n)/\\tau)}{\\sum_{m=1}^{N} exp(cos(z_i^m, z_t^n)/\\tau)} ]$  (1)\nwhere $N$ represents the number of image-caption pairs, $z_i^n = Enc^i(image_n)$ and $z_t^n = Enc^t(text_n)$ are the embeddings of the n-th image and text respectively, $\\tau$ is a temperature parameter, and $cos(\\cdot, \\cdot)$ denotes the cosine similarity function.\nThis alignment enables CLIP to perform zero-shot classification by matching image embeddings with text embeddings of a template \u201ca photo of a <class-name>.\u201d, where <class-name> iterates all the possible classes of a dataset. Without loss of generality, given an image, let $z^i$ denote its CLIP-encoded image embedding, and $z_t$ be the text embedding of a possible class description, i.e., $z_t = Enc^t(\"a photo of a class c.\"). The predicted class $\\hat{y}$ is determined by:\n$\\hat{y} = arg\\ max_c cos(z^i, z_t)$. (2)"}, {"title": "4 CLIPURE: ADVERSARIAL PURIFICATION IN LATENT SPACE VIA CLIP", "content": "In this section, we outline the methodology of our CLIPure, focusing on adversarial purification within CLIP's latent space. We first define purification risk through a Stochastic Differential Equation (SDE) perspective and derive its lower bound in Section 4.1. Section 4.2 introduces the rationale for CLIPure to potentially achieve a smaller purification risk and two variants of modeling sample likelihood. In Section 4.3, we propose normalizing latent vectors to diminish the effect of vector length during purification to align with CLIP's latent space modeled using cosine similarity.\nConsidering that adversarial attacks progressively add perturbations to an image while purification gradually removing noise to restore the original image, we formulate both the attack and purification processes through the lens of Stochastic Differential Equations (SDEs). This framework allows us to propose a measure of purification risk based on the divergence between the attack and purification processes, providing insights into what affects purification effectiveness.\nWe formulate the attack process as a transformation from the benign distribution $p_{ben}(x)$ to an adversarial example distribution $p_{adv}(x)$ by an attack algorithm. Note that for simplicity we use $p(x)$ to represent $p_{ben}(x)$ in this paper. Take untargeted PGD-attack (Madry et al., 2017) for instance, the adversarial attack behavior on a benign sample $x_o$ can be described as:\n$dx = a\\sign(\\nabla_xL(\\theta; x_t, Y_{true}))dt + \\sigma dw_t, x_o \\sim p(x), s.t., ||x_T - x_o|| \\leq \\epsilon$, (3)\nwhere $a$ represents the attack step size, $p(x)$ denotes the distribution of benign samples, $L(\\theta; x_t, Y_{true})$ denotes the loss of $x_t$ classified by the model with parameters $\\theta$ as the ground truth category $Y_{true}$ at attack step t (where $t \\in [0,T]$), $dw_t$ denotes the Wiener process (Brownian motion). The constant $\\sigma$ serves as a scaling factor for the noise component and the adversarial example $x_T$ is bounded by $\\epsilon$ in $l_p$ norm.\nThe corresponding reverse-time SDE (Anderson, 1982) of Eq. 3 describes the process from the adversarial example distribution $P_{adv}$ to the purified sample distribution $P_{pure}$, and is expressed as:\n$dx = [a\\sign(\\nabla_xL(\\theta; x_t, Y_{true})) - \\sigma^2 \\nabla log p(x_t)]dt + \\sigma d\\tilde{w}_t, x_T \\sim P_{adv} (X),$ (4)\nwhere $log\\ p(x_t)$ represents the log-likelihood of $x_t$ concerning the distribution of clean samples, analogous to the score function described in Score SDE (Song et al., 2020), $d\\tilde{w}_t$ represents the reverse-time Wiener process. A detailed discussion on the form of the reverse-time SDE can be found in Appendix A.\nNote that in the reverse-time SDE, t progresses from T to 0, implying that dt is negative. According to Eq. 4, the reverse SDE aims to increase the sample's log-likelihood while simultaneously decreasing the loss of classifying x to Ytrue. In Eq. 4, the purification term is related to the common objective of adversarial purification $x_{pure} = arg\\ max\\ log\\ p(x)$. The classifier guidance term has been employed to enhance purification (Zhang et al., 2024), and their objective aligns well with Eq. 4. We will incorporate this guidance term with CLIPure in Appendix D.3 and see its impact.\nThen, we define the joint distribution of the attack process described by the forward SDE in Eq. 3 as $P_{o:T} = p(x_o = X_{ben}, x_1, ..., x_T = X_{adv}) \\in R^{(T+1) \\times d}$, where each $x_t \\in R^d$. For the purification process defined by the reverse-time SDE in Eq. 4, we denote the joint distribution as $Q_{o:T} = p(x_o = X_{pure}, x_1, ..., x_T = X_{adv})$. Here, $X_{ben}, X_{adv}, and\\ x_{pure}$ denotes the benign sample, adversarial example, and purified sample respectively. We define the purification risk R(Q) by the KL divergence between the reverse SDE (corresponding to the purification process) and the joint distribution of forward SDE (representing the attack process):\n$R(Q) := KL(Q_{0:T} || P_{0:T}) = KL(Q_{0,T}||P_{0,T}) + E_{Q_{0,7}} [KL(Q_{1:T-1|0,T}||P_{1:T-1|0,T})]$\n$> KL(Q_{0,T}||P_{o,T}).$ (5)"}, {"title": "4.1 ADVERSARIAL PURIFICATION RISK", "content": "Then, we focus solely on the purified example, i.e., $KL(Q_{0,T}||P_{o,T})$ rather than the entire purification trajectory. The forward SDE in Eq. 3 describes the transformation from $p(x_{ben})$ to $p(x_{adv})$, enabling us to obtain the conditional probability $p(x_{adv}|x_{ben})$. Simultaneously, the reverse SDE in Eq. 4 supports the transformation from $p(x_{adv})$ back to $p(x_{pure})$, helping us to quantify $p(x_{pure}|x_{adv})$.\nThen we can derive that:\n$R(Q) \\geq KL(Q_{0,T}||P_{o,t}) = KL(p(x_{pure}, x_{adv})||p(x_{ben}, x_{adv}))$\n$= \\frac{1}{2}E_{x_{adv}} [|\\nabla log p(x_{adv})|^2 \\Delta t] - KL(p(x_{adv})||p(x_{ben})),$ (6)\nwhere $\\Delta t$ denotes a small time interval for attack and purification, related to the perturbation magnitude. A detailed proof of the result is provided in Appendix B.\nThe result in Eq. 6 highlights that the lower bound of the purification risk is influenced by two factors: 1) the smoothness of the log-likelihood function at adversarial examples and possibly the sample dimension, as indicated by the $l_2$ norm of $|\\nabla log p(x_{adv})|$, 2) the differences between the likelihood of clean and adversarial samples in the benign example space."}, {"title": "4.2 ADVERSARIAL PURIFICATION IN CLIP'S LATENT SPACE", "content": "In this section, we further explore how to achieve a smaller purification risk R(Q). Considering $E_{x_{adv}} [|\\nabla log p(x_{adv})|^2 \\Delta t]$ within R(Q) in Eq. 6, standard pixel space purification may lead to higher purification risks due to its sparsity and possibly peaked gradient distribution in high dimensionality. Thus, we are curious to investigate purification in latent space where the distribution of sample densities is more uniform and smoother.\n$KL(p(x_{adv})||p(x_{ben}))$ in Eq. 6 implies representations that excel at detecting out-of-distribution adversarial examples are likely to carry a lower risk of purification errors. Huang et al. (2021) suggest that multi-modal latent spaces offer superior quality compared to uni-modal counterparts. Inspired by this observation, CLIP's well-aligned multi-modal latent space, where image embeddings are guided by the semantics of finer-grained words in an open vocabulary, may provide a foundation for purification with a lower risk.\nTo validate the efficacy of different spaces in modeling sample likelihood for adversarial purification, we focus on the term $KL(p(x_{adv})||p(x_{ben}))$ in the lower bound of the purification risk R(Q). As illustrated in Figure 2, we extract 512 samples from ImageNet (Deng et al., 2009) and generate adversarial examples by AutoAttack (Croce & Hein, 2020) on the CLIP (Radford et al., 2021) classifier. We explore four types of likelihood modeling approaches:\nIn Pixel Space: Sample likelihood of the joint distribution of image pixels is estimated by a generative model (we use an advanced diffusion model - EDM(Karras et al., 2022)). Figure 2a indicates that even EDM struggles to distinguish between clean and adversarial sample distributions at the pixel level. We use the Evidence Lower Bound (ELBO) to estimate log-likelihood, expressed as $log\\ p_{\\theta}(x) \\geq -E_{\\epsilon,t} [|| E_{\\theta} (x_t, t) - \\epsilon||^2] + C$, where $\\epsilon \\sim N(0, I)$, and C is typically negligible (Ho et al., 2020; Song et al., 2020)."}, {"title": "4.3 ADVERSARIAL PURIFICATION BASED ON NORMALIZED UNIT VECTORS", "content": "Typically, purification in pixel space is conducted through gradient ascent on the sample x by using the derivative of the log-likelihood log p(x): $x \\leftarrow x + a\\nabla log\\ p(x)$, where a denotes the step size for updates. However, given that the cosine similarities between vectors in CLIP's latent space are the criterion for alignment where vector lengths do not take effect, it is inappropriate to directly apply the typical purification update manner to this space. Thus, we normalize the image vectors to unit vectors at each purification step to diminish the effect of vector length. Specifically, we first normalize the vector $z_i = Enc^i(x)$, obtained from the CLIP model image encoder for an input sample x (potentially an adversarial sample), to a unit vector $u = \\frac{z_i}{|z_i|^2}$. Then we calculate the sample's log-likelihood log p(z) and compute the gradient $g_u$ by the chain rule:\n$g_u = \\frac{\\partial log\\ p(z^i)}{\\partial u} = \\frac{\\partial log\\ p(z^i)}{\\partial z^i} \\frac{\\partial z^i}{\\partial u}$ (9)\nThis gradient $g_u$ is then used to update the direction z for adversarial purification, detailed in Algorithm 1. Note that CLIPure is based on the original CLIP and does not need any extra training.\nWe also attempted adversarial purification by directly optimizing vectors instead of the normalized version. We experimented extensively with various steps, parameters, and momentum-based methods, but found it challenging to achieve robustness over 10% on ImageNet, indicating that it is difficult to find an effective purification path with vector lengths taking effect."}, {"title": "5 EXPERIMENTS", "content": "Datasets. Following the RobustBench (Croce & Hein, 2020) settings, we assess robustness on CIFAR-10 and ImageNet. To compare against CLIP-based zero-shot classifiers with adversarial training (Schlarmann et al., 2024; Mao et al., 2022), we conduct additional tests across 13 image classification datasets (detailed in Appendix C.1). In line with Schlarmann et al. (2024), we randomly sampled 1000 examples from the test set for our evaluations.\nBaselines. We evaluate the performance of pixel space purification strategies employing generative models, including Purify-EBM (Hill et al., 2020) and ADP (Yoon et al., 2021) based on Energy-Based Models; DiffPure based on Score SDE (Nie et al., 2022) and DiffPure-DaLLE2.Decoder based on Decoder of DaLLE2 (Ramesh et al., 2022); GDMP based on DDPM (Ho et al., 2020); and likelihood maximization approaches such as LM-EDM (Chen et al., 2023) based on the EDM model (Karras et al., 2022) and LM-DaLLE2.Decoder which adapts LM to the Decoder of DaLLE-2. Furthermore, we perform an ablation study adapting LM to the latent diffusion model to achieve uni-modal latent space purification using the Stable Diffusion Model (Rombach et al., 2022), denoted as LM-StableDiffusion. Furthermore, we also compare with the state-of-the-art adversarial training methods such as AT-ConvNeXt-L (Singh et al., 2024) and AT-Swin-L (Liu et al., 2024), along with innovative training approaches such as MixedNUTS (Bai et al., 2024) and MeanSquare (Amini et al., 2024), and methods that utilize DDPM and EDM to generate adversarial samples for training dataset expansion: AT-DDPM (Rebuffi et al., 2021) and AT-EDM (Wang et al., 2023). Moreover, we also compare CLIPure with adversarially trained CLIP model according to TeCoA (Mao et al., 2022) and FARE (Schlarmann et al., 2024). Additionally, we also evaluate the performance of classifiers without defense strategies, including CLIP, WideResNet (WRN), and Stable Diffusion. Due to space constraints, we list the details of baselines in Section C.4 in Appendix C.\nAdversarial Attack. Following the setup used by FARE (Schlarmann et al., 2024), we employ AutoAttack's (Croce & Hein, 2020) strongest white-box APGD for both targeted and untargeted attacks across 100 iterations, focusing on an $l_\\infty$ threat model (typically $\\epsilon = 4/255$ or $\\epsilon = 8/255$) as well as $l_2$ threat model (typically $\\epsilon = 0.5$) for evaluation. We leverage adaptive attack with full access to the model parameters and inference strategies, including the purification mechanism to expose the model's vulnerabilities thoroughly. It means attackers can compute gradients against the entire CLIPure process according to the chain rule: $\\frac{\\partial x}{\\partial x_{pure}} = \\frac{\\partial z_{pure}}{\\partial z_i} \\frac{\\partial z_i}{\\partial x}$. We also evaluate CLIPure against the BPDA (short for Backward Pass Differentiable Approximation) with EOT"}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "In this section, we compare CLIPure-Diff and CLIPure-Cos with SOTA methods and examine the model robustness from various perspectives.\nDiscussion of CLIPure-Diff and CLIPure-Cos. We compare the performance of CLIPure-Diff and CLIPure-Cos against AutoAttack across CIFAR-10, ImageNet, and 13 datasets in Tables 1, 2, and 7 respectively, as well as defense against BPDA+EOT and latent-based attack in Table 4 and 5 in Appendix D. Results indicate that both models have achieved new SOTA performance on all the datasets and CLIPure-Cos uniformly outperforms CLIPure-Diff in clean accuracy and robustness. It is probably because the DiffusionPrior component used in CLIPure-Diff models the generation process by adding noise to the original image embeddings encoded by CLIP without diminishing the effect of vector magnitude. Specifically, the noise is added as $z = \\sqrt{\\overline{a}}z_i + \\sqrt{1 - \\overline{a}}\\epsilon, \\epsilon \\sim N(0, I)$. We expect that the performance will be boosted if the generation process also eliminates the effect of vector length. In contrast, CLIPure-Cos has no such issue and it models the likelihood with cosine similarities that are consistent with CLIP.\nComparisons with Purification in Pixel Space. Compared to the SOTA purification methods DiffPure (Nie et al., 2022) and LM-EDM (Chen et al., 2023) (the likelihood maximization approach based on the advanced diffusion model EDM (Karras et al., 2022)), our CLIPure achieves better adversarial robustness (shown in Table 1 and 2) as well as superior inference efficiency (shown in Table 3). Table 1 shows that on CIFAR10 under the $l_\\infty$ and $l_2$ threat models, our CLIPure-Cos showed improvements of 27.1% and 22.5% over LM-EDM and improvements of 27.8% and 14.0% over DiffPure. On the ImageNet dataset, shown in Table 2, CLIPure-Cos achieves a relative increment of 288.2% over LM-EDM and 63.5% over DiffPure. Additionally, the inference efficiency of our CLIPure is multiple orders of magnitude higher than DiffPure and LM-EDM (see Table 3).\nMoreover, in Table 2, we also compared the pixel space purification based on the Decoder component of DaLLE-2 (Ramesh et al., 2022) (LM-DaLLE2.Decoder). Results show that the LM-DaLLE2.Decoder suffers from a drop in accuracy, possibly because the diffusion architecture ADM (Dhariwal & Nichol, 2021) it employs is slightly inferior to EDM in terms of generation quality."}, {"title": "5.2 MAIN RESULTS", "content": ""}]}