{"title": "LEROjD: Lidar Extended Radar-Only Object Detection", "authors": ["Patrick Palmer", "Martin Kr\u00fcger", "Stefan Sch\u00fctte", "Richard Altendorfer", "Ganesh Adam", "Torsten Bertram"], "abstract": "Accurate 3D object detection is vital for automated driving. While lidar sensors are well suited for this task, they are expensive and have limitations in adverse weather conditions. 3+1D imaging radar sensors offer a cost-effective, robust alternative but face challenges due to their low resolution and high measurement noise. Existing 3+1D imaging radar datasets include radar and lidar data, enabling cross-modal model improvements. Although lidar should not be used during inference, it can aid the training of radar-only object detectors. We explore two strategies to transfer knowledge from the lidar to the radar domain and radar-only object detectors: 1. multi-stage training with sequential lidar point cloud thin-out, and 2. cross-modal knowledge distillation. In the multi-stage process, three thin-out methods are examined. Our results show significant performance gains of up to 4.2 percentage points in mean Average Precision with multi-stage training and up to 3.9 percentage points with knowledge distillation by initializing the student with the teacher's weights. The main benefit of these approaches is their applicability to other 3D object detection networks without altering their architecture, as we show by analyzing it on two different object detectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd.", "sections": [{"title": "1 Introduction", "content": "Environment perception is the first module in each automated driving stack. Multiple sensor modalities, like cameras, lidars, and radars are utilized for this task. Radar sensors are of unique interest in perception due to their robustness against poor lighting, challenging weather conditions like rain or snow, and cost-effectiveness [3,5,47]. One exclusive advantage is the ability to measure the relative radial velocity of reflections directly due to the Doppler effect.\nWhile a precise localization of objects is possible with traditional radar sensors without elevation angle measurements [9], it is inherently limited to the horizontal plane. Additionally, it is hard to predict the objects' extent due to the reflections' sparsity. The introduction of 3+1D high-resolution imaging radar sensors has recently mitigated these limitations, at least partially. In addition to measuring the elevation angle of reflections, the density of measurements is increased [14, 20]. Therefore, approaches that only utilize radar sensors for perception of the environment are of particular interest.\nDespite improvements in radar-based object detection, the performance still lags behind other sensor modalities like lidar [5]. One persistent major limitation of 2+1D classic radar and 3+1D imaging radar sensors is the relative sparsity of the point cloud, which limits the detection performance.\nLidar sensors, on the other hand, are well suited for object detection and are therefore frequently employed as a reference for evaluating the performance of different sensor modalities due to their ability to produce an accurate and dense understanding of the scene. Their effectiveness is particularly pronounced in detecting nearby traffic participants without occlusion [53].\nAll currently available datasets that contain 3+1D imaging radar data additionally accommodate lidar sensor data [4,5,8, 32, 35, 40, 63, 65, 69]. The lidar sensor data of these datasets is currently either utilized for labeling, combining multiple sensor modalities for accurate object detection, or comparing the performance of radar-only techniques to another sensor modality. While the majority of series production vehicles may not include lidar sensors due to cost and vehicle design constraints, they will still be available in the training process of learning-based methods. Extending radar-only methods with lidar sensor data in training has been shown to be a viable method for estimating point flow on the imaging radar point cloud [12]. These observations lead to the following research question: Can lidar sensor data be utilized in the training process of imaging radar-based 3D object detectors to improve the object detection performance on radar-only data during inference?"}, {"title": "2 Related Work", "content": "Imaging radar sensors commonly utilize the point cloud as a data representation format instead of the radar tensor, due to its higher computational efficiency. This format is similar to the one used for lidars, enabling the application of object detection methods developed for lidars to radars. Object detection on lidars can be split into two main categories. Point-based methods, like PointNet++ [39], downsample the original point cloud, encode it using a backbone, and finally apply a detection head. Voxel-based methods discretize the point cloud into a 3D grid and apply 3D convolutions to the grid before finally applying a detection head [6, 10,71]. The main drawback of voxel-based methods is the high memory consumption and the loss of spatial information due to the discretization of the point cloud. To overcome the limitation of high memory consumption, [23,27] have proposed the PointPillars network. Pillars are a specific form of voxels that span over the full height of the scene and represent the point cloud in a 2D grid.\nMethods from the lidar domain have been shown to perform reasonably well on radar data [5,37]. However, they are limited due to the sparsity of the radar point cloud. SMURF [30] considers two representations of the radar point cloud to address sparsity. Using kernel density estimation, it utilizes pillarization and density features derived from a multi-dimensional Gaussian mixture distribution. RPFA-Net [57] is a PointPillars [23] based network, which introduces a self-attention mechanism to extract global context information from the radar point cloud. RadarMFNet [48] utilizes a multi-frame radar point cloud representation to address the sparsity of the radar point cloud in conjunction with an anchor-based detector and temporal pooling layers.\nTo use different sensor modalities in training, transfer learning and knowledge distillation (KD) principles can be utilized. While KD is commonly employed across sensor modalities such as camera images and lidar point clouds, its application between 3+1D imaging radar and lidar sensors remains unexplored. Since lidar and imaging radar share a structurally similar data representation as point clouds, an identical base network with different input modalities can be utilized to transfer knowledge between different input modalities. We investigate two approaches to transfer knowledge from lidar-based to radar-based object detectors: a KD-based and a multi-stage training approach. The main principles of the two methods are visualized in Figure 1.\nThe contributions of this work are summarized as follows:\nWe investigate a combination of lidar and radar sensors in the training stage of object detectors to improve radar-only object detection at inference.\nWe investigate three thin-out strategies for lidar point clouds to transfer knowledge from dense lidar to sparse lidar and radar-only object detectors.\nWe propose a multi-stage training procedure to transfer knowledge from dense lidar to sparse lidar and, finally, to radar-only object detection.\nWe modify and analyze several knowledge distillation-based approaches to transfer knowledge from lidar to radar-only object detectors."}, {"title": "3 Method", "content": "Two methods of transferring knowledge from lidar-based to radar-based object detectors are investigated: Knowledge Distillation (KD) (Section 3.1), which is modified for the task of transfer learning and our proposed multi-stage training procedure with sequential point cloud thin-out (Section 3.2). Additionally, the utilized thin-out strategies for lidar point cloud (Section 3.3) are described."}, {"title": "3.1 Knowledge Distillation", "content": "KD is commonly used for two tasks. First, designing computationally efficient models by transferring knowledge from a larger teacher network to a smaller student network [18,49,51]. Second, to transfer knowledge across sensor modalities by utilizing different model architectures for the teacher and student [1,16, 67]. Lidar and radar-based point cloud object detection can utilize the same model structure but with different input modalities. This enables the utilization of KD methods first described for designing computationally efficient models for cross-modal knowledge transfer from lidar- to radar-based object detectors. In this case, the teacher is trained on the full lidar point cloud, while the student is trained on the radar point cloud. Three different loss terms, as described by [7], are employed:\nLogit-KD is the first, classic type of distilling knowledge described by [18]. For 3D object detection, the logit-KD loss $L_{logit}$ is split into a classification $L_{l-cls}$ and regression loss $L_{1-reg}$. These losses are calculated by comparing the student's and teacher's predictions utilizing the 3d detectors' regression loss and bi-linear interpolation between student and teacher output classes.\nFeature-KD is widely utilized in 2D object detection [28,51]. It utilizes a loss term that forces the student network to mimic the teacher's intermediate feature map (feat). A feature mimicking the last layer of the bird's eye view feature encoder, similar to [7], is utilized in this work.\nLabel-KD is a recent distillation approach that leans on the concept of the Logit KD but simplifies and generalizes it. It is first described by [33]. The teacher predictions are filtered by their scores using a score threshold, and an adapted ground truth set is constructed by combining the filtered predictions and the ground truth set. This adapted set is utilized in student training. The loss is split into a classification $L_{cls}$ and regression loss $L_{reg}$. It replaces $L_{label}$ usually calculated on the ground truth set.\nThe three KD loss terms are combined into a joint loss weighted with\n$L_{joint} = \\lambda_{l-reg} L_{l-reg} + \\lambda_{l-cls} L_{l-cls.} + \\lambda_{feat} L_{feat} + \\lambda_{reg} L_{reg} + \\lambda_{cls} L_{cls}$"}, {"title": "3.2 Multi-Stage Lidar Thin-Out Training Procedure", "content": "Utilizing a different input data modality for either pre-training a network on a large dataset or utilizing simulated data in the training process of a point cloud-based network has been shown to improve the object detection performance [54]. The multi-stage training method (MSTM) proposed in this work extends upon this by utilizing a Curriculum learning [2] based training procedure by which the network is trained on iteratively sparsified lidar point clouds, similar to [52], and fine-tuned on the radar point cloud. Figure 2 visualizes our multi-stage training procedure. The network is first trained on the full lidar point cloud until convergence. In the following steps, the lidar point cloud is thinned-out iteratively by a factor of 2 and utilized for training a network whose weights are initialized using the previously trained model. This forces the network to learn features for a good object detection performance on increasingly sparser point clouds. In the second to last step, the lidar point cloud is mixed with the radar points so that the network can translate from the lidar to the radar domain. In the last step, the model is trained only on radar points. A training without multiple stages is called single-stage training method (SSTM) in this work.\nIn addition to the training where only lidar points are utilized in the first training stages, we investigate the utilization of the radar point cloud in conjunction with the lidar point cloud in all stages. The thin-out of the lidar points remains the same and is mixed with the radar point cloud in each step. This conditions the model on radar from the first step in order to prioritize features in the lidar point cloud that relate to a good object detection on radar-only data.\nFor mixing lidar and radar point clouds, the voxel or pillar feature encoder is modified to prioritize the radar point cloud in the random sampling process as done by [34]. Otherwise, the vastly higher number of lidar points, even when thinned-out, might lead to the complete exclusion of radar points."}, {"title": "3.3 Lidar Thin-Out Strategies", "content": "Three different methods for sub-sampling the lidar point cloud are investigated. Figure 2 shows examples for each thin-out stage.\nRandom sampling is the simplest method of sparsifying a point cloud. It neglects the structure and inherent limitations of the point cloud representation, especially for objects far away or with a high degree of occlusion. Neglecting the structure can lead to the complete loss of information for objects represented only by few points.\nK-nearest neighbor sampling approximates the reflection density distribution of the radar point cloud with the lidar point cloud by only keeping lidar reflections close to radar reflections. This algorithm is described in Algorithm 1. Objects not detected by the radar sensor are, therefore, also not represented by the k-nearest neighbor thinned-out lidar point cloud.\nVoxel-based sampling aims to reduce the number of points in each area of the point cloud while keeping the general distribution of the point cloud. This is motivated by the fact that radar sensors do not suffer as much from loss of resolution with distance as lidar sensors. The approach is described in Algorithm 2, and executed iteratively for a sequence of sparsification steps.\nAnother common sparsification method utilized for imitating low-resolution lidar sensors is layer-based sampling [52, 61]. This approach is not investigated because radar sensors do not capture the environment on a layer basis."}, {"title": "4 Experimental Evaluation", "content": "Dataset: All experiments are conducted utilizing the View-of-Delft (VoD) dataset [5]. It contains synchronized data of multiple sensor modalities. The 64-layer lidar sensor and the imaging radar are utilized in this work. A point cloud accumulated over 5 frames [5], which has been shown to improve object detection performance compared to no accumulation, is used for radar data [5,38]. We detected a duplication of identical points in the lidar point cloud, which could adversely affect all sampling methods; thus, we eliminated the duplicated points from the point cloud. Although the VoD dataset is among the best currently available datasets for imaging radar-based object detection, it is limited by its size compared to other automotive datasets without imaging-radar, like [31,45]. Given the absence of publicly available labels and limited online evaluation for the test dataset, we repurpose the validation dataset as a test set. Consequently, we partition the original training set into a new training set (80%) and a dedicated validation set (20%) to ensure robust model training.\nEvaluation Metrics: The primary performance metric utilized to compare the results is the mean average precision (mAP), as used by [15] [37]. Similar to the evaluation of the Waymo data set [46] in [62], we split the results into two distance bins: short-range (SR): 0-30 m and mid-range (MR): 30-50 m. All experiments are conducted utilizing three different random network initializations that are averaged.\nTraining: Most experiments use the PointPillars model [23] as an object detector with the same configuration as utilized by [5]. For imaging radar data, PointPillars has been shown to perform among the best out of multiple state-of-the-art 3D object detection methods while still performing adequately on lidar data [37]. Furthermore PointPillars is a relevant baseline for radar-specific object detection methods [57,58]. To show that the proposed MSTM and KD apply to various object detectors, the most promising approaches from the evaluations on PointPillars are evaluated on DSVT-P [17], as an example for a transformer-based model. All SSTM trainings are conducted with an early stopping policy for a maximum of 125 epochs. For the MSTM the initial training on the full lidar point cloud is conducted for 125 epochs, while each refinement step is trained for 30 epochs. All trainings utilize the Adam optimizer [21] and an adapted learning rate scheduler that reaches its maximum learning rate earlier and has a faster descent than the scheduler described by [43]. This improves the object detection performance on radar data.\nNotation: To distinguish between methods, the following notation is used:\n$\\frac{T}{LS/TO} \\rightarrow T_{KD}$"}, {"title": "4.2 Evalutation of MSTM on Lidar-only and Mixed Radar + Lidar", "content": "To evaluate the applicability of our proposed MSTM to the thinned-out lidar point cloud, we evaluate the training without the last two steps involving radar data. Training is conducted for just the lidar point cloud and the mixed radar + lidar point cloud in all stages. The multi-stage trained network is evaluated after each thin-out stage on the thinned-out lidar (or mixed radar + lidar) point cloud. Thin-out stages up to 1/256 of the original lidar point cloud are considered due to the lidar point cloud containing fewer points than the radar point cloud at 1/256 of the original lidar point cloud. The MSTM is compared to the SSTM, which is trained only on the thinned-out point cloud. The results are shown in Fig. 3, complete quantitative results are given in the supplementary.\nAll thin-out strategies result in an increasing degradation of the detection performance. This is most pronounced when considering a random thin-out, which drops approximately linearly with each thin-out step. K-nearest neighbor and voxel-based sampling consistently perform better than random sampling due to keeping a higher point density in areas around objects, which supports object detection. For k-nearest neighbor sampling, the performance only drops by 0.6 percentage points between $L_{knn}^{SSTM}$ and $L_{knn}^{MSTM}$ due to mostly ground points being removed in the first thin-out stage. For voxel-based sampling, after a sharp performance drop in the first thin-out stage, only a relatively slight performance drop is observable until the 1/16 thin-out stage. In the first stages, the performance drop can mainly be attributed to the pedestrian class, which drops by 12.8 percentage points in the first thin-out stage. In comparison, the detection performance of the car and cyclist classes only decreased by 0.7 and 3 percentage points, respectively. This can be explained by the voxel size of 1 m in each dimension utilized in the voxel-based sampling. A single pedestrian only occupies a small number of these voxels. When the point cloud gets thinned out, the entire object's structure gets lost, making detection difficult. In contrast, the object's structure and physical appearance can be adequately represented when it occupies more voxels, as observed in the car and cyclist classes. In the final thin-out stage of $L_{vox}^{SSTM}$ and $L_{vox}^{MSTM}$, voxel-based sampling performs worse than k-nearest neighbor sampling because many voxels only consist of ground points or points of surrounding background objects.\nThe multi-stage training does not result in a useful knowledge transfer and, therefore, significant performance benefit for all considered thin-out strategies.\nA contrary behavior is observed on the combined radar + lidar point cloud. The MSTM consistently outperforms the SSTM when using k-nearest neighbor or voxel-based sampling. Knowledge from the dense point cloud can be transferred to the thin radar + lidar point cloud. Additionally, performance is consistently higher than training just on lidar, especially for voxel-based sub-sampling, which performs best at lower thin-out stages. At small thin-out stages, the voxel-based sub-sampling can still represent the whole object space and give meaningful environmental information. At the same time, the radar point cloud is sufficiently dense for object detection."}, {"title": "4.3 Evaluation of MSTM with Last Radar-Only Step", "content": "This chapter analyses the performance of the MSTM when applied to radar data, as described in Section 3.2. The MSTM is evaluated for two different procedures. Utilizing only the lidar point cloud and utilizing the mixed radar + lidar point cloud in the first stages. The results of MSTM are shown in Table 2. The lidar thin-out stages up to 1/16 of the original lidar point cloud are considered due to the performance of $L_{MSTM}^{LiMM}_{1/4} rand$ dropping below $R_{SSTM}$."}, {"title": "4.4 Evaluation of Cross-Modality KD", "content": "The configuration of the teacher's training data is of particular interest, as the teacher's performance directly influences the student's. The simplest choice is to train the teacher solely on the lidar point cloud. Section 4.2 and Section 4.3 show that mixing the radar and lidar point clouds can benefit radar-only object detection. Therefore, teachers trained on a mixed point clouds, in addition to the ones solely trained on lidar, are compared. Mixed point clouds containing 1/4 of the original lidar point cloud are considered, as a closer representation of the teachers to the student's data can lead to a better performance. Thin out of 1/4 is chosen as the radar + lidar detection performance with radar and 1/8 of the lidar points is worse than the SSTM on only lidar data. Table 3 shows the performance of all utilized teachers.\nTable 3 shows the results of the teacher network for different training data configurations. It can be observed that, as expected, the mixed radar and lidar point cloud performs the best, only being surpassed by $RL_{1/4/knn}^{SSTM}$ on the cyclist class in short range. No model can be considered the overall second best among the other training sets. The performance varies between vehicle classes.\nThe KD is evaluated individually for each KD method and teacher training set. A joint KD is also considered comprised of all three KD losses. All student networks are initialized (Init) with the weights of the teacher network, as it has been shown to improve the student's performance [7]. Additionally, for the transfer learning between datasets, the pre-training utilizing MSTM in Section 4.3 has been shown to improve the performance on the radar dataset.\nThe results utilizing the KD are shown in Table 4. Just the initialization of the student network with the teacher's weights already results in a performance benefit in the SR with $RL_{1/4/rand}^{SSTM}$ and $RL_{1/4/vox}^{SSTM}$ and as teachers and in the MR with all teachers. This is overall only surpassed by the $RL_{1-1/16/vox}^{MSTM} \\rightarrow R$ showing that the MSTM is substantially better than a simple initialization.\nFor label-KD, the best-performing models are the ones where the teacher performs the best. Specifically $LSSTM \\rightarrow R^{lab}$, which is only surpassed by $RL_{Tnn}^{SSTM} \\rightarrow R^{lab}$ in the MR. For worse-performing teachers, using label-KD loss does not result in a performance benefit due to it replacing the ground truth label loss. Feature-KD requires a teacher who learns features from radar data. This is observable in the SR performance of models, where the teacher is trained on mixed radar + lidar data. Logit-KD works well on teacher datasets which closely resemble the radar point cloud and perform well on the teacher set. This results in good performance of $RL_{1/4/VOX}^{SSTM} \\rightarrow R^{log}$. Besides the KD method the thin-out method utilized in the teacher's training influences the student's performance. Random sampling results in good performance in the SR, while voxel-based sampling results in good MR performance. This is somewhat contrary to what has been observed in the MSTM, where random sampling performs the best in the MR and can be explained by the different thin-out stages utilized in the MSTM and KD. Further qualitative results, as well as detailed quantitative results are given in the supplementary. Overall, initializing the student with the teacher's parameters yields good performance, with further enhancements primarily achievable through feature-KD with a teacher trained on mixed radar + lidar data."}, {"title": "4.5 Evaluation on DSVT as a Transformer-Based Object Detector", "content": "Table 4 shows the results of the MSTM and KD on DSVT-P [17] for selected methods. Similar effects, as observed for PointPillars, are seen on DSVT-P. Initializing the student with the teacher's weights and the MSTM led to a performance benefit. However, contrary to PointPillars, the KD approach does not contribute to any improvements for DSVT-P."}, {"title": "4.6 Limitations", "content": "The best-performing methods in this work apply only to detectors that share an architecture with the target model, as a direct transfer of weights is performed for the best performance. Different models may require different levels and steps in the thinning process. These choices are additional tuning parameters that must be selected appropriately to maximize the benefit of transferring knowledge."}, {"title": "5 Conclusion", "content": "In this paper, we investigated two methods to transfer knowledge from lidar-based object detectorss to radar-only object detection. First, MSTM with sequential sub-sampling of the lidar point cloud, and second, a KD-based approach. For the MSTM, we have investigated three thin-out strategies for the lidar point cloud. These thin-out strategies are also analyzed for the training of the KD teacher network. Both methods can substantially improve the object detection performance of a radar-only object detector. The MSTM with voxel-based thin-out performs the best overall and can improve detection performance by up to 3.5 percentage points. For the KD methods, it is shown that initializing the student with the teacher's parameters, especially a teacher trained on mixed lidar and radar data, can improve the object detection performance on radar-only data, with further enhancement primarily achieved by utilizing feature-KD.\nIn future work, the applicability to further 3D object detection networks and the behavior with more advanced knowledge distillation like the ones utilized by [22] could be investigated. Due to different effects being noticed with the MSTM and the KD methods, combining both methods could be investigated by utilizing the MSTM as a teacher. To overcome the limitations of choosing a strict thin-out strategy, a learnable point cloud thin-out method [24,66] can be used."}]}