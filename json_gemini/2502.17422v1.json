{"title": "MLLMS KNOW WHERE TO LOOK:\nTRAINING-FREE PERCEPTION OF\nSMALL VISUAL DETAILS WITH MULTIMODAL LLMS", "authors": ["Jiarui Zhang", "Mahyar Khayatkhoei", "Prateek Chhikara", "Filip Ilievski"], "abstract": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration into many\ncritical applications, it is important to understand the limitations of their visual\nperception. In this work, we study whether MLLMs can perceive small visual\ndetails as effectively as large ones when answering questions about images. We\nobserve that their performance is very sensitive to the size of the visual subject\nof the question, and further show that this effect is in fact causal by conducting\nan intervention study. Next, we study the attention patterns of MLLMs when\nanswering visual questions, and intriguingly find that they consistently know where\nto look, even when they provide the wrong answer. Based on these findings, we\nthen propose training-free visual intervention methods that leverage the internal\nknowledge of any MLLM itself, in the form of attention and gradient maps, to\nenhance its perception of small visual details. We evaluate our proposed methods\non two widely-used MLLMs and seven visual question answering benchmarks\nand show that they can significantly improve MLLMs' accuracy without requiring\nany training. Our results elucidate the risk of applying MLLMs to visual recogni-\ntion tasks concerning small details and indicate that visual intervention using the\nmodel's internal state is a promising direction to mitigate this risk.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal large language models (MLLMs) (Hurst et al., 2024; Team et al., 2024; Anthropic, 2024;\nWang et al., 2024; Li et al., 2024a; Team et al., 2025; Chen et al., 2025) have greatly progressed the\nstate of multimodal reasoning and planning, and are rapidly being integrated into various downstream\napplications, ranging from robotics (Li et al., 2024b; Chen et al., 2024), biomedicine (Li et al., 2023a),\nautonomous driving (Xu et al., 2024b; Zhang et al., 2023a) to visual mathematical reasoning (Gao\net al., 2023; Zhang et al., 2024c;b) and even food recipe generation (Chhikara et al., 2024). Given\nthe rapidly growing application of MLLMs, especially in critical domains such as biomedicine and\nsecurity, it is crucial to study the limitations of their visual perception to elucidate the potential risks\nthat may affect their downstream applications.\nTo motivate the limitation that will be the focus of this work, we start by presenting three revealing vi-\nsual question answering examples in Fig. 1, in which we ask a popular MLLM BLIP-2 (FlanT5XL) (Li\net al., 2023b) to identify an object's presence or type in each image as we vary the size of the object.\nIn the absence of any prior evidence, we might reasonably expect the MLLM's answer to be invariant\nto the size of the object, because of the MLLM's large representational capacity and pretraining on\na wide variety of images containing objects of various sizes. To the contrary, in Fig. 1 (left), we\nobserve that initially the model does not recognize the existence of a small street sign and assigns a\nlower probability to the correct answer; however, zooming into the image (via more focused visual\ncropping) towards the street sign gradually increases the probability assigned to the correct answer,\nsuggesting that the model gradually perceives more and more relevant details of the street sign."}, {"title": "2 RELATED WORKS", "content": "Multimodal Large Language Models (MLLMs). MLLMs are foundation models capable of\nhandling diverse language and vision tasks. These models fall into two categories: end-to-end\npretrained and modular pretrained. End-to-end models process joint image-language data through\narchitectures such as dual-encoder (Radford et al., 2021), fusion-encoder (Li et al., 2021), encoder-\ndecoder (Cho et al., 2021), and unified transformer (Wang et al., 2022), using objectives like\nimage-text matching, contrastive learning, and masked language modeling. Modular pretrained\nmodels, which dominate recent state-of-the-art approaches, avoid costly full pretraining by adapting\nexisting components: BLIP2 (Li et al., 2023b) and InstructBLIP (Dai et al., 2023) train a Transformer-\nbased connector between a frozen pretrained ViT (Dosovitskiy et al., 2021) image encoder and a\nfrozen LLM, which transforms ViT output tokens into a fixed set of image tokens in the input space\nof the LLM; Qwen-VL (Bai et al., 2023), similarly uses a fixed-length token connector (a single\ncross-attention layer), but trains both the connector and the LLM; LLaVA (Liu et al., 2023b) and\nLLaVA-1.5 (Liu et al., 2023a) instead use a linear projection and a two-layer MLP as their connectors,\nrespectively, and train both. Our work will contribute to a better understanding of the perception\nlimitations of MLLM and improve their perception scalably and without training, offering orthogonal\nbenefits to existing approaches."}, {"title": "3 MLLMS' SENSITIVITY TO THE SIZE OF VISUAL CONCEPTS", "content": "In this section, our goal is to quantitatively study our qualitative observations in Fig. 1 that MLLMs\nstruggle with describing small visual details in images. To that end, we consider the TextVQA dataset,\nin which for each question we can find the image ground-truth bounding box that contains the correct\ntextual answer. We partition its validation set into three groups based on the relative size of the\nground-truth bounding box \\(S = \\frac{A_{bb}}{A_{total}}\\,\\), where \\(A_{bb}\\) denotes the area of the ground-truth bounding box,\nand \\(A_{total}\\) the total area of the image: 1) S < 0.005 (small) consisting of 773 question-image pairs,\n2) 0.005 \u2264 S < 0.05 (medium) consisting of 2411 question-image pairs, and 3) S \u2265 0.05 (large)\nconsisting of 1186 question-image pairs. We chose TextVQA for this study because it contains"}, {"title": "4 Do MLLMS KNOW WHERE TO LOOK?", "content": "The limitation in perceiving small visual concepts can have two primary reasons: 1) they are hard\nto locate in the larger image, and 2) their small details are hard to perceive correctly. In Fig. 1, we\nobserved that the MLLM's incorrect answer may contain partially correct information, hinting that\nit might know where to look in the image. In this section, we quantitatively study that observation\nto answer whether MLLMs' sensitivity to size is rooted in a perception limitation or a localization\nlimitation. To that end, we first utilize the attention maps computed inside the Transformer layers of\nan MLLM to quantify its spatial attention over the image and then compare the total amount of this\nattention inside the ground-truth bounding box to other bounding boxes of the same size.\nMLLMs' Setup. The considered MLLMs process a given image-question pair (x, q) in four steps\n(depicted in Fig. 4): 1) the image is divided into N \u00d7 N non-overlapping patches and processed by\nthe ViT image encoder into N \u00d7 N output tokens; 2) the ViT output tokens are transformed into\nthe input space of the backbone LLM-by either an MLP (LLaVA-1.5) or a Transformer connector\n(BLIP-2, InstructBLIP and Qwen-VL)\u2014which we refer to as image tokens; 3) the image tokens are\nthen prepended to the question tokens and a predefined starting answer token, and fed to the LLM; 4)\nthe LLM is sampled auto-regressively following the starting answer token (we use greedy sampling).\nQuantifying MLLMs' Spatial Attention over the Image. We first measure how important each\nimage token is to the MLLM's decision (answer-to-token attention) by extracting the softmax cross-\nattention of the starting answer token to all image tokens in all layers of the backbone LLM, resulting\nin \\(A_{st}(x, q) \\in R^{L \\times H \\times 1 \\times T}\\), where L, H are the number of layers and heads-per-layer in the LLM,\nand T is the number of image tokens provided to the LLM. We then take the average over attention\nheads to arrive at the answer-to-token attention \\(\\bar{A}_{st}(x, q) = \\sum_{h=1}^{H_c} A_{st}(x, q)\\). Next, we measure\nhow important each image region is to each image token (token-to-image attention). For the MLLMs\nthat use a Transformer connector to resample ViT output tokens into a fixed number of image tokens\n(BLIP-2, InstructBLIP and Qwen-VL), we extract the softmax cross-attention of each image token to\nall ViT output tokens in all layers of the connector, resulting in \\(A_{ti} \\in R^{L_c \\times H_c \\times T \\times N^2}\\), where \\(L_c\\), H\nare the number of layers and heads-per-layer in the connector, T the number of learnable query\ntokens (that become input image tokens to the LLM), and \\(N^2\\) the number of image patches of the ViT\nimage encoder. We then take the average over attention heads to arrive at the token-to-image attention"}, {"title": "5 AUTOMATIC VISUAL CROPPING (VICROP)", "content": "We observed in Sec. 4 that the sensitivity of MLLMs to visual concept size is primarily a perception\nlimitation (rather than a localization limitation). Therefore, one solution to mitigate this limitation is\nto simply train MLLMs with a larger number of image patches while maintaining per-patch resolution\n(hence increasing the image resolution of MLLMs). However, increasing the input image resolution\nby a factor of a, increases the number of ViT input patches (and output tokens) from \\(N^2\\) to \\(a^2 N^2\\),\nwhich in turn increases the softmax attention computation complexity on the order of \\(a^4 N^4\\). Given\nthat this solution is not scalable for current Transformer-based MLLMs, we choose to explore an\nalternative solution that does not require any training and is scalable to any image resolution.\nWe note that several concurrent works have explored the first direction of training MLLMs with\nhigher resolution image patches (Li et al., 2024c; Sun et al., 2024; Li et al., 2024d; McKinzie et al.,\n2024; Xu et al., 2024a; Luo et al., 2024), and notably LLaVA-Next (Liu et al., 2024a) has achieved\nthe VQA state-of-the-art in several VQA benchmarks at the time of writing. We believe our work\nis parallel to these efforts in the following sense: rather than training higher and higher resolution\nMLLMs to enable them to see all resolutions (which is inevitably upper bounded), we explore how\nto smartly adjust the input image towards what an MLLM already can see without any additional\ntraining. We provide evidence showing that our training-free method can provide orthogonal benefits\nto the training-based methods in Appendices D and E.\nInspired by our findings that MLLMs tend to know where to look (Sec. 4) and that visual cropping\ncan mitigate the perception limitation (Sec. 3), in this section we construct three automatic visual\ncropping methods in order to mitigate the perception limitation of MLLMs. These methods seek to\nuse the internal information of an MLLM itself-in the form of attention maps and gradients to find\nthe approximate region of interest in images (i.e., the region containing the subject of a question),\nand then to zoom into that region via visual cropping. One potential drawback of visual cropping is\nthat some questions might need to have a global view of the image. To address this issue, we utilize\nthe fact that MLLMs typically convert the image into a series of tokens. This allows us to directly"}, {"title": "6 VICROP METHOD ANALYSIS", "content": "In this section, we apply our proposed visual cropping methods to two open-source SOTA MLLMS,\nInstructBLIP (Vicuna-7B) (Dai et al., 2023) and LLaVA-1.5 (Vicuna-7B) (Liu et al., 2023a). We\nevaluate their effectiveness in improving the perception of smaller visual concepts on 4 detail-\nsensitive datasets (TextVQA, V* (Wu and Xie, 2023), POPE (Li et al., 2023c),\nDocVQA (Mathew et al., 2021)), and their ability to maintain performance on larger visual concepts\nin 3 general-purpose datasets containing mostly large objects (GQA (Hudson and Manning, 2019),\nAOKVQA (Schwenk et al., 2022), VQAv2 (Goyal et al., 2017)). InstructBLIP uses the hyper-\nparameters N = 16, m = 15, k = 2 and input image resolution of 224 \u00d7 224. LLaVA-1.5 uses\nN = 24, m = 14 and input image resolution of 336 \u00d7 336. When reporting accuracy, we compute\nVQA-score for all benchmarks except GQA. For GQA, we compute accuracy using the official\ncode.."}, {"title": "7 CONCLUSION", "content": "In this work, we qualitatively and quantitatively showed that there exists a perception bias against\nsmall visual details in widely-used MLLMs. Then we found that MLLMs often know where to look\neven if they fail to answer the question, indicating that the bias toward small visual details is rooted in\na perception limitation rather than a localization limitation. To mitigate this limitation, we proposed\nmultiple automatic visual localization methods as scalable and training-free solutions based on models'\ninternal dynamics while answering the visual questions. Through evaluation of multiple multimodal\nbenchmarks, we showed that our method can significantly improve MLLMs' accuracy without\nrequiring any training, especially in detail-sensitive scenarios. Our findings suggest that MLLMs\nshould be used with caution in detail-sensitive applications, and that visual cropping/localization with\nthe model's own knowledge is a promising direction to enhance their performance.\nLimitations and Future Work. The proposed ViCrop methods do not enhance all types of questions\nequally. We have observed that questions concerning relations and counting are particularly difficult\nfor ViCrop methods to help answer. This is expected as the proposed ViCrop can only focus on one\nregion in the image. We leave extending ViCrop to focus on multiple regions simultaneously for\nfuture work. Another limitation of the proposed methods is their time overhead and the addition of\nvisual tokens. While the overhead is reasonable (a few seconds), we believe it can be significantly\noptimized as an inference-time mechanism, for example by utilizing lower precision, and weight\nquantization. Furthermore, Matryoshka Query Transformer (MQT) (Hu et al., 2024) enables MLLMs\nto have varying visual context size during inference. In our current results, we have shown that our\nmethods can work with two different MLLMs with distinct visual context sizes, so it seems entirely\npossible that our method can still work with varying visual context size under MQT, which can further\nreduce our computational cost through rescaling the cropped image. We leave these inference cost\noptimizations to future works. Lastly, we have observed that the proposed methods tend to have some\ncomplementary benefits, and therefore exploring ways to combine them (for example based on the\nprediction uncertainty) is also a very interesting direction for future research."}]}