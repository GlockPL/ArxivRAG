{"title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV Leakage", "authors": ["Huan Yang", "Deyu Zhang", "Yudong Zhao", "Yuanchun Li", "Yunxin Liu"], "abstract": "Running LLMs on end devices has garnered significant atten-tion recently due to their advantages in privacy preservation.With the advent of lightweight LLM models and speciallydesigned GPUs, on-device LLM inference has achieved thenecessary accuracy and performance metrics.\nHowever, we have identified that LLM inference on GPUscan leak privacy-sensitive intermediate information, specifi-cally the KV pairs. An attacker could exploit these KV pairsto reconstruct the entire user conversation, leading to sig-nificant vulnerabilities. Existing solutions, such as Fully Ho-momorphic Encryption (FHE) and Trusted Execution En-vironments (TEE), are either too computation-intensive orresource-limited.\nTo address these issues, we designed KV-Shield, which op-erates in two phases. In the initialization phase, it permutes the weight matrices so that all KV pairs are correspondingly permuted. During the runtime phase, the attention vector is inversely permuted to ensure the correctness of the layer out-put. All permutation-related operations are executed withinthe TEE, ensuring that insecure GPUs cannot access the orig-inal KV pairs, thus preventing conversation reconstruction.Finally, we theoretically analyze the correctness of KV-Shield,along with its advantages and overhead.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the launch of the large language model (LLM) serviceby OpenAI in 2023, various LLM models based on the Trans-former architecture have rapidly emerged. These modelshave enabled groundbreaking applications, such as expert-level programming and advanced smartphone assistants,poised to transform the information service access paradigm,much like search engines and operating systems did in thepast.\nCompared to transmitting privacy-sensitive data over theInternet, on-device execution of LLMs is considered the mostprivacy-preserving solution [20]. Current mobile device man-ufacturers are competitively releasing on-device LLM de-ployment solutions at both the software and hardware levels.Notable examples include Apple's nearly 3-billion-parametermodel and Qualcomm's Snapdragon 8 Gen 3 NPU.\nWhile on-device LLM inference has been validated for ac-curacy and efficiency, it now faces the critical test of security.Unfortunately, the computing cores of mobile devices are vul-nerable to various attacks, particularly information leakage[8]. For instance, the running kernel can be extracted fromnearly all components of mobile GPUs, including shared, lo-cal, and texture memory. The impact of information leakageis magnified for LLMs. Leading LLM inference frameworks,like Meta's LLama [18], utilize memory caching of key-value(KV) pairs to accelerate inference. The KV cache persiststhroughout the entire inference process, lasting from sec-onds to minutes. Breaches in the KV cache can lead to therecreation of the original user conversation. A prime illustra-tion is demonstrated in Leftoverlocal [14] on an AMD GPU,where data leaks in shared memory allowed an attacker tointercept the KV cache and replicate the entire conversation.We have replicated this attack on a Xiaomi 12 equipped witha Snapdragon 8 Gen 1 SoC."}, {"title": "2 BACKGROUND", "content": "This section details the risks of memory leaks on existingmobile devices and describes existing content protectionschemes for large language models."}, {"title": "2.1 Key Value Cache for LLMs", "content": "The self-attention mechanism [19] in Transformer modelsis a core component used to capture dependencies betweendifferent positions in the input sequence. It flexibly focuseson different parts of the sequence to better understand thecontext.\nAs shown in Eq. 1, the input sequence is mapped throughthree weight matrices (linear transformations) to generatequery (Q), key (K), and value (V) vectors. The attention scoresare computed from the dot product of Q and K (where $\\sqrt{d_k}$is the vector dimension), normalized using the softmax func-tion, and then used to obtain a weighted sum of the V vectors,capturing long-range dependencies within the sequence.\nMost large language models, such as LLaMA [18] andQwen [2], are built on the Causal Decoder architecture [22].These models generate tokens autoregressively, determin-ing the next token based on the past prompt and previously"}, {"title": "2.2 Threat Model", "content": "Scene Setting. We consider an adversary capable of ob-serving GPU tasks in the normal world and exploiting vulner-abilities similar to Leftoverlocal[14] to read the high-speedshared cache contents of the device GPU, such as OpenCL'slocal memory or CUDA's shared memory, but unable to di-rectly access the GPU memory to obtain the model's inputsand outputs. Our primary goal is to protect the privacy ofconversations between users and the large language model(LLM), rather than focusing on protecting the model weights.Such that the adversary cannot retrieve KV cache contentsfrom the GPU's high-speed shared cache to reconstruct userconversations. Additionally, we do not consider side-channelattacks on the Trusted Execution Environment (TEE) \u2014 weassume the TEE can safeguard the confidentiality and in-tegrity of its internal programs and data.\nHow does the adversary reconstruct the user conver-sation? As shown in Figure 1, when the user initiates theLLM process, the attacker launches a malicious process tosteal the user's KV cache. The LLM process continuouslysubmits GPU tasks to the GPU execution queue, such as At-tention Kernel and FFN Kernel, to perform model inferenceand generate tokens. The attacker's malicious process contin-uously generates monitor kernels and inserts them after theAttention Kernel. By exploiting vulnerabilities, it accessesthe cached contents in the local memory of the AttentionKernel - the KV cache, and transmits this information tothe attacker. The attacker can determine which open-sourceLLM is being used by analyzing the types of GPU tasks andthe KV contents. Then, the attacker inputs the KV cacheinto the attention module along with a prompt provided bythe attacker, thereby reconstructing the user's conversationcontent."}, {"title": "3 POTENTIAL SOLUTION ANALYSIS", "content": "We analyzed two potential solutions, namely Fully Homo-morphic Encryption (FHE) and running model inferencein a Trusted Execution Environment (TEE), to prevent the"}, {"title": "3.1 The Performance of FHE", "content": "FHE is commonly employed in cloud scenario to safeguardusers' data privacy. It allows for computations on encrypteduser data, yielding decrypted results that are exactly thesame as if the computations were performed on the unen-crypted data[1]. We transition the FHE techniques to theLLM service for user privacy protection. We found thatFully Homomorphic Encryption (FHE) is too heavy forLLM inference, resulting in latency increases by nearly6 orders of magnitude compared to plaintext inference.\nAs show in Figure 2, a FHE-based LLM application consistsof 4 steps: 1) it generates a pair of keys to encrypt the inputand decrypt the output, respectively. 2) when the user sendscontent to the FHE LLM application, the encryption key isfirst used to encrypt the content, ensuring that the FHE-based LLM application can only receive the encrypted data.3) the FHE-based LLM application processes the encryptedinput data through homomorphic computation to derive thelogically encrypted result. 4) upon the computation resultsbeing sent back to the user interface, the decryption key isutilized to decrpt the data, presenting the plain-text resultson the screen."}, {"title": "3.2 Challenges in Trusted Execution Environment", "content": "Another intuitive solution to protect KV pairs is running LLMinference in the TEE, which is designed for privacy-sensitivecode and data. We summarize multiple works utilizing TEEto safeguard conventional deep learning models like ResNet,VGG, and MobileNet, addressing limited memory and lackof GPU acceleration. We will discuss the inspirations fromexisting works and the new challenges posed by LLMs.\nIncrementally feed the model layer into the TEEfor model weights protection[9] [6]. DarkneTZ [9] andT-Slice [6] primarily focus on preventing model weight leak-age, as effective membership inference attacks (MIAs) canreveal information about their training data. As shown inTable 2 and Table 3, TZDRAM is too small for CNNs dueto the limited size of TEE trustworthy memory. DarkneTZaddresses this by slicing the CNN layer-by-layer to enablemodel inference within the TEE. Conversely, T-Slice [6] runsthe entire model in the TEE. It dynamically splits the deeplearning model into units (slices) that can be executed inTrustZone's limited trusted memory without modifying theprotected deep learning model.\nOffloading computation-intensive operators to GPUdevices [15] [7]. TransLinkGuard [7] protects models dur-ing local inference by generating a locked model through re-arranging the weights of the Transformer's fully-connectedlayers. During inference, TransLinkGuard rearranges theintermediate variables in the TEE to prevent model weightleakage. ShadowNet [15] observes that linear layers (includ-ing convolutional and fully connected layers) account forover 99% of the weights and computation time. It outsourcesthese linear layers to untrusted environments (includingGPUs) for acceleration without leaking model weights.\nCompared to the model weights, the KV pairs aremore in need of protection. For model privacy securityof LLMs, KV pairs leakage leads to the recreation of userconversation. This is more direct and dangerous than the se-curity risk of obtaining training data through model weights."}, {"title": "4 DESIGN OF KV-SHIELD", "content": "In this part, we design KV-shield to protect the original KVpairs stolen by a malicious process. We use the TEE and asimply yet effective and efficient permutation operation. Wedesign KV-shield according to the following three principles:\n1) Deploy the model without modification.\n2) Keep the original KV invisible to the insecure GPUs inthe REE(Rich Execution Environment).\n3) Using GPUs in REE as much as possible to improve theinference efficiency of LLM."}, {"title": "4.1 Overview", "content": "As depicted in Figure 3, to protect the original KV pairs, wepermute the weights of the linear layers in the self-attentionoperator. It rearranges the rows or columns of the matrices,as shown in Figure4.By multiplying a matrix by a 01 matrix,we can realize that the rows of the matrix are disrupted. Insuch a way, after the GPU performs the linear layer com-putations, the insecure cache stores the permuted KV pairs.The corresponding permutation matrix is stored in the TEEto ensure that attackers cannot obtain the permutation in-formation. Finally, the results of self-attention are inverselypermuted through the TEE to obtain the correct results.\n1) At the initialization of the LLM process, we randomlypermute the weights of $Linear_q$, $Linear_k$, and $Linear_v$ for"}]}