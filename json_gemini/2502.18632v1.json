{"title": "AUTOMATED KNOWLEDGE COMPONENT GENERATION AND\nKNOWLEDGE TRACING FOR CODING PROBLEMS", "authors": ["Zhangqi Duan", "Nigel Fernandez", "Sri Kanakadandi", "Bita Akram", "Andrew Lan"], "abstract": "Knowledge components (KCs) mapped to problems help model student learning, tracking their\nmastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online\nlearning platforms. However, crafting and tagging KCs to problems, traditionally performed by\nhuman domain experts, is highly labor-intensive. We present a fully automated, LLM-based pipeline\nfor KC generation and tagging for open-ended programming problems. We also develop an LLM-\nbased knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as\nKCGen-KT. We conduct extensive quantitative and qualitative evaluations validating the effectiveness\nof KCGen-KT. On a real-world dataset of student code submissions to open-ended programming\nproblems, KCGen-KT outperforms existing KT methods. We investigate the learning curves of\ngenerated KCs and show that LLM-generated KCs have a comparable level-of-fit to human-written\nKCs under the performance factor analysis (PFA) model. We also conduct a human evaluation to\nshow that the KC tagging accuracy of our pipeline is reasonably accurate when compared to that by\nhuman domain experts.", "sections": [{"title": "1 Introduction", "content": "In student modeling, an important task is to map problems (or items or questions) to specific skills or concepts, referred\nto as knowledge components (KCs). KCs provide an invaluable resource to model student learning [2], estimating their\nmastery [5] levels on fine-grained units of knowledge. Accurately estimating student mastery levels on KCs helps enable\nboth 1) teacher feedback, by showing this information in teacher dashboards, and 2) adaptive and personalized learning\nin online learning platforms or intelligent tutoring systems [17], by tailoring instructions and content sequencing\naccording to student knowledge levels. Identifying fine-grained KCs students struggle [38] with also enables content\ndesigners to develop targeted instructional content and practice problems for students.\nKCs are typically crafted by human domain experts, who also tag problems with KCs that students need to master\nto solve the problem correctly. This process can be highly labor-intensive, prone to bias and errors, and may not be\nscalable. There exist solutions to automate parts of this process using Natural Language Processing (NLP) tools, usually\nemploying classification algorithms [32], to tag KCs to problems, which relies on having a predefined set of KCs.\nRecent advances in Large Language Models (LLMs) have shown potential in developing automated approaches for KC\nidentification in addition to tagging, in domains such as math [28] and science [25]. Automatically generating KCs is\nchallenging since KCs need to satisfy various criteria including being relevant to problems, being specific enough to\nprovide teacher and student support, being generalizable across settings, and satisfying cognitive science principles."}, {"title": "1.1 Contributions", "content": "In this paper, we explore using LLMs to automatically generate KCs for open-ended programming problems. We also\ndevelop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as\nKCGen-KT. Our contributions are summarized as follows:\n1. We develop a fully automated, LLM-based pipeline for KC generation and tagging. We first compute the\nAbstract Syntax Tree (AST) representations of two representative solution codes in the prompt, and then\nprompt GPT-40 [27], an advanced, proprietary LLM, to identify KCs that are required for a problem. Then, to\naggregate KCs across problems and de-duplicate similar ones, we cluster KCs on semantic similarity, followed\nby prompting GPT-40 [27] to summarize each cluster and provide a name. Finally, we automatically tag\nproblems with KCs according to the clustering results.\n2. We develop an LLM-based KT method to leverage the generated KCs for the KT task. Our method leverages\nthe textual content of the KC descriptions to capture student mastery levels on each KC, and predict not only\nthe overall correctness of the student code submission but also the actual code itself.\n3. We conduct extensive quantitative and qualitative evaluations to validate the effectiveness of KCGen-KT. On\nthe CodeWorkout dataset that contains real-world student code submissions to open-ended programming\nproblems, we show that KCGen-KT outperforms existing KT methods specifically developed for programming\nproblems. We also investigate the learning curves for these KCs and show that LLM-generated KCs have a\ncomparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. We also\nconduct a human evaluation to show that the KC tagging accuracy of our pipeline is reasonably accurate when\ncompared to that by human experts."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Knowledge Component Generation", "content": "Traditional methods for KC creation and tagging rely on human domain experts to identify the knowledge requirements\nfor solving a problem [2], a highly time-consuming process. Recent work has proposed automated approaches for\nKC discovery and tagging, employing data-driven approaches including the Q-matrix method [1]. In programming,\n[15] uses a rule-based parser to obtain ASTs with KCs identified at their lowest ontological level, [38] define KCs\nas nodes in an AST followed by a learning curve analysis to identify KCs students struggle with the most in Python"}, {"title": "2.2 Knowledge Tracing", "content": "There exists a wide body of work on KT [5] in the student modeling literature. The classic KT task aims to estimate\na student's mastery of KCs from their responses to past problems and use these estimates to predict their future\nperformance. Classic Bayesian knowledge tracing methods [31, 47] use latent binary-valued variables to represent\nstudent KC mastery. With the widespread adoption of neural networks, multiple deep learning-based KT methods were\nproposed with limited interpretability since student knowledge is modeled as hidden states in these networks. Most of\nthese methods use long short-term memory networks [12] or variants [34, 43], with other variants coupling them with\nmemory augmentation [50], graph neural networks [46], or attention networks [11, 29]. KT methods have been applied\nto many different educational domains, including programming [13, 40, 51]. Recent work has attempted to leverage\nLLMs to develop generative KT methods predicting exact student responses to programming problems [7, 9, 19].\nHowever, to the best of our knowledge, we are the first to present an LLM-based KT method for programming problems\nthat leverages the textual content of KC descriptions, modeling interpretable student mastery levels on each KC, for\nimproved KT performance."}, {"title": "3 Methodology", "content": "In this section, we detail our automated LLM-based approach to generate KCs for programming problems from the\nCodeWorkout [6] dataset, and then introduce KCGen-KT, a strong KT method leveraging the semantics of the generated\nKCs in modeling student learning for improved KT performance.\nFor KC generation, we use GPT-40 [27], an advanced proprietary LLM with strong reasoning and programming\ncapabilities. We generate KCs for a programming problem following five key steps: 1) solution generation, 2)\nconverting solution code to abstract syntax trees (ASTs), 3) generating KCs associated with each problem separately, 4)\ncluster KCs across all problems, and 5) summarize each cluster to obtain a description of each KC. We detail these\nsteps below.\nA programming problem may have multiple code solutions using different strategies that\nmay use different KCs. Therefore, we prompt GPT-40 to generate two unique solutions for each problem, thereby\nenhancing coverage within the (possibly large) solution space. These solutions, in addition to the problem statement,\ncan inform us of what programming skills are needed to solve the problem, hence identifying the associated KCs. We\nnote that writing code is inherently non-linear, with salient relationships and interactions between programming KCs.\nTherefore, to emphasize these structured relationships, we convert each solution code from a linear sequence of tokens\nto its equivalent AST representation using the code-ast [37] library.\nTo generate KCs for a problem, we prompt GPT-40 with the problem\nstatement and the two sample solution codes ASTs. Our approach of using ASTs instead of raw code is inspired by\nprior work generating KCs for math problems by including a step-by-step solution [28]; in our experiments, we also\nfound that this approach results in more precise KC descriptions. We use a simple prompt without extensive meta or\ncontextual information to keep our method generalizable across programming topics."}, {"title": "Clustering KCs for Semantic Equivalence", "content": "Since we can only prompt GPT-40 to generate KCs for each problem\nseparately due to its limited input context length, the resulting KCs aggregated over problems need to be post-processed.\nWe find many KC labels that are semantically equivalent but worded differently across problems. Therefore, we align\nthem by clustering KCs: We first compute the Sentence-BERT [35] embedding of the textual description of each KC,\nthen use HDBSCAN [24], a clustering algorithm, to cluster embedded KCs using cosine similarity as the distance\nfunction. We can tweak parameters in HDBSCAN such as minimum cluster size to control the granularity of KCs to an\nextent."}, {"title": "Labeling KC Clusters and Tagging Problems", "content": "We label each KC cluster by prompting GPT-40 to generate a single\ninformative KC name, summarizing all KCs in the cluster. We then perform a deduplication step among this new set\nof KC names by prompting GPT-40 to merge and label groups of semantically similar KC names, to obtain our final\nset of generated KCs across problems. We use the mapping between the initial set of GPT-40 generated KCs to their\nassigned clusters, and the mapping between clusters to their final summarized KC labels, to tag the final set of KCs to\neach problem."}, {"title": "3.1 Improving Knowledge Tracing via Generated KCS", "content": "We now detail KCGen-KT, an LLM-based KT method that explicitly leverages the semantics of KCs and explicitly\nmodels student mastery levels on each KC.\nFor open-ended programming problems, we define each student response to a problem as\n$x_t := (p_t, \\{w_i\\}, c_t, a_t)$, where $p_t$ is the textual statement of the problem, $\\{w_i\\}$ are the KCs associated with the problem,\n$c_t$ is the student code submission, and $a_t$ is the correctness of the submission; in most existing KT methods, $a_t$ is treated\nas binary-valued (correct/incorrect). Therefore, our goal is to estimate a student's mastery level of each KC given their\npast responses, $X_0,..., x_t$, and use this estimate to predict both 1) the overall binary-valued correctness $a_{t+1} \\in \\{0,1\\}$\nand 2) the open-ended code $c_{t+1}$ submitted by the student on their next attempted problem $p_{t+1}$. Following previous\nwork [40], $a_t = 1$ if the student-submitted code passes all test cases associated with the problem, and $a_t = 0$ otherwise.\nKCGen-KT leverages the KCs associated with a problem in two ways: 1) by improving the problem\nrepresentation using the semantic information of KCs, and 2) by improving the student representation by building an\ninterpretable student profile modeling student mastery levels on KCs.\nFollowing TIKTOC [19], we use an open-source LLM, Llama 3 [21], as the backbone to predict both the overall\ncorrectness and actual open-ended student code in a token-by-token manner, in a multi-task learning approach. KCGen-\nKT differs from OKT [19] by leveraging the content of the KCs, and from Code-DKT [40] by using text embedding\nmethods to embed the textual problem statement."}, {"title": "Student Knowledge on KCs", "content": "For each student, at each timestep $t$, KCGen-KT updates the student's 64-dimensional\nknowledge state vector $h_t \\in \\mathbb{R}^{64}$, through a long short-term memory (LSTM) [12] network as in DKT [34], given by\n$h_t = LSTM(h_{t-1}, P_t, C_t)$. This knowledge state $h_t$ is compressed into a $k$-dimensional mastery vector $m_t \\in [0, 1]^k$,\nwhere $k$ is the total number of KCs, through a linear layer with weights $W_m$ and bias $b_m$, followed by a sigmoid\nfunction to map the values of $m_t$ to be in the range of $[0, 1]$, given by $m_t = \\sigma(W_mh_t + b_m)$. Each dimension $j$ of $m_t$\ndenotes a student's mastery level in $[0, 1]$ on the $j$th unique KC, with larger values denoting higher mastery."}, {"title": "Predictions", "content": "To use LLMs to predict the student response to the next problem, we need to connect student KC knowledge\nwith the textual input space of LLMs. Therefore, following previous work [10, 20], we transform KC mastery levels\ninto soft text tokens, i.e.,"}, {"title": "Knowledge-Guided Response Prediction", "content": "We construct our LLM prompt for the next response prediction by including\nboth 1) the textual statement of the next problem and 2) student mastery levels on the KCs associated with the problem,\nas question: Pt. <KCs with student mastery levels>.\nTo predict the binary-valued correctness of the next student response, we average the hidden states of the last layer of\nLlama 3 that correspond to only the input (knowledge-guided prompt) to obtain a representation $r$, transformed for\ncorrectness prediction using a linear transformation matrix $W_p$ and a sigmoid function, given by $\\hat{a}_{t+1} = \\sigma(W_p. r)$. We\nminimize the binary cross entropy (BCE) loss (for one response):\n$L_{CorrPred} = a_{t+1}\\cdot log\\hat{a}_{t+1} + (1 - a_{t+1}) \\cdot log(1 - \\hat{a}_{t+1}).$\nTo predict student code, we feed the knowledge-guided prompt into Llama 3, which generates the predicted code $\\hat{c}$, in a\ntoken-by-token manner, using the loss:\n$L_{CodeGen} = \\sum_{j=1}^N -log P_\\Theta(\\hat{c}_{t+1}^{P, j} | \\{ \\hat{c}_{t+1}^{P, j'} \\}_{j'=1}^{j-1}),$"}, {"title": "Promoting Interpretability", "content": "To promote interpretability of the student KC knowledge parameters, we use a conjunctive\nmodel [23] and multiply individual student KC mastery levels to obtain an overall mastery level $\\hat{y}_{t+1} = \\prod_{k=1}^{K} m_i^{I(s)}$,\nwhere the indicator function $\\mathbb{I}(s)$ is 1 is the KC $s$ is associated with the problem, and 0 otherwise. We then minimize\nthe BCE loss between this overall KC mastery level for this problem and its binary-valued correctness,\n$L_{KCMastery} = a_{t+1}\\cdot log \\hat{y}_{t+1} + (1 - a_{t+1}) \\cdot log(1 - \\hat{y}_{t+1}).$\nThis loss regularizes the model to be monotonic, i.e., high knowledge on KCs corresponding to high probability of a\ncorrect response, thus promoting the interpretability of student knowledge parameters $m_i$."}, {"title": "Multi-task Learning Objective", "content": "Following previous work [7] showing multiple objectives are mutually beneficial to\neach other, our final multi-task training objective minimizes a combination of all three losses together, with a balancing\nparameter $\\lambda \\in [0, 1]$ controlling the importance of the losses,\n$L_{KCGen-KT} = \\lambda (L_{CodeGen} + L_{CorrPred}) + (1 - \\lambda)L_{KCMastery}$,\nwhere losses are averaged over code submissions by all students to all problems."}, {"title": "4 Quantitative Evaluation: KT Performance", "content": "We conduct extensive quantitative and qualitative evaluations to validate the effectiveness of KCGen-KT including\nevaluating KT performance, a learning curve analysis, and a human evaluation of KC tagging accuracy.\nTo validate the effectiveness of KCGen-KT, we use the CodeWorkout [6] dataset, a large, publicly\navailable real-world programming education dataset previously used in the Second CSEDM Data Challenge [3].\nCodeWorkout contains actual open-ended code submissions from real students, collected from an introductory Java\nprogramming course, together with problem textual statements and human-written KC tags (estimated programming\nconcepts) on each problem.\nIn total, there are 246 students attempting 50 problems covering various programming concepts including conditionals,\nand loops, among others. Following prior work [40], we only analyze students' first submissions to each problem,\nleading to a total of 10, 834 code submissions.\nFor the binary-valued correctness prediction task, following [34, 40], we use standard metrics such as AUC,\nF1 score, and accuracy. For the student code prediction task, following [19], we measure the similarity between"}, {"title": "5 Learning Curve Analysis", "content": "A common method to assess the quality of KCs is examining how well they match cognitive theory; the expected\npattern on the KCs should follow the power law of practice, which states that the number of errors should decrease"}, {"title": "6 Human Evaluation", "content": "We perform a human evaluation to test one small but concrete part of KCGen-KT, which is whether it can accurately\ntag programming problems with KCs. For this purpose, we simply check how well KCGen-KT's KC tags align with\nthose provided by human experts. Two authors of the paper, with experience in teaching programming at the university\nlevel, serve as human annotators. For 10 randomly selected programming problems, we ask them to select relevant KCs\nwhere mastery is necessary for students to correctly solve the problem from the list of 11 KCs generated by KCGen-KT.\nWe first group problems into five major topics, namely, 1) math, 2) string, 3) boolean, 4) array, and 5) functions, and\nthen randomly select two problems from each topic. Annotators are given the problem and its two sample solution\ncodes, along with the list of 11 KCGen-KT generated KCs. We note that this problem tagging setup is similar to the\nLLM input during the KC generation step of KCGen-KT.\nFor each problem p, we have a set A of KCs tagged by KCGen-KT and a set B of KCs tagged by human\nannotators. We employ Intersection over Union (IoU) [8], also known as the Jaccard index [18], to measure the\nsimilarity between these two sets, given by\n$IoU (A_P, B_P) = \\frac{|A_P \\cap B_P|}{|A_P \\cup B_P|}$,\nand then compare the overall IoU averaged across all problems. We measure the F1 score between KCs tagged by\nKCGen-KT and human annotators: since whether each KC is tagged for each problem is binary-valued, we can treat\nthe tagging problem as binary classification, with 1 denoting a KC is tagged for a problem and 0 otherwise. We\nthen compare the resulting 11-dimensional binary vector of KCGen-KT predictions against the \"ground-truth\" human\nannotator tags, using the F1 score metric, averaged across all problems. We also report the inter-rater reliability (IRR)\nbetween KCs tagged by KCGen-KT and human annotators using Cohen's kappa [4].\nThe mean IOU and F1 score between KCGen-KT-tagged KCs and human annotator-tagged KCs, averaged over\nthe two human annotators, are 0.652 and 0.783. For reference, the IOU, F1 score, and Cohen's kappa IRR, between the\ntwo human annotators are 0.887, 0.936, and 0.842, respectively, showing high agreement between the annotators. We\ncan conclude that LLM-based per-problem KC tagging is reasonably accurate but not at the level of human experts\nyet. We also note that the high IRR between human annotators is likely due to having a fixed set of KCs to select\nfrom. Future work should study whether human experts can identify KCs missed by the LLM and develop human-AI\ncollaboration approaches for KC identification."}, {"title": "7 Conclusions and Future Work", "content": "In this paper, we presented a fully automated, LLM-based pipeline for KC generation and tagging for open-ended\nprogramming problems. We also developed an LLM-based knowledge tracing (KT) framework, KCGen-KT, to\nleverage these LLM-generated KCs. KCGen-KT leverages the textual content of KC descriptions to capture student\nmastery levels on each KC, to both predict overall correctness as well as the student code submission. Through\nextensive experiments, we showed that KCGen-KT outperforms existing state-of-the-art KT methods on real-world\nprogramming problems. We investigated the learning curves of generated KCs and showed that LLM-generated KCs\nhave a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. Further, we\nconducted a human evaluation assessing the tagging accuracy of KCs to problems by KCGen-KT, finding the tagging to\nbe reasonably accurate when compared to that by human domain experts.\nThere are many avenues for future work. First, we can obtain LLM-generated correctness labels at a KC level for student\ncode submissions for fine-grained student modeling. Second, we can explore using training objectives to encourage\nKC learning curves to follow the power law of practice [41, 44]. Third, we can perform a human evaluation assessing\nthe quality of the generated KCs on aspects such as relevance and specificity. Fourth, KCGen-KT could incorporate\nfairness regularization into the training objective [48] to explicitly control for fairness across students from different\ndemographic groups. Fifth, we can evaluate our KCGen-KT framework on problems from other domains including\ndialogues [39], math [28], and science [25]."}]}