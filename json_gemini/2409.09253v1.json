{"title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator", "authors": ["Jun Yin", "Zhengxin Zeng", "Mingzheng Li", "Hao Yan", "Chaozhuo Li", "Weihao Han", "Jianjin Zhang", "Ruochen Liu", "Allen Sun", "Denvy Deng", "Feng Sun", "Qi Zhang", "Shirui Pan", "Senzhang Wang"], "abstract": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS)\u00b9, the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simul- taneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin- tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and col- laborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM- based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.", "sections": [{"title": "1 Introduction", "content": "With the network information proliferating exponentially, recom- mender systems (RSs) which discover user interested contents have become essential components of online service platforms [3, 6, 8, 33]. Since user preference evolves over time, sequential recommender systems have attracted great research attention from both academia and industry [24], due to the splendid capability in capturing col- laborative patterns of items in user behavior sequences. In current research literature, sequential recommender systems introduce var- ious deep neural network architectures, including convolutional neural networks (CNNs) [38, 46], recurrent neural networks (RNNs) [26, 37], graph neural networks (GNNs) [44, 45], and Transform- ers [6, 24, 36], to extract collaborative informative within the user historical behaviors represented by item sequences. For further improvement on recommendation performance, the pre-trained language models (PLMs) are adopted to capture the semantic in- formation within the item textual features [14, 15, 28]. Recently, the emergency of large language models (LLMs) pre-trained on large-scale natural language (NL) corpus, such as GPT [32], T5 [9], and LlaMA [40], have revolutionized the recommender sys- tems community, due to the unprecedented capability in semantic understanding and logical reasoning [34, 50, 53].\nThe cornerstone of developing LLM-based recommender sys- tems lies in the integration between the two modalities of item that serves as the fundamental in sequential recommendation, i.e., the collaborative information reflected by historical interaction and the semantic information implicating in item textual feature. Existing efforts to align the collaborative and semantic information in se- quential recommendation can be divided into two main approaches: (i) The first approach verbalizes the historical interaction into tex- tual sequence (e.g., concatenate the item titles and descriptions) and then instructs LLMs on sequential recommendation through natural language prompt [1, 16, 18]. Such an content-based ap- proach further increases the computation expenditure due to the greatly extended input sequence and fails to guarantee in-domain recommendation results [53]. (ii) The second approach incorpo- rates LLMs and RSs through the static index mapping\u00b2. The com- pact item indices can be considered as special tokens and used to extend the LLM vocabulary, aiming to mitigate the computation overhead and guarantee the in-domain legitimate recommenda- tion results [8, 18, 34, 50, 53]. To be more specific, pseudo index based methods emulate traditional ID-based RS and adopt ID-alike words to represent users and items (e.g., <user_100> and <item_128>) [8, 18, 53]. To bring the best of the content-based and the pseudo index based methods, semantic index based methods first em- bed the item/user related content into continuous vector, and then introduce discrete indexing technique which quantizes continu- ous embedding into discrete indices [21, 34, 50]. Notably, different from pseudo ID, the discrete semantic ID is generated based on the item/user representation similarity, which implies that <item_1> is more similar to <item_2> than <item_10>.\nDespite remarkable achievements have been developed, such a static index mechanism based integration paradigm is still con- fronted with the fundamental discrepancy between the semantic and collaborative knowledge. Throughout the static index based recommendation process, each item index is completely domi- nated by the corresponding textual content and the textual encoder [22, 30, 34, 50]. First, the textual content itself implicates semantic inductive bias, probably misleading the downstream RSs. For ex- ample, the film Transformers (July 3, 2007) and the teaching video Transformer Careful Elaboration (October 28, 2021) are highly simi- lar in terms of textual content, yet fractionally overlap within the interaction records. Second, distribution mismatch between the textual encoder embedding and the RS representation is overlooked [19, 21]. Assuming that a shallow textual encoder is employed, the entire RS performance will be constrained by the inferior encod- ing ability. Last but not least, existing static integration paradigms fail to exploit high-order user-item interaction patterns during the recommendation-oriented fine-tune [22, 30, 34, 50]. Only the item co-occurrence pattern within user historical behavior is underlined, high-order interaction patterns, such as the user co-purchase pat- tern and the user preference pattern, remain to be fully utilized.\nIn this paper, we for the first time investigate the generative RSs based on dynamic semantic index mechanism, aiming to address the above problems simultaneously. As illustrated in Figure 1, during the recommendation-oriented fine-tuning, the twin-tower semantic token generator is cooperatively optimized with the recommender module. Note that within the dynamic semantic index mechanism, the textual encoder module and the recommender module share the same LLM backbone, seamlessly aligning the distribution of the textual embedding and the recommender representation, eventually fusing the semantic and collaborative knowledge into a monolithic model. Moreover, the twin-tower semantic token generator contains two homogeneous branches of discrete index module, one of which for the item index learning and the other for the user counterpart. By reasonably aggregating the user and item semantic indices, the recommender module is able to delve into the exploition of high- order user-item interaction patterns. However, it is non-trivial to design the dynamic semantic index based generative recommender system due to the following challenges.\nUnder-trained User/Item Semantic Token. Since LLM-based the generative RSs extend the LLM vocabulary with the semantic tokens of users and items, the semantic tokens can be considered as basic words of a brand-new language which specifically describes the recommendation field [34, 39, 43]. However, the novel language is unfamiliar to the LLM backbone which is pre-trained on large- scale NL corpus. Therefore, there exists considerable discrepancy between the under-trained semantic tokens and the well pre-trained NL tokens. How to instruct LLMs in understanding the under- trained semantic tokens as well as the NL tokens is challenging.\nImplicit High-order User-Item Interaction Pattern. As to the sequential recommendation task, the user historical behav- ior is represented as an interactive item sequence (e.g., [<item_1>, <item_3>, ..., <item_34>]) [6, 24, 36]. Accordingly, the item co-occurrence pattern is explicit and effortless to capture. Nevertheless, the high- order interaction patterns that implicitly contained in user historical behavior are overlooked by existing methods [22, 30, 34, 50, 53]. For example, the user co-purchase pattern (e.g., <user_1> and <user_2>"}, {"title": "3 Background", "content": "In this section, we briefly introduce the sequential recommendation task and the large language model. The key notations are summa- rized in Appendix A for clarity.\nSequential Recommendation. By analyzing the user histori- cal interactions, sequential recommendation aims to identify user preference and predict the suitable item that would be engaged with [6, 24, 26, 36-38, 44-46]. Given a chronologically organized sequence of interacted items s = {01, 02, , 1}, the objective func- tion of sequential recommender system f is to maximize the corre- sponding likelihood defined as follows,\nlog p (01, 02,\u00b7\u00b7\u00b7, 1; f) = \n1-1\nj=1\nlog f (vj+101, 02, \u2026\u2026\u2026, vj). (1)\nFinally, the trained sequential recommender system f* is optimized by maximizing the likelihood function over all the N training in- teraction sequences, that is,\nf* = arg max log p(si; f)\nf i=1\n= arg max Ns-1 log f (vj+101, 02, ..., vj).\nf i=1 j=1 (2)\nLarge Language Model. Transformer-based models with bil- lions learnable parameters trained on large scale corpora [9, 32, 40], i.e., large language models (LLMs), have presented astonishing ca- pabilities in natural language understanding and logical reasoning based on learned knowledge. The mainstream LLMs mostly belong to the decoder-only architecture with superior generative ability [32], which consists of a token embedding layer, a single decoder module, and a correlated tokenizer. Given a natural language sen- tence s = {W1, W2,...}, the tokenizer first converts the sentence into sequence of token index ti whose corresponding word wi is included in the LLM vocabulary, formulated as follows,\nt = {t1, t2,...} = Tokenizer(s)\n= {Tokenizer(w\u2081), Tokenizer(w2),...}. (3)"}, {"title": "4 Methodology", "content": "In this section, we introduce the Twin-Tower Dynamic Semantic (TTDS) Recommender in detail. Holistically, as shown in Figure 2, the TTDS recommender is characterized by the proposed dy- namic knowledge fusion framework, where the semantic knowl- edge within textual content and the collaborative knowledge within historical behavior are sufficiently integrated. Specifically, a twin- tower semantic token generator takes in charge of hierarchically generating semantic index for both item and user, based on the cor- responding LLM textual representation. A dual-modality variational auto-encoder (DM-VAE) is devised to eliminate great discrepancy between the newly-introduced semantic tokens and the well pre- trained NL tokens. Moreover, during the recommendation-oriented fine-tuning, a series of customized tasks are specially designed for modeling the implicit high-order user-item interaction patterns.\n4.1 Problem Formulation\nIn this paper, we focus on sequential recommendation task whose target is predicting suitable items for users according to their his- torical behavior. For each user Uj, the historical behavior is mainly represented by the sequence of interactive items {Ik}, such as Sj = [17, 13,\u00b7\u00b7\u00b7,I26]. Additionally, we use CU, C to denote the related textual content of user Uj and item Ik, respectively. For the item branch, the textual content mostly includes the item title, the detailed description, and other auxiliaries (e.g., brand and category). Similarly, the user-related textual content usually consists of the recent comment, the search query, and the holistic user profile.\n4.2 TTDS Recommender\n4.2.1 Dynamic Knowledge Fusion Framework. To sufficiently integrate the semantic knowledge and the collaborative knowledge, we propose the dynamic knowledge fusion framework for TTDS recommender. Within this framework, the TTDS recommender system consists of a twin-tower semantic token generator and a LLM-based recommender backbone. The former aims to discretize the continuous representation of user/item into corresponding se- mantic index. The latter is not only in charge of converting the textual content of user/item into continuous representation, but"}, {"title": "4.2.2 Hierarchical Index Mechanism", "content": "4, Embed_Token(t) = One_Hot(t). E, (4)\nOne_Hot(t) \u2208 {0, 1, \u00b7\u00b7\u00b7, |V| \u2013 1}L\u00d7|V|, E \u2208 R|V|\u00d7d,\nwhere E is the token embedding table, L is the token sequence length, V is the LLM vocabulary, and d is the latent space dimension. Eventually, the continuous embedding h forward through the LLM decoder module. In addition, for the LLM aiming at generation task, a language model head is appended to transform the final representation h' back into the token index space, as follows,\nh' = Decoder(h), t' = LM_Head(h'). (5)\nAfterwards, the tokenizer is able to translate the newly generated token index sequence into natural language sentence by an inverse table look-up operation as opposed to Formula 4."}, {"title": "4.2.3 Dual-Modality Variational Auto-Encoder", "content": "2k + \u03b2 L (15)\n2k is the weighted hyper-parameter."}, {"title": "4.2.4 High-Order User-Item Interaction Pattern", "content": "to the sequential recommendation task. First", "follows": "You are an recommendation expert. The item Sid11 has been clicked by the following users: SidU4, SidU5, ..., SidU23. Can you predict another user who is interested in this item?\nFurthermore, we also design fine-tuning tasks based on the user recent comments and search queries. The LLM backbone is prompted with the user historical interaction sequence and the human instruction is to deduce the user comment or the search query, which is able to reflect the user preference to a certain degree. Notably, the designed fine-tuning tasks oriented for the high-order user-item interaction patterns can also be formatted as language generation task in a sequence-to-sequence manner, with the op- timization objective defined in Formula 9. The overall objective function of TTDS recommender system is defined as follows,"}]}