{"title": "Low-Confidence Gold: Refining Low-Confidence Samples for Efficient\nInstruction Tuning", "authors": ["Hongyi Cai", "Jie Li", "Wenzhen Dong"], "abstract": "The effectiveness of instruction fine-tuning\nfor Large Language Models is fundamentally\nconstrained by the quality and efficiency of\ntraining datasets. This work introduces Low-\nConfidence Gold (LCG), a novel filtering\nframework that employs centroid-based cluster-\ning and confidence-guided selection for iden-\ntifying valuable instruction pairs. Through a\nsemi-supervised approach using a lightweight\nclassifier trained on representative samples,\nLCG curates high-quality subsets while pre-\nserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-\nfiltered subsets of 6K samples achieve supe-\nrior performance compared to existing methods,\nwith substantial improvements on MT-bench\nand consistent gains across comprehensive eval-\nuation metrics. The framework's efficacy while\nmaintaining model performance establishes a\npromising direction for efficient instruction tun-\ning.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) has been trained\nto follow instructions by specific supervised re-\nsponse data after pre-training stage. Many instruc-\ntion finetuning (IFT) (Taori et al., 2023) datasets\nemerge to realize various downstream tasks, for\nexample: mathematic calculation, sentence anal-\nsis, haiku writing and etc, aiming to strengthen\nthe ability of LLMs in instruction following. To\nsave vast human costs for data annotation, most\nof studies introduce other teacher LLMs (e.g.\ntext-davinci-003 (Brown et al., 2020)) to align\nthe best instructions with corresponding responses.\nHowever, IFT datasets (e.g. Alpaca_52k (Taori\net al., 2023), magpie (Xu et al., 2024)) suffer from\nmisleading content and poor quality, resulting in\nthe bottleneck of post-training performance, even\nthough teacher models replenish the missing parts\nof context and instruction pairs. This highlights the\nneed for effective data filtering methods that iden-\ntify high-quality instruction subsets while reducing\nfine-tuning time and computational costs.\nAlpagasus (Chen et al., 2024) proposed a\nmodel-based approach that introduces proprietary\nLLMs to score data quality in multiple facets, re-\nplacing human annotation by taking advantage of\nthe automated pipeline. However, this leads to\ndatasets that are likely biased by the preference for\nredundant and limited responses (Panickssery et al.,\n2024), which potentially deteriorates the diversity\nof the original data. Ge et al., 2024 emphasizes\nthe necessity of diversity and therefore proposed\nclustering and ranking to select subsets of data.\nFurther, Superfiltering (Li et al., 2024) gains more\ninsights in small open-source LLM that scores the\ninstruction following ability of Alpaca_52k. Al-\nthough the instruction score provides an efficient\nand simple criterion for data selection, it does not\nconsistently correlate with both the quality and di-\nversity of data. Consequently, improvements in\nperformance may not always be guaranteed.\nTo address these challenges, we propose a novel\ndata filtering framework, Low-Confidence Gold\n(LCG) for efficient instruction tuning that signif-"}, {"title": "2 Preliminaries", "content": "Given the Alpaca_52k dataset D = {(xi, Yi)}=1\nwhere N = 52,000, we first cluster instructions\ninto K semantic groups using K-means. Let\n(xi) \u2208 Rd denote the embedding vector of in-\nstruction xi. The clustering objective minimizes:\nmin \u03a3\u03a3 ||(xi) \u2013 \u03bc\u03ba||2"}, {"title": "3 Methodology", "content": "Instruction filtering demands a dual-focus mech-\nanism that intrinsically balances data quality\nand diversity. Traditional supervised methods\nface inherent scalability limitations as manual an-\nnotation becomes prohibitively expensive for large-\nscale instruction datasets (Liu et al., 2022; Longpre\net al., 2023; Liu et al., 2023). Meanwhile, it is\ndifficult to identify suitable and challenging data\nfor LLMs training without introducing proprietary\nLLMs or labors. Our semi-supervised framework\naddresses these limitations through pseudo-label\nrefinement and early-stopped confidence detection,\ncreating dynamic selection boundaries aligned with\nlanguage model learning dynamics.\nCluster-centric pseudo-labeling addresses\ndata distribution challenges in instruction tun-\ning. Traditional sampling methods often struggle\nto balance between common and rare instruction\npatterns, leading to either over-representation of\nfrequent cases or loss of valuable rare examples.\nWe create semantic clustering anchors that natu-\nrally preserve the diversity of instruction patterns.\nBy sampling 3% of data points nearest to cluster\ncentroids, we ensure each semantic category con-\ntributes meaningful examples while maintaining\nthe inherent data distribution characteristics.\nEarly-stopped classifier training induces un-\ncertainty to identify high-quality samples. Lim-"}, {"title": "3.1 Motivation", "content": "Instruction filtering demands a dual-focus mech-\nanism that intrinsically balances data quality\nand diversity. Traditional supervised methods\nface inherent scalability limitations as manual an-\nnotation becomes prohibitively expensive for large-\nscale instruction datasets (Liu et al., 2022; Longpre\net al., 2023; Liu et al., 2023). Meanwhile, it is\ndifficult to identify suitable and challenging data\nfor LLMs training without introducing proprietary\nLLMs or labors. Our semi-supervised framework\naddresses these limitations through pseudo-label\nrefinement and early-stopped confidence detection,\ncreating dynamic selection boundaries aligned with\nlanguage model learning dynamics.\nCluster-centric pseudo-labeling addresses\ndata distribution challenges in instruction tun-\ning. Traditional sampling methods often struggle\nto balance between common and rare instruction\npatterns, leading to either over-representation of\nfrequent cases or loss of valuable rare examples.\nWe create semantic clustering anchors that natu-\nrally preserve the diversity of instruction patterns.\nBy sampling 3% of data points nearest to cluster\ncentroids, we ensure each semantic category con-\ntributes meaningful examples while maintaining\nthe inherent data distribution characteristics.\nEarly-stopped classifier training induces un-\ncertainty to identify high-quality samples. Lim-"}, {"title": "3.2 Centroid Coreset Selection for\nPseudolabels", "content": "In the initial step of our approach, we select a core-\nset from the whole corpus to identify pseudolabels\nby the K-means algorithm, which effectively de-\ntermine each semantic clusters. Given a dataset\nof instruction pairs D = {(xi, yi)}=1, we first\nencode each instruction x\u2081 into a dense vector rep-\nresentation using MiniLM (Wang et al., 2022):\nh\u2081 = AvgPool(MiniLM(xi)) \u2208 R384\nThis geometric progression ensures proportional\ncoverage of both frequent and rare instruction pat-\nterns. Cluster centroids {c;}}=1 are computed via:\n1\n= \u03a3\n|Cj|\nxi ECj\nwhere Cj denotes the set of samples assigned to\ncluster j. Centroid-proximal samples are selected\nas high-confidence candidates:\nDcore = {xi|||hi \u2013 Cj(i)||2 < }"}, {"title": "3.3 Low-Confidence Gold: Calibrating with\nLow-confidence samples to select data", "content": "After determining pseudolabels based on clusters,\nthose annotations can be served for classification\ntraining. Specifically, we train a multi-class classi-\nfier on the core samples Dcore. The model architec-\nture consists of:\nfe(x) = Softmax(W2\u00b7GELU(W1hi+b1)+b2)\nwhere W1 \u2208 R384\u00d7768, W2 \u2208 R768 are learn-\nable parameters, and GELU denotes the Gaussian\nError Linear Unit activation. The model optimizes\ncross-entropy loss:\nL(0)\n1\nDcore\nk\n\u03a3\u03a3\u2161(Yi = j)\n(xi,Yi) j=1\n\u00b7 log po (y = j|xi)\nTraining terminates at epoch T = 3 since we aim\nto keep the model in an early-stopped stage so that\nthey would not overfit to the centroid subset data.\nAfter training, we rank the confidence distribution\ncalculated from softmax function and select the top\nK most uncertained data in each cluster."}, {"title": "4 Experiments", "content": "In this section, we utilize LCG to filter\nAlpaca_52k dataset into 6k and evaluate the sub-\nset by fine-tuning in 2 open-source LLMs: 1)"}, {"title": "6 Limitation", "content": "Our work introduces a semi-supervised training\nparadigm to curate a subset of data for instruction\ntuning based on confidence score. However there\nstill exists several challenges: 1) Even though clas-\nsifiers are tiny and spending low computational\nresources to train, it still takes time and efforts to\ninitially select data with annotated pseudolabels.\n2) It is likely to be hindered by the original biases\nand tasks of the dataset, which might still cause\ninefficiency after selection."}, {"title": "A Extended Analysis of Semi-Supervised\nModel Configurations", "content": "The confidence distribution patterns of our Multi-\nnomialNB baseline, as visualized in Fig. 3, re-\nveal fundamentally different characteristics com-\npared to deep learning architectures. The histogram\ndemonstrates remarkable uniformity across confi-\ndence intervals (0.0-1.0 with 0.1 increments), show-\ning no significant concentration in specific confi-\ndence ranges. This equilibrium phenomenon stems\nfrom the model's inherent probabilistic nature and\nlinear decision boundaries, Zwhich produce well-\ncalibrated confidence estimates despite its simplic-\nity."}, {"title": "A.1 MultinomialNB Implementation", "content": "The confidence distribution patterns of our Multi-\nomialNB baseline, as visualized in Fig. 3, re-\nveal fundamentally different characteristics com-\npared to deep learning architectures. The histogram\ndemonstrates remarkable uniformity across confi-\ndence intervals (0.0-1.0 with 0.1 increments), show-\ning no significant concentration in specific confi-\ndence ranges. This equilibrium phenomenon stems\nfrom the model's inherent probabilistic nature and\nlinear decision boundaries, Zwhich produce well-\ncalibrated confidence estimates despite its simplic-\nity."}, {"title": "A.2 DistilBERT comparative experiment on\nlearning rate", "content": "Our DistilBERT implementation employed a sys-\ntematic exploration of learning rate hyperparame-\nters 1e-4, 1e-5, 1e-6 within the following experi-\nmental framework:\n1. Architecture: DistilBERT-base-uncased (66M\nparameters) with custom classification head.\n2. Optimization: Adam optimizer.\n3. Training regime: 3-epoch constraint to pre-\nvent overfitting in low-data scenarios.\n4. Data alignment: Identical train/test splits\n(stratified sampling) as MultinomialNB for\ndirect comparability.\nThe empirical results (shown in Fig. 4) demon-\nstrate non-monotonic performance relationships\nwith learning rate scaling. Peak accuracy (62%)\nemerged at le-5, while extreme values at both ends\n(1e-4: 36%, 1e-6: 28%) showed substantial perfor-\nmance degradation. This U-shaped accuracy curve\nsuggests existence of optimal learning rate basins\nin semi-supervised BERT fine-tuning.\nThe model exhibited distinct confidence distri-\nbution characteristics at the le-6 learning rate, with\npredictions predominantly clustered in the low-\nconfidence range (0-0.2). However, as revealed\nin Figure 2, comparative analysis across learning\nrates demonstrated minimal performance variation,\nshowing only marginal improvements that corre-\nlated with accuracy increments."}]}