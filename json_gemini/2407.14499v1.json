{"title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery", "authors": ["Sukrut Rao", "Sweta Mahajan", "Moritz B\u00f6hle", "Bernt Schiele"], "abstract": "Concept Bottleneck Models (CBMs) have recently been proposed to address the 'black-box' problem of deep neural networks, by first mapping images to a human-understandable concept space and then linearly combining concepts for classification. Such models typically require first coming up with a set of concepts relevant to the task and then aligning the representations of a feature extractor to map to these concepts. However, even with powerful foundational feature extractors like CLIP, there are no guarantees that the specified concepts are detectable. In this work, we leverage recent advances in mechanistic interpretability and propose a novel CBM approach called Discover-then-Name-CBM (DN-CBM) that inverts the typical paradigm: instead of pre-selecting concepts based on the downstream classification task, we use sparse autoencoders to first discover concepts learnt by the model, and then name them and train linear probes for classification. Our concept extraction strategy is efficient, since it is agnostic to the downstream task, and uses concepts already known to the model. We perform a comprehensive evaluation across multiple datasets and CLIP architectures and show that our method yields semantically meaningful concepts, assigns appropriate names to them that make them easy to interpret, and yields performant and interpretable CBMs. Code available at https://github.com/neuroexplicit-saar/discover-then-name.", "sections": [{"title": "Introduction", "content": "Deep neural networks have been immensely successful for a variety of tasks, yet their 'black-box' nature poses a risk for their use in safety-critical applications. While attribution methods [5, 47, 52] have popularly been used to explain such models post-hoc, they have been shown to often provide explanations unfaithful to the model [2,3,46]. To address this, inherently interpretable models have been proposed [8, 12,30] that constrain the model to yield more faithful and human-understandable explanations in the form of heatmaps, concepts, or prototypes."}, {"title": "Related Work", "content": "Concept-based Explanations (e.g. [1, 28, 30, 37]) aim to express a model's decision via human-understandable concepts. Unlike popularly used post-hoc attribution heatmaps (e.g. [5, 32, 47, 52, 54, 55]) that only inform which regions in the input is influential for the decision, such methods attempt to also answer what high-level concepts are important for the model [1]. In our work, we propose a pipeline for automatically extracting and naming such concepts from CLIP and using them to build interpretable models.\nConcept Discovery [6, 9, 14, 19-22, 37-39, 60] methods have been proposed to better understand models by discovering and extracting semantically meaningful concepts learnt by them. They typically focus on explaining the function of neurons in a model [6,20,37,39], or on discovering features present in an input, and have been shown to be useful for diagnosing model failures [20]. However, these methods assign concepts to individual neurons, which may often be polysemantic and not decodable to human-understandable concepts [18]. Recently, [9] showed that sparse autoencoders (SAEs) can be effective to address the polysemanticity and superposition problem in deep networks [18] and extract mono-semantic concepts from language models (cf. [14]). We extend the setup of [9] to vision and use sparse autoencoders to automatically extract concepts learnt by CLIP.\nExplanations using Language [13, 15, 24, 25, 35, 37, 39, 56, 61] have become popular to express a model's learnt representations [15,37,39] in an easily human-interpretable manner. To decode concepts to language, such methods typically use a large-language model (LLM) such as GPT-3 [10] and learn a mapping from vision features to the LLM input. More recently, [37,39] leverage CLIP, by aligning vision features of the model being explained to the representation space"}, {"title": "Constructing CBMs via Automated Concept Discovery", "content": "In this section, we describe our approach which consists of three stages: discovering the concepts the CLIP model has learnt via a sparse autoencoder (Sec. 3.1), naming those concepts in natural language by leveraging the CLIP text embeddings from a large vocabulary, (Sec. 3.2), and, lastly, training an interpretable concept bottleneck model (CBM) based on the discovered concepts (Sec. 3.3)."}, {"title": "Extracting Concepts Learned by the Model", "content": "To discover the concepts learned by the model, we adapt the sparse autoencoder (SAE) approach as described by [9]. Specifically, we aim to discover concepts by representing the CLIP features in a high-dimensional, but very sparsely activating space. For language models, this has been shown to yield representations in which individual neurons (dimensions) are more easily interpretable [9].\nThe Sparse Autoencoders (SAEs) proposed by [9] consist of a linear encoder f(.) with weights $W_E \\in \\mathbb{R}^{d \\times h}$, a ReLU non-linearity $\\varphi$, and a linear decoder g(.) with weights $W_D \\in \\mathbb{R}^{h \\times d}$. For a given input a, the SAE computes:\n$$SAE(a) = (g \\circ \\varphi \\circ f)(a) = W_D^\\top\\varphi(W_E a).\\qquad(1)$$"}, {"title": "Automated Concept Naming", "content": "Once we trained the SAE, we aim to automatically name the individual feature dimensions in the hidden representation of the SAE. For this, we propose using a large vocabulary of English words, say $V=\\{v_1, v_2, ...\\}$, which we embed via the CLIP text encoder $T$ to obtain word embeddings $E=\\{e_1, e_2, ...\\}$.\nTo name the SAE's hidden features, we propose to leverage the fact that each of the SAE neurons c is assigned a specific dictionary vector $p_c$, corresponding to a column of the decoder weight matrix:\n$$p_c = [W_D]_c \\in \\mathbb{R}^d.\\qquad(3)$$\nIf the SAE indeed succeeds to decompose image representations given by CLIP into individual concepts, we expect the $p_c$ to resemble the embeddings of particular words that CLIP has learnt to expect in a corresponding image caption. Hence, to name the 'concept' neuron c of the SAE, we propose to assign it the word $s_c$ of the closest text embedding in $E$:\n$$s_c = \\underset{v \\in V}{\\text{arg min }} [\\cos (p_c, T(v))].\\qquad(4)$$\nNote that this setting is equivalent to using the SAE to reconstruct a CLIP feature when only the concept to be named is present. As CLIP was trained to optimise cosine similarities between text and image embeddings, using the cosine similarity to assign names to concept nodes is a natural choice in this context."}, {"title": "Constructing Concept Bottleneck Models", "content": "Thus far, we trained an SAE to obtain sparse representations (Sec. 3.1), and named individual \u2018neurons' by leveraging the similarity between dictionary vectors $p_c$ to word embeddings obtained via CLIP's text encoder T (Sec. 3.2).\nSuch a sparse decomposition into named 'concepts' constitutes the ideal starting point for constructing interpretable Concept Bottleneck Models (CBMS) [30,36,59]: for a given labelled dataset $D_{probe}$, we can now train a linear transformation h(.) on the SAE's sparse concept activations, yielding our CBM t(\u00b7):\n$$t(x_i) = (h \\circ \\varphi \\circ f \\circ I)(x_i). \\qquad (5)$$\nHere, $x_i$ denotes an image from the probe dataset. The probe is trained using the cross-entropy loss, and to increase the interpretability of the resulting CBM classifier, we additionally apply a sparsity loss to the probe weights:\n$$\\mathcal{L}_{probe} (x_i) = CE (t(x_i), y_i) + \\lambda_2||\\omega||_1\\qquad(6)$$\nwhere, $\\lambda_2$ is a hyperparameter, $y_i$ the ground truth label of $x$ in the probe dataset, and $w$ denotes the parameters of the linear probe.\nImportantly, note that the feature extractor, the dataset used for concept discovery, and the vocabulary used for naming can be freely chosen. As such, our approach is likely to benefit from advances in any of these directions."}, {"title": "Evaluation of Concept Discovery and Naming", "content": "In this section, we evaluate the effectiveness of using SAEs to discover and name concepts in CLIP vision encoders; see Sec. 5 for an evaluation of CBMs built on the SAEs. In Sec. 4.1, we first evaluate the accuracy and task agnosticity of the discovered concepts qualitatively and quantitatively, in Sec. 4.2, we discuss the impact of the vocabulary V towards the granularity of concept names, and in Sec. 4.3, we evaluate how well semantically similar concepts group together.\nSetup. We use a CLIP [44] ResNet-50 [23] vision encoder for extracting features, and use the corresponding text encoder for labelling the extracted concepts. For additional results using CLIP ViT-B/16 and ViT-L/14 [17], see Appendices C and D. To extract concepts, we follow a setup similar to [9] and train SAEs using the CC3M dataset [53]. Following [37], we use the set of 20k most frequent English words as the vocabulary V (Eq. (4)). For details, see Appendix B.1."}, {"title": "Task-Agnosticity and Accuracy of Concepts", "content": "In this section, we qualitatively and quantitatively evaluate the extracted and named concepts for semantic consistency and accuracy.\nQualitative. To showcase the promise of our proposed approach, in Fig. 3 we visualize the top activating images across four datasets for various concepts that"}, {"title": "Conclusion", "content": "In this work, we proposed Discover-then-Name CBM (DN-CBM), a novel CBM approach that uses sparse autoencoders to discover and automatically name concepts learnt by CLIP, and then use the learnt concept representations as a concept bottleneck and train linear layers for classification. We find that this simple approach is surprisingly effective at yielding semantically consistent concepts with appropriate names. Further, we find despite being task-agnostic, i.e. only extracting and naming concepts once, our approach can yield performant and interpretable CBMs across a variety of downstream datasets. Our results further corroborate the promise of sparse autoencoders for concept discovery. Training a more 'foundational' sparse autoencoder with a much larger dataset (e.g. at CLIP scale) and concept space dimensionality (with hundreds of thousands or millions of concepts) to obtain even more general-purpose CBMs, particularly for fine-grained classification, would be a fruitful area for future research."}, {"title": "Limitations and Broader Impact", "content": "In this work, we proposed DN-CBM, as task-agnostic approach to discovering concepts from CLIP [S16] using sparse autoencoders (SAEs) and then using them to construct a concept bottleneck model across downstream classification tasks. We find that this simple approach shows promise that using a higher dimensional concept space and training SAEs on much larger datasets than CC3M [S20] would provide a richer and more diverse concept representation. We already find that concept discovery from our SAEs using a relatively small dataset like CC3M can lead to performant and interpretable CBMs for datasets such as Places365 [S23], leveraging concepts related to objects, colours, shapes, locations, and associations to reach decisions in an interpretable manner. However, these constitute relatively coarse concepts, and given the nature and size of the CC3M dataset, we do not find highly fine-grained concepts (e.g. bird body parts as used in the CUB dataset [S21]), and mitigating this by scaling up to a \"foundational\" SAE could be a promising direction for future research. Further, one could also explore using better vocabularies for concept naming, e.g. that are more tailored to the type of dataset used for SAE training. Further, note that spurious correlations learnt by CLIP would likely persist in our concept discovery, leading to concepts being activated when features correlated with the concept are present (e.g. see concept 'plane' activated for 'Airport terminal' in the example in Fig. D5, (c)). Mitigating this is orthogonal to this work and a fruitful direction for future research. Overall, we find that despite being simple, our approach is surprisingly effective in finding meaningful concepts, assigning them human interpretable names, and constructing performant classifiers in an efficient and task-agnostic manner, and can further the goal of building more interpretable models."}, {"title": "Implementation Details", "content": "In this section, we provide additional details for each of our evaluations in Sec. 4 and Sec. 5."}, {"title": "Training SAEs and DN-CBM", "content": "In this section, we describe the details for training the sparse autoencoders (SAEs) used for discovering and naming concepts (Secs. 3.1 and 3.2), and then for constructing our concept bottleneck models (DN-CBM) (Sec. 3.3). We implement our code for all our experiments using PyTorch [S13] and use Captum [S7] for visualization.\nFeature Extractors. We use CLIP [S16] ResNet-50 [S5], ViT-B/16 [S4], and ViT-L/14 [S4] pre-trained feature extractors from the official repository\u00b3. We use the output features (after pooling) for discovering concepts using sparse autoencoders.\nDatasets. We train our sparse autoencoders on the CC3M dataset [S20], and then train linear probes for classification on ImageNet [S3], Places365 [S23], CIFAR10 [S8], and CIFAR100 [S8]. To speed up training, we pre-compute features and concept strengths respectively before training, and do not perform any augmentations. Performing such augmentations (e.g. random cropping, flipping, etc.) would likely improve our classification performance further.\nTraining Sparse Autoencoders (SAEs). We train sparse autoencoders following the setup of [S1], using the implementation of [S2]4 (v1.3.0). We train for 200 epochs, and resample every 10 epochs. We perform hyperparameter sweeps using a heldout set over the learning rate {1 \u00d7 10\u22125,5 \u00d7 10\u22125,1 \u00d7 10-4,5 \u00d7 10-4,1 \u00d7 10-3}, L\u2081 sparsity coefficient (\u039b1) {3\u00d710\u22125, 1.5 \u00d7 10-4,3 \u00d7 10-4, 1.5 \u00d7 10-3,3 \u00d7 10-3}, and expansion factors {2,4,8}. For the CLIP ResNet-50 model, we choose the SAE with learning rate 5 \u00d7 10-4, L\u2081 sparsity 3 \u00d7 10-5, expansion factor 8 based on ImageNet zeroshot performance on reconstructions. See also Fig. B1 for an evaluation of the impact of \u039b1 on reconstruction error and sparsity.\nTraining Linear Probes. We train linear probes without bias on the learned concept representations using the Adam optimizer [S6]. In addition to using the cross entropy loss for classification, we apply a L\u2081 sparsity constraint on the weights, and train for 200 epochs. We perform hyperparameter sweeps using a heldout set over the learning rate {1 \u00d7 10-4,1 \u00d7 10-3,1 \u00d7 10-2}, L\u2081 sparsity coefficient (\u039b2) {0, 0.1, 1}. We train such probes over all trained SAEs and pick the probes with the best top-1 validation accuracy for each dataset."}, {"title": "Concept Accuracy on SUNAttributes", "content": "In this section, we describe the process for quantitatively evaluating concept accuracy on the SUNAttributes dataset [S14], as discussed in Sec. 4.1. Following"}, {"title": "Applying Interventions on DN-CBM", "content": "In this section, we describe the details of the evaluation by intervening on the concept bottleneck using the Waterbirds-100 [S15, S19] dataset on our DN-CBM, as discussed in Sec. 5.3.\nTraining Setup. We use our SAE for the CLIP ResNet-50 model, and train a linear probe for the binary classification task of the Waterbirds-100 dataset (i.e. Landbird versus Waterbird) to obtain a concept bottleneck model. Specifically, we use a learning rate of 0.1, L\u2081 sparsity coefficient of 10, and train for 200 epochs. To further improve sparsity, we prune the weights for each class, leaving only the five largest weights and replacing the rest with zeroes. This model obtains an accuracy of 82.8% (see Tab. 2, 'Before Interventions').\nGroup-wise Evaluation. Following [S15, S18, S19], we also report the performance at a group-wise level, i.e. both for the groups found in training (\u2018Landbird on Land', 'Waterbird on Water') and new groups only found in the test set (also known as the 'Worst Groups', i.e. 'Landbird on Water', 'Waterbird on Land').\nInterventions. For each class, we look at the assigned names of the five concepts and manually classify them as 'Bird Concept' and 'Not Bird Concept'; for the full list, see Tab. B1. We find that the bird concepts typically correspond to examples of the type of bird (e.g. 'sparrow' for 'Landbird', 'gull' for 'Waterbird'), which we attribute to the granularity of examples in the CC3M dataset used for the training the SAE and the granularity of the vocabulary. We perform two sets of interventions: (1) keeping only the bird concepts, and (2) removing only the bird concepts. For a full discussion on the results, see Sec. 5.3."}, {"title": "Additional Quantitative Results", "content": "In this section, we provide additional quantitative results. In Sec. C.1 we provide results for the performance of DN-CBM on additional backbones. In Sec. C.2, we discuss the tradeoff between accuracy and sparsity of DN-CBM explanations."}, {"title": "DN-CBM Classification Performance", "content": "In this section, we provide full quantitative results across the three backbones, i.e. CLIP [S16] ResNet-50 [S5], CLIP ViT-B/16 [S4], and CLIP ViT-L/14 [S4]. The classification accuracies across the four datasets, i.e., ImageNet [S3], Places365 [S23], CIFAR10 [S8], and CIFAR100 [S8], for our DN-CBM and the baseline methods for each of these backbones can be found in Tabs. C1 to C3.\nBroadly, we find that our DN-CBM outperforms all the baselines and almost completely bridges the gap in accuracy with linear probes\nFor our DN-CBM, in addition to the proposed task-agnostic setting, we also additionally report accuracies by using the best configuration across SAE configurations, based on classification performance on a held out validation set, and call this DN-CBM \u0442. In other words, the performance reported under DN-CBM T constitutes the setting when our concept discovery is not task-agnostic, i.e., when a separate SAE is selected for each dataset. We find, as expected, that DN-CBM \u0442 slightly outperforms the task-agnostic DN-CBM setting. However, interestingly, the performance difference is very small, showing that our task-agnostic approach, while being more general, also yields highly performant classifiers."}, {"title": "Additional Qualitative Results", "content": "In this section, we provide additional qualitative results. In Sec. D.1, we provide additional qualitative examples of task agnosticity of our discovered concepts. In Sec. D.2, we provide additional examples of clustering concept strength vectors. In Sec. D.3 and Sec. D.4, we provide examples of local and global explanations from our DN-CBM and discuss our findings."}, {"title": "Task Agnosticity of Concepts", "content": "In this section, we provide further qualitative evidence of task agnosticity of our discovered concepts by showing examples of discovered concepts and top activating images for each concept from ImageNet [S3], Places365 [S23], CIFAR10 [S8] and CIFAR100 [S8], using sparse autoencoders trained on each of the three vision backbones, i.e. CLIP [S16] ResNet-50 [S5] (Fig. D1), CLIP ViT-B/16 [S4] (Fig. D2), and CLIP ViT-L/14 [S4] (Fig. D3).\nOverall (Figs. D1 to D3), we find that across backbones, the discovered concepts are highly semantically consistent and map well to the name that is automatically assigned to them. The discovered concepts greatly vary in complexity, from simple concepts such as colours (e.g. \u2018maroon', Fig. D2) to complex concepts such as 'conversation' (Fig. D3). The images activating on each concept are highly visually diverse, while still being semantically related to the concept. For a more detailed discussion, please refer to the captions of Figs. D1 to D3."}, {"title": "Clustering Concept Strength Vectors", "content": "In this section, we provide additional qualitative evidence that our sparse autoencoder (SAE) is able to learn concepts which are semantically consistent, for which we cluster the concept strengths in the SAE latent space and find that the concept strengths are able segregate images well formed groups. Overall (Fig. D4), we find that across backbones, the discovered clusters contain images which are highly similar to each other visually. (Fig. D4) shows the clusters formed for the ImageNet dataset. For a more detailed discussion, please refer to the captions of (Fig. D4)."}, {"title": "Local Explanations from DN-CBM", "content": "In this section, we provide additional examples of local explanations of our DN-CBM on the Places365 and ImageNet datasets, using all three vision backbones (CLIP ResNet-50, CLIP ViT-B/16, and CLIP ViT-L/14). In Fig. D5, we show examples of Places365 images misclassified by the model, and analyze and discuss the misclassifications based on the provided explanations. In Fig. D6, we provide additional examples of correct classifications on Places365. Finally, in Fig. D7, we provide examples of local explanations from ImageNet, including a misclassified example.\nOverall, we find that the provided explanations typically describe the input image well, are highly diverse, and can even help better understand misclassified decisions by the model. For full details, please refer to the captions of Figs. D5 to D7."}, {"title": "Global Explanations from DN-CBM", "content": "In this section, we provide additional qualitative examples of global explanations on the Places365 dataset from our DN-CBM. This figure contains explanations as concept names which contribute the most to the class. Our method is able to explain the class with concepts which are highly relevant (Fig. D8) to it. Overall, we find that across backbones, the classes are explained well with the top-contributing concepts. For detailed discussion, please refer to the captions of (Fig. D8)."}]}