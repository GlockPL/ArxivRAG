{"title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery", "authors": ["Sukrut Rao", "Sweta Mahajan", "Moritz B\u00f6hle", "Bernt Schiele"], "abstract": "Concept Bottleneck Models (CBMs) have recently been proposed to address the 'black-box' problem of deep neural networks, by first mapping images to a human-understandable concept space and then linearly combining concepts for classification. Such models typically require first coming up with a set of concepts relevant to the task and then aligning the representations of a feature extractor to map to these concepts. However, even with powerful foundational feature extractors like CLIP, there are no guarantees that the specified concepts are detectable. In this work, we leverage recent advances in mechanistic interpretability and propose a novel CBM approach called Discover-then-Name-CBM (DN-CBM) that inverts the typical paradigm: instead of pre-selecting concepts based on the downstream classification task, we use sparse autoencoders to first discover concepts learnt by the model, and then name them and train linear probes for classification. Our concept extraction strategy is efficient, since it is agnostic to the downstream task, and uses concepts already known to the model. We perform a comprehensive evaluation across multiple datasets and CLIP architectures and show that our method yields semantically meaningful concepts, assigns appropriate names to them that make them easy to interpret, and yields performant and interpretable CBMs.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have been immensely successful for a variety of tasks, yet their 'black-box' nature poses a risk for their use in safety-critical applications. While attribution methods [5, 47, 52] have popularly been used to explain such models post-hoc, they have been shown to often provide explanations unfaithful to the model [2,3,46]. To address this, inherently interpretable models have been proposed [8, 12,30] that constrain the model to yield more faithful and human-understandable explanations in the form of heatmaps, concepts, or prototypes."}, {"title": "2 Related Work", "content": "Concept-based Explanations (e.g. [1, 28, 30, 37]) aim to express a model's decision via human-understandable concepts. Unlike popularly used post-hoc attribution heatmaps (e.g. [5, 32, 47, 52, 54, 55]) that only inform which regions in the input is influential for the decision, such methods attempt to also answer what high-level concepts are important for the model [1]. In our work, we propose a pipeline for automatically extracting and naming such concepts from CLIP and using them to build interpretable models.\nConcept Discovery [6, 9, 14, 19-22, 37-39, 60] methods have been proposed to better understand models by discovering and extracting semantically meaningful concepts learnt by them. They typically focus on explaining the function of neurons in a model [6,20,37,39], or on discovering features present in an input, and have been shown to be useful for diagnosing model failures [20]. However, these methods assign concepts to individual neurons, which may often be polysemantic and not decodable to human-understandable concepts [18]. Recently, [9] showed that sparse autoencoders (SAEs) can be effective to address the polysemanticity and superposition problem in deep networks [18] and extract mono-semantic concepts from language models (cf. [14]). We extend the setup of [9] to vision and use sparse autoencoders to automatically extract concepts learnt by CLIP.\nExplanations using Language [13, 15, 24, 25, 35, 37, 39, 56, 61] have become popular to express a model's learnt representations [15,37,39] in an easily human-interpretable manner. To decode concepts to language, such methods typically use a large-language model (LLM) such as GPT-3 [10] and learn a mapping from vision features to the LLM input. More recently, [37,39] leverage CLIP, by aligning vision features of the model being explained to the representation space"}, {"title": "Concept Bottleneck Models (CBMs)", "content": "Concept Bottleneck Models (CBMs) [4, 11, 26, 27, 29, 30, 34, 36, 40, 41, 51, 57-59, 63] are a recently popular class of inherently interpretable models (e.g. [8, 12]) that use a concept bottleneck layer (CBL) to extract named concepts and then learn a (typically sparse) linear classifier that predicts by combining such concepts, yielding highly interpretable explanations. While such methods typically require a labelled concept dataset [30] to learn the concept bottleneck, recent works leverage LLMs such as GPT-3 [10] and VLMs such as CLIP [44] to learn such bottlenecks without needing the concept labels [36,40,41,58], making them scale to large datasets such as ImageNet [16] in a performant manner. Given a classification task, such methods first query an LLM for concepts relevant to the task, and use a VLM to learn a concept bottleneck where each neuron aligns to one of the desired concepts. However, it is unclear if the feature extractor can truly recognize all such concepts when specified a priori, since they may often be non-visual [49,58] and their faithfulness has also been called into question [33,49]. Further, the CBL needs to be trained separately for each classification dataset. In contrast, we flip the paradigm and first extract concepts that are detected by the model to train the concept bottleneck, using a dataset independent of the downstream classification task. We then fix the concept bottleneck and train linear classifiers for several datasets and show that this yields highly performant and interpretable models. Similar to us, [26] also first discover concepts before constructing a CBM; however, in contrast our method does not require any external text annotations for the images and the concept discovery can even be done using a dataset different from that of the downstream task."}, {"title": "Explaining CLIP", "content": "Explaining CLIP. Several approaches have been proposed to specifically explain and understand CLIP [44] models [7,41,56]. Similar to [7], we disentangle CLIP features into human interpretable concepts. However, in contrast to [7], we do not optimize for a sparse concept representation per image using a predefined concept set, and instead first apply a general concept discovery framework [9] for extracting human understandable concepts and then name them post hoc in a task-agnostic manner to construct CBMs."}, {"title": "3 Constructing CBMs via Automated Concept Discovery", "content": "In this section, we describe our approach which consists of three stages: discovering the concepts the CLIP model has learnt via a sparse autoencoder (Sec. 3.1), naming those concepts in natural language by leveraging the CLIP text embeddings from a large vocabulary, (Sec. 3.2), and, lastly, training an interpretable concept bottleneck model (CBM) based on the discovered concepts (Sec. 3.3)."}, {"title": "3.1 Extracting Concepts Learned by the Model", "content": "To discover the concepts learned by the model, we adapt the sparse autoencoder (SAE) approach as described by [9]. Specifically, we aim to discover concepts by representing the CLIP features in a high-dimensional, but very sparsely activating space. For language models, this has been shown to yield representations in which individual neurons (dimensions) are more easily interpretable [9].\nThe Sparse Autoencoders (SAEs) proposed by [9] consist of a linear encoder $f(.)$ with weights $W_E \\in \\mathbb{R}^{d \\times h}$, a ReLU non-linearity $\\varphi$, and a linear decoder $g(.)$ with weights $W_D \\in \\mathbb{R}^{h \\times d}$. For a given input $a$, the SAE computes:\n$SAE(a) = (g \\circ \\varphi \\circ f)(a) = W_D \\varphi(W_E a)$."}, {"title": "3.2 Automated Concept Naming", "content": "Once we trained the SAE, we aim to automatically name the individual feature dimensions in the hidden representation of the SAE. For this, we propose using a large vocabulary of English words, say $V=\\{v_1, v_2, ...\\}$, which we embed via the CLIP text encoder $T$ to obtain word embeddings $E=\\{e_1, e_2, ...\\}$.\nTo name the SAE's hidden features, we propose to leverage the fact that each of the SAE neurons $c$ is assigned a specific dictionary vector $p_c$, corresponding to a column of the decoder weight matrix:\n$p_c = [W_D]_c \\in \\mathbb{R}^d$.\nIf the SAE indeed succeeds to decompose image representations given by CLIP into individual concepts, we expect the $p_c$ to resemble the embeddings of particular words that CLIP has learnt to expect in a corresponding image caption. Hence, to name the 'concept' neuron $c$ of the SAE, we propose to assign it the word $s_c$ of the closest text embedding in $E$:\n$s_c = \\underset{v \\in V}{\\arg \\min} [cos(p_c, T(v))]$.\nNote that this setting is equivalent to using the SAE to reconstruct a CLIP feature when only the concept to be named is present. As CLIP was trained to optimise cosine similarities between text and image embeddings, using the cosine similarity to assign names to concept nodes is a natural choice in this context."}, {"title": "3.3 Constructing Concept Bottleneck Models", "content": "Thus far, we trained an SAE to obtain sparse representations (Sec. 3.1), and named individual \u2018neurons' by leveraging the similarity between dictionary vectors $p_c$ to word embeddings obtained via CLIP's text encoder $T$ (Sec. 3.2).\nSuch a sparse decomposition into named 'concepts' constitutes the ideal starting point for constructing interpretable Concept Bottleneck Models (CBMs) [30,36,59]: for a given labelled dataset $D_{probe}$, we can now train a linear transformation $h(.)$ on the SAE's sparse concept activations, yielding our CBM $t(\u00b7)$:\n$t(x_i) = (h \\circ \\varphi \\circ f \\circ I)(x_i)$.\nHere, $x_i$ denotes an image from the probe dataset. The probe is trained using the cross-entropy loss, and to increase the interpretability of the resulting CBM classifier, we additionally apply a sparsity loss to the probe weights:\n$\\mathcal{L}_{probe} (x_i) = CE (t(x_i), y_i) + \\lambda_2||w||_1$\nwhere, $\\lambda_2$ is a hyperparameter, $y_i$ the ground truth label of $x$ in the probe dataset, and $w$ denotes the parameters of the linear probe.\nImportantly, note that the feature extractor, the dataset used for concept discovery, and the vocabulary used for naming can be freely chosen. As such, our approach is likely to benefit from advances in any of these directions."}, {"title": "4 Evaluation of Concept Discovery and Naming", "content": "In this section, we evaluate the effectiveness of using SAEs to discover and name concepts in CLIP vision encoders; see Sec. 5 for an evaluation of CBMs built on the SAEs. In Sec. 4.1, we first evaluate the accuracy and task agnosticity of the discovered concepts qualitatively and quantitatively, in Sec. 4.2, we discuss the impact of the vocabulary V towards the granularity of concept names, and in Sec. 4.3, we evaluate how well semantically similar concepts group together.\nSetup. We use a CLIP [44] ResNet-50 [23] vision encoder for extracting features, and use the corresponding text encoder for labelling the extracted concepts. For additional results using CLIP ViT-B/16 and ViT-L/14 [17], see Appendices C and D. To extract concepts, we follow a setup similar to [9] and train SAEs using the CC3M dataset [53]. Following [37], we use the set of 20k most frequent English words as the vocabulary V (Eq. (4))."}, {"title": "4.1 Task-Agnosticity and Accuracy of Concepts", "content": "In this section, we qualitatively and quantitatively evaluate the extracted and named concepts for semantic consistency and accuracy.\nQualitative. To showcase the promise of our proposed approach, in Fig. 3 we visualize the top activating images across four datasets for various concepts that"}, {"title": "Quantitative", "content": "Quantitative. To not only rely on the visual assessment of a few selected samples, we perform quantitative evaluations to assess the concept consistency and naming accuracy. This is generally challenging as only a few datasets include concept labels, and, even if they do, might describe different concepts in the image than those that were extracted by our task-agnostic approach. To address this, we perform a user study to evaluate concept accuracy. Specifically, we sort concepts based on how well their dictionary vector is aligned to the text embeddings of the name assigned to them (Sec. 3.2), and sample concepts with high, intermediate, and low alignments. We then extract the top activating images for each concept from three datasets (ImageNet, Places365, CC3M), and for each concept, we ask two questions: (1) how semantically consistent the concept is, i.e. if the top activating images map to some human interpretable concept, and (2) how accurate the assigned name is, if so. To evaluate if our SAE yields more"}, {"title": "4.2 Impact of Vocabulary on Concept Name Granularity", "content": "As seen in Sec. 4.1 and Fig. 4, some of the SAE nodes may not map to human interpretable concepts (Fig. 4, left), or may not be named appropriately (Fig. 4, right). The latter could be a result of limitations in the vocabulary: it being finite and only consisting of single words, it is possible that even concepts that the SAE discovers cannot be named accurately.\nTo explore this, in Fig. 5 we visualize examples of concept pairs that are originally assigned the same name (e.g. right: 'bridges'), but visually correspond to distinct modalities of the concept. We find that a more fine-grained name is assigned to the concept when added to the vocabulary V (e.g. 'arch bridge', 'suspension bridge'). Conversely, removing the assigned name 'bridge' from the vocabulary leads to worse names being assigned (e.g. 'prague', 'lisbon'; interestingly, note that the cities contain a prominent arch and suspension bridge, respectively). This suggests that the granularity and size of the vocabulary can significantly affect the name accuracy, and can also serve as a tool for practitioners to control the granularity of assigned names depending on the use case."}, {"title": "4.3 Clustering Concept Vectors", "content": "To further measure semantic consistency, we also evaluate how well semantically related concepts cluster together in the latent concept space. To do this, we perform K-Means clustering on the concept representations across all images in the Places365 dataset, and visualize a random selection of clusters. For each cluster, we compute the cluster centroid and then visualize the strongest concepts. We find that semantically similar concepts and their associated images cluster together in concept space (e.g. farming related concepts and images in the right), showing that our concept-based (latent) representation does indeed result in semantically meaningful and nameable similarities."}, {"title": "5 Evaluation of DN-CBM", "content": "We now present results on the concept bottleneck models (DN-CBM) (Sec. 3.3) built on the discovered and named concepts (Secs. 3.1, 3.2), evaluating accuracy (Sec. 5.1), interpretability (Sec. 5.2), and effectiveness of interventions (Sec. 5.3).\nSetup. Similar to prior work [36, 41], we train linear classifiers on top of the extracted concepts on four datasets\u2014ImageNet [16], CIFAR10 [31], CIFAR100 [31], and Places365 [62] and evaluate them for accuracy and interpretability. We train with various hyperparameters and pick the configurations based on performance on a heldout set. We compare our CBMs with recently proposed label-free approaches: LF-CBM [36], LaBo [58], DCLIP [34] and CDM [41], and also report the linear probe and zero-shot performance of the CLIP model we use as a backbone for reference. We use the respective concept sets of each baseline method, and for a fair comparison, the same feature extractor across methods."}, {"title": "5.1 Classification Performance", "content": "In Tab. 1, we show the classification performance of our DN-CBM on four datasets and two feature extractors and compare them with the baselines. We find that DN-CBM is highly performant across datasets and backbones. Despite being task-agnostic, DN-CBM almost always outperforms the baselines, which use concept sets optimized for the downstream task, showing the generality of our approach. The highest gains are with Places365 (i.e. 52.70\u219253.53 pp. on ResNet-50 and 52.58\u219255.11 pp. on ViT-B/16), which is a scene-classification dataset rich in a wide variety of objects, which correspond to coarser, higher level concepts than e.g. body parts of animals as in ImageNet or CIFAR10, and are likely more well-represented in our concept space trained on CC3\u041c."}, {"title": "5.2 Interpretability of DN-CBM", "content": "Local Explanations (Image-Level). In Fig. 7, we show qualitative examples of local explanations from our DN-CBM, i.e., explanations of individual decisions. For each image, we show the most contributing concepts along with their"}, {"title": "Global Explanations (Class-level)", "content": "Global Explanations (Class-level). In Fig. 9, we show qualitative examples of global explanations from our DN-CBM, i.e., explanations of which concepts contribute the most to a class as a whole. To do this, for each class, we compute the average contribution of all concepts for images from that class, and visualize the set of top concepts. Qualitatively, we find this set to be semantically consistent with what is contained in each class."}, {"title": "5.3 Effectiveness of Concept Interventions", "content": "In addition to understanding model decisions, explanations have also been used to debug models [30] and fix models' reasoning [43, 45, 48]. Specifically, concept bottleneck models allow human interventions on individual concepts to control"}, {"title": "6 Conclusion", "content": "In this work, we proposed Discover-then-Name CBM (DN-CBM), a novel CBM approach that uses sparse autoencoders to discover and automatically name concepts learnt by CLIP, and then use the learnt concept representations as a concept bottleneck and train linear layers for classification. We find that this simple approach is surprisingly effective at yielding semantically consistent concepts with appropriate names. Further, we find despite being task-agnostic, i.e. only extracting and naming concepts once, our approach can yield performant and interpretable CBMs across a variety of downstream datasets. Our results further corroborate the promise of sparse autoencoders for concept discovery. Training a more 'foundational' sparse autoencoder with a much larger dataset (e.g. at CLIP scale) and concept space dimensionality (with hundreds of thousands or millions of concepts) to obtain even more general-purpose CBMs, particularly for fine-grained classification, would be a fruitful area for future research."}, {"title": "A Limitations and Broader Impact", "content": "In this work, we proposed DN-CBM, as task-agnostic approach to discovering concepts from CLIP [S16] using sparse autoencoders (SAEs) and then using them to construct a concept bottleneck model across downstream classification tasks. We find that this simple approach shows promise that using a higher dimensional concept space and training SAEs on much larger datasets than CC3M [S20] would provide a richer and more diverse concept representation. We already find that concept discovery from our SAEs using a relatively small dataset like CC3M can lead to performant and interpretable CBMs for datasets such as Places365 [S23], leveraging concepts related to objects, colours, shapes, locations, and associations to reach decisions in an interpretable manner. However, these constitute relatively coarse concepts, and given the nature and size of the CC3M dataset, we do not find highly fine-grained concepts (e.g. bird body parts as used in the CUB dataset [S21]), and mitigating this by scaling up to a \"foundational\" SAE could be a promising direction for future research. Further, one could also explore using better vocabularies for concept naming, e.g. that are more tailored to the type of dataset used for SAE training. Further, note that spurious correlations learnt by CLIP would likely persist in our concept discovery, leading to concepts being activated when features correlated with the concept are present (e.g. see concept 'plane' activated for 'Airport terminal' in the example in Fig. D5, (c)). Mitigating this is orthogonal to this work and a fruitful direction for future research. Overall, we find that despite being simple, our approach is surprisingly effective in finding meaningful concepts, assigning them human interpretable names, and constructing performant classifiers in an efficient and task-agnostic manner, and can further the goal of building more interpretable models."}, {"title": "B Implementation Details", "content": "In this section, we provide additional details for each of our evaluations in Sec. 4 and Sec. 5."}, {"title": "B.1 Training SAEs and DN-CBM", "content": "In this section, we describe the details for training the sparse autoencoders (SAEs) used for discovering and naming concepts (Secs. 3.1 and 3.2), and then for constructing our concept bottleneck models (DN-CBM) (Sec. 3.3). We implement our code for all our experiments using PyTorch [S13] and use Captum [S7] for visualization.\nFeature Extractors. We use CLIP [S16] ResNet-50 [S5], ViT-B/16 [S4], and ViT-L/14 [S4] pre-trained feature extractors from the official repository\u00b3. We use the output features (after pooling) for discovering concepts using sparse autoencoders.\nDatasets. We train our sparse autoencoders on the CC3M dataset [S20], and then train linear probes for classification on ImageNet [S3], Places365 [S23], CIFAR10 [S8], and CIFAR100 [S8]. To speed up training, we pre-compute features and concept strengths respectively before training, and do not perform any augmentations. Performing such augmentations (e.g. random cropping, flipping, etc.) would likely improve our classification performance further.\nTraining Sparse Autoencoders (SAEs). We train sparse autoencoders following the setup of [S1], using the implementation of [S2]4 (v1.3.0). We train for 200 epochs, and resample every 10 epochs. We perform hyperparameter sweeps using a heldout set over the learning rate {1 \u00d7 10\u22125,5 \u00d7 10\u22125,1 \u00d7 10-4,5 \u00d7 10-4,1 \u00d7 10-3}, L\u2081 sparsity coefficient (11) 3\u00d710\u22125, 1.5 \u00d7 10-4,3 \u00d7 10-4, 1.5 \u00d7 10-3,3 \u00d7 10-3}, and expansion factors {2,4,8}. For the CLIP ResNet-50 model, we choose the SAE with learning rate 5 \u00d7 10-4, L\u2081 sparsity 3 \u00d7 10-5, expansion factor 8 based on ImageNet zeroshot performance on reconstructions. See also Fig. B1 for an evaluation of the impact of A\u2081 on reconstruction error and sparsity.\nTraining Linear Probes. We train linear probes without bias on the learned concept representations using the Adam optimizer [S6]. In addition to using the cross entropy loss for classification, we apply a L\u2081 sparsity constraint on the weights, and train for 200 epochs. We perform hyperparameter sweeps using a heldout set over the learning rate {1 \u00d7 10-4,1 \u00d7 10-3,1 \u00d7 10-2}, L\u2081 sparsity coefficient (12) {0, 0.1, 1}. We train such probes over all trained SAEs and pick the probes with the best top-1 validation accuracy for each dataset."}, {"title": "B.2 Concept Accuracy on SUNAttributes", "content": "In this section, we describe the process for quantitatively evaluating concept accuracy on the SUNAttributes dataset [S14], as discussed in Sec. 4.1. Following"}, {"title": "B.3 User Study", "content": "In this section, we describe the details of our user study to quantitatively measure the consistency and accuracy of our discovered and named concepts, as discussed in Sec. 4.1.\nMethods. We evaluate on nodes from our SAE (Sec. B.1) for the CLIP ResNet-50 model, assigned with names from our naming scheme (Sec. 3.2). As a baseline, we use nodes from the image feature vector of the same CLIP ResNet-50 model, assigned names using CLIP-Dissect [S11] with the same vocabulary.\nSelecting Nodes. To obtain a holistic view of the consistency and name accuracy of the node concepts, we sort nodes based on the alignment with the text embedding vector of the name assigned to them. Specifically, for the SAEs, we sort based on the cosine similarity between the dictionary vector and the text embedding, and for the CLIP features, we sort based on the CLIP-Dissect similarity. We then uniformly at random sample nodes from three bins, where"}, {"title": "B.4 Applying Interventions on DN-CBM", "content": "In this section, we describe the details of the evaluation by intervening on the concept bottleneck using the Waterbirds-100 [S15, S19] dataset on our DN-CBM, as discussed in Sec. 5.3.\nTraining Setup. We use our SAE for the CLIP ResNet-50 model, and train a linear probe for the binary classification task of the Waterbirds-100 dataset (i.e. Landbird versus Waterbird) to obtain a concept bottleneck model. Specifically, we use a learning rate of 0.1, L\u2081 sparsity coefficient of 10, and train for 200 epochs. To further improve sparsity, we prune the weights for each class, leaving only the five largest weights and replacing the rest with zeroes. This model obtains an accuracy of 82.8% (see Tab. 2, 'Before Interventions').\nGroup-wise Evaluation. Following [S15, S18, S19], we also report the performance at a group-wise level, i.e. both for the groups found in training (\u2018Landbird on Land', 'Waterbird on Water') and new groups only found in the test set (also known as the 'Worst Groups', i.e. 'Landbird on Water', 'Waterbird on Land').\nInterventions. For each class, we look at the assigned names of the five concepts and manually classify them as 'Bird Concept' and 'Not Bird Concept'; for the full list, see Tab. B1. We find that the bird concepts typically correspond to examples of the type of bird (e.g. 'sparrow' for 'Landbird', 'gull' for 'Waterbird'), which we attribute to the granularity of examples in the CC3M dataset used for the training the SAE and the granularity of the vocabulary. We perform two sets of interventions: (1) keeping only the bird concepts, and (2) removing only the bird concepts. For a full discussion on the results, see Sec. 5.3."}, {"title": "C Additional Quantitative Results", "content": "In this section, we provide additional quantitative results. In Sec. C.1 we provide results for the performance of DN-CBM on additional backbones. In Sec. C.2, we discuss the tradeoff between accuracy and sparsity of DN-CBM explanations."}, {"title": "C.1 DN-CBM Classification Performance", "content": "In this section, we provide full quantitative results across the three backbones, i.e. CLIP [S16] ResNet-50 [S5], CLIP ViT-B/16 [S4], and CLIP ViT-L/14 [S4]. The classification accuracies across the four datasets, i.e., ImageNet [S3], Places365 [S23], CIFAR10 [S8], and CIFAR100 [S8], for our DN-CBM and the baseline methods for each of these backbones can be found in Tabs. C1 to C3.\nBroadly, we find that our DN-CBM outperforms all the baselines and almost completely bridges the gap in accuracy with linear probes\nFor our DN-CBM, in addition to the proposed task-agnostic setting, we also additionally report accuracies by using the best configuration across SAE configurations, based on classification performance on a held out validation set, and call this DN-CBM \u0442. In other words, the performance reported under DN-CBM T constitutes the setting when our concept discovery is not task-agnostic, i.e., when a separate SAE is selected for each dataset. We find, as expected, that DN-CBM \u0442 slightly outperforms the task-agnostic DN-CBM setting. However, interestingly, the performance difference is very small, showing that our task-agnostic approach, while being more general, also yields highly performant classifiers."}, {"title": "C.2 Sparsity of DN-CBM Explanations", "content": "In addition to being accurate and general, one also desires that the explanations are sparse, since explanations with a large number of concepts are not very human interpretable [S17]. We explore the relationship between sparsity and accuracy of DN-CBM in Fig. C1 by evaluating models across SAE and classifier hyperparameters on both metrics. To measure sparsity, we count the average number of concepts required to reach 0.9 fraction of the original logit value for each image across the dataset and find, quite naturally, that there exists a trade-off between the two metrics across datasets. However, interestingly, we find that one can obtain models that require very few concepts per decision on average by sacrificing only a small amount of accuracy."}, {"title": "D Additional Qualitative Results", "content": "In this section, we provide additional qualitative results. In Sec. D.1, we provide additional qualitative examples of task agnosticity of our discovered concepts. In Sec. D.2, we provide additional examples of clustering concept strength vectors. In Sec. D.3 and Sec. D.4, we provide examples of local and global explanations from our DN-CBM and discuss our findings."}, {"title": "D.1 Task Agnosticity of Concepts", "content": "In this section, we provide further qualitative evidence of task agnosticity of our discovered concepts by showing examples of discovered concepts and top activating images for each concept from ImageNet [S3], Places365 [S23], CIFAR10 [S8] and CIFAR100 [S8], using sparse autoencoders trained on each of the three vision backbones, i.e. CLIP [S16] ResNet-50 [S5] (Fig. D1), CLIP ViT-B/16 [S4] (Fig. D2), and CLIP ViT-L/14 [S4] (Fig. D3).\nOverall (Figs. D1 to D3), we find that across backbones, the discovered concepts are highly semantically consistent and map well to the name that is automatically assigned to them. The discovered concepts greatly vary in complexity, from simple concepts such as colours (e.g. \u2018maroon', Fig. D2) to complex concepts such as 'conversation' (Fig. D3). The images activating on each concept are highly visually diverse, while still being semantically related to the concept. For a more detailed discussion, please refer to the captions of Figs. D1 to D3."}, {"title": "D.2 Clustering Concept Strength Vectors", "content": "In this section, we provide additional qualitative evidence that our sparse autoencoder (SAE) is able to learn concepts which are semantically consistent, for which we cluster the concept strengths in the SAE latent space and find that the concept strengths are able segregate images well formed groups. Overall (Fig. D4), we find that across backbones, the discovered clusters contain images which are highly similar to each other visually. (Fig. D4) shows the clusters formed for the ImageNet dataset. For a more detailed discussion, please refer to the captions of (Fig. D4)."}, {"title": "D.3 Local Explanations from DN-CBM", "content": "In this section, we provide additional examples of local explanations of our DN-CBM on the Places365 and ImageNet datasets, using all three vision backbones (CLIP ResNet-50, CLIP ViT-B/16, and CLIP ViT-L/14). In Fig. D5, we show examples of Places365 images misclassified by the model, and analyze and discuss the misclassifications based on the provided explanations. In Fig. D6, we provide additional examples of correct classifications on Places365. Finally, in Fig. D7, we provide examples of local explanations from ImageNet, including a misclassified example.\nOverall, we find that the provided explanations typically describe the input image well, are highly diverse, and can even help better understand misclassified decisions by the model. For full details, please refer to the captions of Figs. D5 to D7."}, {"title": "D.4 Global Explanations from DN-CBM", "content": "In this section, we provide additional qualitative examples of global explanations on the Places365 dataset from our DN-CBM. This figure contains explanations as concept names which contribute the most to the class. Our method is able to explain the class with concepts which are highly relevant (Fig. D8) to it. Overall, we find that across backbones, the classes are explained well with the top-contributing concepts. For detailed discussion, please refer to the captions of (Fig. D8)."}]}