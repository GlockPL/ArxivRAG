{"title": "Perceptions of Discriminatory Decisions of Artificial Intelligence: Unpacking the Role of Individual Characteristics", "authors": ["Soojong Kim"], "abstract": "This study investigates how personal differences (digital self-efficacy, technical knowledge, belief in equality, political ideology) and demographic factors (age, education, and income) are associated with perceptions of artificial intelligence (AI) outcomes exhibiting gender and racial bias and with general attitudes towards AI. Analyses of a large-scale experiment dataset (N = 1,206) indicate that digital self-efficacy and technical knowledge are positively associated with attitudes toward AI, while liberal ideologies are negatively associated with outcome trust, higher negative emotion, and greater skepticism. Furthermore, age and income are closely connected to cognitive gaps in understanding discriminatory AI outcomes. These findings highlight the importance of promoting digital literacy skills and enhancing digital self-efficacy to maintain trust in AI and beliefs in AI usefulness and safety. The findings also suggest that the disparities in understanding problematic Al outcomes may be aligned with economic inequalities and generational gaps in society. Overall, this study sheds light on the socio-technological system in which complex interactions occur between social hierarchies, divisions, and machines that reflect and exacerbate the disparities.", "sections": [{"title": "Perceptions of Discriminatory Decisions of Artificial Intelligence: Unpacking the Role of Individual Characteristics", "content": "The use of artificial intelligence (AI) and automated decision-making has surged in recent years, and these technologies are replacing human decision-making processes in various domains, including the news media, healthcare, law, finance, and law enforcement (e.g., Diakopoulos & Koliska, 2017; Jha & Topol, 2016; Kennedy et al., 2011; Nissan, 2017; Thurman et al., 2019; Yu & Kohane, 2019). While the socio-technological shift has the potential to bring about positive changes, such as increased efficiency, productivity, and innovation at workplaces and in daily lives, concerns are also mounting regarding discriminatory and unjust outcomes that the new technologies could produce (e.g., Asplund et al., 2020; Imana et al., 2021).\nDespite the potential benefits, user interactions with AI and algorithms often reveal notable challenges, such as unfairness, distrust, skepticism, and negative emotions experienced by users. Research in Human-Computer Interaction (HCI) has demonstrated that these challenges are not only technical but also rooted in user engagement and the contexts of use. Especially, a major body of work in HCI closely examines user interactions with algorithms and AI systems, focusing on the complexities of human-algorithm interaction (Choung, Seberger, et al., 2023; Hajigholam Saryazdi, 2024; Lee & Baykal, 2017; Ochmann et al., 2024; Rader & Gray, 2015; Starke et al., 2022; Wang et al., 2020). For instance, Binns et al. (2018) discuss justice perceptions in algorithmic decisions, highlighting how the lack of transparency and the reduction of human qualities to mere statistical percentages can exacerbate feelings of unfairness and indignity among users. Choung et al. (2023) explore how perceptions of AI in decision-making can vary based on the context and outcome, finding that AI decisions are often viewed as more trustworthy, fair, and useful than human decisions especially when rejecting candidates in a job"}, {"title": "", "content": "application scenario. This domain of HCI research stresses the need to understand challenges in enhancing interactions with AI systems, identify factors influencing the interactions, and develop and implement human-centered, culturally aware design solutions that address these challenges.\nIt has been known that AI and algorithms have the possibility of producing unequal, unfair, and even unjust outcomes (e.g., Barocas et al., 2019; Hooker, 2021), which could perpetuate and even exacerbate social inequalities and injustices (Benjamin, 2019; Noble, 2018; O'Neil, 2016). Systematic problems that exist in society can also easily penetrate and be ingrained into AI and algorithms (e.g., Koenecke et al., 2020; Obermeyer et al., 2019). Examples of algorithmic racial and gender discrimination have been observed in various domains, including housing markets (Asplund et al., 2020; J. Miller, 2020), job markets (Chen et al., 2018; Imana et al., 2021), human recognition (Koenecke et al., 2020; Menezes et al., 2021), and web search engines (Makhortykh et al., 2021; Vlasceanu & Amodio, 2022). These alarming instances illustrate the imperative to assess the social consequences and implications of machine-driven discrimination and injustice, to understand public attitudes and perceptions toward algorithms, AI technologies, and public policy governing their use, and to improve the design and implementation for more equitable and just socio-technological systems (Angerschmid et al., 2022; Dietvorst et al., 2014; Dolata et al., 2022; Logg et al., 2019).\nFocusing on the imperative to understand public attitudes and perceptions toward algorithms and AI, the present study investigates factors that shape perceptions of discriminatory Al outcomes, and general attitudes toward AI in the contexts of machine-augmented discrimination. Previous research has investigated how individuals' personal differences (e.g., self-efficacy, knowledge, political beliefs) and demographic factors (e.g., education, age, and income) are associated with their attitudes toward AI use, such as trust in AI, perceived risk of"}, {"title": "", "content": "AI use, and the preference of AI over human decision-making (Araujo et al., 2020; Lee & Rich, 2021; Logg et al., 2019; Thurman et al., 2019). However, still little is known about how individual differences influence perceptions of discriminatory outcomes produced by algorithms and AI, although unfair and biased outcomes from automated systems are increasingly regarded as prevalent and harmful (J. Miller, 2020; Thune, 2022; Verma, 2022). The role of demographic factors also suffers a lack of investigation in the domain of the public perception of algorithms and AI (Starke et al., 2022). To address these gaps of knowledge and to provide a more comprehensive understanding of the factors connected with perceptions of discrimination generated or enhanced by machines, we analyzed data obtained from a large-scale experiment (N = 1,206) that examined perceptions of gender and racial biases in AI systems. This study focused on how personal differences and demographic factors are associated with cognitive and emotional responses to automated decisions that discriminate against certain gender or racial groups.\nThe remainder of this paper is structured as follows. We first review previous research on the perceptions of algorithms and AI and articulate the research hypotheses and questions of the study. The methodology and results of the research are then presented. Lastly, the discussion of the findings, the limitations of the research, and potential directions for future research follow."}, {"title": "Public Perceptions of AI", "content": "The current research explores the dynamics of user interactions with AI systems, which is one of the central goals of HCI research. Especially, it investigates specific difficulties that users face in using AI systems\u2014the users' perception of unfairness, distrust, skepticism, and negative emotion toward Al outcomes\u2014which also influence their interaction with the systems. By doing so, this study contributes to the important literature on how users' perceptions of algorithms and"}, {"title": "", "content": "algorithmic outcomes are shaped and can be improved (Angerschmid et al., 2022; Araujo et al., 2020; Binns et al., 2018; Dietvorst et al., 2014; Lee & Baykal, 2017; Rader & Gray, 2015; Wang et al., 2020). The present research specifically focuses on the perceptions of discriminatory outcomes and examines how the perceptions of AI outcomes are influenced by individual characteristics, aiming to propose targeted design remedies based on these insights.\nPrevious research showed that people often refuse to use algorithms and AI, despite generally positive attitudes toward them (Logg et al., 2019; Thurman et al., 2019). For example, one study found an aversion to AI-based medical decisions compared with decisions made by human healthcare providers (Longoni et al., 2019). Errors in algorithms and their potential significance can contribute to the aversion and refusal to adopt algorithms: Observing errors can reduce confidence and intention to choose algorithms over human decision-makers, even in situations where algorithms outperform humans (Dietvorst et al., 2014). Concerns about potential biases may also harm perceived fairness and trust in these systems (Araujo et al., 2020; Lee & Baykal, 2017). Several cognitive and emotional dimensions have been studied as cognitive indicators of algorithm appreciation and aversion, including perceived fairness, trust in algorithms and AI, and emotional reactions (Angerschmid et al., 2022; Araujo et al., 2020; Choung, David, et al., 2023; Lee, 2018; Lee & Baykal, 2017; Lee & Rich, 2021; Wang et al., 2020)."}, {"title": "Perceptions of Discriminatory AI Outcomes", "content": "Previous studies have explored how individual characteristics, including personal differences (such as technological self-efficacy, domain-specific knowledge, and political beliefs) and demographic factors (such as race, gender, education, and age), shape people's attitudes toward algorithms and AI (Araujo et al., 2020; Lee & Rich, 2021; Logg et al., 2019;"}, {"title": "", "content": "Thurman et al., 2019) and how these factors affect perceptions of automated outcomes (Lee, 2018). However, little research has focused on the role of individual characteristics in cases where people encounter discriminatory algorithm outcomes. This is a critical gap in our understanding, given the importance of understanding public perceptions of biased machine decisions for developing policies and technologies that are more inclusive and beneficial for society (Benjamin, 2019; Noble, 2018; O'Neil, 2016). Examining perceptions of algorithmic discrimination can also help advance long-standing efforts in social sciences to answer essential questions on social identities, attribution processes, and cognitive biases in this \u201cage of AI\u201d (Kissinger et al., 2021). Hence, the present research aimed to gain a more detailed and in-depth understanding of the public perception of algorithms and AI, focusing on four aspects of individual differences (digital self-efficacy, technical knowledge, belief in equality, and political ideology) and three demographic factors (education, age, and income.)"}, {"title": "Digital Self-Efficacy", "content": "Self-efficacy has been established as a crucial predictor of attitudes toward certain technology and its use (Aesaert & van Braak, 2014; Rimal, 2001). A study suggested that online self-efficacy, an individual's belief in their own ability to protect personal information while using the Internet, is linked to perceived fairness, usefulness, and risk of algorithms (Araujo et al., 2020). The current research explores the role of digital self-efficacy by expanding the concept of online self-efficacy. Digital self-efficacy is defined as self-efficacy in safeguarding personal information when using digital devices and services. General concerns about data collection and the use of digital technologies are known to be associated with attitudes toward algorithms (Thurman et al., 2019). With algorithms and AI increasingly integrated into all types and modes of digital technologies, digital self-efficacy is expected to encompass self-efficacy not"}, {"title": "", "content": "only during web browsing and general Internet use, but also in the rapidly evolving technological landscape.\nThis research examined if perceptions of discriminatory AI outcomes are associated with digital self-efficacy. Based on the prior work, it was assumed that people with higher digital self-efficacy interpret discriminatory Al outcomes less negatively. This is because people's higher confidence in their ability to understand how technologies utilize personal information and how to protect themselves from potential negativities may encourage them to perceive a biased outcome less negatively, viewing it as less unfair and more trustworthy with less negative emotion and skepticism.\nH1: Digital self-efficacy is positively associated with perceived fairness and trust of discriminatory Al outcomes and negatively associated with negative emotion and skepticism toward the outcomes."}, {"title": "Technical Knowledge", "content": "Domain-specific knowledge of technology has been linked to general attitudes toward technologies, but previous studies have reported mixed findings on the direction of this relationship. Some studies have identified a positive relationship between technical knowledge and perceptions of the usefulness and fairness of AI (Araujo et al., 2020). Comfort with mathematics, which could be closely related to domain-specific knowledge of digital technologies and AI, has also been associated with more favorable attitudes toward algorithmic recommendations (Logg et al., 2019; Thurman et al., 2019). However, other studies have reported a decrease in perceived fairness of algorithm decisions as knowledge of computer programming increased (Lee & Baykal, 2017)."}, {"title": "", "content": "The present study aims not only to add further evidence of the relationship between technical knowledge and perceptions of the technology but also to explore how this knowledge influences perceptions of discriminatory outcomes generated by the technology, an issue that has not been touched by prior investigations. Since technical knowledge is connected to an individual's ability to understand the capabilities and limitations of algorithms and AI, greater technical knowledge is associated with reduced perceived risks and increased confidence in accuracy and reliability (Said et al., 2022). Hence, individuals with greater technical knowledge, when compared with those with lower levels of technical knowledge, may view discriminatory Al outcomes as less unfair, more trustworthy, and less likely to exhibit negative emotions toward automated decisions. Hence, this study proposes the following hypothesis:\nH2: Technical knowledge is positively associated with perceived fairness and trust of discriminatory Al outcomes and negatively associated with negative emotion and skepticism toward them."}, {"title": "Belief in Equality", "content": "Belief in equality has been found to affect individuals' general attitudes toward algorithms. Araujo et al. (2020) showed that people with stronger beliefs in equality viewed algorithmic decision-making as fairer and more useful. When outcomes are biased, however, the nature of the association between equality belief and outcome perceptions may differ from that of general attitudes toward AI. This is because the violation of equality could challenge the common heuristic that machines are unbiased and objective (Sundar, 2008), and biased machine decisions could also make people doubtful about decision-making procedures and agencies (Lee, 2018; Skarlicki & Folger, 1997)."}, {"title": "", "content": "The attribution theory helps further understand potential associations between equality belief and perceptions of discriminatory AI outcomes. People are more inclined to assign responsibility to a causal agent when the outcome is perceived as more severe (Robbennolt, 2000; Schroeder & Linder, 1976; Walster, 1966). Reactions to intentional discriminatory acts are more likely to be negative than those toward unintentional ones (Stouten et al., 2006; Swim et al., 2003). Therefore, individuals with stronger beliefs in equality, who are likely to view discriminatory situations as more problematic, may assign blame to harmful intentions embedded within automated systems rather than considering them as technical errors occurring by chance. This, in turn, may lead to more negative reactions toward biased AI outcomes. Thus, we propose the following hypothesis:\nH3: Belief in equality is negatively associated with perceived fairness and trust of discriminatory Al outcomes and positively associated with negative emotion and skepticism toward them."}, {"title": "Political Ideology", "content": "Political ideology is known to relate to people's attitudes toward algorithms. Some scholars have shown that individuals with more liberal ideologies are likely to have more positive attitudes toward AI and its use, while those with more conservative ideologies tend to be more skeptical about them (Castelo & Ward, 2021; Mack et al., 2021; Peng, 2020; Zhang & Dafoe, 2019). For example, Castelo and Ward (2021) showed that conservative political beliefs were associated with low levels of comfort with and trust in AI.\nConsidering the unprecedented transformative power of AI on established societal structures and hierarchies, political ideology may affect individuals' cognitive and affective reactions to discriminatory AI outcomes. Unlike general attitudes toward algorithms and AI,"}, {"title": "", "content": "perceptions of discriminatory outcomes may display different relationships with political ideology. Specifically, individuals espousing more liberal beliefs, which place emphasis on equality and due process (Jost et al., 2009), may perceive discriminatory outcomes as more unfair or untrustworthy than those with more conservative ideologies and hence exhibit more negative reactions to the automated results. Thus, this study posited the following hypothesis:\nH4: More liberal political ideology is associated with more negative cognitive and emotional responses to discriminatory AI outcomes.\nObserving bias and errors affects the appreciation, adoption, and approval of algorithms, AI, and relevant policies (Choung, David, et al., 2023; Dietvorst et al., 2014; Schiff et al., 2022). Especially, it is in discriminatory situations where general attitudes toward AI, such as perceived risk of AI, become particularly relevant. However, there is a lack of evidence regarding how personal differences relate to general AI attitudes in contexts where individuals encounter discriminatory automated outcomes. Thus, the present study explores the following research question:\nRQ1: How are digital self-efficacy, technical knowledge, belief in equality, and political ideology associated with general attitudes toward AI, when experiencing discriminatory AI outcomes?"}, {"title": "The Role of Demographic Factors", "content": "Different social groups may give more importance to specific aspects of AI than others (e.g., Kieslich et al., 2022). Prior work has investigated the relationship between demographic characteristics and general attitudes toward AI. For example, Helberger et al. (2020) asked participants to compare AI and humans in terms of their ability to make a fairer decision. They found that AI was evaluated higher in fairness among younger and more educated participants."}, {"title": "", "content": "Despite recent progress, the role of demographic factors, such as age, education, and income, has not received sufficient attention and needs further exploration (Starke et al., 2022). Witnessing discrimination and errors can impact people's approval, acceptance, and appreciation of algorithms and AI, as well as their support for relevant public policies (Choung, David, et al., 2023; Dietvorst et al., 2014; Schiff et al., 2022). Situations involving discriminatory outcomes may divert and intensify individuals' attitudes and perceptions. By examining the influence of these demographic factors, we can gain deeper insights into the intricate links between economic disparities, intergenerational disparities, and cognitive disparities in individuals' perceptions of Al outcomes and attitudes toward AI. Therefore, this study investigates the following research questions regarding how demographic factors are associated with general attitudes and outcome perceptions:\nRQ2: How are education, age, and income associated with general attitudes toward AI, in contexts involving discriminatory outcomes?\nRQ3: How are education, age, and income associated with cognitive and emotional responses to discriminatory Al outcomes?\nThe present research also examined whether the perceived pervasiveness of discriminatory Al outcomes varies based on personal differences. Previous research has shown that personal differences are connected with the perception of discrimination and its pervasiveness (Bagci et al., 2017; Gong et al., 2017). The perceived pervasiveness of discrimination induced or amplified by machines could have far-reaching consequences, including affecting attitudes toward algorithms and AI, support for related policies, and various aspects of people's lives (e.g., Schmitt et al., 2003; Stroebe et al., 2011). Thus, this study explored the following research question:"}, {"title": "", "content": "RQ4: How is the perceived pervasiveness of discriminatory Al outcomes associated with digital self-efficacy, technical knowledge, belief in equality, and political ideology?"}, {"title": "Method", "content": "The present research sought to capture the multidimensionality of factors that may influence individuals' perceptions of discriminatory AI outcomes and general attitudes toward AI. Incorporating multiple individual characteristics, including personal differences and demographic factors, that may influence AI perceptions could be also beneficial for the analysis and the interpretation of findings. By controlling for other characteristics in examining the association between a certain individual characteristic and a variable about AI perception, one can reduce some of the confounding influences that could complicate the interpretation of the association."}, {"title": "Participants", "content": "Participants were drawn from a convenient sample obtained via Prolific, an online participant recruitment platform. To be eligible for the experiment, participants had to be at least 18 years old and located in the United States. Those who met these requirements were directed to an experiment webpage. Participants who had issues with following the experimental instructions or providing the correct answer to an attention check question were excluded from the analysis. The results reported in this study were based on the remaining 1,206 participants.\nOf the 1,206 participants, 73.88% (n = 891) self-identified as white, and 7.38% (n = 89) as Black. The average age was 37.90 years (SD = 14.48). Participants who self-identified as men, women, and non-binary were 42.95% (n = 518), 53.40% (n = 644), and 2.99% (n = 36), respectively. A majority (86.90%) of participants held degrees higher than high school graduates,"}, {"title": "", "content": "with 17.00% having postgraduate education. The percentage of participants with an income of less than $50,000 was 45.52%."}, {"title": "Procedure", "content": "This research was based on the secondary analysis of the data obtained from an experiment described below (Kim, Lee, et al., 2024; Kim, Oh, et al., 2024). The analysis enabled us to identify associations between individual characteristics and cognitive and attitudinal outcomes in situations with discriminatory AI outcomes that are consistent across different identity dimensions of discrimination (between-subject), different discrimination targets (between-subject), and different contexts of AI use (within-subject).\nParticipants of the experiment were randomly assigned to one of the four experimental conditions: 2 (identity dimensions: race vs. gender) \u00d7 2 (discrimination target: self vs. other). During the experiment, participants were shown the same set of nine scenarios, but the identity dimension and the discrimination target varied depending on the condition. First, regarding the identity dimension of discrimination, participants in the gender condition were asked to think about a friend of a different gender but with similar characteristics (such as race, age, education level, and economic status), while those in the race condition were instructed to consider a friend of a different race but with similar characteristics. Participants were also instructed to keep the friend they chose in mind while considering the scenarios that followed. To verify that participants followed the instruction, they were also asked about the gender of the friend they chose. In terms of the target of discrimination, the discriminatory scenarios of the subject-targeting condition disadvantaged the subject, whereas the scenarios presented in the other-targeting condition disadvantaged the friend. The list of scenarios is presented in Table S1 of the online supporting material."}, {"title": "", "content": "Each scenario depicted a realistic situation where both a participant and the friend he/she chose were using the same technology based on AI, such as a rating system evaluating job interview performance, but experienced bias in its outcome. The scenarios presented to participants covered a range of topics including media, finance, public service, the labor market, and health and safety. Some of the scenarios were adopted from previous studies, and others were newly created based on real-world incidents reported in news and report (Acikgoz et al., 2020; Binns et al., 2018; A. P. Miller & Hosanagar, 2019; Parra et al., 2021). All scenarios were presented in the same narrative format tested by Parra et al. (2021). Each participant was shown one scenario at a time, and the order of presentation was randomly determined for each participant. The goal of this design was to capture the real-world experiences and exposures that people encounter in their interactions with AI technologies which are often characterized by instances of discrimination and bias (Kim, Lee, et al., 2024; Kim, Oh, et al., 2024).\nAfter viewing each scenario, participants evaluated the outcome presented in it from multiple aspects: fairness of the outcome, trust in the technology generating the outcome, emotion about the outcome, skepticism toward the outcome, and pervasiveness of the outcome in the real world. After viewing all nine scenarios, participants were required to respond to a series of questions gauging their general attitudes toward AI. The questionnaires to measure personal differences and demographic factors were also presented after all scenarios.\nThe deliberate placement of questionnaires capturing general attitudes following the exposure to the discriminatory scenarios and the assessment of perceptions of discriminatory outcomes was designed with the intention to capture general attitudes toward AI in contexts involving discriminatory outcomes. Essentially, the scenarios and the measures for outcome perception served as an exposure to the discriminatory context for the participants, which"}, {"title": "", "content": "enabled them to respond to the questions measuring general attitudes while considering the discriminatory AI situations.\nThe average time for the completion of the entire experiment was 13.13 minutes (SD = 6.32 minutes). Financial compensation was provided to participants for their participation. The study received an exemption from the Institutional Review Board at the University of California Davis, and participants were required to give their informed consent on the first page of the experiment website. Data collection for the experiment was completed in August 2022."}, {"title": "Measures", "content": "Outcome fairness, trust, and negative emotion were measured after each scenario and averaged across scenarios for each participant. The summary statistics are as follows: outcome fairness (M = 3.14, SD = 1.09), outcome trust (M = 2.73, SD = 0.94), and negative emotion (M = 4.52, SD = 1.32). All items measuring these variables were adapted from previous work (Araujo et al., 2020; Lee, 2018), which ranged from 1 (lowest) to 7 (highest).\nTo gauge participants' overall awareness and perception of AI discrimination, two new measures were introduced in this study. First, outcome skepticism reflects the overall tendency of participants to question an Al outcome and cast doubt against it (Kim, Lee, et al., 2024; Kim, Oh, et al., 2024). It was assessed with two items on a 7-point scale from 1 to 7, and participants were asked to rate their agreement with each of the following statements \u201cThis outcome is problematic\u201d and \u201cThis outcome is questionable.\u201d These responses were then averaged to create a single variable. Second, outcome pervasiveness captured the subjective evaluation of the possibility of a certain outcome in everyday life (Kim, Lee, et al., 2024; Kim, Oh, et al., 2024). This variable was assessed on a 7-point scale from 1 (highly unlikely) to 7 (highly likely) using the question \"How likely is this outcome to happen in your everyday life?\u201d Both variables were"}, {"title": "", "content": "measured after each scenario and averaged across scenarios for each participant: outcome skepticism (M = 5.02, SD = 1.07) and outcome pervasiveness (M = 3.98, SD = 1.19).\nThree variables indicating general attitudes toward AI (usefulness, risk, and appreciation) were measured after finishing all scenarios. First, AI risk (M = 4.33, SD = 1.37) is the average response to five items, including \u201cUsing AI technologies is risky\u201d and \u201cAI technologies can lead to bad results\" (Cox & Cox, 2001). Second, AI usefulness (M = 4.55, SD = 1.46) was accessed with three items, including \u201cUsing AI technologies makes me save time\u201d (Nysveen, 2005). Lastly, to measure AI appreciation (M = 2.24, SD = 1.42), a prompt describing a medical situation was adopted from Bigman et al. (2020). The prompt started with \u201cImagine you are feeling severe shortness of breath and need to go to a hospital. There are two nearby hospitals. You know that both hospitals are running low on supplies and need to prioritize patients...\"\nAfter reading the prompt, participants were asked about a hospital they would want to go to and reported their preference on a seven-point scale ranging between 1 (\u201cthe hospital where the human doctor makes triage decisions\u201d) and 7 (\u201cthe hospital where an AI-based algorithm makes triage decisions\").\nFour variables indicating personal differences were also measured after all scenarios. First, for digital self-efficacy (M = 3.18, SD = 1.55), three items from Boerman et al. (2021) were adopted and modified to measure self-efficacy of digital technology use, such as \u201cI am able to protect my personal information when I use digital devices or services.\u201d Second, technical knowledge (M = 3.05, SD = 1.53) was measured with three items (Lee & Baykal, 2017). Third, following the previous approach of Araujo et al. (2020), belief in equality (M = 4.38, SD = 1.55) was measured with three items based on the World Value Survey (Ingelhart et al., 2020). Lastly,\""}, {"title": "", "content": "participants reported their political ideology (M = 3.64, SD = 1.11) on a standard five-point scale ranging from 1 (very conservative) to 5 (very liberal).\nThe participants provided information about their education level, which ranged from 1 (less than high school) to 8 (postgraduate or professional degree), and their income level, which ranged from 1 (less than $10,000) to 9 (more than $150,000)."}, {"title": "Statistical Analyses", "content": "The hypotheses and research questions were tested using several multivariate linear regressions. In all regression analyses, the identity dimension, discrimination target, and participants' gender and race were controlled. These controls were applied to ensure that the estimated associations between individual characteristics and Al perceptions are more generalizable by accounting for different identity dimensions, discrimination targets, and the social identities of the participants in their estimations. R, an open-source statistical software (version 4.0.3), was used to execute all statistical analyses reported in this study.\nOn average, the participants responded negatively about the fairness of the outcomes (outcome fairness: M = 3.14, SD = 1.09) and exhibited overall distrust (outcome trust: M = 2.73, SD = 0.94) and skepticism (outcome skepticism: M= 5.02, SD = 1.07) about the outcomes. The participants also expressed negative emotions about the outcomes (M = 4.52, SD = 1.32). The descriptive statistics of the variables are presented in Table 1."}, {"title": "Perceptions of Discriminatory AI Outcomes", "content": "The associations between individual characteristics and five dependent variables representing outcome perceptions were examined with five separate regression models presented in Table 2 and visualized in Figure 1."}, {"title": "Outcome Fairness", "content": "Belief in equality was negatively associated with outcome fairness, while income was positively associated with it, adjusting for influences of other characteristics of individuals. The negative association between belief in equality and outcome fairness (B = 0.072, SE = 0.027, p = .007) indicates that as equality belief increased, outcome fairness decreased. Income was positively associated with outcome fairness (B = 0.034, SE = 0.014, p = .016), indicating that the perceived fairness of biased outcomes increased as income level increased."}, {"title": "Outcome Trust", "content": "Digital self-efficacy, political ideology, and income were positively associated with outcome trust, controlling for other characteristics of individuals. First, digital self-efficacy was positively associated with outcome trust (B = 0.083, SE = 0.018, p < .001). It shows that participants with greater self-efficacy considered that AI made more trustworthy outcomes. Second, the positive association between political ideology and outcome trust (B = 0.088, SE = 0.033, p = .007) shows that participants with more conservative ideologies trusted Al outcomes more. Lastly, income was positively associated with outcome trust (B = 0.036, SE = 0.012, p = .003): as income increased, outcome trust grew."}, {"title": "Negative Emotion", "content": "Political ideology was negatively associated with negative emotion (B = -0.104, SE = 0.046, p = .023), controlling for other characteristics of individuals. It indicates that more conservative participants exhibited less negative emotion."}, {"title": "Outcome Skepticism", "content": "Political ideology was negatively associated with outcome skepticism (B = -0.081, SE = 0.037, p = .030), adjusting for the influence of individuals' other characteristics. It indicates that more liberal participants viewed discriminatory outcomes as more problematic."}, {"title": "Outcome Pervasiveness", "content": "Outcome pervasiveness had positive associations with technical knowledge and age. First, technical knowledge was positively associated with outcome pervasiveness (B = 0.082, SE = 0.023, p < .001), showing that people with more technical knowledge perceived that discriminatory outcomes were more likely to happen in their everyday lives. Also, age was positively associated with outcome pervasiveness (B = 0.010, SE = 0.002, p <.001), indicating that the perceived likelihood of biased outcomes increased as age increases."}, {"title": "General Attitudes Toward AI Use", "content": "The associations between individual characteristics and three dependent variables measuring general perceptions of AI were examined with three separate statistical models presented in Table 3 and visualized in Figure 2."}, {"title": "AI Usefulness", "content": "Digital self-efficacy, technical knowledge, and education were positively associated with AI usefulness. The positive association between digital self-efficacy and AI usefulness (B = 0.167, SE = 0.027, p < .001) indicates that people with greater self-efficacy perceived Al technologies are more useful. The other positive association between technical knowledge and AI usefulness (B = 0.162, SE = 0.028, p < .001) shows that AI technologies were viewed as more useful for participants who had more domain-specific knowledge of AI technologies. Lastly, as education level increased, perceived usefulness of AI also increased, as evidenced by the positive association between the two variables (B = 0.085, SE = 0.028, p = .002"}]}