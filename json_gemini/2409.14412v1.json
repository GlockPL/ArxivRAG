{"title": "COSBO: Conservative Offline Simulation-Based Policy Optimization", "authors": ["Eshagh Kargar", "Ville Kyrki"], "abstract": "Offline reinforcement learning allows training reinforcement learning models on data from live deployments. However, it is limited to choosing the best combination of behaviors present in the training data. In contrast, simulation environments attempting to replicate the live environment can be used instead of the live data, yet this approach is limited by the simulation-to-reality gap, resulting in a bias. In an attempt to get the best of both worlds, we propose a method that combines an imperfect simulation environment with data from the target environment, to train an offline reinforcement learning policy. Our experiments demonstrate that the proposed method outperforms state-of-the-art approaches CQL, MOPO, and COMBO, especially in scenarios with diverse and challenging dynamics, and demonstrates robust behavior across a variety of experimental conditions. The results highlight that using simulator-generated data can effectively enhance offline policy learning despite the sim-to-real gap, when direct interaction with the real-world is not possible.", "sections": [{"title": "Introduction", "content": "Offline reinforcement learning (offline RL) [15, 13] is defined as the task of learning the best possible policy from a fixed and pre-collected dataset without having interaction with the world. This leads to a setting for data reuse and safe policy learning applicable to robotics [19, 25, 18], autonomous driving [4, 6], and healthcare [22].\nDirectly applying online RL algorithms to offline RL settings produces poor results [11, 5, 24] because of the distribution shift between the offline dataset and the learned policy. So it needs designing algorithms specialized for offline RL [25]. Model-based algorithms, which learn a dynamics model from logged experience and perform some sort of pessimistic planning under the learned model, have emerged as a promising paradigm for offline reinforcement learning (offline RL). Some algorithms such as MOPO [24] and MOReL [11] try to do explicit uncertainty quantification for incorporating pessimism. Also, there are some other methods such as COMBO [25], which try to regularize the value function on out-of-support state-action tuples generated via roll-outs under the learned model to estimate a conservative value function without any explicit uncertainty estimation. However, learning dynamic models via deep learning can be difficult and unreliable and requires significant human expertise and hyper-parameter tuning to avoid collapsing [26].\nOn the other hand, for some tasks such as autonomous driving and robotics, in addition to a fixed and pre-collected dataset, we can have access to a simulator that has a dynamic mismatch. Using the simulation-generated data can help to learn a better policy [10]. In this work, our goal is to propose a new algorithm that retains the benefits of offline RL and uses simulation-generated data while removing the need for model learning.\nOur main contribution is developing a new simulation-based offline RL algorithm, COSBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the"}, {"title": "Related Work", "content": "Offline RL algorithms aim to train RL agents using only pre-collected datasets. To tackle the issue of state-conditional action distribution shift, previous methods either (a) explicitly constrain the policy to remain close to the behavior policy or (b) train pessimistic value functions.\nOne approach in offline RL algorithm design is to incorporate conservatism or regularization into the learning process. Model-free offline RL algorithms [5, 12, 23, 8, 13] directly integrate conservatism into policy or value function training without needing a dynamics model. However, these algorithms only learn from the states in the offline dataset, which can result in overly conservative behavior.\nIn contrast, model-based algorithms [11, 24] learn a pessimistic dynamics model that leads to a conservative value function estimate. Model-based RL has shown an early advantage in data efficiency over value-function-based RL by explicitly learning a transition model [20]. This advantage has been repeatedly demonstrated [3, 14, 17]. However, these algorithms rely on accurate uncertainty quantification of the learned dynamics model, which can be challenging for complex datasets or deep networks. Additionally, these methods do not adapt the uncertainty estimates as the policy and value function evolve during learning. Unlike MOReL [11], which uses a hard threshold on uncertainty to avoid drifting to unknown states, MOPO uses a soft reward penalty to handle uncertainty. This soft penalty allows the policy to take some risky actions and then return to safer regions near the behavior distribution without being terminated. COMBO, on the other hand, removes the need for uncertainty quantification.\nJiang et al. [9] use simulator states as goals based on expert states, recovering actions through horizon-adaptive inverse dynamics in an offline imitation learning setting. While the state-dependent discriminator helps focus on the expert distribution, it requires thorough training. Unlike offline imitation learning, which can learn better policies from data with rewards, PerSim [1] learns person-alized policies simultaneously across heterogeneous sources (agents) but needs low-rank latent factor representations. Despite its promising performance on standard RL benchmarks, PerSim cannot yet be used as an out-of-the-box solution for critical real-world problems without a rigorous validation framework."}, {"title": "Background", "content": "In this section, we will review the required theoretical concepts behind the proposed method, including Markov Decision Process, reinforcement learning, offline RL, model-based and model-free RL, and COMBO."}, {"title": "Markov Decision Process and Offline RL", "content": "We consider the standard Markov Decision Process (MDP) M = (S,A,T,r,\u00b5o, \u03b3) where S and A denote the state space and action space, respectively, T(s' s, a) the transition function, r(s, a) the reward function, \u00b5o the initial state distribution, and y \u2208 (0,1) the discount factor. We denote the"}, {"title": "COMBO", "content": "It is difficult to quantify uncertainty in all environments, especially complex ones, which is one of the main drawbacks of model-based algorithms before COMBO. COMBO proposes an offline RL algorithm that optimizes a lower bound for the policy performance without explicitly estimating uncertainty [25]. This approach combines model-free and model-based elements to leverage the strengths of both methods. By learning a dynamics model and using it to generate synthetic data, COMBO mitigates the conservativeness of model-free algorithms and avoids the challenges of uncertainty quantification in model-based approaches. This results in a more robust and efficient learning process that can handle complex environments effectively."}, {"title": "Method", "content": "Model-based RL algorithms have the major challenge that learning dynamic models through deep learning can be complicated and unreliable, requiring significant human expertise to avoid collaps-ing [26]. In this work, we propose a solution: an offline RL algorithm that uses simulation data in a new way to optimize a lower bound on policy performance without learning a dynamic model."}, {"title": "Conservative Policy Evaluation", "content": "With a policy of n, an offline dataset of D, and a simulator S of the environment in which the offline dataset was collected, we intend to learn a conservative estimate of Q. In order to accomplish this, Q-values are penalized on state-action pairs drawn from a distribution that is more likely to be out-of-support while Q-values on trustworthy state-action pairs are pushed up. Implementation is achieved by recursion as follows:\n$Q^{k+1}(s, a) \\leftarrow \\underset{Q}{\\operatorname{argmin}}\\limits_{\\beta} (E_{s, a \\sim p(s,a)}[Q(s,a)] - E_{s, a \\sim D}[Q(s,a)]) + \\frac{1}{2 \\beta} E_{s,a,s^{\\prime} \\sim d_f} [Q(s,a) - B^{Q^k}(s,a)]$\nwhere $B^{Q}(s,a) := r(s,a) +\\gamma Q(s',a')$ associated with a single transition $(s,a,s')$ and $a' \\sim \\pi(.|s')$.\np(s,a) and df are sampling distributions, chosen as\n$p(s,a) = d^{\\pi}_S(s)\\pi(a|s)$\n$d_f(s,a) := fd_\\mathcal{D}(s,a) + (1-f)d_\\pi(s,a)$"}, {"title": "Policy Improvement", "content": "The policy can be improved using the learned conservative critic $\\hat{Q}^{\\pi}$ as follows:\n$\\pi' \\leftarrow \\underset{\\pi}{\\operatorname{argmax}} E_{s \\sim p, a \\sim \\pi(.|s)}[\\hat{Q}^{\\pi}(s,a)]$\nwhere p(s) is the state marginal of p(s,a). The $argmax$ can be approximated with a few steps of gradient descent [25] when policies are parametrized with neural networks. Additionally, to prevent the policy from becoming degenerate an entropy regularization can be used [7]."}, {"title": "Experiments", "content": "We conducted a series of experiments to address the following research questions:\n1. How does the proposed method compare against state-of-the-art offline RL techniques, when using the original dataset (excluding data generated by the simulator)?\n2. What is the best way to utilize simulation data? Should it be concatenated with offline data from the target environment to train COMBO, or should our method be used to incorporate this data instead of relying on generated data from the learned model?\n3. How does changing the simulation dynamics affect the performance, and how large can the gap between simulation and the target environment be to yield useful results?"}, {"title": "Environments", "content": "To evaluate our proposed method, we tested it on two simulation tasks. We used environments with standard dynamics as the target (real) environments and modified the dynamics to create simulation environments with similar but slightly different dynamics. Specifically, we considered the Hopper and Walker2d tasks from the MuJoCo simulation suite."}, {"title": "Implementation Details", "content": "Our implementation is largely based on COMBO [25]. Although there is no official implementation of COMBO, we utilized an open-source version provided by Polixir 1 and adapted it to develop our method. The primary difference is that we do not learn any transition model; instead, we use a simulator with dynamics close to the main environment from which the offline dataset was collected. The rest of the implementation details align with those in the COMBO paper [25]. For a fair comparison, we used all the baselines from this repository."}, {"title": "Simulation Dynamics", "content": "This section describes the dynamics used in the simulator to generate data. For our experiments, we used datasets from D4RL [4] as the target environment, representing a fixed real-world dataset. We then employed a simulator with varying dynamics to simulate the mismatch between the offline dataset from the target environment and an imperfect simulator. We sampled state-action pairs from the offline dataset D, set the simulator to those states, and applied the actions. For each sample, we randomly selected one of the following dynamics:"}, {"title": "Comparison to Baselines", "content": "In response to question Q 1, we compared our method with CQL [13], MOPO [24], and COMBO [25], which are state-of-the-art model-free and model-based algorithms, using only the dataset from the target environment. In this experiment, the baseline algorithms utilized the target environment dataset, specifically hopper-medium. Model-based methods like MOPO and COMBO learn a dynamics model to generate data for their algorithms, whereas CQL, being a model-free algorithm, does not generate additional data. Our method, COSBO, uses a simulator with similar but different dynamics. COSBO employs (s, a) pairs from D and generates data by rolling out from state s and applying action a. Since the simulator's dynamics differ, it transitions to a new state s\" that is not present in D. Essentially, if (s, a, s') is a sample from D, the generated data would be (s, a, s\"). Fig 2 shows the results.\nTo address question Q 2, we investigated whether the performance improvement by COSBO is due to the newly generated data from the simulation or the method itself. We trained baselines using the concatenation of simulation-generated data and the main target dataset and compared them with our proposed method. This means the other methods used the same dataset as our method but with a different approach. As seen in Fig 3, baselines could not match COSBO's performance even with the same dataset. This highlights the effectiveness of our method in utilizing simulation data, rather than merely concatenating it with the target dataset. Notably, while CQL's performance remained unchanged, COMBO's performance improved. An interesting observation with COMBO was the variability in performance due to different random seeds, model weight initialization, and hyperparameters, indicating the sensitivity of model-based methods to these factors. In contrast, our algorithm demonstrated much more stable performance.\nIn addressing question Q 3, we explored how the performance is affected by the degree of difference between the simulator dynamics and the target environment. We generated data by altering the dynamics in the environment, considering several cases with \"Very\" and \"Extreme\" dynamic changes as shown in Table 1. The datasets for the Very/Extreme cases included four dynamic variations: very/extreme light, very/extreme heavy, very/extreme short, and very/extreme long. In each scenario, we changed only one dynamic at a time while keeping all others constant. For example, in the very light version, the mass was 0.3 times that of the target environment, with no change in length, and in the very long version, the length was 2 times that of the target environment.\nFigures 4 and 5 display the results for the Hopper and Walker2d environments, respectively, under varying dynamics. These experiments demonstrate COSBO's robustness across different dynamic scenarios, maintaining superior performance even with significant dynamic mismatches."}, {"title": "Conclusion", "content": "In this paper, we introduced COSBO, a novel offline RL algorithm that leverages simulation data to enhance learning. Our approach addresses the limitations of both model-free and model-based methods by using a simulator with similar but different dynamics to the target environment, thus generating useful and diverse data for training.\nWe conducted extensive experiments to evaluate the effectiveness of COSBO, comparing it to state-of-the-art algorithms such as CQL, MOPO, and COMBO. The results demonstrated that COSBO outperforms these baselines, particularly in scenarios with diverse and challenging dynamics. Our method not only improves performance but also shows greater stability and robustness across different experimental conditions.\nFurthermore, we explored the impact of varying dynamics in the simulation environment and found that COSBO maintains superior performance even with significant discrepancies between the simulator and the target environment. This highlights the potential of our approach to be applied in real-world settings where perfect modeling of the environment is infeasible.\nIn summary, COSBO provides a robust and efficient solution for offline RL by effectively integrating simulation data, thereby overcoming the conservatism of model-free methods and the complexity of uncertainty quantification in model-based methods. Future work could explore extending this approach to more complex tasks and further improving the integration of simulation data to enhance generalization and performance."}]}