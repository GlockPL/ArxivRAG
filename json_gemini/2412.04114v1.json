{"title": "Thermal and RGB Images Work Better Together in Wind Turbine Damage Detection", "authors": ["SERHII SVYSTUN", "OLEKSANDR MELNYCHENKO", "PAVLO RADIUK", "OLEG SAVENKO", "ANATOLIY SACHENKO", "AND ANDRII LYSYI"], "abstract": "The inspection of wind turbine blades (WTBs) is crucial for ensuring their structural integrity and operational efficiency. Traditional inspection methods can be dangerous and inefficient, prompting the use of unmanned aerial vehicles (UAVs) that access hard-to-reach areas and capture high-resolution imagery. In this study, we address the challenge of enhancing defect detection on WTBs by integrating thermal and RGB images obtained from UAVs. We propose a multispectral image composition method that combines thermal and RGB imagery through spatial coordinate transformation, key point detection, binary descriptor creation, and weighted image overlay. Using a benchmark dataset of WTB images annotated for defects, we evaluated several state-of-the-art object detection models. Our results show that composite images significantly improve defect detection efficiency. Specifically, the YOLOv8 model's accuracy increased from 91% to 95%, precision from 89% to 94%, recall from 85% to 92%, and F1-score from 87% to 93%. The number of false positives decreased from 6 to 3, and missed defects reduced from 5 to 2. These findings demonstrate that integrating thermal and RGB imagery enhances defect detection on WTBs, contributing to improved maintenance and reliability.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid expansion of wind energy as a sustainable power source has led to the widespread installation of wind turbines across diverse and often remote locations. Ensuring the efficient and uninterrupted operation of these wind turbines is critical, necessitating regular inspections to detect possible technical defects and mechanical damages [1, 2]. Traditional inspection methods, such as manual visual inspections and rope-access techniques, are time-consuming, risky, and often inadequate for thoroughly examining the complex structures of wind WTBs.\nUAVs have emerged as a transformative technology in the field of wind turbine inspection, offering flexible systems that can adapt to changing environmental conditions and expand monitoring capabilities. UAVs are now widely used in various sectors, including tracking forest fires [3], monitoring crop yields [4], and inspecting green energy facilities [5], such as wind turbines [6, 7]. The use of UAVs significantly increases the efficiency of the monitoring process [8], providing high-resolution images and access to hard-to-reach areas [9], which are often inaccessible or hazardous for human inspectors.\nDespite these advancements, wind turbine monitoring faces several challenges. Wind turbines are subjected to harsh environmental conditions, leading to various forms of degradation such as erosion, cracks, delamination, and lightning strikes. Timely detection of these defects is essential to prevent critical wind turbine failures or costly downtime. However, the sheer size and height of modern turbines, along with their complex blade geometries, make comprehensive inspections difficult [10].\nModern UAV technologies equipped with advanced imaging sensors have the potential to address these challenges. The integration of multispectral cameras, including thermal imaging and RGB cameras [11], provides additional information about the condition of the turbines. Thermal images can reveal temperature anomalies [12] that may indicate"}, {"title": "II. RELATED WORKS", "content": "Recently, multispectral imaging systems have been actively employed in various critical industries, including infrastructure monitoring [25] and energy management [26]. The integration of UAVs significantly enhances the capabilities of these systems [27, 28], owing to UAVs' mobility and ability to access hard-to-reach areas [29]. This combination is especially beneficial for inspecting large structures like WTBs.\nFor instance, Zhang et al. [30] presented a method that combines high-resolution RGB imagery with thermal imaging to enhance defect detection in wind turbines. Although this approach capitalizes on the strengths of both imaging types, it mainly emphasizes data fusion, overlooking the challenges related to precise spatial alignment and integration essential for DL applications in wind turbine inspections.\nAnother work, Morando et al. [31], introduced a UAV-based inspection strategy for photovoltaic (PV) systems, employing both RGB and thermal cameras to track PV modules without GPS. While their technique improves inspection efficiency by optimizing flight paths and minimizing tracking errors, it is tailored specifically for PV plants and does not address the complexities of WTB inspections, such as blade curvature and diverse environmental conditions.\nZhou et al. [32] proposed an adaptive feature fusion module for RGB and infrared images, achieving significant detection accuracy improvements of up to 99%. However, their work is focused on general visual data processing, lacking a specific emphasis on wind turbine monitoring. The absence of considerations for UAV-acquired imagery and the distinctive challenges of turbine inspections limits the method's applicability in this domain.\nZhu et al. [33] addressed defect detection in WTBs through a multifunctional residual feature fusion network utilizing transfer learning. Although their method demonstrates potential, it relies extensively on pre-trained models and does not explore the fusion of UAV-captured thermal and RGB images. This gap may reduce its effectiveness in identifying a broader spectrum of defects, particularly those not easily identifiable through a single imaging modality.\nKwon et al. [34] proposed a calibration technique for active optical thermography, facilitating the detection of various WTB defects. While this approach advances thermal imaging, it lacks the integration of RGB data or sophisticated image processing methods, which could enhance assessment when used together. Similarly, Sanati et al. [35] examined passive and active thermography techniques to improve thermal image quality; however, their approach's accuracy and reliability are limited without incorporating RGB imagery and DL algorithms.\nIn their recent work, Memari et al. [36] successfully integrated multispectral imaging with ensemble learning for enhanced anomaly detection in small WTBs, improving detection accuracy through data fusion. Nevertheless, their study focuses on small turbines and does not address the challenges of scaling the approach to larger turbines or adapting to different environmental conditions, which are critical factors in practical, real-world applications.\nIn summary, taking into account the current limitations in the industry, this study aims to enhance defect detection in WTBs by developing a novel method that effectively composes multispectral images obtained from UAVs. By transforming spatial coordinates into a software coordinate model, optimizing errors, and minimizing them, our proposed method aims to enhance the accuracy and efficiency of inspections. The key scientific contributions of this study include:\n\u2022 Developing an advanced image composition technique that seamlessly integrates thermal and RGB data from UAVs.\n\u2022 Leveraging DL models to process the fused imagery for improved defect detection and classification.\n\u2022 Demonstrating the method's effectiveness in real-world wind turbine inspections, addressing scalability and environmental challenges."}, {"title": "III. METHOD", "content": "Thermal images provide critical information about temperature anomalies in green energy facilities. However, their resolution is lower compared to images obtained by RGB cameras. Combining data from both types aims to synchronise a new detailed state of the object model under study. The method's formalisation is presented in the scheme in Fig. 1.\n{\\bf A. TRANSFORMATION OF SPATIAL COORDINATES}\nBlock 1 of the proposed method involves transforming spatial coordinates into a software coordinate model five steps.\nStep 1.1. Calculate the rotation matrix and translation vector. After receiving the input data, the rotation matrix R and translation vector t are calculated to integrate the spatial coordination of images from multiple sensors into a single coordinate system. It is aimed to minimise errors between the visual point coordinates of each UAV optical sensor.\nStep 1.2. Calculate projection coordinates. The process of calculating projection coordinates (u,v) is conducted using the current parameters. The spatial coordinates on the image plane are determined by the formulas:\n$u=f\\frac{X}{Z}+c_x$ and $v = f\\frac{Y}{Z}+c_y$,\nwhere f is the focal length, and $(c_x,c_y)$ are the principal point coordinates, X are the coordinates of the point in the camera coordinate system, and X are the coordinates of the point in three-dimensional space.\nStep 1.3. Determine an error. We determine the error $e_i$ as the difference between the actual coordinates of points on the image and the projection coordinates calculated by formula (1):\n$e_i = \\sqrt{(u-u_i)^2+(v-v_i)^2}$,\nwhere (u,v) are the actual coordinates of the point on the image and $(u_i, v_i)$ are the calculated coordinates of the point.\nStep 1.4. Optimise parameters. The algorithmic sequence for calculating projection coordinates and determining errors requires optimising the parameters of the mathematical model to minimise error values. For this, an optimisation method using the Jacobian matrix [37], which contains partial derivatives of the errors concerning each model parameter, is applied. The formalisation of this process is given as follows:\n$J = \\begin{pmatrix}\n\\frac{\\partial e_1}{\\partial K_1} & \\frac{\\partial e_1}{\\partial K_2} & ... & \\frac{\\partial e_1}{\\partial D_1} & ... & \\frac{\\partial e_1}{\\partial D_n} \\\\\n\\frac{\\partial e_2}{\\partial K_1} & \\frac{\\partial e_2}{\\partial K_2} & ... & \\frac{\\partial e_2}{\\partial D_1} & ... & \\frac{\\partial e_2}{\\partial D_n} \\\\\n\\vdots & \\vdots & & \\vdots & & \\vdots \\\\\n\\frac{\\partial e_N}{\\partial K_1} & \\frac{\\partial e_N}{\\partial K_2} & ... & \\frac{\\partial e_N}{\\partial D_1} & ... & \\frac{\\partial e_N}{\\partial D_n}\n\\end{pmatrix}$\nwhere $\\frac{\\partial e_i}{\\partial K_i}$ and $\\frac{\\partial e_i}{\\partial D_i}$ are the partial derivatives of error $e_i$ with respect to the model parameters $K_i$ and $D_i$.\nThe error $e_i$ from formula (2) for each point is determined as the difference between the three-dimensional coordinates and the coordinates obtained using the current model. The partial derivative of error $e_i$ to parameter $K_i$ shows how much the value of $e_i$ changes with a small change in parameter $K_i$. At the same time, other parameters remain unchanged and are defined as follows:\n$\\frac{\\partial e_i}{\\partial K_j} = \\lim_{\\Delta K_j \\to 0} \\frac{e_i(K_1, K_2,..., K_j + \\Delta K_j,..., K_m)-e_i(K_1, K_2,..., K_j,..., K_m)}{\\Delta K_j}$\nSimilarly, the partial derivative for $D_i$ is determined. The numerical calculation of the partial derivative concerning the elements of the rotation matrix R is performed, according to the formula:\n$\\frac{\\partial e_i}{\\partial R_{jk}} = \\frac{e_i(R_{jk} + \\Delta R_{jk})-e_i(R_{jk})}{\\Delta R_{jk}}$\nwhere $\\Delta R_{jk}$ is a small change in element $R_{jk}$\n\nStep 1.5. Update parameters. After determining the partial derivatives of errors for all model parameters according to formula (4), the Levenberg-Marquardt method [38] is used to update the parameters and minimise errors. The parameter update process is as follows:\n$\\theta_{k+1} = \\theta_k - (J^T J + \\lambda I)^{-1} J^T e$,\nwhere $\\theta$ is the vector of current model parameters, J is the Jacobian matrix calculated by formulas (3), $\\lambda$ is the damping parameter, I stands for the identity matrix, and e represents the error vector.\nThe process formalised by formula (5) is repeated iteratively until convergence is achieved, i.e., until the changes in the parameters become insignificant or the errors reach a minimum value.\nErrors for all points are given as the sum of the Euclidean distances between their calculated and measured coordinates. The total error is calculated as follows:\n$E = \\sum_{i=1}^{N} \\sqrt{\\frac{X}{Z}+c_u-U_i}^2 + \\sqrt{\\frac{Y}{Z}+c_v-V_i}^2$,\nwhere $u_i = f\\frac{X}{Z}+c_u$ and $v_i = f\\frac{Y}{Z}+c_v$, are the calculated program coordinates along the u and v axes, respectively, f is the focal length, $c_u$ and $c_v$ are the principal point coordinates.\nFormula (6) reflects the differences between the calculated program coordinates and the actual measured values for each point, allowing for the assessment of model accuracy according to the established criteria for each UAV optical sensor separately.\n{\\bf B. COMPOSITION OF VISUAL DATA STRUCTURES}\nBlock 2 of the method performs the composition of visual data structures (see Fig. 2) according to the following steps."}, {"title": "V. CONCLUSIONS", "content": "In this study, we presented a novel method for enhancing defect detection on WTBs by integrating thermal and RGB imagery captured by unmanned aerial vehicles. Our multispectral image composition method effectively combines thermal and RGB images, leading to significant improvements in defect detection performance: the YOLOv8 model's accuracy increased from 91% to 95%, precision from 89% to 94%, recall from 85% to 92%, and F1-score from 87% to 93%. Additionally, the number of false positives decreased from 6 to 3, and missed defects reduced from 5 to 2. Despite these promising results, the proposed approach has limitations, including increased computational complexity due to additional image processing steps and potential challenges in aligning thermal and RGB images under varying environmental conditions.\nFuture work will focus on optimizing the method to reduce processing time, enhancing image alignment techniques, and integrating the approach into real-time UAV inspection workflows to further improve the efficiency and applicability of WTB maintenance."}]}