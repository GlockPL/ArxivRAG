{"title": "Joint Hypergraph Rewiring and Memory-Augmented Forecasting Techniques in Digital Twin Technology", "authors": ["Sagar Srinivas Sakhinana", "Krishna Sai Sudhir Aripirala", "Shivam Gupta", "Venkataramana Runkana"], "abstract": "Digital Twin technology creates virtual replicas of physical objects, processes, or systems by replicating their properties, data, and behaviors. This advanced technology offers a range of intelligent functionalities, such as modeling, simulation, and data-driven decision-making, that facilitate design optimization, performance estimation, and monitoring operations. Forecasting plays a pivotal role in Digital Twin technology, as it enables the prediction of future outcomes, supports informed decision-making, minimizes risks, driving improvements in efficiency, productivity, and cost reduction. Recently, Digital Twin technology has leveraged Graph forecasting techniques in large-scale complex sensor networks to enable accurate forecasting and simulation of diverse scenarios, fostering proactive and data-driven decision-making. However, existing Graph forecasting techniques lack scalability for many real-world applications. They have limited ability to adapt to non-stationary environments, retain past knowledge, lack a mechanism to capture the higher-order spatio-temporal dynamics, and estimate uncertainty in model predictions. To surmount the challenges, we introduce a hybrid architecture that enhances the hypergraph representation learning backbone by incorporating fast adaptation to new patterns and memory-based retrieval of past knowledge. This balance aims to improve the slowly-learned backbone and achieve better performance in adapting to recent changes. In addition, it models the time-varying uncertainty of multi-horizon forecasts, providing estimates of prediction uncertainty. Our forecasting architecture has been validated through ablation studies and has demonstrated promising results across multiple benchmark datasets, surpassing state-of-the-art forecasting methods by a significant margin.", "sections": [{"title": "Introduction", "content": "Digital twins have several applications in various domains, including finance, retail and e-commerce, logistics and transport, healthcare, and many other domains. Digital Twins are useful in finance for risk management, trading, and investment decision-making. They enable financial institutions to simulate different scenarios and identify potential risks before they occur. They can help traders identify profitable opportunities and optimize their trades, while also allowing investors to model different economic scenarios and market conditions for better portfolio allocation strategies. Digital twins are useful in retail and ecommerce for creating virtual replicas of products, stores, and supply chains. This capability can contribute to transforming product design and development, streamlining operations, enhancing customer experiences, and driving sales growth. Digital Twins can be used in electricity pricing, auction, and design to optimize energy efficiency, reduce costs, and improve electricity markets. They can help energy analysts detect potential issues and optimize the layout and design of electricity grids to enhance energy efficiency and reduce costs. They can also assist electricity retailers in optimizing bidding strategies in electricity auctions to increase profits and reduce costs. Load forecasting is a crucial application of Digital Twins in electricity pricing, as it enables electricity distributors to accurately anticipate electricity demand and dynamically adjust pricing in real-time to prevent blackouts or brownouts. The digital twin technology involves creating a digital counterpart of a tangible entity, such as a machine, complex systems, or other physical objects. The creation of a digital twin involves utilizing diverse data sources, such as real-time sensor data, historical data, and other relevant information. By integrating this data into a processing system, the digital twin can effectively observe and record the key functionalities of the tangible entity. For instance, if the tangible entity under consideration is a gas turbine, a digital twin of the physical object would be created to mirror its exact specifications, such as size, shape, and technical features. Real-time sensor data from the turbine, including fuel injection rate, air-fuel ratio, inlet air temperature, and exhaust emissions, would be collected and fed into the digital twin. Subsequently, the digital twin would analyze this data and offer insights into the condition monitoring of the gas turbine. The digital twin can be employed to run simulations and analyze performance concerns for a wide range of applications, including fault diagnosis, safety monitoring, and performance optimization. The digital twin technology offers the opportunity to test potential upgrades to a physical object in a virtual environment prior to real-world implementation. This approach provides valuable insights that can be implemented on the physical object, resulting in the ability to improve operational efficiency, minimize downtime, and reduce maintenance expenses. Of particular interest in this work is digital twin technology for forecasting of complex dynamical systems. Forecasting is a critical aspect of digital twin technology as it enables accurate predictions of the behavior of a physical object, enabling proactive maintenance, operational efficiency improvement and safety monitoring. Furthermore, the digital twin can forecast the expected behavior of the physical object in different scenarios, enabling operators to optimize its performance and reduce downtime, while minimizing risks associated with implementing untested changes on the actual physical object. As a result, it is imperative to develop accurate models of physical systems in order to create Digital Twins that can faithfully replicate the behavior of the physical systems for forecasting purposes."}, {"title": "Related Work on Time Series Forecasting", "content": "Accurately forecasting the behavior of complex dynamical systems, which are characterized by high-dimensional multivariate time series(MTS) in interconnected sensor networks, is crucial for enabling well-informed decision-making in various applications. Forecasting MTS data is challenging due to the intricate relationships among multiple time series variables and the unique features of MTS data, including non-linearity, high-dimensionality and non-stationarity. The spatio-temporal graph neural networks(STGNNs) have become a popular approach to model the relational dependencies between time series variables in the MTS data for multivariate time series forecasting. Several researchers (e.g., [Wu et al., 2019a; Bai et al., 2020a; Wu et al., 2020; Yu et al., 2018a; Chen et al., 2022; Li et al., 2018a]) have contributed to this trend, and their work has significantly advanced the use of GNNs in time series forecasting task. Training STGNNs on the fly is challenging due to their inability to adjust to non-stationary environments and retain past knowledge. The ability of STGNNs to adapt quickly is critical, and successful approaches must handle changes to both new and recurring patterns effectively. However, STGNNs, despite their strong representation learning capabilities, face two major challenges when dealing with time series data streams. Firstly, training STGNNs on data streams in a straightforward manner requires a considerable number of samples to converge. This is because mini-batches or multiple epoch training, commonly used in offline training, are not feasible. Thus, when there is a distribution shift, such neural architectures can become cumbersome and require a large number of samples to learn new concepts effectively, which can ultimately result in suboptimal performance. In essence, the primary challenge lies in the absence of a mechanism within STGNNs to facilitate learning on continuously generated data streams effectively. As a result, the STGNNs must adapt to new trends and patterns in data streams over time. The second challenge arises from the fact that time series data frequently displays recurring patterns that may cease to exist temporarily and then reappear in the future. STGNNs are prone to the catastrophic forgetting phenomenon, whereby the model discards previously acquired knowledge when presented with new data, leading to suboptimal learning of recurring patterns. As a result, this limitation further hinders the overall performance of STGNNs for time series forecasting. Existing STGNNs can learn MTS data dynamics by simultaneously inferring discrete dependency graph structures or by leveraging domain expertise knowledge of predefined relationships among multiple time series variables. While complex dynamical systems consist of interconnected networks, these networks may have higher-order structural relations that extend beyond pairwise associations. Hypergraphs, which provide a more generalized representation of graphs, can effectively model such relations in high-dimensional MTS data. Furthermore, conventional STGNNs prioritize pointwise forecasting and do not offer uncertainty estimates associated with these multi-horizon forecasts. To tackle these challenges, we introduce the Joint Hypergraph Rewiring and Forecasting Neural Framework, which we will refer to as JHgRF-Net for brevity. The proposed framework achieves continual learning by balancing two objectives: (i) leveraging prior knowledge to facilitate rapid learning of current trends and patterns, and (ii) maintaining and updating previously acquired knowledge. The JHgRF-Net framework achieves dynamic balance between rapid adaptation to recent changes and retrieval of similar old knowledge by leveraging the interaction between two complementary components: the Spatio-Temporal Hypergraph Convolutional Network(STHgCN) and the Spatio-Temporal Transformer Network(STTN). The Mixture of Experts(MOE) approach is utilized to design algorithmic architecture for hypergraph time series forecasting. This approach involves using the aforementioned set of complementary modeling approaches, whose predictions are combined to create a robust mechanism capable of improving the overall accuracy of forecasting. The STHgCN neural operator simultaneously infers discrete dependency hypergraph structure and learns MTS data dynamics. The STHgCN neural operator consists of two sequentially operating modules: hypergraph-structure learning(HgSL) and hypergraph representation learning(HgRL). The HgSL module infers the discrete dependency hypergraph structure and performs hypergraph rewiring to modify the hyperedges so that they better reflect the dependencies between hypernodes. This can involve adding or removing hyperedges to optimize the relational structure between hypernodes. The HgRL module models the spatio-temporal dynamics underlying the hypergraph-structured MTS data for multi-horizon forecasting. The STTN neural operator learns the underlying dynamics of MTS data beyond the original sparse relational hypergraph structure through a self-attention mechanism. The STTN neural operator learns the underlying dynamics of MTS data beyond the original sparse relational hypergraph structure through a self-attention mechanism. A gating mechanism is utilized to regulate the information flow from complementary components. This mechanism further distills knowledge and improves the accuracy and reliability of the model's predictions. Moreover, the framework captures time-varying uncertainty in forecasts. As a result, the framework provides accurate multi-horizon predictions and reliable uncertainty estimates of forecasts. Furthermore, the framework is designed to provide superior generalization and scalability for large-scale spatio-temporal MTS forecasting tasks that are commonly encountered in real-world applications."}, {"title": "Problem Formulation", "content": "Let us consider a historical time series dataset with $n$ correlated variables observed over $T$ time steps. The dataset is represented by the notation $\\mathbf{X} = \\{x_1, ..., x_T\\}$, where the subscript indicates the time step. The observations of all the variables at time step $t$ are denoted by $x_t = (x^{(1)}, x^{(2)}, ..., x^{(n)}) \\in \\mathbb{R}^{n \\times c}$, where the superscript refers to the variables. Each sensor can measure multiple physical quantities denoted by $c$. For example, in intelligent transportation systems, the traffic loop detectors or traffic sensors placed across travel lanes can simultaneously measure three parameters: traffic flow, speed, and volume. Therefore, in this particular case, $c = 3$. In MTSF, we use a rolling-window technique to predict the future values of $n$-correlated variables for the forecast horizon. At each time step $t$, we define a look-back window which includes the prior $\\tau$-steps of time series data to predict the next $v$-steps. We use a historical window of $n$-correlated variables, observed over the previous $\\tau$-steps prior to time step $t$, represented by $\\mathbf{X}_{t-\\tau : t-1} \\in \\mathbb{R}^{n \\times \\tau \\times c}$, to predict the future values of $n$-variables for the next $v$-steps, represented by $\\mathbf{X}_{t:t+v-1} \\in \\mathbb{R}^{n \\times v \\times c}$. To capture complex higher-order relationships among variables within the MTS data, we represent the historical data as continuous-time spatial-temporal hypergraphs denoted by $G_t$. Hypergraphs consist of hypernodes($\\mathcal{V}$), representing time series variables and hyperedges($\\mathcal{E}$) that capture hierachial relationships among an arbitrary number of hypernodes. The time-dependent hypernode feature matrix is denoted by $\\mathbf{X}_{t-\\tau : t-1}$. We learn the implicit hypergraph structure through an embedding-based similarity metric learning approach. The incidence matrix $\\mathbf{I} \\in \\mathbb{R}^{n \\times m}$ describes the hypergraph structure, where $I_{p,q} = 1$ if hyperedge $q$ is incident with hypernode $p$, and $0$ otherwise. Hypergraph sparsity is determined by the number of hyperedges in the hypergraph. In a sparse hypergraph, the number of hyperedges($m = |\\mathcal{E}|$) is relatively small compared to the number of hypernodes($n = |\\mathcal{V}|$), while in a dense hypergraph, the number of hyperedges is relatively large. Sparser hypergraphs generally result in more efficient algorithms, due to the impact of hypergraph sparsity on computational efficiency and algorithmic complexity. A hypergraph with more hyperedges has a denser and more complex structure, resulting in a higher level of connectivity among the hypernodes. Conversely, a hypergraph with fewer hyperedges has a sparser structure with fewer connections between the hypernodes. The proposed framework aims to learn a differentiable function $\\mathcal{F}(\\theta)$ that can predict the future estimates $\\mathbf{X}_{t:t+v-1}$, of historical window inputs $\\mathbf{X}_{t-\\tau : t-1}$, given a hypergraph $G_t$. To put it briefly, the function $\\mathcal{F}(\\theta)$ takes in the past observations and hypergraph structure, represented by $[\\mathbf{x}_{t-\\tau}, ..., \\mathbf{x}_{t-1}; G_t]$, and predict future observations, denoted as $[\\mathbf{x}_{t+1}, ..., \\mathbf{x}_{t+v-1}]$. This is mathematically represented as:\n\\[[\\mathbf{X}_{t-\\tau}, ..., \\mathbf{X}_{t-1}; G_t] \\xrightarrow{\\mathcal{F}} [\\mathbf{x}_{t+1}, ..., \\mathbf{X}_{t+v-1}]]\n\nThe MTSF task formulated on the implicit hypergraph($G_t$), can be expressed as shown below:\n\\[\\min_\\theta L_{MAE} (\\mathbf{X}_{t:t+v-1}, \\hat{\\mathbf{X}}_{t:t+v-1}; \\mathbf{X}_{t-\\tau : t-1}, G_t)\\]\nThe function $\\mathcal{F}(\\theta)$ involves a set of parameters $\\theta$ which can be trained to optimize its performance. The predicted future observations is denoted by $\\hat{\\mathbf{X}}_{t:t+v-1}$. To train the learning algorithm, we minimize the loss function denoted by $L_{MAE}$, i.e., the mean absolute error(MAE), which is defined as:\n\\[L_{MAE} = \\frac{1}{\\eta} |\\mathbf{X}_{t:t+v-1} - \\hat{\\mathbf{X}}_{t:t+v-1}|\\]\nHere, $\\mathbf{X}_{t:t+v-1}$ is the actual future MTS data, and $\\eta$ is a scaling factor."}, {"title": "OUR APPROACH", "content": "Our proposed neural forecasting framework consists of two key components: the projection layer and the spatio-temporal feature extractor, as shown in Figure 1. The spatio-temporal inference component includes two distinct methods for hypergraph representation learning: the Spatio-Temporal Hypergraph Convolutional Network(STHgCN) and the Spatio-Temporal Transformer Network(STTN). The STHgCN method employs hypergraph as a mathematical model for learning the underlying higher-order relations of the time series variables. This is achieved by optimizing the discrete hypergraph structure underlying the observed data. It then peforms the gated hypergraph convolution operations on the hypergraph-structured MTS data to model the intricate spatio-temporal dynamics within the latent hypernode-level representations. The final representations can then be used to predict multi-horizon forecasts. The STTN method is a powerful technique for modeling the hypergraph-structured MTS data. The STTN method extends transformer networks to handle arbitrary sparse hypergraph structures with full attention as a useful inductive bias. This enables the model to learn intra- and inter-correlations among the variables without being limited by the hierarchical structural information underlying the MTS data. It leverages task-specific relations between variables beyond the original sparse structure to generate expressive hypernode-level representations that improve forecast accuracy. We use a gating mechanism to regulate the flow of information from the two methods. This enables us to learn optimal representations of the hypernode-level representations that capture the accurate dynamics of complex interconnected sensor networks. To summarize, our framework performs the joint optimization of the different learning components to generate accurate forecasts across multiple forecast horizons, while also ensuring reliable estimates of uncertainty for time-series forecasting tasks."}, {"title": "Projection Layer", "content": "The proposed framework uses a projection layer with gated linear networks(GLN, [Dauphin et al., 2017]) to obtain nonlinear representations of input data. Specifically, the input data $\\mathbf{X}_{t-\\tau : t-1} \\in \\mathbb{R}^{n \\times \\tau \\times c}$ is transformed through a gating mechanism, resulting in $\\hat{\\mathbf{X}}_{t:t+v-1} \\in \\mathbb{R}^{n \\times v \\times d}$, which represents the non-linear transformed input data. It is described as follows:\n\\[\\hat{\\mathbf{X}}_{t:t+v-1} = (\\sigma(\\mathbf{W}_0 \\mathbf{X}_{t-\\tau : t-1}) \\odot \\mathbf{W}_1 \\mathbf{X}_{t-\\tau : t-1})\\mathbf{W}_2\\]\nHere, the trainable weight matrices are $\\mathbf{W}_0, \\mathbf{W}_1 \\in \\mathbb{R}^{c \\times d}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{d \\times v}$, and the element-wise multiplication is denoted by $\\odot$. The utilization of a non-linear activation function $\\sigma$ improves representation learning and enables the framework to effectively learn and model complex patterns present in the MTS data."}, {"title": "Spatial-Inference", "content": "Figure 2 illustrates the spatio-temporal feature extractor of the framework, which consists of two distinct methods(STHgCN and STTN). Further information regarding each method will be elaborated in the subsequent sections."}, {"title": "Spatio-Temporal Hypergraph Convolutional Network(STHgCN)", "content": "The STHgCN method comprises sequentially operating modules, including hypergraph structure learning(HgSL) and hypergraph representation learning(HgRL) modules. The following sections will elaborate on each module and provide more details."}, {"title": "Implicit hypergraph Inferenece", "content": "The HgSL module uses an embedding-based similarity metric learning technique to capture higher-order dependency relationships between different variables in the MTS data and computes an optimal discrete hypergraph structure for a hypergraph-structured representation of the MTS data. In short, the implicit hypergraph provides a spatio-temporal inductive bias that enables a structured representation of the MTS data, capturing the underlying relationships and dependencies among the variables. The hypernodes and hyperedges of the hypergraph are represented by the differentiable embeddings in the $d$-dimensional vector space, $z_i, z_j \\in \\mathbb{R}^{(d)}$, where $1 \\leq i \\leq n$ and $1 \\leq j \\leq m$. By leveraging the learned embeddings to transform the MTS data into a hypergraph-structured time series data, the HgSL module computes the optimal hypergraph topology that captures the task-relevant relationships and dependencies among the variables, making it a powerful tool for learning relational hypergraph structures from complex MTS data. The pairwise similarity($P_{i,j}$) between any pair of $z_i$ and $z_j$ is computed as follows:\n\\[P_{i,j} = \\sigma( \\mathbf{s}([\\mathbf{z}_i || \\mathbf{z}_j])) ;  \\mathbf{S}_{i,j} = 2 \\frac{\\mathbf{z}_i^T \\mathbf{z}_j}{\\lVert\\mathbf{z}_i\\rVert_2 \\lVert\\mathbf{z}_j\\rVert_2} + 1\\]\nwhere $||$ denotes vector concatenation. The differentiable sigmoid activation function is applied to map the pairwise scores to the interval [0,1]. The hyperedge probability over hypernodes of the hypergraph is represented as $P^{(k)}_{i,j} \\in \\mathbb{R}^{n \\times m \\times 2}$, where $k \\in \\{0,1\\}$. The scalar value of $P^{(k)}_{i,j} \\in [0, 1]$ indicates the relationship between a pair of hypernodes and hyperedges, indexed by $(i, j)$. To be precise, $P^{(0)}_{i,j}$ represents the probability of hypernode $i$ being connected to hyperedge $j$, while $P^{(1)}_{i,j}$ denotes the probability that hypernode $i$ is not connected to hyperedge $j$. To accurately and efficiently sample discrete hypergraph structures from the hyperedge probability distribution $P_{i,j}$, we leverage the Gumbel-minisoftmax trick introduced in [Jang et al., 2016]. This technique is powerful in capturing complex relationships among variables in MTS data, making the HgSL module more effective. The connectivity pattern of the hypergraph structure is then represented using an incidence matrix $I \\in \\mathbb{R}^{n \\times m}$, which captures the relationships between hypernodes and hyperedges in the hypergraph. By using the Gumbel-softmax trick, we can learn the hypergraph structure in an end-to-end differentiable manner. Thus, it becomes possible to apply the gradient-based optimization methods during model training, enabling an inductive-learning approach to learn complex underlying structures within the MTS data. The Gumbel-Softmax trick involves using random noise from the Gumbel distribution to perturb the hyperedge probability distribution and then sampling the optimal discrete structure from the distribution using the Softmax function. The incidence matrix is obtained as,\n\\[I_{i,j} = \\frac{\\exp((P^{(k)}_{i,j} + \\epsilon) / \\tau)}{\\sum_{k \\in {0, 1}} \\exp((P^{(k)}_{i,j} + g_{i,j}^{(k)} + \\epsilon) / \\tau)}\\]\nwhere the temperature parameter($\\tau$) of Gumbel-Softmax trick is set to 0.05, and $\\epsilon$ is a small constant added to avoid numerical instability. Random noise, denoted by $g_{i,j}^{(k)} \\sim \\mathcal{G}umbel(0, 1) = - \\log(- \\log(U(0, 1)))$ is sampled from the Gumbel distribution, where U represents the uniform distribution with a range of 0 to 1. We optimize the hypergraph distribution parameters to ensure that the learned hypergraph is sparse, eliminating redundant hyperedges over hypernodes. The forecasting task provides indirect supervisory information that helps to reveal the hypergraph relation structure in the observed MTS data. In summary, the HgSL module learns the latent hypergraph structure of multiple interacting time series variables to create a structured representation of the time series data, which facilitates downstream multi-horizon forecasting with predictive uncertainty estimation."}, {"title": "Hypergraph Attention Network(HgAT)", "content": "The HgAT neural operator extends attention-based convolution operations to non-Euclidean domains, such as hypergraphs. It accurately models the complex hypergraph-structured MTS data, thereby improving multi-horizon forecast accuracy. The HgAT operator captures spatial correlations among time-series variables by encoding relational inductive bias within the hypergraph's connectivity. It performs message-passing schemes to propagate information through the hypergraph-structured MTS data, which is characterized by an incidence matrix (represented by $I \\in \\mathbb{R}^{n \\times m}$) and a feature matrix (represented by $\\mathbf{X}_{t:t+v-1} \\in \\mathbb{R}^{n \\times v \\times d}$) to compute the hypernode representation matrix (represented by $\\mathbf{H}_{t:t+v-1} \\in \\mathbb{R}^{n \\times v \\times d}$). Each row in the matrix $\\mathbf{H}_{t:t+v-1}$ represents the hypernode representations, $h_i \\in \\mathbb{R}^{v \\times d}$. The HgAT operator captures relationships among time-series variables by encoding structural and feature characteristics of spatio-temporal hypergraphs in hypernode representations. It adapts to changes in time-series variable dependencies over time in the hypernode representations $h_i$. The HgAT operator models spatio-temporal correlations among time-series variables in hypergraph-structured MTS data using intra-edge and inter-edge neighborhood aggregation schemes. The intra-edge aggregation considers hypernodes associated with a specific hyperedge, while inter-edge aggregation considers hyperedges connected to a specific hypernode. In hypergraph-structured MTS data, hyperedges capture relationships between multiple time-series variables, which can have varying degrees of correlation and complexity. Let the notation $\\mathcal{N}_{j,i}$ represent a subset of hypernodes $i$ associated with a specific hyperedge $j$. The intra-edge neighborhood of a hypernode $i$, denoted as $\\mathcal{N}_{j,i} \\backslash i$, captures a localized cluster of semantically-corelated time-series variables and their higher-order relationships. The inter-edge neighborhood of a hypernode $i$, represented by $\\mathcal{N}_{i,j}$, includes the set of hyperedges $j$ connected to that hypernode, providing a more comprehensive understanding that each variable may have multiple and potentially complex relationships with other time-series variables in the data. We use attention-based intra-edge neighborhood aggregation to obtain latent hyperedge representations, which leads to a more comprehensive understanding of the MTS data. This approach can be described as follows:\n\\[\\mathbf{h}_i^{(t,l)} = \\sum_{z=1}^{Z} \\sigma(\\sum_{j \\in \\mathcal{N}_{j,i}} (\\alpha_{j,i}^{(t,l,z)} \\mathbf{W}^{(z)} \\mathbf{h}_i^{(t,l-1,z)}))\\]\nwhere the hyperedge representations at layer $l$ are denoted by $\\mathbf{h}_i \\in \\mathbb{R}^{v \\times d}$. Each hypernode's initial representation is its corresponding feature vector,\n\\[\\mathbf{h}_i^{(t,0,z)} = \\mathbf{x}_i^{(t)}\\]\nwhere $\\mathbf{x}_i^{(t)} \\in \\mathbb{R}^{v \\times d}$ represents the $i^{th}$ row of the feature matrix $\\mathbf{X}_{t:t+v-1} \\in \\mathbb{R}^{n \\times v \\times d}$. At each layer, the HgAT operator produces multiple representations denoted by $\\mathbf{h}_i^{(t)}$ of the input data, each with its own set of parameters, and combines them by summation. This enables the HgAT operator to capture various aspects of the relations underlying the intra-edge neighborhood in the hypergraph-structured MTS data. To determine the attention coefficient $a_{j,i}$ for the hypernode $i$ incident with hyperedge $j$, we compute its relative importance as follows:\n\\[e_{j,i}^{(t,l,z)} = ReLU(\\mathbf{W}^{(2)} [\\mathbf{h}_i^{(t,l-1,z)} \\vert\\vert \\mathbf{h}_j^{(t,l-1,z)}])\\]\n\\[\\alpha_{j,i}^{(t,l,z)} = \\frac{\\exp{(e_{j,i}^{(t,l,z)}}}{\\sum_{k \\in \\mathcal{N}} \\exp{(e_{j,k}^{(t,l,z)}})}\\]\nwhere $e_{j,i}$ denotes the unnormalized attention score. The HgAT method utilizes an attention-based inter-edge neighborhood aggregation scheme, which captures complex dependencies and relationships between hyperedges and hypernodes. It generates expressive hypernode representations by summing over ReLU activations of linear transformations of previous layer hypernode representations and weighted hyperedge representations. This is described below,\n\\[\\mathbf{h}_i^{(t,l)} = ReLU (\\mathbf{W}^{(1)} \\mathbf{h}_i^{(t,l-1,z)} + \\sum_{j \\in \\mathcal{N}_{i,j}} \\beta_{i,j}^{(z)} \\mathbf{W}^{(z)} \\mathbf{h}_j^{(t,l,z)})\\]\nThe weight matrices that are trained are represented as $\\mathbf{W}^{(z)}, \\mathbf{W}^{(2)} \\in \\mathbb{R}^{d \\times d}$. The $ReLU$ activation function is used to introduce non-linearity while updating the hypernode-level representations. The attention scores $\\beta_{i,j}$ are normalized and determine the relevance of each hyperedge $j$ that is incident with hypernode $i$. This allows the HgAT operator to focus on the most significant hyperedges, and the attention scores are computed as follows:\n\\[\\Psi_{i,j}^{(,,z)} = ReLU (\\mathbf{W}_{1}^{,(z)} [(\\mathbf{W}_{1}^{(z)} \\mathbf{h}_i^{(t,l-1,z)} \\vert\\vert \\mathbf{W}^{(z)} \\mathbf{h}_j^{(t,l,z)})) \\cdot \\mathbf{W}^{(z)})\\]\n\\[\\beta_{i,j}^{(,,z)} = \\frac{\\exp{(\\Psi_{i,j}^{(,,z)})}}{\\sum_{e \\in e_i} \\exp{(\\Psi_{i,e}^{(,,z)})}}\\]\nwhere $\\mathbf{W}^{(z)} \\in \\mathbb{R}^{d \\times d}$ and $\\mathbf{W}^{'(z)}, \\mathbf{W} \\in \\mathbb{R}^{2d}$ are trainable weight matrix and vector, respectively. $\\vert\\vert$ denotes the concatenation operator. The unnormalized attention score is denoted by $\\Phi_{i,j}$. Batch normalization and dropout techniques are used to enhance generalization and mitigate overfitting. A gating mechanism is employed to selectively combine features from $\\mathbf{h}_i^{(t)}$ and $\\mathbf{h}_j^{(t,e)}$ in a differentiable way. These methods improve the HgAT operator reliability and accuracy for the downstream MTSF task.\n\\[g^{(t)} = \\sigma(\\mathbf{f}_g(\\mathbf{h}_i^{(t,l)}) + \\mathbf{f}_g(\\mathbf{x}_{i}^{(t)}))\\]\n\\[\\mathbf{h}_i^{(t,l)} = (g^{(t)} (\\mathbf{h}_i^{(t,l)}) + (1 - g^{(t)})(\\mathbf{x}_{i}^{(t)}))\\]\nwhere $\\mathbf{f}_g$ and $\\sigma$ denote the linear projections, enabling the HgAT operator to capture the relationships between time-series variables and their temporal changes, resulting in enhanced forecast accuracy. In summary, the HgAT operator is a powerful technique for encoding and analyzing spatio-temporal hypergraphs."}, {"title": "Saptio-temporal Hypergraph Representation Learning", "content": "We present the spatio-temporal hypergraph representation learning(HgRL) module to operate on a sequence of dynamic hypergraphs, where hypergraph structure is fixed, and hypernode attributes change over time, where each hypergraph represents the hypergraph-structured MTS data at a specific time step. The HgRL operator utilizes Gated Recurrent Units(GRU, [Cho et al., 2014b]) to model the spatio-temporal dynamics of the dynamic hypergraph sequence. The computation of the update gate, reset gate, and hidden state in a traditional GRU involves matrix multiplication with weight matrices. In the HgRL module, however, these matrix multiplications are replaced with Hypergraph Attention Networks(HgAT). The HgRL operator analyzes hypergraph-structured MTS data over time. It propagates information between hypernodes across different time steps, which enables the model to capture the complex spatio-temporal dependencies between the hypergraphs. The HgRL operator utilizes the implicit hypergraph topology to propagate information between hypernodes by averaging the hypernode representations in their local neighborhood at each time step computed as follows,\n\\[C_{t:t+v-1} = tanh (\\mathbf{W}_c [f (\\mathbf{I}, \\mathbf{X}_{t:t+v-1}) || (\\mathbf{R}_{t:t+v-1} \\odot \\mathbf{H}_{t-\\tau: t-1})] + B_c)\\]\n\\[U_{t:t+v-1} = \\sigma (\\mathbf{W}_u [f (\\mathbf{I}, \\mathbf{X}_{t:t+v-1}) || \\mathbf{H}_{t-\\tau: t-1}] + B_u)\\]\n\\[R_{t:t+v-1} = \\sigma (\\mathbf{W}_r [f (\\mathbf{I}, \\mathbf{X}_{t:t+v-1}) || \\mathbf{H}_{t-\\tau: t-1}] + B_r)\\]\n\\[\\mathbf{H}_{t:t+v-1} = U_{t:t+v-1} \\odot \\mathbf{H}_{t-\\tau: t-1} + (1 - U_{t:t+v-1}) C_{t:t+v-1}\\]\nwhere $f (\\mathbf{I}, \\mathbf{X}_{t:t+v-1})$ denote the HgAT operator. $\\vert\\vert$, and $\\odot$ denotes the concatenation operation and element-wise multiplication operation. The update and reset gates at time $t$ are represented by the matrices $U_{t:t+v-1}$ and $R_{t:t+v-1}$, respectively. $W_r$, $W_u$, and $W_c$ are learnable weight matrices and $B_u$, $B_r$, and $B_c$ are learnable biases. In summary, the node representation matrix, $\\mathbf{H}_{t:t+v-1}$ captures the spatio-temporal dynamics at different scales underlying the discrete-time dynamic hypergraphs, where each row in $\\mathbf{H}_{t:t+v-1}$ represents the hypernode representations $\\mathbf{h_i}^{(t)} \\in \\mathbb{R}^{v \\times d}, \\forall i \\in V$. Some of the key advantages of T-HGCN operator over traditional methods include its ability to handle large and sparse spatio-temporal hypergraphs. The STHgCN method utilizes useful relational inductive bias encoded in the hypergraph-structured data for modeling the continuous-time nonlinear dynamics of the complex system to disentangle the various latent aspects underneath the data for better forecast accuracy."}, {"title": "Spatio-Temporal Transformer Network(STTN)", "content": "The Spatio-temporal transformer network(STTN) operator is a new extension of transformer networks that incorporates full attention as a desired inductive bias to model MTS data with arbitrary sparse hypergraph structures. This capability enables it to capture fine-grained spatio-temporal dependencies in MTS data, unconstrained by hierarchical structural information underlying the MTS data. By allowing attention to all hypernodes within the hypergraph, the neural operator can span large receptive fields and reason globally about complex dependencies in hypergraph-structured MTS data. As a result, it can serve as a drop-in replacement for existing methods that model hierarchical relationships among time-series variables in MTS data. Additionally, the neural operator is particularly suitable for downstream forecasting tasks in spatio-temporal hypergraphs. The transformer encoder comprises alternating layers of multiheaded self-attention(MSA) and multi-layer perceptron(MLP) blocks to capture both local and global contextual information. To enhance performance and regularize the transformer operator, each block is followed by layer normalization(LN([Ba et al., 2016])) and residual connections. The skip-connections are incorporated through an initial connection strategy inspired by ResNets([He et al., 2016]) to address vanishing gradients and over-smoothing issues and enable the learning of complex and deep representations of the data. Using a space-then-time(STT, [Gao and Ribeiro, 2022]) approach, the STTN first performs a temporal-encoding step to capture the long-term temporal dependencies(intra-dependencies) within the time series variables. This is followed by a spatial-encoding step, which captures the inter-dependencies among the time series variables. We model the intra- and inter-dependencies through a sequential operating temporal and spatial transformer networks, respectively."}, {"title": "Temporal Transformer", "content": "In self-attention mechanism", "tensors": "the query tensor, the key tensor, and the value tensor, where the input tensor is denoted by $\\mathbf{X}_{"}]}