{"title": "Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning", "authors": ["Hyundong Cho", "Karishma Sharma", "Nicolaas Jedema", "Leonardo F. R. Ribeiro", "Alessandro Moschitti", "Ravi Krishnan", "Jonathan May"], "abstract": "Language models are aligned to the collective voice of many, resulting in generic outputs that do not align with specific users' styles. In this work, we present Trial-Error-Explain In-Context Learning (TICL), a tuning-free method that personalizes language models for text generation tasks with fewer than 10 examples per user. TICL iteratively expands an in-context learning prompt via a trial-error-explain process, adding model-generated negative samples and explanations that provide fine-grained guidance towards a specific user's style. TICL achieves favorable win rates on pairwise comparisons with LLM-as-a-judge up to 91.5% against the previous state-of-the-art and outperforms competitive tuning-free baselines for personalized alignment tasks of writing emails, essays and news articles. Both lexical and qualitative analyses show that the negative samples and explanations enable language models to learn stylistic context more effectively and overcome the bias towards structural and formal phrases observed in their zero-shot outputs. By front-loading inference compute to create a user-specific in-context learning prompt that does not require extra generation steps at test time, TICL presents a novel yet simple approach for personalized alignment.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are biased towards generic outputs as they are trained to align to an aggregate preference to be generally useful (Padmakumar and He, 2024; Rafailov et al., 2024; Lee et al., 2023; Wang et al., 2023). Yet end users often have specific needs that call for personalized text generation, such as writing an email (Kumar et al., 2024; Salemi et al., 2023). Hence, adapting LLMs for personalized text generation has drawn a growing interest (Jang et al., 2023; Shaikh et al., 2024; Mysore et al., 2023; Li et al., 2024), but"}, {"title": "Methodology", "content": "Trial-Error-Explain In-Context Learning (TICL) is a method for expanding an ICL prompt through tuning-free correspondences of the three main stages in Trial-and-Error Fine-tuning (TEFT). Therefore, we describe TICL by going through each main stages of TEFT and the corresponding procedure in TICL, as illustrated in Figure 2. TEFT refers to a method that (i) fine-tunes a model with supervised fine-tuning (SFT) on a small set of training data (behavior cloning), (ii) uses the trained model to generate outputs that are considered less preferred to those from the training data (exploration), (iii) and fine-tunes the model further with a preference optimization method, such as DPO (learning). This method has been applied to various tasks in previous work with minor differences, such as machine translation with ReST (Gulcehre et al., 2023), personalized text generation with DITTO (Shaikh et al., 2024), and trajectory planning with ETO (Song et al., 2024).\nIn TICL, we first replace SFT with ICL for behavior cloning (\u00a7??). Exploration is mostly the same except that TICL updates the prompt at the step level while TEFT updates the policy at the epoch level in order to avoid augmenting the prompt with redundant errors (\u00a72.2). Lastly, for learning, we substitute preference optimization with prompt augmentation (\u00a72.3). TICL generates explanations that analyzes the differences between the generated text and the user's actual text. Then, the generated text is labeled as a negative sample and the explanation are added to the prompt. We elaborate on each correspondence and share additional details in the following sections."}, {"title": "Behavioral cloning", "content": "As shown in the first step in Figure 2, TEFT uses supervised fine-tuning (SFT) to fine-tune a model \\(\\pi_\\theta\\) roughly towards the desired behavior with the initial training data. This helps with generating more relevant negative samples in the exploration phase (Song et al., 2024).\nFormally, with the initial training data \\(D^T = \\{(x_i, y_i)\\}_{i=1}^T\\), where x is the task and y is the user's text, SFT uses the negative log likelihood"}, {"title": "Exploration", "content": "The objective of the exploration phase, the middle stage in Figure 2, is to generate negative samples that will be used for learning. TEFT and TICL is mostly similar except for minor technical differences."}, {"title": "Learning", "content": "In the learning phase for TEFT, the preference optimization samples \\(D_{pref}\\) collected from the exploration phase are used for fine-tuning \\(\\pi_\\theta\\) with the use of a reference model \\(\\pi_{ref}\\), usually a pre-trained language model for preventing overfitting and reward hacking, and a preference optimization method, such as direct preference optimization (DPO) (Rafailov et al., 2024):\nPreference optimization \u2192 Prompt augmentation with negative samples and explanations The main benefit of preference optimization is that"}, {"title": "Experimental Setup", "content": "We build on the evaluation setup from DITTO (Shaikh et al., 2024), which uses two public authorship attribution (AA) datasets, because they also study personalized alignment. AA is a classification task of predicting the correct author for some written document, but it can also be reformulated as a personalized alignment task of generating text that is attributed to a specific author. Same as DITTO, we consider 20 authors from two sources: (i) CMCC (Goldstein et al., 2008), which contains emails and essays on controversial topics written by students, and (ii) CCAT (Lewis et al., 2004), which consists of news articles written by journalists. We preprocess the data with the same steps as DITTO to simulate a realistic data scenario for personalization in which per-user data is limited and treated as private. The number of samples per author and per task is small (Zhang et al., 2020), limited to the smallest number of samples available for an author (12) across both datasets to control for data size. In addition, each author's data is used in isolation such that performance does not depend on the total amount of data available among all authors. The train/val/test split is set to 7/2/3."}, {"title": "Evaluation Method", "content": "Pairwise comparison on stylistic similarity with LLM-as-a-judge We evaluate model performance with pairwise comparisons of generated text using LLM-as-a-judge (Zheng et al., 2023; Kim et al., 2023), asking which of the given candidates is more similar to a set of user's texts. A simplified illustration of our evaluation approach is shown in Figure 3.\nSpecifically, we ask GPT-40 which of two candidates outputs generated for an author's prompt in the test set is more stylistically consistent with five examples sampled from the author's training data. This is similar to the method used in DITTO (Shaikh et al., 2024), but we found that we can further increase accuracy by providing more examples, providing a list of style elements to focus on, and generating an explanation before making its final decision. The two main comparisons we are interested in are how our methods and baselines compare with DITTO's outputs and the author's text. The former is a comparison against the prior state-of-the-art and the latter is against the gold output, and therefore serves as an upper bound. In other words, achieving a \u2265 50% win rate against the author indicates that the method is able to produce outputs that are indistinguishable from the author's texts for our LLM-as-a-judge in terms of stylistic consistency."}, {"title": "Baselines", "content": "To measure the relative performance of TICL, we also evaluate on (i) Zero-shot, to capture the base model's vanilla behavior when it is only given the task as input, and (ii) ICL, where N samples of task and output pairs of the author are included in the context as examples to learn from, along with some guidance to focus on style elements.\nIn addition, we evaluate other methods that scale test-time compute to improve downstream performance. (iii) Chain-of-Thought (Wei et al., 2022) is a prompting method that asks to reason about the style elements in the few-shot examples and then generate the response according to the analyzed style and the few-shot examples. (iv) OPRO (Yang et al., 2024)is a prompt optimization method that refines the prompt iteratively using chain-of-thought prompt as the initial prompt. The model self-updates its prompt based on the history of previous prompts and their scores as \u201cgradients\u201d that guide the update. The initial prompt is set to \u201cLet's think step by step\" (Kojima et al., 2022) and the scores are cosine similarity between the Style Embeddings (SE) (Wegmann et al., 2022) of the generated outputs and the reference texts of the validation set. The prompt with the highest score is used at test time.\nWe also compare TICL to (v) DITTO (Shaikh et al., 2024), the state-of-the-art that uses TEFT on a small model, i.e. Mistral 7B Instruct (Jiang et al., 2023), and outperforms GPT-4 combined"}, {"title": "Results and Discussion", "content": "The main evaluation results that compare the baselines against DITTO and against the author's actual reference text is shown in Table 2. Details on the sample size can be found in Appendix A.1.\nVs. DITTO TICL outperforms or performs on par with all other baselines and often outperforms DITTO by a significant margin. We find that explicitly analyzing writing style in the few-shot examples and asking the model to abide by these styles did not lead to consistent improvements. With the exception of Claude 3 Sonnet on CCAT, CoT actually underperforms the simpler few-shot setup. Interestingly, we find that OPRO consistently performs worst. When we analyzed the optimization sequence, OPRO lands on prompts that look sensi-"}, {"title": "Ablating TICL", "content": "We conduct an ablation study to understand how each of the components in TICL contributes to its performance. We observe how the win rates change when we remove the initial ICL setup that substitutes SFT for behavior cloning, explanations, checkpointing (choosing the best performing augmented prompt throughout iterations rather than simply using the final prompt), and both the negative samples and explanations. The last row is equivalent to the few-shot baseline. The results are"}, {"title": "Why does Claude 3 Sonnet outperform GPT-40?", "content": "One striking trend from Table 2 is Claude 3 Sonnet's noticeably stronger performance compared to GPT-40. This is somewhat surprising given that GPT-40 is the latest model that ranks at the top of popular language modeling leaderboards, such as Chatbot Arena. In order to understand Claude's comparative performance over GPT-4o, we examine how performance changes if we (i) use the other model for generating explanations during prompt augmentation (with other model expl.) and (ii) use the fully developed TICL prompt from the other model at test time (with other model TICL).\nExplanations are similar in quality. Results"}, {"title": "Lexical Analysis", "content": "We further explore the models' comparative performances using a lexical analysis, using Fightin' Words model for extracting words with significant differences in between two corpora (Monroe et al., 2008). The method shows different ratios of words that are significantly more frequent in one model over another. Using a p-level of 0.05, we calculate the number of n-grams that are significantly more frequent in the author's text compared to the model's text and vice-versa. Representative results are shown in Table 5.\nTICL overcomes model bias for structural and formal phrases better than ICL. Comparing GPT-40 TICL vs. GPT-40 few-shot elicits TICL improvement over a simple ICL baseline. We see that structural and formal phrases, e.g. \u201cadditionally\u201d and \u201ctherefore\u201d, are more frequent in the few-shot outputs, while TICL contain more colloquial phrases, e.g., \u201cso why\u201d. This suggests that TICL enables models to overcome its biases towards generic formal text and better learn from the given style context.\nClaude 3 Sonnet applies user styles more effectively than GPT-40. When comparing Claude 3 Sonnet and GPT-40 directly, the top words that Claude 3 Sonnet is able to generate more consistently are subjective and assertive words or phrases that are frequently seen in CMCC, e.g., \u201cbelieve\u201d and \u201cfeel that\u201d, and also more casual phrases, e.g., \u201cthings like\u201d and \u201ckind of.\u201d On the other hand, GPT-40's outputs consists of more formal phrases, such as \u201ccrucial to.\u201d This dynamic is further corroborated by the Flesch Readability Ease (FRE) scores (Flesch, 1948). The words that appear significantly more frequently for Claude has a higher FRE score (FRE(A\\B)=120.21) than the score for words that appear significantly more frequenty for GPT-40 (FRE(B\\A=77.21). A higher FRE score means it is easier to understand when read, which correlates to more casual text (Cho et al., 2024)."}, {"title": "Related Work", "content": "Personalization is an overloaded term in the context of language models as it can refer to multiple distinct features. Language models can be personalized to complete tasks as a specific user (Salemi et al., 2023; Zhuang et al., 2024), answer questions according to a user's profile (Kim and Yang, 2024; Dutt et al., 2022), and recommend content based"}, {"title": "Inference-only learning", "content": "One major branch of inference-only learning method focuses on automating prompt engineering, optimizing the wording of a prompt template that is placed prior to asking a model to complete a task (Shin et al., 2020; Zhou et al., 2023; Ma et al., 2024; Yang et al., 2024; Kim and Yang, 2024; Ye et al., 2024), e.g. \u201cLet's think step by step\" from (Kojima et al., 2022). Recent work have operationalized this process to also automate example selection for few-shot examples (Yuksekgonul et al., 2024; Khattab et al., 2023). In contrast, TICL maximizes its understanding of the target task by iteratively augmenting the content used for ICL with contrastive examples and explanations while keeping the prompt template constant.\nAnother prominent branch of inference-only learning is agentic methods that build on intermediate LLM outputs to refine the final output (Chen et al., 2023; Wei et al., 2022; Yao et al., 2023; Sumers et al., 2024; Shinn et al., 2024; Saha et al., 2024). Reflexion (Shinn et al., 2024) is most similar to our work, but it cannot be directly applied without a reliable evaluator for measuring success.\nAnother key difference with TICL is that it front-loads the trial-and-error process ahead of test time and does not augment the prompt a priori by reasoning over few-shot examples."}, {"title": "Conclusion", "content": "In conclusion, we develop TICL, a tuning-free method for personalized alignment that adapts prior work on trial-and-error fine-tuning with scaling inference compute. Instead of fine-tuning a model with synthetic negative samples, TICL uses them to augment the prompt and generate explanations that provide further fine-grained guidance on how to align outputs towards a desirable style. On personalized text generation tasks, TICL outperforms other competitive tuning-free baselines and the previous state-of-the-art. TICL provides an approach for leveraging black box models for personalization without any fine-tuning."}, {"title": "Limitations", "content": "One of the limitations of TICL is that it is more computationally costly than fine-tuned models at test time because of the longer inputs due to the few-shot examples and their corresponding negative samples and explanations. However, inference cost for repeating prompts can be significantly lowered by prompt caching. In addition, inference costs in general are continuously getting lower with optimization efforts that comprehensively span hardware and software (Kwon et al., 2023). As the promise of inference-only methods become increasingly evident, we believe these efforts will be further accelerated and lead to even more favorable conditions for methods such as TICL.\nAnother limitation of TICL is that it is only effective for models that are able to understand long contexts well. In our preliminary experiments, our results on using TICL with smaller models such as Mistral 7B Instruct and GPT-40 mini were poor as these models continued to generate outputs similar in style to what were labeled in-context as bad examples, indicating that they were not able to distinguish between the good and bad examples included in the context. Therefore, the performance of TICL is dependent on a model's long context understanding."}, {"title": "Generative AI Statement", "content": "No generative AI tools were used in the writing of this work."}, {"title": "Embedding-based evaluation", "content": "Universal\nauthorship\nrepresentations\n(UAR) (Rivera-Soto et al., 2021) is a sen-\ntence embedding method that uses a SBERT\nmodel (Reimers and Gurevych, 2019) fine-tuned\nwith a contrastive learning objective such that\nthe representations of sentences or documents\nfrom the same author become closer together than\nwith those of other authors. Style Embeddings\n(SE) is a follow-up to UAR that refines the rep-\nresentations to focus on style rather than content\nby pairing contrastive samples that share similar\ncontent (Wegmann et al., 2022), and therefore we\nfocus on embedding-based comparisons with Style\nEmbeddings.\nWe compute the cosine similarity between can-\ndidate texts' SE with those of the author examples\nand the one with higher cosine similarity is con-\nsidered more stylistically similar. The best score\nfor SE is 78.5% and 63.6% for CMCC and CCAT,\nrespectively, which is significantly lower than GPT-\n40 results but better than GPT-40 mini's results.\nWhile having more examples also helps with SE's\nperformance, it plateaus markedly at three exam-\nples, and there is only minimal gains observed for\nCCAT."}, {"title": "LLM-as-a-Judge", "content": "The pairwise comparison setup is described in Section 3.2. As an alternative, we also consider having LLM-as-a-judge provide ratings for individual instances, so that we can reduce the number of samples that the LLM-as-a-judge need to evaluate and also measure relative performance of each approach based on their aggregate scores. The prompt is similar to the pairwise comparison setup except that only one candidate is shown and the LLM is asked to provide a score from 1-5 on how stylistically similar it thinks the candidate is to the examples. Unfortunately, this result is the poorest performing approach, only achieving 55.0% accuracy for CMCC and 20.0% accuracy for CCAT. The main reason for low accuracy is that the majority of instances were given the same rating such that they were deemed as ties."}, {"title": "Human Evaluation", "content": "We have three human annotators complete the same setup with 100 samples for CMCC only with five samples from each author. Conducting the same evaluation for CCAT was overwhelming for human participants because the average text length in CCAT is much larger. As shown in the results, human performance is lower than the best GPT-4o results. This reinforces findings from previous work that identifying authorship reliably is a difficult task for untrained humans and that model-based classifiers are more reliable and practical for this task (Hallinan et al., 2023; Krishna et al., 2020; Liu et al., 2024; Liu and May, 2024)."}, {"title": "Flesch Reading Ease", "content": "The Flesch Reading Ease (FRE) score is a simple equation for approximating readability of a given text based on the average number of words per sentence and the average number of syllables per word (Flesch, 1948). The formula for calculating FRE is the following:\n\\(FRE = 206.835 \u2013 (1.015 \\cdot ASL) \u2013 (84.6 \\cdot \\frac{N_{sy}}{N_w})\\)\nwhere ASL is average sentence length (\\(\\frac{total words}{total sentences}\\)) and \\(\\frac{N_{sy}}{N_w}\\) is the average number of syllables per word (\\(\\frac{total syllables}{total words}\\)). The higher the score, the easier it is to understand a piece of text"}]}