{"title": "CLIP-based Camera-Agnostic Feature Learning for Intra-camera Person Re-Identification", "authors": ["Xuan Tan", "Xun Gong", "Yang Xiang"], "abstract": "Contrastive Language-Image Pre-Training (CLIP) model excels in traditional person re-identification (ReID) tasks due to its inherent advantage in generating textual descriptions for pedestrian images. However, applying CLIP directly to intra-camera supervised person re-identification (ICS ReID) presents challenges. ICS ReID requires independent identity labeling within each camera, without associations across cameras. This limits the effectiveness of text-based enhancements. To address this, we propose a novel framework called CLIP-based Camera-Agnostic Feature Learning (CCAFL) for ICS ReID. Accordingly, two custom modules are designed to guide the model to actively learn camera-agnostic pedestrian features: Intra-Camera Discriminative Learning (ICDL) and Inter-Camera Adversarial Learning (ICAL). Specifically, we first establish learnable textual prompts for intra-camera pedestrian images to obtain crucial semantic supervision signals for subsequent intra- and inter-camera learning. Then, we design ICDL to increase inter-class variation by considering the hard positive and hard negative samples within each camera, thereby learning intra-camera finer-grained pedestrian features. Additionally, we propose ICAL to reduce inter-camera pedestrian feature discrepancies by penalizing the model's ability to predict the camera from which a pedestrian image originates, thus enhancing the model's capability to recognize pedestrians from different viewpoints. Extensive experiments on popular ReID datasets demonstrate the effectiveness of our approach. Especially, on the challenging MSMT17 dataset, we arrive at 58.9% in terms of mAP accuracy, surpassing state-of-the-art methods by 7.6%.", "sections": [{"title": "I. INTRODUCTION", "content": "ERSON re-identification (Re-ID) involves identifying the same individual across different camera views. It has attracted significant attention because of its applications in person tracking, security systems, and traffic monitoring. Current research primarily focuses on two directions: fully supervised [1], [2], [3], [4], [5] and unsupervised [6], [7], [8], [9]. With the advent of deep learning technologies, fully supervised person ReID has seen significant performance improvements. However, the considerable annotation cost associated with the increasing number of cameras and IDs in real-world scenarios poses a significant challenge for the practical deployment of ReID systems. Conversely, unsupervised person ReID does not require any label information but tends to underperform in complex scenarios involving multiple IDs. In recent years, to combine the advantages and mitigate the drawbacks of supervised and unsupervised methods, the Intra-camera supervision (ICS) approach has been proposed.\nThis approach assumes individual labeling of IDs within each camera without establishing cross-camera identity links. As a result, ICS supervision significantly reduces annotation costs compared to full supervision while still maintaining identification accuracy. Therefore, it is considered a more practical setup for ReID scenarios.\nHowever, the lack of cross-camera annotation information poses a significant challenge for effectively learning pedestrian features in ICS ReID. Specifically, the number of annotated pedestrian training samples within each camera is significantly lower than in fully supervised cross-camera person ReID tasks. Additionally, due to factors such as varying viewpoints,"}, {"title": "II. RELATED WORK", "content": "A. Intra-camera Supervised ReID\nWith the increasing number of cameras and persons in real-world scenarios, the task of annotating a large-scale ID dataset becomes prohibitively costly. To address this issue, a setup known as Intra-camera supervision (ICS) ReID has been proposed, where annotations are performed independently across various cameras, with labels only available for persons within the same camera view. Previous methods approached the ICS ReID problem from two angles: intra-camera supervised learning and inter-camera ID association learning. In intra-camera learning, PCSL [16] and ACAN [17] employ a direct triplet loss [18] to train models, while MATE [10] constructs a multi-branch classifier for each camera. However, when the distribution of intra-camera ID samples is unbalanced and scant, it can result in biased learning. In contrast, Precise-ICS [11] uses a non-parametric classifier and undertakes joint learning, but insufficient intra-camera learning can severely impair the model when persons with high intra-camera sim-ilarity are present. For inter-camera learning, Precise-ICS supervises learning through pseudo-labeling based on the sim-ilarity of person features across cameras. CMT [19] combines contrastive learning with the Mean Teacher [20] paradigm to construct a semi-supervised learning framework. However, the methods above overlook the labeled instance features within the same camera, leading to insufficient intra-camera learning and consequently failing to effectively distinguish pedestrian features within the same camera. Additionally, these methods do not adequately consider the disparities in data distribution across different cameras, failing to fully capture the invariant features of pedestrians across cameras.\nDifferently, we design Intra-Camera Discriminative Loss (ICDL) and Inter-Camera Adversarial Loss (ICAL) methods to effectively enhance the model's ability to distinguish pedes-trian ID features within and across cameras. Additionally, we incorporate high-level semantic features generated by CLIP for each person within a camera to further boost the model's performance.\nB. Unsupervised Person ReID\nIn recent years, unsupervised person ReID [6], [7], [8], [21], [22], [23], [24] tasks have attracted wide attention. These tasks are primarily categorized based on whether additional re-lated data are employed, encapsulating unsupervised domain-adaptive (UDA) ReID and purely unsupervised learning (USL) ReID. The latter, pure unsupervised ReID, presents greater challenges due to its independence from any external data. However, with the successful application of contrastive learn-ing in the unsupervised domain, the performance of USL ReID has significantly increased - notable methods include SPCL [6]'s self-paced contrastive learning procedure that builds a mixed memory bank, fully exploiting all available data. CAP [21] technique divides clusters into multiple camera-perception proxies based on the camera ID to alleviate discrepancies in ID features generated by camera perspective alterations. ClusterContrast [23] directly establishes a simple yet effective cluster-level memory bank, achieving decoupling between feature update rates and the number of images. RTMem [25] employs a real-time memory update strategy, updating cluster centroids by randomly sampling current mini-batch instance features without the need for momentum. In contrast, LP [26] considers two types of additional features from different local views and leverages the knowledge of an offline teacher model to optimize the model. In this study, our work is grounded in the framework of intra- and inter-camera contrastive learning, which is a widely used and effective representation learning method for unsupervised person ReID.\nC. Vision-language Models\nLarge-scale pre-trained vision-language models, integrating copious amounts of textual and visual data, have proven their efficacy across various domains. For example, Contrastive Language-Image Pretraining (CLIP) [13] model, which em-ploys the InfoNCE loss [27] function to jointly train text and image encoders, resulting in significant performance im-provements in numerous downstream tasks. Additionally, to further tap into CLIP's potential, CoOp [28] introduces prompt learning, aiming to uncover the implicit textual cues within images, effectively migrating CLIP to a broader range of downstream tasks. Within the realm of ReID, CLIP has been extensively applied. For instance, CLIP-ReID [29], by aligning image and textual information within a singular embedding space, reinforces the connection between image features and related textual descriptions. CCLNet [30] establishes learnable cluster-aware prompts for person images and generates text descriptions to assist subsequent unsupervised visible-infrared person re-identification training.\nHowever, the immense potential of CLIP in facilitating semi-supervised person ReID learning has yet to be explored. In this paper, we fully integrate CLIP with the ICS ReID task to construct a CCAFL framework, offering new insights for semi-supervised ReID.\nD. Adversarial Learning\nThe application of adversarial learning in person re-identification can be traced back to the use of Generative Adversarial Networks (GANs) [31] to generate realistic person images. For example, [32] proposed a GAN-based method that performs selective sampling of generated data to bridge the gap between domains and enrich the feature space. In recent years, the application of adversarial learning has extended beyond image generation and has been widely applied to various aspects of person re-identification. For instance, in unsupervised domain adaptation for person re-identification, CAWCL [33] employs a Gradient Reversal Layer (GRL) [34] to align the distribution of each camera. However, using traditional domain adversarial learning to eliminate camera styles can negatively impact the model's ability to recognize pedestrians. In clothing change person re-identification, CAL [35] proposed a clothing-based adversarial loss to decouple clothing-independent features. In contrast to these methods,"}, {"title": "III. METHODOLOGY", "content": "A. Overview\nBased on the ICS ReID problem, the training dataset only contains intra-camera IDs and lacks inter-camera IDs. Therefore, in this case, a dataset consisting of C cameras can be represented as D = {D1, D2, ..., Dc}. Specifically, the images of persons from the c-th camera can be represented as Dc = {(xi,Yj,c)}, where xi indicates the i-th person image under this camera, yj (0 < j < Nc) represents the corresponding label, and Nc denotes the number of person IDs under this camera. For instance, in the Market-1501 [14] dataset, which contains six cameras, there are 751 pedes-trian IDs under supervised conditions, meaning these IDs are associated across different cameras. However, in the ICS setting, the IDs for each camera are independently annotated, with the number of pedestrian IDs for each camera being D = {652,541,694, 241, 576, 558}, resulting in a total of 3,262 global IDs. Although multiple cameras may capture the same pedestrian, their global IDs remain distinct. Therefore, each person sample in the training set carries three labels: the intra-camera ID, the camera label, and the global ID. Moreover, since the same pedestrian might be captured by multiple cameras, they have different IDs assigned to them depending on which camera they were captured by. Thus, our primary objective is to learn feature representation for individuals across different cameras.\nRecently, the CLIP model, trained on large-scale datasets, has demonstrated remarkable proficiency in matching image-text descriptions. Its image encoder captures complex and rich visual features, while the text encoder provides enhanced semantic information. Building upon this, we have developed a learning framework that integrates CLIP with ICS-ReID to discern intra- and inter-camera ID identities. The framework consists of three training steps: intra-camera pre-defined prompt learning, intra-camera learning, and inter-camera learning. Through this three-stage training process, the model can deeply explore pedestrian information from different camera angles and effectively guide the estab-lishment of more accurate cross-camera ID associations.\nB. Intra-camera Pred-defined Labels Prompt Learning\nThe current research on ICS ReID commonly adopts a two-stage learning approach, namely intra- and inter-camera stages. In the inter-camera learning phase, pseudo-labels are often assigned to IDs from different camera views using similarity-matching. However, due to variations in perspective, these pseudo-labels tend to be inaccurate, which can hinder the learning process. To address this issue, we integrate text encoder and prompt learning mechanisms from the CLIP"}, {"title": "C. Intra-camera Discriminative Learning", "content": "The primary challenge of ICS ReID lies in annotating IDs within each camera's view and establishing cross-camera ID associations through the analysis of inter-camera charac-teristics. Intra-camera learning, therefore, can be considered a fully supervised problem within a multi-task framework. However, the distribution of sample numbers for each ID within a camera is uneven, with most IDs having only a limited set of training samples. This imbalance can lead to model bias toward learning prominent camera-style features, rather than ID features. Additionally, the lack of cross-camera ID information highlights the significance of intra-camera learning as a preparatory phase for subsequent cross-camera correlation.\nConsidering these factors, we focus on the centroid features of each pedestrian within each camera, as well as the hard positive and hard negative samples for that pedestrian within the camera. This compels the model to learn more accurate intra-camera pedestrian features. This approach also emphasizes the differences between pedestrian IDs within the camera view, providing more reliable features for subsequent inter-camera association steps.\n1) Intra-camera Hybrid Memory Banks Initialization: Firstly, we initialize the intra-camera hybrid memory bank using an image feature encoder. All image features are as-signed to the corresponding camera memory based on different camera IDs. Then, for each image under each camera's pre-defined labels, the centroid features of each pedestrian ID within the camera are stored in the intra-camera centroid mem-ory through averaging, to learn the pedestrian features within each camera. Additionally, the instance features corresponding to each pedestrian ID within each camera are stored in the intra-camera instance memory to learn more discriminative information. The mean feature of the predefined labels of pedestrians within a camera is calculated as follows:\n$\\mu_i^c = \\frac{1}{|N_i^c|}\\sum_{x\\in N_i^c} f(x),$ (4)\nwhere $\\mu_i^c$ represents the mean feature of pedestrian ID i within camera c, $N_i^c$ denotes the set of all images belonging to pedestrian ID i within camera c, and f(x) represents the features of image x after being processed by the image encoder. Therefore, the memory bank $M_{intra}$ is initialized with the mean features of the pedestrian IDs, while $M_{intra}$ is initialized with the instance sample features corresponding to those pedestrian IDs. For intra-camera instance memory, we employ a real-time instance feature memory update strategy. In each iteration, we directly replace $M_{intra}$ in the memory with the current mini-batch instantaneous feature fx:\n$M_{intra} [y] \\leftarrow f(x).$ (5)\n2) Optimization: In each training iteration, the features stored in the aforementioned intra-camera hybrid memory bank are updated using different strategies.\nFor the memory bank $M_{intra}$:\n$\\mu_i^c \\leftarrow \\alpha \\mu_i^c + (1-\\alpha)\\mu^c,$ (6)\nwhere $\\mu$ denotes the average features of the camera c insider ID i in each batch, $\\alpha$ is the momentum updating factor. This update mechanism ensures that the features in the mem-ory bank consistently reflect the latest training information, thereby enhancing the accuracy and stability of the model in learning intra-camera pedestrian features. To this end, given a query image feature f(x), we propose an intra-camera centroid contrastive loss function, which is formulated as:\n$L_{intra1} = - \\sum_{c=1}^{C} \\log \\frac{\\exp (s(f(x_i), \\mu_i^+)/\\tau)}{\\sum_{j=1}^{K_c} \\exp (s(f(x_i), \\mu_j^c)/\\tau)},$ (7)\nwhere $\\mu$ represents the centroid feature for each ID in the c-th intra-camera memory, $K_c$ represents the total number of pedestrian IDs in the camera, and C is the number of cameras. Through the above loss, we can effectively bring the sample closer to the centroid feature of its corresponding ID while"}, {"title": "D. Inter-camera Learning", "content": "Through the aforementioned intra-camera learning, our model effectively identifies each person within the camera's view. However, the abundant learning information of IDs across cameras has yet to be fully utilized. Therefore, in the inter-camera learning process, we have devised an alternating strategy consisting of cross-camera ID association steps and inter-camera contrastive learning steps to facilitate the model's acquisition of view-invariant ID features.\n1) Inter-camera Association: The ICS ReID approach dif-fers from fully unsupervised ReID, which relies on clustering algorithms to obtain pseudo-labels directly. In the case of intra-camera IDs, assigning the same pseudo-labels is not feasible. Therefore, we employ an ID association algorithm based on connected components proposed in [11]. Specifically, we impose two constraints on the clustering process: 1) under the in-camera supervised condition, positive matches among IDs within each camera should not exist, and 2) a maximum of one positive match is allowed per camera. We then constructed an undirected graph G = (V, E) for associations, where the vertex set V represents the accumulated IDs across all cameras, and the edge set E represents a positive pair between IDs i and j. The edge e(i, j) is defined as follows:\ne(i, j)=\n\\{\n1, dist(i, j) <T\u2227c(i) \u2260 c(j)\n\u2227i\u2208 N\u2081(j, c(i)) \u2227 j\u2208 N\u2081(i, c(j));\n0, otherwise.\n(12)\nwhere dist(i, j) represents the distance between the centroid features of the i-th and j-th ID IDs, c(i) indicates the camera to which the IDs belong, and T is the distance threshold. N\u2081(j, c(i)) designates the nearest neighbor of the j-th ID with the i-th ID under the c(i) camera. Using the conditions defined above, a sparsely connected graph is constructed. Then, based on similarity, we identify all connected components and assign inter-camera pseudo-labels to IDs.\n2) Inter-camera Memory Banks Initialization: Based on the successful application of contrastive learning in the field of person re-identification [8], [23], [36], we employ a prototyp-ical contrastive learning paradigm for inter-camera learning. First, upon completion of intra-camera learning, we generate pseudo-labels for IDs using the aforementioned inter-camera association algorithm. Next, we compute the mean features of these samples based on their corresponding pseudo-labels and directly initialize the inter-camera memory bank. This approach provides a stable starting point for prototypical"}, {"title": "E. Inter-camera Adversarial Learning", "content": "Our model can recognize pedestrian identities across differ-ent cameras through the previously described intra- and inter-camera contrastive learning. However, during inter-camera learning, the variance in pedestrian feature distributions be-tween cameras introduces label noise, and the value of prede-fined intra-camera label information is not fully exploited. To address this issue, we propose Inter-Camera Adversarial Loss (ICAL). ICAL penalizes the model's prediction capabilities across different cameras, forcing the backbone network to extract camera-agnostic features. To achieve this, we introduce a new global ID classifier based on camera data, appended to the network. Each training iteration consists of two optimization steps:\n1) Training the Inter-Camera Global ID Classifier: First, we establish a classifier $C^C(\u00b7)$, where each class corresponds to a global ID. We optimize this global ID classifier by minimizing the classification loss $L_{GID}$, defined as the cross-entropy loss between the predicted pedestrian $C^C(f(x))$ and the global label $y$. We perform L2-normalization on the model's output features f(xi) and denote the L2-normalized weights of the j-th global ID classifier as $4j$. We detach these weights before inputting them into the global ID classifier to ensure that the classifier's training does not influence the model itself. Consequently, $L_{GID}$ can be expressed as:\n$L_{GID} = - \\sum_{i=1}^{N} \\log \\frac{\\exp(f(x_i)\\cdot \\varphi_{y_i} / \\tau)}{\\sum_{j=1}^{N_G} \\exp(f(x_i)\\cdot \\varphi_j / \\tau)},$ (17)\nwhere N is the batch size, Ng is the number of global ID classes in the training set, and $\\tau$ is a temperature parameter. By using global IDs as labels during training, our classifier can distinguish pedestrians across different cameras, effectively"}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Evaluation Metrics\nOur method is validated on three large-scale per-son re-identification (ReID) datasets: Market-1501 [14], DukeMTMC-ReID [37], and MSMT17 [15]. Following the ICS setting, we re-annotate individuals within each camera in the training set and add accumulation labels.  summarizes the specifics of the datasets under the ICS setup, including the number of cameras, IDs, and images in the training, gallery, and query sets. Additionally, we provide the accumulated total identity number under intra-camera supervision (#IDICS).\nIn terms of evaluation metrics, we adopt cumulative match-ing characteristics (CMC) [38] including Rank-1, Rank-5, and Rank-10 as well as mean Average Precision (mAP).\nB. Implementation Details\nWe adopt the ResNet50 model pre-trained on CLIP as our feature extractor. All input images are resized to 256\u00d7128 and subjected to data augmentation techniques such as random flipping, cropping, and erasing [39]. During the prompt learn-ing phase, we learn tokens represented as $[X]_1[X]_2... [X]_M$. We utilize the Adam optimizer [40] with a learning rate of 0.00035, which is adjusted using a cosine annealing policy. Our training batch size is 64, and the training process lasts for 60 epochs. For the subsequent training phase, we set the batch size to 128 and employ the PK sample [18]. Within each mini-batch, we sample images based on the labels of each camera. We randomly select 16 IDs, with each ID having 8 images. In this stage, we use the Adam optimizer with a learning rate of 0.00035. The model is warmed up for 10 epochs using a linear growth strategy, and the training phase lasts for 80 epochs. During the initial 5 epochs, we perform only intra-camera learning steps. Subsequently, we alternate between inter-camera association and inter-camera learning steps every epoch. The memory updating rates in Eq. 6 and Eq. 12 are set to 0.1, and the balance weight in Eq. 10 is set to 0.8. The temperature parameter $\\tau$ in Eq. 7, Eq. 8, and Eq. 13 is set to 0.05. The hyperparameter $\\epsilon$ in Eq. 19 is set to 0.8 for all datasets. The distance threshold T in Eq. 11 is set to 1.7 for Market-1501 and 1.5 for both DukeMTMC-ReID and MSMT17. All experiments are conducted using PyTorch on four NVIDIA Tesla A100 GPUs.\nC. Comparison with State-of-the-Art Methods\nIn this section, we compare our proposed CCAFL method with all state-of-the-art ReID methods, The comparison results are summarized in Table II and Table III.\n1) Comparison With Intra-camera Supervised Methods: We compare our proposed CLIP-ICS method with recent ICS ReID methods, including UGA [50], ACAN [17], MATE [10], Precise-ICS [11], PIPRID [51], CDL [52], DCL [53] and CMT [19]. Summarizing the comparison results , we observe that our method outperforms existing ICS methods. Specifically, CCAL surpasses the state-of-the-art CMT method by 1.2% and 2.9% in mAP on Market-1501 and DukeMTMC-ReID datasets, respectively, and achieves a 7.6% improvement on the highly challenging MSMT17 dataset. These results demonstrate the superior performance and generalization ca-pabilities of our method.\n2) Comparison With Fully Supervised Methods: We addi-tionally provide comparisons with four representative fully supervised methods for reference, including PCB [2], BoT [3], ABD-Net [1], AGW [41], LTReID [42], and CLIP-ReID [29]. As shown , our approach even surpasses the performance of well-known fully supervised methods such as PCB [2], BoT [3] and CLIP-ReID [29] on Market1501 and DukeMTMC-ReID datasets. Moreover, on the relatively complex MSMT17 dataset, our ICS ReID method achieves performance comparable to CLIP-ReID in terms of Rank-1. These findings demonstrate that our CCAFL method reduces annotation costs while achieving high-performance levels.\n3) Comparison With Purely Unsupervised Methods: Addi-tionally, our method is comprehensively compared with current state-of-the-art unsupervised methods. Specifically, compared to the state-of-the-art O2CAP method [8], our method out-performs it on MSMT17 dataset by 9.6% in terms of Rank-1 accuracy and 16.5% in terms of mAP. Experimental results indicate that in purely unsupervised tasks, the significant variations of pedestrians across different cameras lead to less satisfactory model performance. In contrast, ICS ReID benefits from more readily obtainable intra-camera annotated labels, which can significantly enhance the performance of ReID models.\n4) Comparison With Unsupervised Domain Adaptation Methods: Finally, to further illustrate that person ReID with only intra-camera labels aligns better with real-world scenar-ios, we also compare our approach with the most represen-tative state-of-the-art unsupervised domain adaptation (UDA) ReID methods. We report their best performance in Table III. Among these methods, our approach surpasses the most advanced CaCL [60] by 22.4% in terms of mAP on MSMT17 dataset. This significant improvement is primarily because UDA methods utilize supervised knowledge from the source domain to bridge the large domain gap in the target domain. In contrast, our approach relies solely on intra-camera labeled data in the target domain for learning, which is much less costly and achieves better results. These findings indicate that having more readily obtainable intra-camera annotation labels offers greater scalability.\nD. Ablation Studies\nIn this section, we conduct detailed ablation experiments on each proposed component in the CCAFL. The results are shown in Table IV.\n1) Baseline: Compared to previous methods, ClusterCon-trast [23] performs better by using contrastive learning and ID-centric memory. Therefore, in both Order 0 and Order 1, we establish corresponding ID-centric memory for intra- and inter-camera scenarios. Our baseline builds upon ClusterCon-trast [23], leveraging its efficient use of contrastive learning and feature centroid memory, resulting in notable accuracy improvements. It is worth mentioning that Order 1 is pre-trained on the CLIP-based model, and owing to its robust large-scale pre-training capability, it achieves better accuracy results compared to Order 0, which is pre-trained on ImageNet.\n2) Effectiveness of the Intra-camera Discriminative Learn-ing: Table IV shows that considering only the central features of IDs within the camera indeed yields considerable results. However, on the large-scale MSMT17 dataset, as the num-ber of pedestrians within each camera increases, insufficient learning of these features can impact model performance. From Order 2, it can be seen that our designed ICDL method achieves an improvement of 1.3% and 2.6% in mAP on Market1501 and MSMT17, respectively, demonstrating its effectiveness. Therefore, properly considering global difficult negative samples can effectively encourage the model to learn fine-grained pedestrian features within each camera."}, {"title": "V. CONCLUSION", "content": "This paper explores the application of CLIP in the semi-supervised task of ICS ReID and proposes a three-stage train-ing strategy for CLIP-based camera-invariant feature learning. First, we utilize the annotated labels within each camera"}]}