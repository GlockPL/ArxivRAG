{"title": "OPEN-VOCABULARY PANOPTIC SEGMENTATION USING BERT PRE-TRAINING OF VISION-LANGUAGE MULTIWAY TRANSFORMER MODEL", "authors": ["Yi-Chia Chen", "Wei-Hua Li", "Chu-Song Chen"], "abstract": "Open-vocabulary panoptic segmentation remains a challenging problem. One of the biggest difficulties lies in training models to generalize to an unlimited number of classes using limited categorized training data. Recent popular methods involve large-scale vision-language pre-trained foundation models, such as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation using another large-scale vision-language pre-trained model called BEiT-3 and leveraging the cross-modal attention between visual and linguistic features in BEiT-3 to achieve better performance. Experiments result demonstrates that OMTSeg performs favorably against state-of-the-art models. Code is available at https://github.com/AI-Application-and-Integration-Lab/OMTSeg", "sections": [{"title": "1. INTRODUCTION", "content": "Image segmentation is a task that involves dividing the input image into regions corresponding to different objects, which can be used in various scenarios [1, 2]. Recently, open-vocabulary image segmentation has attracted more attention and remains a challenging task. This task focuses on segmenting an image into instance-based or semantically consistent regions according to arbitrary text descriptions. The segmentation category labels have not necessarily been seen in the training phase. There are different approaches tackling this problem, but one of the most promising solutions is to leverage pre-trained large-scale vision-language models. CLIP [3] connects text and images using contrastive learning by aligning the semantic meanings of images and texts in a shared vector space, which was the mostly adopted one that serves as a fundamental knowledge base. It is then upgraded from whole-image-level descriptions to region-level panoptic segmentation for open-vocabulary image segmentation.\nA main advantage of the CLIP model over traditional computer vision models is that it can perform zero-shot learning, which means it can classify images into categories that it has never seen before, by simply using natural language descriptions of the categories. Hence, it can handle open-ended tasks, which means it can answer any natural language query about an image, not just predefined ones. However, CLIP uses a contrastive learning framework that consists of two components: an image encoder and a text encoder, which are independent of each other and are only aligned on the final output space of the feature descriptions. Therefore, CLIP does not fully exploit the cross-reference clues between the vision and language modalities in its middle-layer latent representations, restricting its open-vocabulary generalization performance on vision-language or pure vision tasks.\nSpecifically, to achieve open vocabulary image segmentation, a common method is to use a foundation vision-language model that can connect the visual concepts and natural language for an entire image input as a knowledge base. Additional structure added to the knowledge-base model then forms a downstream model, which is used to learn from the data with segmentation labels for only a few classes of objects. Inspired by these limited examples, the downstream model can then learn to segment and label any object, regardless of whether the category has been seen before.\nHowever, simply applying the output-layer embedding of the text and visual encoders of CLIP would be insufficient. To enforce the open-vocabulary capability by learning from limited segmentation examples, layer-wise latent representations have been adopted in several recent studies, e.g., MaskCLIP [4] and SAN [5]. A main issue is that the CLIP model lacks of considering the interaction between the latent representations of the image encoder and text encoder, so the intermediate representation it learns becomes less representative, restricting the performance of employing hierarchical latent representations for open-vocabulary segmentation.\nTherefore, we believe that cross-referred latent representations between the vision and language modalities are crucial for generalizing the image description capability from the whole-image level to the local-regional level. To leverage better the cross-modality information, we introduce an open-vocabulary image segmentation method leveraging the multi-modal foundation model based on BERT pre-training of multiway transformers [6, 7].\nOur approach, Open-vocabulary Multiway Transformer Segmentation (OMTseg), is simple but effective for open vocabulary segmentation. OMTseg is established based on BEIT-3 [6], which is a multiway transformer model that has"}, {"title": "2. RELATED WORKS", "content": "Semantic Segmentation and Instance Segmentation are two fundamental tasks in image-based content analysis. Panoptic Segmentation performs both semantic and instance segmentation in an image to achieve a comprehensive segmentation result, which is a field that bridges semantic-level and instance-level visual clues to conduct image segmentation. Recent studies in sementatic segmentation include SegFormer [8] and Segmenter [9], which employ structured Transformer encoders or Vision Transformer (ViT) to conduct efficient approaches for per-pixel classification. Outstanding approaches in panoptic segmentation include MaskFormer [10], which introduces a unified model to address both tasks using a mask classification approach. This model outperforms traditional per-pixel classification methods, especially when dealing with a large number of classes. Moreover, Mask2Former [11] presents a Masked-attention Mask Transformer that can handle various image segmentation tasks, reducing research effort and setting new benchmarks for panoptic segmentation.\nOur approach, on ther other hand, handles the open-vocabulary segmentation where the testing labels can be arbitrary. Since our approach leverages vision-language foundation models and performs panoptic segmentation for open-vocabulary labels, we adopt Mask2Former [11] as our segmentation header that undertakes the foundation model outputs and extend it to an open-vocabulary panoptic segmention model. In the following, we first give a review of recent vision-language foundation models. Then, we survey the works of open-vocabulary image segmentation. Finally, we briefly introduce the multiway transformers.\n2.1. Vision-Language Foundation Models\nThe recent development of large-scale vision-language models involves the integration of visual and linguistic features, trained on vast amounts of data, resulting in powerful capabilities. Early works like VinVL [12] lays the foundation. It was later expanded by models like CLIP [3], which enhances scalability and widening the applicability across numerous tasks. BLIP and BLIP-2 [13] have bootstrapped language-image pre-training, showing improved capabilities in image captioning. BEIT-3 [6] further contributes by integrating vision self-supervised learning, significantly boosting performance in visual tasks. These models represent a shift towards building robust and scalable solutions for a wide range of vision-language tasks.\n2.2. Open Vocabulary Image Segmentation\nOpen vocabulary segmentation has made significant contributions with the introduction of SimSeg [12], which leverages CLIP for semantic segmentation. Following models like OVSeg [14] and MaskCLIP [4] either fine-tunes CLIP or introduces novel architectures to better manage masked images. ODISE [15] brings together pre-trained text-image diffusion and discriminative models for open-vocabulary panoptic segmentation. SAN [5] introduces a Side Adapter Network for region recognition and FC-CLIP [16] simplifies the segmentation pipeline into a single-stage framework, enhancing the balance between accuracy and cost. These efforts emphasize the importance of using pre-trained vision-language models and innovative architectures to improve open-vocabulary segmentation.\n2.3. Multiway Transformers\nRecently, vision-language and multimodal learning have become popular and yielded various models. A notewor-"}, {"title": "3. PROPOSED METHOD", "content": "The task of Open Vocabulary Panoptic Segmentation (OVPS) involves segmenting an input image I into a set of regions, each associated with a category label and an instance identifier. The challenge lies in handling both seen and unseen categories. Let $C_{seen}$ denote the set of categories seen during training and $C_{unseen}$ the set of categories not encountered during training. The goal is to accurately segment and classify each pixel in I into one of the categories in $C = C_{seen} \\cup C_{unseen}$ and assign instance identifiers for object instances. Formally, for each pixel p in the image I, an OVPS model aims to output a tuple $(c_p, i_p)$, where $c_p \\in C$ is the category label and $i_p$ is the instance identifier. The instance identifier $i_p$ is unique for each object instance and is consistent for 'stuff' categories i.e., uncountable and amorphous regions like grass, sky, etc.\n3.1. OMTSeg\nIn this section, we provide an in-depth description of our OMTSeg model. Fig. 1 depicts an overview of OMTSeg.\nThe input comprises an image and a text string. The image is in RGB format $I \\in R^{H \\times W \\times 3}$ with H and W the height and width of the image, respectively. The text input consists of a sequence of category names, each associated with a special token [WLS] to split the category text from the text input and indicate the features corresponding to that category. E.g., if the target categories include 'person', 'snow', and 'snowboard', the text input is formatted as\n[WLS] person; [WLS] snow; [WLS] snowboard. (1)\nNext, we apply tokenization and word embedding to generate $T_{input} \\in R^{Nw}$, where Nw represents the number of the total word in text input. This structure of text input allows the model to effectively link the visual categories with the linguistic descriptions provided in the text.\n3.1.1. BEiT-3 Backbone\nBEiT-3 is employed as a core feature extraction backbone in OMTSeg, which integrates visual and linguistic inputs. First, The image input I is divided into patches, each linearly transformed into an embedding vector to form a visual feature $V \\in R^{Np \\times d}$, where $N_p$ represents the number of patches and d represents the dimension of each token. The text input $T_{input}$ is similarly encoded into a sequence of embeddings $L \\in R^{Nwxd}$, representing the linguistic features. To obtain spatial and sequential context, positional encodings are added to both visual (V) and linguistic (L) embeddings, resulting in enhanced feature sets (V') and (L'), which can be represented as $[V', L'] = [V, L] + Positional Encoding."}, {"title": "3.1.2. Visual Adapter for BEiT-3", "content": "In the OMTSeg model, the Vision Adapter is specifically designed to enhance the capabilities of the frozen BEiT-3 backbone, making it more adept at handling dense predictions.\nFig. 2 illustrates the overall architecture of the visual adapter. Following [17], the Vision Adapter consists of three main components: Spatial Prior Module (SPM), Spatial Feature Injector (SFI), and Multi-Scale Feature Extractor (MSFE), each tailored to complement the BEiT-3 architecture. SPM operates in parallel with BEiT-3's patch embedding layer, capturing local spatial contexts of images. It utilizes a convolutional stem similar to that in ResNet, comprising a series of convolutions and a max-pooling layer. This is followed by stride-2 3x3 convolutions and 1x1 convolutions, which project the feature maps to d dimensions. SPM is designed to capture spatial features at various resolutions, including 1/8, 1/16, and 1/32 of the original image size. SFI bridges the spatial features from SPM with the features extracted by BEiT-3. For each block of BEiT-3, the spatial features $F_{sp}$ act as keys and values in a cross-attention mechanism and the features $F^{BEIT}_{i}$ serve as queries. This module effectively injects spatial context into the BEiT-3 features as\n$F^{BEIT}_{i+1}$ = $F^{BEIT}_{i}$ + CrossAttention($F^{BEIT}_{i}$, $F_{sp}$). (3)\n$F^{l+1}_{sp}$ = $F^{l}_{sp}$ + FFN(LN($F^{l}_{sp}$)) (4)\n$F^{l+1}_{sp}$ = $F^{l}_{sp}$ + Attention(LN($F^{l}_{sp}$), LN($F^{BEIT}_{i}$)) (5)\nThe incorporation of Vision Adapter into BEiT-3 allows for effective adaptation of the transformer architecture for dense prediction tasks, significantly enhancing its multi-scale and location-aware feature representation capabilities."}, {"title": "3.1.3. Language Prompting", "content": "In OMTSeg, the Language Prompt plays a crucial role. It involves tuning the embeddings of special tokens, namely the [WLS] tokens, to better align the linguistic features with the visual context. Prompt tuning in OMTSeg focuses on the [WLS] tokens, which are designed to encapsulate category-specific information. These tokens undergo an adaptation process whose embeddings are fine-tuned to capture the nuances of each category. The tuning process is as follows:\nEmbed[WLS] \u2190 Embed[WLS] + \u0394Embed (6)\nHere, Embed[WLS] represents the initial embedding of the [WLS] token derived from the embedding of the [CLS] token in the transformer model. \u0394Embed is the trainable adjustment applied to the embedding, allowing each [WLS] token to uniquely represent its corresponding category. The fine-tuned [WLS] token embeddings are then integrated with the visual features from the BEIT-3 backbone. This integration ensures that the model not only captures the visual characteristics of each category but also aligns them with the corresponding textual descriptions. The Language Prompt mechanism in OMTSeg serves as a bridge between language and vision, enabling the model to leverage textual cues for enhanced segmentation performance."}, {"title": "3.1.4. Multiway Segmentation Head", "content": "Multiway Segmentation Head in OMTSeg is a critical component for producing segmentation masks, integrating the processed visual and linguistic features.\nThe architecture of the multiway segmentation head is inspired by Mask2Former [11], featuring multiple transformer decoders equipped with visual and linguistic cross attention to enhanced integration of visual and linguistic features. The architecture includes Masked Multi-Head Cross Attention (MaskedMHCA), Multi-Head Cross Attention (MHCA), Multi-Head Self-Attention (MHSA) and FFN. The Transformer decoder layer in the Multiway Segmentation Head can be represented as follows:\nX = MHCA(MaskedMHCA(X, $F_V$), $F_L$), (7)\nO = FFN(MHSA(X)), (8)"}, {"title": "3.2. Open-Vocabulary Classification", "content": "Unlike traditional close-set segmentation approaches, our method leverages a feature cosine similarity mechanism for classification tasks. This technique allows for a more flexible and descriptive segmentation process, accommodating a wide array of classes without the need for predefined categories. Given an mask feature vector $F_M$ and a corresponding linguistic feature $F_L$, the classification is performed via the cosine similarity between these two vectors. The class label c is then assigned by $c_{class}$ = arg max cos($F_M$, $F_L$), where cos denotes the cosine similarity function, and c ranges over all possible classes. This formulation enables the model to match image segments with their most relevant linguistic descriptors and subsequently implements open-vocabulary classification."}, {"title": "4. EXPERIMENTS", "content": "In this section, we present the experiments to evaluate the proposed OMTSeg for open-vocabulary segmentation.\n4.1. Datasets and Evaluation Criteria\nIn our experiments, we employ six benchmark datasets to thoroughly validate our approach. These include the datasets of COCO Panoptic, ADE20K-150, ADE20K-847, Pascal Context-59, Pascal Context-459, and Pascal VOC.\nCOCO Panoptic is an extension of the original COCO dataset. It serves as our primary training ground owing to its extensive class diversity and size.\nADE20K-150 is a subset of ADE20K [18], which includes 150 classes. It provides a balanced mix of object and stuff categories, thereby enabling a comprehensive evaluation.\nADE20K-847 is the full version of the ADE20K dataset, consisting of 847 categories. It offers a more challenging evaluation scenario.\nPascal Context-59 is derived from the original Pascal VOC, featuring 59 context-aware classes. It allows for an evaluation setting where context plays a significant role.\nPascal Context-459 is an expanded version of Pascal Context-59. This dataset contains 459 classes and provides a more exhaustive setting for evaluation.\nPascal VOC is one of the classic datasets in object segmentation. \"PAS-20\" in Tabel 1 denotes the Pascal VOC dataset with 20 object categories, while \"PAS-21\" indicates a version with 21 categories, including a further background class.\nRegarding the Evaluation Criteria, the metrics we employ are Average Precision, mean Intersection over Union, and Panoptic Quality. Average Precision (AP) is a commonly used metric to evaluate the performance of a segmentation model. AP is calculated as the area under the precision-recall curve. It measures the precision of the model at different recall levels. Mean Intersection over Union (mIoU) reflects the overall quality across all classes. A higher mIoU indicates better performance in capturing the agreement between the predicted and ground truth segmentation masks. Panoptic Quality (PQ) considers two main aspects: semantic segmentation quality and instance segmentation quality. It provides a comprehensive measure of the overall performance of a panoptic segmentation model.\n4.2. Implementation Details\nWe conduct our experiments using BEiT-3-large models as the backbone architectures. For training, we utilize the Lion optimizer with a weight decay of 0.15. The input images are cropped to a fixed size of 640 \u00d7 640. The learning rate is set to 3 x 10-5 and the model is trained for 90,000 iterations with a batch size of 16. The first 600 steps are used for warm-up, followed by a linear decay of the learning rate for the remaining iterations.\n4.3. Experiment Setting and Results\nOpen-Vocabulary Semantic Segmentation: To evaluate the performance of open-vocabulary segmentation, we train our OMTSeg model using the COCO Panoptic training set and perform evaluations on the remaining benchmark datasets including ADE20K, COCO, Pascal Context, and Pascal VOC. Following [15], we utilize mIoU to evaluate the performance on semantic segmentation. We compare our OMTSeg with numerous methods including ZS3Net [19], LSeg [20], GroupViT [21], SimSeg [22], ZegFormer [23], OVSeg [14], SAN [5], OpenSeg [24], ODISE [15] and MaskCLIP [4]. As can be seen in Table 1, OMTSeg achieves more favorable performance against previous methods, which reveals that our model of integrating vision adaptor and text prompt tuning into the cross-attention mechanism can effectively boost the open-vocabulary segmentation performance.\nPanoptic Segmentation: To evaluate the performance on panoptic segmentation, we train OMTSeg on the COCO Panoptic training set and evaluate it on ADE20K and COCO datasets, where these two datasets are commonly employed for the performance evaluation of panoptic segmentation. We use PQ, AP and mIoU to measure the performance for panoptic, instance, and semantic segmentations, respectively. As"}, {"title": "5. CONCLUSION", "content": "We introduced the Open Vocabulary Multiway Transformer Segmentation (OMTSeg), which leverages BEiT-3 with several architectural advancements. Our method is simple but effective and achieves state-of-the-art performance across various datasets. The integration of cross-modal attention, visual adapters, and language prompt tuning has proven to be effective, as evidenced by our comprehensive experimental evaluations and ablation studies. Future work will explore the extension of OMTSeg to other vision-language tasks. The results of OMTSeg pave the way for more advanced and efficient open-vocabulary segmentation models."}]}