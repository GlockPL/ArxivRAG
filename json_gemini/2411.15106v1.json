{"title": "About Time: Advances, Challenges, and Outlooks of Action Understanding", "authors": ["Alexandros Stergiou", "Ronald Poppe"], "abstract": "We have witnessed impressive advances in video action understanding. Increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. Current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context. This survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. We focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. We broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action. This division allows us to identify specific action modeling and video representation challenges. Finally, we outline future directions to address current shortcomings.", "sections": [{"title": "1 Introduction", "content": "For decades, analyzing human actions in videos has been of particular interest to the computer vision community. Videos are prominent in both our social and professional lives. Over time, the analysis of actions has shifted from the well-understood task of action recognition towards the fundamental and broader area of action understanding. Shown in Figure 1, action understanding now includes diverse tasks based on prediction and anticipation with multimodal inputs. The unique challenges and novel computation paradigms are the core focus of our survey.\nIn developmental psychology, action understanding has been explored across several psychological aspects (Thompson et al 2019):\nThe ability to understand the action performed relates to differentiating between analogous actions (Gallese et al 1996; Jeannerod 1994) and conceptual- izing how an action is performed (Spunt et al 2011).\nDetermining the goal of the action has been studied in the context of immediate goals (Calvo-Merino et al 2005; Kohler et al 2002; Rizzolatti et al 2001) in relation to motor functions for the execution of actions and the sensory perception of actions performed by others.\nDetermining the actor's intention refers to identify- ing high-level goals and motivations to perform actions (Kilner 2011). Intentions have been defined as the se- quential grouping of individual actions (Fogassi et al 2005) and their abstract associated target (Uithol et al 2011).\n1.1 Taxonomy of this survey\nInspired by the cognitive aspects of action understand- ing, we define three broad temporal scopes to group sem- inal machine vision action understanding tasks. We vi- sualize action sequence progression in Figure 2 with a currently (partially) performed action followed by a subsequent action. Tasks that require an action to be"}, {"title": "2 Modeling actions in videos", "content": "In this section, we define two general groups of ap- proaches for encoding videos without explicitly relat- ing them to tasks. We start with characterizing key challenges in Section 2.1. Approaches discussed in Sec- tion 2.2 model spatial and temporal information sepa- rately, while works overviewed in Section 2.3 use joint spatiotemporal representations.\n2.1 Challenges in action representation\nThe diversity of the video input poses several chal- lenges. Intra-class variations in the visual appearance of actions of the same category across videos can be due to viewpoint, occlusions, background noise, or lighting conditions. The performances and durations of actions can also significantly deviate. Such variations appear across datasets (Grauman et al 2022; Kay et al 2017; Miech et al 2019; Soomro et al 2012). Training/test set instance distribution variance can also significantly im- pact the performance and overall generalization of the learned semantics. Challenging action instances can be traced to feature representations further from the train- ing set distribution in such cases.\nSince action understanding tasks are increasingly semantic, we also face challenges in the diversity and granularity of the target outputs. Interpretation of the visual input, and sometimes the lack of observable information, increasingly requires higher-level under- standing. Consequently, the relation between visual input and model output becomes more complex. Vo- cabulary limitations present challenges as action cat- egories are often finite. The generalization of models to open-set or cross-domain settings primarily depends on the similarity between seen and unseen instances. Limited inter-class variation further affects good repre- sentation performance of rare coarse-grained concepts of visually similar actions. This issue is more prevalent for tasks that require fine-grained semantic granulari- ties.\n2.2 Separating visual and temporal information\nWe first discuss approaches that process visual and temporal information independently.\nTracking and template matching. Early works (Bobick and Davis 2001) have applied template matching to"}, {"title": "2.3 Jointly encoding space and time", "content": "Time and appearance can also be encoded jointly.\nPart-based representations. SpatioTemporal Interest Points (STIPs) (Laptev and Lindeberg 2003) extended spatial interest point detection methods (F\u00f6rstner and G\u00fclch 1987; Harris et al 1988) to the video domain. Liu and Shah (2008); Oikonomopoulos et al (2005) ex- plored salient points based on peaks of activity varia- tion. STIP features have been quantized in histograms of codewords (Schuldt et al 2004). Several approaches have studied action-relevant temporal locations across viewpoints (Yilmaz and Shah 2006) and view-invariant trajectories (Sheikh et al 2005). Doll\u00e1r et al (2005) pro- posed modeling periodic motions using sparse dis- tributions of points of interest. This feature extractor prompted subsequent works (Niebles et al 2008) with actions classified through a codebook of features.\nHolistic stochastic representations. Actions have also been modeled based on global information. Efros et al (2003) created representations for different body parts and regressed towards representations of pre-classified actions. Subsequent works have explored action de- scriptors focused on object shapes (Gorelick et al 2006; Jia and Yeung 2008), movements (Sun et al 2009), and spatiotemporal salient regions (Wong and Cipolla 2007). They have also extended existing approaches to multiple features and temporal scales (Amer and Todorovic 2012; Liu et al 2008; Zelnik-Manor and Irani 2001; Yang et al 2020b). Later works (Blank et al 2005) adapted and generalized holistic descriptors (Gore- lick et al 2006) by concatenating 2D silhouettes to form space-time shapes corresponding to action per- formances. Sadanand and Corso (2012) similarly pro- posed a bank of volumetrically pooled features con- taining high-level representations of the actions.\n3D CNNs. Orthogonal to hand-crafted features, 2D convolutions have been extended in various ways to 3D spatiotemporal kernels to jointly encode space and time (Baccouche et al 2011; Ji et al 2012; Taylor et al 2010; Tran et al 2015). Subsequent works have demonstrated the potential of adapting image models to video (Hara et al 2018), explored video-specific architectures with spa- tiotemporal volumes across channels (Chen et al 2018c), and tiled 3D kernels (Hegde et al 2018). They have also used channel-separated convolutions (Jiang et al 2019b; Luo and Yuille 2019; Tran et al 2019), temporal resid- ual connections (Qiu et al 2017), global feature fusion (Qiu et al 2019), resolution reduction (Chen et al 2019; Stergiou and Poppe 2021b), and related appearance to spatiotemporal embeddings (Wang et al 2018c; Zhou et al 2018d). Carreira and Zisserman (2017) integrated 3D convolutions into two-stream models for motion-"}, {"title": "3 Video datasets comprising human actions", "content": "Significant efforts have been made to collect video datasets for various action understanding tasks. We explore two broad dataset types based on target tasks and use cases. The first set includes general-purpose datasets for pre-training and model evaluation. The second set of datasets has been collected to evaluate models on specific modalities or domains. The sets are discussed in discussed in Section 3.1 and Section 3.2, respectively.\n3.1 General datasets\nThe past two decades have seen a significant increase in dataset size, leading to more robust baselines. We present widely-adopted benchmarks chronologically in Table 2. The primary focus of initial benchmarks (Schuldt et al 2004; Gorelick et al 2007) has been the categorization of simple actions such as walking and hand waving. Subsequent datasets predominantly com- prised videos from either TV shows/movies (Laptev and P\u00e9rez 2007; Laptev et al 2008; Marszalek et al 2009; Patron-Perez et al 2010; Kuehne et al 2011) or sports footage (Rodriguez et al 2008; Liu et al 2009; Reddy and Shah 2013; Niebles et al 2010). Important steps towards establishing large-scale datasets for the video domain were made with the introduction of Sports-1M (Karpathy et al 2014), YouTube-8M (Abu- El-Haija et al 2016), and Kinetics (Carreira and Zisser- man 2017) that include web-sourced videos of a di- verse range of actions. Evident from their sizes shown in Figure 3, these datasets paved the way as gen- eral benchmarks for models that can subsequently be adapted to smaller, more niche datasets such as UCF- 101 (Soomro et al 2012) and ActivityNet (Caba Heil- bron et al 2015). Despite their size and use in multiple downstream tasks, there is still room to address spe- cific action understanding tasks or modalities supple- mentary to vision. Domains such as egocentric vision, human-object interaction recognition, and hierarchical action understanding have gained popularity, prompt-"}, {"title": "3.2 Domain- and modality-specific datasets", "content": "Apart from general-purpose datasets, several bench- marks have been designed to evaluate model capabil- ities of specific aspects of action understanding. We overview benchmarks in three groups: based on the holistic understanding of scenes from multiple view- points, and with supplementary modalities such as lan- guage and audio.\nMulti-view. Initial efforts to compile multi-view videos included a small number of subjects (Sigal et al 2010) or synthetic data (Ionescu et al 2013). High-quality multi- view videos depend highly on the hardware and setup (Wang et al 2023e). CMU panoptic (Joo et al 2017) cap- tured group interactions within a dome with 480 cam- eras. Interactions included social settings, games, danc- ing, and musical performances. ZJU-Mocap (Peng et al 2021) comprised dynamic videos of human motions from 20 cameras. The Immersive Light Field dataset (Broxton et al 2020) contains videos with 6 degrees of freedom from a camera rig consisting of 46 action cam- eras. Multi-view datasets are collected for a variety of target tasks, including 3D video synthesis of human actions and interactions in indoor (Li et al 2022d) and outdoor (Lin et al 2021b; Yoon et al 2020) settings, dance sequence reconstruction (Tsuchida et al 2019), and the dynamic synthesis of indoor spaces in which actions take place (Tschernezki et al 2024).\nVideo-language. In recent years, language has been in- tegrated into vision methods as a natural extension to represent high-level semantics. Commonly, learning to map textual concepts and visual representations in a shared embedding space has been a widely adopted strategy by many video tasks (Amrani et al 2021; Gabeur et al 2020; Liu et al 2019; Miech et al 2020b). Ini- tial video-language datasets (Chen and Dolan 2011; Xu et al 2016) were based on short video snippets and short textual descriptions of actions. More recent efforts also provide multilingual descriptions (Wang et al 2019b). Video question-answering is a popular language-based task (Jang et al 2017; Lei et al 2018; Li et al 2024d; On- cescu et al 2021; Rawal et al 2024; Xiao et al 2021). The order of instructions has been of great interest in longer videos since the introduction of HowTo100M (Miech et al 2019) and YouCook2 (Zhou et al 2018b). Benchmarks have also been proposed for other long- form tasks such as moment retrieval (Rohrbach et al 2015; Song et al 2024; Yang et al 2024a), frame extrac- tion (Li et al 2024a), multi-modal open-ended question answering (Fu et al 2024; Ying et al 2024), and long- term reasoning (Chandrasegaran et al 2024; Fei et al 2024b; Mangalam et al 2023)."}, {"title": "4 Recognizing observed actions", "content": "The recognition of actions in videos is a fundamen- tal computer vision research theme. Relevant tasks focus on different aspects of the actions observed in full. We start by discussing approaches for optimizing model inputs in Section 4.1. We then overview popular temporal-based recognition tasks in Section 4.2. Tasks based on the semantic relationships between language and video are discussed in Section 4.3, whereas audio- visual and other multi-modal approaches appear in Section 4.4.\n4.1 Video reduction methods\nVideo inputs typically consist of tens to hundreds of highly visually similar frames. The uniform use of all frames can lead to an unsustainable computational bur- den. However, humans process stimuli selectively (Ea- gleman 2010). Several recognition approaches in com- puter vision aim to reduce compute and improve mem- ory utilization by considering inputs selectively.\n4.1.1 Challenges\nReducing frame-level redundancies in videos requires a high-level understanding of each temporal segment's relevance. The distinction and selection of relevant seg- ments directly impact information loss. Long and com- plex scenes present significant challenges to video re- duction methods. This uneven context inclusion requires more efficient utilization of the model's capacity.\n4.1.2 Approaches\nAmong the most common approaches for reducing re- dundancies is frame sampling. Works on frame sampling rely on policy networks that select frames based on the action's complexity (Ghodrati et al 2021; Yeung et al 2016), video context correspondence (Wu et al 2019c), or changes in the target class' probability (Korbar et al 2019). Wang et al (2021b) used a recurrent network to localize action-relevant regions. Subsequent exten- sions targeted early stopping (Wang et al 2022d) and related local and global features to determine action- relevant patches (Wang et al 2022e). Xia et al (2022b) used pseudo labels obtained by computing the embed- ding distance to class centroids to distinguish individ- ual frames as salient and non-salient. Other approaches have used reward functions based on predictions from the selected frames (Wu et al 2020d), combined frame- level and video-level predictions (Gowda et al 2021), optimized towards balancing accuracy and number of frames used (Wu et al 2019b), or removed tokens in transformer architectures (Wu et al 2024b).\nA related set of approaches has extended unimodal frame sampling with audio previewing. Gao et al (2020) used both frame and audio features with a recurrent network to predict the next informative moment in the video. The video resolution used by the model was determined based on discovered informative parts in the audio stream. Similarly, Nugroho et al (2023) used a saliency loss to localize the informative audio segments from which the corresponding video frames can be sampled."}, {"title": "4.1.3 Future outlooks", "content": "Context-aware models can significantly improve both processing times and performance. Although most video reduction methods are primarily evaluated on classification or detection, more recent action under- standing models have been optimized on multi-task and multi-domain objectives. This makes the discov- ery of relevant frames more difficult. For example, suppressing background frames is sub-optimal for episodic memory in which specific locations or at- tributes of objects not directly relevant to a current action need to be inferred. We expect that future re- dundancy reduction methods will focus more on pre- serving general scene information rather than ensuring that semantics of objects in the scene are not lost. This can be potentially achieved by discovering informa- tive frames for diverse tasks or distilling scene context in low-memory representations, e.g.language embed- dings."}, {"title": "4.2 Temporal-based tasks", "content": "The perception of actions across time is a complex ca- pability of human cognition. Understanding the timing of events is crucial for developing motor memory (Ea- gleman 2010) for actions such as moving, speaking, and determining the causality of perceived temporal patterns. The importance of processing temporal in- formation efficiently by computer vision systems has been shown through both standard performance met- rics and semantic benchmarks (Albanie et al 2020; Ster- giou and Deligiannis 2023). We provide a visualization of temporal-based tasks in Figure 5 with the main chal- lenges and task details discussed below.\n4.2.1 Challenges\nFor most tasks, action categories are inferred directly without an explicit notion of their complexity based on levels of abstraction, e.g., atomic movements, compos- ite motions, singular actions, or general activities. Al- though some datasets include action hierarchies (Shao et al 2020; Li et al 2018b), these relationships are only used by a handful of existing works (Long et al 2020). Different levels of abstraction typically have different temporal ranges, which require different approaches to process visual inputs. Moreover, there can be sub- stantial temporal variations across class instances. This leads to larger temporal distances between discrimina- tive information in videos, requiring proper extraction and modeling of long-range dependencies. We discuss solutions to cope with these variations in this section.\n4.2.2 Temporal localization\nA well-established video task is the discovery and clas- sification of the actions performed alongside their tem- poral segments. Temporal Action Localization (TAL) aims to infer the action categories alongside the start and end times of the corresponding locations in untrimmed videos.\nEarly attempts have used Improved Dense Trajec- tories (Wang and Schmid 2013) and Fisher vectors (Oneata et al 2013) to model the temporal dynamics of local points in scenes. Shou et al (2016) was one of the first to approach TAL with a joint action proposal and classification objective with a regional CNN. This joint optimization has been adapted with spatial- and temporal-only networks (Lin et al 2018; Paul et al 2018; Wang et al 2017a), regional proposal selection (Chao et al 2018; Xu et al 2017b), and intra-proposal relation- ships with graph convolutions (Zeng et al 2019) in sub- sequent works. Shou et al (2017) predicted granularities"}, {"title": "4.2.3 Spatiotemporal detection", "content": "SpatioTemporal Action Detection (STAD) is related to TAL but aims to jointly localize actions temporally and spatially detect action-relevant actors and objects. The main challenge of STAD methods is consistently link- ing detections and temporal action proposals across frames. Similar to TAL, two general directions can be used to overview relevant literature.\nTwo stages. Building upon the advancements of image- based object detectors (Girshick et al 2014; Girshick 2015), the majority of STAD approaches first detect objects and then temporally localize actions by track- ing object candidates (Jain et al 2014; Weinzaepfel et al 2015), ROI-pooling RGB and flow features (Peng and Schmid 2016), refining proposals iteratively (Soomro et al 2015), aligning source and target domain fea- tures (Agarwal et al 2020), or using the general action level in the video as context (Mettes et al 2016). Li et al (2018a) built upon prior two-stage detection works and incorporated recurrent proposals to include temporal context. Other approaches that focus on temporal in- formation Singh et al (2017) used the arrow of time with different portions of the video detected at each step. Later benchmarks included longer videos to fo- cus on activity-related tasks (Gu et al 2018) enabling the greater exploration of context with feature banks (Feng et al 2021b; Pan et al 2021a; Tang et al 2020a; Wang and Gupta 2018; Wu et al 2019a, 2022b) and sup- plementary object information (Arnab et al 2021b; Hou et al 2017; Zhang et al 2019d). Additional information such as keyframe saliency maps (Li et al 2020b; Ulu- tan et al 2020), hands and poses (Faure et al 2023), actor-object relations (Sun et al 2018), and SSL (Wang et al 2023d) have also been explored. Alwassel et al (2018) analyzed the benefits of two-stage approaches and showed that they are primarily performant in han- dling temporal context. However, they also note that a significant limitation of two-stage approaches is that features are computed from backbones trained over auxiliary video tasks, potentially missing specific dis- criminative information.\nSingle-stage. Drawing inspiration from single-stage object detection methods (Carion et al 2020; Redmon et al 2016; Liu et al 2016), single-stage STAD approaches use an end-to-end trained unified framework for joint localization and detection (Chen et al 2021b; Girdhar et al 2019; Zhu et al 2024b). Ntinou et al (2024) extended the bipartite matching loss from Carion et al (2020) to spatio-temporal tokens. Other approaches used adap- tive feature sampling (Wu et al 2023d), conditionally modeled visual features based on motion (Zhao and Snoek 2019), and contrasted different views (Kumar and Rawat 2022). Directly predicting tubelets has also been adopted by recent approaches (Gritsenko et al 2024; Kalogeiton et al 2017; Song et al 2019; Yang et al 2019; Zhao et al 2022). Kalogeiton et al (2017) stacked embeddings from a backbone applied over a sliding window and regressed both classes and tubelets over the entire video. Zhao et al (2022) used an encoder- decoder to generate tubelet queries and cross-attended them to visual features. Gritsenko et al (2024) gener- ated candidate tubelets from condensed query repre- sentations cross-attended by features from each frame. Beyond STAD, tubelets have also been used as a self-similarity pretraining objective (Thoker et al 2023) to enforce correspondence of videos from different do- mains but with similar local motions."}, {"title": "4.2.4 Repetition counting", "content": "Video Repetition Counting (VRC) aims to count the number of action repetitions. In contrast to TAL and STAD, VRC is an open-set task and does not require action categories.\nEarly works on signal periodicity (Thangali and Sclaroff 2005) have decomposed signal repetition with a Fourier analysis (Albu et al 2008; Briassouli and Ahuja 2007; Azy and Ahuja 2008; Cutler and Davis 2000; Pogalin et al 2008). Signal-based works have also used the direction of motion flow over time (Runia et al 2018) to count repetitions. Another set of methods ap- proached VRC as a classification task over a finite set of maximum repetitions. Lu and Ferrier (2004) used dy- namic parameters based on the Frobenius norm to clas- sify changes corresponding to action end times. Zhang et al (2021e) fused audio and video representations while Zhang et al (2020) used multiple cycles to re-"}, {"title": "4.2.5 Future outlooks", "content": "Despite the great progress, using unified systems to generalize across tasks remains challenging. For exam- ple, STAD methods (Dai et al 2021; Tirupattur et al 2021) benchmarked on TAL perform lower than TAL- based models as their joint objective of localizing both when and where actions are performed, is significantly more challenging to optimize. Similarly, despite the task similarities between TAL and VRC, standard TAL methods do not generalize to VRC as action interrup- tions and out-of-distribution categories cannot be effec- tively segmented (Hu et al 2022a; Sinha et al 2024). The recent introduction of unified VLMs for multiple video tasks; e.g. (Wang et al 2024d) and their use as a feature extractor in subsequent works (Chen et al 2024b), has shown a promising direction through the use of SSL. Training recipes typically include multiple stages of contrastive and masking pre-text self-supervision ob- jectives to allow the generalization of the model to mul- tiple tasks. Training on SSL pre-text tasks is a promi- nent scheme for many video-based models as shown in Table 3. A possible direction of future research can be the unification of downstream objectives through rele- vant pre-text tasks based on the arrow of time, relation- ships between task-specific embeddings, or clustering embeddings of semantically similar tasks."}, {"title": "4.3 Language semantics in videos", "content": "LLMs have achieved great success in Natural Lan- guage Processing (NLP), and have consequently been adapted for action understanding tasks. The relation- ships between learned context-rich semantic space and visual world attributes are useful for tasks such as cap- tion generation (Seo et al 2022; Sun et al 2019b; Wang et al 2024a), inferring scene information (Anderson et al 2018; Cheng et al 2024), understanding the gen- eral context in highlight detection (Lei et al 2021a), and instructional video learning (Miech et al 2020b). Be- yond their direct applicability to language-based tasks, they can incorporate vision encoders (Ashutosh et al 2023a; Fu et al 2021; Kahatapitiya et al 2024; Song et al 2024; Xu et al 2021; Zellers et al 2021) learning general and semantically-rich representations that can then be"}, {"title": "4.3.1 Challenges", "content": "Visual and language information can provide partly complementary perspectives of a video. However, as information from each modality is often heteroge- neous, specific representations may not be directly matched through cross-modal correspondence, e.g., due to occluded objects or fine-grained visual details about the performance of the action. Such discrepan- cies can arise based on domain knowledge specificity or distribution patterns of the available data (Liang et al 2024b). Modality gap (Liang et al 2022d), shown in Fig- ure 6, is a phenomenon that arises in VLM training in which embeddings of each modality are represented in distinct low-variance regions in the embeddings space. In VLMs trained with cross-modal information maxi- mization (Bain et al 2021; Lei et al 2021b; Li et al 2020a, 2022a; Zhu and Yang 2020) this effect becomes stronger with the enforcement of strong coordinate restrictions based on positive and negative cross-modal pairs. This is also relevant to difficulties in the cross-modal context alignment over local elements for tasks with available ground-truth pairs and global representations for tasks without vision-language pairs. In both cases, aligning language and vision context information at either the word/object level or over groups of instances in the embedding space provides a significant challenge in optimization. For generative tasks, this can also lead to difficulties in modality specific generation. Gener- ating semantic-rich data based on relationships from auxiliary modalities with ambiguous correspondence can impact conditional, stochastic, or auto-regressive generation."}, {"title": "4.3.2 Vision-language retrieval", "content": "Video retrieval sources relevant videos from a dataset based on an input query in natural language. As shown in Figure 7, research works can be categorized into instance- and semantic-based.\nInstance-based. Image methods have explored cross- view ranking (Wang et al 2016a), language to visual attention (Torabi et al 2016), or visual features as em- bedding targets for language encodings (Dong et al 2018). Early adaptation of visual-language approaches to videos have used image-text-video triplets (Otani et al 2016), and related parts of speech to objects and actions (Gabeur et al 2020; Xu et al 2015b).\nIn general, instance-based approaches use a binary score function to rank correspondences. Refinements to this objective have been made through visual-language binding with the inclusion of parts-of-speech in tar- get captions (Wray et al 2019) and dual object-text and action-text models (Liu et al 2019; Mithun et al 2018). A number of methods have studied vision-language pre-training approaches (Ge et al 2022b; Lin et al 2022b; Xue et al 2022). Ge et al (2022b) related verbs and nouns to questions and video segments. Xue et al (2022) studied the correspondences between keyframes and all video frames, subsequently contrasting the keyframe-fused video features to language embeddings.\nSemantics-based. A more challenging task is to re- trieve images based on shared semantics to query images (Gordo and Larlus 2017). Semantic-based ap- proaches primarily use triplet losses that contrastively regress between text queries and corresponding posi- tive and negative visual inputs.\nVideo retrieval works have studied this through ei- ther a contrastive objective based on a support set of videos with similar action categories (Patrick et al 2020) or a semantic similarity scoring function for videos from the same category (Wray et al 2021). Recently, Kim et al (2024c) have utilized prior knowledge in re- trieving text features based on embedding correspon- dences to similar visual features. Chun et al (2021) pro-"}, {"title": "4.3.3 Video Captioning", "content": "A long-standing challenge in computer vision is the generation of high-level descriptions in language. In contrast to retrieval tasks that depend on a fixed vo- cabulary, captioning is a generative task. Starting from matching a small corpus of words to objects in images (Barnard and Forsyth 2001; Barnard et al 2003), current works in the image domain are capable of generating diverse and detailed image descriptions (Mokady et al 2021; Alayrac et al 2022). Video captioning includes further challenges as the appearance of objects and the context of scenes change throughout the video. Given the temporal extent of videos, captioning tasks can be divided into two categories (Table 4)."}, {"title": "4.3.4 Video Question Answering (VideoQA)", "content": "A widely-used benchmark for VLM models is the uti- lization of visual context to answer natural language questions (Antol et al 2015; Goyal et al 2017b). In con- trast to video captioning, it requires understanding parts of objects and the temporal extent of relevant an- swers. Depending on the task setting, answers can be obtained from multi-choice QA, or as a global answer in open-end QA.\nIn a multi-choice QA setting, given a video and a question, the goal is to learn a mapping that returns an answer from a set of possible answers. In open-end QA settings, the answer is instead generated from a model conditioned on the video and question. VQA methods can be divided into two broad groups (Figure 9).\nScene-graphs. Early VideoQA approaches were based on either graph representations (Jiang and Han 2020; Tu et al 2014) or on each modality's heterogeneity. Huang et al (2020a) used object and location-based graph embeddings to relate visual and text features with a cross-modal similarity matrix. Graph represen- tations have also been created from hierarchies of ob- jects and their interactions (Dang et al 2021) as well as over multiple frames (Liu et al 2021b). Alternative approaches defined scales from multiple graph con- volution resolutions to relate cross-scale interactions (Guo et al 2021) or from subgraphs to capture static and dynamic scene objects (Cherian et al 2022). Park et al (2021a) created appearance, motion, and question graphs, learning conditionality by propagating nodes"}, {"title": "4.3.5 Future outlooks", "content": "Advancements in VLMs have enabled the recognition of actions based on their correspondence to a large lexical corpus. Building upon this correspondence, retrieval, captioning, and question-answering models have moved beyond single-instance structural repre- sentations and toward the discovery of abstract cross-"}, {"title": "4.4 Audio-visual and multimodal recognition", "content": "The recognition of actions or activities has been pre- dominantly studied in the vision domain. In contrast, the auditory recognition of actions from sounds emit- ted by objects or actors and their interactions is more sparsely researched. This task presents distinct chal- lenges as the sounds emitted by different objects or actions can be similar.\nTime-frequency spectrograms have been a popular format for representing audio events in videos. Initial audio-based models have been built following image- based object recognition (Gong et al 2021) or video clas- sification (Kazakos et al 2021) CNNs. Attention-based audio methods have used convolutional features (Gu- lati et al 2020; Kong et al 2020) or image-pretrained encoders (Koutini et al 202"}]}