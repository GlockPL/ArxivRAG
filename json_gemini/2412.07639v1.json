{"title": "Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization", "authors": ["Zongkai Liu", "Qian Lin", "Chao Yu", "Xiawei Wu", "Yile Liang", "Donghui Li", "Xuetao Ding"], "abstract": "Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that aims to learn optimal multi-agent policies from pre-collected datasets. Compared to single-agent case, multi-agent setting involves a large joint state-action space and coupled behaviors of multiple agents, which bring extra complexity to offline policy optimization. In this work, we re-visit the existing offline MARL methods and show that in certain scenarios they can be problematic, leading to uncoordinated behaviors and out-of-distribution (OOD) joint actions. To address these issues, we propose a new offline MARL algorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO sequentially updates each agent's policy in an in-sample manner, which not only avoids selecting OOD joint actions but also carefully considers teammates' updated policies to enhance coordination. Additionally, by thoroughly exploring low-probability actions in the behavior policy, In-SPO can well address the issue of premature convergence to sub-optimal solutions. Theoretically, we prove InSPO guarantees monotonic policy improvement and converges to quantal response equilibrium (QRE). Experimental results demonstrate the effectiveness of our method compared to current state-of-the-art offline MARL methods.", "sections": [{"title": "Introduction", "content": "Offline Reinforcement Learning (RL) is a rapidly evolving field that aims to learn optimal policies from pre-collected datasets without interacting directly with the environment (Figueiredo Prudencio, Maximo, and Colombini 2024). The primary challenge in offline RL is the issue of distributional shift (Yang et al. 2021), which occurs when policy evaluation on out-of-distribution (OOD) samples leads to the accumulation of extrapolation errors. Existing research usually tackles this problem by employing conservatism principles, compelling the learning policy to remain close to the data manifold through various data-related regularization techniques (Yang et al. 2021; Pan et al. 2022; Matsunaga et al. 2023; Shao et al. 2023; Wang et al. 2023b). In comparison to the single-agent counterpart, offline Multi-Agent Reinforcement Learning (MARL) has received relatively less attention. Under the multi-agent setting, it not only faces the challenges inherent to offline RL but also encounters common MARL issues, such as difficulties in coordination and large joint-action spaces (Zhang, Yang, and Ba\u015far 2021). These issues cannot be simply resolved by combining state-of-the-art offline RL solutions with modern multi-agent techniques (Yang et al. 2021). In fact, due to the increased number of agents and the offline nature of the problem, these issues become even more challenging. For example, under the offline setting, even if each agent selects an in-sample action, the resulting joint action may still be OOD. Additionally, in cooperative MARL, agents need to consider both their own actions and the actions of other agents in order to determine their contributions to the global return for high overall performance. Thus, under offline settings, discovering and learning cooperative joint policies from the dataset poses a unique challenge for offline MARL.\nTo address the aforementioned issues, recent works have developed specific offline MARL algorithms. These approaches generally integrate the conservatism principle into the Centralized Training with Decentralized Execution (CTDE) framework, such as value decomposition structures (Yang et al. 2021; Pan et al. 2022; Matsunaga et al. 2023; Shao et al. 2023; Wang et al. 2023b), which is developed under the Individual-Global-Max (IGM) assumption. Although these approaches have demonstrated successes in certain offline multi-agent tasks, they still exhibit several limitations. For example, due to the inherent limitations of the IGM principle, algorithms that utilize value decomposition structures may struggle to find optimal solutions because of constraints in their representation capabilities, and can even lead to the selection of OOD joint actions, as we show in the Proposed Method section.\nIn this work, we propose a principled approach to tackle OOD joint actions issue. By introducing a behavior regularization into the policy learning objective and derive the closed-form solution of the optimal policy, we develop a sequential policy optimization method in an entirely in-sample learning manner without generating potentially OOD actions. Besides, the sequential update scheme used in this method enhances both the representation capability of the joint policy and the coordination among agents. Then, to prevent premature convergence to local optima, we encourage sufficient exploration of low-probability actions in the behavior policy through the use of policy en-"}, {"title": "Related Work", "content": "MARL. The CTDE framework dominates current MARL research, facilitating agent cooperation. In CTDE, agents are centrally trained using global information but rely only on local observations to make decisions. Value decomposition is a notable method, representing the joint Q-function as a combination of individual agents' Q-functions (Wang et al. 2021; Son et al. 2019; Rashid et al. 2018). These methods typically depend on the IGM principle, assuming the optimal joint action corresponds to each agent's greedy actions. However, environments with multi-modal reward landscapes frequently violate the IGM assumption, limiting the effectiveness of value decomposition in learning optimal policies (Fu et al. 2022).\nAnother influential class of methods is Multi-Agent Policy Gradient (MAPG), with notable algorithms such as MAPPO(Yu et al. 2022), CoPPO(Wu et al. 2021), and HAPPO (Kuba et al. 2022). However, on-policy learning approaches like these struggle in offline settings due to OOD action issues, leading to extrapolation errors.\nOffline MARL. OMAR (Pan et al. 2022) combines Independent Learning and zeroth-order optimization to adapt CQL (Kumar et al. 2020) for multi-agent scenarios. However, OMAR fundamentally follows a single-agent learning paradigm, which treats other agents as part of the environment, and does not handle cooperative behavior learning and OOD joint actions insufficiently.\nTo enhance cooperation and efficiency in complex environments such as StarCraft II, some existing works employ value decomposition as a foundation for algorithm design. For instance, ICQ (Yang et al. 2021) introduces conservatism to prevent optimization on unseen state-action pairs, mitigating extrapolation errors. OMIGA (Wang et al. 2023b) and CFCQL (Shao et al. 2023) are the latest offline MARL methods, both integrating value decomposition structures. OMIGA applies implicit local value regularization to enable in-sample learning, while CFCQL calculates counterfactual regularization per agent, avoiding the excessive conservatism caused by direct value decomposition-CQL integration. Nonetheless, the IGM principle has been shown to fail in identifying optimal policies in multi-modal reward landscapes (Fu et al. 2022), due to the limited expressiveness of the Q-value network, which poses a potential risk of encountering the OOD joint actions issue in offline settings.\nAn alternative research direction in offline RL applies constraints on state-action distributions, called DIstribution"}, {"title": "Background", "content": "Cooperative Markov Game\nThe cooperative MARL problem is usually modeled as a cooperative Markov game (Littman 1994) $G = (N, S, A, P, r, \\gamma, d)$, where $N = {1,......, N}$ is the set of agent indices, S is the finite state space, $A = \\Pi_{i \\in N} A^i$ is the joint action space, with $A_i$ denoting the finite action space of agent i, $r : S \\times A \\rightarrow R$ is the common reward function shared with all agents, $P : S \\times A \\times S \\rightarrow [0, 1]$ is the transition probability function, $\\gamma \\in [0, 1)$ is the discount factor, and $d \\in \\Delta(S)$ is the initial state distribution. At time step t$\\in$ {1,...,T}, each agent i$\\in$ N at state $s_t \\in S$ selects an action $a^i_t \\sim \\pi^i(\\cdot|s_t)$ and moves to the next state $s_{t+1} \\sim P(\\cdot | s_t, a_t)$. It then receives a reward $r_t = r(s_t, a_t)$ according to the joint action $a_t = {a^1_t,......,a^N_t}$. We denote the joint policy as $\\pi(\\cdot|s) = \\Pi_{i\\in N} \\pi^i(\\cdot|s)$, and the joint policy except the i-th player as $\\pi^{-i}$. In a cooperative Markov game, all agents aim to learn a optimal joint policy $\\pi^*$ that jointly maximizes the expected discount returns $E_{s\\sim d, a \\sim \\pi} [\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)]$. Under the offline setting, only a pre-collected dataset $D = {(s_k, a_k, r_k, s'_k)}^K_{k=1}$ collected by an unknown behavior policy $\\mu = \\Pi_{i\\in N} \\mu^i$ is given and the environment interactions are not allowed.\nIGM Principle and Value Decomposition\nValue-based methods aim to learn a joint Q-function $Q: S\\times A \\rightarrow R$ to estimate the future expected return given the current state s and joint action a. However, directly computing the joint Q-function is challenging due to the huge state-action space in MARL. To address this issue, value decomposition decomposes the joint Q-function Q into individual Q-functions $Q^i$ for each agent: $Q(s,a) = f_{mix}(Q^1(s, a^1),......, Q^N(s, a^N); s)$, where $f_{mix}$ represents the mixing function conditioned on the state (Fu et al. 2022). The mixing function $f_{mix}$ must satisfy the IGM principle that"}, {"title": "The Proposed Method", "content": "OOD Joint Action in Offline MARL\nIn offline MARL, value decomposition methods are more prone to encountering OOD joint actions due to the constraints of the IGM principle in certain scenarios. We use the XOR game, shown in Figure 1, to illustrate this phenomenon. Figure 1(a) shows the reward matrix of the XOR game, while Figure 1(b) depicts the dataset considered in the offline setting. Since it is necessary to minimize temporal difference (TD) error $E_D[(f_{mix}(Q^1(a^1), Q^2(a^2)) - r(a^{1},a^{2}))^2]$ while satisfying the IGM principle, the local Q-functions for both agents are forced to satisfy $Q^i(B) > Q^i(A)$, i = 1, 2 (See Appendix D for a detailed derivation). As a result, both agents tend to choose action B, resulting in the OOD joint action (B, B).\nAnother line in offline MARL research combines MAPG methods and data-related regularization (Pan et al. 2022). However, they can still encounter the OOD joint actions issue although not constrained by the IGM principle. Considering again the above offline task, both learned agents are likely to choose (A, A) due to the data-related regularization. For agent 1, given that its teammate selects action A, choosing action B would yield a higher payoff. The same is true for agent 2, resulting in the OOD joint action (B, B). This situation arises because these methods do not fully consider the change of teammates' policies, leading to conflicting directions in policy updates.\nMAPG methods employing sequential update scheme can effectively address this issue, as they fully consider the direction of teammates' policy updates, thereby avoiding conflicts (Matsunaga et al. 2023; Kuba et al. 2022). In the same scenario as above, but with sequential updates, where agent 1 updates first followed by agent 2, agent 1 would still choose action B for a higher payoff. Then, when agent 2 updates, knowing that agent 1 chose B, it would find that sticking with action A is best. Consequently, sequential-update MAPG methods converge to the optimal policy.\nIn-Sample Sequential Policy Optimization\nInspired by the above discussions, we introduce an in-sample sequential policy optimization method under the behavior-regularized Markov game framework, i.e., Eq.(4). Here we consider the reverse KL divergence as the regularization, which means $f(x,y) = log(\\frac{x}{y})$. The benefit of choosing reverse KL divergence is that the global regularization can be decomposed naturally as $log(\\frac{x}{y}) = \\sum_{i\\in N} log(\\frac{x^i}{y^i})$, making the simplified computation of sequential-update possible. Denoting $1:n$ as an ordered subset {$i_1,......, i_n$} of N, and $-i_{1:n}$ as its complement, where $i_k$ is the k-th agent in the ordered subset and $1:0 = \\emptyset$, the sequential-update objectives are given by:\n$\\pi^{new}_{i_n} = arg max_{\\pi_{i_n}} E_{a_{i_n} \\sim \\pi_{i_n}} \\big [ Q^{\\pi_{old}}_{i_n} (s, a_{i_n}) - \\alpha log(\\frac{\\pi^{new}_{i_n} (a^{i_n} |s)}{\\mu^{i_n} (a^{i_n}|s)}) \\big ]$ (5)\nwhere\n$Q^{\\pi_{old}}_{i_n} (s, a_{i_n}) \\triangleq E_{a_{-i_{1:n}} \\sim \\pi^{\\pi_{old}}_{-i_{1:n}}} [ Q^{\\pi_{old}} (s, a_{-i_{1:n}}, a_{i_n}) ]$.\nHowever, the optimization objective (5) requires actions produced by the policy, which is in a out-of-sample learning manner, potentially leading to OOD actions. In order to achieve in-sample learning using only the dataset actions, we derive the closed-form solution of objectives (5) by the Karush-Kuhn-Tucker (KKT) conditions\n$\\pi^{new}_{i_n} (a^{i_n} |s) = \\mu^{i_n} (a^{i_n}|s) \\cdot exp ( \\frac{Q^{\\pi_{old}}_{i_n}(s, a^{i_n})}{\\alpha})$ (6)"}, {"title": "Maximum-Entropy Behavior-Regularized Markov Game", "content": "The existence of multiple local optima is a common phenomenon in many multi-agent tasks, where finding the global optimum is often extremely challenging. Therefore, near-optimal (or expert) behavior policies can easily fall into or stay near local optima. In such cases, because the data-related regularization enforces the learned policy to remain close to the behavior policy, optimizing the objective in Eq.(5) is more likely to cause the sequential policy optimization method to converge towards a deterministic policy that exploits this local optimum. Moreover, escaping this local optimum becomes challenging, as when one of the agents attempts to deviate unilaterally, the optimization objective (5) impedes this since it hurts the overall benefits.\nWe examine this issue using the M-NE game depicted in Figure 2, with Figure 2(a) showing the reward matrix and Figure 2(b) illustrating the offline dataset. In this game, there are three NEs: (A, A), (B, B), and (C, C), with rewards of 5, 10, and 20, respectively, where (C, C) represents the global optimal NE and other NEs are local optima. On the considered dataset, data-related regularization enforces agents to select A with a high probability. As a result, agents confidently converge to the local optimum (A, A) based on the observed high probability of their teammates choosing A, failing to recognize the optimal joint action (C, C).\nOne way to address this issue is to introduce perturbations to the rewards, preventing sequential policy optimization method from deterministically converging to a local optimum and thereby encouraging it to escape the local optimum and identify the global optimal solution. From a game-"}, {"title": "In-Sample Sequential Policy Optimization", "content": "theoretic perspective, the optimal solution of the perturbed game aligns with the solution concept of quantal response equilibrium (QRE) (McKelvey and Palfrey 1995).\nDefinition 1. For a Behavior-Regularized Markov Game G with a reward function r, denote the perturbed reward as $\\bar r$. Then, a joint policy $\\pi^*$ is a QRE if it holds\n$J(\\pi^*) \\geq J(\\pi^{i*}, \\pi^{-i}), \\forall i \\in N, \\pi^i$, (8)\nwhere $J(\\pi) = E_{\\pi} [\\sum_t \\gamma^t (r(s_t, a_t)- \\alpha f (\\pi(\\cdot|s_t), \\mu(\\cdot|s_t)))]$.\nTherefore, our goal is to design an in-sample sequential policy optimization method with QRE convergence guarantees. One simple and effective way to introduce disturbances is to add policy entropy into the rewards, which is also a commonly used regularization in online RL to improve exploration (Liu et al. 2024; Haarnoja et al. 2018). Therefore, we introduce the following Maximum-Entropy Behavior-Regularized Markov Game (MEBR-MG) problem, which is a generalization of Behavior-Regularized Markov Game (2).\n$max_{\\pi} E [\\sum_{t=1}^{T} (r(s_t, a_t) - \\alpha D_{KL}(\\pi(.|s_t), \\mu(.|s_t)) + \\beta H(\\pi(\\cdot|s_t)))]$ (9)\nwhere $H(\\pi(\\cdot|s_t))$ is policy entropy, and $\\beta \\geq 0$ is a temperature constant. In the following context, we first give some facts about MEBR-MG, and then give the in-sample sequential policy optimization method under MEBR-MG.\nIn MEBR-MG, we have the following modified policy evaluation operator given by:\n$T_{\\pi}Q_{\\pi}(s, a) = r(s, a) + \\gamma E_{s'|s,a} [V_{\\pi}(s')]$, (10)\nwhere\n$V_{\\pi}(s) = E_{a \\sim \\pi} [Q_{\\pi} (s, a) - \\frac{\\alpha}{\\mu^{i} (a^{i}|s)} log \\frac{\\pi^{i} (a^{i}|s)} {\\mu^{i} (a^{i}|s)} + \\beta log \\pi^{i} (a^{i}|s))]$.\nLemma 2. Given a policy $\\pi$, consider the modified policy evaluation operator $T_{\\pi}$ in Eq.(10) and a initial Q-function $Q:S\\times A \\rightarrow R$, and define $Q_{k+1} = T_{\\pi}Q_k$. Then the sequence $Q_k$ will converge to the Q-function $Q_\\pi$ of policy $\\pi$ as $k\\rightarrow \\infty$.\nProof can be found in Appendix A. This lemma indicates Q-function will converge to the Q-value under the joint policy $\\pi$ by repeatedly applying the policy evaluation operator. Moreover, the additional smoothness introduced by the regularization term allows the QRE of MEBR-MG to be expressed in the form of Boltzmann distributions, as demonstrated by the following Proposition 3.\nProposition 3. In a MEBR-MG, a joint policy $\\pi^*$ is a QRE if it holds\n$V_{\\pi} (s) \\geq V_{\\pi^{i*}, \\pi^{-i}}(s), \\forall i \\in N, \\pi^i, s \\in S$.\nThen the QRE policies for each agent i are given by\n$\\pi^{i*}(a|s) \\propto \\mu^{i}(a^{i}|s)$\n$\\cdot exp (\\frac{E_{a^{-i}\\sim \\pi^{-i}} [Q_{\\pi}(s, a^{i}, a^{-i})] - \\beta log \\mu^{i} (a^{i}|s))}{\\alpha + \\beta})$ (11) (12)"}, {"title": "The Practical Implementation of InSPO", "content": "In this section, we design a practical implementation of In-SPO to handle the issue of large state-action space, making it more suitable for offline MARL. More details can be found in Appendix B.\nPolicy Evaluation. According to Eq.(10), we need to train a global Q-function to estimate the expected future return based on the current state and joint action. However, in MARL, the joint action space grows exponentially with the number of agents. To circumvent this exponential complexity, we instead maintain a local Q-function $Q^{\\pi_{i_n}}$ for each"}, {"title": "Experiments", "content": "We conduct a series of experiments to evaluate InSPO on XOR game, M-NE game, Bridge (Fu et al. 2022) and StarCraft II Micromanagement (Xu et al. 2023b). In addition to Behavior Cloning (BC), our baselines also include the current state-of-the-art offline MARL algorithms: OMAR (Pan et al. 2022), CFCQL (Shao et al. 2023), OMIGA (Wang et al. 2023b) and AlberDICE (Matsunaga et al. 2023). Each algorithm is run for five random seeds, and we report the mean performance with standard deviation. For the final results, we indicate the algorithm with the best mean performance in bold, and an asterisk (*) denotes that the metric is not significantly different from the top-performing metric in that case, based on a heteroscedastic two-sided t-test with a 5% significance level. See Appendix C for experimental details.\nComparative Evaluation\nMatrix Game. We evaluate whether InSPO can address the two issues highlighted in previous section using the XOR game and M-NE game shown in Figure 1(a) and Figure 2(a). First, we evaluate the ability of InSPO to handle"}, {"title": "Ablation Study", "content": "Here we present the impact of different components on performance of InSPO. Figure 5(a) shows the converged policy of InSPO without entropy in the M-NE game on the imbalanced dataset. Without the perturba-"}, {"title": "Conclusion", "content": "In this paper, we study the offline MARL problem, a topic of significant practical importance and challenges that has not received adequate attention. We begin with two simple yet highly illustrative matrix games, highlighting some limitations of current offline MARL algorithms in addressing OOD joint actions and sub-optimal convergence issues. To overcome these challenges, we propose a novel algorithm called InSPO, which utilizes sequential-update in-sample learning to avoid OOD joint actions, and introduces policy entropy to ensure comprehensive exploration of the dataset, thus avoiding the influence of local optimum behavior policies. Furthermore, we theoretically demonstrate that InSPO possesses monotonic improvement and QRE convergence properties, and then empirically validate its superior performance on various MARL benchmarks. For future research, integrating sequential-update in-sample learning and enhanced dataset utilization with other offline MARL algorithms presents an intriguing direction."}, {"title": "Proofs", "content": "A.1 Proof of Policy Evaluation\nLemma 6. Given a policy $\\pi$, consider the modified policy evaluation operator $T_{\\pi}$ under MEBR-MGs and a initial Q-function $Q_0: S \\times A \\rightarrow R$, and define $Q_{k+1} = T_{\\pi}Q_k$. Then the sequence $Q_k$ will converge to the Q-function $Q_\\pi$ of policy $\\pi$ as $k \\rightarrow \\infty$.\nProof. With a pseudo-reward $r_\\pi(s,a)\\triangleq r(s,a) - E_{s'|s,a}[\\alpha D_{KL}(\\pi(\\cdot|s'), \\mu(\\cdot|s')) - \\beta H(\\pi(\\cdot|s'))]$, the update rule of Q-function can be represented as:\n$Q(s, a) \\leftarrow r_\\pi(s, a) + E_{s'|s,a,a'\\sim\\pi} [Q(s', a')]$.\nThen, we can apply the standard convergence results for policy evaluation (Sutton 2018).\nA.2 Proof of QRE\nProposition 7. In a MEBR-MG, a joint policy $\\pi^*$ is a QRE if it holds\n$V_{\\pi} (s) \\geq V_{\\pi^{i*}, \\pi^{-i}}(s), \\forall i \\in N, \\pi^i, s \\in S$. (16)\nThen the QRE policies for each agent i are given by\n$\\pi^{i*}(a|s) \\propto \\mu^{i}(a^{i}|s)$\n$\\cdot exp (\\frac{E_{a^{-i}\\sim \\pi^{-i}} [Q_{\\pi}(s, a^{i}, a^{-i})] - \\beta log \\mu^{i} (a^{i}|s))}{\\alpha + \\beta})$ (17)"}, {"title": "Proof of QRE convergence", "content": "B Details of Practical Algorithm\nIn the practical implementation of InSPO, we train the policy network $\\theta^{i_n}$ by loss function\n$J(\\theta^{i_n}) = E_{(s,a^{i_n})\\sim D^{\\pi_{i_n}}} [(\\frac{exp (\\frac{A^{\\pi_{old}}_{i_n}(s, a^{i_n}) - \\beta log \\pi^{i_n} (a^{i_n} |s)}{\\alpha + \\beta})}{\\pi^{i_n} (a^{i_n}|s)} - log \\pi^{i_n} (a^{i_n} |s)})^2]$ (20)"}, {"title": "Similarity Techniques to the Concept of \"sequential\"", "content": "The idea of \"sequential\u201d has been explored in various directions within MARL. Specifically, Ding et al. introduced SeqComm, a communication framework where agents condition their actions based on the ordered actions of others, mitigating circular dependencies that arise in simultaneous communication. MACPF (Wang, Ye, and Lu 2023) decomposes joint policies into individual ones, incorporating a correction term to model dependencies on preceding agents' actions in sequential execution. BPPO (Li et al. 2024) employs an auto-regressive joint policy with a fixed execution order. During training, agents act sequentially based on the prior agents' actions, and update their policies based on the feedback from subsequent agents. This bidirectional mechanism enables each agent adapts to the changing behavior of the team efficiently.\nWhile these works use the concept of \"sequential\" to improve coordination, applying them directly in offline MARL poses challenges. Both policy and value functions in MACPF and BPPO explicitly condition on prior agents' actions, which can be challenging in offline settings where required data may be missing, potentially hindering accurate value function updates."}]}