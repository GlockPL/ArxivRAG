[{"title": "Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization", "authors": ["Zongkai Liu", "Qian Lin", "Chao Yu", "Xiawei Wu", "Yile Liang", "Donghui Li", "Xuetao Ding"], "abstract": "Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that aims to learn optimal multi-agent policies from pre-collected datasets. Compared to single-agent case, multi-agent setting involves a large joint state-action space and coupled behaviors of multiple agents, which bring extra complexity to offline policy optimization. In this work, we revisit the existing offline MARL methods and show that in certain scenarios they can be problematic, leading to uncoordinated behaviors and out-of-distribution (OOD) joint actions. To address these issues, we propose a new offline MARL algorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO sequentially updates each agent's policy in an in-sample manner, which not only avoids selecting OOD joint actions but also carefully considers teammates' updated policies to enhance coordination. Additionally, by thoroughly exploring low-probability actions in the behavior policy, InSPO can well address the issue of premature convergence to sub-optimal solutions. Theoretically, we prove InSPO guarantees monotonic policy improvement and converges to quantal response equilibrium (QRE). Experimental results demonstrate the effectiveness of our method compared to current state-of-the-art offline MARL methods.", "sections": [{"title": "Introduction", "content": "Offline Reinforcement Learning (RL) is a rapidly evolving field that aims to learn optimal policies from pre-collected datasets without interacting directly with the environment (Figueiredo Prudencio, Maximo, and Colombini 2024). The primary challenge in offline RL is the issue of distributional shift (Yang et al. 2021), which occurs when policy evaluation on out-of-distribution (OOD) samples leads to the accumulation of extrapolation errors. Existing research usually tackles this problem by employing conservatism principles, compelling the learning policy to remain close to the data manifold through various data-related regularization techniques (Yang et al. 2021; Pan et al. 2022; Matsunaga et al. 2023; Shao et al. 2023; Wang et al. 2023b). In comparison to the single-agent counterpart, offline Multi-Agent Reinforcement Learning (MARL) has received relatively less attention. Under the multi-agent setting, it not only faces the challenges inherent to offline RL but also encounters common MARL issues, such as difficulties in coordination and large joint-action spaces (Zhang, Yang, and Ba\u015far 2021). These issues cannot be simply resolved by combining state-of-the-art offline RL solutions with modern multi-agent techniques (Yang et al. 2021). In fact, due to the increased number of agents and the offline nature of the problem, these issues become even more challenging. For example, under the offline setting, even if each agent selects an in-sample action, the resulting joint action may still be OOD. Additionally, in cooperative MARL, agents need to consider both their own actions and the actions of other agents in order to determine their contributions to the global return for high overall performance. Thus, under offline settings, discovering and learning cooperative joint policies from the dataset poses a unique challenge for offline MARL.\nTo address the aforementioned issues, recent works have developed specific offline MARL algorithms. These approaches generally integrate the conservatism principle into the Centralized Training with Decentralized Execution (CTDE) framework, such as value decomposition structures (Yang et al. 2021; Pan et al. 2022; Matsunaga et al. 2023; Shao et al. 2023; Wang et al. 2023b), which is developed under the Individual-Global-Max (IGM) assumption. Although these approaches have demonstrated successes in certain offline multi-agent tasks, they still exhibit several limitations. For example, due to the inherent limitations of the IGM principle, algorithms that utilize value decomposition structures may struggle to find optimal solutions because of constraints in their representation capabilities, and can even lead to the selection of OOD joint actions, as we show in the Proposed Method section.\nIn this work, we propose a principled approach to tackle OOD joint actions issue. By introducing a behavior regularization into the policy learning objective and derive the closed-form solution of the optimal policy, we develop a sequential policy optimization method in an entirely in-sample learning manner without generating potentially OOD actions. Besides, the sequential update scheme used in this method enhances both the representation capability of the joint policy and the coordination among agents. Then, to prevent premature convergence to local optima, we encourage sufficient exploration of low-probability actions in the behavior policy through the use of policy entropy. The proposed novel algorithm, named the In-Sample Sequential Policy Optimization (InSPO), enjoys the properties of monotonic improvement and convergence to quantal response equilibrium (QRE) (McKelvey and Palfrey 1995), a solution concept in game theory. We evaluate InSPO in the XOR game, Multi-NE game, and Bridge to demonstrate its effectiveness in addressing OOD joint action and local optimum convergence issues. Additionally, we test it on various types of offline datasets in the StarCraft II micromanagement benchmark to showcase its competitiveness with current state-of-the-art offline MARL algorithms."}, {"title": "Related Work", "content": "MARL. The CTDE framework dominates current MARL research, facilitating agent cooperation. In CTDE, agents are centrally trained using global information but rely only on local observations to make decisions. Value decomposition is a notable method, representing the joint Q-function as a combination of individual agents' Q-functions (Wang et al. 2021; Son et al. 2019; Rashid et al. 2018). These methods typically depend on the IGM principle, assuming the optimal joint action corresponds to each agent's greedy actions. However, environments with multi-modal reward landscapes frequently violate the IGM assumption, limiting the effectiveness of value decomposition in learning optimal policies (Fu et al. 2022).\nAnother influential class of methods is Multi-Agent Policy Gradient (MAPG), with notable algorithms such as MAPPO(Yu et al. 2022), CoPPO(Wu et al. 2021), and HAPPO (Kuba et al. 2022). However, on-policy learning approaches like these struggle in offline settings due to OOD action issues, leading to extrapolation errors.\nOffline MARL. OMAR (Pan et al. 2022) combines Independent Learning and zeroth-order optimization to adapt CQL (Kumar et al. 2020) for multi-agent scenarios. However, OMAR fundamentally follows a single-agent learning paradigm, which treats other agents as part of the environment, and does not handle cooperative behavior learning and OOD joint actions insufficiently.\nTo enhance cooperation and efficiency in complex environments such as StarCraft II, some existing works employ value decomposition as a foundation for algorithm design. For instance, ICQ (Yang et al. 2021) introduces conservatism to prevent optimization on unseen state-action pairs, mitigating extrapolation errors. OMIGA (Wang et al. 2023b) and CFCQL (Shao et al. 2023) are the latest offline MARL methods, both integrating value decomposition structures. OMIGA applies implicit local value regularization to enable in-sample learning, while CFCQL calculates counterfactual regularization per agent, avoiding the excessive conservatism caused by direct value decomposition-CQL integration. Nonetheless, the IGM principle has been shown to fail in identifying optimal policies in multi-modal reward landscapes (Fu et al. 2022), due to the limited expressiveness of the Q-value network, which poses a potential risk of encountering the OOD joint actions issue in offline settings.\nAn alternative research direction in offline RL applies constraints on state-action distributions, called DIstribution Correction Estimation (DICE) methods (Figueiredo Prudencio, Maximo, and Colombini 2024). AlberDICE (Matsunaga et al. 2023) is a pioneering DICE-based method in offline MARL, which is proved to converge to NEs. However, when multiple NEs exist, its convergence results heavily depends on the dataset distribution. If the behavior policy is near a sub-optimal NE, AlberDICE will converge directly to that sub-optimal solution rather than the global optimum. This is primarily because AlberDICE lacks sufficient exploration of low-probability state-action pairs in dataset, leading to premature convergence to a deterministic policy. Additionally, AlberDICE employs an out-of-sample learning during policy extraction, i.e., it uses actions produced by the policy rather the actions in datasets, which could lead to OOD joint actions (Xu et al. 2023a; Kostrikov, Nair, and Levine 2022). Additionally, some works consider using model-based method (Barde et al. 2024) or using diffusion models (Li, Pan, and Huang 2023; Zhu et al. 2023) to solve OOD issue. Furthermore, we also provide a discussion about the concept of \"sequential\" in Appendix E."}, {"title": "Background", "content": "Cooperative Markov Game\nThe cooperative MARL problem is usually modeled as a cooperative Markov game (Littman 1994) $G = (N, S, A, P, r, \\gamma, d)$, where $N = \\{1, ..., N\\}$ is the set of agent indices, S is the finite state space, $A = \\prod_{i \\in N} A^i$ is the joint action space, with $A^i$ denoting the finite action space of agent i, $r: S \\times A \\rightarrow R$ is the common reward function shared with all agents, $P: S \\times A \\times S \\rightarrow [0, 1]$ is the transition probability function, $\\gamma \\in [0, 1)$ is the discount factor, and $d \\in \\Delta(S)$ is the initial state distribution. At time step $t \\in \\{1, ..., T\\}$, each agent $i \\in N$ at state $s_t \\in S$ selects an action $a_t^i \\sim \\pi^i(\\cdot|s_t)$ and moves to the next state $s_{t+1} \\sim P(\\cdot|s_t, a_t)$. It then receives a reward $r_t = r(s_t, a_t)$ according to the joint action $a_t = \\{a_t^1, ..., a_t^N\\}$. We denote the joint policy as $\\pi(\\cdot|s) = \\prod_{i \\in N} \\pi^i(\\cdot|s)$, and the joint policy except the i-th player as $\\pi^{-i}$. In a cooperative Markov game, all agents aim to learn a optimal joint policy $\\pi^*$ that jointly maximizes the expected discount returns $E_{s_0 \\sim d, a \\sim \\pi} [\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)]$. Under the offline setting, only a pre-collected dataset $D = \\{(s, a, r, s')_k\\}_{k=1}^{|D|}$ collected by an unknown behavior policy $\\mu = \\prod_{i \\in N} \\mu^i$ is given and the environment interactions are not allowed.\nIGM Principle and Value Decomposition\nValue-based methods aim to learn a joint Q-function $Q: S \\times A \\rightarrow R$ to estimate the future expected return given the current state s and joint action a. However, directly computing the joint Q-function is challenging due to the huge state-action space in MARL. To address this issue, value decomposition decomposes the joint Q-function Q into individual Q-functions $Q^i$ for each agent: $Q(s, a) = f_{mix}(Q^1(s, a^1), ..., Q^N(s, a^N); s)$, where $f_{mix}$ represents the mixing function conditioned on the state (Fu et al. 2022). The mixing function $f_{mix}$ must satisfy the IGM principle that"}, {"title": "The Proposed Method", "content": "OOD Joint Action in Offline MARL\nIn offline MARL, value decomposition methods are more prone to encountering OOD joint actions due to the constraints of the IGM principle in certain scenarios. We use the XOR game, shown in Figure 1, to illustrate this phenomenon. Figure 1(a) shows the reward matrix of the XOR game, while Figure 1(b) depicts the dataset considered in the offline setting. Since it is necessary to minimize temporal difference (TD) error $E_D[(f_{mix}(Q^1(a^1), Q^2(a^2)) - r(a^1, a^2))^2]$ while satisfying the IGM principle, the local Q-functions for both agents are forced to satisfy $Q^i(B) > Q^i(A), i = 1, 2$ (See Appendix D for a detailed derivation). As a result, both agents tend to choose action B, resulting in the OOD joint action (B, B).\nAnother line in offline MARL research combines MAPG methods and data-related regularization (Pan et al. 2022). However, they can still encounter the OOD joint actions issue although not constrained by the IGM principle. Considering again the above offline task, both learned agents are likely to choose (A, A) due to the data-related regularization. For agent 1, given that its teammate selects action A, choosing action B would yield a higher payoff. The same is true for agent 2, resulting in the OOD joint action (B, B). This situation arises because these methods do not fully consider the change of teammates' policies, leading to conflicting directions in policy updates.\nMAPG methods employing sequential update scheme can effectively address this issue, as they fully consider the direction of teammates' policy updates, thereby avoiding conflicts (Matsunaga et al. 2023; Kuba et al. 2022). In the same scenario as above, but with sequential updates, where agent 1 updates first followed by agent 2, agent 1 would still choose action B for a higher payoff. Then, when agent 2 updates, knowing that agent 1 chose B, it would find that sticking with action A is best. Consequently, sequential-update MAPG methods converge to the optimal policy.\nIn-Sample Sequential Policy Optimization\nInspired by the above discussions, we introduce an in-sample sequential policy optimization method under the behavior-regularized Markov game framework, i.e., Eq.(4). Here we consider the reverse KL divergence as the regularization, which means $f(x, y) = log(\\frac{x}{y})$. The benefit of choosing reverse KL divergence is that the global regularization can be decomposed naturally as $log(\\frac{x}{y}) = \\sum_{i \\in N} log(\\frac{x^i}{y^i})$, making the simplified computation of sequential-update possible. Denoting 1:n as an ordered subset \\{i_1, ..., i_n\\} of N, and -i1:n as its complement, where $i_k$ is the k-th agent in the ordered subset and 1:0 = 0, the sequential-update objectives are given by:\n$\\pi_{i_n}^{new} = arg \\max_{\\pi_{i_n}} E_{a^{\\neg i_n}} \\pi^{\\neg i_n} \\sum_{Q_{i_n^{1:n}}(s, a^{i_n}) - \\alpha log(\\frac{\\pi_{i_n}(a^{i_n}|s)}{\\mu_{i_n}(a^{i_n}|s)})}$ (5)\nwhere\n$Q_{i_n^{1:n}}(s, a^{i_n}) = E_{a^{-i^{1:n}}} \\pi_{-i^{1:n}} \\pi_{i_n^{-1:n-1}}^{old} [Q^{old}(s, a^{-i^{1:n}}, a^{i_n})]$\nHowever, the optimization objective (5) requires actions produced by the policy, which is in a out-of-sample learning manner, potentially leading to OOD actions. In order to achieve in-sample learning using only the dataset actions, we derive the closed-form solution of objectives (5) by the Karush-Kuhn-Tucker (KKT) conditions\n$\\pi_{i_n}^{new}(a^{i_n}|s) \\propto \\mu_{i_n}(a^{i_n}|s) \\cdot exp(\\frac{Q_{i_n^{1:n}}(s, a^{i_n})}{\\alpha})$ (6)"}, {"title": "Maximum-Entropy Behavior-Regularized Markov Game", "content": "The existence of multiple local optima is a common phenomenon in many multi-agent tasks, where finding the global optimum is often extremely challenging. Therefore, near-optimal (or expert) behavior policies can easily fall into or stay near local optima. In such cases, because the data-related regularization enforces the learned policy to remain close to the behavior policy, optimizing the objective in Eq.(5) is more likely to cause the sequential policy optimization method to converge towards a deterministic policy that exploits this local optimum. Moreover, escaping this local optimum becomes challenging, as when one of the agents attempts to deviate unilaterally, the optimization objective (5) impedes this since it hurts the overall benefits.\nWe examine this issue using the M-NE game depicted in Figure 2, with Figure 2(a) showing the reward matrix and Figure 2(b) illustrating the offline dataset. In this game, there are three NEs: (A, A), (B, B), and (C, C), with rewards of 5, 10, and 20, respectively, where (C, C) represents the global optimal NE and other NEs are local optima. On the considered dataset, data-related regularization enforces agents to select A with a high probability. As a result, agents confidently converge to the local optimum (A, A) based on the observed high probability of their teammates choosing A, failing to recognize the optimal joint action (C, C).\nOne way to address this issue is to introduce perturbations to the rewards, preventing sequential policy optimization method from deterministically converging to a local optimum and thereby encouraging it to escape the local optimum and identify the global optimal solution. From a game-theoretic perspective, the optimal solution of the perturbed game aligns with the solution concept of quantal response equilibrium (QRE) (McKelvey and Palfrey 1995).\nDefinition 1. For a Behavior-Regularized Markov Game G with a reward function r, denote the perturbed reward as $\\tilde{r}$. Then, a joint policy $\\pi^*$ is a QRE if it holds\n$J(\\pi^*) \\geq J(\\pi^{i'}, \\pi^{i}), \\forall i \\in N, \\pi^{i}$, (8)\nwhere $J(\\pi) = E_{\\pi}[\\sum_t \\gamma^t (r(s_t, a_t) - \\alpha f(\\pi(\\cdot|s_t), \\mu(\\cdot|s_t))))]$.\nTherefore, our goal is to design an in-sample sequential policy optimization method with QRE convergence guarantees. One simple and effective way to introduce disturbances is to add policy entropy into the rewards, which is also a commonly used regularization in online RL to improve exploration (Liu et al. 2024; Haarnoja et al. 2018). Therefore, we introduce the following Maximum-Entropy Behavior-Regularized Markov Game (MEBR-MG) problem, which is a generalization of Behavior-Regularized Markov Game (2).\n$\\max_{\\pi} E[\\sum_{t=1}^{T} (r(s_t, a_t) - \\alpha D_{KL}(\\pi(\\cdot|s_t), \\mu(\\cdot|s_t)) + \\beta H(\\pi(\\cdot|s_t)))]$, (9)\nwhere $H(\\pi(\\cdot|s_t))$ is policy entropy, and $\\beta \\geq 0$ is a temperature constant. In the following context, we first give some facts about MEBR-MG, and then give the in-sample sequential policy optimization method under MEBR-MG.\nIn MEBR-MG, we have the following modified policy evaluation operator given by:\n$T_{\\pi}Q_{\\pi}(s, a) = r(s, a) + \\gamma E_{s'|s, a} [V_{\\pi}(s')]$, (10)\nwhere\n$V_{\\pi}(s) = E_{a \\sim \\pi} [Q_{\\pi}(s, a) - \\sum_{i \\in N} (\\frac{\\alpha}{\\mu^i(a^i|s)} log \\pi^i(a^i|s) + \\beta log \\pi^i(a^i|s))]$.\nLemma 2. Given a policy $\\pi$, consider the modified policy evaluation operator $T_{\\pi}$ in Eq.(10) and a initial Q-function $Q: S \\times A \\rightarrow R$, and define $Q_{k+1} = T_{\\pi}Q_k$. Then the sequence $Q_k$ will converge to the Q-function $Q_{\\pi}$ of policy $\\pi$ as $k \\rightarrow \\infty$.\nProposition 3. In a MEBR-MG, a joint policy $\\pi^{*}$ is a QRE if it holds\n$V_{\\pi^*}(s) \\geq V_{\\pi^{i'}, \\pi^{i}}(s), \\forall i \\in N, \\pi^{i}, s \\in S$.\nThen the QRE policies for each agent i are given by\n$\\pi^*(a^i|s) \\propto \\mu^i(a^i|s) \\cdot exp(\\frac{E_{a^{-i}} [\\pi^*(s, a^i, a^{-i})] - \\beta log \\mu^i(a^i|s)}{\\alpha + \\beta})$ (12)"}, {"title": "In-Sample Sequential Policy Optimization", "content": "$\\theta_{i_n}^{new} = arg \\min D_{KL}(\\pi_{i_n}^{new}(\\cdot|S), \\pi_{\\theta_{i_n}}^{old}(\\cdot|S))$\n$= arg \\min_{\\theta_{i_n}} E_{(s, a^{i_n}) \\sim D} [-exp(\\frac{\\Delta Q_{i_n}^{1:n}(s, a^{i_n})}{\\alpha}) \\cdot log \\pi_{\\theta_{i_n}}(a^{i_n}|s)],$ (7)\nwhere $\\Delta Q_{i_n}^{1:n}(s, a^{i_n}) \\equiv Q_{i_n^{1:n}}(s, a^{i_n}) - E_{i_n} [Q_{i_n^{1:n}}(s, a^{i_n})]$.\nA potential problem of this method is that it may lead to premature convergence to local optima due to exploitation of vested interests. This concern is especially pronounced when the behavior policy is a local optimum, as we will show in the next subsection.\n##### Algorithm", "sentences": ["Algorithm 1: InSPO \n Input: Offline dataset D, initial policy $\\pi_0$ and Q-function \n $Q_0$ \n Output: $\\pi_K$ \n 1: Compute behavior policy $\\mu$ by simple Behavior Cloning \n 2: for k = 1,..., K do \n 3: Compute $Q_k$ by Eq.(10) \n 4: Draw a permutation $\\pi_{1:N}$ of agents at random \n 5: for n = 1,..., N do \n 6: Update $\\pi_{i_n}$ by Eq.(13) \n 7: end for \n 8: end for", "Proposition 4. The sequential policy optimization procedure under MEBR-MG guarantees policy improvement, i.e., $\\forall s \\in S, a \\in A$,\n$Q_{\\pi^{new}}(s, a) \\geq Q_{\\pi^{old}}(s, a)$, $V_{\\pi^{new}}(S) \\geq V_{\\pi^{old}}(s)$.", "Proposition 5. Joint policy $\\pi$ updated by Algorithm 1 converges to QRE.", "In this section, we design a practical implementation of InSPO to handle the issue of large state-action space, making it more suitable for offline MARL. More details can be found in Appendix B.", "According to Eq.(10), we need to train a global Q-function to estimate the expected future return based on the current state and joint action.", "However, in MARL, the joint action space grows exponentially with the number of agents.", "To circumvent this exponential complexity, we instead maintain a local Q-function $Q_{\\phi^{i_n}}$ for each agent $i_n \\in N$ to approximate $Q_{\\phi_{i_n}^{1:i_n}}(s, a^{i_n})$.", "Besides, $Q_{\\phi^{i_n}}$ should be updated sequentially in conjunction with the policy in order to incorporate the information of updated teammates (i.e., $\\pi^{-i_n}$) into the local Q-function.", "Thus, we optimize the following objective for each local Q-function $\\phi_{i_n}$:\n$\\min_{\\phi^{i_n}} E_{(s, a, s', r) \\sim D} [(\\phi_{i_n}^{1:i_n}(s, a^{i_n}) - y)^2]$, (14)\nwhere $\\mu_{\\phi^{i_n}} = \\mu^{-i_n} (\\pi^{\\neg i_n})^{new} \\mu^{old}(a^{-i_n}|s)$\n$y = y(s, a, s', r) = r + E_{a^{i_n'} \\sim \\pi_{i_n}^{new}} [Q^{\\phi_{i_n}}(s', a^{i_n'})]$\\alpha D_{KL}(\\pi^{old}(\\cdot|s'), \\mu^{i_n} (\\cdot|s')) + \\beta H(\\pi^{old}(\\cdot|s'))$.\nHere we omit the regularization terms for other agents to simplify the computation. Furthermore, to reduce the high variance of importance sampling ratio $\\rho_{\\phi^{i_n}}$, InSPO adopts importance resampling (Schlegel et al. 2019) in practice, which resamples experience with probability proportional to $\\rho_{\\phi^{i_n}}$ to construct a resampled dataset $D_{\\phi^{i_n}}$, stabilizing the algorithm training effectively. Thus, Eq.(14) is replaced with\n$\\min_{\\phi^{i_n}} E_{(s, a, s', r) \\sim D_{\\phi^{i_n}}} [(\\phi_{i_n}^{1:i_n}(s, a^{i_n}) - y)^2]$.", "After obtaining the optimal local value functions, we can adopt the in-sample sequential policy optimization method in Eq.(13) to learn the local policy for each agent:\n$\\theta_{i_n}^{new} = exp[\narg \\min E_{(s, a^{i_n}) \\sim D_{\\phi^{i_n}}} { \\frac{\\Delta Q_{i_n}^{1:n}(s, a^{i_n}) - \\beta log \\mu^{i_n}(a^{i_n}|s)}{\\alpha + \\beta} }\\cdot  log \\pi_{\\theta_{i_n}}^{old}(a^{i_n} |s)]$,where $\\Delta Q_{i_n}^{1:n}(s, a^{i_n}) \\equiv Q_{\\phi_{i_n}^{1:n}}(s, a^{i_n}) - E_{\\mu_{i_n}} E_{\\mu^{a^{i_n}}} [Q_{\\phi_{i_n}^{1:n}}(s, a^{i_n})]$"]}, {"title": "Experiments", "content": "We conduct a series of experiments to evaluate InSPO on XOR game, M-NE game, Bridge (Fu et al. 2022) and StarCraft II Micromanagement (Xu et al. 2023b). In addition to Behavior Cloning (BC), our baselines also include the current state-of-the-art offline MARL algorithms: OMAR (Pan et al. 2022), CFCQL (Shao et al. 2023), OMIGA (Wang et al. 2023b) and AlberDICE (Matsunaga et al. 2023). Each algorithm is run for five random seeds, and we report the mean performance with standard deviation. For the final results, we indicate the algorithm with the best mean performance in bold, and an asterisk (*) denotes that the metric is not significantly different from the top-performing metric in that case, based on a heteroscedastic two-sided t-test with a 5% significance level. See Appendix C for experimental details.\nComparative Evaluation\nMatrix Game. We evaluate whether InSPO can address the two issues highlighted in previous section using the XOR game and M-NE game shown in Figure 1(a) and Figure 2(a). First, we evaluate the ability of InSPO to handle"}, {"title": "Ablation Study", "content": "Here we present the impact of different components on performance of InSPO. Figure 5(a) shows the converged policy of InSPO without entropy in the M-NE game on the imbalanced dataset. Without the perturbations of entropy in the optimization objective, InSPO w/o entropy cannot escape the local optimum. Figure 5(b) shows the policy of InSPO using the simultaneous update scheme instead of sequential policy optimization (denoted as InSPO w/o SPO) on dataset (b) of the XOR game. Due to conflicting update directions, InSPO w/o SPO fails to learn the optimal policy and faces the OOD joint actions issue.\nTemperature a is used to control the degree of conservatism. A too large a will result in an overly conservative policy, while a too small one will easily causes distribution shift. Thus, to obtain a suitable a, we implement both fixed and auto-tuned a in practice (see Appendix B for details), where the auto-tuned a is adjusted by $\\min_\\alpha E_D[|D_{KL}(\\pi^i, \\mu^i) - aD_{KL}|]$, where $D_{KL}$ is the target value. Table 5 gives ablation results for a, which shows that the auto-tuned a can find an appropriate a to further improve performance.\nFurthermore, we explore the impact of update order on performance and the training efficiency of sequential updates. These results are provided in Appendix C."}, {"title": "Conclusion", "content": "In this paper, we study the offline MARL problem, a topic of significant practical importance and challenges that has not received adequate attention. We begin with two simple yet highly illustrative matrix games, highlighting some limitations of current offline MARL algorithms in addressing OOD joint actions and sub-optimal convergence issues. To overcome these challenges, we propose a novel algorithm called InSPO, which utilizes sequential-update in-sample learning to avoid OOD joint actions, and introduces policy entropy to ensure comprehensive exploration of the dataset, thus avoiding the influence of local optimum behavior policies. Furthermore, we theoretically demonstrate that InSPO possesses monotonic improvement and QRE convergence properties, and then empirically validate its superior performance on various MARL benchmarks. For future research, integrating sequential-update in-sample learning and enhanced dataset utilization with other offline MARL algorithms presents an intriguing direction."}, {"title": "Details of Practical Algorithm", "content": "In the practical implementation of InSPO", "Q_{i_n}^{1": "n"}, "s, a^{i_n}) - \\beta log \\mu^{i_n}(a^{i_n} |s)}{\\alpha + \\beta}  log_{\\theta^{i_n}} \\pi(a^{i_n} |s)"], "Q_{i_n}^{1": "n"}, {"Q_{\\phi_{i_n}^{1": "n"}, {"E_{\\pi^{O_N}}[Q_{\\phi_{i_n}^{1": "n"}, {"a_{1": "i-1"}, {"agents": "n$\\pi^{\\neg i_n"}, {"values": "n$J(\\phi^{i_n}) = E_{(s, a, s', r) \\sim D_{\\phi^{i_n}"}, ["Q^{{\\phi_{i_n}}}(s, a^{i_n}) - y)^2"], {}]