{"title": "Joint MoE Scaling Laws:\nMixture of Experts Can Be Memory Efficient", "authors": ["Jan Ludziejewski", "Maciej Pi\u00f3ro", "Jakub Krajewski", "Maciej Stefaniak", "Micha\u0142 Krutul", "Jan Ma\u0142a\u015bnicki", "Marek Cygan", "Piotr Sankowski", "Kamil Adamczewski", "Piotr Mi\u0142o\u015b", "Sebastian Jaszczur"], "abstract": "Mixture of Experts (MoE) architectures have significantly increased computational efficiency in both research\nand real-world applications of large-scale machine learning models. However, their scalability and efficiency\nunder memory constraints remain relatively underexplored. In this work, we present joint scaling laws for dense\nand MoE models, incorporating key factors such as the number of active parameters, dataset size, and the number\nof experts. Our findings provide a principled framework for selecting the optimal MoE configuration under fixed\nmemory and compute budgets. Surprisingly, we show that MoE models can be more memory-efficient than dense\nmodels, contradicting conventional wisdom. To derive and validate the theoretical predictions of our scaling laws,\nwe conduct over 280 experiments with up to 2.7B active parameters and up to 5B total parameters. These results\noffer actionable insights for designing and deploying MoE models in practical large-scale training scenarios.", "sections": [{"title": "1. Introduction", "content": "Recently, language models have grown increasingly large, a trend accelerated by Mixture of Experts (MoE) techniques (Fe-\ndus et al., 2022; Du et al., 2022). MoE models are now widely adopted (Jiang et al., 2024; Dai et al., 2024) and are\ngenerally considered compute-efficient (Ludziejewski et al., 2024; Clark et al., 2022), though often considered memory-\ninefficient (Zadouri et al., 2023). However, the precise trade-offs between compute and memory efficiency have remained\nunclear so far.\nConsider a motivating question: Can an MoE model be the optimal choice when constrained by a fixed memory budget,\nsuch as a single H100 node? Increasing the number of experts has a relatively minimal impact on the cost in FLOPs but can\ndrastically increase memory requirements, often to prohibitive levels depending on the specific hardware and load.\nIn order to answer this question, we derive a joint scaling law for both dense and MoE models, accounting for key factors\nsuch as the number of active parameters, dataset size, and number of experts. This framework provides a rigorous analysis\nof model performance under strict memory constraints. Our findings reveal that, contrary to common assumptions, MoE\nmodels can be more memory-efficient than dense models\u2014that is, MoE models with the same loss and training budget can\nhave lower memory usage than dense models.\nOur work is the first to provide detailed guidance on selecting the optimal number of experts for MoE models, balancing\ncomputational budget and memory. Our conclusions are based on extensive large-scale experiments with over 280 models,\nscaled up to 2.7B active parameters and up to 5B total parameters."}, {"title": "2. Related Work", "content": "Mixture of Experts. Mixture of Experts (MoE) was introduced by Jacobs et al. (1991), who combined a gating network\nwith a set of expert networks. Shazeer et al. (2017) applied MoE to an LSTM-based model (Hochreiter & Schmidhuber,\n1997), scaling the architecture up to 137 billion parameters. In Transformer-based LLMs, MoE is most often applied as a\nreplacement for the feed-forward layer (Lepikhin et al., 2020; Shazeer et al., 2018). It replaces the feed-forward layer's MLP\nwith a set of expert MLPs along with a router, which selects one or more MLPs for each token. With the recent surge in\nLLM research, MoE models are gaining even more traction. This is exemplified by the development of extremely large-scale\nmodels such as DeepSeek-R1 and Qwen2.5-Max (DeepSeek-AI et al., 2025; Team, 2024a). In our work, we use the\nstandard Switch MoE layer (Fedus et al., 2022), which routes each token to one expert and encourages even token-to-expert\nassignment via the addition of a differentiable load-balancing loss."}, {"title": "3. Joint MoE Scaling Laws", "content": "We now derive the functional form of our joint scaling laws for both dense Transformers and MoE, relating the number of\nactive model parameters $N_{act}$, training tokens D, and MoE experts E.\nFixed Number of Experts. Following Hoffmann et al. (2022) and established practices in the literature (Frantar et al., 2023;\nKumar et al., 2024; Ludziejewski et al., 2024), we postulate the following form of the equation:\n$L(N_{act}, D, E) = m(E)N_{act}^{\\mu(E)}+n(E)D^{\\nu(E)} + c(E)$,\nassuming that if we fix the number of experts, the model's performance can be described using Equation 2. In the subsequent\npart, we will postulate how m, \u03bc, n, v, c depend on E, deriving the joint equation.\nConstant Factor. c(E) represents irreducible loss caused by the inherent entropy of the dataset. Thus, it does not depend on\nthe architecture (E in our case):\n$c(E) := c$.\nInteraction of E with Model and Dataset Size. To quantify the interaction between the number of experts and other\ntraining parameters, we gather observations from related work:\n1. Scaling the number of experts (E) can be described as a power law (Clark et al., 2022).\n2. For a fixed number of training tokens (D), as model size ($N_{act}$) increases, the benefit of using an MoE diminishes (Clark\net al., 2022).\n3. For a fixed model size ($N_{act}$), as the number of training tokens increases, the benefit of an MoE grows (Ludziejewski\net al., 2024).\nMotivated by Observation 1, we set\n$m(E) = aE^{\\omega}, n(E) = bE^{\\zeta}$,\nreflecting the power-law relation between E and the loss.\nAdditionally, to ensure flexibility in modeling Observations 2 and 3, we introduce an interaction with the exponents over\n$N_{act}$ and D:\n$\\mu(E) = \\alpha + \\gamma\\ln(E)$,\n$\\nu(E) = \\beta + \\zeta\\ln(E)$.\nNote that if we ignore the second and third terms in Equation 5, this yields a functional form identical to Equation 3.\nEmpirically, we observe a good fit for our formula, as described in Section 5. This shows that our proposed interactions\nbetween E, $N_{act}$, and D can accurately model the performance of MoE models.\nModeling of E. When the number of experts is small, a certain overhead caused, for example, by interference from\nauxiliary losses\u2014can overshadow the benefits of conditional computation. Additionally, employing a very large number\nof experts brings diminishing returns. To address these phenomena, we follow Clark et al. (2022) and use a transformation\nof the number of experts E as given in Equation 4.\nJoint MoE Scaling Law. By combining these observations, we establish the final form of our scaling law:\n$L(N_{act}, D, \\hat{E}) = a\\hat{E}^{\\omega} N_{act}^{\\alpha+\\gamma\\ln(\\hat{E})} + b\\hat{E}^{\\zeta} D^{\\beta+\\iota\\ln(\\hat{E})} + c$.\nWe fit the coefficients in Equation 6 based on the results of our experiments; see Table B. In Section 4, we present the\noutcomes and findings derived from the scaling laws. The details of the training runs, as well as the fitting procedure, are\ndescribed in Section 5."}, {"title": "4. Compute and Memory Optimality", "content": "In this section, we employ our scaling laws to offer recommendations for optimal configurations in different training\nand inference scenarios. Refer to Appendix A for details on counting FLOPs, the relationships between active and total\nparameters, and other technical aspects.\n4.1. Compute Optimality\nA model is considered compute-optimal if it\nachieves the lowest loss among models trained with\nthe same compute budget F. To find such an opti-\nmal configuration, we optimize the following:\n$arg min L(N_{act}, D, E)$\n$N_{act}, D, E$\n$s.t. 6N_{act}D = F$\nOptimal N and D Depend on the Number of Experts. Assuming a given number of experts E, the compute-optimal\ntraining configuration can be achieved by selecting the appropriate trade-off between training tokens and model size.\nIsoFLOP slices comparing the predicted loss with dataset size for selected compute budgets are plotted in Figure 2 (a).\nFor any fixed E, our scaling law has the Chinchilla functional form of Equation 2. Thus, from Hoffmann et al. (2022), the\ncompute-optimal number of tokens and active parameters for the budget F and the number of experts E are given by\n$N_{act}^{opt}(F) = G (\\frac{a}{b})^{\\frac{b}{\\mu(E)+\\nu(E)}}$,\n$D^{opt}(F) = G^{-1} (\\frac{a}{b})^{\\frac{a}{\\mu(E)+\\nu(E)}}$,\nwhere\n$G = (\\frac{\\mu(E) + \\nu(E)}{6} F)^{\\frac{1}{\\mu(E)+\\nu(E)}}$,\n$a = \\frac{\\nu(E)}{\\mu(E) + \\nu(E)}$,\n$b = \\frac{\\mu(E)}{\\mu(E) + \\nu(E)}$.\nWe compare the optimal configurations for several\ncompute budgets in Table 1.\nBoth from comparing the IsoFLOP slices (Figure 2)\nand the values listed in the table, we can see that the\ncompute-optimal configuration for a given compute\nbudget clearly depends on E, with MoE models\nrequiring comparatively larger datasets and corre-\nspondingly fewer active parameters.\nMixture of Experts is Compute Optimal. We\nnow compare the performance across various num-\nbers of experts, with the respective values of tokens\nand active parameters optimized. As illustrated\nin Figure 2, we observe significant compute sav-\nings for MoE models compared to dense models,\nwith a larger number of experts providing more\npronounced benefits.\nThe higher efficiency of MoE in terms of training\ncompute comes at the price of increased memory\nrequirements. However, somewhat surprisingly, we\nfind that MoE models can outperform dense models\nof the same size trained with the same amount of\ntraining compute\u2014a result we describe in more\ndetail in the next subsection."}, {"title": "4.2. Model Memory Optimality", "content": "Often, it is insufficient to consider models solely from the\nperspective of compute optimality, as a compute-optimal\nmodel can be impractically large, preventing its deploy-\nment on available hardware. Additionally, it may only\nbe possible to run a large model with a small batch size\ndue to limited GPU memory, leading to low hardware\nutilization (He, 2022). Therefore, it is natural to consider\na straightforward extension to the notion of compute optimality, specifically model memory optimality. A model is said\nto be memory optimal if, among models trained with the same compute budget F and having at most M parameters, it\nachieves the lowest loss:\n$arg min L(N_{act}, D, E)$\n$N_{act}, D, E$\n$s.t. 6N_{act} D = F, N_{total} \\leq M$\nNote that model memory-matched dense and MoE models differ in the number of active parameters-MoE uses just a\nfraction of them. Intuitively, it should thus have worse performance. However, given some budget, it can be trained on more\ntokens, lowering the loss. Our scaling laws suggest that MoE models can be model memory optimal. We validate this claim\nby training a 1.1B dense model and a model size and FLOP matched $E \\in {2, 4}$ counterparts (Figure 1). Significantly, the\nMoE models attain lower loss even if the dense model is overtrained (i.e., after passing its compute-optimal token count)."}, {"title": "4.3. Total Memory Optimality", "content": "During autoregressive generation, a decoder-only model\nprocesses a single token while storing activations (keys\nand values) for previous tokens in the KV cache. In\nthe case of multi-head attention, its size equals $2T \\times$\n$N_{blocks} \\times d_{model}$, where T is the number of tokens in the\ncache (possibly within multiple sequences in the batch).\nIncluding the cache size yields the optimization criterion:\n$arg min L(N_{act}, D, E)$\n$N_{act}, D, E$\n$s.t. 6N_{act} D = F, N_{total} + 2TN_{blocks} d_{model} \\leq M$\nFor practical values of T, a fair comparison of memory requirements should include the size of the KV cache in addition to\nthe model size. Figure 3 (b) presents the optimal models for a given compute and varying memory constraints when the\nsize of the KV cache is included. Importantly, MoE models compare more favorably to dense models in this graph, and as\nT increases, they outperform dense models at increasingly smaller model sizes. In Figure 1 (b), the $E = {2, 4}$ models\nemploy a smaller KV cache, which means that if memory is constrained, the MoE model can store longer contexts or work\nwith a larger batch size than the dense model."}, {"title": "4.4. Inference Optimality", "content": "Large models, while capable, may also be too costly to operate due to their high computational demands. To account for\nthis drawback, we can further assume that a model will process a number of tokens, $D_{inf}$, over its lifetime and find the best\nmodel whose demands do not exceed a predefined joint training and inference budget:\n$arg min L(N_{act}, D, E)$\n$N_{act}, D, E$\n$s.t. 6N_{act} D + 2N_{act} D_{inf} = F$.\nFigure 3 (c) presents the optimal models for a given compute and varying memory constraints if a joint budget needs to\naccommodate both training and inference demands. We find that, in this scenario, MoE models outperform dense models at\nsmaller scales than in simple compute optimality due to reduced inference FLOPs. The $E = 2$ and $E = 4$ models shown in\nFigure 1 use 36% and 61% less FLOPs per token, respectively, than their dense counterparts."}, {"title": "4.5. Summary", "content": "The concepts of inference optimality and total memory optimality can naturally be combined. Figure 3(c) presents a\ncomparison between different numbers of experts, where the KV cache is included in the model's memory requirements and\nthe compute budget is shared between training and inference. Finally, Figure 4 and Table 2 investigate the optimal E for a\nsample of model sizes, while including the KV cache and considering the inference cost.\nFor practitioners, as a simplification of our analysis, we\npropose a general rule of thumb:\nRule of Thumb. For a fixed total parameter count,\nan MoE model with E \u2264 8 experts outperforms a\ncompute-optimal dense model if trained on E times\nmore tokens while maintaining the same memory\nfootprint.\nFor instance, a compute-optimal 1.1B model trained for\n8B tokens will have worse loss than either a 2-expert,\n1.1B total parameters MoE model trained on 16B tokens\nor a 4-expert, 1.1B total parameters MoE model trained\non 32B tokens. At the same time the MoE models will\nrequire fewer FLOPs per token during inference.\nNote that in the scenario described by the rule of thumb, compute-matched MoE will generally have less than E-times\nlarger dataset and will still surpass dense model (as in Figure 1 (b)), but we wanted to keep this rule simple and conservative.\nFurthermore, while the rule may plausibly apply with E > 8, we prefer to conservatively limit it to E < 8 due to the\nuncertainty of predicting the loss of highly overtrained models (i.e., with a large token-to-parameter ratio). A detailed\ncomparison can be found in Figure 6, illustrating a stronger result where memory- and compute-matched MoE outperform\ncompute-optimal dense models across scales.\nIt is important to recognize that such scaling depends on access to large datasets a concern frequently raised in the\ncontext of scaling LLMs. While many leading organizations have demonstrated that data limitations can be overcome, the\navailability of large-scale datasets varies by organization and domain, particularly outside of NLP. Whether NLP datasets\nare effectively unlimited remains an open question beyond the scope of this work."}, {"title": "5. Fitting the Scaling Law", "content": "In this section, we present the details of the experiments and the procedure for fitting the scaling law parameters, see Table\nB in the Appendix. These results are based on extensive large-scale empirical evidence, including over 280 models with up\nto 5B parameters, trained on a variety of compute budgets. For a comprehensive list of experiments, see Appendix E.\n5.1. Model Hyperparameters\nThe selection of hyperparameters and training details is crucial for ensuring the robustness of scaling laws (Porian et al.,\n2025; Pearce & Song, 2024). In our work, we employ a set of best practices and modern design choices, aiming to provide\naccurate predictions applicable to real-life practice.\nAll models used in this study are decoder-only Transformers trained on the highly filtered FineWeb-Edu (Penedo et al.,\n2024). We use a Transformer model with Switch (Fedus et al., 2022) layers, using standard values of router z-loss of\n0.001 and load balancing loss of 0.01. The GPT-2 tokenizer (Radford et al., 2018) is employed. For better stability, weight\ninitialization follows a truncated normal distribution with a reduced scale of 0.1, as suggested by (Fedus et al., 2022). Mixed\nprecision training is used, with the attention mechanism, RoPE position embeddings (Su et al., 2023) and router always\nmaintained at high precision. The models use the SwiGLU activation (Shazeer, 2020) with hidden size equal to 3dmodel and\nactivate one expert per token (unless the token is dropped due to limited capacity). For evaluation, we increase the capacity\nfactor to ensure dropless processing of the tokens.\n5.1.1. BATCH SIZE RAMP-UP\nPerformance of a deep learning optimization procedure can suffer as a result of using an exceedingly large batch size (Mc-\nCandlish et al., 2018). To mitigate this potential issue, especially early in the training, we employ batch-size ramp-up.\nSimilar strategies are used in contemporary LLM training runs (Rae et al., 2022; Dubey et al., 2024). We increase the batch\nsize from 64K to 128K after 0.5B training tokens and further to 256K after 1B training tokens. Instead of utilizing noise\nscale as a critical batch size predictor (McCandlish et al., 2018) we opted for a straightforward grid to directly predict a\ntransition point beyond which increasing batch size does not impair performance."}, {"title": "5.1.2. LEARNING RATE SCALING", "content": "Kaplan et al. (2020) have shown that scaling laws for hyperparameters can be used to adjust them according to the size of\nthe model in the case of dense Transformers. For MoE models, we find the literature inconclusive-while some (Dai et al.,\n2024) pretrain MoEs with a lower learning rate than corresponding dense models, others (Zoph et al., 2022) report better\nperformance when fine-tuning MoEs with higher learning rates. To address this discrepancy, we derive a scaling law for the\npeak learning rate for MoE based on the number of active non-embedding parameters $N_{act\\e}$ and the number of experts E:\n$LR(N_{act\\e}, E) = exp(8.39 \u2013 0.81ln(N_{act\\e}) \u2013 0.25 ln(E))$,\nand use this equation to set the learning rate in our main\nscaling laws experiments. We fit the coefficients of the\nequation using the least squares method, minimizing the\nerror between the prediction and the optimal learning rate\nfrom the experiment grid. In contrast to Kaplan et al.\n(2020), we use a linear transformation of the parameter\ncount to predict the logarithm of the learning rate, instead\nof directly predicting the learning rate. This approach allows us to avoid the breakdown of the formula above 1010 parameters,\nas mentioned in their work, where the predicted learning rate becomes negative. This phenomenon is independent of the\nactual fit and is simply a property of the formula used. Besides being well-defined in the extrapolation, we argue that optimal\nlearning rates visibly follow this logarithmic trend, as seen in Figure 7 in the Appendix.\nThe second difference between our formula and the one by Kaplan et al. (2020) is the incorporation of the number of\nexperts, allowing us to model the optimal behavior of this hyperparameter across dense models and different MoEs. This\nis an important detail that allows unbiased comparison among different models and ensures each one is optimally tuned.\nFurthermore, it allows us to answer the question of whether MoE should be trained with a lower or higher learning rate.\nWhile our formula accommodates both scenarios, we can clearly see in Figure 7 in the Appendix that increasing E requires\nlower learning rates, resulting in a negative value for the coefficient. Moreover, we verify this thesis by tuning the fit on\nE = 1 and E = 8, and validating it on interpolation at E = 4 and extrapolation at E = 32. In both instances, the validation\npredicts the optimal learning rate for the model configuration or a value with nearly the same performance.\nIn Figure 8 in the Appendix, we perform an ablation of this additional power law on E by repeating our entire fitting\nprocedure without the E component. This shows, especially with extrapolation on E = 32, that dependence on E is crucial,\nand its omission can impair the performance of MoEs. Further details about our scaling rule for learning rates can be found\nin the plots in Appendix D."}, {"title": "5.1.3. LEARNING RATE SCHEDULE", "content": "H\u00e4gele et al. (2024) suggests that a trapezoidal learning rate schedule can yield similar performance to other established\nmethods, such as the cosine schedule. Additionally, it provides a valuable advantage when varying training duration,\nas intermediate checkpoints can be reused. With a cosine schedule, intermediate checkpoints introduce bias into the fit,\naccording to the analysis of Kaplan et al. (2020) by Hoffmann et al. (2022). We employ a constant learning rate schedule\nwith a linear warmup over the initial 130M tokens and with a linear decay from the peak learning rate to 0 over the final\n20% of tokens. For each model size, longer runs reuse intermediate checkpoints from the shorter ones."}, {"title": "5.2. Optimization of Formula Coefficients", "content": "Following Hoffmann et al. (2022), we use the LBFGS algorithm to optimize the coefficients of Equation 6. See Appendix B\nfor details. We observe a good fit with RMSE = 0.0039 on a held-out set of our 30 runs with the lowest loss, and\nRMSEt = 0.0062 on the training dataset. To further verify our formula, we train separate Chinchilla scaling laws\n(Equation 2) for different E using the same hyperparameters and the corresponding subset of the initializations grid. This\napproach serves as a lower bound for the loss of our joint formula on the training dataset, as it can emulate its coefficients;\nhowever, it is more prone to overfitting because effectively more parameters are utilized. Using this approach, we obtain a\nlower error on the training dataset of RMSEP = 0.0059 and marginally higher on the validation RMSEP = 0.0041. We\nbelieve this is a strong confirmation that our joint formula is actually describing how variable E influences training.\nIn Figure 5, we visually verify the extrapolation of the joint fit. Prediction errors are categorized by different numbers of\nexperts, highlighting that our joint formula is not biased for any specific E."}, {"title": "6. Limitations and Future Work", "content": "In our work, we focus on the standard MoE variant, where the size of each expert matches the size of the feed-forward layer\nin the corresponding dense model. Some recent findings (Dai et al., 2024; Ludziejewski et al., 2024; Muennighoff et al.,\n2024; Team, 2024b) suggest that fine-grained MoE models are more efficient and may likely enhance the benefits we report\nfor using MoE. Similarly, adopting a dropless MoE (Gale et al., 2022) approach, instead of relying on a capacity factor,\ncould lead to further improvements. We leave the integration of these MoE improvements for future work.\nMoreover, our Chinchilla-based optimality analysis utilizes FLOPs, which may not accurately reflect the wall-clock training\ntime of models with different architectures. Although comparing models based on the total number of parameters, rather\nthan active parameters, partially alleviates this issue due to the same memory bottleneck, different implementations and\ndistributed training algorithms are not considered in this work.\nWe assumed the Chinchilla scaling law (Equation 2) as the basis for our formulas. While this is well-grounded in the\nliterature, this formula is known to have limitations, particularly for extreme token-to-parameter ratios. Similarly, we\nobserved a regression in the goodness of fit for some heavily undertrained or overtrained runs."}, {"title": "7. Conclusions", "content": "In this work, we derived the joint scaling laws for Mixture of Experts, relating the loss of the model to the number of\nparameters, the number of training tokens, and the number of experts. By considering both compute and memory constraints,\nas well as the expected inference workload, we demonstrated that MoE models can outperform dense models even when\nconstrained by memory usage or total parameters, contrary to common assumptions and intuitions that MoE models are\nmore memory-intensive than dense models.\nOur analysis reveals how optimal training strategies shift as the number of experts varies. This provides a principled\nframework for selecting MoE hyperparameters under given constraints, highlighting the trade-offs between memory and\ncompute performance."}, {"title": "A. Technical Details", "content": "There are several ways to measure the size of a model. The two most important distinctions are whether total or active\nparameters are counted, and whether the parameters in the embedding and unembedding layers are included. Various papers\nassume different notations; notably, Kaplan et al. (2020) use nonembedding parameters, while Hoffmann et al. (2022)\nopt for the parameter count including embedding and unembedding. Throughout our work, we try to make it clear which\nway of counting we are using in each particular instance. When no additional information is given, Nact and Ntotal denote\nrespectively active and total parameters, including the embedding and unembedding.\nIf we let dmodel be the hidden dimension of the model and dyocab be the vocabulary size (50, 257 in our case), then the\nfollowing relations hold:\n$N_{total} = 2d_{model}d_{vocab} + (4+ 9E)N_{blocks} d_{model}$\n$N_{act} = 2d_{mode1}d_{vocab} + 13N_{blocks}d_{model}"}, {"title": "A.2. Counting FLOPS", "content": "Based on Sardana et al. (2024), we assume the cost of training to be $F_{training} = 6N_{act}D_{training}$, and the cost of inference to be\n$F_{inference} = 2N_{act} D_{inference}$. Due to the relatively small number (\u2264 32) of experts used with implicit expert granularity of 1.0\n(Ludziejewski et al., 2024), we can consider the memory and FLOPs cost of routing to be negligible, following Clark et al.\n(2022)."}, {"title": "A.3. Model Configs", "content": "The vast majority of our experiments use a simple rule for scaling the configuration, i.e., $N_{blocks} = N_{heads} = d_{model}/64$ and\nassume these relations hold in all calculations. We base this rule on findings by Kaplan et al. (2020)."}, {"title": "B. Fit Details", "content": "a\n\u03b1\n\u03b4\nb\n\u03b2\n3\n\u03c2\nEstart\nEmax\nC\n35.91\n-0.1889\n-0.2285\n0.0098\n35.98\n-0.1775\n0.5529\n-0.0259\n2.0732\n290.4521\n1.3637\nFollowing Hoffmann et al. (2022), we use the LBFGS algorithm with a learning rate of 1e-4 and a weight decay of\n1e-5 to fit the coefficients of Equation 6, optimizing the Huber loss with \u03b4 = 0.01 over the set of our training runs\ndescribed in the table in Appendix E. Instead of removing outliers and underperforming models from the training set,\nwe underweight them proportionally to the loss. Optimization hyperparameters were manually tuned to minimize error\nover the training dataset. The final fitted coefficients of Equation 6 are within the boundaries of the grid of initializations\ngiven by: \u03b1 \u2208 {0.05, 0.25, 0.5}, \u03b2 \u2208 {0.05, 0.25,0.5}, \u0410 \u2208 {30,100,300}, \u0412 \u2208 {30,100,300}, C\u2208 {0.5,1,2},\n\u03b4\u2208 {-0.5,0,0.5}, \u03b3\u2208 {-0.5,0,0.5}, \u03c9 \u2208 {-0.5,0,0.5}, \u03b6\u2208 {-0.5,0,0.5}. The selected coefficients were those with\nthe lowest score, defined as the sum of RMSE on the training and a held-out extrapolation validation set. The formula in\nEquation 6 was calculated in logarithm, without any exponentials, using only linear transformations and the logsumexp\noperation. It was optimized to predict the logarithm of L, and parameters a, b, and c were optimized in logarithm. All these\nsteps were taken to increase numerical stability and were essential for proper convergence."}, {"title": "C. Compute- & Memory-Matched Models", "content": null}, {"title": "D. Learning Rate Scaling Fit", "content": null}]}