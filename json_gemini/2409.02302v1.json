{"title": "SPEECH FOUNDATION MODEL ENSEMBLES FOR THE CONTROLLED SINGING VOICE\nDEEPFAKE DETECTION (CTRSVDD) CHALLENGE 2024", "authors": ["Anmol Guragain", "Tianchi Liu", "Zihan Pan", "Hardik B. Sailor", "Qiongqiong Wang"], "abstract": "This work details our approach to achieving a leading system\nwith a 1.79% pooled equal error rate (EER) on the evalua-\ntion set of the Controlled Singing Voice Deepfake Detection\n(CtrSVDD). The rapid advancement of generative AI mod-\nels presents significant challenges for detecting AI-generated\ndeepfake singing voices, attracting increased research atten-\ntion. The Singing Voice Deepfake Detection (SVDD) Chal-\nlenge 2024 aims to address this complex task. In this work,\nwe explore the ensemble methods, utilizing speech founda-\ntion models to develop robust singing voice anti-spoofing sys-\ntems. We also introduce a novel Squeeze-and-Excitation Ag-\ngregation (SEA) method, which efficiently and effectively in-\ntegrates representation features from the speech foundation\nmodels, surpassing the performance of our other individual\nsystems. Evaluation results confirm the efficacy of our ap-\nproach in detecting deepfake singing voices. The codes can\nbe accessed at https://github.com/Anmol2059/SVDD2024.", "sections": [{"title": "1. INTRODUCTION", "content": "With the rapid development of generative AI technology, the\nquality of audio synthesis has significantly improved, mak-\ning it increasingly difficult to distinguish between bona fide\nand spoofed audio. However, this progress also poses signif-\nicant risks to human voice biometrics and can deceive both\nautomatic speaker verification systems and their users [1].\nAdditionally, the proliferation of spoofed speech presents a\nserious threat to cybersecurity, as it can be used to manipu-\nlate information, conduct fraud, and bypass security measures\nthat rely on voice authentication. Finding effective ways to\ndetect spoofing attacks and protect users from the threat of\nspoofed speech is becoming increasingly important. There-\nfore, speech anti-spoofing, also known as speech deepfake\ndetection, has emerged [2-6]. It is dedicated to developing\nreliable automatic spoofing countermeasures (CMs), which is\nof utmost importance to society and the ethical applications\nof generative models.\nUnlike speech spoofing, creating deepfakes of singing\nvoices introduces distinct challenges. This complexity arises\nfrom the inherently musical aspects of singing, such as vary-\ning pitch, tempo, and emotion, as well as the frequent pres-\nence of loud and intricate background music [7, 8]. These\nfactors make it more difficult to detect deepfakes in singing\ncompared to regular speech, which typically features a more\nconsistent and predictable sound pattern. Recently, the speech\nanti-spoofing research community has been increasingly fo-\ncusing on this challenging issue, resulting in the development\nof related datasets [7\u20139], challenges [10], and models [11].\nThe Singing Voice Deepfake Detection (SVDD) Challenge\n2024 aims to address these challenges by fostering the devel-\nopment of robust detection systems [10, 12].\nSpeech foundation models are large, pre-trained models\ndesigned to serve as the backbone for various speech-related\ntasks, including speaker verification, speech recognition, and\nmore [13-15]. Many of these models rely on self-supervised\nlearning (SSL) to develop robust speech representations, such\nas WavLM [16] and wav2vec2 [17]. These models excel in\nlearning high-quality representations that can be fine-tuned\nfor specific downstream tasks. Recently, many studies on\nspeech anti-spoofing have adopted this approach and achieved\nstate-of-the-art performance [18-23]. The progress of these\nstudies and their promising performance motivate us to con-\ntinue exploring along this particular line.\nThis work details our participation in the CtrSVDD track\nof the SVDD Challenge 2024. We detect singing voice\ndeepfakes by ensembling models developed using speech\nfoundation models, data augmentation techniques, and var-\nious layer aggregation methods. Specifically, the default\nWeighted Sum aggregation method fixes weights after train-\ning, limiting adaptability to new data. The recently proposed\nAttentive Merging (AttM) method [24], while powerful, can\nlead to overfitting on small datasets. To address these issues,\ninspired by Squeeze-and-Excitation Networks (SENet) [25],\nwe propose the SE Aggregation (SEA) method. This method\ndynamically assigns weights and mitigates overfitting issues,\nenabling our best individual model to achieve an EER of\n2.70% on the CtrSVDD evaluation set. Further investiga-\ntions show that ensembling systems enhances robustness and\nperformance, achieving our best result of 1.79% EER."}, {"title": "2. METHODOLOGY", "content": "2.1. Data Augmentation\nWe employ the RawBoost augmentation [26], which intro-\nduces various types of noise to the audio data to simulate real-\nworld acoustic variations. These augmentation types include:\n\u2022 (1) Linear and non-linear convolutive noise (LnLcon-\nvolutive noise). This involves applying a convolutive\ndistortion to the feature set by filtering the input sig-\nnal with notch filter coefficients, iterating $N_f$ times,,\nand raising the signal to higher powers to simulate real-\nworld distortions.\n\u2022 (2) Impulsive signal-dependent noise (ISD additive\nnoise). This is introduced by adding noise to a random\npercentage of the signal points, scaled by the original\nsignal's amplitude.\n\u2022 (3) Stationary signal independent noise (SSI additive\nnoise). This represents stationary signal-independent\nnoise, which is added uniformly across the signal.\n2.1.1. Parallel Noise Addition\nWe adopt a parallel noise addition strategy to independently\nincorporate multiple noise characteristics. We process the in-\nput feature through both LnL Convolutive Noise and ISD Ad-\nditive Noise algorithms simultaneously, resulting in two sep-\narate noisy signals. These signals are then combined by sum-\nming and normalizing to maintain consistent amplitude lev-\nels. This parallel approach allows each noise type to influence\nthe signal independently, effectively capturing the combined\neffects of convolutive and impulsive noise, and providing a\nrobust simulation of complex noise conditions. This method\nis referred to as the \u2018parallel: (1)+(2)' approach described in\nRawBoost [26].\n2.1.2. Sequential Noise Addition\nWe use a sequential noise addition process to enhance the\nrobustness of our features, incorporating the aforementioned\nthree types of noise. This sequential approach ensures com-\nprehensive noise simulation and results in various combina-\ntions such as 'series: (1)+(2)', 'series: (1)+(3)', and 'series:\n(2)+(3)', following those in RawBoost [26].\n2.2. Individual Models Description\n2.2.1. Frontend\nIn this subsection, we provide a detailed overview of the fron-\ntends used in our individual models, emphasizing their ability\nto efficiently process raw audio data.\nRaw waveform. Following the baseline system described\nin the SVDD challenge 2024 [10], we employ RawNet2 [27]-\nstyle learnable SincConv layers with 70 filters as the fron-\ntend. These SincConv layers are designed to effectively c\u0430\u0440-\nture essential features from raw audio signals, enhancing the\nmodel's ability to process and analyze audio data for subse-\nquent tasks.\nwav2vec2. The wav2vec2 model offers significant advan-\ntages in effectively capturing a wide range of audio features\ndirectly from raw audio inputs [17]. This model excels in ex-\ntracting detailed and nuanced information from audio data,\nwhich can then be utilized for various downstream tasks such\nas speaker verification, speech recognition, and speech anti-\nspoofing. By processing the raw audio waveforms without\nrequiring extensive pre-processing, wav2vec2 enhances the\nability to perform complex audio-related tasks with improved\naccuracy and efficiency. This direct approach not only simpli-\nfies the workflow but also improves the overall performance\nof the subsequent processing and classification tasks [28].\nWavLM. The WavLM [16] is a large-scale pre-trained\nspeech foundation model for addressing the multifaceted na-\nture of speech signals, including speaker identity, paralin-\nguistics, and spoken content. Its robust performance on the\nSUPERB benchmark [29] underscores its potential versatility\nacross diverse speech processing applications. Given its ad-\nvanced capabilities in modeling and understanding complex\nspeech patterns, WavLM holds promise for use in special-\nized area of singing voice deepfake detection. The model's\nability to capture intricate vocal nuances and sequence order-\ning could be instrumental in identifying synthetic patterns in\nsinging voices, thereby contributing to the SVDD task.\n2.2.2. Layer Aggregation Strategy\nThe layer aggregation strategy in speech foundation models\nrefers to the technique of combining information from mul-\ntiple layers to enhance the model's performance in speech-\nrelated downstream tasks like speaker verification, emotion\nrecognition, and anti-spoofing. Each layer in a speech foun-\ndation model captures distinct aspects and features of the\ninput waveform. By aggregating these layers, the model can\nleverage a richer set of features, combining low-level acoustic\ninformation from early layers with higher-level semantic and\ncontextual information from later layers. This process typ-\nically involves techniques such as concatenation, weighted\nsum, or attention mechanisms to effectively aggregate the\nmulti-layer representations [30]. These learned weights allow\nthe model to emphasize more relevant features and reduce\nnoise or less important information. In this work, we explore\nweighted sum and attentive merging (AttM) [24]. Inspired by\nSE [25], we propose SE Aggregation. These three methods\nare illustrated in Fig. 1, and the details are as follows:\nWeighted Sum. The weighted sum method combines\noutputs from multiple neural network layers using adjustable"}, {"title": "2.2.3. Backend", "content": "The audio anti-spoofing using integrated spectro-temporal\ngraph attention networks (AASIST) functions as the model,\nleveraging graph-based attention mechanisms to capture\nspectral and temporal audio features [5]. It includes sev-\neral key components [5]:\n\u2022 The Graph Attention Layer (GAT) computes attention\nmaps between nodes and projects them using attention\nmechanisms. This layer consists of linear layers, batch\nnormalization, dropout, and Scaled Exponential Linear\nUnit (SELU) activation. Separate GAT layers are used\nfor spectral and temporal features.\n\u2022 The Heterogeneous Graph Attention Layer (HtrgGAT)\nprocesses both spectral and temporal feature nodes. It\nprojects each type of node, generates attention maps,\nand updates a master node that represents the aggre-\ngated features. Sequential layers are used to refine\nthese features further.\n\u2022 The graph pooling layer reduces the number of nodes\nby selecting the top-k nodes based on attention scores.\nThis process uses sigmoid activation and linear projec-\ntion to compute the scores, with separate pooling layers\nfor spectral and temporal features.\n\u2022 The residual blocks apply convolutional layers, batch\nnormalization, and SELU activation, similar to ResNet\nblocks, within the encoder to process input features.\n\u2022 The attention mechanism derives spectral and temporal\nfeatures from the encoded features, incorporating con-\nvolutional layers and SELU activation.\n2.2.4. Classifier\nThe classifier outputs the final predictions by utilizing the\nrefined features extracted from the backend model, subse-\nquently performing the classification task. In this work, the\ninput comprises a concatenation of maximum and average\ntemporal features, maximum and average spectral features,\nand master node features from the ASSIST backend. To en-\nhance generalization, dropout is applied to this concatenated\nfeature vector. The output is generated through a linear layer,\nwhich produces logits, representing the raw scores."}, {"title": "2.3. Model Ensembling", "content": "Model ensembling is a strategy where multiple models are\ncombined to improve the overall performance and robustness\nof predictions. The rationale behind this approach is that dif-\nferent models may capture various aspects of the data, and\ncombining them can result in better generalization on unseen\ndata. This method is widely adopted in many works in the\nanti-spoofing task [37,38]. In this work, we ensemble the\nindividual models by averaging their output scores."}, {"title": "3. EXPERIMENTAL SETUP", "content": "3.1. Data Set\nWe utilized the official training and development datasets\nprovided for the CtrSVDD track, available at Zenodo\u00b9. Ad-\nditionally, we incorporated other public datasets including\nJVS [39], Kiritan [40], Ofutan-P2, and Oniku\u00b3 following the\nguidelines and scripts provided by the challenge organiz-\ners [8]. The combined dataset included a diverse range of\nsinging voice recordings, both authentic and deepfake, seg-\nmented and processed to ensure consistency in training and\nevaluation. The details of the dataset partitions, along with\nthe evaluation set statistics from [8], are provided in Table 1.\n3.2. Training Strategy\nWe use the equal error rate (EER) as the evaluation metric. To\nensure reproducibility, we consistently apply a fixed random\nseed of 42 across all systems. Our training process employs\nthe AdamW optimizer with a batch size of 48, an initial learn-\ning rate of $1 \\times 10^{-6}$, and a weight decay of $1 \\times 10^{-4}$. The\nlearning rate is scheduled using cosine annealing with a cy-\ncle to a minimum of $1 \\times 10^{-9}$. For the loss function, we\nutilize a binary focal loss, a generalized form of binary cross-\nentropy loss, with a focusing parameter ($\\gamma$) of 2 and a posi-\ntive example weight ($\\alpha$) of 0.25. To standardize input length,\neach sample is randomly cropped or padded to 4 seconds dur-\ning the training. Our model is trained for 30 epochs, and the\nmodel checkpoint with the lowest EER on the validation set\nis selected for evaluation. All experiments are performed on\na single NVIDIA A100 GPU.\nFor certain experiments marked in Table 2, we employ the\nRawboost data augmentation strategy as introduced in Sec-\ntion 2.1. The RawBoost augmentation is sourced from the\nofficial implementation and follows the default settings [41].\nOur utilization of wav2vec2 also references this implementa-\ntion. The wav2vec2 [17] model used in this work is the cross-\nlingual speech representations (XLSR) model. The imple-\nmentation of WavLM is derived from S3PRL7."}, {"title": "4. RESULTS", "content": "4.1. Baselines\nThe organizers of the CtrSVDD Challenge 2024 provide\ntwo baseline systems, referred to as B01 and B02 in Ta-\nble 2 [8, 10]. B01, based on linear frequency cepstral coef-\nficients (LFCCs), achieved a pooled EER of 11.37%, while\nB02, based on raw waveform, achieved a pooled EER of\n10.39%. We re-implement B02 and obtain an improved\nperformance of 9.45%, slightly better than the official imple-\nmentation.\n4.2. Frontend\nAs indicated in Table 2, when comparing wav2vec2-based\nmodels to WavLM-based models with the same type of aug-\nmentation (M2 vs. M4 for RawBoost 'series: (1)+(2)', and\nM3 vs. M5 for 'parallel: (1)+(2)'), we observe that the\nWavLM-based models consistently perform better. There-\nfore, in this work, we focus more on experimenting with\nWavLM-based models."}, {"title": "4.3. Data Augmentation", "content": "By comparing the wav2vec2-based models trained with and\nwithout 'parallel: (1)+(2)' RawBoost augmentation [26] (M1\nvs. M3), we observe a significant improvement in perfor-\nmance when the augmentation is applied. Further analysis\nbased on various models and layer aggregation techniques\nreveals that the 'parallel: (1)+(2)' configuration consistently\nprovides better results compared to the 'series: (1)+(2)' con-\nfiguration (M2 vs. M3, M4 vs. M5, M6 vs. M7, M8 vs.\nM9), with an average relative performance improvement of\n26.7%. On the other hand, our experiments show that using\ntype (3) of RawBoost (SSI additive noise) [26] does not yield\nmore benefits (M11 and M12). Overall, RawBoost generally\nenhances system performance on the CtrSVDD dataset. No-\ntably, benefiting from 'parallel: (1)+(2)', the WavLM-based\nmodel with our proposed SEA (M9) achieves the best individ-\nual performance on the evaluation set, as shown in Table 2."}, {"title": "4.4. Layer Aggregation Strategies", "content": "As shown in Table 2, when comparing different layer aggre-\ngation methods, we observe that the AttM strategy performs\nsimilarly to the weighted sum method in terms of pooled\nEER. Additionally, the AttM model (M7) achieves the best\nperformance in the most sub-trials. In this work, we simply\nutilize all WavLM layers, while the strength of AttM method\nlies in using fewer encoder layers. This not only lowers in-\nference costs but also boosts performance [24]. This aspect is\nvaluable for exploring in the SVDD task.\nGiven that the weighted sum method lacks a cross-layer\nattention mechanism, which may limit the representation fea-\ntures extracted by the speech foundation model in complex\nmusical scenarios, and that AttM's higher number of train-\ning parameters could lead to overfitting on small datasets, we\npropose the SEA method. Our proposed SEA aggregation\nmethod, based on the WavLM model, consistently outper-\nforms both the Weighted Sum and AttM across different Raw-\nBoost augmentation scenarios, achieving an average relative\nreduction in EER by 16.7% and 19.1%, respectively. With\nthis proposed SEA, we achieve the best individual model per-\nformance of 2.70%, validating its superior performance and\nsuitability for the task of singing voice deepfake detection."}, {"title": "4.5. Model Ensembling", "content": "We explore ensembling models to enhance robustness and\nperformance. The ensembled models and their corresponding\nevaluation EER are shown in Table 3. Specifically, we ex-\nplore the model ensembling strategy by initially ensembling\nthe top 5 individual models based on their performance on\nA09-A14 pooled EER. The El system, composed of M5, M7,\nM8, M9, and M10, achieves a 2.50% EER, outperforming all\nindividual systems. Further investigation includes incorpo-"}, {"title": "5. CONCLUSION", "content": "In this work, we present ensembled systems utilizing speech\nfoundation models, demonstrating significant promise in the\ntask of singing voice deepfake detection (SVDD). Our novel\nlayer aggregation strategy, SE Aggregation (SEA), enables\nthe WavLM-based model to achieve the best performance\nwith a 2.70% EER on the CtrSVDD evaluation set, out-\nperforming all individual models. By implementing data\naugmentation techniques, such as RawBoost, our ensembled\nsystem further achieves a remarkable 1.79% pooled EER\non the CtrSVDD evaluation set. Further analysis validates\nthat model ensembling effectively combines the strengths of\ndifferent models, enhancing both robustness and accuracy.\nThese findings contribute to advancing the field of audio\nanti-spoofing, particularly in SVDD. Future work can ex-\nplore further optimization of layer aggregation techniques\nand broader applications to improve detection systems."}]}