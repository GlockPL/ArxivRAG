{"title": "DISCOVERYBENCH: Towards Data-Driven Discovery with Large Language Models", "authors": ["Bodhisattwa Prasad Majumder", "Harshit Surana", "Dhruv Agarwal", "Bhavana Dalvi Mishra", "Abhijeetsingh Meena", "Aryan Prakhar", "Tirth Vora", "Tushar Khot", "Ashish Sabharwal", "Peter Clark"], "abstract": "Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DISCOVERYBENCH, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations across task complexity. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DISCOVERYBENCH and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.", "sections": [{"title": "1 Introduction", "content": "Knowledge discovery via the scientific process has been a catalyst for human progress for centuries but has, thus far, been a predominantly manual pursuit [16]. Recent breakthroughs in capabilities of large language models (LLMs) to reason and interface with the world using code [9, 40], external tools [41], and interactive agents [51, 32], however, now suggest the possibility of realizing a discovery system that is fully autonomous. Indeed, recent works [33] provide initial evidence for this paradigm within the setting of data-driven discovery, where both search and verification of hypotheses may be carried out using a dataset alone (i.e., after physical experiments and data collection\u00b9), but the extent of this ability remains unclear. We, therefore, aim to systematically evaluate the following question:\nHow good are current state-of-the-art LLMs at automated data-driven discovery?\nIn practice, experiments and analysis are interleaved, not sequential. Our concern in this work, however, is systematically studying the data analysis part of the (interleaved) pipeline."}, {"title": "2 Related Work", "content": "Automated data-driven discovery has been a long-standing dream of AI [33, 21]. Although there have been a range of data-driven discovery systems, from early ones that fit equations to idealized data, e.g., Bacon [23], to more modern ones handling complex real-world problems, e.g., AlphaFold [19], their associated datasets are task-specific and customized to a pre-built pipeline. In contrast, DISCOVERYBENCH aims to be a general test over multiple tasks, including testing whether systems can design appropriate pipelines themselves.\nA number of datasets and tools are available for AutoML, a related technology aimed at automating workflows for building optimal machine learning models [18, 55, 24]. AutoML tools include packages like Scikit [13], and embedded in cloud platforms such as Google Cloud Platform, Microsoft Azure,"}, {"title": "3 Formalization", "content": "We begin by formalizing what we mean by a data-driven hypothesis and how the structure of a complex hypothesis may be viewed as a hypothesis semantic tree.\nA data-driven hypothesis h in H (the space of such hypotheses) is a declarative sentence about the state of the world whose truth value may be inferred from a given dataset D using a verification procedure $V_D$: H \u2192 {supported, unsupported}, for instance, via statistical modeling.\nEach hypothesis may further be expressed using a propositional formula $\\phi$ over a set of sub-hypotheses $h_i \\in H$ using logical connectives, e.g., disjunctions and conjunctions, such that h := $\\phi(h_1,...,h_n)$ and $V_D(h) = \\phi(V_D(h_1),...,V_D(h_n))$. For instance, suppose h is the hypothesis \"for men younger than 20, popularity of product A varies proportional to their age ($h_1$), while there exists an inverse relationship for those older than 40 ($h_2$)\", then h can be expressed as the conjunction $h_1 \\land h_2$.\nInspired by recent work of Thompson and Skau [47], we additionally introduce a structured formalism that breaks a hypothesis down into three hypothesis dimensions:\n\u2022 Contexts (c): Boundary conditions that limit the scope of a hypothesis. E.g., \u201cfor men over the age of 30\u201d or \u201cin Asia and Europe\u201d or unbounded/full dataset when not specified.\n\u2022 Variables (v): Known set of concepts that interact in a meaningful way under a given context to produce the hypothesis. E.g., gender, age, or income. Note that each hypothesis is associated with a target variable and a set of independent variables.\n\u2022 Relationships (r): Interactions between a given set of variables under a given context that produces the hypothesis. E.g., \"quadratic relationship\u201d, \u201cinversely proportional\", or piecewise conditionals.\nWith slight abuse of notation, we can now equivalently define hypothesis h := $\\psi(c, v, r)$, where $\\psi(c, v, r)$ returns the declarative sentence \u201cunder context c, variables v have relationship r.\u201d For instance, for sub-hypothesis $h_1$ in our example above, $c_1$ := \"men younger than 20\", $v_1$ := {gender, consumer_age, product_popularity}, and $r_1$ :=\u201cpopularity is proportional to age\u201d.\nHypothesis Semantic Tree. Observe that each independent variable in a hypothesis may itself be a target variable for a prior hypothesis. To emphasize this hierarchical nature, we introduce the concept of a hypothesis semantic tree whose nodes are variables (independent or derived) and whose sub-trees represent hypotheses, as follows. Consider a hypothesis h. A semantic hypothesis tree $T_h$ with h as the primary hypothesis is a Markov tree whose root node is the target variable of h, each of whose leaf nodes is an independent variable that is not derived further, and each of whose internal nodes is the target variable of an intermediate hypothesis. In other words, each sub-tree $T_{h'}$ rooted at an internal node v of $T_h$ is itself a hypothesis semantic tree for a hypothesis $h'$ with v as the target variable. In particular, a sub-tree rooted"}, {"title": "4 DISCOVERYBENCH", "content": "We now introduce a novel benchmark, DISCOVERYBENCH, for discovering data-driven hypotheses. In this benchmark, a data-driven discovery task is defined as follows: Given one or more task dataset(s) D and a discovery goal G, derive a hypothesis $h = \\psi(c, v, r)$ addressing G with the highest specificity for the context c, variables v, and relationship r supported by D. Optionally, a workflow of deriving such a hypothesis can be outputted to augment information already present in the hypothesis. DISCOVERYBENCH has two components: DB-REAL encompassing data-driven hypotheses and workflows derived from published scientific papers and DB-SYNTH capturing systemic variations in data-driven hypotheses and workflows obtained from synthetically generated datasets. We release our dataset under the ODC-BY license: https://github.com/allenai/discoverybench."}, {"title": "4.1 DB-REAL: Collecting data-driven hypotheses in the wild", "content": "Our goal is to replicate the scientific process undertaken by researchers to search for and validate a hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our data collection follows either a data-first or code-first approach.\nFor the data-first approach: 1) we filter papers based on open public datasets (D) such as National Longitudinal Surveys (NLS), Global Biodiversity Information Facility (GBIF), and World Bank Open Data (WBOD) that have workflow details; 2) we then try to replicate these workflows in Python. For this data-first approach, replication took up to 90 person-hours per dataset, often (30%) not resulting in success. This highlights building data-driven discovery benchmarks from real studies is not only challenging and time-consuming, but automating discovery can also be key for scientific progress and reproducibility.\nThe data-first approach by design is limited to well-known aforementioned public datasets. To improve diversity in domains, datasets (D), and workflows, we also adopted a code-first approach to look beyond popular public datasets. In this approach, we 1) search for code repositories based on scientific papers with available datasets and 2) attempt to replicate them in Python with existing code or from scratch with interpretation of the associated paper. We looked at 785 data points in Zenodo, EU's Open Research Repository, with a filter for computational notebooks. Over 85% of the repositories either had missing code, code that could not be easily translated to Python, or a"}, {"title": "Inferring task difficulty", "content": "Using the hypothesis semantic tree defined in Section 3, we say that the difficulty of a discovery task is proportional to the path length from an observed node to the target hypothesis node in the tree. However, knowing the tree structure from a task dataset alone is impractical due to incomplete a priori information about unobserved intermediate nodes and edges between observed nodes. To infer task difficulty, we, therefore, approximate the path length between the target and leaf nodes using the length of the implementation workflow required to derive a target hypothesis. Specifically, for each step in the workflow, we add 1 to the discovery path length. In some cases, we derive two tasks: easy and hard from the same hypothesis, where for easy, we provide the derived variables as observed variables in the dataset (e.g., BMI), and for hard, it would require deriving intermediate variables (BMI from height and weight) to reach the target. Additionally, given the view of a task dataset as encoding the union of multiple semantic trees rooted at different hypotheses, i.e., a semantic forest F, we further posit that task difficulty increases as the number of trees in the forest (|F|) increases. Intuitively, discovery becomes harder as the hypothesis search space increases. In practice, this setting is observed when a task requires access to multiple datasets."}, {"title": "Forming discovery goals", "content": "By definition, each hypothesis can be fully specified by the declarative sentence as $h := \\psi(c,v,r)$. To systematically construct the discovery goals for the task, we first mask one of each dimension, context c, variable v, relationship r, and generate a discovery goal to"}, {"title": "4.1.1 Features of DB-REAL benchmark", "content": "DISCOVERYBENCH incorporates a broad landscape of data-driven discovery. With over 500 instances of data preparation activities such as cleaning, deduplication, and integration, captures the complexity of real-world data pre-processing for discovery. Tasks also demand a spectrum of statistical methods, from statistical tests to mixture models, and include domain-specific approaches in econometric and ecological modeling, as reflected in the Fig 3\u00b3."}, {"title": "4.2 DB-SYNTH: Generating data-driven hypotheses using LLMs", "content": "To scale data collection, we next introduce a supplementary benchmark, which is synthetically constructed to enable controlled model evaluations. Our goal is to reverse-engineer the process of hypothesis discovery to synthesize datasets and discovery tasks of varying difficulty that require analysis workflows similar to those in the real-world benchmark. Our approach leverages the broad pre-trained knowledge of LLMs in four stages:\nDomain sampling: First, we prompt the model to generate a list of diverse topics or domains along with their natural language descriptions. E.g., \"Ancient architecture\" \u2192 \"Related to historic buildings, architectural marvels, and ancient construction techniques\".\nSemantic tree construction: For each domain, we then build a semantic tree $T_h$, recursively deriving nodes starting from a primary hypothesis h. Specifically, we prompt the model with the domain and a sampled real-world workflow (e.g., \u201cwithin-cluster analysis\u201d) to generate a hypothesis and its target variable. Setting the target variable as root, we then derive child nodes by generating the independent variables required to verify h using $V(\\cdot)$. We operationalize this by generating a column name and description for each child node (along with a data type and range) and a pandas expression [49] over only independent variables in $T_h$ such that its execution results in the target variable. We repeat this with each leaf in $T_h$ as the root of a new semantic sub-tree, generating intermediate hypotheses and a new set of variables until the desired height of $T$ is reached. We also generate a set of distractor columns disjoint from nodes in T, thus resulting in a synthetic semantic forest F.\nData generation: We then construct a task dataset $D := \\{x_i\\}_{i=1}^n$ by generating synthetic data in a bottom-up manner (i.e., from leaves to root) for each node in F. Starting with various sampling strategies for leaf nodes (see more in Sec D), for each subsequent level in F, we create new columns for nodes by simply executing their pandas expressions. Finally, to mimic real-world challenges in data collection, we probabilistically perturb each instance $x \\in x_i$ by adding noise or dropping values to create missing data\u2076. Note that at this stage, D contains a column for each node in F."}, {"title": "Task generation", "content": "For each internal node h in F, we now create multiple task datasets $D_h^{(l)}$ from D, varying the difficulty of the discovery task based on the path length l between h and the observed independent variables in F. Finally, we follow the same strategy for goal formulation as DB-REAL. We generate 903 tasks over 48 diverse domains and assign them to train, dev, and test sets using a 60/20/20 split, where each task is additionally tagged with a difficulty level from 1-4. While we evaluate our agents on the test, the training set can serve as supervised data for improving models."}, {"title": "4.3 Evaluation", "content": "We evaluate task performance by measuring the alignment of the predicted and gold hypotheses in natural language. We take inspiration from recent works in LLM benchmarking [43, 54, 52, 14, 26, 27] and design a model-based evaluation strategy using gpt-4-preview-0125 as the evaluator, conditioned on our structured formalism of data-driven hypotheses.\nRecall the propositional form $h := \\phi(h_1,...,h_n)$ of a hypothesis h that decomposes it into sub-hypotheses. We first use our GPT-4 based evaluator to independently decompose the gold ($h^g$) and predicted ($h^p$) hypotheses into their respective sub-hypotheses $\\{h_i^g\\}_{i=1}^n$ and $\\{h_i^p\\}_{i=1}^m$, asking it to also identify, for each sub-hypothesis $h_k$, its context, variables, and relationship dimensions (prompt in Listing 1). Given this structured representation of the gold and predicted hypotheses, we then compute a hypothesis match score (HMS), which measures the degree to which two hypotheses align on each dimension, as follows.\nTo compute HMS, we match each predicted sub-hypothesis $h_i^p$ with a gold sub-hypothesis $h_j^g$ when their contexts are judged as equivalent by our GPT-4 based evaluator (prompt in Listing 2)\u2078. Let M denote this set of context-matched pairs of predicted and gold sub-hypotheses. At this point, treating each sub-hypothesis context as a single unit, we can compute an F1 score, ctxF1, capturing how aligned the n contexts of sub-hypothesis of $h^g$ with the m contexts of sub-hypotheses of $h^p$. Then, for each matched pair of sub-hypotheses, we measure how well the variables and relations align, using an F1 score for the variables (varF1) and an accuracy score for the relation (relacc). Specifically, for each sub-hypothesis pair in M, we extract the set of interacting variables in the gold and predicted sub-hypotheses using the GPT-4 based evaluator (prompt in Listing 3). We compute the alignment between these two sets of variables as an F1 score, varF1, similar to how ctxF1 was computed. For relationships, we compute relationship accuracy with reference to the relationship between the gold variables (relacc) based on evaluator judgments using the following scoring heuristic: 100 if there is an exact match of the relation, 50 when the predicted relationship is broader than the gold relationship but encompasses it, and 0 otherwise (prompt in Listing 4). Finally, we compute HMS \u2208 [0, 100] as the average alignment of the variable and relationship dimensions over context-matched sub-hypotheses, weighted by the overall context alignment:\n$HMS(h^p, h^g) = ctxF1(h^p, h^g) \\times \\frac{1}{|M|} \\sum_{i=1}^{|M|} (varF1(h_i^g, h_i^p) \\times relacc(h_i^g, h_i^p))$"}, {"title": "5 Experiments", "content": "We benchmark state-of-the-art LLM-based few-shot reasoning methods as discovery agents with two closed models, GPT-40 and GPT-4-0125-preview (GPT-4p), and one open, Llama-3-70B, model powering the reasoning methods. A discovery agent takes the task description, paths to the task dataset(s) D, metadata about the datasets (description, column descriptions), and the goal, G, to produce a natural language (NL) hypothesis specified by context, variables, and relationship.\n\u2022 CodeGen generates the entire code at one go to solve the task, where we provide a demonstration of a solution code in the context. After code execution and based on the result, it generates the NL hypothesis and summarizes the workflow.\n\u2022 ReAct [51] solves the task by generating thought and subsequent codes in a multi-turn fashion."}, {"title": "5.2 Main Results", "content": "Fig 4(left) shows that overall performance for all framework-LLM pairs is low for both DB-REAL and DB-SYNTH, highlighting the challenging nature of the task and the benchmark. Most importantly, effective reasoning prompts such as React and planning with a self-critic (DataVoyager) do not help improve the simple CodeGen agent. But with oracle feedback, Reflexion (Oracle) significantly improves over CodeGen (base) performance. Analysis reveals that almost all non-reflexion agents solve the easiest (in terms of workflow category and length) instances from the benchmark. GPT-40 refuses to hallucinate in the NoDataGuess baseline, whereas surprisingly Llama-3 performs similarly in both data and no-data modes. We additionally observe that the models' performance in DB-REAL and DB-SYNTH are similar, indicating our synthetic benchmark captures complexities of the real workflow but provides a systematic way to analyze the models' performance."}, {"title": "5.3 Analysis", "content": "Context is important. Fig 4(right) shows the trends of the ctxF1 and combined varF1 \u00d7 relacc. A positive trend signifies that to predict variables and relationships accurately, precise and accurate context prediction is necessary. However, correct identification of context is an important first step, although it does not guarantee success.\nWorkflow complexity barrier. Almost all agents struggle more with tasks involving complex statistical techniques, complex data preparation methods, or domain-specific models. The top three workflow categories where the best non-oracle model was highly performant are correlation analysis (55%), data selection (18%), and summary statistics (18%), whereas the lowest three workflow categories are spatial analysis (0%), pollen dating (0%), and ecological modeling (0%).\nDomain knowledge dependency. To check if additional domain helps agents perform better, we collect targeted domain knowledge for the archaeology-related tasks that needed significant domain knowledge during data collection. When added as additional hints, we find that DataVoyager's (GPT-4p) performance jumps from 9.9% (w/ domain knowledge) to 17.5% (w/o domain knowledge).\nPerformance across domains and goal types. Fig 5(a) depicts that biology (0%) and engineering (7%) perform the worst due to their higher dependence on advanced statistical methods, while economics (25%) and sociology (23%) perform better. Additionally, Fig 5(b) shows goals related to discovering a relationship given context and variables are more easily solved than the other two types"}, {"title": "6 Conclusion", "content": "We present DISCOVERYBENCH, the first data-driven discovery benchmark consisting of 264 discovery tasks that capture real scientific workflows extracted from published works. We supplement this with 903 structurally generated synthetic tasks, tailored to evaluate discovery agents at various levels of difficulty. We benchmark state-of-the-art reasoning frameworks with the most advanced LLMs, but the best agent's performance only peaks at 25% underscoring the challenging nature of the task and the benchmark. We hope our timely contribution can increase interest and efforts in making progress on reliable and reproducible autonomous scientific discovery using large generative models."}, {"title": "B Limitations", "content": "We currently filtered domains and tasks that required forecasting, simulation, or very specific modeling (species distribution, infection spread, astrophysics equations for exoplanets) in the benchmark as they were very time-consuming to replicate as well as discover hypotheses. As a result, we discarded more papers focused on natural and physical sciences compared to social sciences, which we plan to include in future benchmarks.\nWe currently do not tackle the challenge of understanding and processing massive datasets, such as the 8.92 petabytes data from the Cancer Genome Atlas (https://portal.gdc.cancer.gov) or the extensive brain data from the Allen Institute (https://alleninstitute.org/division/ brain-science). While the potential to discover new insights from such vast data volumes is significant, ensuring these findings are robust and not subject to p-hacking remains unaddressed by our current methods.\nWe currently do not handle multi-modal data and complex pipelines, such as those needed for analyzing satellite and other geospatial data relevant to climate science and astronomy data. This would involve multiple stages of data processing, the use of various tools, and managing workflow complexities, for example, analyzing thousands of species patterns combined with satellite data to study habitats. So we do not incorporate workflows like those of EarthRanger (https://www. earthranger.com).\nEthical Considerations There could be many potential societal consequences of systems tuned on our proposed benchmark since it involves using LLMs, such as policy misuse, legal ramifications, and false discovery. On the positive side, our proposed benchmark can advance the rate of discovery, leading to an improved standard of living and social well-being."}, {"title": "C Data collection for DB-REAL", "content": "For data-first approach, replication took 15 to 40 person-hours for each NLS-related paper and up to 90 person-hours for the GBIF dataset, where specialized domain knowledge and tools led to higher complexity. All papers replicated in the NLS dataset were included, while less than half of the papers in specialized datasets like GBIF and WBOD were added to DISCOVERYBENCH."}, {"title": "D Data Generation for DB-SY\u039d\u03a4\u0397", "content": "For leaves, we use different sampling strategies based on the data type. Specifically, for categorical nodes, we sample instances with replacement from the range of allowed values, whereas for numeric, we first select a distribution (e.g., normal) and its parameters based on the specified range and then perform sampling. For each subsequent level in F, we create new columns for nodes by simply executing their pandas expressions. To recover from any execution errors, we additionally use a self-refine [30] approach to generate new pandas expressions guided by the execution error logs. Finally, to mimic real-world challenges in data collection, we probabilistically perturb each instance"}, {"title": "E Datasheets", "content": "E.1 Motivation\n\u2022 For what purpose was the dataset created? DISCOVERYBENCH is created to help assess large language models' (LLMs) ability to automate the search and verification of hypotheses purely from a set of provided datasets.\n\u2022 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? Authors belong to the Allen Institute for AI, OpenLocus, and the University of Massachusetts Amherst. The data collection is part of research efforts conducted by the Allen Institute for AI.\n\u2022 Who funded the creation of the dataset? Allen Institute for AI.\nE.2 Collection Process\n\u2022 How was the data associated with each instance acquired? Our goal is to replicate the scientific process undertaken by researchers to search for and validate a hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our data collection follows either a data-first or code-first approach. Each instance has been manually implemented and verified by the authors for solvability.\nE.3 Uses\n\u2022 Has the dataset been used for any tasks already? We use this benchmark to evaluate LLM's ability to search and verify hypotheses purely from a set of datasets.\n\u2022 Are there tasks for which the dataset should not be used? We do not expect the commu- nity members to use this data to train models that can aggravate p-hacking.\nE.4 Distribution and Maintainance\n\u2022 How will the dataset will be distributed? We distribute this benchmark via our GITHUB repository: https://github.com/allenai/discoverybench and https: //huggingface.co/datasets/allenai/discoverybench.\n\u2022 How can the owner/curator/manager of the dataset be contacted? For any benchmark-related queries, please contact: bodhisattwam@allenai.org. For any code- related discussions, please raise an issue in GITHUB: https://github.com/allenai/ discoverybench."}, {"title": "F Composition of DISCOVERYBENCH", "content": "F.1 Metadata structure\n\u2022 id: An identifier for the metadata.\n\u2022 domain: The broad field of study or area of research.\n\u2022 workflow_tags: A set of keywords summarizing the main processes or techniques used in the replication implementation. They provide an overview of the methodological approach and facilitating the identification of relevant analytical techniques.\n\u2022 domain_knowledge:\nContextual information or insights related to the dataset, explaining how certain behav- iors or variables can be interpreted within the field of study."}, {"title": "F.2 Directory structure for DB-REAL", "content": "There may be more than one query per metadata. The train split contains 14 metadata files and 25 queries. The test split contains 144 metadata files and 239 queries. Metadata folders with the same prefixes use the same underlying dataset with either a subset or a preprocessed version. When dealing with a full dataset (i.e., nls_raw), the task becomes substantially harder due to the data preparation required."}, {"title": "F.3 Directory structure for DB-SYNTH", "content": "There is one query per metadata. The train split contains 551 metadata files (queries), the dev split contains 153 metadata files (queries), and the test split contains 200 metadata files (queries)."}, {"title": "G Discovery Agent", "content": "The command discovery_agent.py is used with various options to customize its behavior for discovery tasks. Below are the options explained:\n\u2022 Usage: discovery_agent.py [OPTIONS] QUERY \u2013 Executes the discovery agent with specified options.\n\u2022 Options:\n- -agent_type [coder react]: Specifies the type of agent to use for discovery. The default type is coder. Options include coder for code-related tasks and react for reactive tasks."}, {"title": "\u0397 Evaluation", "content": "Explain about evaluation in a line and then explain the CLI usage here.\nThe command discovery_eval. py is used to evaluate the outputs generated by the discovery agent. Below are the detailed descriptions of the command options:\n\u2022 Usage: discovery_eval.py [OPTIONS] QUERY - Executes the evaluation agent with specified options and a query.\n\u2022 Options:\n-gold_hypo TEXT: Specifies the gold standard hypothesis for comparison. This field is required.\n- -gold_workflow TEXT: Specifies the gold standard workflow to be used as a reference during evaluation.\n- -pred_hypo TEXT: Specifies the predicted hypothesis generated by the discovery agent. This field is required.\n-pred_workflow TEXT: Specifies the predicted workflow generated by the discovery agent.\n-metadata_path TEXT: Specifies the path to the metadata file that is utilized during evalua- tion. This field is required.\n-metadata_type [real|synth] : Determines the type of metadata used in the evaluation, where real indicates actual metadata and synth indicates synthetic metadata. This field is required.\n- -eval_output_path TEXT: Specifies where the evaluation results should be saved.\n-help: Displays the help message and exits, detailing all available command options."}, {"title": "I Experiments", "content": "For GPT-based models, we use OpenAI API (https://platform.openai.com/docs/ models), and for Llama3, we used Together API (https://docs.together.ai/docs/ inference-models)"}, {"title": "J Evaluator Prompts", "content": "We provide below the exact prompts used for our GPT-4 based evaluation of the generated hypothesis against the gold hypothesis."}]}