{"title": "DISCOVERYBENCH: Towards Data-Driven Discovery with Large Language Models", "authors": ["Bodhisattwa Prasad Majumder", "Harshit Surana", "Dhruv Agarwal", "Bhavana Dalvi Mishra", "Abhijeetsingh Meena", "Aryan Prakhar", "Tirth Vora", "Tushar Khot", "Ashish Sabharwal", "Peter Clark"], "abstract": "Can the rapid advances in code generation, function calling, and data analysis\nusing large language models (LLMs) help automate the search and verification of\nhypotheses purely from a set of provided datasets? To evaluate this question, we\npresent DISCOVERYBENCH, the first comprehensive benchmark that formalizes\nthe multi-step process of data-driven discovery. The benchmark is designed to\nsystematically assess current model capabilities in discovery tasks and provide a\nuseful resource for improving them. Our benchmark contains 264 tasks collected\nacross 6 diverse domains, such as sociology and engineering, by manually deriving\ndiscovery workflows from published papers to approximate the real-world chal-\nlenges faced by researchers, where each task is defined by a dataset, its metadata,\nand a discovery goal in natural language. We additionally provide 903 synthetic\ntasks to conduct controlled evaluations across task complexity. Furthermore, our\nstructured formalism of data-driven discovery enables a facet-based evaluation that\nprovides useful insights into different failure modes. We evaluate several popular\nLLM-based reasoning frameworks using both open and closed LLMs as baselines\non DISCOVERYBENCH and find that even the best system scores only 25%. Our\nbenchmark, thus, illustrates the challenges in autonomous data-driven discovery\nand serves as a valuable resource for the community to make progress.", "sections": [{"title": "1 Introduction", "content": "Knowledge discovery via the scientific process has been a catalyst for human progress for centuries\nbut has, thus far, been a predominantly manual pursuit [16]. Recent breakthroughs in capabilities of\nlarge language models (LLMs) to reason and interface with the world using code [9, 40], external tools\n[41], and interactive agents [51, 32], however, now suggest the possibility of realizing a discovery\nsystem that is fully autonomous. Indeed, recent works [33] provide initial evidence for this paradigm\nwithin the setting of data-driven discovery, where both search and verification of hypotheses may be\ncarried out using a dataset alone (i.e., after physical experiments and data collection\u00b9), but the extent\nof this ability remains unclear. We, therefore, aim to systematically evaluate the following question:\nHow good are current state-of-the-art LLMs at automated data-driven discovery?\nIn practice, experiments and analysis are interleaved, not sequential. Our concern in this work, however, is\nsystematically studying the data analysis part of the (interleaved) pipeline."}, {"title": "2 Related Work", "content": "Automated data-driven discovery has been a long-standing dream of AI [33, 21]. Although there\nhave been a range of data-driven discovery systems, from early ones that fit equations to idealized\ndata, e.g., Bacon [23], to more modern ones handling complex real-world problems, e.g., AlphaFold\n[19], their associated datasets are task-specific and customized to a pre-built pipeline. In contrast,\nDISCOVERYBENCH aims to be a general test over multiple tasks, including testing whether systems\ncan design appropriate pipelines themselves.\nA number of datasets and tools are available for AutoML, a related technology aimed at automating\nworkflows for building optimal machine learning models [18, 55, 24]. AutoML tools include packages\nlike Scikit [13], and embedded in cloud platforms such as Google Cloud Platform, Microsoft Azure,"}, {"title": "3 Formalization", "content": "We begin by formalizing what we mean by a data-driven hypothesis and how the structure of a\ncomplex hypothesis may be viewed as a hypothesis semantic tree.\nA data-driven hypothesis h in H (the space of such hypotheses) is a declarative sentence about\nthe state of the world whose truth value may be inferred from a given dataset D using a verification\nprocedure VD: H \u2192 {supported, unsupported}, for instance, via statistical modeling.\nEach hypothesis may further be expressed using a propositional formula \u03d5 over a set of sub-\nhypotheses hi \u2208 Husing logical connectives, e.g., disjunctions and conjunctions, such that\nh := \u03d5(h1,...,hn) and VD(h) = \u03d5(VD(h1),...,VD(hn)). For instance, suppose h is the hy-\npothesis \"for men younger than 20, popularity of product A varies proportional to their age (h1),\nwhile there exists an inverse relationship for those older than 40 (h2)\", then h can be expressed as\nthe conjunction h1 \u2227 h2.\nInspired by recent work of Thompson and Skau [47], we additionally introduce a structured formalism\nthat breaks a hypothesis down into three hypothesis dimensions:\n\u2022 Contexts (c): Boundary conditions that limit the scope of a hypothesis. E.g., \u201cfor men over the\nage of 30\" or \u201cin Asia and Europe\" or unbounded/full dataset when not specified.\n\u2022 Variables (v): Known set of concepts that interact in a meaningful way under a given context to\nproduce the hypothesis. E.g., gender, age, or income. Note that each hypothesis is associated\nwith a target variable and a set of independent variables.\n\u2022 Relationships (r): Interactions between a given set of variables under a given context that produces\nthe hypothesis. E.g., \"quadratic relationship\u201d, \u201cinversely proportional\", or piecewise conditionals.\nWith slight abuse of notation, we can now equivalently define hypothesis h := \u03c8(c, v, r), where\n\u03c8(c,v,r) returns the declarative sentence \u201cunder context c, variables v have relationship r.\u201d For\ninstance, for sub-hypothesis h1 in our example above, c1 := \"men younger than 20\", v1 := {gender,\nconsumer_age, product_popularity}, and r1 :=\u201cpopularity is proportional to age\u201d.\nHypothesis Semantic Tree. Observe that each\nindependent variable in a hypothesis may itself be a\ntarget variable for a prior hypothesis. To emphasize\nthis hierarchical nature, we introduce the concept of\na hypothesis semantic tree whose nodes are variables\n(independent or derived) and whose sub-trees repre-\nsent hypotheses, as follows. Consider a hypothesis h.\nA semantic hypothesis tree Th with h as the primary\nhypothesis is a Markov tree whose root node is the\ntarget variable of h, each of whose leaf nodes is an\nindependent variable that is not derived further, and\neach of whose internal nodes is the target variable\nof an intermediate hypothesis. In other words, each\nsub-tree Th' rooted at an internal node v of Th is itself\na hypothesis semantic tree for a hypothesis h' with v\nas the target variable. In particular, a sub-tree rooted\""}, {"title": "4 DISCOVERYBENCH", "content": "We now introduce a novel benchmark, DISCOVERYBENCH, for discovering data-driven hypotheses.\nIn this benchmark, a data-driven discovery task is defined as follows: Given one or more task\ndataset(s) D and a discovery goal G, derive a hypothesis h = \u03c8(c, v, r) addressing G with the highest\nspecificity for the context c, variables v, and relationship r supported by D. Optionally, a workflow of\nderiving such a hypothesis can be outputted to augment information already present in the hypothesis.\nDISCOVERYBENCH has two components: DB-REAL encompassing data-driven hypotheses and\nworkflows derived from published scientific papers and DB-SYNTH capturing systemic variations in\ndata-driven hypotheses and workflows obtained from synthetically generated datasets. We release our\ndataset under the ODC-BY license: https://github.com/allenai/discoverybench."}, {"title": "4.1 DB-REAL: Collecting data-driven hypotheses in the wild", "content": "Our goal is to replicate the scientific process undertaken by researchers to search for and validate a\nhypothesis from one or more datasets. We focus on six scientific domains where data-driven research\nis the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and\nmeta-science. Our data collection follows either a data-first or code-first approach.\nFor the data-first approach: 1) we filter papers based on open public datasets (D) such as National\nLongitudinal Surveys (NLS), Global Biodiversity Information Facility (GBIF), and World Bank Open\nData (WBOD) that have workflow details; 2) we then try to replicate these workflows in Python. For\nthis data-first approach, replication took up to 90 person-hours per dataset, often (30%) not resulting\nin success. This highlights building data-driven discovery benchmarks from real studies is not only\nchallenging and time-consuming, but automating discovery can also be key for scientific progress\nand reproducibility.\nThe data-first approach by design is limited to well-known aforementioned public datasets. To\nimprove diversity in domains, datasets (D), and workflows, we also adopted a code-first approach\nto look beyond popular public datasets. In this approach, we 1) search for code repositories based\non scientific papers with available datasets and 2) attempt to replicate them in Python with existing\ncode or from scratch with interpretation of the associated paper. We looked at 785 data points in\nZenodo, EU's Open Research Repository, with a filter for computational notebooks. Over 85% of\nthe repositories either had missing code, code that could not be easily translated to Python, or a"}, {"title": "Inferring task difficulty.", "content": "Using the hypothesis semantic tree defined in Section 3, we say that\nthe difficulty of a discovery task is proportional to the path length from an observed node to the\ntarget hypothesis node in the tree. However, knowing the tree structure from a task dataset alone is\nimpractical due to incomplete a priori information about unobserved intermediate nodes and edges\nbetween observed nodes. To infer task difficulty, we, therefore, approximate the path length between\nthe target and leaf nodes using the length of the implementation workflow required to derive a target\nhypothesis. Specifically, for each step in the workflow, we add 1 to the discovery path length. In\nsome cases, we derive two tasks: easy and hard from the same hypothesis, where for easy, we provide\nthe derived variables as observed variables in the dataset (e.g., BMI), and for hard, it would require\nderiving intermediate variables (BMI from height and weight) to reach the target. Additionally,\ngiven the view of a task dataset as encoding the union of multiple semantic trees rooted at different\nhypotheses, i.e., a semantic forest F, we further posit that task difficulty increases as the number of\ntrees in the forest (|F|) increases. Intuitively, discovery becomes harder as the hypothesis search\nspace increases. In practice, this setting is observed when a task requires access to multiple datasets."}, {"title": "Forming discovery goals.", "content": "By definition, each hypothesis can be fully specified by the declarative\nsentence as h := \u03c8(c,v,r). To systematically construct the discovery goals for the task, we first\nmask one of each dimension, context c, variable v, relationship r, and generate a discovery goal to"}, {"title": "4.1.1 Features of DB-REAL benchmark", "content": "DISCOVERYBENCH incorporates a broad landscape of\ndata-driven discovery. With over 500 instances of data\npreparation activities such as cleaning, deduplication, and\nintegration, captures the complexity of real-world data pre-\nprocessing for discovery. Tasks also demand a spectrum of\nstatistical methods, from statistical tests to mixture models,\nand include domain-specific approaches in econometric\nand ecological modeling, as reflected in the Fig 33."}, {"title": "4.2 DB-SYNTH: Generating data-driven hypotheses using LLMs", "content": "To scale data collection, we next introduce a supplementary benchmark, which is synthetically\nconstructed to enable controlled model evaluations. Our goal is to reverse-engineer the process of\nhypothesis discovery to synthesize datasets and discovery tasks of varying difficulty that require\nanalysis workflows similar to those in the real-world benchmark. Our approach leverages the broad\npre-trained knowledge of LLMs in four stages:\nDomain sampling: First, we prompt the model to generate a list of diverse topics or domains along\nwith their natural language descriptions. E.g., \"Ancient architecture\" \u2192 \"Related to historic buildings,\narchitectural marvels, and ancient construction techniques\".\nSemantic tree construction: For each domain, we then build a semantic tree Th, recursively deriving\nnodes starting from a primary hypothesis h. Specifically, we prompt the model with the domain and a\nsampled real-world workflow (e.g., \u201cwithin-cluster analysis\u201d) to generate a hypothesis and its target\nvariable. Setting the target variable as root, we then derive child nodes by generating the independent\nvariables required to verify h using VD(\u00b7). We operationalize this by generating a column name and\ndescription for each child node (along with a data type and range) and a pandas expression4 [49] over\nonly independent variables in Th such that its execution results in the target variable. We repeat this\nwith each leaf in Th as the root of a new semantic sub-tree, generating intermediate hypotheses and a\nnew set of variables until the desired height of T is reached. We also generate a set of distractor\ncolumns disjoint from nodes in T, thus resulting in a synthetic semantic forest F.\nData generation: We then construct a task dataset D := {xi}i=1n by generating synthetic data in\na bottom-up manner (i.e., from leaves to root) for each node in F. Starting with various sampling\nstrategies for leaf nodes (see more in Sec D), for each subsequent level in F, we create new columns\nfor nodes by simply executing their pandas expressions. Finally, to mimic real-world challenges in\ndata collection, we probabilistically perturb each instance x \u2208 xi by adding noise or dropping values\nto create missing data. Note that at this stage, D contains a column for each node in F."}, {"title": "Task generation:", "content": "For each internal node h in F, we now create multiple task datasets Dih from D,\nvarying the difficulty of the discovery task based on the path length l between h and the observed\nindependent variables in F. Finally, we follow the same strategy for goal formulation as DB-REAL.\nWe generate 903 tasks over 48 diverse domains and assign them to train, dev, and test sets using a\n60/20/20 split, where each task is additionally tagged with a difficulty level from 1-4. While we\nevaluate our agents on the test, the training set can serve as supervised data for improving models."}, {"title": "4.3 Evaluation", "content": "We evaluate task performance by measuring the alignment of the predicted and gold hypotheses in\nnatural language. We take inspiration from recent works in LLM benchmarking [43, 54, 52, 14,\n26, 27] and design a model-based evaluation strategy using gpt-4-preview-0125 as the evaluator,\nconditioned on our structured formalism of data-driven hypotheses.\nRecall the propositional form h := \u03d5(h1,...,hn) of a hypothesis h that decomposes it into sub-\nhypotheses. We first use our GPT-4 based evaluator to independently decompose the gold (hg) and\npredicted (hp) hypotheses into their respective sub-hypotheses {hgi}i=1n and {hpj}j=1m, asking it to\nalso identify, for each sub-hypothesis hk, its context, variables, and relationship dimensions (prompt\nin Listing 1). Given this structured representation of the gold and predicted hypotheses, we then\ncompute a hypothesis match score (HMS), which measures the degree to which two hypotheses\nalign on each dimension, as follows.\nTo compute HMS, we match each predicted sub-hypothesis hjp with a gold sub-hypothesis hgi when\ntheir contexts are judged as equivalent by our GPT-4 based evaluator (prompt in Listing 2).8 Let M\ndenote this set of context-matched pairs of predicted and gold sub-hypotheses. At this point, treating\neach sub-hypothesis context as a single unit, we can compute an F1 score, ctxF1, capturing how\naligned the n contexts of sub-hypothesis of hg with the m contexts of sub-hypotheses of hp. Then,\nfor each matched pair of sub-hypotheses, we measure how well the variables and relations align,\nusing an F1 score for the variables (varF1) and an accuracy score for the relation (relacc). Specifically,\nfor each sub-hypothesis pair in M, we extract the set of interacting variables in the gold and predicted\nsub-hypotheses using the GPT-4 based evaluator (prompt in Listing 3). We compute the alignment\nbetween these two sets of variables as an F1 score, varF1, similar to how ctxF1 was computed. For\nrelationships, we compute relationship accuracy with reference to the relationship between the gold\nvariables (relacc) based on evaluator judgments using the following scoring heuristic: 100 if there is\nan exact match of the relation, 50 when the predicted relationship is broader than the gold relationship\nbut encompasses it, and 0 otherwise (prompt in Listing 4). Finally, we compute HMS \u2208 [0, 100] as the\naverage alignment of the variable and relationship dimensions over context-matched sub-hypotheses,\nweighted by the overall context alignment:\nHMS(hp, hg) = ctxF1(hp, hg) \u00d7 (1/|M|) \u2211i\u2208M ((varF1(hgi, hjp) \u00d7 relacc(hgi, hjp))"}, {"title": "5 Experiments", "content": "We benchmark state-of-the-art LLM-based few-shot reasoning methods as discovery agents with\ntwo closed models, GPT-40 and GPT-4-0125-preview (GPT-4p), and one open, Llama-3-70B, model\npowering the reasoning methods. A discovery agent takes the task description, paths to the task\ndataset(s) D, metadata about the datasets (description, column descriptions), and the goal, G, to\nproduce a natural language (NL) hypothesis specified by context, variables, and relationship.\n\u2022 CodeGen generates the entire code at one go to solve the task, where we provide a demonstration\nof a solution code in the context. After code execution and based on the result, it generates the NL\nhypothesis and summarizes the workflow.\n\u2022 ReAct [51] solves the task by generating thought and subsequent codes in a multi-turn fashion.\n\u2022 DataVoyager is a multi-component data-driven discovery agent from [33]. It has four components,\nplanner, code generator, data analysis, and critic, that orchestrate the discovery process.\n\u2022 Reflexion (Oracle) [44] is an extension of CodeGen agent, where at the end of one trial, we provide\nthe \"oracle\" HMS score as an evaluation signal, and it generates a reflection to improve (when\nHMS < 1) in the next trial till it solves the task, or maximum trials (3) are reached.\n\u2022 NoDataGuess guesses the hypothesis (in DB-REAL) just from the dataset description and the goal\nwithout accessing the datasets where we measure LLM's memorization of already published works."}, {"title": "5.2 Main Results", "content": "Fig 4(left) shows that overall performance for all framework-LLM pairs is low for both DB-REAL and\nDB-SYNTH, highlighting the challenging nature of the task and the benchmark. Most importantly,\neffective reasoning prompts such as React and planning with a self-critic (DataVoyager) do not\nhelp improve the simple CodeGen agent. But with oracle feedback, Reflexion (Oracle) significantly\nimproves over CodeGen (base) performance. Analysis reveals that almost all non-reflexion agents\nsolve the easiest (in terms of workflow category and length) instances from the benchmark. GPT-40\nrefuses to hallucinate in the NoDataGuess baseline, whereas surprisingly Llama-3 performs similarly\nin both data and no-data modes. We additionally observe that the models' performance in DB-REAL\nand DB-SYNTH are similar, indicating our synthetic benchmark captures complexities of the real\nworkflow but provides a systematic way to analyze the models' performance."}, {"title": "5.3 Analysis", "content": "Context is important. Fig 4(right) shows the trends of the ctxF1 and combined varF1 \u00d7 relacc. A\npositive trend signifies that to predict variables and relationships accurately, precise and accurate\ncontext prediction is necessary. However, correct identification of context is an important first step,\nalthough it does not guarantee success.\nWorkflow complexity barrier. Almost all agents struggle more with tasks involving complex\nstatistical techniques, complex data preparation methods, or domain-specific models. The top three\nworkflow categories where the best non-oracle model was highly performant are correlation analysis\n(55%), data selection (18%), and summary statistics (18%), whereas the lowest three workflow\ncategories are spatial analysis (0%), pollen dating (0%), and ecological modeling (0%).\nDomain knowledge dependency. To check if additional domain helps agents perform better, we\ncollect targeted domain knowledge for the archaeology-related tasks that needed significant domain\nknowledge during data collection. When added as additional hints, we find that DataVoyager's\n(GPT-4p) performance jumps from 9.9% (w/ domain knowledge) to 17.5% (w/o domain knowledge).\nPerformance across domains and goal types. Fig 5(a) depicts that biology (0%) and engineering\n(7%) perform the worst due to their higher dependence on advanced statistical methods, while\neconomics (25%) and sociology (23%) perform better. Additionally, Fig 5(b) shows goals related to\ndiscovering a relationship given context and variables are more easily solved than the other two types"}, {"title": "6 Conclusion", "content": "We present DISCOVERYBENCH, the first data-driven discovery benchmark consisting of 264 discovery\ntasks that capture real scientific workflows extracted from published works. We supplement this with\n903 structurally generated synthetic tasks, tailored to evaluate discovery agents at various levels of\ndifficulty. We benchmark state-of-the-art reasoning frameworks with the most advanced LLMs, but\nthe best agent's performance only peaks at 25% underscoring the challenging nature of the task and\nthe benchmark. We hope our timely contribution can increase interest and efforts in making progress\non reliable and reproducible autonomous scientific discovery using large generative models."}, {"title": "B Limitations", "content": "We currently filtered domains and tasks that required forecasting, simulation, or very specific modeling\n(species distribution, infection spread, astrophysics equations for exoplanets) in the benchmark as\nthey were very time-consuming to replicate as well as discover hypotheses. As a result, we discarded\nmore papers focused on natural and physical sciences compared to social sciences, which we plan to\ninclude in future benchmarks.\nWe currently do not tackle the challenge of understanding and processing massive datasets, such\nas the 8.92 petabytes data from the Cancer Genome Atlas (https://portal.gdc.cancer.gov)\nor the extensive brain data from the Allen Institute (https://alleninstitute.org/division/\nbrain-science). While the potential to discover new insights from such vast data volumes is\nsignificant, ensuring these findings are robust and not subject to p-hacking remains unaddressed by\nour current methods.\nWe currently do not handle multi-modal data and complex pipelines, such as those needed for\nanalyzing satellite and other geospatial data relevant to climate science and astronomy data. This\nwould involve multiple stages of data processing, the use of various tools, and managing workflow\ncomplexities, for example, analyzing thousands of species patterns combined with satellite data\nto study habitats. So we do not incorporate workflows like those of EarthRanger (https://www.\nearthranger.com).\nEthical Considerations There could be many potential societal consequences of systems tuned on\nour proposed benchmark since it involves using LLMs, such as policy misuse, legal ramifications,\nand false discovery. On the positive side, our proposed benchmark can advance the rate of discovery,\nleading to an improved standard of living and social well-being."}, {"title": "C Data collection for DB-REAL", "content": "For data-first approach, replication took 15 to 40 person-hours for each NLS-related paper and up to\n90 person-hours for the GBIF dataset, where specialized domain knowledge and tools led to higher\ncomplexity. All papers replicated in the NLS dataset were included, while less than half of the papers\nin specialized datasets like GBIF and WBOD were added to DISCOVERYBENCH.\nCitation/Repositories for DB-REAL: List of scientific works from where we have replicated our\ngold workflows and hypotheses:\n1. Sociology: [53, 3, 1, 45, 11]\n2. Biology: [8, 39]\n3. Economics: [35, 4, 48, 38, 34]\n4. Engineering: [2]\n5. Meta-science: [17]\n6. Humanities: [7, 6, 29, 31, 46, 12, 37, 22, 20, 5, 10, 36, 37]\nAll assets come under CC license or open licenses."}, {"title": "D Data Generation for DB-SY\u039d\u03a4\u0397", "content": "For leaves, we use different sampling strategies based on the data type. Specifically, for categorical\nnodes, we sample instances with replacement from the range of allowed values, whereas for numeric,\nwe first select a distribution (e.g., normal) and its parameters based on the specified range and then\nperform sampling. For each subsequent level in F, we create new columns for nodes by simply\nexecuting their pandas expressions. To recover from any execution errors, we additionally use a\nself-refine [30] approach to generate new pandas expressions guided by the execution error logs.\nFinally, to mimic real-world challenges in data collection, we probabilistically perturb each instance"}, {"title": "E Datasheets", "content": "E.1 Motivation\n\u2022 For what purpose was the dataset created? DISCOVERYBENCH is created to help assess\nlarge language models' (LLMs) ability to automate the search and verification of hypotheses\npurely from a set of provided datasets.\n\u2022 Who created the dataset (e.g., which team, research group) and on behalf of which\nentity (e.g., company, institution, organization)? Authors belong to the Allen Institute for\nAI, OpenLocus, and the University of Massachusetts Amherst. The data collection is part of\nresearch efforts conducted by the Allen Institute for AI.\n\u2022 Who funded the creation of the dataset? Allen Institute for AI.\nE.2 Collection Process\n\u2022 How was the data associated with each instance acquired? Our goal is to replicate the\nscientific process undertaken by researchers to search for and validate a hypothesis from\none or more datasets. We focus on six scientific domains where data-driven research is the\ncornerstone of scientific progress: sociology, biology, humanities, economics, engineering,\nand meta-science. Our data collection follows either a data-first or code-first approach.\nEach instance has been manually implemented and verified by the authors for solvability.\nE.3 Uses\n\u2022 Has the dataset been used for any tasks already? We use this benchmark to evaluate\nLLM's ability to search and verify hypotheses purely from a set of datasets.\n\u2022 Are there tasks for which the dataset should not be used? We do not expect the commu-\nnity members to use this data to train models that can aggravate p-hacking.\nE.4 Distribution and Maintainance\n\u2022 How will the dataset will be distributed? We distribute this benchmark via our\nGITHUB repository: https://github.com/allenai/discoverybench and https:\n//huggingface.co/datasets/allenai/discoverybench.\n\u2022 How can the owner/curator/manager of the dataset be contacted? For any\nbenchmark-related queries, please contact: bodhisattwam@allenai.org. For any code-\nrelated discussions, please raise an issue in GITHUB: https://github.com/allenai/\ndiscoverybench."}, {"title": "F Composition of DISCOVERYBENCH", "content": "F.1 Metadata structure\n\u2022 id: An identifier for the metadata.\n\u2022 domain: The broad field of study or area of research.\n\u2022 workflow_tags: A set of keywords summarizing the main processes or techniques used in\nthe replication implementation. They provide an overview of the methodological approach\nand facilitating the identification of relevant analytical techniques.\n\u2022 domain_knowledge:\nContextual information or insights related to the dataset, explaining how certain behav-\niors or variables can be interpreted within the field of study."}, {"title": "F.2 Directory structure for DB-REAL", "content": "There may be more than one query per metadata. The train split contains 14 metadata files and 25\nqueries. The test split contains 144 metadata files and 239 queries. Metadata folders with the same\nprefixes use the same underlying dataset with either a subset or a preprocessed version. When dealing\nwith a full dataset (i.e., nls_raw), the task becomes substantially harder due to the data preparation\nrequired.\n|-test\n|---archaeology\n|---introduction_pathways_non-native_plants\n|---meta_regression\n|---meta_regression_raw\n|---nls_incarceration\n---nls_raw\n---nls_ses\n|---requirements_engineering_for_ML_enabled_systems\n|---worldbank_education_gdp\n---worldbank_education_gdp_indicators\n-train\n---evolution_freshwater_fish\n|---immigration_offshoring_effect_on_employment\n|---nls_bmi\n|---nls_bmi_raw"}, {"title": "F.3 Directory structure for DB-SYNTH", "content": "There is one query per metadata. The train split contains 551 metadata files (queries), the dev split\ncontains 153 metadata files (queries), and the test split contains 200 metadata files (queries).\n|-test\n|---ancient-languages_*_*\n|---artificial-ecosystems_*_*\n---astronomy_*_*\n|---board-games_*_*\n|---coding-competitions_*_*\n|---digital-artistry_*_*\n|---futuristic-technology_*_*\n|---impressionist-art_*_*\n|---machine-learning_*_*\n|---molecular-gastronomy_*_*\n---neuroscience_*_*\n|---philosophical-debates_*_*\n---robotics_*_*\n-train\n--adventure-travel_*_*\n--ancient-architecture_*_*\n--ancient-astronomy_*_*\n|---aviation_*_*\n|---biodiversity-conservation_*_*\n|---cryptic-puzzles_*_*\n|---cryptocurrency_*_*\n|---culinary-arts_*_*\n|---cybersecurity_*_*\n--environmental-activism_*_*\n---fashion-design_*_*\n--fine-arts_*_*\n---literary-classics_*_*\n--marine-biology_*_*\n--marine-conservation_*_*\n---medieval-literature_*_*\n--musical-therapy_*_*\n---photography_*_*\n---robotic-explorers_*_*\n---solar-power_*_*\n---space-tourism_*_*\n---steampunk-culture_*_*\n---theater-productions_*_*\n--underwater-archaeology_*_*\n---urban-gardening_*_*\n---vintage-automobiles_*_*\n|---virtual-reality_*_*"}, {"title": "G Discovery Agent", "content": "The command discovery_agent.py is used with various options to customize its behavior for\ndiscovery tasks. Below are the options explained:\n\u2022 Usage: discovery_agent.py [OPTIONS] QUERY \u2013 Executes the discovery agent with specified\noptions.\n\u2022 Options:\n- -agent_type [coder react]: Specifies the type of agent to use for discovery. The default\ntype is coder. Options include coder for code-related tasks and react for reactive tasks."}, {"title": "\u0397 Evaluation", "content": "Explain about evaluation in a line and then explain the CLI usage here.\nThe command discovery_eval.py is used to evaluate the outputs generated by the discovery agent.\nBelow are the detailed descriptions of the command options:\n\u2022 Usage: discovery_eval.py [OPTIONS] QUERY - Executes the evaluation agent with specified\noptions and a query.\n\u2022 Options:\n-gold_hypo TEXT: Specifies the gold standard hypothesis for comparison. This field is\nrequired.\n- -gold_workflow TEXT: Specifies the gold standard workflow to be used as a reference\nduring evaluation.\n- -pred_hypo TEXT: Specifies the predicted hypothesis generated by the discovery agent. This\nfield is required.\n-pred_workflow TEXT: Specifies the predicted workflow generated by the discovery agent.\n-metadata_path TEXT: Specifies the path to the metadata file that is utilized during evalua-\ntion. This field is required.\n-metadata_type [real|synth] : Determines the type of metadata used in the evaluation,\nwhere real indicates actual metadata and synth indicates synthetic metadata. This field is\nrequired.\n- -eval_output_path TEXT: Specifies where the evaluation results should be saved.\n-help: Displays the help message and exits, detailing all available command options."}, {"title": "I Experiments", "content": "For GPT-based models, we use OpenAI API (https://platform.openai.com/docs/\nmodels), and for Llama3, we used Together API (https://docs.together.ai/docs/\ninference-models)"}, {"title": "J Evaluator Prompts", "content": "We provide below the exact prompts used for our GPT-4 based evaluation of the generated hypothesis\nagainst the gold hypothesis."}]}