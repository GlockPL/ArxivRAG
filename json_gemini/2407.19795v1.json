{"title": "VOLDOGER: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks", "authors": ["Juhwan Choi", "Junehyoung Kwon", "Jungmin Yun", "Seunguk Yu", "YoungBin Kim"], "abstract": "Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision-language tasks remains limited, primarily because of the lack of required datasets. To address these challenges, we propose VOLDOGER: Vision-Language Dataset for Domain Generalization, a dedicated dataset designed for domain generalization that addresses three vision-language tasks: image captioning, visual question answering, and visual entailment. We constructed VOLDOGER by extending LLM-based data annotation techniques to vision-language tasks, thereby alleviating the burden of recruiting human annotators. We evaluated the domain generalizability of various models through VOLDOGER.", "sections": [{"title": "1 Introduction", "content": "Vision-language models have evolved and demonstrated outstanding performance in various tasks (Chen et al., 2023) such as image captioning (Stefanini et al., 2022), visual question answering (VQA) (Wu et al., 2017; de Faria et al., 2023), and visual entailment (VE) (Xie et al., 2019). However, these vision-language models can suffer from domain shift, which is a significant challenge for deep learning models (Wang and Deng, 2018; Fang et al., 2024). Domain shift refers to a phenomenon in which the domain of the data changes between the training and inference phases of a model. For example, an image classification model trained on photos may not perform well when applied to sketch images (Zhou et al., 2022). This issue is prevalent in NLP tasks (Elsahar and Gall\u00e9, 2019; Ramponi and Plank, 2020; Calderon et al., 2023) to vision-language tasks (Chen et al., 2017; Zhao et al., 2017; Yang et al., 2018; Zhao et al., 2020).\nExtensive research has been conducted on domain generalization to mitigate domain shift (Zhou et al., 2022; Wang et al., 2022). These lines of study aim to utilize multiple source domains to enhance the generalizability of the model against out-of-domain target domains. However, the difficulty of collecting annotated data from various source domains may diminish the practicality of domain generalization. Although it is relatively simple to gather data for unimodal tasks such as image classification or text classification (Blitzer et al., 2007; Peng et al., 2019), it may be more difficult to collect data for multimodal tasks because they require a pair of data in each modality.\nConsequently, there is a lack of datasets for domain generalization in multimodal tasks, including vision-language tasks. The absence of a dedicated dataset makes it difficult to explore domain generalization in vision-language tasks. For example, existing studies on domain generalization for image captioning have resorted to compiling multiple datasets with different subjects, each of which contains real photos as input images (Ren et al., 2023). However, this approach fails to fully consider the diversity of domains because it contains only real photographs, thereby not accounting for the domain shift in the style of the input image. Furthermore, recent advancements in generative models have led to a significant increase in the volume of generated content encompassing a diverse array of styles. In view of this challenge, vision-language models must be capable of delivering accurate and consistent results on generated images as well, considering that generative models can easily produce images in designated styles (Zhang et al., 2023a). Hence, a specialized dataset for domain generalization in vision-language tasks is required to address these challenges and ensure a robust performance across diverse image styles.\nHowever, it is difficult to construct such a dataset through the collection and annotation by human annotators. Unlike relatively straightforward tasks such as image classification, where images with various styles can be collected with a simple search (e.g., \"aeroplane painting\") (Peng et al., 2019), creating a vision-language task dataset for domain generalization imposes more severe restrictions. For instance, a dataset for domain generalization in image captioning tasks would require a large set of similar images in different styles, such as cartoons, paintings, and sketches, as well as their descriptions. Moreover, these tasks require more complex human annotation procedures than simple tasks, leading to higher annotation costs and more efforts for quality control (Rashtchian et al., 2010).\nTo address these challenges and effectively construct datasets for domain generalization in vision-language tasks, we propose leveraging large language model (LLM)-based data annotation (Tan et al., 2024). LLM-based data annotation uses LLMs as data annotators to replace human annotators. Researchers have found this strategy to be cost-effective in producing consistent results compared with human annotators (Wang et al., 2021; Ding et al., 2023). However, previous studies on LLM-based data annotation have primarily focused on text data (Li et al., 2023b; Zhang et al., 2023b; He et al., 2023; Bansal and Sharma, 2023). Although recent studies have applied LLM-based data annotation to image captioning tasks, they have not considered image data and relied solely on text input (Choi et al., 2024). In this study, we leverage recent advancements in LLM with improved image interpretation capabilities, such as GPT-4o (OpenAI, 2023, 2024), and explore the use of LLMs as multimodal data annotators by collaborating with recent image generation models (Betker et al., 2023; Esser et al., 2024).\nUsing the proposed multimodal LLM-based data annotation, we constructed VOLDOGER: Vision-Language Dataset for Domain Generalization, which is the first dedicated dataset designed to facilitate domain generalization across three vision-language tasks: image captioning, VQA, and VE. VOLDOGER involves four different styles, which are real photos, cartoon drawings, pencil drawings, and oil paintings. Figure 1 showcases an example of image with various styles consisting VOLDOGER. Based on these source data encompassing various styles, it is possible to train a model with improved domain generalizability using VOLDOGER. In this study, we utilized VOLDOGER to validate the presence of domain shifts in these tasks and to evaluate the effectiveness of existing domain generalization techniques. Our contributions can be summarized as follows:\n\u2022 To the best of our knowledge, this is the first work to establish multimodal LLM-based data annotation while considering multimodal inputs.\n\u2022 We release VOLDOGER, a first dedicated dataset designed to advance research on domain generalization across three vision-language tasks.\n\u2022 Our extensive experiments demonstrate the presence of domain shift and the effectiveness of domain generalization techniques in vision-language tasks."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Domain Generalization for Vision-Language Tasks", "content": "Despite the lack of a dedicated dataset for domain generalization in vision-language tasks, researchers are increasingly exploring this area. For example, a relevant study proposed a framework for domain generalization in image captioning (Ren et al., 2023). They incorporated the use of text data through visual word guidance and sentence similarity based on previous research (Wang et al., 2020). However, although they proposed an effective framework, the datasets they used, such as MSCOCO (Chen et al., 2015) and Flickr30k (Young et al., 2014) exhibited significant overlap. Furthermore, these datasets exhibit limited differences in visual features because they primarily consist of real photos. In contrast, our objective is to create datasets for various vision-language tasks, such as image captioning, that encompass diverse visual styles within images.\nIn the field of VQA, VQA-GEN (Unni et al., 2023) suggested constructing a dataset for domain generalization by modifying an existing dataset, which aligns with the purpose of our work. However, their manipulation strategies for visual features mostly consist of simple noise-based modifications, such as injecting blurs. Moreover, VQA-GEN is not publicly available, which reduces the usability of this study for future work. Another line of research proposed a methodology that enables task generalization on VQA datasets that require image understanding and compositional reasoning (Shrestha et al., 2019; Gamage and Hong, 2021). To the best of our knowledge, this is the first study to explore domain generalization in visual entailment. Moreover, we propose a multimodal LLM-based data annotation pipeline and introduce VOLDOGER, which is a publicly available dataset constructed using our pipeline, to facilitate future advancements in domain generalization for vision-language tasks."}, {"title": "2.2 LLM-based Data Annotation", "content": "As LLMs exhibit various capabilities, researchers have explored leveraging them as data annotators to replace human annotators. For example, automatic annotation through GPT-3 has demonstrated superior downstream model performance compared with human performance at a lower cost (Wang et al., 2021; Gilardi et al., 2023). Furthermore, the capability of GPT-3 to generate labeled data from scratch was demonstrated (Ding et al., 2023). Consequently, the exploration of LLM applications as data annotators continues to expand, underscoring their utility in streamlining and optimizing the data annotation process (Li et al., 2023b; Zhang et al., 2023b; He et al., 2023; Bansal and Sharma, 2023).\nA recent study closely related to our work also suggested data annotation for image captioning tasks, one of the tasks that we address (Choi et al., 2024). However, they made limited use of multimodal data since they did not consider image inputs while annotating the given data. Instead, they only paraphrase the given text input and translate the paraphrases into another language. By contrast, we aim to actively utilize LLMs as multimodal data annotators."}, {"title": "3 LLM-based Data Annotation for Vision-Language Tasks", "content": "In this section, we introduce the proposed framework for multimodal LLM-based data annotation for vision-language tasks, as illustrated in Figure 2. The framework comprises two primary phases: stylized image generation and label annotation. Although the stylized image generation process was shared across the three tasks, the label annotation process varied slightly to accommodate the specific characteristics of each task. The objective of this framework is to convert the given dataset \\(D_{ori}\\) into a transferred dataset with the designated style \\(D_{sty}\\), where the input image \\(x_{ori}\\) is transformed into stylized image \\(x_{sty}\\). We utilize a multimodal LLM \\(M\\) to perform this transformation. For the exact instruction prompts, please refer to Appendix G."}, {"title": "3.1 Stylized Image Generation", "content": "In the first phase, the framework aims to create an image \\(x_{sty}\\) that retains the content and semantics of \\(x_{ori}\\) but has a designated style. This phase consisted of four steps: image decomposition, style injection, image generation, and image verification.\nImage Decomposition. We first input \\(x_{ori}\\) from \\(D_{ori}\\) into \\(M\\) with the instruction \\(P_{ID}\\) to generate a prompt describing semantics in \\(x_{ori}\\), \\(p_{ori} = M(P_{ID}, x_{ori})\\), which can be used to reconstruct \\(x_{ori}\\) through an image generation model \\(G\\).\nStyle Injection. Next, we transform \\(p_{ori}\\) into a stylized prompt \\(p_{sty} = M(P_{SI}, p_{ori})\\) based on instruction \\(P_{SI}\\). The generated \\(p_{sty}\\) retains the content of \\(x_{ori}\\) while incorporating information about the desired style.\nImage Generation. In this step, we pass the stylized prompt \\(p_{sty}\\) to the text-to-image generation model \\(G\\). Subsequently, a transformed image with the desired style \\(x_{sty}\\) is generated by \\(x_{sty} = G(p_{sty})\\). Appendix F.1 provides the generated image \\(x_{sty}\\) and its prompt \\(P_{sty}\\).\nImage Verification. It is important to note that the generated \\(x_{sty}\\) may not fully capture the core semantics of \\(x_{ori}\\). The distinction between \\(x_{ori}\\) and \\(x_{sty}\\) could complicate the subsequent annotation process; that is, the original label may not correspond to \\(x_{sty}\\) if it deviates significantly from \\(x_{ori}\\)."}, {"title": "3.2 Label Annotation for Image Captioning Task", "content": "The image captioning task aims to generate a description \\(y\\) from a given image \\(x\\). Unlike the other two tasks, the image captioning task does not require any additional input besides the image. Our goal is to create a data pair \\(d_{sty} = (x_{sty}, y_{sty})\\) for the task.\nCaption Paraphrasing. Instead of directly assigning the original \\(y_{ori}\\) to the generated \\(x_{sty}\\), we generate a paraphrase of \\(y_{ori}\\) as \\(y_{sty}\\), while considering the style of \\(x_{sty}\\). This process is crucial to avoid duplicating the label data, which can negatively impact the training procedure (Schofield et al., 2017). To this end, we pass \\(x_{sty}\\) and \\(y_{ori}\\) to \\(M\\), obtaining \\(y_{sty} = M(P_{CP}, x_{sty}, y_{ori})\\)."}, {"title": "3.3 Label Annotation for Visual Question Answering Task", "content": "The purpose of VQA task is to answer question \\(q\\) based on the given image \\(x\\). The VQA model takes \\(x\\) and \\(q\\) as inputs and predicts the answer \\(y\\). We aim to create a data pair for the VQA task as \\(d_{sty} = (x_{sty}, A_{sty}, y_{sty})\\).\nAnswer Verification. Although \\(x_{sty}\\) may pass the image verification step, the original label \\(y_{ori}\\) for question \\(q_{ori}\\) may not be valid for \\(x_{sty}\\) owing to minor differences. For instance, if the question \\(q_{ori}\\) was \"How many cups are on the table?\" and \\(x_{ori}\\) had two cups, but \\(x_{sty}\\) contains four cups, the original label \\(y_{ori}\\) \"two\" would no longer be valid for \\(x_{sty}\\).\nTo verify \\(y_{ori}\\) with respect to \\(x_{sty}\\), we utilize \\(M\\) to confirm if \\(y_{ori}\\) is the answer to \\(q_{ori}\\) given \\(x_{sty}\\) based on \\(v_{av} = M(P_{AV}, x_{sty}, q_{ori}, y_{ori})\\). If \\(y_{ori}\\) is a valid answer to \\(q_{ori}\\) given \\(x_{sty}\\), we assign \\(y_{ori}\\) as \\(y_{sty}\\). Otherwise, if \\(y_{ori}\\) is not a valid answer for \\(q_{ori}\\) given \\(x_{sty}\\), we proceed to the answer re-annotation step, as detailed below.\nAnswer Re-annotation. In cases where \\(y_{ori}\\) is incorrect for \\(q_{ori}\\) given \\(x_{sty}\\), We simply employ \\(M\\) to answer \\(q_{ori}\\) and generate \\(y_{sty} = M(P_{AR}, x_{sty}, q_{ori})\\). Question Paraphrasing. Similar to the image captioning task, we paraphrase the given \\(q_{ori}\\) to address the issue of duplication. This step is more crucial in VQA tasks because the allocation of identical question phrases and answers between different images can induce shortcut learning to focus solely on the question (Ramakrishnan et al., 2018; Agrawal et al., 2018; Jing et al., 2020; Guo et al., 2021). To address this concern, we obtain \\(q_{sty}\\), a paraphrased version of \\(q_{ori}\\), using \\(q_{sty} = M(P_{QP}, q_{ori})\\)."}, {"title": "3.4 Label Annotation for Visual Entailment Task", "content": "The visual entailment task is similar to the natural language inference task. Instead of predicting the entailment of a text premise and hypothesis, the visual entailment task involves taking an image as a premise and predicting the entailment of the premise and text hypothesis. In this task, we create a data pair \\(d_{sty} = (x_{sty}, h_{sty}, y_{sty})\\), where \\(h_{sty}\\) represents the hypothesis.\nLabel Verification. It is important to ensure the validity of label \\(y_{ori}\\) in relation to the newly generated \\(x_{sty}\\). To accomplish this, we use \\(M\\) to verify \\(y_{ori}\\) given (\\(x_{sty}, h_{ori}\\)), acquiring \\(v_{LV} = M(P_{LV}, x_{sty}, h_{ori}, y_{ori})\\). If \\(y_{ori}\\) is not the correct label for \\(x_{sty}\\) and \\(h_{ori}\\), we proceed to label the re-annotation step as described below. Otherwise, we assign \\(y_{ori}\\) as \\(y_{sty}\\).\nLabel Re-annotation. If \\(y_{ori}\\) is not valid for \\(h_{ori}\\) given \\(x_{sty}\\), we utilize \\(M\\) to obtain \\(y_{sty} = M(P_{LR}, x_{sty}, h_{ori})\\).\nHypothesis Paraphrasing. Similar to VQA task, the use of identical hypotheses can lead to shortcut learning (Geirhos et al., 2020). To address this issue, we assign the paraphrase of \\(h_{ori}\\) as the hypothesis \\(h_{sty}\\) for \\(x_{sty}\\). We guide \\(M\\) to paraphrase \\(h_{ori}\\) and obtain \\(h_{sty} = M(P_{HP}, h_{ori})\\)."}, {"title": "4 VOLDOGER", "content": "Based on the annotation framework discussed in the previous section, we constructed VOLDOGER, a dedicated dataset for domain generalization for vision-language tasks. In this section, we introduce VOLDOGER and detail the data configuration and statistics of each dataset. In addition to the realistic photos from the original datasets, VOLDOGER includes four distinct image styles: real photos\u00b9,"}, {"title": "4.1 VOLDOGER-CAP", "content": "VOLDOGER-CAP is a part of VOLDOGER designed for image captioning tasks. To construct this dataset, we utilized the UIT-VIIC dataset (Lam et al., 2020), which is a subset of the MSCOCO captioning dataset (Chen et al., 2015) focused on sports images, following a previous study (Choi et al., 2024). Consequently, VOLDOGER-CAP contains 2695 images for training, 924 images for validation, and 231 images for testing each style. Each image is associated with five different captions.\nTo identify the domain gap of each style in VOLDOGER-CAP, we use the maximum mean discrepancy (MMD) (Gretton et al., 2006) to measure the difference in the visual and linguistic features of each domain, following previous studies (Zhang et al., 2021; Chen et al., 2021; Ren et al., 2023). Specifically, we leveraged the encoded vectors of ResNet (He et al., 2016) and BERT (Devlin et al., 2019) to extract features from the domain and computed the MMD distances using these features. In this analysis, we found that VOLDOGER-CAP exhibited a remarkable visual domain gap across every domain compared with the collection of datasets based on real photos, which was adopted by previous study (Ren et al., 2023), revealing the value of VOLDOGER for domain generalization in vision-language tasks."}, {"title": "4.2 VOLDOGER-VQA", "content": "VOLDOGER-VQA is built upon the question and answer from VQA-v2 (Goyal et al., 2017), which utilizes the same images as the MSCOCO dataset and UIT-VIIC. To enhance the efficiency of the data annotation, we extracted images from the UIT-VIIC dataset along with their corresponding questions and answers. To ensure the quality and consistency of LLM-based data annotation, we exclusively used yes/no questions, as they are less ambiguous and require more direct answers than open-ended or multiple-choice questions, which can vary significantly in complexity and interpretation. Consequently, this dataset comprises 2091 images with 4120 questions for training, 711 images with 1452 questions for validation, and 182 images with 340 questions for testing for each style."}, {"title": "4.3 VOLDOGER-VE", "content": "For VOLDOGER-VE, we used the SNLI-VE (Xie et al., 2019) dataset, which served as the primary dataset for the visual entailment task. Similar to the approaches for VOLDOGER-CAP and VOLDOGER-VQA, we used only images related to football. To achieve this, we selected images containing text premise that includes the words \u201csoccer\" or \"football.\" Subsequently, we divided them into training, validation, and test sets in a ratio of 8:1:1. As a result, VOLDOGER-VE comprises 619 images with 7319 hypotheses for training, 77 images with 957 hypotheses for validation, and 78 images with 856 hypotheses for testing each style."}, {"title": "5 Experiment", "content": "In this section, we describe the extensive experiments conducted using our constructed VOLDOGER dataset and present several insights derived from the experimental results."}, {"title": "5.1 Experimental Setup", "content": "First, we briefly introduce the experimental setup used in our experiments. Various models are trained using different backbones for each task. For the domain shift experiment in Section 5.2, we fine-tuned the models with ViT (Dosovitskiy et al., 2021) and CLIP (Radford et al., 2021) encoders with a GPT-2 (Radford et al., 2019) decoder, as well as the BLIP (Li et al., 2022) model on VOLDOGER-CAP for the image captioning task. For the VQA and VE tasks, we trained the models using the ViT and CLIP image encoders with the BERT (Devlin et al., 2019) text encoder as well as the BLIP model on VOLDOGER-VQA and VOLDOGER-VE. Similarly, for the domain generalization experiments in Section 5.3, we trained the models using the ViT and frozen CLIP encoder models. In addition, we included a baseline that leveraged the ViT encoder with a dedicated technique for domain generalization, extending this approach to VQA and VE tasks (Ren et al., 2023).\nTo measure the performance of the model in the image captioning task, we employed various metrics such as BLEU (Papineni et al., 2002), ROUGE_L (Lin, 2004), METEOR (Denkowski and Lavie, 2014), BERTScore (Zhang et al., 2020), and BARTScore (Yuan et al., 2021), following previous study (Choi et al., 2024). The accuracy was used as a metric for the VQA and VE tasks. We report the average performance of the models trained with five random seeds. For more detailed information about the implementation of the experiment, please refer to Appendix B."}, {"title": "5.2 Existence of Domain Shift in Vision-Language Tasks", "content": "First, we investigate the existence of a domain shift using VOLDOGER. To accomplish this, we train each model on a single domain and test it across four domains: Real photo, Cartoon drawing, Pencil drawing, and Oil painting.\nTables 2 and 3 list the experimental results for the three tasks. In these experiments, we observed significant differences between the in-domain and out-domain performances, confirming the existence of a domain shift in response to input images with different styles. Examples of the outputs produced by a captioning model solely using real photos in Table 4 support the experimental results. In these examples, we can observe that the model cannot accurately generate descriptions for images with similar content but different styles. While this phenomenon has been observed in other tasks, such as image classification (Peng et al., 2019), validating its existence in vision-language tasks has been challenging because of the absence of a dedicated dataset. Our study demonstrates that this phenomenon persists in vision-language tasks using VOLDOGER, underscoring the need for future research in this area."}, {"title": "5.3 Effectiveness of Domain Generalization Techniques to Mitigate Domain Shift", "content": "Subsequently, we evaluate the effectiveness of the domain generalization techniques in mitigating the domain shift identified, as discussed in the previous section. In this experiment, we employed the domain generalization method from a previous study using a ViT encoder (Ren et al., 2023). Because this strategy focused solely on the image captioning task, we extended it to VQA and VE tasks, with the modifications detailed in Appendix B. We established two baselines for this experiment: joint training without a dedicated strategy using ViT encoders and fixed CLIP encoders.\nTables 5, 6 and 7 list the experimental results. In general, we found that using multiple source domains enhanced the out-domain performance, as indicated in red in the tables, compared to models trained on a single domain. Additionally, we discovered that the dedicated domain generalization strategy for vision-language tasks is more beneficial for out-domain performance than naive joint training. However, the implementation of such a strategy exhibited a slightly lower in-domain performance than the baselines. This highlights the potential for improvements in domain generalization techniques for vision-language tasks. We believe that the proposed VOLDOGER will play a crucial role in the development and benchmarking of this direction."}, {"title": "6 Conclusion", "content": "In this study, we propose a data annotation framework that leverages multimodal LLMs to construct a dataset with various styles for vision-language tasks. We created VOLDOGER, a dataset for three vision-language tasks with four different image styles by exploiting the proposed pipeline. Using VOLDOGER, we conducted extensive experiments across three tasks using various models. Our experiments confirmed the existence of a domain shift in vision-language tasks when dealing with images in different styles compared with the training data. In addition, we validated the effectiveness of the domain generalization strategy in our setup. We believe that our framework and VOLDOGER will serve as cornerstones for future research on domain generalization for vision-language tasks."}, {"title": "Limitations", "content": "The analysis presented in Appendix C regarding the distribution of labels in each dataset, as depicted in Figure 3 and 4, revealed that the distribution of the label differs from that of the original VQA and VE datasets. This is attributed to the difference between \\(x_{ori}\\) and \\(x_{sty}\\), which is marginal in general, but can alter the label of the question or hypothesis. For instance, the example in Appendix F.3 shows a change in the label regarding the question. In particular, the answer to the question asking the position of the tennis athletes is \u201cYes\u201d for \\(x_{ori}\\) but \u201cNo\u201d for \\(x_{sty}\\). Considering this, in future work, we will focus on improving the proposed annotation method such that it considers the preservation of the label and maintains label distribution."}, {"title": "Ethics Statement", "content": "It should be considered that LLMs and generative models may have potential biases (Gallegos et al., 2023; Zhou et al., 2024; Vice et al., 2024), leading to unintended biases in the dataset created by LLM-based annotation. Consequently, VOLDOGER may also contain several biases. These potential biases do not reflect the viewpoint of the authors."}]}