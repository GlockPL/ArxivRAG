{"title": "VOLDOGER: LLM-assisted Datasets for Domain Generalization in\nVision-Language Tasks", "authors": ["Juhwan Choi", "Junehyoung Kwon", "Jungmin Yun", "Seunguk Yu", "YoungBin Kim"], "abstract": "Domain generalizability is a crucial aspect of\na deep learning model since it determines the\ncapability of the model to perform well on data\nfrom unseen domains. However, research on\nthe domain generalizability of deep learning\nmodels for vision-language tasks remains lim-\nited, primarily because of the lack of required\ndatasets. To address these challenges, we pro-\npose VOLDOGER: Vision-Language Dataset\nfor Domain Generalization, a dedicated dataset\ndesigned for domain generalization that ad-\ndresses three vision-language tasks: image cap-\ntioning, visual question answering, and visual\nentailment. We constructed VOLDOGER by ex-\ntending LLM-based data annotation techniques\nto vision-language tasks, thereby alleviating\nthe burden of recruiting human annotators. We\nevaluated the domain generalizability of vari-\nous models through VOLDOGER.", "sections": [{"title": "1 Introduction", "content": "Vision-language models have evolved and demon-\nstrated outstanding performance in various tasks\n(Chen et al., 2023) such as image captioning (Ste-\nfanini et al., 2022), visual question answering\n(VQA) (Wu et al., 2017; de Faria et al., 2023), and\nvisual entailment (VE) (Xie et al., 2019). However,\nthese vision-language models can suffer from do-\nmain shift, which is a significant challenge for deep\nlearning models (Wang and Deng, 2018; Fang et al.,\n2024). Domain shift refers to a phenomenon in\nwhich the domain of the data changes between the\ntraining and inference phases of a model. For exam-\nple, an image classification model trained on photos\nmay not perform well when applied to sketch im-\nages (Zhou et al., 2022). This issue is prevalent\nin NLP tasks (Elsahar and Gall\u00e9, 2019; Ramponi\nand Plank, 2020; Calderon et al., 2023) to vision-\nlanguage tasks (Chen et al., 2017; Zhao et al., 2017;\nYang et al., 2018; Zhao et al., 2020).\nExtensive research has been conducted on do-\nmain generalization to mitigate domain shift (Zhou\net al., 2022; Wang et al., 2022). These lines of\nstudy aim to utilize multiple source domains to\nenhance the generalizability of the model against\nout-of-domain target domains. However, the dif-\nficulty of collecting annotated data from various\nsource domains may diminish the practicality of\ndomain generalization. Although it is relatively\nsimple to gather data for unimodal tasks such as\nimage classification or text classification (Blitzer\net al., 2007; Peng et al., 2019), it may be more dif-\nficult to collect data for multimodal tasks because\nthey require a pair of data in each modality.\nConsequently, there is a lack of datasets for do-\nmain generalization in multimodal tasks, including\nvision-language tasks. The absence of a dedicated\ndataset makes it difficult to explore domain gen-\neralization in vision-language tasks. For example,\nexisting studies on domain generalization for im-\nage captioning have resorted to compiling multiple\ndatasets with different subjects, each of which con-"}, {"title": "2 Related Work", "content": "2.1 Domain Generalization for\nVision-Language Tasks\nDespite the lack of a dedicated dataset for domain\ngeneralization in vision-language tasks, researchers\nare increasingly exploring this area. For exam-\nple, a relevant study proposed a framework for\ndomain generalization in image captioning (Ren\net al., 2023). They incorporated the use of text\ndata through visual word guidance and sentence\nsimilarity based on previous research (Wang et al.,\n2020). However, although they proposed an ef-\nfective framework, the datasets they used, such"}, {"title": "2.2 LLM-based Data Annotation", "content": "As LLMs exhibit various capabilities, researchers\nhave explored leveraging them as data annotators to\nreplace human annotators. For example, automatic\nannotation through GPT-3 has demonstrated su-\nperior downstream model performance compared\nwith human performance at a lower cost (Wang\net al., 2021; Gilardi et al., 2023). Furthermore, the\ncapability of GPT-3 to generate labeled data from\nscratch was demonstrated (Ding et al., 2023). Con-\nsequently, the exploration of LLM applications as\ndata annotators continues to expand, underscoring\ntheir utility in streamlining and optimizing the data\nannotation process (Li et al., 2023b; Zhang et al.,\n2023b; He et al., 2023; Bansal and Sharma, 2023).\nA recent study closely related to our work also\nsuggested data annotation for image captioning\ntasks, one of the tasks that we address (Choi et al.,\n2024). However, they made limited use of multi-\nmodal data since they did not consider image inputs\nwhile annotating the given data. Instead, they only"}, {"title": "3 LLM-based Data Annotation for\nVision-Language Tasks", "content": "In this section, we introduce the proposed frame-\nwork for multimodal LLM-based data annotation\nfor vision-language tasks, as illustrated in Figure 2.\nThe framework comprises two primary phases: styl-\nized image generation and label annotation. Al-\nthough the stylized image generation process was\nshared across the three tasks, the label annotation\nprocess varied slightly to accommodate the spe-\ncific characteristics of each task. The objective of\nthis framework is to convert the given dataset Dori\ninto a transferred dataset with the designated style\nDsty, where the input image xori is transformed into\nstylized image xsty. We utilize a multimodal LLM\nM to perform this transformation. For the exact\ninstruction prompts, please refer to Appendix G."}, {"title": "3.1 Stylized Image Generation", "content": "In the first phase, the framework aims to create an\nimage xsty that retains the content and semantics of\nXori but has a designated style. This phase consisted\nof four steps: image decomposition, style injection,\nimage generation, and image verification.\nImage Decomposition. We first input Xori from\nDori into M with the instruction PID to gener-\nate a prompt describing semantics in Xori, Pori =\nM(PID, Xori), which can be used to reconstruct\nXori through an image generation model G.\nStyle Injection. Next, we transform pori into a\nstylized prompt psty = M(Psty, pori) based on in-\nstruction Ps1. The generated psty retains the content\nof Xori while incorporating information about the\ndesired style.\nImage Generation. In this step, we pass the styl-\nized prompt psty to the text-to-image generation\nmodel G. Subsequently, a transformed image with\nthe desired style xsty is generated by xsty = G(psty).\nAppendix F.1 provides the generated image X sty\nand its prompt Psty.\nImage Verification. It is important to note that\nthe generated Xsty may not fully capture the core\nsemantics of Xori. The distinction between Xori and\nXsty could complicate the subsequent annotation\nprocess; that is, the original label may not corre-\nspond to xsty if it deviates significantly from Xori."}, {"title": "3.2 Label Annotation for Image Captioning\nTask", "content": "The image captioning task aims to generate a de-\nscription y from a given image x. Unlike the other\ntwo tasks, the image captioning task does not re-\nquire any additional input besides the image. Our\ngoal is to create a data pair dsty = (xsty, Ysty) for\nthe task.\nCaption Paraphrasing. Instead of directly assign-\ning the original Yori to the generated xsty, we gen-\nerate a paraphrase of Yori as Ysty, while considering\nthe style of Xsty. This process is crucial to avoid\nduplicating the label data, which can negatively im-"}, {"title": "3.3 Label Annotation for Visual Question\nAnswering Task", "content": "The purpose of VQA task is to answer question\nq based on the given image x. The VQA model\ntakes x and q as inputs and predicts the answer y.\nWe aim to create a data pair for the VQA task as\ndsty = (xsty, Asty, Ysty).\nAnswer Verification. Although xsty may pass the\nimage verification step, the original label Yori for\nquestion qori may not be valid for Xsty owing to\nminor differences. For instance, if the question\nqori was \"How many cups are on the table?\u201d and\nXori had two cups, but Xsty contains four cups, the\noriginal label Yori \"two\u201d would no longer be valid\nfor x sty\nTo verify Yori with respect to Xsty, we utilize M\nto confirm if Yori is the answer to qori given Xsty\nbased on vav = M(PAV, Xsty, qori, Yori). If Yori is\na valid answer to qori given Xsty, we assign Yori as\nYsty. Otherwise, if yori is not a valid answer for qori\ngiven Xsty, we proceed to the answer re-annotation\nstep, as detailed below.\nAnswer Re-annotation. In cases where Yori is in-\ncorrect for qori given Xsty, We simply employ M to"}, {"title": "3.4 Label Annotation for Visual Entailment\nTask", "content": "The visual entailment task is similar to the natu-\nral language inference task. Instead of predicting\nthe entailment of a text premise and hypothesis,\nthe visual entailment task involves taking an im-\nage as a premise and predicting the entailment of\nthe premise and text hypothesis. In this task, we\ncreate a data pair dsty = (xsty, hsty, Ysty), where hsty\nrepresents the hypothesis.\nLabel Verification. It is important to ensure\nthe validity of label Yori in relation to the newly\ngenerated Xsty. To accomplish this, we use M\nto verify Yori given (xsty, hori), acquiring ULV =\nM(PLV, Xsty, hori, Yori). If Yori is not the correct\nlabel for xsty and hori, we proceed to label the re-\nannotation step as described below. Otherwise, we\nassign Yori as Ysty.\nLabel Re-annotation. If Yori is not valid for\nhori given Xsty, we utilize M to obtain Ysty =\nM(PLR, Xsty, hori).\nHypothesis Paraphrasing. Similar to VQA task,\nthe use of identical hypotheses can lead to short-\ncut learning (Geirhos et al., 2020). To address this\nissue, we assign the paraphrase of hori as the hy-\npothesis hsty for Xsty. We guide M to paraphrase\nhori and obtain hsty = M(PHP, hori)."}, {"title": "4 VOLDOGER", "content": "Based on the annotation framework discussed in\nthe previous section, we constructed VOLDOGER,\na dedicated dataset for domain generalization for\nvision-language tasks. In this section, we introduce\nVOLDOGER and detail the data configuration and\nstatistics of each dataset. In addition to the realis-\ntic photos from the original datasets, VOLDOGER\nincludes four distinct image styles: real photos\u00b9,"}, {"title": "4.1 VOLDOGER-CAP", "content": "VOLDOGER-CAP is a part of VOLDOGER de-\nsigned for image captioning tasks. To construct\nthis dataset, we utilized the UIT-VIIC dataset (Lam\net al., 2020), which is a subset of the MSCOCO\ncaptioning dataset (Chen et al., 2015) focused on\nsports images, following a previous study (Choi\net al., 2024). Consequently, VOLDOGER-CAP con-\ntains 2695 images for training, 924 images for vali-\ndation, and 231 images for testing each style. Each\nimage is associated with five different captions.\nTo identify the domain gap of each style in\nVOLDOGER-CAP, we use the maximum mean dis-\ncrepancy (MMD) (Gretton et al., 2006) to measure\nthe difference in the visual and linguistic features\nof each domain, following previous studies (Zhang\net al., 2021; Chen et al., 2021; Ren et al., 2023).\nSpecifically, we leveraged the encoded vectors of\nResNet (He et al., 2016) and BERT (Devlin et al.,\n2019) to extract features from the domain and com-\nputed the MMD distances using these features. Ta-\nble 1 demonstrates the result of the analysis. In this\nanalysis, we found that VOLDOGER-CAP exhib-\nited a remarkable visual domain gap across every\ndomain compared with the collection of datasets\nbased on real photos, which was adopted by previ-\nous study (Ren et al., 2023), revealing the value of\nVOLDOGER for domain generalization in vision-\nlanguage tasks."}, {"title": "4.2 VOLDOGER-VQA", "content": "VOLDOGER-VQA is built upon the question and\nanswer from VQA-v2 (Goyal et al., 2017), which\nutilizes the same images as the MSCOCO dataset\nand UIT-VIIC. To enhance the efficiency of the\ndata annotation, we extracted images from the UIT-\nVIIC dataset along with their corresponding ques-"}, {"title": "4.3 VOLDOGER-VE", "content": "For VOLDOGER-VE, we used the SNLI-VE (Xie\net al., 2019) dataset, which served as the pri-\nmary dataset for the visual entailment task. Sim-\nilar to the approaches for VOLDOGER-CAP and\nVOLDOGER-VQA, we used only images related to\nfootball. To achieve this, we selected images con-\ntaining text premise that includes the words \u201csoccer\u201d\nor \"football.\" Subsequently, we divided them into\ntraining, validation, and test sets in a ratio of 8:1:1.\nAs a result, VOLDOGER-VE comprises 619 im-\nages with 7319 hypotheses for training, 77 images\nwith 957 hypotheses for validation, and 78 images\nwith 856 hypotheses for testing each style."}, {"title": "5 Experiment", "content": "In this section, we describe the extensive\nexperiments conducted using our constructed\nVOLDOGER dataset and present several insights\nderived from the experimental results."}, {"title": "5.1 Experimental Setup", "content": "First, we briefly introduce the experimental setup\nused in our experiments. Various models are\ntrained using different backbones for each task.\nFor the domain shift experiment in Section 5.2, we\nfine-tuned the models with ViT (Dosovitskiy et al.,\n2021) and CLIP (Radford et al., 2021) encoders\nwith a GPT-2 (Radford et al., 2019) decoder, as well"}, {"title": "5.2 Existence of Domain Shift in\nVision-Language Tasks", "content": "First, we investigate the existence of a domain shift\nusing VOLDOGER. To accomplish this, we train\neach model on a single domain and test it across\nfour domains: Real photo, Cartoon drawing, Pencil\ndrawing, and Oil painting.\nTables 2 and 3 list the experimental results for\nthe three tasks. In these experiments, we observed\nsignificant differences between the in-domain and\nout-domain performances, confirming the existence\nof a domain shift in response to input images with\ndifferent styles. Examples of the outputs produced\nby a captioning model solely using real photos in\nTable 4 support the experimental results. In these\nexamples, we can observe that the model cannot\naccurately generate descriptions for images with\nsimilar content but different styles. While this phe-\nnomenon has been observed in other tasks, such\nas image classification (Peng et al., 2019), vali-\ndating its existence in vision-language tasks has\nbeen challenging because of the absence of a ded-\nicated dataset. Our study demonstrates that this\nphenomenon persists in vision-language tasks us-\ning VOLDOGER, underscoring the need for future"}, {"title": "5.3 Effectiveness of Domain Generalization\nTechniques to Mitigate Domain Shift", "content": "Subsequently, we evaluate the effectiveness of the\ndomain generalization techniques in mitigating the\ndomain shift identified, as discussed in the previous\nsection. In this experiment, we employed the do-\nmain generalization method from a previous study\nusing a ViT encoder (Ren et al., 2023). Because\nthis strategy focused solely on the image caption-\ning task, we extended it to VQA and VE tasks, with\nthe modifications detailed in Appendix B. We es-\ntablished two baselines for this experiment: joint\ntraining without a dedicated strategy using ViT en-\ncoders and fixed CLIP encoders.\nTables 5, 6 and 7 list the experimental results.\nIn general, we found that using multiple source\ndomains enhanced the out-domain performance, as\nindicated in red in the tables, compared to models\ntrained on a single domain. Additionally, we dis-\ncovered that the dedicated domain generalization\nstrategy for vision-language tasks is more benefi-\ncial for out-domain performance than naive joint\ntraining. However, the implementation of such a\nstrategy exhibited a slightly lower in-domain per-\nformance than the baselines. This highlights the po-\ntential for improvements in domain generalization\ntechniques for vision-language tasks. We believe\nthat the proposed VOLDOGER will play a crucial\nrole in the development and benchmarking of this\ndirection."}, {"title": "6 Conclusion", "content": "In this study, we propose a data annotation frame-\nwork that leverages multimodal LLMs to construct\na dataset with various styles for vision-language\ntasks. We created VOLDOGER, a dataset for three\nvision-language tasks with four different image\nstyles by exploiting the proposed pipeline. Using\nVOLDOGER, we conducted extensive experiments\nacross three tasks using various models. Our exper-\niments confirmed the existence of a domain shift\nin vision-language tasks when dealing with images\nin different styles compared with the training data.\nIn addition, we validated the effectiveness of the\ndomain generalization strategy in our setup. We\nbelieve that our framework and VOLDOGER will\nserve as cornerstones for future research on domain\ngeneralization for vision-language tasks."}, {"title": "Limitations", "content": "The analysis presented in Appendix C regarding the\ndistribution of labels in each dataset, as depicted\nin Figure 3 and 4, revealed that the distribution of\nthe label differs from that of the original VQA and\nVE datasets. This is attributed to the difference\nbetween xori and Xsty, which is marginal in general,\nbut can alter the label of the question or hypothesis.\nFor instance, the example in Appendix F.3 shows\na change in the label regarding the question. In\nparticular, the answer to the question asking the\nposition of the tennis athletes is \u201cYes\u201d for xori but\n\u201cNo\u201d for x sty. Considering this, in future work, we\nwill focus on improving the proposed annotation\nmethod such that it considers the preservation of\nthe label and maintains label distribution."}, {"title": "Ethics Statement", "content": "It should be considered that LLMs and generative\nmodels may have potential biases (Gallegos et al.,\n2023; Zhou et al., 2024; Vice et al., 2024), leading\nto unintended biases in the dataset created by LLM-\nbased annotation. Consequently, VOLDOGER may\nalso contain several biases. These potential biases\ndo not reflect the viewpoint of the authors."}, {"title": "3.3 Label Annotation for Visual Question\nAnswering Task", "content": "The purpose of VQA task is to answer question\nq based on the given image x. The VQA model\ntakes x and q as inputs and predicts the answer y.\nWe aim to create a data pair for the VQA task as\ndsty = (xsty, Asty, Ysty).\nAnswer Verification. Although xsty may pass the\nimage verification step, the original label Yori for\nquestion qori may not be valid for Xsty owing to\nminor differences. For instance, if the question\nqori was \"How many cups are on the table?\u201d and\nXori had two cups, but Xsty contains four cups, the\noriginal label Yori \"two\u201d would no longer be valid\nfor x sty\nTo verify Yori with respect to Xsty, we utilize M\nto confirm if Yori is the answer to qori given Xsty\nbased on vav = M(PAV, Xsty, qori, Yori). If Yori is\na valid answer to qori given Xsty, we assign Yori as\nYsty. Otherwise, if yori is not a valid answer for qori\ngiven Xsty, we proceed to the answer re-annotation\nstep, as detailed below.\nAnswer Re-annotation. In cases where Yori is in-\ncorrect for qori given Xsty, We simply employ M to"}, {"title": "F Additional Examples of Annotated Data", "content": "F.1 Additional Examples on Image Prompt\nIn this section, we present an original image xori with decomposed pori and its transformed versions Xsty\nand psty generated through the annotation process described in Section 3. The expressions for psty that\ncontribute to the generation of the stylized image are boldfaced."}, {"title": "6.1 Prompt for Data Annotation for Image Captioning Task", "content": "G.1 Prompt for Data Annotation for Image Captioning Task\nSystem\nYou are an annotator for image captioning tasks.\nYou will help create stylized image and its captions based on user requests.\nUser // Image Decomposition\nPlease generate a detailed prompt for DALL-E3 model to replicate the given image. [Original Image]\nAssistant\nCreate an image of a man preparing food outside an industrial-style workspace. The man is wearing a flat\ncap and a dark short-sleeve shirt and is standing at a brown counter, chopping green onions on a cutting\nboard. Surrounding him on the counter are various fresh vegetables, including green onions, leafy greens,\na whole avocado, and a bowl of eggs. In the background, an open garage door reveals the interior of the\nworkspace with tools, a workbench, and a bicycle leaning against the outside. The floor is concrete and\nthe walls are decorated with hanging tools and shelves. The overall atmosphere should convey a casual,\nindustrious vibe.\nUser // Style Injection\nPlease modify the generated prompt to change the style of the image to a cartoon drawing style.\nAssistant\nCreate a cartoon-style image of a man preparing food outside an industrial-style workspace. The man is\nwearing a flat cap and a dark short-sleeve shirt and is standing at a brown counter, chopping green onions\non a cutting board. Surrounding him on the counter are various fresh vegetables, including green onions,\nleafy greens, a whole avocado, and a bowl of eggs. In the background, an open garage door reveals the\ninterior of the workspace with tools, a workbench, and a bicycle leaning against the outside. The floor is\nconcrete and the walls are decorated with hanging tools and shelves. The overall atmosphere should\nconvey a casual, industrious vibe, with cartoonish exaggerated features and vibrant colors.\nUser // Image Verification\nPlease verify if the image below is a cartoon drawing style image of the original image. The generated\nimage should not exactly match the original image but should capture the essence of the original image.\nStart the response with 'Yes' or 'No'. [Generated Image]\nAssistant\nYes, the provided image is a cartoon-style representation of the original image. The soccer player is\ndepicted in a stylized, animated manner, with exaggerated features typical of cartoons. The attire,\nincluding the white short-sleeved jersey, blue shorts, long white socks, and white and orange cleats,\nclosely matches the original image. The animated background with grass, orange cones, and a goal net\nalso retains elements from the original setting, demonstrating a colorful and whimsical portrayal.\nUser // Caption Paraphrasing\nPlease generate five captions of the generated cartoon drawing style image. The captions should describe\nthe image in different ways. Consider given captions below for reference.\n1. a man cutting up vegetables on top of a food cart.\n2. A man cutting up scallions at an outdoor table\n3. A man chopping vegetables on a white board.\n4. A man stands as he cuts up some vegetables.\n5. A man stands with a knife and onions in front of a garage\nAssistant\n1. A cartoon-styled man slicing vegetables at an outdoor table near a garage.\n2. An animated character chopping fresh scallions on a cutting board outside.\n3. A cartoon figure preparing a meal by cutting vegetables on a white board in front of a workshop.\n4. A man in cartoon form stands at an outdoor table, diligently cutting vegetables.\n5. A cartoon man, standing with a knife and spring onions, prepares food outside a garage filled with\ntools."}, {"title": "G.2 Prompt for Data Annotation for Visual Question Answering Tasks", "content": "G.2 Prompt for Data Annotation for Visual Question Answering Tasks\nWe omitted the image generation process as it is shared across three tasks.\nSystem\nYou are an annotator for visual question answering tasks.\nYou will help create stylized image and its questions based on user requests.\n// Image Generation Process\nUser // Answer Verification\nPlease verify if the question and answer pair below is correct for the generated cartoon drawing style\nimage. Start the response with 'Yes' or 'No'.\nQuestion: Is the person wearing a hat?\nAssistant\nNo, the question and answer pair is not correct. The person in the generated image is not wearing a hat.\nUser // Answer Re-annotation\nPlease answer the question below based on the generated cartoon drawing style image. Start the response\nwith 'Yes' or 'No'.\nQuestion: Is the person wearing a hat?\nAssistant\nNo, the person in the generated image is not wearing a hat.\nUser // Question Paraphrasing\nPlease paraphrase the question below for the generated cartoon drawing style image. The paraphrased\nquestion should have the same meaning as the original question but be rephrased in a different way. Only\nthe question should be paraphrased.\nQuestion: Is the person chopping green onions?\nAssistant\nParaphrased Question: Is the individual slicing green onions!"}, {"title": "G.3 Prompt for Data Annotation for Visual Entailment Tasks", "content": "G.3 Prompt for Data Annotation for Visual Entailment Tasks\nWe omitted the image generation process as it is shared across three tasks.\nSystem\nYou are an annotator for visual entailment tasks.\nYou will help create stylized image and its corresponding hypothesis based on user requests.\n// Image Generation Process\nUser // Label Verification\nPlease verify if given hypothesis pair and its label is correct for the generated cartoon drawing style image.\nStart the response with 'Yes' or 'No'.\nHypothesis: The person is preparing ingredients for a meal in an outdoor kitchen setup.\nLabel: True\nAssistant\nNo, the hypothesis \u201cThe person is preparing ingredients for a meal in an outdoor kitchen setup\" is not\nentailed by the given image.\nUser // Label Re-annotation\nDoes the given hypothesis entail the image? Start the response with 'True', 'False', or 'Undetermined'.\nHypothesis: The person is preparing ingredients for a meal in an outdoor kitchen setup.\nAssistant\nUndetermined. It is unclear in the generated image if the person is preparing ingredients \u201cin an outdoor\nkitchen setup.\"\nUser // Question Paraphrasing\nPlease paraphrase the hypothesis sentence below for the generated cartoon drawing style image. The\nparaphrased hypothesis should have the same meaning as the original sentence but be rephrased in a\ndifferent way. Only the sentence should be paraphrased.\nHypothesis: The person is preparing ingredients for a meal in an outdoor kitchen setup.\nAssistant\nParaphrased Hypothesis: The individual is getting ingredients ready for cooking in an outdoor kitchen\nsetting."}]}