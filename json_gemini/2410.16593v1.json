{"title": "Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs", "authors": ["Haolin Li", "Luana Ruiz"], "abstract": "Graph Neural Networks (GNNs) excel in many graph machine learning tasks but face challenges when scaling to large networks. GNN transferability allows training on smaller graphs and applying the model to larger ones, but existing methods often rely on random subsampling, leading to disconnected subgraphs and reduced model expressivity. We propose a novel graph sampling algorithm that leverages feature homophily to preserve graph structure. By minimizing the trace of the data correlation matrix, our method better preserves the graph Laplacian's rank than random sampling while achieving lower complexity than spectral methods. Experiments on citation networks show improved performance in preserving graph rank and GNN transferability compared to random sampling.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph neural networks (GNNs) are deep neural networks tailored to network data which have shown great empirical performance in several graph machine learning tasks [1], [2], [3], [4]. This is especially true in graph signal processing problems\u2014such as recommender systems on product similarity networks [5], or attribution of research papers to scientific domains [6] in which GNNs' invariance and stability properties [7], [8] play a key role.\nYet, in practice most successful applications of GNNs are limited to graphs of moderate size. The sheer size of many modern networks, typically in the order of several millions, frequently makes these models impractical to train. Good results have been seen by leveraging the GNN's transferability property [9], [10], which states that a GNN with fixed weights produces similar outputs on large enough graphs belonging to the same \"family\", e.g., the same random graph model. This property allows training the GNN on a graph of moderate size, and transferring it for inference on the large graph.\nThe transferability of GNNs is closely related to their convolutional parametrization, and is a consequence of the fact that graph convolutions converge on sequences of graphs converging to a common graph limit [11]. Under certain assumptions on the type of limit, and on how the graphs converge to (or are sampled from) them, it is possible to obtain non-asymptotic error bounds inversely proportional to the sizes of the graphs. Such bounds are then used to inform practical considerations, such as the minimum graph size on which to train a GNN to meet a maximum transference error. Once this is determined, the training graph is obtained by sampling a subgraph of the appropriate size at random from the large graph.\nLearning GNNs on randomly subsampled graphs works reasonably well on average but, for models trained on small subsamples, there is large variance in performance, and the worst-case performance can be quite low; see Figure 2. While this is a natural consequence of subsampling any type of data, on graphs these issues are exacerbated by the fact that the random node-induced subgraphs usually have disconnected components and isolated nodes. This leads to loss of rank, which in turn affects the expressive power of GNNs [12]. Graph sampling algorithms better at preserving matrix rank\u2014such as spectral algorithms, e.g. [13], [14]\u2014or connectivity-such as local algorithms, e.g., breadth-first search[15]\u2014exist, however, spectral methods have high computational complexity, and local methods can be myopic, focusing too much on specific regions of the graph.\nIn this paper, we identify a property of graphs that allows them to be sampled more efficiently without restricting to local regions: feature homophily. Specifically, let G = (V, E) be a graph with node features \\(X \\in \\mathbb{R}^{|V|\\times d}\\). This graph is said to be feature-homophilic if, given that (i,j) is an edge in G, the normalized features X[i, :] and X [j,:] are close. Our first contribution is to introduce a novel definition of feature homophily based on the graph Laplacian. Then, we show that, by sampling nodes that minimize the trace of the correlation matrix XXT, it is possible to improve the trace of the graph Laplacian, which is directly related to graph rank, on homophilic graphs. This heuristic is formalized as a graph sampling algorithm in Algorithm 1. Unlike other graph sampling routines, it does not require sequential node operations, and has complexity O(|V||E|), which is substantially cheaper than other algorithms for large |V| and moderate d.\nWe conclude with an experimental study of the proposed algorithm on homophilic citation networks, in which we compare it with random sampling. We observe that, for the same sampling budget, our algorithm preserves trace better than sampling at random, and leads to better transferability performance in a semi-supervised learning task."}, {"title": "II. PRELIMINARIES", "content": "A graph G = (V, E) consists of two components: a set of vertices or nodes V, and a set of edges \\(E \\subseteq V \\times V\\). Generally, graphs can be categorized as being either directed or undirected based on their edge set E. A graph is undirected if and only if for any two nodes u, v \u2208 V, (u, v) \u2208 E also implies (v, u) \u2208 E (and both correspond to the same undirected edge). In this paper, we restrict attention to undirected graphs.\nLet |V| = n be the number of nodes and m = |E| be the number of edges in G. Then, the graph adjacency matrix is the nxn matrix A with entries\n\\(A[i, j] =\\begin{cases}1 & \\text{if } (i, j) \\in E \\\\0 & \\text{otherwise}.\\end{cases}\\)\nThe Laplacian matrix or graph Laplacian is defined as L = D \u2013 A, where D = diag(A1n) is the so-called degree matrix. From their definitions, and since G is undirected, we can easily infer that A and L are symmetric.\nIn practice, real-world graphs are associated with node data \\(x \\in \\mathbb{R}^n\\) called graph signals, where x[i] corresponds to the value of the signal at node i. More generally, graph signals consist of multiple features, in which case they are represented as matrices \\(X \\in \\mathbb{R}^{n \\times d}\\) with d denoting the number of features.\nThe graph Laplacian plays an important role in graph signal processing (GSP) [16], [17], as it allows defining the notion of total variation of a signal x. Explicitly, the total variation of x is defined as TV(x) = xTLx [18]. Let L = V\u039bVT be the Laplacian eigendecomposition, where \u039b is a diagonal matrix with eigenvalues ordered as \u03bb\u2081 < ... < \u03bbn and V is the corresponding eigenvector matrix. For unit-norm signals, it is easy to see that the maximum total variation is \u03bbn, the largest Laplacian eigenvalue, and the minimum total variation is \u03bb\u2081 = 0, which corresponds to the all-ones eigenvector. Therefore, the Laplacian eigenvalues can be interpreted as graph frequencies, and the eigenvectors as these frequencies' respective oscillation modes.\nGraph Neural Networks. GNNs are layered deep learning models specifically designed for graph-structured data, where each layer consists of two components: a bank of convolutional filters and a nonlinear activation function.\nA graph convolutional filter is the extension of a standard convolutional filter to graph data. More specifically, it consists of a shift-and-sum operation of a signal x on the graph G, which is captured by a matrix S \u2208 Rn\u00d7n encoding the sparsity pattern of the graph, i.e., S[i, j] \u2260 0 if and only if (i, j) \u2208 E or i = j; typical choices are S = A or S = L [19]. The graph shift operator operates on x as Sx.\nGiven any choice of S, the graph convolution is defined as \\(y = \\sum_{k=0}^{K-1} h_k S^k x\\), where h\u2080,...,hK-1 are the filter coefficients or taps. More generally, for X \u2208 [Rnxd and Y \u2208 Rn\u00d7f we can define the convolutional filterbank [4]\n\\(Y = \\sum_{k=0}^{K-1} S^k X H_k\\) (1)\nwhere Hk \u2208 Rd\u00d7f, 0 \u2264 k \u2264 K \u2013 1, mapping features from Rd to Rf\nWe further write the lth layer of a GNN as the following[4]\n\\(X_{\\ell} = \\sigma\\left(\\sum_{k=0}^{K-1} S X_{\\ell-1} H_{\\ell k}\\right)\\) (2)\nwhere \u03c3: R \u2192 R is an entry-wise nonlinearity (e.g., the ReLU or sigmoid). At layer l = 1, X\u2080 is the input data, and the last layer output XL is the output Y of the GNN. For succinctness, in the following we will represent the whole L-layer GNN as a map Y = \u03a6(X, G; H) with H = {H\u2113k}\u2113,k\u00b7\nTransferability. The mathematical property that allows training GNNs on small graph subsamples of larger graphs is their transferability. Explicitly, GNNs are transferable in the sense that when a GNN with fixed weights H is transferred across two graphs in the same \"family\", the transference error is upper bounded by a term that decreases with the graph size. Typical transferability analysis show this by defining graph \"families\" as graphs coming from the same random graph model, or converging to a common graph limit. Here, we consider families of graphs identified by the same graphon, which can be seen as both a generative model and a limit model for large graphs.\nA graphon is a bounded, symmetric, measurable function W : [0, 1]2 \u2192 [0,1] [20], [21]. Graphs can be sampled from W by sampling nodes u1,..., un from [0,1], and sampling edges (ui, uj) with probability W(ui, uj). The graph limit interpretation is more nuanced but has to do with the fact that, on sequences of graphs converging to W, the densities of certain \u201cmotifs\u201d, e.g., triangles, also converge. For graphs associated with the same graphon, we have the following transferability theorem.\nTheorem II.1 (GNN transferability, simplified [22]). Let \u03a6 be a GNN with fixed coefficients, and Gn, Gm graphs with n and m nodes sampled from a graphon W. Under mild conditions, \\(||\u03a6(G_n) - \u03a6(G_m)|| = O(n^{-1} + m^{-1}) \\) w.h.p..\nIn this paper, we will use the transferability property of GNNs, together with a novel graph sampling algorithm, to train GNNs on small graph subsamples and ensure they scale well to large graphs.\nExpressivity. While GNNs achieve remarkable performance in many graph machine learning tasks, they have fundamental limitations associated with their expressive power [23], [24]. In GSP problems specifically, the expressivity of a GNN is constrained by the expressivity of the graph convolution, which in turn is constrained by the rank of the graph shift operator [12]. This is demonstrated in the following proposition.\nProposition II.2 (Expressivity of Graph Convolution). Let G be an n-node symmetric graph with rank-r graph shift operator S, r < n, and x \u2208 R an arbitrary graph signal. Consider the graph convolution \\(\\hat{y} = \\sum_{k=0}^{K-1} h_k S^k x\\). Let V \u2282 Rn be the subspace of signals that can be expressed as y = \u0177 for some h\u2080,...,hK-1. Then, dim(V) \u2264 r + 1.\nProof. See the appendices of the extended version, available here.\nIn other words, the space of signals that can be represented with a graph convolution shrinks with the rank of the graph shift. Rank preservation is hence an important consideration when sampling subgraphs for training GNNs."}, {"title": "III. FEATURE HOMOPHILY AND HOMOPHILY-BASED SAMPLING", "content": "We start by introducing the notion of feature homophily, which is a requirement for our algorithm. In order to make this definition compatible for graphs of different sizes and features, we firstly need to normalize \\(X \\in \\mathbb{R}^{n \\times d}\\) along both the feature and node dimensions. Explicitly, let \\(\\vec{\\mu} = (\\mu_1,\\dots,\\mu_d)^T \\in \\mathbb{R}^d\\) be the mean feature vector and \\(\\vec{\\sigma} = (\\sigma_1,\\dots,\\sigma_d)^T \\in \\mathbb{R}^d\\) the standard deviation vector. We define the normalized graph features \\(\\tilde{X}\\) as:\n\\(\\tilde{X} = (X - \\mathbb{1}_n \\vec{\\mu}^T) \\left(\\frac{1}{\\sqrt{d}} \\mathbb{1}_n \\vec{\\sigma}^T\\right).\\) (3)\nIn other words, \\(\\tilde{X}[i, j] = \\frac{X[i, j] - \\mu_j}{\\sqrt{d} \\sigma_j}\\).\nDefinition III.1 (Feature Homophily). Let G be a graph with Laplacian matrix L, and let \\(\\tilde{X}\\) be the corresponding normalized feature matrix (3). The feature homophily of graph G is defined as:\n\\(h_G = \\frac{1}{n} tr(-\\tilde{L} \\tilde{X} \\tilde{X}^T).\\) (4)\nSince L is positive semidefinite and the trace of the outer product is the product of traces, it is ready to see, by Cauchy-Schwarz, that hG \u2264 0 for any undirected graph G. The larger the feature homophily hG, i.e., the closer it is to 0, the higher the alignment of the data \\(\\tilde{X}\\) (or, more precisely, of its principal components) with the low-frequency eigenvectors of L-which account for most of the graph's global structure, such as its communities. Thus, the data is informative with regards to the graph. On the other hand, highly negative values of hG indicate strong alignment of \\(\\tilde{X}\\) with high-frequency eigenvectors, which tend to be noisier and less descriptive of the graph structure.\nThe following proposition provides a lower bound on the tr(L) in terms of the feature homophily hG.\nProposition III.2 (Lower Bound on tr(L)).\n\\(tr(L)^2 \\geq \\frac{h_G^2}{tr(\\tilde{X} \\tilde{X}^T)^2}.\\) (5)\nProof. See the appendices of the extended version, available here.\nNote that if the graph G has high feature homophily, the right-hand side of (5) is small, and thus the lower bound on tr(L) is approximately vacuous. Conversely, for heterophilic graphs, hG has higher magnitude, so the lower bound is further away from zero.\nAlgorithm. Although simple, the result from Proposition III.2 has important implications for feature-homophilic graph sampling. Suppose we start removing nodes from G according"}, {"title": "Algorithm 1", "content": "Algorithm 1 Node Sampling for Feature-Homophilic Graphs\nRequire: G(V, E), |V| = n; X \u2208 Rn\u00d7d; \u03b3\u2208 [0,1]\nCalculate deletion budget: na \u2190 [(1 \u2212 \u03b3) \u00b7 n]\nCalculate node scores: \u0e23\u0e35\u2190 diag(XXT)\nKeep n na nodes with highest score:\nidx \u2190 argmax(s, descending = True)[nd : n]\nSample graph:\nV\u2190 Vidx\n\u1ebc \u2190 {(u, v) : u, v \u2208 \u00d1, (u, v) \u2208 E}\nX + X [idx, :]\nreturn \u011e(V, \u1ebc); X\nto the diagonal entries of XXT sorted in decreasing order, so that the denominator on the right-hand side of (5) becomes progressively smaller. If G is homophilic with hg \u2248 0, the decrease in the denominator tr(XXT) is likely to lead to an increase of the lower bound on tr(L). Therefore, we obtain lower bounds on tr(L) that are increasingly more meaningful and, as a result, that ensure rank preservation in the sampled graph. This idea is formalized in Algorithm 1.\nComplexity. When d < n, as is often the case in practice, Algorithm 1 offers lower computational complexity than graph sampling algorithms with rank preservation objectives, as demonstrated by Proposition III.3.\n= \nProposition III.3 (Complexity of Algorithm 1). Let G =(V,E) be a graph with |V| = n and |E| = m, and let X \u2208 Rn\u00d7d be the corresponding node feature matrix. For any sampling budget \u03b3, the complexity of Algorithm 1, including computation of hg, is O(dm). If G is known to be feature- homophilic, computation of hg can be bypassed and the complexity simplifies to O(dn).\nProof. See the appendices of the extended version, available here.\nImportantly, the complexity of Algorithm 1 is dominated by the complexity of calculating the feature-homophily, which only has to happen once prior to execution to determine if the graph is homophilic. The complexity is further independent of the sampling budget, as the diagonal elements of XXT only have to be computed and sorted once.\nOn sparse graphs (m < n\u00b2) with moderate feature di- mension d, Algorithm 1 is cheaper than direct maximization of tr(L), which requires O((1 \u2013 \u03b3)n\u00b2) computations\u2014O(n) node degree computations (1 \u2013 \u03b3)n times, as node degrees change each time a node is removed from G. In fact, this is an underestimation, as it does not factor in the cost of breaking ties across nodes with the same degree. Algorithm 1 is also notably cheaper than spectral algorithms such as [13], [14] which are inspired by E-optimal sampling and, without exhaustive search, require greedy routines with complexity at least \u039f(\u03b3\u03b7m). Another advantage of Algorithm 1 is that it does not require sequential execution, unlike maximization of tr(L) and [13], [14], which cannot be parallelized."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we present a further analysis of our sampling algorithm. By comparing the test accuracy between models trained on subgraphs sampled by our heuristic and random baselines, we observe its effectiveness and robustness.\nTrace preservation. Among many ways to evaluate a sam- pling algorithm over a graph, apart from test accuracy bench- marks which we will show in Figure 2, an informative one is the rank of L. However, because the estimation on the rank of L is almost infeasible due to the high dimensionality, trace is often used as an intermediate. With the lower bound we proposed in Proposition III.2, empirically from Figure 1 we see that across all three datasets, our sampling method results in subgraphs with larger adjusted \\(\\frac{tr(L)}{n}\\) at almost all sample rates compared with the average of random baselines. This pattern is especially obvious for PubMed.\nGNN training. As for GNN experiments, for each dataset at a given sample rate, we firstly tune the best configuration for our sampling algorithm as shown in the following, then test the same configuration on random baselines with 50 different runs. Specifically, the valid/test accuracy obtained in the training and evaluation process is based on original full- size graph, which means we only limit the training process to the sampled subgraphs. This pipeline is ideal because it simulates the process of training and model selection, which makes the experiments more indicative for general purposes.\nExperiment details. For each dataset and sample rate, we chose hidden dimension in {64,128}, number of layers in {1,2,3}, number of epochs in {200,300}, learning rate and weight decay in {0.001,0.0001}, GCN model type in {GCN, SAGE} with all ReLU activation between all layers, and used Adam as our optimizer. All of the graphs are configured to be undirected by adding edges of the opposite direction before being fed into the networks.\nIt is worth mentioning that the GNN of our choice is rather simple. This is because the power of a sampling method can be more apparent and easier to identify if model complexity is relatively restrained. In Figure 2, where the box plots represent the distribution of random baseline accuracy, the test accuracy acquired by our model(the red dots) are all better by a distinctive margin except for CiteSeer at sample rate 25%. In other cases, for instance, all three datasets at a sample rate of 62.5%, our heuristic won by around 10% of accuracy."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduced a novel graph sampling algo- rithm that leverages feature homophily to efficiently preserve the structural properties of large graphs. Compared with ran- dom sampling, our proposed sampling heuristic is not only more effective in preserving the trace/rank of the graph, but also achieves superior performance when it is used to sample graphs for GNNs trained via transferability. These empirical results indicate the strong potential of our heuristic. Future work will focus on larger graph datasets such as ogbn-mag, where efficient training on subgraphs is even more pressing."}]}