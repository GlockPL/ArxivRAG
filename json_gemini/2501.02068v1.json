{"title": "THE INTERPLAY BETWEEN DOMAIN SPECIALIZATION AND\nMODEL SIZE: A CASE STUDY IN THE LEGAL DOMAIN", "authors": ["Roseval Malaquias Junior", "Ramon Pires", "Thales Sales Almeida", "Kenzo Sakiyama", "Roseli Romero", "Rodrigo Nogueira"], "abstract": "Scaling laws for language models so far focused on finding the compute-optimal model size and\ntoken count for training from scratch. However, achieving this optimal balance requires significant\ncompute resources due to the extensive data demands when training models from randomly-initialized\nweights. Continual pre-training offers a cost-effective alternative, leveraging the compute investment\nfrom pre-trained models to incorporate new knowledge without requiring extensive new data. Recent\nfindings suggest that data quality influences constants in scaling laws, thereby altering the optimal\nparameter-token allocation ratio. Building on this insight, we investigate the interplay between domain\nspecialization and model size during continual pre-training under compute-constrained scenarios. Our\ngoal is to identify a compute-efficient training regime for this scenario and, potentially, detect patterns\nin this interplay that can be generalized across different model sizes and domains. To compare\ngeneral and specialized training, we filtered a web-based dataset to extract legal domain data. We\npre-trained models with 1.5B, 3B, 7B and 14B parameters on both the unfiltered and filtered datasets,\nthen evaluated their performance on legal exams. Results show that as model size increases, the\ncompute-effectiveness gap between specialized and general models widens.", "sections": [{"title": "Introduction", "content": "Recent advancements in Language Models (LMs) have revealed emerging capabilities primarily attributed as a\nphenomena of scale, achieved by training large models on large datasets using a causal language modeling objective [1,\n2, 3, 4]. This paradigm aligns with scaling laws for Transformer LMs, which suggests that model size should scale\nproportionally with training tokens [5]. Hence, in scenarios with high disponibility of compute and data resources,\ntraining a larger model on all available data across multiple epochs becomes advantageous. Under these conditions,\nissues such as token repetition [6] or data quality [7] diminish in significance.\nHowever, given the high costs associated with training LMs [8, 9, 10, 4], there is a growing demand for techniques that\nadd new knowledge into these models without the need for a complete retraining. Continual pre-training leverages\nexisting knowledge of pre-trained LMs through transfer learning, providing a more compute-efficient alternative to\ntraining from randomly-initialized weights. This process employs techniques such as compute-equivalent replay,\nlearning rate re-warming, and re-decaying to minimize the catastrophic forgetting of past knowledge [11].\nWhen LMs are applied within a particular domain, continual pre-training on domain-specific data becomes advantageous\n[12]. This approach enhances the model's performance within the targeted domain while potentially degrading it\nin general contexts, given the forgetting of past knowledge [13]. Despite this trade-off, significant performance\nimprovements have been observed in specialized models on domains such as medicine [14, 15], law [16, 17, 18], and\nprogramming [12, 19, 20]."}, {"title": "Related work", "content": "Recent studies have demonstrated favorable results for continual pre-training, particularly with domain specialization,\nhighlighting it as a cost-efficient alternative across various domains by leveraging previously acquired knowledge.\nAlthough we do not formalize a scaling law in this work, our observations are grounded in established literature, which\nsuggests variations in scaling laws due to slight changes in data characteristics and training scenarios."}, {"title": "Continual pre-training on domain-specific data", "content": "In [12], the Code Llama series models, with 7B, 13B, 34B, and 70B parameters, were developed by continually training\nLlama 2 [9] on code specific data. It was observed that continual pre-training on code data outperforms training"}, {"title": "Scaling laws", "content": "The work by Kaplan et al. [24] proposes a power-law relationship between performance, training data, and model\nparameters. By extending the analysis from Kaplan et al. with a broader range of models, data, and hyperparameter\ntuning, Hoffman et al. [5] found that model size and token quantity scale together to achieve compute optimal training.\nThe Chinchilla-70B, trained on 1.4 trillion tokens, outperforms models with similar compute budgets but less data,\nsupporting their scaling hypothesis. This proposed power-law remains widely accepted in the literature, supporting\nsubsequent studies that explore scaling laws for LMs in different scenarios, including our own.\nThe authors of [25] explore transfer learning under data-constrained regimes through domain specialization with\ncode data, starting from a model pre-trained on text. They propose a power-law for data transfer, demonstrating that\npre-training offers compute benefits by leveraging prior knowledge acquired during the pre-training process. However,\nwith less data constraint, transfer learning can hinder performance due to the ossification of prior knowledge. While\nthis work shares certain limitations with [24], such as the lack of learning rate tuning for each token budget [5], the\nauthors observed how different training regimes can lead to changes on classic scaling laws. Our study explores a\nsimilar scenario, focusing on the interplay between domain-specific and general training as model size increases, rather\nthan limiting the analysis to transfer learning within a single domain.\nScaling laws for Transformer LMs [5] have proven replicable across multiple studies [26, 6, 27, 28, 29]. Variations in\ntraining regimes can influence these scaling laws, as observed in data-constrained settings [6] and scenarios involving\nhigh-quality data [26]. Our study focuses on domain-specific continual pre-training. Existing studies propose power-\nlaws that help determine the compute optimal mixture ratio between general and domain-specific datasets for this\ntype of pre-training [27, 28, 29]. These power-laws offer guidance on optimal mixture ratios for specific regimes,\nsuch as data-constrained domain-specific continual pre-training, given a target general or domain-specific document\nreconstruction loss [29].\nWhile our work shares similarities with [5] and [25], our goal is not to propose or modify existing scaling laws. Instead,\nwe conduct a study to identify a potential new law specific to our regime, guiding future studies. Unlike existing\nwork on scaling laws for domain-specific continual pre-training [27, 28, 29], our study does not focus on finding the\ncompute-optimal dataset mixture ratio. Instead, we examine a scenario in which data is sourced from the web to\ntrain a specialized model. We explore whether to use domain-specific data exclusively or include all extracted data\nin a compute-constrained setting, hypothesizing that large models may learn domain-specific and general knowledge\nwithout significant drawbacks. While previous studies have explored correlations between training and downstream\nloss [30], our study, to the best of our knowledge, is the first to investigate the interplay between model size and domain\nspecialization, focusing on downstream performance."}, {"title": "Methodology", "content": "We extended the pre-training of the Qwen2.5 [31] models at scales of 1.5, 3, 7 and 14 billion parameters to examine how\nmodel size influences domain specialization, assessing both the benefits and limitations of domain-specific training. We\nselected this model family for its multilingual capabilities and availability in multiple pre-trained sizes, focusing on the"}, {"title": "Evaluation", "content": "Typically, works addressing the scaling laws of Transformer LMs use the perplexity metric to measure performance on\ndocument reconstruction tasks from a development dataset partition, examining how model size relates to performance\ngains [24, 27, 25, 5]. However, our focus is on exploring the interplay between model size and domain specialization\non downstream tasks.\nHence, we evaluate models' ability to solve the first phase of the Bar Association Exams (OAB), designed to assess\nknowledge of the Brazilian legal domain in humans. The OAB is the mandatory bar exam required for anyone wishing\nto practice law in Brazil, serving as an entry level assessment of legal knowledge. Each year, more than 300, 000 law\ngraduates take the exam, yet the pass rate hovers between 20% and 45%, underscoring its difficulty."}, {"title": "Results", "content": "Each specialized model achieves lower perplexity with less compute compared to its general counterpart, as shown\nin Figure 2. While there is no clear trend indicating that the perplexity gap between specialized and general models\nincreases with model size, Figure 1 suggests that the compute savings for specialized models grow as model size\nincreases. For instance, the SGER increases from 1.6\u00d7 at 1.5B parameters to 4.3\u00d7 at 14B parameters, as shown in\nFigure 1b. These results indicate that, as the number of trainable parameters grows, specialized models require fewer\ntokens to reach their minimum perplexity. For example, the 1.5B specialized model requires 33.6B tokens, whereas the\n14B model needs only 12.6B tokens, as indicated in Figure 2.\nThis trend is further supported by the two diverging lines in Figure 1a. Each point on the plot represents the minimum\nperplexity achieved by a trained model. We observed a clear power-law by plotting the perplexity at each checkpoint\nagainst the compute cost required during its training, estimated using the approximation in Equation 1."}, {"title": "Limitations", "content": "We aim to generalize the trends identified in our experiments by finding the most efficient training approach between\ndomain specialization and general continual pre-training. However, this remains a case study. Our experiments were\nlimited to a single dataset, and potential noise may exist due to inaccuracies in the legal content classifier used to create\nthe specialized dataset. Thus, we cannot yet confirm that this trend is broadly applicable, as we are restricted to using\nPortuguese legal data sourced from the web.\nOur conclusions in this study are based on four data points for each trend line in both the specialized and general models.\nAlthough we observed a good fit at scales from 1.5B to 14B parameters, ideally, we would have conducted experiments\nusing larger ranges. However, we are constrained by the compute costs associated with the proposed ablation studies.\nUsing smaller models is also not an option in our study, as models with fewer than 1.5B parameters do not perform\nbetter than chance, likely due to the difficulty of our legal benchmarks. Despite these limitations, the strong fit of the\ntrend lines and benchmark results, achieved after the cutoff date for our pre-training dataset, support these findings."}, {"title": "Conclusion", "content": "In this study, we explored the interplay between model size and domain specialization during continual pre-training in a\ncompute-constrained scenario, aiming to identify trends that reveal the compute-optimal training regime. By narrowing\nour scope to domain specialization for a specific web dataset, we trained models of 1.5B, 3B, 7B, and 14B parameters\non both the unfiltered dataset and a filtered version focusing on Portuguese legal content.\nWhen evaluated on Brazilian legal multiple-choice exams, the results revealed a trend: as model size increases, the\ncompute-effectiveness gap between specialized and general models in the target domain increases. Although these\nfindings are based on a small number of instances and are limited by the natural distribution of legal content in our\ntraining dataset, they provide evidence that specialized training can outperform general training in compute-constrained\nscenarios.\nFuture studies could formalize a scaling law specific to the interplay explored in our work, providing additional evidence\nfor the trends we identified. To achieve this, our methodology could be replicated across model sizes, spanning more\norders of magnitude, to strengthen the evidence base for fitting a power-law. Additionally, extending training to\nincorporate datasets from diverse sources would help determine whether this trend persists across different dataset\nmixtures and domains."}], "equations": [{"equation": "C\u22486ND", "number": "1"}, {"equation": "SGER=\\frac{C_{general}}{C_{specialized}}=\\frac{6ND_{general}}{6ND_{specific}}=\\frac{D_{general}}{D_{specific}}", "number": "2"}, {"equation": "Perplexity = 2^{-log_2 P(correct|prompt,question,choices)}", "number": "3"}]}