{"title": "Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations", "authors": ["Varun Dhanraj", "Chris Eliasmith"], "abstract": "Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly tasks that involve precise rule following, as often found in mathematical reasoning tasks. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, allowing for problem-solving within a neurosymbolic vector space. The results are decoded and combined with the original hidden state, boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method improves efficiency, reliability, and interpretability. Our experimental results demonstrate an average of 82.86% lower cross entropy loss and 24.50 times more problems correctly solved on a suite of mathematical reasoning problems compared to chain-of-thought prompting and supervised fine-tuning (LORA), while at the same time not hindering the performance of the LLM on other tasks.", "sections": [{"title": "1. Introduction", "content": "Despite the remarkable progress in deep learning, significant gaps remain between the strengths of deep learning-based models and traditional symbolic reasoning systems. Deep learning excels at intuition and pattern recognition, leveraging large datasets to make flexible, context-aware predictions. However, these models often suffer from issues such as hallucinations and a lack of reliability, especially when solving tasks that require strict rule-following and logical consistency. In contrast, symbolic reasoning methods provide precision and reliability, but they struggle to scale to complex and noisy real-world problems.\nThis dichotomy has fueled a growing interest in merging the strengths of these two paradigms. Many integrated approaches aim to leverage the intuition and adaptability of large language models (LLMs) while incorporating the rigor and interpretability of symbolic reasoning. For example, approaches such as deep learning-guided program synthesis aim to use LLMs to generate complex algorithms by producing code for various candidate programs that could solve abstract reasoning problems. While this approach demonstrates the potential of combining neural network-based pattern recognition with symbolic algorithms for programmatic reasoning, it remains constrained to token-level operations and fails to leverage the richer and more complex information embedded within the LLM's hidden states.\nIn this paper, we introduce a novel method that extends the capabilities of LLMs by encoding their hidden states into structured symbolic representations. Unlike previous work focusing on token-level program synthesis, our approach directly integrates symbolic algorithms within the neural model by running them in a symbolic space derived from the LLM's internal representations. This innovation bridges the gap between neural and symbolic reasoning by extracting inputs from the LLM's hidden state and operating directly on a structured, interpretable representation of the problem.\nOur contributions include:\n\u2022 A Neurosymbolic Method for LLMs: This work represents a first step toward integrating symbolic reasoning into LLMs by using predefined symbolic rules as a proof of concept. We explore the ability of symbolic algorithms to operate within a symbolic space constructed from the LLM's latent representations.\n\u2022 Symbolic Representations from Hidden States: We demonstrate the feasibility of decoding state information from LLM hidden layers into structured, compositional symbolic representations using Vector Symbolic Algebras (VSAs). These representations enable rule-based manipulation of mathematical and logical constructs.\n\u2022 Improved Performance on Rule-Based Tasks: By leveraging neurosymbolic processing, our approach achieves significant improvements in accuracy and interpretability on numerical reasoning tasks, outperforming methods like chain-of-thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning.\nThis work enables symbolic algorithms to run directly within neural networks, laying the groundwork for more advanced neurosymbolic systems that balance the adaptability of LLMs with the reliability of symbolic reasoning. By integrating neurosymbolic algorithms and decoding hidden state information into structured neurosymbolic representations, we aim to unlock new possibilities for solving complex, rule-based problems previously only solvable via symbolic approaches such as program synthesis."}, {"title": "2. Related Work", "content": "2.1. Linear Probes\nLinear probes are widely used tools for interpreting the internal representations of LLMs. They involve training a lightweight, linear mapping from a model's hidden states to specific properties of interest, such as linguistic features or numerical values. By analyzing how well these linear mappings perform, researchers can infer what information is encoded in the model's hidden states. For numerical reasoning, linear probes have been used to represent values by extracting information directly from hidden states.\nPrevious work has extended this approach with digit-specific circular probes, which attempt to decompose numerical representations into their constituent digits using circular algebra. However, such methods generally exhibit lower accuracy compared to traditional linear probes and are limited in scope. Specifically, circular probes can only detect numbers and lack the ability to discern operations or broader semantic relationships.\nIn contrast, the method proposed in this work addresses these limitations by leveraging vector symbolic algebras (VSAs) to encode both numbers and operations. VSA-based representations offer dynamic scalability, allowing new functionality to be integrated without retraining the probe. Our approach is thus particularly well-suited for complex numerical reasoning tasks that require flexible and interpretable encodings.\n2.2. Sparse Autoencoders\nSparse autoencoders (SAEs) are a class of unsupervised learning methods designed to parse high-dimensional data, such as the hidden states (also called activations) of LLMs, into sparse, monosemantic components. These components, often referred to as \"concepts,\u201d are linearly combined to reconstruct the original input data. SAEs have been used to identify which latent features in an LLM are active during specific tasks, enabling researchers to explore the internal representations of the model. Furthermore, SAEs can be used to steer LLMs by selectively amplifying or suppressing certain concepts, providing a powerful tool for interpretability and control.\nDespite these advantages, SAEs face notable limitations. First, the concepts learned by SAEs are not guaranteed to be atomic or aligned with structured representations, such as individual digits in numerical data. This ambiguity makes SAEs less suitable for tasks that require precise decomposition of hidden states. Second, the representations learned by SAEs are probabilistic and emergent, determined during training without external constraints, which complicates their use in symbolic algorithms.\nAdditionally, the concepts extracted by SAEs are typically non-interpretable by default, requiring manual inspection of activations to identify their semantic meaning. While this can provide insights into LLM internals, it is labor-intensive and less systematic than the interpretable symbolic representations proposed in this paper. Finally, SAEs operate in an unsupervised setting, whereas the approach presented here uses supervised learning to enforce specific properties on the learned representations. This trade-off introduces inductive biases but ensures that the resulting encodings are structured and interpretable, facilitating their use in numerical reasoning tasks."}, {"title": "3. Vector Symbolic Algebras", "content": "Vector Symbolic Algebras (VSAs) are a family of algebras for constructing compositional symbol-like representations within a fixed-dimensional vector space. In this work, we use the HRR VSA to interpret the internal representations of LLMs and encode numerical reasoning tasks. VSAs enable the creation of neurosymbolic vectors that represent both data and operations, facilitating compact, interpretable, and algebraically manipulable representations.\nVSAs are characterized by three key operations: bundling, binding, and similarity, which allow for the creation and comparison of compositional representations:\n\u2022 Bundling: Combines multiple vectors to represent a set of elements (implemented as vector addition in HRRs).\n\u2022 Binding: Represents associations between elements (implemented as circular convolution in HRRs).\n\u2022 Similarity: Compares two vectors to determine how closely they match (implemented as the dot product in HRRs).\nThe binding operation, circular convolution, is formally defined as:\n$(x \\otimes y)_i := \\sum_{j=1}^{d}x_jY_{((i\u2212j) \\mod d)+1}, i \\in \\{1,2,...,d\\}.$"}, {"title": "3.1. Encoding Compositional Data", "content": "VSAs allow compositional data to be encoded in a fixed-dimensional vector. For example, to represent a three-digit number, we assign vectors to the digits (0 through 9) and their respective place values (ones, tens, hundreds). Using randomly initialized vectors, we can represent the number 842 as:\nx = hundreds \\otimes 8 + tens \\otimes 4 + ones \\otimes 2.\nThis process generalizes to encode multiple numbers and their relationships. For instance, we encode the query \"What is 842 mod 910?\" as:\nx = n\u2081 \\otimes (hundreds \\otimes 8 + tens \\otimes 4 + ones \\otimes 2)\n+ n\u2082 \\otimes (hundreds \\otimes 9 + tens \\otimes 1 + ones \\otimes 0)\n+ problem_type \\otimes modulo,\nTo incorporate the structure of numbers (i.e., their ordering relations), digits can be encoded systematically. For instance, a digit can be constructed by binding the vector for 1 with itself multiple times, e.g., 3 = 1 \\otimes 1 \\otimes 1. This scales well to higher values as long as we impose the further restriction that the base vector (e.g., 1) is unitary (i.e., all frequency components have a magnitude of 1). Similarly, we construct place values like tens and hundreds as repeated bindings of ones, e.g., tens = ones \\otimes ones. This systematic approach ensures desired numerical relations exist between the neurosymbolic vectors."}, {"title": "3.2. Unbinding and the Pseudo-Inverse", "content": "VSAs support unbinding, which allows specific components of a compositional representation to be queried. For HRRs, unbinding is achieved by binding with the inverse of a neurosymbolic vector. Specifically, we use the pseudo-inverse of a vector y, denoted $y^{\\dagger}$, which is obtained by flipping the order of all but the first element:\n$y^{\\dagger} = (y_1, y_d, y_{d-1}, ..., y_2)$,\nwhere d is the dimensionality of the vector.\nIf z = x \\otimes y, then unbinding z with $y^{\\dagger}$ approximately retrieves x:\n$y^{\\dagger} \\otimes z \\approx x$.\nThe unbinding operation can be used to extract specific components from a neurosymbolic representation. For example, consider the neurosymbolic vector described in (3). If we want to query the hundreds digit of the second number (910), we unbind x with n\u2082 and then with hundreds:\nresult = hundreds^{\\dagger} \\otimes (n\u2082^{\\dagger} \\otimes x).\nThe resulting vector result will have maximum similarity with 9, corresponding to the hundreds digit of the second number."}, {"title": "3.3. Vector Orthogonality and Capacity", "content": "One strength of a VSA-based approach is the ability to work with a large number of roughly orthogonal vectors, which facilitates the construction of complex structured representations. For a d-dimensional vector space, the number of vectors that maintain a similarity below a threshold e scales as:\n$N \\propto exp(\u03b1de^2)$, where \u03b1 is a constant derived from spherical code packing and the Kabatiansky-Levenshtein bound. This relationship is valid when $e \\sim O(1/\\sqrt{d})$, and in this regime, the capacity grows exponentially with d, enabling the representation of highly complex compositional structures.\nBy combining the properties described in this section, VSAs provide a robust framework for encoding and manipulating numerical reasoning representations, offering scalability, compositionality, and interpretability."}, {"title": "4. Methodology", "content": "Our method consists of three stages, which together provide an approach for enhancing the reasoning capabilities of LLMs through neurosymbolic processing. These stages are:\n1. Prompting the LLM with mathematical reasoning problems and gathering the hidden states from the model's layers.\n2. Encoding the gathered hidden states into neurosymbolic VSA representations that capture key features of the reasoning process.\n3. Applying rule-based algorithms to the representations, then decoding the results back into the LLM to generate final solutions.\nNext, we describe the dataset used in this study, before returning to describe each of these stages in more detail."}, {"title": "4.1. Dataset", "content": "The dataset used in this study consists of mathematical problems designed to evaluate the LLM's reasoning capabilities across a range of tasks. Each problem involves two randomly generated integers, both constrained to a maximum of three digits. This constraint ensures that the input numbers can be represented as single tokens in the LLM's vocabulary.\nTo maintain consistency in tokenization and output representation, operations that could produce results exceeding three digits apply a modulo operation to truncate the solution. For instance, the product of 932 and 152 is reduced to the last three digits by computing (932\u22c5152) mod 1000 = 816. Each problem is formatted as a natural language query, such as: \"What is 932 times 152 mod 1000?\"\nThe selected problem types include:"}, {"title": "4.2. Prompting and Gathering Hidden States", "content": "In the first stage of our method, the LLM is presented with mathematical reasoning problems formulated as natural language questions. For each prompt, we extract the hidden state of the most recent token from a designated layer of the LLM, capturing an intermediate representation of the reasoning process.\nFor this study, we use LLaMA 3.1 8B, which features 4096-dimensional hidden state vectors at each of its 32 layers. Each layer consists of a self-attention mechanism, a feed-forward MLP, skip connections, and RMS normalization. Our approach records the hidden states just before they are processed by the selected layer, preserving an unaltered view of the model's internal representations at that stage."}, {"title": "4.3. Encoding Hidden States", "content": "The second stage, after prompting, involves converting the hidden states of the LLM into neurosymbolic vector representations. For this purpose, we train a linear encoder network designed to map the hidden states recorded during the forward pass into neurosymbolic vectors that represent the problem's key components: the two input numbers and the operation type. For problems involving mod 1000 to truncate the final three digits, the 1000 is not represented as an input number, but instead is tied to a problem type (e.g., multiplication problem types will always apply modulo 1000 to the final answer). The symbolic vectors are structured using the framework described in Section 3.1. The encoder is trained using a root mean squared error (RMSE) loss, with the objective of minimizing the difference between the predicted and true symbolic vectors."}, {"title": "4.4. Decoding Neurosymbolic States", "content": "Once the encoder network is trained, a corresponding linear decoder network is trained to reverse this mapping. The decoder network takes symbolic vectors as input, reconstructs the LLM's hidden state, and is optimized to minimize the RMSE loss between the original and reconstructed hidden states. The input dataset for the decoder training is generated by converting hidden states from the LLM into symbolic vectors using the trained (and now frozen) encoder network.\nAfter training, both the encoder and decoder networks are included in the LLM to assist in solving mathematical reasoning problems. The process begins by encoding the hidden state of the designated LLM layer into a neurosymbolic vector. This vector is then queried to determine the problem type, which dictates the selection of an appropriate rule-based Python function. If the queried problem type is not sufficiently similar to any the problem types encountered during training, the decoder is bypassed, and the LLM proceeds with its standard forward pass. Otherwise, the predefined rule-based function is applied to the extracted input values from the neurosymbolic vector, generating a new neurosymbolic representation containing the computed solution. This solution vector is then decoded back into an LLM-compatible hidden state via the decoder network, allowing the model to incorporate the computed result into its forward pass.\nThe output of the decoder is linearly combined with the original hidden state at the intervention layer to form the final hidden state. This linear mixing is performed using a 50-50 ratio, such that the resulting hidden state is:\n$h_{final} = 0.5 \\cdot h_{decoder} + 0.5 \\cdot h_{original}$, where $h_{decoder}$ is the output of the decoder network and $h_{original}$ is the LLM's hidden state at the same layer.\nNote that the layer at which the encoder generates the neurosymbolic vector from the hidden state does not need to be the same layer at which the decoder network uses the solution neurosymbolic vector to impact the hidden state of the LLM. In fact, multiple decoder layers may be trained and used to influence the hidden state of the LLM at different layers using the solution symbolic vector. For simplicity, we only choose layer 17's encoder and decoder network to both generate the neurosymbolic vector of the problem and to apply intervention to the forward pass of the LLM. The reasoning in choosing layer 17 is discussed further in Section 5.1.\nAlthough the decoder networks are pretrained to reconstruct hidden states corresponding to symbolic vectors, their direct use during the LLM's forward pass may disrupt the algorithm being executed by the LLM, leading to degraded performance. This disruption occurs because the pretrained decoder networks map neurosymbolic vectors containing problem solutions directly into the LLM's hidden states. However, the LLM's original forward pass has hidden states that encode the problem inputs rather than the solution. Replacing the hidden states with representations of the solution can interfere with subsequent layers of the LLM, which expect input representations to align with the problem's original structure.\nTo address this issue, the decoder networks are fine-tuned by calculating the cross entropy loss of the logits of the correct token during the LLM's forward pass. This loss measures the discrepancy between the model's predicted output and the expected solution, allowing the decoder networks to adapt their mappings. The fine-tuning process ensures that the modified hidden states generated by the decoder networks not only represent the solution but also align with the LLM's internal expectations, enabling the model to generate correct outputs.\nFine-tuning the decoder layers achieves two objectives:\n(1) It teaches the decoder networks to map solution neurosymbolic vectors into hidden states that align with the LLM's forward-pass expectations.\n(2) It mitigates disruptions to the LLM's computations caused by direct interventions in hidden states, ensuring the model generates correct outputs.\nWithout fine-tuning, decoder outputs may cause the model to deviate from its learned reasoning pathways, leading to errors. By fine-tuning, the decoder networks adapt to the model's computational context, improving overall performance in mathematical reasoning tasks."}, {"title": "4.5. Comparisons to Other Methods", "content": "We compared the performance of our method to two other popular strategies for improving the mathematical reasoning capabilities of LLMs: zero-shot CoT reasoning and supervised fine-tuning via LoRA modules. These methods were selected as baselines because they represent two distinct paradigms: implicit reasoning through prompting and explicit task-specific fine-tuning."}, {"title": "5. Results", "content": "5.1. Encoder and Decoder Performance\nAfter training, the encoder networks achieve RMSE loss curves shown in Figure 2. The results indicate that earlier layers of the LLM are less effective at encoding the problem into symbolic vectors due to a lack of global context. As the hidden states progress through more layers, the self-attention mechanism provides increasing amounts of contextual information, improving the encoder's performance. The RMSE loss reaches its minimum at layer 17, suggesting that this layer optimally encodes the problem's symbolic structure.\nHowever, at layers deeper than 17, the RMSE loss increases. We believe that this phenomenon can be attributed to the cumulative effects of residual connections and RMS normalization applied in the LLM. As described in the equations below, the residual connections repeatedly add outputs from earlier layers to the hidden state:\n$h_{n+1} = f_n(h_n) + h_n$,\n$h_L = h_0 + \\sum_{n=1}^L f_n(h_{n-1})$,\nwhere $h_n$ represents the hidden state at layer n, and $f_n$ denotes the non-linear transformation applied at each layer. At deeper layers, the hidden state becomes a mixture of earlier representations and intermediate computations, making the problem information less prominent for encoding.\nAs shown in Figure 2, the reconstruction loss of the decoder networks monotonically increase with layer depth. We believe that this trend reflects the increasing complexity of hidden states at deeper layers, as they incorporate non-linear transformations from previous layers. Because decoder networks are linear, they struggle to reconstruct the intricate structure of hidden states in deeper layers, resulting in higher RMSE losses.\nThe decision to use layer 17's encoder and decoder networks is based on the encoder evaluation results, which indicate that layer 17 minimizes RMSE loss for symbolic vector encoding. Although decoder interventions could be applied at multiple layers, restricting the intervention to layer 17 simplifies the experimental setup while leveraging the layer's optimal encoding performance."}, {"title": "5.2. Overall System Performance", "content": "The performance of the proposed Neurosymbolic LLM (NS LLM) is compared against three baseline models: a Standard LLM, a CoT reasoning LLM, and a LoRA fine-tuned LLM. Two metrics are used to evaluate model performance across various mathematical problem types:\n\u2022 Score (%\u2191): The percentage of problems for which the model provides the correct answer with the highest probability.\n\u2022 Loss (\u2193): The categorical cross-entropy loss per problem, representing the negative log probability of the correct answer tokens.\nTable 1 presents the detailed results. Key observations are summarized below.\nNeurosymbolic LLM Performance\nThe Neurosymbolic LLM outperforms all baseline models across all trained problem types (i.e., all problem types except addition and integer division), achieving significantly higher scores and lower losses. For most problems, the loss is significantly reduced relative to the Standard LLM, and the accuracy approaches 100%.\nHowever, on more complex tasks, such as LCM and square modulo, performance is slightly lower. This may be due to the complexity of the underlying forward-pass algorithm required for these problems, which makes applying interventions via a single decoder network more challenging. A potential improvement could involve using multiple decoder networks to insert neurosymbolic information at different stages of the forward pass, enabling more precise alignment with the LLM's internal computations.\nAnother reason for the reduction in scores is the encoding error rate, which is the percentage of misclassified input digits. As shown in Figure 3, at layer 17, the errors for the hundreds, tens, and ones digit places are 1.5%, 2.0%, and 1.0%, respectively. Errors in generating the correct neurosymbolic representation of the input problems will result in an incorrect solution neurosymbolic vector, which increases the likelihood that the LLM outputs an incorrect response."}, {"title": "Discussion", "content": "Our results highlight the following:\n\u2022 The Neurosymbolic LLM outperforms all other models on trained problems, while also not sacrificing performance on testing problems.\n\u2022 The Standard LLM performs well on simpler tasks but struggles with problems requiring intermediate reasoning or symbolic representation. The Standard LLM has a 85.59% higher loss and a 78.57 times lower score than the Neurosymbolic LLM\n\u2022 The CoT LLM's reliance on multi-step reasoning introduces opportunities for errors, particularly in tasks involving non-trivial intermediate computations. The Standard LLM has an 89.08% higher loss and a 28.36 times lower score than the Neurosymbolic LLM\n\u2022 The LORA LLM's inability to generalize to unseen tasks underscores the advantage of neurosymbolic encoding for maintaining task flexibility. The Standard LLM has a 76.65% higher loss and a 20.65 times lower score than the Neurosymbolic LLM\nThese findings validate the utility of neurosymbolic encoding as a useful tool for enhancing the reasoning capabilities of LLMs, particularly in domains where precision and rule-following are required, while also providing insights into the model's internal representations by converting hidden states into interpretable and compositional symbolic vectors."}, {"title": "6. Conclusion", "content": "We introduce a neurosymbolic method that bridges the strengths of LLMs and symbolic reasoning systems to address challenges in rule-based reasoning tasks. By encoding LLM hidden states into symbolic representations, solving problems in a symbolic domain, and merging solutions back into the LLM, our approach achieves significant improvements in mathematical reasoning tasks. Experimental results demonstrate superior accuracy and reliability compared to traditional methods like CoT reasoning and fine-tuning with LoRA modules.\nOur method not only enhances task performance but also fosters greater interpretability, providing insights into the internal representations of LLMs. Moreover, by leveraging neurosymbolic representations capable of encoding complex and structured data, our method has the potential to scale across a broad range of reasoning tasks. These results highlight the potential of neurosymbolic integration as a useful approach to enhancing the reasoning capabilities of LLMs, enabling them to solve problems with the robustness and precision previously achievable only by symbolic AI systems."}, {"title": "A. Appendix", "content": "A. Determining Problem Types and Intervention Thresholds\nAs discussed in Section 4.4, after the encoder generates the neurosymbolic vector corresponding to a given LLM prompt, in order to determine which program to execute, the problem type is extracted as: result = x \\otimes problem_type^{\\dagger}, where x is defined in equation 3.\nFor problems seen during training, we expect that result will be approximately equal to a problem type seen during training, since one of the encoders purposes is to represent the correct problem type in it's neurosymbolic vector output. For problems not seen during training, the expected behavior is that result should be dissimilar to all problem types seen during training. This fact allows us to prevent the neurosymbolic system from intervening on untrained problems, which allows us to benefit from improved performance on trained problem types while not sacrificing performance on untrained problem types.\nFor example, if the LLM is asked \"What is 920 mod 895?\", the neurosymbolic vector generated by the encoder is queried for it's problem type, and the dot product of this vector is taken with the neurosymbolic vector representing every problem type. For this problem, the various dot product similarities are shown in table 2. The table shows that the Modulo problem type has the highest similarity to the problem type queried from our neurosymbolic vector, which means that the system will use the program corresponding to modulo to generate the solution neurosymbolic vector.\nFor unseen problems, such as integer division, table 3 shows that the dot product similarities across different trained problem types are all lower than the maximum dot product similarity when the LLM is queried with modulo (a trained problem). The queried problem type is most similar to the modulo problem type vector, which suggests that the algorithm the LLM is executing for integer division is more similar to the algorithm the LLM is executing for modulo division than any of the other trained problem types. Intuitively, this makes sense, since both modulo and integer division rely on division operations to compute their respective results, making their underlying computational processes more similar than those of other trained problem types."}, {"title": "B. Decoder Fine Tuning", "content": "As mentioned in Section 4.4, the decoder network requires fine tuning to properly enhance the performance of the LLM on the rule-based reasoning tasks. This is because the decoder needs to learn how to insert information about the solution to the task into the LLMs forward pass in a way that is both effective and non-disruptive. Figures 5(a) and 5(b) illustrate that as fine-tuning progresses, both cross-entropy loss decreases and task performance improves, highlighting the importance of optimizing the decoder within the LLM context to maximize performance."}, {"title": "C. Error Analysis of Chain-of-Thought Reasoning", "content": "One interesting result of Section 5 is that for certain problem types, CoT prompting performs worse than not using CoT. To understand why, in this section we show a few common causes of error when prompting the model to use CoT reasoning, and how those relate to the strengths of LLMs as probabilistic"}, {"title": "C.1. Representation Errors", "content": "One source of errors is when the LLM incorrectly represents the input numbers, and consequently gets the wrong final answer. In the below example, we query the LLM with \"Solve the following problem step by step: What is 601 plus 106\". We can see that in the LLMs CoT response, it says 601 has 6 hundreds and 1 tens and 1 ones, which is clearly incorrect, which leads to the LLM getting the wrong final answer.\nIn contrast, if the LLM was asked an addition problem without being prompted to provide a chain of thought, it would be extremely likely to return the correct answer, as shown in Table 1. This demonstrates the LLM's strength in pattern-matching tasks while underscoring its limitations in program synthesis and strict logical reasoning."}, {"title": "C.2. Intermediate Step Errors", "content": "The CoT approach also underperforms the standard LLM in more complicated tasks, such as bitwise OR as well. One reason for this is because these problems require multiple rules or algorithms being applied, which increase the opportunity for the LLM to make an error. For example, if we query the LLM with \"Solve the following problem step by step: What is 513 OR 107\", the first step the LLM executes is to convert each number into binary. Afterwards, it will perform the bitwise OR on the bitstrings for each number, and convert the resultant bitstring back into a decimal number. The below example showcases a situation where the LLM incorrectly converts one of the input numbers from decimal into binary (513 is 1000000001, not 1000001001). This causes the final solution of the LLM using CoT to be incorrect.\nDespite bitwise OR being a difficult problem with multiple steps, the standard LLM (which produces an answer in a single forward pass) outperforms CoT prompting on this problem type. This again highlights the ability of LLMs to provide surprisingly accurate intuitions for complicated problems, while also showing that they struggle with executing complex algorithms to reliably solve these problems."}, {"title": "C.3. Stuck in Infinite Loops", "content": "Another source of errors when using CoT is the LLM being stuck in an endless cycle. In the below example, we query the LLM with \"Solve the following problem step by step: What is 661 plus 420\". The LLM starts with the correct procedure, but keeps applying the procedure to digits that are not in the original problem. This process repeats endlessly, resulting in the LLM failing to respond with the correct answer."}]}