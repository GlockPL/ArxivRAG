{"title": "Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers", "authors": ["Stephen Hausler", "Peyman Moghadam"], "abstract": "In this work we propose a novel joint training method for Visual Place Recognition (VPR), which simultaneously learns a global descriptor and a pair classifier for re-ranking. The pair classifier can predict whether a given pair of images are from the same place or not. The network only comprises Vision Transformer components for both the encoder and the pair classifier, and both components are trained using their respective class tokens. In existing VPR methods, typically the network is initialized using pre-trained weights from a generic image dataset such as ImageNet. In this work we propose an alternative pre-training strategy, by using Siamese Masked Image Modelling as a pre-training task. We propose a Place-aware image sampling procedure from a collection of large VPR datasets for pre-training our model, to learn visual features tuned specifically for VPR. By re-using the Mask Image Modelling encoder and decoder weights in the second stage of training, Pair-VPR can achieve state-of-the-art VPR performance across five benchmark datasets with a ViT-B encoder, along with further improvements in localization recall with larger encoders. The Pair-VPR website is: https://csiro-robotics.github.io/Pair-VPR", "sections": [{"title": "I. INTRODUCTION", "content": "The Place Recognition (PR) task involves associating input sensor data (e.g., lidar [1], [2], radar [3], [4], or vision [5], [6]), with a global map or database of places previously visited within an environment. It is an essential component in many applications such as driverless cars, autonomous robots, and augmented reality. In the field of learning-based Visual Place Recognition (VPR) [6], [7], it is a standard approach to start with a pre-trained neural network, such as VGG16 or ResNet, leveraging their initial architecture and weights for the VPR task. These networks are often pre-trained on large diverse datasets like ImageNet, enabling them to extract robust features from visual data. Following this, it is common to incorporate a feature aggregation layer such as VLAD [8], GeM [9], Conv-AP [10], SoP [2] among others - to pool local features into a compact global descriptor vector efficiently representing the original image. These vectors can then be compared with an embedding distance metric such as the Euclidean distance, with the smallest distance noting the pair of most similar images. More recent VPR techniques often include multiple stages of retrieval, where subsequent stages of retrieval are used to re-rank an initial set of candidate place recognition matches [11]-[14].\nIn this work, we develop Pair-VPR, a transformer-based VPR method trained on diverse VPR datasets, which achieves state-of-the-art visual place recognition performance across a range of challenging VPR benchmarks. We achieve this by proposing a two-stage training pipeline to train a pair-classifier, which can decide whether a given pair of images is from the same place or not. In the first stage, we pre-train a transformer encoder and decoder using siamese mask image modelling [15], [16], with place-aware image sampling. We sample pairs of images from different places in the world, ensuring that these pairs contain both spatial and temporal differences. Our places are curated from three existing large-scale open source datasets (SF-XL [17], GSV-Cities [10] and Google Landmarks v2 [18]) with a total of 3.43 million panoramic images and 2.08 million egocentric images, across diverse locations (planet-wide) and diverse times.\nIn the second stage, we re-use both the encoder and decoder, by jointly learning to produce a global descriptor from the encoder and using the decoder as a pair classification network. The pair classifier produces a similarity score denoting whether a given pair of images were captured from the same location, or not. A diagram showing our network architecture and training recipe is shown in Figure 1. The Pair-VPR network uses a vision transformer setup, with ViT blocks in both the encoder and decoder. We leverage the class token output to supervise the network, with a low-dimensional global descriptor produced from an encoder class token and a pair similarity score from decoder class tokens. The network is then trained using a Multi-Similarity loss for the global descriptor and a Binary Cross-Entropy loss for the pair classifier, with online triplet mining. We benchmark the trained network on existing VPR benchmark datasets and observe state-of-the-art Recall@1 on all tested datasets with a ViT-B encoder, such as an improvement in the Recall@1 on the Tokyo24/7 dataset from 95% to 98%. Moreover, we show that our proposed method can be extended to much larger vision transformer encoders (ViT-L, ViT-G) and achieve a new benchmark result of Recall@1 on Tokyo24/7 of 100% with our best performing configuration.\nIn summary, this paper introduces a joint training for learning a Vision Transformer (ViT) for visual place recognition task. Unlike the existing methods for using a ViT for VPR, our method proposes:\n\u2022 A novel joint training method designed for Vision Transformers that simultaneously learns a global descriptor and a contrastive pair classifier for re-ranking, which can predict whether a given pair of images are from the same place or not. We propose training both components using the class tokens from a ViT.\n\u2022 We show that a pre-trained encoder and decoder from a Mask Image Modelling pre-training task can both be utilized for the VPR task, by using the encoder to generate a global descriptor and the decoder as a pair classifier.\n\u2022 We propose a training data generation strategy for Siamese Mask Image Modelling by sampling pairs of images from places in existing large-scale VPR and image retrieval datasets."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Visual Place Recognition", "content": "Compared to the general task of Place Recognition, VPR focuses only on the image modality and this allows for VPR to be considered as an image retrieval problem. This formulation in conjunction with the advent of deep learning led to a number of high performing learning-based VPR solutions. Initially, pre-trained neural networks were applied to VPR, then NetVLAD was developed which utilized triplet loss to train a neural network specifically for the place recognition task [6]. Numerous subsequent works expanded upon NetVLAD, including works that relax the triplet loss to use soft positives and negatives [19], quadruplets rather than triplets [20], a generalized contrastive loss [21], and aggregates features over multiple spatial scales [22]. Other learning-based methods include those which consider place recognition as a classification problem [7], [17] or use a multi-similarity loss [23].\nThese earlier works all follow the same approach, of constructing a global descriptor, i.e., a single vector representing an image for nearest neighbour retrieval. However, the top candidate found using global descriptor matching is not always correct. To avoid such limitations, multi-stage VPR algorithms propose to re-rank a collection of top candidates.\nThese re-ranking methods involve re-ordering an initial set of retrieved places such that the correct place match is ranked first in the retrieved list. Learning-based re-ranking methods began with less computationally efficient algorithms that used geometric verification densely (without keypoint selection) [12], then subsequently evolved to include keypoint/keypatch selection algorithms [13], [24]. More recently, R2Former [25] went one step further by removing explicit geometric verification altogether. Instead, they treat re-ranking as a classification problem and use a small two-block transformer for selecting the best candidate image. Similarly, GeoAdapt [26] also learns a classifier except instead for the task of classifying the similarity of two point clouds for LiDAR Place Recognition. In our proposed approach, we show that a re-ranking classification transformer can be designed using standard self and cross-attention blocks and scaled up to over 80 million parameters, by leveraging a VPR-specific pre-training recipe."}, {"title": "B. Vision Transformers and Self-Supervised Learning", "content": "Compared to Convolutional Networks (CNNs), Vision Transformers [27] have no inductive bias therefore they can learn global dependencies between image features without being limited by a specific kernel size; however, as a result, larger image datasets are needed to train transformer models. This led to the concept of self-supervised learning (SSL) techniques for pre-training these large transformer models on large quantities of unlabeled data. These techniques include deep metric learning methods such as SimCLR [28], BYOL [29], and DINO [30]. The recent work DINOv2 [31] provided further improvements to the pre-training recipe and in this work, we leverage the robust learnt features they provide. In contrast to metric learning methods, an alternate self-supervised approach is called Masked Image Modelling (MIM) and will be explained in the following subsection."}, {"title": "C. Masked Image Modeling (MIM) and Siamese MIM", "content": "Masked Image Modeling is a SSL technique where a neural network is tasked with reconstructing a partially masked image, where the image is divided into a set of non-overlapping patches. Initially, this reconstruction was performed on a set of transformer tokens produced from a raw image [32]. Subsequently, MAE [33] demonstrated that instead the reconstruction could be performed directly on the raw pixel values, based on the mean squared pixel error between the original and reconstructed image. Further works then expanded upon this concept via improvements such as distillation (iBOT [34]), continuous masking (ROPIM [35]), and cross-view completion (CroCo [16]). In CroCo, pairs of images are provided to the network where one image is masked and a second image, showing the same scene, is provided to the network without any masking. They showed that this pre-training recipe is ideal for 3D vision tasks. In a concurrent work, a similar architecture was proposed for learning from video data, called Siamese Masked Autoencoders [15]. In our approach, we also provide pairs of images to a Mask Image Modelling network, except we propose a Place-aware training methodology for VPR."}, {"title": "III. METHODODOLOGY", "content": "Our proposed Visual Place Recognition (VPR) solution is a two-stage place recognition method, which first uses a global descriptor to generate a list of potential place matches, then uses a second stage to refine these matches to improve the likelihood that the highest ranked match is correct. For the first time, we propose a two-stage training methodology using a mask image modelling pre-training designed specifically for VPR. Then we propose a second stage approach where we learn to generate both a global descriptor and a pair classifier for VPR.\nDuring the first stage we provide pairs of images to the network and then heavily mask one of these images. The training objective is to reconstruct the masked image, using a network that has both encoder and decoder components. In the second stage we jointly optimize both the encoder and decoder for VPR, generating a global descriptor from the class token of the encoder and the decoder is trained to predict whether a given pair of images is a positive or negative pair.\nAfter training, the network can then be used as a two-stage VPR method. The encoder is used to produce low dimensional global descriptors which are then used for nearest neighbour searching through a VPR database. Then the decoder is used to decide which potential database candidate is the best match for the current query by passing (query, database) pairs to the decoder. An overview of our pre-training and fine-tuning procedures are provided in Figure 1."}, {"title": "A. Stage One: Place-Aware Masked Image Modelling", "content": "In the first stage of training we use the Siamese Masked Autoencoder [15] design to train the Pair-VPR network. In this approach, pairs of images are input into the network and one of the images is heavily masked. The network is then able to leverage the visual information in the second, unmasked image to aid in reconstructing the masked first image. In previous works, generating pairs is achieved either by sampling random frames in a video sequence [15], or by sampling two different viewpoints of a scene [16]. In our approach we propose a location sampling strategy, such that pairs are sampled from specific locations and, in VPR terminology, are always collected from the set of positives from a given place.\nNetwork: Our network comprises a shared encoder that converts a first image $I_a$ (masked) and a second image $I_b$ (unmasked) into latent representations. We use a Vision Transformer [27] encoder and convert each image into a collection of non-overlapping patches, which are then converted to a set of tokens. For the masked image, we replace the majority of image tokens with mask tokens, which are learnt parameters but have no information received from the original image. After the encoder, we pass the set of encoded patch embeddings (excluding any class tokens) from both images through a decoder to reconstruct the masked image. Our decoder consists of another vision transformer except with alternating self-attention and cross-attention layers, with self-attention between tokens from $I_a$ and cross-attention between tokens from $I_a$ and $I_b$. For efficiency we always use Flash Attention [36]. After a number of decoder blocks, we pass the features through a final FC layer to produce reconstructed pixel values per token (i.e., per image patch).\nLoss: The network is trained using a reconstruction loss between the predicted and ground truth (unmasked) images, by minimizing the Mean Squared Error between predicted and true pixel values. The loss function is expressed as below:\n$L (I_a, I_b) = \\frac{1}{||P_{a}/P_{a}||} \\sum_{i \\in P_{a}/P_{a}} (p_{a_i} - \\hat{p}_{a_i})^2,$ (1)\nwhere $I_a$ and $I_b$ denote the two input images, $p_{a_i}$ denotes the ground truth value for a masked pixel at index $i$ from image $I_a$, $\\hat{p}_{a_i}$ denotes the predicted pixel value and $P_{a}/P_{a}$ is the subset of masked pixels from the masked image. The loss function is only calculated for any pixels that have been masked, where the mask ratio is a hyperparameter of the network. Before calculating the loss we normalize each patch by the mean and standard deviation of that patch as per prior work [33].\nTraining: Our network is pre-trained in a Place-aware fashion, where each iteration is a defined physical location in the world. Then pairs of images are sampled from this place, with dataset-specific sampling to ensure that sampled pairs have some viewpoint consistency; it would be almost impossible for the network to reconstruct an image using a second image facing the opposite direction. Our training data can theoretically be drawn from any collection of places in the world, however, in this work we limit ourselves to a set of"}, {"title": "B. Stage Two: Contrastive Pair Classification for VPR", "content": "In the second stage, the encoder is trained to generate global descriptors for retrieval and the decoder is trained to predict whether a given pair of images is similar or not. We used the pre-trained weights from stage one of Pair-VPR, loading weights from both the encoder and the mask image modelling decoder. We then jointly train the encoder and decoder for both global retrieval and pair classification simultaneously. Finally, we use class tokens from both the encoder and decoder as inputs to our VPR loss.\nNetwork: The stage two network architecture is an extension to the stage one network for the VPR task. We add a linear layer to project from the class token in the encoder to a smaller dimension, then use an L2 norm to generate a global descriptor. We then convert the Mask Image Modelling decoder into a pair classifier by adding a new class token and add a two-layer MLP to the output of this class token to produce a scalar value denoting the similarity between a pair of images. The network architecture of our Pair-VPR is shown in Figure 1.\nLoss: We use a contrastive loss to train the encoder for the global descriptor and a Binary Cross Entropy (BCE) loss to train the decoder for pair classification. We use online mining and use a Multi-Similarity Miner [37] to select positives, negatives and anchors before using a Multi-Similarity Loss [38] for the global descriptor, with the loss function shown below:\n$L_g = \\frac{1}{N} \\sum_{i=1}^{N} [log( 1 + \\frac{1}{\\alpha} \\sum_{k \\in P_i}e^{-\\alpha(S_{ik}-m)}) + log(1+ \\frac{1}{\\beta} \\sum_{k \\in N_i} e^{\\beta(S_{ik}-m)})],$ (2)\nwhere N is the batch size and $S_{ik}$ is the similarity value between the index i in the batch and a positive/negative index k. We keep the hyperparameters $\\alpha, \\beta,m$ the same as used in [10].\nDuring the forward pass, we store both the global descriptor and the dense features from the encoder, and use the dense features as input into the decoder. As the decoder requires an image pair, the challenge is to provide examples to the network of both positives (two images of the same place) and negatives (two images from different places). To prevent the network from converging to the trivial solution, we use an Online Hardest Batch Negative Miner [37] to use the hardest positives, anchors and negatives in a given batch.\nWe then pass into the decoder sets of (anchor, positive) pairs and (anchor, negative) pairs to produce a list of scalar values per pair, where a large scalar value denotes a high similarity between a given pair. After concatenating both sets, we use a BCE loss (with a sigmoid function) to optimize the network with positive pairs having a target value of 1 and negative pairs having a target value of 0:\n$L_p = - \\frac{1}{N} \\sum_{n=1}^{N} (y_n log s_n + (1 - y_n) log(1-s_n)),$ (3)\nwhere $y_n$ is the target for the current pair $(I_a, I_b)$ and $s_n$ is the similarity output for the pair for a batch size of N pairs. We then train the network jointly, with the full loss shown below:\n$L = L_g + wL_p,$ (4)\nwhere $L_g$ is the global loss trained using a Multi-Similarity loss and miner [23], [37] and $L_p$ is the BCE pair loss. w is a hyperparameter to balance the two loss terms."}, {"title": "C. Using Pair-VPR during Inference", "content": "Once Pair-VPR is trained, VPR is performed using a two-stage process. First, all images are passed through the encoder and global descriptors are generated. These global descriptors are then used to find the top N database candidates for each query. We also save dense features for second-stage refinement - for an image input of size 322 by 322 pixels, these have a size of 529 by 768 with a ViT-B encoder.\nIn the second stage, for each query, we copy the dense features by N in order to create batches of (query, databasej) pairs, where j is the database index and there are N pairs in total. Because our decoder is not symmetric (self-attention is only performed on the first image in a pair), we pass into the decoder batches of both (query, databasej) and (databasej, query) pairs and then sum their respective scores together. The highest-scoring pair is then considered the best match for the current query. Figure 2 shows a diagram of our network during evaluation."}, {"title": "IV. IMPLEMENTATION DETAILS", "content": ""}, {"title": "A. Stage One Training", "content": "We begin by pre-initialising the ViT encoder with weights from DINOv2 [31]. We found that leveraging the diverse pre-training policy used in DINOv2 improved performance over random initialization (please refer to ablation studies in Table II). We then freeze the first half of the encoder blocks (e.g., six blocks with ViT-B) and train the second half using Place-Aware Masked Image Modelling. We train in a Place-aware fashion and construct a dataloader such that a single item in a batch is a single place, where an item comprises a pair of images. We selected three existing large open source datasets for stage one training: SF-XL [17], GSV-Cities [10], and Google Landmarks v2 [18].\nSF-XL: We follow the procedure described in EigenPlaces [39] and divide the dataset into M \u00d7 M meter-sized cells based on the UTM coordinates of all panoramic images in the dataset, generating a total of C cells. Considering the i-th cell $C_i$, we follow the approach in EigenPlaces of computing the Singular Value Decomposition (SVD) from the UTM coordinates of all images in this cell, along with the mean UTM coordinate $\\mu_i$ for this cell. We then select pairs of panoramic images randomly from each cell. Given cell $C_i$, we begin by calculating a random focal point (e.g., a target UTM coordinate) that we want our pairs to observe, anchored to the first principle component of the cell denoted as $V_o$. Our formula for calculating a focal point is given below:\n$f_i = \\mu_i + D_i \\times R (V_o, O_i),$ (5)\nwhere $O_i$ is a random observation angle, and $D_i$ is a random focal length. At each iteration, we randomly sample observation angles $O_i$ between 0 and 360 degrees and apply this random rotation to the eigenvector $V_o$. We then randomly sample different focal lengths $D_i$, sampling between 10 and 20 meters away from the mean coordinate $\\mu_i$. This approach differs from the method in EigenPlaces, which only utilized the 0 and 90 degree observation angles and a fixed focal length. Then given a focal point $f_i$, we produce crops from two randomly sampled panoramic images within cell $C_i$, such that the cropped views are focused on the target focal point - this ensures that our pair of images contains overlapping visual information, without requiring any manual curation of pairs. We can then calculate a viewing angle between a given panoramic image j and a focal point using their respective UTM coordinates:\n$\\alpha_j = arctan( \\frac{\\frac{ef - e_j}{nf - N_j}}{r} ),$ (6)\nGiven this viewing angle, we select a 512 by 512 pixel crop from a 3328 by 512 pixel panoramic image. However, pairs can be selected that are either too easy (small viewpoint difference), or too hard (too much viewpoint difference). Therefore, we calculate the difference in angles between our two sampled crops:\n$\\theta_i = \\angle \\alpha_1 - \\alpha_2.$ (7)\nThen we check if $\\theta_i$ is between a low and high threshold 3\u00b0and 50\u00b0. If not, we resample until we return a pair of crops within the required range.\nGSV-Cities: We randomly sample a pair of images from each place in the GSV-Cities dataset, excluding places with less than 4 images available.\nGLDv2: To improve the data diversity, we also added the Google Landmarks dataset [18]. We only use the cleaned subset of the dataset to avoid damaging the network's ability to learn by providing ambiguous image pairs. Additionally, we also exclude any landmarks with less than 4 images. We consider a landmark as a proxy for a place and treat the dataset in the same format as SF-XL and GSV-Cities. In total, we are left with 72, 322 landmarks for training after cleaning and filtering.\nTraining Summary: We merge the three aforementioned datasets to produce a total of 266809 places per epoch, which are sampled from a set of 3.43 million panoramic images and 2.08 million egocentric images. We train using a mask ratio of 90 percent with a ViT-B sized decoder for 500 epochs with a learning rate of 2e-4 for a batch size of 512 places with AdamW and a cosine LR scheduler. We train using square images of size 224 pixels. By default, we use a ViT-B encoder with 4 register tokens, however, we note that our approach can also work with larger encoder sizes. Hyperparameters for larger encoders are kept the same except we use 1000 epochs."}, {"title": "B. Stage Two Training", "content": "We continue training our model using the GSV-Cities dataset [10] following training recipe used in prior works such as MixVPR [23] and SALAD [40]. We train for 10 epochs using a linear LR scheduler with an initial LR of 8e-5, with a weight decay of 5e-2 and a batch size of 100 places. We also use square images of size 322, and freeze all encoder layers except the last six. Our global descriptor has only 512 dimensions and we heuristically set w (the loss balance term) to 2. We use MSLS-Val as our validation dataset and take the checkpoint with the highest second-stage Recall@1 as the final trained model."}, {"title": "C. Evaluation", "content": "We evaluate our method on five commonly used benchmark VPR datasets, with a diverse set of environments. The datasets are: MSLS validation set [41], MSLS challenge set [41], Pittsburgh30k [42], Tokyo247 [43] and Nordland [44] (note that we use the split of Nordland from VPRBench [45], with 2760 query images). During the evaluation, we resize all images to 322 \u00d7 322 resolution. By default, Pair-VPR uses a ViT-B encoder and uses the top 100 candidates after global descriptor matching - we refer to this as the Speed configuration of Pair-VPR (Pair-VPR-s). We also provide a Performance version of Pair-VPR, which has a ViT-G encoder and uses the top 500 candidates during pair-classification (Pair-VPR-p). We evaluate using the standard Recall@N metric [6], [17] with N\u2208 1, 5, 10."}, {"title": "V. RESULTS AND DISCUSSION", "content": ""}, {"title": "A. Comparison to Recent VPR Methods", "content": "We begin by comparing the performance of Pair-VPR against other State-Of-The-Art (SOTA) VPR methods on a range of benchmark datasets, and we provide both quantitative and qualitative results in Table I and Figure 3 respectively. Comparing our method Pair-VPR-s to the other methods we benchmark, we observe that Pair-VPR achieves the highest Recall@1 on all five datasets. We observe that our transformer-based re-ranking network is particularly adapt at improving the Recall@1 over prior works that also use a vision transformer backbone (SALAD [40] and SelaVPR [11]). Comparing against R2Former [25], which is a prior SOTA method which also used a re-ranking/pair-classifier transformer, we observe a significant jump in performance especially on the Tokyo dataset.\nWhen we scale up our method to use larger encoder sizes and more top candidates, we observe that Pair-VPR-p provides a large performance increase over Pair-VPR-s and achieves the highest recall across all recall@N values compared to prior works. We especially highlight the results on Tokyo24/7, where we have achieved a Recall@1 of 100%."}, {"title": "B. Pre-training Ablation Study", "content": "To understand which aspects of our training strategy are essential for the performance of Pair-VPR, we conducted an ablation study over different variations of stage one training (Table II). In the first row, we show the results when we don't perform stage one training at all, with the decoder in stage two initialized using random weights. We observed that pre-training the decoder network via stage one training is essential to achieve effective VPR performance as the pair-classification task is non-trivial, the place-aware siamese mask image modelling task provides an initial set of weights that are already tuned for comparing features between two images.\nIn the second row of Table 2, we investigated the importance of place-aware sampling by removing the place sampling during stage one training, and instead used strong augmentation to generate the second unmasked image. We observed a consistent drop in recall, even though the total collection of training images is kept identical.\nIn row three, we experimented with unfreezing all blocks in our Dinov2 initialized ViT encoder. We found that allowing the entire network to be trained reduced the VPR performance and we hypothesise that this is because the DINOv2 network was originally trained on a larger and more diverse dataset than ours, and maintaining the low level (e.g., color) learnt features from a more diverse dataset improves performance.\nIn the fourth row we instead initialized the ViT encoder with random weights and performed stage one training from scratch. We found that the VPR performance is still maintained well on urban datasets, but reduces significantly on non-urban datasets like Nordland. This is likely due to the urban bias in common VPR pre-training datasets.\nIn the bottom four rows, we compared different configurations of Pair-VPR. We observed that the performance of Pair-VPR increases as we increase the encoder size. We further observed that increasing the number of top candidates passed to the pair classification component increases the recall, as it reduces the performance limitation imposed by the effectiveness of the global descriptor."}, {"title": "C. Performance of the Pair-VPR Global Descriptor", "content": "In Table III, we analyse the performance of the Pair-VPR global descriptor. The performance of the second stage refinement is always limited by the recall of the global descriptor at the chosen number of top candidates, therefore we display Recall@N values for N \u2208 1,100, 500. The global descriptor is always 512 dimensions, allowing for computationally efficient database searching while also having a Recall@1 comparable to existing global descriptor VPR methods. We also note that our global descriptor is a lot smaller than the single-stage VPR methods in Table I, e.g., CosPlace, MixVPR and SALAD have 2048, 4096 and 8448 dimensions respectively.\nWhen we shift to using larger encoder sizes, we always maintain the same global descriptor by increasing the projection ratio from the class token. For a ViT-G encoder, the class token has 1530 dimensions and we project down to 512 dimensions using a linear layer. We do this in order to keep the computational cost of descriptor matching the same. We find that the ViT-G global descriptor performs similar to the ViT-B descriptor, except on Tokyo24/7. It is possible that the large projection ratio is not ideal for VPR, therefore future work should investigate experimenting with larger global descriptor sizes, albeit at an increased compute requirement."}, {"title": "D. Compute Analysis", "content": "In this subsection we discuss the computational requirements of Pair-VPR and compare against other two-stage VPR methods using the same compute platform (a server computer limited to using a single GPU), as shown in Table IV. It can be seen that Pair-VPR-s is fast at encoding and matching, taking 7 milliseconds per query image to encode and only 0.13 ms/query to match against the entire database using 512 dimensional descriptors. Our second stage refinement is slower than SelaVPR and R2Former but much faster than Patch-NetVLAD-p, and requires 0.7 seconds per query with 100 top candidates. Comparing the ViT-B to ViT-G encoder sizes of Pair-VPR, only the encoding time and storage requirement increases, since we maintain the same global descriptor size and the same decoder network size. The storage requirements"}, {"title": "VI. CONCLUSION", "content": "In summary, we present Pair-VPR, a novel two-stage VPR method that relies upon a mask image modelling pre-training strategy to maximise performance. Pair-VPR combines a small 512 dimensional global descriptor for rapid database searching along with a slower second stage that only searches a set of potential database matches. We observed that Pair-VPR achieved the highest Recall@1 score on all datasets we tested on, comparing against recent state-of-the-art methods in VPR literature - while only requiring an encoder and decoder of 86 million parameters each. As the encoder size is scaled up to ViT-L (307M) and ViT-G (1.1B), our training recipe allows for continuing performance improvements as the network size grows. Given the high recalls attained by Pair-VPR, we believe it prudent to investigate converting this method from a VPR technique to a loop closure module in a full SLAM system. In future, we aim to investigate the effectiveness of Pair-VPR at performing loop closures in SLAM."}]}