{"title": "Modelling brain connectomes networks: Solv is\na worthy competitor to hyperbolic geometry!", "authors": ["Dorota Celi\u0144ska-Kopczy\u0144ska", "Eryk Kopczy\u0144ski"], "abstract": "Finding suitable embeddings for connectomes (spatially embed-\nded complex networks that map neural connections in the brain) is\ncrucial for analyzing and understanding cognitive processes. Recent\nstudies have found two-dimensional hyperbolic embeddings superior\nto Euclidean embeddings in modeling connectomes across species, es-\npecially human connectomes. However, those studies had limitations:\ngeometries other than Euclidean, hyperbolic, or spherical were not\nconsidered. Following William Thurston's suggestion that the net-\nworks of neurons in the brain could be successfully represented in Solv\ngeometry, we study the goodness-of-fit of the embeddings for 21 con-\nnectome networks (8 species). To this end, we suggest an embedding\nalgorithm based on Simulating Annealing that allows us to embed con-\nnectomes to Euclidean, Spherical, Hyperbolic, Solv, Nil, and product\ngeometries. Our algorithm tends to find better embeddings than the\nstate-of-the-art, even in the hyperbolic case. Our findings suggest that\nwhile three-dimensional hyperbolic embeddings yield the best results\nin many cases, Solv embeddings perform reasonably well.", "sections": [{"title": "Introduction", "content": "Connectomes are comprehensive maps of the neural connections in the brain.\nUnderstanding the interactions they shape is a key to understanding cognitive\nprocesses. Given their spatially embedded complexity, shaped by physical"}, {"title": "Prerequisities", "content": ""}, {"title": "Thurston geometries", "content": "By the uniformization theorem, every closed two-dimensional topological sur-\nface can be given spherical (S2), Euclidean (E2), or hyperbolic (H2) geometry,\nthat is, there exists a Riemannian manifold with the same topology as M and\nlocally isometric to a sphere, Euclidean plane, or hyperbolic plane. William\nThurston conjectured [Thu82] that three-dimensional topological manifolds\ncan be similarly decomposed into fragments, each of which can be given one\nof eight Thurston geometries, which are homogeneous Riemannian manifolds.\nThe eight Thurston geometries include:\n\u2022 isotropic geometries: spherical (S\u00b3), Euclidean (E3), and hyperbolic\n(H3).\n\u2022 product geometries: S\u00b2 \u00d7 R and H\u00b2 \u00d7 R, In geometry A \u00d7 B, the distance\ndA\u00d7B between (a1, b\u2081), (a2, b2) \u2208 A \u00d7 B is defined using the Pythagorean\nformula:\n$d_{AxB}((a_1,b_1),(a_2,b_2)) = \\sqrt{d_A(a_1,a_2)^2+d_B(b_1,b_2)^2}$.\nIntuitively, using the Pythagorean formula here means that the third\ndimension is added to S2 or H\u00b2 in the Euclidean way."}, {"title": null, "content": "\u2022 Twisted product geometries: twisted E\u00b2 \u00d7 R, also known as Nil, and\ntwisted H\u00b2 \u00d7 R, referred to as Twist in this paper, also known as the\nuniversal cover of SL(2, R). [KCK20]\n\u2022 Solv geometry, also known as Solve or Sol, which is fully anisotropic.\nThe more exotic Thurston geometries have been successfully visualized\nonly very recently [KCK20, CMST20], and thus are much less known than\nisotropic geometries. We refer to these papers and explanatory videos [Rog23,\nRog22] and demos [CMST22] for detailed explanations of Solv and Nil ge-\nometries. In the rest of this section, we include a brief description of Solv\nand an intuitive explanation of twisted product geometries.\nThe n-dimensional sphere is Sn = {x \u2208 Rn+1 : g(x, x) = 1}, where g is the\nEuclidean inner product, g(x, y) = x1Y1+X2Y2+...+Xn+1Yn+1. The distance\nbetween two points a, b on the sphere is the length of the arc connecting a\nand b, which can be computed as d(a,b) = acos g(a,b). Similarly, we can\ndefine n dimensional hyperbolic geometry using the Minkowski hyperboloid\nmodel. In this model, H\" = {x \u2208 Rd+1 : xd+1 > 0, g\u00af(x, x) = \u22121, where g\u00af is\nthe Minkowski inner product, g\u00af(x, y) = x1Y1+X2Y2+ . . . + XnYn - Xn+1Yn+1\u00b7\nThe distance between two points can be computed as d(a, b) = acosh g\u00af(a, b).\nTypically, tessellations of the hyperbolic plane H\u00b2 are visualized using\nthe Poincar\u00e9 disk model, which is a projection of H\u00b2 to the Euclidean plane\nthat distorts the distances (Figure 1). In each of these tessellations, all the\nshapes (of the same color) have the same hyperbolic size, even though ones\ncloser to the boundary look smaller in the projection.\nTo explain Solv, we should start with the horocyclic coordinate system of\nH2. Horocycles are represented in the Poincar\u00e9 disk model as circles tangent\nto the boundary; these can be seen as hyperbolic analogs of circles with\ninfinite radius and circumference, centered in an ideal point (point on the\nboundary of the Poincar\u00e9 disk). Figure 1c depicts concentric horocycles; the\ndistance between two adjacent horocycles in this picture is log(2), and if two\npoints A and B on given horocycle are in the distance x, then the distance\nbetween their projections on the next (outer) horocycle is 2x. For a point\nP\u2208 H2, we project P orthogonally to Q on the horocycle going through the\ncenter C of the Poincar\u00e9 model. The x coordinate is the (signed) length of\nthe horocyclic arc CQ, and y is the (signed) length of the segment PQ. (This\nis similar to the upper half-plane model [CFK+97], except that we take the\nlogarithm of the y coordinate.) In this coordinate system, the length of the"}, {"title": null, "content": "curve ((x(t), y(t)) : t \u2208 [a, b]) is defined as $\\int_a^b\\sqrt{(x'(t)\\exp^{y}t)^2+y'(t)^2}dt$.\nA similar coordinate system for H\u00b3 defines the length of the curve ((x(t), y(t), z(t)) :\nt\u2208 [a,b]) as $\\int_a^b\\sqrt{(x'(t)\\exp^{z(t)})^2+(y'(t)\\exp^{z(t)})^2+z'(t)^2}dt$. The surfaces\nof constant z are called horospheres; the geometry on a horosphere is Eu-\nclidean. We obtain Solv geometry by switching the sign in this formula.\nThat is, each point also has three coordinates (x, y, and z), but the length of\na curve is now equal to $\\int_a^b\\sqrt{(x'(t)\\exp^{z(t)})^2+(y'(t)\\exp^{-z(t)})^2+z'(t)^2}dt$.\nThe distance between two points is the length of the shortest curve connect-\ning them; this length is difficult to compute [CMST20, KCK22].\nIn Nil, we have well-defined directions at every point, which we can in-\ntuitively call North, East, South, West, Up and Down. However, while in\nEuclidean geometry, after moving 1 unit to the North, East, South, and\nWest, we return to the starting point; in Nil, such a loop results in a move\nby 1 unit in the Up direction. In general, the vertical movement is equal to\nthe signed area of the projection of the loop on the horizontal plane. Twist\nis based on the same idea, but the horizontal plane is now hyperbolic."}, {"title": "Geometric embeddings", "content": "In low-dimensional topology, three-dimensional geometry is incredibly chal-\nlenging; mainly, the Poincar\u00e9 conjecture was the most challenging in three\ndimensions. On the other hand, our interest in two-dimensional and three-\ndimensional geometries is based on their visualization possibilities [KCK20,\nCMST20] and potential application to geometric embeddings.\nFigure 1 shows that hyperbolic geometry has a tree-like, hierarchical\nstructure. This tree-likeness has found application in the visualization and\nmodeling of hierarchical structures [LRP95, Mun98], and then in the mod-\neling of complex networks. The hyperbolic random graph model (HRG)\n[BPK10] is parameterized by parameters N, R, T, a. Each node i \u2208 {1, ..., n}\nis assigned a point m(i) in the hyperbolic disk of radius R; the parameter\na controls the distribution. Then, every pair of points a, b \u2208 {1, ..., n} is\nconnected with probability 1/(1+exp((d-R)/T)), where d is the hyperbolic\ndistance between a and b. A real-world network (V, E) can be also embed-\nded into the hyperbolic plane H\u00b2 by mapping its nodes V to H\u00b2 [BPK10]; an\nembedding is better if the probability of forming the actual observed connec-\ntions (according to the HRG model) is higher. Moreover, graphs generated\naccording to this model have properties typical to scale-free networks, such"}, {"title": "Our contribution", "content": "We need a new embedding algorithm since the previous algorithms may be\nparticularly tailored to the specific geometry [BFKL16], or assume that dg\nis easy to compute, which is not true for Solv. We aim to find good quality\nembeddings of a connectome (V, E) into some geometry G, that is, a map\nm : V \u2192 G. As in the hyperbolic random graph (HRG) model, we assume\nthat our embedding has two parameters: R and T. The probability that\nan edge exists between i and jis $p_1(d) = \\frac{1}{1 + exp((d \u2013 R)/T)}$, where\nd is the distance between m(i) and m(j). We use MLE method to find\nthe embedding, that is, we aim to maximize the likelihood $\\prod_{1<i<j<N} P(i, j)$,\nwhere p(i, j) = p\u2081(dg(m(i), m(j))) in case if the edge between i and j exists,\nand p(i, j) = 1 \u2212 p\u2081(d(m(i), m(j))) otherwise. Equivalently, we maximalize\nthe loglikelihood $\\sum_{1<i<j<N} log p(i, j)$."}, {"title": "Our embedding algorithm", "content": "As in [CKK21], our algorithm is based on a uniform grid in geometry G.\nNatural grids exist in all Thurston geometries of interest [KCK20]. In the\nHRG model, the network is mapped to a disk of radius R; here, we map the\nnetwork to the set D of all grid points in G, which are in the distance at\nmost dr from some fixed origin. We choose dr to fix the number of points\ninside D; in most experiments, we pick M = 20000 points (actually, there\nmay be slightly more points due to ties).\nWe compute the distance de for every pair of points in D, thus obtaining\na |D|\u00d7 |D| array that can be used to quickly find the distance between pairs\nof points. In the case of Solv, it turns out that the method to compute\nthe Solv distances from [KCK20], while applicable to visualization, does not\napply to computing this table of distances due to long ranges. Therefore, for\nlonger distances, we approximate by d(a, b) as the smallest possible d(a, a\u2081)+\nd(a1, a2) + . . . + d(ak, b), where intermediate points are also in D, and each\npair of consecutive points is within the range of the method from [KCK20]."}, {"title": null, "content": "Dijkstra's algorithm is used to find the path (ai).\nNow, we use the Simulated Annealing method to find the embedding.\nThis method assumes R and T and starts with an arbitrary embedding m :\nV \u2192 D. Then, we perform the following for i = 1, . . ., Ns:\n\u2022 Introduce a small change m' to the current embedding m,\n\u2022 Compute L, the loglikelihood of m, and L', the loglikelihood of m'.\n\u2022 If L' > L, always replace m with m'. Otherwise, replace m with m'\nwith probability exp((L' - L)/exp(T)), where the parameter T (known\nas temperature) depends on the iteration index.\nIn Simulated Annealing, we start with a very high temperature (to accept\nall changes and thus explore the full space of possible embeddings without\ngetting stuck on local maxima). Then we proceed to lower and lower tem-\nperatures (not accepting changes that yield much worse embeddings but still\nexperimenting with crossing lower valleys), eventually accepting only the\nchanges that improve the embedding. In our experiments, T decreases lin-\nearly from 10 to -15. We consider local changes of two possible forms: move\nm'(i) for a random i to a random point in D, and move m'(i) for a random\ni to a random point in D that is close (neighbor) to m(i). These changes\nallow computing L' (based on the earlier L) in time O(|V|).\nTo obtain values of R and T, we start with some initial values of R and T.\nOccasionally, during the simulated annealing procedure, we find the values of\nRand T that best fit the current embedding, and we use the new values for\nthe remaining iterations. Since finding the correct values takes time, we do\nit relatively rarely (every |V| iterations with successful moves) and only once\nthe simulated annealing procedure rejects most changes. In our experiments,\nwe repeat this setup 30 times; in the following iterations, we start with the\nvalues of R and T that were obtained in the best embedding found so far.\nThe time complexity of an iteration is O(Ns \u00b7 |V|).\nOur implementation uses the tessellations implemented in RogueViz [KCK23]\nand is based on the existing implementation of Simulated Annealing for find-\ning hyperbolic visualizations [CK17]."}, {"title": "Visualization", "content": "In Figure 2, we present selected embeddings for the human cortex, rat ner-\nvous system, and zebra finch basal-ganglia connectomes. The embeddings"}, {"title": "Experiments", "content": "For our experiments, we use the same set of publicly available connectomes\nas [AS20]\u00b9 (not all connectomes used there are publicly available). See Table\n1.\nWe run 30 iterations of SA to find the best R and T, with Ns = 10000\u00b7|V|.\nWe evaluate the quality of embeddings using the following five measures (all\nranging from from 0 - worst to 1 \u2013 perfect).\nSC Greedy routing success rate. SC is the probability that, for random\npair of vertices (x,y) \u2208 V2, the greedy routing algorithm starting at\nx eventually successfully reaches the target y. This routing algorithm\nmoves in the first step from 1 to x1, the neighbor of x the closest to y\n(that is, dc(m(x1), m(y)) is the smallest). If x1 \u2260 y, we continue to x2,\nthe neighbor of x\u2081 the closest to y, and so on.\nIST Greedy routing stretch. Stretch is the expected ratio of the route length\nfound in the greedy routing procedure to the shortest route length,\nconditional that greedy routing was successful. IST is the reciprocal of\nstretch.\nIMR For an edge (x, y) \u2208 E, rank(x, y) is one plus the number of vertices that\nare closer to x than y but not connected with an edge. MeanRank is\nthe expected value of (x, y) over all edges. We use IMR=1/MeanRank.\nMAP For an edge (x, y) \u2208 E, P(x, y) is the ratio of vertices in distance of at\nmost de(m(x), m(y)) to x which are connected with x. AP(x) is the\naverage of P(x, y) for all y connected with x, and MAP is the average\nof AP(X) over all X."}, {"title": "Comparison at maximum performances", "content": "We start with a naive comparison among the tessellations based on the best\nresults obtained for each tessellation for each connectome. Figures 3, 4, 7, 6,\nand 5 visualize the rankings of the tessellations. 1s (the top) are the highest\nresults, and 0s (the bottom) are the lowest for a given connectome.\nAccording to Figures 3-5 and Tables 4 and 5, we notice that the assess-\nment of the performance of the geometry may vary concerning the quality"}, {"title": "Distribution-based comparison", "content": "Comparison of the maximum performance from the previous section gives us\nintuition about the optimistic scenarios and the limits for our embeddings.\nHowever, due to the nature of Simulated Annealing, the maximum values\nwe obtained are still realizations of random variables; that is why a closer\ninspection, including information about the distributions of the simulation\nresults, is needed. To this end, we will compare geometries using voting rules.\nIn particular, we will be interested in finding Condorcet winners and losers.\nAs Condorcet winner may not exist in the presence of ties, we will refer to\nits simple modification: Copeland rule [MD04].\nGeometry A wins against geometry B if the probability that (for a given\nquality measure) a randomly chosen simulation result obtained by A is greater\nthan a randomly chosen simulation result obtained by B exceeds 0.5. If that\nprobability is equal to 0.5, we have a tie between A and B; otherwise, A\nloses against B. To compute the score for a given geometry, we add 1 for\nevery winning scenario, 0 for every tie, and -1 for every losing scenario. The\ngeometries with the highest and lowest scores become Copeland winners and\nlosers, respectively (we allow for multiple candidates in both cases).\nThe winners based on the Copeland method beat most of the other candi-\ndates in pairwise contests. They should be the best options for embeddings.\nBased on the data in Table 6, we cannot name one universal winner. While\nit seems that H\u00b3 is a sound choice, we also notice that Solv and Twist are"}, {"title": "Zone-function-based comparison", "content": "We have already shown that, contrary to previous results from the literature,\nwe cannot name one universal best geometry to model any connectome. A\nnew interesting question arises if there are relationships between the function\nof the connectome (based on its zone) and the suitability of the geometries\n(using the rankings from the distribution-based comparisons). To find out,\nwe analyze the values of intraclass correlation coefficients (ICC), a widespread\ntool in the assessment of consistency among multiple raters [SF79] when the\nrating scale is ordinal to continuous. The literature suggests that the values\nof ICC below 0.50 indicate poor agreement, between 0.50 and 0.90 suggest\nmoderate to good agreement, and above 0.90: an excellent one [KL16]. To\nobtain an aggregate ranking for the whole zone, for each measure and each\ngeometry we computed the median of its minus rank (the minus gives us\ninformation 50% of ranks achieved by the given geometry are at least this\nhigh in modelling given zone, the lower values, the better). Medians were\nchosen due to their immunity to outliers.\nAccording to data in Table 7, the intra-zone comparisons suggest good\nto excellent agreement no matter the quality measure. Rankings for con-\nnectomes from \u201cother\u201d zone (usually specific cells, e.g., from retina or op-\ntic medula) show relatively lower agreement. On the contrary, the inter-\nzone comparison suggests poor to moderate agreement between the rankings.\nWhile the p-values in significance tests for ICCs in inter-zone comparisons\nsuggest significance at any reasonable significance level (even after Bonferroni\ncorrections), the results for intra-zone comparison after Bonferroni correc-\ntions appear insignificant (so any similarities might be random), apart from\nthe result for the SC measure. Those results are promising for us. They\nsuggest that the choice of the suitable geometry may depend on the function\nof the connectome. For example, our results suggest that the trees are best\nchoice in modelling nervous systems (no matter the quality measure), for\ncortex H\u00b3*, H\u00b2\u00d7R, Nil* or Solv* would be a suitable choice, and Twist may\nbe beneficial for modelling specific cells.\nWhile a natural question arises about the anatomical implications of dif-\nferent best fits for geometries, as well as why different connectomes might\nhave different best geometries, answering this question in a statistically ro-\nbust manner would require a more detailed study on a bigger number of\nsample connectomes."}, {"title": "Robustness checks and threats to validity", "content": "Ideally, there exists optimal embedding of (V, E) into the whole geometry G,\nwhere mopt: V \u2192 G, and some values of R and T are used. Unfortunately,\nthe embedding m found by Simulated Annealing might be worse than mopt\ndue to the following issues:\n\u2022 The radius dr is too small, making mopt simply not fit,\n\u2022 The grid used is too coarse, hence the necessity of making m(i) the\ngrid point to closest to mopt(i), and thus reducing the log-likelihood,\n\u2022 The number of iterations of Simulated Annealing, Ns, is too small\nwhile Simulated Annealing is theoretically guaranteed to find the\noptimal embedding for given R and T with high probability as Ns\ntends to infinity, in practice, we are constrained by computation time\nlimits,"}, {"title": null, "content": "\u2022 The values of the parameters R and T have not been chosen correctly.\nIn this section, we will explain how we combated those issues. We will\nalso check if they affected our results.\nPossibly insufficient size of grids. For comparability, we aimed to keep\nthe number of neurons as close to 20,000 as possible. However, one could\nargue if this is enough. To combat the first two issues, in some geometries, we\nconsider coarser and finer grids: coarser grids are better at handling the first\nissue, and finer grids are better at handling the second issue \u2013 in both cases,\nwe expect that increasing dr and grid density beyond some threshold yields\ndiminishing returns. That is why, based on the results from the previous\nsections, we have added the so-called big versions coarser but larger grids\n(M = 100000) \u2013 for selected, promising manifolds (H\u00b3, H\u00b3*, H\u00b2 \u00d7 R, Solv,\nand Twist). We will denote them with **. See Table 8 for the details."}, {"title": null, "content": "We started by checking for significant differences in favor of big versions of\nmanifolds; to this end, we performed Wilcoxon tests with Bonferroni correc-\ntion for multiple comparisons. Figure 13 depicts the procedure results. Ac-\ncording to our results, in most cases, the differences are insignificant, which\nsuggests that the size of the manifold is not a severe threat to validity. Us-\nage of big versions usually results in better embeddings for Rat connectomes,\nwhich might correlate with a different function of those connectomes com-\npared to others in the sample (they describe nervous systems). Rarely, big\nversions yield worse embeddings than the standard ones usually for Human\nconnectomes; however, no pattern-enabling explanation is noticeable here.\nNext, we checked if the size of the manifolds affects rankings. To this end,\nwe computed weighted Cohen's kappas [Coh68]. In kappas, 0 represents the\namount of agreement expected from random chance, and 1 signifies a perfect"}, {"title": null, "content": "agreement between the raters. Initially, kappas take into account only the\nagreements of the raters. The weighted kappas allow disagreements to be\nweighted differently, which is more suitable for us we are more interested\nin the relative placement of the pairs of the geometries in the ranking than\nin the actual places. If there are slight differences in ranks by two raters,\ne.g., by one, the ranks should remain similar to us as embeddings yielding\ncomparable quality results should still be close to each other. Although there\nare no universal guidelines for interpreting of those coefficients, the literature\nsuggests that the values over 0.61 indicate moderate to substantial agreement\nbetween raters and values exceeding 0.81 \u2013 strong to almost perfect agreement\n[LK77].\nAccording to data in Table 9, rankings obtained from big versions of man-\nifolds in the standard setup of Simulated Annealing (N\u2083 = 10,000 iterations)\nare at least in substantial agreement with rankings based on standard ver-\nsions. The high agreement in rankings based on voting rules is unsurprising.\nIt aligns with the results depicted in Figure 13 we include more information\nfrom the distributions, so the results should be more robust than those based\non max performance (outliers). However, we recommend cautiously treating\nthe results for greedy routing success and stretch.\nPossibly insufficient number of iterations. As Simulated Annealing is\na probabilistic technique for approximating the global optimum of a given\nfunction, one could argue that, e.g., increasing the number of iterations could\nimprove our results (the third issue). The main paper describes the results\nobtained with Simulated Annealing with N\u3002 = 10,000\u00b7|V| iterations per\nsimulation iteration. We also checked if our results differ if we perform Sim-\nulated Annealing with N = 100,000 \u00b7 |V| iterations per simulation iteration"}, {"title": null, "content": "instead. As expected, for log-likelihood, MAP, and MR, we cannot reject the\nhypotheses that the results obtained with larger numbers of iterations are\nusually better. However, surprisingly, for greedy success rate and stretch,\nthe results worsen with the increase in the number of iterations (Figure 14\ndepicts the results of Wilcoxon tests with Bonferroni correction for multiple\ncomparisons).\nWe checked if the number of iterations for Simulated Annealing N, af-\nfected our results regarding rankings with Cohen's kappas. Based on the\ndata in Table 9, we notice that pairs of rankings are in at least substantial\nagreement. The results regarding standard grids (presented in the main part\nof the paper) are robust to N, \u2013 the values of kappas for optimistic scenarios\nare over 0.85. For rankings based on voting rules, they usually exceed 0.80.\nAlthough the agreements of rankings, if we change N, for big grids, are still\nsatisfying (most values of kappas over 0.75), the results from comparison of\nrankings based on standard grids with shorter time for Simulated Anneal-\ning against the big grids with longer time for Simulated Annealing suggests\nthat the big versions of grids might be affected by Ns. Again, we notice\nthat greedy routing success rate and stretch are less immune to the setup of\nSimulating Annealing, so we suggest caution while generalizing the results\nobtained for them.\nAlternative methods of obtaining R and T The fourth issue is chal-\nlenging. As explained in Section 4, the values of R and T have been obtained\nby dynamically adjusting them during the simulated annealing process (A).\nWe have also experimented with other methods: Ris changed, but Tre-\nmain fixed (B), and both R and T remain fixed. We run 30 iterations using\nmethod (A), then 30 iterations using method (B), then 30 iterations using\nmethod (C). The fixed values of R and T are based on the best result (by\nlog-likelihood) obtained in the earlier iterations.\nIf the methods change the results, we should notice level shifts in the\ntime series of the quality measures' values \u2013 level shifts appear as a parallel\nmovement of the trend line. That is why we started by identifying possible\nlocations of the level shifts in our results. Most of the time series (determined\nby a pair animal and geometry) has two level shifts \u2013 around the 30th and\n60th iterations that correspond to the starting points of new methods (Figure\n15).\nWe use OLS regressions to understand the impact of the change in method"}, {"title": null, "content": "on the values of the quality measures. We control for the characteristics of\nconnectomes: number of nodes in the connectome n, number of edges in the\nconnectome m, its density, assortativity and clustering coefficients, and the\nzone of the connectome; we also take into account the number of available\ncells in the grid and its geometry."}, {"title": "Conclusions", "content": "In this paper, we presented an experimental analysis of embeddings of 21 con-\nnectomes to various geometries (both three-dimesnional and two-dimensional).\nTo our knowledge, we are the first to compare embeddings of connectomes\nto all Thurston geometries. Our findings unveil new prospects for connec-\ntome modeling, introducing a novel method based on Simulated Annealing.\nOur results demonstrate the efficacy of this approach it yields superior\nembeddings compared to the state-of-the-art.\nAlthough previous studies suggested that one could find a universal win-\nner geometry for the embeddings (usually pointing at two-dimensional hy-\nperbolic geometry), our results reveals a nuanced scenario when considering\nthe third dimension. We showed that the universal winner ceases to exist\nwhen we consider the third dimension. In particular, H\u00b2 embeddings tend\nto be worse than (non-Euclidean) 3D geometries, even if our H\u00b2 embeddings\nare good - better than [BFKL16, GPASB19, NK17, NK18]. If we were to\nsuggest a set of geometries to pay attention to when modeling connectomes,\nwe would mention three-dimensional hyperbolic geometry, Solv, and product\ngeometries. Surprisingly, three-dimensional Euclidean geometry is a suitable\nchoice for Human connectomes. There is a correlation between the zone\nof the connectome (also its primary function) and the best choice for the\nembedding, e.g., nervous systems tend to be well modeled by trees.\nSome technicalities that do not matter (discrete versions or angularity).\nHowever, the size of the grid or the setup of the Simulated Annealing can\naffect the results. Remarkably, in some experiment, standard grid versions\nhave given significantly better results than the so-called big versions. Since"}]}