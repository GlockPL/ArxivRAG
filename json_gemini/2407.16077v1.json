{"title": "Modelling brain connectomes networks: Solv is\na worthy competitor to hyperbolic geometry!", "authors": ["Dorota Celi\u0144ska-Kopczy\u0144ska", "Eryk Kopczy\u0144ski"], "abstract": "Finding suitable embeddings for connectomes (spatially embed-\nded complex networks that map neural connections in the brain) is\ncrucial for analyzing and understanding cognitive processes. Recent\nstudies have found two-dimensional hyperbolic embeddings superior\nto Euclidean embeddings in modeling connectomes across species, es-\npecially human connectomes. However, those studies had limitations:\ngeometries other than Euclidean, hyperbolic, or spherical were not\nconsidered. Following William Thurston's suggestion that the net-\nworks of neurons in the brain could be successfully represented in Solv\ngeometry, we study the goodness-of-fit of the embeddings for 21 con-\nnectome networks (8 species). To this end, we suggest an embedding\nalgorithm based on Simulating Annealing that allows us to embed con-\nnectomes to Euclidean, Spherical, Hyperbolic, Solv, Nil, and product\ngeometries. Our algorithm tends to find better embeddings than the\nstate-of-the-art, even in the hyperbolic case. Our findings suggest that\nwhile three-dimensional hyperbolic embeddings yield the best results\nin many cases, Solv embeddings perform reasonably well.", "sections": [{"title": "Introduction", "content": "Connectomes are comprehensive maps of the neural connections in the brain.\nUnderstanding the interactions they shape is a key to understanding cognitive\nprocesses. Given their spatially embedded complexity, shaped by physical\nconstraints and communication imperatives, connectomes exhibit properties\ninherent to non-Euclidean geometries. Therefore, a vast amount of recent\nresearch has been devoted to finding the appropriate embeddings for con-\nnectome networks. Recent studies (e.g., [WHKL22, AS20]) have advocated\nfor the superiority of two-dimensional hyperbolic embeddings over Euclidean\nembeddings in modeling connectomes across species, especially human con-\nnectomes. However, those studies had limitations: they restricted the focus\nto Euclidean, hyperbolic, or spherical geometries, neglecting to explore other\npotential embedding spaces.\nOur study expands the perspectives for suitable embeddings. We analyze\nthe goodness of fit (measured with widely used quality measures) of the em-\nbeddings for 21 connectome networks (8 species) to 15 unique tessellations\n(Euclidean, Spherical, Hyperbolic, Solv, Nil, and also product geometries).\nWe consider both two-dimensional and three-dimensional manifolds. Follow-\ning William Thurston's suggestion that the networks of neurons in the brain\ncould be successfully represented in Solv geometry (one of eight so-called\nThurston geometries), we stipulate that this geometry would outperform hy-\nperbolic geometry.\nAgainst this background, our contribution in this paper can be summa-\nrized as follows:\n\u2022 We present a novel embedding method based on Simulated Annealing\n(SA). Experiments show that our algorithm outperforms the state-of-\nthe-art, even for the hyperbolic embeddings, as evaluated by standard\nmeasures (mAP, MeanRank, greedy routing success, and stretch).\n\u2022 To the best of our knowledge, we are the first to compare embeddings\nof connectomes to all Thurston geometries. As a result, we expand\nthe horizons of connectome modeling and open up new possibilities for\nanalysis. We show that connectome modeling is more nuanced than\npreviously presented.\n\u2022 We find that while three-dimensional hyperbolic geometry yields the\nbest results in many cases, other geometries, such as Solv, are worth\nconsidering. Supported by an extensive simulation scheme, our results\nbring confidence and reliability beyond previous studies.\nThis paper is accompanied with supplementary material containing our\nimplementation, data and results [CKK24a] and a video containing our 3D\nvisualizations [CKK24b]."}, {"title": "Prerequisities", "content": null}, {"title": "Thurston geometries", "content": "By the uniformization theorem, every closed two-dimensional topological sur-\nface can be given spherical (S\u00b2), Euclidean (E\u00b2), or hyperbolic (H\u00b2) geometry,\nthat is, there exists a Riemannian manifold with the same topology as M and\nlocally isometric to a sphere, Euclidean plane, or hyperbolic plane. William\nThurston conjectured [Thu82] that three-dimensional topological manifolds\ncan be similarly decomposed into fragments, each of which can be given one\nof eight Thurston geometries, which are homogeneous Riemannian manifolds.\nThe eight Thurston geometries include:\n\u2022 isotropic geometries: spherical (S\u00b3), Euclidean (E\u00b3), and hyperbolic\n(H\u00b3).\n\u2022 product geometries: S\u00b2 \u00d7 R and H\u00b2 \u00d7 R, In geometry A \u00d7 B, the distance\n$d_{A\u00d7B}$ between ($a_1, b_1$), ($a_2, b_2$) \u2208 A \u00d7 B is defined using the Pythagorean\nformula:\n$d_{AxB}((a_1, b_1), (a_2, b_2)) = \\sqrt{d_A(a_1, a_2)^2 + d_B(b_1, b_2)^2}$.\nIntuitively, using the Pythagorean formula here means that the third\ndimension is added to S\u00b2 or H\u00b2 in the Euclidean way.\n\u2022 Twisted product geometries: twisted E\u00b2 \u00d7 R, also known as Nil, and\ntwisted H\u00b2 \u00d7 R, referred to as Twist in this paper, also known as the\nuniversal cover of SL(2, R). [KCK20]\n\u2022 Solv geometry, also known as Solve or Sol, which is fully anisotropic.\nThe more exotic Thurston geometries have been successfully visualized\nonly very recently [KCK20, CMST20], and thus are much less known than\nisotropic geometries. We refer to these papers and explanatory videos [Rog23,\nRog22] and demos [CMST22] for detailed explanations of Solv and Nil ge-\nometries. In the rest of this section, we include a brief description of Solv\nand an intuitive explanation of twisted product geometries.\nThe n-dimensional sphere is Sn = {x \u2208 Rn+1 : g(x, x) = 1}, where g is the\nEuclidean inner product, g(x, y) = $x_1y_1 + x_2y_2 + ... + x_{n+1}y_{n+1}$. The distance\nbetween two points a, b on the sphere is the length of the arc connecting a\nand b, which can be computed as d(a,b) = acos g(a,b). Similarly, we can\ndefine n dimensional hyperbolic geometry using the Minkowski hyperboloid\nmodel. In this model, H\" = {x \u2208 Rd+1 : $x_{d+1}$ > 0, g\u00af(x, x) = \u22121, where g\u00af is\nthe Minkowski inner product, g\u00af(x, y) = $x_1y_1 + x_2y_2 + ... + x_ny_n - x_{n+1}y_{n+1}$.\nThe distance between two points can be computed as d(a, b) = acosh g\u00af(a, b).\nTypically, tessellations of the hyperbolic plane H\u00b2 are visualized using\nthe Poincar\u00e9 disk model, which is a projection of H\u00b2 to the Euclidean plane\nthat distorts the distances (Figure 1). In each of these tessellations, all the\nshapes (of the same color) have the same hyperbolic size, even though ones\ncloser to the boundary look smaller in the projection.\nTo explain Solv, we should start with the horocyclic coordinate system of\nH\u00b2. Horocycles are represented in the Poincar\u00e9 disk model as circles tangent\nto the boundary; these can be seen as hyperbolic analogs of circles with\ninfinite radius and circumference, centered in an ideal point (point on the\nboundary of the Poincar\u00e9 disk). Figure 1c depicts concentric horocycles; the\ndistance between two adjacent horocycles in this picture is log(2), and if two\npoints A and B on given horocycle are in the distance x, then the distance\nbetween their projections on the next (outer) horocycle is 2x. For a point\nP\u2208 H\u00b2, we project Porthogonally to Q on the horocycle going through the\ncenter C of the Poincar\u00e9 model. The x coordinate is the (signed) length of\nthe horocyclic arc CQ, and y is the (signed) length of the segment PQ. (This\nis similar to the upper half-plane model [CFK+97], except that we take the\nlogarithm of the y coordinate.) In this coordinate system, the length of the\ncurve ((x(t), y(t)) : t \u2208 [a, b]) is defined as $\\int_a^b \\sqrt{(x'(t) exp y(t))^2 + y'(t)^2}dt$.\nA similar coordinate system for H\u00b3 defines the length of the curve ((x(t), y(t), z(t)) :\nt\u2208 [a,b]) as $\\int_a^b \\sqrt{(x'(t) exp z(t))^2 + (y'(t) exp z(t))^2 + z'(t)^2}dt$. The surfaces\nof constant z are called horospheres; the geometry on a horosphere is Eu-\nclidean. We obtain Solv geometry by switching the sign in this formula.\nThat is, each point also has three coordinates (x, y, and z), but the length of\na curve is now equal to $\\int_a^b \\sqrt{(x'(t) expz(t))^2 + (y'(t) exp-z(t))^2 + z'(t)^2}dt$.\nThe distance between two points is the length of the shortest curve connect-\ning them; this length is difficult to compute [CMST20, KCK22].\nIn Nil, we have well-defined directions at every point, which we can in-\ntuitively call North, East, South, West, Up and Down. However, while in\nEuclidean geometry, after moving 1 unit to the North, East, South, and\nWest, we return to the starting point; in Nil, such a loop results in a move\nby 1 unit in the Up direction. In general, the vertical movement is equal to\nthe signed area of the projection of the loop on the horizontal plane. Twist\nis based on the same idea, but the horizontal plane is now hyperbolic."}, {"title": "Geometric embeddings", "content": "In low-dimensional topology, three-dimensional geometry is incredibly chal-\nlenging; mainly, the Poincar\u00e9 conjecture was the most challenging in three\ndimensions. On the other hand, our interest in two-dimensional and three-\ndimensional geometries is based on their visualization possibilities [KCK20,\nCMST20] and potential application to geometric embeddings.\nFigure 1 shows that hyperbolic geometry has a tree-like, hierarchical\nstructure. This tree-likeness has found application in the visualization and\nmodeling of hierarchical structures [LRP95, Mun98], and then in the mod-\neling of complex networks. The hyperbolic random graph model (HRG)\n[BPK10] is parameterized by parameters N, R, T, a. Each node i \u2208 {1, ..., n}\nis assigned a point m(i) in the hyperbolic disk of radius R; the parameter\na controls the distribution. Then, every pair of points a, b \u2208 {1, ..., n} is\nconnected with probability $1/(1+exp((d-R)/T))$, where d is the hyperbolic\ndistance between a and b. A real-world network (V, E) can be also embed-\nded into the hyperbolic plane H\u00b2 by mapping its nodes V to H\u00b2 [BPK10]; an\nembedding is better if the probability of forming the actual observed connec-\ntions (according to the HRG model) is higher. Moreover, graphs generated\naccording to this model have properties typical to scale-free networks, such\nas high clustering coefficient and power-law degree distribution [PKS+12].\nMore recently, embeddings into higher-dimensional hyperbolic spaces were\nstudied in the network [JABS23, WHKL22] and the machine learning commu-\nnity (product geometries in [GSGR19]). \u03a4\u03bf our knowledge, twisted product\nor Solv geometry have yet to be studied in this context. We are especially\ninterested in the intriguing suggestion of William Thurston from 1997 that\nthe brain's architecture might be based on Solv geometry [Sch20]. Intuitively,\nthe Solv geometry is based on two hierarchies (the hyperbolic plane y =const\nand the hyperbolic plane x =const), which are opposed to each other due to\nthe opposite sign used with z in the distance formula. This gives us hope\nthat we can use Solv geometry to represent in three dimensions hierarchies\nthat cannot be represented using other two- or three-dimensional geometries\nexhibiting simpler hierarchical structure (H\u00b2, H\u00b3, H\u00b2 \u00d7 R). A similar effect\nof two opposing hierarchies could also be obtained in H\u00b2 \u00d7 H\u00b2. However, that\nis a four-dimensional geometry and, thus, less suitable for visualization. An\npromising property of Nil geometry is that it is a three-dimensional geometry\nwhere the volume of a ball of radius R has \u0398(R4) growth, which suggests\nbetter embedding possibilities than E\u00b3, but worse than the exponentially-\nexpanding geometries."}, {"title": "Our contribution", "content": "We need a new embedding algorithm since the previous algorithms may be\nparticularly tailored to the specific geometry [BFKL16], or assume that dg\nis easy to compute, which is not true for Solv. We aim to find good quality\nembeddings of a connectome (V, E) into some geometry G, that is, a map\nm : V \u2192 G. As in the hyperbolic random graph (HRG) model, we assume\nthat our embedding has two parameters: R and T. The probability that\nan edge exists between i and jis $p_1(d) = 1/(1 + exp((d \u2013 R)/T))$, where\nd is the distance between m(i) and m(j). We use MLE method to find\nthe embedding, that is, we aim to maximize the likelihood $\\prod_{1<i<j<N} P(i, j)$,\nwhere $p(i, j) = p_1(d_G(m(i), m(j)))$ in case if the edge between i and j exists,\nand $p(i, j) = 1 \u2212 p_1(d(m(i), m(j)))$ otherwise. Equivalently, we maximalize\nthe loglikelihood $\\sum_{1<i<j<N} log p(i, j)."}, {"title": "Our embedding algorithm", "content": "As in [CKK21], our algorithm is based on a uniform grid in geometry G.\nNatural grids exist in all Thurston geometries of interest [KCK20]. In the\nHRG model, the network is mapped to a disk of radius R; here, we map the\nnetwork to the set D of all grid points in G, which are in the distance at\nmost dr from some fixed origin. We choose dr to fix the number of points\ninside D; in most experiments, we pick M = 20000 points (actually, there\nmay be slightly more points due to ties).\nWe compute the distance de for every pair of points in D, thus obtaining\na |D|\u00d7 |D| array that can be used to quickly find the distance between pairs\nof points. In the case of Solv, it turns out that the method to compute\nthe Solv distances from [KCK20], while applicable to visualization, does not\napply to computing this table of distances due to long ranges. Therefore, for\nlonger distances, we approximate by d(a, b) as the smallest possible d(a, a\u2081)+\nd(a1, a2) + . . . + d(ak, b), where intermediate points are also in D, and each\npair of consecutive points is within the range of the method from [KCK20].\nDijkstra's algorithm is used to find the path (ai).\nNow, we use the Simulated Annealing method to find the embedding.\nThis method assumes R and T and starts with an arbitrary embedding m :\nV \u2192 D. Then, we perform the following for i = 1, . . ., Ns:\n\u2022 Introduce a small change m' to the current embedding m,\n\u2022 Compute L, the loglikelihood of m, and L', the loglikelihood of m'.\n\u2022 If L' > L, always replace m with m'. Otherwise, replace m with m'\nwith probability exp((L' - L)/exp(T)), where the parameter T (known\nas temperature) depends on the iteration index.\nIn Simulated Annealing, we start with a very high temperature (to accept\nall changes and thus explore the full space of possible embeddings without\ngetting stuck on local maxima). Then we proceed to lower and lower tem-\nperatures (not accepting changes that yield much worse embeddings but still\nexperimenting with crossing lower valleys), eventually accepting only the\nchanges that improve the embedding. In our experiments, T decreases lin-\nearly from 10 to -15. We consider local changes of two possible forms: move\nm'(i) for a random i to a random point in D, and move m'(i) for a random\ni to a random point in D that is close (neighbor) to m(i). These changes\nallow computing L' (based on the earlier L) in time O(|V|).\nTo obtain values of R and T, we start with some initial values of R and T.\nOccasionally, during the simulated annealing procedure, we find the values of\nRand T that best fit the current embedding, and we use the new values for\nthe remaining iterations. Since finding the correct values takes time, we do\nit relatively rarely (every |V| iterations with successful moves) and only once\nthe simulated annealing procedure rejects most changes. In our experiments,\nwe repeat this setup 30 times; in the following iterations, we start with the\nvalues of R and T that were obtained in the best embedding found so far.\nThe time complexity of an iteration is O(Ns \u00b7 |V|).\nOur implementation uses the tessellations implemented in RogueViz [KCK23]\nand is based on the existing implementation of Simulated Annealing for find-\ning hyperbolic visualizations [CK17]."}, {"title": "Visualization", "content": "In Figure 2, we present selected embeddings for the human cortex, rat ner-\nvous system, and zebra finch basal-ganglia connectomes. The embeddings\nexhibit different shapes: e.g., rat nervous system connectomes are star-like,\nwith a group of neurons in the center of the embedding connected to other\nneurons, with few other connections. Different geometries highlight these\ndifferences. E.g., such star-like networks embed well into H\u00b3 or trees. Our\nvisualization engine lets the viewer rotate the embedding and examine the\nspatial relationships in detail. See the videos in the supplementary material."}, {"title": "Experiments", "content": "For our experiments, we use the same set of publicly available connectomes\nas [AS20]\u00b9 (not all connectomes used there are publicly available). See Table\n1.\nWe run 30 iterations of SA to find the best R and T, with Ns = 10000\u00b7|V|.\nWe evaluate the quality of embeddings using the following five measures (all\nranging from from 0 - worst to 1 \u2013 perfect).\nSC Greedy routing success rate. SC is the probability that, for random\npair of vertices (x,y) \u2208 V2, the greedy routing algorithm starting at\nx eventually successfully reaches the target y. This routing algorithm\nmoves in the first step from 1 to x1, the neighbor of x the closest to y\n(that is, dc(m(x1), m(y)) is the smallest). If x1 \u2260 y, we continue to x2,\nthe neighbor of x\u2081 the closest to y, and so on.\nIST Greedy routing stretch. Stretch is the expected ratio of the route length\nfound in the greedy routing procedure to the shortest route length,\nconditional that greedy routing was successful. IST is the reciprocal of\nstretch.\nIMR For an edge (x, y) \u2208 E, rank(x, y) is one plus the number of vertices that\nare closer to x than y but not connected with an edge. MeanRank is\nthe expected value of (x, y) over all edges. We use IMR=1/MeanRank.\nMAP For an edge (x, y) \u2208 E, P(x, y) is the ratio of vertices in distance of at\nmost dc(m(x), m(y)) to x which are connected with x. AP(x) is the\naverage of P(x, y) for all y connected with x, and MAP is the average\nof AP(X) over all X."}, {"title": "Comparison at maximum performances", "content": "We start with a naive comparison among the tessellations based on the best\nresults obtained for each tessellation for each connectome. Figures 3, 4, 7, 6,\nand 5 visualize the rankings of the tessellations. 1s (the top) are the highest\nresults, and 0s (the bottom) are the lowest for a given connectome.\nAccording to Figures 3-5 and Tables 4 and 5, we notice that the assess-\nment of the performance of the geometry may vary concerning the quality\nmeasure; there are also differences across species. In general, trees perform\npoorly in measures other than greedy success rate, and no matter the mea-\nsure, they are always the best choice for Rat's connectomes (nervous system).\nResults for Rat's and Drosophila2's connectomes are also characterized by\nthe relatively high variation among species (Table 3). For other species, the\nbest performances are similar with respect to a quality measure: the differ-\nences in best performance among geometries measured with MAP, greedy\nrate success, and stretch are slight (in most cases, values of CVs are under\n10%); especially for Cat's connectomes, they tend to be negligible (values of\nCVs even under 1%).\nThe results suggest that H\u00b2& and S\u00b3 seem to be inefficient choices: the"}, {"title": "Distribution-based comparison", "content": "Comparison of the maximum performance from the previous section gives us\nintuition about the optimistic scenarios and the limits for our embeddings.\nHowever, due to the nature of Simulated Annealing, the maximum values\nwe obtained are still realizations of random variables; that is why a closer\ninspection, including information about the distributions of the simulation\nresults, is needed. To this end, we will compare geometries using voting rules.\nIn particular, we will be interested in finding Condorcet winners and losers.\nAs Condorcet winner may not exist in the presence of ties, we will refer to\nits simple modification: Copeland rule [MD04].\nGeometry A wins against geometry B if the probability that (for a given\nquality measure) a randomly chosen simulation result obtained by A is greater\nthan a randomly chosen simulation result obtained by B exceeds 0.5. If that\nprobability is equal to 0.5, we have a tie between A and B; otherwise, A\nloses against B. To compute the score for a given geometry, we add 1 for\nevery winning scenario, 0 for every tie, and -1 for every losing scenario. The\ngeometries with the highest and lowest scores become Copeland winners and\nlosers, respectively (we allow for multiple candidates in both cases).\nThe winners based on the Copeland method beat most of the other candi-\ndates in pairwise contests. They should be the best options for embeddings.\nBased on the data in Table 6, we cannot name one universal winner. While\nit seems that H\u00b3 is a sound choice, we also notice that Solv and Twist are\nworthy of attention. Interestingly, for Human connectomes, E\u00b3 outperforms\nother geometries.\nIn Figures 8-12, we provide weighted directed networks constructed upon\nthe voting rules to allow for generalizations. The weights correspond to the\npercent of connectomes for which the source geometry in the edge beats the\ntarget geometry. Embeddings to Twist have a 100% success rate over em-\nbeddings in H\u00b2 (for quality measures different than greedy routing success)."}, {"title": "Zone-function-based comparison", "content": "We have already shown that, contrary to previous results from the literature,\nwe cannot name one universal best geometry to model any connectome. A\nnew interesting question arises if there are relationships between the function\nof the connectome (based on its zone) and the suitability of the geometries\n(using the rankings from the distribution-based comparisons). To find out,\nwe analyze the values of intraclass correlation coefficients (ICC), a widespread\ntool in the assessment of consistency among multiple raters [SF79] when the\nrating scale is ordinal to continuous. The literature suggests that the values\nof ICC below 0.50 indicate poor agreement, between 0.50 and 0.90 suggest\nmoderate to good agreement, and above 0.90: an excellent one [KL16]. To\nobtain an aggregate ranking for the whole zone, for each measure and each\ngeometry we computed the median of its minus rank (the minus gives us\ninformation 50% of ranks achieved by the given geometry are at least this\nhigh in modelling given zone, the lower values, the better). Medians were\nchosen due to their immunity to outliers.\nAccording to data in Table 7, the intra-zone comparisons suggest good\nto excellent agreement no matter the quality measure. Rankings for con-\nnectomes from \u201cother\u201d zone (usually specific cells, e.g., from retina or op-\ntic medula) show relatively lower agreement. On the contrary, the inter-\nzone comparison suggests poor to moderate agreement between the rankings.\nWhile the p-values in significance tests for ICCs in inter-zone comparisons\nsuggest significance at any reasonable significance level (even after Bonferroni\ncorrections), the results for intra-zone comparison after Bonferroni correc-\ntions appear insignificant (so any similarities might be random), apart from\nthe result for the SC measure. Those results are promising for us. They\nsuggest that the choice of the suitable geometry may depend on the function\nof the connectome. For example, our results suggest that the trees are best\nchoice in modelling nervous systems (no matter the quality measure), for\ncortex H\u00b3*, H\u00b2\u00d7R, Nil* or Solv* would be a suitable choice, and Twist may\nbe beneficial for modelling specific cells.\nWhile a natural question arises about the anatomical implications of dif-\nferent best fits for geometries, as well as why different connectomes might\nhave different best geometries, answering this question in a statistically ro-\nbust manner would require a more detailed study on a bigger number of\nsample connectomes."}, {"title": "Robustness checks and threats to validity", "content": "Ideally", "m_{opt}$": "V \u2192 G", "issues": "n\u2022 The radius dr is too small", "grids": "coarser grids are better at handling the first\nissue", "methods": "Ris changed", "of\nconnectomes": "number of nodes in the connectome n", "best": "nevaluations may not be optimal. However"}, {"title": "Modelling brain connectomes networks: Solv is\na worthy competitor to hyperbolic geometry!", "authors": ["Dorota Celi\u0144ska-Kopczy\u0144ska", "Eryk Kopczy\u0144ski"], "abstract": "Finding suitable embeddings for connectomes (spatially embed-\nded complex networks that map neural connections in the brain) is\ncrucial for analyzing and understanding cognitive processes. Recent\nstudies have found two-dimensional hyperbolic embeddings superior\nto Euclidean embeddings in modeling connectomes across species, es-\npecially human connectomes. However, those studies had limitations:\ngeometries other than Euclidean, hyperbolic, or spherical were not\nconsidered. Following William Thurston's suggestion that the net-\nworks of neurons in the brain could be successfully represented in Solv\ngeometry, we study the goodness-of-fit of the embeddings for 21 con-\nnectome networks (8 species). To this end, we suggest an embedding\nalgorithm based on Simulating Annealing that allows us to embed con-\nnectomes to Euclidean, Spherical, Hyperbolic, Solv, Nil, and product\ngeometries. Our algorithm tends to find better embeddings than the\nstate-of-the-art, even in the hyperbolic case. Our findings suggest that\nwhile three-dimensional hyperbolic embeddings yield the best results\nin many cases, Solv embeddings perform reasonably well.", "sections": [{"title": "Introduction", "content": "Connectomes are comprehensive maps of the neural connections in the brain.\nUnderstanding the interactions they shape is a key to understanding cognitive\nprocesses. Given their spatially embedded complexity, shaped by physical\nconstraints and communication imperatives, connectomes exhibit properties\ninherent to non-Euclidean geometries. Therefore, a vast amount of recent\nresearch has been devoted to finding the appropriate embeddings for con-\nnectome networks. Recent studies (e.g., [WHKL22, AS20]) have advocated\nfor the superiority of two-dimensional hyperbolic embeddings over Euclidean\nembeddings in modeling connectomes across species, especially human con-\nnectomes. However, those studies had limitations: they restricted the focus\nto Euclidean, hyperbolic, or spherical geometries, neglecting to explore other\npotential embedding spaces.\nOur study expands the perspectives for suitable embeddings. We analyze\nthe goodness of fit (measured with widely used quality measures) of the em-\nbeddings for 21 connectome networks (8 species) to 15 unique tessellations\n(Euclidean, Spherical, Hyperbolic, Solv, Nil, and also product geometries).\nWe consider both two-dimensional and three-dimensional manifolds. Follow-\ning William Thurston's suggestion that the networks of neurons in the brain\ncould be successfully represented in Solv geometry (one of eight so-called\nThurston geometries), we stipulate that this geometry would outperform hy-\nperbolic geometry.\nAgainst this background, our contribution in this paper can be summa-\nrized as follows:\n\u2022 We present a novel embedding method based on Simulated Annealing\n(SA). Experiments show that our algorithm outperforms the state-of-\nthe-art, even for the hyperbolic embeddings, as evaluated by standard\nmeasures (mAP, MeanRank, greedy routing success, and stretch).\n\u2022 To the best of our knowledge, we are the first to compare embeddings\nof connectomes to all Thurston geometries. As a result, we expand\nthe horizons of connectome modeling and open up new possibilities for\nanalysis. We show that connectome modeling is more nuanced than\npreviously presented.\n\u2022 We find that while three-dimensional hyperbolic geometry yields the\nbest results in many cases, other geometries, such as Solv, are worth\nconsidering. Supported by an extensive simulation scheme, our results\nbring confidence and reliability beyond previous studies.\nThis paper is accompanied with supplementary material containing our\nimplementation, data and results [CKK24a] and a video containing our 3D\nvisualizations [CKK24b]."}, {"title": "Prerequisities", "content": null}, {"title": "Thurston geometries", "content": "By the uniformization theorem, every closed two-dimensional topological sur-\nface can be given spherical (S\u00b2), Euclidean (E\u00b2), or hyperbolic (H\u00b2) geometry,\nthat is, there exists a Riemannian manifold with the same topology as M and\nlocally isometric to a sphere, Euclidean plane, or hyperbolic plane. William\nThurston conjectured [Thu82] that three-dimensional topological manifolds\ncan be similarly decomposed into fragments, each of which can be given one\nof eight Thurston geometries, which are homogeneous Riemannian manifolds.\nThe eight Thurston geometries include:\n\u2022 isotropic geometries: spherical (S\u00b3), Euclidean (E\u00b3), and hyperbolic\n(H\u00b3).\n\u2022 product geometries: S\u00b2 \u00d7 R and H\u00b2 \u00d7 R, In geometry A \u00d7 B, the distance\n$d_{A\u00d7B}$ between ($a_1, b_1$), ($a_2, b_2$) \u2208 A \u00d7 B is defined using the Pythagorean\nformula:\n$d_{AxB}((a_1, b_1), (a_2, b_2)) = \\sqrt{d_A(a_1, a_2)^2 + d_B(b_1, b_2)^2}$.\nIntuitively, using the Pythagorean formula here means that the third\ndimension is added to S\u00b2 or H\u00b2 in the Euclidean way.\n\u2022 Twisted product geometries: twisted E\u00b2 \u00d7 R, also known as Nil, and\ntwisted H\u00b2 \u00d7 R, referred to as Twist in this paper, also known as the\nuniversal cover of SL(2, R). [KCK20]\n\u2022 Solv geometry, also known as Solve or Sol, which is fully anisotropic.\nThe more exotic Thurston geometries have been successfully visualized\nonly very recently [KCK20, CMST20], and thus are much less known than\nisotropic geometries. We refer to these papers and explanatory videos [Rog23,\nRog22] and demos [CMST22] for detailed explanations of Solv and Nil ge-\nometries. In the rest of this section, we include a brief description of Solv\nand an intuitive explanation of twisted product geometries.\nThe n-dimensional sphere is Sn = {x \u2208 Rn+1 : g(x, x) = 1}, where g is the\nEuclidean inner product, g(x, y) = $x_1y_1 + x_2y_2 + ... + x_{n+1}y_{n+1}$. The distance\nbetween two points a, b on the sphere is the length of the arc connecting a\nand b, which can be computed as d(a,b) = acos g(a,b). Similarly, we can\ndefine n dimensional hyperbolic geometry using the Minkowski hyperboloid\nmodel. In this model, H\" = {x \u2208 Rd+1 : $x_{d+1}$ > 0, g\u00af(x, x) = \u22121, where g\u00af is\nthe Minkowski inner product, g\u00af(x, y) = $x_1y_1 + x_2y_2 + ... + x_ny_n - x_{n+1}y_{n+1}$.\nThe distance between two points can be computed as d(a, b) = acosh g\u00af(a, b).\nTypically, tessellations of the hyperbolic plane H\u00b2 are visualized using\nthe Poincar\u00e9 disk model, which is a projection of H\u00b2 to the Euclidean plane\nthat distorts the distances (Figure 1). In each of these tessellations, all the\nshapes (of the same color) have the same hyperbolic size, even though ones\ncloser to the boundary look smaller in the projection.\nTo explain Solv, we should start with the horocyclic coordinate system of\nH\u00b2. Horocycles are represented in the Poincar\u00e9 disk model as circles tangent\nto the boundary; these can be seen as hyperbolic analogs of circles with\ninfinite radius and circumference, centered in an ideal point (point on the\nboundary of the Poincar\u00e9 disk). Figure 1c depicts concentric horocycles; the\ndistance between two adjacent horocycles in this picture is log(2), and if two\npoints A and B on given horocycle are in the distance x, then the distance\nbetween their projections on the next (outer) horocycle is 2x. For a point\nP\u2208 H\u00b2, we project Porthogonally to Q on the horocycle going through the\ncenter C of the Poincar\u00e9 model. The x coordinate is the (signed) length of\nthe horocyclic arc CQ, and y is the (signed) length of the segment PQ. (This\nis similar to the upper half-plane model [CFK+97], except that we take the\nlogarithm of the y coordinate.) In this coordinate system, the length of the\ncurve ((x(t), y(t)) : t \u2208 [a, b]) is defined as $\\int_a^b \\sqrt{(x'(t) exp y(t))^2 + y'(t)^2}dt$.\nA similar coordinate system for H\u00b3 defines the length of the curve ((x(t), y(t), z(t)) :\nt\u2208 [a,b]) as $\\int_a^b \\sqrt{(x'(t) exp z(t))^2 + (y'(t) exp z(t))^2 + z'(t)^2}dt$. The surfaces\nof constant z are called horospheres; the geometry on a horosphere is Eu-\nclidean. We obtain Solv geometry by switching the sign in this formula.\nThat is, each point also has three coordinates (x, y, and z), but the length of\na curve is now equal to $\\int_a^b \\sqrt{(x'(t) expz(t))^2 + (y'(t) exp-z(t))^2 + z'(t)^2}dt$.\nThe distance between two points is the length of the shortest curve connect-\ning them; this length is difficult to compute [CMST20, KCK22].\nIn Nil, we have well-defined directions at every point, which we can in-\ntuitively call North, East, South, West, Up and Down. However, while in\nEuclidean geometry, after moving 1 unit to the North, East, South, and\nWest, we return to the starting point; in Nil, such a loop results in a move\nby 1 unit in the Up direction. In general, the vertical movement is equal to\nthe signed area of the projection of the loop on the horizontal plane. Twist\nis based on the same idea, but the horizontal plane is now hyperbolic."}, {"title": "Geometric embeddings", "content": "In low-dimensional topology, three-dimensional geometry is incredibly chal-\nlenging; mainly, the Poincar\u00e9 conjecture was the most challenging in three\ndimensions. On the other hand, our interest in two-dimensional and three-\ndimensional geometries is based on their visualization possibilities [KCK20,\nCMST20] and potential application to geometric embeddings.\nFigure 1 shows that hyperbolic geometry has a tree-like, hierarchical\nstructure. This tree-likeness has found application in the visualization and\nmodeling of hierarchical structures [LRP95, Mun98], and then in the mod-\neling of complex networks. The hyperbolic random graph model (HRG)\n[BPK10] is parameterized by parameters N, R, T, a. Each node i \u2208 {1, ..., n}\nis assigned a point m(i) in the hyperbolic disk of radius R; the parameter\na controls the distribution. Then, every pair of points a, b \u2208 {1, ..., n} is\nconnected with probability $1/(1+exp((d-R)/T))$, where d is the hyperbolic\ndistance between a and b. A real-world network (V, E) can be also embed-\nded into the hyperbolic plane H\u00b2 by mapping its nodes V to H\u00b2 [BPK10]; an\nembedding is better if the probability of forming the actual observed connec-\ntions (according to the HRG model) is higher. Moreover, graphs generated\naccording to this model have properties typical to scale-free networks, such\nas high clustering coefficient and power-law degree distribution [PKS+12].\nMore recently, embeddings into higher-dimensional hyperbolic spaces were\nstudied in the network [JABS23, WHKL22] and the machine learning commu-\nnity (product geometries in [GSGR19]). \u03a4\u03bf our knowledge, twisted product\nor Solv geometry have yet to be studied in this context. We are especially\ninterested in the intriguing suggestion of William Thurston from 1997 that\nthe brain's architecture might be based on Solv geometry [Sch20]. Intuitively,\nthe Solv geometry is based on two hierarchies (the hyperbolic plane y =const\nand the hyperbolic plane x =const), which are opposed to each other due to\nthe opposite sign used with z in the distance formula. This gives us hope\nthat we can use Solv geometry to represent in three dimensions hierarchies\nthat cannot be represented using other two- or three-dimensional geometries\nexhibiting simpler hierarchical structure (H\u00b2, H\u00b3, H\u00b2 \u00d7 R). A similar effect\nof two opposing hierarchies could also be obtained in H\u00b2 \u00d7 H\u00b2. However, that\nis a four-dimensional geometry and, thus, less suitable for visualization. An\npromising property of Nil geometry is that it is a three-dimensional geometry\nwhere the volume of a ball of radius R has \u0398(R4) growth, which suggests\nbetter embedding possibilities than E\u00b3, but worse than the exponentially-\nexpanding geometries."}, {"title": "Our contribution", "content": "We need a new embedding algorithm since the previous algorithms may be\nparticularly tailored to the specific geometry [BFKL16], or assume that dg\nis easy to compute, which is not true for Solv. We aim to find good quality\nembeddings of a connectome (V, E) into some geometry G, that is, a map\nm : V \u2192 G. As in the hyperbolic random graph (HRG) model, we assume\nthat our embedding has two parameters: R and T. The probability that\nan edge exists between i and jis $p_1(d) = 1/(1 + exp((d \u2013 R)/T))$, where\nd is the distance between m(i) and m(j). We use MLE method to find\nthe embedding, that is, we aim to maximize the likelihood $\\prod_{1<i<j<N} P(i, j)$,\nwhere $p(i, j) = p_1(d_G(m(i), m(j)))$ in case if the edge between i and j exists,\nand $p(i, j) = 1 \u2212 p_1(d(m(i), m(j)))$ otherwise. Equivalently, we maximalize\nthe loglikelihood $\\sum_{1<i<j<N} log p(i, j)."}, {"title": "Our embedding algorithm", "content": "As in [CKK21], our algorithm is based on a uniform grid in geometry G.\nNatural grids exist in all Thurston geometries of interest [KCK20]. In the\nHRG model, the network is mapped to a disk of radius R; here, we map the\nnetwork to the set D of all grid points in G, which are in the distance at\nmost dr from some fixed origin. We choose dr to fix the number of points\ninside D; in most experiments, we pick M = 20000 points (actually, there\nmay be slightly more points due to ties).\nWe compute the distance de for every pair of points in D, thus obtaining\na |D|\u00d7 |D| array that can be used to quickly find the distance between pairs\nof points. In the case of Solv, it turns out that the method to compute\nthe Solv distances from [KCK20], while applicable to visualization, does not\napply to computing this table of distances due to long ranges. Therefore, for\nlonger distances, we approximate by d(a, b) as the smallest possible d(a, a\u2081)+\nd(a1, a2) + . . . + d(ak, b), where intermediate points are also in D, and each\npair of consecutive points is within the range of the method from [KCK20].\nDijkstra's algorithm is used to find the path (ai).\nNow, we use the Simulated Annealing method to find the embedding.\nThis method assumes R and T and starts with an arbitrary embedding m :\nV \u2192 D. Then, we perform the following for i = 1, . . ., Ns:\n\u2022 Introduce a small change m' to the current embedding m,\n\u2022 Compute L, the loglikelihood of m, and L', the loglikelihood of m'.\n\u2022 If L' > L, always replace m with m'. Otherwise, replace m with m'\nwith probability exp((L' - L)/exp(T)), where the parameter T (known\nas temperature) depends on the iteration index.\nIn Simulated Annealing, we start with a very high temperature (to accept\nall changes and thus explore the full space of possible embeddings without\ngetting stuck on local maxima). Then we proceed to lower and lower tem-\nperatures (not accepting changes that yield much worse embeddings but still\nexperimenting with crossing lower valleys), eventually accepting only the\nchanges that improve the embedding. In our experiments, T decreases lin-\nearly from 10 to -15. We consider local changes of two possible forms: move\nm'(i) for a random i to a random point in D, and move m'(i) for a random\ni to a random point in D that is close (neighbor) to m(i). These changes\nallow computing L' (based on the earlier L) in time O(|V|).\nTo obtain values of R and T, we start with some initial values of R and T.\nOccasionally, during the simulated annealing procedure, we find the values of\nRand T that best fit the current embedding, and we use the new values for\nthe remaining iterations. Since finding the correct values takes time, we do\nit relatively rarely (every |V| iterations with successful moves) and only once\nthe simulated annealing procedure rejects most changes. In our experiments,\nwe repeat this setup 30 times; in the following iterations, we start with the\nvalues of R and T that were obtained in the best embedding found so far.\nThe time complexity of an iteration is O(Ns \u00b7 |V|).\nOur implementation uses the tessellations implemented in RogueViz [KCK23]\nand is based on the existing implementation of Simulated Annealing for find-\ning hyperbolic visualizations [CK17]."}, {"title": "Visualization", "content": "In Figure 2, we present selected embeddings for the human cortex, rat ner-\nvous system, and zebra finch basal-ganglia connectomes. The embeddings\nexhibit different shapes: e.g., rat nervous system connectomes are star-like,\nwith a group of neurons in the center of the embedding connected to other\nneurons, with few other connections. Different geometries highlight these\ndifferences. E.g., such star-like networks embed well into H\u00b3 or trees. Our\nvisualization engine lets the viewer rotate the embedding and examine the\nspatial relationships in detail. See the videos in the supplementary material."}, {"title": "Experiments", "content": "For our experiments, we use the same set of publicly available connectomes\nas [AS20]\u00b9 (not all connectomes used there are publicly available). See Table\n1.\nWe run 30 iterations of SA to find the best R and T, with Ns = 10000\u00b7|V|.\nWe evaluate the quality of embeddings using the following five measures (all\nranging from from 0 - worst to 1 \u2013 perfect).\nSC Greedy routing success rate. SC is the probability that, for random\npair of vertices (x,y) \u2208 V2, the greedy routing algorithm starting at\nx eventually successfully reaches the target y. This routing algorithm\nmoves in the first step from 1 to x1, the neighbor of x the closest to y\n(that is, dc(m(x1), m(y)) is the smallest). If x1 \u2260 y, we continue to x2,\nthe neighbor of x\u2081 the closest to y, and so on.\nIST Greedy routing stretch. Stretch is the expected ratio of the route length\nfound in the greedy routing procedure to the shortest route length,\nconditional that greedy routing was successful. IST is the reciprocal of\nstretch.\nIMR For an edge (x, y) \u2208 E, rank(x, y) is one plus the number of vertices that\nare closer to x than y but not connected with an edge. MeanRank is\nthe expected value of (x, y) over all edges. We use IMR=1/MeanRank.\nMAP For an edge (x, y) \u2208 E, P(x, y) is the ratio of vertices in distance of at\nmost dc(m(x), m(y)) to x which are connected with x. AP(x) is the\naverage of P(x, y) for all y connected with x, and MAP is the average\nof AP(X) over all X."}, {"title": "Comparison at maximum performances", "content": "We start with a naive comparison among the tessellations based on the best\nresults obtained for each tessellation for each connectome. Figures 3, 4, 7, 6,\nand 5 visualize the rankings of the tessellations. 1s (the top) are the highest\nresults, and 0s (the bottom) are the lowest for a given connectome.\nAccording to Figures 3-5 and Tables 4 and 5, we notice that the assess-\nment of the performance of the geometry may vary concerning the quality\nmeasure; there are also differences across species. In general, trees perform\npoorly in measures other than greedy success rate, and no matter the mea-\nsure, they are always the best choice for Rat's connectomes (nervous system).\nResults for Rat's and Drosophila2's connectomes are also characterized by\nthe relatively high variation among species (Table 3). For other species, the\nbest performances are similar with respect to a quality measure: the differ-\nences in best performance among geometries measured with MAP, greedy\nrate success, and stretch are slight (in most cases, values of CVs are under\n10%); especially for Cat's connectomes, they tend to be negligible (values of\nCVs even under 1%).\nThe results suggest that H\u00b2& and S\u00b3 seem to be inefficient choices: the"}, {"title": "Distribution-based comparison", "content": "Comparison of the maximum performance from the previous section gives us\nintuition about the optimistic scenarios and the limits for our embeddings.\nHowever, due to the nature of Simulated Annealing, the maximum values\nwe obtained are still realizations of random variables; that is why a closer\ninspection, including information about the distributions of the simulation\nresults, is needed. To this end, we will compare geometries using voting rules.\nIn particular, we will be interested in finding Condorcet winners and losers.\nAs Condorcet winner may not exist in the presence of ties, we will refer to\nits simple modification: Copeland rule [MD04].\nGeometry A wins against geometry B if the probability that (for a given\nquality measure) a randomly chosen simulation result obtained by A is greater\nthan a randomly chosen simulation result obtained by B exceeds 0.5. If that\nprobability is equal to 0.5, we have a tie between A and B; otherwise, A\nloses against B. To compute the score for a given geometry, we add 1 for\nevery winning scenario, 0 for every tie, and -1 for every losing scenario. The\ngeometries with the highest and lowest scores become Copeland winners and\nlosers, respectively (we allow for multiple candidates in both cases).\nThe winners based on the Copeland method beat most of the other candi-\ndates in pairwise contests. They should be the best options for embeddings.\nBased on the data in Table 6, we cannot name one universal winner. While\nit seems that H\u00b3 is a sound choice, we also notice that Solv and Twist are\nworthy of attention. Interestingly, for Human connectomes, E\u00b3 outperforms\nother geometries.\nIn Figures 8-12, we provide weighted directed networks constructed upon\nthe voting rules to allow for generalizations. The weights correspond to the\npercent of connectomes for which the source geometry in the edge beats the\ntarget geometry. Embeddings to Twist have a 100% success rate over em-\nbeddings in H\u00b2 (for quality measures different than greedy routing success)."}, {"title": "Zone-function-based comparison", "content": "We have already shown that, contrary to previous results from the literature,\nwe cannot name one universal best geometry to model any connectome. A\nnew interesting question arises if there are relationships between the function\nof the connectome (based on its zone) and the suitability of the geometries\n(using the rankings from the distribution-based comparisons). To find out,\nwe analyze the values of intraclass correlation coefficients (ICC), a widespread\ntool in the assessment of consistency among multiple raters [SF79] when the\nrating scale is ordinal to continuous. The literature suggests that the values\nof ICC below 0.50 indicate poor agreement, between 0.50 and 0.90 suggest\nmoderate to good agreement, and above 0.90: an excellent one [KL16]. To\nobtain an aggregate ranking for the whole zone, for each measure and each\ngeometry we computed the median of its minus rank (the minus gives us\ninformation 50% of ranks achieved by the given geometry are at least this\nhigh in modelling given zone, the lower values, the better). Medians were\nchosen due to their immunity to outliers.\nAccording to data in Table 7, the intra-zone comparisons suggest good\nto excellent agreement no matter the quality measure. Rankings for con-\nnectomes from \u201cother\u201d zone (usually specific cells, e.g., from retina or op-\ntic medula) show relatively lower agreement. On the contrary, the inter-\nzone comparison suggests poor to moderate agreement between the rankings.\nWhile the p-values in significance tests for ICCs in inter-zone comparisons\nsuggest significance at any reasonable significance level (even after Bonferroni\ncorrections), the results for intra-zone comparison after Bonferroni correc-\ntions appear insignificant (so any similarities might be random), apart from\nthe result for the SC measure. Those results are promising for us. They\nsuggest that the choice of the suitable geometry may depend on the function\nof the connectome. For example, our results suggest that the trees are best\nchoice in modelling nervous systems (no matter the quality measure), for\ncortex H\u00b3*, H\u00b2\u00d7R, Nil* or Solv* would be a suitable choice, and Twist may\nbe beneficial for modelling specific cells.\nWhile a natural question arises about the anatomical implications of dif-\nferent best fits for geometries, as well as why different connectomes might\nhave different best geometries, answering this question in a statistically ro-\nbust manner would require a more detailed study on a bigger number of\nsample connectomes."}, {"title": "Robustness checks and threats to validity", "content": "Ideally, there exists optimal embedding of (V, E) into the whole geometry G,\nwhere $m_{opt}$: V \u2192 G, and some values of R and T are used. Unfortunately,\nthe embedding m found by Simulated Annealing might be worse than $m_{opt}$\ndue to the following issues:\n\u2022 The radius dr is too small, making $m_{opt}$ simply not fit,\n\u2022 The grid used is too coarse, hence the necessity of making m(i) the\ngrid point to closest to $m_{opt}(i)$, and thus reducing the log-likelihood,\n\u2022 The number of iterations of Simulated Annealing, Ns, is too small\nwhile Simulated Annealing is theoretically guaranteed to find the\noptimal embedding for given R and T with high probability as Ns\ntends to infinity, in practice, we are constrained by computation time\nlimits,\n\u2022 The values of the parameters R and T have not been chosen correctly.\nIn this section, we will explain how we combated those issues. We will\nalso check if they affected our results.\nPossibly insufficient size of grids. For comparability, we aimed to keep\nthe number of neurons as close to 20,000 as possible. However, one could\nargue if this is enough. To combat the first two issues, in some geometries, we\nconsider coarser and finer grids: coarser grids are better at handling the first\nissue, and finer grids are better at handling the second issue \u2013 in both cases,\nwe expect that increasing dr and grid density beyond some threshold yields\ndiminishing returns. That is why, based on the results from the previous\nsections, we have added the so-called big versions coarser but larger grids\n(M = 100000) \u2013 for selected, promising manifolds (H\u00b3, H\u00b3*, H\u00b2 \u00d7 R, Solv,\nand Twist). We will denote them with **. See Table 8 for the details.\nWe started by checking for significant differences in favor of big versions of\nmanifolds; to this end, we performed Wilcoxon tests with Bonferroni correc-\ntion for multiple comparisons. Figure 13 depicts the procedure results. Ac-\ncording to our results, in most cases, the differences are insignificant, which\nsuggests that the size of the manifold is not a severe threat to validity. Us-\nage of big versions usually results in better embeddings for Rat connectomes,\nwhich might correlate with a different function of those connectomes com-\npared to others in the sample (they describe nervous systems). Rarely, big\nversions yield worse embeddings than the standard ones usually for Human\nconnectomes; however, no pattern-enabling explanation is noticeable here.\nNext, we checked if the size of the manifolds affects rankings. To this end,\nwe computed weighted Cohen's kappas [Coh68]. In kappas, 0 represents the\namount of agreement expected from random chance, and 1 signifies a perfect\nagreement between the raters. Initially, kappas take into account only the\nagreements of the raters. The weighted kappas allow disagreements to be\nweighted differently, which is more suitable for us we are more interested\nin the relative placement of the pairs of the geometries in the ranking than\nin the actual places. If there are slight differences in ranks by two raters,\ne.g., by one, the ranks should remain similar to us as embeddings yielding\ncomparable quality results should still be close to each other. Although there\nare no universal guidelines for interpreting of those coefficients, the literature\nsuggests that the values over 0.61 indicate moderate to substantial agreement\nbetween raters and values exceeding 0.81 \u2013 strong to almost perfect agreement\n[LK77].\nAccording to data in Table 9, rankings obtained from big versions of man-\nifolds in the standard setup of Simulated Annealing (Ns = 10,000 iterations)\nare at least in substantial agreement with rankings based on standard ver-\nsions. The high agreement in rankings based on voting rules is unsurprising.\nIt aligns with the results depicted in Figure 13 we include more information\nfrom the distributions, so the results should be more robust than those based\non max performance (outliers). However, we recommend cautiously treating\nthe results for greedy routing success and stretch.\nPossibly insufficient number of iterations. As Simulated Annealing is\na probabilistic technique for approximating the global optimum of a given\nfunction, one could argue that, e.g., increasing the number of iterations could\nimprove our results (the third issue). The main paper describes the results\nobtained with Simulated Annealing with Ns = 10,000\u00b7|V| iterations per\nsimulation iteration. We also checked if our results differ if we perform Sim-\nulated Annealing with Ns = 100,000 \u00b7 |V| iterations per simulation iteration\ninstead. As expected, for log-likelihood, MAP, and MR, we cannot reject the\nhypotheses that the results obtained with larger numbers of iterations are\nusually better. However, surprisingly, for greedy success rate and stretch,\nthe results worsen with the increase in the number of iterations (Figure 14\ndepicts the results of Wilcoxon tests with Bonferroni correction for multiple\ncomparisons).\nWe checked if the number of iterations for Simulated Annealing Ns, af-\nfected our results regarding rankings with Cohen's kappas. Based on the\ndata in Table 9, we notice that pairs of rankings are in at least substantial\nagreement. The results regarding standard grids (presented in the main part\nof the paper) are robust to Ns \u2013 the values of kappas for optimistic scenarios\nare over 0.85. For rankings based on voting rules, they usually exceed 0.80.\nAlthough the agreements of rankings, if we change Ns for big grids, are still\nsatisfying (most values of kappas over 0.75), the results from comparison of\nrankings based on standard grids with shorter time for Simulated Anneal-\ning against the big grids with longer time for Simulated Annealing suggests\nthat the big versions of grids might be affected by Ns. Again, we notice\nthat greedy routing success rate and stretch are less immune to the setup of\nSimulating Annealing, so we suggest caution while generalizing the results\nobtained for them.\nAlternative methods of obtaining R and T The fourth issue is chal-\nlenging. As explained in Section 4, the values of R and T have been obtained\nby dynamically adjusting them during the simulated annealing process (A).\nWe have also experimented with other methods: Ris changed, but Tre-\nmain fixed (B), and both R and T remain fixed. We run 30 iterations using\nmethod (A), then 30 iterations using method (B), then 30 iterations using\nmethod (C). The fixed values of R and T are based on the best result (by\nlog-likelihood) obtained in the earlier iterations.\nIf the methods change the results, we should notice level shifts in the\ntime series of the quality measures' values \u2013 level shifts appear as a parallel\nmovement of the trend line. That is why we started by identifying possible\nlocations of the level shifts in our results. Most of the time series (determined\nby a pair animal and geometry) has two level shifts \u2013 around the 30th and\n60th iterations that correspond to the starting points of new methods (Figure\n15).\nWe use OLS regressions to understand the impact of the change in method\non the values of the quality measures. We control for the characteristics of\nconnectomes: number of nodes in the connectome n, number of edges in the\nconnectome m, its density, assortativity and clustering coefficients, and the\nzone of the connectome; we also take into account the number of available\ncells in the grid and its geometry."}, {"title": "Details on tessellations used in our study.", "content": "By a distance between\ntwo points a,b \u2208 D, we mean the length of the shortest geodesic between\na and b. Another option is the graph distance, where two points in D are\nconnected when they correspond to adjacent tessellation tiles. In the case\nof the product geometry H\u00b2 \u00d7 R, we could compute the angular distance,\nwhich is, intuitively, how small an object at b appears to an observer placed\nat a, assuming that the light travels along geodesics. To pick the method of\nmeasuring the distances in our experiment, we started with the preliminary\nlist of tessellations shown in Table 11.\nIn the tessellations marked with (d), distances are computed as the lengths\nof the shortest paths in the graph (D, ED) where two points in D are con-\nnected when they correspond to adjacent tessellation tiles. In contrast, in\nthe tessellations marked with (c), distances are computed according to the\nunderlying geometry."}]}]}