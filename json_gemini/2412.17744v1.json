{"title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation", "authors": ["YANLI WANG", "YANLIN WANG", "SUIQUAN WANG", "DAYA GUO", "JIACHI CHEN", "JOHN GRUNDY", "XILIN LIU", "YUCHI MA", "MINGZHI MAO", "HONGYU ZHANG", "ZIBIN ZHENG"], "abstract": "Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world repository-level code translation benchmark with an automatically executable test suite. We conduct experiments on RepoTransBench to evaluate the translation performance of 11 advanced LLMs. We find that the Success@1 score (test success in one attempt) of the best-performing LLM is only 7.33%. To further explore the potential of LLMs for repository-level code translation, we provide LLMs with error-related feedback to perform iterative debugging and observe an average 7.09% improvement on Success@1. However, even with this improvement, the Success@1 score of the best-performing LLM is only 21%, which may not meet the need for reliable automatic repository-level code translation. Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements.", "sections": [{"title": "INTRODUCTION", "content": "Code translation refers to translating code from one programming language to another while preserving the functionality of the source code [1-4]. It has broad applications, including refactoring code written in outdated languages [5], transitioning from simple but slow languages to more complex and faster ones [6], enabling programming language migration in software development [7\u2013 14], and addressing data scarcity issues through synthetic data generation [15, 16]. Automatic code translation can significantly reduce manual effort and has thus garnered widespread attention in recent years [17-25]. With the popularity of large language models (LLMs), researchers are trying to translate code with LLMs, yielding promising results [26-28]. To evaluate the performance of code translation tools, various benchmarks have been introduced [29-38]. Based on translation granularity, current fine-grained code translation benchmarks can be classified into three levels [1]: snippet-level, function-level, and file-level. Specifically, snippet-level code translation benchmarks, such as CoST [29], XLCost [30], typically refer to evaluating the translation of program segments, that are located between two consecutive code comments. Function-level code translation benchmarks, such as TransCoder-test [31], CodeXGIUE [32], HumanEval-X [33] and EvalPlus [34], focus on evaluating the translation of a"}, {"title": "BACKGROUND", "content": ""}, {"title": "2.1 Code Translation", "content": "Code translation involves converting source code written in one programming language into another language while preserving the original program's functionality and logic [1-4]. This process is essential in software engineering for several reasons, such as migrating legacy systems to modern languages [7, 8], improving code performance by translating to more efficient languages [5] and enabling cross-platform compatibility [6]. As programming languages continue to evolve, the demand for accurate and efficient code translation techniques has grown, making it a critical area of research and development [17-25]. Many studies focus on code translation, which can be broadly divided into two main categories: non-learning-based and learning-based approaches [1]."}, {"title": "2.2 Code Translation Granularity", "content": "Figure 1 shows a comparison of code granularities used in different code translation approaches. Most previous works focus on the translations with a granularity not exceeding a single code file (left-hand side). Specifically, snippet-level code translation [29, 30] typically refers to evaluating the translation of program segments that are located between two consecutive code comments, and each program may consist of one or more code snippets. Function-level code translation [29-34] refers to translating a function into another programming language, with the data sources often being manually crafted datasets [103] or coding practice websites [104]. File-level code translation [35-38, 93, 105] often refers to translating a complete program file into the target language. The data sources are usually from code contest platforms [106-109] or task solutions websites [110-112]. G-TransEval [113] also provides a more fine-grained taxonomy, including token-level, syntax-level, library-level, and algorithm-level, which is part of a function. Unlike the previous fine-grained granularity code translation, repository-level code translation involves migrating an entire repository from one language to another. Recently, repository-level code translation has gradually gained the attention of researchers [1]. As shown in the right-hand side of Figure 1, a typical code repository contains functional code files and test code files and may also include resource files and configuration files. Functional code files refer to those code files that implement specific functionalities of the code repository, such as the files located in the readtime/ directory. Test code files refer to the files used to verify the correctness of the functional code where test_readtime.py is an example. Resource files like those in samples/ folder are used to test the functional correctness of the functional code. The functional code needs to implement how to perform I/O operations with these resources. In addition, for certain language-specific frameworks, it is necessary to complete the configuration file correctly, such as the \u201cpom.xml\u201d file in Java's Maven [41] repositories. Compared with previous fine-grained code translation granularity, repository-level code translation has the following four differences: \u2022 Diff 1: Complex Functional Implementation. Real-world code repositories typically include numerous functions, classes, and import statements to realize complex functionalities. This characteristic requires translators to have an understanding of the entire repository, which may involve a long code length, necessitating that translators have sufficient processing ability. \u2022 Diff 2: Challenging File Management. Code repositories often exhibit complex file structures, necessitating that translators effectively manage these structures during translation. Furthermore, code repositories may require an appropriate setup of configuration files, such as the \u201cpom.xml\u201d file in a Java Maven repository, to make the project compilable. \u2022 Diff 3: Resource Existence. Except for migrating code files, those resource files that may perform I/O operations with code files should also be migrated together. This aspect has not been explored in previous work [1]. \u2022 Diff 4: Test Case Existence. Unlike previously crafted datasets [35, 36], for repository-level code translation, the source repository usually contains corresponding test cases, so making full use of the existing test cases may achieve better performance. Recent studies have shown interest in repository-level code translation. Pan et al. [1] attempt to perform mutual conversion between Python and Java projects [39, 40], but find that the advanced LLMs are largely ineffective, with success rates of 8.1% for GPT-4 and 0% for the rest of the models."}, {"title": "REPOTRANSBENCH", "content": "In this section, we will introduce the two steps of building RepoTransBench : data collection and test case construction. Besides, we will provide the statistics between RepoTransBench and previous fine-grained benchmarks."}, {"title": "3.1 Data Collection", "content": "As shown in Figure 2, the data collection pipeline of RepoTransBench consists of four steps: Preliminary Filtering, Info Updating, Repo Downloading, and Final Filtering."}, {"title": "3.1.1 Preliminary Filtering.", "content": "We select Python as the source language and Java as the target language for our benchmark. Referring to the TIOBE index [46] for programming language popularity, the top five languages are Python, C++, Java, C, and C#. Given Python's popularity for its simplicity, ease of use, and suitability for rapid prototyping, alongside Java's reputation for enterprise-level applications due to its better performance, stability, and faster execution [6], we explore translating Python code to Java. For the data source, we choose to use the training data of StarCoder [44, 45] series models, The Stack and The Stack v2. We download the Python portion information of The Stack and The Stack v2 from Hugging Face [114] and group the information by repository name (\u201cmax_stars_repo_name\u201d for The Stack and \u201crepo_name\u201d for The Stack v2) to aggregate code files belonging to the same repository. To ensure that the repositories we select are publicly recognized, we filter out those with fewer than 10 stars, obtaining the Raw Repo List. It is worth noting that the star count here is outdated and tends to be lower than the actual number. Further filtering will be conducted later."}, {"title": "3.1.2 Information Updating.", "content": "Due to using the early-released data source, some information may be outdated in Raw Repo List, where star counts tend to be lower than the actual numbers, and many repositories are forks (a commonly occurring phenomenon, especially in The Stack), we use the GitHub REST API [115] to update the information and apply further filtering. We check whether a repository exists, removing non-existing repositories. For those repositories forked from others, we redirect to the origin repository and use the original repository information. We also find that although we filter the data from the Python dataset in Section 3.1.1, many repositories have a main language other than Python. To ensure we obtain a Python repository, we require the percentage of Python language to be the largest among all languages in a repository, excluding those frontend languages (e.g., HTML, CSS). We then use the updated star count for further filtering, retaining repositories with more than 50 stars. Then we obtain the cleaned Updated Repo List."}, {"title": "3.1.3 Repository Downloading.", "content": "We clone the raw repositories in the Updated Repo List to the local environment using Git command [116] to enable further filtering."}, {"title": "3.1.4 Final Filtering.", "content": "To ensure the quality of the repositories, we perform further filtering on downloaded repositories. Firstly, we use a blacklist to block repositories with packages which is difficult to implement from scratch. It is worth noting that we aim to evaluate individual repositories. However, due to contributions from the open-source community, a Python repository may include many third-party packages, like pytorch [117], tensorflow [118], etc. These packages do not have corresponding implementations in Java language, and it is challenging to implement these libraries from scratch. Therefore, these libraries are not within the scope of our research. We manually review 200 Python repositories and obtain a package blacklist. If a repository contains any package"}, {"title": "3.2 Test Case Construction", "content": "To evaluate the quality of the translations, test cases of target code are needed. HumanEval-X [33] involves manually rewriting test cases into the target language, whereas EvalPlus [34] utilizes ChatGPT [125] for the automated generation of test cases. However, relying solely on manual translation to generate test cases for repository-level code can be quite labor-intensive, requiring human translators with substantial development expertise in both Python and Java. While test cases generated exclusively by LLMs may not always be reliable. Therefore, we choose to construct test cases of the target language through a collaborative effort involving human translators and LLMs to ensure both quality and efficiency. The test case construction pipeline consists of four steps, including (1) Translation Demonstration, (2) Automatic Translation, (3) Resources Migration and Path Transformation, and (4) Execution-Based Iterative Manual Checking."}, {"title": "3.2.1 Translation Demonstration.", "content": "To ensure the quality of the translation assessment, we standardize the format of the translations, as the implementation of the repository code may vary during translation. We provide a demonstration shown in Figure 3 to help understand the translation process. We will introduce the translation demonstration from three aspects: framework selection, mapping method, and language feature handling."}, {"title": "3.2.2 Automatic Translation.", "content": "Here we translate not only the test code files but also the functional code files to further ensure the correctness of the generated test cases (details in Section 3.2.4). We leverage GPT-40 [126] as the backbone LLM to perform translation. The context window of GPT-40 is 128k. For repositories where the total token count exceeds this limit, we split the repository into several chunks, making sure that each chunk consists of several complete code files and does not exceed the context window. These chunks are then provided to GPT-4o in a multi-turn conversion format. Through the above approach, we obtain the raw translations without corresponding resources."}, {"title": "3.2.3 Resource Migration and Path Transformation.", "content": "As shown in Figure 3, after obtaining the raw translations from the previous step, we manually migrate the resources to the corresponding"}, {"title": "3.2.4 Execution-Based Iterative Manual Checking.", "content": "To ensure the test cases are complete and free from syntax errors and calls to non-existent APIs, which may be difficult to discover and labor-intensive by manual inspection, we use compilability as a metric to evaluate the quality of test cases. We execute the repositories obtained from the previous step to evaluate the compilability. Three of the authors are involved as participants to check the correctness of test cases. These participants have 3-6 years of coding experience. Two of the participants independently check the completeness of the test cases to ensure their test range is consistent with those in the source repository. They also correct errors that occur during the execution until the repository is compileable or more than 30 minutes are consumed. For repositories that fail to be compiled, the two participants ensure the errors do not originate from the test cases by inspecting and fixing related code. Another participant performs a double-check on the results of the previous two participants. For repositories deemed contentious by the three participants, they discuss and achieve a mutually agreed solution. Ultimately, 83% of the repositories compile successfully from the processing results of the two participants, which means the test cases of these repositories are free from syntax errors and calls to non-existent APIs. For the remaining 17% of the repositories, the three participants discuss and maintain that the issues mentioned above are not present in the test cases."}, {"title": "3.3 Statistics of RepoTransBench", "content": "Table 1 shows the statistics of RepoTransBench compared with other existing code translation benchmarks. To ensure fair statistics, we only take into account the portions of multilingual benchmarks where Python serves as the source language. Unlike previous fine-grained benchmarks, RepoTransBench uses the entire repository as its data samples. Since the goal of translation is to translate the entire code repository, it also requires the correct configuration of project files, such as the \u201cpom.xml\u201d file for a Java Maven project. To better evaluate the syntactic and functional"}, {"title": "4 EXPERIMENTAL SETUP", "content": ""}, {"title": "4.1 Research Questions", "content": "We aim to answer the following key research questions (RQs) that explore the utility of RepoTrans- Bench for evaluating diverse LLMs for the repository-level code translation task: \u2022 RQ1 (Performance of Translation): How do the recent advanced general and code LLMs perform in repository-level code translation? \u2022 RQ2 (Performance of Iterative Debugging): Can iterative error-related information feed- back to LLMs further improve their performance? \u2022 RQ3 (Effect of Code Length Functional Complexity): How do code length and functional complexity of the repository affect the performance of translation? \u2022 RQ4 (Error Analysis): What are the main types of errors that occur in repository-level code translation?"}, {"title": "4.2 Model Selection", "content": "As shown in Table 2, we select 11 advanced LLMs from five different companies as our subject LLMs, which include 8 general LLMs (4 open-source and 4 closed-source LLMs) and 3 code LLMs. For general open-source LLMs, we include Meta's three latest LLMs, Llama-3.1-Instruct, with parameter sizes of 8B, 70B, and 405B. The 405B LLM is claimed to be comparable with closed- source LLMs. Additionally, we study DeepSeek-V2.5 [91], a Mixture-of-Experts (MoE) LLM that was updated in early September 2024 by DeepSeek. DeepSeek-V2.5 combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating the general and coding abilities of the two previous versions. The coding and math capabilities surpass many closed-source LLMs [137]. For general closed-source LLMs, we examine three well-known LLMs from OpenAI, GPT-3.5-Turbo-16k [125], GPT-4 [132], and GPT-40 [126]. Furthermore, we also evaluate one of Anthropic's advanced LLM, Claude-3.5-Sonnet [133], in our experiments. For code LLMs, we select three advanced LLMs from"}, {"title": "4.3 Evaluation Metrics", "content": "We evaluate the translation and debugging performance by the following metrics: \u2022 Success@k (test success in k attempts): The metric Success@k measures the percentage of repositories that successfully pass all test cases in at least one of the k selected rounds of experiments. Let $T_i$ represent the set of repositories that pass all test cases in the i-th round of experiments. $|T_i|$ denotes the number of repositories that have test success in round i. The general formula for Success@k is given by:\n\nSuccess@k = \\frac{1}{\\binom{n}{k} \\times N} \\sum_{1 < i_1 < i_2 < ... < i_k \\leq n} |T_{i_1} \\cup T_{i_2} \\cup ... \\cup T_{i_k} |\n\nwhere $\\binom{n}{k}$ is the binomial coefficient, representing the number of ways to choose k rounds from the n rounds of experiments. $T_{i_1} \\cup T_{i_2} \\cup \\cdot\\cdot\\cdot \\cup T_{i_k}$ represents the union of test success sets from the selected k rounds. In our experiments, n = 3 and N = 100 because we repeat the experiments three times under the same settings and the number of repository samples in our benchmark is 100. \u2022 Build@k (build success in k attempts): The metric Build@k measures the percentage of repositories that successfully build in at least one of the k selected rounds of experiments. Let $B_i$ represent the set of repositories successfully built in the i-th round of experiments. $|B_i|$ denotes the number of repositories that build successfully in round i. The general formula for Build@k is analogous to Success@k and is given by:\n\nBuild@k = \\frac{1}{\\binom{n}{k} \\times N} \\sum_{1 < i_1 < i_2 < ... < i_k \\leq n} |B_{i_1} \\cup B_{i_2} \\cup ... \\cup B_{i_k} |\n\nwhere $\\binom{n}{k}$ is the binomial coefficient, representing the number of ways to choose k rounds from the n rounds of experiments. $B_{i_1} \\cup B_{i_2} \\cup \\cdot\\cdot\\cdot \\cup B_{i_k}$ represents the union of build success sets from the selected k rounds. \u2022 APR (average pass rate of test cases): The metric APR measures the average percentage of test cases passed across all repositories and rounds in the benchmark. It is calculated as follows:\n\nAPR = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{N} \\sum_{j=1}^N \\frac{N_{pt}^{(i,j)}}{N_{at}^{(i)}}\n\nwhere $N_{pt}^{(i,j)}$ represents the number of passed test cases for the i-th repository in the j-th round, and $N_{at}^{(i)}$ represents the total number of test cases for the i-th repository. n is the total number of experimental rounds, and N is the total number of repositories in the benchmark. It is worth noting that many previous works use similarity-based metrics like BLEU [138], CodeBLEU [16], or other metrics which calculate the overlapping tokens between references and translations [5, 7, 38, 139, 140] to evaluate the quality of translations. However, similarity-based metrics ignore the syntactic correctness and functional correctness of translations [31]. On the"}, {"title": "4.4 Execution Environment", "content": "To prevent LLMs from generating malicious code that could execute on the local machine and cause damage, we run the generated code in a sandbox (an isolated environment). We use Docker [141] as our code execution space. To check the executability of the Python projects in Section 3.1.4, we install both Python 2.7 [142] and Python 3.10 [143] versions in the Docker container and all needed packages by PyPI pip [144]. For the Java environment, we consistently use the Oracle JDK 11 [145]. Additionally, we bridge Docker's network with the local machine, allowing it to access the internet to ensure the dependencies in the \u201cpom.xml\u201d file can be successfully installed."}, {"title": "4.5 Evaluation Process", "content": "As shown in Figure 5, the evaluation process includes three phases: translation, verification, and debugging. We conduct three rounds of experiments in the same setup. Each round of experiments consists of one translation phase and five debugging phases, with each phase followed by a verification phase. Translation: In previous function-level code translation benchmarks [33, 34], test cases are given to let LLMs know the input and output format. We follow this practice for the repository-level code translation. Given a Python repository, we provide the corresponding Java test cases in the system prompt as the translation goal. We perform a topological sort using tree-sitter [129] on the files within the Python repository based on their call relationships, which ensures that any file being called appears before the file that calls it. Then we provide the sorted Python files and Java repository tree to form the translation prompt. We provide the system prompt and translation prompt to the LLM to generate a series of Java files (including functional code files and configuration files, without test code files). In our experiment, we follow the sampling strategy of the LLMs as adopted by Pan et al. [1], using a temperature of 0.7. For translations that are not fully generated in"}, {"title": "5 EVALUATION RESULTS", "content": ""}, {"title": "5.1 RQ1: Performance of Translation", "content": "Table 3 provides an overview of the code translation performance on studied LLMs. The results illustrate significant differences in the abilities of these LLMs to perform repository-level code translation. Claude-3.5-Sonnet, which is the best-performing LLM among all LLMs, achieves 7.33% on Success@1, 28.33% on Build@1, and 16.5% on APR. However, it achieves only 12% test success even for three attempts. For open-source general LLMs and open-source code LLMs, the best- performing LLM only achieves 6% and 7% on Success@3, respectively. These results indicate that the current capabilities of LLMs are limited and insufficient to consistently generate executable and functionally correct translations."}, {"title": "5.2 RQ2: Performance of Iterative Debugging", "content": "Figure 6 presents the iterative debugging performance of LLMs across five iterations. We observe an average 7.09% improvement on Success@1 across all LLMs. GPT-40 performs best on most metrics after five rounds of iterative debugging. Specifically, the Success@1 and Success@3 increase from 4% to 21% and from 8% to 34%, respectively. Notably, from the 3-th iteration of iterative debugging, GPT-40 surpasses Claude-3.5-Sonnet in all Success@k metrics, despite Claude-3.5-Sonnet being the best-performing LLM in the previous translation-only evaluation. This improvement is likely due to GPT-40's superior debugging capabilities, which allow it to refine and enhance its translations more effectively across iterations. Despite these improvements, the best-performing LLM, GPT-40, only achieves 21% on Success@1 and 34% on Success@3, respectively. This performance may indicate that current LLMs still cannot consistently achieve reliable repository-level code translation."}, {"title": "5.3 RQ3: Effect of Code Length and Functional Complexity", "content": "Figure 7 shows the effect of repository length and complexity on translation quality. We analyze the experiment results of both translation (Section 5.1, \u25a0) and iterative debugging phases (Section 5.2, \u25a0). It is evident that the median for success/build outcomes across almost all four complexity metrics is lower than the median for non-success/non-build outcomes for all the LLMs. This suggests that longer code length (with more #Tokens and #Lines) and higher functional complexity (with more #Functions and #Imports) may be more challenging for LLMs to translate or debug successfully. The increased code length and functional complexity lead to a higher likelihood of failure in generating executable and functional correct translations."}, {"title": "5.4 RQ4: Error Analysis", "content": "We conduct an error analysis across three rounds of experiments, ultimately identifying common errors in repository-level code translation. These errors are classified into five categories: E1 (Configuration File Issues), E2 (Limited Understanding Ability Issues), E3 (Incomplete Generation Issues), E4 (Language Feature Issues), E5 (Encoding Issues). Limited by the space, we provide two cases for each category. E1. Configuration File Issues often arise due to the configuration file (\u201cpom.xml\u201d in a Java Maven project) related content not being configured correctly. Figure 8 (E1.1) shows an error due to an unresolved dependency as the artifact cannot be found in the central Maven repository [146]. Figure 8 (E1.2) shows that the current Java version is unsupported. This occurs as the Java version is not explicitly configured in the \u201cpom.xml\u201d file, the Maven project uses the default version 1.5, and this doesn't match the required version. Except for the two commonly occurring errors, we notice that some APIs are imported in the functional code files, but there is not a corresponding entry in the \"pom.xml\" configuration file. Some LLMs, especially those with limited instruction following capability, sometimes generate content that does not belong in the \u201cpom.xml\u201d file, leading to Leading to build failures. E2. Limited Understanding Ability Issues usually arise due to unfamiliarity with the code context during translation. Figure 8 (E2.1) shows two methods with the same name and parameter type list, leading to a semantic conflict. These occur because the previously generated method is overlooked (which may be due to the long code length) when generating a new function. Figure 8 (E2.2) shows a function being called with incorrect argument types. This happens because the data types in the source language are dynamically defined, making it challenging to identify during translation. In addition to these examples, another error commonly occurring is due to a misunderstanding of the repository structure, leading to the incorrect imports of files or directories. E3. Incomplete Generation Issues often occur due to LLMs' limitation in instruction following and code generation abilities. Figure 10 (E3.1) shows an example where some LLMs may struggle to generate long code. Even though we prompt these LLMs to continue generating, their limited instruction following capability often causes them to restart the generation from the beginning and E4. Language Feature Issues occur frequently in repository-level code translation due to its Functional complexity. Figure 11 (E4.1) shows a case of accessing a non-static class from a static function, which is not allowed in Java language. Figure 11 (E4.2) is an example of instantiating an abstract class (\u201cUnitNode\u201d). However, abstract classes cannot be instantiated directly and must be extended by a subclass that provides implementations for the abstract methods in Java language. Another case is shown in Figure 5, which shows an issue of directly accessing a private member variable. Due to the differences between Python and Java language, it is important to understand the different language features. However, some LLMs may translate tokens from the source language to the target language sequentially while overlooking the specific features, leading to language feature issues that are not permissible in the target language. E5. Encoding Issues occur due to incompatible repository encoding formats and can lead to issues when using special characters, such as emojis. An example is shown in Figure 12 (E5.1), where an error occurs due to using an emoji and US-ASCII encoding, which does not support certain characters. Figure 12 (E5.2) is another example showing that current encoding configuration does not support reading emojis from the resources. To resolve these errors, a possible solution is to make sure to use UTF-8 encoding in the \u201cpom.xml\u201d file."}, {"title": "6 THREATS TO VALIDITY", "content": "Internal Threats. The first potential internal threat is the scope of the tested LLMs. In this paper, we evaluate 11 LLMs, including 8 general models and 3 code models. Due to the lack of large-scale, repository-level code translation datasets, we do not apply fine-tuning methods to these LLMs, which may impact their performance on certain tasks. In future work, we aim to fine-tune these models to achieve more accurate results. Additionally, this threat is mitigated to some extent, as the 3 code LLMs have already been fine-tuned on code-related tasks. Another potential threat is the selection of programming languages. We only evaluate Python and Java, and the results may differ for other languages. However, since these two languages are the most widely used languages, our benchmark and findings remain important for evaluating LLM performance in this task. Furthermore, in the future, we plan to apply the method proposed in this paper to other programming languages to extend our benchmark and findings. External Threats. The first external threat is the uncertainty in LLM output. In Section 4.5, we evaluate LLMs' performance using their outputs from the generations in the translation and debugging phases. However, the generations may not represent the real performance of LLMs due to the uncertainty in generation. To mitigate this threat, we perform three rounds of experiments under the same settings, ensuring that our findings are obtained from consistent performance rather than isolated instances. Another potential threat is the evaluation of translated code quality. In Section 5, we use the test cases translated from the source repositories to test the translated repositories. However, even if the translations pass all the test cases, they could still have poor quality in terms of readability and maintainability. Nevertheless, ensuring executability and functional correctness is a foundational step toward achieving automatic code translation. Besides, we use test cases from real-world repositories to evaluate the basic quality. In the future, we plan to incorporate additional evaluations from various aspects."}, {"title": "7 RELATED WORK", "content": "Many Benchmarks have been introduced to compare the performance of different translation tech- niques objectively. CoST [29] and XLCost [30] introduce a snippet and function-level code transla- tion benchmark. CodeXGLUE [32] includes a dataset for function-level Java-C# code translation. TransCoder-test is the evaluation dataset for TransCoder [31], which includes the function-level code translation on Python, Java, and C++. Some other benchmarks like HumanEval-X [33] and EvalPlus [34] source from HumanEval [103] to construct a function-level code translation bench- mark. G-TransEval [113] provides a more fine-grained taxonomy, including token-level, syntax-level, library-level, and algorithm-level, which is part of a function. CodeNet [35], Avatar [36], xCodeE- val [105] CodeScope [37] and CodeTransOcean [38] introduce file-level code translation benchmarks which source from code contest platforms like codeforces [106], atcoder [107], aizu [108], Google Code Jam [109], etc. or task solutions websites like samples from .Net [110], d2lai [111], rosetta code [112], etc. Although these benchmarks can evaluate the capabilities of existing code trans- lation techniques to some extent, they cannot evaluate the performance of current techniques on real-world repository-level code translation tasks. Recently, pan et al. [1] manually study two open-source repositories (Apache Commons CLI [39] and Python Click [40]) and find that current LLMs struggle to complete the translation tasks of entire repositories. However, they do not provide a sufficient number of repositories and corresponding automatic test suites for evaluation. Besides, the resource and configuration files are ignored in this research."}, {"title": "8 CONCLUSION", "content": "In this paper, we provide a comparison of code translation granularity, introducing the differ- ences between repository-level code translation and previous benchmarks. We collect real-world repositories from The Stack and The Stack v2 to build RepoTransBench. Moreover, we conduct an experiment with 8 advanced general LLMs and 3 code LLMs to evaluate the translation abilities on RepoTransBench. To further explore the possibility of LLMs in repository-level code translation, we provide error-related feedback to the LLMs and observe improvements across all the LLMs after iterative debugging. We also conduct a detailed error analysis. Our work highlights current LLMs\u2019 deficiencies in repository-level code translation and suggests directions for future improvement."}]}