{"title": "On the Effectiveness of Incremental Training of Large Language Models", "authors": ["Miles Q. Li", "Benjamin C. M. Fung", "Shih-Chia Huang"], "abstract": "Training large language models is a computationally intensive process that often requires substantial resources to achieve state-of-the-art results. Incremental layer-wise training has been proposed as a potential strategy to optimize the training process by progressively introducing layers, with the expectation that this approach would lead to faster convergence and more efficient use of computational resources. In this paper, we investigate the effectiveness of incremental training for LLMs, dividing the training process into multiple stages where layers are added progressively. Our experimental results indicate that while the incremental approach initially demonstrates some computational efficiency, it ultimately requires greater overall computational costs to reach comparable performance to traditional full-scale training. Although the incremental training process can eventually close the performance gap with the baseline, it does so only after significantly extended continual training. These findings suggest that incremental layer-wise training may not be a viable alternative for training large language models, highlighting its limitations and providing valuable insights into the inefficiencies of this approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Training large language models (LLMs) has become a cornerstone of advancements in natural language processing (NLP), significantly impacted by improvements in model scaling and optimization techniques. Despite the success of models like GPTs [1] and BERT/ROBERTa [2], [3], scaling these models demands substantial resources, with training time and computational costs increasing significantly as the model size grows [4]\u2013[6]. Efficiently scaling LLMs is critical not only for reducing costs but also for making model training more accessible and environmentally sustainable. Incremental layer-wise training has been proposed as a method to potentially reduce these costs by progressively introducing layers [7]. This approach allows earlier parts of the model to stabilize while incrementally training additional layers, potentially leading to faster convergence and more efficient use of computational resources [8], [9]. However, the effectiveness of this approach remains unclear, particularly concerning its ability to capture required long-range dependencies, as earlier studies indicate that such strategies may not fully generalize when trained incrementally [10].\nThe intuition behind incremental layer-wise training is rooted in the hierarchical learning process of large language models. In these models, lower layers often capture low-level linguistic features such as word embeddings, syntactic patterns, and local dependencies [2], [11]. Higher layers, on the other hand, tend to model high-level abstractions like semantic relationships, contextual understanding, and long-range dependencies [12], [13]. High-level features are essentially combinations of low-level ones, implying that effective learning of high-level representations relies on the prior learning of low-level features. Training all layers simultaneously might therefore be inefficient, as higher layers may struggle to learn meaningful patterns before the lower layers have stabilized their representations [14], [15]. By progressively adding layers, incremental training aims to mirror this natural progression, allowing each layer to specialize and stabilize before serving as the foundation for subsequent layers. This approach aligns with the way neural networks hierarchically construct representations, potentially leading to more effective learning and convergence [14].\nHowever, despite the intuitive appeal of incremental training, its effectiveness has not been thoroughly examined in the context of large-scale language models. While previous research has shown that gradually increasing the model's capacity or context size [6], [16] can be beneficial in some cases, the specific benefits of incrementally adding layers remain uncertain.\nOur study addresses this gap by empirically evaluating incremental training in large-scale language models, analyzing computational efficiency, convergence behavior, and performance against traditional full-layer training. We compare the performance of models trained incrementally with those trained using a traditional approach, where all layers are optimized from the start.\nOur findings indicate that, contrary to initial expectations, the incremental layer-wise training approach does not deliver significant benefits in terms of computational efficiency or performance. While incremental training can eventually reach comparable performance to traditional full-scale training, it does so only after a continual training period, resulting in a higher overall computational cost. Despite early-stage gains, these models require extensive fine-tuning to bridge the performance gap with the baseline, making the incremental approach a less practical choice for large language model training. These"}, {"title": "II. RELATED WORK", "content": "The pursuit of efficient training methods for large-scale neural networks has been an active area of research. Incremental or layer-wise training strategies have been explored in various contexts, aiming to reduce computational costs and memory requirements.\nIncremental Training in Deep Learning\nIncremental training, also known as layer-wise training or progressive stacking, has been applied in deep learning to gradually build up network architectures. Early work by Hinton et al. [7] introduced a fast learning algorithm for deep belief nets, where layers are trained sequentially in an unsupervised manner while keeping the weights of previous layers fixed. Similarly, Bengio et al. [8] proposed Greedy Layer-Wise Training for deep networks, demonstrating that such approaches can initialize deep networks effectively.\nMoreover, the Cascade-Correlation learning architecture [9] incrementally builds neural networks by adding hidden units one at a time, freezing the weights of previously added units. This method aimed to overcome challenges in training deeper networks by simplifying the optimization problem.\nWhile these approaches showed promise in certain settings, particularly in unsupervised pre-training and for shallower networks, they often struggled to match the performance of end-to-end training in supervised tasks for deeper architectures like modern LLMs. The inability to fully capture complex hierarchical representations when layers are trained incrementally has been a consistent challenge [17].\nEfficient Training Techniques for LLMs\nVarious methods have been proposed to improve the efficiency of training LLMs:\n\u2022\n\u2022 Model Pruning: Reducing the number of parameters by removing redundant weights [18].\n\u2022 Knowledge Distillation: Training smaller models to replicate the performance of larger ones [19].\n\u2022 Mixed-Precision Training: Utilizing lower numerical precision to speed up computations [20].\n\u2022 Layer Freezing: Training only a subset of layers while keeping others fixed [21].\nHowever, these methods come with trade-offs between efficiency and model performance, and their applicability to incremental training remains limited.\nProgressive Neural Networks\nProgressive neural networks [22] introduce new columns (networks) when learning new tasks, while keeping previous columns fixed to retain prior knowledge. This approach is beneficial in transfer learning and continual learning scenarios but differs from the incremental layer-wise training of a single task."}, {"title": "D. Cognitive and Biological Inspirations", "content": "Incremental learning is reminiscent of how humans and animals learn, gradually building upon prior knowledge. However, replicating this process in artificial neural networks has proven challenging due to issues like catastrophic forgetting and optimization difficulties [23]."}, {"title": "III. METHODOLOGY", "content": "Building upon earlier approaches that incrementally construct neural networks [8], [9], our goal is to assess whether progressively adding layers during training can improve computational efficiency and model performance in the context of modern LLMs.\nThis incremental approach is motivated by the understanding that higher-level layers depend on the representations learned by lower-level layers. Since high-level features are combinations of low-level ones, training higher layers before the lower layers have adequately learned foundational features may be ineffective and could lead to wasted computational resources [24]. By first training the lower layers to capture basic linguistic features, we provide a stable and informative input for the higher layers to build upon. This approach also addresses the issue of internal covariate shift, as lower layers have sufficient time to stabilize their representations before training progresses to higher layers. This stabilization can reduce the shifting of input distributions for higher layers, leading to more effective optimization [25]. Training the newly added layers in isolation allows them to adapt to the established representations from earlier layers without the interference of simultaneous updates throughout the entire network. This sequential learning process aims to optimize computational resources by avoiding unnecessary computations in higher layers during the early stages of training. The subsequent fine-tuning phase then harmonizes the representations across all trained layers, integrating the newly learned features with the existing model structure. Unlike previous methods that focused on unsupervised pre-training or shallow networks [7], [8], we aim to investigate whether this method provides any practical advantages for deep, transformer-based architectures by examining convergence speed, memory usage, and generalization ability.\nModel Architecture and Notation\nLet the total number of layers in the LLM be denoted by $L$, where $L = n\\times m$, with $s$ being the total number of stages and $m$ the number of layers added in each stage. We denote the layers at stage $i$ as $L_i = \\{l_{(i-1)\\times m+1},l_{(i-1)\\times m+2},...,l_{i\\times m}\\}$. For both incremental training and baseline training, we use the same architecture, dataset, and hyperparameters to ensure a fair comparison.\nStage-wise Training Process\nEach stage $i$ consists of the following two phases, which are designed to evaluate the potential benefits of training newly added layers in isolation before fine-tuning the entire model."}, {"title": "1) Phase 1: Training New Layers:", "content": "During this phase, only the newly added layers $L_i$ are trained while keeping the parameters of all preceding layers $\\{L_1, L_2,..., L_{i-1}\\}$ fixed. The motivation behind this approach is to isolate the training of new layers and prevent the random initialization from negatively impacting the performance of the previously trained parameters. However, our findings suggest that this isolation may not allow the model to sufficiently integrate newly learned features across different layers, which could hinder generalization. The optimization problem for this phase can be formulated as:\n$\\min C(\\theta_i; \\theta_{1:(i-1)}, D)$\\n$\\theta_i$\nwhere $\\theta_i$ represents the parameters of the newly added layers $L_i$, $\\theta_{1:(i-1)}$ denotes the fixed parameters of the previously trained layers, and $D$ is the training data.\n2) Phase 2: Fine-tuning All Layers: After training the new layers, the entire model, consisting of layers $\\{L_1, L_2, ..., L_i\\}$, is fine-tuned together. The goal of this phase is to integrate the newly learned features into the existing model and improve overall optimization. Although this step aims to harmonize the learned features across all layers, our experimental results show that it may not be sufficient to close the performance gap with models trained using a traditional approach. The optimization problem during this phase is given by:\n$\\min L(\\theta_{1:i}; D)$\\n$\\theta_{1:i}$\nwhere $\\theta_{1:i}$ represents the parameters of all layers up to stage $i$.\nIncremental Layer Addition\nThis process of introducing new layers and fine-tuning continues until all $L$ layers have been trained. In our experiments, we varied the number of stages (e.g., 4, 8, and 12 stages) to examine whether the granularity of layer addition impacts the final performance. Optinally, context size and batch size can also be increased throughout the stages as the training is plateaued [6], [16], following common practices aimed at enhancing model capabilities for longer dependencies.\nContinual Training Phase\nAfter completing the incremental training phases, an optional continual training phase can be applied to further improve the model's performance. In this phase, all model layers are optimized jointly, similar to traditional full-layer training. This continual training aims to integrate the learned representations across all layers, potentially closing any performance gaps observed during incremental training. The continual phase is intended to harmonize layer interactions and enhance generalization capabilities beyond what is achievable through isolated layer-wise training."}, {"title": "E. Traditional Training Regime For Comparison", "content": "The effectiveness of the incremental strategy should be compared against a the traditional full-layer training approach where all layers are trained simultaneously from the beginning. Both the baseline and incremental models share the same hyperparameters, architecture, and dataset. The primary difference is that the baseline approach trains all $L$ layers together for the entire duration, while the incremental approach progressively adds layers. This comparison allows us to quantify the trade-offs between computational efficiency and model performance.\nComputational Cost Analysis\nIn this subsection, we analyze the computational cost of incremental layer-wise training compared to traditional full-layer training. Our goal is to determine how many additional tokens of continual training are needed, as a ratio of the baseline training tokens $T$, so that the total computational cost of incremental training plus continual training equals the computational cost of the baseline training on $T$ tokens.\nDefinitions and Assumptions: Let:\n\u2022 $L$ be the total number of layers in the model.\n\u2022 $S$ be the total number of stages in incremental training.\n\u2022 $m$ be the number of layers added per stage, calculated as $m = \\frac{L}{S}$ (assuming $L$ is divisible by $S$).\n\u2022 $L_i$ be the total number of layers up to stage $i$, so $L_i = i \\times m$.\n\u2022 $T$ be the total number of tokens used in baseline training.\n\u2022 $T_{inc}$ be the number of tokens used during the incremental training stages.\n\u2022 $T_{cont}$ be the number of tokens used during the continual training phase.\n\u2022 $c$ be the computational cost per layer per token for the forward or backward pass.\nPhases of Incremental Training:\nEach stage consists of two phases:\n\u2022 Phase 1: Train the newly added layers for $\\frac{T_{inc}}{2S}$ tokens.\n- Forward pass: Involves all layers up to the current stage ($L_i$ layers).\n- Backward pass: Involves only the newly added layers ($m$ layers).\n\u2022 Phase 2: Fine-tune all layers up to the current stage for $\\frac{T_{inc}}{2S}$ tokens.\n- Forward pass: Involves all layers up to the current stage ($L_i$ layers).\n- Backward pass: Involves all layers up to the current stage ($L_i$ layers).\nAssumptions:\n\u2022 The computational cost per token is directly proportional to the number of layers involved in the forward and backward passes.\n\u2022 The cost per layer per token is the same for both the forward and backward passes.\n\u2022 The computational cost during the continual training phase is the same per token as in the baseline training,"}, {"title": "2) Computational Cost of Baseline Training:", "content": "For the baseline model:\nCost per token = $L \\times C_{forward} + L \\times C_{backward} = 2L \\times c$\nTotal computational cost:\n$C_{baseline} = T \\times 2L \\times c = 2TLc$\n3) Computational Cost of Incremental Training: The total computational cost of incremental training includes the costs from all stages and phases.\nPhase 1 of Stage i:\n\u2022 Tokens processed: $\\frac{T_{inc}}{2S}$\n\u2022 Forward pass cost per token: $L_i \\times c$\n\u2022 Backward pass cost per token: $m \\times c$\n\u2022 Total cost per token: $(L_i + m) \\times c$\n\u2022 Total cost: $C_{phase1} = \\frac{T_{inc}}{2S} \\times (L_i + m) \\times c$\nPhase 2 of Stage i:\n\u2022 Tokens processed: $\\frac{T_{inc}}{2S}$\n\u2022 Forward pass cost per token: $L_i \\times c$\n\u2022 Backward pass cost per token: $L_i \\times c$\n\u2022 Total cost per token: $2L_i \\times c$\n\u2022 Total cost: $C_{phase2} = \\frac{T_{inc}}{2S} \\times 2L_i \\times c$\nTotal Cost for Stage i:\n$C_{stage} = C_{phase1} + C_{phase2} = \\frac{T_{inc}}{2S} \\times (3L_i + m) \\times c$\nTotal Incremental Training Cost:\n$C_{incremental} = \\sum_{i=1}^{S} C_{stage_i} = \\sum_{i=1}^{S} \\frac{T_{inc}}{2S} (3L_i + m)$\nSince $L_i = im$, we have:\n$\\sum_{i=1}^{S} (3L_i + m) = \\sum_{i=1}^{S} (3mi + m) = 3m \\sum_{i=1}^{S} i + \\sum_{i=1}^{S} m = 3m \\frac{S(S+1)}{2} + mS = \\frac{mS}{2} (3S+5)$"}, {"title": "2) Computational Cost of Baseline Training:", "content": "Therefore, the total incremental training cost is:\n$C_{incremental} = \\frac{T_{inc}}{2S} \\times \\frac{mS}{2} \\times (3S+5) = \\frac{T_{inc}cm}{2S} \\times \\frac{S}{2} (3S+5)$\nSince $m = \\frac{L}{S}$:\n$C_{incremental} = \\frac{T_{inc}cL}{2S} \\times \\frac{S}{2} (3S+5)$\nSimplify:\n$C_{incremental} = \\frac{T_{inc}cL}{4S} (3S+5)$\n4) Computational Cost of Continual Training: The computational cost per token during continual training is the same as the baseline:\nCost per token = 2L x c\nTotal computational cost:\n$C_{continual} = T_{cont} \\times 2L \\times c = 2T_{cont}Lc$\n5) Total Computational Cost and Equality with Baseline:\nThe total computational cost of the incremental approach is:\n$C_{total} = C_{incremental} + C_{continual}$\nWe set $C_{total} = C_{baseline}$ to find $T_{cont}$:\n$C_{incremental} + C_{continual} = C_{baseline}$\n$\\frac{T_{inc}cL(3S+5)}{4S} + 2T_{cont}Lc = 2TLc$\nSince $T_{inc} = T$:"}, {"title": "B. Evaluation Metrics", "content": "To evaluate the models, we monitored three primary metrics: training loss, validation loss, and accuracy on the HellaSwag benchmark [28]. The training and validation loss were used to assess convergence behavior, while the HellaSwag benchmark provided insights into the generalization capabilities of the models."}, {"title": "C. Results", "content": "1) Training and Validation Loss: Figure 2 presents the training and validation loss curves for both the baseline and incremental models. The baseline model exhibits faster convergence with lower overall training and validation losses throughout the training process. In contrast, the incremental models show higher losses, indicating slower convergence and suboptimal performance at equivalent computational budgets.\nAt the points where the incremental models have expended the same cumulative computational cost as the baseline model trained for 10,000 steps (marked by the large solid circles in the figure), all incremental models display higher training and validation losses compared to the baseline. Specifically, the four-stage incremental model still lags behind the baseline in terms of loss values at this computational budget. Although the four-stage incremental model eventually reaches training and validation losses comparable to the baseline, it does so only after significantly more training steps, highlighting that the incremental approach requires substantially more computational effort to achieve similar performance.\n2) HellaSwag Benchmark Evaluation: Figure 1 presents the accuracy scores on the HellaSwag benchmark for both the baseline and incremental training regimes. The baseline model consistently outperforms the incremental training regimes throughout the training process. At the points of equal cumulative computational cost (indicated by the large solid circles), the incremental models show significantly lower accuracy compared to the baseline trained for 10,000 steps."}, {"title": "3) Training and Validation Loss:", "content": "The four-stage incremental model, for instance, demonstrates a notable performance gap at this point.\nWhile the four-stage incremental model eventually closes the accuracy gap with the baseline, it requires approximately much more than the baseline's computational budget\u2014to achieve comparable performance. This extended training underscores that the incremental approach demands substantially more computational resources to match the baseline's generalization capabilities on the HellaSwag benchmark.\nAnalysis of Results\nThe experimental results indicate that the incremental layer-wise training approach underperforms compared to the baseline when evaluated at the same computational budget. Despite the initial reduction in computational cost per step during the early stages of incremental training, the models require a significant amount of additional continual training to match the total computational cost of the baseline model.\nAt the points where the cumulative computational costs are equal, all incremental models exhibit higher training and validation losses and lower HellaSwag accuracy than the baseline. The four-stage incremental model eventually achieves performance similar to the baseline, but only after substantially more training steps and computational resources. This prolonged process suggests that incremental training does not offer practical benefits over traditional training, as the additional resources required outweigh the early efficiency gains."}, {"title": "V. DISCUSSION", "content": "In this section, we discuss the implications of our findings and discuss assumptions made in our computational cost analysis.\nComputational Efficiency\nWhile incremental training reduces memory usage and computational cost per step in the early stages, achieving performance comparable to the baseline ultimately requires significantly more continual training. The initial computational savings are offset by the extended training time and additional resources needed during the continual training phase. At the same cumulative computational cost, incremental models perform worse than the baseline, indicating lower computational efficiency. This makes the incremental approach less practical for large language model training, as it necessitates additional resources to achieve results similar to traditional full-layer training.\nIncremental Strategies Not Explored\nWhile incrementing batch size and context length as we increase the number of layers could be a potential direction to explore, we did not pursue this approach. Our findings indicated that even without reducing the number of tokens in the batch during the early stages, the incremental training results were not satisfactory. So, there is no point in exploring on that direction further in this study."}, {"title": "C. Assumption on Compute Cost of Forward and Backward Passes", "content": "In our computational cost analysis, we assumed that the computational cost per layer per token is the same for both the forward and backward passes. We acknowledge that this assumption is not entirely accurate, as the backward pass generally requires more computational resources due to gradient computations and the storage of intermediate activations for backpropagation [29]. However, this simplification does not significantly affect our overall conclusions. Even when accounting for the higher computational cost of the backward pass, the incremental training regime still demands substantial continual training to approximate the performance of the traditional baseline. This continual training phase consumes computational resources that are close to the compute budget of the traditional training approach. Therefore, despite the inaccuracy in the initial assumption, our fundamental finding remains valid: incremental layer-wise training does not offer computational efficiency advantages over full-layer training for large language models."}, {"title": "VI. CONCLUSION", "content": "In this paper, we evaluated the effectiveness of incremental layer-wise training for large language models. Our findings demonstrate that, contrary to expectations, this approach does not offer benefits in computational efficiency or model performance. Incremental training regimes underperform traditional full-layer training, even when accounting for the same cumulative computational cost. The need for extensive continual training to match baseline performance makes the incremental approach less practical for large language model training. These results highlight the limitations of incremental layer-wise training and underscore the importance of exploring alternative methods for efficient LLM training."}]}