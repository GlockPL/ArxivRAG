{"title": "Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application", "authors": ["Thomas Hickling", "Maxwell Hogan", "Abdulla Tammam", "Nabil Aouf"], "abstract": "This paper proposes a holistic framework for autonomous guidance, navigation, and task distribution among multi-drone systems operating in Global Navigation Satellite System (GNSS)-denied indoor settings. We advocate for a Deep Reinforcement Learning (DRL)-based guidance mechanism, utilising the Twin Delayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency of the training process, we incorporate an Artificial Potential Field (APF)-based reward structure, enabling the agent to refine its movements, thereby promoting smoother paths and enhanced obstacle avoidance in indoor contexts. Furthermore, we tackle the issue of task distribution among cooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This GCN adeptly represents the interactions between drones and tasks, facilitating dynamic and real-time task allocation that reflects the current environmental conditions and the capabilities of the drones. Such an approach fosters effective coordination and collaboration among multiple drones during search and rescue operations or other exploratory endeavours. Lastly, to ensure precise odometry in environments lacking GNSS, we employ Light Detection And Ranging Simultaneous Localisation and Mapping complemented by a depth camera to mitigate the hallway problem. This integration offers robust localisation and mapping functionalities, thereby enhancing the system's dependability in indoor navigation. The proposed multi-drone framework not only elevates individual navigation capabilities but also optimises coordinated task allocation in complex, obstacle-laden environments. Experimental evaluations conducted in a setup tailored to meet the requirements of the NATO Sapience Autonomous Cooperative Drone Competition demonstrate the efficacy of the proposed system, yielding outstanding results and culminating in a first-place finish in the 2024 Sapience competition.", "sections": [{"title": "I. INTRODUCTION", "content": "The adoption of Uncrewed Aerial Vehicles (UAVs) has grown in critical applications such as search and rescue (SAR) operations due to their ability to access hard-to-reach areas and quickly provide real-time situational awareness, [44]. However, operating in environments denied by the Global Navigation Satellite System (GNSS), such as indoor settings, presents unique challenges for autonomous UAVs, which require the development of intelligent and robust navigation and task management strategies. In recent years, Deep Reinforcement Learning (DRL), has emerged as a promising approach to handling complex decision-making tasks in such scenarios, particularly for cooperative UAV systems [1].\nThis paper presents the development and deployment of a cooperative drone system that adopts deep learning strategies to achieve autonomous navigation, guidance, and planning. This development is conducted within the NATO-funded Sapience program through an Autonomous Cooperative Drone Competition, [32], which promotes technological advancements as part of NATO's humanitarian goals. The competition focuses on improving cooperative autonomous UAV capabilities in GNSS-denied environments for search and rescue applications. Teams were challenged with tasks such as mapping, object/person detection and localisation, and the delivery of medical aid. These tasks were to show the benefit of using autonomous systems to perform tasks such as medical aid delivery without subjecting operators to unnecessary risk.\nThe system developed for this competition employs two UAVs, each equipped with 13-inch propellers, two Intel RealSense D455f cameras [16], a Velodyne 16-line puck Light Detection and Ranging (LIDAR) sensor [52], and an NVIDIA Jetson Orin Nano [38]. The DRL-based guidance for each agent cooperation, real-time navigation, obstacle avoidance, and efficient resource allocation, while aiming to improve response times, operational efficiency, and safety in life-saving scenarios."}, {"title": "II. RELATED WORK", "content": "A. Deep Reinforcement Learning for Autonomous Guidance and Obstacle Avoidance\nUAV guidance using DRL has been studied, [2] and the developed approaches generally fall into two categories: discrete action spaces [56], where the UAV's actions are restricted to specific movements (e.g., move forward, move right), and continuous action spaces [15], where the UAV can select from a range of values for each action, allowing more nuanced control. The choice of DRL algorithms depends on the type of control required. For discrete action spaces, algorithms such as Deep Q-Networks (DQN), [29], [56] are suitable, while for continuous action spaces, algorithms like Deep Deterministic Policy Gradient (DDPG) [23], [13], Proximal Policy Optimisation (PPO) [43], [15], and Twin Delayed Deep Deterministic Policy Gradient (TD3) [8], [11] are more appropriate.\nDRL typically employs unsupervised learning, relying solely on states, actions, and rewards observed during training. This unsupervised approach allows DRL to produce novel solutions unbounded by existing methods; however, it often requires significantly more samples to train effectively, leading to extended training times, especially in complex environments like for UAV guidance.\nTo address this inefficiency, researchers have incorporated expert knowledge into DRL training, enhancing performance through pre-training. Techniques such as Deep Q Learning from Demonstrations (DQfD) [12] and Deep Deterministic Policy Gradients from Demonstrations (DDPGfD) [50] leverage expert-labeled data to pre-train networks, reducing reliance on extensive reward engineering while preserving the benefits of DRL.\nNon-Expert Policy Enhanced DRL (NPE-DRL) [54] and DDPG-APF [13] leverage the Artificial Potential Field (APF) to generate non-expert actions that guide the agent during training. Both methods gradually reduce the influence of APF to allow for exploration and optimal policy discovery. NPE-DRL operates in a discrete action space, while DDPG-APF utilises a continuous action space. Each approach demonstrates improved performance and sample efficiency, enabling the agent to converge on effective solutions with fewer training samples.\nThis paper builds by integrating the APF directly into the reward structure, rather than using it as a separate guiding mechanism. This approach is expected to provide tighter control over the UAV's policy, especially within the more complex indoor environments considered here, which extends beyond the scenarios addressed by NPE-DRL and DDPG-APF."}, {"title": "B. Deep Reinforcement Learning for Graph-Based Task Allocation", "content": "The use of multiple UAVs in cooperative missions has been explored to enhance task efficiency and flexibility [41]. Early task allocation methods relied on rule-based or heuristic optimisation approaches, including centralised systems where a single controller assigns tasks to each UAV and decentralised systems where UAVs independently select their tasks based on predefined rules [55]. While these methods provided foundational insights, they often faced scalability challenges and lacked the adaptability needed for dynamic environments [3]. As a result, researchers have increasingly turned to Machine Learning (ML) approaches, particularly DRL [28], to address these limitations by enabling UAVs to learn adaptive task allocation policies that are robust to environmental changes.\nWithin DRL-based task allocation, various algorithms have been developed for multi-UAV coordination, including Markov Decision Process [28], and Multi-Agent Deep Deterministic Policy Gradient [25]. These DRL approaches rely on reward structures designed to optimise objectives such as maximising task coverage, minimising collisions, and improving efficiency in multi-agent settings. The adaptability of DRL allows UAVs to dynamically allocate tasks in response to changes in mission objectives or environmental constraints, providing advantages over classical optimisation techniques in complex, multi-agent scenarios [28].\nGraph-based representations have become a key approach in modelling spatial layouts and task relationships within multi-UAV systems [4]. Graphs provide a natural way to represent complex spatial relationships, with nodes representing specific locations, tasks, or UAVs, and edges representing viable paths or task dependencies. This structure is particularly beneficial in environments with intricate layouts, dynamic obstacles, or multiple task dependencies, as it allows for more informed decision-making based on the UAVs' spatial context and objectives.\nGraph Attention Networks (GATs) [51], [24] improve upon traditional Graph Convolutional Networks (GCNs) by incorporating an attention mechanism that allows the model to assign varying levels of importance to different nodes and edges. This is especially useful in UAV task allocation, where certain nodes (tasks) or edges (paths) may be more critical than others for efficient navigation and task completion. By selectively focusing on the most relevant nodes, GATs enable prioritisation of tasks in real-time, enhancing coordination and efficiency in multi-agent systems [47].\nThe combination of GATs and DRL leverages the strengths of both approaches: GATs provide the structural awareness necessary for understanding task relationships, while DRL offers a flexible framework for learning task allocation policies based on trial-and-error feedback. This paper will explore how to combine these methods to develop a task allocator that enables UAV cooperation in indoor environments."}, {"title": "C. Adapted LIDAR-SLAM for Navigation", "content": "The \"corridor problem\" in LIDAR-SLAM is a known issue that occurs when using LIDAR for SLAM in narrow, featureless corridors. The problem arises because the lack of unique, distinguishable features in a long, narrow corridor can make it difficult for the LIDAR-SLAM system to localise and distinguish between different parts of the environment accurately.\nZhang et al. [53] solved this issue by utilising radar to create more features with different materials and shapes, returning different radar returns. These additional features help mitigate common issues associated with the corridor problem. By leveraging different feature extraction methods, the limitations of one approach can be compensated by the other. This paper proposes a solution to Z-drift caused by the corridor problem through sensor fusion and converting relative positioning into the global positioning system used by LiDAR-SLAM mapping."}, {"title": "III. AUTONOMOUS GUIDANCE IN UAVS", "content": "Autonomous guidance for UAVs is traditionally facilitated by algorithms such as Dijkstra, A*, and APF. Recent advances in ML, particularly DRL, have enhanced UAVs' capabilities to make complex real-time decisions in search and rescue, mapping, and inspection.\nA. Deep Reinforcement Learning\nDRL combines reinforcement learning (RL) principles [48] with Deep Neural Networks (DNNs) to approximate policies or value functions, enabling agents to operate in large state-action spaces. In DRL, an agent interacts with an environment, selects actions, and receives feedback in the form of rewards, learning an optimal policy to maximise cumulative reward. This approach is particularly valuable in complex environments unsuitable for traditional tabular methods as the large state-action spaces take unfeasible amounts of computer memory.\nKey components of DRL include:\nAgent: The decision-maker in the environment.\nEnvironment: The external system with which the agent interacts.\nState, Action, Reward: Indicators of the environment's current status, possible moves, and feedback for success.\nPolicy: The strategy the agent uses to decide actions.\nPopular DRL algorithms, such as DQN [29], PPO [43], and TD3 [8], each address issues related to stability, exploration, and sample efficiency.\n1) Twin Delayed Deep Deterministic Policy Gradient: TD3 [8] builds on the DDPG [23] algorithm to improve stability in continuous control tasks, addressing common issues like overestimation bias. TD3 follows an actor-critic structure where the actor network outputs actions and the critic networks evaluate the expected returns. TD3 introduces three core innovations:\n1) Clipped Double Q-Learning: Two Q-networks are used, with updates based on the minimum Q-value, reducing overestimation."}, {"title": "2) Delayed Policy Updates:", "content": "The actor is updated less frequently than the critics, allowing the critics to stabilise and prevent premature policy updates.\n3) Target Policy Smoothing: This is a regularisation strategy where the target actions have a small noise signal (smoothing) added before being passed to the target critic networks. This variation prevents overfitting of the Q-networks to small peaks in the value estimate. This smoothing of the target actions leads to a lower variation in the Q targets and lowers the chance of functional approximation error.\nThe training process involves the agent observing the environment, selecting actions through the actor, receiving rewards, and updating the Q-networks and actor based on experiences stored in a replay buffer. These innovations make TD3 effective for precise continuous control in applications like autonomous guidance and cooperative UAV systems.\n2) Artificial Potential Field Based Reward Structure: The design of the reward system is vital to successfully training a DNN using DRL techniques, [34]. Small changes in how the reward is allocated can significantly change the training result. In DRL training, rewards are typically shaped by specific actions and states. Reward shaping is complicated when applied to training for complex goals and environments. Therefore, looking for novel reward-shaping methods to maximise DRL's promise is crucial.\nThe Artificial Potential Field (APF) [18] is a well-known technique used in robotics for path planning and obstacle avoidance. In the APF approach, the environment is modelled as a potential field where attractive and repulsive forces guide the agent (in this case, a drone) towards the goal while avoiding obstacles. The goal generates the attractive potential, which pulls the agent towards it, while the repulsive potential is generated by obstacles pushing the agent away.\nThe potential field is defined as a scalar function over the space, and the agent moves in the direction that minimises this potential. Mathematically, the total potential U is typically composed of an attractive potential $U_{att}$ and a repulsive potential $U_{rep}$:\n$U(q) = U_{att}(q) + U_{rep}(q)$, (1)\nwhere q represents the position of the drone. The attractive potential is often designed to decrease as the drone approaches the goal, and the repulsive potential increases as the drone approaches obstacles.\nThe resultant force acting on the drone is derived from the gradient of the total potential function:\n$F(q) = -\\nabla U(q)$, (2)\nwhich gives the direction of movement for the drone. This force represents the optimal movement direction, balancing attraction to the goal and repulsion from obstacles.\nIn a RL context, we can leverage the optimal movement provided by the APF to design an effective reward function for"}, {"title": "B. Advantages of Using APF in Reward Function", "content": "training a drone in a DRL framework. Since the APF provides a clear direction for the drone to move in, we can compare the action chosen by the drone to this optimal movement and use the similarity between the two as a measure of performance.\nLet $a_{opt}$ represent the optimal action (direction) derived from the APF, and $a_{agent}$ represent the action taken by the drone in a given state. The reward function can be designed to encourage the agent to choose actions that align with the optimal direction. One way to measure this alignment is by using the cosine similarity between the two vectors:\n$R=\\frac{a_{opt} . a_{agent}}{||a_{opt}|| ||a_{agent}||}$, (3)\nwhere \u2022 denotes the dot product and || || represents the magnitude of the vectors. The reward R is maximised when the drone's chosen action aligns perfectly with the optimal movement provided by the APF, and it decreases as the angle between the two vectors increases.\nAlternatively, a simpler formulation could penalise the agent based on the difference between the actions:\n$R= -||a_{opt} - a_{agent} ||$, (4)\nwhere the reward becomes smaller as the chosen action deviates further from the optimal direction.\nB. Advantages of Using APF in Reward Function\nThis approach offers several advantages:\nGuided Learning: By using the APF as a reference for optimal movement, the agent can learn more efficiently as it is guided towards the correct actions early in training.\nSmooth Trajectories: APF typically generates smooth, collision-free paths, ensuring the agent learns to navigate safely in complex environments.\nHybrid Approach: Combining DRL with APF leverages the strengths of classical control (APF) and learning-based approaches (DRL), providing a balance between predefined optimal behaviour and the adaptability of learning.\nThus, using APF as a reference for movement and designing a reward function around the similarity of the agent's actions to this reference can accelerate the learning process and improve the overall drone guidance performance, especially in obstacle-rich environments."}, {"title": "IV. TASK ALLOCATION", "content": "Task allocation in robotics refers to assigning tasks to individual robots in a multi-robot system to achieve collective goals efficiently, [20]. This is a crucial problem, particularly in complex environments, where robots must collaborate, explore, or cover areas while minimising resource usage and avoiding conflicts."}, {"title": "A. Deep Reinforcement Learning for Task Allocation", "content": "Neural networks, particularly DRL and other ML techniques, have increasingly been applied to solve task allocation problems in multi-robot systems [10]. They can learn complex patterns and adapt to dynamic, uncertain environments, making them well-suited for real-time task allocation.\nIn DRL-based task allocation, the task assignment is framed as a decision-making problem where the agent (drone or central controller) learns to allocate tasks through trial and error. A neural network is trained to predict which task should be assigned to which drone to maximise the system's overall performance, such as reducing task completion time or minimising energy usage.\nThe state can include the positions and states of drones, the state of the tasks and their positions, and any other sensor data. The action is then the orders for each agent in the system to perform either for the next step or string a series of tasks to be done. The reward structure can be used to guide the network to optimise for tasks to be selected in a way that tasks are not repeated and that the shortest path is taken to do all tasks."}, {"title": "B. Overview of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs)", "content": "1) Graph Convolutional Networks (GCNs): Graph Convolutional Networks (GCNs) [19] are a type of neural network specifically designed to operate on graph-structured data. GCNs extend the concept of convolution from regular grid data, such as images, to irregular graph data, allowing for the processing of data where relationships between entities are naturally represented as graphs.\nIn GCNs, each node's representation is updated by aggregating information from its neighbouring nodes. The layer-wise propagation rule for a GCN can be written as:\n$H^{(l+1)} = \\sigma (\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}H^{(l)} W^{(l)})$, (5)\nWhere:\n$H^{(l)}$ is the matrix of node features at layer l.\n$\\tilde{A} = A + I$ is the adjacency matrix with added self-loops, where A is the adjacency matrix of the graph, and I is the identity matrix.\n$\\tilde{D}$ is the diagonal degree matrix of $\\tilde{A}$.\n$W^{(l)}$ is the trainable weight matrix at layer l.\n$\\sigma$ is an activation function, such as ReLU.\nThe GCN learns to propagate and combine features from neighbouring nodes, effectively capturing the structural information of the graph. However, GCNs apply uniform weighting to the neighbouring nodes, treating all neighbours equally during aggregation. This limitation can reduce the model's ability to focus on the most important nodes in complex graphs, potentially leading to suboptimal performance in certain scenarios.\n2) Graph Attention Networks (GATs): Graph Attention Networks (GATs) [51] introduce an attention mechanism to address the limitations of GCNs by allowing the model to learn the relative importance of neighbouring nodes. Instead of treating all neighbours equally, GATs compute attention coefficients for each edge, enabling the network to focus on more relevant neighbours when aggregating information.\nThe attention mechanism in a GAT layer computes attention coefficients $a_{ij}$ between node i and its neighboring nodes j as follows:\n$a_{ij} = \\frac{exp (LeakyReLU (a^T [Wh_i||Wh_j]))}{\\sum_{k \\in N(i)} exp (LeakyReLU (a^T [Wh_i||Wh_k]))}$, (6)\nwhere:\n$a_{ij}$ represents the normalised attention coefficient between nodes i and j.\n$h_i$ and $h_j$ are the feature vectors of nodes i and j.\nW is a shared weight matrix.\na is a learnable vector for computing attention.\n|| denotes concatenation.\nN(i) represents the neighbourhood of node i.\nThe resulting attention coefficients $a_{ij}$ are used to compute a weighted sum of the neighbours' features, allowing the model to focus more on important connections. This attention-based aggregation results in node representations that better capture the varying importance of neighbouring nodes. Unlike GCNs, which rely on a fixed adjacency matrix for propagation, GATs do not require a predefined structure and can operate on dynamically changing graphs. This makes GATs more versatile for real-time applications where the graph topology may evolve over time."}, {"title": "V. GLOBAL NAVIGATION SATELLITE SYSTEM DENIED ODOMETRY", "content": "GNSS-denied odometry estimates a vehicle's position, velocity, and orientation in environments where GNSS signals are unavailable or unreliable, such as indoor, underground, or dense urban areas [30]. For UAVs, this capability is essential for autonomous navigation, enabling precise manoeuvring and obstacle avoidance in confined spaces. GNSS-denied odometry is particularly valuable in applications requiring real-time localisation and mapping, such as infrastructure inspection, search and rescue, and disaster response.\nSeveral methods are commonly used for GNSS-denied odometry. Visual odometry (VO) [40] relies on camera data to track motion by analysing changes in visual features between frames, though its accuracy can diminish in feature-poor or low-light areas. IMU-based odometry [31] uses accelerometers and gyroscopes to estimate motion through changes in acceleration and rotational velocity, though it accumulates error (drift) over time, making it more suited for short-term estimates and often requires sensor fusion for long-term accuracy.\nA. LIDAR-SLAM\nLIDAR-SLAM [9], [27], [45] addresses limitations in feature-poor or low-visibility environments by using laser sensors to build a 3D map while simultaneously localising the UAV. This process involves scanning the surroundings to generate a 3D point cloud, which is continuously updated"}, {"title": "1) Challenges in Height (Altitude) Estimation in Enclosed Environments:", "content": "through scan matching as the UAV moves. Loop closure, a crucial aspect of SLAM, helps correct accumulated drift by recognising previously mapped areas, thus improving the consistency of maps and localisation over time. Additionally, graph-based optimisation refines the map and position estimates, reducing errors.\n1) Challenges in Height (Altitude) Estimation in Enclosed Environments: Despite its advantages, LIDAR-SLAM faces challenges in enclosed spaces. Limited fields of view can reduce the data quality in tight areas, while feature-poor settings (e.g., long corridors) make it difficult to differentiate between similar-looking sections, leading to localisation errors [22]. Drift remains a problem without frequent loop closures, and reflective surfaces like glass or metal can distort LIDAR measurements. Operating close to walls or obstacles may also cause incomplete scans, highlighting the need for sensor fusion with IMUs or visual odometry to enhance mapping accuracy in complex environments."}, {"title": "VI. METHODOLOGY", "content": "In this work, key technologies such as LIDAR-SLAM for real-time mapping and localisation, a DRL-based guidance AI for autonomous navigation, and a Graph Attention Network (GAT)-based task allocator for multi-UAV coordination were leveraged to develop a cooperative drone system for indoor SAR operations. The SAPIENCE competition looks to improve technologies like those mentioned above to enable UAVs to operate autonomously, avoid obstacles, and collaborate effectively to cover search areas, ensuring efficient exploration and critical aid delivery in environments where human intervention may be hazardous or infeasible.\nA. UAV and Sensor Suite\nFigure 3, displays the drone developed for this work with annotations to show the additional equipment that was used to provide the platform with additional processing and sensing capabilities. The primary sensors include a Velodyne 16-line puck LiDAR [52] and an Intel RealSense D455f RGB-Depth camera [16]. The companion processor is the NVIDIA Jetson Orin Nano [38], equipped with an NVIDIA Ampere GPU featuring 1024 CUDA cores and 32 Tensor Cores. This enables the processor to run advanced deep-learning algorithms and perform GPU-accelerated tasks efficiently. The systems of operation were developed using Isaac ROS [37], which enabled the configuration of the sensors, interchange of commands to the flight controller, and embedding of the guidance and navigation algorithms developed in this work. In addition, this package also facilitated inter-process communication that accelerated prototyping and development without compromising real-time performance.\nReal-time positioning and mapping are made by integrating the lidarslam_ros2 [42] into the system. This provided the system with accurate positioning, including a heading, in order to undertake its assigned tasks. The difficulty in deploying the LiDAR-SLAM algorithm that is made available in this package is that it is susceptible to drift in the predicted height, given the absence of features.\nB. LIDAR-SLAM Altitude Fix\nThe downward-facing Red, Green, Blue, and Depth (RGBD) camera is utilised to correct for the predicted height drift. First, the median depth was calculated from five equidistance areas in the depth image. One of these was located in the centre of the image, and the other four were in the centre of each quadrant. The final depth was measured as the median of the five values and injected into the control loop.\nThe environment in which the drone was expected to operate was mostly flat, with multiple levels. To detect a change in level, a control loop was devised that would compare the vertical velocity measured by the flight controller's Inertial Measurement Unit (IMU) with the calculated velocity from concurrent depth measurements from the depth camera. Any significant deviations between these two values indicate a change in level, and the calculated relative depth is updated to reflect this.\nC. Twin Delayed deep Deterministic Policy Gradient\nThe TD3 scheme, as previously described, will be utilised to train the two DNNs responsible for providing guidance and task allocation. This scheme enhances the DDPG approach by incorporating three core innovations. The first is an additional critic network for Q-learning. The Q values generated by the two critic networks can be compared, allowing for the utilisation of the minimum Q value during the training process. This methodology effectively constrains the updates applied to the actor network, thereby mitigating the risk of overestimation. The scheme also uses delayed policy updates to stabilise critics before they can update the actor, preventing premature updates. The final core innovation is target policy smoothing, where noise is added to target actions to improve exploration and reduce overfitting.\nAdditional techniques were used to improve the training process for this implementation. The first is a prioritised experience replay, which ranks the experience's quality, consisting of the state, next state, action, reward, and done. When the mini-batch is selected from the memory buffer, the random selection is weighted to be more likely to pick higher learning"}, {"title": "D. Guidance Deep Neural Network Architecture", "content": "potential experiences. By focusing the learning using experiences that the agent found surprising or had significant errors, the agent will converge quicker and more effectively [14]. The TD error defines the learning potential, which is calculated using one of the critic networks. Secondly, to calculate the loss of the critic networks, the Huber loss was used rather than the usual mean squared error loss to protect the training from outliers.\nThe next sections will describe the architecture that will be used to make the decisions needed for the proposed tasks. In the TD3 scheme shown in Fig. 5, these networks play the part of the actor networks. The critic networks are the same as the actor networks but include a branch that inputs the actor's output into the concatenation layer. Unlike the actor networks, the final output is just one Q-value. The target networks are copies of the main networks that stabilise the training process.\nD. Guidance Deep Neural Network Architecture\nThe proposed DNN architecture for the actor network for the UAV perception component of our guidance TD3 DRL scheme integrates three input branches, each processing a different type of perception sensory data-depth images, LIDAR scans, and positional data-before combining them to predict the UAV's motion commands to navigate complex environments.\n1) Branch 1: Depth Image Processing: The first branch processes depth images captured from the UAV's onboard camera. The depth image is passed through 8 convolutional layers [21], each using a LeakyReLU [26] activation function and batch normalisation [17] to stabilise the training process and improve convergence. The convolutional layers are divided into four blocks, each ending with a max-pooling operation to reduce the spatial dimensions and retain important features while providing translational invariance. The output of this branch is then flattened and passed through two fully connected (linear) layers, reducing the feature vector to a size of 128. This vector is further transformed using a tanh [6] activation function, preparing it for concatenation with the outputs from the other branches.\n2) Branch 2: LIDAR Scan Processing: The second branch processes a 1D LIDAR scan, which provides a flat, radial representation of the surrounding obstacles. This input is passed through 5 one-dimensional convolutional layers, also using LeakyReLU activation and batch normalisation. The layers are organised into three blocks, each reducing the dimensionality of the input data while extracting spatial features relevant to obstacle proximity and distribution around the UAV. A final linear layer compresses the resulting feature map into a 128-dimensional vector, followed by a tanh activation, aligning the output with that of the first branch.\n3) Branch 3: Positional Data Processing: The third branch takes a vector containing various positional parameters: the distance and bearing to the goal, the difference in altitude between the UAV and the goal, the relative height of the UAV, and the previous actions in the x, z, and yaw directions. This input is processed through a single fully connected layer"}, {"title": "E. Reward Function for the Guidance DRL", "content": "with a tanh activation, resulting in a 128-dimensional vector. This approach allows the network to incorporate dynamic task-specific parameters directly, aiding the guidance decision-making process.\n4) Fusion and Output Layer: The outputs from the three branches are concatenated into a single 384-dimensional vector. This combined representation is then passed through a series of 5 fully connected layers with LeakyReLU activations. These layers progressively refine the fused features, enabling the network to learn complex relationships between the depth, LIDAR, and positional information. A final linear layer with a tanh activation function outputs the predicted control commands, including the x-velocity, z-velocity, and yaw speed of the UAV. The tanh activation ensures that the output values remain bounded between -1 and 1, which is often desirable for controlling the UAV's speed and orientation in a stable manner.\n5) Discussion of Architectural Choices: The use of convolutional layers in the depth image and LIDAR branches allows the network to capture spatial hierarchies and patterns within the sensor data, which is crucial for understanding the environment's structure. Batch normalisation aids in stabilising training by normalising the inputs of each layer, reducing internal covariate shifts. The LeakyReLU activation mitigates the risk of vanishing gradients in the early layers, ensuring more efficient learning. Tanh activation in the final layers helps ensure smooth control outputs, which is critical for the stability of UAV motion.\nThis multi-branch design enables the UAV to integrate depth-based obstacle detection, LIDAR-based spatial awareness, and task-specific positional data, resulting in a robust perception/guidance strategy suitable for GNSS-denied indoor environments.\nThe layout of the network can be seen in Fig. 4. The three separate branches that process the three inputs to the model are shown with the four 2D convolutional blocks, the three 1D convolutional blocks, and the fully connected layers for the data input. The layout of the FCN can be seen as combining and processing all the data together to produce the commands for the UAVs.\nThis network is trained using the TD3 algorithm shown in Fig. 5, where the network architecture shown in Fig. 4 is used as the actor and target actor networks. The critic networks are the same as the actor networks but add the action produced to the concatenation layer and produce a single Q-value.\nE. Reward Function for the Guidance DRL\nThe APF approach guides the UAV towards the target while avoiding obstacles by generating a set of forces that determine the desired actions for forward movement (x-velocity), turning (yaw speed), and altitude adjustment (z-velocity). The optimal actions are defined based on the distance to obstacles and the relative position of the target.\n1) Attractive Forces Towards the Target: When the UAV is more than 1 meter away from any obstacles, the optimal action"}, {"title": "G. GAT based Task Allocation Architecture", "content": "is determined by attractive forces that guide the UAV towards the target position. These forces are calculated as follows:\n$X_{vel} = clip(cos(\\theta), 0.0, 1.0)$, (7)\n$\\dot{v} = 3 \\cdot clip\\bigg(\\frac{\\theta}{\\pi}, 0.2, -1.0, 1.0\\bigg)$, (8)\n$Z_{vel} = clip (2 \\cdot (target_z - UAV_z), -1.0, 1.0)$. (9)\nwhere:\n$\\theta$ is the heading angle between the UAV and the target.\ntarget and position are the heights of the target and the UAV, respectively.\nclip(x, a, b) limits x to the range [a, b].\nThe term $cos(\\theta)$ ensures that the UAV moves faster when directly facing the target, with a movement restricted to forward directions due to the forward-facing depth camera. The yaw speed adjustment force2 scales the heading error, with the multiplier of 3 enhancing responsiveness, allowing for faster alignment with the target direction. The altitude adjustment force3 corrects the UAV's height relative to the target, adjusting at twice the rate of the height difference.\n2) Repulsive Forces for Obstacle Avoidance: When the UAV is within 1 meter of an obstacle, the behaviour shifts to a repulsive mode, where forces are applied to move the UAV away from nearby obstacles and adjust its orientation. The repulsive forces are calculated based on the relative angle to the obstacle, bobs, which represents the bearing of the obstacle with respect to the UAV's forward direction."}, {"title": "This approach offers several advantages:", "content": "The repulsive force in the forward direction (x-velocity) is calculated as:\n$X_{vel"}, -0.5, "cdot cos(\\theta_{obs})$, (10)\nwhere $O_{obs}$ is the angle to the closest detected obstacle. This formulation ensures that the UAV moves away from obstacles, with the negative sign indicating a backward movement. The cosine term reduces the backward speed when the obstacle is to the sides of the UAV, prioritising movement directly away from obstacles positioned directly in front.\nThe repulsive adjustment to the yaw speed, $\\omega(\\theta_{obs})$, is designed to turn the UAV away from obstacles, depending on their relative position. It is defined as:\n$\\omega(\\theta_{obs}) = \\begin{cases}\n0 & if \\theta_{obs} = \\frac{-\\pi}{2}, \\\\\n0.5 + \\frac{\\theta_{obs}}{\\pi} & if \\frac{-\\pi}{2} < \\theta_{obs} < 0, \\\\\n-0.5 + \\frac{\\theta_{obs}}{\\pi} & if 0 < \\theta_{obs} < \\frac{\\pi}{2}, \\\\\n0 & if \\theta_{obs} = \\frac{\\pi}{2}\n\\end{cases}$ (11)\nThis piecewise function ensures that the UAV adjusts its yaw speed based on the angle of the detected obstacle:\nFor obstacles directly to the left ($\\theta_{obs} = \\frac{-\\pi}{2}$) or right ($\\theta_{obs} = \\frac{\\pi}{2}$), no yaw adjustment is applied (\u03c9 = 0).\nWhen the obstacle is slightly to the left ($\\frac{-\\pi}{2} < \\theta_{obs} < 0$), a positive yaw adjustment is applied to turn the UAV right, away from the obstacle.\nFor obstacles slightly to the right (0 < $\\theta_{obs} < \\frac{\\pi}{2}$), a negative yaw adjustment is applied to turn the UAV left, away from the obstacle.\nThe constants \u00b10.5 provide a base turning speed, and the term $\\frac{\\theta_{obs}}{\\pi}$ adjusts the turning rate based on the angle, creating a smooth transition in yaw speed as the obstacle's relative position changes.\nThe repulsive force in the vertical direction (z-velocity) is calculated similarly to during the attraction periods of flight, shown in equation 9. These repulsive actions ensure the optimal action for the UAV to safely navigate away from obstacles is biased in the training. Combining backward movement with appropriate turning adjustments to maintain a safe distance from nearby objects.\n3) Calculating the reward: The reward function is designed to incentivise the UAV to follow the optimal actions"]}