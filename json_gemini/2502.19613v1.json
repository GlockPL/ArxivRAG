{"title": "Self-rewarding correction for mathematical reasoning", "authors": ["Wei Xiong", "Hanning Zhang", "Chenlu Ye", "Lichang Chen", "Nan Jiang", "Tong Zhang"], "abstract": "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in reasoning-related tasks such as mathematics and coding. Notable examples include ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Gemini (Team et al., 2023). Following the release of GPT4-01, LLMs with strong reasoning abilities have attracted even more attention, along with inference methods that enhance reasoning. A particularly desirable property of such models is their ability to detect inconsistencies and errors in self-generated responses-based on feedback to their prior outputs and correct these errors to produce improved responses. This process is often referred to as self-correction in the literature (Welleck et al., 2022; Madaan et al., 2024; Kim et al., 2024). When an external ground-truth reward model is available, studies (Kim et al., 2024; Qu et al., 2024; Shinn et al., 2024) have shown that LLMs can refine their initial responses based on external gold reward feedback and determine when to terminate the self-correction loop. These approaches have proven effective for both mathematical reasoning and general agent tasks. Moreover, even when relying on imperfect proxy rewards, models can still achieve higher accuracy in revised responses by leveraging feedback from an outcome-based reward model (see Section 5 for empirical results). However, since these reward models are often themselves LLMs, deploying them requires running multiple models during inference, which increases computational costs and deployment complexity. In contrast, without external reward feedback, current LLMs struggle to refine their initial responses solely based on their intrinsic capabilities\u2014a limitation known as intrinsic self-correction (Huang et al., 2023). While reward models are traditionally trained with an additional scalar head for general-purpose chat (Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023) and reasoning tasks (Cobbe et al., 2021a; Lightman et al., 2023), recent work suggests that LLMs themselves can generate reward signals in a generative way. For example, the LLM-as-a-judge approach (Zheng et al., 2023; Dubois et al., 2023) prompts the LLM to evaluate text outputs, effectively serving as a surrogate for human feedback. Another emerging direction explores generative reward models (Zhao et al., 2023; Dong et al., 2024; Zhang et al., 2024b; Mahan et al., 2024; Zhang et al., 2024a), which formulate evaluation tasks as instruction-following problems, using the probability of generating specific tokens as the reward value. These methods leverage LLMs' next-token prediction capabilities, integrate the generation and evaluation into a unified framework."}, {"title": "Self-rewarding reasoning framework", "content": "We introduce a self-rewarding reasoning framework for LLMs, which integrates the generator and reward model into a single LLM, enabling autonomous reasoning, evaluation, and correction. This unification simplifies the model's decision-making process and reduces computational overhead compared to external reward-based approaches."}, {"title": "Algorithmic framework for self-correction", "content": "We focus on the self-correction in mathematical reasoning and propose a two-stage framework that relies only on self-generated data. In the first stage, we use sequential rejection sampling to construct long chain-of-thought (CoT) trajectories that encode both self-rewarding and self-correction behaviors. Fine-tuning models on these trajectories enables them to detect the error in the self-generated responses and revise the previous attempts. In the second stage, we further enhance these patterns through reinforcement learning with rule-based signals."}, {"title": "Empirical validation and analysis", "content": "Through extensive experiments, we show that self-rewarding correction significantly outperforms intrinsic self-correction. Additionally, we conduct ablation studies to investigate the learning dynamics of the proposed framework, providing deeper insights into its behavior and effectiveness. The training codes and datasets are publicly available on GitHub\u00b9."}, {"title": "2. Related Work", "content": "We review the works that are mostly related to our project in this section. Our work aligns with research on self-rewarding alignment (Yuan et al., 2024b; Prasad et al., 2024), where both of our project and their methods share similar spirits that we can unify the generation ability and evaluation ability into a single LLM. These methods leverage iterative DPO-type algorithms, where the model labels its own generated responses to provide training signals for subsequent iterations, enabling self-improvement. In contrast, our approach does not focus on self-improvement during training. Instead, we rely on an external ground-truth reward model to provide learning signals in training. Our study emphasizes inference-time alignment for reasoning-focused LLMs, where self-rewarding signals are employed solely to guide inference rather than training. Our work is closely related to self-correction in LLMs. We refer interested readers to the survey (Pan et al., 2023) for a more comprehensive review and only review some representative approaches that are mostly related to our project. Li et al. (2024) demonstrated that incorporating teacher model reflections into SFT data enhances students' self-reflection abilities in general-purpose conversation tasks. However, for reasoning tasks, Huang et al. (2023) found that current LLMs\u2014without additional training\u2014fail to self-correct purely through intrinsic reasoning (i.e., prompting). This observation is also validated in Qu et al. (2024); Tyen et al. (2023); Zheng et al. (2024). A more in-depth analysis shows that most prior successful studies in this domain depend on external (ground-truth) reward models to determine when to initiate and terminate self-correction (Kim et al., 2024; Qu et al., 2024; Shinn et al., 2024; Madaan et al., 2024). Currently, there is no major work demonstrating that intrinsic self-correction (via prompting or fine-tuning) is reliably effective. Furthermore, because external reward models are typically LLM-based, these methods introduce additional computational overhead by requiring a multi-agent system for inference. Recognizing this challenge, our study explores how LLMs can autonomously evaluate response quality and correct errors without external reward models. Specifically, we introduce a self-rewarding reasoning framework that enables a single LLM to perform error detection and self-correction effectively. Among the works in self-correction, the most relevant work is the recent Kumar et al. (2024), which employed a multi-turn deep RL approach to train self-correcting models. In comparison, this work introduces a new and general self-rewarding formulation for reasoning-focused LLMs, with self-correction as a representative application. Compared to the intrinsic correction and the framework in Kumar et al. (2024), one major difference is that our framework equips models with self-rewarding ability, enabling our models to intelligently scale inference compute by selectively revising the first attempts, which helps to reduce computational overhead by avoiding unnecessary iterations. We will also design experiments to illustrate this idea. Algorithmically, our approach also differs from Kumar et al. (2024). We first use sequential rejection sampling to construct long CoT trajectories with both self-rewarding and self-correction patterns, which serve as warm-up fine-tuning data. We then enhance these behaviors through reinforcement learning (using either DPO-type algorithms or PPO)"}, {"title": "3. Self-rewarding Reasoning Language Models", "content": "We formulate the self-rewarding reasoning process as a multi-turn Markov Decision Process (MDP). After observing an initial prompt $s_1 = x \\in \\mathcal{X}$ from some distribution $d_0$, an LLM, denoted as $\\pi$, will generate an initial reasoning attempt $a^1 \\sim \\pi^1(\\cdot|s^1)$ from the action space $\\mathcal{A}$. The LLM then self-rewards its response by generating an evaluation:\n$y^1 \\sim \\pi^1(\\cdot|s^1, a^1)$.\nIf the model assesses its answer as correct ($y^1 = [\\text{VERIFY}] \\text{ correct}$, details provided later), the generation stops. Otherwise, the LLM proceeds to the next step, generating a refined response and evaluation:\n$(a^2, y^2) \\sim \\pi^2(\\cdot|s^2)$,\nwhere the generation is conditioned on the updated state $s^2 = (s^1, a^1, y^1)$. The self-refinement process continues until the model produces a self-evaluation $y^h$ that assesses the answer as correct.\nWe assume that we have access to the ground-truth verifier $r^* : \\mathcal{X} \\times \\mathcal{A} \\to \\{0, 1\\}$, which determines whether a response is correct. Throughout this study, we use the ToRA verification script (Gou et al., 2023), built on the Python library SymPy for symbolic mathematics. We also present a representative Example 1 to illustrate the process. Following standard post-training practices for LLMs, we adopt a two-stage approach:\n1.  Starting with an initial LLM $\\pi_0$ (e.g., a general-purpose chatbot), we collect demonstration data by a sequential rejection sampling process and fine-tune $\\pi_0$ to get an improved model $\\pi_{\\text{ref}}$, which integrates self-rewarding reasoning abilities.\n2.  We further refine $\\pi_{\\text{ref}}$ using RL, leveraging it as the reference model. This stage can further enhance the model's ability to assess correctness and refine previous responses."}, {"title": "3.1. Self-rewarding Instruction-following Fine-tuning", "content": "To train the LLMs to evaluate the reasoning steps, we formulate this task as an instruction-following task, following prior works (Zhao et al., 2023; Dong et al., 2024; Liu et al., 2023; Ye et al., 2024; Wang et al., 2024; Zhang et al., 2024b). Specifically, we allow models to include reasoning in their evaluations while requiring them to output specific tokens to indicate their evaluation results. We experimented with different token choices, such as: (i) a prompt \"Is the most recent final answer correct (Yes or No)?\" with \"Yes\" and \"No\" as the response tokens, as used in (Xie et al., 2023; Zhang et al., 2024b); (ii) explicit markers such as \"[VERIFY] correct\" and \"[VERIFY] wrong\". Our experiments show no significant performance differences between these choices. During inference, rather than using the likelihood of \"Yes\" as a reward (as in (Zhao et al., 2023; Dong et al., 2024; Zhang et al., 2024b)), we sample the evaluation token from the distribution. This allows us to use a standard inference pipeline without any specific adjustment. We choose these specific tokens primarily for research simplicity. However, we expect that similar results can be achieved even if these special tokens are replaced with more natural language expressions, such as \"wait\", \"aha\", or \"let me re-check the answer\", where one can also leverage the LLMs to complete this paraphrasing process. We employ a rejection sampling approach, similar to STaR (Zelikman et al., 2022) and RAFT (Dong et al., 2023), where we generate a large amount of self-correction trajectories and only preserve the desired trajectories. The major difference is that since the self-correction behavior is sparse in base models and self-rewarding pattern is missing, it is unlikely to collect the desired trajectory directly. In view of this, we sequentially prompt the base model and generate different steps separately. Then, we combine them into long CoT trajectories that incorporate both self-rewarding and self-correction patterns.\nOur data collection process consists of the following steps:\n1. Generating initial reasoning responses: training prompts from datasets such as MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021a) and sample $N_1 = 50$ initial responses $a^1$ per prompt as our base trajectories (see Section 5 for details of experiment setups).\n2. Self-rewarding signal sampling: For each prompt and initial response, we further sample $N_2 = 8$ self-evaluations and keep only one evaluation result that is the same as the ground truth. Then, we split them into $\\mathcal{G}_{\\text{correct}}$ and $\\mathcal{G}_{\\text{wrong}}$ using the ground-truth verifier $r^*$.\n3. Correction sampling: For each prompt and initial response in $\\mathcal{G}_{\\text{wrong}}$, we sample $M_1 = 8$ completions by providing the feedback that the initial response was wrong to collect trajectories that successfully revise incorrect responses. For each prompt and initial response in $\\mathcal{G}_{\\text{correct}}$, however, we also tell the model that the response was incorrect and collect $M_2 = 4$ completions. By doing so, we want to additionally collect \"correct-to-correct\" trajectories in the face of wrong judgment.\nEventually, we collect $8 \\times |\\mathcal{G}_{\\text{wrong}}| + 4 \\times |\\mathcal{G}_{\\text{correct}}|$ full trajectories. Then, we filter the dataset and only keep the following types of data:\n*   wrong $a^1$, $y^1 = [\\text{VERIFY}] \\text{ wrong}$, correct $a^2$;\n*   correct $a^1$, $y^1 = [\\text{VERIFY}] \\text{ wrong}$, correct $a^2$;\n*   correct $a^1$, $y^1 = [\\text{VERIFY}] \\text{ correct}.\nWe provide an example of data collection process in Table 2. We limit the horizon to two iterations due to resource constraint, and preserve at most one trajectory per base sample to control dataset size. Then we fine-tune the LLMs using standard SFT pipeline to maximize:\n$\\sum_{\\mathcal{D}_{IFT_1}} [\\log P(y^1|x, a^1) + \\log P(a^2|x, a^1, y^1)] + \\sum_{\\mathcal{D}_{IFT_2}} \\log P(a^2|x, a^1, y^1) + \\sum_{\\mathcal{D}_{IFT_3}} \\log P(y^1|x, a^1).$"}, {"title": "3.2. KL-regularized Reinforcement Learning", "content": "In this stage, we aim to further enhance the self-rewarding IFT models using reinforcement learning. We consider both deep RL methods (Schulman et al., 2017) and direct alignment algorithms (Zhao et al., 2023; Rafailov et al., 2023; Azar et al., 2023; Liu et al., 2023). To facilitate the reinforcement learning stage, we assume there exists a trajectory-wise reward function $u^*(\\tau)$ for trajectory\n$\\tau = (x, a^1, y^1,...,a^H,y^H)$.\nHowever, instead of learning a proxy reward from data like the BT model in RLHF (Ouyang et al., 2022) or outcome-supervised reward (ORM) in previous mathematical reasoning literature (Lightman et al., 2023), we primarily use the oracle reward\n$u^*(\\tau) = r^*(x, a^H)$,\ni.e., whether the final result is correct or not. The main advantage is that the oracle reward can largely mitigate the risk of reward hacking. This is also referred to as the rule-based RL in the very recent literature (DeepSeek-AI et al., 2025). We will also study the additional rule designs for either reward value assignment (PPO training) or data ranking (DPO training), where an implicit $u^*$ is determined by the set of rules we use.\nFollowing standard RLHF methodologies (Ouyang et al., 2022; Bai et al., 2022), we optimize the following KL-regularized objective:\n$\\max \\mathbb{E}_{x \\sim d_0, a^1 \\sim \\pi_0} \\mathbb{E}_{\\tau \\sim \\pi(\\cdot|x, a^1)} [u^*(\\tau) - \\eta \\sum_{h=1}^H D_{KL}(\\pi^h(\\cdot|s^h), \\pi_{\\text{ref}}(\\cdot|s^h))]$.\nThe optimal policy, as well as its associated optimal value satisfies the following optimality condition (Xiong et al.,"}, {"title": "Proposition 3.2", "content": "We can recursively define the following optimal value functions and optimal policies for a KL-regularized MDP with horizon $H$ and deterministic external observation. For $Q$ value, we have\n$\\begin{cases}\n    Q^*(s^h, a^h, y^h) =  u^*(s^H, a^H, y^H), &\\text{if } h = H, \\\\\n    V^*(s^{h+1}), & \\text{if } h < H-1.\n\\end{cases}$\nAlso, for all $h \\in [H]$, we have:\n$V^*(s^h) = \\eta \\log \\mathbb{E}_{a^h, y^h \\sim \\pi_{\\text{ref}}(s^h)} \\exp(\\frac{Q^*(s^h, a^h, y^h)}{\\eta}) =: Z^h(s^h)$\n$\\pi^h(a^h, y^h|s^h) = \\frac{\\pi_{\\text{ref}}(a^h, y^h | s^h)}{Z^h(s^h)} \\cdot \\exp(\\frac{Q^*(s^h, a^h, y^h)}{\\eta})$.\nWe remark that one advantage of the proposition is that it allows deterministic external message (e.g. instruction prompts) in the state update, which will be useful when we consider a simplified research framework in Section 5.\nWe also adopt Direct Preference Optimization (DPO) (Rafailov et al., 2023; Azar et al., 2023; Zhao et al., 2023; Ethayarajh et al., 2024) to solve Equation 2, primarily due to computational constraints. In particular, we use the multi-turn DPO (M-DPO) framework from Xiong et al. (2024a), since it allows deterministic external observation in the state transition. To facilitate direct preference learning and bypass explicit reward training, we impose the following trajectory-level Bradley-Terry (BT) preference structure (Bradley & Terry, 1952). Specifically, given two trajectories $\\tau_1, \\tau_2$, the probability of $\\tau^1$ being preferred than $\\tau^2$, denoted as $\\tau^1 \\succ \\tau^2$, is\n$P(\\tau^1 \\succ \\tau^2 | \\tau^1, \\tau^2) = \\sigma(u^*(\\tau^1) - u^*(\\tau^2))$,\nwhere $\\sigma(z) = 1/(1 + \\exp(-z))$ is the sigmoid function. Following Xiong et al. (2024a), we take log on both sides of (4), and connect a utility function $u_\\theta$ with associated policy $\\pi_\\theta$ and value $V_\\theta$:\n$\\log \\frac{\\pi_\\theta(y^h|s^h)}{\\pi_{\\text{ref}}(y^h|s^h)} = V_\\theta(s^h) - V_{\\theta}^{h-1}(s^h)$,\n$\\log \\frac{\\pi_\\theta(a^h, y^h|s^h)}{\\pi_{\\text{ref}}(a^h, y^h|s^h)} = Q_\\theta(s^h, a^h, y^h) - V_\\theta(s^h)$.\nFor a pair of trajectories $\\tau^{\\omega}, \\tau^\\prime$ where $\\tau^{\\omega} \\succ \\tau^\\prime$, we have\n$u_\\theta(\\tau^{\\omega}) - u_\\theta(\\tau^\\prime) = \\log \\frac{\\pi_\\theta(a^{\\omega, 1}|x) \\pi_\\theta(y^{\\omega, 1}|a^{\\omega, 1})}{\\pi_{\\text{ref}}(y^{\\omega, 1}|a^{\\omega, 1})} + \\log \\frac{\\prod_{h=1}^H \\frac{\\pi_\\theta(a^{\\omega, h}|s^h)}{\\pi_{\\text{ref}}(a^{\\omega, h}|s^h)}}{\\prod_{h=1}^H \\frac{\\pi_\\theta(a^{\\prime, h}|s^h)}{\\pi_{\\text{ref}}(a^{\\prime, h}|s^h)}} + \\log \\frac{\\pi_{\\text{ref}}(y^{\\omega, 1}|x, a^{\\omega, 1})}{\\pi_{\\text{ref}}(y^{\\prime, 1}|x, a^{\\prime, 1})}$\nTaking this reward difference parameterization into the log-likelihood of the BT model $(\\tau_{\\omega}, \\tau^\\prime) \\in \\mathcal{D} \\log \\sigma(u_{\\theta}(\\tau_{\\omega}) - u_{\\theta}(\\tau^\\prime))$, we obtain the loss function $\\mathcal{L}_{M-DPO}(\\theta)$:\n$\\mathcal{L}_{M-DPO}(\\theta) = - \\mathbb{E}_{(\\tau_{\\omega}, \\tau^\\prime) \\in \\mathcal{D}} \\log \\sigma \\left( \\log \\frac{\\pi_\\theta(a^{\\omega, 1}|x)}{\\pi_{\\text{ref}}(a^{\\omega, 1}|x)} + \\frac{\\pi_\\theta(y^{\\omega, 1}|a^{\\omega, 1})}{\\pi_{\\text{ref}}(y^{\\omega, 1}|a^{\\omega, 1})} + \\frac{\\prod_{h=1}^H \\frac{\\pi_\\theta(a^{\\omega, h}|s^h)}{\\pi_{\\text{ref}}(a^{\\omega, h}|s^h)}}{\\prod_{h=1}^H \\frac{\\pi_\\theta(a^{\\prime, h}|s^h)}{\\pi_{\\text{ref}}(a^{\\prime, h}|s^h)}} - \\frac{\\pi_\\theta(y^\\prime|a^\\prime)}{\\pi_{\\text{ref}}(y^\\prime|a^\\prime)} \\right).$"}, {"title": "4. Experiment Results", "content": "We evaluate models' mathematical reasoning abilities using standard benchmarks, including MATH500 (Hendrycks et al., 2020), Olympiad-Bench (He et al., 2024), and Minerva Math (Lewkowycz et al., 2022). These datasets provide a moderate size for reliable and efficient model evaluation, covering topics such as algebra, geometry, probability, number theory, and calculus. For training, we mainly use the prompts in NumiaMath-CoT dataset (Beeching et al., 2024). Specifically, we use a 50K subset for the self-rewarding IFT stage, a 10K subset for validation and model selection, and the remaining data for RL training. During inference, the model generates up to 4096 tokens, with VLLM 0.5.4 (Kwon et al., 2023) accelerating the process. We employ two categories of metrics to evaluate our models: (1) mathematical reasoning and self-correction and (2) reward model accuracy. First, we follow Kumar et al. (2024) to consider the following metrics to evaluate the models' ability of mathematical reasoning and self-correction.\n1.  accuracy of the first attempt;\n2. accuracy of the final answer;\n3. $\\Delta(t_1, t_2)$: improvement in accuracy from the first attempt to the final answer;\n4. $\\Delta_{\\rightarrow c}(t_1, t_2)$: fraction of problems changed from incorrect to correct;\n5. $\\Delta_{c \\rightarrow i}(t_1, t_2)$: fraction of problems changed from correct to incorrect.\nDue to the nature of the self-rewarding reasoning framework, we additionally include the metrics to measure the accuracy as a reward model. We also defer a more comprehensive understanding of the proposed framework with a slightly simplified template to next section, where we will additionally compute the ratio of modifying a correct answer to incorrect when facing a misleading reward."}, {"title": "1.  RM Accuracy (a, b)", "content": "class-dependent accuracy for correct and incorrect trajectories. In other words, a is the true positive rate and b is the true negative rate;"}, {"title": "2.  Ratio $\\frac{p_c}{p_i}(t_1, t_2)$", "content": "probability of modifying a correct answer to incorrect when facing a misleading reward.\nFor all evaluations, we use zero-shot CoT prompting and greedy decoding following the convention of recent projects with Qwen-2.5-Math models. We use Qwen2.5-Math-7B-base as the base model, which is continuously pre-trained on extensive mathematical and instruction-following data. Sequential rejection sampling (introduced in Section 3.1) is used for data collection, resulting in a dataset of 32K trajectories, where we roughly balance between correct and incorrect first attempts. In fine-tuning, samples are packed into 8192-token blocks and we use a learning rate of 1e-5, a cosine scheduler, and a 0.05 warm-up ratio. Global batch size is set to be 32. We train the models for three epochs and eventually select the one at the end of the first epoch. For iterative DPO training, we adopt setups from Xiong et al. (2024a) with a learning rate of $2 \\times 10^{-7}$, a cosine scheduler, and a batch size of 32. We tune $\\eta \\in \\{0.1, 0.5\\}$ and also train with and without an NLL loss in the DPO objective (Pang et al., 2024; Xie et al., 2024a; Liu et al., 2024). For each iteration, we use 20K prompts and collect 8 responses per prompt. Then, we extract the comparison pairs using the correctness score. If all responses admit the same score, we skip the prompt. A 10K validation set from NuminaMath-CoT is used for model selection. The primary metric for model selection is accuracy at turn 2. When models achieve comparable turn-2 accuracy, we choose the models with higher $\\Delta(t_1, t_2)$ improvement. The best model of these training setups is used as the representative model. For PPO training, we mainly follow a pulic example script of veRL"}, {"title": "4.1. Main Results", "content": "We report the main results in Table 3. Note that there can be an error of 0.1 due to rounding. We first observe that intrinsic self-correction without explicit reward signals typically reduces final test accuracy. Upon analyzing the outputs, we find that models tend to modify their initial responses responses regardless of its correctness, as they lack a mechanism to determine when to refine their answers versus when to terminate the correction process. Moreover, even when given ground-truth rewards, base models with prompting alone achieve only marginal improvement in incorrect-to-correct transitions $\\Delta_{\\rightarrow c}(t_1, t_2)$. For example, on MATH-500 benchmark, prompting with gold reward only leads to $\\Delta_{\\rightarrow c}(t_1, t_2) = 1.4\\%$. We also notice that the STaR/RAFT method, which fine-tunes models on revised incorrect attempts, fails to significantly improve performance. It increases $\\Delta_{\\rightarrow c}(t_1, t_2)$ (incorrect-to-correct transitions) on MATH500 from 1.4% to 5.0%, but still suffers from a $\\Delta_{c \\rightarrow i}(t_1, t_2)$ (correct-to-incorrect transitions) of 6.2%. Additionally, the STaR/RAFT+ variant, which includes correct-to-correct trajectories, becomes more conservative in modifying the initial attempt. While this reduces incorrect corrections ($\\Delta_{\\rightarrow i}(t_1, t_2)$), it also lower $\\Delta_{\\rightarrow c}(t_1, t_2)$, ultimately degrading test accuracy. These findings align with prior studies, and highlight the limitations of intrinsic self-correction, even with training (Huang et al., 2023; Kumar et al., 2024). Across all tasks, self-rewarding reasoning models consistently improve final accuracy with higher $\\Delta(t_1, t_2)$ compared to baseline methods. We notice that fine-tuning on the synthetic trajectories with self-correction behavior yields models with much higher $\\Delta_{i \\rightarrow c}(t_1, t_2)$, suggesting that the models are more good at correcting the error in the self-generated responses. Distint from the STaR/RAFT, models trained with self-rewarding IFT also exhibit significantly lower $\\Delta_{c \\rightarrow i}(t_1, t_2)$, indicating they are better at recognizing when to stop due to the additional self-rewarding signals. For instance, on MATH500, self-rewarding IFT achieves $\\Delta_{i \\rightarrow c} = 5.0\\%$ (vs. 1.4% for intrinsic self-correction); self-rewarding IFT achieves $\\Delta_{c \\rightarrow i} = 0.4\\%$ (vs. 15.4% for intrinsic self-correction and 3.8% for STaR/RAFT+);\nSince STaR/RAFT(+) and self-rewarding IFT use the same data synthesis approach (rejection sampling) but under different self-correction frameworks, these results highlight the advantage of our self-rewarding reasoning framework. We also compare the self-rewarding reasoning models with RL training against their single-turn counterparts. For both the PPO and DPO, the self-rewarding reasoning models achieve higher final test accuracy due to the additional correction step. For instance, the self-rewarding IFT + PPO yields a model with 43.4% final accuracy on OlympiadBench, and 38.4% on Minerva Math, compared to the 39.5% and 33.1%"}, {"title": "5. More Experiment Results with a Two-turn\nConversation Framework and Llama\nModels", "content": "In this section, we continue to investigate the self-rewarding reasoning framework. Previously, we combined multiple reasoning steps into a single long CoT trajectory, which aligns with common practice. However, this approach poses significant challenges for our study, as models\u2014particularly Qwen2.5-Math-7B-base-often fail to strictly follow instructions for evaluating or revising responses based on their history. For instance, models sometimes will also generate the evaluation results using or not to correct the responses even though the self-evaluation result is \u201c[VERIFY] wrong\". Additionally, models can perform multiple rounds of self-evaluation and correction, but these steps are tightly coupled and cannot be easily decoupled into separate stages.\nTo address these issues, we adopt a simplified two-turn conversation framework, where the user provides explicit instructions between different steps. Specifically, after receiving the mathematical problem, the model will first generate the CoT reasoning $a^1$ and self-evaluation $y$. Then, the user provide a deterministic instruction o based on the self-evaluation y:\n1. Since your initial response is self-evaluated as incorrect, there might be an error in the solution above because of lack of understanding of the question. Please correct the error, if any, and rewrite the solution. Put your final answer within { } ;\n2. Since your initial response is self-evaluated as correct, confirm it and provide no further modifications. Put your final answer within { } .\nMeanwhile, when collecting the data, the self-rewarding signal is determined directly by the ground-truth oracle reward with the template designed in Zhang et al. (2024b), without additional reasoning. While this simplification may reduce reward modeling accuracy (Zhang et al., 2024b), it facilitates controlled experimentation by allowing modifications to the self-rewarding signal. Similar frameworks-without the self-rewarding component-have been explored in previous works (Huang et al., 2023; Kumar et al., 2024). Qwen2.5-Math-7B-base serves as a strong and specialized base model, which is pre-trained on a large mathematical corpus. To ensure generality and a more comprehensive evaluation, we experiment with the Llama model series. Specifically, our base models include Llama-3-8B-it and Llama-3-SFT, the latter being fine-tuned on Open-MathInstruct2-1M (Toshniwal et al., 2024a). While both models are generally weaker than Qwen2.5-Math-7B-base, Llama-3-SFT is stronger than Llama-3-8B-it.\nIn this section, we evaluate the models' mathematical reasoning abilities using the MATH and GSM8K benchmarks, which are well-suited to their capacities. For MATH, we use 7.5K training problems during the self-rewarding IFT stage, supplemented by 7.5K prompts from Open-MathInstruct2 for M-DPO training, with a similar setup for GSM8K. Model selection is performed using a 1K validation set from Open-MathInstruct2. Since we formulate the task as a multi-turn chat problem, we can directly use Axolotl's"}, {"title": "5.3. Main Results with Llama Models", "content": "Experiments with Llama models align well with the Qwen model. Our experiments with Llama models show similar trends to those observed with Q"}]}