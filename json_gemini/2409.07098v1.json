{"title": "Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering", "authors": ["Zehao Wang", "Han Zhou", "Matthew B. Blaschko", "Tinne Tuytelaars", "Minye Wu"], "abstract": "Novel view synthesis of indoor scenes can be achieved\nby capturing a monocular video sequence of the environ-\nment. However, redundant information caused by artificial\nmovements in the input video data reduces the efficiency of\nscene modeling. In this work, we tackle this challenge from\nthe perspective of camera selection. We begin by construct-\ning a similarity matrix that incorporates both the spatial\ndiversity of the cameras and the semantic variation of the\nimages. Based on this matrix, we use the Intra-List Diver-\nsity (ILD) metric to assess camera redundancy, formulating\nthe camera selection task as an optimization problem. Then\nwe apply a diversity-based sampling algorithm to optimize\nthe camera selection. We also develop a new dataset, In-\ndoorTraj, which includes long and complex camera move-\nments captured by humans in virtual indoor environments,\nclosely mimicking real-world scenarios. Experimental re-\nsults demonstrate that our strategy outperforms other ap-\nproaches under time and memory constraints. Remarkably,\nour method achieves performance comparable to models\ntrained on the full dataset, while using only an average of\n15% of the frames and 75% of the allotted time.", "sections": [{"title": "1. Introduction", "content": "Novel view synthesis using a monocular RGB camera\nfor indoor environments has significant practical value in\nreal-world applications. This technology enables users to\nreconstruct indoor scenes from self-captured video data,\nmaking it useful for VR/AR applications, such as virtual\ntours. However, these videos often include lengthy se-\nquences with redundant information due to complex camera\nmovements induced by human actions. This redundancy re-\nduces efficiency in modeling scenes, leading to increased\nprocessing time or lower rendering quality. Discarding\ncameras that have redundant information in the video is\nchallenging due to the correlation between cameras and the\nvast number of possible combinations.\nRecent research on camera selection can be categorized\ninto two main groups: iterative model retraining methods\nand pre-selection methods. The first group is relevant to the\nNext-Best-View (NBV) Selection [6, 10, 18] task. The sub-\nsequent optimal view is determined by the reconstruction\nuncertainty or error evaluated from retrained models. This\nsort of multi-round retraining is time-consuming [10]. Pre-\nselection strategies leverage scene information extracted\nfrom captured trajectories before the reconstruction. These\nmethods typically offer improved efficiency [9,27]. How-\never, the approach by Kopanas et al. [9] tends to select cam-\neras with extensive overlap between their frustums and the\nscene, as discussed in Sec. 4.5. This characteristic results\nin numerous redundant frames at these featured locations.\nAdditionally, 3D distance-based strategies, such as those\ndescribed in [18, 27], are primarily designed for object-\ncentric scenes with hemisphere camera distribution. These\nstrategies overlook critical factors in indoor scenes, such as\ncamera orientation. Moreover, the camera held by a human"}, {"title": "2. Related work", "content": null}, {"title": "2.1. Neural Rendering in Indoor Scenes", "content": "The development of neural rendering methods enables\nefficient dense reconstruction of 3D scenes from 2D im-\nages [8, 12, 14, 15, 23]. Despite this progress, reconstruct-\ning indoor scenes poses distinct challenges. Indoor envi-\nronments often contain significant occlusions between ob-\njects, textureless flat surfaces, and fine-grained small struc-\ntures [31]. These characteristics complicate data collection,\nleading to long and intricate trajectories that usually con-\ntain a lot of redundant information. While some approaches\nfocus on optimizing the model structures [16, 24]. Another\nline of work addresses geometric difficulties by incorporat-\ning prior information, such as depth [26, 28, 31], normal\nprior [22, 26, 28, 31], or semantic prior [5]. In this work,\nwe focus on efficiently utilizing cameras from a trajectory\nfor neural rendering of indoor scenes. We design a novel\nsampling strategy that enables the same base models [8, 15]\nto converge to the performance of models trained on full\ntrajectories, using only 10% to 20% of the data, with even\nreduced training time."}, {"title": "2.2. Sampling in Neural Rendering", "content": "The reconstruction quality of a subset of data is strongly\ninfluenced by the sampling strategy, as highlighted in vari-\nous works [3, 17]. Sampling can involve points, pixels, or\ncameras. In computer vision, the classical FPS method [3]\nintroduce a pixel-wise distance measure with a greedy algo-\nrithm to enhance image acquisition. For more related 3D re-\nconstruction fields, earlier works such as [1, 13] emphasize\nthe need for an informative sampling strategy, particularly\nin large-scale and time-intensive traditional methods.\nDespite recent advancements in efficient 3D reconstruc-\ntion methods, such as iNGP [15] and 3DGS [8], the need\nfor efficient sampling persists, especially when the mem-\nory or time has some limit in real applications. Recent\nworks [4, 11, 18] adopt uncertainty-based heuristics. How-\never, these typically require intensive computation for a\nfull rendering procedure per added camera. Furthermore,\nthese methods struggle in scenarios where cameras are more\nfreely distributed, as discussed in [9, 10]. Another branch of"}, {"title": "3. Method", "content": "We first provide an overview of the objectives and algo-\nrithm underlying our strategy in Sec. 3.1. Then, we dis-\ncuss the factors contributing to the similarity measure in\nSec. 3.2. And Sec. 3.3 covers a statistical approach for bal-\nancing these factors."}, {"title": "3.1. Diversity-based Sample Strategy", "content": "We define the measure of redundant information shared\nbetween two cameras by similarity $M[i, j]$. The objective\nof our task is to find an subset $S$ of views from all cam-\neras $D$ that maximizes diversity. In this work, we define\nthe diversity following Intra-List Diversity (ILD) [21, 30],\nwhich is recognized for its stability in distance-based mea-\nsurements. It is defined as:\n\n$ILD(M_S) = \\frac{2}{n(n - 1)} \\sum_{1\\le i < j \\le n} (1 - M_S[i, j]),$ (1)\n\nwhere $M_S$ represents the similarity matrix derived from a\nsample set $S \\in D$, and $n$ is the number of samples. The ob-\njective for optimal subset selection can then be formulated\nas:\n\n$S^* = \\underset{S\\in D}{\\text{arg max}} ILD(M_S).$ (2)\n\nFinding the optimal solution for this objective is an NP-\nhard problem. We adopt a stable greedy solution based on\npairwise measurements, as used in the field of image ac-\nquisition [3], which is designed to select diverse yet repre-\nsentative pixels. This approach is also known as Maximal"}, {"title": "3.2. Diversity Measurement Factors", "content": "Several potential factors are critical in selecting cam-\neras for effective 3D reconstruction include camera dis-\ntance [18, 27], view similarity [1, 27], and scene cover-\nage [9]. Inspired by these insights, we identify specific pair-\nwise measures for these factors and define them in Table 1.\nIn particular, Dist3D and Ang3D are two fundamental com-\nponents in the spatial domain that target on location diver-\nsity and angular diversity respectively. These two measure-\nments capture non-prior information about cameras in 3D\nspace. Additionally, image similarity offers prior informa-\ntion that helps emphasize the selection of visually diverse\ncameras, which can further enhance the global semantic in-\nformation entropy.\nPrevious work incorporates 3D distance in sampling\nstrategies [1, 18, 27], but rarely considers angular diversity.\nNeglecting this aspect can result in suboptimal outcomes.\nFor example, when selecting two cameras out of three that\nprovide similar observations and are positioned closely, as\nillustrated in the Ang3D scenario of Fig. 2, the Ang3D mea-\nsurement can guide the selection towards the camera pair\nwith the greatest angular separation. Therefore, our ap-\nproach integrates this measure with Dist3D and image sim-"}, {"title": "3.3. Factor Balancing", "content": "As indicated by Fig. 5 (Left) in the experiment sec-\ntion, the ILDs of selected camera sets typically show posi-\ntive correlations with their rendering quality. According to\nEqu. 2, our objective is to maximize the ILD with the opti-\nmal camera set. The greater the correlation between ILDS\nand rendering quality, the more likely the solution to our ob-\njective will lead to optimal rendering outcomes. Therefore,\nobtaining weights that make ILDs achieve a better correla-\ntion with rendering quality is what we need.\nIn this setup, a subset of scenes $X$ from the full dataset\nis split and used exclusively only for factor balancing. For\neach scene $x \\in X$, we sample $K$ tuning sets of frames that\ncan exhibit varying levels of reconstruction quality based\non prior information, as detailed in Appendix B. For clar-\nity, the scene notation $x$ is omitted from all metrics in\nthis section. Then, for each scene, we can efficiently ob-\ntain the similarity matrices for each combination of $\\alpha, \\beta$\ntogether with the ILD scores which can be denoted by\n$L_{ild}^{(\\alpha,\\beta)} = \\{ILD(M^{(\\alpha,\\beta)})\\}_{k \\in \\{1,2,\\dots, K\\}}$ for these $K$\nframe sets. Meanwhile, the exact rendering PSNRs for the\ntuning sets can be determined, forming the reconstruction\nquality list $L_{rec}$. The rank correlation between $L_{ild}^{(\\alpha,\\beta)}$ and\n$L_{rec}$ can be computed by Kendall's Tau [7]:\n\n$\\tau(L_{ild}^{(\\alpha,\\beta)}, L_{rec}) = \\frac{2(p - q)}{n(n - 1)},$ (4)\n\nwhere $p$ is the number of concordant pairs, $q$ is the num-\nber of discordant pairs, and $n$ is the number of samples.\nSpecifically, $p$ increments by one if $L_{ild}^{(\\alpha,\\beta)}[i] < L_{ild}^{(\\alpha,\\beta)}[j]$\nand $L_{rec}[i] < L_{rec}[j]$. Otherwise $q$ increments by one. The\noptimal combination can be obtained by:\n\n$\\alpha^*, \\beta^* = \\underset{\\alpha,\\beta}{\\text{arg max}} E_{x \\in X} [\\tau(L_{ild}^{(\\alpha,\\beta)}, L_{rec})],$ (5)\n\nwhere $\\alpha$ and $\\beta$ follow the definition in Eq. 3."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Dataset", "content": "To accurately assess the impact of our sampling strat-\negy in the indoor scenes, we select the widely-used Replica\ndataset [20] as one of our evaluation sets. However, we\nobserve that in natural human motion captures, people may\nstop, move, or rapidly change direction. As a result, camera\nmovements often do not follow a consistent speed. These\ncharacteristics are not adequately captured by the Replica\ndataset. To address this, we create a new dataset called In-\ndoorTraj, where the trajectories are captured within pho-\ntorealistic indoor environments by real humans who have\nfull control over the camera. The recorded trajectories fea-\nture long and complex camera movements. Additionally, a\nseparate and disjoint testing trajectory is provided for each\nscene. This design inspired by [25] ensures that the eval-\nuation faithfully reflects the model's performance. More\ndetails can be found in the Appendix. A."}, {"title": "4.2. Baselines", "content": "In the experiments, we primarily compare three base-\nline sampling strategies with our proposed method. The\nRandom strategy samples a random subset from the full\ndataset. The Uniform strategy samples frames at equal\ntime intervals throughout the sequence. Starting from a ran-\ndom initial frame, this strategy maintains a constant time\nstep, looping back if necessary, to ensure coverage of the\nentire sequence.\nThe third baseline proposed by Kopanas et al. [9], is a\nstrong pre-selection method with publicly available code. It\nquantifies 3D diversity using a composite coverage metric,\nwhich introduces strong dependencies in sequential sam-\npling. We select it as a representative for its line of work.\nOther potential baselines such as FVS in NeRFDirec-\ntor [27], as discussed in Section 3.2, can be treated as ab-\nlated versions of our method. The importance of each com-\nponent is addressed in the ablation study in Section 4.7."}, {"title": "4.3. Implementation Details", "content": "The experiments are conducted using a single NVIDIA\nP100 GPU for 30,000 iterations. We utilize the default\nhyperparameters recommended by the two classical neural\nrendering methods, instant-NGP (iNGP) [15] and Gaussian\nSplatting (3DGS) [8].\nAll the camera locations are scaled to the range [-4,4].\nWe set $\\sigma = 0.5$ in the Gaussian kernel Dist3D to ensure\nsmooth distance measurements. For ImgSim metric, the\nimage features are encoded using CLIP [19] trained on We-\nbLI following [29]. Since aabb scale significantly impacts\nthe performance of iNGP [15], we set the value to 16 for\nReplica dataset and 4 for IndoorTraj dataset to achieve rea-\nsonable reconstruction results."}, {"title": "4.4. Main Results", "content": "In Table. 2, we evaluate the rendering quality of three\nbaseline sampling strategies as well as our own. Except for\nthe deterministic sampling method by Kopanas et al. [9], the\nother three methods generate different sample sets depend-\ning on the random seed. To ensure robustness, we generate\nfive sample sets for each method and report the average ren-\ndering PSNR along with the standard deviation.\nFor the standard Replica dataset, the camera movement\nis smooth with a largely uniform speed within a small re-\ngion. We can find from Table. 2 that our strategy performs\ncomparably to the Uniform strategy, with only a small\nperformance gain. In contrast, the method proposed by\nKopanas et al. [9] performs poorly on the Replica dataset,\nshowing an average of 2.25 PSNR drop even compared\nto the Random selection strategy. This underperformance\nmight be due to the coverage measurement lacking sensitiv-\nity when camera poses are not diverse.\nIn our IndoorTraj dataset, the Uniform strategy is not ef-\nfective anymore. The experiments show that, with the same\nsample size, the average PSNR gap between the Uniform\nstrategy and our approach is approximately 1.46. Due to\nthe more diverse and natural camera movement, the method\nproposed by Kopanas et al. [9] begins to outperform both\nthe Uniform and Random strategies. However, it still re-\nsults in an average 0.89 PSNR lower than ours.\nWe can also observe that there is always a consistently\nlarge performance gap between the Random strategy and\nthe Uniform strategy. Intuitively, the Random strategy can\nlead to clusters of cameras with similar poses capturing re-\ndundant scene information. A more theoretical explanation\ncan be found in Quasi-Monte Carlo (QMC) theory [17],\nwhich demonstrates that the effectiveness of specific sam-\npling patterns in a function approximation problem is theo-\nretically superior to pure Monte Carlo sampling. We high-\nlight this concept here as a reference for those interested in\nexploring this theory further."}, {"title": "4.5. Qualitative Evaluation", "content": "In addition to the quantitative evaluation, we also con-\nduct an analysis of how the selection strategy behaves in\nthe specific scenario. We select the scene openplan-1 as an\nexample. This scene is the largest one within our dataset,\nwhich includes a long training trajectory that spans across\ntwo rooms.\nCamera distribution: In the first column of Fig. 3, we\npresent a top-down heatmap showing the density of selected\ncamera positions. Our strategy shows a smoother cover-\nage of the entire scene. Even in relatively dense regions\nlike $R_6$, the orientations of the cameras are distributed di-\nversely throughout the area. Instead, the method proposed\nby Kopanas et al. [9] tends to densely select the cameras\ndistributed on the corners of the scene, and there are multi-\nple cameras having very similar poses (location and orienta-\ntion) as seen in region $R_4$ and region $R_5$. This is because the\nstrategy prioritizes cameras whose frustum covers a larger\nportion of the scene's bounding box. The Uniform strat-\negy largely relies on the moving speed of the camera. If the\ntrajectory is captured with flexible speeds, as is common in\nuser-generated data, the method will be biased toward the\ntemporally dense area, such as region $R_2$ and region $R_3$.\nThe Random strategy often samples cameras with similar\nposes, such as those in the region $R_1$, and even highly over-\nlapping ones, as seen in $R_0$.\nRendering Quality: From the testing view at $P_1$ the ob-\nservation aligns with the bias that we identified in Kopanas\net al. [9] strategy. Their strategy tends to select cameras\npositioned at corners facing large open spaces, often over-\nlooking scene boundaries. This results in significant arti-\nfacts such as the one in $P_1$ view. For the Random and Uni-"}, {"title": "4.6. Performance Gaps with Varying Sample Ratios", "content": "In this experiment, we evaluate the rendering perfor-\nmance for different sampling strategies across various sam-\nple ratios. We compare our proposed strategy with two\nbaselines: Random and Uniform sampling. The strategy\nfrom Kopanas et al. [9] is excluded for its high time com-\nplexity and known selective biases from previous experi-\nments. Additionally, the performance of the model trained\non the full dataset over the same 30,000 iterations is repre-\nsented by a dashed line in Fig. 4. The base neural rendering\nmodel used in these experiments is 3DGS [8].\nAs shown in Fig. 4, our approach consistently outper-\nforms two other strategies, achieving on average 0.8 higher\nPSNR for kitchen-1 and kitchen-2 scenarios using just 20%\nof data samples. Impressively, it matches or even exceeds\nthe performance of models trained with the full dataset. No-\ntably, as shown in Fig. 4a, it surpasses the full dataset model\nby 0.2 PSNR using only 10% of the data, and this advantage\ngrows to 0.6 PSNR at 20%. Similar trends are observed in\nFig. 4b and 4d.\nThe underperformance of the model trained on the full\ndataset with a 30k iteration training budget suggests under-\nfitting, which points to presence of redundancy. We can also\nobserve that the training trajectories across different scenes\ndiffer in the levels of redundancy, explaining why the per-\nformances converge at different sample ratios. These results\nhighlight the necessity of crafting effective sampling strate-"}, {"title": "4.7. Abalation Study", "content": "To better understand the contribution of each component\nin our multi-factor similarity matrix and the effectiveness of\nthe hyperparameter tuning strategy, we performed an abla-\ntion study. By comparing the results, we can quantify the\nimportance of each aspect in the overall system. The ex-\nperiment setting follows the main experiment and we report\nthe average PSNR in Table. 3. Each row in the table rep-\nresents a different combination of factors included in the\nsimilarity matrix. In the last two rows for each base render-\ning model, we also compare the multi-factor similarity ma-\ntrix using weights obtained from our hyperparameter tuning\nstrategy with those using equal weights.\nFrom Table. 3, we can observe that the model with fac-\ntor balancing consistently performs best across most scenes,\nindicating the importance of all three factors working to-\ngether. For individual factors, Dist3D proves to be the most\ninformative across all scenes. For specific scenes, such as\noffice-3, kitchen-1, and living-1, a similarity matrix that in-\ncludes only Dist3D achieves performance comparable to the\nmulti-factor version. We can observe that, the combination\nof AngDist and Dist3D typically yields the second/third-\nbest rendering results, with a performance gap of less than\n0.2 PSNR on average compared to the full similarity matrix.\nTherefore, in real-world applications, prioritizing time effi-\nciency, most cases can be effectively handled by combining\nAngDist and Dist3D.\nIn Fig. 5, we present the rank correlations for various co-\nefficient configurations of the factors on the tuning sets. We\nselect $\\alpha = 0.7, \\beta = 0.2, \\gamma = 0.1$ that yields the highest\nKendall's Tau as the optimal weights. Compared to equal\nweighting across factors, the balanced weights are more ef-\nfective for producing samples that benefit neural rendering\nmodel training. There is an average of 0.29 PSNR improve-\nment for the IndoorTraj dataset as shown in Table. 3. In ad-\ndition, we evaluate whether the balanced weights enhance\nthe correlation between rendering PSNRs and ILD scores"}, {"title": "4.8. Time efficiency", "content": "In Table 4, we present the time costs of our methods\ncompared to the strategy proposed by Kopanas et al. [9].\nWe set the sample ratio to 5% across all scenes. When ex-\ntracting image features online (w feat. online), our strategy\nis at least five times more efficient than the method in [9]. If\nthe image features are pre-computed (feat. preload), the to-\ntal time cost is reduced to around 2.3 seconds, adding only\na minor delay to the overall training time. We also report\nthe time cost of our method without online feature extrac-\ntion (w/o feat. online), which applies to scenarios where"}, {"title": "5. Conclusion", "content": "This study introduces a camera selection method for\nindoor novel view synthesis from monocular videos.\nOur method models camera selection as an optimization\nproblem by integrating a redundancy-aware metric with\ndiversity-based sampling. By balancing the factors of the\nsimilarity matrix, the proposed ILD-based metric can reflect\nthe quality of the reconstructed rendering to some extent.\nThis metric has potential as a method for estimating ren-\ndering quality before model training. Experimental results\nhighlight the efficiency and robustness of our approach on\nboth the classic indoor dataset and our new dataset. Our\nmethod, using only a subset of cameras, outperforms the\nresults obtained from the full set due to the reduction of\nredundancy. The proposed dataset, which more accurately\nrepresents real human motion patterns, has further accentu-\nated the performance gap between traditional methods and\nour approach. This dataset holds promise for advancing fu-\nture research in related scenarios."}]}