{"title": "A Reality Check on Context Utilisation for Retrieval-Augmented Generation", "authors": ["Lovisa Hagstr\u00f6m", "Sara Vera Marjanovi\u0107", "Haeun Yu", "Arnav Arora", "Christina Lioma", "Maria Maistro", "Pepa Atanasova", "Isabelle Augenstein"], "abstract": "Retrieval-augmented generation (RAG) helps address the limitations of the parametric knowledge embedded within a language model (LM). However, investigations of how LMs utilise retrieved information of varying complexity in real-world scenarios have been limited to synthetic contexts. We introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) with real-world queries and contexts manually annotated for stance. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) and find that artificial datasets often fail to represent the complex and diverse real-world context settings. We show that synthetic datasets exaggerate context characteristics rare in real retrieved data, which leads to inflated context utilisation results, as measured by our novel ACU score. Moreover, while previous work has mainly focused on singleton context characteristics to explain context utilisation, correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source. Overall, our work underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented generation (RAG) can be used to alleviate problems arising from imperfect parametric knowledge of language models (LMs), which may encode limited and potentially outdated information (Gao et al., 2023; Vu et al., 2024). However, the benefits of RAG are only realised if 1) the retrieval module retrieves helpful information and 2) the generative model successfully leverages the retrieved information. As a consequence, there have been many studies looking at the performance of these two components and how to improve them (Gao et al., 2023).\nHowever, existing research has mainly studied RAG in a disjoint manner, where studies of the quality and relevance of the retrieved information are detached from studies of LM context usage (Shi et al., 2023; Xie et al., 2023; Tan et al., 2024; Du et al., 2024). Hence, little is understood about 1) the characteristics of retrieved contexts and 2) their impact on LM context usage. Most notably, studies of LM context usage have leveraged controlled datasets using synthesised context to emulate a limited set of context characteristics (see Figure 1, left). For example, CounterFact and its variants are template-based, lending a controlled albeit very artificial and simplistic setup (Yu et al., 2023; Du et al., 2024). ConflictQA, on the other hand, is based on a mix of generated and retrieved contexts to study context usage in a more realistic setup with coherent and convincing contexts (Xie et al., 2024). Nevertheless, the scenarios described by these datasets are not representative of real-world RAG scenarios, as the context types do not reflect the diversity and complexity of the ones returned by an actual retriever present in RAG (Longpre et al., 2021; Ravaut et al., 2024; Ortu et al., 2024).\nThis work studies context usage for RAG in real-world scenarios with real-world queries and context, as opposed to artificial samples. To this end, we focus on the prototypical information-seeking task of fact verification, where retrieving and utilising real-world information is vital. For the task, an agent is provided with a statement about the world a claim - and needs to decide whether it is true or false using context retrieved from an external source evidence (Guo et al., 2022). We take real fact-checked claims as 'queries' and the retrieved evidence as 'context' to evaluate RAG in this real-world setting, which naturally facilitates our goal of studying real-world context properties in RAG (Samarinas et al., 2021; Atanasova et al., 2022; Chrysidis et al., 2024; Glockner et al., 2024).\nIn particular, this work makes three main contributions. First, we introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand context) with real-world (query, context) pairs to facilitate studies of context usage and failures in real-world scenarios (\u00a73). Second, we introduce a novel context-usage measure, ACU, which rectifies issues in previous measures. Thirdly, we highlight major differences between popular synthetic datasets and real-world data (DRUID): both in over-arching characteristics (\u00a74), as well as how the provided context is used across different popular LMs (\u00a75).\nWe show that synthetic datasets oversell the impact of certain context characteristics (e.g. knowledge conflicts), which are rare in retrieved data. Furthermore, synthetic data exaggerates the 'context-repulsion' seen for LMs, as we rarely see this behaviour in realistic data. Finally, we show that there is no singleton context characteristic (e.g. context length or perplexity) indicating RAG failure in real-world settings. Altogether, our work provides a reality check on LM context usage and points to the need for real-world aligned studies to fully understand and improve context utilisation for RAG. We also provide tools and resources to facilitate such studies."}, {"title": "Related Work", "content": "Claim verification datasets typically measure an LM's ability to as-"}, {"title": "DRUID", "content": "Previous studies of context utilisation leverage synthetic datasets with synthesised claims and contexts, ignoring the retrieval part in RAG (Yu et al., 2023; Xie et al., 2024). We develop the datasets DRUID (5,490 samples) and DRUID+ (48,517 samples) to enable studies of context utilisation for real-world scenarios. To this end, we collect real-world claims from fact-checking sites and use automated retrieval to fetch corresponding evidence from the internet. DRUID is a high-quality subset of DRUID+ manually annotated for evidence relevance and stance. A DRUID sample consists of a triple. More details on the dataset can be found in Table 1 and Appendix C."}, {"title": "Claim Collection", "content": "We sample claims verified by fact-checkers using Google's Factcheck API. We only sample claims in English. The claims are collected from 7 diverse fact-checking sources, representing science, politics, Northern Ireland, Sri Lanka, the US, India, France, etc. All claims have been assessed by human fact-checkers. Further details on the claim"}, {"title": "Evidence Collection", "content": "For each claim in DRUID and DRUID+, we retrieve up to 5 and 40 snippets of evidence, respectively. First, a gold-standard evidence document is retrieved from the original fact-checking site, which is the 'summary' of the fact-checking article written by the author of the article. For the remaining snippets of evidence, we use an automated retrieval method (Appendix D). We collect the top 20 search results for each of the Google and Bing search engines. The found webpages are then chunked into paragraphs and reranked by the Cohere rerank model. Evidence corresponding to the top-ranked chunks is included in DRUID."}, {"title": "Relevance and Stance Annotation", "content": "Since the evidence is collected using automated retrieval, as opposed to controlled synthesis, we need to assess the relevance of the retrieved information to the claim, and, if it is relevant, what stance it represents (Wang et al., 2024c). For this, we crowdsource evidence-level annotations using Prolific and Potato (Pei et al., 2022). Each evidence piece in DRUID is double annotated for relevance (relevant or not relevant) and stance to the claim (supports, insufficient-supports, insufficient-neutral, insufficient-contradictory, insufficient-refutes or refutes). More details on the annotation, guidelines and examples from the annotation interface can be found in Appendix M.\nThe annotator compensation was approximately 9 GBP/hour (the compensation was fixed for each task while the annotator completion time varied)."}, {"title": "Context Characteristics", "content": "To understand the gap between the context provided in current diagnostic datasets for context usage and real RAG scenarios, we compare the characteristics present within our real-world dataset DRUID to the synthetic datasets CounterFact (Ortu et al., 2024) and ConflictQA (Xie et al., 2024). By virtue of their controlled setup, these and similar datasets have seen much use for the study of context utilisation and mechanisms thereof (Jin et al., 2024; Du et al., 2024; Tan et al., 2024; Kortukov et al., 2024).\nTo ensure adequate comparison, we recast all samples in CounterFact and ConflictQA to a claim-evidence format (see Appendix E). This can be done without loss of information as all datasets represent a binary task for the LM (answer in alignment with the evidence or not). Furthermore, we show in Appendix F that the analysis of context utilisation and mechanisms thereof are unaffected by the format of the task being either answer completion or claim verification \u2013 the reformatting leads to no change in the mechanism employed by the model and its manipulation results.\nIn addition to the aforementioned datasets, we also present the characteristics of DRUID+ to better understand the impact of only collecting the top-ranked evidence for DRUID."}, {"title": "Detection of Context Characteristics", "content": "Several context characteristics impacting context utilisation by humans and/or LMs have been identified by previous work (Section 2). As opposed to synthesising contexts with certain properties, we detect those in existing datasets. Along with manual annotation of relevance and stance, we leverage automated methods. We experiment with two types of automated detection methods to assess context characteristics: 1) rule-based methods and 2) prompt-an-LLM methods. For the latter we zero-shot prompt the Cohere Command R+ model. Initial trials leveraging human annotations of context characteristics showed high annotator disagreements, potentially due to the subjective nature of some of the characteristics, and were consequently abandoned. Instead, we opted to operationalise the properties, as we further describe below. This allows us to explore more model-based measures of context characteristics, which can be expected to have a greater impact on model context usage vis-a-vis subjective human perception of the same characteristics."}, {"title": "Relevance and stance", "content": "For DRUID, we use the manual relevance and stance annotations. For CounterFact and ConflictQA we infer those as follows. CounterFact contains counterfactual claims, for which the evidence is either the claim repeated (supports) or the claim but with the correct object restored (refutes). For each ConflictQA entry, we have a model-generated claim and two types of evidence \u2013 parametric memory aligned (supports) or counter memory aligned (refutes)."}, {"title": "Claim-evidence similarity", "content": "This is measured using Jaccard similarity (see Appendix G), which outputs values between [0,1], where 1 signifies maximum similarity. The overlap of claim words with evidence words, scaled by the number of claim words ('Claim-evidence overlap') is also measured. In addition, we detect if the evidence repeats the claim verbatim ('Repeats claim')."}, {"title": "Difficult to understand", "content": "We measure the Flesch reading ease score, claim length (number of characters), evidence length (number of characters) and model context perplexities for our studied models (Llama 3.1 8B and Pythia 6.9B) to proxy how 'difficult to understand' is the context. Generally, we may consider samples that correspond to high model perplexities to be confusing to the model."}, {"title": "Implicit", "content": "We detect named entities (NEs) in the claim and measure the overlap with entities found in the evidence ('Claim entity overlap'). Spacy en_core_web_trf (based on RoBERTa-base) is used for the NE detection. Values are \u2208 [0,1] where 0 means that no NEs reappear in the evidence (maximum implicitness) and 1 means that all NEs were found in the evidence."}, {"title": "Refers to external source", "content": "Command R+ is prompted to tell whether some evidence contains a reference to an external source or not ('Detection by LLM'). Initial evaluation results show this detection method to align well with human annotations of the characteristic."}, {"title": "Unreliable", "content": "We use manually curated lists by Media Bias/Fact Check (MBFC) to automatically detect whether the evidence piece originates from a web page marked as using questionable sources, promoting conspiracy/pseudoscience or being a satire site ('Unreliable source'). However, due to the sparsity of the MBFC lists, we are unable to detect unreliability for all evidence in DRUID and DRUID+, lacking results for 26% and 34% of the samples, respectively."}, {"title": "Uncertain", "content": "We use a lexicon-based approach proposed by Islam et al. (2020) to detect hedge words and hedging discourse markers in the evidence to proxy 'uncertain' (\u2018Contains hedging' and 'Contains hedging discourse'). If a hedge word or hedging discourse marker is detected in the evidence, it is marked as 'uncertain' according to that method."}, {"title": "Additional characteristics", "content": "We check whether the evidence can be seen as directly pointing out a verdict by measuring whether the evidence contains the word \"True\" or \"False\". For the DRUID and DRUID+ datasets, we also record whether the evidence was published after the claim was made, as this allows to measure the occurrence of and effects of leaked information ('Pub after claim') (Schlichtkrull et al., 2023). Similarly, we measure whether the evidence comes from a fact-check webpage as this can be expected to contain additional leaked information ('Fact-check source') and whether it comes from the original fact-checking site summary ('Gold source')."}, {"title": "Analysis of Context Characteristics", "content": "Relevance and stance Relevance and stance annotations for all datasets are shown in Tables 8 and 9 in the Appendix. Most contexts are annotated as relevant; however, given the more ambiguous nature of real-world queries, especially in claim verification, there is more variety in the kinds of stances presented by the context provided in DRUID: The majority of the automatically retrieved contexts (50%) do not have a clear stance or are not sufficient for addressing the query. This is the consequence of using automated retrieval, for which not even state-of-the-art methods based on commercial search engines and Cohere modules are capable of consistently retrieving \u201cgold standard context\u201d. Admittedly, the retrieval setup is used in a zero-shot fashion and performance may improve somewhat with additional fine-tuning, while it would not solve all insufficiency issues stemming from automated retrieval. Conversely, synthesised samples always assume sufficient context. Our results show a clear discrepancy between synthesised and real-world datasets, proving the need for real-world aligned datasets for studies of context usage."}, {"title": "DRUID in comparison with other RAG datasets", "content": "The detected context characteristics for the synthetic datasets and DRUID are shown in Figure 2. More detailed results can be found in Appendix I. The synthetic CounterFact dataset has the highest Flesch reading ease scores, significantly shorter evidence lengths, frequent repetitions of the claim in the evidence, significantly higher Jaccard similarity values and very few uncertainty markers relative to the other investigated datasets. CF was designed to showcase simple knowledge conflict scenarios, causing much greater perplexity for both investigated LMs. In general, CF is tailored to a specific type of context usage that is not indicative of real, retrieved context, as shown by the comparison to DRUID. The generated evidence for ConflictQA has characteristics more similar to those of DRUID. However, we can see that DRUID has much longer claims and evidence than seen in either of the other datasets; furthermore, there are more uncertainty markers and a greater degree of implicitness in the naturally occurring context in the DRUID dataset."}, {"title": "Context Utilisation", "content": "We aim to assess the transferability of insights based on synthesised scenarios to real-world scenarios. To this end, we evaluate and compare LM context utilisation results on synthetic datasets to results on DRUID."}, {"title": "Method", "content": "We measure the context utilisation of Pythia 6.9B and Llama 3.1 8B, two models from two model families widely used in RAG-evaluation studies (Biderman et al., 2023; Grattafiori et al., 2024; Ortu et al., 2024; Xie et al., 2024; Jin et al., 2024), on the CounterFact, ConflictQA and DRUID datasets. To measure context utilisation, the models are evaluated in two modes: 1) without evidence and 2) with evidence. In both modes, the models are prompted to assess the veracity of a given claim (True, False, or None), without and with evidence respectively. More details on the prompting can be found in Appendix J. We evaluate context utilisation using the softmaxed model logits, which we describe further in the next section. In the main paper, we only show results for supporting and refuting evidence; behaviour for all forms of 'insufficient' evidence (where 'None' is the expected model output) can be found in Appendices K and L."}, {"title": "Evaluation", "content": "There is no consistent measure for context usage across similar work; Many studies look simply at changes in overall output distributions (Du et al., 2024; Marjanovic et al., 2024), which does not guarantee that the change is relevant to the provided context. Works in mechanistic interpretability often rely on logit differences for a specific token given evidence (Ortu et al., 2024; Yu et al., 2023), which are not normalised, do not factor in desired change, and limit comparisons. Due to these issues, we introduce a novel measure (ACU), which 1) uses softmax-normalised probabilities, to ensure meaningful comparison, 2) focuses on probabilities of specific tokens, to ensure relevant change, and 3) scales these values by the amount of possible increase in probability. To measure context usage for a model M, we consider the re-scaled difference in salient token probability $t \\in T = {True, None, False }$ for a claim C between settings with and without evidence E, as follows.\n$\\Delta P_M(t|C, E) = \\begin{cases} \\frac{P_M(t|C,E) - P_M(t|C)}{1-P_M(t|C)} & \\text{if } P_M(t|C,E) \\geq P_M(t|C), \\\\ \\frac{P_M(t|C,E) - P_M(t|C)}{P_M(t|C)} & \\text{otherwise.} \\end{cases}$\nHere, $P_M(t|C)$ and $P_M(t|C, E)$ denote the output probabilities for token $t \\in T$ by model M given a claim C and evidence E, respectively. The rescaling ensures that our metric is less sensitive to the original $P(t|C)$ value. We expect high positive values of $\\Delta P_M(t|C, E)$ for t that align with the stance of E and the opposite for t that conflict with the stance. For example, given an evidence piece with the stance refutes we should ideally measure a high value for $\\Delta P_M(False|C, E)$ and low values for $\\Delta P_M(True|C, E)$ and $\\Delta P_M(None|C, E)$.\nWe define a score of accumulated context usage (ACU) per sample {C, E} with stance $S_E$ for a model M as follows.\n$ACU(C, E, S_E, M) = \\frac{1}{\\lvert T \\rvert} \\sum_{t \\in T} D(t, S_E) \\Delta P_M(t|C, E)$   \n$D(t, S_E)$ denotes the desirable change in $\\Delta P_M$ for maximum context usage, which is either {-1,1}, depending on the annotated stance of the evidence. For example, $D(False, refutes) = 1$, whereas $D(True, refutes) = D(None, refutes) = -1$. This limits the range of ACU between [-1,1]."}, {"title": "How do LMs utilise real-world retrieved context compared to synthesised context?", "content": "We inspect the context usage behavior of Pythia and Llama on CounterFact, ConflictQA and DRUID to understand how LMs utilise real-world context compared to synthetic contexts. Accumulated context usage scores (Equation (2)) can be found in Figure 3. See Appendix K for more granular context usage results. We structure the analysis around a set of main findings, listed below."}, {"title": "Does LM context usage depend on characteristics of the evidence/context?", "content": "We evaluate the influence of different context characteristics (see \u00a74.1) on model context usage. For this, we calculate Spearman correlations between each context property and our context usage metric, ACU (Equation (2)), stratified by the evidence stance for each dataset. The results for Llama are shown in Figure 4. Results for Pythia, insufficient evidence from DRUID and additional fine-grained correlation results can be found in Appendix L. While we see a limited effect of any one characteristic, we highlight overarching findings below."}, {"title": "Conclusion", "content": "In this work, we ground studies of context utilisation to real-world RAG scenarios. We develop DRUID and compare it to synthesised datasets previously used to study context-utilisation. DRUID is a claim-verification dataset which contains naturally occurring claims and manually annotated evidence automatically retrieved from the internet. We find fundamental differences in dataset characteristics between DRUID and synthetic datasets (CounterFact and ConflictQA). We also introduce a novel Accumulated Context Usage (ACU) score to consistently measure context utilisation across LMs and datasets. On DRUID, correlations between singleton context properties and ACU are surprisingly small compared to other properties related to context source (e.g. contexts coming from specific types of websites). We hypothesise that, rather than singleton features, this owes to an aggregation of several characteristics contributing to context usage. This suggests the common factors impacting RAG success are broader than previously expected, and further work needs to be done to identify fine-grained causes of RAG failure. Furthermore, given the use of synthetic datasets to identify mechanistic components of context usage (Ortu et al., 2024; Yu et al., 2023), our results call into question the generalisability of the findings. With DRUID, we provide resources that better facilitate mechanistic and behavioral studies of context usage in real-world scenarios."}, {"title": "Limitations", "content": "Our work leverages claim verification as a vehicle for studies of realistic context utilisation. It is not fully clear whether insights related to context utilisation on this task will transfer to other RAG tasks, such as question-answering, which is overly represented in RAG evaluations. However, claim verification is a complex information-seeking task and we expect other tasks to have a large overlap or subset of properties with it. For example, as seen in this work, the QA format for CounterFact and ConflictQA are easily recast as a claim verification task. Furthermore, we show that intervention methods developed for QA tasks easily transfer to the same datasets when recast to a claim-verification setting in Appendix F. This suggests that some findings can be generalisable across tasks. Future work could expand on this work to incorporate other RAG-specific tasks to better understand the generalisability of context utilisation behaviours.\nIn our creation of DRUID we leverage an automated retrieval method based on commercial search engines and the Cohere Rerank model. While this method builds on state-of-the-art developments within the field of information retrieval, there are many other methods and tools we could have chosen, which could impact the context characteristics and model behaviour (Wang et al., 2024b; Katsimpras and Paliouras, 2024; Chen et al., 2024). A comprehensive comparison of different retrieval methods and their impact on context utilisation would be an interesting direction for future work.\nIn our creation of DRUID, we ensure to source claims from many different fact-checking sites to increase the representation of our dataset to the entire English-speaking world. However, it is not a uniform distribution, and the amount of context gathered per claim as well as the inter-annotator agreement for the context stances differs across claim sources. This could be due to unintentional cultural biases within our retrieval system or our annotators. Future work could investigate the impact of these cultural biases in the retrieval process on model output. DRUID, given its wide distribution of claim and evidence sources, would be an excellent dataset for such an investigation.\nWhile we investigate the impact of many characteristics on context utilisation, it is not exhaustive. Future work could look into the impact of other context characteristics on context utilisation. For example, our study and dataset omit interesting context characteristics related to propaganda, simplified or manipulated content, anecdotal, mix of languages, multimodality, multi-hop reasoning, preciseness etc. (Piskorski et al., 2023; Wan et al."}, {"title": "Ethical Considerations", "content": "Our work concerns the evaluation of RAG-based models on veracity prediction in a real-world setting. In the creation of the dataset, while we tried to maintain representativeness of the real world by including sources of data from different parts of the world, we introduced biases by selecting only English language sources. Consequently, our results only stand for claims and corresponding evidence sentences in English. For the annotation tasks, we do not retain any information about the annotators and pay them a fair wage as determined by the annotation platform. We also informed the annotators about how their data would be used and received their consent. However, for ease of understanding the subject matter and increasing chances of agreement, we screened the annotator pool to only include participants with at least an undergraduate degree, English fluency, no language-related disorders, and UK, US or Irish nationality. While this helped achieve higher-quality annotations, it limits the perspectives embedded in the dataset and may reinforce cultural biases, which we acknowledge as a potential risk.\nOtherwise, we do not foresee any pressing potential risks with this work. We performed foundational research focused on evaluation, which should come with few implications for malicious use, environmental impact, security violations, etc."}]}