{"title": "Still More Shades of Null: A Benchmark for Responsible Missing Value Imputation", "authors": ["Falaah Arif Khan", "Nazar Protsiv", "Denys Herasymuk", "Julia Stoyanovich"], "abstract": "We present Shades-of-Null, a benchmark for responsible missing value imputation. Our benchmark includes state-of-the-art imputation techniques, and embeds them into the machine learning development lifecycle. We model realistic missingness scenarios that go beyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR), to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test). Another key novelty of our work is that we evaluate imputers holistically, based on the predictive performance, fairness and stability of the models that are trained and tested on the data they produce.\nWe use Shades-of-Null to conduct a large-scale empirical study involving 20,952 experimental pipelines, and find that, while there is no single best-performing imputation approach for all missingness types, interesting performance patterns do emerge when comparing imputer performance in simpler vs. more complex missingness scenarios. Further, while predictive performance, fairness and stability can be seen as orthogonal, we identify trade-offs among them that arise due to the combination of missingness scenario, the choice of an imputer, and the architecture of the model trained on the data post-imputation. We make Shades-of-Null publicly available, and hope to enable researchers to comprehensively and rigorously evaluate new missing value imputation methods on a wide range of evaluation metrics, in plausible and socially meaningful missingness scenarios.", "sections": [{"title": "INTRODUCTION", "content": "As AI becomes more widely deployed into society, data most importantly, openly accessible high quality AI-ready data becomes a precious shared commodity. Among the factors affecting data quality is data missingness, a prevailing practical challenge of sustained interest to the data management, statistics and data science communities, and to the scientific community writ large.\nSpecifically in data management, debates have been raging since the very inception of the field regarding whether - and how to handle missing values, see Date [10] for a classic example. One difficulty, at the operational level, is that, while missing values are typically denoted with the token null, that may not always be the case. There could be hidden missing values in the data, like when 'AL' is selected as the state on a job application because it's the first item in the pull-down list. Another difficulty, at the semantic level, is that a null can take on many meanings: it may signify that the value is unknown, inapplicable, intentionally withheld, etc.\nIn this paper, we do not concern ourselves with this semantic debate, and we also do not consider hidden missing values. Rather, we focus our attention on a very specific case. We are given a dataset X (a single relation), in which the values of some features are missing, denoted by the marker null. The meaning of null is that the feature does take on a value in the real world (i.e., it is applicable) but that value is unobserved in X. Further, we intend to use X in a machine learning (ML) setting: to train a model or to make predictions about the tuples in X using a model that has already been trained and deployed. This model cannot deal with null values directly, and so these values must be imputed (filled in) as part of data pre-processing.\nAs our starting point, we will use Rubin's missingness framework [49] that, nearly 50 years since it was proposed, still remains the most popular approach to modeling missing data. Consider a dataset X of n samples, each with p features, and an indicator R such that $R_{i,j} = 1$ when the value of the j's feature of $X_i$ is missing: $X_{i,j}$is null, and $R_{i,j} = 0$ when that the value is observed: $X_{i,j}$ is not null. Rubin identified three data missingness scenarios:\nMissing Completely at Random (MCAR). Consider a dataset of job applicants that includes their most recent salary and the number of years of relevant work experience. The MCAR assumption holds, for example, when a job applicant's most recent salary is missing from their record for purely administrative reasons that are uncorrelated with either their (potentially missing) salary or with their (observed) experience. More formally, the probability that a feature value is missing is independent of X: $P(R|X) = P(R)$.\nMissing at Random (MAR). Suppose that job applicants with fewer years of relevant work experience are more likely to withhold their most recent salary (in hopes of being offered a higher salary by the prospective employer), and that this can be explained by observed covariates (i.e., by the number of years of work experience). The MAR assumption holds in this example, where the probability that some feature value is missing depends only on the observed covariates $X_{obs}$, and not on the missing feature values: $P(R|X) = P(R|X_{obs}).$"}, {"title": null, "content": "Missing Not at Random (MNAR). Suppose that a job applicant's most recent salary does not correlate with their number of years of experience but rather is based on some combination of their geographic location and the results of a skills test, neither of which are captured in the data. Further, suppose that individuals whose most recent salary was low may be likely to withhold their salary information (again, in hopes of being offered a higher salary by the prospective employer). The MNAR assumption holds in this example, because missingness is correlated with the missing value itself, and it cannot be explained by the observed covariates (i.e., by the number of years of experience): $P(R|X) \\neq P(R|X_{obs})$.\nMissing value imputation (MVI). Rubin's framework has formed the basis of a large body of technical work on missing value imputation (MVI), which has been covered in several excellent and comprehensive surveys [1, 2, 13, 20, 23, 27, 31, 37, 38, 43, 46-48, 63]. Broadly speaking, MVI approaches fall into several categories: (1) statistical, such as imputing missing observations with the median or mode [51]; (2) learning-based impute-then-classify, which use the observed data to iteratively impute and learn missing values, using k-nearest neighbors [3], clustering [19], decision trees [56], or ensembles [53]; and (3) joint data cleaning and model training [32, 34, 35], based on Rubin's seminal multiple imputation framework [50].\nBeyond Rubin's framework: Mixing scenarios and dealing with missingness shift. Rubin's framework, while clean and amenable to analysis, does not fully capture real-world missingness. First, it has been argued that the assumptions behind MCAR rarely hold in practice, and that most real-world data exists on a continuum between MAR and MNAR, depending on how that data was collected [21]. Second, while it is useful to distinguish between missingness scenarios based on their statistical properties to draw valid inferences, we are likely to encounter multi-mechanism missingness in practice, where several scenarios may co-exist in the same dataset, affecting subsets of its features, tuples, or both [63]. For example, Mitra et al. [44] introduce the notion of a data missingness life cycle and posit that data pre-processing steps such as assimilating data from different sources or from different heterogeneous populations (each with their own missingness patterns) can result in 'structured missingness' that cannot be represented using Rubin's framework. Third, it has been observed that, in the context of data-centric AI, the as- sumptions that hold about missingness in the training data may no longer hold post-deployment, a phenomenon termed missingness shift (by analogy with data distribution shift) [62].\nEvaluating MVI techniques. Despite an abundance of MVI tech- niques using different model architectures being proposed each year, making meaningful progress on learning from incomplete data ultimately comes down to how realistically we model miss- ingness: our proposed methods are only as good as the evaluation scenarios we rigorously test them on. If our evaluation scenarios do not realistically capture the kinds of missingness that we en- counter in practice, then our data cleaning methods are unlikely to be effective. Further, newly proposed MVI techniques are often only evaluated on the correctness of the imputation (imputation quality) and the correctness of the predictive model (accuracy or F1). However, in recent years, the scientific community is starting to uncover trade-offs between MVI efficacy and other normatively important dimensions of model performance, including algorithmic fairness [16, 22], and stability and robustness [63], beyond accuracy. We elaborate on this next.\nMissingness as a form of pre-existing bias. Consider again the job applicant screening example, and suppose that the applicants' gender and age are among the features. Female job applicants who suspect that they have been receiving lower salaries than their male colleagues, despite having comparable qualifications and job experience, may withhold salary information more frequently then men, hoping to narrow the gender wage gap in their next position. As a result there will be more missing salary values for women, and the probability that the value is missing depends on the observed covariate, gender. This situation is consistent with MAR missingness, and it points to pre-existing bias a situation where data reflects a history of societal discrimination [18]. For our final example of pre-existing bias, suppose that disability status is among the features. Job applicants who live with a disability are more likely to omit a value for this feature. Assuming that this feature does not correlate with the other features, this situation is consistent with MNAR, with missingness itself serving as a proxy for disadvantage.\nSummary of Contributions. We implemented an experimental benchmark called Shades-of-Null to rigorously and comprehensively evaluate state-of-the-art MVI techniques on a variety of realistic missingness scenarios (including single- and multiple-mechanism missingness and missingness shift), on a suite of evaluation met- rics (including fairness and stability), in the context of data pre- processing in a machine learning pipeline.\nOur work is (1) novel: to the best of our knowledge, the settings of multi-mechanism missingness and missingness shifts have not been empirically studied before; (2) comprehensive: we evaluate a suite of 8 MVI techniques on 6 benchmark datasets using 5 model types, running a total of 20,952 pipelines, and is the first study of such scale in the missing data domain to the best of our knowledge; (3) normatively grounded: we focus on decision-making contexts such as lending, admissions, and healthcare, where missingness is socially salient. Mitigating social harm such as algorithmic dis- crimination is a leading concern in these domains, and we evaluate the impact of MVI approaches on downstream model fairness and stability (which have been understudied in the context of miss- ing data), in addition to classically studied imputation quality and model correctness metrics.\nAs an additional engineering contribution, while developing the Shades-of-Null benchmark, we improved several state-of-the-art MVI techniques, see Appendix A.2 for details.\nWe make Shades-of-Null publicly available\u00b9 and hope to en- able researchers to comprehensively and rigorously evaluate new missing value imputation methods on a wide range of evaluation metrics, in plausible and socially meaningful missingness scenarios."}, {"title": "RELATED WORK", "content": "Missing value imputation (MVI) techniques. Learning-based ap- proaches have become increasingly popular, and include k-Nearest Neighbors, Decision Trees, Support Vector Machines, clustering,"}, {"title": "BENCHMARK OVERVIEW", "content": "In this work, we start with datasets in which there are no missing values, and then simulate missingness. We make this choice to be able to evaluate the effectiveness of missing value imputation scenarios with respect to the ground truth. Our methodology for simulating missingness is based on evaluation scenarios, defined by the missingness mechanism during train and test, shown in Table 1: (1) single-mechanism missingness, injected similarly into train and test (S1 - S3); (2) single-mechanism missingness, injected differently into train and test (missingness shift) (S4 - S9); and (3)"}, {"title": "Missing Value Imputation (MVI) Techniques", "content": "As discussed in Section 2, there is a large number of competitive MVI techniques in the literature. For our study, we picked 8 techniques, from 5 broad categories in the taxonomy of Emmanuel et al. [13], namely: (1) baseline: deletion; (2) statistical techniques: median-mode and median-dummy; (3) machine learning-based tech- niques: miss-forest [54] and clustering [19]; (4) deep learning- based techniques: datawig [4] and auto-ml [28]; and (5) multi- ple imputation: boostclean [34]. See Appendix A.1 for details. We limit our analysis to one joint-cleaning-and-training approach, boostclean [34], and leave the exploration of other methods, such as ActiveClean [35] and CPClean [32] to future work."}, {"title": "Evaluation Metrics", "content": "Following [23, 38], we evaluate performance of MVI techniques in two ways: directly using imputation quality metrics and indirectly based on downstream model performance metrics.\nImputation Quality Metrics. Shadbahr et al. [52] report that classically used imputation quality metrics are uncorrelated to downstream model performance, and that distributional metrics perform better. To confirm or refute this claim, we use a mix of discrepancy and distributional metrics."}, {"title": "Shades-of-Null Benchmark Architecture", "content": "Figure 1 summarizes the architecture of the Shades-of-Null bench- mark. Our goal when developing this benchmark was to support a large-scale evaluation of the impact of different missingness sce- narios and missing value imputation techniques on different aspects of performance of machine learning models. This benchmark is novel in several ways. First, it handles both single-mechanism and mixed missingness scenarios, including missingness shift. Second, it supports flexible evaluation of imputation quality and model performance - including accu- racy, fairness and stability - for multiple sensitive attributes and protected groups, as well as their intersections.\nThe central component of the architecture in Figure 1 is the benchmark controller that executes the specified evaluation sce- narios and applies error injectors to input datasets, accordingly. It then imputes missing values and preprocesses the imputed datasets using standard scaling for numerical features and one-hot encoding for categorical features. The controller then conducts ML model hyperparameter tuning. Finally, the evaluation module assesses MVI and ML model performance, both overall and for specific subgroups. To facilitate comprehensive model profiling, the evaluation mod- ule utilizes the Virny software library [24], measuring correctness, fairness and stability. We store experimental results in MongoDB.\nShades-of-Null incorporates two optimizations to streamline the execution of multiple experiments. Firstly, it separates miss- ing value imputation and model evaluation stages. This enables the preservation of imputation results, including imputed datasets stored in a file system and imputation performance metrics stored in a database, facilitating their reuse during the subsequent model evaluation stage. Consequently, redundant application of the same MVI on identical datasets with identical seeds for the pipeline is avoided when assessing different models. Secondly, the experimen- tal pipeline within the benchmark is tailored to evaluate MVI and ML models simultaneously on multiple test sets, with varying er- ror rates and missingness types. As both MVI and ML models only perform inference on test sets, the running time for executing a pipeline with one training set and one test set is close to that of a pipeline with one training set and multiple test sets."}, {"title": "Datasets and Tasks", "content": "A summary of the datasets used in this study is given in Table 2. diabetes\u00b2 [55] was collected in India through a questionnaire including 18 questions related to health, lifestyle, and family back- ground. A total of 952 participants are characterized by 17 attributes (13 categorical, 5 numerical) and a binary target variable that repre- sents whether a person is diabetic. Here, sex is the sensitive attribute, with \"female\" as the disadvantaged group.\ngerman\u00b3 [25] contains records of creditworthiness assessments, classifying individuals as high or low credit risks. It contains in- formation on 1,000 individuals characterized by 20 attributes (13 categorical, 7 numerical). Here, sex and age are the sensitive at- tributes, with \"female\" and <25 as the disadvantaged groups.\nfolk [11] was derived from US Census surveys conducted in 2014-2018, and is associated with several tasks. We selected ACS Income, a binary classification task to predict whether a person's annual income is above $50,000. We use 2018 data from Georgia, with 15,000 tuples and 10 attributes (8 categorical, 2 numerical). Here, sex and race are the sensitive attributes, with \"female\" and \"non-White\" as the disadvantaged groups.\nlaw-school4 [58] was gathered through a survey conducted by the Law School Admission Council (LSAC) across 163 law schools in the US in 1991, and contains law school admissions records of 20,798 applicants, characterized by 12 attributes (6 categorical, 6 numerical). The task is to predict whether a candidate would pass the bar exam. Here, sex and race are the sensitive attributes, with \"female\" and \"non-White\" as the disadvantaged groups.\nbank5 [45] contains data from direct marketing campaigns by a Portuguese bank, between 2008 and 2013. It contains information on 40,004 potential customers, with 16 attributes (10 categorical, 6 numerical) and a binary target that represents whether the individ- ual signed up for the product. Here, age is the sensitive attribute, with <25 and >60 as the disadvantaged group.\nheart consists of patient measurements with respect to cardio- vascular diseases, wiht information of 70,000 individuals charac- terized by 11 attributes (6 categorical, 5 numerical), and a target variable that denotes the presence of a heart disease. Here, sex is the sensitive attribute, with \"female\" as the disadvantaged group."}, {"title": "SINGLE-MECHANISM MISSINGNESS", "content": "In our first set of experiments, we compare the performance of MVI techniques in single-missingness scenarios (S1-S3 in Table 1), with the same fraction of errors (30%) in train and test.\nFor correctness, we report results for F1, and defer additional results to Appendix C.2. For fairness, we report metrics based on an intersectional group (e.g., sex&race) for datasets with two sensitive attributes, and a single group (e.g., age) for datasets with one sen- sitive attribute. For stability, we used a bootstrap of 50 estimators, each seeing a random 80% of the train set [12]. Higher values of F1 and label stability are better, and values of TPRD close to zero are better.\nWe evaluate 5 ML types: Decision Tree (DT), Logistic Regression (LR), Gradient Boosted Trees (LGBM), Random Forest (RF), and a Neural Network (historically called the Multi-Layer Perceptron or MLP)."}, {"title": "Correctness of the Predictive Model", "content": "As shown in Figure 3, the efficacy of MVI techniques, and the down- stream effect on the predictive performance of the model, according to F1, depends significantly on the characteristics of the dataset and the missingness mechanism.\nAll the methods we evaluated are competitive for all missingness mechanisms on heart and law-school. boostclean, which uses multiple imputation (MI), is competitive only on small datasets (diabetes and german), and only under MCAR. It performs particu- larly poorly on folk, with as much as a 0.8 drop in F1 compared to other methods. We discuss this unexpected performance of MI further in Section 7. In line with conventional wisdom, deletion significantly worsens predictive performance for small datasets. For diabetes (the smallest dataset), F1 drops by 0.06 for MCAR, 0.08 for MAR and 0.1 for MNAR, compared to ML model trained on clean data. Under MCAR, miss-forest (ML-based) and auto-ml (DL-based) are the best-performing techniques. Under MAR and MNAR there is no clear best-performing technique. For example, under MAR datawig (DL-based) shows good performance, but simple median-mode and median-dummy imputers are also competitive. This underscores the need to evaluate novel DL-based and ML-based approaches more holistically (on a variety of missingness mechanisms) to ensure that they justify the additional training overhead and complexity they introduce, compared to simple statistical methods."}, {"title": "Fairness of the Predictive Model", "content": "Next, we evaluate the fairness of the downstream ML model for different MVI techniques. We present TPRD in Figure 4, with results for other fairness metrics in Appendix C.2. The effect of MVI on fairness (error-disparity) is highly dataset specific, and it is strongly correlated with fairness of the model trained on clean data (no missingness), corroborating the findings of Guha et al. [22]."}, {"title": "Stability of the Predictive Model", "content": "Next, we compare the stability of the downstream model trained using different MVI techniques. Results in Figure 5 show that, as expected, stability depends most strongly on dataset characteristics (mainly size), then on imputation strategy.\nAll imputers preserve or marginally improve stability on bank and heart (largerst datasets, with 40k and 70k tuples, respectively), for all missingness mechanisms. Even deletion does not worsen stability in this case since there is sufficient data even after dropping rows with nulls, and clustering marginally improves stability over the model trained on clean data, for all missingness types.\nOn folk and law-school (medium-sized datasets, with 15k and 20k tuples, respectively), most MVI techniques are competitive, ex- cept for boostclean, which shows a 5% drop in label stability com- pared to the model trained on clean data. boostclean does joint cleaning and model training, and is expected to be sensitive to small changes in the training data, as validated by our findings.\nOn german and diabetes (about 1K samples each) deletion, clustering and boostclean are the worst performing methods on all missingness mechanisms, with label stability 8-10% lower compared to the model trained on clean data. miss-forest and auto-ml are the best performing methods, and match the stability of the model trained on clean data for all missingness types."}, {"title": "Imputation Quality and Fairness", "content": "To better understand the impact of different MVI techniques on model performance, we measure the imputation quality of each imputer before training ML models on the imputed training set. The quality of imputation is evaluated across three dimensions:"}, {"title": "MULTI-MECHANISM MISSINGNESS", "content": "We now evaluate different MVI techniques under multi-mechanism missingness [63], when MCAR, MAR and MNAR co-exist in both train and test data (S10 in Table 1). This is a more realistic setting that is likely to be encountered in practice [21]. We inject 10% of nulls from each mechanism into both train and test sets, for a total of 30% missingness, as in S1-3. In Section 4, we reported results for the best performing ML models for each dataset, which was the Random Forest for diabetes. In this experiment, we focus on diabetes and dive deeper into the performance of different model types."}, {"title": "Correctness of the Predictive Model", "content": "Recall from Section 4.1 that miss-forest was the best performing MVI method for diabetes, under all single-mechanism types, and it was the only method that performed comparably to performance of the model trained on clean data under MNAR. According to Fig- ure 6 (a), miss-forest remains one of the best-performing MVM techniques in terms of F1, for all model types. Further, under single- mechanism missingness, auto-ml (DL-based) was only competitive under MCAR and MAR, however, under multi-mechanism missingness, its F1 is comparable to that of miss-forest, for all model types.\nNotably, while boostclean was not the best performing under any single-mechanism missingness, it is highly competitive under multi-mechanism missingness, and even outperforms miss-forest and auto-ml for Decision Trees (DT), Logistic Regression (LR) and Gradient Boosting (LGBM), indicating the competitive edge of joint cleaning and training over impute-then-classify, under complex missingness scenarios [16, 36]. Finally, as for single-mechanism missingness, deletion continues to show the worst performance, with the largest effect of nearly a 15% drop in F1 compared to the model trained on clean data for LGBM."}, {"title": "Fairness of the Predictive Model", "content": "Recall from Section 4.2 that all MVI methods worsened fairness for the RF model for diabetes under single-mechanism missingness, with clustering being the worst-performing for all missingness types. From Figure 6 (b) we see that the effect of MVI on fairness is highly ML model-specific: for example, clustering improves fairness for LR but worsens it for all other model types, including RF. Note that LR has the worst unfairness (highest TPRD) when the model is trained on clean data, even as compared to deletion with other ML model types. miss-forest, datawig, and auto-ml pre- served fairness under single-mechanism missingness, and continue to show good performance under multi-mechanism missingness, for all model types."}, {"title": "Stability of the Predictive Model", "content": "Recall from Section 4.3 that deletion, clustering and boostclean worsened stability under all three single-mechanism missingness types. In Figure 6 (c), deletion and clustering continue to show poor performance for all model types. Interestingly, boostclean (multiple imputation with joint cleaning and training) is compet- itive under multi-mechanism missingness: it shows higher label stability than all other MVI approaches for the DT, and is competitive with the best performing imputers for MLP and LGBM.\nSimple imputers like median-mode and median-dummy- which were competitive on single-mechanism missingness - are no longer competitive under multi-mechanism missingness, while ML- and DL-based approaches like miss-forest, auto-ml and datawig con- tinue to show good stability."}, {"title": "Imputation Quality and Fairness", "content": "Since multi-mechanism missingness has never been empirically evaluated before (to the best of our knowledge), we additionally re- port performance on imputation quality metrics in Figure 7. We do not include boostclean since it does not provide imputed values, which we could use to assess imputation quality. In line with Shad- bahr et al. [52], we find that distributional metrics are more indica- tive of downstream performance than discrepancy metrics.\ndeletion, median-mode, and clustering all perform poorly on both discrepancy and distributional metrics, and are also the worst performing in terms of downstream model correctness (for F1). datawig, however, has high error on F1 and RMSE, but not on KL, and this is indicative of its good performance of the downstream predictive model."}, {"title": "MISSINGNESS SHIFT", "content": "Next, we evaluate the effect of missingness shift on the correctness, fairness, and stability of the downstream model. For that, we vary missingness mechanisms and missingness rates in train and test sets separately, first by holding the fraction of nulls in the test set constant (at 30%) and varying the fraction of nulls in the train set (10%, 30% and 50%). Then, we hold the fraction of nulls in the train set constant (at 30%) and vary the fraction of nulls in the test set (10%, 20%, 30%, 40% and 50%). Note that we have fewer settings for train set fractions because varying the test set is less computationally demanding.\nWe report results on diabetes. See Appendix D and GitHub repository for additional results on other datasets, with fixed and variable train and test missingness fractions."}, {"title": "Correctness of the Predictive Model", "content": "Train set missingness. As expected, according to Figure 9 the F1 of the predictive model depends both on the fraction of train nulls, and the missingness mechanism. Under MCAR, for low rates of missingness (10% and 30%), the F1 is constant, and then starts decreasing with more nulls (50%). Under MNAR, with deletion the decrease in F1 is monotonic, and this is in line with conventional wisdom. Under MNAR, the missingness depends on the value itself. This means that when values are missing, they cannot be inferred by other values in the observed data. For this reason, training under MNAR is very sensitive to the train missingness rate.\nInterestingly, correctness of ML and DL methods (miss-forest, auto-ml, datawig), and of multiple imputation (boostclean) is relatively insensitive to train set missingness rate. Even at 50% error these methods have sufficient data to learn the missingness pattern reasonably well. This is a striking result given that diabetes was the smallest dataset in our study with only 1,000 samples. Another interesting observation is that clustering is the only method that improves on F1 as the fraction of nulls in the train set increases, for all types of missingness as well as under missingness shift. However, at high rates of training error, even simple imputers like median-mode and median-dummy outperform clustering."}, {"title": "Fairness of the Predictive Model", "content": "Train set missingness. Figure 11 shows that all methods are sensi- tive to the train missingness rate when it comes to fairness. For example, miss-forest has stable performance under MNAR train. However, for MCAR and MAR training, the most fair setting is when the error rate in train and test is the same (30%). For larger (50%) and smaller (10%) train error rates, the model shows increased disparity (unfairness). Most ML and DL-based approaches such as auto-ml, datawig, and miss-forest exhibit this behavior, indicating that parity in train and test error rates is one of the most important factors for fairness."}, {"title": "Stability of the Predictive Model", "content": "Train set missingness. Figure 30 illustrates that, under MCAR and MAR train missingness, most methods with the exception of boostclean, deletion, and clustering show good stability, and are relatively insensitive to the increase in the train error rate. deletion shows the largest drop in stability with an increase in the train error rate, and this is in line with conventional wisdom as dropping rows with nulls decreases the effective train set size, and less training data makes models more unstable.\nTechniques like datawig and boostclean show worse stability under high train error rates, and this effect is the most pronounced under MNAR. miss-forest is, once again, highly robust, and main- tains good stability (comparable to that of the model trained on clean data) under all missingness scenarios, rates and shifts.\nTest set missingness. Figure 31 shows that the test missingness rate has very little effect on the stability, for most MVI approaches, with the exception of clustering, which shows higher instability at higher rates of test error, with the largest effect in scenario S7 (MAR train, MNAR test set)."}, {"title": "EXPERIMENTAL FINDINGS", "content": "Deletion is the worst strategy. Building on evidence in the scientific community [31, 39, 40, 42], our findings indicate that, in addition to hurting predictive performance, deletion also worsens stability and fairness, arguing against the use of this method in practice.\nMultiple imputation shows mixed results. There is conflicting evidence in the literature on the performance of multiple imputa- tion (MI), and our empirical findings are also mixed and somewhat unexpected. Feng [16] and Le Morvan et al. [36] argue that MI has superior predictive performance compared to impute-then- classify. Graham [21] find that MI performs well even with less data and small error rates. In contrast, we find that MI (specifically, boostclean) is only competitive on small datasets, and is highly unstable. This can likely be explained by the complexity-stability trade-offs: MI uses a more complex model class and is able to mini- mize the empirical loss, but due to this very complexity it shows a large variation in predictions under small train set perturbations.\nBest performing methods on single- and multi-mechanism missing- ness are different. A key novelty in our study is the evaluation un- der multi-mechanism missingness. We find that MVI methods such as boostclean, which was not competitive for single-mechanism missingness, performs well under mixed or multi-mechanism miss- ingness - which is a more complex missingness pattern and a more realistic scenario likely to be encountered in practice.\nFairness is highly mechanism-specific. Zhang and Long [60] find that different missing mechanisms lead to different prediction fair- ness when the imputation method is fixed, meaning that an MVI that leads to model fairness under, say, MAR may not lead to fairness under MNAR. We also observe this for all the datasets in our study.\nModel stability depends more on the dataset size and MVI technique than on the missingness scenario. We find that, for large datasets, even simple statistical imputers can preserve stability. In contrast, for small datasets, only a few MVI techniques preserve stability, while deletion, statistical imputation and more complex ML-based approaches all worsen stability.\nSensitivity to missingness rates. For predictive performance, mod- els are more sensitive to missingness during test than during train. Fairness is highly sensitive to both train and test missingness rates, and, for most methods, the model is most fair when there is the same rate of missingness in both. There is little effect of increas- ing test missingness on stability, but most imputers show worse stability at higher train missingness rates.\nNo good methods for MNAR missingness. MNAR is theoretically the hardest setting to model, and we empirically find that most state- of-the-art MVI methods do not attain predictive performance of the model trained on clean data. Further, our analysis of missing- ness shift indicates that training under MNAR is highly sensitive to missingness rates, and even methods that are trained on MNAR but evaluated on \"simpler\" test missingness (MCAR and MAR) per- form poorly. Notably, under multi-mechanism missingness, seeing a small amount of MCAR and MAR data during training boosts MVI performance. Designing MVI techniques that are effective under MNAR is an open research direction.\nImputation quality and fairness metrics are often insufficiently pre- dictive of the correctness, fairness and stability of downstream models. Shadbahr et al. [52] showed that distributional imputation qual- ity metrics are a more reliable indicator of the model's predictive performance, than discrepancy metrics under single-mechanism missingness. Our findings validate this result for multi-mechanism missingness as well. Further, we uncover trade-offs between pre- dictive performance, fairness and stability. Finally, we find that imputation fairness metrics are not predictive of model fairness.\nIt's complicated! Prior experimental work [20, 23, 31, 38, 52] concluded that there is no definitive answer to the question of which missing value imputation technique is best, according to predictive performance. Our findings support this, and further demonstrate trade-offs between model fairness and accuracy, and model stability and accuracy. This is not bad news, but rather an acknowledgement of the inherent complexity of the task of learning from incomplete data, and a call for more rigorous and holistic evaluation protocols- of the kind demonstrated in this work - in order to select the most suitable MVI technique for a given task."}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "Conclusions. We presented Shades-of-Null, a benchmark for re- sponsible missing value imputation. The key novelty of our work is in evaluating predictive"}]}