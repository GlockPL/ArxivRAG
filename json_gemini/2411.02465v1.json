{"title": "See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers", "authors": ["Jiaxin Zhuang", "Leon Yan", "Zhenwei Zhang", "Ruiqi Wang", "Jiawei Zhang", "Yuantao Gu"], "abstract": "Time series anomaly detection (TSAD) is becoming increasingly vital due to the rapid growth of time series data across various sectors. Anomalies in web service data, for example, can signal critical incidents such as system failures or server malfunctions, necessitating timely detection and response. However, most existing TSAD methodologies rely heavily on manual feature engineering or require extensive labeled training data, while also offering limited interpretability. To address these challenges, we introduce a pioneering framework called the Time series Anomaly Multimodal Analyzer (TAMA), which leverages the power of Large Multimodal Models (LMMs) to enhance both the detection and interpretation of anomalies in time series data. By converting time series into visual formats that LMMs can efficiently process, TAMA leverages few-shot in-context learning capabilities to reduce dependence on extensive labeled datasets. Our methodology is validated through rigorous experimentation on multiple real-world datasets, where TAMA consistently outperforms state-of-the-art methods in TSAD tasks. Additionally, TAMA provides rich, natural language-based semantic analysis, offering deeper insights into the nature of detected anomalies. Furthermore, we contribute one of the first open-source datasets that includes anomaly detection labels, anomaly type labels, and contextual descriptions-facilitating broader exploration and advancement within this critical field. Ultimately, TAMA not only excels in anomaly detection but also provide a comprehensive approach for understanding the underlying causes of anomalies, pushing TSAD forward through innovative methodologies and insights.", "sections": [{"title": "1 Introduction", "content": "Web services have undergone significant expansion and advance-ment in recent years [6, 64]. This expansion has led to the generation of vast amounts of time series data, including key performance in-dicators (KPIs) from cloud center and wireless base stations [59, 63]. Anomalies-defined as unexpected deviations from typical patterns in this data-can signal critical events such as device malfunctions and system failures. Consequently, time series anomaly detection (TSAD) techniques for web services have attracted considerable attention [11, 38], demonstrating significant practical value in mon-itoring web systems and ensuring service quality. Despite the ad-vancements in TSAD methodologies, existing approaches often struggle with several key challenges.\nFirstly, different methods specialize in different datasets [45, 65], and an \"one-size-fits-all\" universal solution is missing in the filed of TSAD [46]. Diverse approaches in TSAD demonstrate ongoing advancements, each contributing unique methodologies to tackle the inherent challenges of this field. Classical machine learning (ML) methods [12, 18, 20, 33, 44, 61] are frequently based on strong assumptions or require empirically crafted manual features [51]. In contrast, deep learning (DL) techniques are heavily reliant on hyper-parameters and are either supervised or semi-supervised [9, 35, 41], necessitating normal data for training- except for [2, 59, 71] where other forms of training are required. Noticing that for most real-world datasets, assumptions needed by ML techniques usually do not hold, and training data without anomaly for DL methods are often undesirable, rare, or even unavailable.\nSecondly, most existing techniques offer insufficient interpretabil-ity, providing a limited understanding of the reasons behind how the anomalies are identified [23]. Recently, more works have discussed and attempted to improve the explainability of TSAD algorithms [23, 28], which is also considered a critical disadvantage of most deep learning methods. Toward this topic, the taxonomy of anom-alies has been often mentioned in previous works [4, 13], where"}, {"title": "2 Relate Works", "content": ""}, {"title": "2.1 Time Series Anomaly Detection.", "content": "Many surveys [4, 9, 13, 41] are available in the field of TSAD. Clas-sical methods [12, 44, 61], especially unsupervised methods such as Isolation Forest (IF) [3, 33], and Local Outlier Factor (LoF) [20] are introduced into TSAD in early stages. There are also variants of these classical ML algorithms like Deep Isolation Forest (DIF) [60], which enhances IF by introducing non-linear partitioning. ML methods methods perform exceptionally well on many TASD datasets [45, 58], have been applied widely in industry [51], and serve as strong baselines in recent researches.\nDeep learning methods focus on learning a comprehensive repre-sentation of the entire time series by reconstructing the original in-put or forecasting using latent variables. Among all reconstructing-based models, MAD-GAN [30] is an LSTM-based network enhanced by adversarial training. Similarly, USAD [2] is an autoencoder-based framework that also utilizes adversarial training. MSCRED [66] is designed to capture complex inter-modal correlations and temporal information within multivariate time series. OmniAnomaly [49] addresses multivariate time series by using stochastic recurrent neural networks to model normal patterns, providing robustness against variability in the data. MTAD-GAT [70] employs a graph-attention network based on GRU to model both feature and tempo-ral correlations. TranAD [50], a transformer-based model, utilizes an encoder-decoder architecture that facilitates rapid training and high detection performance. Except reconstructing-based method, GDN [15] is a forecasting-based model that utilizes attention-based forecasting and deviation scoring to output anomaly scores. Ad-ditionally, LARA [11], is a light-weight approach based on deep variational auto-encoders.The novel ruminate block and retraining process makes LARA exceptionally suitable for online applications like web services monitoring.\nThe aforementioned approaches have their strengths and weak-nesses, with every model excelling in specific types of datasets while also exhibiting limitations. For instance, the ML techniques have been foundational, but they often require extensive feature engi-neering and struggle with complex datasets [9]. For DL approaches, reconstruction or forecasting-based models rely on reconstruction error to identify anomalies, they are more sensitive to large am-plitude anomalies and may fail to detect subtle pattern differences or anomalies with small amplitude [28]. In contrast, our proposed method can effectively capture anomalies with slight fluctuations by"}, {"title": "2.2 Time Series Anomaly Analysis.", "content": "Through a review of existing literature, we found that there is a lack of analysis on anomalies in current research. Common methods for analyzing anomalies identified by models involve visualizing the learned anomaly scores or parameters in relation to the ground truth [14, 28], as well as taxonomy of the anomalies [4, 13, 17]. Yet, limited research has investigated the efficacy of proposed models in classifying different types of anomalies. For instance, [29] intro-duced a framework based on Hidden Markov Models for anomaly detection, supplemented by an additional supervised classifier to identify potential anomaly types. GIN [52] employs a two-stage algorithm that first detects anomalies using an informer-based framework enhanced with graph attention embedding, followed by classification of the detected anomalies through prototypical networks. Both aforementioned models rely on supervised training for their anomaly classification processes; consequently, the cor-responding experiments conducted in these studies are limited to single classification datasets. In contrast, leveraging the capabilities of LLMs allows for not only the identification of anomalous data points but also the provision of specific classifications and poten-tial underlying causes for these anomalies, articulated in natural language and achieved in an unsupervised manner."}, {"title": "2.3 LLMs for time series.", "content": "Being pre-trained on enormous amounts of data, LLMs hold gen-eral knowledge that can be applied to numerous downstream tasks [10, 37, 39]. Many researchers attempted to leverage the powerful generalization capabilities of LLMs to address challenges in time series tasks [16, 24, 32, 48]. For instance, Gruver et al. [19] devel-oped a time series pre-processing scheme designed to align more effectively with the tokenizer used by LLMs. This approach can be illustrated as follows:\n0.123, 1.23, 12.3, 123.0 \u2192 12,123,1230,12300\".\nAdditionally, LSTPrompt [34] customizes prompts specifically for short-term and long-term forecasting tasks. Meanwhile, Time-LLM [24] reprograms input time series data using text prototypes and introduces the Prompt-as-Prefix (PaP) technique to further enhance the integration of textual and numerical information. Similarly, SIGLLM [1] is an LLM-based framework for anomaly detection with a moudle to convert time series data into language modality. Most of these efforts focus primarily on forecasting tasks and are largely confined to textual modalities.\nExisting works remain constrained by the limited availability of sequential samples in the training datasets of LLMs [36] and the models' inherent insensitivity to numerical data [43, 62]. Conse-quently, LLMs struggle to capture subtle changes in time series, making it difficult to produce reliable results [36]. Consequently, while we recognize that natural language is a modality in which LLMs excel, it may not be the most effective format for processing time series data."}, {"title": "2.4 Time series as images", "content": "The concept of transforming time series data into images has gained significant attention in recent years. One prominent method [8] highlights the effectiveness of this approach, demonstrating its abil-ity to improve recognition rates by utilizing hierarchical feature representations from raw data. Another innovative work [54] in-troduced Gramian Angular Fields (GAF) and Markov Transition Fields (MTF) as methods to encode time series data into images, and this technique is further explored in [55]. [31] presents a novel perspective by converting irregularly sampled time series into line graph images, and utilizing pre-trained vision transformers for classification. Additionally, TimesNet [57] is a time series anal-ysis foundation model that exploit CV advancing techniques by converting time-series to 2D tensors.\nWith the emergence of LMMs [67], there is potential for en-hanced reasoning capabilities that can accommodate a broader range of tasks beyond single-modal textual inputs [53, 69]. Some research has indicated that these models possess analytical abilities for interpreting charts [68]; however, no studies have yet applied them to the domain of anomaly detection in time series data. This gap highlights the need for further exploration into how LMMs can be effectively utilized to detect and analyze anomalies based on visualized time series data."}, {"title": "3 Methodology", "content": "Previous TSAD methods often rely on manual feature extraction or require large amounts of high-quality training data, along with extensive parameter tuning tailored to specific tasks. This reliance can lead to suboptimal performance and inefficiency. To overcome these limitations, we leverage the perceptual and reasoning capabil-ities of pretrained LMMs. Our approach transforms time series data into visual representations, enabling few-shot, high-performance, and robust anomaly detection. In this section, we first introduce the necessary preliminaries, followed by a detailed presentation of the TAMA framework."}, {"title": "3.1 Preliminary", "content": "Time series anomaly detection and classification. Consider a time series data $x = (x_1, ..., x_T) \\in \\mathbb{R}^T$, where $x_i$ represents the sampled value at timestamp t. In this paper, we focus on univariate time series, while multivariate data will be converted into multiple univariate series. The goal of time series anomaly detection is to identify anomalous points or intervals within the time series x. Specifically, an anomaly detection model outputs a sequence of anomaly scores $s = (s_1, ..., s_T)$, where $s_t$ indicates the anomaly score corresponding to the data point $x_t$. A higher anomaly score suggests that the model perceives a greater likelihood of the point being anomalous, while a lower score indicates a higher probability of the point being normal. By setting a threshold, the set of anoma-lous points can be obtained from the score sequence s. Anomaly classification is a multi-class classification task designed to catego-rize identified anomalous points into specific types of anomalies. These anomaly types can provide insights into the characteristics and possibly the underlying causes of the detected anomalies.\nPreprocessing. We follow common practice by applying mean-variance normalization to preprocess the time series, resulting in"}, {"title": "3.2 Time Series Anomaly Analyzer (TAMA)", "content": "The proposed TAMA framework is illustrated in Figure 2. TAMA comprises three sections that involve the participation of the LMM: Multimodal Reference Learning, Multimodal Analyzing, and Multi-scaled Self-reflection. Within these sections, we utilize specific prompts to guide the LMM in executing designated op-erations. A post-processing module is employed to integrate the model's outputs and obtain the final results. All prompts used in the experiments are detailed in Appendix A.1. Below, we provide a detailed explanation of TAMA's workflow.\nMultimodal Reference Learning. This section leverages the few-shot in-context learning (ICL) capabilities of pretrained LMMs to capture the patterns of normal sequences. The model is provided with a set of normal images $I = {I_i | i \\in {1, ..., n_r}}$, where $n_r$ denotes the number of reference images. These reference images are accompanied by prompts indicating that they represent nor-mal sequences without anomalies. The model is then tasked with generating descriptive outputs for these normal images. These de-scriptions will be used in subsequent interactions with the model,"}, {"title": "helping it to better focus on normal patterns and improving its ability to detect anomalous sequences.", "content": "Multimodal Analyzing. This section utilizes the normal data patterns learned by the LMMs during reference learning to identify anomalies in new samples. Specifically, the LMMs is driven to accomplish two tasks: anomaly detection and classification.\nFor the kth sliding window, the anomaly interval detection task requires the model to output a set of anomaly intervals $A_k = {{(t_s, t_e)^i}}_{i=1}^{m_k}$, where $(t_s, t_e)^i$ represents the ith anomaly interval, and $m_k$ is the number of detected anomaly intervals within the sliding window. We require $t_s \\leq t_e$ ($t_s = t_e$ indicates a point anomaly). Note that the intervals output by the LMMs are the indices within the sliding window, which will be converted to global indices during post-processing.\nBased on the results of anomaly detection, the model is then tasked with classifying each detected interval. Following [27], anomalies are categorized into four types: point, shapelet, seasonal, and trend. We inform the LMMs about the characteristics of each type through natural language descriptions (see Prompt 3 in Appendix). The LMMs will output an anomaly set $Y_k = {y_i}_{i=1}^{m_k}$, where $y_i$ corresponds to the anomaly classification result for the interval $(t_s, t_e)^i$. Moreover, we also require the LMMs to provide a confidence score and an explanation for each detected anomaly interval, denoted as $C_k = {c_i}_{i=1}^{m_k}$ and $T_k = {E_i}_{i=1}^{m_k}$, respectively. Thus, traversing all sliding windows, the total output from the model is given by\n$Z_{raw} = \\{{(A_k, Y_k, C_k, T_k)}_{k=1}^{N}}$\nwhere N is the total number of sliding windows.\nMulti-scaled Self-reflection. This section motivates the LMMS to correct some of its own errors, thereby enhancing the robustness and accuracy of anomaly detection. We provide the LMMs with the outputs from the previous sections, along with zoomed-in images of the detected anomaly regions. The zooming in on the anomaly areas helps prevent the model from repeatedly generating the same"}, {"title": "3.3 Multimodal Reference Learning", "content": "We provide a set of images for LMMs, helping LMMs to better focus on normal pattern and improving the ability to detect anomalous intervals (Section 3.2). To study whether the LMM truly learns from the reference images, we conduct this experiment on the UCR and NASA-SMAP datasets, replacing the normal data with abnormal data, and Multi-scaled Self-reflection is not enabled in this experiment. The results are presented in Table 7. In the table, normal refers to using the normal data as the reference data, while abnormal indicates using abnormal data as reference data. The experimental results under different reference data conditions are evaluated by the AUC-PR without point-adjustment. We can find that normal performs better than abnormal on both UCR and NASA-SMAP datasets, showing that the content for reference learning can notably impact the model's performance, which suggests the LMM can truly learn normal representation from the reference data."}, {"title": "5 Discussion", "content": "In this section. we reflect on the design of TAMA and seek to answer the following research questions:\nRQ1: Is the image modality better than text modality for LMMs in the time series anomaly detection task? (Section 5.1)\nRQ2: How does the additional information on images affect the LMM's analysis? (Section 5.2)\nRQ3: Dose the LMM truly learn the reference data during the multimodal Rederence Learning. (Section 5.3)"}, {"title": "5.1 Modality", "content": "To investigate whether the success of our framework comes from the more advanced model (GPT-40) we choose or the image-modality. we conduct an experiment in NASA-MSL and NASA-SMAP datasets, which both are real-world datasets and have more complex patterns than the UCR dataset.\nThe results are shown in Table 5. We choose different modalities for testing while trying to keep the prompts and procedures as the same. To maintain fairness, we do not add the self-reflection in TAMA. Meanwhile, we also include the results of SIGLLM [1], which uses Mistral-7B, as a reference. Compared to methods using text-modality, TAMA (Image), which use image-modality, has made significant improvement, with a 37.9% increase on NASA-MSL and a 36.9% increase on NASA-SMAP. This result indicates that for anomaly detection tasks, allowing the model to \"see\" time series data (using image modality) is more beneficial. It also demonstrates that multimodal reasoning has tremendous promise in time series anomaly detection tasks."}, {"title": "5.2 Additional Information in Images", "content": "The transformation of raw data into visual formats, such as images, adds crucial information, including plot orientation and auxiliary lines. This study investigates how these elements influence TAMA's"}, {"title": "performance in identifying abnormal intervals based on plot scales.", "content": "We conducted two experiments: the first involved rotating images by 90 degrees before inputting them into TAMA, while the second examined the impact of auxiliary lines, which are perpendicular to the x-axis and align with the scale to aid in locating data points. Both experiments are performed on the UCR and NASA-SMAP datasets. Results are presented in Table 6, where TAMA represents the original model, TAMA-R indicates performance with rotated images, and TAMA-A reflects performance without auxiliary lines. We evaluated using the AUC-PR without point adjustment. The findings demonstrate a notable decline in TAMA-R's performance with rotated images, suggesting that the LMMs are sensitive to im-age orientation. Despite the rotation of axis is disturbed in prompts, the LMM struggles to interpret rotated images accurately, leading to reduced anomaly detection. In contrast, TAMA-A experiences only a slight performance decrease across both datasets, indicating that LMMs can better identify abnormal intervals when auxiliary lines are present.\nThese experiments reveal that LMMs perceive time series images similarly to humans- uxiliary lines enhance anomaly localization accuracy, while image rotation negatively affects performance. This sensitivity may result from the tokenizer's responsiveness to orien-tation or insufficient training data and guidance."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to evaluate TAMA.\nThe experiments include Anomaly Detection and Anomaly Clas-sification, and Ablation Study."}, {"title": "Experimental Settings.", "content": "We select GPT-40 [40] TAMA's default model, and the specific version we used is \"gpt-40-2024-05-13\". To ensure the stability of TAMA and the reproducibility of the results, we set the temperature to 0.1 and set the top_p to 0.3. Besides, we use the JSON mode of GPT-40 to facilitate subsequent result analysis. All the prompts can be found in Appendix A.1. The detailed settings of image conversion has also been provided and discussed in Appendix A.3.\nDatasets. As shown in Table 2, we use a diverse set of real-world datasets across multiple domains for both anomaly detection and anomaly classification tasks. These domains include Web service: SMD [49], industry: UCR [58], NormA [5], NASA-SMAP [21], and NASA-MSL [21], health care: ECG [42], and transportation: Dodgers [22]. All datasets are univariate except for SMD, which is originally multivariate. We convert SMD into a univariate dataset by splitting it channel-wise for our anomaly detection experiment. The full experimental results are available in Appendix A.4.\nDue to the limited availability of datasets with anomaly clas-sification labels, we created an anomaly classification dataset by combining four real-world datasets (UCR, NASA-SMAP, NASA-MSL, and NormA) with manually labeled anomaly types, along with a synthetic dataset generated using GutenTAG [56]. To ensure the accuracy of the anomaly type annotations, cross-validation on"}, {"title": "4.1 Anomaly Detection", "content": "In this section, we evaluate the anomaly detetcion capability of TAMA.\nBaselines. The baseline models used in our experiments include both machine learning algorithms (IF [33], LOF [20]) and deep learning (TranAD [50], GDN [15], MAD_GAN [30], MSCRED [66], MTAD_GAT [70], OminiAnomaly [49], USAD [2] and TimesNet [57]). Besides, the SIGLLM [1] is a baseline model based on the LLM. We reproduce it with GPT-40. All baseline models has been run with the default configurations. For those datasets without default configurations, we managed to optimize the performance by searching the best parameters.\nMetrics. Following the mainstream of TSAD, we evaluated TAMA and other baselines by the point-adjusted F1 [59]. Point adjustment (PA) is a widely used adjustment method in TSAD tasks [4, 26], but it can significantly overestimate the models' per-formances (especially F1), making it a subject of much debate. A detailed explanation and further discussion of are available in Ap-pendix A.2.\nTo fully assess how the models' performance at different levels of sensitivity and specificity, we also adopted two threshold-agnostic metrics, namely AUC-PR and AUC-ROC. We would like to highlight AUC-PR as it is more robust to scenarios with highly imbalanced classes (like TSAD) comparing to AUC-ROC [47].\nMain Results. The experiment results are shown in Table 1. Each metric in the table contains two value: mean and maxima. The mean refers to the average of all sub-series, while the maxima represents the best result among all sub-series. For the maxima value, our method (TAMA) generates comparable results as other baseline models, even better in some datasets. TAMA outperforms almost all baseline models across every dataset in terms of mean metrics, especially on industry and transportation datasets. The exceptional performance of the mean suggests that our approach"}, {"title": "4.2 Anomaly Classification", "content": "In practical applications, it is preferable not only to detect anomaly intervals but also to provide a brief classification indicating their causes. To fully demonstrate the strong reasoning capabilities of the proposed framework and enhance the interpretability of detection results, we conduct classification on the anomaly data.\nThe overall results presented in Table 3 indicate that TAMA, guided by the provided prompts (outlined in Appendix A.1), demon-strates a reliable understanding of each type of anomaly and can accurately classify most anomalies, with the exception of seasonal anomalies. TAMA performs exceptionally well in classifying shapelet anomalies, suggesting that it effectively captures the shape of the in-put sequences. However, it is evident that the framework struggles with seasonal anomalies. We interpret this difficulty as stemming from a lack of relevant materials in the LMM's pre-training stage, which results in a weak understanding of concepts such as \"season-ality\" or \"frequency\". More discussion on TAMA's behavior to each anomaly type is included in Appendix A.6."}, {"title": "4.3 Ablation Study", "content": "There are many hyperparameters affecting the performance of our framework. In this section, some ablation experiments are conducted to evaluate the impact of each hyperparameter, including LMM Selection, Reference Number and Window Size.\nLMM Selection. In this section, we valid that the design of TAMA enhances the capability of LMMs in anomaly detection task. We conduct experiments on the UCR dataset with various LMMs, including GPT-40, GPT-40-mini, Gemini-1.5-pro, Gemini-1.5-flash, and Qwen-vl-max. For each LMM, we conduct experiments both with (+TAMA) and without (Naive) TAMA framework. Due to budget constraints, we only conduct this experiment on the UCR dataset. However, we believe the experimental results can, to some extent, reflect real-world scenarios.\nThe experimental results are presented in the Table 4. To better evaluate the efficacy of our framework (TAMA), we use the original AUC-PR without point-adjustment as the metric in this experiment. The findings reveal that all LMMs exhibit a substantial enhancement in performance on the UCR dataset following their integration into TAMA. This not only validates that TAMA improves the LMMs' abilities in anomaly detection but also confirms the generalizability of TAMA's framework.\nReference Number. As mentioned earlier in Section 3.2, we provide some normal images $I = {I_i|i \\in [1,N]}$ as references to help the LMM learn the distribution of normal data. In this ablation experiment, we investigate the impact of the number of reference images N. The ablation experiment is conducted on UCR and NASA-SMAP datasets. The ablation experiment is conducted on the UCR and NASA-SMAP datasets. To more clearly highlight the differences in performance, we used metrics without PA. Moreover, to avoid"}, {"title": "5 Conclusion", "content": "In this paper, we introduced TAMA, a novel framework that lever-ages large multimodal models for effective time series anomaly analysis. Comprehensive evaluation across multiple metrics demon-strates that TAMA not only surpasses state-of-the-art methods but also provides valuable semantic classifications and insights into detected anomalies. By converting time series data into visual rep-resentations, we have, for the first time, applied large multimodal models to this domain, enabling stronger generalization and more robust interpretative analysis. In summary, TAMA represents a significant advancement in anomaly detection methodologies, with practical implications for real-world applications and new opportu-nities for future research in multidimensional anomaly detection.\nHowever, some limitations should be noted. Firstly, the proposed approach primarily relies on the pre-trained LMMs without fine-tuning. Additionally, in this work, we only consider univariate time-series anomaly detection tasks, whereas in real-world scenarios, it's necessary to incorporate multiple time series for comprehensive judgment. In future works, we plan to explore deploying large models locally and fine-tuning them to achieve better performance and enhanced data security. Furthermore, we are considering the incorporation of multidimensional time series anomaly detection in our subsequent research efforts."}, {"title": "A Appendices", "content": ""}, {"title": "A.1 Prompts", "content": "The design of prompts is based on the documentation of OpenAI2. Writing the steps out explicitly can make it easier for the model to follow them. In our task, we separate the whole task into three specific tasks: Multimodal Reference Learning (see Prompt 1), multimodal Analyzing (see Prompt 3) and Multi-scaled Self-reflection (see Prompt 2). Besides, we also provide some back-ground information, such as siliding windows and additional infor-mation of images. With the JSON mode output of GPT-40, it is very convenient for us to process the output results, requiring a detailed description of the output format in prompts. Based on our practical experience, we find that clear descriptions and a structured format significantly are very helpful for LMM to understand."}, {"title": "A.2 Point Adjustment Metrics", "content": "Given a set of real anomalous intervals $A_T = {{(t_s, t_e)^r}}_{i=1}^{m_T}$ and a set of predicted anomalous intervals $A_p = {{(t_s, t_e)^p}}_{i=1}^{m_p}$, the point-adjusted prediction $A_{PA}$ is defined as:\n$A_{PA} = A_p \\cup {t|t \\in (t_s, t_e)^r, |(t_s, t_e)^r \\cap (t_s, t_e)^p|>0}$, (1)\nwhere the $A_{PA}$ is a set of points, $m_T$ and $m_p$ refer to the total number of real anomalous intervals and the total number of the LLM's prediction, respectively. After point-adjustment, the point-adjusted Recall, Precision and F1 can be calculated as:\n$R = Recall (A_T, A_{PA})$, (2)\n$P = Precision (A_T, A_{PA})$, (3)\n$F1 = \\frac{2 (R+P)}{(RP)}$, (4)\nThe point-adjustment with threshold a is defined as:\n$A_{PA}(\\alpha) = A_p \\cup {t|t \\in (t_s, t_e)^r, \\[(t_s, t_e)^r \\cap (t_s, t_e)^p > \\alpha \\cdot L((t_s, t_e)^r)]}$, (5)\nwhere a refers to the point-adjustment threshold (PAT) from 0 to 1, where 0 represents full point-adjustment $A_{PA}$ and 1 represents original prediction $A_p$, and the $L((t_s, t_e))$ refers to the length of $(t_s, t_e)$."}, {"title": "A.3 Some Suggestions about TAMA", "content": "In this paper, we propose a framework named TAMA to utilize the LMM to analyze time series images. However, we have tried multiple versions and gained valuable practical experience during the development process. Based on our practical experience, we provide some suggestions.\n\u2022 To better parse the output results, choosing the LMM which supports JSON mode output or structured output can be very convenient. If the LMM does not support these output format, we can use GPT-40, which supports structured output, to format the output text.\n\u2022 Assume the period of series data is T, it is recommended to set the sliding window length to at least 3T."}, {"title": "A.6 Performance Evaluation of Type-Specific Anomaly Detection Methods", "content": "Table 9 presents the type-specific anomaly detection performances. To maintain readability, only the F1-score without point adjustment is reported. The results highlight TAMA's outstanding performance in identifying pattern anomalies, including shapelet, seasonal, and trend types, while most baseline models struggle in this aspect with-out point adjustment. For instance, on the UCR-shapelet dataset, TAMA outperformed the second-best detector (GDN) by a substan-tial margin of 293% in terms of the mean F1-score. This superiority stems from TAMA's inherent ability to detect anomalous intervals. However, this characteristic may lead to lower F1-scores in the detection of point anomalies. In the synthetic dataset we generated, labels for point anomalies were strictly defined. While TAMA's in-terval detection always encompassed the ground-truth anomalies, it also produced a significant number of false positives."}, {"title": "A.7 Visualization of anomaly classification", "content": "In Section 4.2, we make a new dataset for anomaly classification by labeling some real-world datasets and generating sequence. We also provide some visualization of these anomalies to better under-stand the different types of anomalies. The visualization of anomaly classification is shown in Figure 7. The dataset contain four classifi-cation: Point, Shapelet, Seasonal and Trend, which are referenced from the work [27]."}, {"title": "4.4 Full Results of Anomaly Detection across All Datasets", "content": "In this section, we present the full results of all datasets in Table 8. Due to the limitation of the space, we only present some of them in the main body. Meanwhile, we also present the variance in this table. Most of datasets contain more than one sub-sequence, to fully present and compare the performance, we evaluate all metrics in all sub-sequence and calculate three values: mean, variance and maxima. In this table, mean and variance are formated as \"mean \u00b1 variance\"."}, {"title": "4.5 Full Results of the PAT Experiment", "content": "In Section 4.1, in order to study the impact of point-adjustment, we re-evaluate the results using the point-adjustment with a thresh-old a (See Appendix A.2). Due to the limitation of the space, we only present results of some datasets in the main body. The full results are presented in Figure 6. As the figure presented, our frame-work achieves outstanding AUC-PR across all datasets at various a, showing that our framwork has better robustness and stability."}]}