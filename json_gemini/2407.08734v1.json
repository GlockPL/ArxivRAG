{"title": "Transformer Circuit Faithfulness Metrics Are Not Robust", "authors": ["Joseph Miller", "Bilal Chughtai", "William Saunders"], "abstract": "Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been\nto discover 'circuits' \u2013 subgraphs of the full model that explain behaviour\non specific tasks. But how do we measure the performance of such circuits?\nPrior work has attempted to measure circuit 'faithfulness' \u2013 the degree\nto which the circuit replicates the performance of the full model. In this\nwork, we survey many considerations for designing experiments that mea-\nsure circuit faithfulness by ablating portions of the model's computation.\nConcerningly, we find existing methods are highly sensitive to seemingly\ninsignificant changes in the ablation methodology. We conclude that ex-\nisting circuit faithfulness scores reflect both the methodological choices of\nresearchers as well as the actual components of the circuit - the task a circuit\nis required to perform depends on the ablation used to test it. The ultimate\ngoal of mechanistic interpretability work is to understand neural networks,\nso we emphasize the need for more clarity in the precise claims being made\nabout circuits. We open source a library at this https URL that includes\nhighly efficient implementations of a wide range of ablation methodologies\nand circuit discovery algorithms.", "sections": [{"title": "1 Introduction", "content": "Mechanistic interpretability (MI) is a form of post-hoc interpretability that attempts to\nreverse engineer neural networks to provide faithful low-level explanations of model\nbehaviour (Olah et al., 2020). One focus of interpretability work on transformer language\nmodels is identifying 'circuits' \u2013 subgraphs of the entire model's computational graph that\nare primarily responsible for the model's output on some task (Wang et al., 2023); where\na task is specific type of problem that a language model has to solve to output correct\nnext-token predictions (ie. sentences that require a specific algorithm to complete correctly).\nA key metric used by mechanistic interpretability (MI) researchers to quantify the quality of\na 'circuit' for some task is it's faithfulness \u2013 that is, the degree to which the circuit captures\nthe performance of the entire model (Zhang & Nanda, 2024). In this work, we study\nvarious small and reasonable seeming variations on methodologies for measuring circuit\nfaithfulness and find that such variations often lead to significantly different faithfulness\nscores. Faithfulness is typically measured by performing a targeted, circuit-dependent\nablation to the model, and observing the effect of this on some metric of the model's output.\nIn the context of MI, an ablation refers to a type of intervention made on the activations\nof a model during its forward pass with the intended purpose of \u2018deleting' some causal\npathway(s), thereby isolating the causal effect of the circuit.\nIn this work, we seek to answer the questions: What do circuit faithfulness metrics actually\nshow? To what extent are they a useful test of the circuit and to what extent are they a\nreflection of the experimental methodology?\nWe begin by reviewing the ways in which MI researchers may vary their ablation methodol-\nogy (Section 3), providing a detailed review of methods for ablating transformer circuits.\nNext, we test these variations on existing circuits discovered by MI researchers (Section 4).\nWe provide detailed case studies of the \u2018Indirect Object Identification' circuit by Wang et al."}, {"title": "2 Related Work", "content": "Circuit Analysis. Circuit analysis is a form of post-hoc interpretability focused on un-derstanding the full end-to-end learned algorithm responsible for some specified narrowbehaviour. A circuit is a subgraph of the full computational graph of the model that (isalleged to) implement some precise behavior. Circuits have been studied in vision models(Cammarata et al., 2021; Olah et al., 2020) and in toy transformer models (Nanda et al., 2023a;Chughtai et al., 2023). More recently, the circuit analysis paradigm has achieved success ininterpreting transformer language models too, with a number of papers discovering circuitsimplementing human understandable algorithms through ablation studies (Wang et al.,2023; Heimersheim & Janiak, 2023; Hanna et al., 2023). To accelerate such studies, recentwork has attempted to automate the process of discovering circuits (Conmy et al., 2023;Syed & Rager, 2023; Kramar et al., 2024), particularly in large language models, as circuitshave historically required a large amount of researcher-effort to uncover. Prior work hassuggested that ideal circuits exist on the Pareto frontier of faithfulness, completeness andsimplicity (description length), as the entire network is trivially optimal for the first twocriteria (Sharkey, 2024).\nActivation Patching. Zhang & Nanda (2024) recommend best practices in ActivationPatching (a form of ablation, defined in Section 3.1) for measuring circuit faithfulness in asimilar work to ours. They compare single layer vs. multi-layer ablation, Resample Ablationvs. Noise Ablation and logit difference vs probability metrics when Node Patching. Westudy a larger set of variations in ablation methodology in this work, enumerating severalmore choices in methodology and arguing that different optimal circuits are defined in partby different ablation methodologies, rather than prescribing a single correct approach toablation.\nFaithful explanations in NLP. We are interested in explaining model behavior in a waythat reflects the underlying reasoning process of the model, a criteria often referred toas faithfulness. In this work we measure faithfulness by studying the fidelity of ablatedmodels - the similarity of the ablated output to the outputs of the full model (Alishahi et al.,2019; Guidotti et al., 2018; Agarwal et al., 2024). As argued by Jacovi & Goldberg (2020),faithfulness should be viewed as a continuum. Any interpretation is an approximation thatwill necessarily fail to capture some aspects of the underlying behavior.\nMechanistic interpretability (MI) attempts to reverse engineer trained machine learningmodels to produce faithful human understandable explanations of model predictions viaanalysis of the low level features and algorithms implemented by the network. Circuitanalysis is just one important direction in this theme of work. Besides circuit analysis, MImore broadly seeks to understand the correct frame to interpret neural network computation(Elhage et al., 2021; Bricken et al., 2023; Cunningham et al., 2023) and to understand thelearned features of models (Li et al., 2023; Tigges et al., 2023; Gurnee & Tegmark, 2024; Bills\net al., 2023). MI has also inspired work in steering model outputs through representationengineering (Turner et al., 2023; Li et al., 2024; Rimsky et al., 2024)."}, {"title": "3 Measuring Faithfulness", "content": "We follow previous works (Wang et al., 2023; Heimersheim & Janiak, 2023; Hanna et al.,\n2023) in defining faithfulness of circuits as the extent to which they encapsulate the full"}, {"title": "3.1 Ablation Methodology", "content": "In the context of MI, an ablation refers to a type of intervention made on the activations\nof a model during its forward pass with the intended purpose of 'deleting' precise causal\npathways. In the language of casual inference, we denote the ablation of all activations\noutside a circuit C on a model M as:\n\nF(x) = M(x | do(a = \u00e3)), a \u2209 C\n\nWhere x is the input to the model, a is an internal activation of the model and \u00e3 is the ablated\nvalue of a. The ablation methodology determines the types of activations and values that a\nand a can be (eg. whether a is a neuron node activation or an edge between attention heads).\nIntuitively, deleting important subcomponents for some task should damage task per-\nformance, and conversely deleting unimportant sub-components should preserve task\nperformance. As such, ablations have arisen as a commonly used tool for localizing model\nbehaviour to specific internal model components. Ablations may be used both to find and\nevaluate mechanistic explanations of model behavior.\nThe concept of ablation overlaps with a related technique, activation patching, in which\nactivations are modified during a model's forward pass to some cached values from a\ndifferent input. \u2018Corrupted' inputs are inputs which are similar to the 'clean' distribution\nbeing studied, but which have crucial differences that drastically change the output. For\nexample, a typical 'corrupt' prompt could retain the structure of a 'clean' prompt, while\nswitching a proper noun, such that the correct next token prediction is changed. In this\nwork we consider activation patching to be a specific type of ablation, and use the term\nResample Ablation interchangeably. But we note that in general, 'patching' means editing\nactivations to some other value, instead of \u2018deleting' them, as ablation typically connotes."}, {"title": "3.1.1 Circuit Granularity", "content": "In this work we study circuits specified at the level of attention heads and MLPs\u00b9. We also\nseparate the input of each attention head into the Q, K and V inputs, but we omit this from\nour diagrams for visual simplicity. This is the most common granularity for mechanistic\ncircuit analysis (Conmy et al., 2023; Wang et al., 2023; Heimersheim & Janiak, 2023; Hanna\net al., 2023; Nanda et al., 2023b)), but previous works have also studied circuits specified\nat the level of layers (Meng et al., 2022), neurons (Vig et al., 2020), subspaces (Geiger et al.,\n2023) and sparse \"features\" (Marks et al., 2024)."}, {"title": "3.1.2 Ablation Component Type (and Associated Model Views)", "content": "Transformers can be described as computational graphs in several different, equivalent\nways. We can choose to write the graph as a residual network (Figure 7a) or a \u2018factorized'\nnetwork in which all nodes are connected via an edge to all prior nodes (Figure 7b) (Elhage\net al., 2021). Or we can write down a 'treeified' network that separates all paths from input\nto output (Figure 8a). All formulations are equivalent but the \u2018factorized' view allows us to\nisolate interactions between individual components and the 'treeified' view allows us to\nisolate chains of interactions from input to output.\nThe component type defines the type of intervention made: we detail three possibilities,\nwith increasing granularity. The more granular approaches are generally more difficult to\nimplement and more computationally expensive.\n(1) Nodes. We may intervene on a node (in the standard, residual view) during the forward\npass, replacing its activation with some other value (Figure 1a). This is the least specific\nform of ablation. Since all downstream nodes 'see' the change there are a large number of\ncausal pathways affected by the ablation, which may result in unintended side-effects. This"}, {"title": "3.1.3 Ablation Value", "content": "When performing a causal intervention on some activation, we may choose what value we\npatch in. The simplest choice is to Zero Ablate, by replacing the activation with a vector of\nzeros (Olsson et al., 2022; Cammarata et al., 2021). Prior work has noted however that the\nzero point is arbitrary (Wang et al., 2023). The next simplest is to apply Gaussian Noise\n(GN) to the token embeddings of the clean input to obtain corrupted activations (Meng\net al., 2022). Both of these approaches can take the model significantly out of distribution\n(Zhang & Nanda, 2024), producing noisy outputs (Wang et al., 2023).\nTwo more principled approaches are Resample Ablation (take an activation from some\nother corrupted input) (Vig et al., 2020; Meng et al., 2022), and Mean Ablation (replace with\nthe mean activation of a node from some distribution) (Wang et al., 2023). These two ablation\ntypes have the desirable property of keeping the model closer to its usual distribution of\nactivations. Importantly, they do not delete all information present in a component. Instead,\nthey delete information that varies across the distribution, while preserving information\nthat is constant across it, allowing us to isolate precise language tasks, while ignoring, say,\ngeneric grammar processing. When Mean Ablating, we have an additional choice in the size\nof the mean ablation dataset (see Section 4.1). We focus on Mean and Resample Ablations in\nthis work."}, {"title": "3.1.4 Token Positions", "content": "Circuits in autoregressive transformers on a narrow distribution are sometimes defined in\nterms of components and token positions. When these token positions are specified, we can\nchoose to either ablate all token positions, or only the token positions not in the specified\nset (Wang et al., 2023). We can modify equation (1) to\n\nF(x) = M(x | do(a\u1d62 = \u00e3\u1d62)), a\u1d62 \u2209 C\n\nwhere a\u1d62 is the activation a at token position i."}, {"title": "3.1.5 Ablation Direction and Testing Circuits", "content": "Ablation typically refers to instances where we run the model on a clean input and change\nactivations to destroy the input signal (Wang et al., 2023; Conmy et al., 2023; Hanna et al.,\n2023; Nanda et al., 2023b). However, we can also run the model on a corrupt input and\nResample Ablate (or Patch) in activations from the clean input (Meng et al., 2022; Heimer-\nsheim & Janiak, 2023). Separately, when evaluating circuits, we can choose to either ablate\nall the components of the circuit or we can ablate all the components not in the circuit (the\ncomplement).\nThe combination of these choices determines the target of our faithfulness metric:"}, {"title": "3.2 Metric", "content": "One further consideration in addition to the ablation methodology is the metric used to\nevaluate the effect of the ablation. We also argue that the choice of metric is important.\nThere are many choices used in the literature, including KL Divergence (Conmy et al., 2023),\ntop-k accuracy Heimersheim & Janiak (2023) and task-specific benchmarks (Hanna et al.,\n2023). In this work we will focus on the metrics used by the respective authors of the circuits\nthat we study, but note these choices are also in general free."}, {"title": "4 Faithfulness Metrics are Sensitive to Ablation Methodology", "content": "In this section, we empirically demonstrate that evaluations of a given circuit's faithfulness\nare highly sensitive to the experimental choices outlined in Section 3 made at evaluation\ntime. We further argue that this sensitivity is important, and may result in practitioners\nfinding fundamentally different algorithms.\nWe provide a case study here on the Indirect Object Identification (IOI) circuit identified\nby Wang et al. (2023), as this is the most studied language model circuit in the literature\n(Conmy et al., 2023; Makelov et al., 2023; Zhang & Nanda, 2024), but find similar results"}, {"title": "4.1 Variance Between Ablation Methodologies", "content": "We now show circuit faithfulness is sensitive to these choices. First we compare the faithful-ness metric when we change the ablation component from nodes to edges - we ablate thecomplement of the set of edges specified by the circuit instead of the complement of the setof nodes in the circuit. As shown in Figure 3, ablating at the edge level returns substantiallyhigher percentages.\nFigure 3 also evaluates the effect of ablation value. We rerun the above experiment usingResample Ablations from the ABC distribution, and find that this results in a systematicallylower faithfulness as compared with mean ablations (statically significant on a t-test withp = 1e - 5 for Node Ablation but not Edge Ablation). Finally, we study the effect of ablatingat every token position, instead of only those specified by the circuit. This consistentlyresults in lower faithfulness scores. It is concerning that the edge-level circuit with specifictoken positions has a median score well over 100%, as this best represents the hypothesis ofWang et al. (2023).\nNext, we discuss sensitivity of the faithfulness metric to both the clean distribution andintricacies of the metric calculation. For these experiments, we perform node-level MeanAblations on the complement of the circuit, split by token position, similarly to Wang et al."}, {"title": "4.2 Variance Between Individual Datapoints", "content": "Even for a fixed ablation methodology and metric, there is significant variation in themeasured faithfulness between individual prompts in the distribution.\nWe show this for the IOI circuit in the figures above, with results for other circuits inAppendix D. The graphs on the right of Figure 4 show a large range of faithfulness scoresattained when we ablate the complement of the nodes in the IOI circuit. Note that thegraphs do not show the full range of datapoints and there are several extreme outliers"}, {"title": "5 Optimal Circuits Are Defined By Prompts and Ablation Methodologies", "content": "We showed in the previous section that measurement details can greatly change the faith-fulness score of an experiment. However, one might ask if this difference matters. In thissection we discuss the consequences of such sensitivity for circuit discovery.\nIf a circuit is specified as a set of edges, it should be tested using edge ablations and if it isspecified with token positions then it should be tested with token-specific ablation. But inother aspects there often isn't a clearly correct methodology. So how should we think aboutthe difference in faithfulness between different methodologies? We study this question insmall toy models, where we have access to the 'ground truth' circuit. We conclude that theoptimal circuit for some distribution cannot be defined unless we also specify the ablationmethodology and metric that we are using to measure it.\nTracr models (Lindner et al., 2023) are tiny transformers that are compiled instead of trained.Since the ground truth algorithm is both simple and known, they provide an excellent setupfor testing circuit discovery algorithms. RASP programs (Rush & Weiss, 2023) are compiledinto the weights of a transformer that implements the program exactly. Following Conmyet al. (2023), we study two Tracr models, Reverse and X-Proportion.\nThe X-Proportion model performs the task of outputting at each token position the propor-tion of previous characters that are 'x's. The model has two layers, with one head in eachattention layer. The first attention layer and the second MLP are not used, so we need onlyconsider the edges between the Input, MLP 0, Attn 1.0 and Output.\nConmy et al. consider the edge from Input to Attn 1.0 to be part of the ground truth circuit(Figure 11). Inspecting the RASP program, we see that the only information in this edge's"}, {"title": "6 Conclusion", "content": "In this work we show existing transformer circuit evaluations are highly sensitive to small\nchanges in the ablation methodology and the metrics used to quantify faithfulness. We\nfurther show that the optimality of a circuit cannot be defined with respect to a set of\nprompts without a precise evaluation methodology\nIf a circuit is specified as a set of edges, it should be tested using edge ablations. And if it\nis specified at a chosen set of token positions it should be tested with these. But in other\naspects there often isn't a clearly correct methodology. Do you want your IOI circuit to\ninclude the mechanism that decides it needs to output a name? Then use zero ablations.\nOr do you want to find the circuit that, given the context of outputting a name, completes\nthe IOI task? Then use mean ablations. The task cannot be separated from the ablation\nmethodology.\nOur work has significant consequences for circuit discovery work, particularly automated\ncircuit discovery algorithms that aim to optimize these faithfulness scores. It suggests that\nassessing the quality of automated methods by measuring the overlap with some 'ground\ntruth' can be misleading, if the ground truth was discovered using a different ablation\nmethodology.\nWe recommend that researchers precisely describe their experimental procedure when\nreporting evaluations of circuits. They should consider which task exactly they are expecting\ntheir circuit to perform."}, {"title": "A AutoCircuit Library", "content": "We release AutoCircuit, a Python library with a highly efficient implementation of Edge\nPatching and various circuit discovery algorithms, with support for TransformerLens mod-els (Nanda & Bloom, 2022). It supports Mean, Zero and Resample Ablations. See our blogpost for more detail on our fast implementation.\nWe test the performance of our implementation by running the ACDC (Conmy et al., 2023)circuit discovery algorithm, which iteratively patches every edge in the model. We comparethe performance of AutoCircuit's implementation to the official ACDC implementation(which is currently the most popular library for patching large numbers of activations). Werun ACDC using both libraries at a range of thresholds for a tiny 2-layer model with only0.5 million parameters and measure the time taken to execute on a single GPU."}, {"title": "B Further Details on Ablation Methodology", "content": ""}, {"title": "C Summary of Tasks Studied", "content": ""}, {"title": "D Further Study of Faithfulness Metrics", "content": "In this section, we provide further analysis demonstrating faithfulness metrics are brittle,\non two other circuits from the existing literature."}, {"title": "D.1 Docstring", "content": "The Docstring Task. The Docstring task (Heimersheim & Janiak, 2023) is a simple task that\ntests a 4 layer, attention-only model's ability to complete a specific part of a standard Python\ndocstring (see Table 4 for an example). All prompts follow a very similar format, with the\nonly difference being the names of the variables in the function. The corrupt distribution\nfollows the exact same format, using a disjoint set of variable names.\nMeasuring Docstring Circuit Faithfulness. Heimersheim & Janiak (2023) test their circuit\nusing a similar methodology to the one which Wang et al. (2023) used to test the IOI circuit.\nThey ablate all nodes in the complement of their circuit. However, unlike Wang et al. (2023)\nthey use a Resample Ablation (also known in this context as Activation Patching), and they\ndo not distinguish different token positions. The metric that they use for faithfulness is the\npercent of highest logit outputs that are the correct answer over some set of prompts."}, {"title": "D.2 Sports Players", "content": "The Sports Players Task. The Sports Players task (Nanda et al., 2023b) is a simple task that\ntests the Pythia-2.8b model's (Biderman et al., 2023) ability to recall the sports of famous\nfootball, baseball and basketball players. See Table 4 for an example. All prompts follow a\nvery similar format, with the only difference being the name of the sports player in question.\nThe corrupt distribution follows the exact same format, with each clean/corrupt pair having\ntwo players of different sports.\nMeasuring Sports Players Circuit Faithfulness. In Figure 10, we test the faithfulness ofthe edge-level sports players circuit, distinguishing token positions while (1) ablating thecomplement with both Resample and Mean Ablations and (2) calculating two differentfaithfulness metrics: correct answer percentage (considering only the three possible sports,following Nanda et al. (2023b)) and answer probability.\nWe find a dramatic difference in correct answer percentage between Resample and MeanAblation. This case is a little different because the authors' aim wasn't to find the full circuitbut to identify the place in the model where factual recall occurs, so this result doesn'tnegate their hypothesis.\nNote that random guessing would achieve 33% accuracy as there are 3 possible sports, andthis is roughly what we see when Mean Ablating the whole model. But Resample Ablatingadds signal from the corrupt prompt, which is always a different sport, explaining the 0%accuracy score for the Ablated Model and the Circuit."}, {"title": "D.3 Further Detail on the X-Proportion Tracr Ground Truth Circuits", "content": "Figure 11: For the Tracr X-Proportion circuit, the edge from Input to Attn 1.0 is only used\nto transfer the positional encoding, so it is not required when using Resample Ablations,\nsince these preserve information that is constant between the clean and corrupt distribution.\nThis illustrates the principle that optimal circuits cannot be defined without an ablationmethodology. (Nodes Attn 0.0 and MLP 1 are not shown as they are not used in this model.)"}, {"title": "E Edge-Based vs. Node-Based Circuit Discovery Methods", "content": "In Section 5, we adapted the Subnetwork Probing (SP) and Head Importance Scoring (HISP)circuit discovery methods to use (or approximate) Edge Ablation. ACDC (Conmy et al.,2023) already uses Edge Ablations, but we can similarly adapt ACDC to use Node Ablations.We compare the performance of the Node Patching versions of ACDC, SP and HISP to theEdge Patching versions, for the Resample Ablation based \u201cground truth\" circuit introducedin Section 5 (Figure E)."}, {"title": "F Clarifying Nomenclature", "content": "Some authors have used different terms for some of the concepts introduced in Section 3. For\ninstance, Activation patching has previously also been called Causal Tracing or Interchange\nIntervention. In the remainder of this section, we summarise how our nomenclature\nrelates to the terminology used by Redwood Research in their series of early mechanistic\ninterpretability transformer-circuits papers. Chronologically, these are Wang et al. (2023);\nChan et al. (2022); Goldowsky-Dill et al. (2023).\nWe first discuss the final, most comprehensive work (Chan et al., 2022), which we refer toas Causal Scrubbing. Causal Scrubbing is a very general approach for evaluating circuitstogether with explanations of the role of nodes within the circuit. It generically comprisesperforming specific branch-based Resample Ablations on the treeified model on both thecircuit and its complement. Causal Scrubbing randomly replaces activations with thosethat your hypothesis predicts will not change the model output. For instance, if we claimthat a given node detects whether the input is even, Causal Scrubbing could patch in anactivation from a different even input, and expects the output not to change. In general,Causal Scrubbing permits an arbitrary number of possible counterfactual inputs.\nGoldowsky-Dill et al. (2023) simplify this setup, dropping the strict requirement of requiringan explanation for each node. This reduces the hypothesis class to the now standard circuitdiscovery problem; does some path matter for task performance or not?\nFinally Wang et al. (2023) perform a further simplified version of path patching to discoverthenode"}]}