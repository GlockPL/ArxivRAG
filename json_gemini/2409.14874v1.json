{"title": "Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images", "authors": ["Ahjol Senbi", "Tianyu Huang", "Fei Lyu", "Qing Li", "Yuhui Tao", "Wei Shao", "Qiang Chen", "Chengyan Wang", "Shuo Wang", "Tao Zhou", "Yizhe Zhang"], "abstract": "We are interested in building a ground-truth-free evaluation model to assess the quality of segmentations produced by SAM (Segment Anything Model) and its variants in medical images. This model estimates segmentation quality scores by comparing input images with their corresponding segmentation maps. Building on prior research, we frame this as a regression problem within a supervised learning framework, using Dice scores (and optionally other metrics) to compute the training loss. The model is trained using a large collection of public datasets of medical images with segmentation predictions from SAM and its variants. We name this model Evany Seg (Evaluation of Any Segmentation in Medical Images). Our exploration of convolution-based models (e.g., ResNet) and transformer-based models (e.g., ViT) revealed that ViT offers superior performance for EvanySeg. This model can be employed for various tasks, including: (1) identifying poorly segmented samples by detecting low-percentile segmentation quality scores; (2) benchmarking segmentation models without ground truth by averaging quality scores across test samples; (3) alerting human experts during human-AI collaboration by applying a threshold within the score space; and (4) selecting the best segmentation prediction for each test sample at test time when multiple segmentation models are available, by choosing the prediction with the highest score. Models and code will be made available at https://github.com/ahjolsenbics/EvanySeg.", "sections": [{"title": "1 Introduction", "content": "Pioneering work by Robinson et al. [ROB+18] demonstrated the feasibility of using segmentation maps alongside raw images to predict Dice coefficient scores for the segmentation quality of the left-ventricular cavity, left-ventricular myocardium, and right-ventricular cavity in cardiovascular magnetic resonance scans. Li et al. [LYH22] then developed a reference-based method to evaluate both image-level and pixel-level segmentation quality. More recently, Wundram et al. [WFM+24] introduced the use of conformal range prediction to estimate the upper and lower bounds of the Dice score in ground-truth-free segmentation evaluation. Chen et al. [CZY24] advanced the field further by employing vision-language models for segmentation quality assessment, incorporating text to specify the segmentation class under investigation (e.g., \u201cSpleen\"). These developments underscore that segmentation quality assessment is increasingly recognized as a crucial step toward the reliable deployment of medical image segmentation models and the facilitation of effective human-AI collaboration in medical image analysis.\nIn parallel, the Segment Anything Model (SAM) [KMR+23], a class-agnostic segmentation model that utilizes points and/or bounding boxes as prompts for segmentation, has gained significant attention since its introduction. Several efforts have been made to adapt, fine-tune, and apply SAM in medical imaging. MedSAM [MHL+24] and SAM-Med2D [CYD+23] both adopt the bounding box and/or point prompt scheme for generating segmentations in medical imaging contexts. Owing to their flexibility and utility, SAM and SAM variants have been widely employed in the field of medical image analysis. Given this context, it is both desirable and essential to have an external system capable of automatically and reliably assessing the quality of the segmentations generated by SAM models. Motivated by this, we propose the development of a segmentation quality evaluator, termed EvanySeg, designed to work alongside SAM models in medical image segmentation. Specifically, when SAM (e.g., the original SAM and/or SAM variants) generates a segmentation prediction, EvanySeg processes this output along with the input image and produces a score that accurately reflects the true segmentation quality. The key highlights of this work can be summarized in the following four main points.\n\u2022 We present a novel approach to enhance the effectiveness and reliability of segment-anything models (SAM) in medical imaging by developing a companion model, named EvanySeg. This model is specifically designed to evaluate the quality of segmentations produced by SAM and its variants, ensuring more accurate and dependable segmentation outcomes.\n\u2022 Our theoretical analysis suggests that developing a segmentation evaluation model is indeed feasible. In many instances, this task is as manageable as, or even simpler than, creating the segmentation model itself. We deliberately designed the model and training procedure to be simple and straightforward, striving for ease of implementation and use in practice.\n\u2022 We trained the segmentation evaluation model using a large collection of public available medical imaging data and tested its performance across various practical applications utilizing both public and private datasets. The results demonstrate the model's effectiveness and utility in estimating segmentation quality for medical images without the need for ground truth masks.\n\u2022 Furthermore, we will make the code and trained models publicly available to foster and advance research for reliable medical imaging AI system."}, {"title": "2 Related Work", "content": "Huang et al. [HWM16] introduced a learning-based approach for estimating segmentation quality using deep learning (DL) networks. They developed three network configurations, focusing primarily on how segmentation masks are integrated with features or images during the estimation of the segmentation quality (SQ) score. Zhou et al. [ZDW20] proposed a sequential network architecture for segmentation quality assessment (SQA). Devries et al. [DT18] employed uncertainty maps to assist in the estimation of quality scores by combining raw images, segmentation maps, and uncertainty maps, which were then input into a DL-based network for SQA. Rottmann et al. [RCH+20] suggested using the dispersion of softmax probabilities to predict the true segmentation intersection over union (IoU). Rahman et al. [RSCD22] introduced an encoder-decoder architecture designed to detect segmentation failures at the pixel level. They extracted multi-scale features from the segmentation model and fed them into the decoder to generate a map that highlights misclassified or mis-segmented pixels. Valindria et al. [VLB+17] proposed Reverse Classification Accuracy (RCA) for SQA, which requires a reference database containing image and ground-truth map pairs. Finally, Robinson et al. [ROB+18] developed convolutional neural networks (CNNs) to directly predict the Dice score for an image and segmentation map pair, utilizing a large dataset of images with ground-truth labels to train the CNN for this prediction task. Wang et al. [WTQ+20] examined the latent distribution of image-segmentation pairs and proposed a generative method to identify surrogate ground truths. More Recently, Wundram et al. [WFM+24] proposed to utilize conformal range prediction to predict the upper and lower bounds of the Dice score in ground-truth free segmentation evaluation. Chen et al. [CZY24] developed a visual-language model for estimating segmentation and label quality in medical segmentation datasets. This model was trained on over 4 million image-label pairs and can predict segmentation quality for 142 body structures in CT (Computed Tomography) images, showing good correlation with ground truth quality measures."}, {"title": "2.2 Segment Anything Models for Medical Images", "content": "Ma et al. [MHL+24] compiled 84 publicly available datasets comprising over one million images from 10 imaging modalities to train a Segment Anything Model specifically for medical images. Similarly, Cheng et al. [CYD+23] developed two versions of their medical SAMs, with one model tailored for 2D images and another for 3D images. More recently, Zhao et al. [ZZW+23] introduced a Segment Anything Model using text prompts (SAT model). This work involved the curation of an extensive dataset that includes over 11,000 3D medical image scans sourced from 31 segmentation"}, {"title": "3 Materials and Methods", "content": "Given an image sample $x \\in \\mathbb{R}^{w\\times h\\times C}$ and a prompt (e.g., a bounding box), a Segment Anything Model (SAM) generates a segmentation prediction map $\\hat{y} \\in {0,1}^{w\\times h}$, where $w$ represents the width and $h$ the height of the image, and $C$ represents the number of channels of the image. To evaluate the segmentation $\\hat{y}$, the conventional approach is to obtain ground truth annotations $y$ from human experts and compare $\\hat{y}$ with $y$ using a predefined evaluation metric, such as the Dice score. Let $\\pi$ denote the evaluation metric; then, the sample-wise quality evaluation result is given by $q = \\pi(\\hat{y}, y)$. In practice, it is highly desirable to predict segmentation quality without relying on ground truth annotations. Following the approach of [ROB+18], we aim to build a regression model, denoted as $\\mathcal{T}$, that directly predicts $q$ based on the segmentation prediction $\\hat{y}$ from SAM and the raw input image $x$. The workflow of EvanySeg, a companion model designed to work alongside SAM, is illustrated in Figure 1."}, {"title": "3.2 Theoretical Analysis", "content": "\"It may not know what a 100% accurate segmentation is, but it can still tell, to some extent, whether a segmentation is good or bad.\"\nTheorem 3.1 Given an image, estimating the segmentation quality (e.g., Dice score) of a segmentation map (Problem A) for this image is no harder than generating a perfectly accurate segmentation map (Problem B) for this image.\nReducing Problem A to Problem B: To solve Problem A (i.e., predicting the Dice score for a given segmentation map $\\hat{y}$), one can first solve Problem B by generating the true segmentation map $y$. Once $y$ is available, calculating the Dice score between $\\hat{y}$ and $y$ becomes a straightforward task. This approach involves solving only one instance of Problem B, followed by a simple Dice score computation.\nReducing Problem B to Problem A: To solve Problem B (i.e., generating a perfectly accurate segmentation map), one could theoretically generate all possible segmentation maps for a given image $x$. Since the number of potential segmentation maps grows exponentially, there are a vast number of possibilities. For each candidate segmentation map, one could then solve Problem A (i.e., predicting the Dice score) to assess how close the generated map $\\hat{y}$ is to the true segmentation map $y$. The optimal segmentation map $\\hat{y}$ would be the one that maximizes the predicted Dice score. Thus, solving Problem B in this manner would require solving an exponential number of instances of Problem A. See Theorem 3.2 for a more efficient way to reduce problem B to problem A, resulting in significantly fewer though still many instances of problem A that need to be solved, rather than an exponentially large number.\nIn summary: Since solving Problem A requires only one instance of Problem B followed by minimal computational effort, Problem B is at least as hard as Problem A (Problem A is no harder than Problem B). Furthermore, because solving Problem B can require solving many instances of Problem A, Problem B can be significantly harder in general. As a result, predicting the Dice score (Problem A) is generally easier than creating an accurate segmentation map (Problem B). Though, in the most challenging cases of Problem A, solving those difficult instances can be equivalent to solving Problem B.\nTheorem 3.2 There exists a set of instances of Problem A, referred to as the core set, for which the complexity of solving this set is equivalent to that of solving Problem B. The size of the set is linearly proportional to the number of pixels in the image.\nWe construct this set as follows. For simplicity of this analysis, we assume that the segmentation task is binary, with the ground truth $y$ represented as a binary map, where 1 indicates the object of interest and 0 indicates the background or other objects. For each pixel in the segmentation map, we can create a pair of segmentation maps that differ only at that pixel. We then invoke the solver for Problem A on these two segmentation maps and compare the output values to determine whether the corresponding pixel should be 1 or 0 in the correct segmentation (the ground truth). By repeating this process for every pixel in the segmentation map, we can generate the ground truth segmentation (solving Problem B). The total number of instances of Problem A being solved in this process is precisely twice the total number of pixels in the image. This set of instances constitutes the core set of Problem A, where solving it leads to the resolution of Problem B and subsequently enables the resolution of the other instances of Problem A outside the core set (exponentially many).\nDefinition 3.1 Given an image $x$, and any segmentation map $\\hat{y}$ of the image, a segmentation quality evaluation model $\\tau$ is considered \"absolutely accurate\" if the following condition holds:\n$\\tau(\\hat{y}, x) - \\pi(\\hat{y}, y) = 0$\nwhere $\\pi(\\hat{y}, y)$ is computed by a pre-defined metric $\\pi$ that measures the quality of the segmentation map $\\hat{y}$ with respect to the ground truth $y$.\nDefinition 3.2 Given any two images $x_1$ and $x_2$ (where $x_1$ can be the same as $x_2$), and any two segmentation maps $\\hat{y}_1$ for $x_1$ and $\\hat{y}_2$ for $x_2$, a segmentation quality evaluation model $\\tau$ is considered \u201crelative accurate\u201d if the following condition holds (assuming $\\pi(\\hat{y}_1, y_1) \\neq \\pi(\\hat{y}_2, y_2)$):\n$(\\tau(\\hat{y}_1, x_1) - \\tau(\\hat{y}_2, x_2)) \\times (\\pi(\\hat{y}_1, y_1) - \\pi(\\hat{y}_2, y_2)) > 0$\nwhere $\\pi(\\hat{y}_1, y_1)$ and $\\pi(\\hat{y}_2, y_2)$ are computed by a pre-defined metric $\\pi$ that measures the quality of the segmentation maps $\\hat{y}_1$ and $\\hat{y}_2$ with respect to their ground truths $y_1$ and $y_2$.\nProposition 3.3 Achieving a segmentation quality evaluation model $\\tau$ that is relative accurate (Definition 3.2) is generally easier than achieving a model that is absolutely accurate (Definition 3.1).\nThe above is straightforward to show, since a model that is absolutely accurate is also relatively accurate, but not vice versa.\nDefinition 3.3 Given any two images $x_1$ and $x_2$ (where $x_1$ can be the same as $x_2$), and any two segmentation maps $\\hat{y}_1$ for $x_1$ and $\\hat{y}_2$ for $x_2$, a segmentation quality evaluation model $\\tau$ is considered $\\beta$-relative accurate if, whenever $|\\pi(\\hat{y}_1, y_1) - \\pi(\\hat{y}_2, y_2)| \\geq \\beta$, the following condition holds (assuming $\\pi(\\hat{y}_1, y_1) \\neq \\pi(\\hat{y}_2, y_2)$):\n$(\\tau(\\hat{y}_1, x_1) - \\tau(\\hat{y}_2, x_2)) \\times (\\pi(\\hat{y}_1, y_1) - \\pi(\\hat{y}_2, y_2)) > 0$\nwhere $\\pi(\\hat{y}_1, y_1)$ and $\\pi(\\hat{y}_2, y_2)$ are metrics that measure the quality of the segmentation maps $\\hat{y}_1$ and $\\hat{y}_2$ with respect to their ground truths $y_1$ and $y_2$.\nProposition 3.4 Given $\\beta_1 < \\beta_2$, achieving a segmentation quality evaluation model $\\tau$ that is $\\beta_1$-relative accurate (according to Definition 3.3) is more challenging than achieving a model that is $\\beta_2$-relative accurate."}, {"title": "3.3 Training Data", "content": "Preparing the data for training EvanySeg is a critical step in this work. To build the evaluation model, which serves as a companion to the Segment Anything Model (SAM), we integrate SAM into the process of preparing the training data for EvanySeg. Several variants of SAM are utilized: the original release from Meta [KMR+23], the later-released MedSAM [MHL+24], and SAM-Med2D [CYD+23]. We employ these three SAM variants in the preparation of training data.\nFor each image $x_i$ where $i = 1,2,..., n$, and for every prompt $p_{i,j}$, the ground truth mask is denoted as $Y_{i,j} \\in {0,1}^{w\\times h}$, where $j = 1, 2, ..., k_i$. Here, $w$ is the width and $h$ is the height of the image, and $k_i$ represents the number of objects in image $x_i$. Each object is specified with a dedicated prompt. Prompts can take the form of bounding boxes or center points. For an image $x_i$ with a prompt $p_{i,j}$, we apply a SAM variant to generate a segmentation output denoted as $\\hat{y}_{i,j}$. We then compute a segmentation quality score $q_{i,j} = \\pi(\\hat{y}_{i,j}, Y_{i,j})$, where $\\pi$ may be as simple as the Dice Similarity Coefficient (DSC). This process is repeated for every image and every prompt. The training data for EvanySeg is thus prepared as $(X_i, P_{i,j}, Y_{i,j}, \\hat{y}_{i,j}, q_{i,j})$, where $i = 1,2,..., n$, and $j = 1,2,..., k_i$. This procedure is repeated for each SAM variant. We utilize a subset of the data curated in SA-Med2D-20M [YCC+23] for constructing the training data for EvanySeg. More details on data preparation can be found in our code release."}, {"title": "3.4 Model Architecture", "content": "EvanySeg consists of two components: the pre-processing model and the regression model. The pre-processing model aims to combine the segmentation map with the input image, and it uses the prompt to crop the regions of interest, constructing the inputs for the regression model. The regression model then takes the output from the pre-processing model and generates a score that reflects the segmentation quality. Detailed designs for the two models are provided below."}, {"title": "3.5 Model Training", "content": "With the prepared training data, defined pre-processing ($\\psi$), and regression models ($\\tau$), we now proceed to describe the training process. We employ a standard multi-epoch mini-batch stochastic gradient descent (SGD) training pipeline to train the EvanySeg. Note that $\\psi$ contains no trainable parameters, and the training process focuses solely on updating the parameters of $\\tau$. For simplicity, we assume the regression model has only one output head. Let $\\theta$ denote the parameters of $\\tau$; model training aims to minimize the following objective with respect to $\\theta$.\n$\\min_{\\theta} \\sum_{m=1}^{M} \\sum_{i=1}^{n} \\sum_{j=1}^{k_i} loss(\\tau(\\psi(x_i, Y_{i,j}, p_{i,j})), \\pi(\\hat{y}_{i,j}, Y_{i,j})).   ~~~~~~~~~~(1)$\nHere, $m$ denote the index for SAM variants, $i$ denote the index for images, and $j$ denote the index for objects within each image. For a given image $x_i$, $k_i$ represents the number of objects present in that image. We utilize three variants of SAM, namely, the original SAM [KMR+23], MedSAM [MHL+24], and SAM-Med2D [CYD+23], thus setting $M$ to 3. By default, we use the mean squared error (MSE) loss as the default loss function. After training, given a test image, with prompt and the segmentation map generated by SAM or a SAM variant, the segmentation quality score is generated by $\\hat{q} = \\mathcal{T}_{trained}(\\psi(x_{test}, \\hat{y}_{test}, p_{test}))$"}, {"title": "3.6 Utilities", "content": "In this section, we describe several key applications of EvanySeg in medical image segmentation.\nIdentifying Poorly Segmented Samples. When deploying a segmentation model on a set of image samples, each accompanied by one or more prompts, EvanySeg assists in identifying low-quality segmentation maps. These flagged outputs can then be reviewed and analyzed by human experts.\nModel Comparison without Ground Truth. When evaluating multiple segmentation models on a set of image samples with corresponding prompts, segmentations can be generated for each sample-prompt pair using all the models under consideration. EvanySeg can then assign a score to each segmentation, enabling a comparison of the models based on their average scores across all samples and prompts."}, {"title": "4 Experiments and Results", "content": "The EvanySeg model was trained based on 107,055 2D images, accompanied by 206,596 object-level ground truth masks. Segmentation predictions for training the EvanySeg model were generated using SAM, MedSAM, and SAM-Med2D. In total, 619,044 object-level image-mask pairs were utilized, with each image and corresponding mask resized to 244 \u00d7 244 pixels for training the segmentation evaluation model. The AdamW optimizer was employed, with a mini-batch size of 128 and a learning rate of 0.0001. The primary training workload was performed using a single Nvidia Tesla A800 graphics card."}, {"title": "4.1 Testing Datasets", "content": "We utilized four public datasets (three ultrasound image datasets: TG3K [WGP+17], TN3K [GCC+23], and DDTI [PVN+15], and one CT image dataset: FLARE21 [Mea22]), along with four private in-house datasets (one MRI image dataset, one CT image dataset, one endoscopic image dataset, one pathology image dataset) to assess the performance of the trained EvanySeg model. The TG3K dataset, derived from 16 ultrasound videos, consists of high-quality 2D ultrasound images in which the thyroid gland occupies more than 6% of the total image area. This dataset is specifically designed for thyroid nodule segmentation and contains 3,585 images. The TN3K dataset includes 3,493 ultrasound images with detailed pixel-level annotations, split into 2,879 images for training and 614 images for testing. Meanwhile, the DDTI dataset comprises 637 ultrasound images. FLARE21 is a diverse CT dataset focused on the segmentation of four abdominal organs: the liver, spleen, pancreas, and kidneys. The dataset presents significant variability in organ shape and size, posing substantial challenges for segmentation models. FLARE21 is provided in a 3D format, from which we extracted slices to generate 2D images for model testing, yielding a total of 8,943 images.\nThe 1st in-house dataset, named MRI-Kidney100, contains MRI scans from 100 individuals, designed for kidney segmentation. This dataset includes 100 3D MRI scans, from which 2D slices along the x-y plane were extracted for model testing. Similarly, the 2nd in-house dataset, named CT-BCT100, comprises CT scans from the same 100 individuals, focused on brachiocephalic trunk (BCT) segmentation. This collection includes 100 3D CT scans, with 2D slices from the x-y plane used for model testing. The 3rd in-house dataset, named Endo-Polyp1000, is a colonoscopy polyp detection dataset consisting of 1000 images, each paired with an expert-annotated mask. This dataset features a diverse range of polyp images. The 4th in-house dataset, named Path-Neuro1406, is a pancreatic neuroendocrine-related segmentation dataset with 1,406 images. In this dataset, we cropped the regions of interest from the large pathological sections stained with hematoxylin and eosin (H&E) according to doctor-annotated masks. The regions of interest vary significantly in shape and size, making segmentation particularly challenging. Image samples of these four in-house datasets can be found in Figures 3, 4, 5 and 6."}, {"title": "4.2 Correlation between the predicted Dice scores and the true Dice scores", "content": "In this section, we evaluate how well the predicted Dice scores by EvanySeg correlate with the true Dice scores obtained using ground truth masks. We use Pearson correlation and Spearman's rank correlation [HK11] for evaluation. Higher correlation scores indicate that EvanySeg is more accurate in predicting segmentation quality, particularly in terms of region overlaps as captured by the Dice score. We utilize all four datasets mentioned earlier for testing. Figures 7, 8, 9, 10, 11, 12, and 13 present scatter plots for TG3K, TN3K, DDTI, FLARE21, MRI-Kidney100, \u0421\u0422-\u0412\u0421\u0422100 and Endo-Polyp1000 datasets, where each point corresponds to a test image sample. Each figure corresponds to one dataset, with the first row displaying results for EvanySeg using ViT-b as its backbone, the second row for ViT-l, and the third row for ResNet101. The x-axis represents the true DSC (computed by comparing the segmentation prediction with the ground truth mask), and the y-axis represents the predicted DSC given by EvanySeg (without using ground truth masks). Several observations emerge from the results presented in the figures. First, ViT as a backbone, in most cases, provides better segmentation quality assessment performance compared to the ResNet counterpart. Second, segmentation predictions produced by MedSAM and SAM-Med2D exhibit significant variation in quality, while SAM generally producing better segmentation pre-dictions than MedSAM and SAM-Med2D. Third, due to these differences in segmentation quality, predicting segmentation quality, or distinguishing between better and worse segmentation outputs, is more challenging for SAM predictions than for MedSAM and SAM-Med2D cases, as it is easier to identify poor-quality segmentations in the latter. This explains why the first columns (SAM predictions) in these three figures show worse segmentation quality assessment performance than the second and third columns, as the task is generally more difficult in the SAM scenario (see Proposition 3.4). Overall, it is evident that EvanySeg predicts DSC with a strong correlation to the true DSC. Figure 14 presents visual examples of test images, their corresponding ground truth segmentations, and segmentation predictions from SAM and its variants. Segmentation quality scores, generated by EvanySeg, are provided for visual inspection of how well the scores reflect actual segmentation quality."}, {"title": "4.3 Evaluation of Segmentation from Non-SAM Models", "content": "We employ a task-specific segmentation model that is not from the SAM family and is not part of the training process of EvanySeg to generate segmentation predictions for testing EvanySeg. In"}, {"title": "4.5 Using EvanySeg for Sample-wise Model Selection", "content": "We then evaluate whether EvanySeg can accurately determine which segmentation model performs best for a given test image when multiple segmentation models are available. The experiments are conducted using the TG3K and FLARE21 datasets, employing the predicted scores and true Dice scores from Section 4.2. Table 2 demonstrates that EvanySeg correctly identifies the best model (among SAM, MedSAM, and SAM-Med2D) approximately 90% of the time on the TG3K dataset, and about 77% of the time on the FLARE21 dataset. In Table 3, we further demonstrate that using EvanySeg as a sample-wise model selector improves segmentation performance (measured by Dice score) compared to scenarios where only a single model is applied without selection. We also present results for the case where the Oracle selector is deployed, representing the upper-bound performance limit (for the selection part)."}, {"title": "5 Discussion", "content": "Data-driven and deep learning approaches have revolutionized the field of medical image segmentation over the past decade. From U-Net [RFB15] to nnU-Net [IJK+21], and now to Transformer-based models [XLL+23], segmentation techniques have continually advanced through the integration of larger and better datasets, improved model architectures, additional prior knowledge, and even multi-modal modeling, including text-based information. Despite these advancements, challenges remain in ensuring the reliability and trustworthiness of the most advanced segmentation models when deployed in clinical practice. Accurately and robustly evaluating segmentation quality without relying on ground truth data is crucial for the reliable deployment of medical AI models. When an evaluator detects low-quality segmentation output, the system can alert human experts to examine the corresponding sample and subsequent results. In this paper, we successfully developed such a segmentation evaluation model as a companion to the Segment Anything Model for medical images. Moreover, we demonstrate that this evaluator can be employed to select the best model for each test sample during test time, thereby actively improving segmentation outcomes.\nOur approach to building a segmentation evaluation model is not particularly novel: The use of deep learning and regression objectives to estimate Dice scores has been previously studied (e.g., [ROB+18]). Similarly, the application of Vision Transformers (ViT) for visual learning in medical data has been extensively explored (e.g., [CLY+21]), and the objective of creating a universal segmentation evaluator has also been proposed in a very recent study ([CZY24]). Our focus is in developing a companion segmentation evaluation model specifically for the Segment Anything Model applied to medical data, which is a new advancement in this domain. We deliberately chose well-established options in model architecture, training methods, and data preparation to demonstrate that, even with a straightforward setup, it is possible to achieve a reasonably effective evaluator for ground-truth-free segmentation quality assessment. We believe that with improvements such as a larger model with a more advanced architecture, better dataset preparation (e.g., more high-quality data), and an enhanced training pipeline (e.g., with data augmentation), EvanySeg could achieve even better performance in estimating scores that more closely align with true segmentation quality measures.\nLimitations. First, despite the high correlation observed between the predicted DSC and true DSC in the experiments and results, a closer examination at a very local level would reveal that the order of the predicted Dice scores can still be noticeably incorrect relative to the true Dice scores (the problem is harder at this level, see Proposition 3.4). Secondly, EvanySeg, as a companion model to SAM, shares SAM's limitation of relying on prompts (e.g., points and/or bounding boxes) for generating segmentation. In some cases, the segmentation which should be generated from a bounding-box or point prompt especially with point prompts is not well-defined. Thirdly, EvanySeg currently works only with 2D medical images, and for the present version of the manuscript, it only supports bounding-box prompts. Point-based prompts will be addressed in the next update."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a segmentation evaluation model designed to complement SAM-type segmentation models, serving as a companion tool for assessing segmentation quality without the need for ground truth masks. Theoretical analysis confirms the viability of this approach, while the development process emphasizes simplicity and clarity, highlighting its potential. Experimental results demonstrate the model's effectiveness, with the generated scores aligning closely with true segmentation quality. We showcase several practical uses for these scores, such as identifying poor segmentations, selecting superior models, and optimizing segmentation outcomes at the sample level across multiple models. Through this paper, along with our code and trained models, we aim to promote the creation of more accurate, faster, and robust ground-truth-free or ground-truth-less-dependent segmentation evaluation tools, advancing research in trustworthy medical AI and enhancing human-AI collaboration in medical image analysis workflows."}, {"title": "4.4 Multiple Regression Outputs for Multiple Evaluation Metrics", "content": "We further experimented with EvanySeg, where the model is designed with two output heads- -one for predicting Dice scores and the other for predicting Hausdorff distances. In Figure 15, we present the scatter plots and computed correlation values for the two-head configuration. Compared to the single-head setup in Figure 7 and 10, we observe a slight decrease in correlation values (for Dice score prediction) when using the two-head regression, where the Hausdorff distance predictions share weights in the ViT architecture with the Dice score predictions. This observation highlights the need for further architectural improvement of EvanySeg to enable more effective and efficient multi-task learning."}]}