{"title": "Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability", "authors": ["Jiri Hron", "Laura Culp", "Gamaleldin Elsayed", "Rosanne Liu", "Ben Adlam", "Maxwell Bileschi", "Bernd Bohnet", "JD Co-Reyes", "Noah Fiedel", "C. Daniel Freeman", "Izzeddin Gur", "Kathleen Kenealy", "Jaehoon Lee", "Peter J. Liu", "Gaurav Mishra", "Igor Mordatch", "Azade Nova", "Roman Novak", "Aaron Parisi", "Jeffrey Pennington", "Alex Rizkowsky", "Isabelle Simpson", "Hanie Sedghi", "Jascha Sohl-dickstein", "Kevin Swersky", "Sharad Vikram", "Tris Warkentin", "Lechao Xiao", "Kelvin Xu", "Jasper Snoek", "Simon Kornblith"], "abstract": "While many capabilities of language models (LMs) improve with increased training budget, the influence of scale on hallucinations is not yet fully understood. Hallucinations come in many forms, and there is no universally accepted definition. We thus focus on studying only those hallucinations where a correct answer appears verbatim in the training set. To fully control the training data content, we construct a knowledge graph (KG)-based dataset, and use it to train a set of increasingly large LMs. We find that for a fixed dataset, larger and longer-trained LMs hallucinate less. However, hallucinating on \u2264 5% of the training data requires an order of magnitude larger model, and thus an order of magnitude more compute, than Hoffmann et al. (2022) reported was optimal. Given this costliness, we study how hallucination detectors depend on scale. While we see detector size improves performance on fixed LM's outputs, we find an inverse relationship between the scale of the LM and the detectability of its hallucinations.", "sections": [{"title": "Introduction", "content": "Despite rapid progress in generative and predictive capabilities, hallucinations remain a significant challenge for large language models (Gemini, 2023; OpenAI, 2023). Although researchers have carefully studied \u201cscaling laws\u201d (Kaplan et al., 2020; Hoffmann et al., 2022)\u2014an empirical phenomenon where LM performance improves as dataset and model size increase-little is known about how hallucinations depend on scale. To fill this gap, the first issue at hand is to precisely define and quantify hallucinations. However, in natural language setting this is very hard as language expressions can be ambitious, and the exact knowledge content in training data is notoriously unclear.\nKnowledge graph (KG), on the other hand, offers full controllablility of its factual content: it is straightforward to query whether a generated fact from an LM indeed exists in the dataset or not, hence offering quantifiable measure of hallucination. Training LMs on KG allows us to study the extent to which LMs misrepresent their training data, and how this phenomenon depends on scale."}, {"title": "Controlling What an LM Knows", "content": "A core challenge in studying LM hallucinations is that we typically do not know what information the model was exposed to during training. Without this knowledge, we cannot determine whether the model output is wrong because (a) it has never been trained on a given fact, (b) it was trained on it but did not memorize it, or (c) the model memorized factually incorrect information that did appear in the training set. To avoid these issues, which are further confounded by various pretraining and finetuning strategies used in state-of-the-art LMs, we train LMs from scratch (Section 2.2) on data specifically constructed to give us perfect control over the information a model sees (Section 2.1). This will later enable us to investigate how model and dataset scale affects hallucinations (Section 3), and their detectability (Section 4).\n2.1 The Knowledge Graph dataset\nWe propose using a Knowledge Graph (KG) as a way of controlling the information a model sees. KGs are structured, factual data, that are often used within organisations to feed knowledge into various applications. We use KG as it provides a repository of information which is self-consistent, and mirrors the structure of information in the real-world; the hope is that this mirrored structure will ensure that the character of any hallucinations we see is somewhat similar to hallucinations we would see from models trained on data more typical for LM training. The main benefit to using a KG, however, is that we know exactly what a model has seen, and since it is structured data, we can easily query the data to see if its predictions are correct.\nThe KG we use (Google, 2012) contains semantic triples: a Subject, Predicate, and Object. We further insert special tokens before each of the Subject, Predicate, and Object, as indicated in Figure 1, and use the concatenated strings to train our LMs. The processing removes the ambiguity of natural language, which makes the task both easier and harder for an LM. The task is easier because the samples are now structured: LMs no longer need to pick up the intricacies of grammar and distinguish different phrasings, and can instead just focus on learning facts. It is, however, also harder, because there is very little correlation between data samples, unless they share items, and thus very little positive transfer between learning one fact to the other.\nIn later sections (Sections 3 and 4), we will be training and evaluating LMs and hallucination detectors. We therefore need to carefully design data splits to fully understand the impact of data. We design datasets that reflect three levels of visibility: 1) a fully visible set (FVS) that both the LMs and detectors are trained on, 2) a partially visible set (PVS) that only the LMs are trained on, and finally 3) an invisible set (IVS) that neither the LM or the detector have seen. We then vary the sizes of FVS and PVS to study the effect of scale.\nTo construct these three sets of triplets, we perform an i.i.d. split at the subject level. Some subject-predicate pairs are associated with multiple objects (e.g., names of tracks on an album). In these cases, we need to ensure that all objects associated with a given subject-predicate belong to the same set, as otherwise we might label correctly deduced object predictions as hallucinations. A similar issue can exist at the subject level, e.g., age can be deduced from date of birth. Several subject-predicate pairs are associated with hundreds of objects. To simplify evaluations, we remove all subject-predicates linked with more than 20 objects. This eliminates extreme long-tails that would be hard to meaningfully analyse.\n2.2 Training LMs on the Knowledge Graph\nWe trained decoder-only transformer LMs (Vaswani et al., 2017) with varying number of non-embedding parameters (3.15M\u20131.61B), on various sizes of the KG dataset (1%-100%). The parameters are optimized with respect to the autoregressive cross-entropy loss over formatted strings created from triplets with special tokens (Section 2.1).\nWhere a single triplet does not fill the context window (256 tokens), we used packing (on average, ~20 triplets fit into the context window). For optimization, we used Adam (Kingma & Ba, 2014) with linear learning rate warmup from 0 to our base learning rate (4K steps), followed by cosine decay down to 5% of the base rate. We varied the total number of steps to study the effect of multi-epoch training (see Tables 2 and 4 for details). The base learning rate is set to a constant divided by the square root of the number of LM's non-embedding parameters. The constant was determined by a hyperparameter search over 2.5, 5, and 10 (due to compute limitations this exploration was not done for all models). The exact learning rates we used can be found in Table 3 in Appendix."}, {"title": "Hallucination Rate and How It Scales", "content": "Scaling laws are an empirical phenomenon where the cross-entropy loss of LMs decay as a power law of the model and training set size (Kaplan et al., 2020; Hoffmann et al., 2022). Since cross-entropy is related to the accuracy of model predictions, one can wonder whether hallucination rates follow a similar trend. Figure 2 shows this is not the case. While for a fixed dataset size, larger, longer-trained models tend to hallucinate less, increasing the dataset size yields a higher, rather than a lower, hallucination rate (top left vs. top right); a similar trend is observed for the cross-entropy loss\u2014evaluated on full triplets, not just the object-in Figure 3. This is because of two factors: (1) many triplets in the KG require memorization (i.e., correct answer cannot be inferred from other data points); (2) each triplet appears in the training set only once. The bottom two plots in Figure 2 demonstrate the necessity of memorization; no model attains less than 50% hallucination rate on data not seen at training time. Examples of facts requiring memorization are names of tracks on an album, or dates of birth.\nAnother consequence of the necessity to memorize and of the lack of repetition is that 20+ epochs are needed to achieve close to minimal hallucination rate for a given LM size. This stands in sharp contrast to the current practice of training LMs for only one or a few epochs (Chowdhery et al., 2023; Hoffmann et al., 2022; Touvron et al., 2023); we show for models trained for fewer (1 or 2) epochs in the beginning of each line in Figure 2.\nAn unfortunate side-effect of training for 20+ epochs seems to be a decreasing ability to generalise to unseen data (note the eventually upwards slope in the bottom plots). This trend is even more pronounced at temp = 0.0 (Figure 8), presenting a trade-off between hallucination rates and other model performance measures. While even the most capable modern LMs hallucinate (Gemini, 2023; OpenAI, 2023), repetition of facts in their training data likely alleviates the issue, as also suggested in Kandpal et al. (2023). Excessive repetition might however be harmful (Hernandez et al., 2022; Xue et al., 2023; Muennighoff et al., 2023), presenting a challenge for curation of training data.\nIt is striking that, in our case, even when training on 1% of the full KG, the hallucination rate on data seen during training remains ~5% for the largest and longest-trained LM. This hallucination rate can be pushed down to ~1.5% by lowering the temperature from 1.0 to 0.0. However, the KG contains a large number of subject-predicate pairs with multiple associated objects (e.g., names of all authors of a particular paper), and reducing sampling temperature leads the model to generate fewer of these objects. In Figure 4, we show how varying temperature affects both precision, defined as 1-hallucination rate, and recall, defined as the average proportion of objects in the original training data that the model generates at least once when sampling 16 completions for each subject-predicate pair. This highlights an issue with focusing on hallucination rate only: an easy way to hallucinate less is to make fewer claims. A related issue is that finetuning LMs to refuse to answer when uncertain may potentially lead to overly conservative behaviour.\nOur best-performing LMs are an order of magnitude larger than the predicted Chinchilla-optimal size for the number of tokens on which we train (Hoffmann et al., 2022), and therefore even larger than the even smaller models geared towards efficient inference (Touvron et al., 2023). This suggests that even if we just wanted LMs to rarely hallucinate on the data that they have seen during pretraining, we would need a computational budget several times higher than what is currently considered optimal. A more efficient alternative may be retrieval-augmentation (Lewis et al., 2020; Borgeaud et al., 2022), or methods for expressing uncertainty or self-correcting (e.g., Dhuliawala et al., 2023; OpenAI, 2023; Li et al., 2023). In Section 4, we study influence of scale on methods that belong to the latter category."}, {"title": "Hallucination Detectability and How It Scales", "content": "4.1 Setup\nIn Section 3, we have seen hallucination rates typically decay with LM size and training length. However, even LMs much larger than currently considered optimal for given training set size\u2014continue to hallucinate ~5% of the time on data seen and ~50% on data unseen during training (Figure 2). Our experiments also exhibit a trade-off between in-distribution and out-of-distribution hallucination rates (Figures 2 and 8). We therefore need to understand whether it is possible to further reduce hallucination rates by other means.\nThere are many types of alternative interventions, ranging from retrieval to model self-correction (see Appendix A). One promising direction is hallucination detectors which try to identify hallucinations either from the LM output itself, or from the LM's internal representations (e.g., Kadavath et al., 2022; Dhuliawala et al., 2023; OpenAI, 2023; Li et al., 2023). Our aim is to understand (i) how the effectiveness of hallucination detectors depends on the scale and training length of the LM they are judging, (ii) what types of detectors perform better, and (iii) if there is evidence other interventions beyond detectors are needed.\nWe explore two types of detection tasks:\n\u2022 sentence: The detector ingests both original subject-predicate and the predicted object tokens, and judges if the object is hallucinated.\n\u2022 token: The detector takes embedding of a token from a given layer of the trained LM, and is asked to say if it is hallucinated. For a given subject-predicate input, a predicted object token is labelled as hallucinated if it and the tokens preceding it do not match the token prefix of any entry in the KG. The goal is to predict the first hallucinated token; any tokens that come after are discarded from the detector's training and evaluation data.\nFor each of the tasks, we experiment with two types of hallucination detectors:\n\u2022 head: Adds a new readout head on top of the pretrained LM that produced the output (remaining LM weights are frozen). For the sentence task, this is equivalent to adding two new tokens to the dictionary (hallucination & non-hallucination), and finetuning the LM readout layer to predict whether the preceding triplet is hallucinated. The same holds for the token task, but the point of prediction changes; we also experiment with training detectors based on token representations from after each transformer block (Figure 10).\n\u2022 full: Takes the pretrained LM as a starting point, and finetunes all its weights. For this setup, we only consider the case where the readout head is attached to the top layer embeddings in the token task.\n4.2 Results\nFigure 5 shows how the pretrained LM size affects overall accuracy across tasks and approaches (Section 4.1). As expected, the full detectors outperform head, as they tend to be more flexible. Since this is also the case for other metrics, we focus on full detectors in the rest of this section. The token task formulation generally yielded better detector accuracy than the sentence task. Detector accuracy also tends to grow with the size of the underlying LM. However, these results are confounded by sensitivity of the accuracy metric to the underlying LM hallucination rate (see Figure 2). For example, if the rate is only 5%, even a naive detector which catches all hallucinations attains 95% accuracy.\nWe use AUC-PR to assess how well our detectors identify hallucinations. In Figure 6, we observe that: (a) sentence task formulation is superior in terms of AUC-PR, and (b) the lower the LM's hallucination rate, the worse the detector's AUC-PR. Per Figure 2, the lowest hallucination rates are achieved by the largest longest trained models. Thus, for a fixed training set size, there is an inverse relationship between the FLOPs spend on the LM pretraining and the detectability of its hallucinations.\nFigure 7 emphasizes the inverse relationship between LM scale and hallucination detectability, showing the PR curves corresponding to the sentence task AUC-PR values in Figure 6. Note the general ordering of the curves, with those corresponding to detectors for the largest LMs being the lowest, and the ones for the smallest LMs being the highest."}, {"title": "Limitations", "content": "Several factors may limit correspondence of our results to behaviour of state-of-the-art (SOTA) LMs. Firstly, the KG differs from the data normally used for LM training (e.g., no repetition of facts in the corpora, no semantic ambiguity, simple sentence structure). Secondly, the LMs we train are significantly smaller than SOTA LMs, for which we adjust by using a proportionally smaller dataset. While we believe the qualitative interpretation of our results would generalize, we cannot rule out appearance of emergent capabilities (Wei et al., 2022) which may, e.g., significantly reduce the hallucination rate on unseen data. Thirdly, we only study hallucinations that result in not remembering something that appeared verbatim in the training data. While this has enabled most of our analysis, some of our conclusions may not translate to other types of hallucinations.\nFor hallucination detectors, we focused on general trends rather than exhaustive coverage of existing methods, and it is possible that alternative methods could have yielded better results. However, the qualitative consistency of the results relating to dependence of detector performance on LM scale suggests these are general properties of hallucination detectors. Finally, dataset imbalance driven by varying hallucination rates between our LMs had a strong effect on performance of the hallucination detectors. We did not experiment with methods to adjust for such imbalance, beyond implicitly measuring performance at different score cut-offs via AUC-PR and the PR curves."}, {"title": "Conclusion", "content": "We explored the relationship between LM hallucinations and their scale. In particular, our paper sheds light on what it would take to eliminate factual hallucinations in the case where the correct answers\u2014and no wrong answers\u2014appear verbatim in the training set. Our study can also be seen as studying how much computation is needed for an LM to memorize all the knowledge content of a dataset. For a fixed dataset size, larger longer trained LMs hallucinate less. However, we found increasing dataset size also increased hallucination rate, given a fixed number of training epochs and LM size. This is because a larger dataset means more facts to learn. Similar to Kandpal et al. (2023), we hypothesise that standard LM training data contains repeated facts\u2014and other repetitive text structures that do not require hard memorization\u2014which makes the effect of dataset size on loss positive.\nWe found achieving a \u22645% hallucination rate on training data requires LMs much larger than currently considered optimal (Hoffmann et al., 2022), trained for much longer (20+ epochs). Moreover, the longer training can hurt LM generalisation, presenting a trade-off between hallucination rate and other performance metrics.\nGiven the difficulty of eliminating hallucinations outright, we explored how detectability of LM hallucinations depends on scale. We found an inverse relationship between LM scale and the detectability of its hallucinations. That is, while larger, longer-trained LMs hallucinate less, detecting their remaining hallucinations becomes increasingly hard."}]}