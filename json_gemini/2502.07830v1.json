{"title": "CAPTURED BY CAPTIONS: ON MEMORIZATION AND ITS MITIGATION IN CLIP MODELS", "authors": ["Wenhao Wang", "Adam Dziedzic", "Grace C. Kim", "Michael Backes", "Franziska Boenisch"], "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP's memorization behavior falls between the supervised and self-supervised paradigms, with \"mis-captioned\" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility\u2014something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-modal models, such as CLIP (Radford et al., 2021), have demonstrated strong performance in representation learning. By aligning visual and textual representations, these models achieve state-of-the-art results in tasks like image retrieval (Baldrati et al., 2022a;b), visual question answering (Pan et al., 2023; Song et al., 2022), and zero-shot classification (Radford et al., 2021; Ali & Khan, 2023; Wang et al., 2023; Zhang et al., 2022). Despite these successes, the mechanisms by which multi-modal models leverage their training data to achieve good generalization remain underexplored.\nIn uni-modal setups, both supervised (Feldman, 2020; Feldman & Zhang, 2020) and self-supervised (Wang et al., 2024b), machine learning models have shown that their ability to memorize their training data is essential for generalization. It was indicated that, in supervised learning, memorization typically occurs for mislabeled samples, outliers (Bartlett et al., 2020; Feldman, 2020; Feldman & Zhang, 2020), or data points that were seen towards the end of training (Jagielski et al., 2022), while in self-supervised learning, high memorization is experienced particularly for atypical data points (Wang et al., 2024b). However, it is unclear how these findings extend to models like CLIP which entail elements from both supervised learning (through captions as supervisory signals) and self-supervised learning (through contrastive loss functions).\nExisting definitions of memorization offer limited applicability to CLIP and therefore cannot fully address the gap in understanding. The standard definition from supervised learning (Feldman, 2020) relies on one-dimensional labels and the model's ability to produce confidence scores for these labels, whereas CLIP outputs high-dimensional representations. While the SSLMem metric (Wang et al., 2024b), developed for self-supervised vision models, could, in principle, be applied to CLIP's vision encoder outputs, it neglects the text modality, which is a critical component of CLIP. Additionally,"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "CLIP. Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) trains multi-modal encoders to map text-image pairs into a shared latent space with semantically equal representations. The core of CLIP is a two-encoder architecture with an image encoder $f_{img}$ and a text encoder $f_{txt}$ that are trained to maximize the similarity between the image and text features for correct text-image pairs, while minimizing the similarity for incorrect pairs. This is achieved using a contrastive loss function $L$ defined as:\n$L=\\frac{1}{N}\\log \\sum_{i=1}^{N} \\frac{\\exp(sim(f_{img}(x_i), f_{txt}(Y_i))/\\tau)}{\\sum_{j=1}^{N} \\exp(sim(f_{img}(x_i), f_{txt}(y_j))/\\tau)},$\nwhere $sim(,\\cdot)$ is the cosine similarity, $\\tau$ is the temperature parameter, and $N$ is the batch size. This training makes CLIP versatile across various downstream tasks, including image classification, retrieval, captioning, and object recognition. There are different versions of CLIP. The popular Language augmented CLIP (LaCLIP) (Fan et al., 2023) augments original CLIP by introducing text augmentations during training in addition to the image augmentations (crops) performed in the original CLIP training to reduce overfitting. We study the impact of this practice on memorization and find it to be a suitable mitigation method.\nMemorization. Memorization refers to a model's tendency to store specific details of individual training examples, rather than generalizing patterns across the dataset (Zhang et al., 2016; Arpit et al., 2017; Chatterjee, 2018; Feldman, 2020). This becomes problematic when models memorize sensitive data, as it has been shown to increase privacy risks (Carlini et al., 2019; 2021; 2022; Song et al., 2017). To date, memorization has been studied within single modalities for supervised and self-supervised learning. In supervised learning, it has been shown that models tend to memorize mislabeled (Feldman, 2020), difficult, or atypical examples (Arpit et al., 2017; Sadrtdinov et al., 2021), and that this memorization improves generalization, especially on long-tailed data (Feldman, 2020; Feldman & Zhang, 2020). Similar findings have been observed in self-supervised learning (SSL) in the vision domain (Wang et al., 2024b), where atypical samples experience high memorization, and a reduction of memorization in SSL encoders leads to decreased performance in various downstream tasks, such as classification, depth-estimation, and segmentation. A connection between memorization and generalization has also been observed in the language domain (Antoniades et al., 2024; Tirumala et al., 2022). In contrast to our work, these papers consider single-modality models. How those insights transfer to multi-modal models remains unclear.\nMemorization in self-supervised learning. Our CLIPMem builds on concepts from the SSLMem metric introduced by Wang et al. (2024b). This metric measures the memorization of an individual data point x by an SSL encoder, based on the alignment of representations from augmented views of x. Let f: Rn \u2192 Rd be an SSL encoder trained using an SSL algorithm A on an unlabeled dataset S = {x}1N. The data augmentations are represented as Aug(x) = {a(x)|a \u2208 Aug}, where a is a transformation function applied to the data point x, mapping from Rn \u2192 Rn. The encoder's output representation for a given data point x is denoted as f(x). For a trained SSL encoder f, the alignment loss for a data point x is defined as\n$L_{align}(f, x) = E_{x',x\" \\sim Aug(x)}[d (f(x'), f(x\"))],$  \nwhere $x', x\"$ are augmented views of x and $d(,\\cdot)$ is a distance metric, typically the l2 distance. SSLMem is then defined as\n$SSLMem(x) = E_{g \\sim A(S\\setminus x)}L_{align} (g, x) - E_{f \\sim A(S)}L_{align}(f, x)$"}, {"title": "3 DEFINING MEMORIZATION OVER MULTI-MODAL ENCODERS", "content": "3.1 PROBLEM SETUP\nConsider a single image-text pair (I, T) from a dataset S and two CLIP models: a model f and a reference model g, trained on dataset S and S' = S \\ {(I,T)}, respectively. We aim to quantify the memorization of (I, T) in f, trained on this data point, by leveraging model g not trained on the data point but otherwise on the same data, in a leave-one-out style of defining memorization (Feldman, 2020). We denote the image encoder in CLIP as $f_{img}$ : Image \u2192 Rd and the text encoder as $f_{txt}$ : Text \u2192 Rd. For the image-text pair (I, T), we denote with $f_{img}(I)$ the output representation of f's image encoder on image I and with $f_{txt}(T)$ the output representation of f's text encoder on text T. To evaluate the alignment between the image and text representations, i.e., to quantify how similar the two representations are, we use cosine similarity $sim(f_{img}(I), f_{txt}(T))$, as defined in the original CLIP paper (Radford et al., 2021).\n3.2 ALIGNMENT WITH CONTRASTIVE OBJECTIVE\nDuring training, the contrastive objective in CLIP maximizes the cosine similarity for correct image-text pairs while minimizing the cosine similarity for all the other N 1 incorrect pairs in any given training mini-batch with N training samples. This means that for a given image I and text T, the training objective pulls $f_{img}(I)$ and $f_{txt}(T)$ closer together in the latent space, while pushing $f_{img}(I)$ away from the representations of all other N 1 unrelated texts, and $f_{txt}(T)$ away from all other images. Hence, the intuition is that the quality of alignment in f, unlike in uni-modal self-supervised learning (Wang et al., 2024b), depends not only on the model's ability to create well-aligned text and image representations for a given text-image pair, but also on its ability to create distant representations for the N \u2013 1 other representations.\nTo formalize this intuition into a metric that quantifies the alignment of f on the image-text pair (I,T), we define Ttest as a set of N \u2013 1 randomly chosen testing samples that were not used in training f or g. Furthermore, when applicable, we denote random augmentations of the training data-e.g., text augmentations in versions like LaCLIP (Fan et al., 2023)\u2014as $T' \\sim Aug(T)$ for texts and I' ~ Aug(I) for images. Then, we define the alignment score of f on (I,T) as\n$A_{align} (f, I,T) = \\frac{E_{(I',T')\\sim Aug(I,T)}[sim(f_{img}(I'), f_{txt}(T'))]}{E_{(_,t) \\in T_{test}} [sim(f_{img}(I), f_{txt}(t))] - E_{(i,_) \\in T_{test}} [sim(f_{img}(i), f_{txt}(T))],$"}, {"title": "3.3 DEFINING MEMORIZATION IN CLIP", "content": "Given our definition of alignment scores, we can define our CLIPMem in a similar vein to the definition of memorization in supervised learning (Feldman, 2020), in the leave-one-out style. Given the image-text pair (I, T) from dataset S and two CLIP models, f and g, trained on dataset S and S' = S \\ {(I,T)}, respectively, we define CLIPMem as\n$CLIPMem(I,T) = A_{align}(f, I,T) \u2013 A_{align}(g, I, T).$\nIf a model f has a significantly higher alignment score than model g on (I,T), this means that f memorizes this data point. Note that taking the difference between f and g is crucial to get a solid estimate of memorization. This is because without \"context\", a high or low alignment score of f does not express much information. The alignment of f can be high without memorizing (I, T), for example, if (I, T) is a simple (but not memorized) training example. In this case, the reference model g will also have a high score, such that the difference is again small. Thanks to this design of our CLIPMem, it will then correctly report low memorization."}, {"title": "4 EMPIRICAL EVALUATION", "content": "4.1 EXPERIMENTAL SETUP\nModels and training. We build our experiments on OpenCLIP (Cherti et al., 2023), an open-source Python version of Open-CLIP (Ilharco et al., 2021). The standard architecture used for the experiments builds on ViT-Base, but we also include experiments using ViT-Large. We train the model on the COCO dataset (Lin et al., 2014). Since COCO is much smaller than OpenCLIP's standard training datasets, we reduce the training batch size to 128 and increase the epoch number from 32 to 100 to achieve similar performance. All other settings strictly follow OpenCLIP. For training DINO, as an example of an SSL vision encoder, we follow the default setting of Caron et al. (2021). The supervised model is trained as a multi-label classifier, also based on ViT-Base (with an additional fully connection layer) based on the first-level annotation captions in the COCO dataset. A full specification of our experimental setup is detailed in Appendix A.2. Additional experiments for measuring memorization on the BLIP (Li et al., 2022) model are presented in Appendix A.6.\nDatasets. We use COCO (Lin et al., 2014), CC3M (Sharma et al., 2018), and the YFCC100M (Thomee et al., 2016a) datasets to pre-train the OpenCLIP models. For the CC3M dataset, we randomly sample 75000 examples from the total of 2.91M data points. We evaluate the models by testing the linear probing accuracy on ImageNet (Deng et al., 2009) with an added classification layer trained on top of the output representations. We use the YFCC100M dataset to simulate an infinite data regime, i.e., using a single training run where no data point is repeated whereas we train iteratively using CC3M and COCO.\nMeasuring memorization. We follow Wang et al. (2024b) to approximate our CLIPMem. Since training a separate pair of models for every data point whose memorization we aim to measure would be computationally intractable, we measure memorization of multiple data points at the same time. Therefore, we divide the original training set in four subsets: (1) Ss, data points that both model f and g were trained on, (2) Sc, data points used only for training f, (3) S\u2081, data points used only for training g, and (4) SE, external \"test\" data points that none of the models was trained on. Note that |Sc| = |S1|, such that f and g have the same number of training data points in total. For our experiments, following a similar approach to Wang et al. (2024b), we want to strike a balance when choosing the size of Sc. If the size is too large, then f and g might differ too much and not yield a strong memorization signal, but if it is too small, we would only have a memorization signal for too few data points. Concretely, for COCO and CC3M, we set |SS| = 65000 and |Sc| = |S1| = |SE| = 5000. Memorization is reported as an average over all data points in Sc for model f, or per individual data point in Sc.\nGenerating captions and images. For generating additional captions for the training images, we rely on GPT-3.5-turbo. For each input image, we provide the representation produced by our trained OpenCLIP model and ask GPT to generate five new captions. Generated sample captions are presented in Figure 18. To generate additional images for the COCO dataset, we use Stable Diffusion v1.5 to generate five new images, one corresponding to each of the five per-image captions in the COCO dataset. Sample generated images are presented in Figure 17."}, {"title": "4.2 STUDYING MEMORIZATION USING CLIPMEM", "content": "We first set out to analyze the general memorization in CLIP in order to identify which data points are memorized. To do this, we quantify CLIPMem over the different training subsets. Our results are presented in Figure 2a. In particular, we observe that CLIPMem for Sc, the data points only used to train model f, is significantly higher than for Ss, the data points shared between the two models. Memorization for Ss is comparable to that for SE, i.e., the external data not seen during training, indicating that f does not memorize these samples. The data in S\u2081 causes negative CLIPMem scores, indicating that this data is memorized by g, not by f. This is the expected behavior according to the definition of our metric. In Appendix A.4, we additionally highlight that memorization increases with model size, i.e., CLIP based on ViT-Large has a higher overall memorization with an average of 0.457 while CLIP based on ViT-Base only reaches 0.438 on average.\nAdditionally, we analyze individual data points according to their reported CLIPMem. We give examples of highly memorized data points in CLIP in Figure 1 and more highly vs. little memorized samples in Figures 13,14,15, and 16 in Appendix A.9. Overall, the samples with high CLIPMem, e.g., in Figure 1 seem to be difficult examples and examples with imprecise or incorrect captions whereas the samples with low CLIPMem are simpler and (potentially consequently) more precisely captioned. In Appendix A.5, we show that these findings also hold when we operate in the infinite data regime, i.e., when we perform only a single training run where no data point is repeated.\nMotivated by this insight and by observations from supervised learning where it was shown that models can memorize random labels (Zhang et al., 2016) and where mislabeled data experiences highest memorization (Feldman, 2020), we test if the same effect can also be observed in CLIP. Therefore, we \"poison\" our CLIP's training data by randomly shuffling the captions among 500 of the 5000 candidate data points in Sc. Thereby, these 500 data points are \"mis-captioned\". We train a model based on this data and see that the mis-captioned examples experience significantly higher memorization (CLIPMem of 0.586) compared to the \"clean\" data points (CLIPMem of 0.440). Even though CLIP trains using a contrastive training objective, the memorization of clean data points is not significantly affected by training the model with the mis-captioned examples, as we can see by their CLIPMem that is 0.438 on the clean model and 0.440 on the poisoned model."}, {"title": "4.3 MEASURING MEMORIZATION IN ONE MODALITY DOES NOT YIELD A STRONG SIGNAL", "content": "To understand how important it is to take both modalities into account in our definition of CLIPMem, we set out to evaluate whether existing practical methods to measure memorization over uni-modal encoders (Wang et al., 2024b) yield a sufficiently strong memorization signal in CLIP. Therefore, we"}, {"title": "4.4 \u039c\u0395\u039cORIZATION BETWEEN MODALITIES", "content": "Our results in Figure 3 indicate that memorization is higher in CLIP's text encoder than in the image encoder (the average SSLMem on Sc in the text encoder is 0.209 vs. 0.168 in the image encoder). To provide further insights into how memorization behaves between the modalities in CLIP, we first analyze the use of augmentations. We compare five cases: (1) no additional augmentations beyond the baseline (image cropping), (2) generating one image using a diffusion model for a given original caption, (3) generating five variations of each image using a diffusion model and randomly selecting one for each training iteration while keeping the caption fixed, (4) using the original image but randomly selecting one of the five COCO captions for each training iteration, and (5) randomly pairing each of the five generated images with one of the five COCO captions.\nAs shown in Figure 14, there is quite a variability in the COCO captions for the same sample. Hence, some images might not fit well with the chosen training caption. This imprecise captioning can cause an increase in memorization. We observe that the effect is mitigated when using the 5 images with the 5 captions (5th case, see Table 1). This phenomenon results most likely from the increased number of possible image-text pairs (25), such that individual incorrect or imprecise pairs are not seen so often during training. For the third case, i.e., row three in Table 1, we generate five images with a diffusion model based on all five captions per image from the COCO dataset. However, as we only use the first caption during training, this would introduce many mis-captioned images which significantly lowers performance and increases memorization. To avoid this problem, we removed 6000 mis-captioned samples.\nOur results in Table 1 highlight that augmenting text during training reduces memorization and increases performance more than augmenting images. However, applying augmentations of both text and images strikes the right balance between the reduction in memorization and the increase in performance. In fact, applying both augmentations reduces memorization most significantly. Overall, these results indicate that memorization in CLIP's is tightly coupled to the captions assigned to the training images with imprecise captions having a destructive effect on CLIP performance and memorization."}, {"title": "4.5 RELATION TO CLIP MEMORIZATION TO (SELF-)SUPERVISED MEMORIZATION", "content": "We further provide insights on whether CLIP's memorization behavior is more alike to the one of supervised learning or SSL. This question is highly interesting since the captions in CLIP can be considered as a form of labels, like in supervised learning, whereas the contrastive training objective on the dataset resembles more SSL. We perform two experiments to gain a better understanding of the memorization behavior of CLIP with respect to supervised learning and SSL.\nFirst, we compare an SSL vision encoder pair f and g with the same architecture as CLIP's vision encoder but trained from scratch on COCO using DINO, i.e., standard SSL training. We train f and g using the same candidates as the pair of CLIP models in our previous experiments. Then, we use the SSLMem metric from Wang et al. (2024b) to quantify memorization in the CLIP vision encoder and the SSL encoder, respectively. The CLIP vision encoder has a significantly lower SSLMem than the SSL encoder (0.209 vs. 0.279). Hence, CLIP vision encoders experience lower SSL memorization than SSL trained encoders. To further investigate the difference, we also report the overlap between the top 10% memorized samples between the two models, measured according to SSLMem. With an overlap of only 47 out of 500 (9.4%) samples, we find that CLIP memorizes significantly different samples than SSL encoders. Wang et al. (2024b) had performed a similar experiment on SSL vs. supervised learning and found that the two paradigms also lead to different samples being memorized. While this is, on the one hand, an effect of the different objective function, the difference between the memorized samples in CLIP and SSL is likely also closely connected to the additional captions that CLIP takes into account. While SSL-trained encoders can memorize atypical images, CLIP encoders can memorize typical images when they have an atypical, imprecise, or incorrect caption.\nAdditionally, we compare the memorization behavior of CLIP against supervised and SSL-trained models on the neuron-level. Therefore, we train two additional ViT-Base models on COCO using supervised training and SSL training with DINO. Then, we apply the UnitMem metric (Wang et al., 2024a) to measures how much individual neurons memorize individual samples from the training data. A high UnitMem suggests that neurons highly memorize individual data points instead of groups/classes of points. It had been shown that supervised learning causes neurons in lower layers to experience low UnitMem, i.e., being responsible for learning joint groups of data points, while neurons in later layers highly memorize individual data points. In contrast, for SSL, UnitMem was shown to remain relatively constant over layers with neurons in lower layers also being able to memorize individual data points. This difference was attributed to the different objective functions where supervised learning's cross entropy loss pulls together data points from the same class, whereas SSL's contrastive loss leads to individual data points being pushed away from each other (Wang et al., 2024a). Our results in Figure 4 highlight that CLIP, in terms of its memorization behavior, is between supervised learning and SSL. At the lower layers, it is much less selective than models trained with SSL, i.e., it focuses on groups of data points rather than memorizing individual data points, similar to supervised learning. Yet, in later layers, CLIP becomes more selective than SSL, i.e., it memorizes individual data points more in individual neurons, but still less than supervised learning which there has a very high average per-layer UnitMem."}, {"title": "4.6 MITIGATING MEMORIZATION WHILE MAINTAINING GENERALIZATION", "content": "The experiments from Table 1 suggest that using augmentations during training can improve generalization while also reducing memorization. This is an unexpected synergy since for both supervised learning (Feldman, 2020) and SSL (Wang et al., 2024b), generalization was shown to decline when memorization decreases. To further study the impact of mitigating memorization in CLIP on downstream generalization, we explore two orthogonal strategies for \"augmenting\" the text modality during CLIP training, first in the input space and second directly in the embedding space. Additionally, we analyze the effect of removing memorized samples from training.\nMultiple captions. We vary the number of captions used during training and report fine-grained insights into the resulting memorization and downstream performance in Figure 5a. Our results"}, {"title": "5 CONCLUSION", "content": "We presented CLIPMem, a formal measure to capture memorization in multi-modal models, such as CLIP. By not only quantifying memorization but also identifying which data points are memorized and why, we provide deeper insights into the underlying mechanisms of CLIP. Our findings highlight that memorization behavior of CLIP models falls between that of supervised and self-supervised models. In particular, CLIP highly memorizes data points with incorrect and imprecise captions, much like supervised models memorize mislabeled samples, but it also memorizes atypical examples. Furthermore, we find that memorization in CLIP happens mainly within the text encoder, which motivates instantiating mitigation strategies there. By doing so, we can not only reduce memorization in CLIP but also improve downstream generalization, a result that challenges the typical trade-offs seen in both supervised and self-supervised learning."}, {"title": "A APPENDIX", "content": "A.1 EXTENDED BACKGROUND\nD\u00e9j\u00e0 Vu Memorization in CLIP. The D\u00e9j\u00e0 Vu memorization framework (Jayaraman et al., 2024) is the only existing other work that attempts to quantify memorization in vision-language models. It uses the text embedding of a training image caption to retrieve relevant images from a public dataset of images. It then measures the fraction of ground-truth objects from the original image that are present in the retrieved images. If the training pair is memorized, retrieved images have a higher overlap in ground truth objects, beyond the simple correlation. While valuable, several aspects warrant further consideration for broader applicability of the framework. First, its focus on object-level memorization ignores non-object information like spatial relationships or visual patterns that can also influence memorization (Feldman, 2020; Wang et al., 2024b). To perform object retrieval, the framework also relies on object detection and annotation tools, which may introduce variability based on the accuracy and robustness of these tools. Additionally, the assumption that public datasets with similar distributions to the training data are readily available may not always hold, necessitating alternative approaches. Moreover, the framework does not analyze why certain images are memorized limiting detailed analysis. Finally, while D\u00e9j\u00e0 Vu must address the challenge of distinguishing between memorization and spurious correlations, CLIPMem avoids this by directly assessing memorization on the output representations of the model. One notable difference between the results of our approach and D\u00e9j\u00e0 Vu's is that their findings show that their mitigation strategies can reduce memorization, but at the cost of decreased model utility. CLIPMem, in contrast, does not observe trade-offs between memorization and performance."}, {"title": "A.2 EXTENDED EXPERIMENTAL SETUP", "content": "General Setup. All the experiments in the paper are done on a server with 4 A100 (80 GB) GPUs and a work station with one RTX 4090 GPU(24 GB). We detail the setup for our model training, both CLIP and SSL (relying on DINO) in Table 2.\nExperimental Setup for SSLMem. To experimentally evaluate memorization using the SSLMem framework (Wang et al., 2024b), the training dataset S is split into four sets: shared set (Ss) used for training both encoders f and g; candidate set (Sc) used only for training encoder f; independent set (S1) data used only for training encoder g; and an additional extra set (S1) from the test set not used for training either f or g. For training encoders, encoder f is trained on Ss U Sc, while encoder g is trained on SSUS1. The alignment losses $L_{align}(f, x)$ and $L_{align}(g, x)$ are computed for both encoders, and the memorization score m(x) for each data point is derived as the difference between these alignment losses, normalized to a range between -1 and 1. A score of 0 indicates no memorization, +1 indicates the strongest memorization by f, and -1 indicates the strongest memorization by g.\nNormalization on CLIPMem. For improved interpretability, we normalize our CLIPMem scores to a range of [-1,1]. A memorization score of 0 indicates no memorization, +1 indicates the strongest memorization on CLIP model f, and -1 indicates the strongest memorization on CLIP model g. We find the normalized CLIPMem score for a dataset using the following process: For each image-text pair (I, T), we first calculate the CLIPMem score as the difference in alignment scores between two CLIP models f and g. Once CLIPMem scores are computed for all data points, we normalize them by dividing each score by the range, which is the difference between the maximum and minimum scores in the dataset. Finally, we report the normalized CLIPMem score for a dataset as the average of these normalized values."}, {"title": "A.3 ADDITIONAL EXPERIMENTS", "content": "A.3.1 MEMORIZATION VS. GENERALIZATION IN CLIP\nExtending evaluation. In Figure 7, we perform the same experiment as in Figure 6, but on a CLIP model trained with 5 captions instead of 1. We observe the same trend, with the difference that the peak is at roughly 500 removed samples rather than 100. This is likely due to the increase in captions (by factor 5) that causes increase in mis-captioned samples.\nVerifying the hypothesis on memorizing mis-captioned samples through supervised learning. We repeat the same experiment in the supervised learning setup to understand where the increase and then decrease in linear probing accuracy stems from. To test our hypothesis that it stems from \"mis-captioned\" samples, we \"poison\" our supervised model by flipping the labels of 200 data points before training. Then, we approximate the memorization metric from Feldman (2020) in our setup and remove highly memorized vs. random data points. In the same vein as in Appendix A.3.1, we first observe an increase in linear probing accuracy when removing memorized samples (instead of random samples). The peak is at roughly 200 data points, i.e., the number of deliberately mislabeled samples. Until the cutoff point at roughly 3200 examples, linear probing accuracy is still higher when removing most memorized rather than random samples, which might suggest that there are other"}]}