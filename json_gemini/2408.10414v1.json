{"title": "Towards Automation of Human Stage of Decay Identification: An Artificial Intelligence Approach", "authors": ["Anna-Maria Nau", "Phillip Ditto", "Dawnie Wolfe Steadman", "Audris Mockus"], "abstract": "Determining the stage of decomposition (SOD) is crucial for estimating the postmortem\ninterval and identifying human remains. Currently, labor-intensive manual scoring\nmethods are used for this purpose, but they are subjective and do not scale for the\nemerging large-scale archival collections of human decomposition photos. This study\nexplores the feasibility of automating two common human decomposition scoring meth-\nods proposed by Megyesi and Gelderman using artificial intelligence (AI). We evaluated\ntwo popular deep learning models, Inception V3 and Xception, by training them on a\nlarge dataset of human decomposition images to classify the SOD for different anatom-\nical regions, including the head, torso, and limbs. Additionally, an interrater study\nwas conducted to assess the reliability of the AI models compared to human forensic\nexaminers for SOD identification. The Xception model achieved the best classification\nperformance, with macro-averaged F1 scores of .878, .881, and .702 for the head, torso,\nand limbs when predicting Megyesi's SODs, and .872, .875, and .76 for the head, torso,\nand limbs when predicting Gelderman's SODs. The interrater study results supported\nAI's ability to determine the SOD at a reliability level comparable to a human expert.\nThis work demonstrates the potential of AI models trained on a large dataset of human\ndecomposition images to automate SOD identification.", "sections": [{"title": "1. Introduction", "content": "Determining the stage of decay (SOD) is an important and common task in hu-\nman remains cases. Knowing the degree of decomposition is vital for estimating the\npostmortem interval (PMI) and identifying human remains [1, 2, 3, 4]. Presently, estab-\nlishing the SOD of a decedent is primarily conducted manually, via visual assessment\nby trained experts using non-metric scoring or staging methods, such as those pro-\nposed by Megyesi et al. [1] and Gelderman et al. [2]. Such non-metric methods, which\nrely on subjective interpretation made by humans, possess a higher susceptibility to\nhuman bias and error, consequently affecting the accuracy of downstream tasks, such\nas estimating the PMI [5, 6, 7, 8]. Furthermore, the PMI estimation formulas derived\nin existing studies, such as those by Megyesi et al. [1] and Gelderman et al. [2], were\ndeveloped using a very small number of samples. Evaluating or improving upon these\nformulas with a much larger sample size, such as over one million photos, would require\nmanual SOD scoring, which is not feasible for such a large sample. Therefore, this work\naims to utilize emerging artificial intelligence (AI) methods to evaluate the feasibility\nof automating the SOD identification task.\nAI is the ability of machines to perform tasks that would typically require human\nintelligence and it has provided innovative approaches to assist in human decision-\nmaking [9, 10]. AI assesses information based on the entirety of acquired facts or data\nusing advanced algorithms, thereby mitigating vulnerability to the subjectivity and\nbiases that trouble humans and affect their decision-making abilities [11]. Additionally,\nAI algorithms can handle large volumes of data, uncovering intricate patterns that\nmight elude human perception [9, 12, 13]. This ability to process, analyze, and interpret\nlarge amounts of data quickly and precisely makes AI a valuable tool in many industries,\nincluding forensic practice and research.\nIn summary, the objective of this study is to evaluate the possibility of automating\ntwo established human decomposition scoring methods, namely Megyesi et al. [1] and\nGelderman et al. [2], using vision-based AI models, known as convolutional neural\nnetworks (CNNs). Specifically, various CNN classification models will be trained and\nevaluated on a large human decomposition image dataset to perform SOD prediction.\nIn addition, an interrater test is conducted to assess and compare the reliability of\nthe models and the human forensic examiners for SOD identification. We hypothesize\nthat similar interrater reliability among human raters and an AI rater suggests the\nfeasibility of using AI for SOD classification and, perhaps, other downstream tasks.\nThe significance of such a finding lies in the potential to develop more accurate SOD\nand PMI estimation methods that are less effort-intensive and subjective. The primary\npurpose of this study is to provide a proof-of-concept for the future advancement and\nintegration of AI-assisted analysis in forensic practice and research."}, {"title": "2. Materials and methods", "content": ""}, {"title": "2.1. Stage of decay scoring methods", "content": "The two human decomposition scoring methods this study attempts to automate\nusing AI are: (1) Megyesi et al. [1] and (2) Gelderman et al. [2]. To account for the\ndifferential decomposition that occurs in different body segments (e.g., limbs do not\nbloat or purge fluid), these two scoring methods independently assess the human body\nin three anatomical regions: (1) the head (including the neck), (2) the torso, and (3)\nthe limbs (including the hands and feet). Based on the morphological features present,\nMegyesi et al. [1] categorizes human decomposition into four high-level linear stages:\nfresh, early decomposition, advanced decomposition, and skeletonization. Gelderman\net al. [2], building upon the work of Megyesi et al. [1], categorizes each anatomical\nregion into six stages, with the lowest indicating no visible changes and the highest\nindicating complete skeletonization."}, {"title": "2.2. The human decomposition dataset", "content": "The human decomposition dataset, a large-scale image collection used to train\nthe models, includes images of decomposing corpses donated to [removed for double\nanonymized review]. The center houses [removed for double anonymized review]. Foren-\nsic experts from the [removed for double anonymized review] captured these images at\nnon-uniform intervals, with one or more days between each capture. The images, taken\nfrom various angles, depict different anatomical areas to illustrate the various stages and\nregions of human decomposition. The image resolutions vary from 2400\u00d71600 pixels up\nto 4900\u00d73200 pixels. The dataset covers the period from 2011 to 2023 and comprises\nover 1.5 million images contributed by more than 800 donors. To train different CNN\nclassifiers on this large human decomposition image dataset to predict the SOD for\nvarious anatomical regions, the following challenges needed to be addressed:\n\u2022 How to best sample from the entire human decomposition dataset such that the\nresulting set of images reflects the dataset's temporal characteristics. In other\nwords, the data used to train the models should consist of images covering the\nentire human decay process, that is, from death to skeletonization.\n\u2022 Once a set of images has been sampled, the challenge is to automate the efficient\nextraction of specific anatomical regions (i.e., head, torso, and limbs). From a\ntime- and cost-effective perspective, it is not feasible to manually perform this\nbody part filtering of over one million images.\nThe following section (Section 2.3) details how these analysis challenges were addressed\nduring data preparation."}, {"title": "2.3. Data processing and labeling", "content": "The human decomposition dataset was processed according to the data pipeline\nshown in Figure 1. The remainder of this section further discusses the individual steps\nof this pipeline.\nThe quality and size of the data used to train the models highly affect a model's\nperformance and generalizability. In other words, the more representative and diverse\nthe training data is, the more likely it is that the model will be able to generalize.\nIn the case of a temporal dataset, such as images documenting human decomposition,\nwhere the subjects' appearance changes over time, it is important to sample the training\ndata in a way that reflects the dataset's characteristics. Therefore, images depicting\nall possible decomposition stages should be included in the training data. As a result,\na small sample of donors, and all images of those donors, was selected over time (i.e.,\nfrom when they first started to decay until fully decomposed), instead of randomly\nselecting images from the entire human decomposition dataset. Since this subset of\nimages included all different kinds of anatomical areas, a previously developed body\npart classification model was used to automatically detect the head, torso, and limbs\nimages to align with Megyesi et al.'s [1] and Gelderman et al.'s [2] scoring methods. The\nnext step was to manually label this subset of images with the SOD labels. Specifically,\nthe two scoring methods, described in Section 2.1, were used by a forensic expert to\nperform manual data labeling of the head, torso, and limbs images. Note that any body\npart misclassifications and/or poor quality images were either corrected or removed at\nthis point to ensure that the final labeled datasets only included high quality images of\nthe head, torso, and limbs. Table 1 shows the SOD terms used in the original literature\n(i.e., Megyesi et al. [1] and Gelderman et al. [2]) and the corresponding new class labels\nused throughout this study. For instance, if the forensic expert, applying Megyesi et\nal.'s [1] method, determined the SOD of a head image to be fresh, then it was assigned\nthe M-SOD1 class label.\nThe image labeling was conducted using an in-house developed data visualization\nand annotation software called [removed for double anonymized review]. The resulting\nlabeled datasets are shown in Table 2, each of which will be used to train a SOD\nclassification model. For example, M-head-data will be used to train a model to predict\nthe SOD (i.e., M-SOD1, M-SOD2, M-SOD3, or M-SOD4) of head images. Finally,\neach labeled dataset was split into a train and test set using a ratio of 80:20. The train\nset was used to train the model and the test set was used to evaluate the model once\ntrained."}, {"title": "2.4. AI model development", "content": "To build the SOD classifiers, transfer learning was applied, which aims to produce\neffective models by leveraging and exploiting previously acquired knowledge [14]. In\nparticular, two CNN architectures, including Inception V3 [15] and Xception [16], pre-\ntrained on the ImageNet dataset [17], were trained using the following two-step transfer\nlearning process: (1) freeze all pre-trained convolutional layers of the base model and\ntrain newly added classifier layers and (2) unfreeze all layers and fine-tune the model\nend-to-end with a low learning rate. The newly added classifier layers consisted of\nfive layers, including one global average pooling layer and one drop-out layer (rate =\n0.3) to alleviate the over-fitting problem motivated by Lin et al. [18], followed by two\nfully-connected layers with 128 and 64 nodes performing down-sampling, and one final\nsoftmax layer with the number of nodes equal to the number of classes for multi-class\nclassification. More precisely, the softmax layer transforms the output of the previous\nlayer into a probability distribution over all the classes as shown by the equation (1),\nwith the class having the highest probability being the final SOD prediction. In equa-\ntion (1), $z_i$ is the $i^{th}$ element of the input vector to the softmax function and $K$ the total\nnumber of classes. To increase the size and diversity of the data, a data augmentation\nlayer was added after the input layer, performing random image flipping (horizontal\nand vertical) and rotation during model training. Figure 2 gives an overview of the\ndeveloped SOD classification framework.\n$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\text{ for } i = 1, 2, ..., K$ (1)\nThe loss function used was Cross-Entropy loss, which takes the predicted probability\ndistribution of the softmax layer and measures how well this distribution matches the\ntrue distribution. To minimize the Cross-Entropy loss function during training, the\nAdaptive Momentum Estimation (Adam) optimizer was employed, with a learning rate\nof 0.001 for the first step and 0.0001 for the second step in the two-step transfer learning\nprocess. It is worth noting that training the Inception V3 and Xception architecture\nfrom scratch (i.e., without transfer learning), transfer learning without freezing the\nbase model (i.e., one-step transfer learning), and freezing only a certain number of\nbase model layers instead of all followed by fine-tuning was also tested. However, the\nproposed two-step transfer learning process significantly improved model performances.\nTo evaluate the performance of the trained SOD classifier models, the confusion\nmatrix on the test data was calculated per class, which summarizes a model's perfor-\nmance by comparing its predicted labels to its true labels. Specifically, the confusion\nmatrix shows the number of correct predictions, such as the true positives (TP) and the\ntrue negatives (TN), as well as the number of incorrect predictions, such as the false\npositives (FP) and the false negatives (FN). The following two performance metrics\nwere then calculated from the per-class confusion matrix: precision (2) and recall (3).\nPrecision measures the accuracy of the positive predictions, while recall measures the\ncompleteness of the positive predictions.\n$\\text{precision} = \\frac{TP}{TP+FP}$ (2)\n$\\text{recall} = \\frac{TP}{TP+FN}$ (3)\nTo combine the per-class precision and recall metrics into a single model evaluation\nmetric, the macro-averaged F1 score (mF1) (4) was calculated and reported, which is\nthe unweighted mean of the per-class F1 scores (5).\n$\\text{mF1} = \\frac{\\sum_{i=1}^n F1_i}{n} \\text{ for } i = 1, 2, ..., n, \\text{ where } n \\text{ is the number of classes}$ (4)\n$F1 = \\frac{2 * (\\text{Precision} * \\text{Recall})}{\\text{Precision} + \\text{Recall}}$ (5)"}, {"title": "2.5. Interrater test", "content": "To ensure that manual rating techniques are reliable, interrater reliability tests are\noften used to assess how similar the ratings are between two or more raters on the same\nset of data samples. In this study, it is also used to assess the reliability of the developed\nmodels. The interrater test involved multiple raters (both human and AI) labeling a set\nof images depicting the same anatomical region using Megyesi et al.'s [1] and Gelderman\net al.'s [2] scoring methods. Specifically, 300 torso images that were not used during\nmodel development were selected. Due to limited resources, the interrater test focused\non the torso only, which provides a good amount of variability. The raters included\nthe developed torso SOD model or Model and three forensic experts well-versed in the\nconsidered scoring methods, including Human 1 (the same human who labeled the data\nthe models were trained with), Human 2, and Human 3. The task for each rater was\nto label the 300 images using once Megyesi et al.'s [1] method and once Gelderman et\nal.'s [2] method. The Model rater performed labeling by predicting the SOD of the 300\nimages, while the human raters were instructed to independently label the 300 images\non [removed for double anonymized review], following a similar set-up used for data\nlabeling in Section 2.3. To ensure randomization across methods, human labeling was\nconducted in batches of images instead of labeling all 300 images with one method and\nthen the other. Specifically, the human raters were presented with batches of 50 images\nat a time, which they were asked to label using one scoring method (i.e., Megyesi et\nal. [1] or Gelderman et al. [2]). This process was repeated until all 300 images were\nlabeled with both scoring methods, resulting in 12 iterations for each human rater ((300\nimages / 50 images) \u00d7 2 methods). Note, the method used to label a batch of images\nwas alternated between the two scoring methods.\nAfter each rater completed labeling, two types of agreements were assessed: (1)\nhuman-human agreement (i.e., the agreement among all three human raters: Human 1,\nHuman 2, and Human 3) and (2) AI-human agreement (i.e., the agreement among the\nModel replacing Human 1, and the other two human raters, Human 2, and Human 3).\nThe reason for the Model replacing Human 1 was to see how the agreement changes\nwhen the human is replaced by the model trained on the data they labeled. To measure\nthe different agreements, the Fleiss' Kappa statistic was used, which measures the\ndegree of agreement among raters over what would be expected by random chance,\nwith values ranging from -1 (no agreement) to 1 (perfect agreement). The Fleiss' Kappa\nvalues will be interpreted based on Landis and Kock's [19] interpretation criteria shown\nin Table 3."}, {"title": "3. Results", "content": ""}, {"title": "3.1. SOD classification", "content": "The SOD classifier models were implemented using Keras and TensorFlow, two\nopen-source machine learning modules written in Python. In particular, the models\nwere built and evaluated on the six manually labeled datasets shown in Table 2. All\nimages were resized to 299\u00d7299 pixels, as that is the required input image size for both"}, {"title": "3.2. Interrater test", "content": "The interrater test results are shown in Table 6. Specifically, a Fleiss' Kappa analysis\nusing SPSS Statistics was conducted for both the human-human agreement and AI-\nhuman agreement across both scoring methods. The reported agreement levels were"}, {"title": "3.3. Discussion", "content": "Overall, the SOD classification results are promising. The Xception architecture\nperformed the best across both scoring methods. The head and torso SOD models\nperformed comparably well; however, the limbs SOD models' performances were not\nas strong. Further analysis of the limb data revealed that some images included hands\nand/or feet covered by a net (to prevent them from being scattered/disarticulated by\nanimal scavengers), which could confuse and distract the model, leading to incorrect\npredictions. Additionally, the examination of the limbs data indicated that some im-\nages included other parts of the body, specifically the torso. Since the torso decays\ndifferently than the limbs, this inclusion could again confuse the model and, conse-\nquently, affect its predictions. Future work will focus on addressing these data quality\nchallenges to ensure the development of a more reliable limbs dataset and hence im-\nproved prediction performance. Another important finding to mention is that although\nthe Gelderman et al. [2] datasets contained more SOD classes (six classes) than the\nMegyesi et al. [1] datasets (four classes), the classification performances were compa-\nrable. This indicates that these AI models are able to learn a higher number of decay\nstages without decreasing prediction performance.\nWhile the SOD prediction performances were promising overall, there is room for\nimprovement, as indicated by the per-class precision and recall values. The sizes of the\nlabeled datasets used in this study are considered rather small for training deep learn-\ning architectures, such as Inception V3 and Xception. Additionally, a deeper analysis\nof the labeled datasets indicated class imbalance (i.e., a disproportionate number of\ninstances of one class compared to another). Having a larger, more diverse, and evenly\nbalanced dataset will make the models more robust and improve their generalization\ncapabilities, which refers to how well a model can react to new and unseen data. How-\never, creating more labeled data where domain expertise is required may be limited by\nboth resource and time constraints. A recent study [removed for double anonymized\nreview] addressed such challenges by developing a domain-aware label propagation algo-\nrithm that leverages different image attributes to automatically perform data labeling,\nthereby reducing manual labeling efforts and costs. Future work will explore integrating\nsuch label propagation methods to obtain larger and more diverse datasets, aiming to\ncreate more robust and accurate SOD classification models.\nIn the interrater test, both the human-human and AI-human agreements showed\nsubstantial agreement when applying the Megyesi et al. [1] scoring method. Using the\nGelderman et al. [2] scoring method, moderate agreement was observed for both the\nhuman-human and AI-human agreements. Across both scoring methods, the level of\nagreement for both the human-human and AI-human agreements was the same. This\nmeans that when the human rater was replaced with the AI model rater, the level of\nreliability stayed the same, supporting AI's ability to perform SOD identification with a\nreliability level comparable to that of an experienced human forensic examiner. Notably,\nthe lower agreement level for the Gelderman et al. [2] method could be attributed to (1)\nits novelty in the field and/or (2) its complexity, being more complicated to apply, as it\ninvolves six decay stages compared to the four stages in the Megyesi et al. [1] method.\nWhile the results demonstrate that human decomposition scoring methods have the\npotential to be automated using AI techniques, there are some important limitations of\nthis work. For one, the data labeling was conducted by a single forensic expert. This\napproach may introduce labeling bias, which can lead to inherently biased training\ndatasets. Models trained on such datasets can inherit these biases, resulting in biased\nmodels [20]. Therefore, future work will focus on creating a so-called \u201cgold standard\u201d"}, {"title": "4. Conclusion", "content": "This study explored the possibility of automating two common human decomposi-\ntion scoring methods, namely Megyesi et al. [1] and Gelderman et al. [2]. Specifically,\ndifferent CNN models, including Inception V3 and Xception, were trained on a large hu-\nman decomposition image dataset to classify the SOD for different anatomical regions.\nAcross both scoring methods, the Xception model achieved the highest classification\nresults, performing comparably well for the head and torso, and slightly lower for the\nlimbs. The interrater reliability study results provided support for AI's ability to auto-\nmate the SOD identification task at a reliability level comparable to a human expert.\nOverall, the study results are promising and provide a proof-of-concept for automating\nhuman decomposition scoring methods using AI."}]}