{"title": "Towards Automation of Human Stage of Decay Identification: An Artificial Intelligence Approach", "authors": ["Anna-Maria Nau", "Phillip Ditto", "Dawnie Wolfe Steadman", "Audris Mockus"], "abstract": "Determining the stage of decomposition (SOD) is crucial for estimating the postmortem interval and identifying human remains. Currently, labor-intensive manual scoring methods are used for this purpose, but they are subjective and do not scale for the emerging large-scale archival collections of human decomposition photos. This study explores the feasibility of automating two common human decomposition scoring methods proposed by Megyesi and Gelderman using artificial intelligence (AI). We evaluated two popular deep learning models, Inception V3 and Xception, by training them on a large dataset of human decomposition images to classify the SOD for different anatomical regions, including the head, torso, and limbs. Additionally, an interrater study was conducted to assess the reliability of the AI models compared to human forensic examiners for SOD identification. The Xception model achieved the best classification performance, with macro-averaged F1 scores of .878, .881, and .702 for the head, torso, and limbs when predicting Megyesi's SODs, and .872, .875, and .76 for the head, torso, and limbs when predicting Gelderman's SODs. The interrater study results supported AI's ability to determine the SOD at a reliability level comparable to a human expert. This work demonstrates the potential of AI models trained on a large dataset of human decomposition images to automate SOD identification.", "sections": [{"title": "1. Introduction", "content": "Determining the stage of decay (SOD) is an important and common task in human remains cases. Knowing the degree of decomposition is vital for estimating the postmortem interval (PMI) and identifying human remains [1, 2, 3, 4]. Presently, establishing the SOD of a decedent is primarily conducted manually, via visual assessment by trained experts using non-metric scoring or staging methods, such as those proposed by Megyesi et al. [1] and Gelderman et al. [2]. Such non-metric methods, which rely on subjective interpretation made by humans, possess a higher susceptibility to human bias and error, consequently affecting the accuracy of downstream tasks, such as estimating the PMI [5, 6, 7, 8]. Furthermore, the PMI estimation formulas derived in existing studies, such as those by Megyesi et al. [1] and Gelderman et al. [2], were developed using a very small number of samples. Evaluating or improving upon these formulas with a much larger sample size, such as over one million photos, would require manual SOD scoring, which is not feasible for such a large sample. Therefore, this work aims to utilize emerging artificial intelligence (AI) methods to evaluate the feasibility of automating the SOD identification task.\nAI is the ability of machines to perform tasks that would typically require human intelligence and it has provided innovative approaches to assist in human decision-making [9, 10]. AI assesses information based on the entirety of acquired facts or data using advanced algorithms, thereby mitigating vulnerability to the subjectivity and biases that trouble humans and affect their decision-making abilities [11]. Additionally, AI algorithms can handle large volumes of data, uncovering intricate patterns that might elude human perception [9, 12, 13]. This ability to process, analyze, and interpret large amounts of data quickly and precisely makes AI a valuable tool in many industries, including forensic practice and research.\nIn summary, the objective of this study is to evaluate the possibility of automating two established human decomposition scoring methods, namely Megyesi et al. [1] and Gelderman et al. [2], using vision-based AI models, known as convolutional neural networks (CNNs). Specifically, various CNN classification models will be trained and evaluated on a large human decomposition image dataset to perform SOD prediction. In addition, an interrater test is conducted to assess and compare the reliability of the models and the human forensic examiners for SOD identification. We hypothesize that similar interrater reliability among human raters and an AI rater suggests the feasibility of using AI for SOD classification and, perhaps, other downstream tasks. The significance of such a finding lies in the potential to develop more accurate SOD and PMI estimation methods that are less effort-intensive and subjective. The primary purpose of this study is to provide a proof-of-concept for the future advancement and integration of AI-assisted analysis in forensic practice and research."}, {"title": "2. Materials and methods", "content": "2.1. Stage of decay scoring methods\nThe two human decomposition scoring methods this study attempts to automate using AI are: (1) Megyesi et al. [1] and (2) Gelderman et al. [2]. To account for the differential decomposition that occurs in different body segments (e.g., limbs do not bloat or purge fluid), these two scoring methods independently assess the human body in three anatomical regions: (1) the head (including the neck), (2) the torso, and (3) the limbs (including the hands and feet). Based on the morphological features present, Megyesi et al. [1] categorizes human decomposition into four high-level linear stages: fresh, early decomposition, advanced decomposition, and skeletonization. Gelderman et al. [2], building upon the work of Megyesi et al. [1], categorizes each anatomical region into six stages, with the lowest indicating no visible changes and the highest indicating complete skeletonization.\n2.2. The human decomposition dataset\nThe human decomposition dataset, a large-scale image collection used to train the models, includes images of decomposing corpses donated to [removed for double anonymized review]. The center houses [removed for double anonymized review]. Forensic experts from the [removed for double anonymized review] captured these images at non-uniform intervals, with one or more days between each capture. The images, taken from various angles, depict different anatomical areas to illustrate the various stages and regions of human decomposition. The image resolutions vary from 2400\u00d71600 pixels up to 4900\u00d73200 pixels. The dataset covers the period from 2011 to 2023 and comprises over 1.5 million images contributed by more than 800 donors. To train different CNN classifiers on this large human decomposition image dataset to predict the SOD for various anatomical regions, the following challenges needed to be addressed:\n\u2022 How to best sample from the entire human decomposition dataset such that the resulting set of images reflects the dataset's temporal characteristics. In other words, the data used to train the models should consist of images covering the entire human decay process, that is, from death to skeletonization.\n\u2022 Once a set of images has been sampled, the challenge is to automate the efficient extraction of specific anatomical regions (i.e., head, torso, and limbs). From a time- and cost-effective perspective, it is not feasible to manually perform this body part filtering of over one million images.\nThe following section (Section 2.3) details how these analysis challenges were addressed during data preparation.\n2.3. Data processing and labeling\nThe human decomposition dataset was processed according to the data pipeline shown in Figure 1. The remainder of this section further discusses the individual steps of this pipeline.\nThe quality and size of the data used to train the models highly affect a model's performance and generalizability. In other words, the more representative and diverse the training data is, the more likely it is that the model will be able to generalize. In the case of a temporal dataset, such as images documenting human decomposition, where the subjects' appearance changes over time, it is important to sample the training data in a way that reflects the dataset's characteristics. Therefore, images depicting all possible decomposition stages should be included in the training data. As a result, a small sample of donors, and all images of those donors, was selected over time (i.e., from when they first started to decay until fully decomposed), instead of randomly selecting images from the entire human decomposition dataset. Since this subset of images included all different kinds of anatomical areas, a previously developed body part classification model was used to automatically detect the head, torso, and limbs images to align with Megyesi et al.'s [1] and Gelderman et al.'s [2] scoring methods. The next step was to manually label this subset of images with the SOD labels. Specifically, the two scoring methods, described in Section 2.1, were used by a forensic expert to perform manual data labeling of the head, torso, and limbs images. Note that any body part misclassifications and/or poor quality images were either corrected or removed at this point to ensure that the final labeled datasets only included high quality images of the head, torso, and limbs. Table 1 shows the SOD terms used in the original literature (i.e., Megyesi et al. [1] and Gelderman et al. [2]) and the corresponding new class labels used throughout this study. For instance, if the forensic expert, applying Megyesi et al.'s [1] method, determined the SOD of a head image to be fresh, then it was assigned the M-SOD1 class label."}, {"title": "2.4. AI model development", "content": "To build the SOD classifiers, transfer learning was applied, which aims to produce effective models by leveraging and exploiting previously acquired knowledge [14]. In particular, two CNN architectures, including Inception V3 [15] and Xception [16], pre-trained on the ImageNet dataset [17], were trained using the following two-step transfer learning process: (1) freeze all pre-trained convolutional layers of the base model and train newly added classifier layers and (2) unfreeze all layers and fine-tune the model end-to-end with a low learning rate. The newly added classifier layers consisted of five layers, including one global average pooling layer and one drop-out layer (rate = 0.3) to alleviate the over-fitting problem motivated by Lin et al. [18], followed by two fully-connected layers with 128 and 64 nodes performing down-sampling, and one final softmax layer with the number of nodes equal to the number of classes for multi-class classification. More precisely, the softmax layer transforms the output of the previous layer into a probability distribution over all the classes as shown by the equation (1), with the class having the highest probability being the final SOD prediction. In equation (1), zi is the ith element of the input vector to the softmax function and K the total number of classes. To increase the size and diversity of the data, a data augmentation layer was added after the input layer, performing random image flipping (horizontal and vertical) and rotation during model training. \n\nsoftmax(zi) = \\frac{e^{zi}}{\\sum_{j=1}^{K} e^{j}} \\quad \\text{for } i = 1, 2, ..., K  \\tag{1}\n\nThe loss function used was Cross-Entropy loss, which takes the predicted probability distribution of the softmax layer and measures how well this distribution matches the true distribution. To minimize the Cross-Entropy loss function during training, the Adaptive Momentum Estimation (Adam) optimizer was employed, with a learning rate of 0.001 for the first step and 0.0001 for the second step in the two-step transfer learning process. It is worth noting that training the Inception V3 and Xception architecture from scratch (i.e., without transfer learning), transfer learning without freezing the base model (i.e., one-step transfer learning), and freezing only a certain number of base model layers instead of all followed by fine-tuning was also tested. However, the proposed two-step transfer learning process significantly improved model performances.\nTo evaluate the performance of the trained SOD classifier models, the confusion matrix on the test data was calculated per class, which summarizes a model's performance by comparing its predicted labels to its true labels. Specifically, the confusion matrix shows the number of correct predictions, such as the true positives (TP) and the true negatives (TN), as well as the number of incorrect predictions, such as the false positives (FP) and the false negatives (FN). The following two performance metrics were then calculated from the per-class confusion matrix: precision (2) and recall (3). Precision measures the accuracy of the positive predictions, while recall measures the completeness of the positive predictions.\nprecision = \\frac{TP}{TP+FP} \\tag{2}\nrecall = \\frac{TP}{TP+FN} \\tag{3}\nTo combine the per-class precision and recall metrics into a single model evaluation metric, the macro-averaged F1 score (mF1) (4) was calculated and reported, which is the unweighted mean of the per-class F1 scores (5).\nmF1 = \\frac{\\sum_{i=1}^{n} F1i}{n} \\quad \\text{for } i = 1, 2, . . ., n, \\text{where n is the number of classes} \\tag{4}\nF1 = \\frac{2 * (Precision * Recall)}{Precision + Recall} \\tag{5}"}, {"title": "2.5. Interrater test", "content": "To ensure that manual rating techniques are reliable, interrater reliability tests are often used to assess how similar the ratings are between two or more raters on the same set of data samples. In this study, it is also used to assess the reliability of the developed models. The interrater test involved multiple raters (both human and AI) labeling a set of images depicting the same anatomical region using Megyesi et al.'s [1] and Gelderman et al.'s [2] scoring methods. Specifically, 300 torso images that were not used during model development were selected. Due to limited resources, the interrater test focused on the torso only, which provides a good amount of variability. The raters included the developed torso SOD model or Model and three forensic experts well-versed in the considered scoring methods, including Human 1 (the same human who labeled the data the models were trained with), Human 2, and Human 3. The task for each rater was to label the 300 images using once Megyesi et al.'s [1] method and once Gelderman et al.'s [2] method. The Model rater performed labeling by predicting the SOD of the 300 images, while the human raters were instructed to independently label the 300 images on [removed for double anonymized review], following a similar set-up used for data labeling in Section 2.3. To ensure randomization across methods, human labeling was conducted in batches of images instead of labeling all 300 images with one method and then the other. Specifically, the human raters were presented with batches of 50 images at a time, which they were asked to label using one scoring method (i.e., Megyesi et al. [1] or Gelderman et al. [2]). This process was repeated until all 300 images were labeled with both scoring methods, resulting in 12 iterations for each human rater ((300 images / 50 images) \u00d7 2 methods). Note, the method used to label a batch of images was alternated between the two scoring methods.\nAfter each rater completed labeling, two types of agreements were assessed: (1) human-human agreement (i.e., the agreement among all three human raters: Human 1, Human 2, and Human 3) and (2) AI-human agreement (i.e., the agreement among the Model replacing Human 1, and the other two human raters, Human 2, and Human 3). The reason for the Model replacing Human 1 was to see how the agreement changes when the human is replaced by the model trained on the data they labeled. To measure the different agreements, the Fleiss' Kappa statistic was used, which measures the degree of agreement among raters over what would be expected by random chance, with values ranging from -1 (no agreement) to 1 (perfect agreement). The Fleiss' Kappa values will be interpreted based on Landis and Kock's [19] interpretation criteria shown in Table 3."}, {"title": "3. Results", "content": "3.1. SOD classification\nThe SOD classifier models were implemented using Keras and TensorFlow, two open-source machine learning modules written in Python. In particular, the models were built and evaluated on the six manually labeled datasets shown in Table 2. All images were resized to 299\u00d7299 pixels, as that is the required input image size for both the Inception V3 and Xception architectures. All models were trained on a single Tesla V100-SXM2 GPU with 32GB of memory. The batch size was set to 32, and the number of epochs was set to 200, with early stopping set to 20 epochs to avoid over-fitting on the training set.\nTable 4 presents the Megyesi et al. [1] SOD classification results on the test data. For each anatomical region, the best SOD classification performance, as indicated by the mF1 scores, was achieved with the Xception architecture. The head SOD classifier achieved an mF1 score of .878, and the torso SOD classifier achieved an mF1 score of .881, performing comparably. However, the limbs SOD classifier performed notably less well, with an mF1 score of .702.\nTable 5 presents the Gelderman et al. [2] SOD classification results on the test data. Similar to the Megyesi et al. [1] results, the best classification performance, as indicated by the mF1 scores, was achieved by the Xception architecture across all anatomical regions. The head SOD classifier achieved an mF1 score of .872, and the torso SOD classifier achieved an mF1 score of .875, performing comparably. However, the limbs SOD classifier performed less well, with an mF1 score of .76.\n3.2. Interrater test\nThe interrater test results are shown in Table 6. Specifically, a Fleiss' Kappa analysis using SPSS Statistics was conducted for both the human-human agreement and AI-human agreement across both scoring methods. The reported agreement levels were determined using Landis and Kock's [19] interpretation criteria, as shown in Table 3. According to the Megyesi et al. [1] results, the Fleiss' Kappa coefficient of the human-human agreement was .67 with a p-value < .001 and a 95% confidence interval (CI) of .628 to .713, indicating substantial agreement. Similarly, the Fleiss' Kappa coefficient of the AI-human agreement was .637 with a p-value < .001 and a 95% CI of .594 to .68, suggesting substantial agreement. Additionally, according to the Gelderman et al. [2] results, the Fleiss' Kappa coefficient of the human-human agreement was .593 with a p-value < .001 and a 95% CI of .558 to .628, indicating moderate agreement. Similarly, the Fleiss' Kappa coefficient of the AI-human agreement was .558 with a p-value < .001 and a 95% CI of .524 to .592, suggesting moderate agreement. In all cases, the Fleiss' Kappa coefficient was statistically significant (i.e., the p-value < 0.05)."}, {"title": "3.3. Discussion", "content": "Overall, the SOD classification results are promising. The Xception architecture performed the best across both scoring methods. The head and torso SOD models performed comparably well; however, the limbs SOD models' performances were not as strong. Further analysis of the limb data revealed that some images included hands and/or feet covered by a net (to prevent them from being scattered/disarticulated by animal scavengers), which could confuse and distract the model, leading to incorrect predictions. Additionally, the examination of the limbs data indicated that some images included other parts of the body, specifically the torso. Since the torso decays differently than the limbs, this inclusion could again confuse the model and, consequently, affect its predictions. Future work will focus on addressing these data quality challenges to ensure the development of a more reliable limbs dataset and hence improved prediction performance. Another important finding to mention is that although the Gelderman et al. [2] datasets contained more SOD classes (six classes) than the Megyesi et al. [1] datasets (four classes), the classification performances were comparable. This indicates that these AI models are able to learn a higher number of decay stages without decreasing prediction performance.\nWhile the SOD prediction performances were promising overall, there is room for improvement, as indicated by the per-class precision and recall values. The sizes of the labeled datasets used in this study are considered rather small for training deep learning architectures, such as Inception V3 and Xception. Additionally, a deeper analysis of the labeled datasets indicated class imbalance (i.e., a disproportionate number of instances of one class compared to another). Having a larger, more diverse, and evenly balanced dataset will make the models more robust and improve their generalization capabilities, which refers to how well a model can react to new and unseen data. However, creating more labeled data where domain expertise is required may be limited by both resource and time constraints. A recent study [removed for double anonymized review] addressed such challenges by developing a domain-aware label propagation algorithm that leverages different image attributes to automatically perform data labeling, thereby reducing manual labeling efforts and costs. Future work will explore integrating such label propagation methods to obtain larger and more diverse datasets, aiming to create more robust and accurate SOD classification models.\nIn the interrater test, both the human-human and AI-human agreements showed substantial agreement when applying the Megyesi et al. [1] scoring method. Using the Gelderman et al. [2] scoring method, moderate agreement was observed for both the human-human and AI-human agreements. Across both scoring methods, the level of agreement for both the human-human and AI-human agreements was the same. This means that when the human rater was replaced with the AI model rater, the level of reliability stayed the same, supporting AI's ability to perform SOD identification with a reliability level comparable to that of an experienced human forensic examiner. Notably, the lower agreement level for the Gelderman et al. [2] method could be attributed to (1) its novelty in the field and/or (2) its complexity, being more complicated to apply, as it involves six decay stages compared to the four stages in the Megyesi et al. [1] method.\nWhile the results demonstrate that human decomposition scoring methods have the potential to be automated using AI techniques, there are some important limitations of this work. For one, the data labeling was conducted by a single forensic expert. This approach may introduce labeling bias, which can lead to inherently biased training datasets. Models trained on such datasets can inherit these biases, resulting in biased models [20]. Therefore, future work will focus on creating a so-called \u201cgold standard\u201d dataset a labeled dataset meticulously crafted and evaluated by multiple forensic experts. Such a dataset would be accepted as the most accurate and reliable of its kind. This step will ensure that the models are trained with accurate and unbiased data, which is vital for developing high-quality models.\nAn additional limitation is that this study is environmental- and climate-specific. The images used to train the models were all taken of donors decaying outdoors in an open-wooded area with the ground consisting of soil, gravel, and dead/decaying plant matter (e.g., rotting wood and shedding leaves). Additionally, the climate of this area is humid subtropical, characterized by high summer and moderate winter temperatures. Therefore, the models may not perform as well on images taken in different climate conditions or environments, necessitating additional training or re-training with images specific to those conditions."}, {"title": "4. Conclusion", "content": "This study explored the possibility of automating two common human decomposition scoring methods, namely Megyesi et al. [1] and Gelderman et al. [2]. Specifically, different CNN models, including Inception V3 and Xception, were trained on a large human decomposition image dataset to classify the SOD for different anatomical regions. Across both scoring methods, the Xception model achieved the highest classification results, performing comparably well for the head and torso, and slightly lower for the limbs. The interrater reliability study results provided support for AI's ability to automate the SOD identification task at a reliability level comparable to a human expert. Overall, the study results are promising and provide a proof-of-concept for automating human decomposition scoring methods using AI."}]}