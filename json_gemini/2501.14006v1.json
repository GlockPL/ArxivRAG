{"title": "Asymmetrical Latent Representation for Individual Treatment Effect Modeling", "authors": ["Armand Lacombe", "Mich\u00e8le Sebag"], "abstract": "Conditional Average Treatment Effect (CATE) estimation, at the heart of counterfactual reasoning, is a crucial challenge for causal modeling both theoretically and applicatively, in domains such as healthcare, sociology, or advertising. Borrowing domain adaptation principles, a popular design maps the sample representation to a latent space that balances control and treated populations while enabling the prediction of the potential outcomes. This paper presents a new CATE estimation approach based on the asymmetrical search for two latent spaces called Asymmetrical Latent Representation for Individual Treatment Effect (ALRITE), where the two latent spaces are respectively intended to optimize the counterfactual prediction accuracy on the control and the treated samples. Under moderate assumptions, ALRITE admits an upper bound on the precision of the estimation of heterogeneous effects (PEHE), and the approach is empirically successfully validated compared to the state of the art.", "sections": [{"title": "Introduction", "content": "In the Neyman-Rubin framework Rubin (2005), causal inference focuses on a simple question: how different would the outcome have been if the treatment had been different? Answering this question, however, raises considerable difficulties, as the true answer is inevitably unknown: the i-th individual either is treated (with outcome $y_i^1$) or not (with outcome $y_i^0$). In both cases, observation of the difference $y_i^1 - y_i^0$ remains inaccessible. Overall, the estimation of the individual causal effect, acknowledged as the fundamental problem of causal inference (Holland, 1986), addresses a machine learning problem with missing values.\nA most natural approach consists in independently modeling outcome $Y^1$ (respectively $Y^0$) from the treated (resp. untreated, or control) samples as functions of their covariate description $X$, allowing the individual causal effect to be estimated from the difference of both models. This approach is valid subject"}, {"title": null, "content": "to a key assumption, stating that treated and control samples are drawn from the same distribution $P(X)$; in other words, the treatment assignment must follow the randomized control trial methodology.\nIn real-life applications however, this assumption rarely holds true. In the medical field the treatment variable $T$ is generally assigned by the doctor, depending on the severity of the individual's condition. In the field of education, the decision of e.g. following a curriculum depends on the individual's background. In such cases, the treated and control distributions $P_1(X) = P(X|T = 1)$ and $P_0(X) = P(X|T = 0)$ are different.\nEstimating the potential outcome models based on the joint distributions $P(X, Y^1)$ and $P(X, Y^0)$ can thus be cast as a learning across domain problem (Ben-David et al., 2006; Ganin et al., 2016), where the two support distributions $P(X|T = 0)$ and $P(X|T = 1)$ differ. Building upon the state of the art (Ben-David et al., 2006), it thus comes naturally to integrate the space of original covariates into a latent space aligning both support distributions (Johansson et al., 2016; Shalit et al., 2017) (more in Section 2).\nA main claim of the paper is that the CATE problem however significantly differs from the mainstream learning across domain setting. Specifically, estimating $P(Y^1|X).P(X)$ from $P(Y^1|X).P_1(X)$ requires all control samples to be 'sufficiently close' from some treated samples (A). Likewise, estimating $P(Y^0|X).P_1(X)$ from $P(Y^0|X).P_0(X)$ requires all treated samples to be 'sufficiently close' from some control samples (B). The difference between these requirements is illustrated on Fig. 1: the leftmost and rightmost images correspond to criteria (A) and (B) respectively, while the center image corresponds to a representation suitable for learning across domains.\nThe main contribution of the present paper is to formalize the intuition that CATE estimation involves two intertwined learning across domain problems, appealing to different requirements. The presented analysis motivates the design"}, {"title": "State of the art", "content": "As large datasets become increasingle available, they support the estimation of causal effects at individual or group level, reflecting the heterogeneity of treatment impacts. The state of the art commonly distinguishes several categories of CATE learners, depending on the main features of the models and notably how these models account for the treatment assignment variable K\u00fcnzel et al. (2019); Nie and Wager (2021); Kennedy (2023).\n2.1 S-learners\nS(ingle)-learners fit a single model with treatment assignment $T$ and covariates $X$ as inputs. The potential outcome models are learned along classical supervised learning, with:\n$\\mu(x, t) \\approx \\mu^t(x) = E[Y|X = x,T = t]$\nand the treatment effect is defined as: $f(x) = \\mu(x, 1) - \\mu(x, 0)$. In the Balancing Neural Network (BNN) approach Johansson et al. (2016), an embedding is used to map the covariate space onto a latent representation, merging the (image of) control and treatment distributions along the domain adaptation principles (Ben-David et al., 2006). The potential outcome $\\hat{\\mu}(x, t)$ is sought as a single neural net, operating on the concatenation of f(x) and the treatment variable t. However, the fact that the treatment assignment variable is given no particular role tends to bias the CATE estimate toward 0 according to K\u00fcnzel et al. (2019).\nBayesian Additive Regression Trees (BART) are built along the same prin-ciples, using regression trees instead of neural nets; in Athey and Imbens (2016), the BART approach is extending to yield confidence intervals. In Wager and Athey (2018), BART is learned using Causal Forests (CF) where the last split of the forest trees corresponds to the treatment assignment.\n2.2 T-learners\nT(wo)-learners differ from S-learners in that they learn different models for both potential outcomes. Inspired by domain adaptation Johansson et al. (2022),"}, {"title": null, "content": "Shalit et al. (2017) define a single latent space for both control and treated samples (more in Caron et al. (2022)). On this latent space, two independent functions are learned to estimate the potential outcomes:\n$\\mu^0(x) \\approx \\mu^0(x) = E[Y|X = x, T = 0]$\n$\\mu^1(x) \\approx \\mu^1(x) = E[Y|X = x,T = 1]$\nwith the individual treatment effect being likewise defined as their difference ($\\hat{\\tau}(x) = \\mu^1(x) - \\hat{\\mu}^0(x)$). In Shalit et al. (2017), the BNN S-learner is extended to the T-learner setting. The embedding $\\phi$ is trained to minimize the Wasserstein distance (Cuturi, 2013) or the Maximum Mean Discrepancy (Gretton et al., 2012) between the image of the control and treatment distributions. The impact thereof is inspected by contrasting CFR (constrained distance) and TARNet (unconstrained). Other approaches aim to preserve local similarity information in the latent space, such as SITE Yao et al. (2018) or ACE (Yao et al., 2019). CFR-ISW (Hassanpour and Greiner, 2019a) and BWCFR Assaad et al. (2021) use context-aware weights, reweighting samples in the latent space according to their estimated propensity scores. MitNet (Guo et al., 2023) differs from CFR by relying on the mutual information between control and treatment distribution, with the advantage that it handles non-binary treatment settings. StableCFR (Wu et al., 2023a) bridges the gap between T-learners and matching methods (Stuart, 2010): minority distribution is up-sampled using nearest-neighbor approaches in latent space.\nTaking inspiration from adversarial domain adaptation (Ganin et al., 2016), ABCEI (Du et al., 2021) leverages the mutual information between the observed and the latent representation to limit the loss of information; CBRE (Zhou et al., 2021) considers an auto-encoder architecture with a specific cycle structure: the loss of information due to the latent representation is prevented by requiring this latent information to support the reconstruction of the samples.\nGenerative models are also leveraged for CATE. CEVAE (Louizos et al., 2017) combines the approach in Shalit et al. (2017) with a Variational Auto-Encoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014); GANITE (Yoon et al., 2018) is based on Generative Adversarial Networks (GAN) (Good-fellow et al., 2014). NSGP (Alaa and Schaar, 2018) and DKLITE Zhang et al. (2020) are based on Gaussian processes, enabling to minimize counter-factual variance and to provide uncertainty intervals.\nRefined neural architectures distinguish confounding variables (covariates that cause both T and $Y^0, Y^1$), and adjustment variables (causes of $Y^0, Y^1$ only), in a linear (D2VD (Kuang et al., 2017)) or non-linear setting (N-D\u00b2VD (Kuang et al., 2022)). DR-CFR (Hassanpour and Greiner, 2019b) improves on such refined architectures, by introducing one latent representation for instrumental variables (causes of T only), one for confounding variables, and one for adjustment variables. The latent space is trained in various ways: by minimizing the MMD between the latent adjustment representation of control and treated samples"}, {"title": null, "content": "(Hassanpour and Greiner, 2019b), by leveraging Contrastive Log-Ratio Upper Bound (Cheng et al., 2020) in MIM-DRCFR (Cheng et al., 2022); by using a deep orthogonal regularizer in DeR-CFR (Wu et al., 2023b); by enforcing the disentanglement using adversarial learning Chauhan et al. (2023). The latent structure is combined with a variational approach in TEDVAE (Zhang et al., 2021); it is yet further refined in SNet (Curth and Schaar, 2021), distinguishing adjustment factors causing $Y^0$ only, $Y^1$ only, and both of them.\n2.3 Other approaches\nX-learners K\u00fcnzel et al. (2019); Stadie et al. (2018); Curth and Schaar (2021) involve a two-step process. In the first step, models of the response functions $\\mu^0, \\hat{\\mu}^1$ and propensity $\\hat{\\eta}$ are learned. In the second step, two CATE estimates are trained: $\\hat{\\tau}^1(x_i) \\approx y - \\mu^0(x_i)$ is optimized on treated samples, while $\\hat{\\tau}^0(x_i) \\approx \\mu^1(x_i) - y$ is optimized on control ones. Finally, the CATE estimate for any given sample is obtained as\n$\\hat{\\tau}_x(x) = (1 - \\hat{\\eta}(x))\\hat{\\tau}_0(x) + \\hat{\\eta}(x)\\hat{\\tau}_1(x)$\nR(obinson)-learners Nie and Wager (2021) extend the CATE typology de-fined by K\u00fcnzel et al. (2019), building upon the Robinson's potential outcome formalization (Robinson, 1988):\n$Y - E[Y|X] = (T - E[T|X])\\tau(X) + \\epsilon$\nwith $\\epsilon$ a centered noise variable. Like X-learners, R-learners proceed along a two-stage approach: they first learn $m(x) \\approx E[Y|X = x]$ and $\\hat{\\eta}(x) \\approx E[T|X = x]$, and in a second stage the CATE estimate $\\hat{\\tau}_R$ is learned by minimizing:\n$\\sum_i (Y_i - m(x_i) - (t_i - \\hat{\\eta}(x_i))\\hat{\\tau}_R(X_i))^2$\nUsing cross-fitting training procedures, this method provably achieves an optimal convergence rate, i.e., as if the true $\\eta$ and $m$ were known.\nRelated approaches include F-learners (K\u00fcnzel et al., 2019) and U-learners (Signorovitch, 2007; Athey and Imbens, 2016; Curth and Schaar, 2021) using other decompositions of the potential outcome models: DR-learners Foster and Syrgkanis (2023); Kennedy (2023) introduce a double robustness methodology (Chernozhukov et al., 2017); B-learners (Oprescu et al., 2023) generalize them in settings with a limited amount of unobserved confounders; and IF-learners (Curth et al., 2021a) leverage efficient influence functions (Hampel et al., 1986) to enforce double robustness.\n2.4 Discussion\nThe state of the art shows the value of a change of representation for tackling CATE; X-learners, unable to align the control and processing distributions, are at a disadvantage."}, {"title": null, "content": "The search for a latent space faces two difficulties. On one hand, there is no consensus about how to align both distributions (using ad hoc penalization terms (Johansson et al., 2016), MMD Shalit et al. (2017), Wasserstein distance, adversarial learning Du et al. (2021)). On the other hand, the latent space is meant to enforce this alignment for both potential outcome models, and must thus achieve some trade-off between both.\nLastly, Zhang et al. (2020) suggests that the latent space should yield a low counter-factual variance (as opposed to, aligning the control and treated distri-butions). Formally, high variance on the counter-factual posterior distribution suggests that there is not enough information in the considered region regarding the counter-factual modeling task. How to evaluate the counter-factual variance outside of the Bayesian framework, however, remains an open question."}, {"title": "Asymmetrical Latent Regularization for Individual Treatment Effect Modeling", "content": "This section first presents the intuition underlying the proposed ALRITE approach. After an overview of ALRITE, the theoretical analysis of the approach, upper bounding the estimation error under mild assumptions, is described and its scope is discussed.\n3.1 Intuition\nFollowing the above discussion, our claim is that CATE must define two latent spaces, allowing counterfactual modeling for both control and treatment samples, i.e. with low counterfactual variance for each distribution. With no loss of generality let us characterize the counter-factual variance w.r.t. treated samples, with an embedding $\\phi$ from input space X onto latent space Z.\nGiven $(x_i, t_i = 1, y_i)$ a treated sample, the aim is to estimate $E[Y^1 - Y^0|X = x_i]$. Under the assumption that $\\phi$ is injective, this estimate coincides with $E[Y^1 - Y^0|\\phi(X) = \\phi(x_i)]$ (the injectivity requirement is relaxed in section 3.5). The variance of the counter-factual estimate for $(x_i, t_i = 1, y_i)$ depends on how far $\\phi(x_i)$ is from $\\phi(x_j)$ for $x_j$ among the control samples.\nAs illustrated in Fig. 3, when a treated point is away from control points in latent space, estimating its counter-factual outcome can be viewed as an out-of-distribution estimation problem. The estimate can be arbitrarily inaccurate (unless strong assumptions, e.g., linearity or high smoothness, are made on the potential outcome models). The high uncertainty on the counter-factual estimate is all the greater the higher the dimension of covariate X and the smaller the number n of samples (as is generally the case). This suggests that no treated (respectively, control) sample should be isolated from the control (resp., treated) samples in latent space."}, {"title": "Overview of Alrite", "content": "3.2.1 Counter-factualizability\nExtending (Johansson et al., 2016; Wu et al., 2023a), with $\\phi$ an embedding from the covariate space X onto latent space Z we define the latent mirror twin of sample $(x_i, t_i, y_i)$ noted $(x_i^m, 1 - t_i, y_i^m,\\phi)$ as the nearest sample with different treatment assignment in latent space:\n$(x_i^m, 1 - t_i, y_i^m) = \\text{argmin}\\{\\|\\phi(x_i) - \\phi(x_j) \\|, (x_j, t_j = 1 - t_i, y_j) \\in D\\}$\nwhere the superscript $\\phi$ is omitted when clear from the context.\nThe counter-factualizability of $x_i$ is defined as its Euclidean distance in latent space to its latent mirror twin $(\\|\\phi(x_i) - \\phi(x_i^m)\\|)$: the smaller the better. As said, $x_i^m$ is used to estimate the counter-factual outcome for its twin $x_i$. Accordingly, a sample that is mirror twin for several other samples matters more for counter-factual estimation, everything else being equal. This intuition is formalized by defining the counter-factual importance weight, noted $w_j$, as:\n$w_j = \\#\\{i \\in [1,n] \\text{ s.t. } (x_i^m,\\phi, 1 - t_i, y_i^m,\\phi) = (x_j, t_j, Y_j)\\}$\nThe notions of latent mirror twin, and counterfactual importance weight are used to estimate the quality of the latent space. Informally speaking, a latent space allowing good counterfactual estimation of the treated samples is such that: i) the treated examples are close to their mirror twin; ii) the $\\mu^0(x) \\approx E[Y|T = 0, X = x]$ model learned on this latent space is of good quality, especially for control x; with a high $w_j$ (since the quality of $\\mu^0(x_j)$ has an impact on the counterfactual estimation of many treated $x_i$)."}, {"title": "Model architecture", "content": "As said (Section 3.1), the accurate counter-factual estimate of samples requires these samples to be counter-factualizable, i.e. close to their mirror twins. As depicted on Fig. 1 however, the requirements of control and treated samples being counter-factualizable are not necessarily satisfied by the same change of representation.\nConsequently, we consider two latent spaces, aimed at ensuring counterfac-tualization of treated and control samples. More formally, the treatment-driven pipeline $P_1$ focuses on CATE for treated samples, while the control-driven pipeline $P_0$ focuses on CATE for control samples.\nThe combination of the CATE estimates built from $P_0$ and $P_1$ is classically defined as:\n$\\hat{\\tau} : x \\in X \\mapsto (1 - \\hat{\\eta}(x))\\tau_0(x) + \\hat{\\eta}(x)\\tau_1(x)$\nwith$\\hat{\\eta}(x)$ estimating the propensity score $P(T = 1|X = x)$ and $\\tau_0$ and $\\tau_1$ denoting the CATE estimates derived from pipelines $P_1$ and $P_0$.\nNote that ALRITE bridges the gap between T-learners and X-learners: each pipeline ($P_0$ and $P_1$) defines a T-learner and a causal estimate; these causal estimates are combined as in X-learners (Eq. 4)."}, {"title": "Training loss", "content": "Let us present the loss used to train control pipeline $P_0= (\\phi, h^0, h^1)$ (the $P_1$ loss follows by symmetry). $P_0$ is trained end-to-end by minimizing a compound"}, {"title": null, "content": "loss enforcing: i) the accuracy of model $h^0$ on the control samples; ii) the accuracy of model $h^1$ on the treated samples (accounting for the fact that control samples with high counterfactual importance weight matter more); iii) the good counter-factualizability of control samples (a small distance to their mirror twin); iv) the regularization of the whole pipeline vector weight noted $\\|P_0\\|$:\n$L(P_0) = \\frac{1}{n_0} \\sum_{t_i=0} (Y_i - h^0(\\phi(x_i)))^2 + \\frac{1}{n_1 + \\beta n_0}\\sum_{t_i=1} (1 + \\beta w_i) (Y_i - h^1(\\phi(x_i)))^2 + \\alpha \\frac{1}{n_0} \\sum_{t_i=0} \\|\\|\\phi(x_i) - \\phi(x_i^m)\\|\\|^2 + \\gamma \\|\\|P_0\\|\\|^2$\nwith $n_0$ and $n_1$ the number of control and treated samples in D, $w_i$ the counter-factual importance weight of $x_i$, and $\\alpha, \\beta, \\gamma$ hyper-parameters of the approach.\n3.3 Algorithm\nThe ALRITE algorithm finally involves three modules: learning $P_0$ (Eq. 5); learning $P_1$; learning the propensity estimate $\\hat{\\eta}$. Finally, the CATE estimates derived from $P_0$ and $P_1$ are aggregated using $\\hat{\\eta}$ (Eq. 4)."}, {"title": null, "content": "As said, the propensity score $\\hat{\\eta}$ can be learned using classical supervised learning. Formally, a cross-entropy loss is used:\n$L(\\hat{\\eta}) = \\frac{1}{N_1} \\sum_{t_i=1} log(\\hat{\\eta}(x_i)) \\frac{1}{n_0} \\sum_{t_i=0} log(1 - \\hat{\\eta}(x_i)) + \\|\\hat{\\eta}\\|^2$\nwith $\\|\\hat{\\eta}\\|^2$ a regularization term. It is desirable that $\\hat{\\eta}$ be calibrated (Zadrozny and Elkan, 2002), i.e. such that\n$\\forall s \\in [0, 1], E[T = 1|\\hat{\\eta}(X) = s] = s$\nHowever the sensitivity of the overall $\\hat{\\tau}$ (defined by aggregating the CATE estimates $\\tau_0$ and $\\tau_1$ respectively derived from $P_0$ and $P_1$, Eq. 4) w.r.t. the propensity score is limited. Let $\\tau_{\\eta}$ denote the aggregation of $\\tau_0$ and $\\tau_1$ using the ground truth $\\eta$. It reads:\n$\\|\\hat{\\tau} - \\tau_{\\eta}\\| = \\|( \\eta - \\hat{\\eta}) (\\tau_0 - \\tau_1)\\|\n$\\leq \\|\\eta - \\hat{\\eta}\\| \\times (\\|\\tau_0 - \\tau\\| + \\|\\tau_1 - \\tau\\|)$\nIn other words, the CATE error due to the error on $\\eta$ is of order 2, as the product of i) the error on the propensity; ii) the error on $\\tau_0$ and on $\\tau_1$.\nRemark: It is straightforward to define an ensemble variant of ALRITE, by splitting the dataset into a training, a validation, and a test dataset. On the training subset, several pipelines $P_0$ and $P_1$ are learned using different hyper-parameter settings. The best pipelines (determined from their factual accuracy on the validation set) are selected and they are aggregated to form the ensemble CATE model. The aggregation is the average of the top-K out of all pipelines, with K a hyper-parameter of the ensemble approach, or a weighted sum, where the weight of each model corresponds to a softmax of hyper-parameter $\\Lambda$ (Appendix C).\n3.4 Formal guarantees\nAs said, a main contribution of the approach is to provide formal guarantees on the PEHE error of ALRITE, considering the within-sample setting, i.e. when the treatment variable T is known. Remind that the within-sample error is not trivial -- contrarily to the training error in supervised learning -- as counter-factual $Y^{1-T}$ is not observed. All proofs are given in Appendix A.1. Our first result states that the PEHE loss associated with a T-learner is upper bounded depending on the counter-factualizability of the samples.\nTheorem 1. Let $\\phi : X \\rightarrow Z$ be an embedding from $X$ to a latent space $Z$. Let us assume that the sought outcome models $\\mu^0$ and $\\mu^1$ can be expressed as $\\mu^0 = \\nu^0 \\circ \\phi$ and $\\mu^1 = \\nu^1 \\circ \\phi$, with $\\nu^0$ and $\\nu^1$ two functions defined on $Z$ with Lipschitz constant $L$."}, {"title": null, "content": "Let $\\hat{\\mu}^0, \\hat{\\mu}^1 : Z \\rightarrow \\mathbb{R}$ be two models trained to approximate $\\nu^0$ and $\\nu^1$ with Lipschitz constant $\\hat{L}$. Then the empirical PEHE associated with $(\\hat{\\mu}^1 - \\hat{\\mu}^0) \\circ \\phi$ is upper bounded by $M_1$ with:\n$M_1 = \\frac{4}{n} \\sum_{i=1}^n \\left[ (1 + w_i) [(\\hat{\\mu}^{t_i} \\circ \\phi)(x_i) - Y_i]^2 + (L^2 + \\hat{L}^2) \\sum_{i=1}^n \\|x_i - x_i^m\\|^2 \\right]$\nBound $M_1$ depends on two terms: i) the factual accuracy on every sample $x_i$, all the more important the higher its counter-factual importance weight $w_i$, and ii) the counter-factualizability of the samples. This bound establishes the soundness of the training loss (Eq. 5), built on both terms.\nBuilding on this result, our second result concerns the hybrid X-learner T-learner scheme of ALRITE.\nTheorem 2. Let $P_0$ and $P_1$ be a control and a treatment pipeline, respectively involving embeddings $\\phi_0$ and $\\phi_1$. Let us further assume that the sought outcome models $\\mu^0$ and $\\mu^1$ can be expressed on the top of each embedding ($\\mu^0 = \\nu \\circ \\phi_0 = \\nu^0 \\circ \\phi_1$ and $\\mu^1 = \\nu \\circ \\phi_0 = \\nu^1 \\circ \\phi_1$), with all $\\nu_j^i$ of Lipschitz constant L for $i, j \\in \\{0,1\\}$.\nLet $\\hat{h}_0^0$ and $\\hat{h}_0^1$ (respectively $\\hat{h}_1^0$ and $\\hat{h}_1^1$) denote the learned models of $\\nu_0^0$ and $\\nu_0^1$ in $P_0$ (resp. $\\nu_1^0$ and $\\nu_1^1$ in $P_1$), with Lipschitz constant $\\hat{L}$.\nFor any i-th sample in the observation dataset D, define\n$\\tau_i = (1 - t_i) ((\\hat{h}_0^0 \\circ \\phi_0)(x_i) - Y_i) + t_i (Y_i - (\\hat{h}_1^1 \\circ \\phi_1)(x_i))$\nLet us further define the counter-factual importance weight of sample $(x_j, t_j, Y_j)$ as the number of samples $(x_i, t_i, Y_i)$ such that $t_i = 1 - t_j$ and $x_i$ is the nearest neighbor of $x_j$ according to $\\phi_{t_i}$:\n$w_j = \\# \\{(x_i, t_i, y_j) \\in D \\text{ s.t. } (x_j, t_j, y_j) = (x_i^{m,\\phi_{t_i}}, 1 - t_i, y_i^{m,\\phi_{t_i}})\\}$\nThen, the within-sample PEHE defined by $\\sqrt{\\frac{5}{n} \\sum_{i=1}^n (\\tau_i - \\tau(X_i))^2}$ is upper bounded by $M_2$, with\n$M_2 = \\frac{5}{n} \\sum_{t_i=1,i=1} w_i [(\\hat{h}_0^0 \\circ \\phi_0)(x_i)-Y_i]^2 + \\sum_{t_i=0,i=1} w_i [(\\hat{h}_1^1 \\circ \\phi_1)(x_i)-Y_i]^2 + (L^2+\\hat{L}^2) \\sum \\|\\phi_{t_i} (x_i) - \\phi_{t_i} (x_i^{m,\\phi_{t_i}})\\|^2 + k_Y$\nwhere $k_Y = \\sum_{i=1}^n (1 + w_i) (Y_i - \\mu^{t_i}(x_i))^2$.\nAs said, Thm. 2 holds in the within-sample setting, as it requires knowledge of the treatment assignment T. It does not generalize directly to the out-of-sample setting, where the (unknown) $t_i$ and $y_i$ are respectively estimated using the propensity and the outcome models.\nFinally, the upper-bound established in Thm. 2 is directly related with the loss used to train the pipelines (Eq. 5), establishing the well-foundedness of the approach, as follows:"}, {"title": null, "content": "Theorem 3. There exists a hyper-parameter setting ($\\alpha_0, \\alpha_1, \\beta_0, \\beta_1$) such that the within-sample empirical PEHE is upper bounded by $M_3$, with:\n$M_3 = 5 [L(P_1) + L(P_0) - \\gamma_0 \\|P_0\\|^2 - \\gamma_1 \\|P_1\\|^2 + k_Y$]\n$\\frac{1}{n_0} \\sum_{t_i=0, i=1} (\\hat{h}_0^0 \\circ \\phi_0 (x_i) - Y_i)^2 - \\sum_{t_i=1, i=1} (\\hat{h}_1^1 \\circ \\phi_1 (x_i) - Y_i)^2$\n$\\frac{1}{n_1} \\sum_{t_i=0, i=1} (\\hat{h}_0^1 \\circ \\phi_1 (x_i) - Y_i)^2 + \\sum_{t_i=1, i=1} (\\hat{h}_1^0 \\circ \\phi_0 (x_i) - Y_i)^2)]$\nNote that this result is not constructive in the sense that it does not give the hyper-parameter setting. Nevertheless, the relation between this bound on the PEHE and the terms in the pipeline loss confirms the soundness of the approach. This bound also depends on the Lipschitz constants of the learned models and of the target models (which only depend on the problem).\n3.5 Discussion\nThe formal guarantees established by Thm 1-3 contrast with the main result of Shalit et al. Shalit et al. (2017) in two ways. A first difference is that Shalit et al. (2017) relies on the invertibility of embedding $\\phi$. In contrast, ALRITE only assumes that the considered embeddings $\\phi$ are such that the sought models can be expressed with no loss of information (there exists $\\nu^0$ and $\\nu^1$ s.t. $\\mu^i = \\nu^i \\circ \\phi$). As a result, ALRITE can fully benefit of the celebrated opportunities offered by a change of representation (Cayton, 2008; Bengio et al., 2013), e.g. to achieve feature selection or dimensionality reduction."}, {"title": "Experimental validation", "content": "After describing the two benchmarks we considered, IHDP and ACIC2016, and the experimental setting, this section reports on the empirical validation of ALRITE. The ALRITE code is publicly available."}, {"title": "Benchmarks", "content": "4.1 Benchmarks\nThe IHDP dataset is the baseline dataset commonly used for benchmarking causal inference models (Shalit et al., 2017; Du et al., 2021; Zhou et al., 2021). The ACIC2016 benchmark originates from the 2016 Atlantic Causal Inference Challenge (Dorie et al., 2017).\n4.1.1 IHDP\nThe IHDP dataset, introduced by Hill (2011), is based on a real-life randomized experiment dataset, the Infant Health and Development Program (Brooks-Gunn et al., 1992). The treatment consists of quality child care and home visits on the health and development of preterm children. While the treatment assignment is randomized in the collected data, treatment imbalance is enforced by Hill (2011), by removing a subpopulation (treated group children with non-white mothers) from the dataset.\nThe dataset contains 747 individuals, 139 of whom are treated; these are described by 25"}]}