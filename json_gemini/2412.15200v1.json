{"title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation", "authors": ["Wang Zhao", "Yan-Pei Cao", "Jiale Xu", "Yuejiang Dong", "Ying Shan"], "abstract": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models.", "sections": [{"title": "1. Introduction", "content": "Procedural Content Generation (PCG) is a powerful mean to design and generate high-quality 3D contents, via algorithmic programs and rules, and has a wide application in the gaming and movie industry. Over decades, a number of"}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Procedural Content Generation and Inverse", "content": "Procedural Content Generation (PCG) is a long-standing research problem in computer graphics and vision community. L-systems [37] were firstly proposed for biological modeling, and later extended to model geometry of plants [44, 57]. To concisely describe different object categories, many domain-specific languages were introduced such as shape grammar [38, 77], split grammar [47, 84] for generating trees [57, 74], man-made facades and buildings [66]. Beyond shapes, PCG is also widely used in generating textures [13] and materials [27, 70]. Utilizing powerful node graph grammar in modern commercial software like Blender [12], Infinigen [59] and Infinigen Indoors [60] developed a broad collection of diverse procedural generators including objects, natural assets, and compositional scenes, greatly facilitating the synthetic data generation. Recently, inspired from the success of Large Language Models (LLMs), many works [2, 28, 30, 32, 34, 73, 78, 88, 94] proposed to leverage the LLM reasoning capability to automatically design or edit procedural generators"}, {"title": "2.2. Diffusion Models for 3D Generation", "content": "Diffusion models have achieved remarkable progress in generative modeling, with increasing popularity in 3D generation. Due to the scarcity of 3D data, early works attempted to utilize 2D diffusion priors through score distillation sampling [54] and its enhancements [9, 36, 46, 67, 83]. This distillation inherently lacks view consistency and 3D priors, often leading to blurry textures and multi-head Janus problem. To mitigate this issue, Zero-1-to-3 [39] proposed to generate novel view images under required camera viewpoints, and reconstruct 3D representation using generated multiview images. Following this line of research, a number of works [40, 42, 58, 71, 72, 82] explored fine-tuning 2D diffusion models to directly generate multiview images via carefully designed view interaction, which greatly improve the view consistency and thus benefit 3D generation.\nWith the advent of large-scale 3D datasets such as Objaverse [14], training 3D native diffusion models is made possible. Different kinds of 3D shape representations are explored such as point cloud, voxel, mesh, implicit functions, etc. Point-E [48] pioneered the denoising diffusion on point cloud. LION [80] and SLIDE [43] further introduced latent point diffusion model with point cloud VAE to enhance the compactness. To directly model 3D surfaces, PolyDiff [1] represented meshes as quantized triangle soups and applied diffusion model on triangle vertex coordinates. MeshDiffusion [41], in another way, utilized deformable marching tetrahedra [69] representation for meshes and trained a diffusion model upon it. By exploiting sparse voxel hierarchy or Octree-base latent voxel representations, XCube [61] and OctFusion [86] managed to relief the memory-resolution trade-off of 3D voxels and train diffusion models over latent voxels, achieving detailed 3D generation results.\nDifferent from above explicit 3D representations, many works focus on implicit representations which features higher compression ratio, infinite decoding resolution, and intrinsic smoothness. SDFusion [10] employed 3D VAE to decode SDF fields from denoised latent variables. 3DGen [23] and Direct3D [85] selected triplane [6] as the representation, while Michelangelo [93], 3DShape2VecSet [90] and CLAY [91] adopt pure 3D shape latent vectors with VAEs to fully unleash the scaling ability.\nImage conditioned inverse PCG can be viewed as image to 3D generation. From this view, DI-PCG essentially takes the procedural generator and its parameters as a powerful, highly compact, editable 3D representation and trains a diffusion model on top of it for high-quality 3D generation."}, {"title": "3. Methods", "content": null}, {"title": "3.1. Preliminaries", "content": "Procedural Generator. Procedural generator defines algorithmic rules with a set of parameters to create an asset. A generator usually handles one specific category of objects, such as a chair, vase, tree, etc. For example, a chair is procedurally constructed by the selected parameters which describe the back type of the chair, the leg height, the numbers of bars, the existence of arms, etc. Theoretically, it can generate infinitely many variants of objects by randomly sampling parameters. In practice, the capability of a generator"}, {"title": "Diffusion Model", "content": "A diffusion model consists of a forward noising and reverse denoising process. The forward process gradually corrupts clean data $x_o$ into a Gaussian distribution $N(0, I)$ by: $q(x_t \\vert x_o) = N(x_t; \\sqrt{\\bar{a}_t}x_o, (1 - \\bar{a}_t)I)$, where $x_o$ is the input data, $t$ is the timestep and $a_t$ are constant hyperparameters. With the reparameterization trick, we can sample $x_t = \\sqrt{\\bar{a}_t}x_o + \\sqrt{1 - \\bar{a}_t}\\epsilon_t$, where $\\epsilon_t \\thicksim N(0, I)$. The reverse process is then defined through a Markov chain: $p_\\theta(X_{t-1} \\vert X_t) = N(\\mu_\\theta(x_t), \\Sigma_\\theta(x_t))$. By parameterizing $\\mu_\\theta$ as a noise prediction network $\\epsilon_\\theta$, the reverse process is trained via the variational lower bound, with the objective reduced to the mean square error (MSE) between the predicted noise and the ground truth noise:\n$L_\\theta = E_{x_o,t,\\epsilon_t} \\vert\\vert \\epsilon_\\theta (x_t, t) - \\epsilon_t\\vert\\vert^2$. \nAfter training, the diffusion model can sample directly on the data distribution of x from a Gaussian distribution noise."}, {"title": "3.2. Diffusion Model for Inverse PCG", "content": "Our proposed DI-PCG considers the procedural generator with its parameters as a controllable 3D shape representation, and carefully designs and trains a diffusion model for the parameters, enabling to efficiently sample the target parameters under condition, as illustrated in Figure 2. Next, we will describe in detail the representation, architecture, condition scheme and the data preparation process.\nRepresentation. We directly treat the parameters of the procedural generator as the parametric representation of the 3D models, and learn to sample it with diffusion models.\nSpecifically, we assume that the given procedural generator provides a list of its controllable random parameters $p = \\{p_0, p_1, p_2, \u2026, p_N\\}$, and each parameter has its own sampling range, e.g. minimum and maximum values for continuous parameters, and all available choices for discrete parameters. If not provided, we manually derive them from the procedural generator's code. Since the procedural generator has both continuous and discrete parameters, which is difficult for the diffusion model to jointly model, we first make the discrete parameters continuous. We uniformly cut [-1, 1] into pieces where each piece corresponds to a discrete choice. To facilitate training, the continuous parameters are also normalized to [-1,1] according to the minimum and maximum values. We denote these canonicalization operations together as a reversible projection from the original parameter set to the normalized continuous representation $x = \\phi(p)$. These normalized parameters $x \\in [-1,1]^{N\\times1}$ are then used in the diffusion noising and denoising process. During inference, the sampled normalized parameters are projected back to the original generator parameters using $p = \\phi^{-1}(x)$, and the 3D asset is then generated via the procedural generator with p.\nModel architecture. Following recent successful practices [4, 8, 91] in both 2D and 3D generative modeling, we employ the Diffusion Transformer (DiT) [53] model. The DiT model, which served as $\\epsilon_\\theta$ in Eq. 1, predicts the noise at each timestep t via cross and self attentions:\n$\\epsilon_\\theta (x_t, t, c) = \\{CrossAttn(SelfAttn(x_t), c)\\}$\nwhere $x_t$ is the noisy version of $x_o$, and c represents the condition features. L denotes the number of attention layers. Since our procedural parameter representation is"}, {"title": "4. Experiments", "content": "To validate the effectiveness of DI-PCG, we conduct detailed experimental evaluations both qualitatively and quan-"}, {"title": "4.1. Qualitative Results", "content": "Image condition. We collect diverse images from internet for all six categories. These images are in multiple styles with different object orientations, delicate geometries, various textures and materials, forming an extensive and challenging test for image-to-3D generation methods. In Figure 3, we show qualitative results on chair, table, and vase categories. Our method can reliably recover appropriate procedural generator parameters, thus deliver high fidelity 3D generated models of neat geometry, standard meshing and precise alignments with condition images. We recommend readers to supplementary materials for more results. We also conduct comparisons with above mentioned strong baselines. As shown in Figure 4, thanks to its parametric representation upon procedural generators, DI-PCG achieves much improved generation results, being able to preserve intricate details such as holes in basket, dandelion petals, etc. As comparison, Shap-E [31] produces noisy surfaces and fails to handle natural objects like flower and dandelion. Michelangelo [93] tends to output smooth geometries thanks to its latent representation design, yet lacks sufficient details or misaligned with the image. InstantMesh [87] and CraftsMan [35] both rely on multi-view diffusion model to dream about the inputs. While more generalizable than direct 3D methods, they suffer from the inconsistency and errors of the generated multi-view images, and also can not recover complex 3D details.\nSketch condition. Thanks to the generalization ability of visual foundation model features and our data augmentation strategy, DI-PCG can directly process sketch image conditions and outputs decent 3D generations, as illustrated in Figure 5. This functionality greatly facilitate the object designs and edits, offering a simple yet effective way to create high-quality 3D assets. More results are included in supplementary materials.\nComparison with MCMC. As the representative sampling method for inverse PCG, MCMC can effectively approximate the parameter distribution, with the presence of powerful scoring metrics and sufficient iterations. We imple-"}, {"title": "4.2. Quantitative Comparison", "content": "For quantitative evaluations, we use the chair category to demonstrate since it is commonly used and widely available in existing datasets. In addition to the evaluation on test split of our generated data, we also test on the ShapeNet [7] chair models to verify its generalization ability. Specifically, we follow the split of 3D-R2N2 [11], and manually filter the test chair models to exclude totally out-of-domain samples such as sofa-like or artistic-designed chairs which are currently impossible for Infinigen [59] chair generator to model. The resulting ShapeNet chairs contain 218 models for testing. We adopt commonly used 3D metrics Chamfer Distance (CD), Earth Moving Distance (EMD), and F-Score. Table 1 summarizes the results. It clearly shows that DI-PCG can reliably fit the procedural generator and inversely estimate the parameters with high accuracy. Moreover, it generalizes beyond the procedurally generated chairs and achieves comparable or even better results than previous SOTA methods on ShapeNet chairs subset."}, {"title": "4.3. Ablation Study", "content": "We conduct ablation studies for different components of DI-PCG. The results are obtained on the above mentioned ShapeNet chair subset, summarized in Table 2. w/o MV & Aug indicates generating training data with single view image and no augmentations, thus the performance is degraded. DI-CLIP denotes using CLIP instead of DINOv2 as the feature for condition. It clearly verifies the effectiveness of DINOv2 features on capturing rich shape features. To study the effect of model size, we train another two diffusion models with small (1.6M parameters) and large (39M parameters) network configurations. As shown in the table, a larger model with more parameters is not necessary and provides no improvements. While small model indeed causes some performance degradation, the trade-off between model size and performance is reasonable and provides more options for different scenarios."}, {"title": "4.4. Editing Application", "content": "Thanks to the explicit and semantically meaningful characteristic of the procedural generator parameters, we can easily adjust specific parameter values to edit the 3D model. Some simple editing examples are shown in Figure 7, where the geometric attributes of the given chair, such as leg height, back types, are easily changed. We argue that this handy editing functionality is not in conflict with the controlling difficulty of PCG. It would be painful to find suitable combinations of tens of parameters from scratch, but it is easy and natural to adjust one or two specific parameters to edit existing 3D models. In this way, DI-PCG, as an efficient and effective inverse PCG method, unleashes the controlling advantage of procedural generation."}, {"title": "4.5. Limitations and Future Works", "content": "As an early attempt to explore diffusion-based inverse PCG for 3D generation, DI-PCG has limitations. First, since DI-PCG relies on off-the-shelf procedural generators, the generation scope is strictly bounded by these generators, i.e. DI-PCG cannot generate out-of-domain objects beyond current generators. Some failure examples in the supplementary materials illustrate this shortage. Second, current DI-PCG only supports image as conditions, while text conditions are widely used in 3D AIGC. Finally, DI-PCG is demonstrated on the object generators, and its applicability on scene-level procedural generation is not verified. Future works include extension to scene generation, more conditions, and automatic generation of procedural generators."}, {"title": "5. Conclusion", "content": "In this paper, we present DI-PCG, an innovative diffusion-based efficient inverse procedural content generation method for creating high-quality 3D assets. By directly modeling procedural generator parameters as diffusion denoising variables, the posterior distribution of parameters given condition images can be efficiently determined by the learned diffusion model. DI-PCG solves the inverse PCG problem with high efficiency and accuracy, validated by both quantitative and qualitative evaluations. It represents a valuable exploration towards a promising path for 3D content generation, where parametric models and algorithmic rules together play the roles."}, {"title": "6. More Implementation Details", "content": "We use six procedural generators from Infinigen and Infinigen Indoors, namely chair, table, vase, basket, flower and dandelion generators. They contain 48, 19, 12, 14, 9, 15 controllable parameters, respectively. These are also the input token lengths of each diffusion models, as the procedural parameters directly serve as the denoising variables. Our code will be released once the paper is public."}, {"title": "7. More Qualitative Results", "content": "Here we show more qualitative results of DI-PCG. The generation results for the chair, table, and vase categories are shown in Figure 9. DI-PCG can handle complex shape variations and details, generating high-quality 3D models from input single images. The results for the basket, flower, and dandelion are shown in Figure 10. These categories intrinsically have a bit fewer variations due to the somewhat limited generality of these three procedural generators from Infinigen. Despite that, our method can capture the geometric details and recover the appropriate parameters for the input images, generating fine 3D geometries.\nDI-PCG can effectively handle sketch input as conditions. We show qualitative examples in Figure 11. In our experiments, we observe that DI-PCG works just as well on sketch inputs as on RGB image inputs. This provides DI-PCG more flexibility and less burden to cooperate with artists.\nWe also provide some visual examples of our quantitative evaluations on DI-PCG\u2019s test split and ShapeNet chair subset. As shown in Figure 12 and 13, compared to existing SOTA reconstruction and generation methods, DI-PCG delivers much better 3D models with neat geometry."}, {"title": "8. Discussions and Failure Cases", "content": "As discussed in the main paper, DI-PCG is limited by the generality and granularity of the given procedural generators. Although the adopted generator from Infinigen can cover a wide range of common variations of the corresponding category, it still has obvious boundaries. Figure 8 shows some failure cases. The input chair images are out-of-domain samples for Infinigen chair generators, thus DI-PCG can not generate precisely aligned 3D models. Instead, it outputs the closest parameter sets to approximate the images. Although bounded by the procedural generator, DI-PCG focus on the efficient inverse ability of PCG, and represents a general tool to easily and effectively control any existing procedural generator, facilitating their usage in 3D content creation. As proceudral generators are getting increasing attention and become mature to develop thanks to the modern design softwares, the available number and cover range of existing procedural generators are rapidly growing, which can further benefit DI-PCG. DI-PCG can be applied for any procedural generator, to greatly enhance its controllability. Moreover, in the future, utilizing AI techniques, such as Large Language Model (LLM), to generate procedural generation programs could be possible and exciting. AI-generated procedural generators and DI-PCG can naturally work together, to form a new paradigm of 3D content generation."}]}