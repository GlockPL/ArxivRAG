{"title": "RbRL2.0: Integrated Reward and Policy Learning for Rating-based Reinforcement Learning", "authors": ["Mingkang Wu", "Devin White", "Vernon Lawhern", "Nicholas R. Waytowich", "Yongcan Cao"], "abstract": "Reinforcement learning (RL), a common tool in decision making, learns policies from various experiences based on the associated cumulative return/rewards without treating them differently. On the contrary, humans often learn to distinguish from different levels of performance and extract the underlying trends towards improving their decision making for best performance. Motivated by this, this paper proposes a novel RL method that mimics humans' decision making process by differentiating among collected experiences for effective policy learning. The main idea is to extract important directional information from experiences with different performance levels, named ratings, so that policies can be updated towards desired deviation from these experiences with different ratings. Specifically, we propose a new policy loss function that penalizes distribution similarities between the current policy and failed experiences with different ratings, and assign different weights to the penalty terms based on the rating classes. Meanwhile, reward learning from these rated samples can be integrated with the new policy loss towards an integrated reward and policy learning from rated samples. Optimizing the integrated reward and policy loss function will lead to the discovery of directions for policy improvement towards maximizing cumulative rewards and penalizing most from the lowest performance level while least from the highest performance level. To evaluate the effectiveness of the proposed method, we present results for experiments on a few typical environments that show improved convergence and overall performance over the existing rating-based reinforcement learning method with only reward learning.", "sections": [{"title": "Introduction", "content": "Recent advancements in reinforcement learning (RL) (Sutton and Barto 1998) have shown promising results in solving complex robotics tasks under the assumption that proper reward functions have been designed (Tang et al. 2024). However, in many real world cases, it is often difficult and challenging to define reward functions properly. In these situations, it is often required to include humans users who either provide demonstrations in an offline setting, called learning from demonstrations (LfD), or provide feedback in an online or offline setting, called reinforcement learning from human feedback (RLHF). Popular LfD methods include inverse reinforcement learning (IRL) (Argall et al. 2009) and behavior cloning (BC) (Torabi, Warnell, and Stone 2018), which take human expert demonstrations as the input and learn policies that mimic the demonstrations without learning the reward functions. In contrast, RLHF takes short video clips or segments and asks humans to provide feedback. The typical form of feedback can take the form of preferences over segment pairs, also known as preference-based reinforcement learning (PbRL) (Christiano et al. 2017), or ratings for individual segments, also known as rating-based reinforcement learning (RbRL) (White et al. 2024). Variants of the PbRL methods include ranking-based RL and crowdsourcing PbRL (Brown, Goo, and Niekum 2020; Chhan, Novoseller, and Lawhern 2024). RLHF methods focus on learning reward functions from the human feedback and then training policies from the learned rewards.\nRecent studies in the fields of LfD and RLHF have shown promising capabilities individually. However, integrating reward learning and policy learning is challenging in the preference setting since preferences are provided in the relative sense without indicating if a segment is \u201coptimal\u201d or \u201csub-optimal\". As a contrary, RbRL utilizes ratings as the feedback and hence allows the evaluation of individual samples based on their actual performance. For example, if a sample is labeled \"Very Good\" (or \"Very Bad\" respectively) which means \u201cdesired\u201d (or \u201cundesired\" respectively) that the policy learning should mimic (or distinguish respectively). Meanwhile, the additional ratings of \"Good\", \"Average\", \"Bad\", indicate that the policy learning should deviate it from the three classes in an descending order. In other words, the learned policy should be closest to the samples in the \"Very Good\" rating, while less closer to the samples in the \u201cGood\u201d rating, and follow a similar trend to be most different from samples in the \u201cVery Bad\u201d rating class. Although such a concept is intuitive in humans' decision making process, the current RbRL approach (White et al. 2024) only used ratings to learn reward without utilizing different rating classes directly in policy learning, which is the focus of the current work.\nIn this paper, we focus on developing a new rating-based reinforcement learning algorithm that integrates the existing reward learning and a new policy learning motivated by how humans update the decision by shaping it away from"}, {"title": "Preliminaries and Background", "content": "In the context of this paper, we consider a Markov Decision Process (MDP) without reward associated but with ratings, which is defined by a tuple $(S, A, P, \\gamma, n)$, where $S$ is the state space, $A$ the action space, $P$ the state transition probability distribution, $\\gamma \\in [0, 1)$ is the discount factor that limits the influence of infinite future rewards, and $n$ represents the number of rating classes. At each state $s \\in S$, the RL agent takes an action $a \\in A$, moves to the next state $s'$ determined by $P(s'|s, a)$, where a length-k trajectory $(s_0, a_0, ..., s_{k-1}, a_{k-1})$ is collected to be rated.\nIn standard RL setting, the environment provides a reward $r:S\\times A\\rightarrow R$ at each interaction between itself and the RL agent. The goal is to learn a policy $\\pi$ that maps states to actions to maximize the expected discounted cumulative rewards. This can be formulated by the state-action value function\n$Q(s, a) = E_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_t, a_t) \\right],$   (1)\nwhere $t$ represents the $t^{th}$ timestep in the training process. The performance of a policy $\\pi$ is normally evaluated by the discounted cumulative rewards\n$J(\\theta) = E_{s\\sim \\mu} [Q(s, \\pi(s|\\theta))],$   (2)\nwhere $\\mu$ represents the initial state distribution and $\\theta$ is the policy network parameter. The policy $\\pi$ defines the agent's behavior by specifying the probability distribution over actions given the current state. The goal of an RL agent is to find an optimal policy $\\pi^*$ that maximizes the expected cumulative reward over time, i.e., $\\pi^* = arg\\ max_{\\pi} E_{\\pi} \\left[ \\sum_{t=0}^{T} \\gamma^{t} r_t \\right]$, where $r_t$ is the reward received at time step t and T is the time horizon.\nNote that the update of policy relies much on the cumulative rewards, which implies the existence of rewards in each state-action pair, also referred to dense reward environments. However, in reward-free environments where rewards are not present, the standard RL methods fail to work due to the lack of (reward) knowledge to guide the policy search."}, {"title": "Rating-Based Reinforcement Learning", "content": "Due to the lack of rewards, the rating-based reinforcement learning (RbRL) (White et al. 2024) learns a reward model $r:S\\times A \\rightarrow R$ that predicts reward $\\hat{r}(s, a)$ for each state-action pair during interaction. Given a length-j segment $\\sigma = (s_1, a_1, ..., s_j, a_j)$, the cumulative discounted reward $\\hat{R}(\\sigma) := \\sum_{t=1}^{j} \\gamma^{t-1}\\hat{r}(s_t, a_t)$ based on $\\hat{r}$ provides an estimated cumulative reward."}, {"title": "Rating-induced Policy Loss", "content": "The key idea of RbRL is to train a reward model $\\hat{r}: (s,a) \\rightarrow R$ that can explain why the existing samples were given their corresponding ratings. First, the cumulative predicted reward is normalized across a batch $\\hat{R}(\\sigma) = \\frac{\\hat{R}(\\sigma)-min_{\\sigma'\\in X} \\hat{R}(\\sigma')}{max_{\\sigma' \\in X} \\hat{R}(\\sigma')-min_{\\sigma'\\in X} \\hat{R}(\\sigma')}$. Then, the probability of each sample in each rating class was computed as\n$Q_{\\sigma}(i) = \\frac{e^{-k(\\hat{R}(\\sigma)-R_i)(\\hat{R}(\\sigma)-R_{i+1})}}{\\sum_{j=0}^{n-1}e^{-k(\\hat{R}(\\sigma)-R_j)(\\hat{R}(\\sigma)-R_{j+1})}},$   (3)\nwhere $R_i$ and $R_{i+1}$ are the lower and upper bound value for the $i^{th}$ class respectively. The reward predictor $\\hat{r}$ was trained by minimizing the cross-entropy loss given by\n$L(\\phi) = - \\sum_{\\sigma \\in X} \\sum_{i=0}^{n-1} \\mu_{\\sigma}(i)log Q_{\\sigma}(i),$   (4)\nwhere $X$ is the set containing all samples, and $\\mu_{\\sigma}(i) = 1$ if the sample is labeled in the class $i$ and $\\mu_{\\sigma}(i) = 0$ otherwise.\nOnce the reward model $\\hat{r}$ was learned, one can use any existing RL algorithm, such as PPO, DDPG, SAC, to train a control policy. Note that the rated samples in this method were only used in reward learning without investigating their value on policy learning directly. For example, it is intuitive that a good policy should deviate more from samples with lower ratings while less from samples with higher ratings. Hence, the rated samples can provide additional values in direct policy shaping via designing an appropriate loss function, which is the focus of the next section."}, {"title": "Overall Loss Function", "content": "The proposed new loss function is given by\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} log(\\pi_{\\theta}) \\hat{R}(\\sigma_{\\theta}) \\right] - \\nabla_{\\theta} \\sum_{i=0}^{n-2} w_i D_{KL}(D_i || D_{\\pi_{\\theta}}),$   (5)\nwhere the first component is the classic loss based on the learned reward and the second component is the new loss, $\\sigma_{\\theta}$ includes the trajectories sampled from the current policy at each training batch, $D_{KL}$ denotes the KL divergence between two different distributions (please refer to Definition 1 below), $w_i$ is the weight applied to the KL divergence in a descending order as rating level moves from lowest to highest, and $D$ denotes the distribution. This new policy gradient loss effectively penalizes the similarities between the current policy and different rating levels of the failed experiences such that the policy is updated in the direction of continu-"}, {"title": "Policy Loss Based on KL divergence", "content": "Since the purpose of our approach is to quantify the deviation between the current policy and different rating levels of failed samples via computing their KL divergencies, we employ multivariate Gaussian distribution to represent the distributions by computing the essential components, including covariance matrix and mean values, of trajectories in low rating classes and those sampled from the current policy. Specifically, we propose to compute the KL divergence between the current policy and the samples in different rating levels of failed samples as follows.\nDefinition 1. For any two distributions, $P$ and $Q$, parameterized by their means $\\mu_p$ and $\\mu_q$ and covariance $\\Sigma_p$ and $\\Sigma_q$. Mathematically, the KL divergence between $P(\\mu_p, \\Sigma_p)$ and $Q(\\mu_q, \\Sigma_q)$ is\n$D_{KL}(P || Q) = \\frac{1}{2} \\left( Tr(\\Sigma_q^{-1} \\Sigma_p) + (\\mu_q - \\mu_p)^T \\Sigma_q^{-1} (\\mu_q - \\mu_p) - k + ln(\\frac{det(\\Sigma_q)}{det(\\Sigma_p)}) \\right),$   (6)\nwhere, $Tr(\\cdot)$ is the trace of a given matrix, $det(\\cdot)$ represents the determination of a given matrix, and $k$ is the number of features.\nDefinition 1 provides a measure of how different the two distributions $P$ and $Q$ are. For example, we have a set of failed trajectories $\\sigma_i = ((s_{i_1}, a_{i_1}, s_{i_1}', a_{i_1}',...), ..., (s_{i_m}, a_{i_m}, s_{i_m}', a_{i_m}',...))$ in rating class $i$ containing m trajectories, where $i$ is not the highest rating class. Another set of trajectories $\\sigma_{\\pi_{\\theta}} = ((s_{\\theta_1}, a_{\\theta_1}, s_{\\theta_1}', a_{\\theta_1}',...), ..., (s_{\\theta_o}, a_{\\theta_o}, s_{\\theta_o}', a_{\\theta_o}',...))$ sampled from the current policy $\\pi$ parameterized by $\\theta$ represents the behaviors of the RL agent in the current training batch. According to Definition 1, the KL divergency between the distribution of rating class $i$ and the distribution of the current policy can be formulated as\n$D_{KL}(D_i || D_{\\pi_{\\theta}}) = \\frac{1}{2} \\left( Tr(\\Sigma_{\\pi_{\\theta}}^{-1} \\Sigma_{D_i}) + (\\mu_{D_i} - \\mu_{D_{\\pi_{\\theta}}})^T \\Sigma_{\\pi_{\\theta}}^{-1} (\\mu_{D_{\\pi_{\\theta}}} - \\mu_{D_i}) + ln(\\frac{det(\\Sigma_{\\pi_{\\theta}})}{det(\\Sigma_{D_i})}) \\right)$   (7)\nSince $D_{KL}(D_i || D_{\\pi_{\\theta}})$ is used to compute the policy gradient in (5), the constant k in (6) can be omitted here.\nConsider the case of 4 different rating classes (\u201c0\u201d, \u201c1\u201d, \u201c2\u201d and \u201c3\u201d). Following the computation of the KL divergence between the current policy and the samples in rating classes 0, 1, and 2 (except class 3, which is the highest rating class and hence not included as explained in the Subsection \"Overall Loss Function\"), the overall loss function can be written as\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} log(\\pi_{\\theta}) \\hat{R}(\\sigma_{\\theta}) \\right] - (w_0 D_{KL}(D_0 || D_{\\pi_{\\theta}}) + w_1 D_{KL}(D_1 || D_{\\pi_{\\theta}}) + w_2 D_{KL}(D_2 || D_{\\pi_{\\theta}})),$   (8)\nwhere $\\hat{R}(\\sigma_{\\theta})$ represents the cumulative predicted rewards of the trajectories sampled by the current policy $\\pi_{\\theta}$, $w_0$, $w_1$ and $w_2$ represents the weights for KL divergencies between rating classes \u201c0\u201d, \u201c1\u201d and \u201c2\u201d and the current policy $\\pi_{\\theta}$, respectively. The weights are assigned in the descending order, namely $w_0 > w_1 > w_2$, such that current policy $\\pi_{\\theta}$ is pushed away from the distribution of rating class \"0\" more than distributions of rating classes \"1\" and \u201c2\u201d.\nThe pseudocode of the proposed new approach is given in Algorithm 1."}, {"title": "Limitations and Future Work", "content": "One limitation of the proposed approach is the variability in individual rating standards, which may be noisy due to differences in how participants interpret and evaluate the environment. Since RbRL relies on users having a basic understanding of the task and environment, these inconsistencies may reduce the reliability of the collected individual ratings. To address this, future work will involve designing"}, {"title": "Algorithm 1: RbRL2.0", "content": "Require: rating classes n, rating buffers $\\mathcal{R}_1, ..., \\mathcal{R}_n$, initial reward predictor $\\hat{R}$, KL divergency weight $w_i$, total training cycles $T$, total training cycle $M$ for $\\hat{R}$\nInitialize RL policy $\\pi_{\\theta_0}$\nfor i = 1, M do\n Sample trajectories $\\sigma$ from $\\pi_{\\theta_0}$\n Rate $\\sigma$ by humans\n Update $\\hat{R}$ based on human ratings\nend for\nfor i = 1, T do\n Extract trajectories $\\sigma_1,.., \\sigma_n$ from rating buffers $\\mathcal{R}_1,..., \\mathcal{R}_n$\n Update policy with $\\hat{R}$\n$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} log(\\pi_{\\theta}) \\hat{R}(\\sigma_{\\theta}) \\right] - \\nabla_{\\theta} \\sum_{i=0}^{n-2} w_i D_{KL}(D_i || D_{\\pi_{\\theta}})$\n Update policy parameter $\\theta_i$\n $\\theta_{i+1} \\leftarrow \\theta_i + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta_i})$\nend for\nfrom these samples differently based on their performance levels.\nFrom the machine learning perspective, the proposed approach creates a new human cognitive-based mechanism, namely, rating, into the training of reinforcement learning algorithms. Moreover, we explore the value of rated samples by following humans' learning from failed experience directly and indirectly. The second term in the proposed policy loss in (5) directly follows the concept of \"learning to differentiate\" inherent in humans' cognitive learning from failure. The first term in the proposed policy loss in (5) indirectly impacts the policy learning by maximizing the estimated reward that can best reflect the rated samples.\nFrom the cognitive science perspective, the proposed approach provides a mathematical approach that allows AI agents to reason its actions following a similar concept of human minds. In particular, the benefits of the proposed method shown in the \"Experiments and Results\" Section demonstrate the value of uncovering cognitive science for their adoption in machine learning to achieve unprecedented performance via harnessing human knowledge."}, {"title": "Experiments and Results", "content": "To evaluate the effectiveness of our proposed method, we compare the new method, labeled as RbRL2.0, with RbRL across 3 DeepMind Control environments (Tassa et al. 2018), namely, HalfCheetah, Walker and Quadruped. These environments are characterized with continuous state and action spaces. Specifically, HalfCheetah is characterized with a 17-dimensional state space, capturing joint positions, velocities, and body orientation, and a 6-dimensional action space, which corresponds to the torques applied to each joint. Walker is characterized with a 24-dimensional state space, representing joint angles, velocities, and torso orientation, and a 6-dimensional action space, controlling forces applied to each limb. Quadruped is characterized with a 78-dimensional state space, including joint positions, velocities, and full body orientation, and a 12-dimensional action space, allowing torque-based control over each leg joint. It is worth noting that HalfCheetah is the simplest environment while Quadruped is the most challenging environment among the three. In other words, we focus on understanding how RbRL2.0 perform in environments with different complex levels. Although there are existing reward functions designed for the three environments, our proposed RbRL2.0 does not require such information. Instead, the original reward functions are used to purely evaluate the performance of the trained policies.\nTo evaluate the effectiveness of our proposed method, we evaluate our method against RbRL across rating classes of 3, 4, 5, and 6, corresponding to training with 2, 3, 4, and 5 KL divergence terms, respectively. The hyperparameters used in our experiments are provided in Table 1. To ensure reproducibility, each case is implemented with 10 runs using different seeds. The performance of both RbRL and the new RbRL2.0 is evaluated based on the original environment reward. Table 2 presents the average cumulative rewards with standard errors over 10 runs. Figure 2 compares the learning curves of both algorithms across different rating classes. It can be seen from these results that RbRL2.0 consistently outperforms RbRL on Walker and Quadruped while performing similarly as RbRL in HalfCheetah. Meanwhile, RbRL2.0 demonstrates faster learning and achieves higher cumulative rewards in most cases. It is worth mentioning that RbRL2.0 and RbRL perform comparably in the HalfCheetah environment, which indicates the limited value of policy loss in this environment. Moreover, a deeper analysis shows that both RbRL and RbRL2.0 perform best when there are 4 rating classes for the HalfCheetach environment. In other words, the inclusion of more rating classes will adversely impact the performance since the HalfCheetach environment is rather simple. Similarly, the inclusion of new KL loss terms may not help or even adversely impact the performance in RbRL2.0. For more complex environments Walker and Quadruped, RbRL2.0 consistently outperforms RbRL across all rating classes, demonstrating the effectiveness of RbRL2.0 in complex environments where RbRL fails to yield good performance. From Figure 2c, it can be observed that RbRL2.0 yields a significantly higher cumulative reward than RbRL with a rating class of 5, underscoring its effectiveness in handling more complex environments."}, {"title": "Limitations and Future Work", "content": "One limitation of the proposed approach is the variability in individual rating standards, which may be noisy due to differences in how participants interpret and evaluate the environment. Since RbRL relies on users having a basic understanding of the task and environment, these inconsistencies may reduce the reliability of the collected individual ratings. To address this, future work will involve designing"}]}