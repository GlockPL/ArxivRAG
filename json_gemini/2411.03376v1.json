{"title": "An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services", "authors": ["Zerui Wang", "Yan Liu", "Jun Huang"], "abstract": "This paper presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud Al services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud Al services' trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.", "sections": [{"title": "I. INTRODUCTION", "content": "ARTIFICIAL Intelligence (AI) as a service is a rapidly ex-panding technology paradigm. The AI market is expected to grow as more companies adopt AI technology to remain competitive. Major cloud providers, including Amazon Web Services [1], Microsoft Azure [2], Google Cloud Platform [3], Alibaba Cloud [4], Oracle Cloud, IBM Cloud, and Salesforce Service Cloud, offer AI as a service. This enables customers to develop and deploy AI models using cloud-based platforms. Cloud AI services commonly achieve learning accuracy by using standard metrics such as precision, recall, and F1 score [1]\u2013[3]. However, certain services such as Alibaba Cloud [4] may not explicitly provide these performance met-rics. Additionally, a recent study [5] conducts a detailed investigation into the maintenance of AI services, focusing on computer vision. This research uncovers inconsistencies and evolution risks in Al services. The explainable AI (XAI) aims to develop models and methods that enable human users to comprehend, trust, and manage AI models [6]. A study [7] discusses the role of XAI in computer vision-based decisions. They emphasize that XAI can promote trust in AI computer vision systems through improved understanding and prediction. Another work [8] presents the XAI criterion that refines the functionality ob-jectives for XAI methods. One criterion involves the analysis of feature influence and feature causality. XAI is increasingly adopted in applications that require transparency, fairness, and trustworthiness in decision-making, particularly in sensitive domains [9]. The limitation of the XAI practices is the existing XAI techniques developed are often tailored to specific types of models or cases [10], which makes the XAI practices less reusable and versatile to other appli-cations. Meanwhile, numerous cloud Al services have been provided by cloud platforms to support general applications across domains [1]\u2013[4]. Together, the trend underscores the need to integrate XAI methods with cloud AI services to foster trust in cloud-based AI applications. XAI demands that activities conducted during an X\u0391\u0399 process be traceable and reproducible [11]. Ensuring that the data utilized in the AI models and XAI methods are trustworthy becomes important. Addressing data provenance in XAI operations is essential for guaranteeing that the gen-erated explanations are reliable, verifiable, and consistently reproducible across diverse settings. Despite the numerous XAI frameworks available, a notice-able gap exists among essential components [10] including data processing, methods configuration, and evaluation met-rics. These components collectively form complex pipelines. This observation motivates our proposal: a design by a cloud-native paradigm based on the microservice architecture. This architectural style benefits from its capacity for the indepen-dent deployment of diverse components, streamlined commu-nication via RESTful APIs, and a built-in adaptability that accommodates the introduction of new XAI methods, substitu-tions of AI models, and adjustments in pipeline configurations. Furthermore, our proposed architecture offers precise record capabilities. This ensures that the provenance of every XAI operation is transparent, facilitating the reproduction of XAI tasks or entire workflows. For black-box models or cloud-based AI services, the task of revealing the internal structures of AI models becomes unfeasible, especially for those AI services that encapsulate models behind standard RESTful APIs. We address this challenge by drawing inspiration from XAI methods that focus on feature influence and causality, for example, SHAP [12]. Besides, we propose a method"}, {"title": "that approximates the black-box AI model with a custom-built model and computes the feature contribution values, providing interpretable insights even from opaque AI models. Confronted by these challenges, our work is directed towards the following research questions.", "content": "* RQ1: How to obtain and evaluate XAI results without unfolding the cloud AI service model structure? We investigate the cloud AI services and XAI methods in Section II-A and II-B. This enables us to understand the communication between cloud Al services and the specific requirements for XAI methods. Subsequently, we briefly summarize the applicable XAI methods in the taxonomy, Section IV-A. We also seek the packaged XAI frameworks listed in Section II-C. However, most frameworks are not explicitly compatible with cloud AI services. Therefore, we propose workflow as Figure 2 that integrates Cloud AI with Post hoc XAI, expressed in Section IV-B. Ultimately, scenarios one and two in Section VII-C and VII-B compute and evaluate the XAI results from integrating three major cloud Al services. * RQ2: What are the essential components required for XAI service architecture to deliver feature contribution explanations for models? To implement XAI within a service-oriented framework, Section V delineates the key architectural components critical for integration with existing cloud-based AI ser-vices. Following this, Section VII presents four illustra-tive scenarios using the designed XAI service architecture to explore typical discovery situations. * RQ3: How to collect XAI provenance data from oper-ations to ensure traceability within the XAI service? Referring to the related works in Section II-D, we notice that the provenance data is necessary for XAI operations. Referring to the key components in XAI operations, we provide a graph format design for the XAI provenance data. Section VI introduces how to automatically collect the provenance data from various XAI operations within the XAI service. By retrieving the provenance data, we can identify differences and edit configurations to the XAI operations. In section VII-D, scenario three, we showcase a scenario that optimizes the model by mod-ifying and executing reproduction. This scenario leads to improvements in both model performance and the XAI evaluation metrics. With the operations traceable and reproducible, we present the cloud-agnostic reproduction in scenario four, section VII-E. In this work, we propose an innovative XAI service architec-ture specifically designed to feature contribution explanations, illustrated through a showcase scenario drawn from computer vision cloud AI services. This method involves the utilization of approximation models to generate images, emphasizing the most contributing features. These masked images then act as inputs to create the AI services' predictions. We calculate the prediction changes value between the original and masked images. Leveraging these prediction changes, we compute a comprehensive explanation summary for the AI services,"}, {"title": "providing a transparent overview. The main contributions are summarized as follows:", "content": "* Design cloud-platform-independent X\u03a7\u0391\u0399 service framework. The open API architecture is independent of the cloud-specific AI service. The architecture accommodates first-class entities in the XAI process as unified micro-services. The communication is open API-based, thus encapsulating the variance of models, XAI methods, and inputs and outputs from feature engineering. * Provide explanations across multiple cloud AI ser-vices. Based on the definition of the XAI consistency metric, we derive an explanation summary cross-validated on multiple clouds to observe both the learning perfor-mance of AI services and data augmentation effects. * Reproduce XAI operations through configure-and-rerun. The configuration of services is the receipt of com-posing an end-to-end explanation workflow. By reserv-ing the configuration of each service given a workflow definition, we accumulate the provenance of how each explanation is produced. Through the coordination center of the XAI framework, we can rerun the XAI workflow to reproduce the explanation. We demonstrate the XAI service architecture with four discovery scenarios in Section VII, including (1) Cloud Al performance evaluation, (2) XAI consistency evaluation, (3) Probing of data augmentation effect, (4) Cloud-agnostic re-production on three major cloud service platforms includes Azure Cognitive Service [2], Google Cloud Vertex AI [3], and Amazon Web Services Rekognition [1]. Our study employs consistency metric [8] to assess the explanations derived from multiple cloud AI services. The experimental results help us observe and discover that data augmentation techniques not only enhance all cloud AI service learning performance but also improve evaluation results from the different \u03a7\u0391\u0399 methods. The adoption of XAI frameworks is designed for data sci-ence and machine learning engineers, effectively functioning as a tool for assessment in the development of complex AI systems. A recent study [13] proposes a multi-level gover-nance pattern that integrates team-level XAI practices with organization-level ethical standards, thereby organizing ethical principles. This work introduces an XAI service framework for AI service practitioners, ensuring alignment with ethical guidelines and organizational values. The remaining sections are structured as follows: Section II explores related works on cloud-based Al services and their explainability challenges. Section III summarizes the employed background knowledge. In Section IV, we delve into post-hoc XAI methods and their integration into cloud services. Section V presents our microservices-based XAI architecture. Section VI emphasizes the tracing and repro-ducibility aspects of XAI operations using provenance data. Section VII presents the setup and results of the experiment. Section VIII evaluates the XAI service from the system aspect. The paper concludes by summarizing our findings in Section IX."}, {"title": "II. RELATED WORKS", "content": "This section begins a survey of the growing use of cloud-based Al services for various applications. There is a lack of XAI in cloud services. Following this, a comprehensive overview of various XAI methods and the corresponding im-plementation frameworks is presented. Lastly, the importance of data provenance within XAI for responsible AI practices is discussed."}, {"title": "A. Cloud-based AI Services", "content": "Cloud-based AI services, which offer customized AI models and pre-trained models through APIs for various tasks, have attracted substantial interest due to their versatility and ease of use [2]. A study [14] provided a comparative analysis of cloud computer vision services, focusing on their accuracy, performance, and cost. However, the study did not delve into the specifics of the AI models used or draw conclusions based on the comparative evaluation. Image classification uses machine learning algorithms to categorize images based on their content, offering potential applications across various fields. The lack of explanation can hinder the adoption and trust of these Al systems. For instance, a survey [15] illu-minated the potential biases in visual datasets, emphasizing the necessity for bias discovery and quantification to ensure fairness and transparency in AI solutions. A recent publication introduces the tool named Threshy [16], which helps software developers assess an AI service's confidence score. Integrating configuration into client applications and monitoring systems represents an advance in the practical use and safe deployment of Al services. Both our work and Threshy [16], aim to assess cloud Al services outputs, but significantly differ in goals and methodologies. While Threshy focuses on decision threshold selection, we concentrate on XAI results from service."}, {"title": "B. The Post-hoc \u03a7\u0391\u0399 Methods", "content": "Post-hoc XAI methods are generally classified into two categories [8]: Model-specific and Model-agnostic methods. Model-specific methods, such as CAM-based techniques [17]\u2013[23], are designed for specific models and require access to key parameters or layer contents of the AI model for generating explanations. In contrast, Model-agnostic methods, such as SHAP [12], are more flexible, capable of producing explanations solely from the input and output of any machine learning model. The taxonomy uses a tree topology to organize the layers of categories of the XAI methods [8]."}, {"title": "C. Frameworks and Packages for Implementing XAI", "content": "There are several frameworks and packages have been developed to facilitate the implementation of XAI. Dalex [24], for example, constructs a wrapper around prediction models to enable their exploration and comparison using a multitude of model-level and prediction-level explanations. IBM's Ex-plainability 360 toolkit [25] incorporates an array of model explanation methods within its Python library. Meanwhile, Microsoft's InterpretML [26] supports eight XAI methodolo-gies. Other libraries such as Captum [27] and OmniXAI [10] provide extensive collections of XAI techniques. Table I shows a comparative summary of various XAI frameworks."}, {"title": "D. Data Provenance for XA\u0399", "content": "Data provenance in the context of XAI is a component used to trace and reproduce operations. A review [40] explores the ethical considerations and presents the implementation of data provenance to ensure the AI system is responsible. Data provenance ensures transparency and accountability [40] of operations. The PROV-DM model [41] offers standardized components of representing provenance information. It defines concepts and relationships to capture entities involved in a process, activities that took place, and their interconnections. Regarding practical implementations, a machine learning pipeline is proposed [42] emphasizing reproducible as a form of data provenance. Renku [43] is an open online platform tracking data, code, and results with Git. It assists researchers in evaluating, reproducing, and reusing data and algorithms. WholeTale [44] also promotes reproducibility by enabling researchers to capture and share data, code, and workflow. The work introduces a system [45] designed to extract, store, and manage both metadata and provenance information for common artifacts in machine learning experiments, including datasets, models, predictions, evaluations, and training runs. The experiment [46] enables provenance to be available as metadata. This study aims to enable the XAI service system to provide native provenance data. The XAI operations are reproducible, as provided graph-formatted provenance data, which includes datasets, models, XAI methods, and opera-tional settings."}, {"title": "III. BACKGROUND", "content": "In this section, we provide a comprehensive background on the selected XAI techniques and evaluation metrics."}, {"title": "A. CAM-based XAI methods", "content": "Methods for feature contribution explanation reveal and visualize the correlations between specific content elements, such as pixels in an image, and the resulting decision from a model. In explaining vision tasks, Class Activation Mapping (CAM) [47] initially employs the global average pooling layer to localize features in Convolutional Neural Networks (CNNs). However, the method needs to modify the original layer of the model. As optimizations, Grad-CAM obtains [17] the localization weight by the average gradient of one layer instead of replacing it. Grad-CAM++ [18] is an improved version that uses second-order gradients. EigenCAM [20] takes the first principle component of the activation without class discrim-ination. LayerCAM [21] spatially weight the activations by positive gradients. XGrad-CAM [22] scales the gradients by the normalized activations."}, {"title": "B. Metrics for Evaluating XAI Techniques", "content": "The previous work [8] reviews and compares XAI eval-uation criteria and metrics. To compare the XAI outcomes objectively, we use consistency metrics [8] to perform the assessment and comparison of various XAI methods across identical datasets. We outline the specific equation to compute the intra-consistency [8], also known as the XAI stability. 1) The Prediction Change Value: The metrics are based on the feature importance, also present as feature contribution, provided by XAI methods. Assume f(x[i]) denote the model's predicted value for a dataset x[i]. We use x[i]+ to symbolize the transformed or augmented data sample, an image under masking. We introduce an equation to compute the difference value induced by the feature, denoted as the prediction change value in Equations 1. \n\n \\delta(x^{[i]})|_{rel} = \\frac{f(x^{[i]+}) - f(x^{[i]})}{f(x^{[i]})}  \\tag{1}\n\n Additionally, the prediction change values of the entire dataset X can be aggregated. The experiment, in Section VII-B, uses histograms to present the distribution of prediction changes. 2) XAI Consistency Evaluation: The XAI consistency met-rics as described [8] include both Intra-Method Consistency and Inter-Method Consistency. In our study, the objective is not to compare inter-XAI methods. Therefore, we specifi-cally focus on Intra-Method Consistency, also known as \u03a7\u0391\u0399 stability evaluation, to assess the quality of the generated explanations by a series of XAI methods. Here, we present the stability evaluation. We define the set of explanations, denoted by E = {\u00a31,\u00a22,...,\u00a2m}, where \u03be\u03af represents the i-th data sample explanation. Next, we define the distance between any two pairs of summaries as fa(\u03be, \u03be\u03af), where fa is the distance function, and g\u00b9 and are two different explanation. Then, we describe the combination of choices for selecting any pair of explanations from E, given by K = (m). This combination represents the number of ways to choose two explanations from the m in total. \n\n  \\overline{f} = \\frac{1}{K} \\sum_{k=1}^{K} f_a(\\xi^i, \\xi^j), (i \\neq j, i \\leq m, j \\leq m) \\tag{2}\n\n Here, f represents the stability metric computed as the average over all K distance values, where each distance value fa is calculated for a specific pair of explanation \u03be' and \u00a7. This metric is used for the assessment of various XAI methods in scenario three, Section VII-D. In this case, the distance value fa is represented by the prediction change value."}, {"title": "IV. THE POST HOC X\u0391\u0399 METHODS WITH FEATURE CONTRIBUTION EXPLANATION", "content": "In this section, we discuss the post-hoc XAI methods. We explore feature contribution explanation techniques. Then, we present a design that integrates feature contribution XAI methods with cloud AI services, addressing RQ1."}, {"title": "A. XAI Methods Taxonomy", "content": "XAI methods are mainly classified into two categories. One is to develop interpretable models rather than to explain deep learning models [48]. Interpretable models, such as linear regression, logistic regression, decision trees, and k-nearest algorithms, make predictions in ways that are relatively under-standable to humans. These models are transparent, allowing researchers to understand decision-making by analyzing inter-nal conditions, such as branches in decision trees. Compared to black-box AI models, interpretable models have the natural advantage of transparency. The other category pertains to the post-hoc explanation methods [49]. This approach is typically applied to AI systems that are already deployed and operational, with the need for explanations emerging after the system has made a decision. Post hoc explanation methods are further grouped into model-agnostic and model-specific. Model-specific methods rely on the structure and properties of the model as information, such as CAM-based techniques [17]\u2013[23]. Therefore, model-specific methods typically require a distinct set of parame-ters and configurations, necessitating the extraction of this data from within the model. This limitation restricts their applicability across various black-box model scenarios. On the other hand, model-agnostic methods provide explanations by analyzing the input-output relationship of the AI system without information about the internal structure of the model. For example, SHAP [12] and LIME [28], model-agnostic methods are flexible and can be applied to various black-box models."}, {"title": "B. Integrating Cloud AI with Post hoc XA\u0399", "content": "Cloud AI services encapsulate AI models with well-defined RESTful APIs [1]\u2013[3]. The opacity of the AI models has limitations on the comprehension of how the prediction is made, even though the prediction produces a high accuracy level. It is a limitation for a certain adoption of the AI services that has an emphasis on transparency of the AI models. We use post hoc XAI methods for feature influence and causality to devise an approximating model that correlates the inputs and outputs to generate an explanation of feature contribution values. Following this principle, we consider computer vision AI service suitable for adopting post hoc X\u0391\u0399 methods for two reasons: (1) A variety of state-of-the-art deep learning models already exist in the field of computer vision as standard models. Even if there is no access to the AI models running within a cloud-based AI service, these established models can serve as suitable approximations; (2) XAI methods specifically designed for elucidating computer vision model explanations are widely available in the literature. We describe the process and data flow in Figure 2 at the run-time. The initial step involves the preparation of the input dataset. Given that cloud model services typically function as a black box, with internal parameters and model structure concealed from the user, we employ approximation models. The original dataset is then used in three distinct steps: 1) Input images are directly submitted to the cloud-based AI service, which returns their prediction outcomes. 2) The images are processed by an approximation model, employing model-specific XAI methods [17]\u2013[23] to generate salience maps. 3) The original images, overlaid with the salience maps, are used to create masked images highlighting only the important features. Subsequently, both the original and masked images are submitted to the cloud Al service for inference, yielding model predictions and confidence values. Comparing these two sets of results allows us to quantify the impact of features. The minimal variance between the two sets indicates similar feature importance values. We also evaluate the XAI consistency metrics, offering insights into the XAI evaluations within cloud Al services. In summary, the exploration showed that post hoc XAI methods, which elucidate the correlation between inputs and outputs, can approximate feature contributions for cloud AI services. As illustrated in Figure 2, the workflow produces explanations for cloud model predictions."}, {"title": "V. \u03a7\u0391\u0399 SERVICE ARCHITECTURE AND API DESIGN", "content": "We design the XAI operations as well as comprehensive API architecture to answer RQ2. This approach led us to define a reference architecture comprising four distinct layers from top to bottom. They are the user interface, the coordination center, the core microservice, and the data persistence. The overview of this reference architecture is illustrated in Figure 3."}, {"title": "A. API Design for Microservices", "content": "One of the key features of this proposed architecture is its non-intrusive application of XAI methods. This approach allows us to apply XAI techniques without significantly dis-rupting or modifying existing system operations. We have completed the API design for this microservice system, with these APIs systematically arranged and hosted on Swag-ger Hub, adhering to OpenAPI 3.0 standards. Detailed API documentation featuring an array of functional interfaces is available in our published Swagger document."}, {"title": "B. API-based Coordination Center", "content": "The coordination center processes requests from the user interface layer and activates the necessary components for task execution. The core functions of the coordination center are managing microservices, overseeing XAI tasks, and retrieving data. To manage microservices, the coordination center works with the user interface to handle service registration and mod-ification. It logs endpoints and functional types and initiates the corresponding microservice upon request. In XAI task management, users submit a task sheet out-lining the task type (either XAI or evaluation), associated"}, {"title": "C. Data Processing Microservice", "content": "Data Processing serves as a critical component in the system architecture. It is primarily responsible for managing data inflow into the system and orchestrating data processing tasks. This microservice takes into account the need for streamlined data management, providing interfaces for users to upload their datasets in specified formats, organize data, and maintain the consistency and integrity of the data. Data Processing allows users to not only upload their data but also assemble them into datasets. Furthermore, it provides the option to apply data augmentation techniques to enhance the variability and volume of the original data. Data augmentation is instrumental in enhancing the robustness of the AI models and improving the performance of XAI methods. The Data Processing Microservice stores data in the user-specified cloud database to ensure data security and ownership. This design addresses multiple aspects of data management, including data import, processing, augmentation, and secure storage. The incorporation of these features ensures that the Data Processing Microservice not only maintains the data life-cycle within the system but also provides users with flexibility and control over their own data."}, {"title": "D. AI Model Microservice", "content": "The AI Model Microservice is specifically designed to handle prediction tasks. This microservice is flexible in in-tegrating various cloud AI services from cloud platforms including Amazon Web Services, Google Cloud Platform, and Microsoft Azure. It communicates with these platforms using their specific APIs, ensuring seamless connectivity and usage of the cloud Al services. By connecting with these platforms, users are given access to a broad spectrum of AI models and tools readily available on the cloud. Another notable feature of the AI Model Microservice is the ability to integrate pre-trained models provided by the users. The integration of user-provided models is facilitated through a well-defined RESTful API specification. Users need to ensure their models adhere to this specification, and once that is done, their models can be easily plugged into the system."}, {"title": "\u0395. \u03a7\u0391\u0399 Method Microservice", "content": "The XAI Method Microservice in the system's architecture provides explanations for AI model predictions. These XA\u0399 methods work based on the contributions of individual features to the final prediction. Once an XAI method is integrated into the system, it can be repeatedly used for various similar types of XAI tasks without the need to consider the development steps again. This approach not only simplifies the procedure of applying XAI methods but also enhances the efficiency of the XAI operations. Furthermore, this microservice provides a seamless connec-tion with the Data Processing Microservice and the AI Model Microservice through RESTful APIs. This enables it to access data and models, generate explanations, and provide users with comprehensible insights into the AI models."}, {"title": "F. Evaluation Microservice", "content": "The Evaluation in the system brings forward a reliable approach to assess both the AI models and the deployed XAI methods. It is crucial to guide AI practitioners in their decision-making process and enhance trust in the AI models and their explanations. This service is equipped to evaluate the performance of AI models and XAI methods by examining the explanation results. It employs the consistency evaluation method referenced in [8]. This method provides a compre-hensive evaluation of the stability and reliability of the XAI methods, thus ensuring that the selected XAI methods yield consistent explanations. The results derived from this evaluation are efficiently stored and maintained for future reference. This also allows for continuous monitoring and comparison of different models and methods, paving the way for continuous improvement and development in the system. In summary, the proposed architecture provides a compre-hensive solution for implementing XAI as a service. It consists of several key components: the User Interface, Coordination Center, Data Processing Microservice, AI Model Microservice, XAI Method Microservice, and Evaluation Microservices. These components work synergistically to enable the effective execution and evaluation of XAI tasks."}, {"title": "VI. DESIGN PROVENANCE META DATA OF X\u0391\u0399 OPERATIONS", "content": "As introduced in RQ3, the purpose of provenance data is to trace and reproduce XAI operations. The design of microser-vices brings another benefit in addition to the integration of heterogeneous components involved in XAI operations. The design helps us understand the relations of these components, focusing on their characteristics with abstraction. For example, we can retrieve the provenance data of two XAI pipelines and identify the difference in the configuration. This helps trace and observe how a certain explanation is produced, improving transparency and trustworthiness. To this purpose, the provenance data needs to capture the properties of XAI services, the configuration of XAI tasks and pipelines, an explanation summary from the XAI method and the data set, and XAI cross-validation results. We apply graph format data to organize, visualize, and understand the relations between XAI provenance data."}, {"title": "VII. ARCHITECTURE USAGE: \u03a7\u0391\u0399-BASED CLOUD AI SERVICE DISCOVERY SCENARIOS", "content": "The usage of the open API architecture is to obtain ob-servations from discovery scenarios on (1) Cross-Cloud AI Service Metrics - assessing Al services perfor-mance variation across multiple cloud services; (2) \u03a7\u0391\u0399 Explanation and Evaluation - feature contribution explanation and consistency evaluation cross clouds; (3) Explainability Improvement - discovering the impact of data augmentation on model explainability across clouds; and (4) Operation Provenance - reproduction of X\u0391\u0399 operations. Our scenarios demonstrate the architecture is cloud compatible and thus extensible to develop further discovery scenarios on multiple clouds."}, {"title": "A. The Microservices Configuration and Initialization", "content": "This section presents configuration settings of each mi-croservice in the framework. The database microservice uses the ImageNet dataset [50], ensuring efficient data storage and access during testing phases. The model microservice integrates cloud AI services via Restful APIs. \u03a7\u0391\u0399 microser-vices are responsible for generating explanations in saliency maps. The coordination center effectively manages these di-verse microservices, ensuring a streamlined execution of tasks leading to the creation of saliency maps. Our evaluation task involves validating explanations produced by approximation models. Additionally, we apply data augmentation techniques to explore potential improvements in model and XAI perfor-mance. The source code for implementation of the XAI service discovery scenarios is available in the GitHub Repository 2. Database Microservice. We implement the database mi-croservice using Azure Blob, a cloud database service that manages data uploading and retrieval for our case studies. Integrate Cloud AI Service. Incorporating the APIs of cloud Al models into a microservices infrastructure is crucial for completing XAI service tasks. Post-hoc XAI methods need access to Al model results in response to various inputs. This integration provides a unified interface and pipeline for executing XAI experiments. Integrate Pre-trained Models. We employ ResNet [51] and DenseNet [52] as approximation models due to their exceptional performance on ImageNet image classification benchmarks [50]. The pre-trained models are encapsulated within microservices, and their output is standardized to ensure compatibility with other services."}, {"title": "B. Scenario One: XAI Consistency Evaluation", "content": "Attribute-based post-hoc XAI methods produce explana-tions by applying feature masking and mutation, noting changes in predictions. Evaluating the variation in explanations across different XAI methods is essential to ensure their trustworthiness. Trustworthiness and transparency are critical technical requirements when applying XAI methods to \u0391\u0399 models [53]. Improved flow by rephrasing As shown in Figure 2, the evaluation consists of three stages, beginning with the preparation of feature changes. cloud AI services cannot directly derive saliency maps. To measure changes from feature masking and mutation, we generate saliency maps using approximation models, applying an XAI method. Next, we overlay the saliency maps onto the original images to create marked images for input into the cloud AI services. Finally, processing both masked and original images through cloud Al services generates two sets of predictions. The differences in predictions are analyzed to explain the importance of the features identified by the saliency maps, us-ing XAI. Figure 2 demonstrates the workflow to approximate feature contribution exploration with evaluation. It should be noted that the above inputs are from the test datasets, not from the training dataset. Then, we use the XAI methods and model predictions to compute prediction changes. Finally, we apply consistency metrics to provide the summarized evaluation. 1) Selection of Approximation Model: In the initial stage, we focus on employing a valid approximation model, as outlined in Figure 2. We consider two candidate approxima-tion models, ResNet [51] and DenseNet [52]. As a result, Figure 5 illustrates the comparative analysis of prediction change values, highlighting the statistical differences between approximation models. Although the differences in prediction"}, {"title": "C. Scenario Two: Data Augmentation Effects on Cloud AI Service Performance", "content": "Cloud service platforms such as Microsoft's Azure Cog-nitive Services [2], Google's Vertex AI [3], and Amazon's Rekognition [1] offer online AI services, including image classification models that can be trained with user-provided datasets. Specifically, Azure Cognitive Services limits the number of distinct labels to fifty for image classification. Cloud AI services primarily provide evaluation metrics such as Precision, Recall, and F1-Score. We devise three cases. One case uses the original ImageNet dataset without data augmentation, while the other two apply data augmentation to the ImageNet dataset. Cloud AI services divide the augmented data into training and test sets. These services remain as black boxes. This scenario is to evaluate the effect of the learning perfor-mance of these cloud AI services with data augmentation. We analyze the Precision, Recall, and F1-score metrics, as detailed in Table II. In particular, data augmentation only reduces precision in Azure AI service since the precision is already high without data augmentation. However, in all the other cases, both CutMix [54] and PuzzleMix [55] augmentation algorithms improve the Precision, Recall, and F1-score. The data augmentation improves the overall cloud model perfor-mance F1 score by 0.11 and 0.13. Summary - The use of data augmentation without the expansion of the training dataset reduces the rate of false negatives and improves the F1-score across three cloud Al services on the ImageNet dataset."}, {"title": "D. Scenario Three: Data Augmentation Effects on Explana-tion", "content": "Data augmentation is a tool for boosting the performance of deep learning models by generating a diverse set of syn-thetic data from existing samples. To further investigate the performance of XAI methods in more complex scenarios, two more advanced data augmentation methods, CutMix [54] and PuzzleMix [55], are introduced and tested in this experiment. CutMix and PuzzleMix blend images in a patch-wise manner and rearrange patches from multiple images, respectively. They have been demonstrated to enhance model performance significantly [55]. However, the impact of these augmentation methods on the interpretability of models and the effectiveness of XAI methods remains unexplored. We commence our exploration by applying the advanced data augmentation techniques, CutMix and PuzzleMix, to our test dataset. The effect of these augmentations on the Al model's performance is summarized in Table II. Upon evaluation of the three cloud AI services, we observe a general improvement in Precision, Recall, and F1-score metrics under the application of CutMix and PuzzleMix, echoing the effec-tiveness of these methods in enhancing model performance as reported in the literature [55]. The histogram in Figure 6 is the visual representation of prediction changes of the test data set for the three cloud Al services. We divide samples into fifty equal bins in the"}, {"title": "E. Scenario Four: Reproduction of XAI Explanation", "content": "From the scenarios described above, we have experienced the complexity of XAI evaluation. XAI evaluation extends beyond merely running a single XAI algorithm. It involves a complex composition of entities and tasks, including datasets, feature masking and mutation, data augmentation, benchmark model approximation, cloud Al services, and various X\u0391\u0399 methods. The scale of these combinations can rapidly escalate the complexity involved in providing explanations. Tracing XAI pipelines and ensuring transparency in XAI explanations are key technical requirements for XAI evaluations. We devise four cases to demonstrate the XAI pipeline provenance in our framework design aspects. Figure 8 illustrates the prove-nance graph of XAI variation using ResNet, Azure Cognitive Service, and Grad-CAM XAI method. The metadata defined in section VI become the vertices in the graph. Each edge in the provenance graph is an API-based service invocation. A red colored vertex indicates a certain variation introduced to the configuration of a pipeline. The blue vertices indicate related vertices in the paths that are affected by the change. A variation results in a new pipeline graph that represents the adjustments. Cross-dataset adaptability. Database microservice is de-signed to serve data via a RESTful API, integrating with cloud data storage services, in the case study, Azure Blob Storage. It ensures the data required by XAI are organized by group and ready for retrieval. By altering the configuration parameters, users can invoke alternative cloud databases, enhancing the system's flexibility. Case one shows a reference pipeline configuration. The dataset can be replaced without changing other configurations in a pipeline. Configurable model integration Case two illustrates the necessity for users to replace the model currently in use, exemplified by replacing the ResNet model with DenseNet."}]}