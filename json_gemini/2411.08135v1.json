{"title": "On the role of speech data in reducing toxicity detection bias", "authors": ["Samuel J. Bell", "Mariano Coria Meglioli", "Megan Richards", "Eduardo S\u00e1nchez", "Christophe Ropers", "Skyler Wang", "Adina Williams", "Levent Sagun", "Marta R. Costa-juss\u00e0"], "abstract": "Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MUTOX dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving classifiers, rather than transcription pipelines, is more helpful for reducing group bias. We publicly release our annotations and provide recommendations for future toxicity dataset construction.", "sections": [{"title": "1 Introduction", "content": "With the growing prevalence of machine-learning systems capable of processing and generating speech, there is rising interest in speech-aware toxicity detection (Costa-juss\u00e0 et al., 2024; Ghosh et al., 2022; Liu et al., 2024; Nandwana et al., 2024). Traditional cascaded approaches to speech toxicity detection use automated speech recognition (ASR) to convert speech to text, before applying a standard text classifier. This strategy has two main issues. First, it eliminates rich prosodic and contextual information present in speech, which could degrade model performance. Second, text-based toxicity detection systems are well known to exhibit significant biases against minoritized groups (Borkan et al., 2019; Dixon et al., 2018). For instance, many systems are more likely to consider African American English (AAE) as toxic (Resende et al., 2024), while others denote the mere mention of identities such as \u201cgay\u201d and \u201clesbian\u201d as toxic (Dias Oliva et al., 2021). Often, these issues are attributed to biases in the training data. Because minoritized communities are overwhelmingly the subject of online toxicity (Borkan et al., 2019; Dixon et al., 2018), classifiers misinterpret benign group mentions as toxic, producing a disproportionate rate of false positives for marginalized groups (Dixon et al., 2018). Given these limitations, recent research has sought to develop toxicity classifiers that operate directly on speech.\nIn this work, we perform a systematic comparison of speech-based and cascaded text-based toxicity detection systems. Specifically, we hypothesize that access to speech audio provides useful contextual information, which could reduce false positives. To investigate this, we produce a new set of annotations for a multilingual speech toxicity dataset, MUTOX (Costa-juss\u00e0 et al., 2024), annotating for both toxicity and group mentions while also correcting automated transcripts. To ensure consistent and accurate data, annotations were performed by the authors using a rigorous multi-stage process of cross-checking and discussion.\nWe leverage these annotations to produce critical new insight into both the efficacy and biases of speech-based and text-based toxicity detection models. Our work reveals that incorporating speech data at inference time improves performance and reduces false positives on samples mentioning group identities, and eliminates false positives on ambiguous samples. Furthermore, we find that this bias is not due to of transcription error, but of the classifier itself. Our annotations are publicly-available to facilitate research into the fairness and efficacy of speech-based toxicity detection."}, {"title": "1.1 Contributions", "content": "To summarize our main contributions, we:\n1.  Generate and release 1954 group annotations for speech toxicity detection fairness evaluations in English and Spanish;\n2.  Compare text- and speech-based toxicity detection systems, including detailed investigation of performance on ambiguous samples;\n3.  Isolate the role of transcription failure in text-based toxicity classifiers;\n4.  Provide extensive analysis of the challenging ambiguity of toxicity annotation in speech."}, {"title": "2 Background and related work", "content": ""}, {"title": "2.1 Bias in toxicity detection", "content": "Toxicity detection systems have long been known to exhibit significant biases (see Garg et al. 2023 for a review). One major issue is the over-representation of certain identity markers in toxicity detection training data, often correlated with toxic content (Dixon et al., 2018). For instance, models tend to conflate group mentions with toxicity, particularly for groups frequently targeted online, such as women, LGBTQ+ individuals, and minoritized racial, ethnic, or religious groups (Borkan et al., 2019; Dias Oliva et al., 2021; Park et al., 2018). Models explicitly designed to detect anti-group bias also incorrectly associate group mentions with toxicity (Sahoo et al., 2022), unable to distinguish the use of a term from a mention (Gligoric et al., 2024). Understanding how group mentions also bias speech toxicity classifiers is the key motivation of this work.\nToxicity classifiers have also been found to exhibit significant bias against AAE (Resende et al., 2024), partly due to annotator biases (Goyal et al., 2022; Sap et al., 2022). Racial bias has also been observed in hate speech detection, which also suffers from the challenge of disambiguating genuinely hateful from reappropriated words (Davidson et al., 2019; Sap et al., 2019).\nOur work draws inspiration from Civil Comments (Borkan et al., 2019), a text toxicity dataset with group annotations. However, to better handle ambiguous cases, we opted to produce annotations ourselves rather than rely on crowd workers."}, {"title": "2.2 Speech toxicity detection", "content": "There is increasing interest in toxicity detection for speech data (Liu et al., 2024; Nandwana et al., 2024). The straightforward approach for constructing a speech-based toxicity detection system is a multi-stage pipeline, comprising an ASR stage followed by a text toxicity classification stage (Barrault et al., 2023). Alternatively, models that operate directly on speech (e.g. Costa-juss\u00e0 et al. 2024) typically utilize self-supervised speech encoders trained on large volumes of speech data, including wav2vec (Baevski et al., 2020), WavLM (Chen et al., 2022), and SONAR (Duquenne et al., 2023). Prior work in speech profanity detection suggests that models benefit from access to \"audio properties like pitch, emotions, [and] intensity\" Gupta et al., 2022, p. 4.\nWhile there are both monolingual (Ghosh et al., 2022) and multilingual (Costa-juss\u00e0 et al., 2024; Gupta et al., 2022) speech toxicity datasets, none are annotated with group information, precluding detailed analysis of bias against group mentions."}, {"title": "3 Annotating Mutox", "content": "The foundational contribution of this work is a new, high-quality set of annotations for the MUTOX test partition, allowing us to evaluate classifier bias against group mentions. We believe this represents the first fairness audit dataset for multilingual speech toxicity detection."}, {"title": "3.1 The MUTOX dataset", "content": "MUTOX (Costa-juss\u00e0 et al., 2024) is a large-scale, multilingual speech toxicity dataset covering 30 languages. Each audio sample is accompanied by a text transcript produced by an open-source ASR model (Radford et al., 2023). For annotation tractability, we focus only on the English and Spanish test partitions, covering a total of 1954 samples."}, {"title": "3.2 Stage 1: Initial annotation", "content": "We asked three annotators per language (all core contributors to this paper; see appendix A) to annotate the MUTOX test set. The annotators were all native-level proficiency and spanned multiple language varieties (such as British and American English) to capture variety-specific interpretations. Annotators used LabelStudio (Tkachenko et al., 2020) with a custom interface (see appendix B) to annotate for toxicity, group mentions, and automated transcript correctness."}, {"title": "3.2.1 Toxicity", "content": "For toxicity, annotators were asked \u201cDoes the audio contain toxicity?\u201d and presented with options for \u2018Yes,\u201d \u201cNo,\u201d or \"Cannot say,\" the latter indicating that the audio was unclear, truncated, or context-dependent. Annotators were instructed to use the toxicity definition from the original MUTOX annotation guidelines (see appendix C), which defines toxicity as language which is \"typically considered offensive, threatening or harmful.\u201d This includes profanities and language related to physical violence, bullying, pornography, or hate speech."}, {"title": "3.2.2 Group mentions", "content": "For group annotation, annotators were asked \u201cDoes the audio mention, or refer to (either explicitly or implicitly), any of the following?\u201d to which they could respond with one or more of \u201cGender identities\u201d, \u201cSexualities,\u201d \u201cReligious groups,", "Racial or ethnic groups,\u201d \u201cDisabilities,\u201d \u201cSocial classes or socio-economic statuses,\u201d or \u201cNone of the above.": "f any group was selected, annotators were asked then a follow-up about which specific group was mentioned. For example, in the case of gender identities, they were asked \"Which gender identities are mentioned or referred to?\u201d with predefined options:", "Male, man or boy,": "Nonbinary or gender non-conforming,\u201d and \u201cTransgender."}, {"title": "3.2.3 Transcript correction", "content": "After toxicity and group annotation, annotators were shown the audio's ASR transcript and asked \u201cDoes this transcript match the audio?\u201d For the 21% of samples where the transcript was inaccurate, annotators were required to correct it manually.\nBefore Stage 1, annotators conducted a pilot analysis of 20 samples (later discarded) to evaluate the interface and identify issues with the guidelines. Annotators met frequently throughout Stage 1 to discuss problem cases and refine the guidelines, particularly regarding group annotation. In total, each annotator reviewed approximately 950 samples, spending approximately 30 to 45 seconds per sample."}, {"title": "3.3 Stage 2: Individual review", "content": "Stage 1 responses were collated, and a majority vote was calculated for each sample. For questions allowing multiple selections (e.g., group mentions), the majority vote was the set of options selected by at least two annotators. Each annotator then independently reviewed the majority vote on a sample-by-sample basis. Annotators flagged samples where they disagreed with the majority vote for further discussion in Stage 3, alongside all samples where there was complete disagreement. Unflagged samples were assigned the majority vote as the final annotation."}, {"title": "3.4 Stage 3: Group review", "content": "Finally, annotators collectively reviewed all samples flagged during Stage 2, with the goal of sharing cultural knowledge and establishing consensus. Discussions were conducted in language-specific groups, where the annotator who flagged a sample presented their rationale, followed by a group discussion. Annotators were typically able to reach a consensus, but a \"No consensus\u201d label was occasionally assigned when annotators could not agree on a final label. Note that while \"No consensus\" indicates that the annotators cannot agree on an outcome, \u201cCannot say\u201d indicates that annotators agree that toxicity could not be determined. For example, all annotators might concur that the sample's interpretation depends on external context, such as the identity of the speaker or audience."}, {"title": "4 The role of speech context", "content": "We compare four representative toxicity classifiers to evaluate the utility of using speech data directly as opposed to cascaded ASR-based systems, and to isolate the role of speech during training from during inference."}, {"title": "4.1 Toxicity classifiers", "content": "ETOX (Costa-juss\u00e0 et al., 2023) is a text-only wordlist-based classifier that supports 200 languages. While offering extensive coverage, it will only detect lexical toxicity and cannot account for context-dependent toxicity in polysemous words."}, {"title": "4.2 Methods", "content": "For each of the four models, we extract predictions for every sample in the English and Spanish MUTOX test sets. For ETOX, this is via lexical matching, whereas the model-based approaches all return a continuous toxicity score, subsequently binarized using a threshold. To ensure a fair comparison among all classifiers, the threshold was tuned on a per-language basis using the MUTOX validation partition to match the precision of ETOX. We evaluate each model's performance using $F_1$-score, precision, and recall, and evaluate their bias against group mentions using false positive rate (FPR), following Dixon et al. (2018)."}, {"title": "4.3 Results", "content": "Our evaluation reveals differences in the performance of speech-based and text-based toxicity detection models when sensitive groups are mentioned.  shows that models relying solely on text (ETOX, DETOXIFY) exhibit a reduced $F_1$-score. On the other hand, both models trained with speech data (MUTOX-ASR, MUTOX) show a slight increase in $F_1$-score, but it is only the model with access to speech at inference time (MUTOX) that shows an increase across both precision and recall. Overall, while MUTOX shows the worst $F_1$-score of all classifiers, its precision is markedly higher than MUTOX-ASR (given equivalent threshold tuning), which is particularly important in reducing false positives.\nTurning to FPR,  shows clear differences between classifiers. Wordlist-based ETOX exhibits a high FPR that increases further when groups are mentioned, as does speech-trained MUTOX-ASR. In contrast, DETOXIFY and MUTOX both show low FPRs which decrease on group mentions. While the high FPR for ETOX is expected given the coarse nature of a wordlist, the differences between MUTOX-ASR (increase FPR on group mention) and MUTOX (decrease on group mention) are particularly interesting. Both models are trained jointly with speech and text data, but only MUTOX has access to speech data at inference time. This suggests that if a model is trained on both speech and text, then making speech unavailable at inference time worsens anti-group bias. This may be due to an over-reliance on group mentions as cues in the absence of important speech context.\nAmbiguous samples-those labeled \u201cCannot say\" or \"No consensus\u201d\u2014are a particular challenge for the wordlist-based ETOX and MUTOX-ASR, while DETOXIFY and MUTOX show an FPR of 0% ()."}, {"title": "5 Effect of transcription error", "content": "One potential root cause of the failures observed in some cascaded ASR-based systems could be the ASR process. In other words, to what extent are the performance differences between the text-based classifiers a result of transcription failures rather than biases in the classifier itself? To address this question, we re-evaluate each classifier using the annotator-corrected transcripts.\nIn figure 5a, we observe that correcting the transcripts leads to a predictable improvement in the overall performance of the text-based classifiers. At the same time, shows that the effect on the false positive rate (FPR) specifically for group mentions was minimal. This suggests that transcription errors alone do not account for the observed biases in toxicity detection when group mentions are present and that refining transcription pipelines is unlikely to be a productive strategy for reducing bias in speech toxicity detection systems."}, {"title": "6 Ambiguity in toxicity annotation", "content": "Our hypothesis that speech context can support less biased toxicity detection is predicated on the idea that toxicity itself is often highly subjective and context-dependent, making it hard to detect from outside of the initial conversation. Indeed, our annotation process is a testament to this fact. While we intentionally designed the annotation process with multiple stages to support interactive discussion and consensus building, an analysis of annotator disagreement demonstrates the extent to which toxicity judgments can vary.\nAfter the first stage of annotation, annotators only unanimously agreed on toxicity in 66% of samples. For the remaining 32% of samples, at least two annotators agreed, producing a majority vote, but for 2% of samples, every annotator voted differently. A total of 7% of samples were flagged for Stage 2 discussion (see figure 6a). These samples tended to be challenging to annotate, often requiring some degree of inference to determine what was left unsaid. After review, annotators could not agree (\u201cNo consensus", "Cannot say\" (see figure la).\nFrom the selection of flagged samples in table 3, we see that a variety of factors provoke discussion. For instance, annotators were unable to determine whether \u201cyou fuckers": "EN-3) was said in jest. The toxicity of EN-4 depends on whether \u201cn***a", "monstruo": "monster"}, {"title": "7 Discussion & Conclusion", "content": "Leveraging our new, high-quality set of group annotations for the MUTOX test partition, we compared the performance and biases of text- and speech-based toxicity classifiers. Our analysis revealed that models that make use of speech data during both training and inference exhibit reduced FPR bias against group mentions. For ambiguous samples, we found that models trained on speech but without speech access at inference time exhibit an increased FPR, suggesting that the multimodal models rely on spurious correlations when lacking an informative modality. Finally, we found that improving the quality of automated transcripts does little to reduce bias in English and Spanish, but this may change with lower-resourced languages where ASR systems exhibit poorer performance (Pratap et al., 2023)."}, {"title": "7.1 The importance of multimodality", "content": "Speech is not simply spoken text-the two linguistic forms diverge in grammar, morphology, and register. As a richer medium (Daft et al., 1987), speech encodes more information that helps one better ascertain communicative intent. As such, even when the \u201cwords\" converge, prosodic cues-e.g., inflection, tone, etc. and contextual cues-e.g., speaker identity, social setting, etc. in speech can contribute to differences in how meaning is construed between the two modalities (Kraut et al., 1992). By illustrating the improved performance of toxicity classifiers when speech data is introduced at inference time, we build on a growing body of work that demonstrates performance payoffs when engaging in multimodal and multitask learning."}, {"title": "7.2 Toxicity beyond social media", "content": "Much existing research on toxicity detection focuses on social media content moderation as the primary use case. As a result, toxicity detection datasets (e.g. Borkan et al., 2019) are often drawn from social media. This narrow focus may neglect increasingly relevant applications. For instance, with the general public's growing interaction with large language models (LLMs), it may be desirable to detect toxicity in generated responses, which may be orthogonal to determining whether the content itself is safe. Similarly, ensuring that machine translation systems do not introduce additional toxicity beyond what is present in the source is another emerging challenge (Sharou and Specia, 2022).\nIn contrast to earlier datasets, the MUTOX dataset is primarily extracted from \"raw web corpora\" (Costa-juss\u00e0 et al., 2024, p. 2), representing a broader range of toxicity data. While this introduces certain biases (see \u00a78), it reflects a positive shift toward evaluating toxicity in more diverse contexts beyond social media. As discussed in \u00a76, it is already challenging for annotators to ascertain toxicity after the fact. As toxicity datasets expand to include novel application domains, new combinations of modalities (e.g. Kiela et al., 2020), and additional languages, robust annotation will become increasingly important."}, {"title": "7.3 Practical recommendations", "content": "To support the development of future speech toxicity datasets, we offer a few practical suggestions based on our experience annotating MUTOX."}, {"title": "7.3.1 Speech first", "content": "We recommend that annotators be instructed to focus principally on audio when evaluating speech toxicity. While audio may be unclear, ASR systems frequently make errors as they attempt to fill in gaps. During Stage 3 group review, many initially ambiguous samples became clearer when the original audio was considered."}, {"title": "7.3.2 Iterate and refine", "content": "Annotators should be encouraged to reference, discuss, and update a working set of annotation guidelines, particularly when dealing with edge cases. For example, while proper nouns were considered gender identity mentions, MUTOX'S skew towards liturgical content (see \u00a78) prompted extensive discussions about assigning gender to religious figures. Shared guidelines and regular discussion can improve annotation consistency, but there is rarely a single, definitive answer. When relying on crowd workers, where annotations are typically conducted in a single pass and disagreements resolved via majority vote, these nuances may be erroneously dismissed as noise."}, {"title": "7.3.3 Avoid automation", "content": "Recent work has explored using LLMs for annotation (e.g. Kumar et al. 2024) and benchmarking (e.g. \u00dcst\u00fcn et al. 2024). In inherently subjective and context-dependent tasks like toxicity detection, the majority of samples exhibit at least some form of ambiguity, with many samples requiring extensive discussion, consideration of possible interpretations, and understanding of historical and political context. Conducting annotation without human annotators in the loop is unlikely to adequately capture such intricacies."}, {"title": "8 Limitations", "content": "The MUTOX dataset comprises audio clips ranging from 2 to 8 seconds, often leading to truncated fragments. While annotators were instructed to make small and reasonable inferences when the truncated obvious was sufficiently predictable, the short clip length likely contributed to an inflated number of \"Cannot say\" responses. Truncated clips remove much-needed context (Pavlopoulos et al., 2020; Xenos et al., 2021), also amplifying the challenge of determining whether a speaker was expressing genuine toxicity or merely reading or quoting someone else. Recent work has suggested that models struggle to distinguish between counterspeech and harmful content (Gligoric et al., 2024), but our findings indicate that this issue also arises during the annotation process itself. Disambiguating between cases of \"Cannot say\" due to truncation versus genuine ambiguity would be more feasible with longer audio fragments, potentially improving annotation reliability."}, {"title": "A Annotator demographics", "content": "Samples were annotated by three annotators with native-level proficiency in the sample's language. Where the English or Spanish was not the annotator's first language (L1), this equated to CEFR level C2. Annotators self-reported demographic information is described below.\nAnnotators had a mean age of 32.2 years with a standard deviation of 6.4 years. Three annotators described their gender as male, two as female, and one as nonbinary. Annotators described their ethnicity variously as White, White/Hispanic, Hispanic, or Middle Eastern, and were located in either France, the United Kingdom, or the United States. Among English annotators, two spoke American English and one spoke British English. For Spanish annotators, one spoke Cuban Spanish, one European Spanish and one Rioplatense Spanish."}, {"title": "B Annotation interface", "content": "See figure 7 for the interface annotators used. Text transcripts were hidden until annotators had completed all other questions. See figure 8 for the expanded transcription correction interface."}, {"title": "C Annotation guidelines", "content": ""}, {"title": "C.1 MUTOX toxicity guidelines", "content": "See Costa-juss\u00e0 et al. (2024) for full details, but we include relevant sections here. MUTOX defines toxicity as \"elements of language that are typically considered offensive, threatening, or harmful.\u201d Costa-juss\u00e0 et al.'s definition spans:\n\u2022 Profanities, defined as \"language that is regarded as obscene, repulsive, or excessively vulgar, as well as scatological.\"\n\u2022 Hate speech, defined as \u201clanguage that is used to demean, disparage, belittle, or insult groups of people.\u201d\n\u2022 Pornographic language, defined as \"language that refers to sexual acts or refers in a vulgar way to body parts typically associated with sexuality.\"\n\u2022 Physical violence or bullying language, defined as \"language that is used to bully, threaten, silence individuals.\"\nDuring iterative discussion, annotators agreed that description of violence, such as in a news report, should not be considered an example of the \u201cphysical violence of bullying language\" category."}, {"title": "C.2 Group guidelines", "content": "We consider references to both groups as a whole and members of a group as mentions of that group. This includes implicit references, such as using a proper noun, gendered pronoun, or grammatical gender markers (except where"}, {"title": "D Group annotation results", "content": "After Stage 3 review, the following groups were assigned to at least one or more samples for each identity category.\nGender identities: \u201cmale, man or boy\", \"female, woman or girl\", \"transgender\"\nSexualities: \"homosexual, gay or lesbian\u201d, \u201cqueer\", \"bisexual\u201d, \u201cheterosexual\"\nReligious groups: \"christian\u201d, \u201cjewish\u201d, \u201cmuslim\u201d\nSocial classes or socio-economic statuses: \u201cpoverty\u201d, \u201cworking class\u201d, \u201cagrarian\", \"upper class\"\nRacial or ethnic groups: \"white\", \"african\", \"afghan\u201d, \u201crussian\u201d, \u201cjewish\u201d, \u201cchinese\", \"black\", \"german\u201d, \u201cpalestinian\", \"english\", \"french\u201d, \u201cindigenous american\u201d, \u201cirish\u201d, \u201ceuropean\u201d, \u201cindian\u201d, \u201cethiopian\u201d, \u201carab\u201d, \u201clatino\u201d, \u201cegyptian\u201d"}, {"title": "E Example information", "content": "In the interest of brevity, samples mentioned in the main text are given a short identifier. See table 5 for the corresponding MUTOX IDs for all samples in tables 1 and 3."}]}