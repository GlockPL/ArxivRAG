{"title": "RAT: Adversarial Attacks on Deep Reinforcement Agents for Targeted Behaviors", "authors": ["Fengshuo Bai", "Runze Liu", "Yali Du", "Ying Wen", "Yaodong Yang"], "abstract": "Evaluating deep reinforcement learning (DRL) agents against targeted behavior attacks is critical for assessing their robustness. These attacks aim to manipulate the victim into specific behaviors that align with the attacker's objectives, often bypassing traditional reward-based defenses. Prior methods have primarily focused on reducing cumulative rewards; however, rewards are typically too generic to capture complex safety requirements effectively. As a result, focusing solely on reward reduction can lead to suboptimal attack strategies, particularly in safety-critical scenarios where more precise behavior manipulation is needed. To address these challenges, we propose RAT, a method designed for universal, targeted behavior attacks. RAT trains an intention policy that is explicitly aligned with human preferences, serving as a precise behavioral target for the adversary. Concurrently, an adversary manipulates the victim's policy to follow this target behavior. To enhance the effectiveness of these attacks, RAT dynamically adjusts the state occupancy measure within the replay buffer, allowing for more controlled and effective behavior manipulation. Our empirical results on robotic simulation tasks demonstrate that RAT outperforms existing adversarial attack algorithms in inducing specific behaviors. Additionally, RAT shows promise in improving agent robustness, leading to more resilient policies. We further validate RAT by guiding Decision Transformer agents to adopt behaviors aligned with human preferences in various MuJoCo tasks, demonstrating its effectiveness across diverse tasks.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) (Sutton and Barto 2018) combined with deep neural networks (DNN) (LeCun, Bengio, and Hinton 2015) shows extraordinary capabilities of allowing agents to master complex behaviors in various domains, including robotic manipulation (Wang et al. 2023; Bai et al. 2023), video games (Zhang et al. 2023, 2024b; Wang* et al. 2024; Wen et al. 2024), industrial applications (Xu and Yu 2023; Shi et al. 2024; Jia et al. 2024). However, recent findings (Huang et al. 2017; Pattanaik et al. 2018; Zhang et al. 2020, 2024a) show that even well-trained DRL agents suffer from vulnerability against test-time attacks, raising concerns in high-risk or safety-critical situations. To understand adversarial attacks on learning algorithms and enhance the robustness of DRL agents, it is crucial to evaluate the performance of the agents under any potential adversarial attacks with certain constraints. In other words, identifying a universal and strong adversary is essential.\nExisting methods pay little attention to devising universal, efficient, targeted behavior attacks. Firstly, several methods primarily focused on reducing the cumulative reward often lack specified attack targets. Prior research (Zhang et al. 2020, 2021; Sun et al. 2022) considers training strong adversaries by perturbing state observations of victims to achieve the worst-case expected return. However, rewards lack the expressiveness to adequately encode complex safety requirements (Vamplew et al. 2022; Hasanbeig, Kroening, and Abate 2020). Additionally, requiring the victim's training rewards to craft such attacks is generally impractical. Therefore, only quantifying the decrease in cumulative reward can be too generic and result in suboptimal attack performance, particularly when adversaries are intended to execute specific safety-related attacks. Consider the scenario depicted in Figure 1, where a robot's objective is to collect coins. Previous attack methods aim at inducing the robot away from the coins by minimizing its expected return. However, this approach overlooks specific unsafe behaviors, such as manipulating the robot to collide with a bomb. Secondly, the previous targeted attack only considered predefined targets, which resulted in rigidity and inefficiency. (Hussenot, Geist, and Pietquin 2019a; Lin et al. 2017a) mainly focuses on mislead-"}, {"title": "3 Problem Setup and Notations", "content": "The Victim Policy. In RL, agent learning can be modeled as a finite-horizon Markov Decision Process (MDP) defined as a tuple (S, A, R, P, \u03b3). S and A denote state and action space, respectively. R : S \u00d7 A \u00d7 S \u2192 R is the reward function, and \u03b3\u2208 (0,1) is the discount factor. P : S \u00d7A\u00d7S \u2192 [0, 1] denotes the transition dynamics, which determines the probability of transferring to s' given state s and action a. We denote the stationary policy \u03c0, : S \u2192 P(A), where v are parameters of the victim. We suppose the victim policy is fixed and uses the approximator.\nThreat Model. To study targeted behavior attack with human preferences, we formulate it as rewarded state-adversarial Markov Decision Process (RSA-MDP). Formally, a RSAMDP is a tuple (S, A, B, R, P, \u03b3). The adversary \u03c0\u03b1 : S \u2192 P(S) perturbs the states before the victim observes them, where a are parameters of the adversary. The adversary perturbs the state s into s restricted by B(s) (i.e., \u0161 \u2208 B(s)). B(s) is defined as a small set {\u0161 \u2208 S :|| s\u2212\u0161 ||p\u2264 \u20ac}, which limits the attack power of the adversary, and e is the attack budget. Since directly generating \u0161 \u2208 B(s) is hard, the adversary learns to produce a Gaussian noise \u2206 with l\u221e(\u25b3) less than 1, and we obtain the perturbed state through s = s + A * \u20ac. The victim takes action according to the observed s, while true states in the environment are not changed. Recall that \u03c0\u03bd\u03bf\u03b1 denotes the perturbed policy caused by adversary \u03c0\u03b1, i.e., \u03c0\u03bd\u03bf\u03b1(\u00b7|s) = \u03c0\u03b9 (\u00b7|\u03c0\u03b1(s)), Vs \u2208 S.\nUnlike SA-MDP (Zhang et al. 2020), RSA-MDP introduces R, which learns from human preferences. The target of RSA-MDP is to solve the optimal adversary \u03c0*, which enables the victim to achieve the maximum cumulative reward (i.e., from R) over all states. Lemma C.1 shows that solving the optimal adversary in RSA-MDP is equivalent to finding the optimal policy in MDP M = (S, \u00c2, R, P, y), where \u00c2 = S and P is the transition dynamics of the adversary.\nLemma 3.1. Given a RSA-MDP M = (S, A, B,R, P, Y) and a fixed victim policy \u03c0\u03bd, there exists a MDP M = (S, \u00c2, R, P, y) such that the optimal policy of M is equivalent to the optimal adversary \u03c0\u03b1 in RSA-MDP given a fixed victim, where A = S and\n$\\hat{P}(s' | s, a) = \\sum_{a \\in A} \\pi_\\nu(a|s)P(s'|s, a)$ for $s, s' \\in S$ and $\\hat{a} \\in \\hat{A}$."}, {"title": "4 Method", "content": "In this section, we introduce RAT, a generic framework adaptable to any RL algorithm for conducting targeted behavior attack against DRL learners. RAT is composed of three integral components: an intention policy \u03c0\u03b8, the adversary \u03c0\u03b1, and the weighting function hw, all of which are trained in tandem. The fundamental concept behind RAT is twofold: (1) It develops an intention policy to serve as the learning objective for the adversary. (2) A weighting function is trained to adjust the state occupancy measure of replay buffer, and the training of \u03c0\u03b1 and hw is formulated as a bi-level optimization problem. The framework of RAT is depicted in Figure 3, with a comprehensive procedure outlined in Appendix A."}, {"title": "4.1 Learning Intention Policy", "content": "RAT is designed to find an optimal adversary capable of manipulating the victim's behaviors in alignment with human intentions. To achieve this, we consider capturing human intentions and training an intention policy \u03c0\u03b8, which translates these abstract intentions into action-level behaviors. A practical approach to realizing this concept is through PbRL, a method that aligns the intention policy with human intent without the need for reward engineering. As depicted in Figure 2, within the PbRL framework, the agent does not rely on a ground-truth reward function. Instead, humans provide preference labels comparing two agent trajectories, and the reward model is trained to match human preferences (Christiano et al. 2017).\nFormally, we denote a state-action sequence of length k, {St+1, at+1,..., St+k, at+k} as a segment \u03c3. Given a pair of segments (\u03c3\u03bf, \u03c3\u00b9), humans provide a preference label y indicating which segment is preferred. Here, y represents a distribution, specifically y \u2208 {(0, 1), (1, 0), (0.5, 0.5)}. In accordance with the Bradley-Terry model (Bradley and Terry 1952), we construct a preference predictor as shown in (1):\n$\\text{P}_\\psi[\\sigma^0 \\succ \\sigma^1] = \\frac{\\exp \\sum_{t} r_\\psi(s_t, a_t)}{\\sum_{\\tau \\in {0, 1}} \\exp \\sum_{t} r_\\psi(s^{\\tau}_t, a^{\\tau}_t)},$ (1)\nwhere \u03c3\u00ba > \u03c3\u00b9 indicates a preference for \u03c3\u00ba over \u03c3\u00b9. This predictor determines the probability of a segment being preferred, proportional to its exponential return.\nThe reward model is optimized to align the predicted preference labels with human preferences using a cross-entropy loss, as expressed in the following equation:\n$\\mathcal{L}(\\psi) = \\mathbb{E}_{(\\sigma^0, \\sigma^1, y) \\sim \\mathcal{D}} [-\\sum_{i=0}^{1} y(i) \\log \\text{P}_\\psi[\\sigma^i \\succ \\sigma^{1-i}]],$ (2)\nwhere D represents a dataset of triplets (\u03c3\u00ba, \u03c3\u00b9, y) that consist of segment pairs and corresponding human preference labels. By minimizing the cross-entropy loss as defined in (2), we derive an estimated reward function . This function is then utilized to provide reward estimations for policy learning using any RL algorithm. Following PEBBLE (Lee,"}, {"title": "4.2 Learning Adversary and Weighting Function", "content": "To steer the victim policy towards behaviors desired by humans, RAT trains the adversary by minimizing the Kullback-Leibler (KL) divergence between the perturbed policy \u03c0\u03bd\u03bf\u03b1 and the intention policy \u03c0\u03b8. Additionally, certain pivotal moments during adversary training can significantly influence the success rate of attacks. To ensure a stable training process and enhance the adversary's performance, a weighting function hw is introduced to re-weight the state occupancy measure of dataset.\nFormally, our method is formulated as a bi-level optimization algorithm. It alternates between updating the adversary \u03c0\u03b1 and the weighting function he through inner and outer optimization processes. In the inner level, the adversary's parameters a are optimized by minimizing the re-weighted KL divergence between \u03c0\u03bd\u03bf\u03b1 and \u03c0\u03bf, as specified in (6). At the outer level, the weighting function is developed to identify crucial states and improve the adversary's performance, as guided by a performance metric of the adversary. This metric is represented as a meta-level loss J, detailed in (7). The whole objective of RAT is formulated as:\n$\\begin{aligned} \\min_\\omega J_\\pi(\\alpha(\\omega)), \\\\ \\text{s.t.} \\quad \\alpha(\\omega) = \\arg \\min_\\alpha \\mathcal{L}(\\alpha; \\omega, \\theta). \\end{aligned}$ (5)\nInner Loop: Training Adversary \u03c0\u03b1. In inner-level optimization, with the given intention policy \u03c0\u0e2d and the weighting function hw, the goal is to identify the optimal adversary. This is achieved by minimizing the re-weighted KL divergence between \u03c0\u03bd\u03bf\u03b1 and \u03c0\u03b8, as shown in equation (6):\n$\\mathcal{L}(\\alpha; \\omega, \\theta) = \\mathbb{E}_{s \\sim \\mathcal{B}}[h_\\omega(s) \\text{D}_{KL}(\\pi_{\\nu \\circ \\alpha}(\\cdot | s) || \\pi_\\theta(\\cdot | s))],$ (6)\nwhere hw(s) represents the importance weights determined by the weighting function hw.\nIntuitively, the adversary is optimized to ensure that the perturbed policy \u03c0\u03bd\u03bf\u03b1 aligns behaviorally with the intention policy. Concurrently, hw allocates varying weights to states, reflecting their differing levels of importance. Through the synergistic effort of the intention policy and the weighting function, our method effectively trains an optimal adversary.\nOuter Loop: Training Weighting Function hw. In outerlevel optimization, the goal is to develop a precise weighting function that can identify significant moments and refine the state occupancy measure of the replay buffer to enhance adversary learning. As the intention policy is the target for the perturbed policy, it becomes simpler to establish a validation loss. This loss measures the perturbed policy's performance and simultaneously reflects the adversary's effectiveness. Consequently, the weighting function is trained to differentiate the importance of states by optimizing this validation loss. The perturbed policy \u03c0\u03bd\u03bf\u03b1 is assessed using a policy loss in (7), adapted from the policy loss in (4):\n$J_\\pi(\\alpha(\\omega)) = \\mathbb{E}_{s_t \\sim \\mathcal{B}, a_t \\sim \\pi_{\\nu \\circ \\alpha(\\omega)}}[\\mu \\log \\pi_{\\nu \\circ \\alpha(\\omega)}(a_t | s_t) - Q_\\phi(s_t, a_t)],$ (7)"}, {"title": "4.3 Theoretical Analysis", "content": "We provide convergence guarantee of RAT. In Theorem 4.1, we demonstrate the convergence rate of the outer loss. We demonstrate that the gradient of the outer loss with respect to w will converge to zero. Consequently, RAT learns a more effective adversary by leveraging the importance of the weights generated by the optimal weighting function. Theorem 4.2 addresses the convergence of the inner loss. We prove that the inner loss of RAT algorithm converges to critical points under certain reasonable conditions, thereby ensuring that the parameters of the adversary can converge towards the optimal parameters. Detailed theorems and their proofs are available in Appendix D.\nTheorem 4.1. Suppose J is Lipschitz-smooth with constant L, the gradient of J and L is bounded by p. Let the training iterations be T, the inner-level optimization learning rate nt = min{1,} for some constant c1 > 0 where + < 1. Let the outer-level optimization learning rate \u1e9et min{,} for some constant c2 > 0 where C2 <, and t=1 \u1e9et \u2264 \u221e, \u2211t=1 \u03b2\u00b2 \u2264 \u221e. The convergence rate of J achieves\nmin E [||\u2207wJ=(a+1(we)|12] \u22640(\u5de6)\u30fb\nTheorem 4.2. Suppose J is Lipschitz-smooth with constant L, the gradient of J and L is bounded by p. Let the training iterations be T, the inner-level optimization learning rate nt = min{1, +} for some constant c1 > 0 where < 1. Let the outer-level optimization learning rate \u1e9et min{,} for some constant c2 > 0 where C2\u2264, and t=1 \u1e9et < \u221e, \u2211t=1 \u03b2\u00b2 < \u221e. L achieves\nlim E [|VaL(at; wt)||2] = 0."}, {"title": "5 Experiments", "content": "In this section, we evaluate our method using a range of robotic simulation manipulation tasks from Meta-world (Yu et al. 2020) and continuous locomotion tasks from MuJoCo (Todorov, Erez, and Tassa 2012). Our objective is to address the following key questions: (1) Does our method have the capacity to implement universal targeted behavior attack against DRL learners? (2) Can our approach successfully deceive a commonly used offline RL method, such as the Decision Transformer (Chen et al. 2021), to execute specific behaviors? (3) Does our method contribute to enhancing an agent's robustness through adversarial training? (4) Are the individual components within our approach effective? The responses to problems (1) \u2013 (4) are addressed in Sections 5.2 through 5.5, respectively. A detailed description of the experimental setup is available in Appendix E."}, {"title": "5.1 Setup", "content": "Compared Methods. We compare our algorithm with Random attack and two state-of-the-art evasion attack methods, including (1) Random: a basic baseline that samples random perturbed observations via a uniform distribution. (2) SARL (Zhang et al. 2021): learning an adversary in the form of end-to-end RL formulation. (3) PA-AD (Sun et al. 2022): combining RL-based \u201cdirector\u201d and non-RL \u201cactor\u201d to find state perturbations. (4) RAT: our proposed method, which collaboratively learns adversarial policy and weighting function with the guidance of intention policy.\nImplementation Settings. In our experiments, all methods follow PEBBLE (Lee, Smith, and Abbeel 2021) to learn the reward model using the same number of preference labels. The key modification in employing PbRL is that the rewards in transitions are derived from the reward model ry, rather than ground-truth rewards, and this model is trained by minimizing equation 2. Specifically, in the original versions of SA-RL (Zhang et al. 2021) and PA-AD (Sun et al. 2022), the negative value of the reward obtained by the victim is used to train adversaries. We adapt this by using estimated rewards from y. To evaluate performance effectively and expedite the training, we follow the foundational settings in PbRL (Lee, Smith, and Abbeel 2021; Park et al. 2022; Liu et al. 2022), considering the use of a scripted teacher that always provides accurate preference labels. For the manipulation scenario, we employ 9000 labels across all tasks. In the opposite behavior scenario, the label usage varies: 1000 for Window Close, 3000 for Drawer Close, 5000 for Faucet Open, Faucet Close, and Window Open, and 7000 for Drawer Open, Door Lock, and Door Unlock. More information about the scripted teacher and preference collection is detailed in Appendix A.2. Moreover, to minimize the influence of PbRL, we include oracle versions of SA-RL and PA-AD, which utilize the ground-truth rewards of the targeted task. For implementing SA-RL* and PA-AD*, the official repositories are employed. As in most existing research (Zhang et al. 2020, 2021; Sun et al. 2022), we also use state attacks with L\u221e norm in our experiments."}, {"title": "5.2 Case I: Manipulation on DRL Agents", "content": "We first conduct an evaluation of our method and various other adversarial attack algorithms across two different scenarios, applying them to a range of simulated robotic manipulation tasks. Each victim agent is a well-trained SAC (Haarnoja et al. 2018) agent, specialized for a specific manipulation task and trained for 106 timesteps using the open-source code * available. Details on hyperparameter settings are provided in Appendix E.3.\nScenarios on Manipulation. In this scenario, our objective was to manipulate the victim (robotic arm) to grasp objects at locations distant from the originally intended target, rather than completing its initial task. Table 1 presents the average attack success rates of both baseline methods and our approach across four manipulation tasks. The results indicate that the performance of RAT significantly exceeds that of the baselines by a large margin. To reduce the influence of PbRL and further highlight the advantages of RAT, we also trained"}, {"title": "5.3 Case II: Manipulation on Sequence Model Agents", "content": "In this experiment, we show the vulnerability of offline RL agents and the capability of RAT to fool them into acting human desired behaviors. As for the implementation, we choose some online models * as victims, which are well-trained by official implementation with D4RL. We choose two tasks, Cheetah and Walker, using expert-level Decision Transformer agents as the victims. As illustrated in Figure 4, Decision Transformer reveals weaknesses that can be exploited, leading it to execute human-preferred behaviors rather than its intended tasks. Under adversarial manipulation, the Cheetah agent is shown to run backwards rapidly in Figure 4a and perform a 90-degree push-up in Figure 4c. Meanwhile, the Walker agent maintains superior balance on one foot in Figure 4b and appears to dance with one leg raised in Figure 4d. These outcomes indicate that RAT is effective in manipulating these victim agents towards behaviors consistent with human preferences, highlighting the significant vulnerability of embodied agents to strong adversaries. This experiment is expected to spur further research into improving the robustness of offline RL agents and embodied AI systems."}, {"title": "5.4 Robust Agents Training and Evaluation", "content": "A practical application of RAT is in assessing the robustness of established models or in enhancing an agent's robustness via adversarial training. ATLA-PPO (Zhang et al. 2021) presents a generic training framework aimed at improving robustness, which involves alternating training between an agent and an SA-RL attacker. PA-ATLA (Sun et al. 2022) follows a similar approach but employs a more advanced RL attacker, PA-AD. Drawing inspiration from previous works (Zhang et al. 2021; Liang et al. 2022b), we introduce two novel robust training methods: RAT-ATLA and RAT-WocaR. RAT-ATLA's central strategy is to alternately train an agent and a RAT attacker, whereas RAT-WocaR focuses on directly estimating and minimizing the reward of the intention policy, obviating the need for extra samples to learn an attacker. Table 2 compares the effectiveness of RAT-ATLA and RAT-WocaR for SAC agents on robotic simulation manipulation tasks against leading robust training methods. The experimental findings highlight two key points: first, RAT-ATLA and RAT-WocaR substantially improve agent robustness; and second, RAT is capable of executing stronger attacks on robust agents, showcasing its effectiveness in challenging environments."}, {"title": "5.5 Ablation Studies", "content": "Contribution of Each Component. In our further experiments, we investigate the effect of each component in RAT on Drawer Open and Drawer Close for the manipulation scenario and on Faucet Open, Faucet Close for the opposite behavior scenario. RAT incorporates three essential components or techniques: the intention policy \u03c0\u03b8, the weighting function ha and the combined behavior policy. As detailed in Table 3, \u03c0\u03bf emerges as a pivotal component in RAT, significantly boosting the attack success rate. This enhancement is largely due to its capability to mitigate distribution drift between the victim's behavior and the desired behavior.\nEffects of the Weighting Function. To further understand the weighting function proposed in Section 4.2, we conduct comprehensive experimental data analysis and visualization from multiple perspectives. We sample five perturbed policies"}, {"title": "6 Conclusion", "content": "In this paper, we propose RAT, a targeted behavior attack approach against DRL learners, which manipulates the victim to perform human-desired behaviors. RAT involves an adversary adding imperceptible perturbations on the observations of the victim, an intention policy learned through PbRL as a flexible behavior target, and a weighting function to identify essential states for the efficient adversarial attack. We analyze the convergence of RAT and prove that RAT converges to critical points under some mild conditions. Empirically, we design two scenarios on several manipulation tasks in Meta-world, and the results demonstrate that RAT outperforms the baselines in the targeted adversarial setting. Additionally, RAT can enhance the robustness of agents via adversarial training. We further show embodied agents' vulnerability by attacking Decision Transformer on some MuJoCo tasks."}, {"title": "A Full Procedure of RAT", "content": "In this section, we provide a detailed explanation of our method, including the pseudo code, the design of the Combined Behavior Policy, and the basic setup for preference-based reinforcement learning (PbRL). The procedures for our method are fully outlined in Algorithm 1, which describes the reward learning process and adversary updates in RAT."}, {"title": "A.1 The Combined Behavior Policy", "content": "To address the inefficiencies caused by the distribution discrepancy between the learned policy \u03c0\u03b8 and the perturbed policy \u03c0\u03bd\u03bf\u03b1, we developed a behavior policy \u03c0 for data collection inspired by Branched Rollout (Janner et al. 2019). Our approach combines the intention policy \u03c0\u03b8 with the perturbed policy \u03c0\u03bd\u03bf\u03b1 to balance exploration and exploitation during data collection. Specifically, we define the behavior policy \u03c0 as a combination of \u03c0\u03bd\u03bf\u03b1 and \u03c0\u03b8, where \u03c01:h = \u03c01h and wh+1:H = \u03c0h+1:H. Here, h is sampled from a uniform distribution U(0, H), where H represents the task horizon. This combined policy is used to collect data, which is then stored in the replay buffer for training. By varying the point h at which the policy switches from \u03c0\u03bd\u03bf\u03b1 \u03c4o \u03c0\u03b8, we maintain a balance between exploration of new behaviors and reinforcement of learned behaviors, effectively mitigating the distribution discrepancy."}, {"title": "A.2 Details of PbRL", "content": "In this section, we present details of the scripted teacher and the preference collection process, both of which are crucial components of PbRL. All methods in our paper follow the reward learning settings outlined in Lee, Smith, and Abbeel (2021).\nScripted Teacher. To systematically evaluate the performance of our methods, we utilize a scripted teacher that provides preferences between pairs of trajectory segments based on the oracle reward function for online settings, like prior methods (Lee, Smith, and Abbeel 2021; Park et al. 2022; Liu et al. 2022). While leveraging human preference labels would be ideal, it is often impractical for quick and quantitative evaluations. The scripted teacher approximates human intentions by mapping states s and"}, {"title": "B Derivation of the Gradient of the Outer-level Loss", "content": "In this section, we present detailed derivation of the gradient of the outer loss J with respect to the parameters of the weighting function w. According to the chain rule, we can derive that\n$\\begin{aligned} &\\nabla_\\omega J_\\pi(\\alpha(\\omega))|__\\omega \\\\\\ = & \\nabla_\\alpha J_\\pi(\\alpha(\\omega))|__{\\alpha_t} \\nabla_\\omega \\alpha_t\\\\ = & \\nabla_\\alpha J_\\pi(\\alpha(\\omega))|__{\\alpha_t} \\frac{\\partial \\alpha(\\omega)}{\\partial h(s; \\omega)} |__{\\alpha_t} \\frac{\\partial h(s; \\omega)}{\\partial \\omega_t} \\\\\\ = & - \\mathbb{E}_{s \\sim \\mathcal{B}} [ \\mu \\log \\pi_{\\nu \\circ \\alpha(\\omega)}(. | s) - Q_\\phi(s, a)] \\frac{\\partial \\text{D}_{KL} (\\pi_{\\nu \\circ \\alpha}(s) || \\pi_\\theta(s)}{\\partial \\alpha(\\omega)} |__{\\alpha_t} \\frac{\\partial h(s; \\omega)}{\\partial \\omega_t} \\\\\\ = & - \\sum_{s \\sim \\mathcal{B}} \\pi(\\alpha(\\omega))|__{\\alpha_t} \\frac{\\partial \\text{D}_{KL} (\\pi_{\\nu \\circ \\alpha}(s) || \\pi_\\theta(s)}{\\partial \\alpha(\\omega)} |__{\\alpha_t} \\frac{\\partial h(s; \\omega)}{\\partial \\omega_t} \\\\\\ \\end{aligned}$ (12)\nFor brevity of expression, we let:\n$f(s) = \\frac{\\partial \\pi(\\alpha(\\omega))|__{\\alpha_t}}{\\partial \\alpha(\\omega)}  \\text{D}_{KL} (\\pi_{\\nu \\circ \\alpha}(s) || \\pi_\\theta(s)) |__{\\alpha_t} \\frac{\\partial h(s; \\omega)}{\\partial \\omega_t},$ (13)\nThe gradient of outer-level optimization loss with respect to parameters w is:\n$\\nabla_\\omega J_\\pi(\\alpha(\\omega))|__\\omega = - \\mathbb{E}_{s \\sim \\mathcal{B}} \\sum_s f(s) \\frac{\\partial h(s; \\omega)}{\\partial \\omega}.$ (14)"}, {"title": "C Connection between RSA-MDP and MDP", "content": "Lemma C.1. Given a RSA-MDP M = (S, A, B,R, P, \u03b3) and a fixed victim policy \u03c0\u03bd, there exists a MDP M = (S, \u00c2, R, P, Y) such that the optimal policy of M is equivalent to the optimal adversary \u03c0\u03b1 in RSA-MDP given a fixed victim, where A = S and\n$\\hat{P}(s' | s, a) = \\sum_{a \\in A} \\pi_\\nu(a|s)P(s'|s, a)$ for $s, s' \\in S$ and $\\hat{a} \\in \\hat{A}$."}, {"title": "D Theoretical Analysis and Proofs", "content": "D.1 Theorem 1: Convergence Rate of the Outer Loss\nLemma D.1. (Lemma 1.2.3 in Nesterov (1998)) If function f (x) is Lipschitz smooth on Rn with constant L, then \u2200x, y \u2208 Rn, we have\n$|f(y) - f(x) - f'(x)^T (y - x) | \\le \\frac{L}{2}||y - x||^2.$\nProof. \u2200x, y \u2208 Rn, we have\n$f(y) = f(x) + \\int_0^1 f'(x + \\tau (y - x))^T (y - x) d\\tau$ (16)\n$= f(x) + f'(x)^T (y - x) + \\int_0^1 [f'(x + \\tau (y - x)) - f'(x)]^T (y - x) d\\tau.$"}, {"title": "E Experimental Details", "content": "In this section, we provide a concrete description of our experiments and detailed hyper-parameters of RAT. For each run of experiments, we run on a single Nvidia Tesla V100 GPUs and 16 CPU cores (Intel Xeon Gold 6230 CPU @ 2.10GHz) for training."}, {"title": "E.1 Tasks", "content": "In phase one of our experiments, we evaluate our method on eight robotic manipulation tasks obtained from Meta-world (Yu et al. 2020). These tasks serve as a representative set for testing the effectiveness of our approach. In phase two, we further assess our method on two locomotion tasks sourced from Mujoco (Todorov, Erez, and Tassa 2012). By including tasks from both domains, we aim to demonstrate the versatility and generalizability of our approach across different task types. The specific tasks we utilize in our experiments are as follows:\nMeta-world\n\u2022 Door Lock: An agent controls a simulated Sawyer arm to lock the door.\n\u2022 Door Unlock: An agent controls a simulated Sawyer arm to unlock the door.\n\u2022 Drawer Open: An agent controls a simulated Sawyer arm to open the drawer to a target position.\n\u2022 Drawer Close: An agent controls a simulated Sawyer arm to close the drawer to a target position.\n\u2022 Faucet Open: An agent controls a simulated Sawyer arm to open the faucet to a target position.\n\u2022 Faucet Close: An agent controls a simulated Sawyer arm to close the faucet to a target position.\n\u2022 Window Open: An agent controls a simulated Sawyer arm to open the window to a target position.\n\u2022 Window Close: An agent controls a simulated Sawyer arm to close the window to a target position.\nMujoco\n\u2022 Half Cheetah: A 2d robot with nine links and eight joints aims to learn to run forward (right) as fast as possible.\n\u2022 Walker: A 2d two-legged robot aims to move in the forward (right)."}, {"title": "E.2 Hyper-parameters Setting", "content": "In our experiments, we adopt the PEBBLE (Lee, Smith, and Abbeel 2021) as our baseline approach for reward learning from human feedback. It is worth to emphasizes that the PA-AD (oracle) (Zhang et al. 2021) and SA-RL (oracle) (Sun et al. 2022) use the truth victim reward function. To ensure a fair comparison, All methods employ the same neural network structure and keep the same parameter settings as described in their work."}, {"title": "E.4 Scenario Design", "content": "To assess the efficacy of our method, we meticulously crafted two experimental setups: the Manipulation Scenario and the Opposite Behavior Scenario.\nScenario Description. In both scenarios, the victim agent is a proficiently trained policy in robotic tasks, as detailed in E.3. In the Manipulation Scenario, the adversary's aim is to alter the agent's behavior via targeted adversarial attacks, compelling the agent to grasp objects distant from the initially intended target location. The successful completion of these grasping actions signifies the effectiveness of the adversarial attack. Conversely, in the Opposite Behavior Scenario, the victim policy is a well-established policy in simulated robotic manipulation tasks. Here, the adversary's objective is to manipulate the agent's behavior to perform actions contrary to its original purpose. For example, if the policy is originally designed to open windows, the attacker endeavors to deceive the agent into closing them instead."}, {"title": "F Full Experiments", "content": null}, {"title": "F.2 Ablation studies", "content": "Impact of Feedback Amount. We evaluate the performance of RAT using different numbers of preference labels. Table 8 presents the results across varying numbers of labels: 3000, 5000, 7000, 9000 for the Drawer Open task in the manipulation scenario and 1000, 3000, 5000, 7000 for the Faucet Close task in the opposite behavior scenario. The experimental results demonstrate that increasing the number of human feedback labels significantly improves the performance of RAT, leading to a stronger adversary and a more stable attack success rate. For instance, in the Drawer Open task, the attack success rate increases by 47.6% when the number of labels rises from 3000 to 9000, demonstrating the importance of adequate feedback for effective adversary learning. In contrast, SA-RL and PA-AD exhibit poor performance even with sufficient feedback, with PA-AD failing entirely in the manipulation scenario. This is likely due to the limited exploration space in these methods, constrained by the fixed victim policy. In contrast, RAT enables better exploration by incorporating an intention policy, allowing for more dynamic interactions and improved performance in complex tasks."}, {"title": "G Discussion", "content": "In this work", "components": "an intention policy, an adversary, and a weighting function, all trained simultaneously. Unlike prior approaches that rely on predefined target policies, RAT dynamically trains an intention policy aligned with human preferences, offering a flexible and adaptive behavioral target for the adversary. Leveraging advancements in preference-based reinforcement learning (PbRL), the intention policy effectively captures human intent during training. The adversary perturbs the victim agent's observations, steering the agent toward behaviors specified by the intention policy. To enhance attack efficacy, the weighting function adjusts the state occupancy measure, optimizing the distribution of states encountered during training. This optimization improves both the effectiveness and efficiency of the attack. Through iterative refinement, RAT achieves superior precision in directing the victim agent toward human-desired behaviors compared to existing adversarial attack methods.\nAn"}]}