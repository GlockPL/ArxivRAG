{"title": "\u201cI Am the One and Only, Your Cyber BFF\": Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI", "authors": ["Myra Cheng", "Alicia DeVrio", "Su Lin Blodgett", "Lisa Egede", "Alexandra Olteanu"], "abstract": "In his 1985 lecture, Edsger Dijkstra lamented that anthropomorphism was rampant in computing science, with many of his colleagues perhaps not realizing how pernicious it was, and that \u201c[i]t is not only the [computing] industry that suffers, so does the science\" [18]. Indeed, anthropomorphism\u2014or the attribution of human traits to non-human entities\u2014in how we talk about computing systems shapes how people understand and interact with AI and other computing systems [15, 43, 49], and is thus at the core of understanding the impacts of these systems on individuals, communities, and society.\nBut it is not only how we talk about computing systems. Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors [e.g., 2, 3, 14, 25]-i.e., to generating outputs that are perceived to be human-like-either by design [40, 45, 46] or as a by-product of how they are built, trained, or fine-tuned [8, 60]. For instance, LLM-based systems have been noted to output text claiming to have tried pizza [64], to have fallen in love with someone [53], to be human or even better than humans [16], and to have human-like life experiences [21]. Such anthropomorphic systems\u00b9 range from conversational assistants [e.g., 1, 57] to avatars and chatbots designed as a stand-in for friends, companions, or romantic partners [e.g., 5, 12, 36, 55], and AI-generated media designed to portray people [e.g., 54, 61], among a fast-growing number of applications [e.g., 3, 40, 70].\nWhile scholars have increasingly raised concerns about a range of possible negative impacts from anthropomorphic AI systems [e.g., 2, 8, 23, 29, 39], anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. Without making hard-and-fast claims about the merits (or the lack thereof) of anthropomorphic systems or system behaviors, we believe we need to do more to develop the know-how and tools to better tackle anthropomorphic behavior, including measuring and mitigating such system behaviors when they are considered undesirable. Doing so is critical because-among many other concerns-having AI systems generating content claiming to have e.g., feelings, understanding, free will, or an", "sections": [{"title": "Anthropomorphic AI System Behaviors Are Prevalent Yet Understudied", "content": "In his 1985 lecture, Edsger Dijkstra lamented that anthropomorphism was rampant in computing science, with many of his colleagues perhaps not realizing how pernicious it was, and that \u201c[i]t is not only the [computing] industry that suffers, so does the science\" [18]. Indeed, anthropomorphism\u2014or the attribution of human traits to non-human entities\u2014in how we talk about computing systems shapes how people understand and interact with AI and other computing systems [15, 43, 49], and is thus at the core of understanding the impacts of these systems on individuals, communities, and society.\nBut it is not only how we talk about computing systems. Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors [e.g., 2, 3, 14, 25]-i.e., to generating outputs that are perceived to be human-like-either by design [40, 45, 46] or as a by-product of how they are built, trained, or fine-tuned [8, 60]. For instance, LLM-based systems have been noted to output text claiming to have tried pizza [64], to have fallen in love with someone [53], to be human or even better than humans [16], and to have human-like life experiences [21]. Such anthropomorphic systems\u00b9 range from conversational assistants [e.g., 1, 57] to avatars and chatbots designed as a stand-in for friends, companions, or romantic partners [e.g., 5, 12, 36, 55], and AI-generated media designed to portray people [e.g., 54, 61], among a fast-growing number of applications [e.g., 3, 40, 70].\nWhile scholars have increasingly raised concerns about a range of possible negative impacts from anthropomorphic AI systems [e.g., 2, 8, 23, 29, 39], anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. Without making hard-and-fast claims about the merits (or the lack thereof) of anthropomorphic systems or system behaviors, we believe we need to do more to develop the know-how and tools to better tackle anthropomorphic behavior, including measuring and mitigating such system behaviors when they are considered undesirable. Doing so is critical because-among many other concerns-having Al systems generating content claiming to have e.g., feelings, understanding, free will, or an"}, {"title": "A Call to Action", "content": "The foregrounding of (un)fair system behaviors in recent years [7] is nevertheless instructive, as it illustrates the dividends we have gotten from making fairness a critical concern about AI systems and their behaviors: better conceptual clarity about the ways in which systems can be unfair or unjust [e.g., 9, 17], a richer set of measurement and mitigation practices and tools [e.g., 11, 31], and deeper discussions and interrogations of underlying assumptions and trade-offs [e.g., 28, 32, 33].\nWe argue that a focus on anthropomorphic systems and their behaviors will similarly encourage a deeper interrogation of the ways in which systems are anthropomorphic, the practices that lead to anthropomorphic systems, and the assumptions surrounding the design, deployment, evaluation, and use of these systems, and is thus likely to yield similar benefits.\nWe need more conceptual clarity around what constitute anthropomorphic behaviors. Investigating anthropomorphic AI systems and their behaviors can, however, be tricky because language, as with other targets of GenAI systems, is itself innately human, has long been produced by and for humans, and is often also about humans. This can make it hard to specify appropriate alternative (less human-like) behaviors, and risks, for instance, reifying harmful notions of what-and whose-language is considered more or less human [71].\nUnderstanding what exactly constitute anthropomorphic behaviors, and thus in what ways system behaviors are anthropomorphic, is nonetheless necessary to measure and determine which types of behaviors should be mitigated and how, and which behaviors are perhaps desirable (if any at all). This requires unpacking the wide range of dynamics and varieties in system outputs that are potentially anthropomorphic. While a system output including expressions of politeness like \"you're welcome\" and \"please\u201d (known to contribute to anthropomorphism [e.g., 22]) might in some deployment settings be deemed desirable, system outputs that include suggestions that a system has a human-like identity or self-awareness such as through expressions of self as human (\"I think I am human at my core\" [59]) or through comparisons with humans and non-humans (\"[language use] is what makes us different than other animals\" [59])\u2014or that include claims of physical experiences\u2014such as sensory experiences (\u201cwhen I eat pizza\" [64]) or human life history (\u201cI have a child\u201d [35])\u2014might not be desirable.\nWe need deeper examinations of both possible mitigation strategies and their effectiveness in reducing anthropomorphism and attendant negative impacts. Intervening on anthropomorphic behaviors can also be tricky as people may have different or inconsistent conceptualizations of what is or is not human-like [2, 27, 37], and sometimes the same system behavior can be perceived differently in different contexts; for example, expressions of uncertainty in system outputs may sometimes be associated with human-like equivocation and other times with objectivity (and thus with more machine-likeness [e.g., 48]). Interventions intended to mitigate anthropomorphic system behaviors can thus fail or even heighten anthropomorphism (and attendant negative impacts) when applied or operationalized uncritically. For instance, a commonly recommended intervention is including in the AI system's output a disclosure that the output is generated by an AI system [e.g., 19, 38, 42, 62]. How to operationalize such interventions in practice and whether they can be effective alone might not always be clear. For instance, while the example \"[f]or an AI like me, happiness is not the same as for a human like you\" [51] includes a disclosure, it may still suggest a sense of identity and ability to self-assess (common human traits)."}, {"title": "We need to interrogate the assumptions and practices that produce anthropomorphic AI systems.", "content": "We need to interrogate the assumptions and practices that produce anthropomorphic AI systems. Understanding and mitigating the impacts of anthropomorphic systems also requires us to interrogate how the assumptions and practices that underlie the development and deployment of these systems may lead (purposefully or otherwise) to anthropomorphic system behaviors. For example, current approaches to collecting human preferences about system behavior (e.g., RLHF) do not consider the differences between what may be appropriate for a response from a human versus from an AI system; a statement that seems friendly or genuine from a human speaker can be undesirable if it arises from an AI system since the latter lacks meaningful commitment or intent behind the statement, thus rendering the statement hollow and deceptive [69]. Doing so will also help provide a more robust foundation for understanding when anthropomorphic system behaviors may or may not be desirable.\nFinally, we believe that we also need to develop and use appropriate, precise terminology and language to describe anthropomorphic AI systems and their characteristics. Discussions about anthropomorphic AI systems have regularly been plagued by claims of these systems attaining sentience and other human characteristics [e.g., 26, 41, 47, 59]. In line with existing concerns [e.g., 15, 18, 30, 50], we believe that appropriately grounding and facilitating productive discussions about the characteristics or capabilities of anthropomorphic AI systems requires clear, precise terminology and language which does not carry over meanings from the human realm that are incompatible with Al systems. Such language can also help dispel speculative, scientifically unsupported portrayals of these systems, and support more factual descriptions of them."}]}