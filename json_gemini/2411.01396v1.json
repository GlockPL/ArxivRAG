{"title": "Exploring the Edges of Latent State Clusters for\nGoal-Conditioned Reinforcement Learning", "authors": ["Yuanlin Duan", "Guofeng Cui", "He Zhu"], "abstract": "Exploring unknown environments efficiently is a fundamental challenge in unsu-\npervised goal-conditioned reinforcement learning. While selecting exploratory\ngoals at the frontier of previously explored states is an effective strategy, the policy\nduring training may still have limited capability of reaching rare goals on the\nfrontier, resulting in reduced exploratory behavior. We propose \"Cluster Edge\nExploration\" (CE\u00b2), a new goal-directed exploration algorithm that when choosing\ngoals in sparsely explored areas of the state space gives priority to goal states\nthat remain accessible to the agent. The key idea is clustering to group states that\nare easily reachable from one another by the current policy under training in a\nlatent space and traversing to states holding significant exploration potential on\nthe boundary of these clusters before doing exploratory behavior. In challenging\nrobotics environments including navigating a maze with a multi-legged ant robot,\nmanipulating objects with a robot arm on a cluttered tabletop, and rotating ob-\njects in the palm of an anthropomorphic robotic hand, CE2 demonstrates superior\nefficiency in exploration compared to baseline methods and ablations.", "sections": [{"title": "1 Introduction", "content": "In recent years, Goal-Conditioned Reinforcement Learning (GCRL) (Andrychowicz et al. (2017))\nhas emerged as a powerful paradigm for training agents to accomplish diverse tasks in complex\nand dynamic environments. GCRL enables agents to learn goal-directed behaviors, allowing them\nto achieve specific objectives in a flexible and adaptive manner. However, a central challenge in\nGCRL lies in guiding agents to effectively explore their environment during training. The exploration\nproblem in GCRL can be viewed as the task of setting goals for the agent during training to guide\nthe agent's environment navigation to collect exploratory data that improves its learning process. In\nthis paper, we address this critical challenge by proposing a novel strategy for selecting exploration-\ninducing goals in GCRL.\nBecause goal-conditioned policies excel at reaching states encountered frequently during training,\na simple strategy is setting goals in less-visited areas of the state space to broaden the range of\nreachable states. However, throughout training, goal-conditioned policies may encounter difficulties\nin reaching arbitrary goals. For example, when instructed to navigate to an unexplored section of a\nmaze environment, a novice agent might instead revisit a previously traversed area that provides low\nexploration value. To address this shortcoming, the environment exploration procedure must set up\nadditional mechanisms to filter out unreachable goals. A common strategy in the literature is to select\ngoals at the frontier of previously explored states and launch an exploration phase immediately after\nthese goals are achieved, adhering to a Go-Explore principle (Ecoffet et al. (2019)). For example,\nSkewfit (Pong et al. (2019)) estimates state densities and selects goals at the frontier from the replay\nbuffer in inverse proportion to their density. Similarly, MEGA (Pitis et al. (2020)) uses kernel density\nestimates (KDE) of state densities and selects frontier goals with low density from the replay buffer."}, {"title": "2 Problem Setup and Background", "content": "Our work focuses on the exploration problem in unsupervised goal-conditioned reinforcement\nlearning (GCRL) settings. In this section, we set up notation and preliminary concepts.\nGCRL. A goal-conditioned Markov decision process (MDP) is defined by the tuple (S, A, G, T, \u03b7)\nwhere the state space S defines the set of all possible agent's observations into the environment, the\naction space A defines all possible actions that the agent can take in each state, G is the set of all\npossible goals that the agent may aim to achieve in the environment, and the transition function T\ndescribes the probability of transitioning from one state to another given an action. It is defined as\n T(s'|s, a), where s' \u2208 S is the next state, s \u2208 S is the current state, and a \u2208 A is the action taken.\n\u03b7: S \u2192 G is a tractable mapping function that maps a state to a specific goal. A goal-conditioned\n\u03c0(a|s, g) represents the agent's strategy for selecting actions based on states and goal commands,\nindicating the probability of taking action a in state s given goal command g \u2208 G. In this paper, for\nease of presentation, we assume S = G and \u03b7 is an identify function.\nOur goal is to develop agents capable of unsu-\npervised exploration when dropped into an un-\nknown environment. During the unsupervised\nexploration stage, there are no predefined tasks\nor goals. The agent sets its own goal command\ng\u2208 Gas it explores the environment. Following\nthis exploration phase, a successful agent should\nbe able to navigate to a wide range of previously\nunknown goal states in the environment upon\ngoal commands.\nModel-based GCRL. Model-based reinforce-\nment learning (MBRL) is an approach where\nan agent learns a model of the environment's\ndynamics to predict future states, enabling more efficient policy learning."}, {"title": "3 State Cluster Edge Exploration", "content": "The major limitation in existing Go-Explore approaches, such as those described in (Pong et al.\n(2019); Pitis et al. (2020)) is that the policy under training can struggle to reach heuristically chosen\nrare goals at the frontier of known states (Hu et al. (2023)). This difficulty arises because the goal\ncommands are selected without a systematic method to filter out unachievable goals for the agent,\nleading to diminished exploratory behavior. In CE2, when choosing goals in sparsely explored areas\nof the state space in the \"Go-phase\", our method gives priority to goal states that remain accessible.\nFor this purpose, the key idea is clustering to group states that are easily reachable from one another by\nthe current policy under training in a latent space, and selecting states holding significant exploration\npotential on the boundary of these clusters as the \"interesting\" goals to explore. In Sec. 3.1, we discuss\nhow to learn a latent space that can represent the reachability relationships between environment\nstates. In Sec. 3.2, we explain how this latent space can be used to cluster states in the replay buffer\nthat are easily reachable from one another. In Sec. 3.3, we demonstrate how the agent can be brought\nto interesting states on the boundary of latent state clusters to effectively explore its environment."}, {"title": "3.1 Latent Space Learning", "content": "Typically, during the learning process of a world model M as a neural network, an essential step\ninvolves encoding states from the original observation space into a latent space using an encoder,\nwhich can then be decoded back to the original observation space by a decoder. This latent space is\nsubsequently used to learn the dynamic model of the real environment Hafner et al. (2019a, 2020).\nIn CE2, we additionally require the latent space can express the temporal distance between different\nstates. In other words, we aim for the distances between various states in the latent space to represent\nthe number of steps required to transition from one another in the real environment (after decoding)\nby the training policy. Therefore, the loss function of training the latent space in CE2 comprises\ntwo components. The first component is the reconstruction loss \\(L_{rec}\\), akin to the latent space loss\nfunction in Dreamer framework (Hafner et al. (2019a, 2020)). It captures the association between the\nlatent space and the re-decoding to the observation space, along with predicting dynamic transition in\nthe latent space. We introduce a second loss term \\(L_{dt}\\) that leverages the temporal distance network\n\\(D_t\\) in Equation 1 to guide the learning of the latent space structure. For any pair of states (s1, s2)\nsampled from the replay buffer, the \\(L_{dt}\\) loss function is formulated as follows (\u03a8 is a learned function\nfor state embeddings in the world model):\n\n\\(L_{dt} = \\frac{1}{2} (||\\Psi(s_1) \u2013 \\Psi(s_2)||^2 - (D_t(\\Psi(s_1), \\Psi(s_2)) + D_t(\\Psi(s_2), \\Psi(s_1))))^2\\)\n\n\\(L_{latent} = L_{rec} + L_{dt}\\)\n\nWe use the loss function \\(L_{latent}\\) to supervise the training of the latent space. The trained latent space\nprovides the agent with a deeper understanding of the real environment, where states that are easily\nreachable from one another in the real environment are closer in proximity within the latent space."}, {"title": "3.2 Latent State Clustering", "content": "To identify the frontier of known states, CE2 conducts state clustering to group states in the replay\nbuffer. States that are easily reachable from one another are classified in the same cluster in the latent\nspace by Gaussian Mixture Models (GMMs), based on the temporal distances between the encoded\nstates. GMMs are probabilistic models that assume all data points are generated by a mixture of a\nfinite number of Gaussian Distributions. We initialize the Gaussian models in the latent space with\n\\(N_c\\) trainable latent centroids \\(c = {c_1,...,c_{N_c}}\\) and a shared variance \u03c3, where \\(N_c\\) represents the\ndesired number of clusters. These \\(N_c\\) latent centroids are initialized by applying the Farthest Point\nSampling (FPS) algorithm(Eldar et al. (1997)) to select a representative subset of states from a batch\nof data sampled from the replay buffer. We provide a detailed description of the FPS algorithm in\nAppendix G.1. After initialization, we optimize the clustering model by maximizing the Evidence\nLower Bound (ELBO) iteratively on sampled batches from the replay buffer with a uniform prior\np(c) to scatter out the latent centroids (Zhang et al. (2021)):\n\n\\(\\log p(z = \\Psi(s)) \\geq E_{q(c|\\Psi(s))} [\\log p(\\Psi(s)|c)] \u2013 D_{KL}(q(c|\\Psi(s))||p(c))\\)\n\nwhere p and q are represented as Gaussian distributions within the GMMs. \\(q(c|\\Psi(s))\\) is the postior\ndistribution over c (the clusters) given an encoded state \\(\\Psi(s)\\). \\(\\log p(\\Psi(s)|c)\\) is the distribution\ndonating the probability of the encoded state \\(\\Psi(s)\\) in cluster c. p(c) is the prior distribution of the\nweight of clusters in GMMs. For each round of optimization, we increase the probability of the\nsampled batches in GMMs by updating the weight of each cluster c in GMMs and the mean and\nvariance of them."}, {"title": "3.3 Exploring the Boundaries of Latent State Clusters", "content": "Assuming we have already trained \\(N_c\\) state clusters in the latent space, each representing part of the\nstate space where the goal-conditioned policy under training is familiar with, how can we utilize these\nstate clusters to plan an exploration strategy? CE2 selects goal states at the edges of these latent state\nclusters for exploration because (1) less explored regions are naturally adjacent to these boundaries,\nand (2) given the easy accessibility between states within each cluster by the training policy, the\nagent's capability extends to reaching states even at the cluster boundaries.\nWe outline our exploration algorithm in Algorithm 1. At line 3, it samples \\(N_{candidate}\\) latent states as\n\\(S_{candidate}\\) from GMMs. A higher sampling quantity ensures sampling from more states at the edges"}, {"title": "4 Experiments", "content": "Our experiments evaluate CE2 over goal-reaching tasks that demand significant exploration to solve.\nWe aim to address the following questions: (1) Does CE2 lead to improved exploration and goal-\nreaching performance? (2) How does CE2 exploration qualitatively differ from those in previous\ngoal-directed exploration methods? (3) Which components of CE2 are crucial to its success?"}, {"title": "4.1 Benchmarks", "content": "We evaluate our method on six hard exploration goal-conditioned RL tasks: Point-Maze, Ant-Maze,\nWalker, 3-Block Stacking, Block Rotation and Pen Rotation. Point-Maze: A blue point is placed\nat the bottom left of the maze and be trained to explore the structure of maze. Ant-Maze: An ant\nrobot must master intricate four-legged locomotion behaviors and maneuver through narrow hallways.\nWalker: A 2-legged robot needs to learn how to control its leg joints to walk on a flat plane to move\nforward or backward. In 3-Block Stacking, a robot arm with a two-fingered gripper operates on a\ntabletop with three blocks. The goal is to stack the blocks into a tower configuration. The agent needs\nto learn pushing, picking, and stacking, as well as discovering intricate action paths to accomplish\nthe task within the environment. Previous solutions have relied on methods like demonstrations,\ncurriculum learning, or extensive simulator data, highlighting the task's difficulty. The Gymnasium\nBlock Rotation and Pen Rotation tasks involve manipulating a block and a pen, respectively, to\nachieve a random target rotation along all axes. Pen Rotation is particularly challenging due to the\npen's thinness, requiring precise control to prevent it from dropping. For evaluation, we use the most\nchallenging goals, such as the farthest goal locations, in Point Maze, Ant Maze, Walker, and 3-Block\nStacking. In the other two environments, we utilize random goals as defined by the environment. For\nmore settings and information about the environments, please refer to the Appendix E."}, {"title": "4.2 Baselines", "content": "In the unsupervised GCRL setting, we compared CE\u00b2 with state-of-the-art methods based on the\nGo-Explore strategy, which has demonstrated high efficiency in this setup: PEG (Hu et al. (2023))\nand MEGA (Pitis et al. (2020)). MEGA commands the agent to rarely seen states at the frontier\nby using kernel density estimates (KDE) of state densities and chooses low-density goals from the\nreplay buffer. PEG selects goal commands to guide an agent's goal-conditioned policy toward states\nwith the highest exploration potential given its current level of training. This potential is defined as\nthe expected accumulated exploration reward during the Explore-phase.\nIn scenarios where environment goal distributions are available to the agents, we compare CE2-G\nwith GC-Dreamer (illustrated in Sec. 2), PEG-G, MEGA-G and L3P. Similar to CE2-G, PEG-G\nand MEGA-G augment GC-Dreamer with the PEG and MEGA Go-Explore strategies, respectively.\nIn these methods, the replay buffer D contains not only trajectories sampled by the goal-conditioned\npolicy \u03c0 commanded by environment goals but also exploratory trajectories sampled using the\ncorresponding Go-Explore strategies. L3P trains a latent space using temporal distances and performs\nclustering in this latent space, similar to CE2-G. However, L3P does not employ a Go-Explore\nstrategy. Instead, it constructs a directed graph with cluster centroids as nodes and utilizes online\nplanning with graph search to determine subgoals for task execution."}, {"title": "4.3 Results", "content": "CE2 Results. Fig. 3 depicts the mean learning\nperformance of all the unsupervised GCRL tools\nin terms of the agent's goal-reaching success rate\naveraged over 5 random seeds. The evaluation\ngoal distribution is revealed to the agent only at\ntest time. In all tasks except PointMaze, CE2\nsignificantly outperforms PEG and MEGA in\nterms of both learning performance and learning\nspeed. On PointMaze, CE2 performs compara-\nbly with the baselines. Although MEGA can\nset goal commands in sparsely explored areas"}, {"title": "4.4 Exploration Process", "content": "Fig 6 shows the evolution of state clusters (learned in a latent space) during the training process\nfor Ant Maze (in different colors). The red points represent the selected goal commands used\nto induce exploration. We observe that the self-directed exploration goals set by CE2 improve\nprogressively as the agent's capabilities increase, consistently targeting the cluster edges that require\nfurther exploration and are within the agent's reach. We compare the exploration targets generated by\nCE2 with those produced by the MEGA and PEG approaches throughout the training process in the\nAnt Maze environment in Appendix H.2."}, {"title": "4.5 Ablation Study", "content": "In the ablation experiment, our goal is to determine the individual contributions of each component\nto our method's overall performance. The \"Go-phase\" of the Go-Explore procedure in CE2 consists"}, {"title": "5 Related Work", "content": "Our method addresses the challenging and inefficient exploration problem inherent in goal-\nconditioned reinforcement learning (RL) settings with sparse rewards, commonly used in robotics\nand control fields (Ghosh et al. (2019); Liu et al. (2022); Plappert et al. (2018)). In goal-conditioned\nRL, agents are trained to achieve various goals based on predefined commands, with rewards typically\nbeing binary, indicating positive feedback from the environment only upon reaching the specified\ngoal. This sparse reward setting significantly complicates achieving sample efficiency and effective\nlearning processes (Ren et al. (2019); Florensa et al. (2018); Trott et al. (2019)). To mitigate this\nchallenge, various methods have been proposed. Some reshape the sparse reward function into a\ndenser form by incorporating metrics such as distance between achieved and desired goals(Trott\net al. (2019)) or temporal distance (Hartikainen et al. (2019); Mendonca et al. (2021)). Additionally,"}, {"title": "6 Conclusion", "content": "We present CE2, a novel Go-Explore mechanism designed to tackle hard exploration problems in\nunsupervised goal-conditioned reinforcement learning tasks. While CE2 outperforms prior explo-\nration approaches in challenging robotics scenarios, the requirement to learn state clusters to identify\nfrontier states and the reliance on world models to determine exploration potential introduce nontrivial\ncomputational costs. Exploring whether CE2's Go-Explore strategy can be effectively applied to\nmodel-free GCRL settings remains an interesting avenue for future work."}, {"title": "Reproducibility Statement", "content": "The codebase of our method is provided on https://github.com/RU-Automated-Reasoning-Group/CE2.\nFor hyperparameter settings and baselines' pesudocode, please refer to Appendix F and G.3."}, {"title": "Appendix", "content": "A Discussion\nWhy does CE2 perform better than the original Go-Explore mechanism?\nOur algorithm, CE2, tackles the core challenge in the Go-Explore mechanism: how to select an\nexploration-inducing goal command g and effectively guide the agent to g? Previous approaches,\nsuch as MEGA, set exploratory goals at rarely visited regions of the state space. However, in these\napproaches, the policies under training may have limited capability of reaching the chosen rare\ngoals, leading to less effective exploration. Our contribution is a novel goal selection algorithm that\nprioritizes goal states in sparsely explored areas of the state space, provided they remain accessible\nto the agent. This is the key factor in why CE2 outperforms the MEGA and PEG baselines in our\nbenchmark suite in Fig. 3. As visualized in Fig. 11 in the appendix for the Ant Maze environment,\nCE2 enhances exploration efficiency by consistently setting exploratory goals within the current\npolicy's capabilities. In contrast, MEGA and PEG often set goals that are unlikely to be reachable by\nthe current agent.\nWhy don't we choose the original Go-Explore as a direct baseline?\nAs discussed above, the core challenge in the Go-Explore mechanism lies in selecting goal states\nthat effectively trigger further exploration upon being reached. However, the original Go-Explore\nmethod (Ecoffet et al. (2019)) does not prescribe a general goal selection method, instead opting for a\nhand-engineered novelty bonus for each task (e.g. task-specific pseudo-count tables). CE2 is more\nrelated to recent instantiations of Go-Explore that automatically selects exploration-inducing goals in\nless-visited areas of the state space to broaden the range of reachable states, e.g. MEGA and PEG.\nTherefore, we compare our method with these tools instead of Ecoffet et al. (2019) in environments\nwhere these tools are applicable, to evaluate the strength of our goal selection method.\nWhy our clustering algorithm does not structure the latent space in the learning process?\nWhile our clustering algorithm does not directly structure the latent space, it requires the latent\nspace to be organized in a specific manner to be effective. In other words, the latent space learning\nalgorithm is a key prerequisite for the latent state clustering algorithm. Specifically, our latent space\nlearning algorithm structures the latent space such that states easily reachable from one another in the\nreal environment (as determined by the learned temporal distance network as Equation 1) are also\nclose together in the latent space. The clustering algorithm leverages this structure-property to ensure\nthat the latent state cluster boundaries align with the frontier of previously explored states. As such,\nCE2 can efficiently generate exploratory goals at the frontier at training time."}, {"title": "B Extended Related Work", "content": "Model-based reinforcement learning (MBRL) has seen significant advancements in recent years,\ndriven by the development of sophisticated world models and planning algorithms. One notable\napproach is Stochastic Ensemble Value Expansion (STEVE) (Buckman et al. (2018)), which enhances\nsample efficiency by leveraging ensemble models to reduce overfitting and uncertainty in value\nestimates. Similarly, the work by Chua et al. (Chua et al. (2018)) demonstrates that probabilistic\ndynamics models can be effectively used to achieve high performance in a small number of trials. In\nthe realm of combining model-based and model-free methods, Deisenroth and Rasmussen (Deisenroth\nand Rasmussen (2011)) introduced PILCO, a data-efficient policy search method that uses Gaussian\nprocesses for dynamics modeling. More recent advancements include the integration of large pre-\ntrained models for world model construction and task planning, as explored by Guan et al. (Guan et al.\n(2023)). The Dreamer framework by Hafner et al. (Hafner et al. (2019a)) utilizes latent imagination\nto learn behaviors directly from pixel observations, and its extensions (Hafner et al. (2020, 2023))\nhave shown impressive results in mastering diverse domains. The Recurrent World Models by Ha\nand Schmidhuber (Ha and Schmidhuber (2018)) also contribute to this line of work by facilitating\npolicy evolution through latent space planning. Several approaches focus on improving exploration\nstrategies within MBRL. For instance, the use of cross-entropy methods for Monte-Carlo Tree Search\n(Chaslot et al. (2008)) and the Curious Replay mechanism (Kauvar et al. (2023)) have been proposed\nto enhance exploration efficiency. The work by Wagenmaker et al. (Wagenmaker et al. (2024)) further"}, {"title": "C Extended Background", "content": "C.1 Dreamer World Model\nWe use the world model structure M of recurrent state-space model (RSSM) of Dreamer(Hafner\net al. (2019a,b, 2020, 2023)) to learn the dynamics. The complete model state of the RSSM is the\nconcatenation of deterministic states and stochastic states, with the latter being generated by the\nformer. The deterministic state ht can used to get the prior state 2t and posterior state zt. The 2t\naims to predict the posterior without access to the current input state xt while the posterior state zt is\nconcluded by integrating the encoded information of current input state xt. The deterministic state ht\nis updated by the recurrent transition function ff using the concatenation (ht, zt) or (ht, 2t) as input.\nThe world model is summarized in Fig 8, and the formulas of components are shown in Equation 8:\nC.2 Temporal Distance Training in LEXA\nThe goal-reaching reward r is defined by the self-supervised temporal distance objective (Mendonca\net al. (2021)) which aims to minimize the number of action steps needed to transition from the current\nstate to a goal state within imagined rollouts. We use bt to denote the concatenate of the deterministic\nstate ht and the posterior state zt at time step t.\n\\(b_t = (h_t, z_t)\\)\nThe temporal distance Dt is trained by sampling pairs of imagined states bt, bt+k from imagined\nrollouts and predicting the action steps number k between the embedding of them, with a predicted\nembedding \\(\\hat{e_t}\\) from bt to approximate the true embedding \\(e_t\\) of the observation xt."}, {"title": "D Limitations and Future Work", "content": "Our method clusters in the latent space, which necessitates a well-trained latent space. This latent\nspace must not only accurately reconstruct the original state space and facilitate dynamic prediction\nbut also reflect the reachable distances between different states. Therefore, training this latent space\nrequires a temporal distance predictor that can accurately estimate the number of action steps needed\nbetween two states. We utilize the temporal distance predictor network from LEXA, which constructs\nintrinsic goal-conditioned rewards, and this network is trained using simulated trajectories. Compared\nto training the temporal distance predictor with real trajectories, using simulated trajectories offers\ngreater stability. Our method requires the temporal distance predictor to reliably estimate the number\nof action steps needed to transition from one state to another, which is a crucial prerequisite for\nensuring the effectiveness of CE2. Moreover, Although CE2 has achieved remarkable success in\ntasks such as stacking blocks and rotating objects, there remain more challenging tasks that CE2\nneeds to address. For instance, environments such as inserting a peg into a hole or fluid tasks in\nManiSkill2(Gu et al. (2023)).\nBesides, our realization of CE\u00b2 is based on Dreamer, a model-based reinforcement learning(MBRL)\nagent known for its higher sample efficiency but greater computational demands compared to model-\nfree alternatives. This increased resource requirement stems from the necessity to develop a world\nmodel. In CE2, this world model is utilized to train policies and value functions through simulated\ntrajectories. At the same time, CE2 use the PEG as the filter of exploration potential, which rely on\nworld model to select goals that guide exploration. Creating a model-free version of CE2 would\nsimplify both its computational and conceptual aspects, a task we plan to undertake in future research."}, {"title": "E Environments", "content": "E.1 3-Block Stacking\nIn this task, a robot is required to stack three blocks into a tower. In PEG, evaluations are conducted\non goals of varying difficulty levels, including 3 easy goals(picking up a single block), 6 medium\ngoals(stacking two blocks), and 6 hard goals(stacking three blocks). We evaluate our agent solely on\nthe 6 hard goals. At the same time, we use only 3 hard goals provided by the training environment as"}, {"title": "F Baselines", "content": "In this section, we present the pseudocode for all baseline methods. Note that, except for L3P, each\nbaseline employs a different strategy for sampling data in the real environment within this framework.\nTherefore, we first display the general training framework for MBRL and subsequently provide the\npseudocode for each baseline's data sampling method in the real environment."}, {"title": "F.1 Go-Explore", "content": "To enhance the exploration area of the explorer, we adopt the Go-Explore strategy (Ecoffet et al.\n(2019)). This approach initially employs a goal-conditioned policy, \u03c0G, to approach a specified goal\ng as closely as possible, a process referred to as the Go-phase. Following this, the explorer, \u03c0\u0395, is\nused to further explore the environment starting from the terminal state of the Go-phase, known as\nthe Explore-phase.\nThe effectiveness of the trajectories generated by the Go-Explore strategy heavily depends on the\nchoice of the goal g during the Go-phase. Thus, establishing an efficient goal selection mechanism\nfor the Go-phase is crucial. If the chosen goal g is too easy, the explorer will not sufficiently explore\nthe environment. Conversely, if the goal g is too difficult, the goal-reaching policy \u03c0G will be unable\nto approach it effectively. Therefore, the objective is to develop a goal selection mechanism that\nidentifies a goal g capable of guiding the agent to a region with high exploration potential during the\nGo-phase."}, {"title": "F.2 GC-Dreamer", "content": "GC-Dreamer follows a goal-conditioned approach where trajectories are collected by goal-conditioned\npolicy G, and the goals are returned from the training environment."}, {"title": "F.3 PEG", "content": "PEG adopts a strategy where trajectories are collected by optimizing a specific equation using the\nMPPI method. The optimized goal is then used to guide exploration through the GO-EXPLORE\nalgorithm."}, {"title": "F.4 PEG-G", "content": "PEG-G combines the utilization of goals from the environment with those generated by optimizing\nthe equation using MPPI. This approach alternates between the two strategies based on the episode\nindex."}, {"title": "F.5 MEGA", "content": "For model-based MEGA, we directly utilize the implementation method described in the PEG paper.\nThis involves transplanting MEGA's KDE model and using a goal-conditioned value function within\nthe LEXA framework to filter goals based on reachability. The PEG paper has demonstrated that their\nimplementation of MEGA achieves superior performance compared to the original MEGA baseline."}, {"title": "F.6 MEGA-G", "content": "Similar to PEG-G, MEGA-G alternates between using goals from the environment and MEGA goal\npicking strategy."}, {"title": "F.7 MEGA+PEG", "content": "MEGA+PEG combines the strategies of MEGA and PEG. this baseline firstly employs MEGA to\nsample a batch of candidate goals, all of which have low density in the replay buffer. Subsequently,\ntheir exploration potential is evaluated using PEG, with the most valuable one selected as the\nexploration goal."}, {"title": "F.8 CE2-noPEG", "content": "CE2-noPEG utilizes a goal-picking strategy based on Gaussian Mixture Model (GMM) clustering to\ngenerate goals for exploration without employing PEG optimization. The goal picked by this method\nis sampled at the edge of our latent space clusters, which is the main contribution of our paper."}, {"title": "F.9 L3P", "content": "Our implementation of L3P follows the original code provided in the L3P paper(Zhang et al. (2021)).\nFor more details on the pseudocode and specific implementation, please refer to the descriptions in\ntheir paper."}, {"title": "G Implementation Details", "content": "G.1 Farthest Point Sampling (FPS) Algorithm"}, {"title": "H Additional Experiments", "content": "H.1 Space explored image for 3-Block Stacking\nIn 3-Block Stacking, we innovatively designed a method based on the coordinates of three blocks to\ndemonstrate the degree of exploration in the environment, showed in Fig 10. Firstly, we establish\nconnections between the coordinates of three blocks in three-dimensional space, forming a spatial\ntriangle. This spatial triangle serves to express the relative positions and distances of the three blocks\nwithin the space. Subsequently, we project this spatial triangle onto the xy-plane. The summation of\nthe lengths of the sides of the projected triangle on the xy-plane reflects the dispersal of the three\nblocks within the xy-plane, while the total sum of the z-coordinates of the three blocks indicates their\nrelative positions in height Utilizing the former as the x-axis and the latter as the y-axis, we depict a\nschematic illustration of the spatial exploration of 3-Block Stacking. We observe that CE2 exhibits\na more targeted and in-depth exploration around the target space (highlighted within the red box)\ncompared to PEG. Simultaneously, we observed that PEG tends to conduct numerous explorations\nin the upper-left region of the exploration graph, which are often futile and irrelevant to the goal of\ncompleting block stacking. This highlights the advantage of CE2, which benefits from the constraints\nimposed by clustering and avoids blindly exploring areas. Moreover, sampling at the edges of clusters\nensures the profitability of exploration, enabling more efficient exploration of the vicinity of the goal\nspace compared to PEG.\nH.2 More Exploration Process\nWe present a comparison of exploration targets generated by CE2, MEGA and PEG approaches over\nthe training process in the Ant Maze environment. In the Fig 11, red points represent the generated\nexploration targets by different methods. We observe that the exploration targets generated by CE2 are\nsignificantly superior to those generated by MEGA and PEG. Specifically, CE2 consistently generates\npoints located at the forefront of agent exploration and within the agent's reachable capability\nrange. In contrast, the targets generated by MEGA exhibit greater dispersion and sparsity, which\nare disadvantageous for concentrated exploration of forefront regions. Moreover, PEG consistently\ngenerates targets outside the Maze channels, rendering these exploration targets not only far beyond\nthe agent's capability range but also meaningless."}, {"title": "NeurIPS Paper Checklist", "content": "1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper's contributions and scope?\nAnswer: [Yes", "nJustification": "Our abstract and introduction clearly articulate the primary claims of the\npaper", "paper.\nGuidelines": "n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well"}]}