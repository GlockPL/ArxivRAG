{"title": "Governing AI Agents", "authors": ["Noam Kolt"], "abstract": "The field of AI is undergoing a fundamental transition\u2014from systems that can produce synthetic content upon request to autonomous agents that can plan and execute complex tasks with only limited human involvement. Companies that pioneered the development of generative AI tools are now building AI agents that can be instructed to independently navigate the internet, perform a wide range of online tasks, and serve as artificial personal assistants and virtual coworkers. The opportunities presented by this new technology are tremendous, as are the associated risks. Fortunately, there exist robust analytic frameworks for confronting many of these challenges, namely, the economic theory of principal-agent problems and the common law doctrine of agency relationships. Drawing on these frameworks, this Article makes three contributions. First, it uses agency law and theory to identify and characterize problems arising from AI agents, including issues of information asymmetry, discretionary authority, and loyalty. Second, it illustrates the limitations of conventional solutions to agency problems: incentive design, monitoring, and enforcement might not be effective for governing AI agents that make uninterpretable decisions and operate at unprecedented speed and scale. Third, the Article explores the implications of agency law and theory for designing and regulating AI agents, arguing that new technical and legal infrastructure is needed to support governance principles of inclusivity, visibility, and liability.", "sections": [{"title": "INTRODUCTION", "content": "Following a family member's death on November 11, 2022, Jake Moffatt\nbooked a next-day flight to Toronto with Air Canada, costing nearly $600.1\nMoffatt did so in reliance on the airline's chatbot, which informed them that\na reduced bereavement rate would be available provided they submitted a\nrequest within 90 days. Upon submitting the request, however, Moffatt was\ntold by Air Canada that the information provided by the chatbot was incorrect\nand did not reflect the airline's policy. To recover their losses, Moffatt filed\na claim in the Civil Resolution Tribunal in British Columbia. Responding to\nMoffatt's allegation of negligent misrepresentation, Air Canada argued that\n\u201cit cannot be held liable for information provided by one of its agents . . .\nincluding a chatbot\u201d and suggested that \u201cthe chatbot is a separate legal entity\nthat is responsible for its own actions.\u201d2 On February 14, 2024, the Tribunal\nissued its decision, holding Air Canada responsible for its chatbot \u201cagent\u201d\nand awarding damages to Moffatt.3\n1 Moffatt v. Air Canada, [2024] B.C.C.R.T. 149 (Can.).\n2 Id. at para. 27\n3 Id at para. 32."}, {"title": "I. AI AGENTS", "content": "Agents are and have always been pervasive.27 Individuals and\norganizations delegate activities to others when they lack the skills to\nundertake those activities themselves or when delegation is more efficient.\nAgency is, put simply, an \u201cancient device for getting business done.\u201d28 It is\ntherefore no surprise that the field of Al has for decades strived to build\nartificial agents that can autonomously perform complex or costly tasks on\nbehalf of humans.29 Thanks to recent advances in AI, this vision is now"}, {"title": "A. Beyond Language Models", "content": "Following the release of ChatGPT in November 2022, language models\nand generative AI have become ubiquitous.30 College students use language\nmodels to write term papers.31 Companies use image and video generators to\nproduce marketing content.32 These applications, however, all share one thing\nin common: the Al systems function as tools. Individuals and companies\ndecide whether or not to use a model, select which model to use, and deploy\nthe model to perform a narrowly scoped task. A human remains, by and large,\nin full control.\nAI agents are different. They are not mere tools, but actors. 33 Rather than\nsimply produce synthetic content, AI agents can independently accomplish\ncomplex goals on behalf of humans.34 According to Mustafa Suleyman, a"}, {"title": "B. Goal, Plan, Action", "content": "How do Al agents work? In short, they are comprised of a language model\nthat serves as the agent's \u201cbrain\" (a widely used anthropomorphism),39\nwhich can then use a variety of external resources (known as \u201cscaffolding\")\nto accomplish the goals set for it.40 These resources fall into three categories:\nplanning, memory, and tool use. For planning, AI agents decompose large\ntasks into smaller, more manageable tasks. One popular method involves\nagents literally instructing themselves to \"think step by step\" when\nconfronted with a new task.41 For memory, AI agents can use external storage\nsources, including vector databases, which enable fast retrieval of\ninformation needed to perform the task at hand.42 For tool use, AI agents call\napplication programming interfaces (APIs) to access external websites and\nsoftware, which significantly extend the agent's abilities. 43 For example, tool\nuse can enable an Al agent to query a search engine, access an organization's\nproprietary data, or provide user credentials to autonomously execute a\nfinancial transaction.44"}, {"title": "C. Concerns", "content": "The range of risks posed by AI technology, including language models,\nis wide and growing. It includes risks of \u201challucinations,\u201d toxic outputs, bias,\ndiscrimination, environmental harms, and leakage of sensitive personal data,\nto name just a few. 51 Because Al agents rely extensively on language models,\nAl agents are susceptible to all of these familiar risks. AI agents, however,\nalso pose new, and potentially more concerning, risks. For example, while a\nchatbot can provide instructions on how to conduct an online phishing\ncampaign, an AI agent can actually execute those instructions. The stakes, in\nother words, are higher because AI agents not only communicate with\nhumans but take actions that can directly affect individuals' rights and\ninterests.52\nThese concerns are likely to grow as AI agents become more capable and\nare delegated more important activities. According to computer scientists at\nthe University of Toronto and Stanford University, \"[t]he failure of [AI]\nagents to follow instructions can lead to a new and diverse array of serious\nrisks, ranging from financial loss, such as when conducting transactions with\nbanking tools, to substantial property damage or even life-threatening\ndangers when operating robots that interact with the physical environment.\"53\nSome of these harms have already materialized. For example, several studies\ndemonstrate that AI agents can autonomously hack websites.54 Other harms\nare more speculative but are nonetheless being actively studied. For instance,\nresearchers who previously tested the capabilities of large language models\nare now turning their attention to test whether AI agents can engage in\n\u201cautonomous replication and adaptation,\u201d such as by acquiring resources\n(e.g., creating a Bitcoin wallet) and producing copies of themselves (e.g.,\nbuilding additional Al agents).55\nSome of these concerns could compound as Al agents increasingly\ncommunicate and interact with one another. The risk is that a failure in one\nsystem could rapidly propagate to others, particularly if AI agents rely upon\neach other to coordinate in carrying out activities. 56 For example, in the case\nof Al agents tasked with running an online retail business, agents acting on\nbehalf of competing vendors may learn to collude with one another, 57\nsetting prices or policies that ultimately harm consumers.58 In the longer run,\nthese networks of interacting AI agents may become more complex and less\ntransparent, making it difficult for humans to effectively monitor the\nactivities they conduct, let alone intervene to address problems that arise.59\nProfessor Jonathan Zittrain employs a powerful analogy to describe such\nproblems:\nWith no framework for how to identify what they are, who set them up, and\nhow and under what authority to turn them off, [AI] agents may end up like\nspace junk: satellites lobbed into orbit and then forgotten. There is the potential\nfor not only one-off collisions with active satellites, but also a chain reaction of\ncollisions. 60\nOf course, these risks give rise to challenging tradeoffs. The more capable\nAI agents become and the greater the scope of activity delegated to them, the\ngreater the efficiency and productivity gains. These greater gains, however,\nare accompanied by correspondingly larger risks. Fortunately, there exist\ntime-tested analytic frameworks for rigorously understanding and\ncharacterizing the tradeoffs arising from the use of agents, namely the\neconomic theory of principal-agent problems and the common law of agency.\nThese frameworks, it is hoped, can both shed light on the problems facing the\nuse of Al agents and, possibly, gesture toward potential solutions."}, {"title": "II. EVERGREEN AGENCY PROBLEMS", "content": "While highly capable AI agents are only now being developed, computer\nscientists have long grappled with the potential consequences of this\ntechnology. Writing in Science magazine in 1960, MIT professor Norbert\nWiener made the following observation:\nIf we use, to achieve our purposes, a mechanical agency with whose\noperation we cannot efficiently interfere once we have started it, because the\naction is so fast and irrevocable that we have not the data to intervene before\nthe action is complete, then we had better be quite sure that the purpose put into\nthe machine is the purpose which we really desire and not merely a colorful\nimitation of it. 61\nThis enduring challenge is commonly known as the alignment problem.62\nGiven Al agents optimize the goals set for them, an incomplete or inaccurate\nrepresentation of that goal can have undesirable consequences.63 This is\nparticularly the case where Al agents encounter novel scenarios that their\ndesigners did not contemplate. 64 The problem, however, is broader.\nAccording to UC Berkeley professor Stuart Russell, co-author of the leading\ntextbook on AI, \"[o]ne of the most common patterns involves omitting\nsomething from the objective that you do actually care about. In such cases\n... the AI system will often find an optimal solution that sets the thing you do\ncare about, but forgot to mention, to an extreme value.\u201d65 In other words,\nhighly capable AI agents are likely to successfully achieve measurable goals,"}, {"title": "A. Information Asymmetry", "content": "The problem of information asymmetry, which features prominently in\nthe economic literature on principal-agent relationships, concerns an agent\nthat has access to better or different information than the principal. 76 This\ninequality of information can arise prior to the selection of an agent and the\ndelegation of activity to an agent.77 For example, a job candidate is likely to\nhave more information about their competencies (or incompetencies) than\ntheir prospective employer. Inequality of information can also persist after an\nagent has acted.78 For instance, an employer may find it difficult to reliably\nascertain whether an employee has in fact carried out their work effectively.\nThe difficulty arises due to the prohibitive costs of directly monitoring an\nagent's performance and the risk of performance metrics being uninformative\nor misleading.79 These agency costs typically increase as agents are tasked\nwith more complex activities, including activities which a principal cannot\nperform themselves, and are granted more discretion in carrying out those\nactivities. 80\nAI agents present similar problems. Individuals or organizations seeking\nto use an Al agent are likely to have limited information about the agent's\nabilities and limitations prior to deploying it, especially if deployment is in a\nnovel setting or application. For example, it may be difficult to determine,\nbefore the fact, whether a \u201cgeneric\u201d AI personal assistant can perform a\nspecialized business task. In addition, even after the fact, it might be difficult\nto determine whether an AI agent has accomplished its goals effectively and\nethically. Of course, the more complex and difficult-to-measure the goals, the\nmore acute these problems. Notably, goals that are difficult to incorporate\ninto performance metrics prior to deployment are also likely to be difficult to\nmeasure after deployment. 81\nThe analog between conventional problems of information asymmetry in\nhuman-only contexts and the issues facing AI agents are further underscored\nby agents' duties under common law. The Restatement (Third) of Agency\nprovides that \"[a]n agent has a duty to use reasonable effort to provide the\nprincipal with facts that the agent knows or should know when . . . the\nagent knows that the principal would wish to have the facts or the facts\nare material to the agent's duties to the principal.\"82 For example, an agent\nmust disclose any breach of obligations owed to the principal.83 And it is the\nagent that bears the onus of showing that it made sufficient disclosure to the\nprincipal.84 In addition, the agent is subject to an overarching duty to act\nhonestly, including in its communications with the principal. 85"}, {"title": "B. Authority", "content": "The challenge of governing agents, however, does not only concern the\navailability of information about an agent, but also the scope of authority\ngranted to an agent. Professor Tamar Frankel, in her seminal work on\nfiduciary law, captures the problem succinctly: \"the purpose for which the\nfiduciary is allowed to use his delegated power is narrower than the purposes\nfor which he is capable of using that power.' .\"90 The common law goes to great"}, {"title": "C. Loyalty", "content": "Under the common law, the duties of agents extend beyond merely acting\nwithin the scope of authority granted to them. Agents are also subject to an\noverarching \u201cfiduciary duty to act loyally for the principal's benefit in all\nmatters connected with the agency relationship.\"103 This duty of loyalty\noperates to address the ever-present concern that agents will exploit their\nposition and fail to act in the principal's best interests. 104 The fiduciary duty\nof loyalty can therefore be seen as \"shift[ing] the [agent's] legal duty from\nself-serving to other-serving. \"105 Concretely, the duty of loyalty prohibits\nagents from (i) acquiring material benefit from transactions taken on behalf\nof the principal, 106 (ii) supporting an adverse party in a transaction connected\nwith the agency relationship, 107 (iii) competing with the principal or assisting\ntheir competitors, 108 or (iv) using property or confidential information of the\nprincipal for its own purposes or those of a third party. 109\nTo what extent do AI agents generate the kind of problems these rules\nwere designed to address? On the one hand, there is only limited (though\ngrowing) evidence that current AI agents pursue \u201cself-interest,\u201d110 which sets\nthem apart from traditional human agents for whom self-interest is the\nprimary cause of agency problems. 111 On the other hand, in many instances\nAI agents nevertheless fail to act in a user's interests. For example, these"}, {"title": "D. Delegation", "content": "The discussion of agency problems thus far has largely focused on the\ncase of a single principal delegating tasks to a single agent. The reality of\nagency relationships, however, is more complicated. Agents can, and often\ndo, appoint \"subagents\u201d to assist in the performance of their obligations. 124\nFor example, if an agent lacks the skills to carry out a task itself (or do so in\na cost-effective manner) it might seek out help from another actor that can.\nA single agent may at times also act on behalf of multiple principals, known\nas \"coprincipals. \".125 The complexity of such relationships is readily apparent\nin the Restatement (Third) of Agency:\nAn agent's relationships with multiple principals may evolve. For example,\na subagent may become an agent for coprincipals if the subagent's appointing\nagent and that agent's principal become coprincipals. It is also possible for the\nsame agent to have more than one such relationship as to the same transaction\nor matter, for example if a subagent is an agent for another party in the same\ntransaction or matter. 126\nThe situation is further complicated where the role of an agent is unclear:\nIn some common situations, the legal relationships among parties are\nambiguous because it is unclear whether an actor is an agent who acts for one\nparty to a transaction, a subagent, an agent who acts for more than one principal,\nor is a provider of services who does not act as agent or subagent for any party\nto the transaction. The same actor may occupy different roles at successive\npoints in an ongoing interaction among the same parties.127\nComparable complexities can arise with Al agents. For example,\nAutoGPT, a rudimentary AI agent discussed above, can create additional AI\nagents to assist in carrying out tasks.128 As with traditional subagents, these\nAl subagents may develop complex relationships with one another and\nengage in potentially undesirable behavior, such as inter-agent collusion. 129\nEven if these new agents can be likened to traditional subagents, they present\nnovel questions concerning the issues of information asymmetry, authority,\nand loyalty. For example, in which circumstances should an Al agent be\nauthorized to appoint a subagent? Should AI agents be entitled to appoint\nhuman subagents (e.g., engage human crowdworkers to perform a task that\nthe AI agent cannot itself perform)?130 Should subagents be required to make\ndisclosures only to the AI agent that created or appointed them, or also to the\nhuman principal that appointed the original AI agent? How should subagents\nresolve conflicts of interest between that AI agent and the human principal,\nespecially given that the human principal may be at a significant information\ndeficit vis-\u00e0-vis a subagent compared with the AI agent?\nThe common law of agency provides some guidance on these questions.\nAn agent can typically appoint subagents in only two scenarios.131 The first\nis if \"the agent reasonably believes, based on a manifestation from the\nprincipal, that the principal consents to the appointment of a subagent.\"132\nThe second is in \u201cemergencies and other unforeseen circumstances, when\ncommunication between agent and principal is not feasible\u201d and the agent is\nrequired \"to take action to protect the principal's interests.\u201d133 While this rule\ncould possibly be adapted to govern an Al agent's authority to appoint\nsubagents, the widespread delegation of activities to additional agents,\nincluding humans, raises issues concerning human-AI interactions and\nbroader systemic effects of integrating AI agents into existing social and\neconomic structures.134 Although conventional agency law and theory can\nshed light on these issues, they are unlikely to offer comprehensive solutions."}, {"title": "III. THE LIMITS OF AGENCY LAW AND THEORY", "content": "The economic theory of principal-agent problems and the common law\nof agency were developed around a particular type of agent: human beings.\nWhether acting as individuals or as part of recognized legal structures such\nas corporations, human beings are at the center of these frameworks. 135 \u03a4\u03bf"}, {"title": "A. Incentive Design", "content": "The goal of incentive design is to motivate an agent to act in the\nprincipal's interests. This can be achieved by harnessing the agent's self-\ninterest to act in ways that also promote the principal's interests.136\nTraditional mechanisms for incentive design fall into two general categories:\ncarrots and sticks.137 The former positively incentivizes or rewards the agent,\nwhile the latter negatively incentivizes or deters the agent. Examples of\ncarrots include profit sharing rules and pay-for-performance regimes in\nwhich an agent's financial returns are directly or indirectly tied to those of\nthe principal. 138 Examples of sticks include the imposition of financial and\nother penalties. 139 Many aspects of agency law, including fiduciary\nobligations, reinforce these mechanisms for incentive design. 140\nThe central problem with applying these mechanisms to AI agents is that\nAI agents are wired differently to human beings.141 In an influential article,\nProfessor Mark Lemley and Bryan Casey help clarify the problem: \u201crobots\ndon't necessarily care about money. They will maximize whatever they are\nprogrammed to maximize.\"142 In other words, the human quality of self-\ninterest traditionally harnessed by economists and lawyers to design incentive\nstructures is arguably absent in the case of Al agents. 143 Consequently,\nmechanisms like pay-for-performance regimes, such as equity compensation\nplans, cannot be readily applied to AI agents, at least not without instilling in\nthese agents some notion of \u201cself-interest\u201d. And, while hardwiring \u201cself-\ninterest\" into Al agents could become technically feasible, it might be\ncounterproductive. AI agents that respond to traditional human incentives,\nsuch as promoting their own financial interests, might exacerbate rather than\nmitigate the alignment problem. Philosopher Matthew Oliver distills the\nirony of the problem:\nAn Al program could be programmed to care about its own resources, but\nthis would create conflicts of interest whenever the program worked on behalf\nof someone else. If AI programs were programmed in this way, then we would\nhave artificially created the very conflicts of interest that agency law tries to\nsolve. 144\nA further problem concerning incentive design mechanisms for AI agents\nis that they target the wrong problem. In the case of traditional human agents,\nthe problem is one of motivation. Harm often occurs due to an agent's\""}, {"title": "B. Monitoring", "content": "A core strategy for identifying and predicting risks that arise in agency\nrelationships is monitoring. Monitoring aims to reduce the information\nasymmetry between the principal and the agent by actively tracking and\nexposing instances of problematic conduct, such as agents failing to comply\nwith instructions or acting outside their mandate.147 Effective monitoring is\nneeded for a principal to exercise control over an agent. Without a thorough\nunderstanding and detailed record of problematic conduct, a principal cannot\ntake remedial action, let alone devise preemptive measures to prevent such\nconduct from occurring. In traditional agency contexts, such as employment\nrelationships, mechanisms for monitoring include ongoing supervision and\nperiodic performance reviews. 148 Monitoring in corporate governance,\nmeanwhile, is facilitated through financial audits, shareholder meetings, and\nreporting obligations. 149\nAlthough essential to tackling agency problems, traditional monitoring\nmechanisms face significant challenges. One challenge is that complete\nmonitoring is rarely, if ever, possible. 150 Acquiring \"perfect information\" is\neither too costly or requires specialized skills that the principal lacks and\ndue to which the agent was retained in the first place.151 For example, a\ncompany may lack the knowledge to determine whether an external engineer\ncomplied with the applicable safety standards. Another challenge is that the\nbehavior of some agents may be genuinely unobservable.152 For instance, it\nmay be impossible for a client (or anyone else) to prospectively evaluate\nwhether an investment advisor made sound decisions. At the time of the\ndecision there might not exist information or standards to make such an\nevaluation.\nThe advent of Al agents compounds these familiar problems and\nintroduces new challenges. The activities of highly capable AI agents will be\ncostly to monitor, especially if they operate at superhuman speed and scale. 153\nIn addition, Al agents could be tasked with activities the performance of\nwhich is difficult to evaluate, such as making business decisions concerning\nthe operation of an online retail store.154 Moreover, AI agents might take\nhighly unpredictable or unintuitive actions, resulting from a combination of\ntheir powerful emergent abilities,155 brittleness,156 and vulnerabilities.157\nOnce again, such actions are likely to be difficult to monitor. 158\nAny attempt to overcome these challenges must begin by recognizing that\nhuman oversight and monitoring are not, on their own, an adequate solution\nfor the challenges presented by Al agents. Professors Rebecca Crootof,\nMargot Kaminski, and Nicholson Price describe \u201cthe basic tautological\nchallenge of relying on humans to monitor the performance of systems\ndesigned to improve on human performance.\u201d159 In other words, relying too\nheavily on human oversight is both impractical and undermines the very\npurpose of developing and using AI agents.\nA promising though potentially perilous\u2014direction involves engaging\nadditional Al agents to monitor the activity of the (original) AI agents.\nThis governance strategy, previously studied by economists and legal\nscholars in the context of human agents,160 is now being explored by Al\nresearchers. 161 The goal is to develop mechanisms for \u201cscalable oversight,\"162\nwhich includes developing Al monitoring agents that can contend with the\nsuperhuman scale and speed of other Al agents. 163 Of course, these\nmonitoring agents are susceptible to the same risks as other AI agents.\nMoreover, to the extent this Al-based monitoring regime increases trust in\nand reliance on Al agents, any failure could have broad and lasting\nrepercussions.164 Finally, even if these significant challenges are overcome,\""}, {"title": "C. Enforcement", "content": "Strategies for governing agent behavior rely on effective enforcement.\nTo constrain an agent's behavior, duties of loyalty, incentive design, and\nother governance strategies need to be supported by mechanisms that impose\nconsequences in the event an agent engages in problematic behavior. 167\nBroadly speaking, these mechanisms can be divided into three categories: (i)\ntermination of the agency relationship; (ii) imposition of legal penalties; and\n(iii) informal or extra-legal sanctions.\nUnder the common law of agency, a principal can terminate the agency\nrelationship by revoking the agent's authority. 168 An agency relationship may\nalso terminate \u201cupon the occurrence of circumstances on the basis of which\nthe agent should reasonably conclude that the principal no longer would\nassent to the agent's taking action on the principal's behalf.\"169 In some\nagency relationships\u2014but not necessarily those recognized as such by the\ncommon law-agents can be subject to a range of penalties, including\nfinancial penalties, the loss of a license or legal permit, and potentially\ncriminal liability. 170 In addition, agents may be subject to informal or extra-\nlegal sanctions, such as reputational harm and social stigma associated with\ncertain actions. While the foregoing mechanisms are primarily punitive in\nnature, they also serve as a deterrent by making it costly for agents to breach\ntheir obligations.171\nApplying these enforcement mechanisms to Al agents is very difficult.\nTo begin with, it may be costly or impractical to terminate AI agents if they\nare deployed in high-stakes settings (where termination would result in\nsignificant economic losses) or if the agents are capable of resisting attempts\nto shut them down.172 Another problem stems from the fact that AI agents do\nnot necessarily have the same interests or motivations as human agents.173\nBecause Al agents do not, by default, explicitly value financial resources or\npersonal freedom, it is unclear how the imposition of financial penalties or\nincarceration could be applied to penalize and deter these artificial agents. 174\nA similar problem arises with respect to informal and extra-legal sanctions in\nthe case of Al agents that are not sensitive to reputational or psychological\nconsequences associated with enforcement actions.175\nImportantly, the suggestion that these problems can be overcome by\nencoding human-like interests and motivations in AI agents is riddled with\ndifficulties and, as discussed, could backfire.176 Designing AI agents that\nvalue their own financial resources could produce hazardous conflicts of\ninterest between Al agents and their human principals. 177 Meanwhile,\ndesigning Al agents that value their own personal freedom could incentivize\nagents to resist efforts to shut them down178 and, thereby, hamper one of the\nkey mechanisms for preventing problematic agent behavior.\""}, {"title": "IV. IMPLICATIONS FOR AI DESIGN AND REGULATION", "content": "Governing agents is an age-old challenge. As we have seen, Al agents\npresent many problems long recognized by lawyers and economists,\nincluding issues of information asymmetry, authority, and loyalty. This new\nform of agent, however, complicates these familiar problems and introduces\nnew challenges. In particular, AI agents are not currently amenable to several\nof the main conventional mechanisms for governing agents. Incentive design,\nmonitoring, and enforcement do not have simple analogs for Al agents.\nAccordingly, rather than attempt to haphazardly adapt these traditional tools\nto a new context, the following section proposes a multi-pronged governance\nstrategy tailored to the particular agency problems associated with AI agents.\nThis strategy, which is centered around three guiding principles\u2014inclusivity,\nvisibility, and liability\u2014aims to lay the groundwork for building the technical\nand legal infrastructure needed to ensure that Al agents operate reliably,\nsafely, and ethically."}, {"title": "A. Inclusivity", "content": "The AI alignment problem has traditionally been cast as the challenge of\nensuring that a single AI agent reliably pursues the goals of a single person.179\nAlthough such \u201csingle-single\u201d alignment remains an open challenge, it is not\nthe only challenge facing Al agents. 180 As a broader range of people and\ngroups use these artificial agents, focusing only on the alignment between a\nsingle AI agent and a single person might miss the mark. 181 Because different\npeople have different (and often conflicting) goals, achieving single-single"}, {"title": "B. Visibility", "content": "The benefits of rigorously studying and tracking AI agents are manyfold.\nFirst, visibility can help identify current and anticipated problems stemming\nfrom AI agents.194 Second, visibility can facilitate interventions that prevent\nor mitigate these problems. 195 Third, visibility can assist in evaluating the\nefficacy of strategies for governing AI agents.196 Without such visibility,\nconsumers are unlikely to entrust AI agents with consequential activities and\npolicymakers will lack assurances that these agents operate safely and in the\npublic interest. Achieving adequate visibility should, therefore, be a priority\nfor both developers and regulators of AI agents.\nWhile many features of AI agents hinder visibility, other features of the\ntechnology can augment visibility. For example, unlike traditional human\nagents, Al agents can be designed to automatically produce detailed records\nof their activities.197 On some level, the actions of Al agents are arguably\nmore transparent and legible than those of human agents. Developers of AI\nagents have a comprehensive understanding of the technical architecture of\nthese systems and the data on which they are trained (which is not the case\nfor human agents).198 In addition, developers can access the so-called\n\u201cinternal monologue\u201d of AI agents, that is, the agents' series of intermediate\nreasoning steps used to make decisions and plan actions.199 Finally,\nresearchers can run experiments on AI agents that would not be possible in\nthe case of human agents. For instance, one study deliberately designed AI\nagents that engage in deceptive behavior in order to test the effectiveness of\nvarious technical safeguards.200\nDespite these opportunities, visibility into AI agents remains limited. In\naddition to operating at a speed and scale that defy traditional monitoring\nmechanisms, 201 current Al agents are, in two distinct senses, black boxes.\nFirst, on a technical level, efforts to systematically study and characterize the\noperation of language models on which AI agents are built are still in their\ninfancy. For example, attempts to reverse-engineer language models focus\nmainly on \u201ctoy models,\u201d which are much smaller than the models typically\nused in commercial settings.202 Second, on an institutional level, actors\noutside the leading companies building Al agents have only limited\ninformation regarding these systems, including information about their\ndesign, training data, and safety testing.203 For example, external actors\ncannot access the training data of the premier models released by OpenAI,\nGoogle, and Meta.204\nOvercoming these challenges requires a combination of technical and\nlegal infrastructure.205 Even in the absence of a systematic understanding of\nthe underlying language models on which Al agents are built, legal and\ntechnical tools can significantly improve visibility into the operation of AI\nagents. For example, one governance proposal advocates developing agent\nidentifiers (to indicate the involvement of an Al agent in a given activity),\nreal-time surveillance (to continuously track and analyze agent activities),\nand logging (to record and document agent activities). 206 Another proposal\naims to expand external auditor access to the underlying language models on\nwhich Al agents are built. 207 By providing auditors with further information\nrelating to state-of-the-art models (including their code and training data),\nauditors will be able to more effectively identify and anticipate safety issues\narising from AI agents and, if necessary, hold the relevant actors accountable."}, {"title": "C. Liability", "content": "Imposing liability on actors responsible for unsafe AI agents aims to\nachieve two main goals. It can compensate parties harmed by such agents and\nincentivize actors developing and operating AI agents to act more cautiously", "questions": 1, "many hands problem.": 9, "sic.": "will be held liable Yet, this\nprinciple of agency law would create dangerous perverse incentives . . .\noperators of AI programs could avoid liability by failing to control the Al\nprograms they operate.212\nAn alternative, and more pragmatic, criterion for allocating liability\namong the actors involved in designing, operating, and using AI agents is to\nconsider each actor's (i) ex ante ability to prevent harm and (ii) resources to\nremedy harm ex post. This criterion could be evaluated by assessing an\nactor's access to information about an Al agent (e.g., results of safety tests),\nits ability to alter the agent's design or operation, 213 and, more broadly, an\nactor's technical and financial resources that could support preventive\nmeasures or remedy harms after the fact.214\nTurning to the second question concerning the establishment of a liability\nregime for Al agents\u2014namely, determining the circumstances in which\nliability should arise\u2014the law of agency is a more helpful guide. The\nRestatement (Third) of"}]}