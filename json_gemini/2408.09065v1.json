{"title": "Linking Robustness and Generalization: A k* Distribution Analysis of Concept Clustering in Latent Space for Vision Models", "authors": ["Shashank Kotyan", "Pin-Yu Chen", "Danilo VAsconcellos Vargas"], "abstract": "Most evaluations of vision models use indirect methods to assess latent space quality. These methods often involve adding extra layers to project the latent space into a new one. This projection makes it difficult to analyze and compare the original latent space. This article uses the k* Distribution, a local neighborhood analysis method, to examine the learned latent space at the level of individual concepts, which can be extended to examine the entire latent space. We introduce skewness-based true and approximate metrics for interpreting individual concepts to assess the overall quality of vision models' latent space. Our findings indicate that current vision models frequently fracture the distributions of individual concepts within the latent space. Nevertheless, as these models improve in generalization across multiple datasets, the degree of fracturing diminishes. A similar trend is observed in robust vision models, where increased robustness correlates with reduced fracturing. Ultimately, this approach enables a direct interpretation and comparison of the latent spaces of different vision models and reveals a relationship between a model's generalizability and robustness. Results show that as a model becomes more general and robust, it tends to learn features that result in better clustering of concepts.", "sections": [{"title": "Introduction", "content": "The rapid advancements in computer vision and deep learning have led to the development of powerful vision models capable of extracting intricate features from visual data (Radford et al. 2021; Jia et al. 2021; Cherti et al. 2023). These models are central to various applications, from object recognition to image generation. Typically, their generalizability is measured through zero-shot classification performance (Wortsman et al. 2022), making the evaluation of vision models indirect. However, these evaluations often rely on a projection of the learned latent space, which may not fully capture the quality or nuances of the underlying representations and offer little insight into improving them. Understanding the structure and quality of a latent space is crucial for gaining insights into how vision models process and organize visual information. Traditional methods like t-SNE (Maaten and Hinton 2008) and UMAP (McInnes, Healy, and Melville 2018) offer visualizations of this high-dimensional space by reducing its dimensions. While these methods provide insights about a latent space, they are less effective when comparing multiple latent spaces. As vision models become more sophisticated, there is a growing need for methods that offer a more detailed and interpretable analysis of the latent space beyond mere visual inspection.\nIn this context, we focus on local neighborhood structures within the high-dimensional latent space analyzed using the k* distribution proposed by (Kotyan, Ueda, and Vargas 2023) for evaluating and comparing the latent spaces. This method preserves the neighborhood information, similar to t-SNE (Maaten and Hinton 2008) and UMAP (McInnes, Healy, and Melville 2018), in the local neighborhood of samples and directly examines the distribution and clustering of individual concepts. By doing so, k* Distributions offers a more nuanced understanding of the relationships within the latent space and provides objective comparisons between the latent spaces.\nThe k* distribution assesses the index of the nearest neighbor from a different concept (class), offering insights into the cohesion and fracture in distributions belonging to similar concepts. This enables meaningful comparisons between the distribution of samples belonging to different classes and multiple latent spaces. Moreover, it is complementary to the existing analyses available. By providing the understanding about the structure of distribution of samples in the latent space, this approach augments the insights from dimensionality reduction visualizations or projection-based evaluation, making the interpretability and comparison of the latent spaces easier."}, {"title": "Contributions", "content": "Quantification of Quality of Latent Space:\nWe quantify the quality of latent space based on the Skewness coefficient of k* Distribution as described in Figure 1. We derive a True Skewness Coefficient \u0393k* as a more accurate measure of latent space's quality, and we derive an Approximate Skewness Coefficient \u0393\u00b4 * that can be used across concepts from multiple datasets.\nLarge Scale Study of Robust Vision Models:\nWe compare the different types of robust models available at RobustBench Library (Croce et al. 2020) and note that more robust models have less degree of fracturing in the latent space. This indicates that as the models become more robust, they can cluster the individual concepts correctly, narrowing the tradeoff between accuracy and robustness.\nLarge Scale Study of CLIP-based Vision Models:\nWe compare the different pre-trained CLIP models available at OpenCLIP Library (Ilharco et al. 2021) and observe that there is a visible less degree of fracturing in the latent space by CLIP models that are better at generalizability for most evaluated datasets. This suggests that as models become better at generalizing to other datasets, they also become better at clustering the individual concepts."}, {"title": "Related Works", "content": "Analyzing the Latent Space of Neural Networks\nAnalyzing the latent space of deep neural networks poses a significant challenge due to the high-dimensional nature of the features. To address this, researchers have developed various dimensionality reduction techniques that allow for the visualization and interpretation of these complex spaces in lower dimensions, typically 2D or 3D. Among the most widely used techniques are t-SNE (Maaten and Hinton 2008) and UMAP (McInnes, Healy, and Melville 2018), which are used for their ability to preserve local and global structures, respectively. These techniques are part of a broader family of methods designed to make the high-dimensional latent space more interpretable by projecting it into a more manageable form, including classical approaches like PCA (Hotelling 1933) and MDS (Kruskal 1964), as well as more recent methods like Diffusion Maps (Coifman and Lafon 2006) and TriMap (Amid and Warmuth 2022).\nInterpretation using dimensionality reduction techniques largely depends on the organization of the latent space. When the latent space is well-structured, and the encoded information aligns with meaningful patterns, these visualizations can be highly effective, providing insights that correlate with established interpretations. However, where the latent space lacks a clear structure, the utility of these methods diminishes. Without predefined organization, the resulting plots often appear as amorphous clusters, providing little actionable insight (Sivaraman, Wu, and Perer 2022).\nBeyond dimensionality reduction, other approaches focus on visualizing the interactions between neural network features and the latent space. By analyzing the activation patterns of hidden units in response to specific inputs, researchers can gain insights into which features are emphasized by the network (Mahendran and Vedaldi 2015; Olah et al. 2018). Tools like Activation Atlas (Carter et al. 2019) offers a way to explore how combinations of features are represented, further illuminating the structure of the learned latent space.\nHowever, these visualization techniques often involve complex hyperparameter tuning, making it difficult to achieve consistent and fair comparisons across different latent spaces, especially when comparing vision models with varying dimensionalities. The interpretability of these visualizations becomes increasingly complicated when multiple models are involved, as the differences in their latent space structures can lead to incomparable results. This complexity underscores the need for more robust and interpretable methods to analyze and compare latent spaces across models (Gleicher et al. 2011; Arendt et al. 2020; Cutura et al. 2020; Boggust, Carter, and Satyanarayan 2022; Sivaraman, Wu, and Perer 2022)."}, {"title": "Evaluating the Latent Space of Neural Networks", "content": "Recent research has focused on evaluating the effectiveness of vision models through their performance in downstream tasks, particularly zero-shot transfer classification across multiple datasets like proposed in VTAB (collection of 19 different image-classification datasets) (Zhai et al. 2019) and ELEVATER (collection of 20 different image-classification datasets)(Li et al. 2022). In these studies, a vision model's performance on a variety of datasets is used as a proxy for measuring its generalization capabilities, with improved results across diverse datasets interpreted as evidence of broader visual concept coverage. This performance is often evaluated using Zero-shot classification which is typically categorized into two main types: a) (Traditional) Class-Level Zero-Shot, which measures how well a vision model generalizes to unseen object categories, and b) (More recent) Task-Level Zero-Shot, which assesses the vision model's ability to generalize to entirely new datasets.\nWhile these evaluations provide useful insights, they are often evaluated indirectly using projection of the latent space and do not directly measure the intrinsic quality of the vision model itself. Language-Free Vision Models, for instance, rely solely on a vision model that produces feature vectors from images, followed by a randomly initialized linear layer acting as the classifier (Dosovitskiy et al. 2020). Although these can be adapted to different tasks, this often depends on external factors, such as the effectiveness of the added classifier, rather than the model's inherent capability to capture visual information. On the other hand, Language-Augmented Vision Models integrate image and text encoders, projecting features into a shared space (Radford et al. 2021; Jia et al. 2021). This approach enables zero-shot learning by comparing image features with averaged text features representing different categories. However, the success of such models may be more attributable to the alignment between visual and textual representations than to the vision models' standalone performance.\nMoreover, adaptation techniques like random-initialized adaptation, where a linear layer is added to a pre-trained vision model without using language features (Kornblith, Shlens, and Le 2019), and language-initialized adaptation, which can either initialize the linear layer with text features or directly integrate visual and text features into a single projection, further complicate the evaluation process (Zhou et al. 2022; Wortsman et al. 2022). These methods introduce additional variables that can obscure the true quality of the image encoder, making it difficult to isolate and assess its effectiveness independently of the adaptation process.\nIn summary, existing evaluation methods provide valuable insights into the performance of vision models across various tasks. However, these methods often rely on indirect measures and are influenced by factors unrelated to the models' inherent capabilities. Conversely, dimensionality reduction techniques offer a more direct interpretation of the learned latent space but pose challenges when comparing multiple latent spaces. In this article, we evaluate and analyze the latent spaces of vision models using the k* distribution, aiming to enhance the interpretability of dimensionality reduction techniques and facilitate meaningful comparisons across different tasks."}, {"title": "k* Distribution for Latent Space Analysis", "content": "The k* distribution proposed by (Kotyan, Ueda, and Vargas 2023) provides a robust method for analyzing the structure of hyperdimensional latent spaces learned by Vision Models, focusing on local neighborhood dynamics. This approach is instrumental in understanding how samples and clusters are distributed within the latent space by associating them with their respective concepts (classes). By analyzing k* distribution, one can gain valuable insights into the patterns and formations of clusters and their underlying structure within the latent space.\nAt the core of this methodology is the k* value, which represents the index of the kth nearest neighbor that belongs to a different concept (class) than the test sample. This value measures the neighborhood uniformity; a high k* value suggests that a sample is surrounded by many neighbors from the same class, indicating a well-formed homogeneous cluster. On the other hand, a low k* value points to the proximity of a neighbor from a different class, signaling potential overlap or a fragmented neighborhood. This approach assesses the homogeneity and cohesion of class clusters and helps identify whether they are concentrated or dispersed across multiple regions within the latent space."}, {"title": "Mathematical Framework", "content": "Consider a set of sample-label pairs X:\n(X1,Y1), (X2,Y2), ..., (xn, Yn) where x represents the input samples and Y denotes their corresponding labels. For a given concept (class) c, let Sc denote the set of all samples with the same label c:\n$S_c = \\{x_i | \\forall x_i \\in X \\text{ such that } Y_i = c\\}$                                                                                                                                                                                              (1)\nThe kth nearest neighbour x of a sample xp is defined as,\nxq where, q\u2208 arg min distance(xq, Xp)\nP                                                                                                                                                                                                                            (2)\nsuch that       P\u2081 = X \u2212 {x | \u2200j < i}\nwhere distance(a, b) is the distance between two samples a and b. Using this, we can construct a sorted local neighborhood space Np of sample (xp) as defined:\n$N_p = (x_1, x_2, ... x_m)$                                                                                                                                                                                                              (3)\nhere, distance(x, xp) < distance(x, xp), where i < j.\nThe k* value of a test sample (xp, Yp) is the index of the nearest neighbor that has a different label than Yp, formally defined as:\nk = arg min{x | x \u2208 Np, Y \u2260 Yp},\n(Xp, Yp)\nP                                                                                                                                                                                                    (4)\nwhere i is the index of the nearest neighbor, Yp is the label of test sample xp and YP is the label of the nearest neighbor (sample) x that differs compared to label Yp. Thus, the k* distribution k*(\u00b7) of concept (class) c can be written as,\nk* (Sc) =\nk*\nScxp Sc\n(5)"}, {"title": null, "content": "here Sc is the number of samples of concept (class) c. Finally, the Skewness Coefficient of k* distribution (k*) can be defined as\nYk* =\n1 \u03a3(k* (S) \u2013 \u03bck*)3\n/2\n(6)\n(\u03a3(k* (S) \u2013 \u03bck*)2 )2\nwhere \u03bc\u03b5* is the mean of k* distribution. It measures the asymmetry of the k* distribution about its mean \u00b5k*. Positive skewness indicates a distribution skewed towards lower k* values, indicating fracturing, while negative skewness indicates a distribution skewed towards higher k* values, indicating clustering.\nWe can denote, the skew of k* distribution k* for each ith concept (class) available in our sample-label pairs X as Vi,k*. In this case, the S in the equation corresponds to the subset of dataset points as defined in Equation 1. Using the individual k* distributions Yi,k*, we can define the Approximate Skewness Coefficient \u0393\u00b4k* as,\nC\n\u0393* =Viki\n(7)\ni=1\nFor measuring the True Skewness Coefficient \u0393\u03ba* using the overall k* distribution computed for the entire evaluated sample-label pairs X, Equation 6 can be rewritten as,\n\u0393\u03ba* =\n\u03a3(k* (\u03a7) \u2013 \u03bc\u03ba*)3\n3/2\n(8)\n(\u03a3(k*(X) - \u03bc*)2)\nNote: The Approximate Skewness Coefficient * is not a true metric as the skew coefficient Yk is not a linear statistic. However, it is an approximation assuming that the individual Vi,k* are independent and have low variance. Further, Central Limit Theorem (CLT) and the Law of Large Numbers ensure that this Approximate Skewness Coefficient \u0393\u00b4* will converge to the True Skewness Coefficient \u0393\u03ba* as the number of concepts (classes) increase, provided that the individual Vi,k* remain independent. This assumption that the individual Vi,k* are independent is also useful when we want to compare the concepts from different datasets, i.e., sample-label pairs X."}, {"title": "Patterns in Latent Space Distribution", "content": "Pattern A (\u2605) Fractured distribution of samples:\nIn this latent space configuration, multiple clusters of testing samples are observable; each separated in the latent space. Consequently, most points exhibit low k* values, as they belong to smaller clusters. Conversely, no points display high k* values, given the presence of points from another class distribution situated between the various sub-clusters of the testing class. The k* distribution for this clustered distribution of samples in latent space is markedly positively skewed (k* > 0.5), i.e., skewed towards lower k* values.\nPattern B (4) Overlapped distribution of samples:\nThis latent space configuration represents the scenario when samples from two or more classes overlap. Consequently, some points possess low k* values, suggesting their location in the overlapping region, while others have high k* values, signifying their deep embedding within a class cluster. Due to the diverse distribution of samples in this latent space, the k* distribution appears nearly uniform (-0.5 < \\k* < 0.5).\nPattern C () Clustered distribution of samples:\nA homogeneous cluster of testing samples is prevalent in this latent space arrangement. As a result, most points boast high k* values, indicating their deep placement within the cluster. Simultaneously, some points may exhibit low k* values as they reside on the cluster's periphery; these edge samples might be closer to points from another class distribution than the majority within the cluster. Owing to this concentrated distribution of samples, the k* distribution for this clustered distribution of samples in latent space is strongly negatively biased (k* < -0.5), i.e., skewed towards higher k* values, symbolizing a dense cluster."}, {"title": "Analyzing Robust Image Encoders", "content": "Experimental Setup\nIn this study, we evaluate the robust vision models available at RobustBench Library (Croce et al. 2020) These models have been comprehensively evaluated through various adversarial attacks to be classified as robust. The repository offers pre-trained weights for these models, which we use for our analysis.\nResults\nFigure 2 compares two variants of robust Wang2023Better_WRN-70-16 model: an L2 robust variant and an Lo robust variant. Visual inspection reveals that understanding the latent space using t-SNE and UMAP is subjective, making it difficult to compare latent spaces using these methods. Furthermore, limited insights can be derived from this comparison. In contrast, comparing the k* Distribution between the variants reveals a significant difference. Additionally, each class can be individually analyzed, providing further insights into individual classes. The k* Distribution allows us to compare and conclude that the L2 robust variant of the model is better at evaluating individual concepts. In contrast, the L\u221e robust variant tends to fracture the latent space more."}, {"title": null, "content": "To further determine the differences between the L2 robust, Lo robust, and Corruptions robust variants, the k* distribution is computed across all robust models available in (Croce et al. 2020). The average number of fractured, overlapped, and clustered classes is also computed. This analysis, reported in Table 1 shows that the number of fractured classes increases in the order of L\u221e robust, L2 robust, and Corruptions robust.\nAdditionally, when \u0393* and \u0393\u00b4* are computed for each model and averaged, a noticeable difference in quantifying the latent space is observed. We also notice a high variation in the individual skewness coefficient Yi,k*, accounting for the difference in approximation. However, \u0393\u00b4\u2217 is consistently less than \u0393k*. The individual analyses of the models are provided in the appendix.\nTo analyze the quality of the latent space with respect to performance, a comparison of the models based on the Average Skewness Coefficient \u0393* over both Natural Accuracy and Robust Accuracy is visualized in Figure 3. The comparison includes L\u221e, L2, and Corruptions robust models for CIFAR-10, L\u221e robust models for CIFAR-100, and Loo and Corruptions robust models for ImageNet-1k."}, {"title": null, "content": "The figure shows that as the model improves in both Natural Accuracy and Robust Accuracy, there is a degradation in the degree of fracturing evaluated using \u0393k*\nThis analysis demonstrates that as models become more robust, they better interpret the concepts (classes) by improving the clustering of the concept's samples. However, it is also noted that \u0393* > 0 for all L\u221e robust and L2 robust models, and most Corruptions robust models, indicating that while these methods improve clustering, the overall latent space remains fractured, preventing the formation of n homogenous clusters, where n is the number of concepts."}, {"title": "Analyzing CLIP-based Image Encoders", "content": "Experimental Setup\nOpen CLIP models, as provided by (Ilharco et al. 2021) are utilized to evaluate CLIP-based vision models in this study. These models have undergone extensive evaluation on various datasets, and the repository offers both evaluation scripts and pre-trained weights, which are leveraged for our analysis. We evaluate the models on different datasets inspired by evaluation strategies defined by VTAB (Zhai et al. 2019) and ELEVATER (Li et al. 2022) encompassing multiple categories as described below,\nNatural Image Datasets\nThese datasets contain natural images captured using standard cameras for classical vision problems. The classes may represent Generic, Fine-Grained, or Abstract objects. This group includes: Caltech-101 (Fei-Fei, Fergus, and Perona 2004), CIFAR-10, (Krizhevsky, Hinton et al. 2009), CIFAR-100 (Krizhevsky, Hinton et al. 2009), Country211 (Radford et al. 2021) Describable Textures (DTD) (Cimpoi et al. 2014), FGVC Aircraft (Maji et al. 2013), Food-101 (Bossard, Guillaumin, and Van Gool 2014), GTSRB (Stallkamp et al. 2012), Oxford Flowers-102 (Nilsback and Zisserman 2008), Oxford IIIT-Pets (Parkhi et al. 2012), Pascal VOC2007 (Everingham et al. 2010), and Stanford Cars (Krause et al. 2013).\nSpecialized Image Datasets\nThese datasets contain images captured through specialist equipment for specialized problems. It contains three subgroups of: a) Remote-sensing, consisting of datasets such as Resisc45 (Cheng, Han, and Lu 2017) and Eurosat (Helber et al. 2019), b) Medical Images, consisting of datasets such as Patch Camelyon (Veeling et al. 2018).\nStructured Image Datasets\nThese datasets assess comprehension of the structure of scene and contain datasets of a variety of tasks, such as 3D dataset like Clevr (Johnson et al. 2017), handwritten digits like MNIST (LeCun, Cortes, and Burges 2010), optical character recognition like Rendered SST-2 (Radford et al. 2021), and frames captured from a car like KITTI Dataset (Geiger et al. 2013).\nImageNet-1k-like Image Datasets\nThese datasets belong to the same domain as the original ImageNet-1k (Deng et al. 2009) like, ImageNet-Sketch (Wang et al. 2019), ImageNet-v2 (Recht et al. 2019), ImageNet-A (Hendrycks et al. 2021b), ImageNet-O (Hendrycks et al. 2021b), ImageNet-R (Hendrycks et al. 2021a), ObjectNet (Barbu et al. 2019)."}, {"title": "Results", "content": "k**\nFigure 4 a comparison of different Open CLIP models is presented using the Approximate Skewness Coefficient \u0393. Comparision using True Skewness Coefficient \u0393k* is presented in the appendix. The x-axis displays the accuracy of the models, allowing us to correlate performance with the quality of the latent space. A general trend emerges where a lower Approximate Skewness Coefficient F\u2081* is associated with better accuracy in zero-shot classification across the evaluated datasets.\nHowever, this trend does not apply universally to all datasets. For structured datasets, the evaluated models consistently fracture the latent space similarly, indicating that current training strategies do not enhance concept understanding for these datasets. Results for Rendered SST 2 further reveal that vision models struggle with clustering the characters and semantic meaning, indicating why vision models struggle to generate the text when employed in image generation.\nDatasets like Patch Camelyon, GTSRB, ImageNet-Sketch, and ImageNet-O also deviate from this pattern. These datasets have minimal deviation in Approximate Skewness Coefficient *, indicating that current models do not improve the clustering of concepts for these datasets. We also note that while datasets like Patch Camelyon are highly fractured, datasets like GTSRB, ImageNet-Sketch, and ImageNet-O have less degree of fracturing, but the latent space is highly overlapped for these samples. Improvement in performance for these datasets, therefore, can be attributed to better projection to linearly separate the concepts rather than the vision model's intrinsic property."}, {"title": "Conclusion", "content": "In this study, the k* Distribution is utilized to directly examine the latent space of various types of vision models (such as robust and CLIP-based), revealing insights into how individual concepts are structured in these models' latent spaces. By introducing skewness-based metrics, both true and approximate, the study quantifies the quality of these latent spaces. The findings highlight that current vision models often fragment the distributions of individual concepts within the latent space. However, as models improve in generalization and robustness, the degree of fracturing decreases. This suggests that better generalization and robustness are associated with a more coherent clustering of concepts in the latent space. The quantification of the analysis of k* Distribution offers a direct and interpretable approach for comparing latent spaces, establishing a clear relationship between a model's generalization, robustness, and the quality of its latent space."}]}