{"title": "Generative Topology Optimization: Exploring Diverse Solutions in Structural Design", "authors": ["Andreas Radler", "Eric Volkmann", "Johannes Brandstetter", "Arturs Berzins"], "abstract": "Topology optimization (TO) is a family of computational methods that derive near-optimal geometries from formal problem descriptions. Despite their success, established TO methods are limited to generating single solutions, restricting the exploration of alternative designs. To address this limitation, we introduce Generative Topology Optimization (GenTO) \u2013 a data-free method that trains a neural network to generate structurally compliant shapes and explores diverse solutions through an explicit diversity constraint. The network is trained with a solver-in-the-loop, optimizing the material distribution in each iteration. The trained model produces diverse shapes that closely adhere to the design requirements. We validate GenTO on 2D and 3D TO problems. Our results demonstrate that GenTO produces more diverse solutions than any prior method while maintaining near-optimality and being an order of magnitude faster due to inherent parallelism. These findings open new avenues for engineering and design, offering enhanced flexibility and innovation in structural optimization.", "sections": [{"title": "1 Introduction", "content": "Topology optimization (TO) is a computational design technique to determine the optimal material distribution within a given design space under prescribed boundary conditions. A common objective in TO is the minimization of structural compliance, also called strain energy, which measures the displacement under load and is inverse to the stiffness of the generated design.\nDue to the non-convex nature of TO problems, these methods generally provide near-optimal solutions, with no guarantees of converging to global optima (Allaire et al., 2021). Despite this, TO has been a critical tool in engineering design since the late 1980s and continues to evolve with advances in computational techniques, including the application of machine learning.\nTraditional TO methods also produce a single design, which limits its utility. Engineering problems often require designs that balance performance with other considerations, such as manufacturability, cost, and aesthetics. These are associated with uncertainty, implicit knowledge, and subjectivity and are hard to optimize in a formal framework. Generating multiple diverse solutions allows for exploring these trade-offs and provides more flexibility in choosing designs that meet both technical and real-world constraints. However, conventional TO methods cannot efficiently explore and"}, {"title": "2 Background", "content": null}, {"title": "2.1 Topology optimization", "content": "Topology optimization (TO) is a computational method developed in the late 1980s to determine optimal structural geometries from mathematical formulations (Bends\u00f8e & Kikuchi, 1988). It is widely used in engineering to design efficient structures while minimizing material usage. TO iteratively updates a material distribution within a design domain under specified loading and boundary conditions to enhance properties like stiffness and strength. Due to the non-convex nature of most TO problems, convergence to a global minimum is not guaranteed. Instead, the goal is to achieve a near-optimal solution, where the objective closely approximates the global optimum. There are four prominent method families widely recognized in TO. In this work, we focus on SIMP and refer the reader to Yago et al. (2022) for a more detailed introduction to TO.\nSolid Isotropic Material with Penalization (SIMP) is a prominent TO method we adopt for GenTO. SIMP starts by defining a mesh with mesh points $x \\in X, \u0456 \\in {1, ..., N}$ in the design region. The aim is to find a binary density function at each mesh point $p(x_i) \\in {0,1}$, where $p(x_i) = 0$ represents void and $p(x_i) = 1$ represents solid material. To make this formulation differentiable, the material density p is relaxed to continuous values in [0, 1]. A common objective of SIMP is to minimize the compliance C, which is a measure of the structural flexibility of a shape. Intuitively, the lower the compliance, the stiffer the shape. The SIMP objective is then formulated as a constrained optimization problem:\nmin : $C(\\rho) = u^TKu$\ns.t. : $V = \\sum_{i=1}^N \\rho_i v_i \\leq V^*$\n$0 \\leq \\rho_i \\leq 1 \\forall i \\in N$ (1)\nwhere u is the displacement vector, K, is the global stiffness matrix, V is the shape volume, and V* is the target volume. The density field is optimized iteratively. In each iteration, a finite element (FEM) solver computes the compliance and provides gradients to update the density field p. To encourage binary densities, intermediate values are penalized by raising p to the power p > 1. Hence, the stiffness matrix is\n$K_\\rho = \\sum_{i=1}^N K_i$ (2)\nwhere Ki describes the stiffness of solid cells and depends on material properties.\nAnnealing is employed to make the continuous relaxation closer to the underlying discrete problem (Kirkpatrick et al., 1983), enhancing the effectiveness of gradient-based optimization methods. Annealing gradually adjusts the sharpness of a function during the optimization. For TO, this is often done by gradually increasing the penalty p or by scheduling a sharpness filter. A common choice is the Heaviside filter:\nH (\u03c7, \u03b2) = 0.5 + $\\frac{tanh(\u03b2(x -0.5))}{2 tanh(0.58)}$ (3)"}, {"title": "2.2 Shape generation with neural networks", "content": "Neural fields offer a powerful framework for geometry processing, utilizing neural networks to model shapes implicitly. Unlike conventional explicit representations like meshes or point clouds, implicit shapes are defined as level sets of continuous functions. This approach enables high-quality and topologically flexible shape parametrizations (Chen & Zhang, 2019). The two prevalent methods for representing implicit shapes are Signed Distance Functions (SDF) (Park et al., 2019; Atzmon & Lipman, 2020) and density (or occupancy) (Mescheder et al., 2019b) fields. We opt for the density representation due to its compatibility with SIMP optimization.\nA neural density field employs a neural network $f_\\theta : R^{d_x} \\rightarrow (0,1)$ with parameters @ and the shape \u03a9 is defined as a level-set of $f_\\theta$. For \u03c4\u2208 R, \u03a9 := {$x \\in R^{d_x}|f_\\theta(x) > \u03c4$}, the boundary \u2202\u03a9 := {$x \\in R^{d_x}|f(x) = \u03c4$}, where $d_x \u2208 {2,3}$ for 2D or 3D shapes.\nConditional neural fields. While a neural density field represents a single shape, a conditional neural field represents a set of shapes with a single neural network (Mehta et al., 2021). To this end, a modulation code z \u2208 Rdz is provided as additional input to the network. The resulting network $f_\\theta(x,z)$ parametrizes a set of shapes, which is modulated by z. There are different ways to incorporate the modulation vector into the network, such as input concatenation (Park et al., 2019), hypernetworks (Ha et al., 2017), or attention (Rebain et al., 2022).\nCompression perspective. A neural implicit shape can be regarded as a lossy compression of a shape into the weights of the neural network (Xie et al., 2022). As an illustrative example, consider the memory requirements of a high-fidelity shape in a voxel representation compared to a neural density representation. While the neural implicit shape might not perfectly approximate the voxel representation, it requires fewer bits of storage. For conditional neural fields, the compression factor is usually higher, as the network does not have to store the mutual information between the shapes for each shape individually."}, {"title": "3 Method", "content": null}, {"title": "3.1 Generative Topology Optimization", "content": "Definitions. Let $X \\subset R^{d_x}$ be the domain of interest in which there is a shape $\u03a9 \\subset X$. Let $Z \\subset R^{d_z}$ be a discrete or continuous modulation space, where each $z \\in Z$ parametrizes a shape $\u03a9_z$. The possibly infinite set of all shapes is denoted by $\u03a9_Z = {\u03a9_z|z \\in Z}$. The modulation vectors {$z$} are either elements in a fixed, finite set Z or sampled according to a continuous probability distribution p(Z) on an interval $[a, b]^n \\subset R^n$. For brevity, we will use $z \\sim p(Z)$ for both of these cases.\nFor density representations a shape $\u03a9_z$ is defined as the set of points with a density greater than the level $\u03c4 \\in [0, 1]$, formally $\u03a9_z = {x \\in X | p_z(x) > \u03c4}$. While some priors work on occupancy networks (Mescheder et al., 2019a) treat \u03c4 as a tunable hyperparameter, we follow Jia et al. (2024) and fix the level at \u03c4 = 0.5. We model the density $p_z(x) = f_\\theta(x, z)$, corresponding to the modulation vector z at a point x using a neural network $f_\\theta$, with @ being the learnable parameters.\nGenTO aims to solve a constrained optimization problem with the objective of minimizing the expected compliance of multiple shapes subject to a volume constraint on each. At the core of our method is the diversity constraint $\u03b4({\u03a9_z})$, defined over multiple shapes to make them less similar. This leads to the following constrained optimization problem:"}, {"title": "3.2 Diversity", "content": "The diversity loss, defined in Equation 4, requires the definition of a dissimilarity measure between a pair of shapes. Generally, the dissimilarity between two shapes can be based on either the volume of a shape \u03a9 or its boundary \u2202\u03a9. GINNS (Berzins et al., 2024) use boundary dissimilarity, which is easily optimized for SDFs since the value at a point is the distance to the zero level set. However, this dissimilarity measure is not applicable to our density field shape representation. Hence we propose a boundary dissimilarity based on the chamfer discrepancy in the We also developed a volume-based dissimilarity (detailed in Appendix C.1), however we focus on boundary diversity due to its promising results in early experiments.\nDiversity on the boundary via differentiable chamfer discrepancy. To define the dissimilarity on the boundaries of a pair of shapes (\u0398\u03a91, \u0398\u03a9\u2082), we use the one-sided chamfer discrepancy (CD):\nCD(\u0398\u03a91, \u0398\u03a92) = $\\frac{1}{|\u0398\u03a9_1|}$ $\\sum_{x \\in \u0398\u03a9_1}$ $\\min_{y \\in \u0398\u03a9_2}$ $||x - y||_2$ (6)\nwhere x and y are sampled points on the boundaries. To use the CD as a loss, it must be differentiable w.r.t. the network parameters 0. However, the chamfer discrepancy CD(\u0398\u03a91, 8\u03a92) depends only on the boundary points $x_i \\in \u0398\u03a9$ which only depend on $f_\\theta(x_i)$ implicitly. Akin to prior work (Chen et al., 2010; Berzins et al., 2023; Mehta et al., 2022), we apply the chain rule and use the level-set equation to derive\n$\\frac{\\partial CD}{\\partial \\theta}$ = $\\frac{\\partial CD}{\\partial x}$ $\\frac{\\partial x}{\\partial \\rho}$ $\\frac{\\partial \\rho}{\\partial y}$ $\\frac{\\partial y}{\\partial \\theta}$ = $\\frac{\\partial CD}{\\partial x}$ $\\frac{\\nabla_x f_\\theta}{f_\\theta^2}$ $\\frac{\\partial y}{\\partial \\theta}$ (7)\nwhere y = p is the density field in our case. We detail this derivation in Appendix C.3.\nFinding surface points on density fields. Finding surface points for densities is substantially harder than for SDFs. For an SDF f, surface points can be obtained by flowing randomly initialized points along the gradient \u2207 f to the boundary at f = 0. However, density fields g do not satisfy the eikonal equation |\u2207g| \u2260 1, causing gradient flows to easily get stuck in local minima. To overcome this challenge, we employ a robust algorithm that relies on dense sampling and binary search, as detailed in Algorithm 2."}, {"title": "3.3 Formalizing geometric constraints", "content": "The compliance and volume losses are computed by the FEM solver on a discrete grid. In addition, we use geometric constraints similar to GINNs. This leverages the continuous field representation"}, {"title": "4 Experiments", "content": "For all our GenTO experiments, we employ the WIRE architecture (Saragadam et al., 2023), which uses wavelets as activation functions. This imposes an inductive bias towards high-frequencies, while being more localized than, e.g., a sine activation (Sitzmann et al., 2020). We use a 2-dimensional modulation space $Z \\subset R^2$, where most runs use $Z = [0, 1]^2$.\nWe perform 3 different runs for GenTO. GenTO (single) optimizes a single shape and serves as a baseline for compliance. The diversity constraint is not included, as it is trivially 0. GenTO (equidistant) optimizes M = 9 shapes in a single iteration in all experiments. The M modulation vectors zj are taken from $Z \\subset R^2$ in an equidistant 3 \u00d7 3 grid at the beginning of training. GenTO (uniform) optimizes a set of shapes, with the number of shapes M being either 9 or 25, depending on the experiment. In every iteration, M modulation vectors are sampled uniformly from Z. By uniformly sampling from Z the network learns a set of shapes.\nFor training GenTO, the FEM solver mesh resolution (see 1) is kept constant throughout training. Further experimental details and hyperparameters can be found in Appendix A."}, {"title": "4.1 Problem definitions", "content": "We apply our method to common linear elasticity problems in two and three dimensions, described below and depicted in Figure 3.\nMesserschmitt-B\u00f6lkow-Blohm beam (2D). The MBB beam is a common benchmark problem in TO and is depicted in Figure 3a. The problem describes a beam fixed on the lower right and left edges with a vertical force F applied at the center. As the problem is symmetric around x = L/2, we follow Papadopoulos et al. (2021) and only optimize the right half.\nThe concrete dimensions of the beam are H = 1, L = 6 and the force points downwards with F = 1.\nCantilever beam (2D). The cantilever beam is illustrated in Figure 3b. The problem describes a beam fixed on the left-hand side and two forces F1, F2 are applied on the right-hand side.\nThe dimensions are H = 1, L = 1.5, h = 0.1 and the forces F\u2081 = F2 = 0.5.\nJet engine bracket (3D). We apply GenTO to a challenging 3D task, namely the optimization of a jet engine bracket as defined by Kiis et al. (2013). The design region for this problem is given via a"}, {"title": "4.2 Baselines", "content": "TO for a single solution. The main baseline for TO for a single solution is a standard implementation of SIMP. We rely on FeniTop (Jia et al., 2024), a well-documented implementation of SIMP. Single-shape reference solutions for all problems are shown in Appendix A.3.\nFeniTop applies a smoothing filter to the densities before passing them to the FEM solver. This avoids checkerboard patterns caused by the numerical error of FEM (Sigmund & Petersson, 1998). As GenTO, FeniTop also uses contrast filters (more details on filters in Appendix A.1).\nTO for multiple solutions. The deflated barrier method (DB) (Papadopoulos et al., 2021) is the state-of-the-art method for finding multiple solutions to TO problems. In contrast to GenTO, DB is a sequential algorithm and cannot perform multiple solver steps in parallel. For linear elasticity, the deflated barrier method only provides an implementation for the cantilever and MBB beam. A comparison between GenTO and DB for the jet engine bracket is omitted, given DB's complex mathematical framework, slower performance (see Section 5), and hyperparameter sensitivity. Notice that the available implementation of DB refines the mesh during training, which increases the runtime per solver step during optimization."}, {"title": "4.3 Metrics", "content": "Unlike DB and TO, which generate single solutions, GenTO has the capability to produce a potentially infinite set of shapes. $\u03a9_z = {\u03a9_z | z \\in Z}$. During training, shapes are sampled according to a probability distribution p(Z). Hence, our metrics are based on the expected value over p(Z).\nQuality. For structural efficiency, we use the expected compliance E[C] := $E_{z\\sim p(z)}[C(\u03a9_z)]$, as well as minimum compliance min(C) := $min_{z \\in Z} C(\u03a9_z)$ and maximum compliance max(C) := $max_{z \\in Z} C(\u03a9_z)$. To verify that the volume constraint is satisfied, we use the expected volume E[V] := $E_{z\\sim p(Z)}[V(\u03a9_z)]$ over obtained shapes. We use the FenicsX numerical solver (Baratta et al., 2023) to compute compliance and volume values. For each problem, we use a high-resolution mesh to compute the final compliance metrics for all methods. This minimizes the discretization error in the compliance computation and prevents numerical artifacts due to the mesh dependency of the solver.\nDiversity. A diversity measure shall capture several natural properties, accounting, e.g., for the number of elements and the dissimilarity between elements of a set. Leinster (2021) states 6 natural properties and rigorously proves that only the Hill numbers $D_q(S)$, q \u2208 R over a set S fulfill these properties all at once. If q = 2, the Hill number is interpretable, as it corresponds to the expected dissimilarity if 2 elements of a set are sampled with replacement. Hence, we choose $D_2(\u03a9_Z)$ to measure the diversity of shapes $\u03a9_Z$, we follow Leinster (2021) and report the expected Wasserstein-1 distance between the shapes in the set:\n$E[W_1] := E_{z_i,z_j\\sim p(Z)} [W_1(\u03a9_{z_i}, \u03a9_{z_j})]$ (8)\nwhere $W_1(\u03a9_{z_i}, \u03a9_{z_j})$ is the sliced-1 Wasserstein distance (Flamary et al., 2021), a computationally cheaper variant of the Wasserstein distance. Importantly, zi, zj are sampled with replacement as rigorously derived by Leinster (2021).\nComputational cost. To compare the computational cost of different methods, we report the number of solver steps, since for large meshes (e.g., more than 10K elements) this becomes the main computational bottleneck. In addition, we use the number of iterations of the network (i.e. optimizer steps), as these take into account the parallelization of GenTO. We also report wall clock time, as it gives a sense of the practicality of a method."}, {"title": "5 Results", "content": null}, {"title": "5.1 Main results", "content": "The quantitative results of our experiments are summarized in Table 1. Qualitatively, we show different results in Figures 1, 4, and 5. To validate that GenTO can produce good single results on par with the baseline, comparisons of FeniTop and GenTO (single) are shown in the Appendix in Figures 6, 7 and 8.\nGenTO is more diverse. Across all our experiments, we observe that GenTO finds more diverse solutions than DB. Note, that the solutions also vary in their topology, i.e. have a different number of holes.\nGenTO is faster. Compared to DB, GenTO with equidistant sampling is more than an order of magnitude faster in terms of wall clock time. The main reason is the parallelism of GenTO, while DB is an inherently sequential algorithm. Note, that DB performs mesh refinement in certain stages, which increases the time per solver step. GenTO with equidistance sampling is approximately on par with DB in terms of total solver steps."}, {"title": "5.2 Ablations", "content": "We perform ablations to highlight important design decisions of GenTO. The main findings are described in the following and additionally visualized in Appendix A.5.\nSensitivity to penalty p. For SIMP, the density is penalized with an exponent p (see Equation 2). Our experiments show that GenTO is sensitive to the choice of p, as it sharpens the loss landscape. This becomes more apparent when looking at the derivative of the stiffness of a mesh element $K_i (p_i) = p p^{p-1} K$. E.g., for p = 3 the gradient is quadratically scaled by the current density value. This implies that the higher p, the harder it is to escape local minima. We do not observe a large performance impact in the 2D experiments. However, in 3D, we observe that p = 3 instead of p = 1.5 leads to convergence to an undesired local minimum, illustrated in Figure 15a.\nAnnealing is necessary for good convergence. We find that GenTO requires annealing to achieve good convergence. Figure 15b demonstrates a failed run without \u1e9e scheduling the Heaviside function (Equation 3).\nDifferent frequency bias for x and z. The impor-"}, {"title": "6 Conclusion", "content": "This paper presents Generative Topology Optimization (GenTO), a novel approach that addresses a limitation in traditional TO methods. By leveraging neural networks to parameterize shapes and enforcing solution diversity as an explicit constraint, GenTO enables the exploration of multiple near-optimal designs. This is crucial for industrial applications, where manufacturing or aesthetic constraints often necessitate the selection of alternative geometries. Our empirical results demonstrate that GenTO is both effective and scalable, generating more diverse solutions than prior methods while adhering to mechanical requirements. Our main contributions include the introduction of GenTO as the first data-free, solver-in-the-loop neural network training method and the empirical demonstration"}, {"title": "Limitations and future work", "content": "While GenTO shows notable progress, several areas warrant further investigation to fully realize its potential. Qualitatively, we observe, e.g. in Figure 4b, that shapes produced by GenTO can contain floaters (small disconnected components) and have surface undulations. This warrants further investigation and can be preliminarily corrected by post-processing steps (Subedi et al., 2020).\nMore research is needed to explore different dissimilarity metrics for the diversity loss, as this could further enhance the variety of generated designs. Improving sample efficiency is also crucial for practical applications, as the current method may require significant computational resources. Promising directions include using second-order optimizers and using the mutual information of concurrent solver outputs at early training stages. Lastly, future work could train GenTO on coarse designs and subsequently refine them with classical TO methods, combining the strengths of both approaches to achieve even better results."}, {"title": "A Implementation and experimental details", "content": null}, {"title": "A.1 Smoothing and contrast filtering", "content": "Filtering is a general concept in topology optimization, which aims to reduce artifacts and improve convergence. Helmholtz PDE filtering is a smoothing filter similar to Gaussian blurring, but easier to integrate with existing finite element solvers. By solving a Helmholtz PDE the material density p is smoothened to prevents checkerboard patterns, which is typical for TO (Lazarov & Sigmund, 2011). Heaviside filtering is a type of contrast filter, which enhances the distinction between solid and void regions. The Heaviside filter function equals the sigmoid function up to scaling of the input. We therefore denote ph as the Heaviside-filtered p as defined in Equation 3 Whereas \u1e9e is a parameter controlling the sharpness of the transform, similar to the inverse temperature of a classical softmax (higher beta means a closer approximation to a true Heaviside step function). Note that in contrast to the Helmholtz PDE filter, the heaviside filter is not volume preserving. Therefore the volume constraint has to be applied to the modified output."}, {"title": "A.2 Model hyperparameters", "content": "We report additional information on the experiments and their implementation. We run all experiments on a single GPU (NVIDIA Titan V12), but potentially across multiple CPU cores (up to 32). For single-shape training, the maximum GPU memory requirements are less than 2GB for all experiments.\nFor the model to effectively learn high-frequency features, it is important to use a neural network represenation with a high frequency bias (Sitzmann et al., 2020; Teney et al., 2024). Hence, all models were trained using the real, 1D variant of the WIRE architecture (Saragadam et al., 2023). WIRE allows to adjust the frequency bias by setting the hyperparameters w\u2642 and 80. For this architecture, each layer consists of 2 Multi-layer perceptron (MLPs), one has a periodic activation function cos(x), the other with a gaussian $e^{(\u03b4_0x)^2}$. The post-activations are then multiplied element-wise."}, {"title": "C Diversity", "content": null}, {"title": "C.1 Diversity on the volume", "content": "As noted by Berzins et al. (2024), one can define a dissimilarity loss as the L\u00ba function distance\nd(\u03a9, \u03a9j) = $\\int_{X}$ (fi(x) - fj(x))Pdx (C1)"}, {"title": "C.2 L\u00b9 distance on neural fields resembles Union minus Intersection", "content": "To derive that the distance metric\nd(\u03a9, \u03a9) = $\\int_{X} \\sqrt{(f_i(x)-f_j(x))^2dx}$ (C3)\nfor p = 1 and fi, fj \u2208 {0,1} corresponds to the union minus the intersection of the shapes, we consider the following cases:"}, {"title": "C.3 Diversity on the boundary via differentiable chamfer discrepancy", "content": "We continue from Equation 7:\n$\\frac{\\partial L}{\\partial \\theta}$ = $\\frac{\\partial L}{\\partial x}$ $\\frac{\\partial x}{\\partial \\rho}$ $\\frac{\\partial \\rho}{\\partial y}$ $\\frac{\\partial y}{\\partial \\theta}$ = $\\frac{\\partial L}{\\partial x}$ $\\frac{\\nabla_x f_\\theta}{f_\\theta^2}$ $\\frac{\\partial y}{\\partial \\theta}$ (C5)\nThe center term $\\frac{\\partial y}{\\partial \\theta}$ and the last term can be obtained via automatic differentiation. For the first term, we derive where L is the one-sided chamfer discrepancy CD(\u0398\u03a91, \u0398\u03a92).\n$\\frac{\\partial }{\\partial \\theta}$ $\\frac{1}{|\u0398\u03a9_1|}$ $\\sum_{x \\in \u0398\u03a9_1}$ $min_{Y \\in \u0398\u03a9_2}$ $||x - Y||_2$ = $\\frac{1}{|\u0398\u03a9_1|}$ $\\sum_{x \\in \u0398\u03a9_1}$ $\\frac{x-y}{min ||x - y||}$ (C6)\n(C7)\nThis completes the terms in the chain rule."}, {"title": "C.4 Finding surface points", "content": "We describe the algorithm to locate boundary points of implicit shapes defined by a neural density field Algorithm 2.\nOn a high level, the algorithm first identifies points inside the boundary where neighboring points lie on opposite sides of the level set. Subsequently, it employs binary search to refine these boundary points. The process"}]}