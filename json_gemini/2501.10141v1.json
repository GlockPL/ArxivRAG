{"title": "Enhancing UAV Path Planning Efficiency Through Accelerated Learning", "authors": ["Joseanne Viana", "Boris Galkin", "Lester Ho", "Holger Claussen"], "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields such as surveillance, reconnaissance, and telecommunications. This study aims to develop a learning algorithm for the path planning of UAV wireless communication relays, which can reduce storage requirements and accelerate Deep Reinforcement Learning (DRL) convergence. Assuming the system possesses terrain maps of the area and can estimate user locations using localization algorithms or direct GPS reporting, it can input these parameters into the learning algorithms to achieve optimized path planning performance. However, higher resolution terrain maps are necessary to extract topological information such as terrain height, object distances, and signal blockages. This requirement increases memory and storage demands on UAVs while also lengthening convergence times in DRL algorithms. Similarly, defining the telecommunication coverage map in UAV wireless communication relays using these terrain maps and user position estimations demands higher memory and storage utilization for the learning path planning algorithms. Our approach reduces path planning training time by applying a dimensionality reduction technique based on Principal Component Analysis (PCA), sample combination, Prioritized Experience Replay (PER), and the combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss calculations in the coverage map estimates, thereby enhancing a Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. The proposed solution reduces the convergence episodes needed for basic training by approximately four times compared to the traditional TD3.", "sections": [{"title": "I. INTRODUCTION", "content": "Unmanned Aerial Vehicle (UAV) path planning is a challenging task that involves optimizing general UAV commands and controllers according to external factors (obstacles, telecommunication metrics, energy efficiency, security) [1], [2], [3], to achieve set goals in complex environments.\nDeep Reinforcement Learning (DRL) is increasingly being used to solve this problem, leading to significant achievements in various applications such as delivery, security, and monitoring [4], [5]. Introducing telecommunication parameters into these problems is essential for addressing coverage and network holes, but it also adds another layer of complexity to path planning. For example, calculating the wireless coverage area accurately is still a challenging task that requires precise measurements as well as extensive amounts of data (i.e., region maps, user locations, Base station locations and others), and may vary across countries due to differences in city infrastructure and telecommunication parameters [6]. Additionally, multiple UAVs can be used as relays as in [7] where inter-UAV coordination is an additional complexity. Several authors have proposed solutions which use DRL algorithms, and which include telecommunication parameters as part of path planning. Notably the evolution of such path planning algorithms is connected to the evolution of the DRL algorithms. In [8] path planning algorithms using Deep Q Learning were proposed which integrate beam tracking and path planning to achieve reasonable connectivity. In [9] and [10], policy gradient algorithms including Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) were utilized to solve this problem. Such solutions frequently include offline training and the use of small state-spaces based on sensor inputs. These approaches may not be suitable for addressing large state-space problems, such as those related to convergence timing. Even though the authors have proposed ways to overcome long convergence time in these papers, it is well known that the convergence time increases with the size of the action and state-spaces, as the agents explore the state-space before achieving convergence.\nThe TD3 algorithm is a significant advancement in the field of DRL. By extending the DDPG algorithm, TD3 introduces several key components aimed at enhancing training stability and improving performance. Its twin Q-value networks, target networks, and exploration strategy contribute to more efficient learning in complex environments. Through the integration of these components, TD3 addresses common challenges such as sample inefficiency and overestimation bias, leading to more robust and reliable training outcomes. TD3 has recently been applied to UAV path planning problems as in [9], [10]. Despite its advantages, TD3 is not without limitations. One of the primary challenges associated with TD3 is its sensitivity to environmental dynamics. In tasks with non-stationary or highly stochastic environments, TD3 may struggle to adapt effectively, resulting in sub-optimal performance. Additionally, the computational complexity of TD3, stemming from its twin network architecture and multiple critics, can pose challenges in terms of resource requirements and training time. Finally, TD3 faces instability when handling large state-spaces.\nIn UAV wireless communications, the authors in [11] propose new algorithms to overcome such limitations by using optimal subspace and graph symmetry to decrease complexity. The authors in [12] suggest aggregating state-action pairs and combining different Markov chains formed in the wireless multiple access problem. A common insight from both papers is that large state-spaces require extensive storage, memory, and training time to produce an efficient model. A well-known technique for reducing state-spaces in Machine Learning (ML) is Principal Component Analysis (PCA), which can enhance ML performance by reducing computational costs and focusing on the most informative aspects of the data [13].\nThe objective of this paper is to optimize the size of the state-space to expedite convergence in UAV relay path planning algorithms. Specifically, we aim to improve convergence speed in path planning learning problems by applying four combined techniques: 1) Reducing the state-space size using PCA; 2) Adding a number of previous versions of new reduced state-spaces in the samples; 3) Modifying conventional TD3 by adding prioritized experience replay (PER); 4) Using the combination of Mean Squared Error (MSE) and the Mean Absolute Error (MAE) in the critic losses calculations. Through these steps, we demonstrate that faster convergence can be achieved, leading to more efficient and effective learning in UAV relay scenarios. To the best of our knowledge, we are the first to apply these techniques to a UAV relay path planning problem."}, {"title": "II. SYSTEM MODEL", "content": "We consider a UAV assisted wireless communication system where UAVs act as radio relays to assist the communication of terrestrial users in a rural environment, who are unable to achieve the minimum quality of service from a ground base station (BS) directly. The relay UAV is represented as U and the set of users is represented as J = {1,2,..., J}. Each user gets associated with a UAV and the UAVs then connect to the ground BS via the wireless backhaul link as presented in Fig 1. The BS-UAV and the UAV-user links are operating on orthogonal frequencies, so co-channel interference between UAV relays does not occur.\nWe assume it is possible for the UAV relays to obtain terrain maps such as in [14]. The system knows the BS positions based on wireless connection establishment, the users locations are determined using localisation techniques, and overall relay UAV wireless coverage is determined based on measured pathloss and shadowing effects, which are used to calculate the channel propagation models. Fig. 2 provides an illustration of our system and how the UAV operates. The UAV collects the terrain topology and the ground devices (i.e., BSj, and users) reference distances to the UAV,and estimates the air-to-ground wireless coverage. The dots represent the users. The pink square indicates the base station location, and the triangle represents the relay UAV providing coverage to the users. The line connecting the UAV and the users signifies the wireless connection. In this scenario, all users are connected to the base station via the UAV relay."}, {"title": "A. Channel Model", "content": "The channel gain between UAV i and user j is calculated as:\n$h_{i,j}^{bs} = PL_{i,j}^{bs} + KED_{i,j} + F_{j}$, (1)\n$h_{i,j}^{uav} = PL_{i,j}^{uav} + KED_{i,j} + F_{j}$, (2)\nwhere $PL_{i,j}$ is the distance-dependent free-space path loss, $KED_{i,j}$ is the loss due to terrain blockage which we modelled as knife-edge diffraction (KED) loss [15], and $F_{j}$ is the loss due to vegetation absorption at the location of user j.\n$PL_{i,j}$ is calculated using a free-space path loss calculation, with a path-loss exponent of 2.13 [16]:\n$PL_{i,j}^{bs} = -(20.0log_{10}(d_{i,j}) +20.0 log_{10}(f_{c}) -147.55)$. (3)\n$PL_{i,j}^{uav} = -(21.3 log_{10}(d_{i,j})+21.3 log_{10}(f_{c}) -157.2)$. (4)\nwhere $d_{i,j}$ is the distance in meters between UAV i and user j, and $f_{c}$ is the carrier frequency in Hz.\nKEDij is used to capture the transition from line-of-sight (LOS) to non-line-of-sight (NLOS) conditions when blockage due to terrain occurs. The expressions are provided in [15], and are omitted here due to page constraints.\n$F_{j}$ is obtained using geographical forest cover datasets [17], where $F_{j}$ is set to 5 dB if the user j is located in a forested location, and set to zero if the user is in an open area. Given the UAV relays have a transmit power $P_{uav-relay}$ at a specific altitude z, the received power at user j which is being covered by UAV i is given as $P_{user_j} = P_{uav-relay} + h_{i,j}^{uav}$ and the received power at the uav from bs is $P_{uav-relay} = P_{bs} + h_{i,j}^{bs}$. Given that UAV relays have a transmit power $P_{uav-relay}$ at a specific altitude z, the received power at user j, covered by UAV i, is given by $P_{user_j} = P_{uav-relay} + h_{i,j}^{uav}$, where $h_{i, j}^{uav}$ represents the received power at user j from UAV i. Similarly, the received power at the UAV from the base station (BS) is expressed as $P_{uav-relay_n} = P_{bs} + h_{i,j}^{bs}$, where $h_{i,j}^{bs}$ denotes the received power at the UAV from the BS."}, {"title": "B. Coverage Estimation", "content": "To estimate the overall coverage when a UAV is at a given horizontal location (x, y), we integrate the UAV transmission power $P_{uav-relay}$ and the channel gains h from the UAV transmissions on the ground, as defined in Eq. (1), considering losses and blockages described by Eq. (5).\n$C_{xy} = \\int_{A}^{} (P_{uav-relay} + (h_{uav-relay_n})^2 ) [dB]$ (5)\nThe integral sums up the contributions of the transmitted power over the area A, accounting for terrain losses, and provides an overall estimate of the coverage area made available for users by the relay UAV."}, {"title": "C. TD3 algorithm for Path Planning", "content": "We are using the TD3 algorithm for UAV relay path planning. This algorithm applies online learning, allowing the agent to optimize UAV path planning by interacting with the environment and receiving rewards as feedback. The overall goal is to optimize the user coverage considering their received power $P_{user_n}$ based on the transmit power of the UAV relay $P_{uav-relay}$, and taking into account UAV relay locations, as well as user and base station positions.\n$P1 : max {P_{user_j}, P_{uav-relay} },$ (6a)\nSubject to: $X_{min} \\leq X_{uava} \\leq X_{max} \\forall x \\in R,$ (6b)\n$Y_{min} \\leq Y_{uavu} \\leq Y_{max} \\forall y \\in R,$ (6c)\n$Z_{min} \\leq Z_{uav} \\leq z_{max} \\forall z \\in R.$ (6d)\nTaking the above in consideration, we design the algorithm as in the following.\n1) State Representation: The state-space includes information such as the positions of base stations $C_{BS}$ = $(X_{BS1}, Y_{BS1}, Z_{BS1})$, UAVS $C_{UAV}$ = $(X_{UAV1},Y_{UAV1}, Z_{UAV1})$, and users $C_{user}$ = {$(X_{user1}, Y_{user1}, Z_{user1}), (X_{user2}, Y_{user2}, z_{user2}),...$}, as well as environmental factors like terrain information acquired and stored in the UAV and signal strength.\nThe terrain information is represented as a grid or matrix denoted as M. Each cell contains information about a specific region's topography, including details on tree locations and sea level.\nAdditionally, another matrix C contains the coverage power of the environment as calculated in equation (5). The matrix C has dimensions x \u00d7 y, where x represents the x Cartesian coordinates and y represents the y Cartesian coordinates. For instance, $C_{xy}$ signifies the coverage power at the x-coordinate and y-coordinate of matrix C.\nThe coverage map is represented as:\n$C = \\begin{bmatrix}\nC_{11} & C_{12} & \\cdots & C_{1y} \\\\\nC_{21} & C_{22} & \\cdots & C_{2y} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nC_{x1} & C_{x2} & \\cdots & C_{xy}\n\\end{bmatrix}$ (7)\nIn this representation, each cell $C_{xy}$ contains information about the coverage in the corresponding area of the environment.\n2) Action Space: The action space pertains to the 3D path planning directions for all relay UAVs. Each point within this space denotes a new \u2206 movement direction applied to a given UAV's current position, expressed as follows:\nPoint: $(x_1, y_1,z)_1$\nPoint: $(x_2, y_2,z)_2$\n:\nPoint: $(x_n, y_n, z)_n$ (8)\n3) Reward Function: The reward function is designed to reflect the objective of maximizing the area covered by the transmitting UAV relay for users connected to the wireless communication system. To expedite convergence, reward metrics associated with the overall expected user power sensitivity and the distance between the UAV and users are adopted:\n\u2022 Reward 1:\n$r_1 = \\sum_{i=1}^{N} l_i$ (9)\nr1 defines the reward based on the feasibility of locations within the permissible operational limits where the UAV can navigate after taking a given action. r\u2081 increases when the UAV remains within the map boundaries. Here, $l_i$ refers to the coverage region indicator given by:\n$l_i =\\begin{cases}\n1 & \\text{if } X_{min} \\leq X_{i} \\leq X_{max},\\\\\n& Y_{min} \\leq Y_{i} \\leq Y_{max}, \\\\\n& Z_{min} \\leq Z_{i} \\leq Z_{max} \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (10)\nwhere $l_i$ determines whether the new $\u2206(x_i, y_i, z_i)$ from the action space lies within predefined coverage ranges for the UAV relay.\n\u2022 Reward 2:\n$r_2 = \\frac{\\frac{d_{uavu, final}}{N}}{\\frac{d_{uavinit}}{(\\sum_{j=1}^{N} || users,j \u2013 d_{uav final} || )}}$ (11)\nr2 involves normalizing the UAV and user distances, providing a ratio indicator of their proximity. A positive ratio indicates the UAV moved closer to the users, resulting in a positive reward, while a negative ratio indicates the UAV moved farther away.\n\u2022 Reward 3:\n$r_3 =  \\frac{min(P_{susers})}{(\\sum_{i=1}^{N} P_{susers_i})}$ (12)\nr3 evaluates the overall available power to users relative to the minimum expected received power value $min(P_{susers})$.\n\u2022 Total Reward:\n$r_t = c_1r_1 + c_2r_2 + c_3r_3$ (13)\nThe overall reward rt for the algorithm is a weighted sum of individual rewards, with $c_1$, $c_2$, and $c_3$ denoting the respective weights."}, {"title": "III. THE DIMENSIONALITY REDUCTION ALGORITHM FOR STATE SPACE", "content": "The high dimensionality of the state-space associated with the coverage map C, the base station, and user locations poses challenges for DRL algorithms due to the large state-space, which leads to increased computational complexity and slower convergence. To address this issue, we explore dimensionality reduction algorithms to compress the state representation while preserving relevant information. The approach is to use PCA, which identifies the principal components in the data and projects the original high-dimensional space onto a lower-dimensional subspace. By retaining the most significant features while discarding redundant information, PCA can effectively reduce the dimensionality of large state-spaces such as coverage maps. The dimensionality reduction occurs because each variance in the data provided in the maps can be associated with its covariance matrix, eigenvalues, and eigenvectors as in:\n$M = \\frac{1}{n-1} C^T C$ (14)\n$E v_i = \\lambda v_i$ (15)\nwhere M is the covariance matrix of the coverage map, E is the result of the eigenvalue decomposition, $V_i$ are the eigenvalues, and $v_i$ are the eigenvectors respectively. After sorting the eigenvectors based on variance from lowest to highest, the algorithm uses them as features to estimate the coverage map.\nIn our work, we introduce three modifications to TD3, which we refer to as Enhanced-TD3 (E-TD3). The process begins by computing the coverage map averages for each batch size and saving the overall PCA coefficients to achieve the target variance. The results from this phase correspond to an equivalent coverage map with dimension reduction. We then concatenate three additional PCA samples, referred to as \"new coverage map sample,\" into the input and proceed with training.\nIn E-TD3, we apply prioritization sampling, combining Mean Squared Error (MSE) and Mean Absolute Error (MAE) in the critic loss calculations to reduce overestimation. We determine the new coordinates for the UAV relays based on the PCA components using convolutional, dense, and LeakyReLU layerslu with a negative slope of 0.01, layers as shown in Table I."}, {"title": "IV. RESULTS", "content": "This section presents the results and insights gained from our UAV experiment using TD3, TD3+PCA, and E-TD3, along with details of the data used to train all the algorithms. Unless explicitly stated, the general network and DRL parameters employed in the experiment are described in Tables II and III, respectively."}, {"title": "A. Convergence Rate", "content": "Figure 4 compares the learning curves of TD3, TD3+PCA and E-TD3 for a fixed users scenario. We randomly simulate the system 100 times, train across 500 episodes in each simulation, and record the average rewards across all steps for each episode.\nAs shown in Fig. 4, all the comparative algorithms converge quickly. E-TD3 achieves fast convergence after approximately 120 episodes, TD3+PCA achieves similar rewards after 300 episodes and TD3 converges after 450 episodes. The rapid convergence can be attributed to the dimensionality reduction, timed compression to different samples, and decayed noise used in E-TD3. First, decreasing the state-space's dimensionality using the pca_scores and combining previous timed samples of it with the average mean in the batch_size effectively accelerates the convergence rate. We chose three previous samples to stay within our overall coverage map, as dimensionality reduction provides an encoded map with a size of 22% of the general map. Second, with dynamic e decay in exploration, data utilization efficiency is guaranteed. At the early stage of the training process, more explorations are attempted. With more training episodes, the proportion of exploration noise decreases, and UAVs make decisions mainly based on the trained policy with exploitation. As a result, E-TD3 obtains more rewards than its comparative counterparts. In this case, the priority is given to learning the best paths, but the accuracy of learning the patterns in the coverage is also relevant."}, {"title": "B. Comparison between coverage maps in the batch size", "content": "Fig. 5 presents the mean absolute error (MAE) between non-PCA and PCA coverage maps for one batch_size.A lower MAE signifies a higher similarity between the coverage maps, with a value close to 0 in db indicating a close match between the overall maps and higher values suggests complete dissimilarity.\nWith this comparison, we conclude that even though the we use only 22% of the original coverage map available encoded with pca and the average scores per batch_size, the coverage maps are similar to each other the average difference in each coverage map on the batch size is approximately 3db, such that we are able to achieve good UAV positioning while using lower dimensions."}, {"title": "V. CONCLUSIONS", "content": "In this paper we presented a dimensionality reduction learning algorithm for target coverage estimation in UAV wireless communication relays. The algorithm uses TD3 architecture and two proposed extensions. The results showed that the best-accomplished performance was achieved by our proposed E-TD3, where the coverage maps were reduced via PCA, and previous timed samples were compressed and used for training. This technique reveals that training with dimensionality reduction-added samples and changes in the loss estimations using harmonic averages is more effective for fast convergence and learning. Furthermore, we show that although PCA-encoded data uses less information for training, the recovered coverage maps are statistically similar to the overall coverage maps, which is fundamental in UAV relay scenarios."}, {"title": "VI. FUTURE WORK", "content": "In future research, we plan to extend our reinforcement learning implementation in UAV scenarios. Key areas of focus will include adapting the algorithm for multi-UAV coordination to enhance network coverage and reliability. We also aim to incorporate dynamic environment modeling to handle real-time changes in obstacles or user locations. Finally, we intend to validate our approach through real-world experiments. These advancements will contribute to more robust and efficient UAV-based wireless communication systems."}]}