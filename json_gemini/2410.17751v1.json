{"title": "VISAGE: Video Synthesis using Action Graphs for Surgery", "authors": ["Yousef Yeganeh", "Rachmadio Lazuardi", "Amir Shamseddin", "Emine Dari", "Yash Thirani", "Nassir Navab", "Azade Farshad"], "abstract": "Surgical data science (SDS) is a field that analyzes patient data before, during, and after surgery to improve surgical outcomes and skills. However, surgical data is scarce, heterogeneous, and complex, which limits the applicability of existing machine learning methods. In this work, we introduce the novel task of future video generation in laparoscopic surgery. This task can augment and enrich the existing surgical data and enable various applications, such as simulation, analysis, and robot-aided surgery. Ultimately, it involves not only understanding the current state of the operation but also accurately predicting the dynamic and often unpredictable nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis using Action Graphs for Surgery), leverages the power of action scene graphs to capture the sequential nature of laparoscopic procedures and utilizes diffusion models to synthesize temporally coherent video sequences. VISAGE predicts the future frames given only a single initial frame, and the action graph triplets. By incorporating domain-specific knowledge through the action graph, VISAGE ensures the generated videos adhere to the expected visual and motion patterns observed in real laparoscopic procedures. The results of our experiments demonstrate high-fidelity video generation for laparoscopy procedures, which enables various applications in SDS.", "sections": [{"title": "1 Introduction", "content": "Surgical data science (SDS) is an emerging field that focuses on the quantitative analysis of pre-, intra-, and postoperative patient data [21,29]; it can help to decompose complex procedures, train surgical novices, assess outcomes of actions, and create predictive models of surgical outcomes [20,25,14,18]. In recent years, there have been multiple works on surgical action recognition [27,28,33]. However, SDS faces several challenges, such as the scarcity and heterogeneity of surgical data, the variability and complexity of surgical processes, and the ethical and legal issues of data collection and sharing. To overcome these challenges, we introduce the novel task of surgical video generation, which aims to synthesize realistic and diverse videos of surgical procedures that can be used for various applications, such as simulation, analysis, and robot-aided surgery. Surgical video generation can provide a valuable solution to augment and enrich the real data, and to enable the exploration and evaluation of different surgical scenarios and techniques. Video generation [30] is a challenging task that aims to synthesize realistic and coherent video sequences from a given input, such as a text [15], an image [26], flow maps [38], or a video [34]. Video generation has been extensively studied in the computer vision and machine learning literature, and various methods have been proposed, such as generative adversarial networks (GANs) [24] and autoregressive models [37]. Bar et al. [3] propose conditioning video synthesis models on action graphs [12], while WALDO [24] introduces a framework for conditioning the model on segmentation [42,41] and flow maps [11]. However, most of these methods suffer from limitations, such as mode collapse, blurriness, artifacts, and temporal inconsistency [39]. Moreover, most of these methods are not suitable for laparoscopic video generation, as they do not incorporate domain-specific knowledge and constraints, such as anatomy, surgical actions, and camera motion.\nIn this work, we introduce the novel task of future video generation in laparoscopic surgery, which aims to generate realistic and diverse videos of laparoscopic procedures conditioned on a single frame and an action triplet, defining the action to be performed. For example, given a frame of a cholecystectomy (gallbladder removal) and a triplet of \"cut,\" \"cystic duct,\" and \"clip,\" the task is to generate a video of the surgeon cutting the cystic duct and applying a clip. This task is challenging, as it requires modeling the complex dynamics and interactions of the surgical scene, as well as generating high-fidelity and temporally coherent video frames.\nTo address this task, we propose VISAGE (VIdeo Synthesis using Action Graphs for Surgery), which leverages the power of action scene graphs and diffusion models. An action scene graph is a structured representation of the organs, the surgical tools, and their relations, which capture the sequential nature of the"}, {"title": "2 Method", "content": "In this section, we describe the video generation pipeline and the conditioning of the diffusion model.\nDefinitions We have a dataset Pdata of videos and their corresponding action triplets (s, p, o) \u2208 G, such that x, G ~ Pdata, where x \u2208 $R^{K\u00d73\u00d7H\u00d7W}$ is a sequence of K frames in RGB format, with height and width Hand W, and G is the set of sequential actions performed in the video defined by the (s,p,o) which are the subject, object, and predicate, respectively. The goal of the model is to generate the subsequent video frames x1,...,xk given G and 10. The diffusion model utilized for video generation is denoted by g and parameterized by 0. The diffusion model comprises an image encoder and an action graph encoder denoted by E1, EG, and a decoder denoted by D."}, {"title": "2.1 Video Latent Diffusion Model", "content": "Latent Diffusion Models (LDMs) [32] are generative models based diffusion process. The process comprises a forward (noising) process and a reverse (denoising) process. In the forward process, a data sample xo is encoded into a latent representation zo using an encoder E. Then Gaussian noise is gradually added over a series of steps, resulting in a sequence of noisier latent variables until a pure noise latent variable zy is reached:\n$Zt = \\sqrt{1 - B_t}zt_{-1} + \\sqrt{ B_t}et, Et ~N(0, I)$"}, {"title": "2.2 Action Conditioned Video Generation", "content": "An action scene graph consists of nodes and edges, where each node represents an organ or a surgical tool, and each edge represents the performed action. To condition the diffusion model on the action scene graph, we use the graph encoder E\u00e7 to encode the action scene graph into a latent vector h, which is then fused with the encoded initial frame from Er. The diffusion model is trained on a denoising objective of the form.\n$Ex,h,e,tWt ||20(Atx + \u03c3\u03c4\u03b5, G) \u2013 x||2$\n, where (x,G) are data-conditioning pairs, t ~ \u0418([0,1]), \u0454 ~ N(0,I), and \u03b1\u03c4, \u03c3\u03c4, Wt are functions of t that influence sample quality.\nConditioning The diffusion model is conditioned on the action graph using the extracted feature embeddings h from Eg. We propose and evaluate two types of feature encoding for the graph. The first variation encodes the action graph using a learnable embedding layer, where the embeddings are learned implicitly through optimizing the video generation process. The second variation employs CLIP [31] text embeddings for the organ, the tool, and the action. Additionally, we fine-tune the CLIP text encoder using the image generation objective.\nFeature Fusion We propose two variations of feature fusion between the graph and image embeddings. The first variation introduces a linear layer that combines the latent features through concatenation. The second variant employs a cross-attention layer for the future fusion of the action graph conditioning and the image features.\nSampling Process To generate a video conditioned on the frame xo and the triplet (s, p, o), we first encode the action scene graph into a latent vector h =\nEg(s, p, o). We then initialize the noise sample zy by adding Gaussian noise to the single frame and then apply the reverse process of the diffusion model to denoise zy into zo. At each timestep t, we use the fused latent vector z\u0142 to condition the network ge on the action scene graph and the initial frame."}, {"title": "2.3 Data Preprocessing", "content": "The videos in CholecT50 [28] dataset consist of an average of 1.7K frames, which is larger than the number of frames our baseline models can process. Therefore, we need to divide the videos into scenes, which are smaller clips of the videos in the dataset. To be able to generate temporally consistent videos, generative video models are better trained on datasets that consist of videos that are also temporally consistent [4]. However, raw videos are prone to containing motion inconsistencies, such as jumps between scenes, which we will call scene cuts, and still videos with very little motion as static scenes. When trained on static scenes, the model might learn to predict little to no motion, or when trained on videos with scene cuts, the model might learn to predict jumps between frames. Therefore, we process the dataset before training to mitigate possible issues. Due to SVD's [4] limitation for the maximum number of frames to process for each video, which is 7, we fix 7 as the frame count of each scene. We obtain 7-framed scenes after applying the methods mentioned in the following sections, which filter the videos to eliminate scene cuts and static scenes.\nScene Cut Detection To detect scene cuts, which are sudden changes in a sequence of images, we first use the PySceneDetect\u00b9 library. PySceneDetect analyzes videos by comparing the neighboring frames for changes in intensity, brightness, and HSV color space between frames and can also look for fast camera movements. The threshold for the sensitivity in detection can be defined, which we set to 0.27 as suggested. It returns a list of frame IDs where a cut is detected, which is our rough start to divide a video. After the first division by PyScene Detect, we further divide the detected scenes into 7-framed portions, where we add another rule regarding the triplet annotations. We force that all frames in the 7-framed scene at least have one common triplet. This allows us to use the triplet information also as a cut detection method since a frame with no common triplet with other frames in the scene would mean a jump in the depicted motion. We also eliminate the scenes that include frames with empty triplet annotation, such as an undefined triplet or undefined elements for the predicate, instrument, or target.\nStatic Scene Detection Static scenes in our videos occur in two ways: Scenes with little motion and scenes that are completely blacked out due to privacy. We detect the latter simply by looking at pixel intensities and eliminating scenes, including all-black frames. They can also be detected by the empty triplet check. The first type of static scene is harder to detect. As in [4], we tried to detect the static scenes by using optical flow to calculate a motion score; however, the scores did not represent the motion well enough to be used in detection. Therefore, the static scenes of the first type are detected by the empty triplets or triplet elements; for example, scenes where only the (target) organ is in the frame and no action is performed can be detected by checking the triplet annotations."}, {"title": "3 Experiments and Results", "content": "Dataset. For all the experiments, we use the CholecT50 Dataset [28], which is a laparoscopic video dataset of 50 videos for recognizing and localizing surgical action triplets. Each action triplet includes a predicate defining the action, an object (surgical instrument), and a target (organ), which are used as annotations of the individual frames in videos. A frame can be annotated by zero or up to 3 triplets. In addition, the surgical phase for each frame is annotated. There are a total of 100 triplets, 10 predicates, 6 instruments, 15 targets, and 7 phases.\nImplementation Details. All our models were trained for 10,000 iterations using Adam Optimizer and a learning rate of 1e-5. We applied an L2 loss between the latent space of the generated 7-frames and their corresponding ground truth to guide the training process effectively.\nEvaluation Metrics. We employ the common metrics in video generation for computer vision applications to assess the performance of our model and the baselines. Frechet Video Distance (FVD) [35] compares the probability distribution of generated videos with real videos. Peak Signal-Noise Ratio (PSNR) measures the fidelity of reconstruction. Learned Perceptual Image Patch Similarity (LPIPS) quantifies the perceived similarity between two videos. Structural Similarity Index Measure (SSIM) measures changes in structural information between original and generated videos.\nBaselines. We evaluated four distinct video generation architectures on the CholecT50 dataset [28], specifically WALDO [24], CoDi [34], Stable Video Diffusion (SVD) [4], and Latent Flow Diffusion Model (LFDM) [26]. The comparison among these models involved conducting inference procedures on each model, followed by a comprehensive assessment of the generated outputs through both qualitative and quantitative analyses. It is important to note that these models were originally trained on natural video datasets; thus, the application of these models to egocentric videos from laparoscopic cholecystectomy represents an out-of-distribution scenario relative to their training data. The selection of"}, {"title": "3.2 Results", "content": "Quantitative Results Table 1 shows the comparison of different SOTA architectures on the surgical video generation task. As it can be seen, LFDM [26] has the lowest FVD and, on the other hand, the highest SSIM among inference-only models. It also ranks high in PSNR and LPIPS. VISAGE achieves the best overall performance across all models except for SSIM. SSIM captures the structural similarity between the frames, while FVD compares the general distribution of the generated data compared to the real distribution. VISAGE improves the FVD by a large margin compared to the baseline models, demonstrating its ability to adhere to the real data distribution. It is noteworthy that, although more complex GAN-based models such as WALDO receive additional information such as the segmentation and the flow map, they fail in generating realistic samples and are prone to mode collapse.\nQualitative Results Figure 2 demonstrates the qualitative comparison of the SOTA models compared to VISAGE. CoDi fails to generate realistic results, and Waldo suffers from artifacts in the generated samples. LFDM, SVD, and VISAGE were able to generate results resembling the real data. However, VISAGE generates the fine details, such as the tool tip, which is a key component of the surgery, with higher accuracy."}, {"title": "4 Conclusion", "content": "In this paper, we have presented a novel task of future video generation in laparoscopic surgery, which can benefit various aspects of surgical data science, such as simulation, analysis, and robot-aided surgery. We have proposed VISAGE, a generative model that leverages action scene graphs and diffusion models to synthesize realistic and diverse videos of laparoscopic procedures conditioned on a single frame and an action triplet. We have evaluated our method on the CholecT50 dataset, and we have shown that it can generate high-quality and temporally coherent videos that adhere to the domain-specific constraints of laparoscopic surgery. We have also compared our method with several baselines and ablations, and we have demonstrated its superiority in terms of various metrics and qualitative evaluations. Our work opens up new possibilities for future research on surgical video generation, such as incorporating more complex action sequences, improving the diversity and controllability of the generated videos, and exploring the applications of our method in surgical education and training."}]}