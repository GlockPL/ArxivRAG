{"title": "Deep Reinforcement Learning for Multi-Objective Optimization: Enhancing Wind Turbine Energy Generation while Mitigating Noise Emissions", "authors": ["Mart\u00edn de Frutos", "Oscar A. Marino", "David Huergo", "Esteban Ferrer"], "abstract": "We develop a torque-pitch control framework using deep reinforcement learning for wind turbines to optimize the generation of wind turbine energy while minimizing operational noise. We employ a double deep Q-learning, coupled to a blade element momentum solver, to enable precise control over wind turbine parameters. In addition to the blade element momentum, we use the wind turbine acoustic model of Brooks Pope and Marcolini. Through training with simple winds, the agent learns optimal control policies that allow efficient control for complex turbulent winds. Our experiments demonstrate that the reinforcement learning is able to find optima at the Pareto front, when maximizing energy while minimizing noise. In addition, the adaptability of the reinforcement learning agent to changing turbulent wind conditions, underscores its efficacy for real-world applications. We validate the methodology using a SWT2.3-93 wind turbine with a rated power of 2.3 MW . We compare the reinforcement learning control to classic controls to show that they are comparable when not taking into account noise emissions. When including a maximum limit of 45 dB to the noise produced ( 100 meters downwind of the turbine), the extracted yearly energy decreases by $22 \\%$. The methodology is flexible and allows for easy tuning of the objectives and constraints through the reward definitions, resulting in a flexible multi-objective optimization framework for wind turbine control. Overall, our findings highlight the potential of RL-based control strategies to improve wind turbine efficiency while mitigating noise pollution, thus advancing sustainable energy generation technologies.", "sections": [{"title": "1. Introduction", "content": "In recent years, the aerodynamic design of wind turbines has undergone significant advances, reaching nearoptimal efficiency through substantial investments in aerodynamic optimization, as well as advancements in manufacturing techniques and materials. Consequently, the focus has shifted towards addressing the issue of noise generated by wind turbines, which is emerging as a competitive factor within the wind energy industry. Numerous studies have explored the correlation between wind turbine sound power levels and public-reported perceptions of annoyance (Wolsink et al., 1993; Pedersen et al., 2009). Accurate prediction of wind turbine noise under real operational and atmospheric conditions is crucial to design quieter turbines and complying with imposed noise regulations (Davy et al., 2018). This necessity underscores the importance of fast turnaround methods to incorporate noise calculations into design and optimization processes, as well as to assess noise in real time during operation. Such efforts not only optimize wind resource utilization, but also minimize the impact on the quality of life of nearby communities and wildlife.\n\nAerodynamic noise poses a significant limitation to further exploiting wind energy resources. This type of noise results from the turbulent flow interacting with the airframe, necessitating a detailed resolution of the flow for accurate far-field noise prediction. However, computational fluid dynamics (CFD) solvers, while capable of simulating the flow field, incur a high computational cost which escalates further when resolving the acoustic field. Consequently, numerical approaches to wind turbine noise prediction remain challenging.\n\nTherefore, most noise prediction models for wind turbines are based on aeroacoustic semi-empirical models rather than numerical simulations (Wagner et al., 1996). Despite these obstacles, wind turbines remain an essential component in the generation of clean and renewable energy. However, effective control strategies are imperative to optimize their performance under variable wind conditions.\n\nWind turbine control systems are designed to maximize energy generation while ensuring structural integrity and safe operation (Njiri and S\u00f6ffker, 2016; Novaes Menezes et al., 2018; Andersson et al., 2021). Given the dynamic nature of wind, adaptive control strategies are essential, with classic mechanisms including adjustments to yaw angle and rotational speed, as well as blade pitch angle modulation. Leveraging real-time wind measurements, turbine dynamics, and advanced control algorithms enables simultaneous adjustments to rotor speed, and pitch, enhancing energy generation, reducing fatigue loads, and extending turbine lifespan.\n\nThe emergence of reinforcement learning (RL) presents novel opportunities for wind turbine control by enabling data-driven adaptive decision-making (Le Clainche et al., 2023; Garnier et al., 2021). RL, a machine learning approach, involves an agent learning to make decisions in an environment to maximize cumulative rewards over time (Sutton and Barto, 1998). Applied to wind turbines, RL offers autonomous learning of control inputs to maximize power generation by capturing complex non-linear relationships between wind conditions, turbine states, and actions. RL-based control methods adapt in real-time to changing wind conditions, offering significant advantages in wind turbine operation.\n\nThis paper reviews recent advances in RL-based control strategies for wind turbines, focusing on pitch angle and rotor speed modulation. Previous studies have proposed RL algorithms with comprehensive reward definitions, showcasing their efficacy in optimizing wind turbine performance under varying wind conditions. For example Chen et al. (2020) proposed a RL pitch controller to maintain the nominal power using an adaptive dynamic programming algorithm, reducing the energy consumption of the pitch actuator. Xie et al. (2023) developed a data-driven model to perform a torque-pitch controller, modeling the dynamics and using RL to control the wind turbine. Sierra-Garcia et al. (2020) discussed different reward definitions for wind turbine controllers, while Puech and Read (2023) and Saenz-Aguirre et al. (2019) developed RL methods for yaw control avoiding control parameter tuning. Kushwaha et al. (2020) and Wei et al. (2016) employed Q-learning RL methods for maximum power point tracking (MPPT) control of the generator speed. Overall, these studies demonstrate the adaptability of RL systems to realistic wind conditions, thereby enhancing overall energy generation and efficiency of wind farms."}, {"title": "2. Methodology", "content": "In this section, we detail the methodology for integrating Deep Reinforcement Learning (DRL) with the dynamic control of a wind turbine. We begin by describing the model of the wind turbine, focusing on how both the power output and the noise levels are computed, and validate the methodology using field measurements for a SWT2.3-93 wind turbine with a rated power of 2.3 MW. Subsequently, we detail the setup of the DRL algorithm, which is designed to maximize power generation within specified noise constraints, demonstrating the application of advanced machine learning techniques to real-world energy optimization challenges.\n\n### 2.1. Wind turbine modeling using OpenFAST\n\nOne critical requirement for incorporating a wind turbine solver into the DRL control framework is the ability to perform rapid evaluations, as the DRL training process requires a large number of simulations. To meet this need, we have chosen to employ an efficient Blade Element Momentum Theory (BEMT) solver. Specifically, we use OpenFAST (National Renewable Energy Laboratory, 2024), a well-known open-source software for simulating wind turbine dynamics and acoustic predictions.\n\nBEMT, is known for its efficiency and offers a simple yet accurate method for estimating the aerodynamic forces and energy generation of wind turbines. Its ability to perform rapid function evaluations is crucial for training and validating the agent within a reasonable timeframe. In the realm of BEMT, the wind turbine blade is segmented into smaller sections along its span. The aerodynamic forces exerted on each section are computed based on the local wind conditions and the airfoil's geometry. These local flow conditions, defined for every section and time step, encompass the wind's speed and direction, along with the turbulence intensity. Polar curves for each airfoil section are used to compute the aerodynamic forces (lift, drag and moment coefficients). By integrating the forces along the span of the blades, we can derive the overall power and thrust generated by the wind turbine.\n\nAdditionally, OpenFAST includes an aeroacoustic module that enables the computation of noise levels generated by the wind turbine at specific observer locations. To determine the aerodynamic noise sources from wind turbine blades, various semi-empirical noise models are included (Zhu et al., 2005) and we select the Brooks Pope and Marcolini model. The sound pressure level (SPL) for each blade segment is calculated based on individual noise mechanisms. The cumulative effect of these mechanisms yields the noise source for each airfoil. Finally, the noise sources from all blade segments are combined as uncorrelated noise sources, contributing to the overall computation of the wind turbine's sound power level. The essential aspect of this process is the precise identification and modeling of the various noise mechanisms associated with each blade section. These mechanisms can be categorized into two groups: turbulent inflow noise and airfoil self-noise. OpenFAST implements the turbulent inflow model presented by Moriarty et al. (2004) and, among the airfoil self-noise models described by Brooks et al. (1989), we have specifically selected: turbulent boundary layer trailing edge noise and tip vortex formation noise.\n\n### 2.1.1. Validation of OpenFAST with a SWT2.3-93 wind turbine\n\nThe onshore wind turbine selected for the study is based on the SWT2.3-93, which performs a rated power of 2.3 MW . This turbine has undergone extensive in field experimental testing and complete details on its geometry and benchmark data are available in the open access repository Zenodo (through the work of the European project Christophe et al. (2022)). The airfoil polar curves are available in the airfoil catalog compiled by Bertagnolio et al. (2001). More details can be found in Matthew (2012).\n\nAll the open-source information enables us to create a SWT2.3-93 OpenFAST model. Additionally, the benchmark results from the Zenodo dataset can be utilized to validate the model. Figure 1 presents the validation of both the power output and the sound pressure level (SPL) of the wind turbine. Figure 1a compares the experimental power curve (from the Zenodo database) with that generated by the OpenFAST solver, showing good agreement. Meanwhile, Figure 1b shows the one-third octave SPL for frequencies ranging 10 Hz to 10 kHz , comparing the Zenodo dataset with results from OpenFAST. These comparisons cover three different operational conditions, detailed in table 1. The Zenodo acoustic results are computed for an observer positioned 100 meters downstream from the wind turbine, at ground level. We observe good agreement with the experimental data for the three operating conditions. We conclude that OpenFAST, with the acoustic model of Brooks Pope and Marcolini, provides accurate predictions of power generation and acoustics, and is therefore a valid tool to perform multi-objective optimization.\n\n![img-0.jpeg](img-0.jpeg)\n\n(a) Power curve for the SWT2.3-93 wind turbine. The manufacturer curve is obtain from the Zenodo open access repository.\n\n![img-1.jpeg](img-1.jpeg)\n\n(b) One-third octave SPL diagram for three different operational points. The measurements of the SPL spectrum are obtained from the Zenodo open access repository.\n\nFigure 1: Comparison of Zenodo dataset benchmarks and OpenFAST modeling of the SWT2.3-93 wind turbine.\n\n| Operation point | $\boldsymbol{U}_{\\infty}(\\mathbf{m} / \\mathbf{s})$ | $\boldsymbol{\\Omega}(\\mathbf{r p m})$ | $\boldsymbol{\theta}$ (degrees) |\n| :--: | :--: | :--: | :--: |\n| OP1 | 6 | 13 | 3 |\n| OP2 | 8 | 14 | -2 |\n| OP3 | 9.5 | 17 | 5 |\n\nTable 1: Operational conditions studied on the Zenodo aeroacoustic dataset. The operation point is defined by the wind speed $U_{\\infty}$, the rotational speed $\\Omega$ and the blade pitch angle $\theta$.\n\n### 2.1.2. Sensitivity to control parameters\n\nThe selected parameters to control the wind turbine power and noise include the rotational speed $\\Omega$ and the blade pitch angle $\theta$. Before discussing the RL setup, it is crucial to illustrate the sensitivity of these parameters for the two performance metrics: the power coefficient and the overall sound pressure level.\n\nIn Figure 2 the sensitivity analyses for both rotational speed and blade pitch angle are displayed for a single incoming wind speed $\\left(U_{\\infty}\right)$. The values for one-third octave SPL, power coefficient and overall sound pressure level are shown for different operational conditions. Figure 2a depicts the influence of $\\Omega$, increasing the rotational speeds makes both the sound pressure level and the power coefficient rise, highlighting the tradeoff between maximizing power and minimizing noise. The entire SPL spectrum increases uniformly when increasing $\\Omega$, due to a higher relative velocity in the blades and leading to a rise in SPL regardless of the noise mechanism. A similar analysis is presented in Figure 2b for the blade pitch angle. Although the general conclusion about the opposition between power and noise remains valid, the SPL spectrum behaves differently across frequencies. The pitch angle mainly affects trailing edge noise leading to changes at relatively low frequencies ranging from 10 Hz to 1 kHz .\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 2: Sensitivity analyses for control parameters in relation to OSPL, power coefficient $\\left(C_{p}\right)$, and one-third octave SPL (dB A) spectra."}, {"title": "2.2. Design of a reinforcement learning control", "content": "Reinforcement Learning is a branch of machine learning that focuses on how agents should take actions in an environment to maximize cumulative reward. Unlike supervised learning, where the model learns from a labeled dataset, RL is driven by agent-environment interactions. The agent takes actions based on the current state of the environment and receives feedback in the form of rewards. The state represents the situation of the environment at a given time, while the actions are the possible moves the agent can make. The reward is the feedback indicating the immediate benefit or cost of an action, guiding the agent toward better actions over time. In particular, in this work we use Q-learning RL, which is detailed in the next section.\n\n### 2.2.1. Reinforcement Learning for Multi Objective Control\n\nQ-learning is a widely recognized reinforcement learning algorithm (Watkins and Dayan, 1992). It is categorized under model-free RL algorithms, implying that it operates without the necessity for prior knowledge or explicit models that represent the dynamics of the system. The fundamental component of Q-learning is the Q-value, which quantifies the anticipated cumulative reward for executing a specific action in a given state. The Q-value is updated iteratively via the Bellman equation, which formulates the optimal action-value function in terms of the maximum expected future reward. During the learning process, the wind turbine interacts with the environment, transitions between states, and takes actions according to its current policy. The Q-learning algorithm employs an $\\epsilon$-greedy exploration-exploitation trade-off to strike a balance between exploring new actions ( $\\epsilon$ times) and exploiting current knowledge ( $1-\\epsilon$ times) to maximize cumulative rewards. In RL the cumulative reward is computed taking into account that a reward received immediately is worth more than a reward received in the future, specifically, each time step the reward is discounted by $\\gamma$, the discount rate.\n\nInitially, the Q-values are arbitrarily initialized. As the wind turbine explores the environment and receives feedback in the form of rewards, the Q -values are updated using the temporal difference error. The temporal difference error represents the discrepancy between the observed reward and the predicted reward based on the Q-values. Through repeated iterations, the Q-learning algorithm gradually converges to an optimal policy. In this state, the wind turbine learns the best actions to take in different states, thereby maximizing power generation while minimizing noise. In our case, this agent-environment interactions for the wind turbine control are illustrated on Figure 3.\n\n![img-3.jpeg](img-3.jpeg)\n\nFigure 3: Flow diagram of the reinforcement learning control methodology.\n\nDeep Q-Network (DQN) is a variant of Q-learning that employs a deep neural network to estimate Q-values (Mnih et al., 2013). It replaces the traditional lookup table with a neural network, enabling generalizations across states to handle large state spaces efficiently. In this study, a Double Deep Q-Learning (DDQN) is employed. DDQN is an extension of DQN that uses two neural networks: the primary network and the target network. The primary network select the action and the target network evaluates its Q-value. This way of decoupling the action selection and evaluation addresses the overestimation of Q-values, often observed in DQN algorithms due to the maximization bias, (Van Hasselt et al., 2016). The weights of the primary network are obtained by minimizing the following loss function:\n\n$$\\mathcal{L}(\\phi)=\\mathbb{E}_{(s, a, r, s^{\\prime})}\\left\\{\\left(r+\\gamma Q_{\\phi^{\\prime}}\\left(s^{\\prime}, \\arg \\max _{a} Q_{\\phi}\\left(s^{\\prime}, a\right)\right)-Q_{\\phi}(s, a)\right)^{2}\right\\}$$\n\nwhere $r$ is the reward, $s$ is the state of the environment, $a$ denotes a possible action that the agent can take, $Q(s, a)$ is the Q-Function and $\\phi$ and $\\phi^{\\prime}$ are the set of weights of the primary and target network, respectively. The loss function $\\mathcal{L}(\\phi)$ quantifies the residual of the Bellman equation, which defines formally the optimal values of the Q-values, (Sutton and Barto, 1998). The set of weights from the target network, $\\phi^{\\prime}$, is updated using a soft update rule to enhance the stability of the learning process, (Lillicrap et al., 2015).\n\n$$\\phi^{\\prime} \\leftarrow \tau \\phi+(1-\tau) \\phi^{\\prime}$$\n\nTo train the DDQN, an experience replay buffer is utilized. During the training phase, the agent interacts with the environment and stores the experiences (state, action, reward, next state) in the replay buffer. Subsequently, random batches of experiences are sampled from the replay buffer to train the network and update its weights. This process helps to break the correlation between consecutive samples and improves stability during the learning process.\n\nAn additional consideration in solving this reinforcement learning problem is the need to balance maximizing power output with minimizing noise impact. These objectives are inherently conflicting, placing this problem within the Multi-Objective Reinforcement Learning (MORL) framework. MORL extends traditional reinforcement learning to handle problems involving multiple, often conflicting, objectives.\n\nVarious strategies exist for addressing MORL problems. One of the simplest methods is to define the reward using a scalarized function that combines the rewards for each objective into a single global reward, thus transforming the problem into a single-objective reinforcement learning task (Van Moffaert et al., 2013). Another approach involves Pareto optimization, which aims to find a set of optimal policies that lie on the Pareto Front, where no other policy is superior in all objectives (Van Moffaert and Now\u00e9, 2014). There are already methods that apply these MORL approaches using deep learning implementations (Mossalam et al., 2016). In this work, a scalarized method is adopted to define a reward that balances the two objectives of maximizing power and minimizing noise."}, {"title": "2.2.2. State-action structure", "content": "The state of the agent must include all the necessary information about the environment to enable the agent to take the best possible action. If the state lacks relevant information, the agent may not be able to achieve optimal performance. The state of the agent is defined by the incoming wind conditions, specifically the wind speed, $U_{o o}$, along with the control variables of the wind turbine, which are the rotational speed, $\\Omega$, and the blade pitch angle, $\theta$. To fit within the DDQN framework, the state space $\\mathcal{S}$ needs to be bounded. Some variables (rotational speed and pitch) are bounded by mechanical/structural limitations, whereas the wind speed is bounded by physical range. Note that these can be tuned for specific wind turbines and geographic sites. We include an additional constraint on the tip speed ratio $\\lambda=\\frac{\\Omega R}{U^{2}}$, with the blade radius $R=46.5 \\mathrm{~m}$, to ensure the correct behavior of the BEMT solver. The specific values of all the constraints are outlined below:\n\n- $U_{o o} \\in[4,16] \\mathrm{m} / \\mathrm{s}$,\n- $\\Omega \\in[6,18] \\mathrm{rpm}$,\n- $\theta \\in[-5,10]$ degrees,\n- $\\lambda \\in[3,12]$.\n\nThe actions available to the agent involve either increasing or decreasing the control variables. Since Q-learning is defined for a discrete action space $\\mathcal{A}$, the control variables can only be adjusted by fixed increments. Five distinct actions are defined: two for each control variable (one for increasing and one for decreasing), and one for maintaining the current state (doing nothing). The specific fixed increment for each possible action is determined based on the sensitivity analysis detailed in Section 2.1.2. Since the rotational speed is more sensitive to both power generation and sound pressure level compared to the pitch angle, the incremental adjustments for each variable has been designed so that their corresponding actions have effects of the same magnitude. The actions that the agent can take are specified as follows:\n\n- $a_{1}$ : increase $\\Omega$ by 0.5 rpm ,\n- $a_{2}$ : decrease $\\Omega$ by 0.5 rpm ,\n- $a_{3}$ : increase $\theta$ by 1 degree,\n- $a_{4}$ : decrease $\theta$ by 1 degree,\n- $a_{5}$ : do nothing.\n\nIt is important to note that the transition between states is not deterministic a priori. Although we can freely adjust the control variables, the wind conditions depend on the environment and are beyond our control. This motivates the use of a model-free reinforcement learning method, as model-based approaches only guarantee convergence if the transition function between states is known."}, {"title": "2.2.3. Reward definition", "content": "The reward function is key when defining the RL algorithm, as it is the only feedback to quantify how successful are the actions taken by the agent. Therefore, the reward must be carefully crafted for each specific problem to learn an appropriate policy. As mentioned in Section 2.2.1, this is a multi-objective optimization problem (or MORL), requiring a specific strategy to address the two conflicting objectives: maximizing power extraction while minimizing noise generation. In this work, we choose to blend the two objectives through a linear function, to define the overall reward. The reward can be expressed as follows:\n\n$$r=r_{\\mathrm{PW}}+r_{\\mathrm{SPL}}$$ \n\nwhere $r_{\\mathrm{PW}}$ denotes the reward associated to the power objective and $r_{\\mathrm{SPL}}$ the one related to the SPL one.\n\nAs we already discussed in our previous work Soler et al. (2024), the reward power component should encourage the agent to obtain the highest energy generation possible, regardless of the wind conditions. To achieve this, we use the power coefficient $C_{p}$ of the wind turbine. We set this reward to increase linearly from 0 to 1 , with 1 corresponding to the maximum possible value of the power coefficient within the state space, $C_{p, \text { nom }}$. Therefore, the reward power component reads,\n\n$$r_{\\mathrm{PW}}=\\frac{C_{p}}{C_{p, \text { nom }}}$$ \n\nThe reward term related to sound generation, $r_{\\mathrm{SPL}}$, is highly dependent on the specific problem being modeled. First, we need to select the observer locations where the SPL is computed, typically in critical areas where noise mitigation is a priority. In this work, we decide to set one observer 100 m downstream the wind turbine, see Figure 4. Next, we decide how to penalize the sound generation (SPL) in the reward function. We opt to use a ReLU activation function that begins penalizing once the SPL exceeds a certain threshold, SPL $_{\text {thr }}$. Below this threshold, the agent focuses solely on maximizing power. Additionally, we define a $\\Delta \\mathrm{dB}$ value that specifies how much the SPL threshold can be exceeded before the reward becomes -1 . Beyond this point, no matter how much power the agent generates, the total reward will be negative. Therefore, $\\mathrm{SPL}_{\\mathrm{thr}}+\\Delta \\mathrm{dB}$ serves as an effective noise limit. For this specific application, we defined $\\mathrm{SPL}_{\\mathrm{thr}}=45 \\mathrm{~dB}$ and $\\Delta \\mathrm{dB}=5 \\mathrm{~dB}$, but note that these values can be adapted to specific sites or regulations. The reward noise component can be seen in Figure 5 and reads as follows:\n\n$$r_{\\mathrm{SPL}}=-\\operatorname{ReLU}\\left(\\frac{\\mathrm{SPL}-\\mathrm{SPL}_{\\mathrm{thr}}}{\\Delta \\mathrm{~dB}}\right)$$\n\n![img-4.jpeg](img-4.jpeg)\n\nFigure 4: Directivity map of Sound Pressure Level (SPL) generated by OpenFAST. The wind direction is oriented along the positive $x$ axis (left to right), perpendicular to the wind turbine rotor. The operational conditions are $U_{\\infty}=12 \\mathrm{~m} / \\mathrm{s}, \\Omega=16.5 \\mathrm{rpm}$ and $\theta=-1$ degree. The observer location ' $\times$ ' is situated 100 m downwind.\n\n![img-5.jpeg](img-5.jpeg)\n\nFigure 5: Reward noise component.\n\nIn addition, we need to include the bounds of $\\mathcal{S}$ in the reward. To make the agent learn the limits, it receives punishments whenever it performs a forbidden action, that is, an action that leads to a state $s_{t+1} \notin \\mathcal{S}$. In such cases, the agent receives a negative reward with a value of $r=-3$ and the action is revoked so that the control variables remain the same. The punishment is set to -3 to differentiate it from the possible negative reward of $r_{\text {SPL }}$. This distinction is made because exceeding the $\\mathcal{S}$ limits is considered worse than generating noise above the threshold. Finally, the reward function for the agent is the following\n\n$$r\\left(s_{t}, a_{t}, s_{t+1}\right)=\\left\\{\begin{array}{cc} \\frac{C_{p}\\left(s_{t+1}\right)}{C_{p, \text { nom }}}-\\operatorname{ReLU}\\left(\\frac{\\operatorname{SPL}\\left(s_{t+1}\right)-\\operatorname{SPL}_{\text {the }}}{\\Delta \\mathrm{dB}}\right) \\ -3 & \text { if } s_{t+1} \notin \\mathcal{S} \\end{array}\right\\}$$"}, {"title": "2.2.4. Neural Network architecture", "content": "When using DQN, neural networks (NN) are employed to approximate the Q-Function. Typically, the NN is designed to approximate the Q-Vectors, $\boldsymbol{q}(s)$, which represent the Q-Values in the state $s$ for all possible actions. That is, $\boldsymbol{q}(s)_{i}=Q\\left(s, a_{i}\right)$. This approach is used because $\\mathcal{A}$ is a discrete space, and encoding these discrete actions as inputs can be problematic; it is more convenient to create a mapping between real subspaces. The neural network map is defined as $\boldsymbol{q}_{\\phi}(\boldsymbol{s}): \\mathcal{S} \\subset \\mathbb{R}^{3} \rightarrow \\mathbb{R}^{5}$, where $\\phi$ denotes all the NN weights, the output space dimension is $|\\mathcal{A}|=5$ and $\boldsymbol{s}$ denotes the state vector, which is $\boldsymbol{s}=\\left[U_{\\infty}, \\Omega, \theta\right]^{T}$.\n\nThe neural network architecture employs a Multi-Layer Perceptron structure, consisting of two dense hidden layers with Rectified Linear Unit (ReLU) activation functions. The final layer, uses a linear activation function instead of ReLU. This allows the Q-values to take on any sign, rather than being restricted to positive numbers. The number of dense layers and their sizes were determined through extensive trial and error. Ultimately, two layers with 128 and 64 neurons, were found to be sufficient to accurately represent the Q-Function. The architecture of the Q-Network used to train the DDQN agent is shown in Figure 6.\n\n![img-6.jpeg](img-6.jpeg)\n\nFigure 6: Q-Network architecture."}, {"title": "2.2.5. Training of the Wind Turbine DDQN Agent", "content": "The main ideas of Q-Learning have already been explained in Section 2.2.1. However, here the specific details of the DDQN training to design the wind turbine controller are included. During the training phase, the agent faces random steady wind conditions during short episodes of 20 time steps. This allows the agent to adapt to virtually any wind, even if the wind speed changes faster than 20 time steps. This adaptability is achieved because experiences are stored in the replay buffer, and batches are selected randomly. Consequently, the specific temporal evolution of states during the agent's experience is not critical, provided that the stored transitions comprehensively represent all actual transitions in the system.\n\nThe Double Deep Q-Network (DDQN) agent is trained using the hyperparameters listed in Table 2. To illustrate the influence of each parameter on the training process, a pseudocode for the DDQN training is presented in Algorithm 1. The effectiveness of the learning progress during training is assessed by displaying the Q-values of the state-action pairs encountered by the agent, as shown in Figure 7. As the agent learns, the Q-values of the actions taken at each state are expected to increase, as depicted in the figure. For the implementation, we utilized OpenAI Gym (Towers et al., 2023) to create the environment, serving as a bridge between the reinforcement learning formulation and the OpenFAST wind turbine solver. TF-Agents (Guadarrama et al., 2018) was employed to develop the agent and manage the entire training process. All neural networks were constructed using the Keras API (Chollet et al., 2015), and the Adam optimizer was used for training (Kingma and Ba, 2014).\n\nAlgorithm 1 Double Deep Q-Learning Algorithm\n\n1: Initialize primary network $Q_{\\phi}$ with random weights $\\phi$\n\n2: Initialize target network $Q_{\\phi^{\\prime}}$ with weights $\\phi^{\\prime}=\\phi$\n\n3: Initialize replay buffer $\\mathcal{R}$\n\n4: Set hyperparameters: $\\alpha$ (learning rate), $\\gamma$ (discount factor), $\\epsilon$ (exploration probability), $\tau$ (soft update parameter), $N$ (number of training iterations), $n$ (number of steps taken every training step, $m$ (period of update of the target network).\n\n5: Initialize random state $s$\n\n6: for iter=1 to N do\n\n7: for n steps do\n\n8: $\\quad$ Select action $a$ using $\\epsilon$-greedy policy from $Q_{\\phi}$\n\n9: $\\quad$ Take action $a$, compute reward $r$ and new state $s^{\\prime}$ using the wind turbine solver\n\n10: $\\quad$ Store transition $\\left(s, a, r, s^{\\prime}\right)$ in replay buffer $\\mathcal{R}$\n\n11: if last step of the episode then\n\n12: $\\quad$ Initialize random state $s$\n\n13: else\n\n14: $\\quad$ Update current state: $s \\leftarrow s^{\\prime}$\n\n15: end if\n\n16: end for\n\n17: Sample random mini-batch of transitions $\\left(s_{j}, a_{j}, r_{j}, s_{j}^{\\prime}\right)$ from $\\mathcal{R}$\n\n18: Estimate loss function from eq. (1) with the mini-batch transitions\n\n19: $\\quad$ Update the primary set of weights $\\phi$ using a gradient optimizer.\n\n$\\phi \\leftarrow \\phi-\\alpha \nabla_{\\phi} \\mathcal{L}$\n\n20: each $m$ iterations do Soft update of the target network weights $\\phi^{\\prime}$ using eq. (2) rule.\n\n21: end for\n\n![img-7.jpeg](img-7.jpeg)\n\nFigure 7: Average Q-Values taken during the training process of the DDQN agent.\n\n| Parameter | Value |\n| :-- | :-- |\n| Environment interactions | 200 k |\n| Steps of the environment per training iteration | 5 |\n| Maximum capacity of the replay buffer | 50 k |\n| Batch size | 64 |\n| Learning rate | $5 \\mathrm{e}-4$ |\n| Discount factor | 0.95 |\n| Epsilon value for the epsilon greedy policy | 0.50 |\n| Tau soft update parameter | 0.1 |\n| Period of update of the target network | 20 |\n\nTable 2: Training parameters used for the DDQN agent."}, {"title": "3. Results", "content": "The agent's performance is assessed under various wind conditions. First, we validate the operational point that the agent reaches under constant wind conditions, assessing its optimality via a Pareto front. Next, we evaluate the agent's ability to adapt to turbulent wind conditions, comparing its control strategy against a classic controller. Finally, we estimate the agent's annual energy production and compare it against a classic control strategy designed to maximize energy extraction.\n\n### 3.1. Steady wind validation\n\nThe simplest test for evaluating the agent is to assess its performance under steady wind conditions. In this scenario, with unchanging wind conditions, the agent should identify, reach and maintain the state that maximizes the cumulative reward. This optimum state does not depend on the initial conditions of pitch and rotor speed, as the wind conditions are steady. However, it is important to note that the agent's actions are discrete, limiting its ability to reach every possible state.\n\nTo validate the performance and robustness of the agent, a Pareto diagram is used. The agent is tested for different initial conditions (with the same steady wind speed) to determine if it can consistently reach optimal states (at the pareto front), regardless of the initial state. To illustrate the agent's trajectory (sequence of state-action-reward) for each initial condition, these trajectories are displayed on a power coefficient - sound pressure level diagram, along with values for 1000 random states from $\\mathcal{S}$. Figure 8 presents the Pareto diagram, illustrating the agent's trajectory from four different initial conditions. It is clear that, regardless of the initial condition, the agent successfully achieves high power outputs up to the maximum permissible decibel level. Furthermore, the agent demonstrates robust performance by consistently avoiding the maximum limit of SPL (dB A) while remaining close to the limit to maximize power. It can be seen that the RL does not always reach the same final state, but that the optima are relatively close to each other. This suggests the existence of local optimum. In addition, the discrete nature of the Q-learning actions, may not allow the agent to reach certain optima, since not all states are reachable from an initial state. Despite these issues, the agent consistently avoids acoustic penalties and achieves high power outputs, with power coefficients ranging from 0.26 to 0.30 .\n\nFor completness, Table 3 shows the initial conditions of the control variables for the trajectories displayed on Figure 8, as well as the final state control variables.\n\n![img-8.jpeg](img-8.jpeg)\n\nFigure 8: Pareto diagram at a wind speed of $10 \\mathrm{~m} / \\mathrm{s}$, showing the power coefficient $C_{p}$ and sound pressure level SPL (dB A) computed for 1000 random states within the state space. Different agent trajectories are displayed on the Pareto diagram. The numbering of the initial conditions correspond to Table 3.\n\nOverall, the agent robustness has been tested on a simple steady wind scenario. The agent is able to reduce the wind turbine noise to admissible levels while maximizing power. Furthermore, the agent finds optimum operational conditions regardless of the initial condition, showing the robustness of the algorithm. In other words, the neural network which approximates $Q(s, a)$ has covered all his input space $\\mathcal{S} \times \\mathcal{A}$."}, {"title": "3.2. Control Strategy for Experimental Winds", "content": "The agent capabilities are now tested over experimental wind conditions. We compare the energy extraction between our agent and two controllers that are designed solely to maximize power. By doing so, we can demonstrate how much power we need to sacrifice to keep the wind turbine at acceptable decibel levels. The performance of the three controllers are going to be compared. These controllers include:\n\n- Classic wind turbine controller: Standard wind turbine controller designed to reach the power curve of the wind turbine, using torque or pitch control depending on whether the wind speed is above or below rated wind speed. Details can be seen in Appendix B.\n\n- Power DDQN: Agent designed to maximize solely power. It is trained with no noise penalization, that is only the power reward is included, see eq. (4).\n\n- Quiet DDQN: Agent designed to maximize power without producing more that 45 dB decibel levels at 100 m downwind of the rotor. It is trained with the complete reward definitions including power and noise, see eq. (6).\n\nThe wind data used to validate the control performance under real wind conditions was obtained from the measurement and instrumentation data center (MIDC) of NREL, see Jager and Andreas (1996). These daily wind measurements are available as open-source. For this study, wind speed and wind direction measurements at an 80 m height from June 1, 2023, to June 1, 2024, are selected. Figure 9a displays a wind rose illustrating the wind speed and wind direction of this dataset. Since this study concentrates on torque-pitch control, it is assumed that the incoming wind is consistently aligned with the wind turbine, a condition typically managed by yaw control. Therefore, we assume perfect alignment and only the wind speed distribution is used in the subsequent results.\n\nNote that the Power DDQN agent considers power optimization uniquely. Therefore, it is only applicable to the below rated wind speed region defined on Appendix B. When the wind speed exceeds the rated value, the control strategy maintains nominal power rather than maximizing it. To achieve this behavior with a reinforcement learning agent, the reward function would need to be modified. Consequently, the wind speed distribution used to validate the agent under turbulent wind conditions is restricted to below-rated wind speeds, enabling a meaningful comparison between the three controllers.\n\nThe control performance of the three agents is analyzed in detail over an 8 -hour time span, using the wind speed distribution shown in Figure 9b. The RL controllers are allowed to control each minute. In the next section, we will estimate the annual energy production for all controllers.\n\n![img-9.jpeg](img-9.jpeg)\n\n(a) Wind rose for one year of data.\n\n![img-10.jpeg](img-10.jpeg)\n\n(b) Wind speed distribution for 8 hours.\n\nFigure 9: Wind conditions on the experimental wind environment.\n\nFigure 10 shows the results from the different controllers over the first 8 hours of the yearly dataset. Figures 10a and 10b display the control parameters evolution while Figures 10c and 10d illustrates the power and the noise generated 100 m downwind. It is noted that the Power agent matches the power extraction achieved by the classic control strategy, essentially implementing the same control approach but with the discrete actions defined for the reinforcement learning agent. Since neither of these controllers is designed to consider the acoustics of the wind turbine, both generate high levels of noise when the wind speed is sufficiently high. In contrast, the Quiet agent can match the power generation of the power-oriented controllers when the wind speed is moderate. When wind speeds are higher, it extracts as much power as possible while keeping noise levels below the threshold value. Moreover, all three controllers maintain a constant pitch value to maximize power extraction. However, the Quiet agent adjust the pitch angle to reduce the noise levels when the wind speeds get higher.\n\n![img-11.jpeg](img-11.jpeg)\n\nFigure 10: Control results for the three different agents on the 8 hour experimental wind environment.\n\nThis test shows the flexibility of the RL strategy for control and highlights the possibility of including multiobjectives. In addition, we see that there is no need to have an a priori knowledge of the turbine performance (e.g., the power curve or rated maximum power) since the RL will learn these characteristics when trained."}, {"title": "3.3. Annual wind energy estimation", "content": "There are different methodologies to obtain an estimate of the annual generation of wind energy, (GarciaBustamante et al., 2009). The standard procedure is based on decoupling the wind turbine from the wind distribution of the particular site. It considers the observed wind speed frequency histogram to fit a theoretical probability density function (PDF) for the wind speed. It also requires a transfer function that models the relation between power output and wind speed. Typically, the Weibull distribution is used to fit the wind speed frequency histogram. Garc\u00eda-Bustamante et al. (2008) showed that although the Weibull distribution may not be substantiated for most sites, it does not include important errors on the energy estimations. The probability density function of the Weibull distribution is given by:\n\n$$f_{U}(u)=\\left(\\frac{k}{c}\right) \\cdot\\left(\\frac{u}{c}\right)^{k-1} \\exp \\left(-\\left(\\frac{u}{c}\right)^{k}\right)$$\n\nwhere $U$ denotes the random variable that models the wind speed. The fit of the shape and scale parameters $k$ and $c$ are established from the mean and variance of the wind speed, $\\mu_{U}=\\mathbb{E}[U]$ and $\\sigma_{U}^{2}=\\mathbb{V}[U]$. The specific relations can be seen in Spiru and Simona (2024) work and are the following:\n\n$$k=\\left(\\frac{\\sigma_{U}}{\\mu_{U}}\right)^{-1.086}$$\n\nand\n\n$$c=\\frac{\\mu_{U}}{\\Gamma\\left(1+\\frac{1}{k}\right)}$$\n\nwhere $\\Gamma$ denotes the special gamma function. This formulation can be employed to obtain the Weibull probability density function that represents the one-year experimental data reported by Jager and Andreas (1996). This is illustrated in Figure 11.\n\n![img-12.jpeg](img-12.jpeg)\n\nFigure 11: Wind speed frequency histograms of one year experimental data and PDF of the adjusted Weibull distribution. The shape and scale parameters are $k=1.195$ and $c=4.837$ respectively.\n\nRegarding the transfer function between power and wind speed, various strategies exist (Abolude and Zhou, 2018). The Theoretical Power Curve (TPC) does not account for control mechanisms. Furthermore, since our wind turbine control strategy considers acoustic generation, the wind turbine will exhibit a significantly different Effective Power Curve (EPC). It is necessary to compute an EPC that accurately represents the transfer function between power and wind speed for our specific control scenario. The EPC can be computed using simulations of the wind turbine control. The agent is faced against a turbulent wind that covers all the range of interest of wind speed, mainly between cut-in and cut-off wind speed. This turbulent wind must be representative of the turbulent nature of the wind that the wind turbine is going to face during operation. Once the simulation is done, all the pairs of data points $\\left(U_{\\infty}, C_{p}\right)$ can be used to obtain a transfer function for the power coefficient $C_{p}\\left(U_{\\infty}\right)$. A subset of 100 hours of the experimental wind measurements from the MIDC (Jager and Andreas, 1996) has been used to obtain the EPC of the SWT2.3-93 wind turbine using the Classic Control and the Quiet DDQN agent already introduced on Section 3.2. Figure 12 illustrates the results of this simulations, showing the operational laws of control for each agent on Figures 12a and 12b and the SPL and power associated on Figures 12c and 12d respectively. The behavior is as expected, the Quiet Agent does not increase the rotational speed above 10.5 rpms to avoid surpassing the SPL threshold and uses the pitch to reduce noise if needed, which explains the high variance bars on the pitch (see fig. 12b) and low ones in the rotational speed (see fig. 12a). Meanwhile the classic control can increase the rotational speed freely and the pitch is only use in above-rated wind speed scenarios, see Appendix B. The large standard deviations on the classic control pitch are due to the PID control, which is dynamically adjusting to the turbulent wind. In Figure 12c it is illustrated how the classic control matches on average the TPC. However, it is not able to adjust perfectly to the turbulent wind, showed by its high variance on the above-rated wind speed region. The Quiet agent achieves less power than the classic one but is able to maintain the sound pressure level below the specified threshold of 45 dB A , see fig. 12d.\n\nTraditional approaches use only the average value or polynomial fits of the historical/simulated data to construct the EPC. All these methods do not capture the variance of the data in the model. To account for this variability on the EPC model we introduce a statistical method. For simplicity, we model the power coefficient $C_{p}\\left(U_{\\infty}\right)$, which is obtained by non-dimensionalizing the EPC data. A Gaussian Process Regression (MacKay et al., 1998) can be employed to model the power coefficient at each wind speed as a Gaussian probability distribution, $C_{p}\\left(U_{\\infty}\right) \\sim N\\left[\\mu_{C_{p}}\\left(U_{\\infty}\right), \\sigma_{C_{p}}\\left(U_{\\infty}\right)\right]$. Figure 13a shows the power coefficients points obtained after the wind turbine control simulation. This data is used to fit the Gaussian Process (GP) model and obtain the mean and standard deviation of the power coefficient as functions of the wind speed, this fit is also included in Figure 13a. The $C_{p}$ distribution for specific values of the wind speed is illustrated on Figure 13b where it is compared with the histogram of the power coefficient from the data.\n\n![img-13.jpeg](img-13.jpeg)\n\n(a) Power coefficient EPC. The data points represent all the observed $\\left(U_{\\infty}, C_{p}\right)$ pairs observed during the simulation performed to obtain the EPC. The Gaussian Process Regression fit, mean and confidence interval is included.\n\n![img-14.jpeg](img-14.jpeg)\n\n(b) Comparison of the $C_{p}$ frequency histograms and the Gaussian probability distribution ( - ) obtained with the GP for different wind speed values. The wind speed values for each histogram are specified in Figure 13a with vertical colored lines. The order is the following: Top Left: $\\square$, Top Right: $\\square$, Bottom Left: $\\square$, Bottom Right: $\\square$.\n\nFigure 13: Power coefficient obtained from simulating the Quiet DDQN controller over a 100 hour experimental wind speed period.\n\nAssuming the Weibull probability distribution for the wind speed $U_{\\infty}$ combined with the GP model for the $C_{p}$ distribution, the estimation of the annual wind energy can be performed by computing the expectation of the wind energy $E_{w}$. Mathematical details on the statistical distributions are provided in Appendix A. Table 4 presents the annual energy estimation for each control strategy. It is important to note that the Power DDQN controller is applicable only in the below-rated wind speed region. Therefore, when computing the annual wind energy generation with this control, we impose nominal power for wind speeds above the rated value. The Power DDQN controller is included in the comparison to ensure that it remains competitive with the Classical Control within the below-rated wind speed range. The Quiet DDQN controller is able to control the wind turbine without surpassing the sound pressure level threshold selected and obtains an $78 \\%$ of the annual energy production compared to the Classical Control. Additionally, in Table 4 it is shown the average standard deviation across the wind speed for the power coefficient GP fit. There, it is shown that the Quiet agent exhibits the control strategy with the least variance.\n\n| Controller | $\\mathbf{E}_{\\mathbf{w}}[\\mathbf{M W h}]$ | $\\hat{\\sigma}_{\\mathbf{C}_{\\mathbf{p}}}$ |\n| :--: | :--: | :--: |\n| Quiet DDQN | 2722 | 0.024 |\n| Power DDQN | 3541 | 0.042 |\n| Classic Controller | 3458 | 0.057 |\n\nTable 4: Annual wind energy generation for the three different controllers. Additionally, the average standard deviation across wind speed for each controller is presented, $\\hat{\\sigma}_{\\mathrm{C}_{\\mathrm{p}}}=\\int_{\\mathrm{C}_{\\mathrm{ml}}}^{\\mathrm{C}_{\\mathrm{ml}}} \\sigma_{\\mathrm{C}_{\\mathrm{p}}}(u) f_{U}(u) d u$, details on Appendix A at eq. (A.5)."}, {"title": "4. Conclusions", "content": "In conclusion, integration of reinforcement learning with wind turbine control holds promise for optimizing energy generation and efficiency while minimizing acoustic environmental impact. A DDQN reinforcement learning agent can replicate the control strategy of a standard wind turbine controller without prior explicit knowledge of the wind turbine, relying solely on a wind turbine solver for experiential learning. Moreover, advanced control strategies can be readily implemented by modifying the reward function. In this work, an RL controller is defined to maximize power output while maintaining acceptable decibel levels, thereby incorporating acoustic effects into the control strategy. This demonstrates that MORL is capable of dynamically balancing two different objectives effectively.\n\nAn effective power curve is computed from control simulations of turbulent wind data. This allow to characterize the reinforcement learning control strategy, obtaining the operational laws and obtaining an annual wind energy estimation. The methodology is validated using a SWT2.3-93 wind turbine with a rated power of 2.3 MW. We evaluate the yearly energy production for a realistic site. The DDQN reinforcement learning control provides similar energy production that a traditional control. The methodology presented allows for the inclusion of noise limits leading to a $22 \\%$ reduction in the annual energy extraction when activating a maximum allowed noise of 45 dB ( 100 meters downwind downwind of the turbine).\n\nFurther research directions include investigating Multi-Agent Reinforcement Learning algorithms for cooperative control of wind turbines within farms, which could enhance overall system performance while controlling noise at the farm level."}]}