{"title": "Deep Reinforcement Learning for Multi-Objective Optimization: Enhancing Wind Turbine Energy Generation while Mitigating Noise Emissions", "authors": ["Mart\u00edn de Frutosa", "Oscar A. Marino", "David Huergo", "Esteban Ferrera"], "abstract": "We develop a torque-pitch control framework using deep reinforcement learning for wind turbines to opti-\nmize the generation of wind turbine energy while minimizing operational noise. We employ a double deep\nQ-learning, coupled to a blade element momentum solver, to enable precise control over wind turbine parame-\nters. In addition to the blade element momentum, we use the wind turbine acoustic model of Brooks Pope and\nMarcolini. Through training with simple winds, the agent learns optimal control policies that allow efficient\ncontrol for complex turbulent winds. Our experiments demonstrate that the reinforcement learning is able to\nfind optima at the Pareto front, when maximizing energy while minimizing noise. In addition, the adaptability of\nthe reinforcement learning agent to changing turbulent wind conditions, underscores its efficacy for real-world\napplications. We validate the methodology using a SWT2.3-93 wind turbine with a rated power of 2.3 MW.\nWe compare the reinforcement learning control to classic controls to show that they are comparable when not\ntaking into account noise emissions. When including a maximum limit of 45 dB to the noise produced (100\nmeters downwind of the turbine), the extracted yearly energy decreases by 22%. The methodology is flexible\nand allows for easy tuning of the objectives and constraints through the reward definitions, resulting in a flexible\nmulti-objective optimization framework for wind turbine control. Overall, our findings highlight the potential of\nRL-based control strategies to improve wind turbine efficiency while mitigating noise pollution, thus advancing\nsustainable energy generation technologies.", "sections": [{"title": "1. Introduction", "content": "In recent years, the aerodynamic design of wind turbines has undergone significant advances, reaching near-\noptimal efficiency through substantial investments in aerodynamic optimization, as well as advancements in\nmanufacturing techniques and materials. Consequently, the focus has shifted towards addressing the issue of\nnoise generated by wind turbines, which is emerging as a competitive factor within the wind energy industry.\nNumerous studies have explored the correlation between wind turbine sound power levels and public-reported\nperceptions of annoyance (Wolsink et al., 1993; Pedersen et al., 2009). Accurate prediction of wind turbine\nnoise under real operational and atmospheric conditions is crucial to design quieter turbines and complying\nwith imposed noise regulations (Davy et al., 2018). This necessity underscores the importance of fast turn-\naround methods to incorporate noise calculations into design and optimization processes, as well as to assess\nnoise in real time during operation. Such efforts not only optimize wind resource utilization, but also minimize\nthe impact on the quality of life of nearby communities and wildlife.\nAerodynamic noise poses a significant limitation to further exploiting wind energy resources. This type\nof noise results from the turbulent flow interacting with the airframe, necessitating a detailed resolution of\nthe flow for accurate far-field noise prediction. However, computational fluid dynamics (CFD) solvers, while\ncapable of simulating the flow field, incur a high computational cost which escalates further when resolving\nthe acoustic field. Consequently, numerical approaches to wind turbine noise prediction remain challenging."}, {"title": "", "content": "Therefore, most noise prediction models for wind turbines are based on aeroacoustic semi-empirical models\nrather than numerical simulations (Wagner et al., 1996). Despite these obstacles, wind turbines remain an\nessential component in the generation of clean and renewable energy. However, effective control strategies are\nimperative to optimize their performance under variable wind conditions.\nWind turbine control systems are designed to maximize energy generation while ensuring structural integrity\nand safe operation (Njiri and S\u00f6ffker, 2016; Novaes Menezes et al., 2018; Andersson et al., 2021). Given the\ndynamic nature of wind, adaptive control strategies are essential, with classic mechanisms including adjust-\nments to yaw angle and rotational speed, as well as blade pitch angle modulation. Leveraging real-time wind\nmeasurements, turbine dynamics, and advanced control algorithms enables simultaneous adjustments to rotor\nspeed, and pitch, enhancing energy generation, reducing fatigue loads, and extending turbine lifespan.\nThe emergence of reinforcement learning (RL) presents novel opportunities for wind turbine control by\nenabling data-driven adaptive decision-making (Le Clainche et al., 2023; Garnier et al., 2021). RL, a machine\nlearning approach, involves an agent learning to make decisions in an environment to maximize cumulative\nrewards over time (Sutton and Barto, 1998). Applied to wind turbines, RL offers autonomous learning of control\ninputs to maximize power generation by capturing complex non-linear relationships between wind conditions,\nturbine states, and actions. RL-based control methods adapt in real-time to changing wind conditions, offering\nsignificant advantages in wind turbine operation.\nThis paper reviews recent advances in RL-based control strategies for wind turbines, focusing on pitch an-\ngle and rotor speed modulation. Previous studies have proposed RL algorithms with comprehensive reward\ndefinitions, showcasing their efficacy in optimizing wind turbine performance under varying wind conditions.\nFor example Chen et al. (2020) proposed a RL pitch controller to maintain the nominal power using an adap-\ntive dynamic programming algorithm, reducing the energy consumption of the pitch actuator. Xie et al. (2023)\ndeveloped a data-driven model to perform a torque-pitch controller, modeling the dynamics and using RL to\ncontrol the wind turbine. Sierra-Garcia et al. (2020) discussed different reward definitions for wind turbine\ncontrollers, while Puech and Read (2023) and Saenz-Aguirre et al. (2019) developed RL methods for yaw con-\ntrol avoiding control parameter tuning. Kushwaha et al. (2020) and Wei et al. (2016) employed Q-learning\nRL methods for maximum power point tracking (MPPT) control of the generator speed. Overall, these stud-\nies demonstrate the adaptability of RL systems to realistic wind conditions, thereby enhancing overall energy\ngeneration and efficiency of wind farms.\nIn this paper, we introduce a reinforcement learning-based dynamic control method designed to maximize\npower output while adhering to specified maximum decibel levels. The paper is structured as follows. First, we\nsummarize the methodology in Section 2. There, we include the wind turbine model, validating the aeroacoustic\nmodel with three different wind conditions. Additionally, the multi objective reinforcement learning strategy is\nexplained, we provide details on the reward, the neural network architecture and the training procedure. Second,\nin Section 3 we validate the controller with simple steady winds to later challenge the method with turbulent\nwind conditions obtained from experimental measurements. We end with conclusions and outlooks."}, {"title": "2. Methodology", "content": "In this section, we detail the methodology for integrating Deep Reinforcement Learning (DRL) with the\ndynamic control of a wind turbine. We begin by describing the model of the wind turbine, focusing on how\nboth the power output and the noise levels are computed, and validate the methodology using field measurements\nfor a SWT2.3-93 wind turbine with a rated power of 2.3 MW. Subsequently, we detail the setup of the DRL\nalgorithm, which is designed to maximize power generation within specified noise constraints, demonstrating\nthe application of advanced machine learning techniques to real-world energy optimization challenges."}, {"title": "2.1. Wind turbine modeling using OpenFAST", "content": "One critical requirement for incorporating a wind turbine solver into the DRL control framework is the\nability to perform rapid evaluations, as the DRL training process requires a large number of simulations. To\nmeet this need, we have chosen to employ an efficient Blade Element Momentum Theory (BEMT) solver.\nSpecifically, we use OpenFAST (National Renewable Energy Laboratory, 2024), a well-known open-source\nsoftware for simulating wind turbine dynamics and acoustic predictions.\nBEMT, is known for its efficiency and offers a simple yet accurate method for estimating the aerodynamic\nforces and energy generation of wind turbines. Its ability to perform rapid function evaluations is crucial for\ntraining and validating the agent within a reasonable timeframe. In the realm of BEMT, the wind turbine blade\nis segmented into smaller sections along its span. The aerodynamic forces exerted on each section are computed"}, {"title": "", "content": "based on the local wind conditions and the airfoil's geometry. These local flow conditions, defined for every\nsection and time step, encompass the wind's speed and direction, along with the turbulence intensity. Polar\ncurves for each airfoil section are used to compute the aerodynamic forces (lift, drag and moment coefficients).\nBy integrating the forces along the span of the blades, we can derive the overall power and thrust generated by\nthe wind turbine.\nAdditionally, OpenFAST includes an aeroacoustic module that enables the computation of noise levels gen-\nerated by the wind turbine at specific observer locations. To determine the aerodynamic noise sources from\nwind turbine blades, various semi-empirical noise models are included (Zhu et al., 2005) and we select the\nBrooks Pope and Marcolini model. The sound pressure level (SPL) for each blade segment is calculated based\non individual noise mechanisms. The cumulative effect of these mechanisms yields the noise source for each\nairfoil. Finally, the noise sources from all blade segments are combined as uncorrelated noise sources, con-\ntributing to the overall computation of the wind turbine's sound power level. The essential aspect of this process\nis the precise identification and modeling of the various noise mechanisms associated with each blade section.\nThese mechanisms can be categorized into two groups: turbulent inflow noise and airfoil self-noise. OpenFAST\nimplements the turbulent inflow model presented by Moriarty et al. (2004) and, among the airfoil self-noise\nmodels described by Brooks et al. (1989), we have specifically selected: turbulent boundary layer trailing edge\nnoise and tip vortex formation noise."}, {"title": "2.1.1. Validation of OpenFAST with a SWT2.3-93 wind turbine", "content": "The onshore wind turbine selected for the study is based on the SWT2.3-93, which performs a rated power\nof 2.3 MW. This turbine has undergone extensive in field experimental testing and complete details on its\ngeometry and benchmark data are available in the open access repository Zenodo (through the work of the\nEuropean project Christophe et al. (2022)). The airfoil polar curves are available in the airfoil catalog compiled\nby Bertagnolio et al. (2001). More details can be found in Matthew (2012).\nAll the open-source information enables us to create a SWT2.3-93 OpenFAST model. Additionally, the\nbenchmark results from the Zenodo dataset can be utilized to validate the model. Figure 1 presents the validation\nof both the power output and the sound pressure level (SPL) of the wind turbine. Figure la compares the\nexperimental power curve (from the Zenodo database) with that generated by the OpenFAST solver, showing\ngood agreement. Meanwhile, Figure 1b shows the one-third octave SPL for frequencies ranging 10Hz to 10kHz,\ncomparing the Zenodo dataset with results from OpenFAST. These comparisons cover three different operational\nconditions, detailed in table 1. The Zenodo acoustic results are computed for an observer positioned 100 meters\ndownstream from the wind turbine, at ground level. We observe good agreement with the experimental data\nfor the three operating conditions. We conclude that OpenFAST, with the acoustic model of Brooks Pope and\nMarcolini, provides accurate predictions of power generation and acoustics, and is therefore a valid tool to\nperform multi-objective optimization."}, {"title": "2.1.2. Sensitivity to control parameters", "content": "The selected parameters to control the wind turbine power and noise include the rotational speed \u03a9 and the\nblade pitch angle 6. Before discussing the RL setup, it is crucial to illustrate the sensitivity of these parameters\nfor the two performance metrics: the power coefficient and the overall sound pressure level.\nIn Figure 2 the sensitivity analyses for both rotational speed and blade pitch angle are displayed for a\nsingle incoming wind speed ($U_\\infty$). The values for one-third octave SPL, power coefficient and overall sound\npressure level are shown for different operational conditions. Figure 2a depicts the influence of \u03a9, increasing\nthe rotational speeds makes both the sound pressure level and the power coefficient rise, highlighting the trade-\noff between maximizing power and minimizing noise. The entire SPL spectrum increases uniformly when\nincreasing \u03a9, due to a higher relative velocity in the blades and leading to a rise in SPL regardless of the\nnoise mechanism. A similar analysis is presented in Figure 2b for the blade pitch angle. Although the general\nconclusion about the opposition between power and noise remains valid, the SPL spectrum behaves differently\nacross frequencies. The pitch angle mainly affects trailing edge noise leading to changes at relatively low\nfrequencies ranging from 10Hz to 1kHz."}, {"title": "2.2. Design of a reinforcement learning control", "content": "Reinforcement Learning is a branch of machine learning that focuses on how agents should take actions in\nan environment to maximize cumulative reward. Unlike supervised learning, where the model learns from a\nlabeled dataset, RL is driven by agent-environment interactions. The agent takes actions based on the current\nstate of the environment and receives feedback in the form of rewards. The state represents the situation of\nthe environment at a given time, while the actions are the possible moves the agent can make. The reward is\nthe feedback indicating the immediate benefit or cost of an action, guiding the agent toward better actions over\ntime. In particular, in this work we use Q-learning RL, which is detailed in the next section."}, {"title": "2.2.1. Reinforcement Learning for Multi Objective Control", "content": "Q-learning is a widely recognized reinforcement learning algorithm (Watkins and Dayan, 1992). It is cat-\negorized under model-free RL algorithms, implying that it operates without the necessity for prior knowledge\nor explicit models that represent the dynamics of the system. The fundamental component of Q-learning is the\nQ-value, which quantifies the anticipated cumulative reward for executing a specific action in a given state. The\nQ-value is updated iteratively via the Bellman equation, which formulates the optimal action-value function in"}, {"title": "", "content": "terms of the maximum expected future reward. During the learning process, the wind turbine interacts with\nthe environment, transitions between states, and takes actions according to its current policy. The Q-learning\nalgorithm employs an e-greedy exploration-exploitation trade-off to strike a balance between exploring new\nactions (e times) and exploiting current knowledge (1 \u2013 \u0454 times) to maximize cumulative rewards. In RL the\ncumulative reward is computed taking into account that a reward received immediately is worth more than a\nreward received in the future, specifically, each time step the reward is discounted by y, the discount rate.\nInitially, the Q-values are arbitrarily initialized. As the wind turbine explores the environment and receives\nfeedback in the form of rewards, the Q-values are updated using the temporal difference error. The temporal\ndifference error represents the discrepancy between the observed reward and the predicted reward based on\nthe Q-values. Through repeated iterations, the Q-learning algorithm gradually converges to an optimal policy.\nIn this state, the wind turbine learns the best actions to take in different states, thereby maximizing power\ngeneration while minimizing noise. In our case, this agent-environment interactions for the wind turbine control\nare illustrated on Figure 3."}, {"title": "", "content": "Deep Q-Network (DQN) is a variant of Q-learning that employs a deep neural network to estimate Q-values\n(Mnih et al., 2013). It replaces the traditional lookup table with a neural network, enabling generalizations across\nstates to handle large state spaces efficiently. In this study, a Double Deep Q-Learning (DDQN) is employed.\nDDQN is an extension of DQN that uses two neural networks: the primary network and the target network.\nThe primary network select the action and the target network evaluates its Q-value. This way of decoupling the\naction selection and evaluation addresses the overestimation of Q-values, often observed in DQN algorithms\ndue to the maximization bias, (Van Hasselt et al., 2016). The weights of the primary network are obtained by\nminimizing the following loss function:\n$L(\\phi) = E_{(s,a,r,s')}\\left[\\left(r + \\gamma Q_{\\phi'}\\left(s', \\arg \\max_a Q_{\\phi}(s', a)\\right) - Q_{\\phi}(s,a)\\right)^2\\right].$   (1)\nwhere r is the reward, s is the state of the environment, a denotes a possible action that the agent can take, Q(s, a)\nis the Q-Function and \u03c6 and \u03c6' are the set of weights of the primary and target network, respectively. The loss\nfunction L(4) quantifies the residual of the Bellman equation, which defines formally the optimal values of the\nQ-values, (Sutton and Barto, 1998). The set of weights from the target network, d', is updated using a soft\nupdate rule to enhance the stability of the learning process, (Lillicrap et al., 2015).\n$\\phi' \\leftarrow \\tau \\phi + (1 - \\tau)\\phi'.$ (2)\nTo train the DDQN, an experience replay buffer is utilized. During the training phase, the agent interacts with"}, {"title": "", "content": "the environment and stores the experiences (state, action, reward, next state) in the replay buffer. Subsequently,\nrandom batches of experiences are sampled from the replay buffer to train the network and update its weights.\nThis process helps to break the correlation between consecutive samples and improves stability during the\nlearning process.\nAn additional consideration in solving this reinforcement learning problem is the need to balance maxi-\nmizing power output with minimizing noise impact. These objectives are inherently conflicting, placing this\nproblem within the Multi-Objective Reinforcement Learning (MORL) framework. MORL extends traditional\nreinforcement learning to handle problems involving multiple, often conflicting, objectives.\nVarious strategies exist for addressing MORL problems. One of the simplest methods is to define the\nreward using a scalarized function that combines the rewards for each objective into a single global reward,\nthus transforming the problem into a single-objective reinforcement learning task (Van Moffaert et al., 2013).\nAnother approach involves Pareto optimization, which aims to find a set of optimal policies that lie on the Pareto\nFront, where no other policy is superior in all objectives (Van Moffaert and Now\u00e9, 2014). There are already\nmethods that apply these MORL approaches using deep learning implementations (Mossalam et al., 2016). In\nthis work, a scalarized method is adopted to define a reward that balances the two objectives of maximizing\npower and minimizing noise."}, {"title": "2.2.2. State-action structure", "content": "The state of the agent must include all the necessary information about the environment to enable the agent to\ntake the best possible action. If the state lacks relevant information, the agent may not be able to achieve optimal\nperformance. The state of the agent is defined by the incoming wind conditions, specifically the wind speed, $U_\\infty$,\nalong with the control variables of the wind turbine, which are the rotational speed, \u03a9, and the blade pitch angle,\n\u03b8. To fit within the DDQN framework, the state space S needs to be bounded. Some variables (rotational speed\nand pitch) are bounded by mechanical/structural limitations, whereas the wind speed is bounded by physical\nrange. Note that these can be tuned for specific wind turbines and geographic sites. We include an additional\nconstraint on the tip speed ratio $\u03bb = \\frac{\\Omega R}{U}$, with the blade radius R = 46.5 m, to ensure the correct behavior of the\nBEMT solver. The specific values of all the constraints are outlined below:\n\u2022 $U_\\infty \\in [4,16]$ m/s,\n\u2022 \u03a9\u2208 [6, 18] rpm,\n\u2022 \u03b8\u2208 [\u22125, 10] degrees,\n\u2022 \u03bb\u03b5 [3, 12].\nThe actions available to the agent involve either increasing or decreasing the control variables. Since Q-learning\nis defined for a discrete action space A, the control variables can only be adjusted by fixed increments. Five\ndistinct actions are defined: two for each control variable (one for increasing and one for decreasing), and one for\nmaintaining the current state (doing nothing). The specific fixed increment for each possible action is determined\nbased on the sensitivity analysis detailed in Section 2.1.2. Since the rotational speed is more sensitive to both\npower generation and sound pressure level compared to the pitch angle, the incremental adjustments for each\nvariable has been designed so that their corresponding actions have effects of the same magnitude. The actions\nthat the agent can take are specified as follows:\n\u2022 a\u2081: increase \u03a9 by 0.5 rpm,\n\u2022 a2: decrease 2 by 0.5 rpm,\n\u2022 a3: increase @ by 1 degree,\n\u2022 a4: decrease @ by 1 degree,\n\u2022 a5: do nothing.\nIt is important to note that the transition between states is not deterministic a priori. Although we can freely\nadjust the control variables, the wind conditions depend on the environment and are beyond our control. This\nmotivates the use of a model-free reinforcement learning method, as model-based approaches only guarantee\nconvergence if the transition function between states is known."}, {"title": "2.2.3. Reward definition", "content": "The reward function is key when defining the RL algorithm, as it is the only feedback to quantify how\nsuccessful are the actions taken by the agent. Therefore, the reward must be carefully crafted for each specific\nproblem to learn an appropriate policy. As mentioned in Section 2.2.1, this is a multi-objective optimization\nproblem (or MORL), requiring a specific strategy to address the two conflicting objectives: maximizing power\nextraction while minimizing noise generation. In this work, we choose to blend the two objectives through a\nlinear function, to define the overall reward. The reward can be expressed as follows:\n$r = r_{pw} + r_{SPL},$ (3)\nwhere $r_{pw}$ denotes the reward associated to the power objective and $r_{SPL}$ the one related to the SPL one.\nAs we already discussed in our previous work Soler et al. (2024), the reward power component should\nencourage the agent to obtain the highest energy generation possible, regardless of the wind conditions. To\nachieve this, we use the power coefficient Cp of the wind turbine. We set this reward to increase linearly from\n0 to 1, with 1 corresponding to the maximum possible value of the power coefficient within the state space,\n$C_{p,nom}$. Therefore, the reward power component reads,\n$r_{pw} = \\frac{C_p}{C_{p,nom}}$  (4)\nThe reward term related to sound generation, $r_{SPL}$, is highly dependent on the specific problem being modeled.\nFirst, we need to select the observer locations where the SPL is computed, typically in critical areas where noise\nmitigation is a priority. In this work, we decide to set one observer 100 m downstream the wind turbine, see\nFigure 4. Next, we decide how to penalize the sound generation (SPL) in the reward function. We opt to use a\nReLU activation function that begins penalizing once the SPL exceeds a certain threshold, SPLthr. Below this\nthreshold, the agent focuses solely on maximizing power. Additionally, we define a AdB value that specifies\nhow much the SPL threshold can be exceeded before the reward becomes -1. Beyond this point, no matter\nhow much power the agent generates, the total reward will be negative. Therefore, SPLthr + AdB serves as an\neffective noise limit. For this specific application, we defined SPLthr = 45 dB and AdB = 5 dB, but note that\nthese values can be adapted to specific sites or regulations. The reward noise component can be seen in Figure 5\nand reads as follows:\n$r_{SPL} = -ReLU\\left(\\frac{SPL - SPL_{thr}}{AdB}\\right).$   (5)"}, {"title": "", "content": "In addition, we need to include the bounds of S in the reward. To make the agent learn the limits, it receives\npunishments whenever it performs a forbidden action, that is, an action that leads to a state $S_{t+1} \\notin S$. In such\ncases, the agent receives a negative reward with a value of r = -3 and the action is revoked so that the control\nvariables remain the same. The punishment is set to -3 to differentiate it from the possible negative reward of\n$r_{SPL}$. This distinction is made because exceeding the S limits is considered worse than generating noise above"}, {"title": "", "content": "the threshold. Finally, the reward function for the agent is the following\n$r(S_t, A_t, S_{t+1}) = \\begin{cases}\n\\frac{C_p(S_{t+1})}{C_{p,nom}} - ReLU\\left(\\frac{SPL(S_{t+1}) - SPL_{thr}}{AdB}\\right), & \\text{if } S_{t+1} \\in S \\\\\n-3 & \\text{if } S_{t+1} \\notin S.\n\\end{cases}$   (6)"}, {"title": "2.2.4. Neural Network architecture", "content": "When using DQN, neural networks (NN) are employed to approximate the Q-Function. Typically, the NN\nis designed to approximate the Q-Vectors, q(s), which represent the Q-Values in the state s for all possible\nactions. That is, $q(s)_i = Q(s, a_i)$. This approach is used because A is a discrete space, and encoding these\ndiscrete actions as inputs can be problematic; it is more convenient to create a mapping between real subspaces.\nThe neural network map is defined as $q_\\phi(s) : S \\subset \\mathbb{R}^3 \\rightarrow \\mathbb{R}^5$, where \u03c6 denotes all the NN weights, the output\nspace dimension is |A| = 5 and s denotes the state vector, which is s = [$U_\\infty$, \u03a9, \u03b8]T.\nThe neural network architecture employs a Multi-Layer Perceptron structure, consisting of two dense hidden\nlayers with Rectified Linear Unit (ReLU) activation functions. The final layer, uses a linear activation function\ninstead of ReLU. This allows the Q-values to take on any sign, rather than being restricted to positive numbers.\nThe number of dense layers and their sizes were determined through extensive trial and error. Ultimately,\ntwo layers with 128 and 64 neurons, were found to be sufficient to accurately represent the Q-Function. The\narchitecture of the Q-Network used to train the DDQN agent is shown in Figure 6."}, {"title": "2.2.5. Training of the Wind Turbine DDQN Agent", "content": "The main ideas of Q-Learning have already been explained in Section 2.2.1. However, here the specific\ndetails of the DDQN training to design the wind turbine controller are included. During the training phase,\nthe agent faces random steady wind conditions during short episodes of 20 time steps. This allows the agent\nto adapt to virtually any wind, even if the wind speed changes faster than 20 time steps. This adaptability is\nachieved because experiences are stored in the replay buffer, and batches are selected randomly. Consequently,\nthe specific temporal evolution of states during the agent's experience is not critical, provided that the stored\ntransitions comprehensively represent all actual transitions in the system.\nThe Double Deep Q-Network (DDQN) agent is trained using the hyperparameters listed in Table 2. To illus-\ntrate the influence of each parameter on the training process, a pseudocode for the DDQN training is presented\nin Algorithm 1. The effectiveness of the learning progress during training is assessed by displaying the Q-values\nof the state-action pairs encountered by the agent, as shown in Figure 7. As the agent learns, the Q-values of\nthe actions taken at each state are expected to increase, as depicted in the figure. For the implementation, we\nutilized OpenAI Gym (Towers et al., 2023) to create the environment, serving as a bridge between the reinforce-\nment learning formulation and the OpenFAST wind turbine solver. TF-Agents (Guadarrama et al., 2018) was\nemployed to develop the agent and manage the entire training process. All neural networks were constructed\nusing the Keras API (Chollet et al., 2015), and the Adam optimizer was used for training (Kingma and Ba,\n2014)."}, {"title": "3. Results", "content": "The agent's performance is assessed under various wind conditions. First, we validate the operational point\nthat the agent reaches under constant wind conditions, assessing its optimality via a Pareto front. Next, we\nevaluate the agent's ability to adapt to turbulent wind conditions, comparing its control strategy against a classic\ncontroller. Finally, we estimate the agent's annual energy production and compare it against a classic control\nstrategy designed to maximize energy extraction."}, {"title": "3.1. Steady wind validation", "content": "The simplest test for evaluating the agent is to assess its performance under steady wind conditions. In\nthis scenario, with unchanging wind conditions, the agent should identify, reach and maintain the state that\nmaximizes the cumulative reward. This optimum state does not depend on the initial conditions of pitch and\nrotor speed, as the wind conditions are steady. However, it is important to note that the agent's actions are\ndiscrete, limiting its ability to reach every possible state.\nTo validate the performance and robustness of the agent, a Pareto diagram is used. The agent is tested for\ndifferent initial conditions (with the same steady wind speed) to determine if it can consistently reach optimal\nstates (at the pareto front), regardless of the initial state. To illustrate the agent's trajectory (sequence of state-\naction-reward) for each initial condition, these trajectories are displayed on a power coefficient - sound pressure\nlevel diagram, along with values for 1000 random states from S. Figure 8 presents the Pareto diagram, illustrat-\ning the agent's trajectory from four different initial conditions. It is clear that, regardless of the initial condition,\nthe agent successfully achieves high power outputs up to the maximum permissible decibel level. Furthermore,\nthe agent demonstrates robust performance by consistently avoiding the maximum limit of SPL (dB A) while\nremaining close to the limit to maximize power. It can be seen that the RL does not always reach the same final\nstate, but that the optima are relatively close to each other. This suggests the existence of local optimum. In\naddition, the discrete nature of the Q-learning actions, may not allow the agent to reach certain optima, since\nnot all states are reachable from an initial state. Despite these issues, the agent consistently avoids acoustic\npenalties and achieves high power outputs, with power coefficients ranging from 0.26 to 0.30.\nFor completness, Table 3 shows the initial conditions of the control variables for the trajectories displayed\non Figure 8, as well as the final state control variables."}, {"title": "3.2. Control Strategy for Experimental Winds", "content": "The agent capabilities are now tested over experimental wind conditions. We compare the energy extraction\nbetween our agent and two controllers that are designed solely to maximize power. By doing so, we can\ndemonstrate how much power we need to sacrifice to keep the wind turbine at acceptable decibel levels. The\nperformance of the three controllers are going to be compared. These controllers include:\n\u2022 Classic wind turbine controller: Standard wind turbine controller designed to reach the power curve of\nthe wind turbine, using torque or pitch control depending on whether the wind speed is above or below\nrated wind speed. Details can be seen in Appendix B.\n\u2022 Power DDQN: Agent designed to maximize solely power. It is trained with no noise penalization, that is\nonly the power reward is included, see eq. (4).\n\u2022 Quiet DDQN: Agent designed to maximize power without producing more that 45 dB decibel levels at\n100 m downwind of the rotor. It is trained with the complete reward definitions including power and\nnoise, see eq. (6).\nThe wind data used to validate the control performance under real wind conditions was obtained from the\nmeasurement and instrumentation data center (MIDC) of NREL, see Jager and Andreas (1996). These daily\nwind measurements are available as open-source. For this study, wind speed and wind direction measurements\nat an 80 m height from June 1, 2023, to June 1, 2024, are selected. Figure 9a displays a wind rose illustrating\nthe wind speed and wind direction of this dataset. Since this study concentrates on torque-pitch control, it is\nassumed that the incoming wind is consistently aligned with the wind turbine, a condition typically managed\nby yaw control. Therefore, we assume perfect alignment and only the wind speed distribution is used in the\nsubsequent results.\nNote that the Power DDQN agent considers power optimization uniquely. Therefore, it is only applicable\nto the below rated wind speed region defined on Appendix B. When the wind speed exceeds the rated value, the\ncontrol strategy maintains nominal power rather than maximizing it. To achieve this behavior with a reinforce-\nment learning agent, the reward function would need to be modified. Consequently, the wind speed distribution\nused to validate the agent under turbulent wind conditions is restricted to below-rated wind speeds, enabling a\nmeaningful comparison between the three controllers.\nThe control performance of the three agents is analyzed in detail over an 8-hour time span, using the wind\nspeed distribution shown in Figure 9b. The RL controllers are allowed to control each minute. In the next\nsection, we will estimate the annual energy production for all controllers."}, {"title": "", "content": "acoustics of the wind turbine, both generate high levels of noise when the wind speed is sufficiently high. In\ncontrast, the Quiet agent can match the power generation of the power-oriented controllers when the wind speed\nis moderate. When wind speeds are higher, it extracts as much power as possible while keeping noise levels\nbelow the threshold value. Moreover, all three controllers maintain a constant pitch value to maximize power\nextraction. However, the Quiet agent adjust the pitch angle to reduce the noise levels when the wind speeds get\nhigher."}, {"title": "3.3. Annual wind energy estimation", "content": "There are different methodologies to obtain an estimate of the annual generation of wind energy, (Garcia-\nBustamante et al., 2009). The standard procedure is based on decoupling the wind turbine from the wind\ndistribution of the particular site. It considers the observed wind speed frequency histogram to fit a theoretical\nprobability density function (PDF) for the wind speed. It also requires a transfer function that models the\nrelation between power output and wind speed. Typically, the Weibull distribution is used to fit the wind speed\nfrequency histogram. Garc\u00eda-Bustamante et al. (2008) showed that although the Weibull distribution may not\nbe substantiated for most sites, it does not include important errors on theenergy estimations. The probability\ndensity function of the Weibull distribution is given by:\n$f_v(u) = \\left(\\frac{k}{c}\\right)\\left(\\frac{u}{c}\\right)^{k-1} exp\\left(-\\left(\\frac{u}{c}\\right)^k\\right),$ (7)\nwhere U denotes the random variable that models the wind speed. The fit of the shape and scale parameters k\nand c are established from the mean and variance of the wind speed, $\u03bc_U$ = E[U] and $\u03c3_U^2$ = V[U]. The specific"}, {"title": "", "content": "relations can be seen in Spiru and Simona (2024) work and are the following:\n$k = \\left(\\frac{\u03c3_U}{\u03bc_U}\\right)^{-1.086}$  (8)\nand\n$c = \\frac{\u03bc_U}{\u0393(1 + \\frac{1}{k})}$  (9)\nwhere \u0393 denotes the special gamma function. This formulation can be employed to obtain the Weibull probabil-\nity density function that represents the one-year experimental data reported by Jager and Andreas (1996). This\nis illustrated in Figure 11."}, {"title": "", "content": "Regarding the transfer function between power and wind speed, various strategies exist (Abolude and Zhou,\n2018). The Theoretical Power Curve (TPC) does not account for control mechanisms. Furthermore, since our\nwind turbine control strategy considers acoustic generation, the wind turbine will exhibit a significantly different\nEffective Power Curve (EPC). It is necessary to compute an EPC that accurately represents the transfer function\nbetween power and wind speed for our specific control scenario. The EPC can be computed using simulations\nof the wind turbine control. The agent is faced against a turbulent wind that covers all the range of interest\nof wind speed, mainly between cut-in and cut-off wind speed. This turbulent wind must be representative of\nthe turbulent nature of the wind that the wind turbine is going to face during operation. Once the simulation\nis done, all the pairs of data points (U\u221e, Cp) can be used to obtain a transfer function for the power coefficient\nCp(U). A subset of 100 hours of the experimental wind measurements from the MIDC (Jager and Andreas,\n1996) has been used to obtain the EPC of the SWT2.3-93 wind turbine using the Classic Control and the Quiet\nDDQN agent already introduced on Section 3.2. Figure 12 illustrates the results of this simulations, showing\nthe operational laws of control for each agent on Figures 12a and 12b and the SPL and power associated on\nFigures 12c and 12d respectively. The behavior is as expected, the Quiet Agent does not increase the rotational\nspeed above 10.5 rpms to avoid surpassing the SPL threshold and uses the pitch to reduce noise if needed, which\nexplains the high variance bars on the pitch (see fig. 12b) and low ones in the rotational speed (see fig. 12a).\nMeanwhile the classic control can increase the rotational speed freely and the pitch is only use in above-rated\nwind speed scenarios, see Appendix B. The large standard deviations on the classic control pitch are due to the\nPID control, which is dynamically adjusting to the turbulent wind. In Figure 12c it is illustrated how the classic\ncontrol matches on average the TPC. However, it is not able to adjust perfectly to the turbulent wind, showed\nby its high variance on the above-rated wind speed region. The Quiet agent achieves less power than the classic\none but is able to maintain the sound pressure level below the specified threshold of 45 dB A, see fig. 12d.\nTraditional approaches use only the average value or polynomial fits of the historical/simulated data to\nconstruct the EPC. All these methods do not capture the variance of the data in the model. To account for this\nvariability on the EPC model we introduce a statistical method. For simplicity, we model the power coefficient\nCp(U\u221e), which is obtained by non-dimensionalizing the EPC data. A Gaussian Process Regression (MacKay\net al., 1998) can be employed to model the power coefficient at each wind speed as a Gaussian probability\ndistribution, $C_p(U_\\infty) \\sim N[\\mu_{cp}(U_\\infty), \u03c3_{c_p}(U_\\infty)]$. Figure 13a shows the power coefficients points obtained after\nthe wind turbine control simulation. This data is used to fit the Gaussian Process (GP) model and obtain the"}, {"title": "", "content": "mean and standard deviation of the power coefficient as functions of the wind speed, this fit is also included in\nFigure 13a. The C, distribution for specific values of the wind speed is illustrated on Figure 13b where it is\ncompared with the histogram of the power coefficient from the data."}, {"title": "", "content": "Assuming the Weibull probability distribution for the wind speed U\u221e combined with the GP model for the\nCp distribution, the estimation of the annual wind energy can be performed by computing the expectation of\nthe wind energy Ew. Mathematical details on the statistical distributions are provided in Appendix A. Table 4\npresents the annual energy estimation for each control strategy. It is important to note that the Power DDQN\ncontroller is applicable only in the below-rated wind speed region. Therefore, when computing the annual wind\nenergy generation with this control, we impose nominal power for wind speeds above the rated value. The\nPower DDQN controller is included in the comparison to ensure that it remains competitive with the Classical"}, {"title": "", "content": "Control within the below-rated wind speed range. The Quiet DDQN controller is able to control the wind\nturbine without surpassing the sound pressure level threshold selected and obtains an 78% of the annual energy\nproduction compared to the Classical Control. Additionally, in Table 4 it is shown the average standard deviation\nacross the wind speed for the power coefficient GP fit. There, it is shown that the Quiet agent exhibits the control\nstrategy with the least variance."}, {"title": "4. Conclusions", "content": "In conclusion, integration of reinforcement learning with wind turbine control holds promise for optimizing\nenergy generation and efficiency while minimizing acoustic environmental impact. A DDQN reinforcement\nlearning agent can replicate the control strategy of a standard wind turbine controller without prior explicit\nknowledge of the wind turbine, relying solely on a wind turbine solver for experiential learning. Moreover,\nadvanced control strategies can be readily implemented by modifying the reward function. In this work, an RL\ncontroller is defined to maximize power output while maintaining acceptable decibel levels, thereby incorporat-\ning acoustic effects into the control strategy. This demonstrates that MORL is capable of dynamically balancing\ntwo different objectives effectively.\nAn effective power curve is computed from control simulations of turbulent wind data. This allow to charac-\nterize the reinforcement learning control strategy, obtaining the operational laws and obtaining an annual wind\nenergy estimation. The methodology is validated using a SWT2.3-93 wind turbine with a rated power of 2.3\nMW. We evaluate the yearly energy production for a realistic site. The DDQN reinforcement learning control\nprovides similar energy production that a traditional control. The methodology presented allows for the inclu-\nsion of noise limits leading to a 22% reduction in the annual energy extraction when activating a maximum\nallowed noise of 45 dB (100 meters downwind downwind of the turbine).\nFurther research directions include investigating Multi-Agent Reinforcement Learning algorithms for coop-\nerative control of wind turbines within farms, which could enhance overall system performance while control-\nling noise at the farm level."}, {"title": "Appendix A. Statistical details for the EPC model", "content": "Let us consider a two dimension random variable X = (Cp, U). This random variable model the probabil-\nity of obtaining a certain wind speed with a certain power coefficient. The wind speed marginal distribution\naccounts for the global wind speed distribution of the localization of the wind turbine. Meanwhile, the power\ncoefficient distribution measures the performance of the wind turbine at different wind speeds.\nThere exist an a priori unknown joint probability density f(cp, u). The power generated by the wind turbine,\nP, is a function of this random variable, so it is itself a random variable,\n$P = \\frac{1}{2} \\rho A C u^3$\nThe wind energy, Ew, that the wind turbine extracts from the wind for a given period can be written as\n$E_w = \\int P dt$. However, this statistical model does not include information about the temporal evolution of Cp\nand U. We can compute the expectation of the wind energy using the expectation of the power over a period of\ntime.\n$E[E_w] = \\Delta T E[P] = \\Delta T \\int \\int \\frac{1}{2} \\rho A C u^3 f(c_p, u) dc_p du,$ (A.1)\nwhere AT denotes the period of time. Notice that this only make sense if the unknown wind speed evolution\nU\u221e(t) fits in the annual distribution modeled by the random variable U.\nAlthough we do not know the joint PDF, we know that the wind speed random variable U follows a Weibull\ndistribution. Therefore, the marginal probability density function of U, fu(u) is a weibull PDF that follows\neq. (7).\nOn the other hand we can obtain the distribution of power coefficient for each wind speed value. This would\nbe the conditioned power coefficient PDF, that is $f_{c_p}(c_p|U = u)$. From this two PDF we can obtain the joint\nPDF, using the following relation:\n$f_{c_p}(c_p | U = u) = \\frac{f(c_p, u)}{f_U(u)}$ (A.2)\nIn this work, the conditional power coefficient PDF is obtained using a Gaussian Process Regression algorithm.\nTherefore, its density function is the following:\n$f_{c_p}(c_p | U = u) = \\frac{1}{\\sqrt{2 \\pi \u03c3_{c_p}(u)^2}} exp(\\frac{c_p - \u03bc_{c_p}(u)}{\u03c3_{c_p}(u)} ^2)$ (A.3)\nwhere the mean $\u03bc_{c_p}(u)$ and standard deviation $\u03c3_{c_p}(u)$ are obtained from the wind turbine control simulations.\nFinally, the expectation of the wind energy can be compute as follows:\n$E[E_w] = \\Delta T E[P] = (\\frac{1}{2} \\rho A) \\int_{U_{in}}^{U_{off}} (\\int_0^{C_{p,nom}} c_p f_{c_p}(c_p | U = u) dc_p) u^3 f_U(u) du$ (A.4)\nNotice that the inner integral is the expectation of the conditional distribution, $E[C_p | U = u] = \u03bc_{c_p}(u)$. Hence,\nthe expectation of the wind energy only requires the mean of the distribution fitted by the GP. The variance of\nthe control, $\u03c3_{c_p}(u)^2$, has no influence on the estimation of the wind energy. However, it gives us information\nabout the control and can be useful to measure, this can be done computing the expectation of the variance.\n$E[\u03c3_{c_p}(U)] = \\int_{U_{in}}^{U_{off}} \u03c3_{c_p}(u) f_U(u) du$"}, {"title": "Appendix B. Standard Wind Turbine Control Strategy", "content": "The control strategy for variable-speed horizontal-axis wind turbines can be divided into four regions based\non wind speed. Although each region definition may vary depending on the specific control design, the funda-\nmental objectives within each region are as follows:\n\u2022 Region I: When the wind speed is below the cut-in value, the turbine cannot operate.\n\u2022 Region II: At wind speeds above the cut-in threshold but below the rated speed, the primary objective is\nto optimize power generation. This is achieved by adjusting the rotor speed to align with the power curve\nof the wind turbine, utilizing a predetermined lookup table.\n\u2022 Region III: When wind speeds exceed the rated value, the focus shifts to maintaining a consistent rotor\nspeed across a broad range of wind velocities. This is typically achieved through adjustment of the blade\npitch, commonly implemented using a proportional-integral-derivative (PID) control strategy, although\nthere are more sophisticated approaches (Gambier and Yunazwin Nazaruddin, 2018).\n\u2022 Region IV: When the wind speed surpasses the cut-off value, the turbine must be shut down for safety.\nThe transition between regions II and III, sometimes referred to as Region II \u00bd, is characterized by maintaining a\nconstant rotor speed. Although there are different options depending on the specific control design. Figure B.14\nillustrates these regions on the power curve of the wind turbine. Further details on classical wind turbine control\nstrategies can be found in the works of Burton et al. (2011) or \u00c5str\u00f6m and H\u00e4gglund (2006).\nThe controller module in OpenFAST facilitates the customization of controllers. In this study, the wind\nturbine controller is derived from the OpenFAST implementation from Mulders and Van Wingerden (2018),\ntailored to suit the characteristics of the SWT wind turbine."}]}