{"title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design", "authors": ["Sakhinana Sagar Srinivas", "Venkataramana Runkana"], "abstract": "Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes. Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models. Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task. Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions. Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.", "sections": [{"title": "1 Introduction", "content": "Molecule design is an interdisciplinary approach that involves identifying a target molecule or property to enhance, such as a drug with increased efficacy or a material with superior characteristics. Advancements in science and technology have accelerated the discovery and development of novel drugs, advanced materials, and innovative chemical processes. This iterative process begins with (a) identifying a target molecule or property to improve, followed by (b) employing computational methods to explore the vast chemical space and optimize potential candidate structure and composition. The cycle continues with (c) synthesizing and testing promising candidates in the laboratory until the desired characteristics are achieved. The transformer architecture[28] has revolutionized various fields in computer science, including language understanding[5], text generation[17, 2], image understanding[7], and multi-modal generation[19, 23]. Utilizing this architecture to scale language models has established itself as a universal approach for enhancing generalization performance. In recent times, the emergence of foundational Large Language Models (LLMs)[2, 3, 27], which are built upon transformer architectures, has significantly revolutionized performance in various natural language processing tasks by enabling enhanced linguistic comprehension and logical reasoning abilities. Different learning strategies such as Zero-Shot Chain of Thought (Zero-shot-CoT[29]) and Few-Shot (In-Context) Learning (Few-shot-ICL[22, 6]) are utilized to leverage the emerging abilities of general-purpose LLMs for a wide variety of specialized tasks across various domains. The former employs task-specific instructions without relying on downstream task-based demonstrations, utilizing the inherent knowledge that the language model acquired during training to generate outputs. In contrast, Few-shot-ICL supplements instructions with a handful of demonstrations, presented as input-output pairs, to foster contextual understanding and facilitate task-specific adaptation, thereby generating relevant output. Recently, there has been a surge in the evolution of generative AI, such as \u201cDALL\u00b7E", "Make-A-Video\"[24": "from Meta AI \u2014 a text-to-video diffusion model that generates realistic, engaging, and creative videos from text, among others. Inspired by recent developments in next-generation AI, \u201cText-Based Molecule Design", "FrontierX": "LLM-MG, where the goal is to task LLMs with a knowledge-infused prompt that consists of a few demonstrations(input-output pairs) for the text2mol task, along with task-specific instructions, where the output is chemical SMILES representations of the corresponding query technical descriptions. Our experiments on benchmark datasets provide empirical evidence supporting the framework's effectiveness in text-based molecule design tasks."}, {"title": "2 Proposed Method", "content": "The Large Language Models (LLMs), such as ChatGPT[2], Meta's LLaMA[27] \u2014 that have been pre-trained on large text corpora and operate based on a \u201cprompt and predict", "pre-train, fine-tune\" approach, offer more affordable flexibility for fine-tuning with minimal labeled data and provide access to logits or token embeddings, aiding interpretability. While smaller LMs can learn complex patterns, they often fall short in reasoning and generalization abilities compared to LLMs, which generate more coherent and contextually relevant responses. To alleviate resource constraints, Language Modeling as a Service (LMaaS[25": "offers access to LLMs through text-based API interactions, while remaining scalable and cost-effective. However, the potential of LLMs for text-conditional de novo molecular generation tasks remains largely underexplored. Our proposed approach for the text2mol task leverages LLMs by utilizing: (a) their predictive ability to provide a top-R ranked list of chemical SMILES representations; and (b) their generative ability to offer auxiliary explanations as justifications for their predictions by conditioning on the augmented prompt. Furthermore, we fine-tune two different small-scale LMs using (a) generated explanations from LLMs and (b) input technical descriptions of molecules to compute their respective contextualized token embeddings which capture semantic coherence and contextual relevance for text-to-molecule generation tasks. We utilize weighted attention mechanism to compute both original and explanatory text-level embeddings from their respective context-aware token embeddings. In addition, we transform the LLMs' top-R predictions of chemical SMILES representations into predictive embeddings. We use a hierarchical multi-head attention mechanism to integrate various embeddings into unified cross-modal embedding for input into a transformer decoder, generating the chemical SMILES representation."}, {"title": "Evaluation LLMs & LMs", "content": "In this work, we evaluated three popular LLMs: text-davinci-003, ChatGPT, and Google BARD, in order to thoroughly compare their distinct strengths. text-davinci-003 was the earliest LLM released by OpenAI and was tailored for a broad spectrum of linguistic tasks. GPT-3.5-turbo is a substantial improvement over the GPT-3 base models, demonstrating remarkable performance on a wide range of linguistic tasks while also being cost-effective. Google BARD[1] stands out due to its extraordinary scale, complexity, and an impressively extensive vocabulary compared to the GPT-3.5 models. In addition to these, our study also incorporates a pre-trained smaller LM, DeBERTa, which is an improved version of the BERT[5] architecture."}, {"title": "Knowledge-Augmented Prompts", "content": "In our work, we offer essential context and task-specific instructions by using input natural language descriptions of the target molecule to prompt LLMs in a zero-shot setting to generate corresponding chemical SMILES representations. In this scenario, the primary task-specific instructions involve the translation of these descriptions into chemical SMILES representations. We create an augmented prompt that incorporates both the task-specific instructions and a few demonstrations. These demonstrations, which establish the context, are grounded in the downstream text2mol task and comprise input-output pairs (i.e., technical descriptions and their corresponding chemical SMILES representations). This approach facilitates knowledge-augmented prompting of the LLMs for zero-shot text-to-molecule generation tasks. The construction of an augmented prompt involves sampling text-molecule pairs from the training data that are relevant to the target molecule descriptions. We then prepend these pairs to the task-specific instructions to form an augmented prompt, which is used to query the LLMs in a zero-shot setting for the generation of chemical SMILES representations. To evaluate the impact of the quality and quantity of sampled text-molecule pairs on the performance of text-conditional de novo molecule generation tasks, we employ two different sampling strategies. The quality of these pairs is determined by the sampling methods used to identify pairs similar to the target molecule descriptions. We navigate through the training dataset using two semantic search-retrieval methodologies - random and scaffold to sample text-molecule pairs relevant to the target molecule descriptions. The random approach involves arbitrarily sampling K text-molecule pairs from the training dataset. In contrast, the scaffold technique employs semantic similarity methods, specifically text-embedding-ada-002 from OpenAI, to evaluate the similarity between the molecular textual descriptions in the training dataset and the target molecule descriptions. It then selects the top-K most relevant text-molecule pairs, where the hyperparameter K is set using a random search technique. We employ the different sampling strategies to analyze the effectiveness of augmenting prompts with relevant text-molecule pairs in language-conditioned molecule generation tasks. In short, unlike traditional supervised learning, LLMs (a) predict the chemical SMILES representations and (b) generate textual explanations for their predictions, utilizing the inherent knowledge embedded within the language model's parameters, all conditioned on the augmented prompt, without needing any parameter updates."}, {"title": "Querying LLMS", "content": "We access LLMs with LMaaS[25] platforms via text-based API interaction, necessitating solely text-based input and output. We create a customized zero-shot prompt template to query LLMs to translate textual descriptions into chemical SMILES representations. The LLMs' response serves the dual purpose of (a) providing detailed textual explanations for the underlying rationale, (reasoning or logic), behind the predictions and (b) generating a list of the top-R ranked chemical SMILES representations. Subsequently, we fine-tune smaller downstream LMs using the generated auxiliary explanations. The custom augmented prompt format is as follows:\nBelow are the textual descriptions \u2013 chemical SMILES representation pairs. Generate the chemical SMILES representation for the textual description provided below.\nQuerying LLMs (a) predicts the top-R ranked chemical SMILES representations and (b) provides auxiliary explanations as logical justifications for its predictions."}, {"title": "(LLMs Response) [top-R ranked predictions \u2013 Auxiliary Explanations]", "content": "In the next section, we will discuss the use of auxiliary explanations and original textual descriptions for fine-tuning various downstream smaller LMs for domain customization. Later, we will transform the LLMs top-R predictions of chemical SMILES representations into predictive embeddings.\nFine-tuning LMs for Domain-Specific Customization: Our novel approach leverages the integration of a smaller language model (LM) to extract relevant information from the original molecular textual descriptions and auxiliary explanations generated by LLMs, thereby aiding downstream tasks. The intermediary LM serves as a bridge between the LLM and the downstream layers that generate chemical SMILES representations. To elucidate further, we fine-tune pre-trained LMs, denoted as LMexp and LMorg, to compute context-aware token embeddings by passing the text sequences generated by LLMs (referred to as Sexp) and original textual descriptions (referred to as Sorg) through the LMexp and LMorg models, respectively, as described below:\n$h_{\\text{exp}} = \\text{LM}_{\\text{exp}}(S_{\\text{exp}}) \\in \\mathbb{R}^{(m \\times d)}$; $h_{\\text{org}} = \\text{LM}_{\\text{org}}(S_{\\text{org}}) \\in \\mathbb{R}^{(n \\times d)}$\nwhere both contextualized embeddings $h_{\\text{exp}}$ and $h_{\\text{org}}$ capture not only the contextual information of the tokens but also encapsulate the semantic relationships among tokens within their respective textual content. Here, m and n represent the number of tokens in $S_{\\text{exp}}$ and $S_{\\text{org}}$, while d represents the token embedding dimension. We employ a softmax attention mechanism to compute a weighted sum of the contextualized token embeddings, encoding the auxiliary explanations and original textual descriptions into single fixed-length vectors or embeddings denoted as $Y_{\\text{exp}}$ and $Y_{\\text{org}}$ and computed as follows,\n$\\alpha_i = \\text{softmax}(q_i); q_i = u^T h_{\\text{exp}}^i \\\\ \\beta_i = \\text{softmax}(r_i); r_i = v^T h_{\\text{org}}^i$\n$Y_{\\text{exp}} = \\sum_{i=0}^{m} \\alpha_i h_{\\text{exp}}^i \\in \\mathbb{R}^{(d)}$; $Y_{\\text{org}} = \\sum_{i=0}^{n} \\beta_i h_{\\text{org}}^i \\in \\mathbb{R}^{(d)}$\nwhere u and v are differentiable vectors. The explanatory text-level embedding, represented as $Y_{\\text{exp}}$, encapsulates domain-specific knowledge retrieved from foundational LLMs to support its predictions. The original text-level embedding, denoted as $Y_{\\text{org}}$, captures the overall context and semantics within the original textual descriptions by extracting the most pertinent and task-relevant information.\nLLMs Prediction Embeddings: As mentioned earlier, the LLMs not only provide the auxiliary textual explanations but also predict the top-R ranked chemical SMILES representations list, which can be informative. For each target molecule in the text2mol task, the top-R predictions are converted into one-hot encoded vectors $P_{i,1},..., P_{i,R} \\in \\mathbb{R}^{C}$, where C represents the total number"}, {"title": "Cross-modal Attention Layer", "content": "We compute the cross-modal embedding, denoted as $Y_{\\text{cross}}$, using a hierarchical multi-head attention mechanism that integrates the original text-level embedding $Y_{\\text{org}}$, the explanatory text-level embedding $Y_{\\text{exp}}$, and the prediction embedding $Y_{\\text{pred}}$. This mechanism provides a robust framework for integrating diverse information encapsulated from different modalities, addressing several key aspects critical to the performance of cross-modal learning tasks. It involves hierarchical implementation of multi-head attention mechanisms. We employ two layers, each focusing on different aspects of the input embeddings, enabling more complex interactions and potentially leading to more scalable and efficient models. In the initial layer, we apply a multi-head attention mechanism to the mono-domain embeddings, specifically the original text-level embedding $Y_{\\text{org}}$ and the explanatory text-level embedding $Y_{\\text{exp}}$, to obtain unified mono-domain embeddings denoted as $Y_{\\text{uni}}$. In the subsequent layer, we utilize the multi-head attention mechanism on the cross-domain embeddings, comprising the prediction embedding $Y_{\\text{pred}}$ and the unified mono-domain embeddings $Y_{\\text{uni}}$, to compute the cross-modal embeddings denoted as $Y_{\\text{cross}}$. For the first layer, we compute the Query, Key, Value projections for the original text-level embedding $Y_{\\text{org}}$ for each head h as follows:\n$Q_{\\text{org}}^h = Y_{\\text{org}} W_{\\text{Qorg}}^h; K_{\\text{org}}^h = Y_{\\text{org}} W_{\\text{Korg}}^h; V_{\\text{org}}^h = Y_{\\text{org}} W_{\\text{Vorg}}^h$\nSimilarly, the Query, Key, Value projections for explanation text-level embedding $Y_{\\text{exp}}$ for each head h as follows:\n$Q_{\\text{exp}}^h = Y_{\\text{exp}} W_{\\text{Qexp}}^h; K_{\\text{exp}}^h = Y_{\\text{exp}} W_{\\text{Kexp}}^h; V_{\\text{exp}}^h = Y_{\\text{exp}} W_{\\text{Vexp}}^h$\nWe concatenate the keys and values from both original and explanatory text-level embeddings, which provides a powerful way to integrate information from the mono-domain embeddings into a unified, rich representation.\n$K_{\\text{concat}}^h = [K_{\\text{org}}^h, K_{\\text{exp}}^h]; V_{\\text{concat}}^h = [V_{\\text{org}}^h, V_{\\text{exp}}^h]$\nWe use softmax attention to integrate complementary information from the mono-domain embeddings, focus on contextually relevant information, and semantically align them through an attention mechanism. The softmax function is applied to the keys for each query.\n$A_{\\text{uni}}^h = \\text{Softmax} (\\frac{(Q_{\\text{org}}^h + Q_{\\text{exp}}^h) K_{\\text{concat}}^h}{\\sqrt{d_h}})$\nEach head outputs a new vector representation that highlights the most relevant features in the mono-domain embeddings(both original and explanation text-level), according to the attention mechanism for that specific head, which is tailored to capture specific aspects or relationships within the data.\n$O_{\\text{uni}}^h = A_{\\text{uni}}^h V_{\\text{concat}}^h$\nFinally, all the head-specific outputs are concatenated and linearly transformed to create the unified mono-domain embedding as follows,\n$O_{\\text{concat}} = [O_{\\text{uni}}^1, O_{\\text{uni}}^2,..., O_{\\text{uni}}^H]$\n$Y_{\\text{uni}} = O_{\\text{concat}} W_{\\text{Ouni}}$\nwhere $W_{\\text{Qorg}}^h, W_{\\text{Korg}}^h, W_{\\text{Vorg}}^h, W_{\\text{Qexp}}^h, W_{\\text{Kexp}}^h, W_{\\text{Vexp}}^h, W_{\\text{Ouni}}$ are the learnable weight matrices. Here, $d_h$ represents the dimensionality of the key/query/value for each head, and H is the number of heads. $Y_{\\text{uni}}$ denotes the unified mono-domain embeddings. The unified embeddings can learn and integrate complementary, diverse information present in both the $Y_{\\text{org}}$ and $Y_{\\text{exp}}$ embeddings. These unified embeddings facilitate semantic alignment among similar features across different embeddings and enable the identification of contextual relevance between distinct yet related $Y_{\\text{org}}$ and $Y_{\\text{exp}}$ mono-domain embeddings. The next step involves computing the cross-modal embedding $Y_{\\text{cross}}$ using a second layer of a multihead attention mechanism that integrates both $Y_{\\text{pred}}$ and $Y_{\\text{uni}}$. We compute the Query, Key, and Value projections for the prediction embedding $Y_{\\text{pred}}$ for each head h as follows:\n$Q_{\\text{pred}}^h = Y_{\\text{pred}} W_{\\text{Qpred}}^h; K_{\\text{pred}}^h = Y_{\\text{pred}} W_{\\text{Kpred}}^h; V_{\\text{pred}}^h = Y_{\\text{pred}} W_{\\text{Vpred}}^h$\nSimilarly, we compute the Query, Key, Value projections for the unified embedding $Y_{\\text{uni}}$ for each head h as follows:\n$Q_{\\text{uni}}^h = Y_{\\text{uni}} W_{\\text{Quni}}^h; K_{\\text{uni}}^h = Y_{\\text{uni}} W_{\\text{Kuni}}^h; V_{\\text{uni}}^h = Y_{\\text{uni}} W_{\\text{Vuni}}^h$\nWe concatenate the keys and values from both the prediction and unified embeddings, thereby facilitating a robust integration of insights from the cross-domain embeddings into a synergized and enriched representation.\n$K_{\\text{cross}}^h = [K_{\\text{uni}}^h, K_{\\text{pred}}^h]; V_{\\text{cross}}^h = [V_{\\text{uni}}^h, V_{\\text{pred}}^h]$\nWe utilize a softmax attention mechanism to merge and align information from different domains, thereby prioritizing contextually relevant information and ensuring semantic alignment. The softmax function is applied to the keys for each query, described as follows:\n$A_{\\text{cross}}^h = \\text{Softmax} (\\frac{(Q_{\\text{uni}}^h + Q_{\\text{pred}}^h) K_{\\text{cross}}^h}{\\sqrt{d_h}})$\nIn the multi-head attention mechanism, each head processes both embeddings(unified and predictive embeddings) to highlight important patterns, focusing on specific relationships or aspects within the data, enhancing performance in cross-modal learning tasks.\n$O_{\\text{cross}}^h = A_{\\text{cross}}^h V_{\\text{cross}}^h$\nFinally, all the head-specific outputs are concatenated and linearly transformed to create the final cross-modal embedding as follows,\n$O_{\\text{cross}} = [O_{\\text{cross}}^1, O_{\\text{cross}}^2,..., O_{\\text{cross}}^H]$\n$Y_{\\text{cross}} = O_{\\text{cross}} W_{\\text{cross}}$\nwhere $W_{\\text{Quni}}^h, W_{\\text{Kuni}}^h, W_{\\text{Vuni}}^h, W_{\\text{Qpred}}^h, W_{\\text{Kpred}}^h, W_{\\text{cross}}$ are the learnable weight matrices. $Y_{\\text{cross}}$ denotes the cross-domain embeddings. Implementing the hierarchical attention mechanism facilitates the structured integration of information from different modalities. This mechanism employs multi-head attention method, using multiple sets of learned weight matrices to emphasize various aspects or relationships within the data. Consequently, this approach has the potential to foster robust and enriched embeddings capable of capturing complex patterns. Additionally, it aids in focusing on contextually pertinent information and achieving semantic alignment across different embeddings, thereby enhancing the capacity to identify and utilize crucial features in the input data."}, {"title": "Output Layer", "content": "We then utilize a transformer decoder[28] to generate chemical SMILES representations character by character, using cross-modal embeddings ($Y_{\\text{cross}}$) that incorporate global context through the hierarchical multi-head self-attention mechanism. We implement a softmax layer to transform the decoder's output, creating a probability distribution over potential elements for each position in the SMILES strings. For our sequence generation tasks, we minimize the categorical cross-entropy loss to penalize the proposed framework based on the negative log-likelihood of the ground-truth chemical SMILES strings under the predicted probability distribution, thus facilitating the generation of valid molecules. In summary, by integrating multi-modal embeddings, namely $Y_{\\text{org}}$, $Y_{\\text{exp}}$, and $Y_{\\text{pred}}$, our approach enables the concurrent capture of complementary information, ultimately enhancing the overall performance of the framework."}, {"title": "3 Experiments & Results", "content": "Our study utilized the ChEBI-20 dataset[8], a bidirectional text-to-molecule translation dataset comprising 33,010 text description-molecule pairs with a predefined split ratio of 80:10:10 for training, validation, and test sets, respectively. We utilized 26,407 text description-molecule pairs from the training set for demonstrations (input-output mappings) for constructing a knowledge-augmented prompt to query LLMs. We used the MolT5 model[8], as a predominant baseline, which is an encoder-decoder transformer architecture pretrained on a large unannotated dataset specifically for the text2mol translation task, building upon the foundations of the T5[18] model. We evaluated the performance of our proposed framework on the text2mol task, comparing it with several variants of the MolT5[8] and T5[18] models, as well as with general-purpose sequence-to-sequence models, such as the RNN-GRU and Vanilla Transformer models. In addition, various variants of few-shot (ICL) prompting of GPT-based models - reflecting the fact that this technique uses few-shot learning to prompt off-the-shelf GPT-based models to perform molecular property prediction for new, unseen molecules - referred to as baselines, are evaluated for comparison with our proposed framework. The configurations include different variants of the GPT-4 model, namely, (a) the zero-shot approach, (b) the scaffold sampling technique with K=10 or K=5, and (c) the random sampling technique with K=10 for constructing augmented prompts. In addition, we use GPT-3.5 and davinci-003 models, both employing the scaffold sampling technique with K=10 to construct knowledge-augmented prompts."}, {"title": "Evaluation Metrics", "content": "To comprehensively evaluate the quality and similarity of the generated chemical SMILES representations compared to the ground-truth SMILES representations, we employed a range of distinct evaluation metrics, categorized into three types. These metrics include (a) chemical similarity measures, such as the FTS (Fingerprint Tanimoto Similarity) [26] and the FCD (Fr\u00e9chet ChemNet Distance) [16], as well as (b) natural language processing metrics like the BLEU (Bilingual Evaluation Understudy) score, Exact Match [8], and Levenshtein distance [14]. In addition, (c) we utilized the RDKit library [13] to validate the generated molecules. We delineate the metrics as follows: (a) We employ the FTS[26] metric to gauge the chemical similarity between the ground-truth and generated chemical compounds represented as SMILES strings(notation to represent chemical structures as text) by comparing their MACCS, RDK, and Morgan fingerprints[21, 13, 4]. (b) In addition, we utilize the FCD metric[16], which leverages latent information from a pretrained model[16] to predict molecular activity[8]. The FCD is calculated by measuring the distance between the mean embeddings of two sets of chemical SMILES strings (generated and ground-truth) in the latent space of the pretrained model. A lower FCD score indicates a greater similarity between the corresponding two sets of molecules. (c) We also apply natural language processing metrics to evaluate the quality of the chemical SMILES strings generated by our framework. These metrics encompass the following: (i) BLEU this measures the similarity between two text strings, with a higher BLEU score denoting better similarity. (ii) Exact Match[8] \u2013 this quantifies the percentage of generated chemical SMILES strings that are identical to the ground-truth strings. (iii) Levenshtein distance[14] \u2014 this calculates the minimum number of single-character edits required to modify the generated chemical SMILES strings to match the ground-truth strings, with a lower value indicating closer similarity. By utilizing these diverse metrics, we attain a nuanced understanding of the efficacy of our text-conditional de novo molecule generation framework. Higher FTS scores and lower FCD scores signify better chemical similarity and closer molecular activity resemblance, respectively. In the context of matching chemical SMILES strings from the perspective of natural language processing, higher BLEU and Exact Match scores are preferred to achieve better alignment with the ground-truth SMILES strings, while a lower Levenshtein distance indicates fewer required edits, denoting superior similarity. The RDKit library assists in verifying the validity of the generated molecules, with a higher proportion indicating successful generation[13]."}, {"title": "Experimental Setup", "content": "We used the ChEBI-20 dataset[9] with an 80:10:10 split: 80% for training, 10% for validation, and 10% for testing. The training set was utilized to update the learnable parameters, the validation set to select optimal hyperparameters, and the test set to evaluate generalization performance of the proposed framework. Our scalable and efficient framework offers a unified solution for integrating LLMs and LMs. We configured the hyperparameters of our framework with a batch size of 32, trained it for 100 epochs, and a hidden or embedding dimension(d) of 128. Additional hyperparameters include the number of attention heads (H) set to 4, and the dimensionality of Key/Query/Value (dh) is 32. To optimize the training process, we utilized the Adam optimizer [12], initially setting the learning rate to 1e-3. Additionally, we incorporated a learning rate decay scheduler, which reduced the learning rate by half whenever the validation loss did not improve for 10 consecutive epochs. Furthermore, we applied early stopping to prevent overfitting on the validation data. We evaluated our approach using the following LLMs: GPT-4.0, GPT-3.5-turbo, GPT-3.0-text-davinci-003, and Google Bard. In our approach, we chose not to fine-tune hyperparameters individually for each LLM, opting instead to maintain consistent settings across all language models. This strategy simplifies experimentation, ensures uniform conditions, facilitates result comparison, and promotes consistency. Moreover, it underscores the versatility of our framework, which can be used with any off-the-shelf LLM without the need for computationally expensive hyperparameter tuning. We used the Scaffold technique with K=16 to sample demonstrations (input-output mappings) from the training data to construct augmented prompts for querying LLMs in few-shot settings. In addition, we query LLMs to generate the top-R ranked chemical SMILES strings predictions list and set the hyperparameter R as 4. To maximize computational resource utilization, we harnessed eight V100 GPUs, each equipped with 8 GB of GPU memory, for training deep learning models built upon the PyTorch framework. Considering the context length limitations imposed by LLMs, which restrict the maximum sequence length that a typical LLM can process at a time to 4096 tokens, we implemented strategies to mitigate the high computational costs associated with prompting LLMs. This approach included running each experiment twice and reporting the average results. Our approach prioritizes both resource optimization and accuracy, aiming to achieve the best possible outcomes while minimizing the computational footprint. Our evaluation incorporated several metrics, and we present the results for the test datasets and compare the performance against well-known baselines."}, {"title": "Results", "content": "The experimental results of the proposed framework and the baseline models performance on the text2mol task are presented in Tables 2 and 3. The results of the baseline models are reported from earlier studies [10, 8]. The results undeniably demonstrate the superior performance of the FrontierX: LLM-MG framework, especially when combined with the GPT-4 backbone and employing the Scaffold technique with K set to 16. This optimal combination excels in generating accurate molecular structures that closely resemble the ground truth, surpassing all baseline models across various evaluation metrics."}, {"title": "Ablation Studies", "content": "Our proposed framework operates through a series of interconnected stages via a progressively structured multi-step pipeline. Beginning with step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to (i) generate top-ranked (top-R) SMILES strings predictions along with (ii) explanatory justifications for their predictions. In step (b), these generated explanations are used to (i) fine-tune a smaller pre-trained language model (LMexp) for domain customization to obtain contextualized token embeddings and utilize (ii) a weighted sum-pooling attention mechanism to compute text-level embeddings denoted as Yexp from the token embeddings for task-specific adaptation. Moving to step (c), the top-R predictions from the LLMs are transformed to compute prediction embeddings Ypred. Concurrently, in step (d), we fine-tune another small-scale language model (LMorg) on the original textual descriptions of molecules to compute context-aware token embeddings, and then compute the original text-level embeddings Yorg through a weighted attention mechanism. In step (e), our proposed framework obtains a cross-modal embeddings, Ycross, through a hierarchical multi-head attention mechanism that integrates the original text-level embeddings Yorg, explanatory text-level embeddings Yexp, and prediction embeddings Ypred. We conduct empirical research to understand the significance and contribution of each distinct method within the proposed framework, evaluating its learned embeddings to achieve optimal results. We perform ablation studies to assess the impact of disabling individual methods on the overall performance of our framework. To determine the contribution of each method to the framework's performance, we create various ablated variants by disabling individual methods and evaluate them using benchmark datasets for text2mol tasks. We choose the proposed FrontierX: LLM-MG framework as the reference baseline for the ablation studies. Our robust strategy not only validates the efficacy of the diverse methods but also substantiates the rationale, providing a strong basis for their design choices and justifying their inclusion within the framework. The ablated variants without the explanatory text-level embeddings, prediction embeddings, and original text-level embeddings are referred to as \"w/o yexp\", \"w/o ypred\", and \"w/o yorg\", respectively. The ablation study findings are summarized in Table 4. All the ablation study experiments were conducted with the FrontierX: LLM-MG framework using the GPT-4 backbone and Scaffold sampling technique with K = 16, by disabling certain methods as discussed earlier."}, {"title": "4 Conclusion", "content": "In this study, we pioneered the text2mol approach, inaugurating a transformative paradigm where chemistry meets language models, expediting scientific advancements. Through the creation of FrontierX: LLM-MG, we demonstrated the efficacy of using large language models for seamless and efficient translation between textual descriptions and chemical SMILES representations. Acknowledging the limitations of current methods, our research highlights a promising horizon in molecule design, potentially ushering in an era of accelerated innovation and interdisciplinary collaboration. Our study illustrates the transformative impact of integrating molecular design with language models, offering an innovative approach to molecule generation that can catalyze groundbreaking developments in science and technology."}, {"title": "5 Technical Appendix", "content": "In our study, we employ knowledge-augmented prompting with LLMs for zero-shot text-to-molecule(text2mol) translation task by leveraging the pre-existing knowledge embedded within the language model parameters. LLMs are capable of generating chemical SMILES representations from textual descriptions through entity recognition, grammar understanding, symbol mapping, and structure validation, which marks significant progress in molecule generation via language models. This knowledge-augmentation prompting technique allows LLMs to adapt to new, unseen molecule textual descriptions using a few task-specific demonstrations, thereby eliminating the need for fine-tuning with labeled data for task-specific adaptation. The approach involves creating knowledge-augmented prompts that combine task-specific instructions with demonstrations (input-output pairs) sampled from training data relevant to the target molecule textual descriptions determined using off-the-shelf semantic similarity techniques. In this context, each pair consists of a textual description of a molecule (input) and its corresponding SMILES representation (output), where the task-specific instruction is to convert the target molecule textual descriptions into the standardized chemical SMILES notation. This approach aligns the LLM's capabilities with the text2mol task by crafting knowledge-augmented prompts that blend specific instructions with relevant demonstrations, selected based on semantic similarity. This strategic alignment facilitates accurate chemical SMILES strings generation without necessitating language model parameter updates. We have employed two sampling strategies random"}]}