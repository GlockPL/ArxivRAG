{"title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design", "authors": ["Sakhinana Sagar Srinivas", "Venkataramana Runkana"], "abstract": "Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes. Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models. Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task. Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions. Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.", "sections": [{"title": "1 Introduction", "content": "Molecule design is an interdisciplinary approach that involves identifying a target molecule or property to enhance, such as a drug with increased efficacy or a material with superior characteristics. Advancements in science and technology have accelerated the discovery and development of novel drugs, advanced materials, and innovative chemical processes. This iterative process begins with (a) identifying a target molecule or property to improve, followed by (b) employing computational methods to explore the vast chemical space and optimize potential candidate structure and composition. The cycle continues with (c) synthesizing and testing promising candidates in the laboratory until the desired characteristics are achieved. The transformer architecture[28] has revolutionized various fields in computer science, including language understanding[5], text generation[17, 2], image understanding[7], and multi-modal generation[19, 23]. Utilizing this architecture to scale language models has established itself as a universal approach for enhancing generalization performance. In recent times, the emergence of foundational Large Language Models (LLMs)[2, 3, 27], which are built upon transformer architectures, has significantly revolutionized performance in various natural language processing tasks by enabling enhanced linguistic comprehension and logical reasoning abilities. Different learning strategies such as Zero-Shot Chain of Thought (Zero-shot-CoT[29]) and Few-Shot (In-Context) Learning (Few-shot-ICL[22, 6]) are utilized to leverage the emerging abilities of general-purpose LLMs for a wide variety of specialized tasks across various domains. The former employs task-specific instructions without relying on downstream task-based demonstrations, utilizing the inherent knowledge that the language model acquired during training to generate outputs. In contrast, Few-shot-ICL supplements instructions with a handful of demonstrations, presented as input-output pairs, to foster contextual understanding and facilitate task-specific adaptation, thereby generating relevant output. Recently, there has been a surge in the evolution of generative AI, such as \u201cDALL\u00b7E", "Make-A-Video\"[24] from Meta AI \u2014 a text-to-video diffusion model that generates realistic, engaging, and creative videos from text, among others. Inspired by recent developments in next-generation AI, \u201cText-Based Molecule Design\"[8] (also known as text2mol) represents a novel cross-domain task in chemistry that involves generating chemical SMILES representations from the corresponding technical descriptions of molecules expressed in natural language. Unlike traditional methods of de novo molecule generation, the text2mol task extracts information from technical descriptions of molecules, identifying aspects such as the specified structure, properties and functional groups, to generate chemical SMILES representations with desired characteristics. Existing models[8, 10] in the literature for the text2mol task face challenges in achieving optimal performance and utility, particularly in scenarios where data is scarce and unbalanced. LLMs like ChatGPT[2], while proficient in linguistic comprehension, are black-box in nature, resource-intensive, and lack interpretability. Smaller language models(LMs) like BERT[5], although flexible and interpretable, may lag in reasoning and generalization, resulting in less coherent and contextually relevant responses compared to LLMs. Navigating these challenges requires a delicate balance between performance, efficiency, and interpretability. Our study introduces a novel approach for the text2mol task by combining the strengths of both LLMs and small-scale LMs. LLMs predict a ranked list of chemical SMILES representations while providing explanations as justifications for these predictions, conditioned on the input prompt. These textual explanations, in conjunction with original technical descriptions of molecules, are used to fine-tune small-scale LMs to obtain context-aware token embeddings that capture the essence of both the generated explanations and original text, respectively. Concurrently, the top-ranked predictions generated by LLMs are transformed to obtain prediction embeddings. By integrating these various embeddings through a hierarchical multi-head attention mechanism, the framework inputs a unified cross-modal embedding into a transformer decoder to generate chemical SMILES representations that align with original technical descriptions. In this study, we explored the use of knowledge-augmented LLM prompting for zero-shot text-conditional molecule generation, a sequence-to-sequence cross-domain task. We present a powerful new tool, FrontierX: LLM-MG, where the goal is to task LLMs with a knowledge-infused prompt that consists of a few demonstrations(input-output pairs) for the text2mol task, along with task-specific instructions, where the output is chemical SMILES representations of the corresponding query technical descriptions. Our experiments on benchmark datasets provide empirical evidence supporting the framework's effectiveness in text-based molecule design tasks. The workflow of the proposed approach is illustrated in Figure 1.\"\n    },\n    {\n      \"title\": \"2 Proposed Method\",\n      \"content\": \"The Large Language Models (LLMs), such as ChatGPT[2], Meta's LLaMA[27] \u2014 that have been pre-trained on large text corpora and operate based on a \u201cprompt and predict\" approach (utilizing natural language prompts to generate the subsequent contextual word or phrase, aligning with human-like responses) \u2014 have revolutionized language modeling with their proficiency in linguistic comprehension and advanced logical reasoning abilities, providing improved performance on general-purpose NLP tasks. While LLMs are inherently black box in nature, they possess remarkable capabilities. However, their widespread adoption for applications in various downstream tasks is hindered by the unavailability of logits or token embeddings, which limits explainability. Additionally, they require significant computational resources for fine-tuning on labeled data for task-specific adaptation or for repurposing for domain-customization. In contrast, the small-scale language models (LMs), such as BERT[5] and DeBERTa[11], following a \u201cpre-train, fine-tune\" approach, offer more affordable flexibility for fine-tuning with minimal labeled data and provide access to logits or token embeddings, aiding interpretability. While smaller LMs can learn complex patterns, they often fall short in reasoning and generalization abilities compared to LLMs, which generate more coherent and contextually relevant responses. To alleviate resource constraints, Language Modeling as a Service (LMaaS[25]) offers access to LLMs through text-based API interactions, while remaining scalable and cost-effective. However, the potential of LLMs for text-conditional de novo molecular generation tasks remains largely underexplored. Our proposed approach for the text2mol task leverages LLMs by utilizing: (a) their predictive ability to provide a top-R ranked list of chemical SMILES representations; and (b) their generative ability to offer auxiliary explanations as justifications for their predictions by conditioning on the augmented prompt. Furthermore, we fine-tune two different small-scale LMs using (a) generated explanations from LLMs and (b) input technical descriptions of molecules to compute their respective contextualized token embeddings which capture semantic coherence and contextual relevance for text-to-molecule generation tasks. We utilize weighted attention mechanism to compute both original and explanatory text-level embeddings from their respective context-aware token embeddings. In addition, we transform the LLMs' top-R predictions of chemical SMILES representations into predictive embeddings. We use a hierarchical multi-head attention mechanism to integrate various embeddings into unified cross-modal embedding for input into a transformer decoder, generating the chemical SMILES representation.\",\n      \"subsections\": [\n        {\n          \"title\": \"Evaluation LLMs & LMs\",\n          \"content\": \"In this work, we evaluated three popular LLMs: text-davinci-0032, ChatGPT3, and Google BARD4, in order to thoroughly compare their distinct strengths. text-davinci-003 was the earliest LLM released by OpenAI and was tailored for a broad spectrum of linguistic tasks. GPT-3.5-turbo is a substantial improvement over the GPT-3 base models, demonstrating remarkable performance on a wide range of linguistic tasks while also being cost-effective. Google BARD[1] stands out due to its extraordinary scale, complexity, and an impressively extensive vocabulary compared to the GPT-3.5 models. In addition to these, our study also incorporates a pre-trained smaller LM, DeBERTa, which is an improved version of the BERT[5] architecture. Table 1 presents a comprehensive summary of the technical specifications of these language models.\"\n        },\n        {\n          \"title\": \"Knowledge-Augmented Prompts\",\n          \"content\": \"In our work, we offer essential context and task-specific instructions by using input natural language descriptions of the target molecule to prompt LLMs in a zero-shot setting to generate corresponding chemical SMILES representations. In this scenario, the primary task-specific instructions involve the translation of these descriptions into chemical SMILES representations. We create an augmented prompt that incorporates both the task-specific instructions and a few demonstrations. These demonstrations, which establish the context, are grounded in the downstream text2mol task and comprise input-output pairs (i.e., technical descriptions and their corresponding chemical SMILES representations). This approach facilitates knowledge-augmented prompting of the LLMs for zero-shot text-to-molecule generation tasks. The construction of an augmented prompt involves sampling text-molecule pairs from the training data that are relevant to the target molecule descriptions. We then prepend these pairs to the task-specific instructions to form an augmented prompt, which is used to query the LLMs in a zero-shot setting for the generation of chemical SMILES representations. To evaluate the impact of the quality and quantity of sampled text-molecule pairs on the performance of text-conditional de novo molecule generation tasks, we employ two different sampling strategies. The quality of these pairs is determined by the sampling\"\n        },\n        {\n          \"title\": \"Querying LLMS\",\n          \"content\": \"We access LLMs with LMaaS[25] platforms via text-based API interaction, necessitating solely text-based input and output. We create a customized zero-shot prompt template to query LLMs to translate textual descriptions into chemical SMILES representations. The LLMs' response serves the dual purpose of (a) providing detailed textual explanations for the underlying rationale, (reasoning or logic), behind the predictions and (b) generating a list of the top-R ranked chemical SMILES representations. Subsequently, we fine-tune smaller downstream LMs using the generated auxiliary explanations. The custom augmented prompt format is as follows:\nBelow are the textual descriptions \u2013 chemical SMILES representation pairs. Generate the chemical SMILES representation for the textual description provided below.\nQuerying LLMs (a) predicts the top-R ranked chemical SMILES representations and (b) provides auxiliary explanations as logical justifications for its predictions.\n(LLMs Response) [top-R ranked predictions \u2013 Auxiliary Explanations]\"\n        },\n        {\n          \"title\": \"Fine-tuning LMs for Domain-Specific Customization\",\n          \"content\": \"Our novel approach leverages the integration of a smaller language model (LM) to extract relevant information from the original molecular textual descriptions and auxiliary explanations generated by LLMs, thereby aiding downstream tasks. The intermediary LM serves as a bridge between the LLM and the downstream layers that generate chemical SMILES representations. To elucidate further, we fine-tune pre-trained LMs, denoted as $LM_{exp}$ and $LM_{org}$, to compute context-aware token embeddings by passing the text sequences generated by LLMs (referred to as $S_{exp}$) and original textual descriptions (referred to as $S_{org}$) through the $LM_{exp}$ and $LM_{org}$ models, respectively, as described below:\n$h_{exp} = LM_{exp}(S_{exp}) \\in \\mathbb{R}^{(m \\times d)}; h_{org} = LM_{org}(S_{org}) \\in \\mathbb{R}^{(n \\times d)}$\nwhere both contextualized embeddings $h_{exp}$ and $h_{org}$ capture not only the contextual information of the tokens but also encapsulate the semantic relationships among tokens within their respective textual content. Here, m and n represent the number of tokens in $S_{exp}$ and $S_{org}$, while d represents the token embedding dimension. We employ a softmax attention mechanism to compute a weighted sum of the contextualized token embeddings, encoding the auxiliary explanations and original textual descriptions into single fixed-length vectors or embeddings denoted as $Y_{exp}$ and $Y_{org}$ and computed as follows,\n$\\alpha_{i} = softmax(q_{i}); q_{i} = u^{T}h_{i}^{exp}; \\beta_{i} = softmax(r_{i}); r_{i} = v^{T}h_{i}^{org}$\n$Y_{exp} = \\sum_{i=0}^{m} \\alpha_{i}h_{i}^{exp} \\in \\mathbb{R}^{(d)}; Y_{org} = \\sum_{i=0}^{n} \\beta_{i}h_{i}^{org} \\in \\mathbb{R}^{(d)}$\nwhere u and v are differentiable vectors. The explanatory text-level embedding, represented as $Y_{exp}$, encapsulates domain-specific knowledge retrieved from foundational LLMs to support its predictions. The original text-level embedding, denoted as $Y_{org}$, captures the overall context and semantics within the original textual descriptions by extracting the most pertinent and task-relevant information.\"\n        },\n        {\n          \"title\": \"LLMs Prediction Embeddings\",\n          \"content\": \"As mentioned earlier, the LLMs not only provide the auxiliary textual explanations but also predict the top-R ranked chemical SMILES representations list, which can be informative. For each target molecule in the text2mol task, the top-R predictions are converted into one-hot encoded vectors $P_{i,1},..., P_{i,R} \\in \\mathbb{R}^{C}$, where C represents the total number\"\n        },\n        {\n          \"title\": \"Cross-modal Attention Layer\",\n          \"content\": \"We compute the cross-modal embedding, denoted as $Y_{cross}$, using a hierarchical multi-head attention mechanism that integrates the original text-level embedding $Y_{org}$, the explanatory text-level embedding $Y_{exp}$, and the prediction embedding $Y_{pred}$. This mechanism provides a robust framework for integrating diverse information encapsulated from different modalities, addressing several key aspects critical to the performance of cross-modal learning tasks. It involves hierarchical implementation of multi-head attention mechanisms. We employ two layers, each focusing on different aspects of the input embeddings, enabling more complex interactions and potentially leading to more scalable and efficient models. In the initial layer, we apply a multi-head attention mechanism to the mono-domain embeddings, specifically the original text-level embedding $Y_{org}$ and the explanatory text-level embedding $Y_{exp}$, to obtain unified mono-domain embeddings denoted as $Y_{uni}$. In the subsequent layer, we utilize the multi-head attention mechanism on the cross-domain embeddings, comprising the prediction embedding $Y_{pred}$ and the unified mono-domain embeddings $Y_{uni}$, to compute the cross-modal embeddings denoted as $Y_{cross}$. For the first layer, we compute the Query, Key, Value projections for the original text-level embedding $Y_{org}$ for each head h as follows:\n$Q_{org}^{h} = Y_{org}W_{Qorg}^{h}; K_{org} = Y_{org}W_{Korg}^{h}; V_{org} = Y_{org}W_{Vorg}^{h}$\nSimilarly, the Query, Key, Value projections for explanation text-level embedding $Y_{exp}$ for each head h as follows:\n$Q_{exp}^{h} = Y_{exp}W_{Qexp}^{h}; K_{exp} = Y_{exp}W_{Kexp}^{h}; V_{exp} = Y_{exp}W_{Vexp}^{h}$\nWe concatenate the keys and values from both original and explanatory text-level embeddings, which provides a powerful way to integrate information from the mono-domain embeddings into a unified, rich representation.\n$K_{concat}^{uni} = [K_{org}^{h}, K_{exp}^{h}]; V_{concat}^{uni} = [V_{org}^{h}, V_{exp}^{h}]$\nWe use softmax attention to integrate complementary information from the mono-domain embed-dings, focus on contextually relevant information, and semantically align them through an attention mechanism. The softmax function is applied to the keys for each query.\n$A_{uni}^{h} = Softmax (\\frac{(Q_{org}^{h} + Q_{exp}^{h}) K_{concat}^{uni^T}}{\\sqrt{d_{h}}})$\nEach head outputs a new vector representation that highlights the most relevant features in the mono-domain embeddings(both original and explanation text-level), according to the attention mechanism for that specific head, which is tailored to capture specific aspects or relationships within the data.\n$O_{uni}^{h} = A_{uni}^{h} V_{concat}^{uni}$\nFinally, all the head-specific outputs are concatenated and linearly transformed to create the unified mono-domain embedding as follows,\n$O_{concat} = [O_{uni}^{1}, O_{uni}^{2},..., O_{uni}^{H}]$\n$Y_{uni} = O_{concat} W_{ouni}$\nwhere $W_{Qorg}^{h}, W_{Korg}^{h}, W_{Vorg}^{h}, W_{Qexp}^{h}, W_{Kexp}^{h}, W_{Vexp}^{h}, W_{ouni}$ are the learnable weight matrices. Here, $d_{h}$ represents the dimensionality of the key/query/value for each head, and H is the number of heads. $Y_{uni}$ denotes the unified mono-domain embeddings. The unified embeddings can learn and integrate complementary, diverse information present in both the $Y_{org}$ and $Y_{exp}$ embeddings. These unified embeddings facilitate semantic alignment among similar features across different embeddings and enable the identification of contextual relevance between distinct yet related $Y_{org}$ and $Y_{exp}$ mono-domain embeddings. The next step involves computing the cross-modal embedding $Y_{cross}$ using a second layer of a multihead attention mechanism that integrates both $Y_{pred}$ and $Y_{uni}$. We compute the Query, Key, and Value projections for the prediction embedding $Y_{pred}$ for each head h as follows:\n$Q_{pred}^{h} = Y_{pred} W_{Qpred}^{h}; K_{pred}^{h} = Y_{pred} W_{Kpred}^{h}; V_{pred}^{h} = Y_{pred} W_{Vpred}^{h}$\nSimilarly, we compute the Query, Key, Value projections for the unified embedding $Y_{uni}$ for each head h as follows:\n$Q_{uni}^{h} = Y_{uni} W_{Quni}^{h}; K_{uni}^{h} = Y_{uni} W_{Kuni}^{h}; V_{uni} = Y_{uni} W_{Vuni}^{h}$\nWe concatenate the keys and values from both the prediction and unified embeddings, thereby facilitating a robust integration of insights from the cross-domain embeddings into a synergized and enriched representation.\n$K_{cross}^{h} = [K_{uni}^{h}, K_{pred}^{h}]; V_{cross}^{h} = [V_{uni}^{h}, V_{pred}^{h}]$\nWe utilize a softmax attention mechanism to merge and align information from different domains, thereby prioritizing contextually relevant information and ensuring semantic alignment. The softmax\"\n        },\n        {\n          \"title\": \"Output Layer\",\n          \"content\": \"We then utilize a transformer decoder[28] to generate chemical SMILES represen-tations character by character, using cross-modal embeddings ($Y_{cross}$) that incorporate global context through the hierarchical multi-head self-attention mechanism. We implement a softmax layer to transform the decoder's output, creating a probability distribution over potential elements for each position in the SMILES strings. For our sequence generation tasks, we minimize the categorical cross-entropy loss to penalize the proposed framework based on the negative log-likelihood of the ground-truth chemical SMILES strings under the predicted probability distribution, thus facilitating the generation of valid molecules. In summary, by integrating multi-modal embeddings, namely $Y_{org}$, $Y_{exp}$, and $Y_{pred}$, our approach enables the concurrent capture of complementary information, ultimately enhancing the overall performance of the framework.\"\n        }\n      ]\n    },\n    {\n      \"title\": \"3 Experiments & Results\",\n      \"content\": \"Our study utilized the ChEBI-20 dataset[8], a bidirectional text-to-molecule translation dataset comprising 33,010 text description-molecule pairs with a predefined split ratio of 80:10:10 for training, validation, and test sets, respectively. We utilized 26,407 text description-molecule pairs from the training set for demonstrations (input-output mappings) for constructing a knowledge-augmented prompt to query LLMs. We used the MolT5 model[8], as a predominant baseline, which is an encoder-decoder transformer architecture pretrained on a large unannotated dataset specifically for the text2mol translation task, building upon the foundations of the T5[18] model. We evaluated the performance of our proposed framework on the text2mol task, comparing it with several variants of the MolT5[8] and T5[18] models, as well as with general-purpose sequence-to-sequence models, such as the RNN-GRU and Vanilla Transformer models. In addition, various variants of few-shot (ICL) prompting of GPT-based models - reflecting the fact that this technique uses few-shot learning to prompt off-the-shelf GPT-based models to perform molecular property prediction for new, unseen molecules - referred to as baselines, are evaluated for comparison with our proposed framework. The configurations include different variants of the GPT-4 model, namely, (a) the zero-shot approach, (b) the scaffold sampling technique with K=10 or K=5, and (c) the random sampling technique with K=10 for constructing augmented prompts. In addition, we use GPT-3.5 and davinci-003 models, both employing the scaffold sampling technique with K=10 to construct knowledge-augmented prompts. For more details and information on the baselines, please refer to the earlier works[10, 8].\"\n    },\n    {\n      \"title\": \"3.2 Evaluation Metrics\",\n      \"content\": \"To comprehensively evaluate the quality and similarity of the generated chemical SMILES repre-sentations compared to the ground-truth SMILES representations, we employed a range of distinct evaluation metrics, categorized into three types. These metrics include (a) chemical similarity measures, such as the FTS (Fingerprint Tanimoto Similarity) [26] and the FCD (Fr\u00e9chet ChemNet Distance) [16], as well as (b) natural language processing metrics like the BLEU (Bilingual Evaluation Understudy) score, Exact Match [8], and Levenshtein distance [14]. In addition, (c) we utilized the RDKit library [13] to validate the generated molecules. We delineate the metrics as follows: (a) We employ the FTS[26] metric to gauge the chemical similarity between the ground-truth and generated chemical compounds represented as SMILES strings(notation to represent chemical structures as text) by comparing their MACCS, RDK, and Morgan fingerprints[21, 13, 4]. (b) In addition, we utilize the FCD metric[16], which leverages latent information from a pretrained model[16] to predict molecular activity[8]. The FCD is calculated by measuring the distance between the mean embeddings of two sets of chemical SMILES strings (generated and ground-truth) in the latent space of the pretrained model. A lower FCD score indicates a greater similarity between the corresponding two sets of molecules. (c) We also apply natural language processing metrics to evaluate the quality of the chemical SMILES strings generated by our framework. These metrics encompass the following: (i) BLEU this measures the similarity between two text strings, with a higher BLEU score denoting better similarity. (ii) Exact Match[8] \u2013 this quantifies the percentage of generated chemical SMILES strings that are identical to the ground-truth strings. (iii) Levenshtein distance[14] \u2014 this calculates the minimum number of single-character edits required to modify the generated chemical SMILES strings to match the ground-truth strings, with a lower value indicating closer similarity. By utilizing these diverse metrics, we attain a nuanced understanding of the efficacy of our text-conditional de novo molecule generation framework. Higher FTS scores and lower FCD scores signify better chem-ical similarity and closer molecular activity resemblance, respectively. In the context of matching chemical SMILES strings from the perspective of natural language processing, higher BLEU and Exact Match scores are preferred to achieve better alignment with the ground-truth SMILES strings, while a lower Levenshtein distance indicates fewer required edits, denoting superior similarity. The RDKit library assists in verifying the validity of the generated molecules, with a higher proportion indicating successful generation[13].\"\n    },\n    {\n      \"title\": \"3.3 Experimental Setup\",\n      \"content\": \"We used the ChEBI-20 dataset[9] with an 80:10:10 split: 80% for training, 10% for validation, and 10% for testing. The training set was utilized to update the learnable parameters, the validation set to select optimal hyperparameters, and the test set to evaluate generalization performance of the proposed framework. Our scalable and efficient framework offers a unified solution for integrating LLMs and LMs. We configured the hyperparameters of our framework with a batch size of 32, trained it for 100 epochs, and a hidden or embedding dimension(d) of 128. Additional hyperparameters include the number of attention heads (H) set to 4, and the dimensionality of Key/Query/Value (dh) is 32. To optimize the training process, we utilized the Adam optimizer [12], initially setting the learning rate to le\u00af\u00b3. Additionally, we incorporated a learning rate decay scheduler, which reduced the learning rate by half whenever the validation loss did not improve for 10 consecutive epochs. Furthermore, we applied early stopping to prevent overfitting on the validation data. We evaluated our approach using the following LLMs: GPT-4.0, GPT-3.5-turbo, GPT-3.0-text-davinci-003, and Google Bard. In our approach, we chose not to fine-tune hyperparameters individually for each LLM, opting instead to maintain consistent settings across all language models. This strategy simplifies experimentation, ensures uniform conditions, facilitates result comparison, and promotes consistency. Moreover, it underscores the versatility of our framework, which can be used with any off-the-shelf LLM without the need for computationally expensive hyperparameter tuning. We used the Scaffold technique with K=16 to sample demonstrations (input-output mappings) from the training data to construct augmented prompts for querying LLMs in few-shot settings. In addition, we query LLMs to generate the top-R ranked chemical SMILES strings predictions list and set the hyperparameter R as 4. To maximize computational resource utilization, we harnessed eight V100 GPUs, each equipped with 8 GB of GPU memory, for training deep learning models built upon the PyTorch framework. Considering the context length limitations imposed by LLMs, which restrict the maximum sequence length that a typical LLM can process at a time to 4096 tokens, we implemented strategies to mitigate the high computational costs associated with prompting LLMs. This approach included running each experiment twice and reporting the average results. Our approach prioritizes both resource optimization and accuracy, aiming to achieve the best possible outcomes while minimizing the computational footprint. Our evaluation incorporated several metrics, and we present the results for the test datasets and compare the performance against well-known baselines.\"\n    },\n    {\n      \"title\": \"3.4 Results\",\n      \"content\": \"The experimental results of the proposed framework and the baseline models performance on the text2mol task are presented in Tables 2 and 3. The results of the baseline models are reported from earlier studies [10, 8]. The results undeniably demonstrate the superior performance of the FrontierX: LLM-MG framework, especially when combined with the GPT-4 backbone and employ-ing the Scaffold technique with K set to 16. This optimal combination excels in generating accurate molecular structures that closely resemble the ground truth, surpassing all baseline models across\"\n    },\n    {\n      \"title\": \"3.5 Ablation Studies\",\n      \"content\": \"Our proposed framework operates through a series of interconnected stages via a progressively structured multi-step pipeline. Beginning with step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to (i) generate top-ranked (top-R) SMILES strings predictions along with (ii) explanatory justifications for their predictions. In step (b), these generated explanations are used to (i) fine-tune a smaller pre-trained\"\n    },\n    {\n      \"title\": \"5 Technical Appendix\",\n      \"subsections\": [\n        {\n          \"title\": \"5.1 Study of Knowledge-Augmented Prompting\",\n          \"content\": \"In our study, we employ knowledge-augmented prompting with LLMs for zero-shot text-to-molecule(text2mol) translation task by leveraging the pre-existing knowledge embedded within the language model parameters. LLMs are capable of generating chemical SMILES representations from textual descriptions through entity recognition, grammar understanding, symbol mapping, and struc-ture validation, which marks significant progress in molecule generation via language models. This knowledge-augmentation prompting technique allows LLMs to adapt to new, unseen molecule textual descriptions using a few task-specific demonstrations, thereby eliminating the need for fine-tuning with labeled data for task-specific adaptation. The approach involves creating knowledge-augmented prompts that combine task-specific instructions with demonstrations (input-output pairs) sampled from training data relevant to the target molecule textual descriptions determined using off-the-shelf semantic similarity techniques. In this context, each pair consists of a textual description of a molecule (input) and its corresponding SMILES representation (output), where the task-specific instruction is to convert the target molecule textual descriptions into the standardized chemical SMILES notation. This approach aligns the LLM's capabilities with the text2mol task by crafting knowledge-augmented prompts that blend specific instructions with relevant demonstrations, selected based on semantic similarity. This strategic alignment facilitates accurate chemical SMILES strings generation without necessitating language model parameter updates. We have employed two sampling strategies random and scaffold to evaluate the impact of both the quality and quantity of demonstrations in the knowledge-augmented prompt, which is utilized for querying LLMs during text-based de novo molecule generation. The scaffold strategy utilizes a semantic similarity method to sample the top-K relevant text-molecule pairs, using OpenAI's text-embedding-ada-002 technique7. The random technique involves the arbitrary selection of K text-molecule pairs without any prior knowledge in a non-deterministic manner. The study compares the effectiveness of both strategies in enhancing the language-conditioned molecule generation task using off-the-shelf pre-trained LLMs, including Google Bard and other GPT model family variants. We conducted experiments to compare and contrast the performance of the \u201cRandom": "nd \u201cScaffold\u201d sampling strategies, and to identify the optimal number of demonstrations."}, {"title": "5.2 Impact of Hierarchical Multi-Head Attention(HMHA) Mechanism", "content": "In our work, we compute the cross-modal embedding, denoted as $Y_{cross}$, through a hierarchical multi-head attention (HMHA) mechanism that integrates the original text-level embedding ($Y_{org}$), explanatory text-level embedding ($Y_{exp}$), and prediction embedding ($y_{pred}$). To determine the impact of the HMHA mechanism on the framework performance, we conducted ablation study. We refer to the ablated variant without the HMHA mechanism as \"w/o HMHA\". We substitute the HMHA mechanism with dual-stage linear operators in the ablated variant to compute cross-modal embeddings. The findings of the ablation study are summarized in Table 6. We conducted the experiment using the FrontierX: LLM-MG framework with GPT-4 backbone, where we replaced the HMHA mechanism with linear operators, as discussed earlier. The experimental results support the inclusion of the hierarchical multi-head attention mechanism (HMHA) to generate cross-modal embeddings, aiding in the generation of more valid chemical SMILES representations in the text2mol task."}, {"title": "5.3 Hyperparameter Tuning", "content": "To enhance the performance of our FrontierX: LLM-MG framework, we embarked on meticulous hyperparameter tuning through detailed experimentation and analysis. We chose random search as a strategy to adeptly navigate the hyperparameter space, pinpointing the optimal framework configuration on the benchmark dataset, in lieu of more computationally intensive approaches such as grid search or Bayesian optimization. This strategy enabled us to obtain optimal results on the validation subset of the benchmark dataset, as evidenced by several evaluation metrics. We conducted hyperparameter optimization on the FrontierX: LLM-MG-W/GPT-4 variant of our framework. We utilized the Scaffold sampling technique with K = 16 for constructing augmented prompts. The primary key hyperparameters within this framework include batch size (b \u2208 {32, 48, 64}) and the embedding dimension (d \u2208 {64, 128, 196, 256}). Table 7 presents the results of hyperparameter tuning on the"}, {"title": "5.4 Molecule captioning", "content": "Molecule captioning is a crucial task in the field of computational chemistry, serving as a bridge between complex chemical data and human comprehension. It involves generating detailed and correct textual descriptions that accurately describe a chemical SMILES representation in the mol2text task. This stands in contrast to the text2mol task, which entails generating chemical SMILES representations from detailed and factual textual descriptions. Meanwhile, the mol2text task helps to translate complex chemical structures into understandable language, enhancing our understanding of molecules with potential applications spanning multiple fields, including drug discovery, materials science, and chemical synthesis. To evaluate the quality of the generated text in the mol2text task, we employ traditional metrics commonly used in natural language processing and machine translation, including BLEU, ROUGE, and METEOR, as described below:"}, {"title": "5.4.1 FrontierX: LLM-MG-mol2text task", "content": "We have modified the FrontierX: LLM-MG pipeline for the text2mol task to adapt it for the mol2text task. The workflow of the proposed approach is illustrated in Figure 2. Given a chemical SMILES representation, it can generate the technical descriptions of the molecule. We construct a knowledge-infused prompt using task-specific instructions and a few demonstrations (input-output mappings) for the downstream mol2text task. The task-specific instructions involve translating chemical SMILES representations into their corresponding technical descriptions. The primary objective of the prompt engineering method is to enhance the context-awareness of language models and improve their ability to provide relevant and accurate responses. This enhancement is achieved through learning from demonstrations, rather than relying on conventional supervised learning methods of fine-tuning with labeled data for the mol2text task. The knowledge-infused prompts guide the language models to generate technical descriptions based on the provided instructions. Next, we fine-tune small-scale, pre-trained language models (LMs) using the generated explanations, which facilitates domain customization and yields context-aware token embeddings. To create a text-level embedding that encapsulates the generated technical descriptions, we utilize a weighted sum-pooling attention mechanism on the contextualized embeddings. Additionally, the unimodal encoder, which is implemented with a multi-head attention mechanism, integrates the mono-domain"}, {"title": "5.4.2 LLM Prompting", "content": "Knowledge-infused LLM prompting, a method of prompt engineering, involves crafting effective prompts or input queries to elicit desired responses from language models. This technique enhances language models by combining their natural language understanding and generation capabilities with access to external factual information(demonstrations), making them more versatile for task-specific applications. Consequently, it enables the LLM to generate responses enriched with accurate, contextually relevant information. Knowledge-infused prompting enables LLMs to adapt to new tasks without the need for explicit, gradient-based fine-tuning with gold-standard annotated data for"}, {"title": "5.4.3 Ablation Studies", "content": "Our proposed framework operates in a structured, multi-step pipeline. In step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to generate textual descriptions. In step (b), we use these generated explanations to fine-tune a smaller, pre-trained language model (LMexp) for domain-specific cus-tomization, resulting in context-sensitive token embeddings. We employ a weighted sum-pooling attention mechanism for task-specific adaptation to compute text-level embeddings, denoted as Yexp, from the contextualized token embeddings. In parallel"}]}