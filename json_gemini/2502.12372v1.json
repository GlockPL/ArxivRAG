{"title": "Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation", "authors": ["Joy Mahapatra", "Soumyajit Roy", "Utpal Garain"], "abstract": "Monitoring factual inconsistency is essential for ensuring trustworthiness in data-to-text generation (D2T). While large language models (LLMs) have demonstrated exceptional performance across various D2T tasks, previous studies on scaling laws have primarily focused on generalization error through power law scaling to LLM size (i.e., the number of model parameters). However, no research has examined the impact of LLM size on factual inconsistency in D2T. In this paper, we investigate how factual inconsistency in D2T scales with LLM size by exploring two scaling laws: power law and exponential scaling. To rigorously evaluate and compare these scaling laws, we employ a statistical validation framework consisting of three key stages: predictive performance estimation, goodness-of-fit assessment, and comparative analysis. For a comprehensive empirical study, we analyze three popular LLM families across five D2T datasets, measuring factual inconsistency inversely using four state-of-the-art consistency metrics. Our findings, based on exhaustive empirical results and validated through our framework, reveal that, contrary to the widely assumed power law scaling, factual inconsistency in D2T follows an exponential scaling with LLM size.", "sections": [{"title": "Introduction", "content": "Data-to-text (D2T) generation (Lin et al., 2024; Li et al., 2024) converts semi-structured data (e.g., tables) into natural language, with applications in conversation systems, automated journalism, and other fields. A key challenge in D2T is factual inconsistency (Li et al., 2022; Huang et al., 2023) when generated text fails to entails with input facts-leading to hallucinations that undermine trust in D2T models (Figure 1). Therefore, it is essential to monitor and mitigate factual inconsistency in order to construct trustworthy D2T models.\nLarge language models (LLMs) have achieved remarkable success in D2T, primarily due to their massive model sizes (parameter counts) and training on vast text corpora (Lorandi and Belz, 2024). Several studies shows that LLMs often adhere to scaling laws, typically power laws, governing generalization error or perplexity in relation to model size (Kaplan et al., 2020; Hoffmann et al., 2022). These scaling laws play a crucial role in predicting model performance, guiding hyperparameter tuning, estimating computational costs, and optimizing resource allocation (Hendrycks, forthcoming; Zhang et al., 2024). Existing LLM scaling laws in D2T focus on generalization loss or test perplexity (Bahri et al., 2024), overlooking factual inconsistency. Understanding how factual inconsistency scales with LLM model size can help researchers and practitioners optimize model selection and enhance trustworthiness in D2T, highlighting a key research gap.\nIn this paper, we address the research gap by examining scaling laws for factual inconsistency in D2T with respect to LLM size. Unlike prior studies that focus solely on power law scaling, we explore both power law and exponential scaling with a rigorous three-stage statistical validation framework. This framework comprises three key stages: predictive performance estimation (evaluating Huber loss on held-out data), goodness-of-fit assessment"}, {"title": "Related Work", "content": "Data-to-text generation (D2T) and factual inconsistency. Data-to-text generation (D2T)(Lin et al., 2024) aims to transform non-textual, semi-structured data\u2014such as tables, graphs, or slot-value pairs (meaning representation, MR)\u2014into human-readable text. It can be categorized into three types based on source representation: graph-to-text(Gardent et al., 2017; Nan et al., 2021), table-to-text (Bao et al., 2018), and meaning representation (MR)-to-text (Novikova et al., 2017; Juraska et al., 2019). Recently, LLMs have become foundational models for D2T due to their extensive pre-training on large text datasets (Zhang et al., 2022) and their high model capacity (Scao et al., 2022). Moreover, with parameter-efficient fine-tuning techniques (Dettmers et al., 2023) and prompt-based learning (Lester et al., 2021), LLMs have gained widespread popularity for D2T tasks (Raffel et al., 2020; Lewis et al., 2020; Scao et al., 2022), often outperforming earlier models in generation quality and overall performance (Ge et al., 2023). In D2T, LLMs are often prone to generating factually inconsistent text, presenting a key research challenge. Factual inconsistency, defined as the lack of factual entailment between generated text and input data, contributes to hallucinations and undermines model reliability. Evaluation methods include human assessment (gold standard but costly) and automatic metrics (scalable but debated). Recently, trained automatic metrics (Fabbri et al., 2022; Zha et al., 2023) have shown strong correlations with human judgments, making them promising for factual inconsistency evaluation.\nScaling law for LLM. Scaling laws for LLMs describe how their performance scales with key factors such as model size (number of parameters) and training data size. Hestness et al. (2017) demonstrated that deep language models follow a power law scaling, laying the foundation for scaling law research. Kaplan et al. (2020) expanded this by systematically analyzing model size, data size, and computational efficiency, reinforcing the dominance of power law scaling in LLM performance. As research on scaling laws has expanded, various studies have explored their applications across different task domains, including close-ended text generation (Bansal et al., 2022) and open-ended text generation (Kaplan et al., 2020). Recent investigations have further examined scaling in diverse paradigms, such as sparse modeling (Frantar et al., 2024) and parameter-efficient fine-tuning (Zhang et al., 2024). Additionally, joint scaling laws\u2014such as additive and multiplicative formulations\u2014are gaining prominence in multi-factor scaling setups (Hoffmann et al., 2022; Zhang et al., 2024). Scaling laws offer several key advantages, including optimizing hyperparameter tuning (Hendrycks, forthcoming), estimating training costs (H\u00e4gele et al., 2024), and setting realistic expectations for model performance (Hoffmann et al., 2022). A recent study by Bahri et al. (2024) further reinforces the theoretical foundations of scaling laws."}, {"title": "Scaling Law Models and Training", "content": "Moving beyond existing studies, we formulate the scaling law for factual inconsistency in D2T concerning LLM size by considering two models\u2014power law scaling (following a power law function) and exponential scaling (following an exponential function). The power law scaling model ($M_{pow}$) is defined as follows:\n$M_{pow} : f(x) = \\begin{cases} A x^{\\alpha} + B & x \\geq 0 \\\\ 0 & \text{otherwise} \\end{cases}$\nWhere A and B are case-specific parameters, \u03b1 is the power law exponent, x represents LLM size, and f(x) denotes factual inconsistency.\nSimilarly, the exponential scaling model ($M_{exp}$) is defined as follows:"}, {"title": "Statistical Validation Framework", "content": "To empirically study the two scaling models under limited data, we employ a structured three-stage statistical validation framework, consisting of predictive performance estimation, goodness-of-fit assessment, and comparative analysis, as detailed below.\n\u2022 Stage I: Predictive performance estimation.\nThis validation stage ensures how well the scaling laws generalize in terms of their predictive ability on unseen data. To achieve this, we evaluate the scaling laws on held-out data using Huber loss. Given the limited data availability, we employ five-fold cross-validation for predictive performance assessment.\n\u2022 Stage II: Goodness-of-fit assessment. Predictive performance alone is not sufficient to validate a scaling law; assessing its goodness-of-fit is also crucial for its acceptance. Therefore, in this stage, we evaluate the goodness-of-fit of the scaling law models using a goodness-of-fit test-specifically, an F-test for regression (Weisberg, 2005; Siegel, 2016). The test statistic for the F-test is calculated as follows:\n$F_{stat} = \\frac{SSR_R}{d f_r} / \\frac{SSR_E}{d f_e}$\n$F_{stat} \\sim F\\text{-distribution}(x)$\nHere, SSR and SSRE represent the sum of squared residuals for the reduced and exact models, respectively. Similarly, dfr and dfe denote the degrees of freedom for the reduced and exact models, respectively. We consider our scaling models (Mpow and Mexp) as exact models, while the reduced model is represented by a simple mean-response model. Since the F-test applies only to linear regression models, we use a log transformation to convert our scaling models into their linear forms. We perform the F-test with a significance level of p < 0.05, which is often considered a moderate range. If both scaling models qualify in the goodness-of-fit assessment, we proceed to Stage III.\n\u2022 Stage III: Comparative analysis. In this final stage of validation, we compare the two scaling law models, Mpow and Mexp, through hypothesis testing to determine which better explains the data. Since power law and exponential scaling models are not nested hypotheses, the standard likelihood-ratio test is not applicable. Instead, we employ Vuong's likelihood-ratio test (Vuong, 1989) for comparison. The test statistic for Vuong's likelihood-ratio test is computed as follows:\n$V_{stat} = \\frac{\\sqrt{n} \\text{ mean}(d)}{\\sqrt{\\text{Var}(d)}}$\n$V_{stat} \\sim normal(0, 1)$\nWhere n represents the sample size, and d denotes the n-sized sample of the log-likelihood differences between the two scaling law models. We conduct Vuong's likelihood ratio test at a stringent significance level of p < 0.005"}, {"title": "Experiment Setup", "content": "We utilize five well-known D2T datasets, covering three major D2T types: DART and WebNLG for graph-to-text, WikiTableText for table-to-text, and E2E and ViGGO for MR-to-text. All datasets are sourced from (Kasner et al., 2023) and (Wolf et al., 2020). The E2E dataset (Novikova et al., 2017; Dusek et al., 2018) contains over 37K MR-text pairs from the restaurant domain, with an average text length of approximately 21 words. ViGGO (Juraska et al., 2018) includes 7K MR-to-text instances spanning nine dialogue acts in the video game domain, with an average text length of around 14 words. Both E2E and ViGGO are closed-domain datasets. WikiTableText (Bao et al., 2018) is an open-domain D2T dataset consisting of approximately 13K table-to-text pairs extracted from Wikipedia tables. DART (Nan et al., 2021) contains nearly 70K knowledge graph triplets, with an average text length of 34 words. WebNLG (Gardent et al., 2017) focuses on RDF-to-text generation, comprising around 38K samples with an average text length of 30 words. Both DART and WebNLG are open-domain datasets.\nWe incorporate three widely used LLM families in our experiments: Pythia, OPT, and BLOOM. Examining multiple families offers broader insights than focusing on a single family. Pythia is a suite of eight decoder-only autoregressive models (70M-12B parameters), following a GPT-style (Brown et al., 2020) architecture with flash attention. All Pythia models are trained on the Pile dataset in the same order. We consider OPT (Zhang et al., 2022), which includes six models, each being a decoder-only transformer (130M-13B parameters), trained on datasets including Reddit, the Pile, and ROBERTa, following the training details outlined in (Brown et al., 2020). BLOOM (Scao et al., 2022) is another decoder-only LLM trained on the ROOT dataset. We include six BLOOM models in our study. A summary of these LLM families, their models, and corresponding sizes is provided in Table 1."}, {"title": "Fine-tuning for D2T", "content": "All LLMs are fine-tuned separately on each of the five D2T datasets. Given the large model sizes, full fine-tuning is computationally expensive. To mitigate this, we use QLoRA (Quantized Low-Rank Adapter) (Dettmers et al., 2023), a parameter-efficient fine-tuning method, with a learning rate of 1.00e-04 and r = 16 (reduced rank) for the attention module."}, {"title": "Quantification for Factual Inconsistency", "content": "We define factual inconsistency as the inverse of factual consistency, computed as 1 \u2212 z (where z is the factual consistency score ranging from 0 to 1). To evaluate factual inconsistency in LLMs for D2T, we use four state-of-the-art automatic metrics that strongly correlate with human judgments: ALIGNSCORE (measures consistency through information alignment) (Zha et al., 2023), QAFACTEVAL (assesses consistency via question generation and answering) (Fabbri et al., 2022), SUMMAC-CONV (leverages natural language inference) (Laban et al., 2022), and UNIEVAL-FACT (employs unified training) (Zhong et al., 2022). Given their high agreement with human annotations, these metrics provide a strong foundation for our study."}, {"title": "Decoding Strategies", "content": "Given the importance of decoding strategies in D2T, we include both deterministic (greedy and beam search) and stochastic (nucleus and top-k sampling) methods for a comprehensive analysis. However, due to space constraints, we present here the results using nucleus sampling, while results"}, {"title": "Results", "content": "This section presents our empirical results and the validation framework's evaluation of factual inconsistency scaling in D2T based on the two scaling laws. We report findings from the standpoint of automatic metrics used to assess factual inconsistency."}, {"title": "Findings from ALIGNSCORE", "content": "Figure 3 illustrates both fitted scaling laws for factual inconsistency measured by ALIGNSCORE. Exponential scaling generally outperforms power law scaling, except for minor deviations, such as larger margins of error (MOE) in the BLOOM family for the E2E dataset. Table 2 presents our statistical validation results. Note that we successfully verified the normality assumption on residuals before applying our validation framework. Most Huber loss values are low, confirming the predictive reliability of both scaling laws. However, in several cases within the BLOOM family, one or both scaling models fail the goodness-of-fit test, while both pass for OPT and Pythia. This confirms that reliable predictive performance alone is not always sufficient to pass the goodness-of-fit test. Stage III results (Table 2) indicate that exponential scaling is generally preferred over power law scaling, except"}, {"title": "Findings from QAFACTEVAL", "content": "Figure 4 suggests that when factual inconsistency is measured using QAFACTEVAL, both scaling laws fit well across most datasets and LLM families. Power law scaling shows a larger margin of error (MOE) compared to exponential scaling, with extremely high MOE for OPT and BLOOM families on the E2E dataset. Here we also verified the normality assumption before applying our validation framework. Low losses in stage I results (Table 3) indicate strong predictive performance for both scaling laws. Stage II and III results (Table 3) show that both scaling law not qualify for goodness-of-fit in the BLOOM family, while both laws are qualified for goodness-of-fit in Pythia and OPT, with exponential scaling outperforming power law scaling. Thus, based on Pythia and OPT, we observe that exponential scaling appears more suitable when factual inconsistency is measured using QAFACTEVAL in D2T."}, {"title": "Findings from SUMMAC-CONV", "content": "Figure 5 shows both fitted scaling laws when factual inconsistency is measured using SUMMAC-CONV. In most cases, exponential scaling provides a better fit than power law scaling. Some datasets, like E2E, exhibit a slightly larger margin of error (MOE), particularly in the BLOOM model family, which can be considered a minor exception. From Table 4, we observe that, except for a few cases in WikiTableText, the Huber loss values remain low, ensuring the predictive quality of both scaling laws. Additionally, both scaling law fails to qualify in several goodness-of-fit tests across multiple datasets in BLOOOM family. However, in the other two LLM families, where both scaling laws pass the test, exponential scaling consistently outperforms power law scaling."}, {"title": "Findings from UNIEVAL-FACT", "content": "Figure 6 illustrates how both scaling laws perform across all LLM families and D2T datasets when measuring factual inconsistency with UNIEVAL-FACT. Table 5 presents the validation framework results, consistently showing that exponential scaling captures factual consistency better than power law scaling. The only exception is WikiTableText in the BLOOM family (highlighted in yellow), where power law scaling surpasses exponential scaling."}, {"title": "Discussion", "content": "Our results demonstrate that when factual inconsistency is measured using the four automatic metrics, exponential scaling consistently outperforms power"}, {"title": "Conclusion", "content": "This paper shows that factual inconsistency in D2T generally follows exponential scaling with respect to LLM size, rather than the commonly assumed power law scaling. Our findings are validated through a structured three-stage statistical framework, ensuring robustness in our conclusions. Moreover, we conduct a comprehensive empirical study using three major LLM families across five D2T datasets, measuring factual inconsistency inversely with four state-of-the-art consistency met-"}, {"title": "Limitations", "content": "While our study provides a thorough empirical analysis of scaling laws for factual consistency in LLMs, validated through a structured three-stage framework, it is important to acknowledge the following limitations:\nEmpirical basis without theoretical guarantee. Our findings are entirely based on empirical observations, relying on the datasets and LLM families incorporated in this study. We do not provide a formal theoretical guarantee for the observed scaling behavior, making our conclusions inherently dependent on the data and models used.\nNon-universality of scaling law parameters. Scaling law parameters are not universally applicable across different datasets, models, and task domains. While our results indicate a strong preference for exponential scaling, this does not guarantee that the same trend will persist across all datasets or model architectures, even when using the same set of parameters. Therefore, applying these scaling laws\u2014including our own findings\u2014requires careful consideration and validation within the specific context of use.\nReliance on automated metrics without human evaluation. In this study, factual inconsistency is estimated inversely from automated factual consistency metrics. While these metrics have demonstrated strong correlations with human judgments, we do not incorporate direct human evaluations of factual consistency. This remains a limitation of our work, though it presents a clear direction for future research to further validate and refine our findings with human annotation studies."}, {"title": "Appendix", "content": "In the main paper, we discussed the crucial role of decoding strategies in data-to-text generation (D2T) and presented results based on nucleus sampling. Here, we extend our analysis by presenting empirical results from our validation framework for both power law and exponential scaling, using three additional decoding strategies\u2014greedy, beam search, and top-k decoding. Among these, greedy and beam search are deterministic, while top-k decoding falls under stochastic methods. For"}, {"title": "Discussion", "content": "Across all three decoding strategies, we consistently observe that exponential scaling outperforms power law scaling in nearly all cases for the Pythia and OPT LLM families across the four factual inconsistency metrics. While this trend is dominant, we identify a few noteworthy exceptions:\nGoodness-of-fit test failures in BLOOM and OPT. In the BLOOM and OPT families, we frequently find cases where the scaling laws fail to qualify the goodness-of-fit test (Stage II), despite demonstrating low predictive loss in Stage I. This indicates that strong predictive performance alone is not always sufficient for a model to align well with the expected scaling trend. In other words, higher predictive performance does not necessarily imply goodness-of-fit.\nHigh margin of error in E2E dataset. The margin of error (with a 95% confidence interval) tends to be significantly higher in the E2E dataset, particularly for the BLOOM and OPT model families. This suggests a higher variance in factual inconsistency measurements, potentially due to dataset-specific characteristics or the way these models generalize.\nAberrant behavior in E2E and ViGGO with deterministic decoding. A particularly intriguing anomaly is observed in the E2E and ViGGO datasets (Figures 7 to 14), where factual inconsistency increases with LLM model size under deterministic decoding strategies (greedy search and beam search). This contradicts the general trend seen with stochastic decoding strategies (nucleus and top-k sampling), where inconsistency decreases with model size. We hypothesize that this aberrant behavior may be attributed to one or both of the following factors:\nDeterministic decoding bias. Since greedy search and beam search select high-likelihood tokens, they might reinforce factual errors present in the training data rather than mitigating them.\nClosed-domain nature of E2E and ViGGO. These datasets focus on"}]}