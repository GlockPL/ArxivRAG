{"title": "3D ADAPTIVE STRUCTURAL CONVOLUTION NETWORK FOR\nDOMAIN-INVARIANT POINT CLOUD RECOGNITION", "authors": ["Younggun Kim", "Beomsik Cho", "Seonghoon Ryoo", "Soomok Lee"], "abstract": "Adapting deep learning networks for point cloud data recognition in self-driving vehicles faces\nchallenges due to the variability in datasets and sensor technologies, emphasizing the need for\nadaptive techniques to maintain accuracy across different conditions. In this paper, we introduce\nthe 3D Adaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for 3D\npoint cloud recognition. It combines 3D convolution kernels, a structural tree structure, and adaptive\nneighborhood sampling for effective geometric feature extraction. This method obtains domain-\ninvariant features and demonstrates robust, adaptable performance on a variety of point cloud\ndatasets, ensuring compatibility across diverse sensor configurations without the need for parameter\nadjustments. This highlights its potential to significantly enhance the reliability and efficiency of\nself-driving vehicle technology.", "sections": [{"title": "1 INTRODUCTION", "content": "Self-driving vehicles have ushered in a new era of transportation, promising to improve safety, efficiency, and con-\nvenience with their advanced capabilities. Central to their operation is the accurate perception of the surrounding\nenvironment, a task crucially supported by Light Detection and Ranging (LiDAR) sensors. Distinct from cameras and\nradar, LiDAR excels by providing high-precision 3D spatial data. Operating across various channels, such as 16, 32,\nor 64, and rotating 360 degrees, LiDAR is able to create a comprehensive point cloud of obstacles over substantial\ndistances, a capability that is crucial for navigation in urban areas. This feature is particularly beneficial for the effective\ndetection of 3D objects and navigating complex environments filled with numerous objects and distinct features.\nAdapting deep learning networks for point cloud data recognition introduces distinct challenges, especially with domain\nvariability illustrated in Fig. 1 (a), such as differences in dataset characteristics or sensor manufacturers. Research in\nthis area primarily explores how diversity in datasets affects deep learning models. It is observed that models trained\non point cloud data under one set of conditions or with a specific sensor often face performance issues when applied\nto data from different conditions or sensors. For instance, a shift from training on high-density point cloud data to\ntesting on lower-density data, due to cost considerations, can lead to a noticeable drop in model performance. This\nscenario highlights the sensitivity of deep learning models to the specific attributes of point cloud data, emphasizing the\nnecessity for adaptive techniques capable of handling variations in data domains and acquisition methods with different\nplatforms or sensor suites.\nPointNet [1], the first network to encode point cloud data, revolutionized its treatment by organizing it into one-\ndimensional arrays for processing through a multi-layer perceptron (MLP). This approach preserves the data's un-\nstructured nature via max-pooling across MLP layers without converting it to structured formats. However, despite its\npioneering method and improvements by PointNet++ [2], it overlooks the inherent 3D structure of point cloud data.\nInstead, it uses a rotation matrix to approximate spatial relationships, which can lead to overfitting and struggles with\ndifferent sensor setups. This limitation hampers its ability to handle the dynamic nature of point cloud data across\nvarious environments. Additionally, current encoding methods, including PointNet's, often fail to capture the full 3D\ncontext, simplifying environmental variations such as different vegetation types or road configurations.\nMeanwhile, graph-based methods such as Dynamic Graph CNN (DGCNN) [3] can explain 3D geometric information\nmore deeply, utilizing graph-based convolution networks alongside a k-nearest neighbor strategy to form dynamic\ngraphs that enhance local feature understanding. This approach facilitates a nuanced comprehension of point-to-\npoint relationships, contributing to more sophisticated feature extraction. However, DGCNN's complexity introduces\ncomputational challenges and a dependency on the precision of input graphs, where suboptimal graph constructions can\nundermine the outcomes. Addressing these complexities requires innovative solutions to ensure rigid and comprehensive\nfeature extraction.\nIn this paper, we present a 3D Adaptive Structural Convolution Network (3D-ASCN) as the optimal method for encoding\nin LiDAR-based 3D recognition tasks within the self-driving dataset. By electing optimal neighborhoods of each point,\napplying 3D convolution kernels in combination with a 3D geometric tree structure, and utilizing cosine similarity\nand Euclidean distance, our approach effectively extracts geometric features that are more appropriate and maintains\nconsistency in features extracted from LiDAR data, irrespective of regional variations or changes in LiDAR sensors.\nThrough our proposed method of structural encoding, we have achieved significant improvements in performance across\ndifferent domains, including changes in datasets or LiDAR sensor configurations. The main contributions of this study\ncan be summarized in three folds as follows:\n\u2022 We propose novel LiDAR-invariant 3D convolution kernels to train 3D structural perspective by combining\nCosine similarity and Euclidean distance term. Our network shows stability and consistent performance in\nvarying point cloud datasets.\n\u2022 An adaptive neighborhood sampling method is proposed, based on principal components of the 3D covariance\nellipsoid, which allows us to exploit highly notable geometric features by selecting the optimal neighborhood\nnumbers.\n\u2022 The proposed 3D-ASCN method demonstrates domain-invariant feature extraction across diverse types of\nLiDAR data and platforms. The robust performance is demonstrated with a variety of real-world point cloud\ndatasets, showcasing the structural features' domain invariance. When tested on high-resolution point clouds\nfrom vastly different areas and resolutions, the model maintains its highest robustness and adaptability to\nvarious sensor configurations without necessitating parameter adjustments.\nThe subsequent sections of this paper are organized as follows. Section 2 presents a comprehensive literature review\nof the point cloud encoding networks with respect to LIDAR object classification. Section 3 illustrates the proposed\n3D-ASCN configuration and classification network architecture. In Section 4, the paper demonstrates the evaluation of\nthe proposed adaptation concerning varying LiDAR channels. Lastly, Section 5 concludes the paper."}, {"title": "2 RELATED WORK", "content": "2.1 3D point cloud-based Networks\n2.1.1 Pointwise MLP networks\nPointNet [1] processes unordered 3D point cloud data using shared fully connected layers and channel-wise max-pooling\nto extract global features. However, it primarily learns key point representations and does not encode local geometric\ninformation, making it sensitive to variations in translation and scaling. To address this, researchers have proposed\nsorting 3D points into ordered lists where neighboring points have smaller Euclidean distances. For example, one study\n[4] sorts points along different dimensions and uses Recurrent Neural Networks (RNN) to extract features, while others\n[5, 6] use a kd-tree to convert 3D points into a 1D list followed by 1D CNNs. However, this approach can struggle\nto preserve local geometric information. PointMLP [19] asserts that detailed local geometric information might not\nbe essential for point cloud analysis. It utilizes a pure residual network without a complex local geometry extractor.\nInstead, it includes a lightweight geometric affine module, which significantly improves inference speed. PointMLP\nshows outstanding performance on both the ModelNet40 dataset [20], which is ideal for CAD-based analysis, and the\nScanObjectNN dataset [21], which represents real-world indoor environments. PointNet++ [2] based on a hierarchical\nfeature learning paradigm to recursively capture local geometric structures demonstrates promising results and has\nbecome the cornerstone of modern point cloud methods due to its local point representation. Based on PointNet++,\nPointNeXt [22] improves the training and training strategies to enhance the performance of PointNet++ and introduces\na separable MLP along with an inverted residual bottleneck design within the PointNet++ framework. PointNeXt\nsurpasses state-of-the-art methods in 3D classification.\n2.1.2 Graph convolution networks\nThe Dynamic Graph CNN (DGCNN) [3] represents a significant advancement in 3D point cloud processing with its\ngraph-based convolutional networks. It uses a k-nearest neighbor algorithm to create a dynamic graph that captures\nlocal point-to-point relationships for improved feature extraction. However, DGCNN is computationally intensive due\nto the complexity of dynamic graph adjustments. Its performance also hinges on the quality of the input graph, where\nsuboptimal or noisy graphs can affect results, and refining the graph construction process is complex. To improve\ngraphical point cloud relationships in convolution, Lin et al. propose a 3D Graph Convolutional Network [7] to learn\ngeometric properties of point clouds. Their approach stands out for considering geometric values and adjusting network\nparameters through directional analysis. The model uses deformable graph kernels to handle translation, scale, and\nz-axis rotation. However, its effectiveness is mainly evaluated on ideal CAD-based datasets, which do not fully represent\nrealistic occlusion scenarios in point cloud data. Additionally, it does not adequately account for the distances between\npoints, potentially missing detailed interval nuances during parameter training, which is crucial for self-driving datasets\nwith consistent but small-scale patterns despite variations in LiDAR technology. Furthermore, although both DGCNN\nand GCN adopt the number of the neighborhoods as a constant value when selecting neighborhoods, each point in the\npoint cloud will probably require a different number of neighborhoods. Accordingly, not only may this approach be\nable to extract distorted local region features, but if the neighborhood is set to be large, the degree of distortion will\nincrease as the network becomes more advanced. Therefore, the optimal number of neighborhoods must be selected for\nevery point by variably adjusting the number of neighborhoods.\n2.2 Adaptive neighborhood sampling\nTo describe the 3D structure surrounding a point using geometric features, it is essential to define a local neighborhood\nthat encompasses all considered 3D points. Various strategies exist for defining these local neighborhoods around a\nspecific 3D point. The most commonly used neighborhood definitions include Spherical Neighborhood [13], Cylindrical\nNeighborhood [12], and K-Nearest Neighborhood (K-NN) [14]. Spherical neighborhood approach involves defining the\nneighborhood as all 3D points within a sphere of a predetermined radius centered on the point. Cylindrical neighborhood\nmethod consists of all 3D points whose 2D projections fall within a circle of a fixed radius around the point's projection.\nThe k-nearest neighbors definition uses a fixed number of the closest neighbors to the point in 3D space.\nThese methods, which rely on a constant scale parameter, offer a simple way to select neighborhoods. The parameter is\neither a fixed radius or a set number of neighbors. However, the choice of scale parameter often depends on heuristic\nor empirical knowledge across point cloud datasets. To circumvent making strong assumptions about the local 3D\nneighborhoods of each point, more recent studies have aimed at determining an optimal neighborhood size for each\nindividual point, thereby enhancing the uniqueness of the derived features. Most approaches focus on optimizing the\nnumber of closest neighbors (k) for each point, which could be done through the methods that consider factors like"}, {"title": "3 3D ADAPTIVE STRUCTURAL CONVOLUTION NETWORK (3D-ASCN)", "content": "Our 3D adaptive structural convolution network (3D-ASCN) is specifically designed to process data captured by LIDAR\npoint cloud sensors. This network ingests in 3D point cloud data and analyzes it to identify important geometric\nstructures. Specifically, 3D-ASCN extracts 3D structural context features, including geometric information, ensuring\nperformance does not depend on the point density of the dataset and enables domain-invariant point cloud recognition.\n3.1 Receptive Field for 3D-ASCN\nA 3D point cloud object is referred to as P, consisting of N points represented by P = (p_n|n = 1, 2, . . ., N). p_n\nrepresents a 3D coordinate within the three-dimensional space, thus belonging to the set $R^3$. For each point in the\npoint cloud, a specific derived feature, F(p), is associated, which is a vector of dimension D. This vector, F(p) \u2208 $R^D$,\nencapsulates certain characteristics or attributes of the point. In order to grasp the local structural and geometric features\nassociated with each point $p_n$, we define the receptive field, denoted as $R^M$, which consists of M neighborhood points.\nThis receptive field for the point $p_n$, with a size of M, is established as shown in Fig. 1 (b) and defined as:\n$R^M = \\{p_n,p_m| \\forall p_m \u2208 N(p_n, M)\\}$\nwhere N(p_n, M) formally represents the M nearest neighbor points of $p_n$, and the directional vectors $d_{m,n}$ =\np_m - p_n will be used for later farthest neighborhood selection among points $p_m$ in a receptive field and for structural\nconvolution purposes. The features encompassed within the input points $p_n$, with a size of M, can be expressed as\n$\\{F(p_n), F(p_m)| \\forall p_m \u2208 N(p_n, M)\\}$. These features are computed during the structural convolution operation.\n3.2 3D Structural Kernels\nTo perform convolution in 3D point cloud structures, we compose direction-based kernels and distance-based kernels.\n3.2.1 Direction-based kernel\nSimilar to other 3D Graph Convolution Kernel concepts [7], Kernel $K^S$ is computed for graph computation. We design\nit as a direction-based kernel, denoted as $K^S_{dir}$, where S indicates the number of branches. More precisely, it consists of\nS branches to train the directions and weights of the directional vectors. In order to define the direction, $K^S_{dir}$ requires\nthe center kernel $k_{dir}^c$ = (0,0,0), from which each support includes $k_{dir}^s, k_{dir}^s, k_{dir}^c$. $K^S_{dir}$ is composed of the\ncenter $k_{dir}^c$ and supports for the directional vectors $k_j \u2208 R^3$ and its combination $K^S_{dir} = \\{k_{dir}^c, k_{dir}^s, k_{dir}^s,..., k_{dir}^s\\}$.\nThen, with direction-based weight vectors defined as w(kdir) \u2208 RD for each kernel point $k_{dir}^s$, we achieve a part of the\nconvolution operations through the weighted sum of features corresponding to the directional weights of F(p). Since\nit is designed to train the structures in 3D, the directional vectors, represented by $k_{dir}^s - k_{dir}^c = k_{dir}^s$, are among the\ntrained kernel parameters.\n3.2.2 Distance-based kernel\nWe propose a distance-based kernel, denoted as $K^S_{dist}$, to reflect the distance between each point as a learning\nparameter. Simillar to the direction kernel, S denotes the number of branches, and each branch contains weights for the\ndistance relationship between points, defined as $w(k_{dist}) \u2208 R^D$. Then, we perform a part of convolution operations\nvia the weighted sum of features corresponding to the distance-based weights of F(p). Therefore, with both the\ndirection-based kernel and distance-based kernel included, the total trainable kernels in this network are defined as\n$\\{w(k_{dir}^s), (k_{dir}^c, w(k_{dir}^s)), w(k_{dist}^s), w(k_{dist}^s) | s = 1,2,..., S\\}$"}, {"title": "3.3 Adaptive neighborhood sampling", "content": "Since our network obtains local region geometric information through the distance and directional relationships\nbetween each point $p_n$ in the point cloud and the neighborhoods, all $p_n$ require different neighborhood sizes based on\ngeometric properties. Additionally, the point density in the captured 3D point cloud data varies with the type of LiDAR\nsensor, and this density significantly influences the selection of the number of neighborhoods. Our 3D-ASCN is not\nonly capable of extracting excellent geometric information from a point cloud by selecting the optimal number of\nneighborhood points for each point $p_n$, but it also remains invariant in LiDAR channel changes.\nK-NN [14] algorithm is applied to each point as the neighborhood search method for $p_n$, allowing more flexibility\nconcerning the geometric size of the neighborhood. Instead of arbitrarily choosing a fixed number for the receptive\nfield parameter M, we opt for an automatic approach to determine the best value for M. Considering a point cloud\ncomposed of N points in three-dimensional space, with M as a variable from the set of natural numbers N, each point\npn = (X, Y, Z)\u2191 lies in R3, and its M-neighborhood defines the extent of each receptive field $R^M$. To capture the\nlocal structure around each point $p_n$, we compute the 3D covariance matrix, also referred to as the structure tensor\nS, which is a symmetric positive definite matrix in $R^{3\u00d73}$. This tensor always has three non-negative eigenvalues\n$\u03bb_1, \u03bb_2, \u03bb_3 \u2208 R$ that are ordered such that $\u03bb_1 \u2265 \u03bb_2 \u2265 \u03bb_3 \u2265 0$ and correspond to an orthogonal set of eigenvectors. To\ndetermine the optimal neighborhood size for each point $p_n$, eigenvalues representing the principal components [11] of\nthe 3D covariance ellipsoid are obtained. The neighborhood sizes M of pn is optimized by minimizing the entropy of\nthese eigenvalues, known as eigenentropy, defined as Eq. (2), which serves as a quantification of the spatial coherence\nor variability within the ellipsoid.\n$E_\u03bb = -(e_1 ln(e_1) + e_2 ln(e_2) + e_3 ln(e_3))$\nwhere $e_j = \u03bb_j / \\sum_{i=1}^{3}\u03bb_i (j = 1,2,3)$, representing each normalized eigenvalue. Specifically, we measure the\neigenentropy in $[M_{min}, M_{max}]$, indicating the number of potential neighbors of $p_n$, and all integer values within this"}, {"title": "3.4 Structural Convolution Operation", "content": "Using the specified inputs and kernel definitions for 3D point cloud data, we propose the 3D Structural Convolution\nprocess, as shown in Fig. 2. We determine the similarity between the receptive field $R^M$ and the direction-based kernel\n$K^S_{dir}$, represented as $Conv_{dir}(R^M, K^S_{dir})$, to derive point-specific features through cosine similarity. Additionally,\nwe calculate the product of the distance to the farthest neighbor in $R^M$ and the weights in $K^S_{dist}$, represented as\n$Conv_{dist}(R^M, K^S_{dist})$, to obtain point-specific local features via Euclidean distance. These extracted features are then\nconcatenated and processed through an MLP, enabling accurate feature extraction of the structural context without\nintroducing bias towards either distance or directional context during training.\nUnlike 2D CNNs, where kernels and image patches both have grid structures, our method involves combinations of\n3D vectors instead of grids. For the purpose of convolution in 3D point clouds, we achieve our structural convolution\noperation with direction-based kernels and distance-based kernels. Initially, we assess the similarity between features\nwithin the receptive field of pn (i.e., F(pn), F(pm) for all pm \u2208 N(pn, M), as specified in Eq. (1)) and the weight\nvectors of the directional kernel $K^S_{dir}$, centered around $k_{dir}^c$ with S supports (namely, $w(k_{dir}^s), w(k_{dir}^s)$ for all s =\n1,2,..., S). We consider every pairing of (pm, $k_{dir}^c$). Consequently, the direction-based convolution between a\nreceptive field and a direction-based kernel can be described as follows:\n$Conv_{dir} (R^M, K^S_{dir}) = (F(p_n), w(k_{dir}^c)) + \\sum_{s=1}^{S} \\underset{m\u2208(1,M)}{max} sim(p_m, k_{dir}^s)$\nwhere the symbol (\u00b7) denotes the inner product operation, and the function sim calculates the inner product between\nthe features F(pm) and the directional weights $w(k_{dir}^s)$, utilizing cosine similarity [7] to define this interaction:\n$sim(p_m, k_{dir}^s) = (F(p_m), w(k_{dir}^s)) \\frac{(d_{m,n}, k_{dir}^s)}{||d_{m,n}|| ||k_{dir}^s||}$\nMoreover, to account for the influence of spatial relationships among neighboring points on structural characteristics,\nwe assess the Euclidean distances from point $p_n$ to its most distant neighbor and then multiply them with the weights\nof the kernel $K^S_{dist}$ centered at $k_{dist}^c$ (precisely, $w(k_{dist}^s), w(k_{dist}^s),\u2200s = 1,2,..., S$). we consider all conceivable\ncombinations of (pm, $k_{dist}^c$) to perform the distance-based convolution that merges the a receptive field with the kernel,\nexpressed as:\n$Conv_{dist}(R^M, K^S_{dist}) = \\sum_{s=1}^{S} w_{dist}^s \\underset{m\u2208(1,M)}{max} (||d_{m,n}||)$\nNote that, since Eq. (3) indicates the convolution between a receptive field and a direction-based kernel, and Eq. (5)\nindicates the convolution between a receptive field and a distance-based kernel, the convolution operations in Eq. (3)\nand Eq. (5) must be applied to all receptive fields, all direction-based kernels, and all distance-based kernels. Finally,\nby concatenating the outputs from the above convolution operations and passing them through an MLP, 3D structural\nconvolution is achieved as follows:\n$Str - Conv(R^M, K^S_{dir}, K^S_{dist})$\n$= MLP(Conv_{dir} (R^M, K^S_{dir}) Conv_{dist} (R^M, K^S_{dist}))$\nConcatenating distance-based features and direction-based features, and then processing them through a Multi-Layer\nPerceptron (MLP), enables learning without bias towards either direction-based or distance-based kernels.\n3.5 Classification Architecture\nIn the design of the 3D-ASCN, we have modified the convolution and pooling layers. The framework consists of five\nconvolution layers and three max-pooling layers, with adjustments made to the number of parameters to suit our needs"}, {"title": "4 EVALUATION", "content": "The proposed algorithm is evaluated through comparisons with PointNet [1] and DGCNN [3]. The newtwork is\nevalauted through the object classification accuracy and the evaluation metric [15, 16] is shown as follows:\n$Accuracy(%) = \\frac{\\text{the # of correctly classified classes}}{\\text{the # of total ground truths}} \u00d7 100$\n4.1 Implementation Detail\nOur model incorporates three pivotal components, including adaptive neighborhood sampling, direction-based kernel,\nand distance-based kernel. In the beginning, the 10 nearest neighbors is sampled for each point using the KNN algorithm\n[14]. The 3D-ASCN then evaluates the eigenentropy for each possible neighborhood size, from 3 to 10, to select\nthe optimal number of neighbors. To accommodate kernels requiring a consistent size, the maximum neighborhood\nsize previously determined is used. For cases with fewer than 10 neighbors, blanks are processed as the central\nlocation to ensure uniformity in input size, with the distance and cosine similarity of the central point set to zero. The\n3D-ASCN applies both direction-based and distance-based kernels across all layers, with each layer featuring a fully\nconnected (FC) layer for extracting distance features and another for direction features. This setup includes stacking\nfive convolutional layers in a manner akin to 3D-GCN [7].\nExperiments are conducted with our model on three outdoor point cloud datasets: KITTI [17], nuScenes [18], and\nPanKyo [23]. These datasets are provided in the given link\u00b9 and originate from various countries and manufacturers.\nThis diversity is crucial for assessing not only the model's inferencing capabilities but also its robustness across multiple\ndomain shifts. Especially, they all have different channels. nuScenes has 32 channels, KITTI has 64 channels, while\nPanKyo has 128 channels. We evaluated the robustness of our model using these channel shifts. In addition, to\nseamlessly align the existing classes, we reduce the number of classes to three: Pedestrian, Car, and Truck.\n4.2 Classification on single-domain\nA detailed evaluation was conducted to assess the algorithm's performance, particularly its behavior under conditions that\nmight lead to overfitting, by examining it from within-domain perspectives. Tab. 1 displays the classification performance\nwithin a singular domain. While 3D-ASCN shows a tendency towards underfitting in the general automotive dataset, a\nconsequence of removing scale invariance and possessing low inductive bias, 3D-ASCN demonstrates performance on\npar with models known to overfit in specific domains, such as PointNet, DGCNN, pointMLP, and pointNeXt."}, {"title": "4.3 Channel shift robustness", "content": "An evaluation was conducted across different domains to assess the models' robustness in various contexts. The\nexperiments specifically aimed to examine the performance and robustness of models by training them in one domain\nand testing them in another, with a focus on domain invariance. These findings offer crucial insights into the models'\nperformance across various datasets, facilitating a comprehensive assessment of their effectiveness.\nTab. 2 shows the performance comparison between 3D-ASCN and other methods for the cross-domain classification\ntask of autonomous driving datasets. Our method demonstrates classification performance that consistently ranks\namong the top two across all domain shift scenarios. It especially outperforms the second-best methods by 26.3% when\nshifting from nuScenes to KITTI, by 30.5% when transitioning from nuScenes to Pankyo, and by 7.4% when moving\nfrom Pankyo to nuScenes. Moreover, the average accuracy of 3D-ASCN surpasses the second-best method, pointMLP,\nby 25.3%, as illustrated in Tab. 3.\nConsequently, Both the kernels based on cosine similarity, the kernels based on Euclidean distance, and adaptive\nneighborhood selection methods were effective in enabling structural learning capabilities amid general changes in\ndatasets."}, {"title": "4.4 Ablation study", "content": "The key to Structural Convolution is to extract features of 3D structural context by combining Cosine similarity and\nEuclidean distance terms and, through this, ensure that the model has robustness in situations where the LiDAR sensor\nchanges. For this purpose, we considered three methods of convolution operations, as shown in Tab. 4. When using\nonly direction-based kernels, performance deteriorates in situations where the Lidar sensor is changed. In contrast, the\nrobustness of the model can be obtained by using both direction-based kernels and distance-based kernels. Moreover,\nConcatenating distance-based features and direction-based features, and then passing them through a Multi-Layer\nPerceptron (MLP), enables learning without bias towards either direction-based or distance-based kernels. Therefore,\nour structural convolution achieved the best performance both in situations where the domain was constant and in\nsituations where the domain changed.\nTab. 5 illustrates the impact of adaptive neighborhood sampling. Specifically, we set the fixed M to three. The network's\nperformance improves by selecting the number of neighbors adaptively, particularly in situations involving domain\nchanges, showing performance enhancement. When trained and tested within the same domain, such as both being\nin Pankyo, the performance was the same because the optimal number of neighbors was close to three. These results\ndemonstrate that adaptive M generally offers great utility in scenarios involving cross-domain changes."}, {"title": "5 CONCLUSION", "content": "In this study, we introduce the 3D-ASCN, a novel model designed for feature extraction with minimal inductive\nbias. The 3D-ASCN framework incorporates a Distance Feature Extraction technique alongside an Adaptive Nearest\nNeighbor approach, strategically devised to mitigate the scale-invariant bias inherent in the GCN. Consequently, while\n3D-ASCN demonstrates modest inference performance within a singular domain, it exhibits enhanced robustness\nagainst domain shifts, including channel reduction. This adaptability is particularly significant in addressing the\nsensor-dependency challenge prevalent in point cloud classification tasks, thereby potentially advancing the versatility\nof autonomous driving technologies. In light of these findings, our future research endeavors will focus on exploring\nefficient learning methodologies utilizing small datasets. Specifically, we aim to investigate self-supervised learning\nparadigms based on raw point clouds, with the objective of achieving superior performance within individual domains."}]}