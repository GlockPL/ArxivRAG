{"title": "EvRepSL: Event-Stream Representation via Self-Supervised Learning for Event-Based Vision", "authors": ["Qiang Qu", "Xiaoming Chen", "Yuk Ying Chung", "Yiran Shen", "Senior Member", "IEEE"], "abstract": "Event-stream representation is the first step for many computer vision tasks using event cameras. It converts the asynchronous event-streams into a formatted structure so that conventional machine learning models can be applied easily. However, most of the state-of-the-art event-stream representations are manually designed and the quality of these representations cannot be guaranteed due to the noisy nature of event-streams. In this paper, we introduce a data-driven approach aiming at enhancing the quality of event-stream representations. Our approach commences with the introduction of a new event-stream representation based on spatial-temporal statistics, denoted as EvRep. Subsequently, we theoretically derive the intrinsic relationship between asynchronous event-streams and synchronous video frames. Building upon this theoretical relationship, we train a representation generator, RepGen, in a self-supervised learning manner accepting EvRep as input. Finally, the event-streams are converted to high-quality representations, termed as EvRepSL, by going through the learned RepGen (without the need of fine-tuning or retraining). Our methodology is rigorously validated through extensive evaluations on a variety of mainstream event-based classification and optical flow datasets (captured with various types of event cameras). The experimental results highlight not only our approach's superior performance over existing event-stream representations but also its versatility, being agnostic to different event cameras and tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "INSPIRED by human vision system, Silicon Retina [1] has shown a novel and effective form of perceptual sensing, kindling the nascent discipline of neuromorphic engineering. In recent years, this bio-inspired technology has attracted considerable attention from both academia and industry due to the availability of prototype event cameras or Dynamic Vision Sensors (DVS) [2]\u2013[4]. Event cameras show a number of unique advantages over active pixel sensors (APS), i.e., the standard frame-based cameras, in a range of computer vision tasks [5]\u2013[10] including visual odometry/SLAM [11]\u2013 [13], object classification [14]\u2013[17], object tracking [18], and optical flow estimation [19]\u2013[22]. Different from conventional frame-based cameras, which create synchronized frames at predetermined rates, the pixels of event cameras are capable of independently capturing microsecond-level intensity changes and generating a stream of asynchronous \u201cevents\u201d. The unique design of event cameras enables several benefits over frame- based cameras. First, the temporal resolution of event cameras is up to tens of microseconds, allowing them to record intricate motion phases or high-speed motions without blurring or rolling shutter issues. Second, event cameras enable a substantially higher dynamic range (up to 140dB [2]) than the frame-based cameras (60dB), allowing them to function well under challenging lighting conditions. Third, they consume significantly less resources, including energy, bandwidth, and processing power, since events are sparse and only triggered when intensity changes happen. For instance, DVS128 sensor platform consumes 150 times less energy than a conventional CMOS camera [2]. These properties make event cameras desirable for computer vision tasks with specific latency, resource, and operating environment constraints.\nEvent cameras produce a stream of events that encode the time, location, and polarity (positive or negative) of brightness changes. Consequently, each individual event conveys little information about the scene. This characteristic of event- streams necessitates the use of integration encoding or repre- sentation techniques that aggregate both spatial and temporal information prior to undertaking any event-based applications. Moreover, the unique spatial-temporal structure makes it dif- ficult to be processed using well-researched computer vision algorithms. Some researchers address this issue by introducing spiking neural networks (SNNs) to process events as spik- ings [14], [23]\u2013[25]. However, the lack of specialized hardware and efficient backpropagation methods limits the use of SNNs. Other researchers have developed event-stream representa- tions to convert event-streams into formatted-tensors that are compatible with cutting-edge computer vision methods [16], [19]\u2013[21], [26]. Voxel-grid representation [20], for instance, represents event-streams in a space-time histogram in which each voxel accounts for a particular pixel and time interval. This representation is compatible with convolutional neural networks (CNNs) and preserves both spatial and temporal information.\nHowever, the majority of existing event-stream representa- tions suffer from the noisy nature of event-streams. Baldwin et al. [27] summarized four types of random noise in event- streams, including background activities (events occurring with no intensity changes), holes (events not occurring with intensity changes), stochastic arrival time of the events, and stochastic number of events for a determined intensity change. They have substantial influence on the precision and band-"}, {"title": "II. RELATED WORK", "content": "In this section, we review the works related to event-stream representation. In general, representations of event-streams can be categorized as either event-spikings or formatted-tensors. The event-spikings preserve the original property of event- streams (especially asynchrony) and feed to recurrent models like SNN for vision tasks such as object classification [14], [23], [24], [30], [31], and optical flow estimation [25], [32]. However, the absence of specialized hardware and com-putationally efficient backpropagation methods still restricts the application of SNNs in complex real-world applications. Recursive methods are another options for event-spikings. For example, phased-LSTM [33] adds event timestamps as an additional input to recurrent LSTM, in order to keep asynchronous temporal information. Since events are fed se- quentially into models, it is difficult for such models to capture spatial information in long sequences.\n\nRather than focusing on eventwise denoising, we introduce an event-stream representation generator, Rep- Gen, that leverages a theory-inspired self-supervised learning framework to directly enhance the quality of event-stream representation, as conceptually depicted in Fig. 1. Specifically, to preserve the temporal pattern inherent to event-streams, we formulate a novel event-stream representation grounded in spatial-temporal statistics, termed as EvRep. Following this, we theoretically establish the intrinsic relation between asynchronous event-streams and synchronous video frames. Using this established relationship as a foundation, we train our RepGen in a self-supervised manner, taking EvRep as its input. Consequently, event-streams are transformed into superior representations, designated as EvRepSL, via the trained RepGen without subsequent fine-tuning or retraining. Through extensive experiments, we validate that EvRepSL significantly bolsters the performance of event-based tasks. The contributions of this work are summarized as follow:\n\u2022  We propose EvRep, a new event-stream representation based on spatial-temporal statistics to preserve the tem- poral pattern inherent to event-streams for event-based vision, compatible with the conventional computer vision tasks.\n\u2022  Based on the theoretical relation between APS frames and event-streams, we design a novel self-supervised learned RepGen to derive high-quality representation, EvRepSL, from EvRep. The learned RepGen can then generate high-quality representation for event-streams without the need of frames.\n\u2022 Through comprehensive evaluations on diverse event- based tasks and datasets, we demonstrate that our pro- posed EvRepSL can significantly enhance the accuracy of classification and optical flow estimation tasks. More- over, our experimental results underscore the represen- tation's agnosticism towards varying event cameras and tasks (no fine-tuning is needed), highlighting its versatil-"}, {"title": "A. Event-Stream Representation", "content": "The mainstream of event-stream representation is to convert event streams into formatted-tensors [16], [19]\u2013[21], [26], [34], [35]. Sironi et al. [16] convert events into histograms of averaged time surfaces (HATS) for object classification tasks. Gehrig et al. [21] introduce a learnable event encoder for transforming representations into formatted-tensors. Maqueda et al. [26] employ a simpler representation to address steering prediction, where positive and negative polarity events are stacked into two-channel image-like representation. On the top of two-channel representation, Zhu et al. [19] stack two addi- tional channels containing ratios characterizing temporal prop- erties for optical flow estimation. Then spatial-temporal voxel grid [20] is proposed for accommodating spatial-temporal information of event-streams. Bi et al. [36] achieve state- of-the-art performance in event-based object classification, where graphs are constructed based on events and then go through a graph neural network (GNN). Barchid et al. [37] introduce a method that converts asynchronous event streams into sparse, expressive N-bit event frames, achieving state-of- the-art performance and robustness against image corruptions"}, {"title": "B. EfficientNets and UNet", "content": "In this section, we review existing deep learning models to identify a suitable backbone for the proposed self-supervised learning approach, particularly in the context of event repre- sentation learning. Due to the nature of event cameras, which are designed for fast and efficient data capture, it is essential to choose an algorithm that mirrors these qualities. Convolu- tional Neural Networks (CNNs) are generally faster and more efficient than transformer-based architectures, making them a more suitable choice for event-based data processing.\nSeveral CNN architectures have set benchmarks in con- ventional image classification, including VGGNet [48], In- ceptionNet [49], ResNet [50], MobileNet [51], [52], SENet [53], EfficientNet [54], and the recent EfficientNetV2 [55]. Among these, EfficientNetV2 [55] is chosen as the backbone for its superior efficiency in training time and parameter usage, making it particularly well-suited for event representation learning, where speed and resource management are critical.EfficientNetV2 is developed through training-aware neural architecture search and scaling, resulting in a model that minimizes both training time and computational cost, while maintaining high accuracy. In this work, we leverage the MBConv and Fused MBConv layers from EfficientNetV2 to construct the backbone of the proposed self-supervised learning model, RepGen. The encoder-decoder structure of RepGen follows the general design of UNet [56], but with a shared downsampling encoder and two specialized upsampling decoders, optimized for distinct tasks (explained in Section III-B2)."}, {"title": "III. METHODOLOGY", "content": "In this section, we will describe the framework of event- stream representation via self-supervised learning in details. It starts with a new representation of event-streams consisting of three statistical channels, termed as EvRep (see Section III-A). Based on the imaging principle of event camera [2], we theoretically derive a number of key parameters that need to be estimated to improve the quality of the EvRep (see Section III-B1). Then, a event-stream representation generator, RepGen, is trained via a self-supervised manner to learn the key parameters obtaining refined representation of the event-streams, i.e., EvRepSL (see Section III-B2, III-B3). At last, lightweight deep learning models are designed to accommodate the EvRepSL of event-streams for different vision tasks, i.e., classification and optical flow estimation (see Section III-C, III-D)."}, {"title": "A. The Design of EvRep", "content": "We propose the initial representation, EvRep, directly from the raw event stream to preserve as much event information as possible. It is known that each event contains three essential components: a spatial component (x, y), a polarity component p (representing the direction of brightness change), and a temporal component t (the event's timestamp). To comprehen- sively retain these three components, we design EvRep with three pixel-wise statistical channels: the event spatial channel (Ec), which captures the spatial information by recording the number of events at each pixel; the event polarity channel (E1), which captures the polarity of brightness changes by integrating positive and negative events; and the event temporal channel (ET), which preserves the temporal information by accounting for the distribution of events over time.\nSpecifically, in the time interval [to,t1], $n_{x,y}$ events ${x,y}_{i=0}^{n_{x,y}-1}$, are generated at the pixel (x, y) from a w x h image plane. Then the event spatial channel Ec at pixel (x, y) is:\n$E_c(x,y) = n_{x,y}$.\n(1)\nThe event polarity channel E1 is calculated by integrating the events with polarities at any pixel (x, y):\n$E_1(x,y) = \\sum_{i=0}^{n_{x,y}-1} p_i$, where $p\\in \\{-1,1\\}$.\n(2)"}, {"title": "B. Event-stream Representation via Self-supervised Learning", "content": "Event-streams are normally considered as noisy due to the unstable hardware and lighting condition [27]\u2013[29]. Therefore, EvRep from the statistical properties of the event-streams are inherently low-quality. Conventional approaches seek to sup- press the noises in the event-streams directly [27], [57], [58], however, the performance of noise reduction is limited by the issues on obtaining the accurate eventwise groundtruth under practical settings. Instead of denoising the event-streams, we propose a self-supervised learning strategy on EvRep directly to improve its representation quality. Considering the fact that a number of event cameras can generate both event-streams and APS frames simultaneously, we exploit the theoretical correlation between the event-streams and APS frames to refine the event-stream representation in a self-supervised learning approach to derive EvRepSL.\n1) Theoretical Derivation: Before introducing the detailed design of the self-supervised learning framework, we first derive the theoretical relation between the event-streams and APS frames. According to the principal of event camera [2], an event is generated when an log-intensity change is detected (over threshold), i.e.,\n$d_i = log I_{i+1} - log I_i$.\n(6)\nwhere $I_{i+1}$ and $I_i$ are the intensity values of two consecutive events generated at pixel (x, y). The accumulated log-intensity change within [to, t1] (time interval between two consecutive video frames fo and f\u2081) can be approximated as,\n$\\sum_{i=0}^{n_{x,y}-1} d_i = logI_1^{x,y} - logI_0^{x,y} + logI_2^{x,y} - logI_1^{x,y} + logI_3^{x,y} - logI_2^{x,y} ... + logI_1^{x,y} - logI_0^{x,y}$.\n(7)\nBy cancelling out the same items, the equation above can be rewritten as,\n$\\sum_{i=0}^{n_{x,y}-1} d_i = logI_1^{x,y} - logI_0^{x,y}$.\n(8)\nThen dy can be decomposed as,\n$d_i = sign_i^{x,y} * \\theta_i^{x,y}$.\n(9)\nwhere $sign_i^{x,y}\\in \\{-1,1\\}$. $\\theta_i^{x,y}$ is the absolute value of the threshold for producing an event at pixel (x, y). When [to, t1] is the interval between two consecutive frames, we assume $\\theta_i^{x,y}$ is a constant threshold for the pixel px,y within this short period (normally tens of milliseconds).\nFor event-streams, $sign_i^{x,y}$ corresponds to the polarity of the event e, so the integral channel Er of EvRep within [to, t1] can be reformulated as:\n$E_I(x,y) = \\sum_{i=0}^{n_{x,y}-1} sign_i^{x,y}$.\n(10)\nWith equation Eq. (10), the accumulative log-intensity change within [to, t1] can be expressed as,\n$\\sum_{i=0}^{n_{x,y}-1} d_i = \\theta^{x,y} \\sum_{i=0}^{n_{x,y}-1} sign_i^{x,y} = \\theta^{x,y} * E_I(x,y)$.\n(11)\nBy incorporating Eq. (8) into Eq. (11), we have\n$\\theta^{x,y} * E_I(x,y) = logI_1^{x,y} - logI_0^{x,y}$.\n(12)\nThen we assume a linear relationship between intensity I and the corresponding normalized frame f, i.e.,"}, {"title": "2) Self-Supervised Representation Learning", "content": "According to the relation between frames and events derived above, we propose RepGen, trained with a self-supervised learning to estimate the key parameters and improve the quality of the event-stream representation. RepGen can be trained with any event-based datasets containing both video frames and event- streams, e.g., IJRR [13], HQF [60], DDD17 [61] collected by different hybrid frame-event platforms. Then the trained RepGen is a device-and-task-agnostic representation enhancer compatible with any event-based vision task without further fine-tuning.\nAs shown in Fig. 3, RepGen starts with taking the EvRep = {E1, EC, ET} as input to the encoder henc. EvRep is obtained from accumulating the events between the two consecutive frames fo and f1. The encoder consists of multiple convo- lutional and (Fused) MBConv [52], [55] layers followed by MaxPooling for feature extraction. The MBConv utilizes the inverted bottleneck structure [52] and depthwise convolutional layers [51] to improve memory efficiency. In addition, a squeeze-and-excitation unit [53] is inserted in the middle of MBConv in order to adaptively recalibrate channel-wise feature responses. The fused version of MBConv substitutes the depthwise convolutional layers with regular convolutional layers, which has been shown to be more effective in large spatial dimensions [55]. Then two non-sharing decoders haec and haec with the same network architecture (except for the last layer as shown in Fig. 3) are adopted to decode the low- dimensional features with upsampling layers to obtain the estimates E1 and \u03b8. The decoder consists of the blocks of combination of spatial bilinear interpolation and (Fused) MBConv layers. A 1\u00d71 convolutional layer without activation function is applied to finalize haec and Relu activation function is adopted to ensure the non-negative property of \u03b8. At last, according to Eq. (16), the estimate of the next frame, f1, can be outputted with estimated {&1 *,0} and known fo.\nAs f1 is known in the self-supervised learning phrase, RepGen can be trained according to the estimation error of f1. The learning objective is then defined as:\n$\\lbrace \\theta_{henc}, \\theta_{haec}^I, \\theta_{haec}^\\theta \\rbrace = arg\\min_{\\lbrace \\theta_{henc}, \\theta_{haec}^I, \\theta_{haec}^\\theta \\rbrace} \\frac{1}{N} \\sum_{j=1}^{N} ||f_1^j - \\hat{f}_1^j||_1$.\n(17)\nwhere N is the total number of video frames and the loss function is the Mean Absolute Error (MAE) between the estimate and the actual frame f1.\nThe trained RepGen can generate two more channels of representation with EvRep as the input. The first one is a learned integral of events, i.e., \u00carfd and the second one is the"}, {"title": "3) Detailed Design of RepGen", "content": "Inspired by Efficient- Netv2 [55] and UNet [56], we design an efficient W-shape network for RepGen. The detailed design of each layer is displayed in purple font in Fig. 3, where w \u00d7 h is the event- stream resolution and c is a hyper-parameter that determines the model complexity. The detailed parameter settings of all model layers are also outlined in Table I."}, {"title": "C. Classification with EvRepSL", "content": "Once RepGen has been trained in self-supervised learning, it is capable of converting any event-stream collected by different event cameras to EvRepSL. For downstream tasks, the converted EvRepSL as a formatted tensor can be readily feed to deep neural networks. It is worth noting that, different from other event-based representation learning approaches, e.g., EST [21], the weights of RepGen are need not to be fine-tuned to adapt to the downstream tasks. Therefore, the use of RepGen saves significant training cost. Object classification is selected as the primary event-based task to validate our proposed method, given its foundational significance in event- based vision [16], [33], [65]. In the classification task, we adopt the same network architecture as the encoder of RepGen as the backbone and a classification block as the tail, termed as Eff-Classifier. The classification block consists of three layers of 1 \u00d7 1 convolutional layers and a global average pooling layer. Softmax is applied to obtain the final classification results. The loss function of multi-class cross entropy [66] is applied to adapt to the classification task:\n$l_{cls} = - \\sum_{i=1}^{N} \\sum_{c=1}^{NC} Y_{i,c} log(\\hat{y}_{i,c})$.\n(20)\nwhere N represents the number of samples, and C is the number of classes. The term Yi,c is a binary indicator, where Yi,c=1 if class c is the correct classification for sample i, and Yi,c = 0 otherwise. The term \u0177ic denotes the predicted probability that sample i belongs to class c. The loss function computes the sum of the negative log probabilities of the correct class for each sample, providing a measure of how well the predicted probabilities match the true labels.\nNote that the weights of RepGen are frozen after self- supervised learning, allowing EvRepSL to be used for down- stream tasks without requiring fine-tuning. This versatility enables EvRepSL to function similarly to traditional represen- tations, seamlessly integrating with various tasks. Our experi- ments show that, even with a minimal downstream model Eff- Classifier, EvRepSL outperforms significantly larger models that rely on existing representations, proving its capacity to boost performance in diverse architectures. Since EvRepSL is a structured tensor, it is compatible with any traditional classi- fiers. In our experiments, we utilize a very lightweight model, Eff-Classifier, to demonstrate its effectiveness. Additionally, we include results from a larger model, ResNet50 [50], in the ablation study to validate whether the proposed representation can enhance performance across different model sizes."}, {"title": "D. Optical Flow Estimation with EvRepSL", "content": "Optical flow estimation is selected as the second event- based task to validate our proposed method. Contrasting with object classification, which seeks a high-level understanding of event-streams, optical flow estimation offers a low-level, pixel-wise analysis, representing a distinct facet of event- based tasks. Once EvRepSL is obtained from RepGen, it can also be utilized to generate high quality input for optical flow estimation. We design Eff-FlowNet by adopting similar network design as RepGen as backbone, but with only one decoder which outputs the two-dimensional optical flow for all pixels. The robust Charbonnier loss [67] is used as the loss function with the same setting in [21]:\n$l_{flow} = \\sum_{i} [(\\hat{u}_i - u_i)^2 + \\epsilon^2]^\\alpha$,\n(21)"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we will first describe how the RepGen is trained with hybrid frame-event dataset (IJRR [13]) in a self- supervised learning approach. Then we evaluate and compare the performance of our proposed EvRepSL with other event- stream representations and methods on different datasets for classification or optical flow estimation."}, {"title": "A. Self-Supervised Representation Learning", "content": "We train RepGen using IJRR [13] dataset in a self- supervised learning approach. IJRR dataset is collected by DAVIS240C event camera which outputs both video frames and event-streams simultaneously. The frame rate of video is approximately 24fps and the spatial resolution of the pixel array, for both video frame and event-stream, is 240 \u00d7 180. IJRR contains 27 subsets and each subset records different scenes with different speeds of movement and various poses. It is worth noting that IJRR dataset is only used to train the RepGen network for representation refinement and is irrelevant to the datasets used for evaluating its performance on different vision tasks.\nWhen training RepGen with IJRR dataset, data augmenta- tion techniques are applied to alleviate the problem of overfit- ting. First, frames and event-streams (in EvRep representation) are randomly rotated by 90, 180 or 270 degrees. Then different frame rates (24fps, 12fps and 8fps) are adopted by skipping some of the frames evenly. Finally, model is also trained with frames and events in reversed order to improve the consistency and interpretability of the proposed approach since Eq. 15 can be rewritten as:\n$f_0^{x,y} = exp[\\theta^{x,y} * (-E_1^{x,y})] * (f_1^{x,y} + k) - k$.\n(22)\nAll experiments are conducted on a system equipped with an AMD 5950X processor, RTX 3090 graphics card, and 32GB RAM, running on Ubuntu 20.04 with the PyTorch framework [74]. 80% of the frame pairs and the event-streams between them are randomly selected for training and the rest is used for validation. We train the model using the Adam optimizer [75] for 300 epoch with batches of size 16. The learning rate starts with 10e-3 and decreases by a factor of 10 for every 100 epoch."}, {"title": "B. Evaluation for Event-based Classification Task", "content": "1) Evaluation Setup: In this section, we evaluate and compare our proposed EvRep and EvRepSL with SOTA event-stream representations on five datasets for classification,"}, {"title": "C. Event-based Optical Flow Estimation", "content": "1) Evaluation Setup: In this section, we evaluate and com- pare our proposed EvRep and EvRepSL with SOTA methods with various types of representation on MVSEC [62], a dataset popularly used for optical flow estimation [20]\u2013[22], [34], [42], [77]. MVSEC is collected by a stereo DAVIS system paired with a LIDAR to obtain the groundtruth of optical flow [62]. During the evaluation, we follow the configura- tion in [21] for dataset split, i.e., to train the model with outdoor sequences and evaluate it with indoor sequences. We compare our methods with 10 different existing methods with 8 different types of representation. They are LIF-EV- FlowNet [77] with two-channel [26], EV-FlowNet [19] with four-channel [19], Zhu et al.'s [20] and Paredes et al.'s ap- proaches [42] with voxel-grid [20], Deng et al.'s approach [34] with EvSurface [34], EST [21] with a learnable MLP rep- resentation [21], Matrix-LSTM [22] with a recurrent surface [22], Spike-FlowNet [78] with former-latter event groups [78], STE-FlowNet [35] with Gaussian weighted polarities [35], and Stoffregen et al.'s approach [60] with voxel-grid [20]. Except for Stoffregen et al.'s [60], the majority of the existing approaches focus on estimating the sparse optical flow (pixels with triggered events). In this paper, both sparse and dense estimation results of ours will be presented. To further verify the effectiveness of EvRepSL, we also train Eff-FlowNet with four popular representations including two-channel [77], four- channel [19], voxel-grid [20], and EvSurface [34].\nAverage End Point Error (AEE) is used as the evaluation metric in this part and it is widely adopted for evaluating the accuracy of optical flow estimation [20]\u2013[22], [34], [42], [77]. AEE measures the Euclidean distance between the expected and predicted flows:\nAEE = $\\frac{1}{S}\\sum_{(x,y) \\in S} ||F_{pred}(x, y) - F_{gt}(X,Y)||_2$,\n(23)"}, {"title": "D. Ablation Studies", "content": "Evaluating Each Channel of EvRepSL. In this section, we extend the ablation studies on the proposed representation. The comparison between EvRep and EvRepSL has already been presented in previous sections across all tasks (Tables II, IV, V), where a clear performance boost is observed when incorporating the two learned channels (EvRep vs. EvRepSL). To further investigate the contribution of each channel, we progressively add new channels to the representation and re- port the results in Table VI. The table shows that performance steadily improves as channels are added. Comparing {E1,Ec} with EvRep highlights the effectiveness of introducing the event temporal channel Er. Additionally, the table validates that both learned channels significantly contribute to the overall performance, with erfd having a particularly notable impact, indicating its effectiveness.\nEvaluating Effectiveness of Er. To further assess the role of E1, we conducted additional ablation studies on different combinations of the proposed method with and without Er, as"}, {"title": "V. CONCLUSION", "content": "In this study, we introduced EvRepSL, a new event- stream representation developed through self-supervised learn- ing, leveraging the relationship between APS frames and event-streams. The RepGen model, once trained on hybrid frame-event datasets, generates refined representations for various event-based vision tasks without requiring retrain- ing. Our comprehensive evaluation across classification and optical flow estimation tasks on public datasets shows that EvRepSL consistently outperforms existing methods. Addi- tionally, EvRepSL demonstrates versatility, handling different event cameras and tasks with ease, making it a strong foun- dation for future research in event-based vision."}]}