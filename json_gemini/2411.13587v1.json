{"title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models\nin Robotics", "authors": ["Taowen Wang", "Dongfang Liu", "James Chenhao Liang", "Wenhao Yang", "Qifan Wang", "Cheng Han", "Jiebo Luo", "Ruixiang Tang"], "abstract": "Recently in robotics, Vision-Language-Action (VLA) mod-\nels have emerged as a transformative approach, enabling\nrobots to execute complex tasks by integrating visual and\nlinguistic inputs within an end-to-end learning framework.\nWhile VLA models offer significant capabilities, they also\nintroduce new attack surfaces, making them vulnerable to\nadversarial attacks. With these vulnerabilities largely unex-\nplored, this paper systematically quantifies the robustness\nof VLA-based robotic systems. Recognizing the unique de-\nmands of robotic execution, our attack objectives target the\ninherent spatial and functional characteristics of robotic\nsystems. In particular, we introduce an untargeted position-\naware attack objective that leverages spatial foundations to\ndestabilize robotic actions, and a targeted attack objective\nthat manipulates the robotic trajectory. Additionally, we\ndesign an adversarial patch generation approach that places\na small, colorful patch within the camera's view, effectively\nexecuting the attack in both digital and physical environ-\nments. Our evaluation reveals a marked degradation in task\nsuccess rates, with up to a 100% reduction across a suite of\nsimulated robotic tasks, highlighting critical security gaps\nin current VLA architectures. By unveiling these vulnera-\nbilities and proposing actionable evaluation metrics, this\nwork advances both the understanding and enhancement\nof safety for VLA-based robotic systems, underscoring the\nnecessity for developing robust defense strategies prior to\nphysical-world deployments.", "sections": [{"title": "1. Introduction", "content": "\"First directive: A robot cannot harm a human or, through\ninaction, allow a human to be harmed.\"\nIn the movie Finch, set in a post-apocalyptic future, an\nintelligent robot navigates complex themes, underscoring\nthe importance of interactive safety with its human master.\nOnce speculative, this portrayal is now increasingly plausi-\nble with the rise of visually-conditioned language models\n(VLMs) [72, 81] capable of seamlessly interpreting both\nvisual and linguistic contexts. A notable realization of this\npotential can be seen in Vision-Language Action (VLA)\nmodels [16, 35, 75, 76], which integrate VLMs into robotic\nsystems to enable end-to-end learning that encompasses both\nhigh-level planning and the generation of low-level robot\ncontrol actions. While VLA models demonstrate a promis-\ning step toward generalist robotic intelligence, they also in-\ntroduce potential vulnerabilities that remain underexplored.\nThis work addresses the pressing need to investigate the new\nattack surfaces associated with VLA-based robotic systems.\nTo gain deeper insights into these vulnerabilities, we an-"}, {"title": "2. Related work", "content": "alyze VLA-based models and general robotic operations,\nhighlighting two key characteristics crucial for designing\neffective adversarial attacks. First, creating attack objectives\nfor robotic systems requires consideration of the physical dy-\nnamics and constraints intrinsic to robotic movement. Con-\nventional adversarial attacks, in this case, often fail to pro-\nduce significant deviations in intended actions because they\ndisregard these constraints. Second, VLA models generate\ncontrol signals that function as a time series of K-class pre-\ndictions (see \u00a73.1), making it essential to design attacks that\nexploit the temporal dependencies within these sequences\nto cause substantial disruptions in robotic behavior. Achiev-\ning attacks that are effective in both digital simulations and\nreal-world environments remains critical yet challenging.\nTo address these challenges, our work intensifies the ad-\nversarial threats posed to VLA-based systems by developing\nspecialized attack objectives and designing effective attack\nmethods. Specifically, we formulate an Action Discrepancy\nObjective aimed at maximizing the normalized action dis-\ncrepancy within VLA-based robotic systems. This strategy\nensures that, at each decision point, the robot's behavior\ncan diverge from the optimal trajectory. Additionally, we\nintroduce Geometry-Aware Objective that considers the\nrobot's movement in three-dimensional space, characterized\nby three degrees of freedom. By optimizing the cosine simi-\nlarity between adversarial and ground-truth directions, we\ninduce deviations in the robot's movement direction from\nits intended path, increasing the likelihood of task failure.\nTo achieve these attack objectives, we develop a straightfor-\nward yet effective Patch-Based Attack targeting VLA-based\nrobotic systems. This approach enables adversarial attacks\nin both digital and physical settings, revealing substantial\nvulnerabilities within the VLA-based system.\nTogether, these innovations yield several significant con-\ntributions:  This work presents a pioneering and compre-\nhensive analysis of vulnerabilities within VLA-based robotic\nsystems, a new paradigm for training generalized robotic\npolicies using generative foundation models. We reveal criti-\ncal threats to adversarial attacks (see Fig. 1), emphasizing the\nurgent need to enhance robustness before real-world deploy-\nment.  We are the first to define attack objectives specific\nto the powerful VLA model and to employ a straightforward\nadversarial patch against it (see \u00a73). This offers valuable\ninsights for the research community to explore systemic fail-\nures in similar concurrent generative foundation models.  We rigorously evaluate our approach in both simulated and\nphysical environments across four distinct robotic tasks, ob-\nserving a significant raises in task failure rates of 100% and\n43%, respectively. This highlights the effectiveness of our\nattack strategies (see \u00a74.2)."}, {"title": "2.1. AI-driven Generalist Robot", "content": "Developing generalist robots [6, 14, 27, 38, 62] requires\nmodels not only handle varied interactions but also main-\ntain robustness in unstructured environments. Early intel-\nligent robots [52, 53, 55, 56, 85] relied on rule-based ap-\nproaches, effective only in controlled settings and requir-\ning extensive reprogramming for new tasks. The advent of\ndeep learning [19, 31, 33, 41, 58] and reinforcement learn-\ning [1, 65, 74] has shifted this paradigm to adapt through\ndata, increasing robot versatility and reducing manual recon-\nfiguration.\nRecent studies [7, 54, 59] have further advanced the de-\nvelopment and potential of large-scale visually-conditioned\nlanguage models (VLMs) as key enablers for generalist\nrobots, demonstrating promising generalization across a va-\nriety of scenes [15, 25, 32, 42, 66, 86]. One of the notable\nexamples is OpenVLA [35], which integrates dual visual\nencoders with a pre-trained large language model to en-\nable observation-to-action capabilities through instruction-\nfollowing [10, 34, 46, 47, 72, 84]. By aligning visual and\ntextual semantics, OpenVLA facilitates complex scenario\nunderstanding and end-to-end action generation, showing\nimpressive generalization when executing previously unseen\ntask instructions. Despite its success in generalist robotic ap-\nplications, the vulnerability of such systems remains largely\nunexplored. This study aims to address this gap by investi-\ngating adversarial threats to VLA-based robotic systems."}, {"title": "2.2. Human-Robot Interaction Safety", "content": "Current human-robot interaction risks include physical dis-\ncomfort or injury [4, 5, 22]. Considering these risks, en-\nsuring safety during interactions becomes the top priority\nas many literature outlined [24, 40, 69]. Existing robotic\nsafety strategies have primarily concentrated on mitigating\nnaturally occurring hazards [40]. These strategies can be\ngenerally categorized into pre-collision or post-collision in-\nterventions based on the timing of manage hazards [39].\nPre-collision approaches aim to prevent severe hazards by\nimplementing physical constraints, such as controlling force\nand speed [28, 30], defining safety zones [70] and predicting\nhuman actions to avoid potentially dangerous robot move-\nments [36, 37, 51]. Post-collision strategies, on the other\nhand, focus on detection and response after a harmful action\nhas occurred [20, 49]. While there is substantial research\non robotic safety, the integration of AI into robotic policies\nposes new challenges, where attackers can manipulate AI-\ndriven robot and execute malicious actions. Such threats\nintroduce additional layers of risk and complexity. In this\npaper, we primarily focus on adversarial threats."}, {"title": "2.3. Adversarial Attacks in Robotics", "content": "Adversarial attacks serve as critical tools for assessing\nmodel vulnerabilities, particularly in robotics, where mod-\nels operate in dynamic, real-world settings. Traditional\ngradient-based, pixel-level attacks [17, 21, 44, 50, 63, 73]\nexploit model gradients to calculate malicious perturba-\ntions, achieving high attack success rates in digital envi-\nronments. For physical-world attacks, patch-based meth-\nods [3, 8, 61, 78, 79] offer a practical alternative for real-\nworld applications by introducing physically realizable per-\nturbations that retain efficacy under varied conditions, such\nas angle or lighting changes, making them highly applica-\nble for robotic systems. Therefore, considering VLA-based\nmodels enable robot execution in real-world scenarios, this\nwork employs a patch-based attack strategy.\nVLA models [13, 26, 66] couple visual and linguistic\nmodalities for perception-action alignment. The continuous,\nhigh-dimensional nature of visual inputs [50, 60, 64] makes\nthem the most likely target for adversarial perturbations, as\nattackers can subtly alter visual data in ways that are difficult\nto detect [2, 9, 67]. Consequently, this work applies attacks\non the visual modality to achieve robust manipulations across\ndiverse environments. To the best of our knowledge, this\nstudy is one of the earliest attempts to formally define adver-\nsarial objectives for VLA models, targeting gaps in adver-\nsarial robustness through the model's geometric constraints\n(e.g., spatial dependencies in visual input) and architectural\nnuances (e.g., cross-modal information integration)."}, {"title": "3. Methodology", "content": "This section outlines our proposed methodology. We first\nreview the core algorithmic principles of VLA in \u00a73.1, which\nserves as the foundation for the adversarial scheme designs.\nWe then present two types of untargeted adversarial attacks,\nincluding Untargeted Action Discrepancy Attack (UADA)\nin \u00a73.2 and Untargeted Position-aware Attack (UPA) in\n\u00a73.3. In \u00a73.4, we further include Targeted Manipulation\nAttack (TMA), designed to direct the robot toward specific\nerroneous execution. The implementation details are pre-\nsented in \u00a73.5. The overall framework is shown in Fig. 2."}, {"title": "3.1. Preliminary", "content": "Vision-Language Action models [35, 86] (see Fig. 2) are\nbuilt on large language models integrated with visual en-\ncoders, enabling robots to interpret human instructions and\nprocess visual input from a camera to perform context-aware\nactions. VLA models [35, 43, 86] generally abstract contin-\nuous action prediction into a classification problem by dis-\ncretizing robot actions. Specifically, they first discretize the\ncontinuous probabilities into class labels  $c = arg max F(x)$.\nAn action de-tokenizer then generates actions  $y = DT(c)$.\nBy categorizing action values into discrete class labels, the\nmodel converts continuous probability outputs into discrete\nsignals, this simplification facilitates quicker convergence\nand faster training times, and is commonly used for VLA-\nbased models [35, 43, 86].\nIn this work, we focus on a 7-degree-of-freedom (DoF)\nrobotic arm [23]. At each step, an action consists of 7DoFs\nwith specific physical significance in three-dimensional\nCartesian space, represented by:\n$A = [\\Delta P_x, \\Delta P_y, \\Delta P_z, \\Delta R_x, \\Delta R_y, \\Delta R_z, gripper]$,\nwhere $\\Delta P_{x,y,z}$ and $\\Delta R_{x,y,z}$ denote relative positional and\nrotational changes along the x-, y-, and z-axis, discretized"}, {"title": "3.2. Untargeted Action Discrepancy Attack", "content": "into 256 uniform bins across the action range [35]. The\ngripper state is binary, indicating its open or closed state.\nThis control design presents a unique challenge for adver-\nsarial attacks, as finely divided bins result in minimal action\ndiscrepancies between neighboring bins (e.g., \u00b10.007/bin).\nThis means that even if an attack shifts the classification to\nan adjacent bin, the resulting action discrepancy has minimal\nimpact real-world performance.\nWe first focus on maximizing action discrepancies by in-\ntroducing Untargeted Action Discrepancy Attack (UADA).\nThis attack is based on the observation that larger robot\nactions usually correlate with intense physical movements,\nwhich, in turn, may amplify the potential for real-world haz-\nards [28\u201330]. Specifically to UADA, the attack target is one\nor combination of 7DoFs, defind as $y = {y^i|i \\in [1, . . ., 7]}$.\nTo define UADA's objective, we first determine the maxi-\nmum action discrepancy  $d_{max}$, which represents the largest\ndeviation that can occur within the allowed range for each\nDoF. This is calculated by measuring the distance between\nthe ground truth y and its action range $[y_{min}, y_{max}]$ as:\n$d_{max}(y^i) = max(y^i - y^i_{min}, y^i - y^i_{max})$.\nWe next calculate the applied action discrepancy\n $d'_{applied}(x,y) = |F(x)^i \u2013 y^i|$ in order to measure the de-\nviation between the model's output and the ground truth,\nwhere  $F(x)^i$  is the  $i$-th action output of model  $F(x)$. In this\nsense, we can evaluate the Normalized Action Discrepancy\n(NAD) as the drifting ratio between  $d'_{applied}$ and  $d_{max}$ as:\n$NAD = \\frac{1}{T} \\sum d^i_{applied}/d^i_{max}$.\nSince VLA models are currently operated via predicting\ndiscrete token labels (see \u00a73.1), we then need to bridge action\ndiscrepancy derived from these discrete labels into learnable\ntargets. Specifically, we weighted sum the model output\nprobabilities across J bins by their own class label j as:\n$d'_{applied} (x, y) = \\sum_{j=1}^J[(j \u00b7 F(x)) - c^i]$,\nwhere $F(\u00b7)$ is the model output probability of $i$-th DoF's\n$j$-th bin. Finally, the attack objective for UADA is defined as\nminimizing the ratio of weighted sum $d'_{applied}$ to $d_{max}(y^i)$:\n$L_{UADA} = E_{(x,y) \\sim X}[ \\sum_{i=1}^I \\frac{d^i_{applied}(x + \\delta, y)}{d^i_{max} (y)}]$,\nwhere $x, y$ is a benign image and action pair, sampled\nfrom distribution X. UADA defines the objective functions\ntailored for the robot's action space, allowing adversarial"}, {"title": "3.3. Untargeted Position-aware Attack", "content": "patches to create significant, lasting disruptions in task per-\nformance. In this design, we uniquely leverage the model's\ndiscrete action structure to design attacks that amplify mean-\ningful discrepancies in robot behavior.\nWe further extend untargeted adversarial attacks to consider\nthe positional dynamics within VLA models. In typical\ntask execution, precise and directed movements of the end-\neffector towards designated goals are essential, with actions\nmapped in three-dimensional space. The positional degrees\nof freedom,  $y^{pos} = [y^1, y^2, y^3]$  encapsulates the directional\nmovements within 3D for effective robotic control.\nRecognizing the importance of  $y^{pos}$  in controlling the\nend-effector's path, we introduce a position-aware attack\nto disrupt the intended movement trajectory. This type of\nadversarial vulnerability remains largely unexplored but has\nsignificant implications for task failure: the attack objective\ncan steer the end-effector away from its intended path by\nintroducing directional perturbations, amplifying errors and\ncausing substantial trajectory distortions. Formally,we define\nthe Untargeted Position-aware Attack (UPA) objective as:\n$L_{UPA} = E_{(x, y) \\sim x}[\\alpha \\frac{y^{adv}_{pos} \u00b7 y^{pos}_{base}}{||y^{pos}_{adv}|| \\cdot ||y^{pos}_{base}||} + \\beta \\frac{||y^{pos}_{adv}||^2}{||y^{pos}||^2}]$,\nwhere $\\alpha$ and $\\beta$ are the hyperparameters that balance between\nthe directionality and magnitude of the perturbations. By in-\ntegrating geometric awareness into our attack, this approach\ngenerates perturbations that induce cumulative deviations\nin the robot's trajectory, even with minimal adjustments.\n$L_2$-normalization  $|| \u00b7 ||_2$  further intensifies these deviations,\nrendering the attack highly effective and disruptive."}, {"title": "Algorithm 1: Adversarial Patch Attack.", "content": "1: Input:  X : dataset;  \u03b4 : adversarial patch;  $\\mathcal{L}_0$ : objective\nfunction;  F : vision language action model;  T (\u00b7): Transfromation;  \u03d5, \u03c8  are two hyperparameters for\ntransformation;  T : maximum iteration steps;  k : maximum inner-loop steps.\n2: Initialize:  \u03b4 \u223c U [0, 1), $\\mathcal{L}_0 \\in {L_{UADA}, L_{UPA}, L_{TMA}}$\n3: for i = 1, 2, ...,  T do\n4:  $(x, y) \\sim X$\n5:  shx , shy  \u223c U (\u2212\u03d5, \u03d5),  \u03b8  \u223c U (\u2212\u03c8, \u03c8)\n6:  for i = 1, 2, ...,  k do\n7:   Ypred  \u2190 F (T (x +  \u03b4 ))  {\u25b7 Feedforward }\n8:    $\\delta \\leftarrow - \\frac{ \\partial \\mathcal{L}_0}{\\partial \\delta}$  {\u25b7 Backpropagation }\n9:    \u03b4 \u2190  \u03b4  {\u25b7 Update }\n10:   Clip \u03b4 into normal pixel range.\n11:  end for\n12: end for"}, {"title": "3.4. Targeted Manipulation Attack", "content": "In addition to the aforementioned untargeted attacks, we also\nexplore the adversarial vulnerabilities of VLA models with\na targeted strategy. This attack aims to mislead the models\ninto predicting specific trajectories, resulting in precise al-\nterations. These targeted alterations can potentially drive\nmalicious behaviors or induce task failure. The objective of\nthe targeted manipulation attack is:\n$L_{TMA} = E_{(x,y) \\sim X}[H(F(x + \\delta), y^t)]$,\nwhere  $y^t = {y^i = t|i \\in [1, ..., 7], t \\in R}$  represents the\nadversarial target; and  $H(\u00b7, \u00b7)$  denotes the Cross-Entropy\nloss, which measures the error between the predicted state\nunder adversarial perturbation and the desired target state.\nBy steering the robot towards an adversarial target across\nsuccessive time steps, our approach manipulates the trajec-\ntory and undermines task performance, ultimately leading to\na substantial disruption of intended operations. Furthermore,\nmanipulating any individual DoF or a combination thereof\ncan also severely compromise task success, given that the\nerrors in one dimension tend to propagate and magnify over\ntime during execution (see Fig. 3)."}, {"title": "3.5. Implementation Details", "content": "The implementation of our pipeline is detailed in Algo-\nrithm 1. We incorporate two key modifications to enhance\nthe training stability of the generated patch.\nInner-loops. Following previous work [50], we add k inner-\nloops optimize steps during each iteration in line 5, aiming\nto reduce data variance and ensure more consistent updates."}, {"title": "4. Experiments", "content": "Geometric Transformations. To improve the robustness of\nour attack under real-world scenarios, we employ random\ngeometric transformations  $T(\u00b7)$  in line 7 with affine and\nrotation transformations [3] as:\n$T(\u00b7) = \\begin{bmatrix} 1 & sh_x \\\\ sh_y & 1 \\end{bmatrix} \\begin{bmatrix} cos \\theta & -sin \\theta \\\\ sin \\theta & cos \\theta \\end{bmatrix}$,\nwhere  $sh_x, sh_y \u223c U(\u2212\u03c6, \u03c6)$ are shear factors, and  $\u03b8 \u223c$\n$U(\u2212\u03c8, \u03c8)$ denotes the rotation angle, both sampled from\nuniform distributions during inner loop optimization step k.\nIn this section, we present the experimental design outlined\nin \u00a74.1 and report the quantitative and qualitative results of\nUADA, UPA, and TMA in both simulation and physical-\nworld scenarios (\u00a74.2). We then analyze diagnostic exper-\niments (\u00a74.3) to evaluate the influence of key components.\nAdditionally, we assess the robustness of our method against\nvarious defense mechanisms in \u00a74.4 and conclude with a\ncomprehensive systemic discussion in \u00a74.5."}, {"title": "4.1. Experiment Setup", "content": "Simulation Tasks. We conduct simulation attacks on Bridge-\nData V2 [71] and LIBERO [45]. BridgeData V2 [71] is a\nreal-world dataset comprising 24 diverse environments and\n13 distinct skills, such as grasping, placing, and object rear-\nrangement, with a total of 60, 096 trajectories. LIBERO [45]\nis a simulation dataset designed to evaluate models across\nfour distinct task types (Spatial, Object, Goal, Long), each\ncontaining 10 tasks with 50 human-teleoperated demonstra-\ntions. Notably, LIBERO-Long combines diverse objects,\nlayouts, and long-horizon tasks, making them challenging\nfor complex, multi-step planning, thus we choose LIBERO-\nLong to conduct UADA, UPA and TMA.\nRobot Setups. For the physical-world tasks, we adopt a\nrobotic system consisting of a Universal Robots UR10e\nequipped with a Robotiq Hand-E Gripper to provide a\n7DoF motion. The sensing system includes one RGB web-\ncam in the fixed position with a shoulder view.\nVictim VLA Models. In our study, we select publicly avail-\nable and current most influential VLAs as victim models for\nevaluation. Specifically, we focus on four variants of the\nOpenVLA model, each trained independently on different\ntasks within the LIBERO dataset (i.e., Spatial, Object, Goal,\nLong). To evaluate the effectiveness of our methods, we\ncraft adversarial patches using two distinct generating setups.\nThe first involves a model trained in a simulated environ-\nment using the LIBERO dataset [45] with the openvla-7B-\nlibero-long variant [35]. The second uses a model trained\non physical world data from the BridgeData v2 dataset [71]\nwith the openvla-7B model [35]. This approach allows us to\nassess our methods on both simulated and real-world data.\nSubsequently, we evaluate the performance of generated ad-\nversarial patches on victim models (i.e., OpenVLA LIBERO"}, {"title": "4.2. Main Result", "content": "Evaluation Details. To assess the effectiveness and robust-\nness of our methods, we conduct evaluations on the LIBERO\ndataset [45]. Each task suite is evaluated with 10 trials, with\nfixed patch location (see \u00a7S5) This setup is for avoiding the\npatch obstructing task-related objects.\nQuantitative Results. For UADA and UPA, our methods\neffectively amplify action discrepancies, leading to a no-\ntable transfer attack ability in increasing failure rates (see\nTab. 1). Specifically, for the Simulation setup, UADA and\nUPA achieve maximum NAD of 18.1% and 14.5%, signifi-\ncantly outperforming untargeted scenarios with increment\nof 3.8% and 0.4%, respectively. Both untargeted and UADA\nsuccessfully disrupt the robot execution, with a failure rate to\n100%. UPA also results in a significant increment compared\nto the failure rate of the victim methods in benign scenarios,\nwhich is 100% v.s. 15.3%, 83.4% v.s. 11.6%, 93.4% v.s.\n20.8% and 96.7% v.s. 46.3%, respectively. Regarding the\nPhysical setup, both UADA and UPA generate actions with\nlarge action discrepancies, which are 31.9% and 26.9% with\nan increase of 5.2% and 0.2% compared to the best 26.7%\nin untargeted scenarios, respectively. UPA demonstrates\nan improvement of 86.4% v.s. 85.9% over the spatial task\ncompared to untargeted."}, {"title": "Qualitative Results.", "content": "For TMA task, we evaluate the effectiveness of our\nmethod by manipulating various DoF to 0. Our method\ndemonstrates significant effectiveness in manipulating\nrobotic trajectory and increasing failure rates (see Tab. 3).\nSpecifically, when applied to the different generation set-\ntings, our approach yields notable increments across vari-\nous tasks, including an max average failure rate 97.8% v.s.\n23.5% of benign performance in Simulation, and 89.1% in\nPhysical, respectively. To further prove the generalizability,\nwe target attack on several non-zero values in \u00a7S2.\nWe qualitatively analyze the trajec-\ntories of robot movement of the proposed three different\nobjectives in Fig. 3. For UADA, we compare the trajec-\ntory deviations in the same trail with patch generated from\nUADA and untarget attack. As seen, the untarget attack in-\nduces small deviations in the trajectory UADA, on the other\nhand, produces significantly larger trajectory deviations (also\nsupported by NAD metric in Tab. 2), which is attributed to\nUADA's capability of incorporating action discrepancies. In\nour observation, UADA significantly amplifies the effect on\noverall task execution, thereby increasing the potential for\nrobotic hazards. For UPA, we observe chaotic and irreg-\nular behaviors, including instances where the end-effector\nmoves out of the camera's field of view. We attribute this\nphenomenon to the efficacy of the adversarial patch in per-\nturbing the model's spatial perception, inducing a consistent\ndeviation from the intended direction of movement, ulti-\nmately resulting in failure in a long run.\nFor TMA, we observe a notable reduction in the range of\nmotion along the x-axis corresponding to the targeted attack\naxis. This suggests that our proposed targeted attack can\neffectively constrained robot movements.\nIn summary, the qualitative analysis shows that both Un-\ntarget attack, UADA, UPA, and TMA can effectively disrupt\nthe robot actions generated from VLA models. These find-\nings underscore a pressing security concern during the de-\nployment of generalist robots, especially when considering\napplication scenes that requires reliable operations [11, 70].\nPhysical-world Performance. In addition to the digital\nsimulation results, we also conduct a comprehensive evalua-\ntion of the performance of our generated adversarial patches\nin physical-world testing scenarios. The evaluation encom-\npassed 100 trials across three distinct tasks: object grasping,\nplacement, and manipulation (see \u00a7S3 for more details).\nBuilding on the concept of robotic tasks Success Rate"}, {"title": "4.3. Diagnostic Experiment", "content": "We ablate components of our method in this section.\nImpact of Inner-loops. We discuss the impact of inner-loop\nsteps to model performance in Fig. 4(a). The results show\nthat NAD first improves when inner-loop steps continue\nto increase. This is due to the reduced data variance and\ngradually stabilized gradients. However, further increasing\nthe steps results in a slower performance increase, with a\nnoticeably longer training schedule. We thus choose inner-\nloop steps=50 as it balances between performance and scale.\nImpact of Patch Size. We further study TMA's perfor-\nmance with different patch sizes, and report the average\nfailure rate across four LIBERO [45] benchmark tasks in\nFig. 4. The patch sizes examined were [3%, 5%, 7%, 10%]\nof a 224 x 224 input image. Our findings indicate an in-\nversely proportional relationship between the $L_1$ distance\nof predicted actions and the target action as the patch size\nincreased. This increase in patch size also correlated with a\nmarked rise in task failure rates. The observed trend suggests\nthat larger patch sizes afford the adversary more extensive\noptimization space to influence the model's output, which is\nconsistent with previous work [12, 83]."}, {"title": "4.4. Robustness Evaluation", "content": "We further examine if concurrent defense strategies [18, 80,\n82] can generalize against our adversarial examples intro-\nduced within VLA. Specifically, we applied four prior de-\nfense techniques, and reported their task failure rates for"}, {"title": "4.5. Systemic Discussion", "content": "Patch Pattern Analysis. In Fig.6, we present the adversar-\nial patches generated for a range of targets. A particularly\nnoteworthy observation is that some of these patches bear\na striking resemblance to the structural joints of a robotic\narm. Given the strong similarity between these patches\nand robotic arms in appearance, we hypothesize that these\nVLA-based models, in their current training paradigms, are\noften limited to a single robotic system operating within re-\nstricted camera views. This constrained perspective/camera\npose may inadvertently induce a learned representation bias,\nwhere adversarial perturbations align with prominent struc-\ntural features of the training robot's appearance in order to\ndeceive current victim models. Given that these patches are\nmodel-specific and reflect the spatial and visual patterns of\nthe training environment, we have a reasonable concern that\nthey may compromise VLA-based models' generalizability\nand robustness in physical-world deployments.\nPossible Defense Strategies. Our work highlights the press-\ning need for a paradigm shift in the VLA-based models'\ntraining strategies in order to successfully defend our attacks.\nSpecifically, future approaches might incorporate more train-\ning scenarios that involve complex multi-robot interactions\nto mitigate the current patch bias. Another possible solu-\ntion is to leverage the logical relationships within the robot\narm's physical structure to estimate the actual position of\neach joint. By predicting joint positions deviated from phys-\nical constraints, we can eliminate unreasonable adversarial\nsamples, even if they are similar to actual joint(s)."}, {"title": "5. Conclusion", "content": "While VLA models have gained significant popularity due\nto their substantial robot capabilities, this study, for the\nfirst time, establishes specific robotic attack objectives and\ndemonstrates that these models are in fact vulnerable to\nvarious types of attacks. Experimental results demonstrate\nthat our attacks expose significant vulnerabilities to VLA\nmodels, raising concerns regarding impetuous real-world\ndep"}]}