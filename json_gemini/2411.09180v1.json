{"title": "LEAP:D - A NOVEL PROMPT-BASED APPROACH FOR DOMAIN-GENERALIZED AERIAL OBJECT DETECTION", "authors": ["Chanyeong Park", "Heegwang Kim", "Joonki Paik"], "abstract": "Drone-captured images present significant challenges in object detection due to varying shooting conditions, which can alter object appearance and shape. Factors such as drone altitude, angle, and weather cause these variations, influencing the performance of object detection algorithms. To tackle these challenges, we introduce an innovative vision-language approach using learnable prompts. This shift from conventional manual prompts aims to reduce domain-specific knowledge interference, ultimately improving object detection capabilities. Furthermore, we streamline the training process with a one-step approach, updating the learnable prompt concurrently with model training, enhancing efficiency without compromising performance. Our study contributes to domain-generalized object detection by leveraging learnable prompts and optimizing training processes. This enhances model robustness and adaptability across diverse environments, leading to more effective aerial object detection.", "sections": [{"title": "1. INTRODUCTION", "content": "Object detection in aerial imagery is rapidly progressing in tandem with advancements in deep learning. However, the unique characteristics of drone imagery pose significant challenges, impacting object appearance and shape due to varying shooting conditions. As a result, there is considerable interest in employing domain generalization techniques to address these challenges. Factors such as drone altitude, angle, and weather conditions contribute to variability in shooting conditions. Variations in drone altitude can affect object sizes, while camera viewing direction can alter object shapes. Moreover, weather conditions and the time of day when images are captured cause significant differences in lighting, further influencing the imagery.\nTo simply enhance the object detection performance in drone captured images, [1, 2] conducted research on methods for processing high-resolution images. The Nuisance Disentangled Feature Transform (NDFT) is a technique designed to disentangle and separate nuisance features from important features. Wu et al. collected shooting condition information as metadata, which defines each environment as a \"domain\" and allows for classification specific to each domain [3]. Adversarial training during backpropagation is used by introducing negative values into the backbone network, ensuring the model does not overly respond to domain-specific features but instead leverages domain-invariant features for object detection. Lee et al. proposed an improved version of NDFT, called A-NDFT, by introducing feature replay and a slow learner strategy to address the learning speed limitations observed in NDFT, significantly enhancing training speed while maintaining comparable performance [4]. Jin et al. proposed the Fine-Grained Feature Disentanglement (FGFD) module, which uses the one-stage detector YOLOv5 [5] as the backbone network to differentiate between domain-specific and domain-invariant feature maps [6].\nThere has been a significant surge in interest around vision-language representation learning tasks, largely due to training on large-scale image-text pairs, which leads to notable generalization performance. This advantage has been effectively harnessed to deliver remarkable results in image classification. Specifically, active research is underway in domains that demand high generalization capabilities, such as few-shot learning, zero-shot learning, and open-vocabulary learning. Zhou et al. proposed the Context Optimization (CoOp) approach, which addresses concerns about the time-consuming nature of using manually crafted prompts for labeling [7], similar to CLIP [8]. To address this issue, they replaced manual prompts with learnable prompts, reducing labeling time while maintaining performance comparable to CLIP. Building on this concept, Zhou et al. introduced Conditional Context Optimization (CoCoOP), where they incorporated a subnetwork called Meta-Net to conditionally pass information extracted from the image encoder to the learnable prompt, leading to high performance in vision-language tasks [9].\nIn the field of domain-generalized object detection, a noticeable trend has emerged toward incorporating large-scale"}, {"title": "2. PROPOSED METHOD", "content": "2.1. Problem Statement\nGiven an input image x and a manual prompt $X_{mp}$ from the training dataset $X_{train}$, the visual encoder $F(\u00b7)$ in CLIP converts x into the visual embedding v, while the textual encoder $G(\u00b7)$ maps $x_{mp}$ to the textual embedding $t_{mp}$. This process is formalized by the following equations:\n$v = F(x)$,\n$t_{mp} = G(x_{mp})$.\nThe input manual prompt $X_{mp}$ follows a specific format: \u201cAn {altitude condition} altitude {view condition} view of a {weather condition} day taken by a drone\u201d. As a result, the predicted probability can be expressed as:\n$p(\\hat{y} = y|I) = \\frac{exp(sim(v, t_{mp})/T)}{\\Sigma exp(sim(v, t_{mp})/T)}$,\nwhere the function sim(\u00b7) measures the similarity between the visual and textual representations and is defined as:"}, {"title": "2.2. Architecture Overview", "content": "The proposed approach builds upon the LGNet architecture [11], utilizing an object detection model that incorporates Faster R-CNN [12] and Feature Pyramid Network (FPN) [13]. Additionally, the CLIP model is used during training to filter out domain-specific features. With intermediate features f obtained from FPN, the Feature map Squeeze Network (FSN) aligns the dimensions of the visual embeddings v with the intermediate features f. The aligned vector, f\u2019 = FSN(f) is trained to resembe the visual embeddings v, while being dissimilar to textual embeddings t. In this context, the similarity between the f\u2019 and v reflects domain-invariant features, while the similarity to t, generated through domain-specific prompts, indicates domain-specific features. During training, the object detection model learns to preserve domain-invariant features and reduce the impact of domain-specific features, resulting in a domain-generalized object detection model. Consequently, the trained object detection model operates independently of CLIP and during inference and remains unaffected by variations in shooting conditions."}, {"title": "2.3. Domain-specific Learnable Prompt", "content": "Domain-specific learnable prompts $x_{lp}$ is constructed as follows:\n$x_{lp} = [e_1][e_2]...[e_{n-1}][e_n]$,\nwhere $[e_1][e_2]...[e_{n-1}][e_n]$ represents the learnable prompts, and we set n = 8. The textual embedding of $X_{lp}$ is then denoted as $t_{lp} = G(x_{lp})$. The predicted probability is computed as:\n$p(y' = y|I) = \\frac{exp(sim(v, t_{lp})/T)}{\\Sigma exp(sim(v, t_{lp})/\\tau)}$,\nwhere we use cross-entropy loss, denoted as $L_{clip}$ to fine-tune the CLIP."}, {"title": "5. CONCLUSION", "content": "In this study, we presented LEAP:D, a domain-generalized aerial object detection approach that leverages learnable prompts for drone imagery. Our comparative experiments revealed that LEAP:D outperforms baseline models and other state-of-the-art methods. These results highlight both the efficiency and effectiveness of our approach, emphasizing its promise as a leading solution for domain-generalized aerial object detection using large-scale vision-language models. By utilizing learnable prompts, our method achieves high accuracy, effectively addressing various shooting conditions and reinforcing the robustness of domain-generalized detection."}]}