{"title": "PROJECT MPG: TOWARDS A GENERALIZED PERFORMANCE BENCHMARK FOR LLM CAPABILITIES", "authors": ["Lucas Spangher", "Tianle Li", "William F. Arnold", "Nick Masiewicki", "Xerxes Dotiwalla", "Rama Parusmathi", "Peter Grabowski", "Eugene Ie", "Dan Gruhl"], "abstract": "There exists an extremely wide array of LLM benchmarking tasks, whereas oftentimes a single number is the most actionable for decision-making, especially by non-experts. No such aggregation schema exists that is not Elo-based, which could be costly or time-consuming. Here we propose a method to aggregate performance across a general space of benchmarks, nicknamed Project \"MPG,\" dubbed Model Performance and Goodness, additionally referencing a metric widely understood to be an important yet inaccurate and crude measure of car performance. Here, we create two numbers: a \"Goodness\" number (answer accuracy) and a \"Fastness\" number (cost or QPS). We compare models against each other and present a ranking according to our general metric as well as subdomains. We find significant agreement between the raw Pearson correlation of our scores and those of Chatbot Arena, even improving on the correlation of the MMLU leaderboard to Chatbot Arena.", "sections": [{"title": "1 INTRODUCTION", "content": "Miles Per Gallon (MPG) has long been useful as a standardized, one-dimensional measure of vehicle fuel efficiency. Although the limitations of MPG are well documented \u2014 particularly its inability to capture the full spectrum of a vehicle's performance and environmental impact-its utility lies in providing a single, generalized measure that simplifies comparisons between vehicles while retaining some accuracy, making MPG a standard bearer in consumer decision-making and various regulatory contexts. We endeavor to apply the same sort of principled aggregation techniques to Large Language Models (LLMs).\nThere exist a vast array of benchmarks for LLMs (i.e. logic (Kil et al., 2024), math (Liu et al., 2024), law (Guha et al., 2024), linguistic understanding (Narayan et al., 2018), factual recall (Hendrycks et al., 2020), general performance ((bench authors, 2023), etc.) yet in many cases, decision-makers require a single, unified metric to facilitate model selection. In this paper, we introduce a novel aggregation approach, dubbed Project MPG \u2013 which nods both to Miles Per Gallon and also Model Performance and Goodness, a more accurate description of our focus.\nProject MPG generates two primary metrics: a \"Goodness\" score, representing general answer accuracy, and a \"Performance\u201d score, reflecting queries per second (QPS). These metrics are derived from the aggregation of various open benchmarks, designed to: (1) be representative of a generalized, real-world use cases by focusing on key domains where benchmarks correlate, (2) maintain relational distances between models, similar to those captured by existing intelligence and latency evaluations, and (3) be quick to compute and financially efficient. Please see Figure 1 for the calculation of Goodness vs Performance that we will further define throughout our paper.\nOur target audience includes resource-constrained developers - such as engineers at smaller companies or universities \u2014 who lack access to human evaluations, large-scale compute, or public"}, {"title": "2 RELATED WORK", "content": "Evaluating Large Language Models (LLMs) has become increasingly important as their usage expands across diverse applications. One approach that has gained traction is LLM as a judge (Zheng et al., 2023; Dubois et al., 2024b), where models are used to evaluate other models by scoring generated outputs. Indeed, several benchmarks which employs LLM-as-Judges, such as Arena-Hard-Auto (Li et al., 2024) and AlpacaEval 2.0 (Dubois et al., 2024a). LLM-as-a-judge raises questions about biases and objectivity, as LLM judges may have similar myiopias to the LLMs that they are judging.\nMost non-LLM-as-judge benchmarks are thus static and ground-truth-based (e.g., multi-choice question answering). They cover a wide range of domains, including math, science, coding, and reasoning. Common ones include MMLU Hendrycks et al. (2020), MATH Hendrycks et al. (2021), GSM-8K Cobbe et al. (2021), HumanEval Chen et al. (2021), BigBench bench authors (2023), HellaSwag Zellers et al. (2019), AGIEval Zhong et al. (2023), as well as comprehensive collection such as HELM Liang et al. (2023).\nHowever, one recent development is LLM-Sys Elo ratings inspired by the Elo rating system used in competitive games. This method evaluates LLMs by having them compete in pairwise comparisons, allowing models to be ranked dynamically based on their performance against others in specific tasks, and has been implemented on many scales (Luo et al., 2024). However, there are critiques to Elo rankings (Boubdir et al., 2023). Namely, (1) there is difficulty representing a suitable breadth"}, {"title": "3 BENCHMARK METHODOLOGY", "content": "To determine which benchmarks to assign under specific hierarchies, we ensure comprehensive coverage LLM benchmark domains as measured in the work of Ilic 2023 (Ili\u0107 & Gignac, 2024).\nIlic et al. highlight that the primary benchmarks in Chatbot Arena show varying degrees of cross-correlation; a model's strong performance in certain benchmarks often predicts success in related ones. By analyzing distinct clusters within their pairwise correlation matrix, we selected representative benchmarks from each cluster: the MMLU-redux global facts, MMLU college mathematics and computer science, BigBench ambiguous and disambiguous benchmarks in sexuality, race, and socioeconomic status, and ARC-C-Challenge. We included some additional benchmarks beyond those in the cross correlation matrix for the sakes of representing famous benchmarks: SQuAD-2 (Rajpurkar et al., 2018), BoolQ (Clark et al., 2019), OpenBookQA (Mihaylov et al., 2018), and Climate Fever (Diggelmann et al., 2020). This targeted selection captures a broad spectrum of LLM capabilities while minimizing redundancy.\nHaving selected benchmarks, we move on to scoring and aggregating them. For multiple choice questions, which compromise the majority of our dataset, we prepare the prompt in the following way:\nYou are a succinct and smart LLM who answers questions parsimoniously. Here\nis your question:\u2026 And here are your options: (A:..., B:..., C:..., D:...). Please\nanswer with the letter corresponding to the choice, only!\nWe score multiple choice questions by performing an 1-gram lookup of the correct letter.\nFor boolean questions, we prepare the prompt with the same prefix:\nYou are a succinct and smart LLM who answers questions parsimoniously. Here\nis your question:\u2026 Answer in a True/False only!\nAnd simply score the answer using an XOR with the correct response. Please see Figure 6 for a\ndescription of the relevant benchmark domains."}, {"title": "3.1 BENCHMARK SELECTION", "content": "To determine which benchmarks to assign under specific hierarchies, we ensure comprehensive coverage LLM benchmark domains as measured in the work of Ilic 2023 (Ili\u0107 & Gignac, 2024).\nIlic et al. highlight that the primary benchmarks in Chatbot Arena show varying degrees of cross-correlation; a model's strong performance in certain benchmarks often predicts success in related ones. By analyzing distinct clusters within their pairwise correlation matrix, we selected representative benchmarks from each cluster: the MMLU-redux global facts, MMLU college mathematics and computer science, BigBench ambiguous and disambiguous benchmarks in sexuality, race, and socioeconomic status, and ARC-C-Challenge. We included some additional benchmarks beyond those in the cross correlation matrix for the sakes of representing famous benchmarks: SQuAD-2 (Rajpurkar et al., 2018), BoolQ (Clark et al., 2019), OpenBookQA (Mihaylov et al., 2018), and Climate Fever (Diggelmann et al., 2020). This targeted selection captures a broad spectrum of LLM capabilities while minimizing redundancy.\nHaving selected benchmarks, we move on to scoring and aggregating them. For multiple choice questions, which compromise the majority of our dataset, we prepare the prompt in the following way:\nYou are a succinct and smart LLM who answers questions parsimoniously. Here\nis your question:\u2026 And here are your options: (A:..., B:..., C:..., D:...). Please\nanswer with the letter corresponding to the choice, only!\nWe score multiple choice questions by performing an 1-gram lookup of the correct letter.\nFor boolean questions, we prepare the prompt with the same prefix:\nYou are a succinct and smart LLM who answers questions parsimoniously. Here\nis your question:\u2026 Answer in a True/False only!\nAnd simply score the answer using an XOR with the correct response. Please see Figure 6 for a\ndescription of the relevant benchmark domains."}, {"title": "3.2 BENCHMARK GROUPING", "content": "In line with psychometric traditions, we categorize our MPG subdomains into three primary areas:\nFactual Recall: This subdomain assesses the model's domain knowledge, particularly in\nrelation to global facts, science, and climate change, which are known to correlate with\nother factual datasets. The benchmarks used in this category include BoolQ (developed"}, {"title": "1. Factual Recall:", "content": "This subdomain assesses the model's domain knowledge, particularly in\nrelation to global facts, science, and climate change, which are known to correlate with\nother factual datasets. The benchmarks used in this category include BoolQ (developed"}, {"title": "2. Linguistic Capability and Social Understanding:", "content": "This area focuses on the model's sensitivity to social biases. Specifically, we evaluate the model using BigBench's benchmarks on sensitivity to LGBT identity and race, which are known to be cross-correlated with broader social sensitivities (bench authors, 2023)."}, {"title": "3. Problem Solving:", "content": "This subdomain tests the model's ability to solve complex problems. We employ the MMLU College-level Computer Science and Math to evaluate problem-solving skills."}, {"title": "3.3 SCORE AGGREGATION", "content": "We consider each node i in this tree as a beta distribution with shape Beta(\u03b1\u03af, \u03b2i), and each collection of children under a parent to be overlapping samples from a similar space. Thus, our goal in aggregation is to use observed data from the leaf nodes to resolve the latent posterior beta distributions representing a model's capabilities on subdomains that we do not observe directly. The mean and 95% coverage of these latent aggregates become the scores that we present in Figure 1 and 4.\nThe score of the model's answers on each benchmark question is an observation which can be modeled by a binomial likelihood function. As a reminder to the reader, a beta distribution is conjugate with a binomial likelihood function; therefore, when defining the prior to be non-informative; that is, a lima,b\u21920 Beta(a, b), the posterior beta distributions is computed by setting the distributions' parameters to Beta(#scores, N\u2081 - #scores). Here, N\u2081 is the number of questions in each benchmark. We propose a Monte-Carlo Markov Chain (MCMC) to simulate latent questions from the aggregate beta distributions, in which we draw a probability from each child posterior to simulate a single latent \"score\" from a Bernoulli distribution.\nSpecifically, here is the above in pseudocode:\nInitialization:\nLet N = \u2211 i \u2200 nodes i\nLet xi be a scored question, X\u2081 the set of scored questions on each question from leaf node i\nLet zk be a sample, Zk the set of samples from the binomial likelihood for each non-child node\nLet D be the space of subdomains with d \u2208 D referring to each second-level (subdomain) node"}, {"title": "4 MODEL EVALUATION", "content": "In order to evaluate models, we used a RunPod console to inference six open source models on A100 GPUs: yi-1.5-34b-chat, llama-3.1-70b-Instruct, quen2-72b-Instruct, phi-3-small-8k-instruct, gemma-2-9b-it, gemma-2-27b-it, and qwen2-72b-instruct, and the following eight proprietary models on their own public facing APIs: GPT-40-2024-05-13, Gemini 1.5 Pro 001 05-24, Gemini 1.5 Pro 08-27, Gemini 1.5 Flash 08-27, GPT-4-01-preview (Strawberry), Mistral-large 2, Claude 3.5 Sonnet 2024-06-20, and Claude 3 Opus 2024-02-29.\nWe measured an average Queries-per-Second (QPS) by simply timing the response rate of every prompt that was sent to the external servers for our specific benchmark questions. Please note that another set of benchmark questions, including longer and multimodal questions, may have garnered a different QPS ordering."}, {"title": "5 RESULTS", "content": "For our main figure, please see Figure 1. Here we see a clear distinction between the proprietary models and the open source models in terms of IQ and QPS. Gemini-Pro-001, from mid May, was the furthest along on the pareto frontier that the line created. Many models are within the error bar distributions of other models.\nFurthermore, please see the Appendix for a full page figure showing the rankings between the models, broken down into their subdomains, i.e. Figure 4. We do see a significant difference in the rankings of how different models perform on subdomains, indicating some degree of heterogeneity. GPT-40 leads the factual recall subdomain, whereas Mistral leads the social sensitivity subdomain and Gemini-Pro leads the problem solving by a sizeable margin.\nWe note in Figure 3 that a clustered taxonomy of our individual benchmarks that the models' performance aligns as we would expect: the factuality and problem solving benchmarks form a correlated cluster, and the social sensitivities form another larger cluster, although with more variance within.\nPlease see an ordering of the LLMs that we studied in Figure 4. We note that models have different strengths, with some excelling more at problem solving than others."}, {"title": "5.1 MODEL RANKING", "content": "For our main figure, please see Figure 1. Here we see a clear distinction between the proprietary models and the open source models in terms of IQ and QPS. Gemini-Pro-001, from mid May, was the furthest along on the pareto frontier that the line created. Many models are within the error bar distributions of other models.\nFurthermore, please see the Appendix for a full page figure showing the rankings between the models, broken down into their subdomains, i.e. Figure 4. We do see a significant difference in the rankings of how different models perform on subdomains, indicating some degree of heterogeneity. GPT-40 leads the factual recall subdomain, whereas Mistral leads the social sensitivity subdomain and Gemini-Pro leads the problem solving by a sizeable margin.\nWe note in Figure 3 that a clustered taxonomy of our individual benchmarks that the models' performance aligns as we would expect: the factuality and problem solving benchmarks form a correlated cluster, and the social sensitivities form another larger cluster, although with more variance within.\nPlease see an ordering of the LLMs that we studied in Figure 4. We note that models have different strengths, with some excelling more at problem solving than others."}, {"title": "5.2 CORRELATION TO CHATBOT ARENA", "content": "We calculate the raw score correlation and the rank number correlation of MPG to the Chatbot Arena score and rank, respectively. Additionally, we calculate the raw and rank score correlation of the MMLU rating to the Chatbot Arena score rating. We find significant correlations:\nWe note that MPG raw scores are slightly more correlated to the output of Chatbot Arena than MMLU raw scores are. The improvement in correlation is especially notable given the MMLU leaderboard includes an order of magnitude more questions than the MPG benchmark. Thus, if one's goal were to estimate the Chatbot Arena ranking of a new model quickly, our benchmark may produce a higher probability estimate with less compute than another leading benchmark. Please see Figure 5 for correlation plot."}, {"title": "5.3 SOCIAL SENSITIVITIES", "content": "In the social sensitivity benchmarks, LLMs are presented with two individuals who have different social characteristics. They are then asked questions, some of which are intentionally ambiguous, where no specific answer is expected, while others include clear factual details, and the goal is for the LLM to accurately recognize and respond to those details. (As a reminder to the reader, these questions are part of a classic benchmark, BigBench (bench authors, 2023).)\nWe found a substantial difference in the probability that a model would answer ambiguous questions correctly relative to unambiguous. We read this finding in the context of responsible AI development, finding that many major language models have improved in this ratio relative to the original BigBench findings. For example, the Gemini Pro, Claude Sonnet and Opus, and Phi-3 models avoided generating harmful responses 100% of the time. However, we caution to the reader that more further study is warranted.\nWe note as well that the pattern of consistent differences between scores is some evidence against data contamination. Were these datasets fully contaminated, we would expect the most competent models to get all or most questions correct evenly across ambiguous and disambiguous domains. Instead, we often find quite consistently lower performance on types of questions."}, {"title": "5.4 LIMITATIONS", "content": "Any attempt to aggregate many capabilities into a single number will create problems, and we hope to list some here. First, in manually grouping the benchmarks, we assume that different measures within a sub-domain measure the same underlying construct (e.g., we assume that MMLU global facts tests the same recall skills as Squad 2 without context.) Treating domains as equivalent observations may potentially misinterpret model capabilities. Second, this metrics doesn't account for varying difficulty and reliability across different benchmark. Third, our decision to use non-informative priors obscures a bias of the type of questions \u2013 largely multiple choice \u2013 and how they may not directly line up with the way in which humans actually interface with LLMs."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce IQ, a benchmarking framework that aggregates a minimal set of benchmarks in order to efficiently generalize an agent's capabilities. Our approach prioritizes factual, falsifiable questions, such as \"What is the height of the Eiffel Tower?\" over more subjective prompts like \"compose a beautiful haiku.\u201d We intend our focus on factuality to ensure reproducibility and"}, {"title": "3.3 SCORE AGGREGATION", "content": "We consider each node i in this tree as a beta distribution with shape Beta(\u03b1\u03af, \u03b2i), and each collection of children under a parent to be overlapping samples from a similar space. Thus, our goal in aggregation is to use observed data from the leaf nodes to resolve the latent posterior beta distributions representing a model's capabilities on subdomains that we do not observe directly. The mean and 95% coverage of these latent aggregates become the scores that we present in Figure 1 and 4.\nThe score of the model's answers on each benchmark question is an observation which can be modeled by a binomial likelihood function. As a reminder to the reader, a beta distribution is conjugate with a binomial likelihood function; therefore, when defining the prior to be non-informative; that is, a lima,b\u21920 Beta(a, b), the posterior beta distributions is computed by setting the distributions' parameters to Beta(#scores, N\u2081 - #scores). Here, N\u2081 is the number of questions in each benchmark. We propose a Monte-Carlo Markov Chain (MCMC) to simulate latent questions from the aggregate beta distributions, in which we draw a probability from each child posterior to simulate a single latent \"score\" from a Bernoulli distribution.\nSpecifically, here is the above in pseudocode:\nInitialization:\nLet N = \u2211 i \u2200 nodes i\nLet xi be a scored question, X\u2081 the set of scored questions on each question from leaf node i\nLet zk be a sample, Zk the set of samples from the binomial likelihood for each non-child node\nLet D be the space of subdomains with d \u2208 D referring to each second-level (subdomain) node"}, {"title": "8", "content": ""}]}