{"title": "GraphSPNs: Sum-Product Networks Benefit From Canonical Orderings", "authors": ["Milan Pape\u017e", "Martin Rektoris", "V\u00e1clav \u0160m\u00eddl", "Tom\u00e1\u0161 Pevn\u00fd"], "abstract": "Deep generative models have recently made remarkable progress in capturing complex probability distributions over graphs. However, they are intractable and thus unable to answer even the most basic probabilistic inference queries without resorting to approximations. Therefore, we propose graph sum-product networks (GraphSPNs), a tractable deep generative model that provides exact and efficient inference over (arbitrary parts of) graphs. We investigate different principles to make SPNs permutation invariant. We demonstrate that GraphSPNs can (conditionally) generate novel and chemically valid molecular graphs, being competitive to, and sometimes even better than, existing intractable models. We find out that (Graph)SPNS benefit from ensuring the permutation invariance via canonical ordering.", "sections": [{"title": "1 INTRODUCTION", "content": "Graphs are a fundamental framework for representing real or abstract objects and their hierarchical interactions in a diverse range of scientific and engineering applications, such as discovering new materials [Choudhary et al., 2022], developing personalized diagnostic strategies [Chandak et al., 2023], and estimating time of arrival [Derrow-Pinion et al., 2021]. Nonetheless, designing probabilistic models for graphs is challenging. Graphs can exhibit highly complex and combinatorial structures, making it difficult to capture their probabilistic behavior effectively. While traditional approaches struggle to handle this problem, deep generative models-which rely on expressive graph neural networks [Wu et al., 2020, Zhang et al., 2020]\u2014have recently made significant progress in this direction [You et al., 2018, Simonovsky and Komodakis, 2018, De Cao and Kipf, 2018, Shi et al., 2020, Jo et al., 2022].\nThe complication with these models is their limited ability to carry out probabilistic inference tasks beyond mere sampling and, typically, exact likelihood evaluation. For example, arbitrary conditioning with an already trained model allows us to sample conditionally on an existing (part of a) graph. The conditioning significantly reduces the space of possible solutions, which is enormous in some domains [Reymond et al., 2012]. Other inference mechanisms, including marginalization, maximum a posteriori estimation, and expectation, have great potential in designing graphs with desired profiles.\nSum-product networks (SPNs) [Poon and Domingos, 2011] are deep generative models for fixed-size tensor data. Their essential feature is that they are tractable, which means that-under certain assumptions\u2014they guarantee to answer a large family of complex probabilistic queries exactly and efficiently [Vergari et al., 2021, Choi et al., 2020]. The existing work on SPNs for graphs is limited to representation learning [Zheng et al., 2018, Errica and Niepert, 2023]. Therefore, we propose GraphSPNs, deep (one-shot) generative models for tractable probabilistic inference on graphs.\nThere are two main challenges in designing such models. (i) SPNs are probability distributions defined on a fixed-dimensional space. Hence, we have to deal with the fact that each instance of a graph has a different number of nodes and edges. (ii) Graphs are permutation invariant objects [Veitch and Roy, 2015]. Consequently, our models must be agnostic to re-ordering the nodes in a graph. In other words, the probability of a graph has to remain unchanged when permuting the nodes (we want to recognize the same graph up to all its permutations). We address (i) in a pragmatic way, using virtual nodes. This principle is frequently adopted in deep generative models for graphs [Madhawa et al., 2019]. However, the real difficulty is to deal with (ii). Indeed, learning a permutation invariant distribution is hard since the number of modes of the target data distribution is much higher than a non-invariant, canonical distribution. Therefore, this paper investigates different techniques to make GraphSPNs permutation invariant."}, {"title": "2 BACKGROUND", "content": "Sum-product networks. A tensorized SPN [Peharz et al., 2020a,b, Loconte et al., 2024] is a deep learning model of a probability distribution, p(X), over a fixed-size tensor, X \u2208 X. The network contains several layers of computational units (similar to neural networks [Vergari et al., 2019]). Each layer is defined over its scope, \\(\\psi \\subseteq X\\), i.e., a subset of the input. There are three types of layers, depending on the units they encapsulate: sum layer \\(L_s\\), product layer \\(L_p\\), and input layer \\(L_I\\). The units of input layers are user-defined probability distributions, \\(p(\\psi)\\). For \\(n_I\\) units, an input layer computes \\(p_i(\\psi)\\) for \\(i \\in (1, . . . , n_I)\\) and outputs an \\(n_I\\)-dimensional vector of probabilities \\(1\\). The units of product layers are factored distributions, applying conditional independence over a pair-wise disjoint partition of their scope. A product layer receives outputs from \\(n\\) layers, \\((l_1, . . . , l_n)\\), and computes either an Hadamard product, \\(l = \\prod_{i=1}^{n}l_i\\), or Kronecker product, \\(l = \\bigotimes_{i=1}^{n} l_i\\). The units of sum layers are mixture distributions. For \\(n_S\\) units, a sum layer receives an \\(n\\)-dimensional input, \\(l\\), from a previous layer and computes \\(Wl\\), where \\(W\\) is an \\(n_S \\times n\\) matrix of row-normalized weights. The output (layer) of a tensorized SPN is typically a sum layer.\nThe key benefit of SPNs is that they provide tractable inference over arbitrary subsets of \\(X\\). However, to this end, the sum units have to satisfy smoothness, the product units have to satisfy decomposability, and the input units have to be tractable [Choi et al., 2020, Vergari et al., 2021].\nPermutation invariance. Exchangeable data structures, including sets, graphs, partitions, and arrays [Orbanz and Roy, 2014], have a factorial number of possible configurations (orderings). Permutation invariance says that no matter a selected configuration, the probability of an exchangeable data structure has to remain the same. To define the permutation invariance of a probability distribution, let \\(S_n\\) be a finite symmetric group of a set of \\(n\\) elements. This is a set of all \\(n!\\) permutations of \\([n] := (1, . . ., n)\\), where any of its members, \\(\\pi \\in S_n\\), permutes an \\(n\\)-dimensional vector, \\(X := (X_1, ..., X_n)\\), as follows: \\(\\pi X = (X_{\\pi(1)},..., X_{\\pi(n)})\\). The probability distribution \\(p\\) is permutation invariant iff \\(p(X) = p(\\pi X)\\) for all \\(\\pi \\in S_n\\).\n\\(X\\) is permutation invariant if \\(p(X)\\) is.\nSPNs are only partially permutation invariant [Pape\u017e et al., 2024], and so, for this paper, we consider SPNs as permutation-sensitive distributions.\nProblem definition. Let \\(G := (X, A)\\) be an \\(n\\)-node graph characterized by a feature matrix, \\(X \\in \\mathcal{X}^n\\), and an adjacency tensor, \\(A \\in \\mathcal{A}^{n\\times n}\\). For example, to express \\(G\\) with \\(q\\) types of nodes and \\(r\\) types of edges, we use one-hot encoding to define \\(\\mathcal{X} := (0,1)^q\\) and \\(\\mathcal{A} := (0,1)^{r+1}\\), where the extra category \\((r+1)\\) in \\(\\mathcal{A}\\) is to reflect the fact that there can be no connections between two nodes. Our objective is to learn a tractable probability distribution over graphs, \\(p(G)\\), given a collection of observed graphs, \\((G_1,...,G_N)\\), where each \\(G_i\\) has a different number of nodes and edges. We would like our GraphSPNs to inherit the ability to tractably answer the same inference quires as the conventional SPNs."}, {"title": "3 GRAPH SUM-PRODUCT NETWORKS", "content": "A graph sum-product network (GraphSPN) is a probability distribution over a graph, \\(p(G)\\), where \\(G\\) is random in its values and size. We conveniently use the fact that \\(G\\) is represented by the feature matrix and the adjacency tensor, \\(G := (X, A)\\), and design the GraphSPN as a joint probability distribution of \\(X\\) and \\(A\\),\n\\[p(G) := g_n(p_{m,n}^{spn}(X, A)),\\]\nwhere \\(g_n\\) is an operator ensuring the permutation invariance, and \\(p_{m,n}^{spn}\\) is a permutation-sensitive SPN, as described in the next paragraph. The joint support of \\(p_{m,n}^{spn}\\) leaves us with many choices to reorganize the elements of \\(X\\) and \\(A\\). We first concatenate \\(X\\) and \\(A\\) to form an \\(n\\) by \\(n + 1\\) matrix and then vectorize this matrix in a row-wise manner (Figure 1). In this way, there is always a node followed by its associated edges, \\((X, A) := (X_1, A_1, ..., X_n, A_n)\\), where \\(X_i \\in \\mathcal{X}\\) and \\(A_i \\in \\mathcal{A}^n\\) are the \\(i\\)th slice of \\(X\\) and \\(A\\) along the first dimension, respectively.\nVirtual node-padding. SPNs assume fixed-size inputs (Section 2). To deal with the varying size of \\(G\\), we design the SPN in (1) as \\(p_{m,n}^{spn}(X, A) := p_{m,n}^{spn}(padding_{m,n}(X, A))\\)."}, {"title": "Exact permutation invariance", "content": "The general way to design permutation invariant \\(p(G)\\) is to make \\(g_n\\) in (1) an average over all permutations \\(\\pi \\in S_n\\) of \\((X, A)\\),\n\\[p(G) := \\frac{1}{n!} \\sum_{\\pi\\in S_n} p_{m,n}^{spn}(\\pi X, \\pi A),\\]\nwhere \\(\\pi A := (\\pi A_{\\pi 1},..., \\pi A_{\\pi n})\\). This approach can be seen as artificially extending each instance of \\((X, A)\\) to \\(n!\\) of its permutations. All the components \\(p_{m,n}^{spn}\\) of the mixture (2) are an identical SPN, i.e., the components share the same parameterization. This approach is computationally costly, as it requires \\(n!\\) passes through \\(p_{m,n}^{spn}\\), the scope of which has \\(m(m + 1)\\) entries. Note that (2) is known as the Janossy distribution [Daley and Vere-Jones, 2003]."}, {"title": "Sorting", "content": "Any graph \\(G\\) has up to \\(n!\\) equivalent permutations, \\(\\pi G, \\pi \\in S_n\\). If we ensure that \\(p_{m,n}^{spn}\\) always sees the same, user-defined, canonical representation of \\(G\\), then the permutation invariance of (1) is guaranteed. One way to find such a representation is to sort each instance of \\(G\\) before entering \\(p_{m,n}^{spn}\\). Then, composing a sorting function, \\(sort\\), with \\(p_{m,n}^{spn}\\) avoids the summation over \\(n!\\) terms in (2), and thus circumvents the use of \\(g_n\\), as follows:\n\\[p(G) := p_{m,n}^{spn}(sort(X, A)),\\]\nwhere it holds that \\(sort(\\pi X, \\pi A) = sort(X, A)\\) for all \\(\\pi \\in S_n\\). The computational complexity of (3) depends on a specific sorting algorithm, e.g., \\(O(n \\log n)\\). After the sorting, there is the need for only a single forward pass through \\(p_{m,n}^{spn}\\), a significant reduction compared to (2). However, certain orderings are more suitable than others [Montavon et al., 2012, Niepert et al., 2016, Defferrard et al., 2016], and the right ordering often depends on domain knowledge."}, {"title": "Approximate permutation invariance", "content": "The exact invariance (2) is computationally infeasible for all but small graphs. Indeed, even for \\(G\\) with, say, 8 nodes, we have to perform 8! (40320) passes through \\(p_{m,n}^{spn}\\), which is prohibitive when \\(p_{m,n}^{spn}\\) is large. This permutation mechanism will slow down the inference as well. For example, marginalizing out certain node(s) has to be done 8! times. Therefore, we consider different ways to relax the exact permutation invariance, which have been used to design approximately permutation invariant neural networks, i.e., transformations \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}\\) [Murphy et al., 2019b,a, Wagstaff et al., 2022].\nWe cast them into their probabilistic interpretation, such that they are probability distributions \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}^+\\)."}, {"title": "k-ary sub-graphs", "content": "The k-ary permutation invariance reduces the complexity of (2) by averaging over all k-node induced sub-graphs of \\(G\\),\n\\[p(G) := \\frac{(n-k)!}{n!} \\sum_{t\\in T_k} p_{k,n}^{spn}(X_t, A_{tt}).\\]\nThe k-node induced sub-graphs are represented by the feature sub-matrix, \\(X_t := (X_{t_1}, ..., X_{t_k}) \\in \\mathcal{X}^k\\), and the adjacency sub-tensor, \\(A_{tt} := (A_{t_1t_1},..., A_{t_kt_k}) \\in \\mathcal{A}^{k\\times k}\\). They are selected by a set of k indices, \\(t := (t_1, ..., t_k) \\in T_k\\), where \\(T_k\\) is the set of all ways to choose k elements out of n unique elements (i.e., the set of all k-tuples of \\([n]\\)). The main benefit of this method is that (i) the scope of \\(p_{k,n}^{spn}\\) has a fixed size, \\(k(k + 1)\\), which also reduces the overall size of \\(p_{m,n}^{spn}\\), and (ii) the number of passes through \\(p_{k,n}^{spn}\\) is lower, \\(n!/(n - k)! < n!\\). The expressivity of this approach grows with \\(k\\). For \\(k = 1\\), we obtain a probabilistic version of Deep Sets [Zaheer et al., 2017], whereas for \\(k = n\\), we recover (2). For high \\(k\\), we capture more dependencies among the nodes, whereas for low \\(k\\), we sacrifice some of them."}, {"title": "Random sampling", "content": "The exact invariance (2) is a marginal distribution, \\(p(G) := \\sum_{\\pi\\in S_n} p(\\pi)p_{m,n}^{spn}(\\pi X, \\pi A)\\), where the permutation \\(\\pi\\) is the latent variable, and \\(p(\\pi) := 1/n!\\) is the uniform distribution on \\(S_n\\). This interpretation suggests to reduce the complexity of the full permutation invariance by approximating (2) with the random average,\n\\[p(G) := \\frac{1}{N} \\sum_{\\pi\\in S_N} p_{m,n}^{spn}(\\pi X, \\pi A).\\]\nHere, \\(S_N\\) is a subset of \\(N < n!\\) random permutations from the full set \\(S_n\\), which are sampled without repetition. The expressivity of (5) depends on \\(N\\). Even for \\(N = 1\\), we can still achieve sufficient approximate permutation invariance, assuming we draw a new sample at each training step and perform the training sufficiently long. For \\(N = n!\\), we recover (2). In the inference regime, it is important to use \\(N > 1\\). However, we can suffer from the risk of drawing less meaningful permutations for a given graph."}, {"title": "4 EXPERIMENTS", "content": "We evaluate GraphSPNs in terms of their capacity to capture the underlying data distribution and their ability to generate realistic and novel graphs. We perform the experiments in the context of the computational design of molecular graphs. We provide the code at https://github.com/\nmlnpapez/GraphSPN."}, {"title": "Molecule generation", "content": "Deep generative models for molecular graphs have recently gained significant attention. They hold great potential in important applications, such as discovering drugs and materials with desired chemical properties. Given a dataset of molecular graphs, the task is to learn a probability distribution of chemically valid molecules, \\(p(G)\\). This is an intricate combinatorial problem, where not all combinations of atoms and bonds can be connected, but the connections must satisfy chemical valency constraints. We want \\(p(G)\\) to generate molecules that were not seen during the training and satisfy the chemical valency rules."}, {"title": "Dataset", "content": "We test GraphSPNs on QM9 [Ramakrishnan et al., 2014] dataset, a standard benchmark often used to assess deep generative models for molecular design. QM9 contains around 134k stable, small organic molecules with at most 9 atoms of 4 different types."}, {"title": "Metrics", "content": "We adopt the standard metrics for the molecule generation task. Validity (V) and Validity w/o check (V w/o check) are the percentages of chemically valid molecules (i.e., those not violating the chemical valency rules) in all generated molecules with and without any correction mechanisms, respectively. Uniqueness (U) is the percentage of all generated molecules that are valid and unique (i.e., not a duplicate of some other generated molecule). Novelty (N) is the percentage of valid molecules not in the training data."}, {"title": "GraphSPN variants", "content": "We consider the following GraphSPN variants: sort, k-ary, and rand that implement (3), (4), and (5), respectively, and the none variant with no invariance properties (\\(g_n\\) in (1) is identity)."}, {"title": "Results", "content": "Table 1 shows that all the GraphSPN variants achieve 100% validity. This is because we use the posthoc validity correction mechanism from [Zang and Wang, 2020] to satisfy the valency rules. However, the effectiveness of this correction depends on the validity w/o check. Naturally, if we have to correct more (possibly large) molecules, this quickly becomes computationally expensive. From this perspective, the sort variant delivers the best results, as its validity w/o check is highest. This result is because sort imposes the canonical ordering of atoms in each molecule, which is not satisfied by the other GraphSPN variants. \\(p_{m,n}^{spn}\\) of the none, rand, and sort variants has the same architecture. Still, it is hard for the none and rand variants to capture the underlying data distribution. This result leads us to conclude that departing from the canonical ordering yields a more complicated distribution (with more modes), which is harder to capture sufficiently well, as evidenced by the poor ability to learn the essence of this problem, i.e., the valency rules. In the top of Table 1, we show baseline models (see Section C). They are permutation-sensitive and also rely on the canonical ordering. We can see that the sort variant is competitive to\u2014and sometimes even outperforms\u2014these baselines.\nTo illustrate that GraphSPNs provide tractable probabilistic inference, Figure 2 displays conditional molecule generation for the sort variant. Here, it can be seen that each molecule's newly generated part varies in size and composition."}, {"title": "5 CONCLUSION", "content": "The study of permutation invariance in the SPN community has received limited attention. This paper has the ambition to make a few steps towards improving upon this state. We have investigated different ways of ensuring the permutation invariance of SPNs and proposed GraphSPNs, a novel class of tractable deep generative models for exact and efficient probabilistic inference over graphs. We show that GraphSPNs are competitive with existing deep generative models for graphs. Among the proposed GraphSPN variants, the best performance was achieved with the one based on canonical ordering. This shows that the canonical ordering simplifies the target data distribution and makes it easier for an SPN-which is less densely connected than the models based on neural networks to be trained successfully."}, {"title": "A MORE NOTES ON PERMUTATION INVARIANCE", "content": "Figure 3 illustrates the fundamental principles underlying the GraphSPN variants discussed in Section 3."}, {"title": "B TRACTABILITY", "content": "Tractability. An SPN is tractable if it answers probabilistic queries exactly and efficiently. Here, exact means that the answers do not involve any approximation or heuristic, and efficient means that the answers can be obtained (computed) in polynomial time [Choi et al., 2020]. We are interested in probabilistic queries that can collectively be expressed in terms of the following expectation:\n\\[v(f) := \\int f(G)p(G)dG,\\]\nwhere \\(f (G)\\) is a function that allows us to formulate a desired query over (a part of) \\(G\\). We defer concrete examples of \\(f (G)\\) to Section B.1. The expectation (6) admits a closed-form solution only if both \\(f\\) and \\(p\\) satisfy certain structural constraints.\nAssumption 1. (Constraints on p). We consider the following constraints. (i) Smoothness: each \\(u \\in L_s\\) satisfies \\(\\forall a, b \\in in(u) : \\psi_a = \\psi_b\\), where \\(in(u)\\) is the set of inputs of a \\(u\\)-unit. (ii) Decomposability: each \\(u \\in L_p\\) satisfies \\(\\forall a, b \\in in(u) : \\psi_a \\cap \\psi_b = \\emptyset\\). (iii) Tractable input layers: \\(\\int f_u(\\psi_u) p(\\psi_u) d\\psi_u\\) is tractable for each \\(u \\in L_I\\).\nAssumption 1(iii) requires that each \\(p_i(\\psi)\\) is from a tractable family of probability distributions [Barndorff-Nielsen, 1978] and \\(f_i(\\psi)\\) is such that the integral admits an algebraically closed-form solution.\nAssumption 2. (Constraints on f). The function \\(f (G) := f(X_1, A_1 . . ., X_n, A_n)\\) is omni-compatible [Vergari et al., 2021] with respect to \\(p_{m,n}^{spn}\\).\nProposition 1. (Tractability of GraphSPNs.) Let \\(p(G)\\) be a GraphSPN (1) satisfying Assumptions 1, and let \\(f(G)\\) be a function satisfying Assumption 2. Then, the integral (6) is tractable.\nProposition 1 covers all the GraphSPN variants (2), (3), and (5), except the k-ary sub-graphs (4). Indeed, the average in (2) and (5) can be seen as a sum unit with fixed uniform weights, which satisfies the smoothness assumption. The sorting operation in (3) does not affect the tractability. However, regarding the k-ary GraphSPNs, the average in (4) does not satisfy the smoothness assumption (i.e., it is not a smooth sum unit), as each of its children has a different scope. To make the k-ary variant tractable, we must perform the smoothing [Shih et al., 2019, Choi et al., 2020]."}, {"title": "B.1 TRACTABLE INFERENCE QUERIES OVER GRAPHS", "content": "Querying r-node sub-graphs. An omni-compatible function \\(f\\) (Assumption 2) that targets an r-node sub-graph of \\(G\\) can be expressed as follows:\n\\[f(X, A) := \\prod_{i\\in \\alpha}h(X_i) \\prod_{\\substack{i,j \\in \\alpha}}h(A_{ij}) \\prod_{\\substack{i\\in \\alpha,k \\notin \\alpha}}h(A_{ik}) \\prod_{t \\notin \\alpha}h(X_t) \\prod_{\\substack{u \\notin \\alpha, v \\in \\alpha}}h(A_{vu}) \\prod_{\\substack{l \\notin \\alpha, m \\notin \\alpha}}h(A_{lm}),\\]\nwhere \\(\\alpha := {1, ..., a_r} \\subseteq {1, . . ., n}\\) specifies indices of \\(r\\) nodes presented in a queried sub-graph.\nMarginalizing selected nodes and (or) edges. The marginal query is realized by an appropriate choice of \\(h\\) for \\(X_i\\) and \\(A_{ij}\\). The choice \\(h(X_i) := \\mathbb{1}_B(X_i)\\), where \\(\\mathbb{1}_B\\) is the indicator function and \\(B := \\mathcal{X}\\), corresponds to marginalizing node feature of i-th node in the graph \\(G\\). The evidence query is obtained for \\(B := X_i\\). Similarly, choosing \\(h(A_{ij}) := \\mathbb{1}_B(A_{ij})\\) and \\(B := \\mathcal{A}\\) corresponds to marginalizing edge between nodes i and j in the graph \\(G\\). Again, the evidence query is given by setting \\(B := A_{ij}\\). We demonstrate an example of the marginal query for \\(\\alpha := {2}\\) in Figure 3."}, {"title": "C EXPERIMENTAL DETAILS", "content": "Preprocessing. We use the RDKit library [Landrum et al., 2006] to first kekulize the molecules and then remove the hydrogen atoms. The final molecules contain only the single, double, and triple bonds. Since we aim to test the permutation invariance of various GraphSPNs, we randomly permute the atoms in each molecule during the preprocessing.\nBaselines. We compare GraphSPNs with various molecular deep generative models: grammar variational autoencoder (GVAE) [Kusner et al., 2017], character VAE (CVAE) [G\u00f3mez-Bombarelli et al., 2018], regularizing VAE (RVAE) [Ma et al., 2018], junction tree VAE (JT-VAE) [Jin et al., 2018], graph VAE (GraphVAE) [Simonovsky and Komodakis, 2018], graph real-valued non-volume preserving (GraphNVP) flow [Madhawa et al., 2019], graph autoregressive flow (GraphAF) [Shi et al., 2020], graph discrete flow (GraphDF) [Luo et al., 2021], molecular flow (MoFlow) [Zang and Wang, 2020], modular flow (ModFlow) [Verma et al., 2022], and graph residual flow (GRF) [Honda et al., 2019].\nArchitecture. To implement the permutation-sensitive part of GraphSPNs, we adopt the EinSum networks [Peharz et al., 2020a]. The expressive power of this monolithic and tensorized variant of SPNs is driven by four hyper-parameters: the number of layers \\(n_l \\in {1,2,3}\\), the number of sum units \\(n_S \\in {10, 40, 80}\\), the number of input units \\(n_I \\in {10, 40}\\), and the number of repetitions \\(n_R \\in {10, 40, 80}\\). The input units are categorical distributions. Molecules are undirected acyclic graphs. To satisfy this constraint, we use only the lower triangular part of \\(A\\) in the sampling procedure.\nFor the rand variant of GraphSPNs (5), we use \\(S_m\\), i.e., 20 permutations. For the k-ary variant (4), we use \\(k = 2\\) to obtain the average with only 72 elements. To impose the canonical ordering with the sort variant (3), we use the RDKit library.\nLearning. The results for the baseline methods in the top part of Table 1 are obtained from their respective papers. For all the GraphSPN variants in the bottom part of Table 1, we minimize negative log-likelihood for 40 epochs using the ADAM optimizer [Kingma and Ba, 2014] with 256 samples in the minibatch, step-size \\(\\alpha = 0.05\\) and decay rates \\(\\beta_1 = 0.9\\) and \\(\\beta_2 = 0.82\\). All experiments are repeated 5 times with different initialization of the model's parameters. We sample 4000 molecules to compute the metrics introduced in Section 4."}, {"title": "DADDITIONAL RESULTS", "content": "Figure 5 shows unconditional samples of molecules from the sort variant (3) that was trained on the QM9 dataset. The resulting molecules resemble those from the training data, yet they are all newly discovered molecules."}]}