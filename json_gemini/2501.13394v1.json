{"title": "Concurrent Learning with Aggregated States via Randomized Least Squares Value Iteration", "authors": ["Yan Chen", "Qinxun Bai", "Yiteng Zhang", "Shi Dong", "Maria Dimakopoulou", "Qi Sun", "Zhengyuan Zhou"], "abstract": "Designing learning agents that explore efficiently in a complex environment has been widely recognized as a fundamental challenge in reinforcement learning. While a number of works have demonstrated the effectiveness of techniques based on randomized value functions on a single agent, it remains unclear, from a theoretical point of view, whether injecting randomization can help a society of agents concurently explore an environment. The theoretical results established in this work tender an affirmative answer to this question. We adapt the concurrent learning framework to randomized least-squares value iteration (RLSVI) with aggregated state representation. We demonstrate polynomial worst-case regret bounds in both finite- and infinite-horizon environments. In both setups the per-agent regret decreases at an optimal rate of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$, highlighting the advantage of concurent learning. Our algorithm exhibits significantly lower space complexity compared to (Russo 2019) and (Agrawal, Chen, and Jiang 2021). We reduce the space complexity by a factor of K while incurring only a \u221aK increase in the worst-case regret bound, compared to (Agrawal, Chen, and Jiang 2021; Russo 2019). Additionally, we conduct numerical experiments to demonstrate our theoretical findings.", "sections": [{"title": "Introduction", "content": "The field of reinforcement learning (RL) is dedicated to designing agents that interact with an unknown environment, aiming to maximize the total amount of reward accumulated throughout the interactions (Sutton and Barto 2018). When the environment is complex yet the learning budget is limited, an agent has to efficiently explore the environment, giving rise to the well-known exploration-exploitation trade-off. A large body of works in the RL literature have addressed the challenge related to smartly balancing this trade-off. Among the many proposed methods, algorithms based on randomization are receiving growing attention, both theoretically (Russo and Van Roy 2014; Fellows, Hartikainen, and Whiteson 2021) and empirically (Osband et al. 2016; Janz et al. 2019; Dwaracherla et al. 2020), due to their effectiveness and potential scalability in large applications.\nRandomized least-squares value iteration (RLSVI) represents one example of such randomization-based algorithms (Osband et al. 2019). On a high level, RLSVI injects Gaussian noise into the rewards in the agent's previous trajectories, and allows the agent to learn a randomized value function from the perturbed dataset. With judicious noise injection, the resultant value function approximates the agent's posterior belief of state values. By acting greedily with respect to such randomized value function, the agent effectively executes an approximated version of posterior sampling for reinforcement learning (PSRL), whose efficacy has been substantiated in previous works (Osband, Russo, and Van Roy 2013; Russo and Van Roy 2014; Xu, Dong, and Van Roy 2022). Compared with PSRL, RLSVI circumvents the need of maintaining a model of the environment, severing huge computational cost. Since its advent, RLSVI has been studied extensively in theoretical contexts, such as (Russo 2019) and (Ishfaq et al. 2021).\nIn this work, we look into RLSVI from the perspective of concurrent learning (Silver et al. 2013). Specifically, concurrent RL studies the problem where a cohort of agents interact with one common environment individually, yet are able to share their experience with each other in order to jointly improve their decisions. Such a collaborative setting has useful applications in a variety of realms, such as robotics (Gu et al. 2017), biology (Sinai et al. 2020), and recommendation systems (Agarwal et al. 2016). It is worth mentioning that, although all agents share the identical goal, coordination between agents is nontrivial. In fact, as shown in (Dimakopoulou and Van Roy 2018; Dimakopoulou, Osband, and Van Roy 2018), a poorly coordinated multi-agent algorithm can drastically undermine the overall learning performance. Results in the existing theoretical literature on concurrent RL have demonstrated that PSRL (Osband and Van Roy 2017; Kim 2017; Osband and Van Roy 2014) in a coordinated style is provably efficient (Chen et al. 2022), compared to the earlier cooperative UCRL type of algorithms 1. As pointed out in (Chen et al. 2022), \u201cthese sample-complexity guarantees notwithstanding, concurrent UCRL algorithms suffer from the critical"}, {"title": "Finite-Horizon Concurent Learning", "content": "In this section, we consider a finite-horizon Markov Decision Process (MDP) $M = (H,S, A, P, R)$. There are N agents interacting with the same environment across K episodes. Each episode contains H periods. For episode $k \\in [K]$, period $h \\in [H]$, and agent $p \\in [N]$, we use $s_{k,h}^p$ to denote the state that the agent resides in, $a_{k,h}^p$ the action that the agent takes, and $r_{k,h} = r(s_{k,h}^p,a_{k,h}^p)$ the reward that it receives, where $r : S \\times A \\rightarrow [0, 1]$ is a deterministic reward function. Let the information set $H_k = \\{(s_{k,h}^p,a_{k,h}^p,r_{k,h}^p): h \\in [H]\\}$ be the trajectory during episode k for all the agents. The transition kernel P is defined as $P_{h,s,a}(s')= \\mathbb{P}(S_{h+1} =s' | S_h = s, a_h= a)$. The expected reward that an agent receives in state s when it follows policy \u03c0 at step h is represented by $R_{h,s,\\pi(s)} = \\mathbb{E} [\\Sigma_{a \\in A}\\pi_h(a|s) \\cdot r(s, a)]$. We assume that all agents start from a deterministic initial state $s_1 = \\{s_1^p\\}_{p\\in[N]}$ with $s_{k,1}^p = s_1,\\forall k, p$. In this work we consider deterministic rewards, which can be viewed as mappings from S to A, but all our results apply to the environments with bounded rewards without change. We say that agent $p \\in [N]$ follows policy \u03c0 if for all $h \\in [H]$, $a_h^p = \\pi_h(s)$. We use $V^{\\pi} \\in \\mathbb{R}^{|S|}$ to denote the value function associated with policy \u03c0 in period $h \\in [H]$, such that\n$V_h^{\\pi}(s)= \\mathbb{E} \\left[ \\sum_{j=h}^H \\gamma^{j-h} R_{j,S_j,\\pi(S_j)}(S_j)  \\mid S_h = s \\right],$ \nwhere the expectation is taken over all possible transitions, and we set $V_{H+1}^{\\pi}(s) = 0$ for all $s \\in S$. The optimal value function is denoted as $V_h^{*}(s) = \\max_{\\pi \\in \\Pi} V_h^{\\pi}(s)$, which is the value function associated with the optimal policy. For all $s \\in S, h \\in [H]$, and policy \u03c0, the value function is the unique solution to the Bellman equations\n$V_h^{\\pi}(s) = R_{h,s,\\pi(s)} + \\sum_{s' \\in S}P_{s,h,\\pi(s)}(s')V_{h+1}^{\\pi}(s').$\nWhen \u03c0 is the optimal policy $\\pi^*$, there should be $V_h^{*}(s) = V_h^{\\pi^*}(s)$, and we have\n$V_h^{*}(s) = R_{h,s,\\pi^{*}(s)} + \\sum_{s' \\in S}P_{s,h,\\pi^{*}(s)}(s')V_{h+1}^{*}(s').$\nFor each policy \u03c0, we also define the state-action value function of Q-function of state-action pair (s, a) as the expected return when agent takes action a at state s, and then follows policy \u03c0, so that\n$Q_h^{\\pi}(s,a) = R_{h,s,a} + \\mathbb{E} \\left[ \\sum_{j=h}^H  R_{j,S_j,\\pi(S_j)}(S_j) | S_h = s, a_h = a\\right].$\nCorrespondingly, we define\n$Q_h^{\\pi}(s, a) = R_{h,s,a} + \\sum_{s' \\in S}P_{h,s,a}(s')V_{h+1}^{\\pi}(s'),$\nwhere we use the notation $P_hV(s, a) = \\mathbb{E}_{s'\\sim P_{s,a}}[V(s')]$. Thus by definition $Q_h^{\\pi}(s, a)$ is the maximum realizable expected return when the agent starts from state s and takes action a at period h. From the optimality of $V^*$, we have\n$V_h^{*}(s) = \\max_{a \\in A} Q_h^{\\pi^*}(s, a), \\forall h \\in [H], s \\in S.$\nFurthermore, under the assumption that the reward is bounded between 0 and 1, we have\n$0 \\leq V_h^{\\pi} \\leq V_h^{*} \\leq H, \\forall h \\in [H], \\pi \\in \\Pi.$"}, {"title": "Regret under Finite Horizon Case", "content": "The goal of an RL algorithm is for the agents to learn a good policy through consecutively interacting with the random environment, without prior knowledge about the transition probability P and the reward R. Formally, given $\\pi = \\{\\pi_k^p\\}_{k\\in[K],p\\in[N]}$, with each agent $p \\in [N]$ taking policy $\\pi_k^p$ during each episode $k \\in [K]$, the cumulative expected regret incurred over K periods and N agents is defined as\n$\\text{Regret}(M, K, H, N, \\pi) = \\sum_{p=1}^{N} \\sum_{k=1}^{K} V_1^{*}(s_1) - V_1^{\\pi_{k,p}}(s_1).$\nEmpirical Estimation Define $n_{k,h}(s, a)$ to be the number of times action a has been sampled in state s, period h during episode k by all the agents $p \\in [N]$:\n$n_{k,h}(s,a) = \\sum_{p=1}^{N} \\mathbb{1}\\{(s_{k,h}^p,a_{k,h}^p) = (s,a)\\}.$\nDefine the empirical mean reward for period h during episode k by\n$R_{h,s,a} = \\left(\\sum_{p=1}^{N} \\mathbb{1}\\left\\{(s_{k-1,h-1}^p,a_{k-1,h-1}^p) = (s,a)\\right\\}\\right)^{-1} r_{k-1,h}^p,$\nand $\\forall s' \\in S$, define the empirical transition probabilities for period h during episode k as\n$P_{ks,a}(s') = \\frac{\\sum_{p=1}^{N} \\mathbb{1}\\{(s_{k-1,h-1}^p,a_{k-1,h-1}^p,s_{k-1,h}^p) = (s, a, s')\\}}{n_{k-1,h}(s, a)}.$\nIf (h, s, a) is never sampled during episode k \u2212 1, we set $R_{h,s,a} = 0 \\in \\mathbb{R}$ and $P_{h,s,a} = 0 \\in \\mathbb{R}^{S}$. Note that $R_k$ and $P_k$ are computed from the trajectory from episode k \u2013 1."}, {"title": "Aggregated-state Representations", "content": "Many RL algorithms aim to estimate the value of each state-action pair (e.g. under a tabular representation), but this can be infeasible in some setup where SA is large, since both the required sample size and computational cost will scale up at least linearly in SA. One alternative approach is to consider aggregated-state representations (Dong, Van Roy, and Zhou 2019; Wen and Van Roy 2017; Jiang et al. 2017), which reduces complexity and can accelerate learning by focusing on aggregated state-action pairs. This method partitions the space of state-action pairs into \u0393 blocks, each block can be viewed as an aggregate state, so that the value function representation only needs to maintain one value estimate per aggregated state. Formally, let I be the set of all aggregated states, and let $\\phi_h : S \\times A \\rightarrow \\Gamma$ be the mapping from state-action pairs to aggregated states at period h. Without loss of generality, we let $\\phi \\coloneqq \\{\\phi_h\\}_{h=1}^H \\coloneqq [\\Gamma]$. We define the aggregated representation as follows:\nDefinition 1. We say that $\\{\\phi_h\\}_{h=1}^H$ is an $\\epsilon$-error aggregated state-representation (or $\\epsilon$-error aggregation) of an MDP, if for all $s, s' \\in S, a, a' \\in A$ and $h \\in [H]$ such that $\\phi_h(s, a) = \\phi_h(s', a')$, we have\n$|Q_h^{*}(s, a) - Q_h^{*}(s', a')| \\leq \\epsilon.$\nWhen $\\epsilon = 0$ in Definition 1, we say that the aggregation is sufficient, and one can guarantee that an algorithm finds the optimal policy as $K \\rightarrow \\infty$. When $\\epsilon > 0$, there exists an MDP such that no RL algorithm with aggregated state representation can find the optimal policy (Van Roy 2006). In this case, the best we can do is to approximate the optimal policy with the suboptimality bounded by a function of $\\epsilon$."}, {"title": "Finite-horizon Algorithm and Regret Bound", "content": "The concurrent RLSVI algorithm for the finite-horizon case is given as Algorithm 1. We initialize the all the aggregated state values as H, i.e. $Q_0(\u03b3) = H$ for all $h \\in [H]$. At the beginning of each episode, all the agents restart at initial states $S_1 = \\{s_1^p\\}$, with agent p starting from state $s_1^p$. During pre-round (episode 0), each agent randomly samples their initial trajectory $\\{\\tilde{s}_{0,1}^{p},\\tilde{a}_{0,1}^{p},\\tilde{r}_{0,1}^{p},...,\\tilde{s}_{0, H}^{p},\\tilde{a}_{0, H}^{p},\\tilde{r}_{0, H}^{p}\\}_{h=1}^H$, with $\\tilde{s}_{0,1}^p = s_1$.\nDuring each episode $k \\in [K]$, each agent samples a random vector with independent components $w_k^p \\in \\mathbb{R}^{HSA}$, where $w_{k}^{p}(h, s, a) \\sim N(0, \\sigma^2(h, s, a))$ and $\\sigma_k(h, s, a) = \\sqrt{\\frac{\\beta_k}{N_{k-1,h}(\\phi_h(s,a)) + 1}}$, where $\\beta_k$ is a tuning parameter, $N_{k-1,h}(\\phi_h(s, a))$ is the total number of times that aggregated state $\\phi_h(s, a)$ is reached at period h across all agents during episode k \u2212 1. Given $w_{k}^{p}$, we construct a randomized perturbation of the empirical MDP for agent p as\n$M_{k}^{p} = (H,S, A, P_k, \\tilde{R_k} + w_k^p),$"}, {"title": "Worst-case Regret Bound", "content": "Let M be the set of MDPs with episode number K, horizon H, state space size S, action space size A, transition probabilities P, and rewards R bounded in [0,1]. Let N be the number of agents interacting in the same environment. We use M (K, H, S, A, P, R) to denote an MDP in M.\nWe now provide our first main result for the finite-horizon case.\nTheorem 2. Suppose $\\{\\phi_h\\}_{h\\in[H]}$ is an $\\epsilon$-error aggregation (defined as in Definition 1) of the underlying MDP. For a tuning parameter sequences $\\beta = \\{\\beta_n\\}_{n\\in\\mathbb{N}}$, \u03b1 = $\\{\\alpha_n\\}_{n\\in\\mathbb{N}}$, $\\xi = \\{\\{\\xi_n\\}_{n\\in\\mathbb{N}\\}$, where $\\beta_n = H^3 \\log(2H\\Gamma n)$, $\\alpha_n = \\frac{1}{1+n}$, and\n$\\xi_n = \\epsilon + \\frac{2\\alpha_n H \\sqrt{\\log(2KHN/\\delta)}}{\\sqrt{\\max\\{n,1\\}}} + \\frac{2\\alpha_n \\sqrt{\\beta_{k-1}} \\sqrt{\\log(2KHN/\\delta)}}{\\sqrt{\\max\\{n, 1\\}}}$.\nWith probability 1 \u2013 \u03b4, we have\n\\text{Regret} (M, K, N, \\pi, RLSVI_{\\beta,\\alpha,\\xi}) \\\\< 2\\epsilon KHN + 8KH^2\\sqrt{\\Gamma N}\\sqrt{\\log(6KHN/\\delta)} + 32H^2\\sqrt{K\\Gamma N}\\sqrt[1]{\\log(3HKN/\\delta)} \\\\+2KH^{5/2}\\Gamma\\sqrt[4]{\\log(2KH\\Gamma)}\\sqrt[4]{\\log(6KHN/\\delta)}."}, {"title": "Infinite-horizon Concurrent Learning", "content": "We now turn to the infinite-horizon case. Consider an unknown fixed environment as $M = (T, A, S, P, r, N)$, with N agents interacting in M. Here A = [A] is the action space, S = [S] is the state space, P(s' | s, a) is the transition probability from s\u2208 S to s' \u2208 S given action a \u2208 A. After agent p selects action $a_t^p$ at state $s_t^p$, the agent observes $s_{t+1}^p$ and receives a fixed reward $r_{t+1}^p = r(s_t^p, a_t^p)$ where r \u2208 [0, 1]. A stochastic policy \u03c0 can be represented by a probability mass function \u03c0(\u00b7|st) that an agent assigns to actions in A given situation state st. For a policy \u03c0, we denote the average reward starting at state s as\n$\\lambda_{\\pi}(s) = \\liminf_{T\\rightarrow\\infty} \\mathbb{E} \\left[ \\frac{1}{T}\\sum_{t=0}^{T-1} r_{t+1} | s_1 = s\\right],$\nFor any state s \u2208 S, denote the optimal average reward as $\u03bb\u2217(s) = \\sup_\u03c0 \u03bb_\u03c0(s)$. We consider weakly-communicating MDP, which is defined as follows:\nDefinition 3 (Weakly-communicating MDP). A MDP is weakly communicating if there exists a closed subset of states, where each state within is reachable from any other state within that set under some deterministic stationary policy. And there exists a transient subset of states (possibly empty) under every policy.\nFor any_s, s' \u2208 S and a \u2208 A, denote $P_{s,a,s'} = P(s'|s, a)$. For each policy \u03c0 define transition probabilities under \u03c0 as $P_{s,\\pi,s'} = \\sum_{a \\in A}\\pi(a|s)P_{s,a,s'}$, and reward as $r'_{s,\\pi} = \\sum_{a \\in A}\\pi(a|s)r(s,a)$.\nPseudo-episodes We extend our concurrent learning framework for the finite-horizon case to the infinite-horizon case by incorporating the idea of pseudo-episodes from (Xu, Dong, and Van Roy 2022). Suppose time step t is the beginning of a pseudo-episode when we sample a random variable H ~ Geometric (1 \u2013 \u03b7), where Geometric (1 \u2013 \u03b7) is geometric distribution with parameter 1 \u2212 \u03b7. The agents compute new policies respectively according to the collected trajectories from last pseudo-episode, and sample their own MDPs respectively at time steps t + 1, . . ., t + H \u2212 1. Now the beginning of the next pseudo-episode is set as t + H. We use $H_{t_1,t_2} = \\cup_{p=1}^N \\cup_{i=t_1}^{t_2} \\{s_i^p, a_i^p, r_i^p\\}$ to denote the trajectories of all agents from time step $t_1$ to time step $t_2$.\nFor policy \u03c0, denote the \u03b7-discounted value function as $V^{\\eta} \\in \\mathbb{R}^S$, then we have\n$V^{\\pi} := \\mathbb{E}_H  \\sum_{h=0}^{H-1} \\eta^h r_{t+1} \\Bigg|\\pi, \\\\\n=\\mathbb{E}\\Bigg[ \\sum_{h=0}^{H-1} \\eta^h \\mathbb{E} \\left[ r | P_{H_t} \\right] \\Bigg|\\pi, M \\Bigg],$\nwhere the expectation is taken over the random episode length H. A policy is said to be optimal if $V_p^1 = \\sup_\\pi V^\\pi$. For an optimal policy, we also write $V^1(s) = V^{\\pi^\\star}(s)$ as the optimal value. Note that $V^{\\eta} \\in \\mathbb{R}^S$ satisfies the Bellman equation $V = r + \\eta P_{\\pi}V^{\\eta}$. For any (s, a), define $Q^{\\eta} (s, a) = r(s,a) + \\eta PV^{\\eta}(s)$, where we use the notation that $PV(s) = \\mathbb{E}_{s'\\sim P_{s,\\pi}(s)} [V(s')]$. Correspondingly, define\n$Q^{\\eta}(s,a) = r(s, a) + \\eta PV(s').$\nBy definition, we have $V^{\\eta} (s) = \\max_{a\\in A} Q^{\\eta}(s, a)."}, {"title": "Discounted Regret", "content": "To analyze the algorithm over T time steps, consider $K = \\arg \\max\\{k: t_k < T\\}$ as the number of pseudo-episodes until time T. We use the convention that $t_{K+1} = T + 1$. Given a discount factor \u03b7 \u2208 [0,1), define the \u03b7-discounted regret up to time T as Regret\u03b7(\u039c, \u03a4, \u03c0) = $\\sum_{k=1}^K \\Delta_k$, where \u2206k is the total regret of all N agents over pseudo-episode k: $\\Delta_k = \\sum_{p=1}^N V_1^*(s_1) - V_1^{\\pi_{k,p}}(s_{k,1}^p)$, where $V = V^{\\pi}$, policy $\\pi^{k,p}$ is computed from the trajectory $H_{t_k-1,t_k-1}$ from pseudo-episode k \u2212 1 by agent p, and a ~ $\\pi_t^{k,p}(. | s_t^p)$, $s_{t+1}^p \\sim P(\\cdot|s_t^p, a_t^p)$, $r_t^p = r(s_t^p, a_t^p)$ for $t \\in E_k$, and $E_k$ denotes the time steps within pseudo-episode k. So the discounted regret is a random variable depending on the algorithm's random sampling, and the random lengths of the pseudo-episodes, and as a result,\n$\\Delta_k =  \\mathbb{E}_H \\sum_{h=0}^{H_k-1} \\sum_{p=1}^N \\eta^h \\left( \\lambda^* - \\frac{1}{H_k}r_{t+1}^{k,p}  \\Big| M \\right]  = \\mathbb{E}  \\sum_{h=0}^{H_k-1} \\sum_{p=1}^N \\eta^h \\left( V^* - V_h^{k,p}  \\Big| M \\right].$\nRegret The optimal average reward $\u03bb\u2217$ is state-independent under a weakly-communicating MDP. The agent p selects a policy \u03c0p and executes it within the kth pseudo-episode. The cumulative expected regret incurred by the collection of policies $\\pi= \\{\\pi^{k,p}\\}_{k\\in[K],p\\in[N]}$ over T time steps and across N agents with the fixed environment M is\nRegret (M, T, N, \\pi) := := EK [\u2211k=1 \u0394k].\nwhere the expectation is taken over the random seeds used by the randomized algorithm, conditioning on the true MDP M. In the following, we denote $(s_{k,h}^p, a_{k,h}^p,r_{k,h}^p)$ as the state, action and reward for agent p during pseudo-episode k and period h."}, {"title": "Empirical Estimation", "content": "We let nk(s, a) be the total number of times that (s, a)-pair appears during the kth pseudo-episode, such that\n$n_k(s,a) =  \\sum_{p=1}^{N} \\sum_{t=t_{k}}^{t_{k+1}-1} \\mathbb{1}\\{(a_t^p) = (s,a)\\} .\nThen \u2200s', the empirical estimate $P^k (s'|s, a)$ of the transition probability for pseudo-episode k is\n$P^k(s'|s, a) = \\frac{ \\sum_{p=1}^{N} \\sum_{t=t_{k-1}}^{t_{k}-1} \\mathbb{1}\\{(s_t^p, a_t^p, s_{t+1}^p) = (s, a, s')\\}}{n_{k-1}(s, a)}.$\nThe empirical estimate of the corresponding reward is\n$R^k(s, a) = \\frac{ \\sum_{p=1}^{N} \\sum_{t=t_{k-1}}^{t_{k}-1} \\mathbb{1}\\{(a_t^p) = (s,a)\\} \\sum_{t \\in E_{k-1}}r(s,a)}{n_{k-1}(s, a)}.$"}, {"title": "Aggregated States", "content": "We extend the aggregated states in the finite-horizon case to the infinite horizon case. Let I be the set of all aggregated states, and let \u03c6 : S \u00d7 A \u2192 \u03a6 be the mapping from state-action pairs to aggregated states. We let \u03a6 = [\u0393]. The aggregated representation for the infinite-horizon case is defined as follows:\nDefinition 4. We say that \u03c6 is an e-error aggregated state-representation (or e-error aggregation) of an MDP, if for all s, s' \u2208 S, a, a' \u2208 A such that \u03c6(s, a) = \u03c6(s', a'), we have |Q(s, a) \u2013 Q(s', a')| \u2264 \u03f5.\nWe are now ready to present the concurrent learning algorithm for the infinite-horizon case and the theoretical guarantee, as detailed in the next section."}, {"title": "Infinite-Horizon Algorithm and Regret Bound", "content": "The concurrent learning algorithm for the infinite-horizon MDP is summarized as Algorithm 2. Our result is based on the following definition of reward averaging time proposed by (Dong, Van Roy, and Zhou 2022).\nDefinition 5. The reward averaging time $\u03c4_\u03c0$ of a policy \u03c0 is the smallest value $\u03c4 \\in [0,\\infty)$ such that $\u2200T \u2265 0, s \u2208 S$,\n$\\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E} \\left[ r_{t+1} | s_0 = s \\right] - \u03bb_{\\pi} (s)  < \u03c4.$\nTypically the regret bounds established in the literature requires assessing an optimal policy within bounded time. Examples include episode duration (Osband, Russo, and Van Roy 2013; Jin et al. 2018), diameter (Auer, Jaksch, and Ortner 2008), or span (Bartlett and Tewari 2012). Policies that require intractably large amount of time are infeasible in practice. So we impose the following assumption:\nAssumption 6. For any weakly communicating MDP M with state space S and action space A, \u2203\u03c4 < \u221e such that \u03c4\u2217 < \u03c4.\nWhen \u03c0\u2217 is an optimal policy for M, $\u03c4\u2217 := \u03c4_{\u03c0^\u2217}$ is equivalent to the notion of span in (Bartlett and Tewari 2012). Let M be the set of infinite-horizon weakly-communicating MDPs with state space size S, action space size A, rewards bounded in [0, 1] that satisfy Assumption 6."}, {"title": "Numerical Experiments", "content": "We present numerical results for both finite-horizon and infinite-horizon cases in Figure 1. For the finite-horizon case (S, A, K, H) or the infinite-horizon case (S, A, T), the transition probabilities are drawn from a Dirichlet distribution, and rewards, fixed as deterministic, are uniformly distributed on [0, 1], forming inherent features of the MDP class.\nThe finite-horizon case settings are (i) K = 20, H = 30, S = 5, A = 5; (ii) K = 25, H = 40, S = 10, A = 10; (iii) K = 30, H = 50, S = 20, A = 20. The infinite-horizon case settings are with T = 300 and (i) S = 5, A = 5; (ii) S = 20, A = 20; (iii) S = 30, A = 30, where \u03b7 = 0.99 in the pseudo-episode sampling. Under each setting, we compare the results for N = 1, 3, 5, 7, 10, 15, 20, 30, 40, 50, with \u03f5 = 0.\nFor each agent number N, we sample 500 MDPs from the defined class and compute the maximum cumulative regret, simulating the worst-case regret emphasized in our theory. For the infinite-horizon setup, we average the regret over 50 geometric segmentations of [T] per MDP and compute the worst-case regret across 500 MDPs. Both Figure 1a and Figure 1b show a $1/\\sqrt{N}$ decreasing trend for the per-agent regret, aligned with our theoretical results for both finite-horizon and infinite-horizon cases."}, {"title": "Discussion", "content": "In this work, we extend the Randomized Least Square Value Iteration algorithm to a concurrent learning framework with e-error aggregated states representations, where N agents learn in parallel and share data with each other according to some strategically designed schedule. For both finite-horizon episodic MDP and infinite-horizon MDP, we provide concrete algorithms respectively and prove corresponding upper bounds for the worst-case regret. We also conduct simulated experiments of the proposed algorithms to demonstrate the validity of our theoretical results. Future research directions include deriving a sharp lower bound and extending concurrent learning framework to more general Thompson sampling-based algorithms."}]}