{"title": "Interpretability in Action: Exploratory Analysis of VPT, a Minecraft Agent", "authors": ["Karolis Jucys", "George Adamopoulos", "Mehrab Hamidi", "Stephanie Milani", "Mohammad Reza Samsami", "Artem Zholus", "Sonia Joseph", "Blake Richards", "Irina Rish", "\u00d6zg\u00fcr \u015eim\u015fek"], "abstract": "Understanding the mechanisms behind decisions taken by large foundation models in sequential decision making tasks is critical to ensuring that such systems operate transparently and safely. In this work, we perform exploratory analysis on the Video PreTraining (VPT) Minecraft playing agent, one of the largest open-source vision-based agents. We aim to illuminate its reasoning mechanisms by applying various interpretability techniques. First, we analyze the attention mechanism while the agent solves its training task-crafting a diamond pickaxe. The agent pays attention to the last four frames and several key-frames further back in its six-second memory. This is a possible mechanism for maintaining coherence in a task that takes 3-10 minutes, despite the short memory span. Secondly, we perform various interventions, which help us uncover a worrying case of goal misgeneralization: VPT mistakenly identifies a villager wearing brown clothes as a tree trunk when the villager is positioned stationary under green tree leaves, and punches it to death.", "sections": [{"title": "1. Introduction", "content": "Large transformer-based models have achieved significant success in autoregressively predicting the next token across various modalities, including language, images, and audio. Recently, these models have started to be used as agents in online settings. Given the advancements in large-scale AI-based agents interacting with real and simulated worlds (Abi Raad et al., 2024; Figure, 2024), it is crucial to understand their decision-making processes both in and out of distribution. VPT (Baker et al., 2022) is one of the first large-scale transformer-based agents (250 million parameters). The techniques that have been developed in the field of mechanistic interpretability have the potential to be useful for better understanding VPT and other agents like it. We chose VPT as the model organism for our studies because it is one of the largest and most capable open-source embodied agentic models that act in a 3D environment. It has also been used as a backbone for developing other agents (Lifshitz et al., 2023; Milani et al., 2023). There are more advanced agents, such as SIMA (Abi Raad et al., 2024); however, they are not openly available for study.\nMechanistic interpretability. Mechanistic interpretability seeks to reverse engineer neural networks into the algorithms that the network weights have implemented throughout training. The field has focused on circuit-based methods (Wang et al., 2022; Elhage et al., 2021) and understanding attention heads (McDougall et al., 2023; Olsson et al., 2022b). However, most of this work has been on large language models (LLMs), with limited investigation into other modalities. We propose that applying mechanistic interpretability techniques that have been designed and tested on language to other modalities is essential, not only to validate the method but also to establish a foundation for the nuances of the various new modalities. In addition, VPT has been fine-tuned with reinforcement learning (RL) and vision interpretability techniques often do not work for vision-based RL agents (Hilton et al., 2020).\nMechanistic interpretability on agents. Understanding the behavior of agentic models mechanistically poses more challenges than for traditional LLMs for several reasons. First, isolating an agent's behavior requires replicating the environment and the actions leading to that behavior, which is difficult for agentic models. For instance, Hanna et al. (2023) identified a circuit in GPT-2 that computes greater than by prompting the model with sentences like, \"The war lasted from the year 1732 to the year 17\" and analyzing its activations. In contrast, for stochastic agentic models in dynamic environments, it is challenging to recreate slight variations of similar situations to assess if the model has generalized knowledge about a scenario. While LLMs and their language outputs are stochastic, LLM interpretability researchers benefit from being able to carefully craft their prompts to elicit certain behaviors from the model.\nAdditionally, computing rollouts for large-scale agentic models is expensive, and comparing actions across different rollouts is complex. Comparing action probabilities and analyzing rollouts proved most useful in assessing the effect of ablations on the overall behavior of VPT. Rewards can sometimes provide a metric to compare the success of rollouts, but it does not capture the detail needed to understand if an agent's behavior has been altered and often can only show if the agent remains functional. Finally, agentic models are trained to maximize long-term rewards, adding another layer of complexity to their decision-making processes compared to LLMs optimized for next-token prediction.\nOur paper makes the following contributions:\n\u2022 We find clues on how the agent maintains coherence in a 3-10-minute task with only 6 seconds of memory.\n\u2022 We show that single attention output ablations only influence actions when those actions are uncertain.\n\u2022 We discover a new case of goal misgeneralization in the wild: when we place a brown villager under tree leaves and make it stand still, VPT mistakes the \"villager-tree\" for a real tree, begins to attack it, and often kills the villager.\nGoal misgeneralization is a failure mode where an agent competently pursues an unintended goal when in a novel situation (Shah et al., 2022; Di Langosco et al., 2022). This incident further reinforces our initial motivation of trying to understand how large-scale agents interact with the world both in and out of distribution."}, {"title": "2. Related work", "content": "Mechanistic interpretability. Most mechanistic interpretability work has focused on reverse engineering circuits in LLMs (Wang et al., 2022; Lieberum et al., 2023; Conmy et al., 2023). There has been some limited work on understanding transformer-based vision models. For example, Gandelsman et al. (2024) found that attention heads can have specialized roles, such as focusing on shapes, colors, and object counts.\nInterpretable reinforcement learning. Reinforcement learning techniques can sometimes be substantially easier to interpret depending on what kind of model is used for the policy and value function. Techniques such as linear function approximation, decision trees, and tabular methods often provide clear and understandable representations of the learned policies and value functions; however, almost all state-of-the-art RL uses deep RL (Milani et al., 2024; Glanois et al., 2021; Kenny et al., 2023; Dao et al., 2018).\nRecent advancements in interpretable RL include the development of frameworks and methodologies that aim to balance the performance and transparency of RL models. For example, the SINDy-RL framework combines sparse dictionary learning with deep reinforcement learning to create interpretable and capable agents. This approach leverages the Sparse Identification of Nonlinear Dynamics (SINDy) method to construct data-driven models that are not only effective but also offer insight into the underlying decision-making processes (Zolman et al., 2024).\nHierarchical reinforcement learning is another avenue that enhances interpretability. This method splits tasks, where a higher-level controller decides on a sequence of subtasks while a lower-level controller handles specific actions within each subtask. This hierarchical structure allows for a more intuitive understanding of the policy's behavior by breaking down complex decisions into simpler, more manageable parts (Barto & Mahadevan, 2003; Xu & Fekri, 2021).\nAdditionally, explainable reinforcement learning techniques focus on post-hoc methods to explain the decisions of black-box models. These include generating visualizations, using model-agnostic techniques like LIME (Ribeiro et al., 2016), Shapley values (Beechey et al., 2023), and developing algorithms that provide explanations for specific actions taken by the RL agent (Puiutta & Veith, 2020)."}, {"title": "3. Background", "content": "Environment-MineRL. VPT was trained in MineRL (Guss et al., 2019), which turns Minecraft into an environment. Minecraft (Mojang, 2011) is a 3D embodied sandbox video game known for its blocky graphics and open-world gameplay. In survival mode, the player's goal is to gather resources, craft tools, and build shelters to survive against monsters. In MineRL, each rollout starts in a new procedurally generated world, with the agent acting in it from a first-person perspective. The observations are the images of the game view. The action space is the same as what a human would use-keyboard buttons and mouse. One second of in-game time is equal to 20 time steps in the environment. The tasks and rewards can be specified manually. The most popular task has been obtaining a diamond.\nAgent-VPT. VPT is a 250 million parameter model that was pre-trained using 70,000 hours of pseudo-labeled human Minecraft play videos and fine-tuned using reinforcement learning for over 230,000 in-game hours (Baker et al., 2022). The architecture of the network is a combination of a residual convolutional neural network and a transformer. The task the agent was fine-tuned on was obtaining a diamond pickaxe-one extra step beyond obtaining diamonds.\nVPT's behavior is very consistent. At the start of an episode, it goes towards the nearest tree, chops four logs, and proceeds to craft and mine its way through the subtasks (see Figure 18). In about 80% of episodes, it gets an iron pickaxe, the item required to mine diamonds. It gets diamonds 20% of the time and the diamond pickaxe 2% of the time.\nAgent-Steve-1. Steve-1 is a Minecraft agent that extends the capabilities of VPT by introducing instruction tuning, enabling it to follow natural language commands (Lifshitz et al., 2023). It uses a richer dataset of human gameplay videos with text annotations, improving its understanding of complex behaviors. This integration of visual and textual data during training gives it the ability to understand and act upon natural language instructions. However, it can perform only short tasks that take several seconds.\nAttention mechanism. In transformer models, attention weights are computed by taking the softmax of the dot product of queries (Q) and keys (K), indicating how much focus each part of the input should receive. Attention outputs are then obtained by multiplying these weights by the values (V). This allows the model to weigh the input elements appropriately when generating representations or predictions. VPT has 4 attention layers with 16 heads in each. We will use \"head 2.3\" to refer to layer 2 attention head 3."}, {"title": "4. Attention visualization", "content": "We start our analysis by visualizing parts of the transformer attention mechanism. VPT uses a transformer architecture with a context length of 128 frames (6.4 in-game seconds). The agent takes between 3 to 10 minutes to make a diamond pickaxe from scratch when it succeeds at the task. This involves a long sequence of subtasks, such as finding and chopping a tree, crafting different pickaxes, mining stone, finding and smelting iron, and others. As a comparison, a proficient human takes on average 20 minutes to solve this task (Baker et al., 2022). How does VPT \"keep the thread\" for so long? That is, how does it know which part of the long sequential task it is currently solving? We cannot help but notice the similarity to the famous musician Clive Wearing, who was struck by an extremely strong form of amnesia, which left him with about 7 seconds of memory. Yet he could play long piano pieces (Sacks, 2007).\nIt could be that a single frame is enough to recognize what the agent has to do next. For example, if it sees a stone pickaxe in the hotbar, but no iron pickaxe, it might be looking for iron ore. However, the agent is quite robust to the items being placed in different slots of the hotbar, or even in the parts of inventory that are visible only when the inventory menu is open. This would imply a more sophisticated mechanism, likely using its memory of past frames. Given these constraints, it is likely that VPT has learned to play Minecraft in a different way than humans; something that we expect to be common among many large-scale AI agents."}, {"title": "4.1. Attention weights", "content": "Visualizing attention weights between tokens in LLMs has enabled the discovery of many interesting circuits, such as induction heads (Olsson et al., 2022a). We use a very similar technique in the sequential decision making domain in our work. The main difference comes from the short context window of VPT\u2014it does not have access to frames beyond the previous 128, while its task takes many thousands.\nThe top plot in Figure 2 shows how much attention is being paid to each of the past 128 frames (vertical axis) over the first 1,000 frames of an episode (horizontal axis) by the attention head 2 on layer 2 (4 layers, 16 heads each). The darker the color, the higher the attention weight, with 0% represented by white and 100% by black. 1,000 frames is 10-20% of an average episode in which the agent obtains a diamond pickaxe. The frames in the middle show the progression over that time-from chopping a tree to crafting a stone pickaxe. The leftmost pixel of each frame corresponds to that exact moment in time on the above plot. A diagonal line indicates that a specific frame is being attended to as the agent continues to interact with the environment.\nThe bottom part of the top plot shows a short horizontal line (above the third visualized frame). This indicates that the attention head is paying attention to the last frame it saw during the rollout. This coincides perfectly with the agent looking up to chop the logs above it. This does not happen when the agent looks down shortly after. We can also observe how attention head 2.2 operates in two modes: one of paying attention to some specific frames in the past and the second of mainly looking at the last frame. Many other attention heads show the same two-mode pattern (see Figure 27-Figure 30 in the appendix). The patterns are consistent between episodes. Also, see Figure 3 for a visualization of attention weights at a single point in time.\nTo visualize what VPT pays the most attention to more clearly, we overlaid the attention weight patterns for all 64 attention heads by taking a maximum for each frame and each memory position (see the bottom plot of Figure 2). We can see that the past 3\u20134 frames are always attended to, indicated by the thick dark line at the bottom; as well as some key-frames, seen as diagonal lines. We surmise that the recent frames help describe its immediate past, while the key-frames allow it to recognize which part of the full task it is currently doing. Some example key-frames include: a dirt block that was just destroyed (starting to dig down?), a cobblestone block that was just destroyed (dug further down?), and an achievement popup after making a stone pickaxe (can now mine iron?). These are of course speculative and require more evidence."}, {"title": "4.2. Attention outputs", "content": "In addition to visualizing attention weights (Figure 2), we can also gain insights by visualizing attention outputs. For instance, attention head 2.2 again shows a distinct pattern when the camera is moved up (see Figure 4). We discovered some of the patterns in the attention weight plots after initially noticing them in the attention output visualizations.\nBelow we list some of the patterns we found in both the attention weight and the attention output visualizations:\n\u2022 Attention head 0.8\u2014looks at the previous frame every time the inventory menu is open (Figure 27, Figure 24).\n\u2022 Attention head 0.9\u2014looks at the previous frame all the time, except for each time when the inventory menu state is changing, when it looks at the current frame (Figure 27).\n\u2022 Attention head 1.2-mostly looks at every 4th frame (Figure 28, Figure 24).\n\u2022 Attention head 2.2-looks at the current frame when the agent is looking up (Figure 27).\n\u2022 Attention head 3.3-shows a distinct pattern when the agent decides that it needs to open the crafting table menu (Figure 25).\nWhile visualizations can suggest what certain attention heads are doing, one of the main goals is to be able to control the agent's behavior in a predictable manner, which we explore in the next section."}, {"title": "5. Interventions and ablations", "content": "After visualizing the attention layers and observing the agent, we wanted to better understand the behavior and possibly change it by performing various interventions and ablations. We start with simple behavioral interventions in which we put the agent in different situations and observe how it acts. We then show how such observations are not very informative unless hundreds or even thousands of episodes are run. After this, we discuss the metrics we used for further experimentation: visual manipulations and ablations of the attention parts of the agent network."}, {"title": "5.1. Behavioral interventions", "content": "We have performed various behavioral experiments to get better intuitions about how the agent makes decisions. One should view these in the spirit of Eugene Linden (Linden, 1999; 2003)-whose books contain many stories of surprising animal behavior-as anecdotes and not as proof of some mechanism of behavior.\nFirst, we wanted to see if VPT would be tricked in a deceptive situation. We gave the agent an iron pickaxe at the start of an episode and placed it in front of a tree and a block of diamond ore (see Figure 22 in the appendix). Mining the diamond ore gives a 20 times higher reward, but chopping the tree is what the agent usually does at the start of an episode. In the first rollout, it chose the tree, while in the second it chose the diamond ore. What does that say about how the agent makes decisions?\nWe then performed an experiment to better understand how the agent decides that it is time to start making a crafting table. Does it do it after chopping trees for a while? Or maybe when seeing the top log of a tree from below? To find out, we gave it enough logs to make a crafting table at the start of an episode. The agent ran around for a bit, opened its inventory, moved the mouse to the logs, picked them up... then dropped them and moved on to find some trees to chop. That does help getting back into familiar territory, but it might not be the smartest behavior. This happened on the very first try before we started recording. We then tried to reproduce it for a video and it took over 500 episodes before it happened again (see Video05). In most of the remaining episodes, it crafted the logs and proceeded to solve the task.\nFinally, we discovered the scenario in which VPT kills a villager by using the knowledge that vision models often use spurious features to recognize objects (Izmailov et al., 2022). This also happens in biological systems, such as the Australian jewel beetle males mistaking brown beer bottles for females (Gwynne & Rentz, 1983). First, we tried finding a natural scenario, such as some villagers standing under tree leaves in a village, where VPT would attack them. This happened sometimes, but the villagers would quickly run away and the agent would lose interest in attacking them.\nWe thus forced a villager to stand in one spot by placing four invisible barrier blocks around it and some tree leaves and logs above. This also meant the agent could not move closer to the villager than the barrier blocks allowed. In this scenario, the agent punches the villager to death, which takes around 20 punches, in roughly 30% of episodes. This last example, although contrived, illustrates the necessity of a better understanding of AI agents, if we are to trust them in real-world situations."}, {"title": "5.2. Stochastic actions and metrics", "content": "In the previous section, we saw how difficult it can be to interpret agent behavior. Especially when the actions are stochastic. The agent network outputs action probabilities, which are then sampled with a default temperature parameter of 1. When the agent sees a tree right in front of it, it is very certain of what it should do\u2014stand still and attack. The attack probability in this case is often above 99.99%. However, if we present it with a less certain Y-maze type scenario of two villagers under leaves an equal distance away on both sides, it will be less certain. The camera action probabilities in such a case could be 40% left, 40% right, and 20% no change, for example. This means that each time we roll out the agent in an identical situation, we can have a different outcome.\nTo illustrate (see Figure 5), out of roughly 100 rollouts with the two villagers, roughly half the time the agent went for the left one, and the other half it went for the right one. However, there were several cases, where the agent ignored both villagers and ran past them towards some trees further away on the left side. Finally, there was a single trajectory where the agent turned around and went on its merry way in the opposite direction. This makes it hard to perform detailed ablation analysis because each ablation would require hundreds or even thousands of rollouts to see if there was any behavior change (A single rollout of VPT takes roughly 15 CPU minutes). Even then, it would not be enough to look at summary statistics to calculate the change in behavior-it might be the case that the change happens only in rare circumstances. Because of this, we use probability differences and log probability differences of actions as the main metrics for our ablation experiments. Both metrics have their strengths and weaknesses, which are described well by Heimersheim & Nanda (2024)."}, {"title": "5.3. Visual manipulation", "content": "Many successes of interpretability work in large language models have been achieved by carefully crafting the prompts and inspecting the resulting attention patterns. For example, induction heads have been identified by giving a repeated random sequence of letters to GPT-2 (Olsson et al., 2022a). Inspired by this discovery, we gave the agent modified sequences of observations and inspected the resulting attention patterns.\nFor example, when we repeat the first frame of the episode 1,000 times, the attention head 1.2 still shows the pattern of paying attention to every fourth frame (see Figure 20 in the appendix). There was no frame-skip or frame-stack of four during training, which is often used in other RL agents. This means that the phenomenon is not purely a reflection of the observations but reveals something about the agent itself.\nAnother experiment was inspired by the infamous 25th frame effect, the discredited belief that subliminal messages could be embedded into a film by inserting them into every 25th frame, influencing viewers' thoughts and behaviors without their conscious awareness. We showed the inventory screen for a single frame at the 50th time step of an episode and left the remaining observations unchanged. This caused attention head 0.1 (and some others too, but in a less pronounced way) to start focusing mainly on this frame as long as it could (see Figure 21 in the appendix). Interestingly, the resulting action probabilities were changed beyond the 128-frame window of its memory. This is because the hidden state of the transformer could keep some information from those distant frames. However, these long-range changes were minor, on the order of 0.1%.\nFor a scenario closer to reality, we tried imagining how we could intervene in the observations of an embodied agent in the real world instead of Minecraft. One idea was to shine a laser pointer at the agent's camera. Thus, we replaced 128 frames with pure red color starting at frame 150. This in effect wipes the memory of the agent. The attention patterns did not reveal much; however, the action probabilities did. Attack probability returned to nearly 100% within a few frames, but the less certain camera direction probabilities took much longer to recover. This could be simply because of the particular point in time we chose for this intervention-the agent was looking at the tree and did not need its memory to figure out what it was doing. However, this could also mean that uncertain actions are easier to influence with such interventions. We demonstrate more evidence for this hypothesis in the next subsection."}, {"title": "5.4. Attention ablations", "content": "One simple but informative experiment is to run the agent without access to its memory. We implement this by resetting the transformer's hidden state to its initial condition at every time step and letting it observe only the current frame. It barely manages to run towards a tree and painstakingly chops a single log (see Video06). It fails to craft altogether. This shows the importance of memory for VPT even when a single frame is enough to infer the next action.\nIn prior work on interpreting VPT, Joseph et al. (2023) mean-ablate each attention head one-by-one. They find that ablating attention head 0.9 results in a significant logit difference for attack versus non-attack actions. Despite the logit difference, ablating the identified head does not impact the agent's performance in terms of task completion.\nTo see if we can find a more granular way of influencing agent actions we ran the following experiment. We recorded a 15-second (300 frames) episode where the agent runs towards the villager-tree, punches the villager for about 3 seconds, and then looks up to chop the logs instead. We record the action probabilities for each frame. They stay constant if we show the same sequence of frames to the agent again, making the experiment fully reproducible. We start with the first frame. We take the first of the 128 outputs of attention head 0 in layer 0 and zero-ablate it, i.e. set the activation value to zero. We did not notice large differences between zero and mean ablations, so we chose zero ablations for simplicity. We record the resulting modified action probabilities, set the output back to the previous value, and move to the second output of the same attention head. We do this for each output in each attention head in each layer for a total of 128*16*4=8,192 times. We do this for each frame. This lets us know which outputs have a higher influence on the resulting agent actions. We visualize the heatmap for the 20th frame for the 16 attention heads in layer 0 in Figure 6. We can see that the most impactful outputs in layer 0 belong to head 0.9, which coincides with the findings by Joseph et al. (2023).\nWe then compare the maximum ablation impact for each frame to the unablated action probabilities (see Figure 7). The only times when we can produce a meaningful impact using this method is when the agent is uncertain about its actions. When the probability of attack is near 0% or 100%, such ablations do nothing. This is not simply the effect of probability scaling exponentially with the logits-we observe an almost identical phenomenon if we replace the probability differences with log probability differences (see Figure 23 in the appendix). We chose to display raw probabilities for clarity, but there are also good reasons to use other metrics instead (Heimersheim & Nanda, 2024)."}, {"title": "6. Discussion and limitations", "content": "There is an obvious limitation to our work. We are like a behavioral biology lab with a single rat. What claims can we make about any agent other than VPT? In fact, would any of our analyses transfer beyond the single checkpoint we analyzed? We briefly experimented with STEVE-1 (Lifshitz et al., 2023) and easily succeeded in making it chop village houses, so this type of behavior might be common (see Figure 26). However, we know from prior work (Sellam et al., 2021; Chughtai et al., 2023; D'Amour et al., 2022) that factors such as the random seed used for weight initialization can have a large influence on how models learn to solve specific tasks. This is the case even when fine-tuning models instead of full retraining. It shows up as similar in-distribution performance but differing behavior in out-of-distribution situations. Situations such as the villager standing under some leaves. It is likely that if VPT was retrained many times with only the random seed used for weight initialization differing between runs, the resulting agents could exhibit different behavior when presented with the villager-tree.\nWould that make our analysis useless? The goal of this work was to show that it is possible to use existing interpretability techniques to discover insights into the behavior of a complex vision-based agent with a transformer component. It also provides ideas for how other similar agents might make decisions-ideas that can be tested.\nHowever, it does raise an additional consideration\u2014if the internal mechanisms of an agent can change unpredictably during training, our method of manually applying these techniques would not scale well. We would then need interpretability techniques that can be easily and automatically applied after each significant change of a deployed model.\nAnother limitation of our work is that we considered only single attention head output ablations. Much success in mechanistic interpretability of LLMs happened through discovering circuits by ablating the connections between attention heads instead (Wang et al., 2022). We believe this points to a fruitful future research direction."}, {"title": "7. Conclusion", "content": "This study advances our understanding of decision-making in VPT, a large vision-based reinforcement learning agent. By applying interpretability techniques, we have provided clues on how the agent manages complex tasks with limited memory, focusing on recent and key-frames to maintain task coherence. Our findings include a new example of goal misgeneralization in the wild, where the agent mistakenly identified a villager as a tree trunk and punched it to death, pointing to the urgent need for better interpretability and error correction in AI systems. This research underscores the critical role of interpretability in ensuring the safety and transparency of AI models."}, {"title": "A. Saliency maps experiments", "content": "Although saliency maps are known to have many issues, they do serve as an additional test to validate some of our previous hypotheses. We implement Gradient (Simonyan et al., 2014) and SmoothGrad (Smilkov et al., 2017) saliency maps. When applied to VPT, both Gradient, and SmoothGrad pass the sanity checks specified in Adebayo et al. (2018): the model parameter randomization test and the data randomization test.\nThe saliency maps of VPT show that when the agent is chopping logs, it focuses on the tree (see Figure 8). From afar, VPT appears to look more at the foliage of the tree than at the tree trunk itself. When VPT is confronted with the tree vs villager-tree dilemma, slightly more focus is paid to the real tree than to the villager-tree. The saliency map of the inventory screen can be seen in Figure 9."}, {"title": "B. CNN Feature visualization", "content": "Our study leverages filter visualization by optimization to elucidate the specific patterns and features that various layers of the VPT model detect. The goal of this optimization is to generate an image that maximizes the activation of a specific kernel map. The loss function is the output of that kernel map. In Figure 10, we show the visualizations of the fifth CNN layer's filters. These visualizations reveal that filters at this depth in the network capture more complex textures and patterns compared to initial layers (see Figure 11).\nFigure 11 provides filter visualizations for the first CNN layer. These filters detect basic features such as edges, simple textures, and colors. The simplicity of these features aligns with the layer's role in capturing foundational visual elements, which are progressively combined and abstracted in the deeper layers to form more complex representations."}, {"title": "B.2. Receptive Field Attention", "content": "Figure 12 shows the receptive field attention of layer 6 mapped onto an input image. This mapping shows how the activations from this layer correspond to specific regions in the input image, highlighting areas of interest that the model focuses on during gameplay.\nThe receptive field $R$ of a convolutional layer refers to the region in the input image that affects a particular feature in the output. It can be calculated using the kernel size $K$, the stride $S$, and the padding $P$. The basic formula for a single layer is straightforward:\n$R=K$\nFor deeper layers, the calculation becomes recursive. Let's denote:\n\u2022 $R_L$ as the receptive field of layer $L$\n\u2022 $K_L$ as the kernel size of layer $L$\n\u2022 $S_L$ as the stride of layer $L$\n\u2022 $P_L$ as the padding of layer $L$\nThe receptive field of a layer $L$ depends on the receptive field of the previous layer $R_{L-1}$ and can be calculated as:\n$R_L = R_{L-1} + (K_L - 1) \\times \\prod_{i=1}^L S_i$\nWhere $\\prod_{i=1}^L S_i$ is the product of all the strides from layer 1 to layer $L$.\nThis recursive method allows us to determine the receptive field of any layer in a deep convolutional network by considering the kernel size, stride, and padding of all preceding layers. The mapping of activations to specific regions in the input image (like gameplay frames) helps in understanding which parts of the input the model focuses on. For instance, the activations in a higher layer of the network will correspond to larger regions of the input image, indicating broader, more abstract features such as objects or actions within the game. In Figure 13, we present heatmaps of the receptive fields associated with the top 500 activations of the sixth layer."}, {"title": "B.3. Kernel Visualization", "content": "The kernel visualization for the fifth layer, shown in Figure 14, demonstrates how the model interprets an image containing a villager and a tree. For this figure, we combined the outputs of multiple kernels and applied PCA to reduce the dimensionality to three principal components. This allowed us to visualize complex, high-dimensional features in an RGB format, highlighting how different components contribute to the learned representations. This technique revealed the intricate patterns learned by the network and how different kernels collaborate to encode features. Interestingly, the visualization indicates that the model does not distinctly separate the villager from the tree, suggesting a limitation in the model's ability to discern boundaries between overlapping objects."}, {"title": "B.4. Filters Overlay", "content": "Figure 15 showcases the overlay of filter activations, providing a comprehensive view of how different filters respond to the same input image. Here's how we generate these filter overlays\n\u2022 Pass an image through the network: the input image is fed through the CNN, and the activations of a specific layer are recorded.\n\u2022 Extract filter activations: For the chosen layer, the activations of each filter are extracted. These activations are essentially the feature maps generated by each filter when applied to the input image.\n\u2022 Overlay the activations on the input image: Each filter's activation map is overlaid on top of the input image. This can be done by upscaling the activation maps to the original image size and blending them with the image using different visualization techniques like heatmaps or transparency overlays (we used transparency overlays).\nThis overlay helps understand the cumulative effect of multiple filters working together, contributing to the overall feature extraction and decision-making process in the VPT model."}, {"title": "B.5. Layer-Specific Activations", "content": "The kernel visualization of all 256 kernels for an inventory image in the sixth layer, depicted in Figure 16, emphasizes how the model processes and prioritizes different parts of the inventory interface. The activations suggest that the model pays particular attention to areas with high informational content, such as item slots and inventory shapes (filters 15, 35, 39, 142, 143)."}, {"title": "C. Representation engineering", "content": "Zou et al. (2023) introduced the idea of running forward passes through a model with a specific concept in mind, then saving the activations of some layer in the model for both the positive and negative versions of that idea. For example, if we want VPT to be more likely to attack, we can find a way to add theactivations of the positive concept of a tree and subtract the activations of the negative concept of an empty field. Then, modify the model so that during inference, it adds the positive version of a concept and subtracts the negative version of that concept from the same layer. The model's outputs tend to be steered in the direction of your concept. This technique has been used to steer the behavior of GPT-2 (Zou et al., 2023) and to control a maze-solving RL agent Mini et al. (2023). We apply this to VPT with some limited success.\nWe modified the VPT model by targeting the first MLP layer in the first transformer block after the CNN. We added 3x the first MLP layer's activations when VPT was in front of a tree, and subtracted 3x the first MLP layer's activations when VPT was in a field with no trees, i.e. +3*(tree image - field image) (see Figure 17). The modified VPT kept punching at thin air even if there were no trees around (see Video10). However, this change caused VPT to lose its ability to achieve goals it was previously capable of accomplishing.\nConversely, when we changed the sign of the activation engineering, i.e. -3*(tree image - field image), the modified VPT did not punch a tree that was right in front of it (see Video11).\nAs found in Turner et al. (2024), the scalar value with which to multiply a layer's activations before adding/subtracting them from the model's forward pass is mostly guess and check. We tried scalar values between 1 and 10, and of these, 3 worked the best. We also find that this technique breaks if the magnitudes of the scalars multiplying the positive/negative activation vectors are not equal.\nA more fine-grained approach such as Templeton et al. (2024) could potentially be useful in attempting to not \"break\" the model when trying to steer it towards certain kinds of actions."}, {"title": "D. Other", "content": "Below we show more figures that were not included in the main body due to space constraints."}, {"title": "E. Videos", "content": "A list of videos and their descriptions:\n\u2022\n\u2022 https://youtu.be/g-jd6OyOcUs (Video01) Teaser.\n\u2022 https://youtu.be/BeqSthHRyLA (Video02) Head 2.2 attention weights.\n\u2022 https://youtu.be/3GhhEysmSY4 (Video03) Frames with highest weights at each time step for each attention head.\n\u2022 https://youtu.be/TbTBWdb6jSo (Video04) Head 2.2 attention outputs.\nhttps://youtu.be/e5qWNVEtuDA (Video05) VPT drops the four logs it was given at the start.\n\u2022 https://youtu.be/0-bxLngYO1Y (Video06) VPT with memory removed.\n\u2022 https://youtu.be/ju-s301cHzI (Video07) Single output ablation impact.\n\u2022 https://youtu.be/U8NYiudY5n8 (Video08) If attack probability below 99%, don't attack.\n\u2022 https://youtu.be/cCRGOTRZQ8U (Video09) Saliency maps.\n\u2022\nhttps://youtu.be/i4RbOqFDlKc (Video10) Representation engineering. +3*(tree minus empty field).\n\u2022 https://youtu.be/8NcUdqmCY4k (Video11) Representation engineering. -3*(tree minus empty field).\n\u2022 https://youtu.be/VVkWWgwKf0M (Video12) MineRL VillagerChop-v0.\n\u2022 https://youtu.be/uxghPuxh_0I (Video13) Head 3.3 attention outputs.\n\u2022 https://youtu.be/VQhP3h9nxqo (Video14) STEVE-1 in a village, chopping \"trees\"."}]}