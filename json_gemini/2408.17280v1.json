{"title": "Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts", "authors": ["Rhui Dih Lee", "Laura Wynter", "Raghu Kiran Ganti"], "abstract": "We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE) from trained models. The toolkit can be used for creating a mixture from models or from adapters. We perform extensive tests and offer guidance on defining the architecture of the resulting MOE using the toolkit. A public repository is available at https://github.ibm.com/Rhui-Dih-Lee/moetify.", "sections": [{"title": "Introduction", "content": "Mixture of Experts (MOE) models, like Mixtral, have been shown to perform very well, often better, than larger, dense models like LLaMa-70b [1, 2, 3]. In addition, MOE models activate fewer parameters for each token than dense models, and hence can offer faster inference response times. These two attributes have contributed in making MOE models very popular. In fact, while its architecture is not known publicly, GPT-4 [4] is rumoured to be a sparsely-activated (e.g. 2-activated experts per-token) MOE model, where that design choice, if accurate, may have been made to get a larger capacity with smaller inference cost due to the high number of inference calls that GPT-4 sees.\nGiven the nature of an MOE model, composed of several \"experts,\" it is tempting to cast the expert sub-models as each having an application domain expertise. In the well-known MOE models, though, this is not the case: the \"experts\" are not specialised during training. The architecture of an MOE is designed so that a gating function, also known as a router, decides which experts handle each token. However, the routers and the expert modules are trained simultaneously. While it is in theory possible to modify the training of an MOE model so that the MOE expert modules comprise real \u201cexperts\u201d, the training process would be lengthy and difficult, and may not deliver the desired outcome. The creation of MOE models can, however, be achieved at nearly-zero cost by mixing multiple trained, and fine-tuned, models. Furthermore, by mixing multiple trained models into an MOE, one can select the trained models so that they represent domain experts.\nWe wish to enable rapid and low-cost MOE model creation to augment the capabilities of a given source model of interest. One needs only to select additional models with the same architecture as the source model as experts, and then combine the trained expert models with the source model of interest into an MOE. By selecting domain-specialised, trained models of interest to augment the capabilities of the source model, the resulting MOE model can deliver the promise of a true Mixture of Domain Experts.\nWe provide a general-purpose toolkit for using trained models in a Mixture of Domain Experts MOE with a focus on the flexibility offered. While a router, or gate, can be trained on a small amount of relevant data to improve the performance of the Mixture of Domain Experts MOE, we find that it is not always necessary. Hence our toolkit offers the flexibility to create a Mixture of Domain Experts MOE in multiple ways including without router training. When there are only a few high-quality experts, our Gate-less MOE architecture can be the best solution. We find that the Gate-less architecture is competitive with and can even outperform router-based architectures yet is cheaper to produce. To reduce the inference cost of the Gate-less MOE when the number of expert modules increases, we also propose a Noisy MOE that performs nearly as well as the Gate-less MOE, does not require training, and offers a lower inference cost. In addition, our toolkit offers the capability to train the router or train a combination of the router and the embedding layers. We also offer the possibility to create the MOE from trained LoRA adapters.\nWe cover in Section 2 the related work including other libraries offering a similar functionality. In Section 3, we describe the methodology proposed and implemented in our open source repository. Section 4 provides detailed experimental results using the proposed methodology. In particular, we justify the development of the Gate-less and the Noisy MOE and contrast it with"}, {"title": "Related work", "content": "Evidence, such as in [5], shows that models specialised, through fine-tuning, to a particular domain outperform generalist models on their domains of interest. In cases where an MOE model comprises multiple domain-specialised expert models, it was shown in [6] that a mixture-of-multiple-experts model can outperform their respective source expert models.\nTaking it one step further, we can imagine a set of trained models, each having a skill in a particular domain. Each MOE can mix a targeted subset of skill-based models to satisfy the distinct needs of each individual user. Given the exceptionally low cost of creating these mixed MOE models, they can be customised rapidly on demand, for each use, with only the skills of interest.\nThere have been a few other efforts to enable mixture of experts model creation from trained models, the first of which is due to Charles Goddard [7, 8] who created the Mergekit repository. The recommendation provided there is to set the router weights from the hidden states in the FFN of each expert obtained when running each expert on a set of targeted prompts. Specifically, the author suggests compiling of a list of positive and negative prompts and then using a provided script which combines their hidden state representations by averaging and normalizing them, for each expert.\nThe Mergekit library was used to create a series of MOE models docu-mented in a Hugging Face blog article [9] which includes numerical results with the resulting MOE models. While the MOE performs well, it does not, on the vast majority of the tests used for evaluation, perform better than its expert models. In particular, the MOE performs worse than the model used as the base and as one of the experts on most of the tasks used to evaluate it. However, the same author posted an earlier mixed MOE that did outperform its constituent models [10] though it was not included in the blog article [9]. Later, a similar library [11] was created by a different team, however, no experimental results were provided to demonstrate if or how well the resulting model works.\nThe authors of [12] propose a similar approach but require the experts to be LoRA adapters and the use of a single linear-layer router shared across all of the LORA layers. Code is not provided. We note that a LoRA-adapter based architecture can be achieved with our methods and toolkit, along with further flexibility that we provide in the definition of the experts and the"}, {"title": "Augmenting an LLM with other expert LLMs", "content": "We are interested in augmenting the capabilities of a large language model to improve its performance on multiple, related domains, and to do so at a low computational cost. When one has available pre-trained, fine-tuned domain expert models, as is the case on the Hugging Face Model Hub[15], augmenting a given model to address multiple, related domains becomes an appealing and feasible task.\nOur MOE Model Mixing toolkit swaps the FFN layers of each expert model, along with a gate, in place of the FFN layers of a base model. See Figure 1. In [8], it was suggested to set the router parameters as the hidden state representations for each expert. Specifically, it was suggested that passing a set of positive and negative prompts through each expert model, and then averaging and normalising the hidden states thus obtained, the resulting values can be used as the router weights of each expert. We found that using this type of hidden representation in the gate does not work well.\nThe authors of [11] assume that users of a mixed MOE will fully fine-tune the resulting MOE. This, we found, is not needed. We suspect that fine-tuning"}, {"title": "Experimental results", "content": "We wish to extend the capabilities of a model using additional pre-trained and tuned expert models. First, we demonstrate this using the InstructLab-enhanced Merlinite model released by IBM instructlab/merlinite-7b-lab [16] as our base model as well as one of our experts. We make use of the following additional models as experts, sourced from Hugging Face Model Hub [15]: openchat/openchat-3.5-1210 [17], NousResearch/Hermes-2-Pro-Mistral-7B [18], and meta-math/ MetaMath-Mistral-7B [19]. All three additional expert models have permissible usage licenses and, based on their performance, should enable merlinite to perform better in math and general language capabilities. Later, in Section 4.3, we perform ablation studies on llama3-8B models and also compare against a low-cost adapter-based MOE, including an approach similar to that of [12].\nOur evaluations are performed using TruthfulQA [20]; AGIEval [21]; GPT4ALL [22], comprising piqa, openbookqa, boolq, arceasy, arcchallenge, winogrande, and hellaswag; MMLU 5-shot [23]; BBH-few shot [24]; the math domain, comprising MathQA [25] and GSM8K 5-shot [26]; and the Finance domain [27], comprising convfinqa, finheadline, fiqa, and fpb.\nWhen the routers are trained, the hyperparameters used were set to num_train_epochs = 1, per_device_train_batch_size = 1, gradient \u0430\u0441-cumulation_steps = 16, learning_rate=1e-04, and lr_scheduler_type as constant. For router training, instruction tuning is performed using a dataset combining 50K examples each from metamath [19], pubmedQA [28] and FLAN [29]. When performing extended pre-training of the routers, we com-bine 50K examples each from wikidump [30] and open-web-math [31]. The 2x MOE in the next section mixes Merlinite with MetaMath.", "sections": [{"title": "Low-cost MOE creation is a viable approach", "content": "The main results using the Merlinite model are provided in Figure 2. The key observation is that low-cost creation of an MOE from trained expert models is a viable approach to improving the performance of a model in a cost-effective manner. The first four bars from the left in each section of the"}, {"title": "Router training can be beneficial but is not required", "content": "We consider several paradigms for training the router, including extended pre-training, instruct-tuning of the router and instruct-tuning of both the router and o-projection layers. We find that the decrease in loss is moderate during router training, implying that the ability of the router to learn is somewhat limited. We conjecture that the capacity of the gate can be insufficient to learn a complex routing policy with a small or moderate amount of data. Furthermore, we observe that, in many cases, router training is simply not necessary to achieve good performance from the mixed MO\u0395.\nWe train the routers of the 2X MOE and the 4X MOE and examine the loss curves from the training. We also compare the results of the evaluation tests across these training paradigms. See Figures 3 and 4. While the loss appears to decrease more when instruct-tuning both routers and embedding layers, the results are not borne out in evaluation, shown on the right in"}, {"title": "Ablation study on llama3-8B", "content": "We examine several variants of the methodology on a different model, llama3-8B. The experimental setup enables a comparison with LoRA adapter-based experts as well as numerous choices for the router. The Self-MOE approach of [12] is similar to but not the same as that tested here as we add a router to each FFN layer of the base model, while Self-MOE uses a single global router. In that reference, the base models, not the instruct-tuned models, are used for the MOE base as well as for the experts which are subsequently fine-tuned. For that reason, we do the same, but we evaluate the instruct-tuned base model as a point of comparison.\nSpecifically, we wish to answer the following questions:\n\u2022 What is the impact of the MOE base model? We thus evaluate a FLAN-trained, a medical-domain-trained, and a math-trained llama3-8B as the MOE base.\n\u2022 How should expert models be included in the MOE? We assess using the full FFN layers, the FFN and attention layers, a fine-grained variant (called fgmlp, for 'fine-grained MLP') in which a there are three routers for each FFN layer (one each for gate, up and down), and using LoRA adapters as each expert.\n\u2022 Should one train after creating the MOE? We compare the our Gate-less and Noisy MOE which do not undergo training to training the router and training the router and embedding layers using llama3-8B models.\n\u2022 How does the MOE perform compared to the most relevant baselines: the base model, the instruct-tuned base model, and the expert models alone?\nThe hyperparameters are as follows. The expert models were trained using num_train_epochs = 3, per_device_ train_batch_size = 8, gradient_accumulation_steps = 2, learning_rate = 1e-06, and lr_scheduler_type as constant. When the routers are trained, as before, the hyperparameters used were set to num_train_epochs = 1, per_device_train_batch_size = 1, gradient_accumulation _steps = 16, learning_rate=1e-04, and lr_scheduler_type as constant. The three experts were obtained by fine-tuning the llama3-8B base model on 50K examples each from FLAN, metamath, and pubmedqa (pubmedqa training data only). When training is performed on the routers (or routers and embedding layers), in all cases the dataset used for training combines the same three datasets used metamath, FLAN, pubmedqa (pubmedqa training data only) in equal proportions of the same 50K examples each that were used to train the experts. The evaluations are performed on the same tests as above along with medical domain tests from pubmedqa [28] and multimedqa [32].\nMOE base model has an impact on MOE performance quality The base model used for the MOE has a noticeable impact, as can be seen from bars 4-6 (dark-blue, green and red) in Figure 5. The MOE with a math-trained base performs the best on the GSM8K math test and the MOE with a medical-trained base performs best on the medical tests. Performance is comparable on the general knowledge and reasoning tests. For GSM8K-COT, possibly due to the importance of the question-answer format, the FLAN-instruct-trained base performs better than the MOE with the math-trained base.\nFFN mixing is best overall but LoRA adapter MOE mixing is competitive The choice as to which modules to mix into an MOE can be seen to be application and expert-model-dependent. However, overall FFN mixing is a better choice than mixing LoRA adapters into an MOE, as evidenced by comparing the 9 bars from bars 4-12 (dark blue to light purple) with the last two bars representing LoRA-adapter experts, (blue and cyan)."}]}, {"title": "Conclusions", "content": "We propose low-cost creation of an MOE from a given source model by mixing it with other expert models having the same architecture. We show that the best results are obtained when the source model is completed by well-performing models and that a Gate-free policy, which is the cheapest option to create, is often the best or at least highly competitive with the best approach. A noisy gate can be used to reduce inference cost as compared to the Gate-free MOE, still not requiring any training, with generally only minor performance degradation. Both of these model mixing procedures allow for swapping in and out of expert models into an MOE at practically zero cost. We also offer the possibility to train the routers and examine the benefits that router training provides. As expected, results vary according to the base and expert models employed and datasets used. For that reason, the toolkit we provide the capability to use Gate-free, Noisy MOE, or router-training, and offer both FFN-based expert mixing as well as LoRA-adapter-based expert"}]}