{"title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems", "authors": ["Ibrahim Alabdulmohsin", "Xiaohua Zhai"], "abstract": "Recent research in language modeling reveals two scaling effects: the well-known improvement from increased training compute, and a lesser-known boost from applying more sophisticated or computationally intensive inference methods. Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time. For a given fixed model architecture and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. These advantages are maintained even when compared to state-of-the-art recursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM. Finally, stochastic RINS not only can enhance performance further but also provides the flexibility to optionally forgo increased inference computation at test time with minimal performance degradation.", "sections": [{"title": "1. Introduction", "content": "There has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically (Mukherjee et al., 2003; Hestness et al., 2017; Johnson et al., 2018; Rosenfeld et al., 2019; Kaplan et al., 2020; Ghorbani et al., 2021; Alabdulmohsin et al., 2022; Bansal et al., 2022; Zhai et al., 2022). Generally, the performance of deep neural networks f(x) (such as its error rate or log-perplexity) often follows a power law \\(f (x) \\sim \\beta x^{-c} + \\varepsilon\\) as one varies a dimension of interest x, such as the size of the training data or model parameters. These \u201cscaling laws,\" as they are known in the literature, have been used, among others, to determine the training data size needed to achieve a specified level of accuracy (Cho et al., 2015; Beleites et al., 2013; Figueroa et al., 2012) and to optimize the model architecture (Kaplan et al., 2020; Hoffmann et al., 2022; Alabdulmohsin et al., 2024b). They have also been justified theoretically (Bahri et al., 2021; Hutter, 2021; Sharma & Kaplan, 2022).\nBesides this conventional approach of scaling the training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. Whereas OpenAI's GPT-03 has prominently demonstrated this recently (OpenAI, 2024), earlier work had already explored this direction. For example, chain-of-thought (CoT) prompting has shown that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of language models (Wei et al., 2024). This is similar to the demonstrated success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference in order to enhance code generation performance. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such as mathematics and coding-when measured by coverage-holds for up to four orders of magnitude of inference compute. These approaches suggest that, like training compute, inference compute follows systematic scaling patterns that can be leveraged to improve models. Refer to the survey (Welleck et al., 2024) for more details.\nRecently, it has also been noted that language exhibits a \u201cself-similar\" (fractal) nature, meaning that similar patterns repeat across different scales of its representation, from individual words to entire sentences and paragraphs (Alabdulmohsin et al., 2024a). Inspired by this finding, we examine if model recursion, which can be interpreted as a particular form of scale-invariant decoding, offers a complementary approach for scaling inference time in language models. To this end, we examine an extensive set of parameter-sharing strategies and, indeed, identify the best to be a special form of recursion, which we term Recursive INference Scaling (RINS). We show that RINS yields significant performance"}, {"title": "2. Recursive Inference Scaling", "content": "RINS builds upon the concept of model recursion but recasts it as a powerful inference-time scaling strategy. It leverages a simple yet profound idea: use your existing architecture and training compute budget as is, but exploit the self-similar structure of language by recursively applying an early portion of your network to refine its output. In turn, this simple strategy improves performance significantly, with the primary cost being a scaled inference time.\nRecursion has shown promise in language modeling, with recent work by Liu et al. (2024) demonstrating that recursive architectures outperform similarly sized vanilla models trained on the same number of tokens. Notably, Liu et al. (2024) finds that a simple \"repeat-all-over\u201d (RAO) approach, where the entire model is recursively applied, surpasses more complex parameter-sharing strategies in language.\nHowever, while Liu et al. (2024) demonstrate the sample efficiency of recursive architectures, in which models are compared when trained on the same number of tokens, their analysis does not explicitly account for the increased computational cost of recursive operations during training. Hence, it remains unclear whether the performance gains observed in prior work come from the inherent advantages of model recursion or simply from having increased the training compute. Indeed, our findings suggest that for moderately sized models (over 1 billion parameters), the performance gains of the \"repeat-all-over\u201d (RAO) strategy can be matched by extending the training of a standard model to consume an equivalent compute. RINS, by contrast, significantly outperforms all other baselines on a compute- and parameter-matched setup, including when scaling inference by increasing the context length (see Figure 2).\nCrucially, a stochastic variant of RINS, in which the number of recursion rounds is sampled at training time from a binomial distribution, not only can enhance performance further, such as in multimodal systems, but also provides the flexibility to optionally forgo increased inference computation at test time with minimal performance degradation. See Section 4 for details.\nEfficiency. We conduct our experiments mostly on small models, which are typically intended for deployment environments with stringent memory limitations (Liu et al., 2024). Given the direct relation between a model's memory footprint and its parameter count (e.g. a 1 billion parameter model with 16-bit floating-point precision requires 2GB of DRAM), the ability to enhance accuracy while maintaining a fixed parameter count is highly desirable. RINS achieves this by enabling significant performance gains without increasing parameter count for the same training compute."}, {"title": "2.1. Overview", "content": "Before describing how RINS works, we briefly formalize definitions. Let X be a fixed domain, often the space of sequences of soft tokens of embedding dimension d. Let L = {l1, l2, ..., ln} be a fixed set of n unique blocks, where each block lk: X \u2192 X is a function mapping from the input space X to the same output space X. By \u201cunique\u201d here we simply imply that such blocks (which typically comprise of multiple layers each) are not constrained to share the same parameters. Let G(L) be the space of all possible computation graphs representing neural network architectures that can be constructed by composing blocks from the set IL, while f \u2208 G(L) be one specific architecture.\nFormally, let C(f) be the actual computational cost (in FLOPs) of training f \u2208 G(L) on a dataset sampled i.i.d. from distribution D, considering only the forward pass. Also, L(f) is a performance metric of interest (e.g., validation loss or error rate) for model f. We assume that a lower value of L indicates better performance.\nDefinition 2.1. For a fixed set of unique blocks L and a training compute budget c, a recursive architecture f* \u2208 G(L) is considered \u201cbetter\u201d than another architecture f \u2208 G(L) if C(f*) \u2264 C(f) \u2264 c and E[L(f*)] < E[L(f)].\nIn other words, we search for the architecture f*, constructed only from the set of blocks L, that minimizes the loss under the constraint of a bounded training compute c.\nModel recursion offers a simple, plug-in approach for scaling inference time. By applying some layers iteratively to their own output, we effectively increase the computational path length during inference without altering the underlying model architecture. This allows us to exploit the benefits of increased inference compute. Importantly, it is complementary to other techniques like chain-of-thought (CoT) prompting (Wei et al., 2024), which achieve performance gains by eliciting longer generation sequences, and repeated sampling (Li et al., 2022; Chen et al., 2021)."}, {"title": "Taxonomy", "content": "As discussed in Section 1, language has been shown to exhibit self-similarity, meaning that similar patterns repeat across multiple levels of granularity, from the structure of individual sentences to the composition of entire texts (Alabdulmohsin et al., 2024a). This observation suggests that recursive (scale-invariant) decoding could be particularly beneficial for language processing.\nTo determine the extent to which this is true, we systematically explore a vast space of parameter sharing strategies. We use the following taxonomy based on two key concepts: (1) signature and (2) degree. The \"signature\" of an architecture describes how parameter sharing is applied across different blocks of the model. For example, an architecture with the signature AB2C indicates that the model is partitioned into three unique blocks (A, B, and C) of equal size. The processing flow in this architecture would be: apply block A on the input, apply block B to A's output, apply block B again to its own output (hence the exponent), and finally apply C to the output of B. Note that while the signature AB2C has the same parameter count as a simpler ABC architecture, it incurs approximately 33% more compute due to the repeated application of block B. This highlights the trade-off between computational cost and performance using recursion that we will carefully study in this work.\nOn the other hand, \u201cdegree\u201d specifies whether recursion is applied only at the level of entire blocks, or if it extends to individual components within blocks, as illustrated in Figure 1 (bottom). A degree of 2, for example, means that each of the blocks A, B, and C in our example have themselves the same recursive pattern used in the overall model. This adds another dimension to our taxonomy, allowing for a finer-grained classification of recursive architectures. Note that degree makes notation concise, but is not necessary; e.g. (ABB)2 is equivalent to ABB CDD CDD. By systematically varying both signature and degree, we can comprehensively explore the space of recursive models and identify optimal configurations. Figure 3 provides a detailed pseudocode.\nWith this framework, we now state our main investigation:\nWhich family of architectures (i.e. signatures and degrees) lead to better performance according to Definition 2.1 under fixed compute budget?\nTo reiterate, this is a non-trivial question because it is conceivable for a non-recursive model to outperform all others given that it sees more training examples, utilizing the compute budget without resorting to recursion. For instance, increasing the context length in language modeling can be inferior to training on more examples within a limited compute budget. We present examples of this in Section 3.\nNevertheless, our analysis reveals that Recursive INference Scaling (RINS) emerges as a clear winner. Its consistent"}, {"title": "Definition 2.2.", "content": "RINS corresponds to architectures with degree 1 and signature AB, for some integer r > 1.\nIn other words, RINS partitions a given model into two equally-sized blocks A and B. For example, A would correspond to the first 6 layers while B would correspond to the second 6 layers in a 12-layer neural network (ignoring the embedding layer and classifier head). The first block A is recursively applied on its own output r times before passing the output to B. See Figure 1 for illustration and Figure 3 for a detailed pseudocode. In Section 6, we show how the optimal value of r depends on the allocated compute budget."}, {"title": "Main Claim.", "content": "Our claim can be summarized as follows. Once a language model is chosen and the training compute budget is planned, one should enable RINS, which does not change the model size, and train the new RINS-enabled model for the same amount of compute. This is achieved by partitioning the original model into two equally-sized blocks and applying recursion to the first block alone. Thus, neither model size nor training compute is affected. Our empirical results demonstrate that RINS-enabled models will consistently outperform baseline models. In addition, stochastic RINS with some positive skip probability ps > 0 (see Section 4) offers the option of forgoing scaling inference at test time with little performance degradation."}, {"title": "3. Experimental Results", "content": "In this section, we study the impact of various parameter-sharing strategies in language modeling, following the taxonomy introduced in Section 2. We show how RINS emerges as a clear winner. All experiments are carried out using the Big Vision codebase (Beyer et al., 2022).\nSetup. First, we train a decoder-only transformer language model (Vaswani et al., 2017) with relative position embedding"}, {"title": "4. Stochastic Recursive Inference Scaling", "content": "Next, we investigate the effect of stochastically dropping blocks during training, inspired by the regularization technique of stochastic depth (Huang et al., 2016). Our primary goal is to determine whether this approach can further enhance the performance of RINS while simultaneously offering the flexibility of reverting to non-recursive inference without significant degradation in model quality.\nTo recall, RINS has the signature AB for some r > 1. To implement stochastic RINS, we introduce a skip probability ps \u2208 [0,1) and sample during training the number of recursion rounds in each step to be 1 + \\(\\eta\\), where \\(\\eta\\) is a binomial random variable with probability of success 1 ps and number of trials r \u2013 1. Thus, block A is always executed at least once. During inference, we are free to choose how to scale inference compute by setting the value of r > 1. See the detailed pseudocode in Figure 3.\nHere, we train bigger models with 1 billion parameters. We use an embedding dimension 2,048 and MLP dimension 8,192. All models have 18 decoder blocks. We train for 500K steps and compare RINS with signature A\u00b3B against the non-recursive baseline.\nFigure 5 summarizes the advantages of stochastic RINS. Notably, we observe that as ps > 0 increases, stochastic RINS mitigates the performance degradation incurred when scaling is not applied at inference time, while still offering big gains when inference time is scaled. Not surprisingly, though, scaling inference time is less effective when ps increases, suggesting a tradeoff between flexibility at inference time and the magnitude of potential gains from scaling. As shown in Figure 6, similar conclusions hold even in the asymptotic (infinite-compute) regime, under the assumption that loss follows a power law relation (Kaplan et al., 2020)."}, {"title": "5. Other Modalities", "content": "As previously discussed, the performance gains in RINS are consistent with the self-similar nature of language. By performing a recursive, scale-invariant decoding, RINS introduces an inductive bias that encourages the model to"}, {"title": "5.1. Vision", "content": "when trained on longer sequence lengths (i.e., higher image resolution) to match the inference cost of recursive architectures, surpasses all other methods. This starkly differs from the results observed in language modeling, where RINS provides significant gains even when compared against models that scale inference by increasing the sequence length."}, {"title": "5.2. Contrastive Mutlimodal Systems", "content": "Finally, we also study the impact of RINS in vision-language pretraining, motivated by the fact that such models also process natural language. We pretrain SigLIP-B/16 models (Zhai et al., 2023), which are contrastive models trained using the sigmoid loss on English-language image-text pairs. We follow (Zhai et al., 2023) in most aspects. Images are resized to 256 \u00d7 256 and we use 16 \u00d7 16 patch sizes. Texts, on the other hand, are tokenized using C4 tokenizer (Raffel et al., 2020) with a vocabulary size of 32K, and we keep a maximum of 64 tokens. The optimizer is Adam with learning rate 10-3 and weight decay 10-4, using an inverse square root decay schedule with 5K warm-up and cool-down steps. For the baseline, we use"}, {"title": "6. Further Analysis", "content": "Next, we investigate the influence of Recursive INference Scaling (RINS) on the data scaling behavior of language"}, {"title": "6.1. Data scaling laws", "content": "models. Specifically, we fit a power law of the form \\(\\varepsilon(x) = \\beta x^{-c} + \\varepsilon_{\\infty}\\), to the log-perplexity loss \\(\\varepsilon(x)\\) as a function of the training FLOPs x. This allows us to analyze the impact of RINS on both the scaling exponent c and the asymptotic performance limit \\(\\varepsilon_{\\infty}\\), revealing whether the performance gains in RINS stem from an improved scaling exponent, a lower asymptotic limit, or a combination of both. We use the 600M-parameter language models in Figure 4 whose signature is AB, for r \u2208 {1,2,3,4}, where r = 1 corresponds to the non-recursive baseline. As shown in Figure 7, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models."}, {"title": "6.2. Optimal Number of Recursion Rounds", "content": "We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law \\(f_r(x) = B_r x^{-c_r} + E_r\\), then cr increases with r while er decreases. Furthermore, the coefficient Br also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger Br, its faster convergence (cr) and lower asymptotic limit (\\(\\varepsilon_{\\infty}\\)) will eventually lead to superior performance. In other words, using the higher recursion level is advantageous eventually, which is consistent with the experimental results."}, {"title": "7. Discussion and Related Works", "content": "The premise that extending inference time enhances the quality of language model outputs finds support in cognitive science. Human deliberation, often associated with System 2 thinking, is linked to improved decision-making capabilities (Lawson et al., 2020). Mirroring this, numerous computational strategies have been developed to scale inference within Large Language Models (LLMs). Prompt-based methods like Chain-of-Thought (CoT) (Wei et al., 2024) and critique-then-generate approaches (Ankner et al., 2024) are prominent examples. Other iterative refinement methods like ReAct (Yao et al., 2023), Self-Refine (Madaan et al., 2024), and Reflexion, which incorporate feedback and reflection, also improve inference quality. Even simpler strategies, capitalizing on the stochastic nature of LLM decoding that generate multiple responses and select among them, such as self-consistency (Wang et al., 2023), have shown efficacy that scales well across multiple orders of magnitude of inference compute (Brown et al., 2024).\nHowever, the assumption of monotonic improvement with increased inference calls is not guaranteed to hold. As argued by Chen et al. (Chen et al., 2024), repeated sampling may lead to convergence towards the most probable, but not necessarily the optimal, solution. This is particularly pertinent for challenging instances where the probability of a correct answer is below chance (i.e., < 0.5).\nIn this work, we introduce a complementary approach, called Recursive INference Scaling (RINS), which can be integrated with other techniques.. RINS identifies a specific form of model recursion as an effective inference scaling strategy, demonstrating significant performance gains over various other recursive architectures, including the Repeat All Over (RAO) strategy identified by Liu et al. (2024) as state-of-the-art for mobile LLMs.\nRecursion is a form of parameter sharing, a technique that has been explored for a while. Illustrative examples include Universal Transformers (Dehghani et al., 2019) and ALBERT (Lan et al., 2020). Despite their innovative design, their scaling exponents were smaller than in the vanilla transformer (Tay et al., 2022a) so they failed to subsume it (Tay et al., 2022b). RINS, by contrast, improves both the scaling exponents and asymptotic performance limits.\nWe hypothesize that the efficacy of RINS in language tasks stems from its exploitation of language's self-similarity (Al-"}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of artificial intelligence, specifically in the area of language modeling and multimodal systems. There are many potential societal consequences of advancing AI, both positive, such as improved accessibility to high-quality healthcare and education, and negative, if such systems are misused. Our intention with RINS is to contribute to the ongoing effort to improve the performance and capabilities of these models. While we recognize the general ethical considerations that accompany the advancement of language models and AI, we do not feel that this particular work raises unique or unaddressed ethical concerns beyond the established considerations within the field."}]}