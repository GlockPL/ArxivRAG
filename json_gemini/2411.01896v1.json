{"title": "MBDRes-U-Net: Multi-Scale Lightweight Brain\nTumor Segmentation Network", "authors": ["Longfeng Shen", "Yanqi Hou", "Jiacong Chen", "Liangjin Diao", "Yaxi Duan"], "abstract": "Accurate segmentation of brain tumors plays a key role in the diagnosis and treatment of brain tumor\ndiseases. It serves as a critical technology for quantifying tumors and extracting their features. With the\nincreasing application of deep learning methods, the computational burden has become progressively\nheavier. To achieve a lightweight model with good segmentation performance, this study proposes the\nMBDRes-U-Net model using the three-dimensional (3D) U-Net codec framework, which integrates\nmultibranch residual blocks and fused attention into the model. The computational burden of the model\nis reduced by the branch strategy, which effectively uses the rich local features in multimodal images\nand enhances the segmentation performance of subtumor regions. Additionally, during encoding, an\nadaptive weighted expansion convolution layer is introduced into the multi-branch residual block,\nwhich enriches the feature expression and improves the segmentation accuracy of the model.\nExperiments on the Brain Tumor Segmentation (BraTS) Challenge 2018 and 2019 datasets show that\nthe architecture could maintain a high precision of brain tumor segmentation while considerably\nreducing\nthe\ncalculation\noverhead. Our\ncode\n.", "sections": [{"title": "1. Introduction", "content": "The glioma, which can be categorized into low-grade glioma (LGG) and high-grade glioma (HGG)\nsubtypes, is the most common primary malignant brain tumor. It has a high incidence, recurrence rate,\nand mortality, but a low cure rate, which makes treatment challenging. Treatment also requires accurate\nmedical imaging of the tumors and the processing and analysis of images, which rely heavily on\nphysicians. Consequently, the accurate segmentation of brain tumors is key for medical diagnosis and\npathological analysis, as shown in Fig 1(e). This task includes the division of several subareas that is,\nthe enhanced core (EnC), peritumoral edema (PTE), and non-enhanced core (NEC) areas [1]. Among\nthe many medical imaging methods, magnetic resonance imaging (MRI) offers considerable\nadvantages in the treatment of gliomas. It can provide extensive tumor information and is\nnon-invasive; moreover, it does not expose the patient's body to radiation during imaging. There are\nfour common modalities (Fig. 1(a)\u20131(d))\u2014that is, fluid-attenuated inversion recovery (FLAIR),\nT1-weighted (T1), contrast-enhanced T1-weighted (T1c), and T2-weighted (T2) modalities. Notably,\ndifferent imaging effects emphasize different tissue characteristics and tumor diffusion regions [2].\nDoctors usually manually mark a large number of MRI scans, layer-by-layer and piece-by-piece; the\ndivision of tumor regions is dependent on their experience and expert cognition. This work is tedious,\ntime-consuming, and prone to differences of opinion; therefore, accurate automated segmentation has\nimportant research implications.\nMany attempts have been made to solve the multimodal MRI brain tumor segmentation problem,\nincluding edge segmentation, region-based [3], atlas-based [4], and machine learning-based methods.\nAlthough they have low computational complexity, these methods rely heavily on artificial annotation\nand a large number of datasets for pre-training, which can be time-consuming and result in poor\nsegmentation performance.\nSince 2012, the multimodal brain tumor segmentation (BraTS) challenge (jointly sponsored by the\nInternational Association for Medical Image Computing and Computer-Assisted Interventions\n(MICCAI) and other organization), has highlighted the progress of deep-learning algorithms in brain\ntumor segmentation tasks. The performance of U-Net [5] and its variants in multimodal brain tumor\nsegmentation tasks has made considerable progress. From the three-dimensional (3D) U-Net model [6]\nto the TransBTS [7] and SwinUNETR models [8], studies have continuously improved the\nperformance of brain tumor segmentation methods. Moreover, from the initial integration of the U-Net\narchitecture to various attempts to introduce transformers, these models have introduced innovation and\nprogress to the field of brain tumor segmentation. NN-U-Net [9] confirmed the prominent role of the\nU-Net architecture in image segmentation tasks, with the basic U-Net architecture being effective and\ncompetitive across multiple BraTS tasks.\nMoreover, because the attention mechanism enables neural models to focus accurately on all relevant\nelements of the input information, it has become an important component for improving the\nperformance of deep neural models. For example, the squeeze-and-excitation (SE) module in SENet"}, {"title": "2. Related Work", "content": "In recent years, deep learning methods, particularly CNNs, have been widely used in medical image\nprocessing applications. CNN-based models can accurately capture the local features of\ntwo-dimensional (2D) and 3D medical images through learning. Kamnitsas et al. proposed a fully"}, {"title": "2.1 Convolutional Neural Network Model", "content": "the winner of the ImageNet Competition in 2017-assigned different weights to different\npositions of the image from a channel domain perspective using a weight matrix to obtain more\nimportant channel-feature information. However, compared to the attention mechanism that focuses\nonly on the channel, a module that combines the channel and spatial attention mechanism can achieve\nbetter results. For example, the convolutional block attention module (CBAM) [11] successively infers\nan attention diagram along the channel and spatial dimensions.\nAlthough 3D convolutional neural networks (CNNs) have achieved significant results in brain tumor\nsegmentation, these models are usually too complex to calculate and generate a large number of\nparameters, incurring a high computational overhead. Attempts have been made to alleviate this\nproblem by using a lightweight model architecture; however, the segmentation performance of\nlightweight models falls short of that achieved by advanced models. Improving segmentation accuracy\nwhile maintaining a lower computational cost remains a difficult problem to solve.\nAccordingly, we proposed a multimodal-based 3D brain tumor lightweight model (MBDRes-U-net),\nwhich not only solves the problem of single features based on single-modal image representation but\nalso relieves the heavy computational burden caused by complex models. A residual block based on\nmultibranch parallel convolution was used in the proposed model to replace the common 3D\nconvolution block; the computational complexity was reduced using group convolution. An adaptive\n3-D dilation convolution operation was introduced in the encoder to obtain multiscale feature\nrepresentation, and a 3D multi-attention module (SCA3D) was added to the encoding stage to enable\nCNNs to focus attention on the tumor area. The proposed model exhibits specific advantages over other\nsubmodels of the same type in terms of parameter quantity and model complexity. We evaluated the\nmodel on the BraTS 2018 and BraTS 2019 datasets, obtaining good segmentation performance on both.\nThe contributions of this study can be summarized as follows:\n1) In this study, a 3D lightweight codec model, MBDRes-U-net, was proposed for the multi-modal\nMR image segmentation of brain tumors. MBDRes-U-net was developed on a symmetric\nencoder-decoder architecture that integrated the new residual block and attention mechanism.\nWith its simple model structure, it can be used as a baseline for 3D brain tumor segmentation, thus\npromoting research on brain tumor MRI image segmentation.\n2) A new residual block based on a multibranch atrous convolution was proposed, which\nsimultaneously enlarged the acceptance domain and reduced the number of parameters, thereby\nsolving the computational overhead problems caused by 3D convolution. Additionally, we\nintroduced adaptive atrous convolution instead of an ordinary atrous convolution, enriching the\nfeature representation.\n3) A plug-and-play 3D attention module (3D SACA), better suited to brain tumor segmentation, was\nproposed. An improved combination mechanism between the spatial and channel attention was\nexplored."}, {"title": "3. Methods", "content": "framework of the 3D U-Net architecture. Considering that the segmentation target tumor region is\nlocated in the MRI output, we added a multibranch 3D SACA mixed attention module in front of the\nencoder so that the model could focus more attention to the region of interest before extracting features.\nDuring the encoding stage, a $3x3x3$\nconvolution with a step size of 2 was first used for\ndownsampling to reduce the display memory, after which multi-scale features were extracted through\nsix multi-branch extended convolution residual (MBDRes) blocks with adaptive extended convolution\nlayers. In the decoder, a trilinear interpolation method was used to up-sample the feature maps. These\nup-sampled feature maps were then concatenated with high-resolution features obtained from skip\nconnections. Subsequently, the concatenated features was passed through a decoding convolution block\nfollowed by an MBRes block to progressively recover the original resolution step-by-step. Next, all the\nchannel information was fused using a $1\u00d71\u00d71$\nconvolution with a step size of 1. Finally, a\nsegmentation map was obtained using the SoftMax function to realize end-to-end segmentation."}, {"title": "3.1 MBDRes-U-Net", "content": "The architecture of the MBDRes-U-Net model is shown in Fig. 2. The algorithm uses the codec"}, {"title": "3.2 Multibranch Residual Block", "content": "The era of 3D CNNs enabled the full utilization of the characteristics of MRI 3D data. However, when\nthe 3D convolution kernel runs on the entire feature mapping channel, the computational\ncomplexity-that is, the floating-point operations per second (FLOPS)-grows exponentially.\nConsequently, a 3D CNN incurs a high computational cost during training.\nAfter the pre-activated residual block (Fig, 2(a)) used in ResNET v2 [25] is extended to a 3D\nconvolution model (Fig. 2(b)), group convolution-an effective model acceleration method (including\nResNeXt [26] and ShuffleNet [27] models) can be introduced to alleviate the computational burden.\nSuppose that the ResNET v2 unit [25] is divided into g parallel branches, and the kernel size is constant\nat $3x3x3$\nthen the original parameter quantity of (b) is\n$Params(b) = 3 \u00d7 3 \u00d7 3 \u00d7 ((Cinx Cmid + Cmid \u00d7 Cout))$. The parameter quantity of the\nresidual\nblock\nafter\nmulti-branch\ngrouping\nis\n$Params(b') = g \u00d73\u00d73\u00d73\u00d7 ((Cin/gxCmid/g + Cmid/g x Cout/g)) = Params(b)/g$,\nwhich is reduced by g times.\nThis grouping strategy can effectively reduce the number of model parameters and accelerate the\ncalculations. However, the multiple branches generated by channel grouping work independently and\nin parallel, affecting the normal information interaction between channels and reducing the learning\nability of the model. To solve the problem of a lack of channel information exchange (Fig 2(c)), we can\nadd a $1\u00d71 \u00d71$ convolutional layer at the beginning and end of the residual block to serve as the\ninformation route between each branch. Additionally, the residual connection can be arranged outside\nthe unit such that information at a lower level can be transmitted directly to a higher level without\ngenerating additional parameters, increasing the learning ability of the model. Thus, the MBR block\ncan be denoted an MBRes block."}, {"title": "3.2 3D SACA", "content": "Considering that the size of the convolution kernel in the traditional convolution is limited (which leads\nto its limited acceptance domain), to expand the receptive field of the model, learn the multiscale\nfeatures of brain tumor MRI, and capture 3D spatial correlations, we introduced an adaptive weighted\nexpanded convolution layer in the encoder part to replace the conventional convolution operation; thus,\nwe obtained a multibranch expanded convolution residual block (denoted the MBDRes block).\nCapturing multiscale information is an effective strategy that has been used successfully before.\nTokunaga et al. proposed a semantic segmentation task in pathology using three parallel CNNs and\nweighted concatenation to extract multiscale information [28]. In the dense fused maxout network\n(DFMN) [21] proposed by Chen et al., three parallel convolution layers with different expansion rates\nwere used as the weighted sum.\nThe structure of the adaptive weighted expansion convolution layer comprises three parallel 3D\nexpansion volume integration branches, the expansion rates of each branch being 1, 2, and 3,\nrespectively (Fig. 3(d)). Three weights ($W1, W2, and W3$) are assigned to each branch\nafter initialization, and the results of each branch are then added. The initialization of this weight\nensures that each branch has the same impact on the model initially.\nThere is a considerable imbalance in the BraTS dataset, in which tumor regions account for only 1.5%\nof the MRI images and enhanced tumors (ETs) account for only 11% of the whole tumor (WT) images\n[29]. To eliminate the impact of large-area backgrounds on the segmentation, the learning ability of the\nmodel between the spatial details and advanced morphological features was balanced, the model being\nmore focused on the tumor region.\nA 3D attention mechanism can be introduced, and the 3D space and channel attention can be extracted\nusing the feature relationship between the 3D space and channel, as follows:\n$X_c' = X_cx o (Fe(avgpool(X_c)))$ (1)\n$X_s' = X_s \u00d7 \u03c3(F(X_s))$ (2)\nIts structure is shown in Fig. 4. A channel split [20] operation can be introduced to divide the input\nfeature $X \u2208 Rcxhxwxd$\ninto two parts by channel, $X_c \u2208 Rcxhxwxd$\nand\n$X_S E REcxhxwxd$,\nwhere C, H, W and D denote the number, height, width, and depth of channels\nin the feature map, respectively.\n$X_c$ first carries out average pooling to obtain the global channel\ninformation, before passing the information into the channel excitation module to obtain\nc\u00d71 \u00d71 \u00d71\nchannel correlation. $X_S$ is introduced into the spatial excitation module, and\nc\u00d71\u00d71\u00d71\nthe spatial feature correlation is aggregated to dimension through a point\nmultiplication operation of the 3D convolution layer, so that spatial attention weighting can be realized.\nConsequently, the model is able to adaptively adjust the feature responses at different spatial positions.\nNext, $X_c'$ and $X_s'$ are aggregated. We can then fuse the residuals to reduce the sparsity\ncaused by the parallel excitation. Finally, channel shuffling is used to solve the Information exchange\nproblem caused by the branch strategy."}, {"title": "4. Results and Discussion", "content": "We evaluated the MBDRES-U-NET model on the following datasets:\n1) BraTS 2018 dataset: The training dataset comprised 285 samples; the validation dataset comprised\n66 samples.\n2) BraTS 2019 dataset: The training dataset comprised 335 samples; the validation dataset comprised\n125 samples.\nThe volume of each was $240 x 240 x 155$\nThe tags for tumor segmentation included the\nbackground (tag 0), necrotic and non-enhancing tumors (tag 1), peritumoral edema (tag 2), and\nGD-enhancing tumors (tag 4). To facilitate model training, the datasets were pretreated as follows:\n(i) The datasets were acquired by multiple mechanisms resulting in uneven intensity; hence, we\nstandardized the MRI images using the Z-score method.\n(ii) The background information of the brain tumor image is meaningless for segmentation; therefore,\nwe randomly cropped the data to $128 x 128 x 128$\nvoxel inputs.\n(iii) To prevent over-fitting, we used the following data enhancement strategies to add training data:\n0.5 for axial, coronal, and sagittal random reversals; the random rotation angle interval was\n$[-10\u00b0,, +10\u00b0]$.\nThe effectiveness of the model was evaluated based on the computational complexity and segmentation\naccuracy. Validation was conducted using the validation dataset through an online portal provided by\nthe organizers of the BraTS Challenge. Specifically, the segmentation accuracy was measured using the"}, {"title": "4.1 Experimental Details", "content": null}, {"title": "4.1.1 Dataset and data pretreatment", "content": null}, {"title": "4.1.2 Assessment indicators", "content": null}, {"title": "4.1.3 Experimental setup", "content": "We ran the experimental code in Python 3.6 using 16 lots, trained the model for 500 cycles on three\nparallel NVIDIA A30 GPUs, and built all experimental models using the PyTorch framework. The\nAdam optimizer was used, and the learning rate was set to 0.001."}, {"title": "4.2 Comparison Experiments with State-of-the-art Methods", "content": "To verify the performance of the proposed model, we compared the MBDRes-U-Net segmentation\nperformance with that of other advanced models (including the 3D U-Net, U-Net-based, CNN with\nTransformer, and other lightweight models) on the BraTS 2018 and 2019 datasets. The comparison\nresults are presented in Tables 1 and 2. Compared to the non-lightweight models, the proposed model\nexhibits the advantage of low model complexity, enabling more efficient segmentation of the WT and\nTC.\nThe parameters of MBDRes-U-Net parameters are one-fourth of those of the traditional 3D U-Net\nmodel (Table 1). Moreover, the computational complexity is reduced by 1643.75 G, and the\nsegmentation accuracy is considerably improved (being 3.2%, 1.8%, and 13.6% higher than that of the\n3D U-Net model in the ET, WT, and TC segmentation, respectively). Compared with those of the\n3D-ESP-Net and S3D-U-Net models, although the number of parameters for MBDRes-U-Net increase\nmarginally, the computational complexity is just one-third of both, and the segmentation accuracy is\nconsiderably improved. In comparison to the DMF-Net model with similar parameters, while the Dice\ncoefficient of the MBDRES-U-net model is 0.1% lower, the Dice coefficients of the WT and TC\nincreases by 0.7% and 1.8%, respectively. Although the proposed model is not as lightweight as the\nHMNet model, the Dice coefficients of the ET, WT, and TC increase by 0.5%, 0.2%, and 1.0%,\nrespectively, and the Hausdorff distance reduces by 0.002, 0.389, and 2.037 mm, respectively. The\noverall mean score is 0.6% higher than that of the HDC-Net model, which has approximately the same\ncomputational complexity. Compared to that of the latest ADHDC-Net brain tumor segmentation\nmodel, although the score of the MBRes-U-Net model in ET is 0.3% lower, those of the WT and TC\nare 0.5% and 0.3% higher, respectively, the average score of the MBRes-U-Net model being 0.17%\nhigher than that of the non-lightweight ADHDC-Net model. Consequently, the proposed method is a\nmore efficient algorithm that can achieve comparable segmentation accuracy."}, {"title": "4.3 Ablation Experiment", "content": null}, {"title": "4.3.1 Adaptive weighted expansion convolution layer", "content": "Comparative ablation experiments, as listed in Table 3, were conducted to verify whether the adaptive\nweighted dilation convolution layer and adaptive weighting algorithm were required."}, {"title": "4.3.2 Multibranch fused attention", "content": "An ablation experiment was conducted on the BraTS 2018 dataset to assess the need for attention\nmodules and the effectiveness of the parallel branching strategy. The experimental results are\nsummarized in Table 4."}, {"title": "5. Conclusions", "content": "In this study, we proposed a novel 3D U-Net model-namely, the MBDRes-U-Net model-which\ncomprised a multibranch residual block and integrated multibranch fused attention. In particular, we\nplaced an attention block in the encoder to ensure the model paid more attention to the region of\ninterest before extracting the features. In the encoder, we introduced an adaptive weighted expansion\nconvolutional layer to replace the common convolutional layer to form a multibranch expansion\nconvolutional residual block and extract the multiscale features, enriching the feature representation of\nthe image. Additionally, the MBDRes-U-Net model was compared with advanced methods using the\nBraTS dataset. Through quantitative analysis of the MBDRes-U-Net segmentation results and labels, it\nwas evident that the MBDRes-U-Net model exhibited better computational efficiency, fewer\nparameters, and optimal segmentation performance. In the future, we plan to evaluate the application of\nthe MBDRes-U-Net model to other typical medical image segmentation tasks."}]}