{"title": "EXAQ: Exponent Aware Quantization For LLMs Acceleration", "authors": ["Moran Shkolnik", "Maxim Fishman", "Brian Chmiel", "Hilla Ben-Yaacov", "Ron Banner", "Kfir Yehuda Levy"], "abstract": "Quantization has established itself as the primary approach for decreasing the com- putational and storage expenses associated with Large Language Models (LLMs) inference. The majority of current research emphasizes quantizing weights and activations to enable low-bit general-matrix-multiply (GEMM) operations, with the remaining non-linear operations executed at higher precision. In our study, we dis- covered that following the application of these techniques, the primary bottleneck in LLMs inference lies in the softmax layer. The softmax operation comprises three phases: exponent calculation, accumulation, and normalization, Our work focuses on optimizing the first two phases. We propose an analytical approach to determine the optimal clipping value for the input to the softmax function, enabling sub-4-bit quantization for LLMs inference. This method accelerates the calculations of both $e^x$ and $\\sum(e^x)$ with minimal to no accuracy degradation. For example, in LLaMA1-30B, we achieve baseline performance with 2-bit quantization on the well-known \"Physical Interaction: Question Answering\" (PIQA) dataset evaluation. This ultra-low bit quantization allows, for the first time, an acceleration of approx- imately 4x in the accumulation phase. The combination of accelerating both $e^x$ and $\\sum(e^x)$ results in a 36.9% acceleration in the softmax operation.", "sections": [{"title": "1 Introduction", "content": "In recent years, the landscape of natural language processing (NLP) has been transformed by large language models (LLMs), showcasing unparalleled capabilities in contextual understanding and common sense reasoning. These capabilities are particularly evident as models are scaled up, driving research efforts towards further enlarging model dimensions [5, 24]. However, the substantial size of modern LLMs imposes considerable computational demands, making them resource-intensive in terms of training, fine-tuning, and inference processes. Consequently, there has been a surge in efforts to alleviate memory consumption and computational requirements. Among the promising approaches is quantization, a technique that involves representing parts of the model with lower bit widths, thereby reducing resource usage without compromising performance.\n\nThe foundation of LLMs lies in the attention mechanism [25], which encompasses intensive general- matrix-multiply (GEMM) operations, coupled with non-linear operations like softmax. Consequently,"}, {"title": "2 Motivation", "content": "This section aims to illustrate the considerable computational demand imposed by the softmax operation, highlighting the advantages of improving its runtime efficiency. To establish a strong foundation for our argument, we conduct experiments to measure the runtime consumption using the \"LLaMA-2-7B\" LLM model on the Gaudi-2 accelerator, which is equipped with a high-speed network card for optimal performance.\n\nIn Fig.1, we depict the proportion of time allocated to each operation during the model's execution in BF16 format. This graphical representation emphasizes the softmax layer as the main computational"}, {"title": "3 Exponent-Aware Quantization (EXAQ)", "content": "In this section, we introduce a novel quantization method, \"exponent-aware quantization\" (EXAQ),\nspecifically designed for softmax inputs. The softmax function, defined as $softmax(x_i) = \\frac{e^{x_i}}{\\sum e^{x_i}}$,\noperates by exponentiating its input logits and normalizing these values to form a probability\ndistribution. Moreover, usually for numeric stability the maximum of x is subtracted before the\nexponent function. Our method focuses on minimizing the quantization error of the exponentiated\noutputs, ensuring a more precise representation of the softmax function's output. We manipulate\nthe quantization in the input domain but target the mean squared error of the exponentiated outputs,\nminimizing $MSE(e^x, e^{Q(x)})$.\nInspired by the ACIQ paper [3], we limit the range of the tensor by clipping its values. While this\nintroduces some distortion to the original tensor, it significantly reduces the rounding error in the part\nof the distribution containing most of the information. Since $x < 0$, we set a threshold $C < 0$, so that\nif $x < C$, then $x = C$. Clipping is particularly useful because it preserves the less negative values,\nwhich after exponentiation become significantly larger compared to very negative values that become\negligible after the exponential function is applied. Values in the range are quantized on a smaller\nscale, improving resolution for the more common and important values. The method approximates\nthe optimal clipping value analytically from the distribution of the tensor by minimizing the MSE\nbetween $e^x$ and $e^{Q(x)}$. This analytical threshold is simple to use during run-time and can easily be\nintegrated with other quantization techniques."}, {"title": "3.1 Problem Formulation", "content": "We begin by modifying the inputs for the function $e^x$ through the subtraction of the maximum value,\nmax(x), from these inputs. Thus, it is assumed that x < 0. The MSE due to quantization and\nclipping can be expressed as a sum of two integrals: one for the quantization error for $x \\in [C, 0]$ and\nanother for the clipping error for $x < C$. The quantization error integral is given by:\n\n$MSE_{quant} = \\int_{C}^{0} (e^{Q(x)} - e^x)^2 \\cdot f(x) dx,$\n\nwhere f(x) represents the probability density function of x, assumed to be gaussian distributed with\nmean \u00b5 and standard deviation \u03c3. The clipping error integral is:\n\n$MSE_{clip} = \\int_{-\\infty}^{C} (e^{C} - e^x)^2 \\cdot f (x) dx$\n\nThus, total MSE is given by\n\n$MSE = MSE_{clip} + MSE_{quant}$ \n$= \\int_{-\\infty}^{C} (e^{C} - e^x)^2 \\cdot f(x) dx + \\int_{C}^{0} (e^{Q(x)} - e^x)^2 \\cdot f(x) dx.$"}, {"title": "4 Algorithm implementation", "content": "The computation of the softmax function is typically broken down into three essential steps: (1)\nExponent calculation: this step involves computing $e^x$ for each input x (2) Accumulation: this step\ninvolves summing all the exponential values to form the denominator of the softmax function, and\n(3) Normalization: this step divides each exponential value by the computed sum to produce the\nfinal softmax output. Our algorithm primarily focuses on steps (1) and (2), leveraging the ultra-low\nprecision of softmax inputs facilitated by the EXAQ method, to accelerate these two operations.\nFig.4 compares the original softmax algorithm and our optimized 2-bit version, highlighting the\ncomputational efficiencies achieved."}, {"title": "4.1 Exponent calculation", "content": "We replace the traditional direct exponent calculation (line 4 in Algo.2) with the following two steps:\n(1) We quantize each element in the normalized tensor into a 2-bit integer (line 4 in Algo.2). (2) We\nutilize pre-computed values from a lookup table $LUT_{exp}$ to derive the exponents of the quantized\nvalues (lines 5-6 in Algo.2). This $LUT_{exp}$ maps between all possible quantized values and their\nresulting exponents and is notably compact as it needs to store only 4 values. This approach not only\nreduces memory usage but also speeds up the algorithm since the exponential values can be retrieved\nin a single cycle."}, {"title": "4.2 Accelerated denominator accumulation", "content": "Originally, the accumulation process within the softmax layer's denominator requires summing up\nN exponential outputs (Algo.1 lines 7-12). In contrast, our algorithm requires only N/4. Since"}, {"title": "5 Experiments", "content": "This section details the experimental framework used to evaluate the performance of our quantization\nmethod. We evaluate the accuracy across various language tasks, comparing our softmax quantization\nmethod (EXAQ) to the naive quantization method (NAIVE). Our method achieves state-of-the-art\naccuracy scores in almost all experiments."}, {"title": "5.1 Accuracy experiments", "content": "5.1.1 Experimental settings\nOur accuracy experiments focus on the inference setting and are conducted on 8 RTX A6000 GPUs,\nutilizing a batch size of 4 for all evaluations. We use the LLaMA-1 models [23], specifically the\n7B, 13B, 30B and 70B variants, and assess these models on a variety of question-answering and"}, {"title": "5.2 Runtime experiments", "content": "We conducted runtime experiments to evaluate the overall performance of our algorithm, isolating\nthe softmax operation to measure its runtime. Results are shown in Table.3. Our optimized algorithm\ndemonstrates a significant improvement in runtime performance for the softmax operation, achieving\nan enhancement of 36.9%."}, {"title": "6 Related work", "content": "6.1 Language models acceleration\nIn the quest to optimize neural networks for practical deployment, particularly LLMs, studies\nlike [16, 27] have introduced innovative approaches to reduce computational demands. [16], for\nexample, implements an integer-only quantization scheme for Transformers that conducts all inference\noperations with integer arithmetic, using INT32 for softmax inputs. Similarly, [27] applies selective\nquantization to Transformers, focusing specifically on GEMM layers while keeping softmax in FP32\nprecision."}, {"title": "6.2 Softmax acceleration", "content": "Other works focus on softmax acceleration as it has become a bottleneck in recent years for LLMs.\n[22] proposes to use basic-split calculation method, which allows to split the exponentiation calcu-\nlation of the softmax into several specific basics which are implemented by LUTs and multipliers.\n[17, 13, 21, 29] compute the exponential operations of integer and fractional parts separately using\na combination of LUTs and piecewise linear (PWL) function fitting. [17, 13] also accelerate the\ndivision operation by replacing the divider with shifter units. All works use LUT much bigger\nthan ours method (4 entries) except [13] which requires a fine-tuning phase to achieve a small LUT\nsize and is not applicable for the post-training quantization paradigm we work with. [15] utilizes\nquantization-aware training to clip the range of the softmax inputs, subsequently quantizing them to\nINT8 and replacing the exponential operation with shifter units. [21] targets the softmax acceleration\nbut requires a fine-tuning phase to mitigate major accuracy losses. [12] splits the exponential opera-\ntion to high-bits/low-bits and uses Taylor series expansion to approximate the exponential calculation\nand reduce computation. [9] accelerate the attention mechanism by reducing the number of writes\nand reads operations between the HBM and SRAM.\n\nThe most closely related works to ours are [14] and [26], both of which aim to accelerate the softmax\noperation and, like our method, do not require a fine-tuning phase. [14] addresses only the exponent\ncalculation acceleration, disregarding the denominator. In contrast, one of the key advantages of our\napproach is its significant improvement in the denominator accumulation, reducing the number of\nrequired accumulations by a factor of 4. Additionally, for the exponent calculation, [14] uses two\n256-entry LUTs and a multiplication, taking 3 cycles, whereas our approach uses a single 4-entry\nLUT, requiring just one cycle. This significantly enhances both runtime and memory efficiency. A\ndetailed comparison is in C.1. [26] introduces two methods for softmax acceleration: one using two\n1D-LUTs combined with a multiplier, and another using a combination of 1D-LUT and 2D-LUT,\nwithout a multiplier. Both methods, like ours, accelerate the exponent calculation by accessing one\nLUT in one cycle. However, our approach significantly accelerates the denominator accumulation by\na factor of 4, while their method focuses on enhancing the division phase. The distinct enhancements\nof each method suggest a potential synergy, where our improved denominator accumulation could\ncomplement their division optimizations for a complete softmax enhancement. A detailed comparison\nis in C.2"}, {"title": "7 Discussion", "content": "7.1 Summary\nIn this study, we analyze the execution time of various operations during LLMs inference and\ndemonstrate that the softmax operation emerges as one of the primary bottlenecks. Moreover, we\nanticipate that as GEMM acceleration advances, the bottleneck will become even more critical.\nBased on this conclusion, we introduce EXAQ - an analytical approach aimed at reducing the dynamic\nrange of the input of the exponent, thereby enabling quantization below 4 bits and accelerating the\nexponent calculation. Additionally, leveraging ultra-low quantization, we propose a method to\naccelerate the accumulation step by up to 4 times. The proposed full solution is able to get 36.9%\nacceleration in the softmax operation.\nWe demonstrate that our proposed method achieves minimal to no degradation for the first time,\nin 2-bit and 3-bit quantization across various LLM sizes (7B, 13B, 30B and 70B) and a range of\nevaluated tasks."}, {"title": "7.2 Limitations", "content": "In our work, we focused on minimizing the quantization error of the exponential output. A more\nprecise approach, however, would involve minimizing the quantization error of the softmax outputs\nor the entire attention block. This alternative approach was not explored in the current research and is\nidentified as an important avenue for future work. Additionally, our methodology was tested only\nduring the inference stage of the model's lifecycle. Exploring its effects during the training phase\nremains an area for future investigation."}, {"title": "Appendix", "content": "A Further discussion\nA.1 Broader impacts\nThe acceleration of large language model (LLM) runtime significantly impacts modern life, particularly as\ntools like ChatGPT and Gemini become more integrated into daily use. Speeding up these models is essential\nfor ongoing development and growth in this area, as it tackles a critical bottleneck: the processing speed and\nefficiency of the algorithms. Moreover, enhancing these models' speed and reducing their memory footprint not\nonly improves their performance but also makes them more accessible to a wider audience. This allows more\nusers to customize and advance these models for their specific needs and developments.\nB Standard Deviation Range\nC Comparing our algorithm to the latest algorithms\nC.1 A competitive comparison against [14]\nA key advantage of our approach is its significant improvement in denominator accumulation, reducing the\nnumber of required accumulations by a factor of 4, a notable acceleration, as the method proposed in [14] does\nnot address this aspect. The method in [14] quantizes FP16 inputs to 16-bit fixed-point and calculates exponents\nusing two separate 256-entry LUTs, followed by a multiplication. This process requires 3 cycles for the two\nLUT accesses and the subsequent multiplication. In contrast, our algorithm quantizes inputs to 2-bit integers and\nuses a single ultra-small LUT with only 4 entries. This streamlined approach reduces the process to just one\ncycle for the single LUT access, significantly enhancing both runtime and memory efficiency compared to the\nmethod in [14].\nC.2 A competitive comparison against [26]\nThis work supports softmax inputs in integer format and introduces two methods to accelerate the softmax opera-\ntion via approximation. The first method employs two 1-dimensional lookup tables (1D-LUTs) to approximate\n$e^x$ and, combining these outputs with a multiplier to produce the final result. The second method combines a\n1D-LUT and a 2D-LUT (2-dimensional lookup table). In this approach, the output from the 1D-LUT and the\nresults from the accumulated denominator are used as the indices [i, j] for the 2D-LUT, which directly contains\nthe final softmax result, thereby eliminating the need for multiplication or division. However, this approach has\nbeen noted to cause an additional drop in accuracy. Additionally, a de-quantization phase is conducted if the next"}]}