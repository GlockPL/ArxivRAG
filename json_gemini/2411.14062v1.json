{"title": "MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image Generation Perspective", "authors": ["Hailang Huang", "Yong Wang", "Zixuan Huang", "Huaqiu Li", "Tongwen Huang", "Xiangxiang Chu", "Richong Zhang"], "abstract": "Large Multimodal Models (LMMs) have demonstrated remarkable capabilities. While existing benchmarks for evaluating LMMs mainly focus on image comprehension, few works evaluate them from the image generation perspective. To address this issue, we propose a straightforward automated evaluation pipeline. Specifically, this pipeline requires LMMs to generate an image-prompt from a given input image. Subsequently, it employs text-to-image generative models to create a new image based on these generated prompts. Finally, we evaluate the performance of LMMs by comparing the original image with the generated one. Furthermore, we introduce MMGenBench-Test, a comprehensive benchmark developed to evaluate LMMs across 13 distinct image patterns, and MMGenBench-Domain, targeting the performance evaluation of LMMs within the generative image domain. A thorough evaluation involving over 50 popular LMMs demonstrates the effectiveness and reliability in both the pipeline and benchmark. Our observations indicate that numerous LMMs excelling in existing benchmarks fail to adequately complete the basic tasks, related to image understanding and description. This finding highlights the substantial potential for performance improvement in current LMMs and suggests avenues for future model optimization. Concurrently, our pipeline facilitates the efficient assessment of LMMs performance across diverse domains by using solely image inputs.", "sections": [{"title": "1. Introduction", "content": "We have witnessed the quick progress of large multimodal models (LMMs) [5, 7, 12, 21, 28, 39, 50], which efficiently utilize the strength of LLMs [23, 32\u201334, 37, 40] in processing both visual and textual inputs. Compared to text, visual images are characterized by their high level of abstraction and information density, while exhibiting strong spatial correlations and structural complexity. The development of comprehensive benchmarks is crucial for enhancing and accurately evaluating LMMs. Specifically, many popular benchmarks [16, 17, 22, 27, 29, 42, 45, 49], provide standardized evaluations for LMMs by assessing multimodal tasks across various datasets. However, these benchmarks frequently depend on traditional datasets, resulting in issues of data leakage and limited task diversity. Although many various tasks such as VQA [4], Image Caption [24], and OCR [46] are exquisitely designed, most existing benchmarks only focus on evaluating the understanding performance of LMMs.\nOn the other hand, the understanding and generation of images remain disparate fields, with the most formidable models [1, 15, 33] in their respective domains adhering to distinct paradigms. For instance, GPT-4 [33], which is grounded in the next token prediction paradigm, exhibits a formidable capacity for image comprehension. Similarly, Flux [1] has achieved noteworthy success in text-to-image synthesis by leveraging diffusion models [20, 35]. This divergence underscores the complexity of achieving a unified approach to image processing and synthesis, as the state-of-the-art techniques continue to evolve along separate trajectories. Furthermore, Large Multimodal Models (LMMs) are extensively employed to generate training data for generative models. It is noteworthy that LMMs excel in image-to-text tasks, while diffusion models are particularly effective in text-to-image tasks. A robust understanding of an image implies that LMMs can distill its essential information into text prompts, which can then be used by text-to-image models to reconstruct the scene to a certain extent. This process can be viewed as a form of \u201ccompression\u201d.\nHence, it is both reasonable and significant to evaluate the performance of LMMs using diffusion models. Our work aims to bridge this gap by providing a comprehensive evaluation pipeline.\nIn this paper, we propose MMGenBench-Test, a comprehensive benchmark designed to evaluate LMMs across 13 distinct image patterns (refer to Fig. 1), and MMGenBench-Domain, which focuses on assessing LMMs performance within the generative image domain. To achieve this, we introduce a pipeline that initially allows LMMs to generate image-prompt from input images, then employs text-to-image generative models to create new images. Finally, we use an image representation model to obtain the embeddings of images and perform post-processing to assess the performance of LMMs in image understanding and description. We conduct extensive experiments on over 50 popular LMMs, demonstrating the effectiveness and reliability of both the pipeline and benchmark. Notably, our findings reveal that numerous LMMs excelling in existing benchmarks fail to address the basic tasks of image understanding and description, highlighting the substantial potential for improvement and optimization in future models. This straightforward automated evaluation pipeline also enables efficient assessment of LMMs across diverse domains through image inputs alone, providing a flexible and scalable benchmarking tool.\nIn summary, our contributions are as follows:\n\u2022 The proposed MMGenBench is the first automated pipeline, designed to evaluate the capabilities of LMMs in image understanding and description by solely utilizing images. This pipeline utilizes text-to-image models and image representation models for automated evaluation, thereby markedly minimizing human involvement and improving the efficiency and objectivity of the evaluation procedure.\n\u2022 We developed MMGenBench-Test, a comprehensive benchmark designed to evaluate LMMs across 13 image patterns, and MMGenBench-Domain, which assesses the performance of LMMs in the generative image domain.\n\u2022 Our study includes a broad evaluation of over 50 popular LMMs, providing critical insights into their capabilities and limitations in basic image understanding and description tasks."}, {"title": "2. The MMGenBench Benchmark", "content": "The proposed pipeline for LMMs, based on text-to-image generative models and image representation models, consists of three components: image-prompt generation, new image generation, and quantitative metric computation.\nThe input modalities for the LMM primarily consist of two parts: the image used for model understanding and inference, and the prompt guiding the inference direction. To facilitate a standardized workflow, we employ a manually crafted, normalized prompt $\\text{Part}$. This prompt constrains the LMM\u2019s comprehension and reasoning across five dimensions: role, definition explanation, task instruction, key points and requirements, and output format (see Appendix C.1 for details). This process can be formalized as follows:\n$P_{\\text{gen}} = \\text{LMM}(I, \\text{Part}).$\nThrough the above process, we obtain the image-prompt (the detailed description of an image) $P_{\\text{gen}}$ output by the LMM, which will serve as the input for the subsequent stage.\nIn this stage, the main process involves the utilization of text-to-image models to generate new images based on the given image-prompt. Theoretically, the generation quality is highly dependent on the chosen text-to-image model. To minimize variable interference, we standardize the evaluation by selecting four state-of-the-art models: FLUX.1-dev [1], Stable Diffusion 3.5 [15], Kolors [2] and Lumina [18]. The comparison results across these models provide cross-validation of generation effectiveness. This phase is represented as:\n$I_{\\text{gen}} = G(e; P_{\\text{gen}}, \\theta),$\nwhere $I_{\\text{gen}}$ denotes the generated image, $e$ represents a randomly sampled variable from the latent space, typically following a certain distribution (e.g., Gaussian distribution), $\\theta$ represents the model parameters, and $G$ denotes the image generation function.\nWe quantify the functionality of LMM by evaluating the similarity between the input image $I$ and the generated one $I_{\\text{gen}}$. Since most generative models introduce a certain degree of randomness in their inference process, achieving pixel-level consistency between images is challenging. To address this, we conduct comparisons at the representational level. Specifically, we utilize the Unicom model [6] to encode each image and obtain its respective representation. This phase is encapsulated as:\n$F_I = \\text{Encoder}(I), F_{I_{\\text{gen}}} = \\text{Encoder}(I_{\\text{gen}}),$\nwhere $F_I$ and $F_{I_{\\text{gen}}}$ represent the features extracted from the input image $I$ and the generated image $I_{\\text{gen}}$, respectively. To provide a quantitative evaluation, we compute both a similarity score and a generation quality score based on these feature representations.\nSIM-Score is a metric for evaluating the similarity between two features, commonly using cosine similarity. In this study, SIM-Score is calculated as follows:\n$\\text{SIM-Score}(I, I_{\\text{gen}}) = \\frac{F_I \\cdot F_{I_{\\text{gen}}}}{||F_I|| ||F_{I_{\\text{gen}}}||},$\nwhere $\\cdot$ denotes the dot product operation, and $|| ||$ represents the vector norm, used for normalization. The SIM-Score ranges between -1 and 1, where a value of 1 indicates maximum similarity, 0 denotes no similarity, and -1 signifies complete opposition.\nThe FID Score (FID-Score) measures the difference between the distributions of generated images and real images. A lower FID Score indicates that the generated images have higher quality and greater similarity to the real images. The calculation is primarily based on the below equation:\n$\\text{FID} = ||\\mu_x - \\mu_y||^2 + \\text{Tr}(\\Sigma_x + \\Sigma_y - 2(\\Sigma_x \\Sigma_y)^{1/2}),$\nwhere $\\mu_x$ and $\\mu_y$ denote the means of the input image features $F_I$ and the generated image features $F_{I_{\\text{gen}}}$, respectively, while $\\Sigma_x$ and $\\Sigma_y$ represent their covariances. The notation Tr() denotes the trace of a matrix.\nAlthough the existing benchmarks can evaluate the image understanding capabilities of LMMs from various dimensions, as mentioned in Sec. 1, they still lack the evaluation of the basic image understanding of LMMs and the ability to describe the images clearly.\nTo effectively measure the understanding and description capabilities of LMMs across various types of images, we constructed a high-quality test set MMGenBench-Test for 13 image patterns using the JourneyDB test set [36]. We proposed a multi-stage method for extracting and annotating image patterns as illustrated in Fig. 4. To ensure the results\u2019 accuracy, we manually double-checked the image patterns and performed the final annotations.\nIn addition, we have also constructed a dataset in the \u201cimage generation\u201d domain, termed MMGenBench-Domain, to evaluate the ability of LMMs in the understanding and describing \u201cgenerated images\u201d task. It is important to emphasize that our proposed pipeline can measure the ability of LMMs to understand and describe images in any domain. By utilizing images from a particular domain, we can easily assess the performance of LMMs specific to that domain.\nIn the MMGenBench-Test dataset, we constructed a high-quality test set containing 1,284 images across 13 distinct image patterns (see Fig. 1). The distribution of images per pattern is shown in Fig. 3, which illustrates that each pattern contains a minimum of 114 images. Please note that an image may contain multiple patterns. For instance, the first image annotation in Fig. 4 contains four image patterns: \u201cSurreal\u201d, \u201cNatural\u201d, \u201cArtistic\u201d and \u201cColor\u201d. These 13 image patterns are carefully designed so that the dataset can measure the image comprehension and description capabilities of LMMs across diverse dimensions. Additionally, all samples in the dataset are manually checked and annotated to ensure their overall quality.\nTo construct MMGenBench-Domain, we randomly sampled 10, 000 images from the JourneyDB validation set. By utilizing the proposed pipeline, we can evaluate the image understanding and descriptive performance of LMMs within this domain, obviating the need for additional data.\nTo construct MMGenBench-Test, we first extract all images from the JourneyDB test set and process them using the pipeline, shown in Fig. 4. For the domain-specific dataset, we use the JourneyDB validation set for its construction. Subsequently, we will elaborate on each processing step.\nWe extract image patterns from existing images to measure the understanding and description ability of LMMs across various image categories. Given that we possess only images but no additional information, we utilize the powerful GPT-4o model to extract and analyze the underlying image patterns. The task is executed by GPT-4o in three sequential steps: 1) providing a detailed description of the image; 2) annotating the possible patterns based on the image features and description; and 3) explaining the rationale behind the annotated image patterns. By utilizing our carefully designed prompts (refer to Appendix B.3), GPT-4o can perform the task effectively. An example is presented in Fig. 8 in Appendix B.2, wherein the image description, alongside the extracted image patterns and rationales, aligns accurately with the image content. By annotating each image in the JourneyDB test set, we ultimately obtained a total of 1868 image patterns.\nBy leveraging GPT-4o, we extracted an extensive range of image patterns and observed that the semantic meanings across different patterns may be consistent. We conducted thorough statistics of the image patterns, by quantifying the occurrences of each type and ranking them in descending order (as seen in Appendix B.1). Subsequently, GPT-4-turbo was utilized to generate the summary. To ensure the accuracy and validity of the extracted patterns, we carefully designed model prompts for this task (shown in Appendix B.3). This procedure ultimately resulted in a hand-crafted list of 13 distinct image patterns, each thoroughly described in Appendix B.1.\nWe conducted a re-annotation of the JourneyDB test set using the previously identified 13 image patterns. Similar to Image Pattern Extraction, GPT-4o was employed to generate descriptions for each image. Subsequently, these descriptions were annotated based on the predefined image patterns, accompanied by the underlying reasons. A key difference in this process is that re-annotation is restricted to the specified 13 image patterns, excluding any other patterns. Given the extensive number of images in the test set, it is essential to select a subset of the images for subsequent operations. The re-annotated images were systematically classified into distinct image patterns, each containing a specified number of images. To construct the final dataset, we randomly sampled a subset of images from each pattern with a selection probability of $\\frac{100}{N}$, where $N$ represents the total number of images within the pattern. To mitigate potential annotation errors, we manually verified and annotated each image. The final dataset was established by voting between the model predictions and manual annotations.\nConstructing high-quality datasets is an extremely expensive task. Therefore, it is crucial to develop an efficient and convenient method for creating domain-specific datasets when evaluating the image understanding and description capabilities of LMMs in a new domain. By leveraging the proposed pipeline, our method enables a seamless evaluation of LMM performance across various domains, only depending on the availability of domain-specific images. In this study, we randomly select 10, 000 images from the JourneyDB validation set to create our domain-specific dataset. To quantify LMM performance within this domain, we calculate both FID-Score and SIM-Score."}, {"title": "3. Experiment", "content": "In the evaluation of LMMs, we selected over 50 models that exhibited strong performance on the VLM leaderboard. We mainly focus on open-source models, which include Qwen2-VL [38], InternVL2 [11], LLaVA-OV [25], Ovis [31], etc. Additionally, we evaluated three closed-source API models: GPT-4o, Qwen-VL-Max, and Qwen-VL-Plus. To process the image-prompt generated by LMMs and create new images, we selected four text-to-image generation models: FLUX.1-dev, Stable Diffusion 3.5, Kolors and Lumina. During the final evaluation phase, the Unicom image representation model was utilized to extract image features. Further details about our baseline models are provided in the Appendix C.2.\nDuring the image-prompt generation process in LMMs, we utilize the default settings of VLMEvalKit [14], modifying only the predefined query prompt and the image to meet the task-specific requirements. Subsequently, we employ four distinct text-to-image models to generate new images and extract features using an image representation model. We then compute both the SIM-Score and FID-Score for evaluation. Our analysis, detailed in Appendix C.2, reveals a high degree of consistency among the results produced by the four generative models. Therefore, unless otherwise stated, the reported results are obtained using the FLUX.1-dev text-to-image model."}, {"title": "3.2. Main Results", "content": "As shown in Table 1 and 2, the SIM-Score of the most advanced LMMs on MMGenBench-Test is below 0.600. Specifically, GPT-4o achieved a score of 0.566, which is lower than the 0.599 obtained by the open-source model InternVL2-76B. Notably, there is no clear correlation between the model size and performance across different series of models. This indicates the importance of training data and the training process in developing LMMs with strong capabilities of image understanding and description. Furthermore, it is observed that models excelling on existing benchmarks do not necessarily perform well on MMGenBench-Test, such as LLaVA-OV, which only scores 0.494. In the MMGenBench-Domain dataset, the SIM-Score results align closely with those of the MMGenBench-Test. However, there is a significant difference in FID-Score because MMGenBench-Domain includes 10,000 images, thereby improving the accuracy of its FID-Score measurement. Therefore, we propose using SIM-Score as the primary metric. Detailed results and comprehensive analysis are provided in the following sections as well as in Appendix C.3."}, {"title": "Image Patterns Revealing Model\u2019s Strengths and Limitations", "content": "Fig. 5 shows the accuracy of the best-performing LMMs on the MMGenBench dataset. We observed that LMMs exhibit superior performance on image patterns such as \u201cArtistic\u201d, \u201cSurreal\u201d, \u201cSymbol\u201d, \u201cColor\u201d, etc. Conversely, their performance declines on patterns such as \u201cContextual\u201d, \u201cOrientation\u201d, \u201cCount\u201d, \u201cMotion\u201d, etc. This discrepancy suggests that LMMs are proficient in tasks requiring coarse-grained image understanding and description. In contrast, their abilities for fine-grained understanding and description are more challenging, as these tasks require the understanding of the complex contextual relationships within images and the precise linguistic description.\nFig.6(a) shows the performance results of the same series of models with varying parameter sizes, while Fig.6(b) presents the performance results of the same model and parameters under different training protocols and datasets. We can see that as the model parameters increase, the performance of Qwen2-VL improves from 0.487 to 0.553, and the results of InternVL2 increased from 0.476 to 0.599. Additionally, by using improved training data and protocols, Ovis1.6 outperforms Ovis 1.5 by 0.06. Given that our evaluation is based on a single input image, LLaVA-OV-SI demonstrates superior performance to LLaVA-OV."}, {"title": "3.3. Analysis", "content": "We identify three common problems that significantly affect the performance of LMMs in image understanding and description. Please refer to the Appendix D for more details."}, {"title": "Failure in Following Instructions", "content": "As shown in Fig. 7, we have selected various LMMs to demonstrate a portion of the qualitative examples. It is observed that certain LMMs fail to strictly follow instructions when generating the image-prompt. Specifically, some LMMs prepend a prefix to the image-prompt (e.g., RBDash-72B [3] in Fig. 7), while others append explanatory content following the image-prompt (e.g., MiniCPM-Llama3-V2.5 [44] in Fig. 7). Additionally, it is also evident that the ability to follow instructions does not appear to be correlated with the parameter size of different series LMMs. For example, both Qwen2-VL-2B and InternVL2-2B accurately generate the image-prompt in accordance with the given instructions. We argue that an effective LMM, especially when undergoing instruction tuning, should be capable of accurately following instructions and successfully completing the task."}, {"title": "Inability in Generating Detailed Image-prompt", "content": "Most of the image-text pairs used in the training of existing LMMs have relatively short image captions. Consequently, numerous models exhibit suboptimal performance on the MMGenBench Benchmark, which requires the model to understand the image and provide a detailed description. As shown in Fig. 7, LLaVA-OV-72B [25], despite its extensive 72B parameters, produces relatively short image-prompt, which hinders its ability to excel on the MM-GenBench Benchmark. Additionally, we observed that for LMMs in the same series, smaller LMMs tend to generate shorter image-prompt, as evidenced in the Qwen2-VL and InternVL2 series. Based on these findings, we hypothesize that incorporating image-description pairs during the training phase of LMMs could achieve superior results."}, {"title": "Model Overfitting", "content": "Existing LMMs frequently undergo task-specific training to meet established benchmarks. This specialized training can lead to overfitting, which may compromise the basic image understanding and description capabilities of LMMs. For instance, the xGen-MM [43] results in Fig. 7 demonstrate an overfitting towards the notion of \u201csafety\u201d, despite the given image not presenting any safety risk. This observation suggests that in adapting models to various tasks, it is crucial to maintain the basic image understanding and description capabilities of LMMs."}, {"title": "4. Conclusion", "content": "In this study, we present a straightforward automated evaluation pipeline to comprehensively evaluate the ability of LMMs in image understanding and description. Based on this pipeline, we introduce MMGenBench-Test and MMGenBench-Domain to evaluate LMM performance across different image patterns and domain-specific images. By evaluating over 50 widely-used LMMs, we demonstrate the reliability and effectiveness in both the pipeline and benchmarks. In conclusion, our method provides a more fundamental evaluation of existing LMMs, identifies specific gaps in their image understanding and description capabilities, and establishes a robust foundation for further research and model improvement in these areas. All our data and code are open source."}]}