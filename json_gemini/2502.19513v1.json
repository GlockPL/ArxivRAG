{"title": "MIXTRAINING: A Better Trade-Off Between Compute and Performance", "authors": ["Zexin Li", "Jiancheng Zhang", "Yinglun Zhu", "Cong Liu"], "abstract": "Incorporating self-supervised learning (SSL) before standard supervised learning (SL) has become a widely used strategy to enhance model performance, particularly in data-limited scenarios. However, this approach introduces a trade-off between computation and performance: while SSL helps with representation learning, it requires a separate, often time-consuming training phase, increasing computational overhead and limiting efficiency in resource-constrained settings. To address these challenges, we propose MIXTRAINING, a novel framework that interleaves several SSL and SL epochs within a unified mixtraining training phase, featuring a smooth transition between two learning objectives. MIXTRAINING enhances synergy between SSL and SL for improved accuracy and consolidates shared computation steps to reduce computation overhead. MIXTRAINING is versatile and applicable to both single-task and multi-task learning scenarios. Extensive experiments demonstrate that MIXTRAINING offers a superior compute- performance trade-off compared to conventional pipelines, achieving an 8.81% absolute accuracy gain (18.89% relative accuracy gain) on the TinyImageNet dataset while accelerating training by up to 1.29\u00d7 with the ViT-Tiny model.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have achieved remarkable success in various domains, including computer vision and natural language processing. However, their reliance on extremely large-scale datasets, such as ImageNet-21K and MTEB, poses a significant challenge in data-limited scenarios. When data is scarce or costly to acquire, achieving high accuracy becomes significantly more difficult, underscoring the need for methods that can effectively utilize limited resources.\nA promising approach for data-limited scenarios is self-supervised learning (SSL), which leverages unlabeled data to learn informative representations and has shown notable improvements across various downstream tasks. However, these benefits often come with a substantial computational cost, as SSL typically requires an additional and often prolonged training phase. This extended training can be a major bottleneck in compute-constrained settings, making it crucial to balance the trade-off between performance gains and computational efficiency. To address this critical challenge, we develop a new training framework MIXTRAINING, which adds an additional mixtraining phase in between the vanilla self-supervised learning phase (SSL) and supervised learning phase (SL). At a high level, the mixtraining phase \u201cmerges\" several self-supervised learning epochs and supervised learning epochs together, featuring a smooth transition between two objectives (see Fig. 2). Unlike the conventional SSL+SL pipeline that treats self-supervised learning and supervised learning as separate stages, which often causes abrupt transitions, MIXTRAINING"}, {"title": "Contributions", "content": "We develop a novel MIXTRAINING framework that offers a Pareto improvement over the conventional self-supervised learning plus supervised learning pipeline. The main features and contributions of MIXTRAINING are highlighted as follows.\n\u2022 Better Compute-Performance Trade-off. Compared to the standard SSL+SL pipeline, MIXTRAINING features a smooth transition between two objectives (for better accuracy) and an optimized computation allocation. We conduct extensive experiments across various datasets and settings and demonstrate the efficacy of MIXTRAINING. As an example, MIXTRAINING achieves 18.89% performance gain (8.81% absolute) on the TinyImageNet dataset and 1.29\u00d7 speedups for the ViT-Tiny model (Fig. 1).\n\u2022 Versatility and Plug-and-Play Integration. MIXTRAINING is designed to be model-agnostic and can be effortlessly integrated into various settings, such as single-task and multi-task learning settings. Its modular structure allows researchers to plug and play different SSL and SL components without extensive"}, {"title": "2 Methodology", "content": "We briefly introduce the standard self-supervised learning plus supervised learning (SSL+SL) framework in Section 2.1. We introduce our MIXTRAINING framework in Section 2.2, which consists of its design intuition (Section 2.2.1) and operational procedure (Section 2.2.2). We provide extensions of the MIXTRAINING framework to more general settings in Section 2.2.3."}, {"title": "2.1 Background: The Standard Self-supervised Learning plus Supervised Learn- ing Framework", "content": "The standard self-supervised learning plus supervised learning framework typically consists of two phases: self-supervised learning phase (SSL) and supervised learning phase (SL). In the self-supervised learning phase, a backbone model with a self-supervised learning head is trained to help the model learn general feature representations. Specifically, this process usually relies on learning from unlabeled data with reconstruction tasks or predicting manually masked tokens. The backbone model is further refined in the supervised learning phase, together with a supervised learning head. This step adapts the model to downstream tasks, usually achieving better performances than directly training the downstream tasks. Fig. 2(a) shows a general pipeline of the standard SSL+SL framework, which has now become the go-to approach for training large models."}, {"title": "2.2 A New Framework: MixTraining", "content": "While the standard SSL+SL framework has achieved remarkable success, its self-supervised learning and supervised learning phases are completely separated, leaving no room for further optimization. To enable"}, {"title": "2.2.1 Design Intuition behind Mix Training", "content": "While the standard SSL+SL framework has achieved remarkable success, its self-supervised learning and supervised learning phases are completely separated, leaving no room for further optimization. To enable closer interactions between these two phases, we propose a novel MIXTRAINING framework as shown in Fig. 2 which effectively merges several self-supervised learning and supervised learning epochs into an additional mixtraining phase, featuring a smooth transition between learning objectives.\nMIXTRAINING introduces a new mixtraining phase that allows joint updates of self-supervised and supervised objectives, which is in contrast with the standard SSL+SL framework where one first updates the self-supervised objective and then updates the supervised objective. At a high level, the benefits of this phase are rooted in the dedicated joint optimization objective. The joint objective can be easily understood as a weighted average of the self-supervised objective and the supervised objective to balance these two objectives. We next explain the design intuition behind the mixtraining phase to achieve both computation gains and accuracy gains.\nAccuracy Gains. Since the self-supervised learning phase aims at learning general representations and the following supervised learning phase aims at learning task-specific information, intuitively, these two phases optimize the model in different directions. The standard SSL+SL framework features an abrupt change in optimization directions during the transition from self-supervised learning and supervised learning (Fig. 2(a)), which may cause instability in model performance. Indeed, recent research shows that, in certain settings, supervised learning can lead to worse model performance. In our MIXTRAINING framework, the mixtraining phase creates a middle ground (i.e., \u0430 weighted combination of two objectives), allowing a rather smooth transition from the self-supervised learning objectives to the supervised learning objective, as illustrated in Fig. 2(b). We hypothesize that such a smooth transition avoids instability in phase transition, thus allowing the model to better adapt to the target task and achieve higher accuracy (we empirically verify this hypothesis in Section 3).\nComputation Gains. In the standard SSL+SL framework, we first run self-supervised learning passes with data $(x_{ssl}, y_{ssl})$, and then run supervised learning passes with data $(x_{sl}, y_{sl})$. This process involves forward/backward passes of both $(x_{ssl}, y_{ssl})$ and $(x_{sl}, y_{sl})$, and strictly follows a sequential order to compute each sub-processes (top part of Fig. 3). In contrast, MIXTRAINING aims to jointly optimize self-supervised and supervised objectives. Specifically, it \u201cmerges\" $(x_{ssl}, y_{ssl})$ and $(x_{sl}, y_{sl})$ into a mixed data $(x_{mix}, y_{mix})$ and thus merging the separate forward passes over the backbone model into a single pass, and use its result for both self-supervised head and supervised head; from the computation aspect, we also merge the backward passes of self-supervised learning and supervised learning tasks over the backbone model into a single pass (bottom left part of Fig. 3). Since the size of the backbone model is usually much larger than the size of self-supervised and supervised heads, the merge of forward/backward passes over the backbone model allows us to reduce computation compared to the synchronous setting substantially. Additionally, the merge of forward/backward passes over the backbone model allows better parallelization of forward/backward passes over the self-supervised and supervised heads (right part of Fig. 3), which further speeds up the computation."}, {"title": "2.2.2 The MixTraining Procedure", "content": "In this section, we introduce the operational procedure of our MIXTRAINING framework in detail. Besides the number of self-supervised learning epoch $e_{ssl}$ and the number of supervised learning epoch $e_{sl}$, MIX- TRAINING takes as input a hyperparameter MIX-RATIO $\\rho\\in [0, 1]$ to determine the number of self-supervised learning/supervised learning epochs $e_{mix}$ to be merged into the mixtraining phase. We set\n$e_{mix} = [\\rho min(e_{ssl}, e_{sl})]$.\nOur MIXTRAINING framework then operates by running (i) the vanilla self-supervised learning phase for $e_{ssl} - e_{mix}$ epochs, (ii) the mixtraining phase for $e_{mix}$ epochs, and (iii) the vanilla supervised learning phase for $e_{sl} - e_{mix}$ epochs, as shown in Algorithm 1.\nSince the self-supervised learning and supervised learning phases are standard, in the following, we mainly discuss the mixtraining phase. In the mixtraining phase, we (i) design a mixing function $g$ to generate a mixed dataset, and (ii) design a joint optimization objective as supervision signal. We next highlight the design choice for these two parts."}, {"title": "The Mixed Dataset", "content": "The goal of creating a mixed dataset $D_{mix} = g(D_{ssl}, D_{sl})$ is to extract information stored in self-supervised learning dataset $D_{ssl} = \\{x_i\\}_i$ and supervised learning dataset $D_{sl} = \\{(x_i, y_i)\\}i$, featuring a smooth transition between two learning objectives Fig. 2 (b). In the simple case where self- supervised learning and supervised learning use the same feature representation (i.e., $x_i$), we can simply set $g(D_{ssl}, D_{sl}) = D_{sl}$. We remark that the importance of this simple case is usually overlooked: conducting self-supervised learning and supervised learning on the same ImageNet dataset allows one to boost the top-1 classification accuracy to 84.9% from 82.5%, without using extra data.\nWe next discuss the general case where the self-supervised learning dataset is not the same as the supervised learning dataset, i.e., $D_{ssl} \\ne D_{sl}$. Inspired by the mixup method in machine learning to improve the generalization and robustness to adversarial examples, we consider a randomized mixing function $g$, which randomly mixes up data points from both datasets. Specifically, we set\n$D_{mix} = g(D_{ssl}, D_{sl}) = \\{(x_{mix}, y_{sl}) : x_{mix} = \\lambda x_{sl} + (1 - \\lambda) x_{ssl}, (x_{sl}, y_{sl}) \\in D_{sl}, x_{ssl} \\in D_{ssl}\\},$"}, {"title": "The Joint Optimization Objective", "content": "Let $\\theta$ denote the model parameters. Let $l_{ssl}(x; \\theta)$ denote the self- supervised loss, e.g., MSE reconstruction loss for masked autoencoders, and let $l_{sl}(f(x), y; \\theta)$ denote the supervised loss, e.g., cross-entropy for image classification. Let $D_{ssl}$ represent the SSL dataset, which contains examples $\\{x_i\\}i$, and $D_{sl}$ represent the SL dataset, which also contains examples $\\{(x_i, y_i)\\}i$. These datasets serve as the sources for self-supervised and supervised learning, respectively. The classical SSL+SL framework first optimizes $min_{\\theta} E_{x, y\\sim D_{ssl}} [l_{ssl}(x; \\theta)]$ and then optimizes $min_{\\theta} E_{x, y\\sim D_{sl}} [l_{sl}(f(x), y; \\theta)]$.\nTo integrate self-supervised and supervised learning objectives, MIXTRAINING considers a weighted combina- tion of these two objectives and optimizes the following goal:\n$\\min_{\\theta} E_{x, y\\sim D_{mix}} [\\alpha l_{ssl}(x; \\theta) + (1 - \\alpha) l_{sl}(f(x), y; \\theta)]$,\nwhere $D_{mix}$ is the mixed dataset and $\\alpha$ is a hyperparameter LOSS-RATIO designed to balance the focus between learning general representations (by optimizing self-supervised loss $l_{ssl}(x; \\theta)$) and achieving specific target (by optimizing supervised loss $l_{sl}(f(x), y; \\theta)$)."}, {"title": "2.2.3 Extensions to Multi-Task Learning", "content": "While specific choices of the mixed dataset and the joint optimization objective are provided in this section, we remark that these two sub-components are designed in a modular way: researchers have flexibility in selecting their own way of creating the mixed dataset and the joint optimization objective, tailored to specific training goals. For instance, while Algorithm 1 in designed primarily for single-task learning, it can be seamlessly integrated into the multi-task setting. To do that, in mixtraining phase of multi-task learning, we can modify Eq. (3) to incorporate the mixed training objectives for all tasks. We conduct extensive experiments in Section 3 and show that MIXTRAINING is effective in both single-task and multi-task settings."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Setup", "content": "Datasets. We conduct experiments on standard computer vision datasets, including CIFAR-10, SVHN, CIFAR-100, and TinyImageNet. For single-task learning, we consider datasets CIFAR-10, CIFAR-100, TinyImageNet; for multi-task learning, we consider datasets CIFAR-10 and SVHN."}, {"title": "Models", "content": "Our model consists of three components: a shared backbone model, a classification head for SL, and a reconstruction head for SSL. We use the standard ViT-Tiny (ViT-T) as the backbone and the classification head. For the reconstruction head, we use the masked autoencoder (MAE) decoder of depth 2."}, {"title": "Baselines", "content": "We evaluate the performance of our algorithm (Algorithm 1) against the following baselines:\n\u2022 Supervised learning (SL). Conduct standard supervised learning on the backbone and the classification head with cross-entropy loss for $e_{sl}$ epochs.\n\u2022 Self-supervised learning + supervised learning (SSL+SL). Conduct self-supervised learning on the backbone and the reconstruction head with MSE loss for $e_{ssl}$ epochs and then conduct standard supervised learning with cross-entropy loss for $e_{sl}$ epochs."}, {"title": "Evaluation Metrics", "content": "For each method, we measure its performance by the accuracy on the downstream classification task and its computation cost by the end-to-end training latency (on the same machine). We report the average accuracy and latency over 4 runs with different random seeds. We calculate the speedups of our method as the ratio between the latency of SSL+SL and MIXTRAINING."}, {"title": "Other Implementation Details", "content": "We perform experiments on various data limitation levels by randomly select a fraction of $p$ data points from the original dataset; we choose $p \\in \\{10\\%, 25\\%, 50\\%, 75\\%, 100\\%\\}$. In our main experiments, we set training epoch $e_{sl} = e_{ssl} = 100$, LOSS-RATIO $\\alpha = 0.5$, and MIX-RATIO $\\rho = 0.5$; we conduct detailed parameter studies for these quantities in Section 3.3. We defer our experimental details to Appendix A."}, {"title": "3.2 Main Results", "content": "We conduct experiments on the single-task setting in Section 3.2.1, with the same dataset for SSL and SL, and Section 3.2.2, with different datasets for SSL and SL. We conduct experiments on the multi-task setting in Section 3.2.3."}, {"title": "3.2.1 Performance Analysis of Single-Task Learning Setting", "content": "In this section, we evaluate the performance of MIXTRAINING in the single-task setting with various data limitation levels. We conduct experiments on CIFAR-10, CIFAR-100, and TinyImageNet datasets, and show the results in Table 1. As expected, the comparison between SSL+SL and SL reflects a compute-performance trade-off: SSL+SL achieves better accuracy at the cost of larger latency. Our MIXTRAINING method achieves a Pareto improvement over the SSL+SL baseline: MIXTRAINING achieves higher accuracy and lower latency in 15 out of 15 settings. Compared to baselines, MIXTRAINING achieves significant accuracy gains: for instance, on the full TinyImageNet dataset, MIXTRAINING achieves 18.89% relative accuracy gain (8.81% absolute accuracy gain) over SSL+SL and 32.17% relative accuracy gain (13.50% absolute accuracy gain) over SL. The accuracy gains are more significant under limited data: for instance, on the TinyImageNet dataset and at data limitation level of 10%, MIXTRAINING achieves 105.58% relative accuracy gain (10.78% absolute accuracy gain) over SSL+SL and 189.92% relative accuracy gain (13.75% absolute accuracy gain) over SL. In terms of computation cost (reflected as training latency), MIXTRAINING saves more compute and achieves 1.30\u00d7 speedup compared to SSL+SL. These results show that MIXTRAINING provides a better compute-performance trade-off compared to the standard SSL+SL pipeline in the single-task setting across various data limitation levels."}, {"title": "3.2.2 Self-supervised and Supervised Learning on Different Datasets", "content": "In this section, we evaluate the performance of MIXTRAINING in settings where the full self-supervised learning dataset and supervised learning dataset are different. We conduct self-supervised learning on the TinyImageNet dataset, and supervised learning on the CIFAR-10 or CIFAR-100 datasets. We show the results in Table 2. The comparison between SSL+SL and SL can still reflect the compute-performance trade-off: SSL+SL achieves better accuracy at the cost of larger latency. Our MIXTRAINING method achieves a Pareto improvement over the SSL+SL baseline: MIXTRAINING achieves higher accuracy and lower latency in all settings. Compared to baselines, MIXTRAINING achieves significant accuracy gains: for instance, on the CIFAR-10 dataset, MIXTRAINING achieves 5.58% relative accuracy gain (4.71% absolute accuracy gain) over SSL+SL and 9.09% relative accuracy gain (7.43% absolute accuracy gain). In terms of computation cost (reflected as training latency), MIXTRAINING saves more compute and achieves 1.32\u00d7 speedup compared to SSL+SL. These results show that MIXTRAINING can provide a better compute-performance trade-off compared to the standard SSL+SL pipeline for the more general setting with different self-supervised learning and supervised learning datasets."}, {"title": "3.2.3 Performance Analysis of multi-task Setting", "content": "In this section, we assess the effectiveness of MIXTRAINING in the multi-task setting under different levels of data constraints. We perform the experiments on CIFAR-10 and SVHN datasets and show the results in Table 3. As expected, the comparison between SSL+SL and SL demonstrates a compute-performance trade-off: SSL+SL achieves better accuracy at the cost of larger latency. Our MIXTRAINING approach achieves a Pareto improvement over the SSL+SL baseline: MIXTRAINING achieves higher average accuracy in 4 out of 5 settings and lower total latency in 5 out of 5 settings. Compared to baselines, MIXTRAINING achieves significant accuracy gains especially when data is limited: for instance, at data limitation level of 10%, MIXTRAINING achieves 15.39% relative accuracy gain (9.35% absolute accuracy gain) over SSL+SL and 36.06% relative accuracy gain (18.58% absolute accuracy gain) over SL. In terms of computation cost (reflected as training latency), MIXTRAINING saves more compute and achieves 1.31\u00d7 speedup compared to SSL+SL. These results show that MIXTRAINING provides a better compute-performance trade-off compared to the standard SSL+SL pipeline in the multi-task setting across various data limitation levels."}, {"title": "3.3 Parameter Study", "content": "In this section, we explore the impacts of varying LOSS-RATIO $\\alpha$, MIX-RATIO $\\rho$, and training epochs $e_{ssl}$ and $e_{sl}$ for MIXTRAINING."}, {"title": "3.3.1 Impact of loss-ratio a", "content": "We study the impact of varying hyperparameter LOSS-RATIO $\\alpha$ on model accuracy in this section. We conduct experiments with $\\alpha \\in \\{0.01, 0.1, 0.5, 0.9, 0.99\\}$ and report the accuracy in Table 4; we didn't report the latency since varying $\\alpha$ doesn't change the overall computation cost. As shown in Table 4, LOSS-RATIO $\\alpha = 0.5$ generally leads to good accuracy gains either achieving the highest accuracy (3 out of 5) or achieving the second-best accuracy (1 out of 5). This indicates that a well-chosen $\\alpha$ should appropriately balance self-supervised learning and supervised learning objectives in MIXTRAINING. An $\\alpha = 0.5$ allows the model to"}, {"title": "3.3.2 Impact of mix-ratio $\\rho$", "content": "We study the impact of varying the hyperparameter MIX-RATIO $\\rho$ on model accuracy in this section. We conduct experiments with $\\rho \\in \\{0.25, 0.5, 0.75, 1\\}$ and report the accuracy in Table 5; we didn't report latency since large $\\rho$ reduces latency, as analyzed in Section 2.2. In the single-task setting, $\\rho = 0.5$ or $\\rho = 0.25$ leads to better accuracy; in the multi-task setting, $\\rho = 0.25$ leads to better accuracy. Since larger $\\rho$ reduces latency (Section 2.2), its selection should be guided by individual priorities, such as accelerating the learning process or achieving higher accuracy."}, {"title": "3.3.3 Impact of training epochs", "content": "We study the impact of varying training epochs $e_{ssl}$, $e_{sl}$ on model accuracy and training latency. For simplicity, we set $e_{ssl} = e_{sl}$ and choose its value from $\\{50, 75, 100\\}$. We present results of single-task learning under 10% data limitation, and defer the complete experimental results to Appendix B. We see that the SL baseline doesn't benefits from the added computation: increasing the training epochs from 50 to 100 leads to minimal or no accuracy gains. On the other side, SSL+SL and MIXTRAINING achieves higher accuracy with added compute. Compared to SSL+SL, MIXTRAINING achieves higher accuracy and lower latency in 9 out of 9 settings; these improvements demonstrate a better compute-performance trade-off."}, {"title": "3.4 MixTraining Learns Robust Representations", "content": "We evaluate the effectiveness of MIXTRAINING in preserving self-supervised objectives by visualizing several raw and reconstructed images in Fig. 4. The standard SSL+SL approach (second row) significantly dete- riorates model's reconstruction ability, failing to reconstruct the original images effectively. This degradation is likely due to the supervised learning phase interfering with the knowledge acquired during self-supervised learning. On the other hand, MIXTRAINING (third row) preserves its reconstruction ability: images recon- structed by the model trained with MIXTRAINING closely resemble the original images, albeit with some"}, {"title": "4 Related Work", "content": "Self-Supervised Learning. Self-supervised learning (SSL) has become a widely adopted method in various domains, including computer vision and natural language processing. The standard self-supervised learning pipeline consists of two stages: the self-supervised learning stage to learn rich feature representations and the supervised learning stage to adapt to downstream tasks. MIXTRAINING integrates several self-supervised learning epochs and supervised learning epochs into a new mixtraining phase, featuring a smooth transition between two objectives (for better accuracy) and an optimized computation allocation.\nEfficient Training. Various methods have been proposed to reduce computation costs in model training, including model compression, pruning, parameter-efficient strategies and data-efficient strategies. These methods focus on a different direction in the compute-performance trade-off by reducing the compute with minimal performance loss. MIXTRAINING can be integrated with these approaches to further improve its efficiency.\nOther related work. Exploring the synergy between self-supervised and supervised learning, previous studies have focused on domain adaptation, mitigating catastrophic forgetting and improving data efficiency. MIXTRAINING introduces a new mixtraining phase that interpolates the self-supervised learning phase and the supervised learning phase, and carefully analyze its compute-performance trade-off."}, {"title": "5 Conclusion", "content": "We introduced MIXTRAINING, an innovative framework that interleaves multiple self-supervised learning (SSL) and supervised learning (SL) epochs within a unified training phase, enabling a smooth transition between the two learning objectives. By enhancing the synergy between SSL and SL, MIXTRAINING achieves significant accuracy improvements while consolidating shared computation steps to reduce computational cost. Extensive experiments demonstrate that MIXTRAINING offers a superior compute-performance trade-off compared to the conventional SSL+SL pipeline: MIXTRAINING achieves substantial improvements in model accuracy while significantly accelerating training latency. Furthermore, modular design of MIXTRAINING allows for seamlessly integration across various settings, including both single-task and multi-task learning settings."}, {"title": "Future Directions", "content": "An important next step is to evaluate MIXTRAINING on larger-scale experiments with more powerful models, such as ViT-Giant, and larger datasets, such as ImageNet-21K. However, due to computational constraints, we were unable to conduct experiments on these larger-scale settings."}, {"title": "A Additional Implementation Details", "content": ""}, {"title": "A.1 Software and Hardware Dependencies", "content": "All of the codes are based on PyTorch\u00b9 with timm library. All experiments in this paper are running on one NVIDIA RTX 6000 Ada GPU."}, {"title": "A.2 Reuse Implementations of Different Self-supervised Learning Tasks", "content": "MAE. Reuse operation in the MAE process is more complicated, starting with an unmasked encoder forward pass to obtain intermediate features, and then truncating these features based on a predefined masked ratio. A random mask is applied to these truncated features for reconstructing input images. The MAE loss is calculated only from unmasked patches, while the finetuning head computes classification loss using the full intermediate features."}, {"title": "A.3 Implementation Details of Computer Vision Models", "content": "ViT-T. Our ViT-T implementations are largely derived from the seminal work from Wu et al. (2022) and shrink the decoder to 2 layers as suggested in the reproduction challenge by Charisoudis et al. (2023). Specifically, we set the embedding dimension (\u2018emb_dim') to 192, with the encoder and decoder configured to 12 and 2 layers respectively, alongside 3 heads each for both encoder and decoder. The masking ratio is maintained at 0.75 as He et al. (2022) suggests. For the CIFAR-10, CIFAR-100, and SVHN datasets, the image resolution is standardized to 32x32 pixels with a patch size of 2, while the image resolution is 64x64 pixels with a patch size of 4 for TinyImageNet to ensure uniform computational complexity across all experiments.\nPreprocessing. For the preprocessing of CIFAR-10. CIFAR-100 and SVHN, we adopt simple preprocessing as in He et al. (2016), which randomly crops the images to a size of 32 \u00d7 32 pixels, with a padding of 4 pixels on each side of the image, then randomly flips the images horizontally with a 50% probability. For TinyImageNet, we follow preprocessing in the reproduction challenge by Charisoudis et al. (2023), aiming to maintain consistency with established benchmarks and facilitate fair comparison."}, {"title": "B Additional Experimental Results", "content": "We report additional experimental results under various data limitation levels and training epochs, in both single-task setting and multi-task setting. MIXTRAINING generally achieves Pareto improvements over the"}, {"title": "B.1 Additional Experimental Results in Single-Task Setting", "content": "We report experimental results in the single-task setting under various data limitation levels. Results with 10% data limitation level is provided in Table 6, and results with {25%, 50%, 75%, 100%} data limitation levels are presented below. MIXTRAINING achieves higher accuracy and lower latency in 36 out of 36 settings over SSL+SL: MIXTRAINING achieves up to 83.07% relative accuracy gain (14.38% absolute accuracy gain) and up to 1.30\u00d7 speedups."}, {"title": "B.2 Additional Experimental Results in Multi-Task Setting", "content": "We report experimental results in the multi-task setting under various data limitation levels. Results with {10%, 25%, 50%, 75%, 100%} data limitation levels are presented below. MIXTRAINING achieves higher accuracy in 24 out of 30 settings and lower latency in 30 out of 30 settings over SSL+SL: MIXTRAINING achieves up to 34.72% relative accuracy gain (17.25% absolute accuracy gain) and up to 1.31\u00d7 speedups."}]}