{"title": "A Unified Energy Management Framework for Multi-Timescale Forecasting in Smart Grids", "authors": ["Dafang Zhao", "Xihao Piao", "Zheng Chen", "Lingwei Zhu", "Zhengmao Li", "Ittetsu Taniguchi"], "abstract": "Accurate forecasting of the electrical load, such as the magnitude and the timing of peak power, is crucial to successful power system management and implementation of smart grid strategies like demand response and peak shaving. In multi-time-scale optimization scheduling, rolling optimization is a common solution. However, rolling optimization needs to consider the coupling of different optimization objectives across time scales. It is challenging to accurately capture the mid- and long-term dependencies in time series data. This paper proposes Multi-pofo, a multi-scale power load forecasting framework, that captures such dependency via a novel architecture equipped with a temporal positional encoding layer. To validate the effectiveness of the proposed model, we conduct experiments on real-world electricity load data. The experimental results show that our approach outperforms compared to several strong baseline methods.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the increasing penetration of renewable energy sources has brought challenges to grid stability. Load forecasting plays a crucial role in power system operation and planning, as it provides the basis for decision-making, optimal scheduling, and maintenance for the power system [1]. Accurate forecasting of electricity demand, magnitude, timing of peak load is essential for efficient power system operation and strategic planning. Major consumption of electricity by e.g. buildings, as well as smart grid technologies like the demand response programs and peak shaving, all require accurate forecasting to properly manage and plan system stability, cost reduction and grid resilience [2].\nIn practice, multi-timescale forecasting is required to make timely decisions and support diverse operational needs, from immediate load balancing to long-term capacity planning. Building load can be regarded as superposition of linear or nonlinear time series signals of different time scales, ranging from daily, weekly, seasonal patterns to noise [3], [4], as depicted in Figure 1. These features often contain complex, nonlinear interactions between short-term fluctuations and long-term trends, making it challenging to build models that can effectively balance between high and low-frequency components. Additionally, the extraction of meaningful patterns across scales demands advanced feature extraction techniques to separate noise from valuable signals. As real-world data typically display heterogeneity and non-stationarity, an additional layer of difficulty is imposed on the task and requires computationally efficient models that are both adaptable to changing conditions and robust to noises, which is vital for any downstream tasks.\nRecently, there is a growing interest in leveraging deep learning models for enhancing performance in time series forecasting, often with models like the Long-Short Term Memory (LSTM), Convolutional Neural Networks (CNN), among others [5]\u2013[8]. These models excel at learning nonlinear, robust and general representations that have been proven crucial to capturing diverse patterns within time series data. However, a key limitation of current deep learning models in time series forecasting is that they generally require input data to be on a uniform scale, but existing learning frameworks are largely designed for one-to-one mappings [9]. Due to the fixed size and scale of model inputs, they cannot directly accommodate multi-scale data inputs or produce multi-scale outputs [10].\nThe restriction to a single time scale hinders the ability to capture dependencies across different time scales, particularly when short-, mid-, and long-term relationships coexist in the data. Moreover, it complicates the optimization scheduling process, as it demands rolling optimization approaches that account for overlapping objectives across forecast horizons. Accurately capturing these dependencies across time scales is therefore critical for improving forecast quality and optimizing"}, {"title": "II. MULTI-TIMESCALE ENERGY MANAGEMENT", "content": "Efficient optimization strategies are crucial for improving energy efficiency, reducing costs, and ensuring grid stability. Most research primarily focused on formulating efficient day-ahead scheduling plans for energy systems. However, the diversity of multi-energy and load characteristics means that relying solely on day-ahead scheduling plans might not always be able to meet optimization objectives.\nTo address this challenge, multi-timescale optimization strategies have been proposed to optimize energy management across different time scales [11]\u2013[13]. For example, in a campus-level energy management system, the optimization process spans multiple time scales, such as short-, middle- and long-term scheduling, as illustrated in Figure 2a.\n\u2022 Short-term scheduling focuses on optimizing the daily operation of energy systems, such as the HVAC systems, lighting and plug loads.\n\u2022 Middle-term scheduling targets energy system operations over a period, i.e., week or month, including energy storage, demand response and peak shaving. It also plays a key role in long-term energy management by addressing capacity planning, overall load estimation, and periodic energy analysis.\n\u2022 Long-term scheduling is responsible for long-term energy-saving policy formulation and electricity procurement, aligning with the sustainability goals.\nThese multi-timescale strategies provide a holistic approach to managing energy systems, enhancing their adaptability and efficiency across varying temporal resolutions. A multi-timescale optimization process is typically implemented using a rolling optimization approach, which involves solving optimization problems at different time scales iteratively. Figure 2b illustrates an example of the transition from monthly planning to daily rolling planning. The month-ahead optimization schedules overall and individual energy usage, providing periodic energy analysis. This enables building operators to analyze and optimize energy configurations and formulate medium- and long-term energy strategies. The day-ahead optimization determines the operational schedule for the next 24 hours. The system not only considers the load information for the current day but also forecasts the load for the following day to enhance scheduling accuracy. The intra-day rolling optimization addresses the uncertainty in energy usage. For"}, {"title": "III. METHOD", "content": "This section explains how Multi-pofo forecasts building load at daily, weekly, monthly, and yearly levels. Traditional prediction tasks operate on a single time scale with consistent observation intervals, represented as $Y = f(X)$, where $X\\in R^L$ denotes the input sequence of length L, and Y denotes the target output. In the multi-scale forecasting task, we deal with input sequences of varying lengths and need to learn representations across multiple scales. We face two main challenges: (1) Variable Input Sizes: due to different scales, the number of observations varies, leading to input data of different lengths. Specifically, for each scale $i\\epsilon {1,2,3,4}$ corresponding to day, week, month, and year, the input length $L_i$ changes. (2) Learning Representations Across Multiple Scales: A multiscale prediction model needs to learn representations for data at different scales simultaneously, expressed as $Y_i = f_i(X_i)$, where $X_i \\in R^{L_i}$ and $Y_i \\in R^H$ are the input and output at scale i. This is because the larger-scale observations aggregate those at smaller scales, indicating correlations among observations from multiple scales. To address these challenges, we propose a method that learns a unified representation $z \\in R^D$ for data across different scales. The overview of proposed Multi-pofo as illustrated in Figure 3. Multi-pofo comprises three main components: multiscale embedding, multiscale encoder, and prediction module.\nWe apply two operations to handle input data of different lengths due to varying scales: zero padding and multiscale embedding. In zero padding, for each scale i, we pad the input sequence $X_i$ with zeros to reach the maximum length $L_{max} = max_i L_i$. This ensures that all input sequences have the same length, which is important for batch processing in neural networks. The padded input is defined as: $X_i = [X_{i,1}, X_{i,2},..., X_{i, L_i}, 0, ..., 0] \\in R^{L_{max}}$, where zeros are added to reach length $L_{max}$.\nIn the multiscale embedding, we create a scale embedding vector $s_i \\in R^4$ to represent the scale i. This vector helps the model distinguish between different scales. The i-th element of $s_i$ is 1, and the rest are zeros $s_i = [0, ..., 1, ...,0]$, where the 1 is at the i-th position. We then concatenate the zero-padded input $X_i$ and the scale embedding $s_i$ to form the final embedded input for scale i:\n$X_i = [X_i; s_i] \\in R^{L_{max}+1}$.\nThis combined input includes both the time series data and the scale information, allowing the model to learn scale-specific features while maintaining a unified input format.\nThe shared encoder is a three-layer MLP with the ReLU activation function. It processes the embedded inputs and obtain a unified representation $z \\in R^D$. We denote the input to the encoder as $X_i$ for each scale i. The encoder consists of multiple fully connected layers with activation functions defined as $z = Encoder(X_i)$. The model learns a unified representation that captures common features among multiple scales by sharing the encoder across all scales. To ensure that z effectively represents the inputs, a decoder mirroring the encoder architecture is tasked with input reconstruction. The decoder reconstructs the input sequences: $X_i = Decoder(z)$. We train the encoder and decoder by minimizing the reconstruction loss: $L_{recon} = \\sum_{i=1}^I ||X_i - \\hat{X_i}||_2^2$. This training stage helps the encoder learn a good representation z that captures the essential features of the inputs. After this stage, we freeze the encoder parameters to preserve the learned representation.\nUsing the frozen unified representation z, we train a separate prediction module to predict the target outputs $\\hat{Y} \\in R^H$. The prediction module is implemented as a fully connected layer:\n$\\hat{Y} = f(z) = W_{pred}z + b_{pred}$,\nwhere $W_{pred} \\in R^{H\\times D}$ and $b_{pred} \\in R^H$ are the weights and biases of the prediction layer. Our objective is to minimize the prediction loss between the predicted outputs $\\hat{Y}$ and the ground truth Y as $L_{pred} = ||Y - \\hat{Y}||_2$. In many forecasting tasks, H is set to 1 when predicting a single future value, such as the maximum load in the next period. In this stage,"}, {"title": "IV. EXPERIMENTS", "content": "To evaluate Multi-pofo, we conducted extensive experiments using a real-world campus-level electricity load dataset. The dataset consists of electricity load measured from Suita Campus, Osaka University, Japan, spanning the years 2015 to 2020, with a temporal resolution of 30 minutes. This dataset comprises 313 independent circuits, covering various campus facilities such as lighting, cafeterias, hospitals, libraries, and more. We designate the initial four years (2015-2019) as the training set and the final year (2020) as the testing set. The data was organized into four scales: daily, weekly, monthly, and yearly, each corresponding to distinct temporal resolutions. Min-max scaling was applied to normalize the data, ensuring consistency and enhancing model performance. For each scale $i \\in {daily, weekly, monthly, yearly}$, the model learns a function $f_i$ that maps an input sequence $X_i$ of length $L_i$ to the predicted maximum value $Y_i$:\n$X_i = {x_{t-L_i+1}, . . ., x_t } \\rightarrow Y_i = max(x_{t+1},..., x_{t+L_i-1})$\nTo train the model, we employed a two-stage approach. In the first stage, we jointly trained the shared encoder and the reconstruction decoder using a mean squared error (MSE) loss function for 50 epochs. This phase ensures that the encoder effectively captures the essential features of the input data by reconstructing the original inputs. Once the encoder was adequately trained, we froze its parameters to retain the learned representations. In the second stage, we trained the prediction decoder separately for 30 epochs, optimizing it with the MSE loss to map the encoded representations to the target maximum values accurately."}, {"title": "V. CONCLUSION", "content": "In this study, to address the challenges of multi-timescale forecasting in energy management, we proposed Multi-pofo, a unified framework for energy management that can receive varied time series inputs while providing Multi-scale power forecasting. Multi-pofo used zero padding to align the input length of different observation scales, i.e., daily, weekly, and monthly. We also developed a novel temporal positional encoding layer to effectively index these scales. A simple MLP encoder then captured complex temporal dependencies across multiple timescales, enabling diverse forecasting tasks. Future work will focus on optimizing forecasting efficiency and conducting broader, more comprehensive evaluations."}]}