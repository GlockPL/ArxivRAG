{"title": "Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data", "authors": ["Puneet Kumar", "Sarthak Malik", "Balasubramanian Raman", "Xiaobai Li"], "abstract": "The ability to generate sentiment-controlled feedback in response to multimodal inputs-comprising both text and images-addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment. A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability. Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics. It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback.", "sections": [{"title": "1 INTRODUCTION", "content": "THE process of multimodal feedback synthesis involves generating responses to multimodal inputs in a way that mirrors human feedback [1]. Controlling sentiments in feedback, a capability inherent to humans, remains a challenge for machines [2]. The ability to control sentiments in feedback synthesis has wide-ranging applications. For instance, it can lead to more empathetic patient care in healthcare, more accurate consumer insights in marketing, and more engaging educational material development. Such systems can predict the mental states of patients, assess user responses to products, analyze social behaviors, evaluate multimodal educational content, and predict user engagement in advertisements [3], [4], [5]. Crucially, the controllability aspect of these systems enables them to adopt different roles in human-computer interaction (HCI), such as responding in varied styles to suit the audience's needs - from encouraging the stressed or depressed, acting as a strict coach, to being delightful or bubbly in lighter contexts. The ability to control sentiments in feedback enriches HCI, making it more personalized.\nThis paper introduces the task of controllable feedback synthe-sis for input images and text. It proposes a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset for this purpose, created by crawling Facebook news articles. A feedback synthesis system has been developed that generates sentiment-controlled feedback with positive or negative sentiments. It employs two networks for textual and visual modalities, each comprising an encoder, a decoder, and a control layer. The encoders use a text transformer [6] and a Faster Region-Based Convolutional Neural Networks (R-CNN) model [7] to extract features, which the decoder combines for feedback generation. The control layer, placed after the decoder, selectively activates or deactivates neurons corre-sponding to the positive and negative sentiments. The system also includes a similarity module, which ensures that generated feedback closely aligns with the input's context, enhancing the relevance and accuracy of the response. Furthermore, the contribution of various textual and visual features toward generating uncontrolled and controlled feedback has been analyzed using the K-Average Additive exPlanation (KAAP) interpretability technique.\nThe proposed task of controllable feedback synthesis differs from existing related tasks. Unlike multimodal summarization [8], which condenses information, or dialog generation [9], which focuses on conversation flow, it requires creating contextually apt and sentiment-aligned feedback. Importantly, our work focuses on generating \u2018opinions', rather than reusing \u2018knowledge' or \u2018facts'. Our approach uniquely allows for the generation of controlled opinions, which can vary just as human opinions do. Visual question answering [10] and sentiment-aware conversation [11] involve extracting or inferring sentiments but do not generate sentiment-influenced feedback. Similarly, sentiment-controlled text generation [12], though sentiment-centric, needs the multimodal dimension of our approach. These tasks are empirically found incapable of synthesizing meaningful sentiment-controlled feedback [1], [13]. Furthermore, although Generative Pre-Trained Transformer (GPT) and Large Language Models (LLMs) [14] excel in summarization and question answering through knowledge retrieval, our approach fundamentally differs. Instead of relying on pre-existing data, our system interprets multimodal inputs to generate novel, sentiment-controlled feedback that mirrors human interaction. This highlights the distinctive contribution of our approach compared to GPT and other LLMs, which primarily leverage existing knowledge."}, {"title": "2 RELATED WORKS", "content": "This section explores the state-of-the-art in the tasks related to controllable feedback synthesis and highlights research gaps.\n2.1 Multimodal Summarization\nMultimodal summarization combines the inputs from various modalities to produce comprehensive summaries [15]. In this direction, Chen and Zhuge [16] used recurrent networks to align text with corresponding images. In another work, Zhu et al. [17] focused on extractive summarization, selecting relevant images and texts using a pointer generator network. Their work was extended by Zellers et al. [18] for video summarization, using self-attention mechanisms to choose relevant video frames based on semantic content. Shang et al. [19] emphasized timing in video summarization with time-aware transformers. In other studies, Zhao et al. [20] explored audiovisual video summarization including sensory input and Xie et al. [21] used visual aesthetics while generating multimodal summaries.\nDespite the advancements mentioned above, multimodal sum-marization faces the challenge of modality bias [22], [23], and it has not been explored for affect synthesis. Moreover, multi-modal summarization approaches do not use human comments in model training. In contrast, the proposed system is trained on human-generated comments alongside text and image inputs to generate sentiment-controlled feedback. Furthermore, unlike multimodal summarization, which condenses the given information, controllable feedback synthesis produces feedback that aligns with sentiments and fits the context.\n2.2 Visual Question Answering\nVisual Question Answering (VQA) combines visual perception with interactive question answering [10]. Chen et al. [24] explored controlled generation versus standard answering methods in VQA,"}, {"title": "2.3 Dialogue Generation", "content": "In dialogue generation, particularly in visual dialogue (VisDial), computational models are designed to engage in dialogues about the visual content present in images. In this direction, Kang et al. [31] developed the Dual Attention Network (DAN) that utilized the understanding of visual references in dialogues. This approach was further advanced by Jiang et al. [32], who implemented a region-based graph attention network for enhanced image-related questioning. In another works, Xu et al. [33] and Zhao et al. [34] worked on modeling the conversations utilizing conditional variational autoencoders for diverse dialogue scenarios.\nRecent developments in VisDial include the work of Chen et al. [35], who applied contrastive learning for better cross-modal comprehension in visual dialogue. Contributions from Wang et al. [36] and Kang et al. [37] involved enhanced dialogue generation through generative self-training. Liu et al. [38] and Liu et al. [39] also contributed to closed-loop reasoning and counterfactual visual dialogue for unbiased knowledge training. Despite aforementioned developments, existing VisDial methods do not generate sentiment-controlled feedback. The proposed task stands apart as it synthesizes feedback that is not only contextually relevant but is also tailored to the sentiment of the conversation.\n2.4 Sentiment-Aware Conversation Agents\nIn the context of sentiment-aware conversational agents, Das et al. [11] laid the groundwork by using multiple generators and a multi-class discriminator to create text with diverse sentiment tones. Building upon this, Shi et al. [40] introduced a framework lever-aging user sentiments to guide dialogue actions. In another work, Kong et al. [41] explored sentiment-constrained dialogue generation using generative adversarial networks. In recent developments, Firdaus et al. [42] presented a system that integrates sentiments into conversational agents. Their SEPRG model complemented it [43], which focuses on sentiment-aware, sentiment-controlled personalized response generation.\nIn other works, Hu et al. [44] developed a speech sentiment-aware agent for empathetic responses, and Saha et al. [45] integrated sentiment-aware mechanisms into conversation gen-eration. Unlike the aforementioned works that primarily focus on recognizing the users' sentiments and using the same to influence dialogue actions, the proposed system generates (as opposed to recognizing) the feedbacks with the desired sentiment tone."}, {"title": "2.5 Sentiment-Controlled Text Generation", "content": "Recent natural text generation research trends include sentiment-controlled text generation. In this context, Zhou et al. [12] introduced the Emotional Chatting Machine (ECM), employing a seq2seq framework with internal and external memories for managing emotional states. In another work, Wang and Wan [46] generated sentimental texts using mixture adversarial networks. Huang et al. [47] contributed to automatic dialogue generation with a focus on expressed emotions. Building on these, Zhong et al. [48] introduced an affective attention mechanism with a weighted cross-entropy loss for generating affective dialogue.\nThe sentiment-controlled text generation methods do not use human comments to train their models. On the contrary, the proposed system captures the wider sentiment context from multimodal (text and images) inputs. Moreover, it is trained on human-generated comments aside from text and images to learn the contextual diversity from the comments. Furthermore, our work generates 'opinions' in contrast to generation of 'knowledge' or 'facts' by the above mentioned tasks. Unlike facts, the opinions can be controlled in a similar way how humans do. This area has not been addressed by the tasks mentioned earlier, while our work actively contributes towards it."}, {"title": "3 PROPOSED SYSTEM", "content": "This section describes the formulation of controllable feedback synthesis task and the architecture of the proposed system.\n3.1 Task Formulation\nGiven an environment E = {T, I1, I2, . . ., In }, where T denotes input text and I1, I2, ..., In denote n input images. Each image is comprised of m objects 01,02,...,0m, while the text T is made up of a dictionary of k words w\u2081, W2,..., Wk. The task is to generate a feedback towards environment E under a sentiment-controlled manner where 'sentiment-controlled' implies the feedback aligns with a specified sentiment S, with S being either 0 for a negative sentiment or 1 for a positive sentiment.\n3.2 Proposed System\nThe proposed system, as illustrated in Figure 1, employs two networks to process textual and visual data. Each network consists of an encoder, a decoder, and a control layer. The input data is processed by the encoder-decoder framework, followed by the control layer in both networks. The encoders, utilizing a text transformer [6] and a Faster R-CNN model [7], extract textual and visual features from the data. The decoder then uses these features to generate the feedback. The proposed system also includes a similarity module that evaluates the contextual similarity of the gen-erated feedback with the human comments and an interpretability module that analyses the influence of different features on feedback generation. The proposed system implements both visual and textual attention to optimize information extraction and enhance overall performance. In the context of visual feature extraction, a pre-trained VGG network for global features' extraction and Faster R-CNN for local features' extraction have been used."}, {"title": "3.2.1 Textual Encoder", "content": "The textual encoder is based on the transformer [6] and comprises global encoding and textual attention mechanisms. The global encoding uses a convolution-gated unit to enhance textual repre-sentation, aiming to minimize repetition and maintain semantic relevance. Following the transformer model principles outlined in [6], the textual attention component includes multi-headed self-attention with a self-attention layer and a feed-forward layer. These components are further augmented with positional embedding to capture token positioning, and the process is finalized with normalization to yield the textual context vector z. The feed-forward network (FFN) within this setup features an input and output layer of dimension 512 and a hidden layer of dimension 2048. The FFN's output for a particular input x is determined as specified in Eq.1.\nFFN(x) = max(0,xW1 + b1)W2 + b2\nwhere, b1, b2, W1, and W2 represent the bias terms and weight matrices, respectively. In the self-attention mechanism, the query, key, and value weight matrices are initially randomized in the encoder and updated during training."}, {"title": "3.2.2 Visual Encoder", "content": "The top three images from each sample are inputted into a visual encoder, using blank images for samples with fewer than three images. Their extracted features are concatenated into a visual context vector z. Feature extraction employs a pre-trained Faster R-CNN model [7]. As per Eq. 2, CNN layers output feature maps for the Region Proposal Network (RPN) to generate anchor boxes with binary scores based on Intersection Over Union (IoU) values [49]. These anchors undergo classification and regression, leading to classified boxes. A total of 1601 classes are assigned to these boxes, with their features combined to form a global feature vector. The Faster R-CNN has been chosen for object detection due to its efficiency and accuracy in handling small and varied objects, especially for non-real-time settings. The decision to use the top three images in the visual encoder is a considered trade-off between not losing significant visual content and avoiding the addition of blank images in posts with fewer images as most of the posts have at least 3 images.\nObjectiveness Score = \\begin{cases}\nPositive; & IoU > 0.7 \\\\\nNegative; & IoU < 0.3 \\\\\nNo score; & 0.3 < IoU < 0.7\n\\end{cases}"}, {"title": "3.2.3 Attention", "content": "The attention mechanism in both the encoder and decoder operates on three vectors: Q (query), K (key), and V (value). The output of the self-attention layer, denoted as zi, is computed by multiplying the ith input vector of the encoder with the respective weight matrices W(Q), W(K), and W(V). This computation yields the attention head matrix z, as detailed in Eq. 3, whose dimensionality is equivalent to the length of the input sequence.\nz = Attention(Q, K, V) = softmax(\\frac{Q.KT}{\\sqrt{d_k}})V\nwhere Q, K, and V represent matrices that contain all queries, keys, and values, respectively, with dk as the scaling factor and KT denoting the transpose of K. To achieve a comprehensive subspace representation, the mechanism computes multiple attention heads using distinct sets of Query, Key, and Value matrices. The queries, keys, and values undergo projection head times, resulting in multiple attention heads h1, h2, ..., hhead, where head signifies"}, {"title": "3.2.4 Decoder", "content": "The textual and visual decoders have an identical architecture comprising two main blocks: a self-attention block and an encoder-decoder attention block. These blocks are enhanced with positional encoding and normalization to improve efficiency and accuracy. They work separately. The textual decoder is fed with the textual context vector zt, while the visual decoder receives the visual context vector zi. Additionally, both decoders are provided with the ground-truth comment as input. The functioning of the attention layers is as follows:\n\u2022 Self-attention layer: This layer utilizes future position masking to concentrate exclusively on prior positions in the output sequence. This layer's query, key, and value weight matrices are initially set to random values in the decoder and are progressively refined throughout the training process.\n\u2022 Encoder-decoder attention layer: In this layer, queries are produced within the decoder. For the textual and visual context vectors, the context vectors z\u0142 or zi are used as keys and values in the matrices, respectively.\nLate fusion has been applied using concatenation, preserving the distinct information of image features. A gated-convolution unit (GCU) has been introduced for textual feature encoding to minimize the repetition in the generated feedback."}, {"title": "3.2.5 Control Layer", "content": "The control layer is positioned following the decoder and just before the feedback generation phase. Its purpose is to introduce perturbations into the feedback synthesis model, thereby constrain-ing the generated feedback to align with the desired sentiment. The control layer uses two masks, one for positive and the other for negative sentiments. These masks alter the input vector through element-wise multiplication, as described in Eq. 5. The role of the control layer is to fine-tune the sentiment in feedback so that it corresponds with the targeted sentiment tone. It accomplishes this by selectively activating or deactivating neurons to align the feedback with the chosen sentiment. This layer functions similarly to a modified dropout layer, designed to impact feedback sentiment through the regulation of neuron activity.\nO = \\begin{cases}\nmask1 * I; & Sentiment = 0 \\\\\nmask2 * I; & Sentiment = 1\n\\end{cases}\nwhere O and I denote the output and input vectors, respectively. Each mask in the setup blocks x% of neurons, targeting different neuron sets. As a result, (100 \u2013 2x)% of neurons are trained for both sentiments, while x% are specialized for a specific sentiment. This configuration directs the feedback towards the desired sentiment tone, with x% set at 10% in this framework. The control mechanism trains (100 \u2013 2x)% of neurons on all samples for feature extraction, while x% are dedicated to learning positive and negative sentiments in samples with corresponding sentiment tones. This method ensures the network's comprehensive under-standing of both general and specific sentiment aspects in feedback. During inference, to generate sentiment-specific feedback, neurons trained for the contrasting sentiment are deactivated. For example, to produce positive feedback, neurons associated with negative sentiment are turned off, and vice versa. This approach is crucial for controlling the output sentiment independently of the input text's sentiment, focusing on steering the generated sentence's sentiment."}, {"title": "3.2.6 Similarity Module", "content": "The similarity module quantitatively assesses the semantic simi-larity between the feedbacks generated by the proposed system and human comments. It employs a pre-trained Sentence-BERT (SBERT) model [50] to generate sentence embeddings from the feedbacks and comments and use them for similarity comparisons. The individual comments and feedbacks are first transformed into vectors in an n-dimensional embedding space, where n denotes the size of the embeddings generated by the model. Each dimension within this space encapsulates a distinct linguistic feature or attribute, thereby comprehensively representing the sentence's semantic characteristics.\nFollowing the generation of these embeddings, we compute the cosine similarity between the vector representations of the generated feedback and the human-provided comment. The cosine similarity score is a robust metric for this purpose, as it effectively captures the sentence vectors' orientation (and thus the semantic similarity) while remaining agnostic to their magnitude. It provides a relevance measure, reflecting the relevance and contextual similarity between the feedbacks and comments."}, {"title": "3.3 Interpretability", "content": "In this work, we have extended the K-Average Additive explanation (KAAP) interpretability technique introduced in our previous work [51]. It is based on Shapley Additive exPlanations (SHAP) that is an approximation of Shapley values [52]. As depicted in Fig. 2, it has been incorporated to assess the influence of each visual and textual feature on the sentiment portrayed by the generated feedback. We hypothesize that varying sentiments produced from identical inputs (text + images) should reflect in differential feature importance. It is expected that key features will differ for negative versus positive sentiments. When identical inputs are processed to portray varied sentiments, the model should adjust its focus across different image and text segments, thereby validating our controllability hypothesis.\nSHAP Values Computation: The SHAP values for the features denote their contribution to the model's prediction. For a model f, the SHAP value for feature i is defined as per Eq. 6.\n\\varphi_i(f) = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|!(|F| - |S| - 1)!}{|F|!}[f(S \\cup \\{i\\}) - f(S)]\nwhere F denotes the complete feature set, S a subset excluding i, and f (S) the model's prediction using features in S.\nThe computation of SHAP values requires exponential time theoritically which is approximated by dividing the input into k parts as illustrated in Eq. 7. For each modality, the input is repeatedly divided into k segments, determining each segment's impact on model predictions. A feature vector X with n features is segmented into k parts.\nX = [X_1, X_2, ..., X_k], where X_i \\subseteq X and \\bigcup_{i=1}^k X_i = X\nFor simplicity with k = 2, the fundamental computation of SHAP values is denoted in Eq. 8. It is extended for other values of k. The optimal values of kimg and ktxt, representing the number of segments to divide the image and text into, have been determined experimentally.\nI\\{f_1\\} + I\\{f_2\\} = I\\{f_1, f_2\\} = I\\{null\\}"}, {"title": "K-Average Additive exPlanation (KAAP):", "content": "The KAAP value for feature i is calculated by averaging the SHAP values across the k divisions of the feature vector using Eq. 9.\nKAAP_i = \\frac{1}{k} \\sum_{j=1}^k \\varphi_i(X_j)\nThe KAAP values directly indicate the significant image features for predictions. For input image Ximg of dimensions 128 x 128, the KAAP values for a given k are computed by segmenting the input along both axes. For text data Xtxt, we derive the feature vector and divide it into k segments. Text division considers each word as a feature, acknowledging that sentiments are conveyed by words, not individual letters."}, {"title": "4 DATASET CONSTRUCTION", "content": "A large-scale dataset named controllable multimodal feedback synthesis (CMFeed) dataset\u00b9, has been constructed for controllable feedback synthesis. It contains 61734 comments from 3646 posts compiled by crawling news articles from Sky News, NYDaily, FoxNews, BBC News, and BBC NW through Facebook posts. The collection process utilized the NLTK2 & newspaper3k\u00b3 libraries.\nThe CMFeed dataset consists of multiple images per sample, corresponding news text, post likes and shares, and human comments. The comments for each post have been sorted based on Facebook's 'most-relevant' criterion, which prioritizes the comments with the highest likes and shares. The comments have been preprocessed in the following manner the emoticons have been converted to words using the demoji library; blank comments have been removed; contractions have been expanded; special"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "5.1 Training Protocols\nThe baselines and proposed system described in the following sections have been trained for 60 epochs on Nvidia V100 GPU. The experiments have been performed using 5-fold cross-validation and 80%-20% training-testing split using the following parameters.\n\u2022 General parameters Batch-size: 16, learning rate: 0.001, network optimizer: Adam, loss function: cross-entropy loss, activation function: ReLU.\n\u2022 Parameters for the transformer model \u2013 Encoder embedding dimensions: 100, decoder embedding dimensions: 100, en-coder hidden units dimensions: 128, decoder hidden units"}, {"title": "5.2 Evaluation Metrics", "content": "Feedback synthesis is a one-to-many task, i.e., many feedbacks can be generated for one pair of image-text input. Hence, computing ac-curacy of generated feedbacks is not feasible. Instead, we evaluate the generated feedbacks against the ground-truth comments using the metrics to evaluate semantic relevance and their ranks. For semantic relevance evaluation, BLEU [57], CIDEr [58], ROUGE [59], SPICE [60], and METEOR [61] have been used. The BLEU is a precision-based metric, ROUGE is a recall-based metric, while METEOR combines precision and recall. On the other hand, CIDEr is a consensus-based evaluation metric, and SPICE is based on evaluating the sensitivity of n-grams. Higher values of these metrics denote more semantic similarity between the generated feedback and ground-truth comment. For rank based evaluation, 'Recall@k' [62] and 'Mean Reciprocal Rank' [63] have been used as per the following definitions. As we find top k results and evaluate them, these rank-based metrics are suitable for the evaluation.\n\u2022 Mean Reciprocal Rank: For calculating Mean Reciprocal Rank (MRR), first, the similarity of the generated feedback is compared with all the ground-truth comments. If the most similar comment is ranked k, then the rank of the jth feedback is given by Eq. 10.\nrank_j = k\nwhere k denotes the kth comment when sorted by the relevance whereas rank; is the Rank of the jth feedback. Finally, MRR is calculated as the average of the reciprocal ranks of all the generated feedback samples as per Eq. 11.\nMRR = \\frac{1}{n} \\sum_{j=1}^n \\frac{1}{rank_j}\nwhere n is the number of generated feedback samples, while rank; denotes the jth feedback's rank.\n\u2022 Recall@k: Recall@k counts the number of data samples matching any top-k relevant data samples. Adapting Recall@k for evaluating the generated feedback, the number of feed-backs similar to any of the top-k comments sorted according to relevance is calculated. To find if the generated feedback is similar to any comment, the rank of that feedback as calculated in Eq. 11 is used. Finally, the Recall@k can be formulated according to Eq. 12. According to this, if the rank of the feedback is in top-k, then a score of 1 is assigned to the feedback, else 0. The summation of all scores is done to calculate the Recall@k as shown in Eq. 12.\nRecall@k_i = \\begin{cases}\n1 & if \\ rank_i \\in [1,..., k]\n\\end{cases}\nRecall@k = \\sum_{i=1}^n Recall@k_i\nwhere Recall@ki and ranki is the Recall@k and rank of ith feedback respectively whereas Recall@k denotes final score. Furthermore, the sentiments of the generated feedbacks have been computed and sentiment classification accuracy has been analysed along with the 'Control Accuracy' which is the difference between the accuracies of controlled and uncontrolled feedbacks."}, {"title": "5.3 Baseline Models", "content": "The following baselines models have been constructed along with the proposed model. It is critical to highlight that the Controllability module, outlined in Section 3.2.5, is a consistent feature across all the subsequent architectural descriptions.\n\u2022 Baseline 1: The first baseline utilizes Gated Recurrent Units (GRU) as textual encoder and VGG network as visual encoder. An early fusion method is applied to integrate visual and textual modalities.\n\u2022 Baseline 2: Maintaining GRU for textual encoding and VGG for visual encoding, this baseline utilizes a late fusion approach for combining the visual and textual data.\n\u2022 Baseline 3: This baseline implements a combination of a Transformer and a Gated Convolutional Unit (GCU) for textual encoding. It uses Faster RCNN with an additional visual attention mechanism for visual encoding. A late fusion strategy with averaging is used here.\n\u2022 Baseline 4: The third baseline replaces the textual encoder with GPT-2 [64] while continuing to use Faster RCNN for visual data encoding with visual attention. It also combines the modalities using a late fusion with averaging. It has been empirically observed that GPT-2 based model generated good feedbacks only for textual input; however, it did not generate good feedbacks for multimodal input.\n\u2022 Proposed System: The proposed system incorporates Trans-former as the textual encoder and Faster RCNN as the decoder and it uses concatenation along with late fusion."}, {"title": "5.4 Results", "content": "The generated feedbacks have been evaluated quantitatively and qualitatively for a holistic assessment of their relevance with respect to ground-truth human comments.\nSemantic Relevance Evaluation: The generated feedbacks' se-mantic relevance with human comments has been evaluated. The feedbacks are generated to reflect the same sentiment class as reflected by the corresponding comments and then the feedbacks are evaluated using the BLEU, CIDER, ROUGE, SPICE, and METEOR metrics. As depicted in Table 3, the proposed model has obtained the best values for these metrics in most cases."}, {"title": "Rank-based Evaluation", "content": "The generated feedbacks are further evaluated using the MRR and Recall@k metrics. As observed in Table 4, 76.58% feedbacks are relevant to any top 10 comments. The MRR observed is 0.3789, denoting that most generated feedbacks are contentually similar to one of the top-3 comments. The variations in sentiment classification accuracy and MRR varied differently for different models. For example, baseline 4 has lower MRR but high sentiment classification accuracy, whereas it"}, {"title": "5.5 Ablation Studies", "content": "Following ablation studies have been conducted to evaluate the im-pact of various parameters on the proposed system's performance.\n5.5.1 Effect of Number of Control Layers and Value of Control-Parameter\nThe control layer has been used after the decoder and before the text generation phase to apply 'control' or constraints on the text generation. Here, it is crucial to decide a) the number of control layers and b) the suitable value for the control parameter. Regarding the number of control layers, we experimented with 1, 2, 3, and 4 control layers. The best performance has been observed using the 1 control layer, which decreased slightly for 2 control layers, decreased further for 3 layers, and decreased significantly for 4 layers. Further, regarding the control parameter, we experimented with its values of 5%, 10%, 15%, and 20%. The results show that more control can be achieved with the increasing value of the control parameter; however, fewer neurons will get trained for the entire training data, causing a degradation in the result quality. As depicted in Table 6, a control value of 10% results in better performance than the other values. Hence, 1 control layer with control value of 10% have been used in the final implementation."}, {"title": "5.5.2 Effect of Beam-size", "content": "The beam-size is a search parameter that refers to the number of options the model keeps at each step of the prediction, controlling the breadth of the search for the best output sequence. It keeps only the top k predictions, where k is the beam size. A larger beam size allows the model to explore more possibilities, potentially improving output quality; however, it increases computational requirements and may also degrade the output because of repetitive text generation. We experimented with beam-size values of 2, 5, 10, 15, and 20. The corresponding sentiment classification accuracies and MRR values have been summarized in Table 6. As the beam-size value of 5 provides the best performance and computational complexity trade-off, it has been used in the final implementation."}, {"title": "5.5.3 Effect of Division Factor for KAAP technique", "content": "The suitable values of the division factors kimg and ktxt used in Section 3.3 have been decided experimentally using the dice coefficient [65]. It measures the similarity of two data samples; the value of 1 denotes that the two compared data samples are completely similar, whereas a value of 0 denotes their complete dis-similarity. For each modality, we computed the KAAP values at k \u2208 {2,3...,30} and analyzed the dice coefficients for two adjacent k values. For image & text, the dice coefficient values converge to 1 at k values of 5 and 20 respectively. Hence, the same have been used by the proposed system."}, {"title": "5.6 Visualization and Interpretability", "content": "Fig. 3 shows sample results, highlighting the features being focused on during uncontrolled and controlled feedbacks' generation. In image plots, red and blue represent the most and least contributing pixels, respectively whereas for text plots, yellow and blue indicates the most and least important textual features.\nIn Fig. 3a, positive sentiments are indicated by smiling faces and a family setting, while negative sentiments are associated with the depiction of aging, particularly in the older face. The expression of the girl on the left, a blend of smiling and discomfort, captures attention in both positive and negative contexts. The middle girl's face, predominantly smiling, is highlighted in red for positive and blue for negative sentiments. Fig. 3b shows that dark areas contribute to negative sentiment, whereas faces are linked to positive sentiment. In negatively controlled settings, the crowd is focused; in positively controlled settings, the focus shifts to individual people. In Fig. 3c, positive sentiments downplay the importance of the gun, concentrating more on the number plate. In the uncontrolled setting, the focus is primarily on the words. For Fig. 3d, the facial features are highlighted red for positively controlled and blue for negatively controlled settings for the first image. The second image associates positive sentiment with light and text and negative sentiment with darkness."}, {"title": "5.7 Human Evaluation", "content": "The sentiments of the generated feedbacks have been evaluated by 50 evaluators, comprising 25 males and 25 females, with an"}, {"title": "6 DISCUSSION AND CONCLUSIONS", "content": "The proposed system, trained on human comments and image-text inputs under sentiment constraints, generates human-like feedback with appropriate sentiments, as evidenced by evaluation metrics in Tables 3 and 4. It effectively manages sentiment in feedback, excluding negative content such as 'hatred,' and produces sentiment-specific responses, simulating human sentiment in context. The control layer allows for feedback generation with desired sentiments, utilizing different non-keywords and varying keywords, especially with higher control parameter values. The challenge of determining comment relevance in the CMFeed dataset was addressed by adopting Facebook's relevance criteria."}]}