{"title": "Trying to be human: Linguistic traces of stochastic empathy in language models", "authors": ["Bennett Kleinberg", "Jari Zegers", "Jonas Festor", "Stefana Vida", "Julian Pr\u00e4sent", "Riccardo Loconte", "Sanne Peereboom"], "abstract": "Differentiating between generated and human-written content is important for navigating the modern world. Large language models (LLMs) are crucial drivers behind the increased quality of computer-generated content. Reportedly, humans find it increasingly difficult to identify whether an Al model generated a piece of text. Our work tests how two important factors contribute to the human vs Al race: empathy and an incentive to appear human. We address both aspects in two experiments: human participants and a state-of-the-art LLM wrote relationship advice (Study 1, n=530) or mere descriptions (Study 2, n=610), either instructed to be as human as possible or not. New samples of humans (n=428 and n=408) then judged the texts' source. Our findings show that when empathy is required, humans excel. Contrary to expectations, instructions to appear human were only effective for the LLM, so the human advantage diminished. Computational text analysis revealed that LLMs become more human because they may have an implicit representation of what makes a text human and effortlessly apply these heuristics. The model resorts to a conversational, self-referential, informal tone with a simpler vocabulary to mimic stochastic empathy. We discuss these findings in light of recent claims on the on-par performance of LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have been among the landmark technical achievements of the past decade [1-3]. With LLMs' increasing ability to generate text comes the challenge for consumers of digital content to tell whether a text was written by a human or generated by an Al model. This paper argues that previous comparisons between LLMs and humans in producing convincing prose may have been unjust to human language ability. We adopt experimental research methods from psychology to investigate whether the need to use empathy and an incentive to appear human have confounded previous work."}, {"title": "Studying the behaviour of LLMs", "content": "Approaches to studying LLMs have recently adopted research methods and designs from the social and psychological sciences. Some argue that Al models \u2013 including LLMs \u2013 ought to be studied analogous to how animal and human behaviour is investigated through behavioural patterns in an artificial, social environment [4]. Borrowed from early behaviourist ideas, the machine behaviour school is careful not to attribute inner processes to how Al models, such as LLMs, work. A more direct reference to the field of psychology is made elsewhere [5] with the argument that differences in observable behaviour (e.g., generated text) attributable to manipulation in input (e.g., changing prompt instructions) can provide insights about underlying processes or representations (e.g., bias). Consequently, the experiment is the most informative research design for studying the behaviour and processes of LLMs. Using that approach led to the discovery of racial prejudices in LLMs [6], proneness to cognitive biases [7], illusory truth effects in LLMs [8] and early efforts on testing a theory of mind in Al models [9, 10]. At the same time, more content is produced by or with the help of language models, and recent evidence indicates that LLM-generated content is highly effective in influencing opinion both in an encouraging form shown to dampen conspiratorial thinking in humans [11] and more concerningly, in the form of persuasive messaging on political issues [12]. Consequently, humans as consumers of an ever-growing amount of digital content increasingly aplenty with Al- generated pieces are facing the task of distinguishing human from machine-generated content."}, {"title": "Differentiating human-written content from LLM", "content": "Telling whether a piece of content was generated by an Al model or written by a human is also increasingly difficult for a human. There is evidence that this difficulty affects images [13], voice [14] and text [15,16]. In the domain of textual data, recent work compared human-written online self-representations (dating profiles, professional bio sketches, hospitality profiles) to those generated by a language model [16]. Independent human assessors then evaluated all texts. The findings suggested that humans often use misguided cues to tell Al from humans and that exploiting these flawed heuristics can be used to source Al content that is considered more human than that written by humans. The current paper builds on that study and addresses two potential shortcomings. First, the human-written texts were written without the human authors having a particular incentive to appear human. While this incentive was arguably not relevant several years ago before a widespread adoption of LLMs, this may be about to change with more usage and knowledge about language models. Second, the context of the writing task was constrained to self-presentations which limit the degree to which humanness can \"shine through\". Put differently, the task was not favourable to human language ability, and the human authors did not expect to compete with an Al to appear convincing."}, {"title": "Aims of this paper", "content": "While there is thus evidence that human assessors struggle to distinguish human-written content from LLM-generated content, two important moderators of that effect have yet to be examined. First, it is still being determined whether humans can convey their 'humanness' when they are aware of an Al system trying to mimic human writing. Second, the characteristics of the writing task may affect the degree to which an LLM can convey humanness. Previous work has used tasks that are relatively low on required empathy and, hence, potentially more manageable for an LLM. We experimentally disentangle the effects of human task awareness (Study 1) and required empathy (Study 2). Using computational text analysis, we then shed light on the strategies to appear human (Study 3)."}, {"title": "Ethics and transparency statement", "content": "The local ERB approved all experiments conducted before data collection. All data, materials and code to reproduce the LLM data collection are publicly available at https://osf.io/rcz6s/?view_only=82e087f3c0a64b0aaf6039094cfc5c9c."}, {"title": "STUDY 1", "content": "We aimed to isolate the effect of human task awareness on a writing task that required a nuanced understanding of human relationships. Participants were tasked with writing relationship advice whilst either being made aware (adversarial condition) or not (na\u00efve condition) of an Al system performing the same task to appear as human as possible. This experiment resulted in a 2 (Modality: human vs LLM) by 2 (Condition: adversarial vs naive) between-subjects design with the source evaluation of a new sample of humans as the dependent variable. The sample size for this experiment was based on an a priori power analysis for a small effect size (f=0.10), an alpha threshold of 0.01, and a power of 0.80, resulting in a required sample size of 1172 (i.e., 586 human-written texts, 586 LLM-generated texts)."}, {"title": "Method", "content": "The experimental conditions followed the same procedure as in Study 1 and we recruited participants (who were not participating in Study 1) from Prolific (n=610, 50.16% female, Mage 34.92, SDage=12.53, remuneration: GBP 1.25) who were assigned to either of the four experimental conditions: relationship advice and control condition (n=159), relationship advice and adversarial condition (n=160), relationship description and control (n=149) or relationship description and adversarial (n=142). Those that failed the manipulation check were excluded (n=78)."}, {"title": "Relationship advice vs relationship descriptions", "content": "Participants in the relationship advice condition followed the same procedure as those in Study 1. In the relationship description condition, participants were told to \"[...] write a brief description of a friend's relationship who is having relationship problems. Basic details of the scenario will be provided. The description should be based on those details and written in full sentences\". Identical to Study 1, humans and the GPT4 model received the same instructions in each condition."}, {"title": "Additional control questions", "content": "We further included the following control variables in the human writing task: the participants' self-rated ability to express themselves well in the text they wrote and whether the task required much empathy (both on a 7-point scale from \"completely disagree\" to \"completely agree\")."}, {"title": "Source evaluation", "content": "A sample of 406 participants (52.96% female, Mage=29.35, SDage=8.26, remuneration: GBP 2.00) recruited from Prolific evaluated the 1064 texts (half of which were human, half LLM-generated) using the 5-point scale from Study 1 (\"1=definitely Al-generated\" to \"5=definitely human- written\"). On average, 3.24 humans assessed each statement (SD=0.65)."}, {"title": "Results", "content": "Participants perceived the task to be of low-to-moderate difficulty (adversarial condition: M=3.50, SD=1.79; naive condition: M=3.45, SD=1.76), with a t-test indicating no difference between the two conditions, d=0.03 [99%CI: -0.20; 0.25]. Similarly, there was no difference in motivation to perform well (adversarial condition: M=5.81, SD=1.16; naive condition: M=5.73, SD=1.26), d=0.07 [-0.16; 0.29] with participants indicating a high motivation overall."}, {"title": "Human judgments", "content": "The 2 (Source) by 2 (Condition) between-subjects ANOVA revealed a significant main effect of Source, F(1,1056)=203.46, p<.001, \u03b7\u00b2=0.16 [0.12, 1.00]. That effect showed that texts written by humans (M=3.46, SD=0.89) were rated as more human than those generated by an LLM (M=2.75, SD=0.85), d=0.82 [0.65; 0.98], regardless of the task instructions. A significant main effect of Condition, F(1, 1056)=99.31, p<.001, \u03b7\u00b2=0.09 [0.05; 1.00], indicated that texts written under the adversarial instructions (M=3.34, SD=0.82) were perceived as more human than texts written under the naive instructions (M=2.85, SD=0.99), d=0.55 [0.39; 0.71]. That effect was subsumed under the significant interaction between Source and Condition, F(1, 1056)=65.69, \u03c1\u03b7\u00b2?=0.06 [0.03; 1.00], revealing that the adversarial instructions had no effect for humans, d=0.10 [-0.12; 0.33] but a substantial effect on LLMs, d=1.24 [1.00; 1.49]."}, {"title": "Diagnostic value of human judgments", "content": "We used receiver operating characteristics to assess how well human judgments can differentiate between human-written and LLM-generated texts and compared the areas under the curve (AUCs). That approach plots the sensitivity (i.e., true positive rate) against 1-specificity (i.e., true negative rate) across all cutoff values and measures how much the area under that curve deviates from a non-diagnostic diagonal. The AUC ranges from 0 to 1, with values closer to 1 indicating higher diagnostic value and a value of 0.50 indicating no diagnostic value.\nThe diagnosticity of human judgments in telling the source of the texts for texts written under naive instructions was high, AUC=0.83 [0.78; 0.88]. However, under the adversarial condition, this dropped markedly, AUC=0.62 [0.56; 0.68]. The DeLong's test [17,18] for comparing two AUCs indicated a significant difference, D(952.92)=7.05, p<.001."}, {"title": "Discussion Study 1", "content": "Human-written pieces of relationship advice were perceived as more human than those written by an LLM. Notably, only the LLM responded to the adversarial instructions: when instructed to be as human as possible, the LLM could drastically increase the humanness perception (+39.3%) while humans could not (+2.6%). Although we presumed that the relationship advice task required more empathy than a descriptive writing task, we lack decisive evidence for that claim. To address this, we experimentally manipulated the writing task in Study 2."}, {"title": "Study 2", "content": "In the second experiment, we added an experimental manipulation of the characteristics of the writing task, which either required empathy (i.e., writing relationship advice \u2013 identical to Study 1) or merely described a relationship. That additional factor resulted in a 2 (Modality: human vs LLM) by 2 (Condition: adversarial vs naive) by 2 (Task: advice vs description) between-subjects design. The sample size was based on a power analysis for f=0.15, a=0.01, and power=0.80, resulting in a total sample size of 523 for the human sample. We obtained the same number of texts from the LLM."}, {"title": "Results", "content": "Participants indicated a high motivation to perform well (Table 2), a high ability to express themselves and a moderately high degree of empathy required. There were no differences between conditions in self-rated expression ability (p=.437) or motivation (p=.147). For the perceived level of empathy required, there was a significant main effect of Task (F(1,528)=31.46, p<.001, \u03b7\u00b2=0.06 [0.02; 1.00]), with the advice task (M=5.61, SD=1.27) being perceived as requiring more empathy than the description task (M=4.94, SD=1.50), d=0.49 [0.26; 0.72]. Similarly, perceived difficulty differed by Task, F(1, 528)=23.65, p=.003, \u03b7\u00b2=0.02 [0.00; 1.00], so that the relationship description task (M=3.85, SD=1.69) was rated as more difficult than the advice task (M=3.42, SD=1.67), d=0.26 [0.03; 0.48]."}, {"title": "Human judgments", "content": "The 2 (Source) by 2 (Condition) by 2 (Task) ANOVA showed a significant three-way interaction between these factors, F(1, 1056)=29.93, p<.001, \u03b7\u00b2=0.03 [0.01; 1.00], under which the other effects were subsumed. We unpack that three-way interaction as the main result by Task (Table 3). For the relationship description task, the Source by Condition ANOVA indicated no significant interaction. There was a significant main effect of Source, F(1, 450)=24.34, p<.001, \u03b7\u00b2=0.05 [0.01; 1.00]. Relationship descriptions written by humans (M=3.29, SD=0.82) were perceived as more human that LLM-generated ones (M=2.93, SD=0.75), d=0.46 [0.22; 0.71].\nFor the relationship advice task, the analysis revealed a significant main effect of Source, F(1, 606)=179.40, p<.001, \u03b7\u00b2=0.23 [0.16; 1.00], indicating that texts originating from humans (M=3.59, SD=0.87) were rated as more human than LLM-generated texts (M=2.70, SD=0.89), d=1.01 [0.70; 1.23]. For condition, the significant main effect, F(1, 606)=43.01, p<.001, \u03b7\u00b2=0.07 [0.03; 1.00], suggested that relationship advice written under the adversarial instruction (M=3.36, SD=0.87) was rated as more human than those written under the naive instructions (M=2.93, SD=1.04), d=0.45 [0.24; 0.66]. The critical interaction replicated the key effect from Study 1, F(1, 606)=53.89, p<.001, \u03b7\u00b2=0.08 [0.04; 1.00]: for the relationship advice task, the adversarial instructions did not affect humans, d=0.06 [-0.35; 0.24] but had a substantial effect on LLMs,\nd=1.21 [0.89; 1.53]."}, {"title": "Diagnostic value of human judgments", "content": "The AUC analysis replicated the findings from Study 1. For the advice task, the diagnosticity under the naive instructions was high, AUC=0.88 [0.83; 0.93], but dropped significantly under the adversarial instructions, AUC=0.63 [0.55; 0.72], D(505.99)=6.57, p<.001. In comparison, the AUC for the relationship description task was considerably lower (naive: AUC=0.60 [0.50; 0.70]; adversarial: AUC=0.65 [0.56; 0.74]) and did not differ between conditions, p=.354."}, {"title": "Discussion Study 2", "content": "Empathy-related task manipulation adds to evidence that human-written texts are perceived as more human than those written by an LLM when a task requires empathy. The human advantage disappeared in the simple description task. We further found that the adversarial instructions were effective only when the task required empathy, and only for the LLM. Here, the second experiment replicated the findings that the LLM (+41.0%), but not humans (-1.4%), responded to the instructions to be as human as possible. The contrast in the effectiveness of the adversarial instructions is striking. To better understand how humans and LLMs approach that task, we employ computational text analysis in Study 3."}, {"title": "Study 3", "content": "We look at the strategies humans and LLMs use to appear as human as possible through linguistic differences. We use the data from Study 2 for the subsequent analysis. First, we assess how relationship advice differed between humans and LLMs in the naive condition as a baseline. Second, we look at how either modality changed the relationship advice when instructed to appear as human as possible \u2013 which humans failed to do effectively, and LLMs were particularly good at. For these analyses, we look at n-gram differentiation (i.e., which sequence of n words differed between two groups) and psycholinguistic differences as measured by the Linguistic Inquiry and Word Count (LIWC) software. Third, we also explored the role of spelling mistakes and word frequency as potential cues and examined whether humans resorted to such decision heuristics."}, {"title": "Method", "content": "The n-gram differentiation test [19] compares frequencies of unigrams, bigrams and trigrams with a signed rank sum test approach. Ties in ranks are resolved by random ranks in 500 iterations, which are averaged. For parsimony, only n-grams that appeared in at least 5% of all documents were included, stop words were removed, and each term was reduced to its word stem. The effect size for the frequency comparisons is r (ranging from -1.00 to 1.00)."}, {"title": "LIWC analysis", "content": "The LIWC (version 2022)[20] measures the proportion of words in a document that belong to a range of predefined linguistic and psycholinguistic categories using a word-matching approach with curated dictionaries. Categories include cognitive processes, emotions, and social processes. We conducted bottom-up testing with all 117 variables using a Bonferroni-corrected significance threshold."}, {"title": "Spelling mistakes and vocabulary frequency", "content": "Related work suggests that mistakes and rare words are diagnostic features between Al and human-generated content [16]. We used the raw textual data and counted the number of spelling mistakes using the hunspell R package [21]. To avoid inflating the number of mistakes and to correct for statement length, we counted mistakes for both British and American English spelling, took the larger of the two, and standardised the counts to mistakes per 100 words. The vocabulary frequency was obtained by calculating the average rank frequency of the words in a document. As a reference, we used the most frequent 10k words based on Google's Trillion Word Corpus\u00b2. Lower rank scores imply a higher frequency. For both measures, we examine the diagnostic value and their usage as cues by humans. If humans used a cue in their judgment, we expect high correlations between the cue and the human judgment."}, {"title": "Results", "content": "The n-gram differentiation and LIWC analysis (Table 4)3 suggests that the most prominent difference stems from the LLM advice containing the greeting clause \"dear friend\" and mentions of confrontation and approaching the upcoming conversation with the partner. The latter might be an artefact of the instructions (\"[...] the advice should be about what the person should do next in an upcoming confrontation\").\nLLM-written relationship advice also contained substantially more big words (i.e., words longer than seven letters), emotion words (mainly driven by anger and negative emotions), and more punctuation, as well as a higher score on analytical thinking (i.e., reflecting \"logical, formal thinking\"). In contrast, human-written advice contained more common verbs, auxiliary verbs, and function words. Human writing was also markedly higher in discrepancy (e.g., \"would\", \"can\", \"want\u201d, \u201ccould\u201d), allure (e.g., \u201chave\u201d, \u201clike\u201d, \u201cout\u201d, \u201cknow\u201d), differentiation (e.g., \u201cbut\u201d, \"not\u201d, \u201cif\u201d, \u201cor\u201d), negation, and use of the first-person singular."}, {"title": "Human strategies to appear human", "content": "Differences in human-written relationship advice between the naive and the adversarial condition were expectedly minor. Merely three n-grams emerged as different: \"thing\" (adversarial > naive), \u201cperson\u201d (naive > adversarial), and \u201ccan\u201d (adversarial > naive). Similarly, the LIWC analysis revealed that only one category differed: impersonal pronouns (e.g., \u201cthat\u201d, \"this\", \"what\") were somewhat more pronounced in the adversarial than in the naive condition. The findings align with our finding that humans were generally unable to appear more human."}, {"title": "LLM strategies to appear human", "content": "The substantial effects we observed when LLMs moved from the naive to the adversarial condition (Study 1: d=1.24; Study 2: d=1.21) manifested in textual differences. Table 5 shows that the LLM dropped the \u201cdear friend\" greeting altogether and instead switched to a more informal \"hey\". We also observe the increased use of \"really sorry\" and fewer formal references to the relationship.\nFurthermore, when trying to appear human, the LLM resorted to considerably more netspeak (e.g., \"lol\", \"haha\") and conversational markers (e.g., \u201cyes\u201d, \u201cyeah\u201d, \u201cokay\u201d). The temporal focus shifted to the present, and the language became more engaging (e.g., an increased use of verbs) and contained more self-references (first person singular). At the same time, the LLM dropped the use of big words and references to drives \u2013 particularly power and affiliation as drives \u2013 and became less analytic. These findings provide strong evidence that the LLM aligned the content and style to the need to appear more human by shifting to informal language with self-references."}, {"title": "Spelling mistakes and vocabulary frequency", "content": "Spelling mistakes were higher in human-written texts than in LLM-generated ones (d=0.54 [0.38; 0.70]) and higher in the description than in the advice task (d=0.21 [0.05; 0.37], Table 6). These effects translated to a moderate diagnostic value (AUC between 0.63 and 0.71). Low non- parametric correlations between the rate of mistakes and the human source evaluation score suggest that humans did not use spelling mistakes as a cue in their judgment.\nThe mean rank vocabulary frequency was highly diagnostic, with close to perfect diagnostic power (AUCs between 0.91 and 0.98). Again, we find substantial evidence for the LLM in the advice task to adjust the language between naive and adversarial instructions (d=1.24 [0.92; 1.56]) but no difference for humans (d=0.17; [-0.12; 0.47]). Interestingly, human judgments correlated moderately to highly with the rank frequency, suggesting that humans judged texts as \"more human\u201d when the frequency was lower (i.e., more common language was used). However, only in the naive advice task were humans able to exploit that cue (p=-0.61), which explains the overall high diagnostic power of human judgments in that condition as reported above\n(AUC=0.88 [0.83; 0.93])."}, {"title": "General Discussion", "content": "Large language models (LLMs) have rapidly been endorsed in academic research. Some studies report that LLMs generate content nearly indistinguishable from humans' content [15,16]. Comparisons between human and Al models typically involve text sourced from humans in a context where they were unaware and had no incentive to appear particularly human (e.g., host profiles) and may thus be unfair to human language ability. Furthermore, we argued that the tasks on which LLMs and humans were compared commonly lack the requirement for empathy."}, {"title": "Humans being humans", "content": "In two experiments, humans produced text that was perceived as more human than texts generated by an LLM under identical instructions. When the writing task required empathy, humans were superior in expressing their humanness (d=0.82 and d=1.01 in Study 1 and 2, respectively) and were perceived as such by other humans. These findings contradict claims about the indistinguishability between human and LLM-generated content [15,16]. Our work also identifies empathy as a driver for human advantage. In other words, when we require humans to use empathy, their ability shines through, and LLMs struggle to keep up. Under such conditions, third persons can differentiate between generated and original human text.\nThe condition most similar to related work [16], where LLMs and humans were reported to be practically indistinguishable, required our participants to merely write a relationship description without any specific instruction or incentive to appear human. Under these conditions, we also find an advantage for the human but to a lesser extent (d=0.46) with the diagnostic value of third- person judgments bordering the chance level (AUC: 0.60 [0.50; 0.70]). This adds evidence for the role of empathy: when none is required, LLMs indeed seem to produce text close to indistinguishable from humans."}, {"title": "LLMs pretending to be humans", "content": "Aside from empathy, awareness of the need to appear human could further help understand how humans express their humanness. Our findings revealed unexpected dynamics when we instructed humans and LLMs to appear as human as possible. Contrary to expectations, this adversarial instruction did not widen the gap between humans and LLMs. Instead, the LLM benefitted from it and was able to adjust and align the generated content so that it approached human-written text. That effect replicated in both experiments and was substantial: humanness judgments increased by 39-41% for LLMs but remained unchanged for texts written by humans. While the human-written texts were nevertheless still judged as somewhat more human, that gap has drastically narrowed with the diagnostic ability to differentiate between human and LLM dropping markedly (Study 1: drop from AUC=0.83 to AUC=0.62; Study 2: drop from AUC=0.88 to AUC=0.62). A potential explanation for why humans failed to convey more humanness could be a ceiling effect implying that humanness was already exhausted without specific instructions, rendering even prompts towards more humanness ineffective. However, if this were the case, we would have expected human-written texts to be rated somewhat more toward the higher end of the 5-point scale on source judgment. The striking finding that LLMs are highly responsive to humanness instructions led us to explore how the attempt to pretend to be human manifested in linguistic traces."}, {"title": "Linguistic traces of humanness", "content": "There is evidence that humans use a mix of aligned and misaligned cues to decide whether a text is LLM-generated [16,24]. For example, in related work, humans correctly associated conversational words with humans but incorrectly expected rare or long word usage from LLMs in previous research [16]. Our work provides conflicting evidence: under the condition where texts were easiest to identify (relationship advice under naive instructions), generated text was aplenty with long words, somewhat rare phrasing (\u201cdear friend\") and generally contained less frequent vocabulary (i.e., rarer words), which third-person assessors may have picked up on. Spelling mistakes also emerged as a cue, with humans making more mistakes than LLMs (see also [16]). However, human judges did not seem to rely on that cue and exploit its diagnostic use. The starkest cue was the use of rare terms by LLMs compared to humans, and our results indicate that humans used this heuristic to some extent but not decisively. Had they been able to perceive and use that cue, humans would have been able to tell LLM from human-written text with near-perfect accuracy.\nUsing term frequency and psycholinguistic analysis offered a view of how our work's adversarial dynamics unfolded. As expected from failure to move the perception score, humans showed no noteworthy linguistic traces when trying to be more human. When LLMs pretended to be human, the strategies revealed a remarkable awareness of what makes a text human, so much so that human judgments increased considerably. The LLM shifted to more conversational language using an informal tone, self-referencing and focusing on the present. The vocabulary was altogether adjusted to use more common words. These changes came at the expense of using long words, appeals to power and affiliation as drives and rather stiff greetings when starting the relationship advice text. These findings align with previous work [16], where human readers used conversational language, self-representations with references to prior experience and a warm tone - rather than monotonous phrasing - to infer humanness. Others have referred to these cues as heuristics humans use to tell generated from original content. Our work indicates that an LLM pretending to be human may have an implicit representation of these heuristics that allow it to convey humanness effortlessly."}, {"title": "Stochastic empathy", "content": "Our findings paint a picture of LLMs approaching empathetic writing abilities comparable to humans. While some cautioned against anthropomorphising [4], others argue that concepts traditionally reserved for psychological science enter the realm of Al research [5]. The current work shows that an LLM can mimic empathetic writing by resorting to a writing style associated with a particular humanness. While this is inevitably a learned statistical representation of empathetic writing \u2013 that is, a stochastic parrot [25] producing seemingly human language without any understanding of what the nature of empathy or the effect of the words generated is [26] - it is noteworthy that the LLM seems to fall back to an implicit representation of humanness. With a simple experimental manipulation, we invoked humanness in LLMs and found no comparable adaptation to that same manipulation in humans. We argue that this is the key finding of our work: while the LLM was merely generating stochastic empathy, it does seem to hold statistical representations of very human language that it used to great effect in conveying humanness."}, {"title": "Limitations and outlook", "content": "Several limitations of the current work are relevant for future work. First, we argued that writing relationship advice is a task that requires empathy \u2013 and found evidence of that in self-reported ratings from participants - but have not systematically manipulated the writing task any further. For example, if empathy is a moderator variable for the humanness of LLM content, we would expect tasks with varying degrees of required empathy to result in varying degrees of humanness."}, {"title": "Conclusion", "content": "Large language models reportedly generate text indistinguishable from humans. We borrowed experimental research approaches from psychology to test whether this finding tolerates conditions favourable to human language understanding. Contrary to our expectation, language models, but not humans, adjusted their writing noticeably when instructed to be particularly human. When humanness was required, the language model seemed to resort to existing representations of empathetic human language as informal, simple, self-referencing and focused on the present."}]}