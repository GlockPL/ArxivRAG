{"title": "Graph Anomaly Detection via Adaptive Test-time Representation Learning across Out-of-Distribution Domains", "authors": ["Delaram Pirhayati", "Arlei Silva"], "abstract": "Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present AdaGraph-T3, a novel test-time training framework for cross-domain GAD. AdaGraph-T3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that AdaGraph-T3 significantly outperforms existing approaches, achieving average improvements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best competing model.", "sections": [{"title": "1 Introduction", "content": "Graph Anomaly Detection (GAD) is a critical task to identify unusual patterns (or outliers) in graph-structured data [24, 2]. This problem has many real-world applications such as in e-commerce [45], social networks [33], fraud detection [12], and cybersecurity [17]. A key limitation of existing GAD models is that they face unique cross-domain challenges that distinguish them from general graph learning tasks. These include inconsistent definitions of normal/anomalous patterns and heterogeneous feature spaces between domains (e.g., Facebook vs. Amazon datasets). These applications would benefit from GAD models capable of adapting across Out-Of-Distribution (OOD) and heterogeneous graphs.\nA major motivation for our work is cybersecurity. Network intrusion detection systems play a key role in identifying malicious network activity associated with cyberattacks [31, 14]. Due to the volume and dynamic nature of these attacks, modern intrusion detection systems increasingly rely on large amounts of data and machine learning to assist cybersecurity experts in detecting potential intrusions. However, one of the major challenges in data-driven cybersecurity is the lack of sufficient labeled data for training supervised models. This is particularly critical for emerging applications, where traces of labeled intrusions are scarce and the ability to leverage attacks or other types of anomalous behavior from existing labeled datasets would be greatly beneficial.\nIn this paper, we investigate the task of unsupervised domain adaptation for GAD focusing on the transfer of knowledge from a labeled source to an unlabeled target domain. While domain adaptation for graphs has been extensively studied [29, 37, 47, 42, 35, 9], cross-domain graph anomaly detection is an emerging area [36, 8]. We propose using Test-Time Training (TTT) [30] to address key challenges in cross-domain graph"}, {"title": "2 Related Work", "content": "anomaly detection. Unlike traditional domain adaptation methods that require continuous access to source data, TTT allows us to distill source knowledge into model parameters and adapt to target domains using only self-supervision. This is particularly advantageous for graph anomaly detection where source data may be sensitive or unavailable during the adaptation phase to new domains.\nOur approach, Adaptive Graph Test-Time Training (AdaGraph-T3), has four major innovations compared to existing work on cross-domain GAD. First, it leverages a homophily-based affinity score [26, 4] for self-supervised learning based on the observation that normal nodes are more similar to their neighbors than anomalous ones, a pattern that remains consistent across domains (see Figure 1). Second, it introduces Normal Structure-preserved Attention Weighting (NSAW), which dynamically learns continuous edge importance weights through the attention mechanism. Third, AdaGraph-T3 applies source and target-specific encoders that are trained end-to-end to handle both distribution shifts and heterogeneous feature spaces. Fourth, to address the extreme class imbalance challenge in GAD [23], our approach employs class-aware regularization during source training-with a stronger regularization to the minority class.\nWe compare our solution against both graph domain adaptation and graph anomaly detection approaches from the literature using multiple cross-domain datasets. For instance, we show that labeled anomalies in the Amazon dataset (source) can improve the GDA accuracy on the Facebook dataset (target). The experiments show that AdaGraph-T3 significantly outperforms the alternatives in most of the settings. The contributions of this paper can be summarized as:\n\u2022 We propose the first test-time training (TTT) framework for cross-domain graph anomaly detection (AdaGraph-T3). TTT enables adaptation to new target domains without requiring direct access to source data or target labels at test time. Our approach leverages homophily-based self-supervision which captures universal patterns, as anomalous nodes typically exhibit lower homophily compared to normal nodes.\n\u2022 Our message-passing technique introduces homophily-based attention weights that naturally suppress anomalous influences on normal nodes while preserving graph structure, enabling robust anomaly detection.\n\u2022 We propose several strategies specifically designed for the cross-domain GAD problem such as data-specific encoders, class-aware regularization, and an effective early-stopping strategy to prevent overfitting during test-time adaptation.\n\u2022 AdaGraph-T3's performance is demonstrated through extensive experiments on cross-network tasks using multiple datasets and baselines that span both the domain adaptation and the anomaly detection literature. Our approach achieves average improvements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best competing model."}, {"title": "2.1 Graph Domain Adaptation (GDA)", "content": "While graph domain adaptation (GDA) methods have shown success in transferring knowledge across different domains [29], they face fundamental limitations when applied to anomaly detection tasks. As traditional GDA methods are designed for node classification scenarios with balanced classes and consistent label semantics.\nTraditional Graph Domain Adaptation (GDA) approaches, such as GRADE [39], AdaGCN [6], and UDA-GCN [40] and spectral regularization methods [44] focus on minimizing distribution shifts across"}, {"title": "2.2 Graph Anomaly Detection (GAD)", "content": "Graph Anomaly Detection (GAD) [24, 2] focuses on identifying abnormal nodes in graph-structured data. Traditional approaches like Oddball [1] rely on power-law relationships between local graph features, while more recent deep learning-based approaches are more generalizable. For instance, DOMINANT [7] employs a graph autoencoder to identify anomalies based on graph reconstruction. ComGA [22] introduces a tailored GCN to learn distinguishable node representations by explicitly capturing community structure. Self-supervised techniques have emerged as powerful tools for GAD, with methods like CoLA [21], SL-GAD [48], HCM-A [11], and TAM [26] introducing various approaches to handle node interactions and structural patterns. While these methods have shown success in single-domain scenarios, they don't address the challenges of cross-domain knowledge transfer. First, anomalies behave differently across domains- fraudulent users in e-commerce networks exhibit different patterns compared to social networks. Second, domains often have different feature spaces and graph structures.\nAdaGraph-T3 bridges both GDA and GAD through a unified approach that combines (1) test-time"}, {"title": "3 Problem Definition", "content": "We address unsupervised node-level anomaly detection with domain adaptation, aiming to identify abnormal nodes in a target graph by leveraging information from both the target and a source graph. The source dataset may contain labeled information but is assumed to be out-of-distribution (OOD) relative to the target, with different feature sets and label distributions. Given source dataset \\(D_s = (G_s, X, Y)\\) and target dataset \\(D_t = (G_t, X_t)\\), where \\(G_s/G_t = (V, E)\\) represents a (source or target) underlying graph with nodes V and edges \\(E \\subset V \\times V\\) such that \\(uv \\in E\\) if there is a link between nodes u and v. Here, \\(X_s = \\{x_v|\\forall v \\in V_s\\}\\) and \\(Y_s = \\{y_v|\\forall v \\in V_s\\}\\) represent the node features and their corresponding labels, respectively, and the same holds for target data \\((X_t, Y_t)\\). Notably, \\(y_v \\in \\{0,1\\}\\) (normal or anomaly) for nodes in both \\(V_s\\) and \\(V_t\\). Our goal is to detect anomalies in the target graph. The key challenge is that target labels are completely unavailable (unsupervised setting), requiring us to leverage source labels \\(Y\\) as auxiliary information, even though source and target features may have different dimensionalities \\((x \\in \\mathbb{R}^{p_s}\\) for source nodes and \\(X_u \\in \\mathbb{R}^{p_t}\\) for target nodes)."}, {"title": "4 Adaptive Graph Anomaly Detection via Test-Time Training (AdaGraph-T3)", "content": "We propose AdaGraph-T3 (Adaptive Graph Anomaly Detection via Test-Time Training), a domain adaptation method particularly designed for graph anomaly detection that integrates attention-based message-passing, node homophily patterns, and domain-specific encoders to handle feature shifts and heterogeneous features across domains. AdaGraph-T3 addresses the challenging and novel scenario where the target data is out of distribution (OOD) relative to the source data and where source and target datasets have different feature spaces.\nAn overview of our approach is provided in Figure 2. AdaGraph-T3 leverages the advantages of Test-time Training, especially the ability to adapt to new target datasets in the TTT phase without the need for labeled anomalies. Moreover, AdaGraph-T3 incorporates technical innovations tailored for cross-domain GAD, including the use of domain-specific encoders, Normal Structure-preserved Attention Weighting (NSAW), and class-aware regularization.\nOur framework operates in two phases. The training phase combines supervised and self-supervised losses to learn from the source dataset. The test-time training phase only uses self-supervised learning to adapt to the target data. We will detail each stage of our solution together with our key contributions in the next sections."}, {"title": "4.1 Backbone GNN and Projection head", "content": "The backbone architecture of AdaGraph-T3 is a Graph Neural Network (GNN) that operates on source and target graph data. Our model can apply any GNN architecture, including Message-passing Neural Networks (MPNNs). We denote the set of neighbors at node v as \\(N(v) = \\{u|(u, v) \\in E\\}\\). For each layer"}, {"title": "4.2 Homophily-based Self-supervised Loss", "content": "Test-time training requires a self-supervised loss \\(L_{self}\\) applied during both training and test-time training stages. We propose the use of a previously introduced homophily-based affinity score, which is tailored for unsupervised anomaly detection [26, 4].\nThe local affinity score measures the similarity or connection strength between a node and its neighbors. It is based on the one-class homophily phenomenon, where normal nodes tend to have a stronger affinity with their neighbors compared with anomalies. This enables these scores to be applied for unsupervised GAD. To the best of our knowledge, our work is the first to apply affinity scores for the GAD problem within the test-time training framework.\nMore formally, the local affinity score is computed by comparing a node's representation against those of its immediate neighbors in the graph using various similarity metrics such as cosine similarity, Euclidean distance, Jensen-Shannon divergence, and Wasserstein distance [20, 5, 28]. By taking the average similarity with neighbors of a node v, we obtain a single anomaly score \\(s(v)\\) based on how well a node is associated with its local graph structure:\n\\(s(v) = \\frac{1}{|N(v)|}\\sum_{u \\in N(v)} sim(h_v^l, h_u^l),\\)\nwhere \\(N(v_i)\\) denotes the neighbors of node \\(v_i\\), \\(h_u^l\\) represents the learned node embedding for u, and \\(sim(a, b) = \\frac{ab}{\\|a\\| \\cdot \\|b\\|}\\) describes the cosine similarity. The unsupervised task consists of maximizing the affinity score for each node.\nThis test-time task is scalable, as it only requires local computations, making it efficient even for large graphs. Additionally, it can be adapted to different types of graphs and node features by choosing appropriate node representation learning techniques and similarity measures. The total self-supervised loss combines the homophily-based loss with a regularization term:\n\\(L_{self} = L_{s(v)} + \\lambda_{reg}L_{reg},\\)\nwhere \\(L_{reg} = \\sum_{i \\in V} (\\sum_{j \\in N_i^c} \\frac{sim(h_i, h_j)}{|N_i^c|^2})\\) minimizes the similarity between non-connected nodes to maintain structural distinctiveness in the embedding space, with \\(N_i^c\\) representing the set of nodes not connected to node i and \\(\\lambda\\) controlling the regularization strength.\nIt has been noted by previous work [26, 4] that enforcing similarity across all connected nodes might inadvertently cause non-homophily nodes to become similar. Normal Structure-preserved Graph Truncation (NSGT) attempts to preserve normal node structure by making binary decisions to remove edges between dissimilar nodes based on distance. However, this approach risks losing important structural information and requires setting explicit thresholds for edge removal decisions. In the next section, we introduce our enhanced message-passing approach through attention-based NSAW, a simpler and more flexible approach to suppress the adversarial effect of non-homophily edges in equation 5."}, {"title": "4.3 Normal Structure-preserved Attention Weighting (NSAW)", "content": "We propose Normal Structure-preserved Attention Weighting (NSAW) as an alternative to Normal Structure-preserved Graph Truncation [26]. NSAW applies a robust extension to graph attention [32] as a more flexible way to suppress the effect of anomalous nodes on the representation of normal nodes during message-passing. These attention weights are learned end-to-end as part of the GNN training process.\nFor each layer l, we compute attention weights between connected nodes using a learnable transformation matrix \\(U^l \\in \\mathbb{R}^{p_l \\times p}\\), where p represents the attention dimension. Given representations \\(H^l \\in \\mathbb{R}^{N \\times p_l}\\) at layer l, the attention scores are computed as:\n\\(A^l = softmax(\\phi(H^lU^l)(\\phi(H^lU^l))^\\top \\odot M),\\)\nwhere \\(\\phi\\) is ReLU activation, M is the adjacency matrix, and \\(\\odot\\) is element-wise multiplication. The use of M ensures that attention weights are only computed between connected nodes in the graph.\nRobust Symmetric Attention. To defend against the adversarial influence of anomalous nodes, we enforce symmetry in the attention weights through a symmetric minimum operation:\n\\(A_{i,j}^l = min\\{A_{i,j}^l, A_{j,i}^l\\}, for all \\(i \\neq j\\)\nNSAW's symmetric construction provides a crucial defensive mechanism through two complementary aspects. First, when a normal node v connects to both normal nodes \\(\\{v_1, v_2, ..., v_k\\}\\) and anomalous nodes \\(\\{a_1,a_2,...,a_m\\}\\), the attention naturally assigns high weights to normal neighbors and low weights to anomalous ones due to feature similarity. As illustrated in the Figure 3, when a normal node assigns low attention \\(\\alpha(v \\rightarrow a) \\approx 0\\) to an anomalous node (thin arrow), even if the anomalous node attempts to assign high attention back (thick arrow), the minimum operation then ensures that the reverse influence is also minimized as \\(\\alpha(a \\rightarrow v) = min(\\alpha(a \\rightarrow v), \\alpha(v \\rightarrow a)) \\approx 0\\) (shown by the dashed arrow), regardless of the original attention weight. Second, through the learnable attention parameters \\(U^l\\) and the self-supervised loss, the architecture gradually learns to suppress the influence of anomalous patterns during training, further strengthening this defensive mechanism through the GNN's message passing structure.\nThe computed attention weights modulate the message passing in the GNN architecture. For a node v, the attention-weighted messages from its neighbors are aggregated as follows:\n\\(h_v^l = M(\\{\\tilde{A}^{l-1}[u, v] \\cdot h_u^{l-1} | u \\in N(v)\\}),\\)\nwhere M(\u00b7) combines the weighted messages using sum aggregation followed by ReLU activation."}, {"title": "4.4 Class-aware Regularization", "content": "Class imbalance poses a major challenge in GAD. As shown in Table 1, the abnormality rate of all datasets we're using is less than 7%. Our class-aware regularization enhances the model's ability to handle class imbalance during the source training phase by applying stronger regularization to the minority class (i.e., anomalous nodes). When minimizing similarity between non-connected nodes, it assigns higher weights to anomalous nodes, ensuring they maintain their distinctive patterns rather than being overshadowed by the majority class representations. Here's the mathematical formulation:\n\\(L_s = \\sum_{i \\in V} \\sum_{j \\in N_i^c} w_j sim(h_i, h_j)\\)\nwhere \\(N_i^c\\) represents the set of nodes not connected to node i, \\(sim(h_i, h_j)\\) is the cosine similarity between node embeddings, and \\(w_j\\) is the class-based weight defined as:\n\\(w_j = \\begin{cases} \\alpha & \\text{if node j is anomalous} \\\\ 1 & \\text{otherwise} \\end{cases}\\)\nwhere \\(\\alpha > 1\\) is the weighting factor for anomalous nodes. We set \\(\\alpha\\) inversely proportional to the class ratio This regularization can only be applied during source training where we have access to node labels, while target training relies solely on the homophily-based loss due to its unsupervised nature."}, {"title": "4.5 Test-Time Training", "content": "We can now describe our framework for anomaly detection with domain adaptation (AdaGraph-T3), summa- rized in Figure 2. Following the test-time training paradigm, AdaGraph-T3 is based on two tasks, the main (supervised) task and the auxiliary self-supervised learning (SSL) task. In the training phase, AdaGraph-T3 is trained based on both tasks using a joint loss minimized based on the source dataset. Subsequently, the test-time training phase leverages only the SSL task using the target dataset, which is assumed to be unlabeled.\nTraining phase. During training phase, AdaGraph-T3 applies the source graph \\(G_s\\) to learn an encoder \\(\\theta_s\\), a decoder \\(\\theta_m\\), and a predictor \\(\\theta_{pred}\\) by minimizing a loss function \\(L_{train}\\) that is a combination of a supervised loss \\(L_{sup}\\), for which we apply cross-entropy, and a self-supervised loss \\(L_{self}\\), for which we apply the homophily-based affinity scores introduced in Section 4.2:\n\\(L_{sup} = \\sum_{v \\in V_s} y_v log \\hat{y}_v + \\lambda_s L_s\\)\n\\(L_{self} = \\frac{1}{|N(v)|} \\sum_{u \\in N(v)} sim(h_v^l, h_u^l) + \\lambda_{reg}L_{reg}\\)\n\\(L_{train} = \\sum_{v \\in V_s} (L_{sup}(v, y; \\theta_m, \\theta_s, \\theta_{pred}) + \\lambda L_{self}(v; \\theta_m, \\theta_s))\\)\nwhere y is the predicted label (normal or anomaly) and \\(\\lambda, \\lambda_s\\), and \\(\\lambda_{reg}\\) are weight hyperparameters. We describe the source encoder \\(\\theta_s\\) (projection) and decoder \\(\\theta_m\\) (GNN) in Section 4.1. The predictor \\(\\theta_{pred}\\) is a multi-layer perception that maps node embeddings to label predictions."}, {"title": "5 Experiments", "content": "We will compare the proposed approach (AdaGraph-T3) against multiple baselines using six graph datasets. Code availability: Our source code is available for reproducibility purposes as an anonymous repository.\u00b9"}, {"title": "5.1 Experimental Setup", "content": "Datasets. We apply six datasets from diverse domains such as online shopping reviews, including Amazon (AMZ) [25], YelpChi [27], YelpHotel (HTL), and YelpRes (RES) [8], and social networks, including Reddit (RDT) [16] and Facebook (FB)[18]. For our cross-domain analysis, we examine scenarios where feature spaces are homogeneous (same features) and heterogeneous (same features) across domains. Table 1 presents a summary of the dataset statistics. A detailed description of each dataset is provided in Appendix B."}, {"title": "5.2 Cross-domain Graph Anomaly Detection", "content": "We present the results comparing our approach against various domain adaptation baselines in Table 2. We use different datasets as source and target domains. We incorporated a projection head into all baseline models to address feature sets' mismatches across source and target domain datasets. This addition enables fair comparison by allowing each method to handle the discrepancy in feature spaces.\nOur proposed method (AdaGraph-T3) outperformed the baselines in most scenarios. AdaGraph-T3 achieved the highest AUROC and AUPRC scores in 5 out of 6 tasks. For instance, in terms of AUROC,"}, {"title": "5.3 Representation Embedding", "content": "The effectiveness of our test-time training approach is visually demonstrated in Figure 4, which presents the two-dimensional Kernel PCA (polynomial kernel) embeddings of the GNN outputs for both source and target models. Importantly, we have used the same embedding space derived from the source data to project the target data, enabling the mapping of model predictions across domains.\nOn the left side, we observe a clear separation between normal and anomalous samples in the source domain (Amazon), indicating that the source model has effectively learned to distinguish between these classes. The right figure shows the transferability of our approach to the target domain (Reddit). Despite the inherent differences between source and target datasets, the decision boundary learned from the source domain maintains its discriminative power when applied to the target, demonstrating that our test-time training strategy successfully adapts the pre-trained source model to the new domain.\nWe provide additional visualizations of source and target embeddings for other datasets in Appendix E."}, {"title": "5.4 Early Stopping", "content": "This section evaluates our MMD ratio-based early-stopping strategy across different domain adaptation scenarios. Figure 5 illustrates how the MMD ratio evolves together with the Area Under the Precision-Recall Curve (AUPRC) during training. We marked the specific epoch when the model was selected. The results show that the ratio effectively captures the convergence of domain adaptation without the need for labeled validation data."}, {"title": "6 Ablation Studies", "content": "We perform four ablation studies to evaluate different aspects of AdaGraph-T3. The results support the design of our model and demonstrate the relevance of each of its components.\nGAD Results (Source-free). We evaluate a single-domain version of AdaGraph-T3 against six state-of-the- art GAD baselines using four datasets. Our goal is to isolate the impact of GATD3's self-supervised learning from its cross-domain adaptation. The results are shown in Table 3. We follow the same protocol and use the same datasets as [26], so we report their results for the baselines. AdaGraph-T3 outperforms the GAD baselines across most of the datasets.\nNSAW as a learnable and flexible edge weighting approach. We proposed NSAW as a learnable edge- weighting mechanism to suppress the adverse effect of non-homophily edges in anomaly detection. Table 3 compares our method against TAM [26] (using Normal Structure-preserved Graph Truncation). The results show that AdaGraph-T3 outperforms TAM using most datasets. This is evidence that NSAW is a more flexible alternative to NSGT by allowing GATD3 to learn continuous attention weights.\nClass-aware regularization. In Table 5, we compare the performance of AdaGraph-T3 with and without the class-aware source regularization approach described in 4.4. The results show that the regularization helps increase the sensitivity of the source model to anomalous nodes. Regularization improves the accuracy of the model in most settings. In particular, the results show that regularization improves the performance in terms of AUPRC, which is better at capturing class imbalance than AUROC."}, {"title": "7 Conclusion", "content": "In this paper, we have introduced AdaGraph-T3, a novel test-time training framework for graph anomaly detection when the testing data is out-of-distribution and from heterogeneous application domains. Specifically, our approach addresses scenarios where the distributions and feature spaces differ between the source and target datasets using dataset-specific encoders. AdaGraph-T3 combines the advantages of test-time training with multiple innovations focused on graph anomaly detection. For instance, by leveraging the distinctive distribution pattern of node homophily between normal and anomalous nodes, we have presented a tailored SSL task along with a robust attention-based edge-weighting mechanism (NSAW) to enhance the generalization of the learned representations. Moreover, AdaGraph-T3 performs model election using an MMD-based early-stopping criterion. Our experiments have illustrated that AdaGraph-T3 outperforms state-of-the-art graph domain adaptation baselines.\nOur work opens several avenues for future investigation. We are interested in applying AdaGraph-T3 to cybersecurity applications, where label scarcity and distribution shifts are a challenge. Moreover, we want to understand how explainers trained in the source domain can improve the interpretability of target anomalies. Finally, we will study theoretical guarantees for cross-domain GAD based on the similarity between source and target datasets."}, {"title": "A Early stopping criterion", "content": "We propose an adaptive early-stopping strategy that monitors distribution shifts between source and target domains using Maximum Mean Discrepancy (MMD). Let \\(f_t \\in \\mathbb{R}^{n_t \\times P}\\) denote the target embedding, \\(f_r^n \\in \\mathbb{R}^{n_n \\times P}\\) and \\(f_r^a \\in \\mathbb{R}^{n_a \\times P}\\) denote the normal and attack source embeddings respectively, where \\(n_t, n_n, n_a\\) are the number of samples and p is the common feature dimension. The MMD ratio score at epoch t is computed as:\n\\(MMD_n(t) = MMD(f_t, f_r^n)\\)\n\\(MMD_a(t) = MMD(f_t, f_r^a)\\)\n\\(score(t) = \\frac{1}{\\nu} \\sum_{v \\in V} \\frac{max(MMD_n(t), MMD_a(t))}{min(MMD_n(t), MMD_a(t))}\\)\nFor target attack samples, \\(MMD_n(t)\\) would be larger than \\(MMD_a(t)\\), making the score \\(MMD_n(t)/MMD_a(t)\\), which increases as samples align better with attack source features. Conversely, for target normal samples, the score becomes \\(MMD_a(t)/MMD_n(t)\\), increasing as samples align with normal source features. The training stops at epoch T if:\n\\(score(t) < score^*(t - \\tau) \\,\\, \\forall t \\in [T - \\tau + 1, T]\\)\nwhere \\(score^*(t)\\) is the best score up to epoch t and \\(\\tau\\) is the patience parameter. A higher score indicates better domain adaptation as it represents a stronger alignment with the correct source class features."}, {"title": "B Data description", "content": "Amazon [25]: The Amazon dataset consists of product reviews from the Musical Instruments category. Users with more than 80% helpful votes are labeled as benign entities, while those with less than 20% helpful votes are considered fraudulent entities. For each user (represented as a node in the graph), 25 handcrafted features are used as raw node features. The graph structure is defined by the U-P-U (User-Product-User) relation, which connects users who have reviewed at least one common product.\nReddit [16]: The Reddit dataset consists of forum posts from the Reddit platform, focusing on user behavior and content. In this dataset, users who have been banned from the platform are labeled as anomalies. Each post's textual content has been vectorized to serve as one of the 64 attributes for the corresponding user or post.\nFacebook [18]: The Facebook dataset represents a social network structure derived from the Facebook platform. Users establish connections with other users, forming a network of relationships. Fraudulent users are assumed to be anomalies of the network. Each node represents one user with 576 node attributes.\nYelpChi [27]: The Yelp spam review dataset comprises hotel and restaurant reviews, categorized as either filtered (spam) or recommended (legitimate) by Yelp. We utilize 32 handcrafted features as raw node features for each review in the Yelp dataset. In the graph the reviews serve as nodes. The graph's structure is defined by the R-U-R (Review-User-Review) relation, which connects reviews posted by the same user.\nYelpHotel [8]: A graph dataset focused on hotel reviews from Yelp, where users and hotels are nodes connected by review edges. Each review contains ratings, text, and detailed metadata about both hotels and reviewers.\nYelpRes [8]: A graph dataset containing restaurant reviews from Yelp, where users and restaurants are nodes connected by review edges. Each review has ratings, text, and metadata about both users and restaurants"}, {"title": "C Cross-domain performance", "content": "Tables 6 and 7 present the cross-domain and in-domain adaptation performance of AdaGraph-T3. The results are shown using AUROC (%) and AUPRC (%) metrics, respectively. We evaluated every possible combination of four real-world datasets (Amazon, Facebook, YelpChi, and Reddit) as source and target domains."}, {"title": "D Transfer performance with respect to source-target homophily", "content": "Our analysis reveals that the direction of transfer plays a crucial role in cross-domain anomaly detection performance. Transferring from a higher homophily domain to a lower homophily domain generally results in better performance improvements. This suggests that when the source domain has higher homophily, the homophily-based test-time training loss function is more effective at guiding the model's adaptation to the target domain. For example, transferring from Reddit (higher homophily) to Facebook (lower homophily) achieves significantly better performance than the reverse direction. This finding provides practical guidance, indicating that domains with stronger homophilic structures may serve as better source domains for transfer learning in cross-domain anomaly detection tasks (see Figure 6)."}, {"title": "E Additional embedding visualizations", "content": "In Figure 7, we show 2D embeddings of the learned model for different pairs of source and target (similar to Figure 4)."}]}