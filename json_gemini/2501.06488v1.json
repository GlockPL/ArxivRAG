{"title": "NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References", "authors": ["Qiang Qu", "Yiran Shen", "Xiaoming Chen", "Yuk Ying Chung", "Weidong Cai", "Tongliang Liu"], "abstract": "Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the \"same instance, similar representation\" assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).", "sections": [{"title": "I. INTRODUCTION", "content": "PHOTOREALISTIC view synthesis has emerged as a cornerstone in modern computer vision, bridging the gap between captured imagery and artificially rendered content for a wide array of applications such as film production, telepresence, robotics, and digital content creation [1]\u2013[3]. By generating novel viewpoints from existing data, recent Neural View Synthesis (NVS) techniques, including Neural Radiance Fields (NeRF) [1] and 3D Gaussian Splatting [4], have demonstrated remarkable promise in producing highly detailed and consistent scenes. This rapid progress underscores a growing imperative to rigorously evaluate the perceptual quality of neurally synthesized scenes (NSS), thereby guiding the development of robust NVS methods and advancing our understanding of how humans perceive visual content [5]\u2013[7].\nHowever, assessing the quality of NSS poses challenges, requiring comprehensive evaluations of spatial fidelity, view-to-view consistency, and perceptual quality [5] (shown in Fig. 1). Current assessment approaches predominantly employ full-reference image quality methods, such as PSNR, SSIM [8], and LPIPS [9]. These methods require testers to designate a subset of views as reference images, against which NVS-generated views are compared, to evaluate the quality of a NSS. However, the scarcity of reference images for dense views further exacerbates the challenge of conducting a thorough quality assessment (as demonstrated in Fig. 2). For example, datasets like LLFF [10] and DTU [11] provide only a limited number of reference views, inadequate for the evaluation of the densely synthesized views. Consequently, this limitation underscores the increasing necessity for no-reference methods in the quality assessment of NSS. Although no-reference methods are more relevant in practical scenarios, they face significant challenges, as they cannot directly evaluate the fidelity of the synthesized views against the references [12]-[14]."}, {"title": "II. RELATED WORK", "content": "Quality assessment in multimedia content has been a critical area of research, with methodologies broadly categorized into full-reference and no-reference approaches, depending on whether they require access to the original, unaltered media for comparison [6], [15], [16]. Full-reference methods, as the name suggests, rely on a complete reference media for quality score prediction. Conversely, no-reference methods assess quality independently of the original media, a necessity in scenarios where the reference is unavailable or inapplicable [6], [16]. This distinction is particularly pertinent in Novel View Synthesis quality assessment, where datasets like LLFF present challenges for full-reference evaluations due to their sparse image availability [10]. Our focus, therefore, is on advancing no-reference quality assessment techniques.\nImage quality assessment. The field of image quality assessment (IQA) has seen extensive exploration, yielding a variety of full-reference metrics tailored for 2D images. These include PSNR, SSIM [8], MS-SSIM [17], IW-SSIM [18], VIF [19], FSIM [20], GMSD [21], VSI [22], DSS [23], HaarPSI [24], MDSI [25], LPIPS [9], and DISTS [26]. These methods range from PSNR, which quantifies image reconstruction quality by comparing signal power against corrupting noise, to SSIM and its variants that evaluate perceptual aspects like structural integrity and texture. Advanced metrics like LPIPS leverage deep learning to capture nuanced visual discrepancies, offering insights into perceptual quality beyond traditional methods [9].\nFor no-reference IQA, techniques such as BRISQUE [12], NIQE [27], PIQE [28] and CLIP-IQA [14] assess image quality by analyzing inherent image properties, with BRISQUE, for instance, evaluating naturalness degradation through local luminance statistics [12]. The most recent no-reference quality assessment methods include CONTRIQUE [29], and Re-IQA [30]. CONTRIQUE [29] trains a deep Convolutional Neural Network (CNN) to learn robust and perceptually relevant representations, which a linear regressor then maps to quality scores in a no-reference setting. Similarly, Re-IQA introduces a Mixture of Experts approach, training two separate encoders to capture high-level content and low-level image quality representations achieving state-of-the-art performance on diverse image quality database [30].\nVideo Quality Assessment. The assessment of video quality extends the principles of IQA to dynamic visual content, incorporating no-reference video quality assessment (VQA) methods such as Video-BLIINDS [31], VIIDEO [32], FAST-VQA [33], FasterVQA [34], DOVER [13], DOVER-Mobile [13] as well as full-reference methods such as STRRED [35], VMAF [36], and FovVideoVDP [37]. FAST-VQA introduces an efficient, end-to-end deep framework for VQA with a novel fragment sampling strategy, while FasterVQA further enhances computational efficiency and accuracy. DOVER disentangles the aesthetic and technical dimensions of user-generated content, offering an effective measure of perceived video quality. Meanwhile, VMAF fuses multiple metrics to align closely with human perception, and FovVideoVDP adopts a viewer-centric approach by accounting for gaze position and foveation. These techniques can be readily applied to NSS, treating synthesized view sequences in a manner analogous to video streams, thereby providing deeper insight into the perceptual quality of neurally rendered scenes.\nLight-field quality assessment. Light-field imaging (LFI) introduces an angular dimension to visual content, necessitating specialized assessment methods (LFIQA) such as NR-LFQA [38], Tensor-NLFQ [39], ALAS-DADS [15] and LFACon [16]. ALAS-DADS innovates with depthwise and anglewise separable convolutions for efficient and comprehensive quality assessment in immersive media [15]. LFACon further refines this approach by incorporating anglewise attention mechanisms, optimizing for both accuracy and computational efficiency in evaluating light-field image quality [16]. These methodologies are particularly relevant for NSS, where synthesized views can be organized into a light-field matrix, mirroring the angular diversity of traditional light-field cameras.\nNSS quality assessment. The NSS quality assessment has predominantly relied on full-IQA methods, including PSNR, SSIM, and the perceptually-driven LPIPS [1], [40]\u2013[45]. These methodologies facilitate a direct comparison between the synthesized images and their reference counterparts, serving as a metric for evaluating both similarity and perceptual quality. However, as highlighted in Section I, such methods exhibit a bias towards the reserved reference views and fail"}, {"title": "III. METHODOLOGY", "content": "As illustrated in Fig. 4, the proposed self-supervised, no-reference quality assessment methodology encompasses two primary stages: the self-supervised quality representation learning stage and the perceptual quality estimation stage.\nThe self-supervised quality representation learning stage aims to train a neural network capable of generating effective quality representations without the need for reference views or human perceptual annotations. During this stage, unlabeled NSS undergo a contrastive pair preparation process (elaborated in Section III-C) to create diverse pairs for subsequent training. These pairs are then feed to AdaptiSceneNet, a neural network designed for compatibility with varying scenes (detailed in Section III-D), which processes the pairs separately but under shared weights to produce quality representation pairs. We introduce a multi-branch guidance adaptation (described in Section III-B) to direct the quality representation learning process.\nIn the perceptual quality estimation stage (depicted on the bottom of Fig. 4), the pretrained AdaptiSceneNet is employed on new NSS with its weights frozen. The perceptual quality estimation module then applies linear regression to map the quality representations output by AdaptiSceneNet to human perceptual scores. It is crucial to highlight that the quality representations are empirically validated to generalize across different scenes and NVS methods. This generalization capability is shown in the distinctiveness between the unlabeled dataset used for pretraining in the first stage and the dataset utilized for evaluation in the second stage, particularly in terms of scene variety and the NVS methods employed for scene generation."}, {"title": "B. Multi-branch guidance adaptation for quality representation learning", "content": "Self-supervised learning needs appropriate objectives that enable the model to generate representations wherein the inputs sharing similar attributes yield similar representations [47]. For instance, in semantic representation learning scenarios, crops derived from the same image are expected to possess similar representations [47]\u2013[49]. However, the assumption significantly diverges when applied to learning quality representations for NSS. It is not safe to presume that crops or clips from the same NSS instance share similar perceptual qualities. This is demonstrated by Fig. 5 from the Fieldwork dataset [5], which shows that the quality of clips can vary significantly within the same NSS. One possible explanation is that the additional dimensions, compared to 2D images, introduce greater variability in quality across different clips. This is supported by the statistics of inter-NSS and intra-NSS quality shown in Fig. 6, where more than 70% of NSS exhibit greater internal quality variance than the quality variance observed across different NSS in dataset LLFF [10]. A deeper reason is that NVS methods are more susceptible to overfitting on the sparse training views provided in each scene [1], [40], [42], [50]. This can lead to higher perceived quality in synthesized views that are closer to the training views. This observation raises a critical question: if the commonly held \u201csame instance, similar representation\" assumption is not valid, what can we rely on for self-supervised learning? Another issue is that the effectiveness of self-learned representations hinges largely on the availability of a large corpus of images [29], [30], [47]. This requirement makes self-supervised learning more challenging for training with limited NSS datasets, where the volume of training examples may be insufficient to achieve generalized representation.\nTo address these issues, we design multiple objectives for quality representation learning. The key idea is to utilize quality scores from various full-reference quality assessment methods, along with the proportion of replaced views in an NSS as complementary heuristic cues (detailed in Section III-C)."}, {"title": "Different from widely used discrete positive/negative labels for contrastive pairs", "content": "Different from widely used discrete positive/negative labels for contrastive pairs [29], [30], [47], [49], this approach provides continuous and comprehensive targets, enhancing the effectiveness of learning. Another advantage of this approach is that, unlike popular InfoNCE-like loss functions [29], [30], [47]\u2013[49], it does not require loading a batch of negative pairs alongside a positive pair for each gradient update step (as shown in the equation 1), and hence reduce memory usage. This efficiency is particularly relevant in the context of the extensive data volumes associated with NSS.\nA workflow of the proposed multi-branch guidance adaptation is illustrated in the right segment of Fig. 4. The quality representations are passed through three distinct non-linear projectors: an IQA-guided branch, a VQA-guided branch, and a replacement ratio branch. The IQA-guided branch utilizes insights from full-reference IQA methods to assess the static quality of NSS, while the VQA-guided branch employs full-reference VQA methods for evaluating dynamic quality aspects. Specifically, a lower IQA or VQA score for a contrastive pair implies a greater disparity in quality. The replacement ratio, termed as REP, serves as a complementary heuristic, suggesting that a higher proportion of view replacements in NSS likely results in more distinct representations. Consequently, our objective function is formulated to reflect that greater differences in replacement ratios, VQA scores, or IQA scores should correspond to more divergent representations (i.e., less similarity). Formally, given a set of contrastive pairs, {(81,8%)}1, and denoting the model as H, we define each branch, \u03c8 \u2208 \u03a8 := {IQA, VQA, REP}, of learning loss as follows:\nl(H) = | sim [\u03c4\u03c8 \u00b7 \u0397(s\u2081), \u03c4\u03c8 \u00b7 H(s)] \u2013 V(si, s) |,\nwhere sim(u, v) = uTv/||u||||v|| denotes the cosine similarity between 12 normalized vectors u and v, and \u03c4\u03c8 represents the non-linear MLP projector [47] corresponding to each branch. The output from IQA, VQA methods, and the replacement ratio, REP, are rescaled to the cosine similarity value range of [-1,1].\nManual Branch Weighting (MBW). An intuitive approach to learning in a multi-branch scenario involves manually assigning weights and experimenting with different weight combinations to find the optimal configuration. The objective is thus to learn an optimal representation model H* such that:\nH* = arg min \u03a3\u03a3 (\u03bb\u03c8 1 (\u0397)),\nwhere \u03bb\u03c8 denotes the weight assigned to each loss component, adjustable through tuning. This formulation underscores our strategy to adaptively learn quality representations from NSS, leveraging a combination of heuristic cues and quality assessment insights. We performed a grid search over a predefined search space to identify the optimal weights, as demonstrated in the ablation studies in Section IV-D. The optimal results of MBW are also presented in the experimental results in Section IV."}, {"title": "Adaptive Quality Branching (AQB)", "content": "Adaptive Quality Branching (AQB). The MBW approach requires expensive training for each weight combination and makes it challenging to find the optimal configuration within a limited discrete search space. To address this, we adopt an automatic weighting method, AQB, inspired by [51], that converges toward an optimal configuration efficiently. The likelihood for each branch, \u03c8 \u2208 \u03a8, can be defined using a Gaussian distribution:\np(\u03c8(s1, s2)|dH (s1, s2)) = N (dH(s1, s2), \u03c3\u03c8),\nwhere dH(s1,s2) := sim [\u03c4\u03c8 \u00b7 H(s1), \u03c4\u03c8\u00b7 H(s2)] represents the estimated similarities between the embeddings of contrastive pairs {(s1,s2)} given a representation model H, and \u03c3\u03c8 is the observed branch-dependent noise. The multi-branch probabilistic model is then given by:\np(\u03a8|dH) = p({\u03c8(s1,s2)}|dH(S1, S2))\n\u03a8\n= \u03a0 p(\u03c8(s1, s2)|dH (s1, s2))\n\u03c8 \u03a8\n= \u03a0 N (dH (s1, s2), \u03c3\u03c8).\n\u03c8\nThe log-likelihood for the joint multi-branch probabilistic model [52] can then be written as:\nlogp(\u03a8|dH) \u03b1 -\n\u03a8\n1\n2\u03c3\u03c8 2\n||\u03c8(s1, s2) \u2013 dH (s1, s2)||2\n+ log \u03c3\u03c8,\nThe learning objective can then be formulated as minimizing the negative log-likelihood:\nH* = arg min \u03a3\u03a3 1(\u0397, \u03c3\u03c8),\nwhere the loss function for each branch is define as:\n1(\u0397, \u03c3\u03c8) =\n1\n2\u03c3\u03c8 2\nsim (\u03c4\u03c8\u00b7 H(s1), \u03c4\u03c8\u00b7 H(s2))\n-V(s1, s2) + log \u03c3\u03c8,\nwhere \u03c3\u03c8 represents the learnable noise parameter for each branch, the term 2 dynamics dynamically scales the loss of each branch based on its noise, and the term logo serves as a regularizer to prevent \u03c3\u03c8 from becoming too large.\nBy integrating branch-wise noise into the loss function, we ensure that the model adaptively balances the learning process across different branches, reducing the dominance of any single branch and enabling optimal performance across all branches. The refined objective function properly weights each loss component by its corresponding noise, leading to a balanced and robust training process that leverages the shared quality representations more effectively. The optimal results for both MBW and AQB are presented in the experimental results in Section IV-D."}, {"title": "C. Contrastive pair preparation", "content": "The preparation of pairs is crucial for acquiring robust representations, particularly in semantic representation learning. For instance, SimCLR [47] utilizes data augmentation techniques such as random cropping, color distortion, and Gaussian blur, designating crops from the same image instance as positive pairs and others as negative. However, these methods may not be sufficient within the context of NSS. Given the limited availability of NSS, the likelihood of generalizing self-supervised learned representations is reduced, highlighting the need for a more varied set of contrastive pairs from a restricted dataset. As discussed in Section III-B and illustrated in Fig. 5, the \"same instance, similar representation\" assumption proves unreliable for quality representation learning in NSS, making traditional positive/negative partitioning inappropriate.\nTo address these challenges, we have developed an algorithm for the preparation of contrastive pairs specifically tailored to NSS, as depicted in the leftmost block of Fig. 4. According to the objectives outlined in Section III-B, we do not classify pairs as positive or negative, allowing us to create NSS pairs at any quality distance and maximize diversity by applying varying levels of distortion. We further randomize crops and clips to ensure the model accommodates NSS with different numbers of views and spatial resolutions. Additionally, we randomly replace views with those from another NVS method to enhance variability. Importantly, to facilitate the effectiveness of quality representation learning, we ensure that each NSS pair represents the same scene, thereby controlling for semantic consistency.\nFormally, the proposed pair preparation algorithm is outlined in Algorithm 1 titled Preparing Contrastive Pairs. Each pair, maintaining identical semantic information, captures variations in scene quality through controlled distortions and modifications. Starting with an unlabeled dataset D and a target of N pairs, the algorithm creates a set P of pairs. It selects an NSS randomly from D, applies random cropping and orientation adjustments to form a base scene 81, and then duplicates, distorts, and partially replaces views in 81 with those generated by an alternate NVS method to produce its counterpart 82. These steps introduce necessary variability and contrast, essential for training models to discern subtle visual differences. The algorithm also adjusts the number of views and spatial resolutions randomly, ensuring the model's compatibility with various NSS sizes. The replacement ratio, REP, moderates the disparity between 81 and 82, serving as a heuristic cue for self-supervised learning (as detailed in Section III-B). By repeatedly forming such pairs, the algorithm compiles a diverse set P ready for subsequent self-supervised learning."}, {"title": "D. AdaptiSceneNet", "content": "As mentioned in the previous sections, NSS encompass a wide range of views with varying spatial resolutions, requiring a versatile model architecture to accommodate these differences effectively. To this end, we developed AdaptiSceneNet, which integrates spatial convolutions with Transformer encoders [53], as depicted in Fig. 4. AdaptiSceneNet comprises two primary modules: the Viewwise Multi-Scale Quality Extraction module and the Anglewise Quality Feature Fusion module. The Viewwise Multi-Scale Quality Extraction module utilizes several residual convolutional blocks [54], refining output features through additional convolutions to capture multi-scale quality features from each view, from low to high-level attributes. This methodical approach to feature extraction, inspired by LPIPS [9], ensures a thorough quality analysis across scales and includes an adaptive average pooling layer to handle varying resolutions [55]. The Anglewise Quality Feature Fusion module employs positional encoding and several layers of transformer encoding, to effectively integrate these quality features within the angular domain."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Unlabeled dataset for self-supervised learning. Our model was trained on a compact dataset derived from Nerfstudio [56], encompassing 17 scenes, each synthesized using eight NVS methods and the respective variants. These included Ten-soRF [57], Instant-NGP [50], K-Planes [58] (with the far value set to 20, 100, 1000), Nerfacto-default [56], Nerfacto-huge [56], and 3D Gaussian Splatting [4]. This selection was strategically made to encompass NVS methods that diverge significantly from those employed within the evaluation dataset, thereby examining the proposed method's capability to generalize across different NVS methods. The scenes within this dataset vary considerably in their complexity, with the number of views per scene ranging from 120 to 600. This diversity in scene composition and viewpoint count serves to test the adaptability and generalization of our self-supervised learning model across a broad spectrum of NVS-generated content.\nLabeled datasets for evaluation. The evaluation of NVS-SQA leveraged three distinct labeled NSS datasets-Lab [5], LLFF [10], and Fieldwork [5] each presenting unique challenges. The Lab dataset features six real scenes captured with a 2D gantry system in a laboratory setting, offering a uniform grid of training views and reference videos ranging from 300 to 500 frames for a thorough quality assessment. In contrast, the LLFF dataset comprises eight real scenes captured with a handheld cellphone, providing a sparse selection of 20-30 test views per scene, with positional data computed via COLMAP [46]. The Fieldwork dataset includes nine real scenes from diverse environments, such as outdoor urban landscapes and indoor museum settings, characterized by intricate backgrounds and variable lighting conditions, with reference videos typically containing around 120 frames. To synthesize NSS for each scene, ten NVS methods were utilized, spanning a variety of models with explicit and implicit geometric representations, rendering models, and optimization strategies, ensuring an unbiased evaluation of the framework's generalizability. Moreover, methods such as NeRF [1], Mip-NeRF [40], DVGO [41], Plenoxels [42], NeX [44], LFNR [45], IBRNet [59], and GNT [60] were involved, including both cross-scene (GNT-C and IBRNet-C) and scene-specific (GNT-S and IBRNet-S) models. The human perceptual labels were collected in a comparison-based manner and in format of Just-Objectionable-Difference (JOD) units [5]. It is important to highlight that the JOD scale's utility and the comparative analysis it facilitates are particularly pronounced when conducted within the context of individual scenes [5].\nEvaluation protocol. We largely adhere to the evaluation protocol from [16], [29], [30], but in a more stringent manner. Our evaluation consists of two primary objectives: first, to determine if the proposed method can simultaneously adapt to three different datasets; second, to test the method's ability to perform cross-dataset validation. Following the self-supervised learning phase, the model's weights are frozen, and the model is applied to the three evaluation datasets to generate quality representations. A one-time linear regression is performed on a randomly sampled half of the integrated dataset to align these quality representations with human scores. The model's performance is then evaluated on the remaining half of the dataset to assess the generalizability of the NVS-SQA framework across the integrated datasets. This approach is designed to thoroughly evaluate the framework's capability to adapt and perform effectively amidst the varied complexities posed by the Lab, LLFF, and Fieldwork datasets simultaneously. Additionally, we conduct cross-dataset experiments to further assess the adaptability of the proposed method across various datasets.\nTraining setup. The training regimen for the model was executed using the ADAM optimizer [64], spanning 200 epochs with a batch size of 16. Consequently, once the training on an unlabeled dataset is complete, the model is adept at being applied proficiently to novel scenes across varied datasets after minor linear regression. The computational experiments underpinning this research were performed on a high-specification desktop equipped with an AMD 5950X processor, an RTX 4090 GPU, and 128GB of RAM, running on the Windows 10 operating system. The implementation was carried out in PyTorch [65]."}, {"title": "B. Comparison with no-reference quality assessment methods", "content": "We benchmarked our approach against a diverse set of no-reference quality assessment methods spanning multiple domains, including IQA (TV [66], BRISQUE [12], NIQE [27], PIQE [28], CLIP-IQA [14], CONTRIQUE [29], Re-IQA [30]), VQA (VIIDEO [32], Video-BLIINDS [31], FAST-VQA [33], FasterVQA [34], DOVER [13], DOVER-Mobile [13]), and LFIQA (NR-LFQA [38], Tensor-NLFQ [39], ALAS-DADS [15], LFACon [16]).\nThe analysis underscores the superior performance of NVS-SQA across all datasets. Within the Fieldwork dataset, NVS-SQA achieved an increment of 32.3% in the SRCC, amounting to 0.222, relative to its nearest competitor. In the LLFF dataset, it demonstrated an improvement, with a 69.7% increase in SRCC (0.287) and a 92.3% rise in the KRCC (0.300), compared to the second-best method. Similarly, for the Lab dataset, NVS-SQA recorded an enhancement, with SRCC increasing by 226% (0.486) and PLCC by 218% (0.465). Moreover, NVS-SQA exhibits the smallest standard deviation across most of the metrics, demonstrating its stability relative to other methods."}, {"title": "Cross-dataset evaluation.", "content": "To further assess the generalization of the proposed method, we conducted a cross-dataset evaluation, the results of which are detailed in Table II. Specifically, in this setup, the model is regressed on two datasets (A and B) and then tested on the remaining one (C). For example, the Fieldwork results are obtained by regressing on LLFF and Lab. Comparing these results with those from Table I indicates that the performance of the proposed method does not plunge during cross-validation, and even further widens the performance gap with the second-best in some cases. In two datasets, the method's performance relative to the second-best quality assessment method shows even greater improvement. Specifically, in the Fieldwork dataset, NVS-SQA achieved an increase of 39.4% in the SRCC and a 44.8% enhancement in the KRCC compared to the second best. Similarly, in the LLFF dataset, it recorded a 215% rise in KLCC."}, {"title": "C. Comparison with full-reference quality assessment methods", "content": "No-reference quality assessment is more challenging than full-reference methods, as it evaluates content without an original reference [16], [31], [35]. This requires advanced algorithms to infer quality metrics directly from the content, simulating human perception without reference points [15], [27], [32]. Consequently, developing no-reference methods demands a deeper understanding of perceptual quality and more sophisticated computational models [16], [29], [31].\nAlthough NVS-SQA does not require references, we compared it with various full-reference quality assessment methods to determine if its performance is comparable to those that rely on references."}, {"title": "D. Ablation study", "content": "We first apply manual weighting MBW (as described in Section III-B) with a comprehensive grid search to ascertain the optimal combination of weights for the learning objectives, as specified in Equation 2, within the search space {0, 0.1, 0.2, 0.5, 1, 1.5, 2}. The search culminated in the identification of the most effective weight combination:\n{XIQA : 1.5, XVQA : 1, AREP : 0.2}. We then applied the auto-weighting method AQB (introduced in Section III-B) to evaluate its effectiveness."}, {"title": "E. Comparison with the fully supervised method", "content": "Furthermore, we conducted a comparative analysis between the proposed NVS-SQA framework and our previously introduced fully-supervised method, NeRF-NQA [7]. Given that NeRF-NQA was developed utilizing perceptual labels for training, we accordingly fine-tuned NVS-SQA to ensure an equitable comparison. As delineated in Table V, our method surpasses NeRF-NQA across all evaluation metrics within the three datasets under consideration. Beyond mere performance metrics, NVS-SQA distinguishes itself as an end-to-end learning framework that facilitates straightforward training, in contrast to NeRF-NQA, which does not support end-to-end learning [7]. Specifically, end-to-end learning facilitates the direct transformation of raw data into final outputs, enhancing model efficiency and accuracy by eliminating manual feature engineering [54], [67]. This approach not only simplifies the development process but also improves the scalability and adaptability [67]. Additionally, unlike NeRF-NQA, which relies on external tools such as COLMAP [46]-known for generating noisy sparse points in some cases [68], [69]\u2014NVS-SQA operates independently. This independence further emphasizes its practicality and reliability for the quality assessment of NSS, as it does not depend on external systems that may introduce noise or errors into the process."}, {"title": "F. Quantitative results", "content": "In addition to the numerical comparisons reported in Table X, we provide example cases in Fig. 8, where PSNR, SSIM, and LPIPS (i.e., the most widely used quality assessment methods for NSS) fail to align with actual human preferences.\nEach row in Fig. 8 presents a pair of neurally synthesized scenes (NSS) generated by different Neural View Synthesis (NVS) methods, along with human and NVS-SQA preferences, compared against the outcomes from PSNR, SSIM, and LPIPS.\nFor instance, in the top row (\"Giraffe\"), although PSNR and SSIM favor the right-hand result, perceptual inspection reveals substantial blurring and missing object details. By contrast, NVS-SQA and human observers correctly identify the left-hand image as exhibiting superior perceptual fidelity. A similar discrepancy is evident in the \"Intr-Animals\" example, where the left-hand NSS contains clear artifacts (outlined"}, {"title": "V. CONCLUSION", "content": "We present NVS-SQA, the first no-reference, self-supervised learning framework for NSS quality assessment, addressing challenges with unlabeled and limited datasets. By introducing NSS-specific contrastive pair preparation and multi-branch guidance adaptation inspired by heuristic cues and full-reference scores, NVS-SQA surpasses existing no-reference methods and even several full-reference metrics across diverse datasets, demonstrating strong generalization to unseen scenes and NVS methods. Additionally, we establish a benchmark for self-supervised NSS quality assessment and open-source our code and datasets to advance research in this domain, setting a new standard for NSS quality learning."}]}