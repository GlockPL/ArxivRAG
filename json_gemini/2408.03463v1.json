{"title": "Identifying treatment response subgroups\nin observational time-to-event data", "authors": ["Vincent Jeanselme", "Chang Ho Yoon", "Fabian Falck", "Brian Tom", "Jessica Barrett"], "abstract": "Identifying patient subgroups with different treatment responses is an important\ntask to inform medical recommendations, guidelines, and the design of future\nclinical trials. Existing approaches for subgroup analysis primarily focus on Ran-\ndomised Controlled Trials (RCTs), in which treatment assignment is randomised.\nFurthermore, the patient cohort of an RCT is often constrained by cost, and is not\nrepresentative of the heterogeneity of patients likely to receive treatment in real-\nworld clinical practice. Therefore, when applied to observational studies, such ap-\nproaches suffer from significant statistical biases because of the non-randomisation\nof treatment. Our work introduces a novel, outcome-guided method for identify-\ning treatment response subgroups in observational studies. Our approach assigns\neach patient to a subgroup associated with two time-to-event distributions: one\nunder treatment and one under control regime. It hence positions itself in be-\ntween individualised and average treatment effect estimation. The assumptions of\nour model result in a simple correction of the statistical bias from treatment non-\nrandomisation through inverse propensity weighting. In experiments, our approach\nsignificantly outperforms the current state-of-the-art method for outcome-guided\nsubgroup analysis in both randomised and observational treatment regimes.", "sections": [{"title": "1 Introduction", "content": "Understanding heterogeneous therapeutic responses between patient subgroups is the core of treatment\nguidelines and the development of new drugs. Identifying such subgroups is valuable to inform\nclinical trials by identifying subgroups not responding to existing drugs, and to direct healthcare\nresources to those who might benefit most, and away from those who may be most harmed [11]. For\ninstance, subgroup analysis in the BARI trial of patients with coronary artery disease supported the\nuse of coronary artery bypass graft over percutaneous interventions for patients with diabetes, and\nthe opposite for patients without diabetes [18], shaping subsequent guidelines. Figure 1 illustrates\nthis idea: two groups may present opposite treatment responses, and would benefit from different\nrecommendations. Our work aims to uncover such subgroups.\nRandomised controlled trials (RCTs) remain the gold standard for identifying subgroups of treatment\neffects. In RCTs, patients are randomly assigned to control or treated groups, allowing researchers to\nassess the impact of an intervention or treatment. However, RCTs are costly and time-consuming,\nwith restricted patient cohorts unrepresentative of the real-world diversity of patients who may receive\ntreatment and treatment strategies [16].\nOur work introduces a novel approach that diverges from traditional RCT-based methodolo-\ngies [34, 36] by leveraging routinely collected observational data to identify patient subgroups.\nObservational studies encompass larger and more diverse cohorts reflective of real-world practices,\noffering the potential to uncover subgroups of treatment responses, that could be missed in RCTs.\nPrior works [5, 9, 29] have leveraged observational data while addressing biases from non-random\ntreatment assignments [4, 14, 15]. However, this body of literature primarily focuses on estimating\n(i) averaged treatment effects at the population level or (ii) individualised treatment effects, thereby\noverlooking the identification of treatment effect subgroups.\nAddressing this critical gap in the literature, our work uncovers patient subgroups with distinct\ntreatment responses using observational data. Unlike previous methods relying on RCTs, we introduce\na mixture of neural networks as an extension to traditional outcome-guided models to uncover\nsubgroups delineated by non-linear, higher-dimensional combinations of available covariates, without\nrequiring parametrisation of the time-to-event distributions or treatment responses.\nOur work extends Neural Survival Clustering [19] to estimate treatment effects,\nintroducing the novel Causal Neural Survival Clustering (CNSC). This methodology is the first\nneural network approach to simultaneously estimate treatment effects and discover subgroups in\nobservational settings, addressing both censoring and non-random treatment assignments without\nparameterising the survival distribution or treatment effects.\nSection 2 formalises the problem of treatment subgroup discovery and non-randomisation correction.\nThen, we introduce the proposed monotonic neural network implementation in Section 2. Finally,"}, {"title": "2 Method", "content": "In this section, we describe our method, referring to Appendix A for technical proofs.\n2.1 Problem setup\nLatent survival subgroup analysis. Our goal is to uncover subgroups with similar treatment\nresponses, guided by the observed times of occurrence of an outcome of interest. Patients are\nassigned to subgroups based on their covariates at treatment time. Each subgroup is characterised by\ntwo distributions, known as survival functions: one under treatment and one under control regimes.\nThese distributions characterise the probability of observing the event of interest after a given time\nunder a given treatment.\nConsider the random variables associated with observed covariates X, an indicator identifying\nwhether the event of interest, in our analysis death, was observed D, and the observed time of event\nT. Formally, we define the latter random variables as $T := \\min(C, T')$ and $D := \\mathbb{1}(C > T')$, where\nC is random variable of the (right)-censoring time, i.e. the time at which the patient left the study\nbefore experiencing the event of interest, and $T'$ is the partially observed random variable associated\nwith the time of the event of interest if there was no censoring. When a patient is censored, T = C\nand D = 0, otherwise the event is observed and $T = T'$ and $D = 1$. To estimate the associated\nsurvival likelihood, we further assume, as commonly done in survival analysis, that the censoring\ntime C is not informative for the time of the event of interest T'.\nAssumption 1 (Non-informative censoring). The censoring time C is independent of the time of the\nevent of interest T' given the covariates X. Formally, $T' \\perp \\!\\!\\!\\perp C | X$.\nCentral to our problem is the additional variable Z associated with the latent, unobserved subgroup\nmembership. Following Jeanselme et al. [19], we aim to identify a pre-specified $K \\in \\mathbb{N}$ number of\nsubgroups. We assume that Z depends on X and influences $T'$. As we are interested in recovering\nthe survival functions associated with the latent subgroups, we ignore the potential dependence of $T'$\non X, considering the individual survival distribution as a mixture of the different subgroups.\nAssumption 2 (Mixture modelling). The event time T' is completely determined by the patient's\ngroup membership Z. Formally, $T' \\perp \\!\\!\\!\\perp X | Z$.\nWhile this assumption may hurt individual performance as individual covariates do not directly\ninform individual survival but group membership only, it improves the interpretability of the model\nby separating subgroup membership from survival profiles. In conclusion, we aim to recover Z and\nthe survival distributions associated with each subgroup from the observed X, T and D.\nGiven the previously described dependencies, the expected survival S for a given patient with\ncovariates x is the survival when marginalising over the different subgroups:\n$S(t | X = x) := P(T' > t | X = x) = \\sum_{k=1}^{K} P(Z = k | X = x)P(T' \\geq t | Z = k)$                                                                                               (1)\nNote that the last term is linked to the quantity commonly used in the survival literature: $\\Lambda_k(t) :=\n- \\log P(T' > t | Z = k)$, the group-specific cumulative hazard, with derivative with respect to\ntime (t), the group-specific instantaneous hazard, corresponding to the increase in the probability\nof observing the event of interest given survival until that time t. Estimating the two probability\ndistributions $P(Z = k | X = x)$ and $P(T' > t | Z = k)$ is our core interest.\nOne can maximise the log-likelihood associated with the\nobserved realisation ${x_i, t_i, d_i}$ of the random variables X,T, D for patient i (we omit i where"}, {"title": "2.2 Estimating the quantities of interest with neural networks", "content": "The previous section discussed the quantities one must estimate -here parameterised by neural\nnetworks- to uncover subgroups of treatment effects: the assignment function $P(Z | X)$, the\ncumulative incidence $A_k$ characterising survival under the two treatment regimes, and the weights w.\nFigure 3 illustrates the overall architecture and the neural networks used to estimate these quantities.\nSimilar to [19], a multi-layer perceptron G with final Softmax layer assigns\neach patient characterised by covariates x its probability of belonging to each subgroup, characterised\nthrough a K-dimensional vector of probabilities.\n$G(x) := [P(Z = k | x)]_{k=1}^{K}$\nEach subgroup k is represented by a vector $l_k \\in \\mathbb{R}^L$ of dimension L,\na latent parametrisation of the cumulative hazard functions. The vector $l_k$ is concatenated with t\nand used as input to a neural network M with monotonic positive outcomes, with a final SoftPlus\nlayer to model the cumulative hazard functions under both treatment regimes $A_k(t) := (A_k(t | A =\n0), A_k (t | A = 1))$. We use the following transformation to ensure that no probability is assigned to\nnegative times, a limitation raised concerning previous monotonic neural networks [46]:\n$A_k(t) := t \\cdot M(l_k,t)$\nBy modelling these two cumulative intensity functions, we can estimate the Group Average Treatment\nEffect (GATE), $\\hat{I}_k (t)$, in subgroup k as\n$\\hat{I}_k(t) := E[\\tau(t, X) | Z = k)] = \\exp(-A_k(t | A = 1)) - \\exp(-A_k(t | A = 0))$\nUnder treatment randomisation, as in RCTs, the previous compo-\nnents would accurately model the observed survival outcomes and identify subgroups of treatment\neffects by maximising the factual likelihood. As previously discussed, to account for the treatment\nnon-randomisation in observational studies, we weigh the factual likelihood using the propensity\nscore of a patient estimated through a multi-layer perceptron W with a final sigmoid transformation\nas\n$W(x) := P(A = 1 | x)$"}, {"title": "2.3 Training procedure", "content": "First, the network W is trained to predict the binary treatment assignment by minimising the cross-\nentropy of receiving treatment. Then, training all other components relies on minimising the weighted\nfactual log-likelihood introduced in Eq. (6). The use of monotonic neural networks results in the\nefficient and exact computation of the log-likelihood as automatic differentiation of the monotonic\nneural networks' outcomes provides the instantaneous hazards $A_k$ necessary for computing the\nsurvival function derivative used in the log likelihood [19, 20, 40]."}, {"title": "3 Experimental analysis", "content": "As counterfactuals are unknown in observational data, we adopt\u2014as is common practice in this\nresearch field-a synthetic dataset in which underlying survival distributions and group structure\nare known. Appendix C accompanies these synthetic results with the analysis of heterogeneity in\nadjuvant radiotherapy responses for patients diagnosed with breast cancer. Our code for the synthetic\ndataset, the model, and the reproduction of all experimental results is available on Github6.\n3.1 Data generation\nWe generate a population of 30,000 divided into $K = 3$ subgroups. We draw 10 covariates\nfrom normal distributions and survival distributions for each treatment regime following Gompertz\ndistributions [37] parameterised by group membership and individual covariates. This choice reflects\na setting in which time-to-event distributions under treatment or control regimes are different functions\nof the covariates. This simulation is more likely to capture the complexity of real-world responses,\ncontrary to traditional evaluations of subgroup analysis, which often assume a linear treatment\nresponse, i.e., a simple shift between treated and untreated distributions. Further, we implement\ntwo treatment assignment scenarios: a randomised assignment in which treatment is independent of\npatient covariates, similar to RCTs (Randomised), and one in which treatment is a function of the\npatient covariates as observed in observational studies (Observational). Finally, non-informative\ncensoring times are drawn also following a Gompertz distribution. Details of the generative process\nof our synthetic dataset are deferred to App. \u0412.1.\n3.2 Empirical settings"}, {"title": "4 Related work", "content": "While survival subgrouping has been proposed through mixtures of distributions [19, 32, 33] to\nidentify different phenotypes of patients, the machine learning literature on phenotyping treatment\neffects with time-to-event outcomes remains sparse. The current literature focuses on estimating\npopulation or individual treatment effects [9, 22, 45, 55]. While these models help to understand the\naverage population-wide response to treatment and aim to estimate individualised treatment effects,\nthey do not provide an understanding of the patient groups that may benefit more or be harmed more\nby treatment. Identifying such groups aligns with, and is thus more useful for, drafting medical\nguidelines that direct treatment to those subgroups most likely to benefit from it.\nDiscovering intervenable subgroups is core to medical practice, particularly identifying subgroups\nof treatment effect, as patients do not respond like the average [6, 43, 44] and the average may\nconceal differential treatment responses. Identification of subgroups has long been used to design\nRCTs. Indeed, subgroups identified a priori can then be tested through trials [8, 41]. A posteriori\nanalyses have gained traction to uncover subgroups of patients from existing RCTs to understand the\nunderlying variability of responses.\nThe first set of subgroup analysis methodologies consists of a step-wise approach: (i) estimate the\nITE and (ii) uncover subgroups using a second model to explain the heterogeneity in ITE. Foster\net al. [11], Qi et al. [38] describe the virtual twins approach in which one models the outcome using\na decision tree for each treatment group. The difference between these decision trees results in the\nestimated treatment effects. A final decision tree aims to explain these estimated treatment effects\nto uncover subgroups. Similar approaches have been explored with different meta-learners [53],\nor Bayesian additive models [17], or replacing the final step with a linear predictor to uncover the\nfeature influencing heterogeneity [7]. However, Guelman et al. [12] discuss the drawbacks associated"}, {"title": "5 Conclusion", "content": "This work introduces a novel methodology to uncover subgroups of treatment responses in observa-\ntional time-to-event data. Our methodology addresses, to date, unaddressed challenges in subgroup\ndiscovery, specifically non-randomised treatment assignment and censoring. While modelling the\nobserved outcome, the approach simultaneously identifies patient subgroups with different treatment\nresponses in a joint approach. These groups may motivate the design of future clinical trials and\nsupport the development of guidelines to better target treatment to subgroups that would most benefit\nfrom them.\nOur experiments demonstrate the capacity of our proposed method to uncover underlying groups of\ntreatment effects. However, causal modelling relies on empirically unverifiable assumptions. The\ninvalidity of these assumptions is a lesser concern in hypothesis generation, as offered by the proposed\nmodel, than in individualised estimates aimed at treatment recommendations.\nServing the purpose of a hypothesis-generating tool, we invite practitioners to further study the sub-\ngroups, identified in observational studies, in RCTs to (i) validate the estimated responses, (ii) identify\npotential alternative treatments with improved responses, and (iii) inform clinical guidelines."}, {"title": "A Proof", "content": "A.1 Individualised Treatment Effect\nThis section derives the individualised treatment effect expression introduced in Eq. (3).\n$\\tau(t, x) := E(\\mathbb{1}(T_1 \\geq t) - \\mathbb{1}(T_0 \\geq t) | X = x)$\n$= E(\\mathbb{1}(T_1 \\geq t) | X = x) - E(\\mathbb{1}(T_0 \\geq t) | X = x)$\n$= P(T' > t | A = 1, X = x) - P(T' > t | A = 0, X = x)$ (Under Assumption 4 and 5)\nA.2 Upper-Bound Simplification\nIn Section 2, we claim that under the considered DAG presented in Figure 2,\n$IPM(q_{Z|X}^{A=0},q_{Z|X}^{A=1}) = 0$\nwith $A_i$ being the treatment assignment. The following derives this result:\n$q_{Z|X}^{A=a} := q(Z | X, A = a) = q(Z | X)$ (Under Assumption 3)\nwith q, the density function. From this expression, $q_{Z|X}^{A=0}= q_{Z|X}^{A=1}$, which results in the distance\nbetween these distributions being null."}, {"title": "B Synthetic analysis", "content": "B.1 Data generation\nWe consider a synthetic population of $N = 30,000$ patients with 10 associated covariates $X \\in \\mathbb{R}^{10}$\ndivided into $K = 3$ subgroups. The following data generation does not aim to mimic a particular\nreal-world setting but follows a similar approach to [34]. The following describes our generation\nprocess:\nEach patient's membership Z is drawn from a multinomial with equal probability.\nGroup membership informs the two first covariates through the parametrisation of the bivariate normal\ndistribution with centres $c_k$ equal to (0, 2.25), (-2.25, -1), and (2.25, -1). All other covariates are\ndrawn from standard normal distributions. Formally, this procedure is described as:\n$Z \\sim Mult \\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right)$\n$X[1,2] | Z = k \\sim MVN(c_k, I^2)$\n$X[3:10] \\sim MVN(0, I^8)$\nwith MVN denoting a multivariate normal distribution, and $I^n$, the identity covariance matrix of\ndimension n.\nFor each subgroup, event times under treatment and control regimes are drawn\nfrom Gompertz distributions, with parameters that are functions of group-specific coefficients ($\\beta^0$\nand $\\Gamma^0$ for the event time when untreated and $\\beta^1$ and $\\Gamma^1$ when treated) and the patient's covariates.\n$\\beta^0 | Z = z \\sim MVN(0, I^{10})$\n$\\Gamma^0 | Z = z \\sim MVN(0, I^{10})$\n$T_0 | Z, X, \\beta^0, \\Gamma^0 = (z, x, \\beta^0, \\gamma^0) \\sim Gompertz \\left(\\omega_0(\\beta^0, x), \\delta_0(z, x)\\right)$\n$\\beta^1 | Z = z \\sim MVN(0, I^{10})$\n$\\Gamma^1 | Z = z \\sim MVN(0, I^{10})$\n$T_1 | Z, X, \\beta^1, \\Gamma^1 = (z, x, \\beta^1, \\gamma^1) \\sim Gompertz \\left(\\omega_1(\\beta^1, x), \\delta_1(\\Gamma^1, x)\\right)$"}]}