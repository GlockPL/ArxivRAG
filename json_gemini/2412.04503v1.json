{"title": "A Primer on Large Language Models and their Limitations", "authors": ["Sandra Johnson", "David Hyland-Wood"], "abstract": "This paper provides a primer on Large Language Models (LLMs) and identifies their strengths, limitations, applications and research directions. It is intended to be useful to those in academia and industry who are interested in gaining an understanding of the key LLM concepts and technologies, and in utilising this knowledge in both day to day tasks and in more complex scenarios where this technology can enhance current practices and processes.", "sections": [{"title": "1 Introduction", "content": "The world of artificial intelligence (AI) is increasingly penetrating all aspects of our personal and professional lives. This proliferation of AI tools and applications are being met with a mixture of excitement, scepticism and even dread [78]. Excitement at the seemingly endless potential of AI applications such as LLMs, especially when they are integrated \u201cwithin broader systems\" [13], scepticism as the realisation dawns that LLMs are in fact fallible as evidenced by hallucinations and hence not the golden bullet that can solve all problems [19, 21], and a feeling of dread for those who believe that LLMs and AI have the potential to detrimentally impact our lives and make people redundant [78].\nThe ability of some LLMs to pass Theory of Mind (ToM) [64][32] and Turing Tests [7][42] suggests support for the Computational Theory of Mind (CTM), that cognition may be substrate independent. These findings challenge biological essentialism and open new avenues for creating sophisticated AI systems capable of human-like reasoning and interaction. Viewed another way, these studies could be taken to provide evidence for those critical of both the Turing Test and Theory of Mind tests in assessing cognition in humans and animals.\nHowever, it should be noted that LLMs by themselves have no self monitoring (also called phenomenal consciousness or subjective experience) or internal, updatable model of their external environment (that is, a model of itself as a being in a world). Both of these conditions are required in some reasonable theories consciousness [33]. Those omissions alone may be taken as evidence against LLMs having any form of consciousness as that term is currently understood.\nWe begin this paper by providing an overview covering the basics of LLM in Section 2: An Overview of LLMs, in order to provide a common understanding and vocabulary for these natural language modelling approaches."}, {"title": "2 An Overview of LLMS", "content": "The emergence of LLMs is preceded by an extensive body of research in language modelling [72] where word sequences were scored with the aid of a probabilistic model possibly dating back to 1976 [28, 72]. Natural language processing (NLP) modelling further evolved over time with recurrent neural networks (RNNs) becoming the model of choice for autoregressive language modelling tasks such as translation. However, RNNs typically process one token at a time because they are designed for sequential processing, making parallel processing hard to achieve and limiting the capture of long range sequences. The latter is often referred to as the \"vanishing gradient problem\" [23, 58].\nRNNs are further constrained due to their heavy use of computing resources. Although more computing power is available these days, relieving some of the constraints, the key change maker was the publication of the seminal paper \"Attention Is All You Need\" [67]. The authors introduced a model called the Transformer, signalling the arrival of what is commonly known as 'transformer architecture', which revolutionised NLP. As the title implies the researchers discovered that it was possible to rely solely on self-attention and feedforward layers without recurrent connections as in RNNs [67]. Moreover, with the introduction of Reformer, a revised version of Transformer, the same performance was obtained in a far more memory-efficient way and on much longer sequences of words [31].\nSeveral LLMs are based on this architecture, such as bidirectional encoder representations from transformers (BERT) [12] and variations of BERT: a lite BERT (ALBERT), Robustly opti-"}, {"title": "2.1 AI Project Development utilising LLMs", "content": "The hype around AI and particularly LLMs sparked the realisation that there are real benefits to be had if this technology can be integrated in every day personal activities and in businesses and organisations to improve productivity through automation of repetitive trivial tasks, and consequently enable employees, students and researchers to dedicate more time on interesting and complex tasks.\nWhen embarking on an AI/LLM project to harness the power of this technology, we observe that the life cycle of such a project consists mainly of four distinct phases [5]:\n1. Project scoping\n2. Model selection\n3. Model adaption and alignment\n4. Application integration"}, {"title": "2.2 Choosing a Foundational LLM", "content": "The choice of which LLM to use in an AI project for a given task or functionality may appear daunting due to the sheer number of LLMs currently available, and the emergence of a seemingly endless stream of new LLMs, including updated versions of existing LLMs, each purporting to improve on the previous version (Figure 3, page 5) [77]. Moreover, pre-training and fine tuning techniques for models differ depending on the desired capabilities of the language model [68].\nLLMs have evolved from relatively simple language tasks such as text generation to models capable of performing more complex tasks, such as those illustrated in Figure 4, page 6 [77].\nWhen weighing up the options, we need to take several aspects into consideration. The number of parameters in models vary widely and can limit the range of devices it can run on, as well as the task objectives and applications that are best suited to a particular LLM. Smaller models do not necessarily perform worse than large models, especially if the model is being optimised to perform a specific task well, rather than aiming to cater for multiple use cases. Determining model size is just one consideration when choosing an existing LLM, adapting an existing model, or building a new fit-for-purpose model from scratch.\nIt is helpful to keep the Transformer LLM framework Figure 5(a) and high level architecture Figure 5(b), page 6, in mind when choosing a suitable foundational LLM model.\nThe encoder component encodes the model input, entered as a \"prompt\", with contextual understanding and produces one vector per input token. The decoder processes the input to-"}, {"title": "2.2.1 Decoder-only models", "content": "Decoder-only models are autoregressive models pre-trained to predict the next token based on previous tokens, making them well suited to text generation (e.g. for creative writing or content generation), autocompletion (e.g. autocompletion of sentences or lines of code), language translation, and text summarisation.\nThere are several well known decoder-only LLMs, such as the GPT series of models from OpenAI, LLAMA and open pretrained transformer (OPT) from Meta, Claude from Anthropic, peer-to-peer (p2p) from Google and Gopher from DeepMind."}, {"title": "2.2.2 Encoder-only models", "content": "Encoder-only models are auto-encoding models, well suited to tasks that involve understanding and extracting meaning from text, such as word classification, named entity recognition, question answering and sentiment analysis.\nKey foundational encoder-only models include BERT [12], variant RoBERTa, and ELECTRA [11].\nMLM, most frequently used to pre-train encoder models [12, 46], is a self-supervised learning technique that randomly masks tokens in an input sequence with the aim of learning the masked tokens based on the surrounding context provided by the unmasked tokens [48, 45]. Hence MLM differs from CLM by using unmasked tokens both before and after masked tokens, providing a bi-directional understanding of context instead of being limited to the words that precede it (Figure 7, page 8).\nAlternatively, techniques such as replaced token detection (RTD), which was used to train ELECTRA, may be used [11]. With RTD instead of masking tokens as in MLM, a small generator model replaces some tokens with plausible alternatives and the encoder (discriminator) is then trained to detect the tokens that have been replaced [45]."}, {"title": "2.2.3 Encoder Decoder models", "content": "Encoder-decoder models are sequence-to-sequence models, and suited to tasks that require both understanding and generation of text, such as translation, summarisation, question answering and dialogue systems.\nTwo notable sequence-to-sequence LLMs are T5 [56] and BART [34] with both aiming to denoise corrupted inputs, via slightly different pre-training approaches."}, {"title": "2.3 Pre-training Foundational LLMs", "content": "LLMs are pre-trained on vasts amounts of textual data using a variety of strategies and techniques, which is often followed by more specific fine-tuning of the model to suit its intended use [9]. Interestingly, LLaMA was trained exclusively on publicly available data sources [66] (Figure 9).\nThe general trend has been to train ever larger models because large pre-trained Transformer models were found to be capable of performing tasks for which they had not been specifically trained on [68]. Conversely, Hoffman et al. [24] found that training a smaller model on more data for a given compute budget is more performant than only increasing model size while keeping the size of the training data unchanged. However, focussing on the optimal combination of model size and training dataset size does not take into account the importance of the speed of inference [66]. Instead Touvron et al. [66] concluded that training smaller models for longer results in faster inference.\nPre-training of LLMs is performed using data labels. Adding labels to data prior to training (supervised learning) typically requires human annotation which is infeasible when training on very large corpora. Supervised learning for LLMs is more typically used when training a model"}, {"title": "2.3.1 Self-supervised learning", "content": "SSL is the most popular machine learning technique for LLMs. This approach is often referred to as the \"dark matter of intelligence\" [4] and includes learning methods such as CLM, MLM, Span-Level Masking, and Contrastive Learning, where models learn without the need to explicitly apply external labels to the data.\nELECTRA, also an encoder-only mode like BERT uses an alternative pre-training method to MLM called \"replaced token detection\" [11].\nVarious SSL approaches have been used in training foundational LLMs (Table 1 on page 11). Moreover, as can be seen from summary table 1, page 11, some notable LLMs combine multiple self-supervised approaches to leverage the strengths of each method, e.g. BART, BERT and T5 [56, 34]."}, {"title": "2.4 Adapting LLMs for Specific Use Cases", "content": "Large pre-trained Transformer models were found to be capable of performing tasks for which they had not been specifically trained on [68]. This is known as \u201czero-shot\" inference [68]. However, when the output from the LLM for a certain task is less than satisfactory, there are two main techniques to achieve better results: in-context learning and fine-tuning."}, {"title": "2.4.1 In-context learning", "content": "In-context learning (ICL) refers to the capability of pre-trained LLMs to perform new tasks by leveraging information provided within the context window, without any explicit parameter updates or fine-tuning [9].\nInstead of adjusting weights through gradient descent, the model adapts its behaviour based on examples, instructions, or demonstrations included in the prompt [70]. Figure 12 (page 12) shows prompting with none, one and two examples in the context window. This strategy allows LLMs to generalise to a wide range of tasks using natural language interactions [9]."}, {"title": "2.4.2 Fine-tuning", "content": "Fine-tuning is the process of updating pre-trained LLM weights by training on specific datasets for chosen tasks [9, 56]. Supervised fine-tuning (SFT) [12], reinforcement learning with human feedback (RLHF) [52], and parameter efficient fine-tuning (PEFT) [26] are three of the most popular fine-tuning approaches for LLMs.\nHowever, there are several other fine-tuning approaches that can be employed. They can broadly be categorised as full model fine-tuning (e.g. SFT [12] and RLHF [52]), PEFT (e.g. low-rank adoption (LoRA) [27]), and model compression and deployment optimisation (e.g. quantisation-aware fine-tuning as used for Q8BERT LLM [75]). A visualisation of this grouping and associated fine-tuning techniques in each category are shown as a mind map in Figure 15 on page 14.\nSometimes a single technique may be insufficient in delivering the desired outcomes, and instead we can combine multiple fine-tuning strategies, leveraging the strengths of each technique in order to address shortcomings, such as solving multiple constraints simultaneously or the need to optimise for performance, efficiency, and alignment. For example, combining RLHF with LoRA would yield models that are both aligned with human preferences and parameter-efficient."}, {"title": "2.5 Creating a bespoke LLM", "content": "In some instances it may be preferable to develop a bespoke LLM instead of fine-tuning one of the popular foundational models. To do this, the following steps provide a general approach:\n1. Data Selection and Preparation\n\u2022 Data gathering: The foundation of any LLM is the data it learns from. Therefore, identifying the appropriate and relevant data sources is an important first step in developing a bespoke LLM and requires a clear understanding of the key objectives of the LLM. The data gathering exercise typically involves obtaining extensive text data from various sources such as books, websites, and articles, but in other cases the inclusion of a highly diverse corpus of text data may be less relevant and attention is instead focussed on sourcing only a few, but high quality datasets to train the model on. Nonetheless, some additional refinements may be achieved at a later stage by employing a variety of learning approaches as discussed in Section 2.4 Adapting LLMs for Specific Use Cases.\n\u2022 Preprocessing: The data typically needs some degree of preprocessing, such as data cleansing to remove noise and irrelevant content, normalisation to standardise text formats, and tokenisation to convert text into a format that the model can understand.\n\u2022 Annotation: If supervised learning is involved, this stage may also include annotating the data with labels.\n\u2022 Training data: Finally, the dataset is split into training, validation, and test datasets to enable effective learning and unbiased evaluation. The training data allocation is usually set around 15%.\n2. Model Design and Configuration\nChoosing the right model architecture is critical to achieving the desired performance. For LLMs, Transformer-based architectures are commonly used due to their ability to capture long-range dependencies in text. This step involves configuring the model's parameters, such as the number of layers, hidden units, and attention heads, to balance performance"}, {"title": "2.6 Interacting with LLMS", "content": "There is a myriad of ways in which we can interact with LLMs, depending on the desired end goal(s). Some interactions such as fine-tuning (See Section 2.4.2, page 13) adjust the base model while others focus on the most effective way to perform tasks and extract information without adjusting the underlying model. Figure 15, page 14 visually summarises the various approaches.\nTable 3, page 19, gives a brief overview of these methods, but arguably the most common and well known way of interacting with LLMs is through a chat bot such as ChatGPT (by OpenAI), BARD (by Google), Claude (by Anthropic) and Bing Chat (by Microsoft, powered by OpenAI) [7, 16, 20].\nIn Section 2.6.1, page 18, we discuss prompt engineering in more detail, a simple and effective way for most users to harness the knowledge, and explore the functionality, of an LLM. However, a flexible, powerful and effective way of interacting with LLMs is through application program interfaces (APIs), but that requires a higher level of technical expertise. Several of the well known pre-trained LLMs provide APIs, some are open source and others not. The more popular APIs are: Hugging Face's Transformers Library and Inference API, Google Cloud's Natural Language API, IBM Watson Language Translator API, APIs to access BERT can be obtained via Google Research BERT repository or through Hugging Face's BERT model webpage, and APIs for OpenAI GPT-4o and GPT-40 mini."}, {"title": "2.6.1 Prompt Engineering", "content": "Prompt engineering is a technique used to maximise the effectiveness of an existing LLM without altering its internal structure. The process comprises three parts: the prompt itself is the model input, model inference is the generation of text in response to the prompt, and lastly completion is the resulting output text. The context window is the all the text and memory that is available.\nBy carefully crafted prompts, users can harness these models more effectively, leading to better outcomes in tasks ranging from simple queries to complex problem solving, but it has limitations. One effective strategy to improve model outcomes is by including examples inside the context window (Figure 12, page 12). This process is called in-context learning and the variations of in-context learning are: [9]:\n\u2022 zero-shot inference - no examples provided\n\u2022 one-shot inference - one example provided\n\u2022 few-shot inference - more than one example provided\nWe can also view prompt engineering as a complementary technique to fine-tuning by using it to generate training data or as an interim solution to improve the model's performance. A general guide for progressing on to fine-tuning is when the number of examples (few shot learning) is growing to more than 5 or 6, with diminishing improvements in LLM output. Nonetheless, the research study by Brown, T.B., et al. [9] used a few dozen examples in their few-shot settings (Figure 13, page 12)."}, {"title": "3 LLMs Orchestrated with Other Technologies", "content": "Orchestration of LLMs with traditional information retrieval systems has been explored since the early stages of this technology. Google researchers developed a platform in 2017 to speed up the creation and maintenance of production platforms when combining components of their TensorFlow machine learning system [6]. Some of those same techniques are present in more modern systems today. In 2018 medical informatics researchers combined image caption-generation en-"}, {"title": "4 Risks and Mitigations", "content": ""}, {"title": "4.1 Catastrophic Forgetting", "content": "Catastrophic forgetting, or catastrophic interference, is when neural networks, including LLMs, become less performant on tasks that they previously excelled at [40, 76, 35]. In other words, they essentially \"forget\" previously learned information [40]. This behaviour is typically observed when LLMs are fine-tuned sequentially on different tasks or datasets, a process known as continual learning [30]. The underlying cause is that the fine-tuning exercise updates the model's weights to optimise performance on the new task. Several strategies have been proposed to prevent catastrophic forgetting, such as:\n\u2022 Regularisation-based method: Adding regularisation terms to penalise significant changes"}, {"title": "4.2 Model Collapse", "content": "LLMs are trained on many public data sources as described in Section 2 An Overview of LLMs. Since many users are using LLMs to generate content that is being put onto those same public fora, future versions of those LLMs are very likely to ingest content generated by earlier versions of themselves. It is not difficult to envision a future in which LLMs become trained on an ever-increasing amount of machine-generated content and a decreasing amount of human-generated content. The ramifications are intriguing; without a change in the way the models are trained their weights will be increasingly influenced by machine-generated content. Human-generated content could even become a minority input for some models.\nThe unintended or unrecognised prevalence of machine-generated content in training data"}, {"title": "4.3 Jailbreak Attacks", "content": "A jailbreak is an adversarial attack in which users craft specific prompts designed to bypass the model's ethical safeguards. These jailbreak prompts trick the model into generating harmful or unethical responses, circumventing its alignment with moral guidelines [73, 54]. Users may craft jailbreak prompts for various reasons, including:\n\u2022 Bypassing restrictions: Some users may want to elicit responses that are blocked by default, such as unethical or illegal content that the LLM would typically refuse to generate.\n\u2022 Malicious intent: Jailbreaks can be used to manipulate the LLM into generating content for harmful purposes, such as misinformation, hate speech, or instructions for illegal activities like fraud or cybercrime.\n(An example of tricking a chatbot to generate a blackmail letter is shown in Figure 19, page 24).\n\u2022 Security and Research: Researchers or security personnel might craft jailbreaks to gain a thorough understanding of the vulnerabilities in the AI system, which they can then guard against.\n\u2022 Entertainment: Others might use jailbreaks for humour or entertainment, pushing the LLM to say things it wouldn't normally say.\nXie et al. [73] propose several strategies to defend against jailbreaks. AI models can employ \"self-regulation techniques\" like system-mode self-reminders, which wrap user queries in prompts that remind the model to behave ethically. This method significantly reduces the success rate of jailbreak attacks by reinforcing the model's ethical guidelines. Other safeguards include RLHF to continually align the model with moral values, and content filtering systems that automatically detect and block adversarial prompts [73]."}, {"title": "4.4 Hallucinations and their Impacts", "content": "As anyone who has played, however briefly, with LLMs and multimodal AI models knows, they can sometimes produce output that goes very rapidly from amazing to badly wrong. These forays into fantasy are generally known as hallucinations. We argue that the term \u201challucinations\" is misleading and has, in fact, been the cause of much misunderstanding about the limitations of LLMs.\nLLMs do not hallucinate, they produce bullshit, in that word's technical sense.\nNaturally, we recognise that language changes over time. The Cambridge Dictionary has already added a second definition to \u201challucinate\u201d specifically related to AI systems7. Nevertheless, the semantics seems worthy to us of pursuit because it increases the explanatory power of our ability to conceptualise LLMs.\nIn the seminal paper, On bullshit, by philosopher Harry Frankfurt [15] \u201cbullshit\u201d is described as a form of communication where the speaker is indifferent to the truth. This indifference to the truth means that bullshit doesn't necessarily involve untrue statements; rather, it involves a total disregard for the truth. He argues that this makes bullshit a greater threat to truth than outright"}, {"title": "4.5 Areas of Less-Than-Human Performance", "content": "One of the now-classic ways to trick an LLM into giving a bad answer, or to show how the technology fails, is the prompt, \"How many times does the letter r occur in the word 'strawberry'?\"\nMost LLMs will answer \"2\", which is incorrect. The correct answer should be \"3\". The LLMs get this wrong because they never see the word \"strawberry\" in their input. Instead, they only see a number representing a token for that word. That is, they cannot reason over the word because they only receive a number.\nThere are ways to work around this problem. For example,\n1. Can you list all letters in the word \"strawberry\" in the order that they appear?\n2. How many times does the letter r appear in the list that you generated?\nChatGPT 4 or 40 will correctly answer \u201c3\u201d.\nNew approaches to reasoning in LLMs are addressing these limitations while at the same time introducing new performance penalties. ChatGPT 01-preview (intentionally code-named \"Strawberry\") will correctly answer \u201c3\u201d to the initial question because it does parse the word and then double check itself. However, as of this writing the process takes around 22 seconds. No doubt additional research will improve that performance."}, {"title": "5 Conclusions", "content": "Based on the LLM literature we reviewed, we endeavoured to describe the technology, and the potential of these language models when used \"out of the box\" (e.g. foundational models) or adapted for specific use cases or tasks, or as part of an orchestrated system. We highlighted potential positive and negative impacts of LLMs and strategies used to mitigate the latter. The paper is aimed at providing students, practitioners, researchers, and decision makers an overview and insight into the various aspects this technology and its potential with some caveats.\nA prudent strategy to minimise unexpected consequences of misbehaving AI tools including LLMs is continual evaluation of the accuracy and correctness of the output [54]. There are several tools that assess the relative performance of LLMs which can aid in choosing an LLM that is well suited for specific tasks and scenarios. The rate of development of AI, and LLMs specifically, is rapid and hence it is important to check regularly whether the current tool is still fit for purpose [54], and this rate of progress and innovation of LLMs continues unabated. Since we started our background research into LLMs and their applications, we have seen the emergence of an exciting new suite of models and architectures in software and hardware.\nMost notably the recent announcement of the arrival of Liquid Foundation Models (LFMs)10 in September 2024 by Liquid AI Inc, a spin-off startup of MIT. In contrast to traditional transformer foundational models, LFMs utilise a different architecture, known as liquid neural networks [14]. These neural networks are typically smaller, highly efficient, and adept at adjusting dynamically to changes in input data. These models are also generally much smaller than the"}]}