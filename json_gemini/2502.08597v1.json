{"title": "Learning in Markets with Heterogeneous Agents:\nDynamics and Survival of Bayesian vs. No-Regret Learners", "authors": ["David Easley", "Yoav Kolumbus", "\u00c9va Tardos"], "abstract": "We provide an analysis of the performance of heterogeneous learning agents in asset markets with\nstochastic payoffs. Our agents aim to maximize the expected growth rate of their wealth, but have different\ntheories on how to learn to do this best. Our main focus is on comparing Bayesian learners and no-regret\nlearners in market dynamics. Bayesian learners with a prior over a finite set of models that assign positive\nprior probability to the correct model have posterior predicted probabilities that converge exponentially fast to\nthe correct model. Consequently, such Bayesians survive even in the presence of agents who invest according\nto a correct model of the stochastic process. Bayesian learners with a continuum prior converge to the correct\nmodel at a rate of O((log T)/T).\nOnline learning theory provides no-regret algorithms for maximizing the log of wealth in this asset market\nsetting, achieving a worst-case regret bound of O(log T) without assuming that there is a steady underlying\nstochastic process, but comparing to the best fixed investment rule. This regret, as we observe, is of the\nsame order of magnitude as that of a Bayesian learner with a continuum prior. However, we show that\neven such low regret may not be sufficient for survival in asset markets: an agent can have regret as low\nas O(log T), converging to the correct model at a rate of O((log T)/T), but still vanish in market dynamics\nwhen competing against an agent who always invests according to the correct model, or even against a\nperfect Bayesian with a finite prior. On the other hand, we show that Bayesian learning is fragile, while\nno-regret learning requires less knowledge of the environment and is therefore more robust. Specifically, any\nno-regret learner will drive out of the market an imperfect Bayesian whose finite prior or update rule has even\nsmall errors. In our analysis, we formally establish the relationship between the notions of survival, vanish-\ning, and market domination studied in economics and the framework of regret minimization, thus bridging\nthese theories. More broadly, our work contributes to the study of dynamics with heterogeneous types of\nlearning agents and their impact on markets. Our results highlight the importance of exploring this area further.", "sections": [{"title": "1 Introduction", "content": "Learning how to invest in asset markets has been a central topic of study in both the economics and finance\nliterature, as well as in computer science, where algorithmic approaches to decision-making and market dynamics\nhave received growing attention. Generally, players in markets aim to maximize the utility they gain from their\nresulting wealth. A standard utility function used in both economics and computer science is the log of the\nplayer's wealth, leading to the objective of maximizing the growth rate of wealth\u00b9 (see references below).\nHowever, the two literatures take different approaches to learning-how learning agents evaluate the feedback\nthey receive and what methods they use to select their strategies\u2014and they ask very different questions about"}, {"title": "1.1 Related Work", "content": "No-regret learning: No-regret learning is a fundamental concept in the study of learning in games, tracing\nback to early work in game theory [14, 24, 49, 71] and subsequently developed in influential papers on no-regret\nalgorithms [8, 16, 42, 44, 50, 59]. See [26, 51, 75] for broad introductions. The dynamics of no-regret learning\nalgorithms in repeated games have been extensively studied in various domains, including repeated auctions\n[1, 12, 34, 35, 39, 46, 62, 64], budgeted auctions [10, 38, 40, 41, 66], portfolio selection (as we discuss below)"}, {"title": "1.2 Roadmap and Summary of Our Results", "content": "To the best of our knowledge, this work is the first to analyze the dynamics of no-regret learners against agents\nusing other theories of learning, to study the survival of no-regret learners in investment scenarios (in contrast\nto only their regret level), and to establish a connection between regret minimization and Bayesian learning in\ninvestment settings.\nWe show that, on the one hand, a Bayesian learner dominates the market against any player whose regret\nincreases with time (including logarithmic regret). Such a player may have vanishing regret, but will also have a\nvanishing share of the market value. On the other hand, we show that Bayesian learning is fragile. If the Bayesian\nlearner is not perfect but, instead, has some level of noise in its prior or update rule, it may initially gain a large\nshare of the market, but in the long run, a no-regret learner will dominate, driving the Bayesian out of the market.\nThe paper is structured as follows. In Section 2 we present the market model and provide useful preliminary\nanalysis, discussing relative wealths and the role of relative entropy in characterizing survival for stationary\ninvestment rules. Section 3 analyzes the relation between regret and wealth shares in the market, showing how\nregret can be used to compare different learners. In Section 4, we restate in the language of our market setting\nseveral known analyses of Bayesian learning that will be important for our comparison of Bayesian learners,\nno-regret learners, and imperfect Bayesians. In Section 5, we analyze the regret of a player who knows the\ncorrect stochastic model of market states and show that this analysis leads to a characterization of survival\nconditions in competition with a perfect Bayesian. Section 6 studies imperfect Bayesians, showing that Bayesian\nlearners with inaccurate priors or updates vanish when competing against no-regret learners that converge to the\ncorrect model. Finally, in Section 7, we present and discuss simulation results for several scenarios of competition\nbetween no-regret learners, perfect Bayesians, and imperfect ones, to complement our analysis and provide\nfurther intuition.\nWe conclude the introduction with an a informal summary of our main theorems and analyses:\nRegret and Survival: Theorem 3.1 characterizes the regret condition for survival in a competitive asset market: an\nagent survives if and only if its regret remains bounded by a constant relative to every competitor at all times.\nRemarkably, if two players both have similar regret asymptotics with respect to the best strategy in hindsight\n(e.g., both have O(log T) regret) but with different coefficients, the agent with the larger constant almost surely\nvanishes from the market."}, {"title": "2 Model and Preliminaries", "content": "We use the market structure as in [17] in our analysis of wealth and price dynamics. At each time in discrete steps\nindexed by t, a state is realized. There is a finite number of states $s \\in \\{1, . . ., S\\}$ distributed i.i.d. with probability\n$q = (q_1,..., q_S)$ where $q_s \\geq \\Delta > 0$ for all $s$. At each time, there is a complete set of assets. Assets are also indexed\nby s with one unit of asset s purchased at time t paying one unit of wealth at time t + 1 if state s occurs at time\nt + 1 and 0 otherwise. These assets, known as Arrow securities, are more general than they may seem; any asset\nstructure that permits arbitrary reallocation of wealth across states can be written as a simple transformation\nof Arrow securities. This asset market model, originally introduced in [4], is the standard general equilibrium\nmodel of an economy with a complete set of assets, see [3] Chapter 19. Asset prices at time t are represented by\n$p_t = (p_{1t},..., p_{St})$.\nThere are N agents indexed by n = 1, . . ., N. The wealth of agent n at time t is $w_t^n$. Agents begin with initial\nwealths $w_0^n > 0$ for agent n. These initial wealths are normalized to sum to one.\u00b3 Thus, if state s occurs at time\nt + 1, agent n will have time t + 1 wealth, $w_{t+1}^{n} = \\frac{a_{st}^n w_t^n}{p_{st}}$.\nIn a market equilibrium, the price of asset s must be such that the aggregate demand for asset s equals the\naggregate supply of one. So the price is given by\n\n$p_{st} = \\frac{\\sum_n a_{st}^n w_t^n}{\\sum_n w_t^n}$\n\nNote that asset prices sum to one as the aggregate wealth is one."}, {"title": "3 Comparing Learners", "content": "As we noted above, an investor who knows the probabilities $q_s$ maximizes the expected growth rate of their\nwealth at each time t using the investment rule $a_t^n = q$. Next, consider an agent n with a sequence of investment\nprofiles $a^n_t$. We will first compare the wealth of this agent to that of an agent who uses the investment rule $a = q$,\nand then, we will compare different types of learning agents.\nAs we saw in the previous section in Equation (6), the log of the wealth ratio of an agent at time T compared to\nan investor using the optimal $a = q$ rule can be expressed as\n\n$\\log(r_T^{nm}) = \\sum_{t=1}^T \\sum_{s=1}^S 1\\{s_t=s\\} \\log(\\frac{a_{st}^n}{q_s}) - \\sum_{t=1}^T \\sum_{s=1}^S 1\\{s_t=s\\} \\log(a_{st}^m) + \\log(r_0^{nm})$\n\nThe main goal of this paper is to compare different theories on how to best learn to invest. One theory is Bayesian\nlearning, assuming that the investor n has a Bayesian prior over the possible asset return probabilities. At each\nstep, a Bayesian learner uses the expected return probability implied by their beliefs, updating their beliefs\naccording to the observed states by using Bayes rule. One can also use multi-armed bandit learning methods\ngiven such a prior of possible asset return distributions, and have the agent invest using one of the distributions in\nits prior at each step, aiming to learn which one is the best. A different theory would consider the maximization\nproblem of Equation (10) as a concave maximization problem and use no-regret learning algorithms to optimize\nthe value."}, {"title": "4 Bayesian Learners", "content": "Next, we consider the standard Bayesian learning results. We restate and summarize in this section known results\nfor completeness and to provide a framework for our comparison of Bayesian and no-regret learning.\nConsider a Bayesian watching the process on states in the market. Suppose that the Bayesian does not know\nthe probability q, and instead considers a finite number of possibilities for q: models $O^1, . . ., O^K$. We assume that\n$O_s^k > \\Delta$ for all k and s. See the remark at the end of this section for connection to results for Bayesian learners\nwith a continuum prior. The Bayesian has a prior $\\lambda_0 = (\\lambda_0^1, ..., \\lambda_0^K)$ with $\\lambda_0^k > 0$ for all k. Let $n_s^t$ be the number of\ntimes that state s occurs by time t. The posterior probability of model $O^k$ is, by Bayes rule,\n\n$\\lambda_t^k = \\frac{\\lambda_0^k \\prod_s (O_s^k)^{n_s^t}}{\\sum_l \\lambda_0^l \\prod_s (O_s^l)^{n_s^t}}$"}, {"title": "4.1 An Investment Rule for the Bayesian", "content": "We next generate an optimal investment rule for the Bayesian from the predicted probability on states. A Bayesian\ntrader n with predicted probability $q_t$ who wants to maximize the expected growth rate of wealth selects at time\nt an investment rule that solves:\n\n$\\underset{a_{st} \\geq 0}{Max} \\sum_{s=1}^S q_t \\log(\\frac{a_{st} w_t^n}{p_{st}})$\n\ns.t. $\\sum a_{st} = 1$,\n\n$\\underset{a_{st} \\geq 0}{Max} \\sum_{s=1}^S q_t \\log(a_{st} \\frac{w_t^n}{p_{st}})$\n\ns.t. $\\sum a_{st} = 1$.\n\nThe solution is clearly $a_{st} = q_t$. The Bayesian agent should \u201cbet its beliefs.\u201d"}, {"title": "4.2 Wealth Dynamics for the Bayesian", "content": "A Bayesian agent that maximizes the expected growth rate of its wealth uses the investment rule $a_t^n = q_t$\nat each time t (i.e., betting its beliefs). We consider the evolution of the wealth of the Bayesian relative to any other\ntrader who uses an investment rule that does not depend on future states, i.e., it's time t value depends only on\nstate realizations through time t. The log ratio of their wealths is, from Section 2\n\n$\\log(r_T^{nm}) = \\sum_{t=1}^T \\sum_{s=1}^S 1\\{s_t=s\\} \\log(\\frac{q_{st}^n}{q_s}) - \\sum_{t=1}^T \\sum_{s=1}^S 1\\{s_t=s\\} \\log(a_{st}^m) + \\log(r_0^{nm})$.\n\nNote that\n\n$\\sum_{t=1}^T [\\log(\\frac{a_{st}^n}{q_s})] - \\sum_{t=1}^T [\\log(\\frac{a_{st}^m}{q_s})] = \\sum_{t=1}^T I_q(a^m_t) - \\sum_{t=1}^T I_q(a^n_t) + \\log(r_0^{nm})$.\n\nIf one of the models that the Bayesian considers is the correct model, q, then $I_q(a^m_t)$ converges exponentially to 0\nalmost surely. So in this case, $\\sum_{t=1}^T I_q(a^m_t)$ is finite almost surely. The relative entropy $I_q(a^m_t)$ is at least 0 for any\ninvestment rule and is 0 if $a^m_t = q$ for all t. Thus, the sums of relative entropies are at least equal to the negative\nof the finite sum of relative entropies for the Bayesian.\nSubtracting the means from the sums in Equation (20) yields a martingale, and the arguments in [17] imply the\nfollowing conclusion:\nProposition 4.2. Let $r_T^{nm}$ be the wealth ratio of a Bayesian learner n competing against an agent m playing a = q\nfor all t. Then $\\liminf_{t \\rightarrow \\infty} E[r_T^{nm}] > 0$ almost surely; i.e., the Bayesian survives."}, {"title": "5 The Regret of Bayesian Learners", "content": "In this section, we further analyze the relationship between an agent's regret to its wealth level and long-term\nsurvival in market dynamics, and use this analysis to derive the regret level of Bayesian learners. This relationship\ndepends on the competition. In particular, we saw in Theorem 3.1 that the learners who survive (have a positive\nexpected wealth level in the limit) are only those who maintain a constant regret gap relative to the best competitor\nin the market.\nAs mentioned in Section 3, using the state distribution q as the investment rule (if q were known) maximizes the\nexpected growth rate among constant strategies. However, even this strategy incurs some regret, as it generally\ndiffers from the best strategy in hindsight. We now ask: what is the regret level of using the true state distribution\nq as the investment rule? This regret level represents the best attainable expected regret. Thus, by Theorem 3.1,\nevaluating this regret level would serve as the benchmark for determining which regret levels ensure survival\n(with high probability, see Definition 2.1) against any competitors who do not have information about future\nstate realizations. To set the ground, let us first consider the following definition:\nDefinition 5.1. Fix an arbitrary history of state realizations $s_1, . . ., s_T$ and denote the empirical distribution of\nthis history by $\\hat{q}$, where $\\hat{q}_s = \\frac{1}{T} \\sum_{t=1}^T 1\\{s_t=s\\}$. A \"magic agent\u201d indexed also by $\\hat{q}$, is an agent playing the (eventual)\nempirical distribution in every step. That is, $a^T_t = \\hat{q}_s$ for all t $ \\in [T]$.\nNote that this agent uses information regarding future realizations of random states, which is not available to\nreal agents in a stochastic environment. Next, we derive the regret level of using the correct distribution of states\nq as the investment strategy.\nTheorem 5.2. An agent using the state distribution q as the investment strategy for all t has a constant expected\nregret. The expected regret, denoted R, depends only on the number of states S.\nProof. Consider an agent indexed by q using the state distribution q as its investment strategy; i.e, $a^T_t = q$ for all\nt, and we denote the seqeunce by $q_{1:T}$. Agent 1 competes with a magic agent. The regret of the agent playing\nstrategy q is denoted $R_T (q_{1:T})$, and denote its expectation by $R = E[R_T (q_{1:T})]$. On one hand, using the fact that $\\hat{q}$\nhas no regret by definition, from Equation 16 we have\n\n$R_T (q_{1:T}) = \\log(r^{1\\hat{q}}) - \\log(r^{\\hat{q}\\hat{q}})$.\n\nOn the other hand, taking an expectation of Equation 6, we have\n\n$E [\\log(r^{1\\hat{q}})] = E[\\sum_t \\sum_s 1\\{s_t=s\\} \\log(\\frac{q_s}{\\hat{q}_s})] + \\log(r^{1\\hat{q}})$\n$= T E[\\sum \\frac{n_s}{T} \\log(\\frac{q_s}{\\hat{q}_s})] + \\log(r^{1\\hat{q}}) = T \\cdot E[I_{\\hat{q}}(q)] + \\log(r^{1\\hat{q}}).$"}, {"title": "6 Imperfect Bayesians", "content": "In the preceding sections, we saw that a perfect Bayesian is optimal in the sense that it survives almost surely in\nthe market and drives out any player with regret increasing over time. In this section, we explore a scenario\nwhere a Bayesian learner suffers small errors, either in its set of prior distributions or in its update rule. We find\nthat, while perfect Bayesians are optimal, Bayesian learning is also very fragile; for example, it is key that the\ncorrect probabilities is one of the model they consider. Specifically, we show that an imperfect Bayesian would\neventually vanish from the market, either against the state distribution q or in competition with any player with\nsub-linear regret. This holds even if the Bayesian's errors are very small and have zero mean."}, {"title": "6.1 Bayesian Learning with Inaccurate Priors", "content": "As mentioned in Section 4, a Bayesian learner with q not within the support of its prior converges to using the\nbest strategy within its prior (i.e., the one closest to q in relative entropy), denoted here by q'. This convergence\nproperty leads to the following result.\nTheorem 6.1. A Bayesian agent with the state distribution q not in its support incurs regret linear in T, and vanishes\nfrom the market in competition against any no-regret learner.\nNote that this contrasts with the case of Theorem 5.4, where a perfect Bayesian dominates the market in\ncompetition with any learners with regret increasing in time.\nProof. Let n be the index of a Bayesian agent with q not within its support, and let q' $ \\neq $ q be the strategy within\nthe support that is closest to q in relative entropy. By Proposition 4.1, the strategy used by the Bayesian agent,\n$a^n_t$, converges almost surely to q'. Hence, intuitively, for sufficiently large t, the strategies remain bounded away\nfrom q, leading to linear regret from that point onward. In more detail, define $I_0 = I_q(q') > 0$. In every sample\npath i (i.e., an infinite sequence, indexed by i, of state realizations and the ensuing sequence of $a^n_t$), except on a\nset of measure zero, there exists a time $T_i$ such that for all t > $T_i$, we have $I_q(a^n_t) > I_0$. The regret accumulated up\nto time $T_i$ is constant (possibly positive or negative). The expected regret difference relative to strategy q at t > $T_i$\nis $E[U^t (q) \u2013 U^t (a^n_t)] \\geq I_0$, and so the expected regret relative to q until time T is at least $I_1 \\cdot (T \u2013 T_i) + const$.\nBy Theorem 5.2, the strategy q incurs only constant regret, so the regret of the Bayesian agent converging to q'\ngrows linearly in T. Now, consider a no-regret learner. Such a learner has sublinear worst-case regret and thus\nsublinear expected regret. By Theorem 3.1, the imperfect Bayesian agent who has linear regret vanishes."}, {"title": "6.2 Bayesian Learning with Noisy Updates", "content": "Next, we consider a different type of imperfect Bayesian learner that does have q in its prior, but in every\nstep performs slightly inaccurate \u201ctrembling hand\u201d updates. Also here, we demonstrate that the optimality of\nBayesian learning is very fragile, even when the correct distribution lies in the support. To model this, we define\na noisy Bayesian learner as one who at each step, either slightly over-weights the current observation or slightly\nover-weights its current prior, such that, in expectation, both the data and the prior receive the correct weights in\nevery step (i.e., the errors in weight have zero mean).\nFor concreteness, consider the following scenario of a learner attempting to learn a distribution of states. Suppose\nthere are two states, $s_t = 0$ with a fixed probability $q \\in (0, 1)$, and $s_t = 1$ otherwise. The learner considers two\nmodels: $\\theta_a = q$, which is the correct model, and $\\theta_b \\neq q$, with $\\theta_b < 1$. The log-likelihood is given by\n\n$L(s_t) = (1 - s_t) \\log(\\frac{\\theta_a}{1 - \\theta_a}) + s_t \\log(\\frac{1 - \\theta_a}{1 - \\theta_b})$.\n\nNow suppose that in every step t the Bayesian learner performs \u03b7-noisy updates where it over- or under-weights\nthe data compared to the prior with a small excess weight $\u03b7_t$, where $\u03b7_t$ has zero mean. Specifically, for a parameter\n$\u03b7 > 0, \u03b7_t = \u03b7$ with probability 1/2 and $\u03b7_t = \u2212\u03b7$ otherwise. We find that even a tiny error with zero mean can have\na significant impact, essentially breaking the learning process.\nTheorem 6.5. For any $\u03b7 > 0$, the Bayesian learner with \u03b7-noisy updates does not converge.\nProof. Let $e > 0$. The log-likelihood ratio under the noisy update rule takes the following form:\n\n$\\log(\\frac{P_t(\\theta_a)}{P_t(\\theta_b)}) = (1 + \\eta_t)L(s_t) + (1 - \\eta_t) \\log(\\frac{P_{t-1}(\\theta_a)}{P_{t-1}(\\theta_b)})$\n\n$= (1 + \\eta_t) \\sum_{t=0}^{t-1} L(s_{t-\\tau}) (1 - \\eta_t) + \\log(\\frac{P_0(\\theta_a)}{P_0(\\theta_b)}) \\prod_{k=0}^{t-1} (1 - \\eta_k)$,\n\nwhere the empty product equals one, and we define $\u03b7_0 = 0$. The products can be simplified:\n\n$\\prod_{t=0}^{t} (1 - \\eta_t) = (1 - \\eta)^{n_+(t)} (1 + \\eta)^{n_-(t)}$,\n\nwhere $n_+(t)$ is a binomial random variable counting the number of times $\u03b7_{\\tau \\leq t} = \u03b7$, and $n_\u2212(t) = t \u2212 n_+(t)$. The\nlast term can be written as\n\n$\\log(\\frac{P_0(\\theta_a)}{P_0(\\theta_b)}) \\cdot (1 - \\eta)^{n_+(t)} (1 + \\eta)^{n_-(t)} = \\log(\\frac{P_0(\\theta_a)}{P_0(\\theta_b)}) \\cdot [(\\frac{1 - \\eta}{1 + \\eta})^{n_+(t)}] \\cdot (1 + \\eta)^t.$"}, {"title": "7 Simulations", "content": "Imperfect Bayesian Learning: Consider the following toy example. There is market with two assets where\nagent 1 is a Bayesian learner and agent 2 is a UCB learner, starting from equal wealth levels.10 The distribution of\nstates is q = (0.7, 0.3), i.e., state 1 occurs with probability 0.7, and state 2 otherwise. The Bayesian learner has a\nprior consisting of three hypotheses: (0.8, 0.2), (0.9, 0.1), (0.3, 0.7), where the first hypothesis is closest to q in\nrelative entropy, but still has an error. The UCB learner considers a similar set of distributions as its action set:\n(0.7, 0.3), (0.9, 0.1), (0.3, 0.7), but the first of these being the correct distribution q. The example demonstrates\nthe trade-off between short-term gains for the inaccurate Bayesian from converging quickly to a distribution\nclose to the correct one, and the long-term dominance of the learner who converges to the correct distribution."}]}