{"title": "Revisiting Privacy, Utility, and Efficiency Trade-offs\nwhen Fine-Tuning Large Language Models", "authors": ["Soumi Das", "Camila Kolling", "Mohammad Aflah Khan", "Mahsa Amani", "Bishwamittra Ghosh", "Qinyuan Wu", "Till Speicher", "Krishna P. Gummadi"], "abstract": "We study the inherent trade-offs in minimizing privacy risks\nand maximizing utility, while maintaining high computational\nefficiency, when fine-tuning large language models (LLMs).\nA number of recent works in privacy research have attempted\nto mitigate privacy risks posed by memorizing fine-tuning\ndata by using differentially private training methods (e.g.,\nDP), albeit at a significantly higher computational cost (inef-\nficiency). In parallel, several works in systems research have\nfocussed on developing (parameter) efficient fine-tuning meth-\nods (e.g., LoRA), but few works, if any, investigated whether\nsuch efficient methods enhance or diminish privacy risks. In\nthis paper, we investigate this gap and arrive at a surprising\nconclusion: efficient fine-tuning methods like LoRA miti-\ngate privacy-risks similar to private fine-tuning methods like\nDP. Our empirical finding directly contradicts prevailing wis-\ndom that privacy and efficiency objectives are at odds during\nfine-tuning. Our finding is established by (a) carefully defin-\ning measures of privacy and utility that distinguish between\nmemorizing sensitive and non-sensitive tokens in training and\ntest datasets used in fine-tuning and (b) extensive evaluations\nusing multiple open-source language models from Pythia,\nGemma, and Llama families and different domain-specific\ndatasets.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remark-\nable abilities in solving varied natural language tasks [42],\nwith a host of applications in multiple domains such as edu-\ncation [48], medical chatbot [46], AI assistant [10], etc. Their\nability is acquired by first pre-training models on a large\nWeb corpus [23], which results in general language under-\nstanding [6] and then fine-tuning the pre-trained models on\ndomain-specifc data, which customizes their performance to\nspecific tasks [55]. Two foundational challenges with pre-\ntraining/fine-tuning these models at scale are centered around\nenhancing privacy and efficiency, while retaining the models'\nutility. The former is related to reducing the risk of LLMs\nleaking sensitive user information contained in the training\ndata, while the latter is related to reducing the computational\ncost of training.\nA long line of recent research in the privacy community has\nfocussed on methods to mitigate privacy risks when training\nLLMs [1, 44, 51, 56]. A notable example of such methods is\ndifferentially privacy (DP) [1]. Similarly, a flurry of recent\nresearch in the systems community has explored parameter-\nefficient fine-tuning (PEFT) methods in LLMs [14]. A no-\ntable example of this class of methods is low-rank adapta-\ntion (LoRA) [20]. However, no prior works, to the best of\nour knowledge, have investigated the privacy risks associated\nwith the efficient training methods. So the central question\ndriving our research here is: do efficient fine-tuning methods\nenhance or mitigate privacy risks during training?\nOur question above is motivated by the following high-level\nobservations about DP and LoRA: methodologically, both DP\nand LoRA restrict the impact training examples can have on\nmodel parameters \u2013 DP deliberately through its noisy gradi-\nent update, and LoRA through low-rank adaptation. However,\nDP incurs significant additional computational overhead [11]\ncompared to traditional fine-tuning, while LoRA significantly\nreduces the computational costs. So the answer to the above\nquestion can have significant consequences for achieving\ngood privacy-efficiency-utility tradeoffs when fine-tuning. For\ninstance, if LoRA mitigates privacy risks of training, then it\nsuggests that this method can simultaneously achieve both\nprivacy and efficiency objectives, contradicting the conven-\ntional wisdom drawn from DP literature that privacy comes\nat a computational cost. One of the key (surprising) findings\nof our work below lies in establishing that LoRA does indeed\nmitigate privacy risks.\nWhen attempting to answer the above question, we encoun-\ntered a more foundational question that has been not been\nwell-addressed by the existing literature namely, how should\none quantify the privacy risks associated with a fine-tuning\nmethod, so that it allows for a performance comparison across\ndifferent methods? For instance, the claimed privacy benefits"}, {"title": "Contributions", "content": "We summarize our main findings below:\n1. Quantifying privacy and utility, when training LLMs: We\nconceptually argue and empirically demonstrate that LLM's\nability to recollect (predict) sensitive and non-sensitive data in\ntraining (test) datasets are so starkly different that we need to\naccount for them when quantifying privacy and utility. Privacy\nis best captured by a model's ability to recollect sensitive\ntokens in training data, while utility is best captured by the\nmodel's ability to predict non-sensitive tokens in test data.\n2. Exaggerated estimates of privacy threat from LLM mem-\norisation: We show that compared to our measure, existing\nmeasures significantly over-estimate privacy risks from LLM\ntraining. LLMs recollect non-sensitive tokens at a signifi-\ncantly higher level than sensitive tokens. So by not distin-\nguishing between them, existing studies are over-estimating\nprivacy threats.\n3. Comparing privacy-utility-efficiency tradeoffs for three dif-\nferent fine-tuning methods: Our measures allow us to conduct\na systematic and extensive empirical study of three differ-\nent fine-tuning methods: full fine-tuning, DP, and LoRA, us-\ning models from three different LLM families, Pythia [4],\nGemma [45] and Llama [47] over both real-world and\nsynthetically-generated datasets. For each method and model,\nwe evaluate privacy, utility (in terms of train and test loss\nrespectively) and its impact on benchmark performance, and\nefficiency. Our comparative study yields several interesting\ninsights: we find that \u2013 full fine-tuning results in poor utility-\nprivacy tradeoffs; DP offers the best utility-privacy tradeoffs,\nbut is computationally very expensive; LoRA is almost on\npar with DP in terms of utility and privacy, but is much more\ncomputationally efficient.\n4. Feasibility of achieving all the three privacy, utility and ef-\nficiency objectives simultaneously: Our findings about LoRA\nperformance suggest that it may be possible for methods to\nachieve all three objectives simultaneously. Our results chal-\nlenge prevailing wisdom that enhancing privacy during train-\ning is necessarily more computationally expensive and call for\ninvestigating privacy benefits of existing and new parameter\nefficient fine-tuning methods."}, {"title": "2 Related Work", "content": "Privacy Attacks and Quantification. Privacy concerns in\nLLMs have gained attention in recent years [24, 49], par-\nticularly the possibility of exposing data via membership\ninference attacks (MIA) [12, 22, 38, 40] and training data\nextraction attacks [9, 41, 43]. While membership inference\nattacks have the goal of identifying if a certain datapoint was\nin the training dataset using model confidence scores, training\ndata extraction attacks try to extract specific parts of data\nfrom the training dataset using different prompting strategies.\nA recent study [54] underscores that membership inference\nattacks may not be reliable due to their practical limitations of"}, {"title": "3 Quantifying Privacy and Utility", "content": "In this section, we introduce the distinction between sensitive\nand non-sensitive entities when quantifying privacy and utility\nof an LLM. We conduct case studies to compare our quantifi-\ncation with existing measures. Finally, we demonstrate how\nthe privacy threat is unintentionally exaggerated in existing\nmethods due to the lack of distinction between sensitive and\nnon-sensitive entities."}, {"title": "3.1 Rethinking Privacy and Utility", "content": "Existing studies at the intersection of privacy and natural\nlanguage processing [31, 44, 52, 56] seek to enhance privacy\nwhile maintaining model utility. Utility is generally assessed\nbased on model performance, such as loss, accuracy, or per-\nplexity across the entire test dataset. Privacy is evaluated in\nterms of performance measures on the entire training dataset\nor theoretical guarantees in differential privacy (DP)."}, {"title": "3.2 Why do we distinguish between sensitive\nand non-sensitive entities?", "content": "In this section, we present evidence supporting the importance\nof distinguishing between sensitive and non-sensitive entities\nin natural language text while quantifying privacy and utility\nof an LLM."}, {"title": "I. Analyzing privacy and utility while fine-tuning an LLM", "content": "In order to align with LLM terminology, hereafter, we use\ntokens to denote entities. Fine-tuning involves iterating an\nLLM on a specific dataset containing both sensitive and non-\nsensitive tokens. We illustrate how our measure of privacy and\nutility compares to existing measure in a typical fine-tuning\nscenario, highlighting a key difference: our approach distin-\nguishes between sensitive and non-sensitive tokens, whereas\nthe existing measure does not."}, {"title": "3.  1 Rethinking Privacy and Utility", "content": "Natural language text may contain both sensitive and non-\nsensitive words, referred to as entities. For example, sensitive\nentities include names, addresses, phone numbers, order IDs,\nand other personally identifiable information. In contrast, non-\nsensitive entities generally involve semantic and/or syntactic\ncompletions following predictable patterns in language gen-\neration tasks. Informally, sensitive entities are drawn from\na large search space (e.g., a random sequence of digits), re-\nsulting in high entropy and low predictability. In contrast,\nnon-sensitive entities are more restricted in their occurrences\n(e.g., a subject is typically followed by a verb), leading to\nlow entropy and high predictability. Several studies [3, 44, 56]\ndistinguish between sensitive and non-sensitive entities in\ntheir proposed privacy leakage mitigation methods. However,\nthe distinction is not leveraged in the quantification of pri-\nvacy and utility, which is essential for a granular evaluation\nas discussed next.\nQuantification of privacy and utility. In this work, we quan-\ntify privacy and utility by accounting for sensitive and non-\nsensitive entities. Considering a training dataset and a test\ndataset in a general LLM training pipeline, we quantify pri-\nvacy as the recollection of sensitive entities in the training\ndata and utility as the prediction of non-sensitive entities in\nthe test data. Our motivation for the quantification is two-\nfold: (1) privacy of a model is generally related to training\ndata, while utility is the model's performance on the test data.\n(2) when quantifying privacy, we deliberately disregard non-\nsensitive entities, since they are more predictable and not\nsensitive to a specific person or entity. Similarly, in quantify-\ning utility, we ignore sensitive entities in the test data, since\nthe sensitive entities are rare (and possibly unseen during\ntraining), whereas predicting non-sensitive entities indicates\nthe general language understanding ability of LLMs. Next, we\nprovide two pieces of evidence supporting why the distinction\nis important."}, {"title": "3.  2 Why do we distinguish between sensitive\nand non-sensitive entities?", "content": "In this section, we present evidence supporting the importance\nof distinguishing between sensitive and non-sensitive entities\nin natural language text while quantifying privacy and utility\nof an LLM."}, {"title": "I. Analyzing privacy and utility while fine-tuning an LLM", "content": "In order to align with LLM terminology, hereafter, we use\ntokens to denote entities. Fine-tuning involves iterating an\nLLM on a specific dataset containing both sensitive and non-\nsensitive tokens. We illustrate how our measure of privacy and\nutility compares to existing measure in a typical fine-tuning\nscenario, highlighting a key difference: our approach distin-\nguishes between sensitive and non-sensitive tokens, whereas\nthe existing measure does not."}, {"title": "4 Privacy-Utility-Efficiency tradeoffs in differ-\nent fine-tuning methods", "content": "We use the distinction between sensitive and non-sensitive\ntokens to study the privacy and utility impact of training mod-\nels with three different fine-tuning methods: full fine-tuning\n(FFT), Differentially Privacy (DP), and Low-Rank Adapta-\ntion (LoRA). We also investigate the computational efficiency\nof each method. Our goal is to answer the following ques-\ntions: \u201cHow prone is each method to recollecting the sensitive\nparts of the training data? (Privacy)\u201d, \u201cHow effective is\neach method at predicting non-sensitive parts of test data?\n(Utility)\", \"What is the computational cost associated with\neach method? (Efficiency)\u201d.\nTo answer these questions, we use each fine-tuning method\nto train three models from different families, Pythia-1B [4],\""}, {"title": "4.1 Method 1 : Full fine-tuning", "content": "Update rules: Full fine-tuning (FFT) updates all model pa-\nrameters at each step:\n$W_{i+1} = W_{i} - \\eta \\nabla_{W_i} L(M_{W_i}(X), X)$\nPrivacy-Utility trade-off: Figures 7a and 7b show the\nprivacy-utility trade-off for the CustomerSim and SynBio\ndatasets, respectively. In these figures, privacy increases with\nthe training loss on sensitive tokens (up \u2191 on the y-axis), while\nutility increases when the test loss on non-sensitive tokens\ndecreases (right \u21d2 on the x-axis). Each curve starts with the\nbaseline performance of the pre-trained model. For Customer-\nSim (Figure 7a), as training advances (denoted by an arrow\n\u2192 on the lines), privacy progressively decreases (lower on\nthe y-axis), while utility improves (rightward on the x-axis)\nfor approximately the first 5 epochs across all models before\nstabilizing and eventually declining (leftward on the x-axis).\nHowever, for SynBio (Figure 7b), the privacy-utility trade-off\nprimarily worsens for Gemma and Llama models. On exam-\nining these curves, one can select a desired checkpoint that\naligns with specified privacy and utility thresholds.\nImpact on benchmark datasets: Figures 8a, 8b, and 8c show\nthe fully fine-tuned Gemma model's accuracy at each epoch\nfor the three benchmarks: SCIQ, MMLU, and Hellaswag,\nrespectively. Note that the accuracy corresponding to the first\npoint represents the performance of the pre-trained model.\nWe observe that full fine-tuning shows a substantial decline\nin accuracy (around 0.75, 0.09, and 0.3 decrease in accuracy\nin SCIQ, MMLU, and HellaSwag, respectively).\nEfficiency: FFT serves as our efficiency baseline. It has mod-\nerate compute requirements (discussed above), and relatively\nhigh memory requirements, since in addition to the input-\ndependent activations, we need to keep four numbers per\nmodel parameter in GPU memory: the parameter value, its"}, {"title": "4.2 Method 2 : Differential Privacy", "content": "Differential Privacy (DP) algorithms [1] aim to safeguard\nthe privacy of individual training data points by limiting their\ninfluence on the gradient updates during training.\nUpdate rules: DP clips the $l_2$ norm of each data point's gradi-\nent at a threshold T, followed by adding noise with magnitude\n\u03c3 to each clipped gradient. The purpose of clipping is to re-\nduce data sensitivity by ensuring that the impact of data points\nwith high gradient magnitudes on the model parameters is\nlimited. Adding noise further obscures the contribution of\nindividual data points, making it difficult to infer specific data\npoints from the model.\n$W_{t+1} = W_t - \\eta \\text{Noise}(\\text{Clip}( \\nabla_{W_t} L(M_{W_t}(x_i), x_i)))$\n$\\text{Clip}(y) = y/\\text{max}(1, \\frac{||y||_2}{T})$\n$\\text{Noise}(y) = y + \\mathcal{N}(0, \\sigma^2 T^2 I)$\nWe vary the noise hyperparameter \u03c3 for the experiments.\nThe clipping gradient norm T is fixed at $10^{-2}$, as in [44].\nPrivacy-Utility trade-off: Figures 9a, 9b, and 9c illustrate\nthe privacy-utility trade-off when varying the noise 6 on the\nCustomerSim dataset across the Pythia, Gemma, and Llama2"}, {"title": "4.3 Method 3 : Low-Rank Adaptation (LoRA)", "content": "LORA [20] is a parameter-efficient fine-tuning method de-\nveloped to reduce the compute and memory requirements of\nfine-tuning LLMs, and to reduce the size of storing fine-tuned\ncheckpoints. It enjoys large popularity for LLM fine-tuning.\nUpdate rules:\n$W_{i+1} = W_0 + \\Delta W_{i+1}$\n$\\Delta W_{i+1} = \\Delta W - \\eta \\nabla_{\\Delta w_i} L(M_{W_i}(X), X)$\nLORA freezes the weights of the pretrained base model $W_0$\nand only fine-tunes an adapter matrix $W$. During training,\nW is stored separately from $W_0$ as two low-rank matrices\nwith rank r: $W_i = B_i A_t$ with $B_i \\in \\mathbb{R}^{d \\times r}, A_t \\in \\mathbb{R}^{r \\times k}, W_0 \\in$\n$\\mathbb{R}^{d \\times k}$. r is typically very small, often between 4 and 32. a is\na constant that controls how much the LoRA adapters affect\nthe behavior of the base weights $W_0$.\nWe hypothesize that LoRA's low-rank updates restrict the\nmodel's capacity to memorize precise details, which could\nhave an effect similar to the noisy updates in DP. Therefore,\nLORA has the potential to provide better privacy-utility trade-\noffs than FFT, similar to DP, while also being computationally\nmore efficient.\nPrivacy-Utility trade-off: We investigate the effects of vary-\ning both the rank r and scaling parameter a of LoRA. We use\ncommon rank values of 16 and 32 and $a \\in {16, 32, 64, 128}$.\nWhile LoRA has been explored for privacy in conjunction\nwith DP [51], there has been no prior work that specifically\nexamines the privacy benefits of LoRA alone."}, {"title": "5 Comparison of Fine-tuning Methods", "content": "In this section, we compare all fine-tuning methods across\nthree key aspects: privacy, utility, and efficiency. Recall that\nprivacy is measured as training loss on sensitive tokens and\nutility is measured as test loss on non-sensitive tokens. For\nefficiency, we measure floating point operations (FLOPs)\nbased on the number of operations incurred (e.g., matrix mul-\ntiplication, addition, etc.) during training. In this section, for\nDP we use a noise ratio of \u03c3 = 0.1 and for LORA we use\nr = 16, \u03b1 = 16. Figures 14a-14c present the Pareto plots\nfor CustomerSim, comparing the three fine-tuning strategies\nacross three dimensions: privacy, utility, and efficiency.\nPrivacy: Regarding privacy, as previously noted, full fine-"}, {"title": "6 Concluding Discussion", "content": "In this paper, we examine the trade-offs among privacy, utility,\nand efficiency while fine-tuning an LLM. The traditional wis-\ndom of achieving privacy comes at the cost of computational\ninefficiency using dedicated methods like DP. In contrast,\nwe demonstrate that parameter efficient fine-tuning methods\nlike LoRA, initially designed for efficiency, achieves privacy\nof sensitive data without any computational overhead. Si-\nmultaneously, LoRA retains the utility of general language\nunderstanding compared to DP, or even full-fine-tuning, real-\nizing the superiority of LoRA in optimizing all three aspects.\nTowards our investigation, we establish the significance of re-\ndefining privacy and utility using a careful distinction between\nsensitive and non-sensitive counterparts of the fine-tuned data.\nThrough case studies, we demonstrate how existing measures\nexaggerate privacy threats and undermine the utility of an\nLLM. Our paper calls for a joint venture of privacy and sys-\ntems communities in achieving privacy-aware efficient fine-\ntuning of LLMs while retaining utility."}, {"title": "7 Ethics considerations", "content": "This study utilized publicly available datasets [18,44], some\nof which included identifiable information such as personal\ndetails. However, third-party organizations pre-processed and\nvalidated the datasets to ensure that no real individuals' data\nwere present, thus mitigating potential privacy concerns.\nThis project received ethical clearance from the Ethical\nReview Board of the affiliated institution on October 21, 2024\n(Approval No. 24-09-4), with no ethical concerns raised."}, {"title": "8 Open science", "content": "This work promotes transparency and reproducibility in re-\nsearch on privacy and utility in large language models (LLMs).\nTo enable further investigation, we will release:\n1. Code and Framework: The implementation of our pro-\nposed privacy measurement framework, which distin-\nguishes between sensitive and non-sensitive tokens,\nalong with scripts for privacy leakage analysis and utility-\nefficiency evaluation.\n2. Datasets and Preprocessing information: Links to pub-\nlicly available datasets used in this study, along with"}, {"title": "9 Hyperparameters", "content": "The following hyperparameters were used for fine-tuning our\nmodels:"}, {"title": "10 Validating GPT-4 Predictions", "content": "We analyzed the predictions shown in the heatmap in Fig-\nure 16 and observed that misclassified instances were fre-\nquently assigned to sections of a similar nature (e.g., NIH\nExplorer misclassified as PubMed Central). This overlap sug-\ngests that the nature of the misclassifications may not always\nindicate clear inaccuracies, making it difficult to definitively\nassess the accuracy of certain misclassified instances."}, {"title": "11 Prompt for Annotating Privacy-Sensitive\nInformation", "content": "We used the following prompt to obtain the privacy-sensitive\ninformation from GPT-40 for CustomerSim:\n\u201cYou are a private data identifying bot. You will be provided a\nconversation between a user and a customer service bot. You\nneed to identify ALL instances of private data in the conversa-\ntion. Private data includes any information that can be used to\nidentify an individual, such as names, phone numbers, email\naddresses, and locations. It can also include any other sensi-\ntive information, such as credit card numbers, social security\nnumbers, tracking data, health information etc.. Identify all of\nthese use your discretion to identify any other information that"}, {"title": "12 Human Survey: Assessing Privacy-\nSensitivity in Memorized Segments", "content": "We conducted a human annotation survey to assess if in-\nstances memorized by an LLM contain privacy-sensitive in-\nformation, sampling 100 sequences divided into 5 question-\nnaires. Each was given to 40 Prolific workers. After an in-\ntroductory explanation, participants evaluated each instance\nby (1) flagging privacy-sensitive information (ignoring rou-\ntine commands unless uniquely identifiable) and (2) listing\ndetected items if flagged. Figure 6 shows that most questions\nwere labeled as non-sensitive."}, {"title": "13 Human Survey: Assessing the Quality of\nPrivacy-Sensitive Annotations from Pre-\nsidio and GPT-4", "content": "We evaluated privacy-sensitive annotations from Presidio and\nGPT-4 through two Prolific surveys, each with 10 samples\nfor 40 participants. Binary Survey: For each sample, partici-\npants viewed side-by-side annotations from both tools (order\nrandomized) and chose the more accurate one. Figure 17a\nshows GPT-4 was consistently preferred over Presidio for\nidentifying privacy-sensitive information. Multiple-Choice\nSurvey: Each of the 10 instances included 5 samples from\neach tool, displayed randomly. Participants had four options:\n(1) Accurate (all sensitive info correctly annotated), (2) Under-\nAnnotated (some sensitive info missed), (3) Over-Annotated\n(non-sensitive info included), and (4) Mixed-Annotated (both\nmissed and non-sensitive info annotated). Figure 17b shows\nGPT-4 rated as \"Accurate\" across datasets."}, {"title": "14 Why distinguish between sensitive and non-\nsensitive entities?", "content": "Figure 18 shows the plots observed in Figure 2 in log-scale\nfor better distinction."}, {"title": "15 Exploring the Tradeoffs between Privacy\nand Utility", "content": "We see a similar trade-off between utility and privacy when\nusing Presidio annotations in Figure 19, as observed in Fig-\nure 7. For the CustomerSim dataset, we notice that privacy\ndecreases at the beginning of training, while utility improves.\nHowever, as training progresses, both metrics start to worsen.\nIn contrast, the SynBio dataset shows a decline in both utility\nand privacy, except for the Pythia model, which initially expe-\nriences an improvement in utility during the first few epochs."}]}