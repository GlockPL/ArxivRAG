{"title": "PRIVACY-PRESERVING FEDERATED LEARNING WITH\nDIFFERENTIALLY PRIVATE HYPERDIMENSIONAL COMPUTING", "authors": ["Fardin Jalil Piran", "Zhiling Chen", "Mohsen Imani", "Farhad Imani"], "abstract": "Federated Learning (FL) is essential for efficient data exchange in Internet of Things (IoT) environ-\nments, as it trains Machine Learning (ML) models locally and shares only model updates. However,\nFL is vulnerable to privacy threats like model inversion and membership inference attacks, which\ncan expose sensitive training data. To address these privacy concerns, Differential Privacy (DP)\nmechanisms are often applied. Yet, adding DP noise to black-box ML models degrades performance,\nespecially in dynamic IoT systems where continuous, lifelong FL learning accumulates excessive\nnoise over time. To mitigate this issue, we introduce Federated HyperDimensional computing with\nPrivacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI) framework that\ncombines the neuro-symbolic paradigm with DP. FedHDPrivacy carefully manages the balance\nbetween privacy and performance by theoretically tracking cumulative noise from previous rounds\nand adding only the necessary incremental noise to meet privacy requirements. In a real-world\ncase study involving in-process monitoring of manufacturing machining operations, FedHDPrivacy\ndemonstrates robust performance, outperforming standard FL frameworks\u2014including Federated\nAveraging (FedAvg), Federated Stochastic Gradient Descent (FedSGD), Federated Proximal (Fed-\nProx), Federated Normalized Averaging (FedNova), and Federated Adam (FedAdam)-by up to 38%.\nFedHDPrivacy also shows potential for future enhancements, such as multimodal data fusion.", "sections": [{"title": "1 Introduction", "content": "The rapid expansion of the Internet of Things (IoT) has led to the widespread integration of sensing and computing\ntechnologies, connecting a vast array of devices to support applications in areas, including smart cities [1] and digital\nmanufacturing [2]. This interconnected landscape has facilitated the emergence of intelligent IoT applications, where\nArtificial Intelligence (AI) models play a crucial role in deriving actionable insights from the data produced by IoT\ndevices, such as in traffic management [3]. Traditionally, these AI-driven tasks were handled by centralized data\ncenters [4]. However, this approach is increasingly challenged by the impracticality of transmitting large volumes of\nIoT data to distant servers and the heightened risk of privacy breaches [5]. Relying on third-party data centers for AI\nprocessing can compromise sensitive information, including financial and healthcare records [6]. Therefore, there is a\npressing need for novel AI methods that not only protect privacy but also enhance the efficiency and intelligence of IoT\nnetworks and applications.\nFederated Learning (FL) has emerged as a powerful approach for building intelligent, privacy-preserving systems within\nthe IoT. FL operates as a decentralized AI model to be trained directly on IoT devices, referred to as clients, without the\nneed to transfer raw data to a central server [7]. As shown in Figure 1, IoT devices act as clients in this architecture,\ncollaborating with a central server to refine a global model. The process begins with the server initializing the global\nmodel with set parameters, which are then distributed to the clients. Each client uses its locally generated data to update\nthe model and sends the updates back to the server. The server aggregates the locally updated models, improving the\nglobal model in an iterative manner. This decentralized approach leverages the computational capabilities of distributed\nIoT devices, enhancing training efficiency while ensuring data privacy [8].\nIn IoT environments, data is continuously generated and frequently change. This dynamic nature necessitates the\nimplementation of continuous learning within the FL framework. Continuous learning, or lifelong learning, allows\nmodels to continuously incorporate new data, enabling them to adapt to evolving environments and remain effective in\nreal-time applications. By regularly updating the model with new data streams, FL ensures that the global model stays"}, {"title": null, "content": "current and responsive to changing conditions across distributed devices [9]. This capability is particularly valuable in\nIoT systems, where the model must continually learn from new information to maintain high performance in dynamic,\nreal-world scenarios.\nWhile FL improves privacy by keeping user data on individual devices rather than transmitting it to a central server,\nit does not fully eliminate privacy risks. If an attacker gains access to either the clients' or server's models, they can\nexploit these models to perform model inversion or membership inference attacks, extracting sensitive and confidential\ninformation about the training samples. In a model inversion attack, the attacker analyzes the model's outputs or\ngradients to reconstruct the original training data, effectively revealing sensitive information used to train the model [10].\nFor example, by querying the trained model, an adversary can reverse-engineer the data to infer specific details about\nthe training samples. In contrast, a membership inference attack allows an adversary to determine whether a particular\ndata point was included in the training set [11]. This type of attack can reveal the presence or absence of specific\nindividuals in the dataset, leading to severe privacy breaches.\nBoth attacks pose significant risks to user privacy, even in seemingly secure environments. Techniques such as\ngenerative regression neural networks have shown that it is possible to extract sensitive information from shared model\nparameters [12]. Even in black-box scenarios, where the internal workings of the model are hidden, attackers can infer\nwhether certain individuals were part of the training set or recover features of their data [10]. As depicted in Figure 1,\nthese vulnerabilities enable adversaries to exploit FL systems and compromise the confidentiality of participants [13].\nAttackers can access ML models through four primary methods. The first method is eavesdropping, where adversaries\nintercept communication between clients and the central server. As illustrated in Figure 1, Since FL involves multiple\nrounds of communication between clients and the server, unprotected channels present a high risk of interception,\npotentially allowing attackers to access the global and local models and their parameters [14]. The second method is by\nposing as a malicious participant, where attackers disguise themselves as legitimate clients to interact directly with the\ncentral server [15]. This gives them access to the global model, enabling them to extract sensitive information or infer\nthe presence of specific data. The third method is server compromise, where attackers successfully hack into the central\nserver, gaining control over the global model and exposing sensitive data collected from multiple clients [16]. Lastly, in\nscenarios involving an untrusted server, clients may fear that the server itself could analyze the local models it receives,\npotentially identifying sensitive training data or determining if specific data was used in training [17]. In all four cases,\nattackers aim to exploit the models by using model inversion and membership inference attacks to extract sensitive\ninformation about the training samples, whether they gain access to the global or local models.\nAs vulnerabilities in ML models within FL frameworks are identified, the need for robust defense mechanisms\nbecomes increasingly critical. Recent studies have investigated various strategies to enhance privacy in FL. One\nwidely adopted method is anonymization, which involves generalizing or redacting specific attributes in a dataset\nto prevent the easy identification of individual records. However, with the advent of sophisticated privacy attacks,\nsuch as model inversion [10], which can reconstruct training data even with limited access to the models, traditional\nanonymization techniques have proven inadequate. Attackers often find ways to circumvent these defenses, particularly\nin high-dimensional datasets where anonymization fails to offer strong protection against the disclosure of sensitive\nattributes.\nHomomorphic Encryption (HE) has been introduced as a robust privacy-preserving technique that allows computations\nto be performed directly on encrypted data, eliminating the need for decryption [18]. While HE offers strong theoretical\nprivacy protection by enabling model training on encrypted datasets, its practical application in modern ML systems\nis hindered by significant computational overhead. This challenge is particularly evident in deep learning scenarios,\nwhere the processing of large datasets and complex models is common. Consequently, HE is often more applicable\nin situations where models are pre-trained and deployed as services [19]. Another promising technique is Secure\nMultiparty Computation (SMC), which enables multiple parties to jointly compute a function using their private inputs\nwithout disclosing these inputs to one another or a central server. This method removes the need for a trusted third\nparty and provides strong privacy guarantees in theory [20]. However, SMC also faces challenges due to its high\ncomputational and communication demands, making it less feasible to train intricate models in FL environments.\nDifferential Privacy (DP) has emerged as a more practical and robust solution for safeguarding privacy in AI systems.\nWhen applied to an ML model, DP protects against model inversion and membership inference attacks, making it a\nsuitable approach for securing FL frameworks. Recent advancements have enhanced the viability of DP, with companies\nlike Google, Microsoft, and Apple successfully integrating DP into their data collection practices to improve user privacy\nwhile retaining data utility [21, 22, 23]. DP achieves this by adding carefully calibrated noise to the data or model,\nensuring that individual records remain private during analysis [24, 25]. This noise addition ensures that the inclusion\nor exclusion of any single data point does not significantly impact the overall analysis, thereby preserving individual\nprivacy. As illustrated in Figure 2, DP can effectively neutralize model inversion attacks, where adversaries attempting\nto reconstruct training samples by querying the model will only retrieve random, non-informative signals unrelated to"}, {"title": null, "content": "the actual training data. Additionally, DP can thwart membership inference attacks by preventing adversaries from\ndetermining whether a particular sample was part of the training set.\nDP's ability to protect sensitive information while preserving data utility makes it a preferred choice among privacy-\npreserving techniques. In a FL structure, as depicted in Figure 2, DP can be applied to local models before they are\ntransmitted to the central server. As a result, these secured local models contribute to the global model, which is formed\nby aggregating the secured local models, thereby maintaining the security of the global model as well. The level of\nprivacy can also be adjusted at the server level, ensuring that the local models remain secure and resistant to potential\nbreaches by an untrusted server. This configuration protects the global model from server-side attacks, making it\ndifficult for adversaries to compromise the server. Additionally, the communication between the server and clients is\nsecure because the updates exchanged are already secured, rendering eavesdropping attacks ineffective. Furthermore,\nsince the server is secure, a malicious participant cannot extract any information from the secured global model through\ninteraction with the server. In conclusion, while anonymization, HE, and SMC each offer specific advantages for privacy\nprotection in FL, DP stands out as the most effective and practical solution, providing a strong balance between privacy\nand usability in real-world applications.\nWhile DP is highly effective at safeguarding sensitive information and maintaining data utility, it also presents challenges\nrelated to balancing privacy and accuracy. In the context of FL combined with DP, the amount of noise introduced\nmust be carefully managed to balance privacy and accuracy. Excessive noise can degrade model performance, while\ninsufficient noise can lead to privacy breaches. Therefore, evaluating privacy leakage for a given privacy budget is\ncrucial before deploying models or releasing datasets [12]. Moreover, ML models often function as complex black-box\nsystems, where their internal mechanisms are not easily understood or transparent. This lack of transparency is\nparticularly problematic in safety-critical fields, where the traceability and explainability of decision-making processes\nare crucial [26]. Additionally, the secure handling of data is essential, especially when dealing with sensitive and\npersonal information. In the FL framework, where data privacy is paramount, DP is widely adopted to protect sensitive\ninformation by adding noise to the data or model parameters. However, implementing DP within FL presents a\nsignificant challenge: as noise is added in each training round to maintain privacy, the cumulative effect can lead to\nsubstantial degradation in model performance. This issue is exacerbated by the black-box nature of ML models, making\nit difficult to determine the optimal amount of noise to add without compromising the model's accuracy [27].\nWe introduce Federated HyperDimensional computing with Privacy-preserving (FedHDPrivacy), an eXplainable\nArtificial Intelligence (XAI) framework for FL, to address the privacy challenges. This framework combines DP and\nHyperDimensional computing (HD) to ensure both security and accuracy during model aggregation. By integrating XAI\ntechniques, FedHDPrivacy allows for precise calculation of the noise required in each round, tracking the cumulative\nnoise added in previous rounds and adjusting the noise accordingly in subsequent rounds. This approach ensures\nthat only the minimal necessary noise is introduced, effectively balancing privacy and accuracy. The FedHDPrivacy\nframework offers a robust solution by controlling the noise added to both the clients' and the server's models in each\nround, thereby avoiding the degradation of model performance due to excessive noise. This secure and explainable FL\nframework not only safeguards confidential information but also preserves the utility and efficiency of the AI models\ninvolved. Moreover, our framework is specifically designed to address the challenges of continuous learning in IoT\nsystems under the FL paradigm. It ensures that the model remains up-to-date and effective in dynamic environments,\nthereby enhancing the practical utility of FL in real-world IoT scenarios. Our contributions in this paper are summarized\nas follows:"}, {"title": null, "content": "1. We introduce FedHDPrivacy, an explainable framework that enhances transparency and understanding in the\ninteractions between ML models and DP within an FL structure, ensuring both security and interpretability.\n2. Our framework accurately determines the necessary noise levels to secure both client and central server models,\nwhile preventing the overestimation of DP noise by computing the cumulative noise from previous rounds.\n3. FedHDPrivacy ensures an optimal balance between privacy protection and model accuracy by avoiding\nexcessive noise addition throughout the iterative training process.\nThe remainder of this paper is structured as follows: Section 2 reviews existing FL frameworks, examines DP as a\nmechanism for safeguarding ML models in FL, and explores the integration of HD with FL and DP. In Section 3,\nwe define key concepts related to HD and DP. Section 4 outlines the proposed FedHDPrivacy framework and its\nimplementation. Section 5 details the experimental setup used in a real-world IoT scenario. The results of the\nexperiments are presented and analyzed in Section 6. Finally, Section 7 summarizes the contributions of this work in\ndeveloping privacy-preserving FL models for IoT and offers suggestions for future research directions."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Federated Learning", "content": "FL offers significant advantages for IoT applications by enabling local data processing on devices while only transmitting\nmodel updates to a central server. This approach minimizes the risk of exposing sensitive information to third\nparties, ensuring compliance with strict data privacy regulations, such as the general data protection regulation [28].\nConsequently, FL is well-suited for secure IoT systems. Moreover, by leveraging the computational power and diverse\ndatasets available across various IoT devices, FL can speed up the training process and achieve higher accuracy\ncompared to centralized AI methods, which often face challenges due to limited data and resources [29]. Additionally,\nthe decentralized structure of FL improves the scalability of intelligent networks.\nFL has progressed through the development of various frameworks aimed at addressing the challenges of distributed\noptimization and the diversity of client data and resources. Federated Stochastic Gradient Descent (FedSGD) and\nFederated Averaging (FedAvg), both introduced by McMahan et al. [8], offer foundational approaches for FL. FedSGD\nfocuses on aggregating gradients from multiple clients, while FedAvg aggregates model parameters by averaging them\nacross clients, providing a straightforward yet effective method for distributed learning. Federated Proximal (FedProx),\ndeveloped by Li et al. [30], enhances FedAvg by incorporating a proximal term into the local objective functions, which\nmitigates the effects of client heterogeneity and improves convergence. Federated Normalized Averaging (FedNova),\nintroduced by Wang et al. [31], addresses the issue of objective inconsistency by normalizing client updates, ensuring\na more robust and consistent aggregation process. Additionally, Federated Adam (FedAdam), proposed by Reddi et\nal. [32], extends the Adam optimizer to the federated setting. FedAdam adapts the learning rates for each client by\nleveraging second-order moment estimates, thus offering better convergence properties, especially in non-identically\ndistributed data scenarios. These FL frameworks facilitate the distribution of the learning process among clients without\ntransmitting raw data, thereby safeguarding confidential information. However, they remain vulnerable to model\ninversion and membership inference attacks, underscoring the need for DP to secure these frameworks against such\nthreats."}, {"title": "2.2 Differential Privacy", "content": "Several studies have explored the integration of DP within FL frameworks to enhance system security. Gong et al.\nproposed a framework that combines DP with HE to protect client gradients from the central server, although challenges\nremain in fully preventing information leakage even with the application of DP [33]. Wu et al. applied DP to multi-task\nlearning in FL, perturbing parameters with Gaussian noise to maintain privacy, yet their method remains vulnerable to\ncertain attacks, such as label-flipping and model inversion [34]. Cyffers et al. introduced a decentralized FL approach\nusing a relaxed version of local DP, enabling data analysis across devices while balancing privacy and utility [24].\nHowever, the decentralized nature of this protocol leaves it open to data poisoning and other attacks [35]. Tran et al.\ndeveloped a secure decentralized training framework that eliminates the need for a central server by rotating the master\nnode role among clients [36], but this approach still struggles to fully protect against collusion attacks [35]. Zhao et al.\nproposed the sharing of partial gradient parameters combined with Gaussian noise and introduced an additional proxy\nto enhance client anonymity [37]. While this method strengthens privacy, it requires further evaluation against inference\nattacks. Yin et al. suggested a privacy-preserving strategy using functional encryption and Bayesian DP, which secures\nclient-server communication and monitors privacy leakage [38]. However, this method relies on a trusted third party,\nintroducing the risk of model inversion attacks.\nDue to the dynamic nature of IoT, FL must support continuous training over time, adapting to newly generated data\nin each communication round. While DP secures the training samples by adding noise in each round, the cumulative\neffect of this noise can eventually degrade model accuracy. To address this issue, an XAI model is required to monitor\nand manage the cumulative noise from previous rounds. By accurately tracking this noise, the XAI model can ensure\nthat only the necessary additional noise is introduced, calculated as the difference between the required noise and the\ncumulative noise from earlier rounds. This approach helps maintain a balance between privacy and accuracy over the\nlifetime of the FL process."}, {"title": "2.3 Hyperdimensional Computing", "content": "HD takes significant inspiration from the architecture of the human brain, utilizing high-dimensional vectors to simulate\ncognitive functions [39, 40, 41]. This advanced approach allows HD to mimic brain-like methods for processing,\nanalyzing, and storing information across a variety of cognitive tasks [42, 43, 44]. Several studies have explored the\napplication of HD within FL frameworks. Zhao et al. [45] developed an FL framework using HD, demonstrating\nthe energy and communication efficiency of their model. While they highlighted the robustness of HD against noise,"}, {"title": null, "content": "they did not address the privacy implications. Li et al. [46] introduced an HD-based FL framework aimed at reducing\nthe computational and communication overhead associated with local training and model updates. Hsieh et al. [47]\nbipolarized model parameters to decrease communication costs in an HD-based FL framework, yet they did not provide\nspecific defenses against model inversion and membership inference attacks. As an XAI model, HD is susceptible to\nmodel inversion and inference attacks [48], making it imperative to integrate privacy-preserving mechanisms to protect\nFL systems based on HD from adversarial threats.\nSome studies have explored privacy-preserving techniques for HD models. Hernandez-Cano et al. [49] implemented\nintelligent noise injection that identifies and randomizes insignificant features of the model in the original space.\nHowever, their work does not clearly define how much noise is necessary to achieve a specific level of privacy. Khaleghi\net al. [50] introduced DP to HD models to enhance security, but they did not predict the impact of noise on the model's\naccuracy. Jalil Piran et al. [48] investigated the transparency of combining DP with HD models and predicted the effect\nof noise on HD's performance at various privacy levels. Nonetheless, they did not consider these aspects within a\nFL structure. Therefore, it is essential to explore the transparency and explainability of HD models in an FL context,\nallowing for the precise addition of noise to both client models and the global model. This approach will help secure\ntraining data while minimizing the impact on model accuracy."}, {"title": "3 Preliminary", "content": null}, {"title": "3.1 Hyperdimensional Computing", "content": "HD is a computational paradigm that leverages high-dimensional representations to perform complex information\nprocessing tasks [51, 52]. By employing large-scale vectors, HD captures intricate patterns and relationships in data,\nmaking it particularly well-suited for modeling and analyzing cognitive processes [53, 54]. The HD framework,\nillustrated in Figure 3, is structured into four key phases: encoding, training, inference, and retraining. Each phase\nis meticulously designed to model different aspects of cognitive processing. The process begins with the encoding\nphase, where input data are transformed into high-dimensional hypervectors, forming the foundation for the subsequent\ntraining phase. During training, hypervectors are aggregated to create distinct class representations, crucial for the\nmodel's ability to identify and classify new data during the inference phase. The cycle concludes with the retraining\nphase, which allows for continuous refinement and improvement of the model based on new data and insights. This\ndynamic process mirrors the brain's capacity for learning and adaptation, showcasing HD's ability to handle complex\nlearning tasks with remarkable accuracy and flexibility."}, {"title": "3.1.1 Encoding", "content": "The encoding phase is essential for transforming input data into hypervectors, which form the foundation of the\nhigh-dimensional computational framework. This process ensures that every component of a hypervector plays an equal\nrole in representing the encoded information, thereby maintaining a balanced and comprehensive data representation.\nAn input sample, denoted as x, is mapped into a hypervector H as shown in Equation (1). This mapping follows the\nprinciples of the random vector functional link model, a technique that generates random projections to map input"}, {"title": "3.1.2 Training", "content": "In the training phase, class hypervectors, denoted as $C_s$, are created for each class s, where s ranges from 1 to S\n(the total number of classes). These hypervectors are formed by summing all associated hypervectors for each class,\nas described in Equation (2). Here, $H_s$ represents the hypervectors linked to class s. This summation combines\nthe features of all training samples within a class into a single high-dimensional representation, crucial for accurate\nclassification. The aggregation process captures the shared characteristics of each class, enabling the HD model to\neffectively distinguish between different classes during inference. Creating class hypervectors is a key step in the HD\ntraining process, laying the groundwork for accurate pattern recognition and prediction capabilities."}, {"title": "3.1.3 Inference", "content": "In the inference phase, the class of a query hypervector is identified by comparing it to the class hypervectors generated\nduring training. This comparison relies on cosine similarity, a measure that determines how closely two vectors are\naligned in the high-dimensional space. Cosine similarity is calculated by taking the dot product of the two vectors and\nnormalizing it by the product of their magnitudes. During this phase, the HD model computes the similarity between\nthe query hypervector $\\hat{H}^q$ and each class hypervector $C_s$. The class associated with the hypervector that shows the\nhighest similarity to $\\hat{H}^q$ is selected as the most likely classification for the query. This method allows the HD model to\nclassify new data by applying the patterns learned during training and exploiting the spatial relationships inherent in\nhigh-dimensional vectors."}, {"title": "3.1.4 Retraining", "content": "Retraining plays a crucial role in enhancing the accuracy and adaptability of the HD model, particularly in continuous\nlearning environments like FL. Instead of retraining the model from scratch with all previous data, retraining in HD\nfocuses on correcting misclassifications by fine-tuning the model using new training samples. This process involves\ncomparing the hypervectors of new data points with the existing class hypervectors. If a hypervector, $\\hat{H}^u$, is mistakenly\nclassified into an incorrect class $s'$ rather than its true class s, the model is updated to correct this error. The class\nhypervectors are adjusted by adding the hypervector to its correct class and subtracting it from the incorrect one, as\nexpressed in the following equations:"}, {"title": null, "content": "This adjustment refines the class hypervectors by integrating the correct information into the appropriate class while\nremoving the influence from the incorrect classification. By continuously applying these updates, the HD model\nincrementally improves its classification accuracy, becoming better suited to handle changes in data distribution or the\nintroduction of new patterns. This iterative process of refinement is particularly valuable in FL, where it enables the\nmodel to adapt to new data without the need to start the training process anew, thereby making HD a highly effective\noption for continuous learning."}, {"title": "3.2 Differential Privacy", "content": "In this work, we address the vital challenge of protecting privacy within FL frameworks for IoT systems. A key"}]}