{"title": "Exploring Query Efficient Data Generation towards Data-free Model Stealing in Hard Label Setting", "authors": ["Gaozheng Pei", "Shaojie lyu", "Ke Ma", "Pinci Yang", "Qianqian Xu", "Yingfei Sun"], "abstract": "Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. The adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (QEDG). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real MLaaS scenario and five datasets.", "sections": [{"title": "Introduction", "content": "The widespread adoption of cloud services by Microsoft (Kang et al. 2024), Google (Borra 2024), and Amazon (Wittig and Wittig 2023) suggest that machine learning as a service (MLaaS) (Ribeiro, Grolinger, and Capretz 2015) may become the predominant mode of interaction between human and models in the future. From the user's perspective, these models operate as black boxes, providing feedback based exclusively on user inputs. This interaction method safeguards the privacy of MLaaS training data and ensures the security of the model. This prompts the inquiry: has the security concern regarding MLaaS for classification been effectively mitigated?\nThe answer is negative. One severe security threat is model stealing, also known as model extraction (Liang et al. 2022b; Tram\u00e8r et al. 2016; Truong et al. 2021; Sanyal, Addepalli, and Babu 2022). In this process, an unauthorized party aims to replicate the decision of a target model by creating a substitute model. Attackers generate samples using a generative model, then acquire the target model's predictions for these samples and use them as supervisory information to iteratively refine the substitute model, until either the query budget is exhausted or the behavior of the substitute model closely matches that of the target model. Once the substitute model emulates the behavior of the target model, attackers can utilize it for malicious purposes, including adversarial attacks. The current methods within cooperative game frameworks for stealing models still have a significant issue (Zhang et al. 2022; Zhang, Chen, and Lyu 2022). This can be understood by examining the entire process of model stealing. The attacker synthesizes features using pseudo labels via generator (Goodfellow et al. 2020). As shown in the upper panel of Figure 1, the generator mainly produces synthetic features by minimizing the classification loss function of the substitute model. This strategy confers excessive confidence in the substitute model. The target and substitute models would have the same prediction of these synthetic features. Correspondingly, the synthetic features with target model predictions could not guide the update of the substitute model effectively. As a result, simulating the target model's decision-making requires a large number of synthetic features. Blindly increasing the number of queries to receive more feedback is unrealistic in real confrontation scenarios due to the risk of malicious behavior being detected (Juuti et al. 2019). Therefore, attackers should aim to generate the minimum number of synthetic features necessary, using these limited features to identify discrepancies between the target and substitute models. Ideally, these synthetic features should mainly occupy the disputed area, where the target and substitute models have different predictions of the same synthetic features.\nTo this end, we dissect the generation process of synthetic features and introduce a novel data-free model stealing method named Query Efficient Data Generation (QEDG). Firstly, we implement a harmony loss to ensure the generated samples closely approach the classification boundary of multiple classes. Secondly, we apply a diversity loss by enlarging the gap between samples within the same class, thereby achieving a uniform distribution of generated samples along the decision boundary of the substitute model. Furthermore, we propose a query-free augmentation, which allows for the acquisition of more diverse supervised information without increasing the number of queries to the target model. The main contributions of this paper are summarized as follows:\n\u2022 By dissecting the traditional synthetic process, we reconstruct the principal of the generator to ensure that the generated samples are close to and uniformly distributed along the decision boundary of the substitute model which not only causes some samples to fall into disputed areas, but also ensures that the generated samples cover as large a decision boundary surface as possible.\n\u2022 In order to make full use of the supervisory information obtained by interacting with the target model, a query-free sample augmentation is proposed to obtain multiple pieces of supervisory information through single query. Such a mechanism increases the diversity of samples.\n\u2022 Motivated by theoretical analysis, we introduce the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We perform extensive experiments on four datasets using three metrics, where the results show that our method achieves better performance with a smaller number of queries."}, {"title": "Proposed Framework", "content": "Here we will fully identify the potential threats of the adversary in data-free model stealing.\nGoal. To implement the adversarial attack with partial knowledge of the victim, the adversary aims to acquire a substitute model that could imitate the inference of the target model. Let $\\Theta_T \\in \\mathbb{R}^{P_T}$ and $\\Theta_S \\in \\mathbb{R}^{P_S}$ be the target and substitute model parameters, where $\\mathcal{D}_T \\neq \\mathcal{D}_S$. Given $x \\in \\mathbb{R}^D$ of a test sample $z = \\{x, y^*\\}$, the adversary hopes\n$\\mathcal{T}(x, \\Theta_T) = \\mathcal{S}(x,\\Theta_S),$ (1)\nwhere $\\mathcal{T}: \\mathbb{R}^D \\times \\mathbb{R}^{P_T} \\rightarrow \\mathbb{R}$ is the decision function of the target model, and $\\mathcal{S} : \\mathbb{R}^D \\times \\mathbb{R}^{P_S} \\rightarrow \\mathbb{R}$ corresponds to the substitute model. It is noteworthy that the predictions of the target and substitute model could be inconsistent with the true label as $y^* \\neq \\mathcal{T}(x, \\Theta_T) = \\mathcal{S}(x, \\Theta_S)$.\nKnowledge. Although $\\Theta_T$ is agnostic to the model stealing attackers, he/she can get the feedback of the target model:\n$\\hat{y}_g \\stackrel{\\text{def}}{=} \\mathcal{U}_T(x_g, \\Theta_T)$ (2)\nwith the synthetic sample $x_g$ and construct $z_g = \\{x_g,\\hat{y}_g\\}$ to train the substitute model. Note that here we are considering a stricter scenario where the target model can only return the label, rather than a probability vector. The data-free attacker does not possess more knowledge than normal users in MLaaS scenario.\nAbility. The existing attackers (Zhang et al. 2022; Zhang, Chen, and Lyu 2022) generate $x_g$ by minimizing the classification loss of the substitute model. This strategy confers excessive confidence in the substitute model. Such a behavior turns out to be a mirage. The target and substitute model could have the same prediction of $x_g$\n$\\mathcal{S}(x_g, \\Theta_S) = \\hat{y}_g.$ (3)\nYet the predictions of the substitute model would be still inconsistent with the target model on the true test samples\n$\\mathcal{S}(x,\\Theta_S) \\neq \\mathcal{T}(x, \\Theta_T).$ (4)\nThis issue requires a large number of synthetic samples $\\{z_g\\}$ to intimate the decision boundary of the target model. The \"optimistic\" attackers need to continuously interact with the target model to query $\\{\\hat{y}_g\\}$ and construct $\\{z_g\\}$. However, excessive interactions with the target model can lead to the attacker's samples $x_g$ being identified as illegal inputs. In a real confrontation scenario, the adversary faces a paradox: he/she needs more feedback from the target model but the risk of his/her malicious behavior being detected increases. Different from the existing \"optimistic\" attackers, the proposed \"pessimistic\" adversary makes every effort to minimize the number of queries, i.e. generate as few synthetic samples as possible. Consequently, the adversary pays attention to the synthetic $x_g$ with different predictions from the target and substitute models\n$\\mathcal{S}(x_g, \\Theta_S) \\neq \\hat{y}_g.$\nA Parsimonious Generation Process\nThe above threat modeling shows that minimizing the classification loss function of the substitute model is the main reason for the inefficiency of existing attack methods. Specifically, at the $t$ step, with the given pseudo label $y \\in \\{0,1\\}^K$ where K is the number of classes, the existing methods adopt a classification loss function ($L_{clf}$) like the cross-entropy to generate the synthetic feature\n$x_g^{(t)} \\in \\arg \\min_{x} L_{clf} (\\mathcal{S}(x,\\Theta_S), y).$ (6)\nThen the attacker queries the target model $\\mathcal{T}(\\cdot)$ about $x_g^{(t)}$. The parameter of the substitute model will be updated by $z_g^{(t)} = \\{x_g^{(t)}, \\hat{y}_g^{(t)}\\}$ as\n$\\Theta_S^{(t+1)} \\in \\arg \\min_{\\Theta} L_S (\\mathcal{S}(x_g, \\Theta), \\hat{y}_g^{(t)}),$ (7)\nwhere $L_S$ is the objective function of the substitute model. When (3) holds with some occasional good synthetic samples, (7) would update $\\Theta_S$ very slowly. To reduce the number of interactions, the adversary needs to efficiently discover the differences between the target model and the substitute model. This inspires us to dissect the generation process of the synthetic features. The synthetic features generated by the cross-entropy function are separated by the substitute model. The corresponding predictions of the target and substitute models could be consistent (see the upper panel of Fig. 1). As a consequence, the \"pessimistic\" attacker tries to generate the disputed synthetic features with the given labels y and the current model parameter $\\Theta_S$. The synthetic features $x_g^{(t)}$ should lie as close as possible to the decision boundary of $\\mathcal{S}(\\cdot, \\Theta_S)$. Such synthetic features have a high probability in the disputed area as (5) if the substitute model can not imitate the target model.\nWe analyze the requirements of the desirable synthetic features through Figure 2. Generally speaking, the disputed example $x_g^{(t)}$ for $\\mathcal{S}(\\cdot)$ means that the prediction\n$\\mathcal{S}(x_g^{(t)}) \\stackrel{\\text{def}}{=} \\mathcal{S}(x_g^{(t)}, \\Theta_S) = [S_1(x_g^{(t)}), S_2(x_g^{(t)}),..., S_K(x_g^{(t)})]$ (8)\nlacks the discriminative capability. Treating $\\mathcal{S}(x_g^{(t)})$ as a group of random variables, we hope it holds a small variance. Let $Var(\\mathcal{S}(x_g^{(t)}))$ be the variance of $\\mathcal{S}(x_g^{(t)})$\n$Var (\\mathcal{S}(x_g^{(t)})) = \\frac{1}{K} \\sum_{k=1}^{K} (S_k (x_g^{(t)}) - \\overline{\\mathcal{S}}(x_g^{(t)}))^2,$ (9)\nwhere $\\overline{\\mathcal{S}}(x_g^{(t)})$ be the mean of $\\mathcal{S}(x_g^{(t)})$. Thus, we introduce (9) as the loss function to emphasize the harmony of the synthetic features, as comes below:\n$L_{harm} = \\frac{1}{N} \\sum_{x} Var(\\mathcal{S}(x)).$ (10)\nwhere N is the total number of the synthetic features. However, simply minimizing (10) can only generate the synthetic features that lie in the junction of different classes (the center of Figure 2). Furthermore, we need to generate the synthetic features along the decision boundaries between any two classes. To extend the range of synthetic features, we try to increase the diversity of the synthetic features belonging to the same class. Specifically, the other loss function is designed as follows:\n$L_{div} = \\frac{2}{N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1,i \\neq j}^{N} d(x_i, x_j),$ (11)\nwhere $d : \\mathbb{R}^D \\times \\mathbb{R}^D \\rightarrow \\mathbb{R}^+$ is the distance or dissimilarity function of the synthetic features. Here we choose to use cosine similarity. The final object function of the generator is a combination of (6), (10), and (11) as\n$L_G = L_{clf} + \\alpha \\cdot L_{harm} + \\beta \\cdot L_{div},$ (12)\nwhere $\\alpha$ and $\\beta$ are hyper-parameters. Our strategy for selecting hyper-parameters is to ensure that the magnitudes of the three different loss functions are as close to the same scale as possible.\nQuery-free Augmentation and Substitute Model Training\nOnce we obtain the synthetic data $x_g$ using (12), we expect the outputs of the $\\mathcal{T}(x_g)$ and $\\mathcal{S}(x_g)$ to be as consistent as possible. However, using only one batch of features synthesized in the first stage of the current epoch, it will suffer from catastrophic forgetting (Binici et al. 2022; Do et al. 2022). The memory bank (MB) can effectively address this issue. Different from (Yuan et al. 2024) which only stores all previously synthesized features in the first stage, we also store the supervisory information from the target model. In the hard-label setting, the supervisory information refers to the label predicted by the target model for the features $x_g$:\n$MB = MB \\cup \\{(x_g,\\hat{y}_g)\\}, where \\hat{y}_g = \\mathcal{T}(x_g).$ (13)\nWe observe that HEE (Yuan et al. 2024) applies strong data augmentation to the features $x_g$ stored in the memory bank. However, strong data augmentation can change the original labels. This raises the question: is it possible to achieve feature diversity without altering the original labels? We conducted experiments on both grayscale datasets like MNIST and colored images like CIFAR-10. For non-disputed samples, we applied three simple data augmentations, including horizontal flipping, vertical flipping, and rotation by a certain angle. In Figure 3, we found that the labels of non-disputed samples remained high consistent rate before and after these data augmentations. The intuition behind this experimental phenomenon is that since non-disputed samples are correctly classified, they are further away from the decision boundary of the target model, making them more robust to simple data augmentations. Therefore, we retain the original labels of the non-disputed samples after data augmentation, thereby introducing more supervised information without additional query costs. It is worth noting that these augmented samples are not stored in the memory bank.\nWith the samples stored in the memory bank and the samples after random augmentation, we can update the parameters $\\Theta_S$ of the substitute model. We assign different weights to the loss of disputed samples and non-disputed samples, respectively. The loss function of the substitute model $\\mathcal{S}(\\cdot)$ at t step is as follows:\n$L_{S}^{(t)} (x_g, y_g; \\Theta) = \\begin{cases} \\gamma \\cdot L(x_g, y_g; \\Theta), & \\text{if } \\mathcal{S} (x_g, \\Theta^{t-1})) \\neq y_g, \\\\ L(x_g, y_g; \\Theta), & \\text{otherwise,} \\end{cases}$ (14)\nwhere\n$L(x_g, y_g, \\Theta) = L_{clf}(\\mathcal{S}(x_g, \\Theta), \\hat{y}_g),$ (15)\nand $\\gamma$ is the hyper-parameter to control the importance of the disputed samples. The whole process of the proposed method is summarized as Algorithm 1."}, {"title": "Theoretical Analysis", "content": "This part establishes the query complexity of the proposed active model stealing attack. Compared to the existing passive attackers, the following theorem states that the active attacker can easily steal any target model whose correct probability is greater than or equal to 1/2 + c with some constant $c > 0$ for all synthetic features. We show that the consistency rate between the target and substitute models increases by only a logarithmic multiplicative factor of the normal active learning query complexity.\nAn active learning algorithm (A,T) refers to an interaction between two players - the oracle T and the learner A. A chooses $x \\in X$ and sends it to T, who responds with $y \\in Y$. (x, y) is then used by A to obtain f from a hypothesis class F, which satisfies $f(x) = y$. We assume that the oracle T adopts $f^* \\in F$ to response x as $y:= f^*(x)$. In this setting (Alabdulmohsin, Gao, and Zhang 2015; Chen, Hassani, and Karbasi 2017; Zhang, Shen, and Awasthi 2020), the different between f and $f^*$ is measured by\n$error(f) = Pr_{x \\sim X} \\{f(x) \\neq f^*(x)\\}.$ (16)\nThen q($\\epsilon$, $\\delta$) denotes the query complexity of A as A chooses $f \\in F$ such that error(f) < $\\epsilon$ with probability at least 1 \u2212 $\\delta$.\nTheorem. Let F be a hypothesis class and (A,T) refer to an active learning algorithm as described above with the query complexity of q($\\epsilon$,$\\delta$). Suppose that an adversary S disguises as A but he/she can only receive imperfect feedback as\n$\\phi(f^*,x) \\stackrel{\\text{def}}{=} Pr[y_g \\neq f^*(x)] > 0, \\forall x \\in X,$ (17)\nwhere $y_g$ is the random variable that represents the feedback of T to the S's query x. If $max_x \\phi(f^*,x) < 1/2$, A could obtain $f \\in F$ such that\n$Pr\\{error(f) < \\epsilon\\} > 1 - 2\\delta,$ (18)\nwith query complexity\n$Q = \\frac{8}{(1 - 2 \\cdot max_x \\phi(f^*,x))^2} \\cdot q(\\epsilon,\\delta) \\cdot ln \\frac{q(\\epsilon,\\delta)}{\\delta}.$ (19)\nIt is noteworthy that (21) is a common situation for model stealing adversary. On the one hand, the oracle only exists is an ideal setting and the MLaaS API could provide some wrong predictions with the given queries. On the other hand, the potential defense mechanism would perturb the predictions of the target model to prevent model stealing. We provide the proof details in the supplementary materials."}, {"title": "Experiment Setup", "content": "We conducted experiments on five well-known datasets: MNIST, FMNIST, SVHN, CIFAR10, CIFAR100. We unified the target models of different methods, as well as the substitute models of different methods, noting that the target models and substitute models are different. We separately tested different model structures, including ResNet (He et al. 2016), VGG (Simonyan and Zisserman 2015), and our own designed CNN model. Furthermore, we adopted the architecture described in (Zhang et al. 2022) and employed the same generator as used in StyleGAN (Karras, Laine, and Aila 2019).\nWe compare our approach with the following state-of-the-art (SOTA) methods. DaST (Zhou et al. 2020) and DFME (Truong et al. 2021) and HEE (Yuan et al. 2024) are three data-free model stealing methods that do not take into account the number of queries. DFTA (Zhang et al. 2022) and IDEAL (Zhang, Chen, and Lyu 2022) are query efficient and achieve improvements over the above methods.\nWe employ three metrics for evaluation. Accuracy measures the precision with which the substitute model performs predictions on the actual test set. Motivated by theoretical analysis and (16), we also consider consistency metric. To further exclude the possibility that agreement may occur by chance between target model and substitute model. we adopt Cohen's kappa (McHugh 2012). It measures the agreement between target model and substitute model. This metric is not affected by the accuracy of the target model. The calculation formula for Cohen's kappa is as follows:\n$\\kappa = \\frac{p_o - p_e}{1 - p_e}$ (20)\nwhere $p_e$ represents the relative consistency between the two models and is calculated by summing of the products of the marginal probabilities for each class. $p_o$ represents the accidental consistency between the two models and is calculated by dividing the sum of the diagonal elements of the confusion matrix by the total number of samples. To further test the consistency between the substitute model and the target model, we also test the attack success rate. We employ three common attack methods to generate adversarial examples: FGSM (Goodfellow, Shlens, and Szegedy 2014), BIM (Kurakin, Goodfellow, and Bengio 2017), and PGD (Madry et al. 2018).\nWe train the substitute model using an SGD optimizer with a batch size of 256, an initial learning rate of 0.01, a momentum of 0.9, and no weight decay. For the generator, we adopt a batch size of 256, utilizing an adam optimizer (Kingma and Ba 2015) with a fixed learning rate of 0.001. For both MNIST and FMNIST, the perturbation bound is respectively set to 0.3, 0.2, and 0.1 with a step size a of 0.01. For SVHN, CIFAR10 and CIFAR100, the perturbation bound is set to 32/255, 20/255, 8/255 with a step size a of 0.01. In untargeted attacks, we only attack images correctly classified by the target model. In targeted attacks, we only attack images correctly classified by the target model with labels different from their original labels. This is as the same as DFTA (Zhang et al. 2022)."}, {"title": "Experiment Results", "content": "We conducted experiments in real-world scenarios using Microsoft's Azure API service. This service restricts users to uploading their own datasets, and we utilized the MNIST dataset for tests, consistent with (Zhang, Chen, and Lyu 2022). The architecture and parameters of the model provided by the service are not disclosed. For the substitute model, we employed ResNet18. From the Figure 4, we can observe that our method achieves an accuracy comparable to DFTA's accuracy at 100k queries when using only 20k queries. Moreover, with a sufficient number of queries, our method is capable of achieving even higher accuracy.\nWe used ResNet18 as the substitute model and ResNet34 as the target model. We tested three evaluation metrics: consistency rate, accuracy, and attack success rate. The perturbation bound was set to 0.3. The attack success rates for other perturbation bounds can be found in the appendix. We also tested results when both the target and substitute models were CNN structures. These results are also in the appendix. Table 2 shows the accuracy and consistency rate. Our method outperforms others by a significant margin in both accuracy and consistency rate, even with fewer queries. This is because our method generates more samples in the disputed region, enabling each query to effectively guide the substitute model's update. In contrast, other methods generate overly confident samples for the substitute model. Few samples fall into the disputed area, resulting in significant query waste. We test the success rates of targeted and untargeted attacks using three attack methods. Table 1 shows that regardless of the attack method, our method achieves the highest attack success rates with the fewest queries. This indicates that the decision boundary learned by our substitute model is most similar to the target model's.\nDue to the simplicity of grayscale images, we extended our experiments to more complex datasets like SVHN and CIFAR10, CIFAR100 which consist of color images. We used the same model structure and evaluation metric. The perturbation bound was set to 32/255. The attack success rates for other perturbation bounds can be found in the appendix. We also tested results with the target model set to VGG19. These results are also in the appendix. We used more queries on both datasets because the decision boundaries of the target models are more complex. Table 2 shows that our method surpasses others in consistency rate and accuracy, requiring only 30k queries compared to 50k or even 250k needed by other methods. In addition, we also test the transfer attack success rate under different attack methods. From the table 3, it can be seen that our method achieves a higher transfer attack success rate in most cases with fewer query number. We also plotted line charts showing the relationship between accuracy, consistency, and the number of queries in the appendix. Figure shows that the accuracy and consistency of other methods fluctuate, while ours steadily increase.\nWe conducted a hyperparameter analysis on the MNIST dataset and found that optimal performance can only be achieved when the two loss functions are on the same order of magnitude. We set $\\alpha = 5$ and $\\beta = 0.7$. Regarding the selection of $\\gamma$, the experiments revealed that assigning a higher weight to contentious samples can improve the performance of our method. However, if the weight is too high, the performance will quickly decline. Here we set $\\gamma = 5$.\nIn this section, we conducted comprehensive ablation experiments on CIFAR10 to assess the significance of each component of our method, with the results presented in Table 4. Our approach primarily consists of three improvement components: query-free augmentation (QA), $\\mathcal{L}_{harm}$, and $\\mathcal{L}_{div}$. We tested the experimental effects of adding or removing each module separately, resulting in a total of six sets of experiments. Under otherwise identical conditions, using $\\mathcal{L}_{div}$ yielded higher performance compared to using QA and $\\mathcal{L}_{harm}$, and we achieved the best performance when all three components are used simultaneously. Because $\\mathcal{L}_{harm}$ prefers the centers of decision surfaces with multiple classes, if only $\\mathcal{L}_{harm}$ appears without $\\mathcal{L}_{div}$, it leads to negative optimization of the loss function. This is evidenced by the experiments, where the results with only $\\mathcal{L}_{harm}$ and without $\\mathcal{L}_{div}$ are the worst. The role of QA is to increase the diversity of samples without increasing the number of queries. Experimental results also indicate that having QA can significantly improve the model's performance."}, {"title": "Conclusion and Limitations", "content": "To reduce the number of queries required in model stealing, we dissect the existing synthetic process and reconstruct the principal of the generator to ensure that the synthetic samples are close to and uniformly distributed along the decision boundary of the substitute model. The generated samples should obtain different predictions of the target and substitute models and cover as large a decision boundary surface as possible. Furthermore, a query-free sample augmentation is proposed to make full use of the supervisory information obtained by interacting with the target model. Then we establish the query complexity of the proposed active adversary with imperfect feedback of the target model. We further introduce the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. Our method outperformed the current state-of-the-art competitors on the real MLaaS scenario and five datasets. However, both our method and existing approaches generate samples that lack any semantic information."}, {"title": "Related Work", "content": "Deep learning has been applied to various domains, but it faces a wide range of security challenges including poisoning attack (Ma et al. 2021), sequential manipulation(Ma et al. 2024, 2022), backdoor attack(Liu et al. 2024; Liang et al. 2024), adversarial attack and model stealing, etc.\nBlack-box attacks refer to situations where attackers construct adversarial samples without accessing the model's internal structure and parameters. Black-box attacks are mainly of two types: query-based and transfer-based. Query-based attacks can be divided into score-based attacks and decision-based attacks. Score-based attacks (Liang et al. 2022a; Chen et al. 2017; Guo et al. 2019; Andriushchenko et al. 2020; Liang et al. 2021) update adversarial samples by observing changes in loss indicated by DNN's output scores, such as logits or probabilities. However, score-based attacks can be defended by injecting parametric noise(He, Rakin, and Fan 2019) or slightly modifying the output logits to mislead the attack(Chen et al. 2022). In real-life scenarios, MLaaS usually returns only the top-1 labels instead of logits or probabilities. Decision-based attacks rely only on DNN's decisions, such as top-1 labels, to generate adversarial examples(Brendel, Rauber, and Bethge 2018). Since decision-based attacks cannot perform greedy updates, they start with a different sample and aim to keep the DNN's prediction wrong, requiring thousands of queries to achieve a non-trival success rate. The other type is transfer-based attacks(Jia et al. 2022; Wang et al. 2021b; Wang and He 2021; Xie et al. 2019; Gu et al. 2023; Gao et al. 2025; Jia et al. 2023), which rely on the transferability of adversarial samples. They involve training a substitute model on a training dataset and using its white-box characteristics to construct adversarial samples to attack the target model. However, due to data privacy and commercial value concerns, MLaaS providers may not disclose their training datasets.\nIn real-world scenarios, due to data privacy and commercial value, MLaaS providers do not disclose their training datasets, and the models' structures and parameters are opaque. MLaaS providers also only return the top-1 labels for user inputs. This scenario is both more challenging and more practical. Therefore, many methods now aim to steal target models without any prior knowledge about the target model. These methods can be broadly categorized into two types. The first type (Sanyal, Addepalli, and Babu 2022; Pal et al. 2020; Sun et al. 2022) tries to steal the target model's functionality using proxy data. However, obtaining proxy data isn't always easy. If there are significant differences between the proxy data and the target model's training data, it can affect the substitute model's training. The second type does not use any real data (Chandrasekaran et al. 2020). It generates a synthetic dataset using generators to create fake data from noise. DaST (Zhou et al. 2020) was the first to steal the functionality of a black-box model without real data. However, the generator size increases significantly as the class number of the victim dataset grows. DDG (Wang et al. 2021a) then modified the generator architecture to compress its size and explore more effective data. These methods model the generator and substitute model as a zero-sum game, requiring many queries. DFME (Truong et al. 2021) and MAZE(Kariyappa, Prakash, and Qureshi 2021) estimate the black-box using zero-order gradient estimation (Chen et al. 2017) to update the generator. Gradient estimation also requires many queries. More queries make an attacker easier to detect. DFTA(Zhang et al. 2022) and IDEAL(Zhang, Chen, and Lyu 2022) transforms the zero-sum game into a cooperative game, reducing the number of queries. In this paper, we explore guiding the generator to produce samples inconsistent with both the substitute and target models' predictions within a cooperative game framework, using fewer queries. This allows for more efficient updates of the substitute model."}, {"title": "Theoretical Analysis", "content": "Theorem. Let F be a hypothesis class and (A,T) refer to an active learning algorithm as described above with the query complexity of q($\\epsilon$, $\\delta$). Suppose that an adversary S disguises as A but he/she can only receive imperfect feedback as\n$\\phi(f^*,x) \\stackrel{\\text{def}}{=} Pr[y_g \\neq f^*(x)"}]}