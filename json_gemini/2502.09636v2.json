{"title": "Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?", "authors": ["Sougata Saha", "Saurabh Kumar Pandey", "Harshit Gupta", "Monojit Choudhury"], "abstract": "In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-40 in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines.", "sections": [{"title": "Introduction", "content": "Whether performing outdoor activities, household chores, or surfing the internet, our perception and understanding of the world are varied. Each of us has a distinct worldview, which causes us to understand and internalize information distinctly. Although our knowledge, and hence our worldview, are individual-specific (Collins and Gentner, 1987; Jonassen and Henning, 1999; Denzau et al., 1994), our culture does contribute to the variation by shaping our worldview and understandability (Bender and Beller, 2013; Cole and Packer, 2019). This effect of culture is also evident in the online world, where we might find text from distinct online sources hard to understand due to a lack of common ground between us and the writer of the text (Meyer, 2014; Korkut et al., 2018). For example, people unfamiliar with the Arabic culture might not understand the meaning of the dishes \"Machboos\" and \"Luqaimat\" from the review text \"The Machboos was perfectly spiced, and the Luqaimat was a real treat.\" Or, someone unfamiliar with the US culture and not exposed to fast food might not understand that \"golden arches\" refers to \"McDonald's\" in the text \"Let's go to the golden arches for a quick bite.\".\nThe recent advancements in AI and NLP make us question whether and how Large Language Models (LLMs) can help overcome such barriers in online cross-cultural communication. Here, we consider a use case where LLMs act as cultural mediators by identifying, categorizing, and explaining text spans (Culture-Specific Items) from online reviews that users from distinct cultures might find hard to understand. Although numerous use cases of LLMs in cross-cultural communication are conceivable (Singh et al., 2024; Liu et al., 2024), the following fundamental and overarching questions need answering before implementing them across any domain. Firstly, although studies discuss the importance of cross-cultural communication (Bourges-Waldegg and Scrivener, 1998), the need for LLMs and AI assistants in general for cross-cultural communication has not been studied. Secondly, the cross-cultural performance parity of LLMs in such settings is unclear and unmeasured. Although studies have shown LLMs to exhibit Anglo-centric biases (Johnson et al., 2022; Dwivedi et al., 2023), this does not mean they perform well in those cultures and poorly in others. We refer to this as the equitability, defined as a model's capability to maintain the parity between its performance and the user's need across cultures. Through our specific use-case of LLMs as a cultural mediator, we answer these fundamental questions, a prerequisite of using them to overcome cross-cultural communication barriers.\nThe contributions of this paper are as follows:\n\u2022 We perform a user-study spanning participants from the USA, Mexico, and India to measure their difficulty in understanding Goodreads English reviews of books from the USA, India,"}, {"title": "Culture: A Primer", "content": "Culture is a complex concept, and is defined as the \"Way of life of a collective group of people distinguishing them from other groups\" (Blake, 2000; Monaghan et al., 2012; Parsons, 1972; M\u00fcnch and Smelser, 1992), making it experiential (Geertz, 1973; Bourdieu, 1977). It provides a common ground to groups through shared experiences and creates distinct \"worldviews\" between people from different cultures (Collins and Gentner, 1987; Jonassen and Henning, 1999; Denzau et al., 1994; Bang et al., 2007; McHugh et al., 2008) by shaping their cognition (Mishra, 2001; Nisbett and Norenzayan, 2002; Bender and Beller, 2013; Cole and Packer, 2019). This difference in perception due to culture affects all aspects of life, where literary works composed by individuals from one culture might not be perceived similarly by individuals from a different culture and hamper cross-cultural communication (Meyer, 2014; Korkut et al., 2018).\nIn NLP, these variations can broadly be due to differences in linguistic form and style, common ground, aboutness, and values (Hershcovich et al., 2022). Among demographies, these differences vary across dimensions or semantic proxies, ranging from physical items such as food and materials to abstract constructs like emotions and values (Thompson et al., 2020; Adilazuarda et al., 2024). Newmark (2003) terms such concepts as Culture-Specific Items (CSIs), identifying and addressing which is crucial for cross-cultural communication. Thus, to technologically facilitate cross-cultural communication, it is essential to identify and gauge the extent of the variation of such CSIs, which can inform appropriate technological advancements for mitigation."}, {"title": "User Study", "content": "To measure the extent of the difficulty in comprehension due to culture and establish a need for cross-cultural tools, we conducted an online user study where we showed participants book reviews written in English and asked them to highlight text spans they did not understand. The length of the spans was unrestricted and could be individual words or even multiple sentences. Our goal was to quantify how much of things people do not understand and, out of that, how much is cultural (CSIs). Measuring this cultural knowledge gap can serve as an evaluation benchmark for tools that facilitate cross-cultural communication."}, {"title": "Dataset", "content": "The study was performed over 57 English review texts of books originating from Ethiopia, India, and the USA, which represent distinct cultures per Inglhart-Welzel's world cultural map (Inglehart and Welzel, 2010). The reviews were sampled from the Goodreads dataset collected by Wan and McAuley (2018); Wan et al. (2019) in 2017, a sizeable publicly available book-review dataset spanning 2.3 million books from multiple countries and languages. We sampled the 57 reviews in two steps. First, we prompted GPT-40 (Achiam et al., 2023) to list the top 20 famous authors of fictional and non-fictional books from each country and restricted to reviews of books from these authors with at least 50 reviews and review lengths between 50 and 200 words. We randomly sampled 3,100 reviews while ensuring at least 250 reviews from each country. Next, to ensure enough cultural diversity in our study, for each review text, we prompted GPT 3.5 to identify CSIs for people from our country of study, India, Mexico, and the USA, with varied literary preferences: fiction or non-fiction. Section 4.1 discusses the prompt in detail. The model identified CSIs in almost all review texts, with varied levels of familiarity. We randomly sampled 57 reviews containing at least one unfamiliar CSI as the final dataset for our study.\nIt is worth mentioning that from the perspective of generalizability, the current sample size suffices because we did not seek to establish concrete quan-"}, {"title": "Method", "content": "We conducted two small-scale internal and one external pilot study to determine the questionnaire2. Pilot 1: Similar to the task formulation for GPT 3.5 in the Step 2 filtering, in the first pilot, we showed a review text to six participants and asked them to identify spans mentioning CSIs that they did not understand or were unfamiliar with, given their cultural background. The participants were researchers from India, Ethiopia, and the USA. The key takeaways from the study were: (i) Although the participants cumulatively identified 13 CSIs (detailed in Table 2 Appendix A), the question forced ethnocentrism, where one had to internally assume a source culture of the review text before determining CSIs that might be common in the source culture but exotic to their culture. (ii) Such a formulation forced participants to generalize their culture, which they found difficult. (iii) It is difficult to distinguish the unknowability of CSIs borne out of cultural difficulty and otherwise.\nPilot 2: Incorporating their feedback, we reformulated the task in Pilot 2. Instead of asking what one does not understand due to their culture, we asked participants to identify spans they generally found hard to understand or were unfamiliar with. Additionally, we asked several other questions (detailed in Appendix A) with multiple choices to capture their understanding of the review from different aspects and provoke their thoughts around the task."}, {"title": "LLM-assisted post processing", "content": "Since the study only asked participants to identify spans they found hard to understand, we processed the results using GPT-40 to categorize them as CSIs (cultural spans) or non-CSIs. We first created a taxonomy by interactively prompting ChatGPT using the template outlined in Appendix B.1. We first provided a few review texts and their highlighted spans and instructed ChatGPT to generate categories and subcategories as the initial taxonomy. Then, we iteratively fed the model its proposed taxonomy along with marked spans from an additional 20 annotators, enabling it to continuously update and refine the taxonomy based on this new input. This iterative process allowed the model to enhance the taxonomy effectively.\nThe final taxonomy classified each span into \"Cultural and Linguistic,\" \"Cultural and Non-Linguistic,\" \"Non-Cultural and Linguistic,\" \"Non-Cultural and Non-Linguistic,\" and \"Poor Annotation\", listed in Appendix B.2. Adhering to the human-in-the-loop validation process by Shah et al. (2023), we consulted two linguistic experts to review and validate the generated taxonomy. This step ensured clarity and robustness, preventing over-generalization and ambiguity in the definitions and confirming that our taxonomy effectively captures the diversity and complexity of the spans while maintaining consistency across annotations. We used GPT-40 to annotate each identified span using our taxonomy. The model inputs each review text and a list of all spans highlighted as non-understandable by participants from any country and associates each span with one of the five classes. Also, since the pilot studies indicated that human annotations tend to be noisy, with some marked spans being superfluous while some reflecting low-quality annotations, we instructed the model to cluster the spans semantically based on the context. This clustering process helped filter out low-quality and irrelevant annotations while resolving ambiguities related to span lengths, allowing us to focus on meaningful data. Appendix B.2 depicts the prompt used to post-process using"}, {"title": "Evaluation", "content": "The GPT-4o annotations were evaluated by two human experts comprising graduate students from Linguistics and Computer Science. Each expert evaluated 120 random annotations and had 54 overlapping samples. They assessed the span type and the cluster assigned by GPT-40 by marking the correctness with binary flags. The evaluators agreed on 48 out of 54 (89%) cases for span type and 52 out of 54 (96%) for the cluster assignment. For the 54 overlapping annotations, at least one evaluator found the span type labels correct in 94% of cases and the cluster labels correct in all cases. Although there is an approximate 10% margin of error for incorrect span type classification and subsequently 15% error margin for cluster assignment, the high agreement scores indicate that GPT-40's annotations are reliable. Hence, we cluster all spans identified by the participants from the study as per the clusters assigned by the model. Henceforth, we perform all span-level analysis on its cluster representative."}, {"title": "Analysis", "content": "1. What is the overall and cultural difficulty of understanding concepts from reviews? Out of 611 total evaluations encompassing all participants from all countries, 312 (51%) evaluations had no difficult-to-understand spans highlighted. The remaining ones had at least one difficult-to-understand span, and 200 (33%) had some culturally difficult spans. At a review level, all 60 reviews had at least one difficult-to-understand element, where 50 reviews (83%) had culturally difficult-to-understand elements, while others were non-cultural. On average, a participant evaluated 12 reviews, of which six were hard to understand. Four of the six hard-to-understand reviews had cultural elements.\n2. What is the overall difficulty by each culture and how much of it is cultural? Figure 1 plots the proportions of hard-to-understand evaluations by a participant's location and the book's country of origin. We observe the following: (i) Participants from all locations found the reviews from Ethiopia easier to understand and did not contain difficult spans, and evaluators from Mexico did not find such spans in 85% of cases. Also, in each country, the proportion of the evaluations highlighting culturally difficult spans was lower than the reviews of books from other countries. (ii) Considering participants and books from India and the USA, culturally difficult spans are lowest for the participants from their own country. For evaluators from India, 34% of unfamiliar spans from the reviews of Indian books were cultural, compared to 48% for USA book reviews. For the USA, 41% of unfamiliar spans from the reviews of Indian books were cultural, compared to 33% for USA book reviews, (iii) For evaluators from Mexico, the proportions of evaluations comprising non-difficult spans and culturally and non-culturally difficult spans were similar for books originating from India and the USA. (iv) Overall, the highest proportion of evaluations highlighting culturally difficult spans were for US-origin books by evaluators from India (48%), which was followed by India-origin book reviews by evaluators from the USA (41%).\nThis cross-cultural gap in understanding indicates that people from all locations might benefit from tools such as cultural reading assistants"}, {"title": "How much of the reviews and spans are cultural?", "content": "We measure the review and span-level agreements between participants from the same country and compare them against the agreements when the participants are from different countries. We create all combinations of inter and intra-country annotators and compute Krippendorf's alpha to measure the inter-annotator agreement within a country and between each pair of countries. Figure 2 captures the agreement between annotators within a country and across countries. We observe the following at a review level: (i) Annotators from the same country agree more amongst themselves on which reviews are generally hard to understand than annotators across countries except for the USA. We observe a similar trend for reviews containing CSIs, where within a country, annotators agree more on which reviews contain difficult CSIs than inter-country. (ii) For generally difficult and CSI-containing reviews, participants from Mexico agree more among themselves (0.27/0.34) than with reviewers from India and the USA. (iii) Similarly, Indian participants agree more amongst themselves (0.2/0.26) than from other countries. However, they align more with the USA (0.14/0.2) reviewers than Mexicans (0.10/0.12). (iv) Interestingly, reviewers from the USA agree more with Indian and Mexcian participants than among themselves. (v) At a span level, there is a lack of consensus across all countries, which testifies that understandability is individual-specific, which was already indicated by our pilot studies. Also, the confounding factors for the low value are likely due to errors from the GPT-40 post-processing, as mentioned in Section 3.3.2. However, annotators from the same country seem to agree more on culturally difficult spans than other kinds of spans. The intra-country agreements at the \"overall\" and \"cultural\" levels significantly increased from 0.02 to 0.14 for India, 0.06 to 0.14 for Mexico, and 0 to 0.11 for the USA, indicating that although different annotators might find different things hard to understand, there is a level of consensus on CSIs that they find hard to understand. This also implies that CSIs are a set of harder-to-understand constructs in text and, therefore, are good targets for circumventing the cold-start problem (Hu et al., 2008) of personalization for user-facing systems. Personalization systems can use such culture-specific information when it does not know anything about a user. Such systems can use culture-specific information when user-specific details are unavailable."}, {"title": "Benchmarking GPT-4o as a Cultural Assistant", "content": "We designed a prompt to evaluate GPT-40 (Achiam et al., 2023) as a cross-cultural reading assistant by assigning it the role of a cultural mediator which"}, {"title": "Analysis", "content": "We cluster the GPT-40-identified spans by matching their embedding-based cosine similarity scores (Reimers and Gurevych, 2019) with the span clusters from the user study (similarity threshold=0.5 and measured their overlap. As depicted in Figure 1, combining all the CSIs identified by GPT-40 across all countries and genre preferences, 96 (83%) overlap with the 245 unique spans marked as difficult to understand by humans, and 70 (60%) overlap with the 116 user-identified culturally difficult spans. Interestingly, 26 (22%) of the GPT-40-identified CSIs, although considered difficult, are not deemed cultural by humans. We hypothesize this to be confounded by two things. First, as discussed in Section 4, the GPT-40-based cultural classification of the user-identified spans has an error margin of approximately 15%. Second, since the user study encompassed 50 participants, their identified spans most likely do not capture all the difficult spans for someone from the same culture, contrary to the CSIs identified by GPT-40, which are more encompassing. This generalization of the model is further evident from Figure 5, which measures the overlap of spans between fiction and non-fiction readers from each country. For all countries, the overlap between GPT-40-identified spans for both groups is consistently higher than those identified by the users."}, {"title": "Discussion", "content": "Although our study encompasses Goodreads book reviews, we believe the findings apply to other domains. Since the books and their reviews were random, they do not pertain to any single domain and often comment on different aspects of life. Our studies suggest that although understandability varies by person, CSIs are an agreed set of items that people do not understand within a culture. Furthermore, the CSIs are distinct across cultures. This finding can benefit any domain and application that spans cultures, such as translation, education, intercultural training, content personalization, etc. In a rapidly globalizing world, where the definition of culture has shifted from a traditional nationality or ethnicity-centric definition to include digital communities (Birukou et al., 2013), our study establishes the need for tools that enable cross-cultural communication and demonstrates where current state-of-the-art LLMs, such as GPT-40, are lacking. Recent tools such as Culturally Yours (Pandey et al., 2025), an AI-based reading assistant that enables cross-cultural communication and personalizes to each user, further attest to the practical implications of our study."}, {"title": "Related Work", "content": "Communicating across cultures is crucial in today's global world. Although several theories and frameworks exist that define the nature of cross-cultural communication (Gudykunst, 2003; Tannen, 1983; Hurn et al., 2013; Gardner, 1962), practical applications implementing such concepts are still nascent. Computationally, a considerable amount of work has been done in HCI in defining the considerations of cross-cultural tools and systems (Bourges-Waldegg and Scrivener, 1998; Kyriakoullis and Zaphiris, 2016; Heimg\u00e4rtner, 2018). Cultural adaptation is also extensively studied in machine translation (Bassnett, 2007; Trivedi, 2007; Sperber et al., 1994), Aixel\u00e1 (1996) introduced the term \"Culture-Specific Items\" (CSIs), and Newmark (2003) further presented a taxonomy of CSIs and proposed methods to tackle each type of CSI. Recently, Zhang et al. (2024) introduces the ChineseMenuCSI dataset where they integrate the translation theories to create an annotated dataset with CSI vs Non-CSI labels. Singh et al. (2024) leverages open-sourced LLMS to adapt CSIs from the USA TV series Friends, for an Indian audience. Advances in LLMs have garnered studies that detect biases in LLMs (Tao et al., 2023; Kharchenko et al., 2024; Duan et al., 2023; Lin et al., 2025). Using curated cultural datasets, most methods probe LLMs and test their knowledge and reasoning capabilities in culture-specific settings (Nadeem et al., 2021; Nangia et al., 2020; Wan et al., 2023; Jha et al., 2023; Li et al., 2024b; Cao et al., 2023; Tan-"}, {"title": "Limitations", "content": "In this work, we perform a user study on 57 review texts from Goodreads. We limit our study to demographics of India, USA, and Mexico, in the English language. We consider genre preference as the only semantic proxy under study. A large-scale multilingual study across more diverse cultures worldwide and at a larger intersection of demographic and semantic proxies would help strengthen the findings of our work. We limit our evaluation to using GPT-40 since it is known to be better than other models, a full-fledged assessment of other closed-source and open-source models needs to be performed."}, {"title": "Ethical Considerations", "content": "We do not capture any personally identifiable information regarding the users involved in user study. We follow standard ethical guidelines (Rivers and Lewis, 2014) and do not attempt to track users across sites or deanonymize them. We also adhere to the minimum wage policies of all demographics involved in the user study. We filter out all the offensive/adult content from the review texts presented to the users. We plan to release our data and the annotations to foster further research on developing AI assistants for efficient cross-cultural communication."}, {"title": "Appendix", "content": ""}]}