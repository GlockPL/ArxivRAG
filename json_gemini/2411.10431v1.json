{"title": "MITIGATING PARAMETER DEGENERACY USING JOINT CONDITIONAL DIFFUSION MODEL FOR WECC COMPOSITE LOAD MODEL IN POWER SYSTEMS", "authors": ["Feiqin Zhu", "Zhongjing Jiang", "Dmitrii Torbunov", "Yihui Ren", "Tianqiao Zhao", "Meng Yue", "Amirthagunaraj Yogarathnam"], "abstract": "Data-driven modeling for dynamic systems has gained widespread attention in recent years. Its inverse formulation, parameter estimation, aims to infer the inherent model parameters from observations. However, parameter degeneracy, where different combinations of parameters yield the same observable output, poses a critical barrier to accurately and uniquely identifying model parameters. In the context of WECC composite load model (CLM) in power systems, utility practitioners have observed that CLM parameters carefully selected for one fault event may not perform satisfactorily in another fault. Here, we innovate a joint conditional diffusion model-based inverse problem solver (JCDI), that incorporates a joint conditioning architecture with simultaneous inputs of multi-event observations to improve parameter generalizability. Simulation studies on the WECC CLM show that the proposed JCDI effectively reduces uncertainties of degenerate parameters, thus the parameter estimation error is decreased by 42.1% compared to a single-event learning scheme. This enables the model to achieve high accuracy in predicting power trajectories under different fault events, including electronic load tripping and motor stalling, outperforming standard deep reinforcement learning and supervised learning approaches. We anticipate this work will contribute to mitigating parameter degeneracy in system dynamics, providing a general parameter estimation framework across various scientific domains.", "sections": [{"title": "1 Introduction", "content": "Dynamic system modeling is a fundamental study across different scientific fields. Data-driven machine learning provides a new paradigm to model the system dynamics due to its potential of implementing more accurate and efficient simulations [Wang et al., 2023]. This encompasses two fundamental forms: forward surrogation and inverse modeling. Forward surrogation predicts the system's evolution from initial states, while inverse modeling deduces the model's inherent properties from observation data [Kadeethum et al., 2021]. Inverse modeling, which includes techniques such as parameter identification, plays a crucial role in understanding and emulating system dynamics. However, the inverse problem of system dynamics is complex and challenging due to parameter degeneracy and unidentifiability, where non-unique solutions exist that produce identical observation outputs [Lederman et al., 2021].\nIn power systems, load modeling uses several types of models to represent the aggregation behavior of various end-user load devices in the distribution system [Kim et al., 2023]. As the dynamic performance of end-user loads becomes increasingly complex with technological advances [NER, December 2016], the Western Electricity Coordinating Council (WECC) has developed the state-of-the-art composite load model (WECC CLM) [NER, December 2016, WEC, April 2021], which is capable of emulating more categories of load devices such as single-phase induction machines, power electronic-interfaced loads, as well as the distributed energy resources (DERs), which are being increasingly integrated into the power system. Though the structure of WECC CLM has been specified, this aggregated model is still like a \"grey box\", as the true values of the model parameters are not fully known.\nAs a mathematical approximation, the parameters of WECC CLM cannot be tested on-site unlike the physical entity. The load survey is a direct approach to estimate the parameters. However, it is time-consuming and require high granularity for satisfactory accuracy. An alternative is to infer from measurement data under disturbances. The measurement-based approaches have been widely investigated by researchers. In [Wang and Wang, 2014], it's formulated as a nonlinear optimization problem, which aims to find the optimum parameters that minimize the bias between the transient trajectories with estimated parameters and real measurements. The recursive least square (RLS) method is utilized to linearize the system model and identifies model parameters by minimizing the sum of the squares of the residuals in a recursive way. However, there are more than one hundred parameters and dozens of differential equations in WECC CLM, yielding a highly nonlinear and high-dimentional optimization problem with complex interaction among parameters. It is difficult to be competent in effectively solving this problem.\nEncouraged by the extraordinary capability of machine learning (ML) technology in solving complex tasks, researchers also investigate different kinds of ML-based methods for WECC CLM parameterization. In [Wang et al., 2020, Bu et al., 2020, Xie et al., 2021a], the parameter calibration problem is transformed to a markov decision process, and reinforcement learning (RL) is introduced to search for the best parameters. Some techniques, such as two-stage hierarchical framework, and evolutionary learning with sensitivity weight incorperation, are introduced to improve the accuracy. However, its optimality performance degenerates with the increase of action space and model complexity. In [Afrasiabi et al., 2023], a multi-residual deep learning structure is established to capture the spatial-temporal features and estimate the wide-area CLM parameters by learning the mapping between observations and model parameters. However, the supervised learning method fails to represent the one-to-many mapping between observations and parameters [Hu et al., 2023a]. Different from the deterministic methods, generative models learn the underlying distributions of data and deduce probabilistic solutions. Based on Bayes' theorem, several generative models, including generative adversarial network (GAN), conditional variational autoencoder (CVAE) and conditional masked autoregressive flow (CMAF) are also exploited to learn the posterior distribution of the parameters for WECC CLM [Khodayar and Wang, 2021, Khazeiynasab et al., 2022, Tan et al., 2024]. However, one difficulty in practical application is ensuring the generalizability of parameters across different fault events. The CLM parameters carefully selected during one fault event may not achieve satisfactory performance in another fault. This is primarily due to the issue of parameter degeneracy, as previously discussed. Recent works with RL have explored training parameter identification agents using multiple events directly or adopting multi-task learning approaches [Hu et al., 2023b, Xie et al., 2021b]. However, multi-event environments are inherently non-stationary, which can degrade the performance of the learning agents. Additionally, multi-task learning approaches risk negative transfer, where knowledge learned from one task hinders the learning of another.\nIn recent years, generative artificial intelligence (AI) is taking center stage in the AI domain, with the emergence of a number of advanced generative models [Cao et al., 2023, Zhang et al., 2023]. Diffusion probabilistic models employ a forward and reverse diffusion process, enabling them to accurately capture complex data distributions and embrace high-quality data generation [Ho et al., 2020, Du et al., 2023, Yang et al., 2023]. The conditional structure enables diffusion models to flexibly control its generation process towards the expected style. In addition to data generation, diffusion models also have an outstanding performance in solving inverse problems such as image restoration with the ability to learn the intricate patterns and dependencies among data [Kawar et al., 2022, Daras et al., 2024]."}, {"title": "2 Preliminary", "content": "In this section, we will present the structure of WECC CLM, formulate its parameterization problem based on Bayes' theorem. In addition, the fundamentals of the diffusion probabilistic model will also be introduced."}, {"title": "2.1 WECC CLM", "content": "WECC CLM is a state-of-the-art aggregated model of electric loads. The structure of WECC CLM is presented in Figure 1 [WEC, April 2021]. It consists of three three-phase induction motors with different characteristics (motors A, B and C), one single-phase induction motor (motor D), one power electronic load, one static load, and one distributed energy resource (DER). This enables it to represent different electric characteristics of power loads, and flexibly changes the fractions of each composition according to the actual power load. The detailed mathematical representation of WECC CLM is explained in Appendix A."}, {"title": "2.2 Parameterization Problem", "content": "WECC CLM parameterization is to properly choose the parameters for each type of aggregated models so that it's capable of duplicating the dynamic behaviors of the detailed power loads, as seen in Figure 1. Measurement-based WECC CLM parameterization can be regarded as an inverse problem, the model parameters will be estimated according to the measurement data collected at the interconnection point of transmission system and the power loads, including active and reactive power trajectories, especially under fault disturbances, as formulated in (1).\n$y = F (\\theta) \\Rightarrow \\theta = F^{-1}(y)$\n(1)"}, {"title": "2.3 Probabilistic Diffusion Model", "content": "Diffusion model is an advanced generative model, that defines a Markov chain of diffusion steps to slowly add random noise to data, and then learn to reverse the diffusion process [Ho et al., 2020]. It constructs new data samples from the noise by training with variational inference. Diffusion model belongs to the family of latent variable models and stands out due to its straightforward formulation, efficient training, and outstanding performance in generating high-quality samples. As an extension, conditional diffusion model allows for controlled generation of samples based on the additional inputs of conditions.\nThe diffusion process (forward process) is a parameterized Markov process from original data $x_0 \\sim q(x_0)$ to the latent variable $x_T$. In each diffusion step, Gaussian noise is added to the original data, as expressed by (4).\n$q(x_{1:T}|x_0) = \\prod_{t=1}^{T} q(x_t|x_{t-1}), q(x_t|x_{t-1}) = N(x_t; \\sqrt{(1 - \\beta_t)}x_{t-1}, \\beta_t I)$\n(4)\nwhere $q(x_{1:T}|x_0)$ is an approximate posterior, $N(\\mu, \\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, $\\beta_1, \\beta_2, \\cdots, \\beta_T$ is the variance schedule of the noising process. Thus we have $x_t = \\sqrt{(1 - \\beta_t)}x_{t-1} + \\sqrt{\\beta_t}\\epsilon$ by sampling $\\epsilon \\sim N(0, I)$.\nThe reverse process is defined as the joint distribution expressed by (5). It's a Gaussian transition that gradually denoises the data from $x_T \\sim N(0, I)$, and finally achieve the real distribution $x_0$.\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t), p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t,t))$\n(5)\nwhere $\\mu_\\theta(x_t, t)$ and $\\epsilon_\\theta(x_t, t)$ respectively denote the parameterized mean and variance, which are approximated by the denoise neural network.\nTraining objective is to optimize the variational bound on negative log-likelihood of the joint distribution $p_\\theta$ and $q$, which can also be transformed to the expression (6). Therefore, training the reverse process of diffusion model will be simplified to predict the Gaussian noise $\\epsilon$.\n$\\min L(\\theta) \\Rightarrow \\min E_{x_0,\\epsilon} ||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, t)||^2$\n(6)\nwhere $\\alpha_t = 1 - \\beta_t$, and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$"}, {"title": "3 JCDI Framework", "content": "Formulated WECC CLM parameterization as an inverse problem, we present JCDI, a novel framework that estimates WECC CLM parameters from measured power trajectories, as seen in Figure 2. JCDI employs the conditional diffusion model to capture the complex distributions among parameter space, and deduce the parameter posterior distribution given specific observations. Specifically, JCDI uses power trajectories as conditions to guide the diffusion process, and gradually refines parameter estimates through denoising. A denoise neural network structure, inverse grid transformer (IGT), is innovatively developed to predict the added noise in the denoise process. Therein, a transformer encoder is exploited to characterize the dependencies between different model parameters and power trajectories, and a multi-event joint conditioning scheme is incorporated to mitigate parameter degeneracy."}, {"title": "3.1 IGT architecture", "content": "We design a novel denoise neural network named IGT based on transformer encoder, as presented in Figure 3. Transformer is a revolutionary neural network architecture in natural language processing. It converts the texts into numerical representations (tokens), and captures the long-range dependencies among different words in the context based on the attention mechanism [Vaswani et al., 2017]. In our architecture, the transformer tokens are a concatenation of three types of inputs: 1) model parameters that are being denoised, 2) encoded power trajectories, 3) encoded diffusion time step t. All of them are tokenized with linear modules. The power trajectories act as the conditions, they are fed into the trajectory encoders, which employ Residual Network (ResNet) to extract trajectory features [He et al., 2016]. The diffusion step t is sinusoidally embedded, and included as a part of inputs for obtaining the diffusion step information for the diffusion model. The transformer encoder is composed a stack of multi-head attention followed with feed forward layers, and layer normalization is utilized for each sublayer. The attention model calculates the correlation of input elements (attention weight) and then produces the output representation by making weighted summation of the inputs: First, the input tokens are transformed into three variables, the query Q, key K, and value V, by multiplying their corresponding transformation matrices, as seen in (7). Next, the similarity between Q and K (the attention score), will be quantatized by their dot product, then it's scaled by $\\sqrt{d_k}$ and goes through softmax operation to obtain the attention weight. Finally the attention output is calculated by the weighted sum of value V, as expressed by (8). On this basis, the multi-head attention is implemented by performing several attention functions with different linear projections in parallel, and then concatenating the outputs. Therefore, the multi-head attention in this architecture is expected to represent the correlation among the unknown parameters of WECC CLM, as well as the relationship of model parameters and trajectory conditions, thus to improve the parameter estimation performance.\n$Q = W_Q \\cdot X, K = W_K \\cdot X, V = W_V \\cdot X$\n(7)\n$Attention (Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\cdot V$\n(8)\nwhere $W_Q$, $W_K$, and $W_V$ are the transformation matrices, X represents the input tokens, and $d_k$ denotes the dimension of keys."}, {"title": "3.2 Multi-event joint condition", "content": "Due to parameter degeneracy, the traditional parameter estimation methods using single disturbance may deduce various combinations of parameters, while some of them are not generalizable to other disturbances. Here, we address this challenge by the multi-event joint conditioning scheme: power trajectories under different fault events are input as the conditions to deduce model parameters simultaneously. Therefore, the posterior distribution of model parameters will be the joint probability that satisfies multiple trajectory conditions, which equals the product of individual ones due to the independence of fault events, as expressed by (9). This serves to reduce parameter estimation uncertainties, thereby producing more robust and generalizable solutions.\n$P(\\theta|y_{1,0}, y_{2,0}, \\cdots, y_{N,0}) = \\frac{P(y_{1,0}, y_{2,0}, \\cdots, y_{N,0}|\\theta) \\cdot P(\\theta)}{P(y_{1,0}, y_{2,0}, \\cdots, y_{N,0})} = \\frac{P(y_{1,0}|\\theta) \\cdot P(y_{2,0}|\\theta) \\cdot \\cdots \\cdot P(y_{N,0}|\\theta) \\cdot P(\\theta)}{P(y_{1,0}) \\cdot P(y_{2,0}) \\cdot \\cdots \\cdot P(y_{N,0})} \\propto P(y_{1,0}|\\theta) \\cdot P(y_{2,0}|\\theta) \\cdot \\cdots \\cdot P(y_{N,0}|\\theta) \\cdot P(\\theta)$\n(9)\nwhere $y_{i,0}, i = 1:N$ denotes the measurement under the i-th fault event."}, {"title": "4 Simulation studies", "content": ""}, {"title": "4.1 Simulation settings", "content": "Model Configuration To validate the proposed algorithm, WECC CLM is supplied by the IEEE 39-bus transmission system. The voltage profiles at bus 9 of the transmission system under different electric fault disturbances are fed into WECC CLM, and generate the system responses. In this work, three distinct fault events, including three-phase grounding bus faults that occur at bus 27, 5, 9 in the transmission system with fault clearing times of 135, 135, 44 ms, respectively, are selected to train the proposed JCDI. The selection of these specific fault events is motivated by the different dynamic behavior of the load system: Fault event 2 (FT_2) results in power electronic load tripping, Fault event 3 (FT_3) induces motor D stalling, while Fault event 1 (FT_1) exhibits neither phenomenon.\nParameter Selection There are more than 100 unknown parameters in WECC CLM. Some of these parameters demonstrate significantly impact on the system dynamics, while others have a minor effect, and are difficult to estimate. To solve the parameterization problem efficiently and effectively, we conduct global sensitivity analysis with Sobol method across different fault events, and select 32 sensitive parameters for identification. The details of Sobol sensitivity analysis are presented in Appendix B.\nEvaluation Metrics To have a quantitative evaluation of the parameter identification result, two evaluation indices, that respectively measure the estimation accuracy of model parameters and power trajectories, are utilized in this paper.\nFor parameter estimation accuracy, the mean absolute percentage error (MAPE) is commonly used to measure the relative accuracy [de Myttenaere et al., 2016]. However, it will become very large when the actual parameter value is close to zero. Therefore, we define a variant of MAPE, the mean absolute range percentage error (MARPE), to assess the parameter prediction accuracy, as expressed by (10). In MARPE, the absolute parameter errors are divided by the possible range of parameters, i.e., $U_B \u2013 L_B$, instead of the actual parameter value $\\theta_0$. Therefore, the evaluation will not be affected by the values of actual parameters within the same range.\n$MARPE = 100\\frac{1}{N}\\sum_{i=1}^{N} \\frac{|\\theta_i - \\theta_0|}{U_B - L_B}$\n(10)"}, {"title": "4.2 Results", "content": "A. Training progress The proposed JCDI is trained for 5000 epochs until the training and testing losses level off. And then, the desired trajectory is injected to the well-trained model, and the model parameters are inferred probabilistically. The algorithm implementation details are explained in the Appendix C. In order to verify the effectiveness of multi-event joint conditioning, the proposed JCDI, which is conditioned on three fault events (FT_1, FT_2 and FT_3), is compared with CDI, the diffusion-based parameter estimation conditioned on a single event FT_1.\nFigure 4 illustrates the training progress, including the evolution of losses and prediction errors for both CDI and JCDI. As the number of epochs increases, they decrease smoothly for both the training and testing sets. Compared with CDI, JCDI achieves lower final loss and smaller predicted parameter and trajectory errors. However, it is worth noting that even in the later stages of training, some predicted model parameters still deviate from their actual values, while the trajectory errors remain within a very small range. This suggests that the trained model can still accurately predict power trajectories even in the presence of these parameter deviations.\nB. Trajectory analysis Substituting the inferred parameter sets into the WECC CLM, we deduce the post-fault trajectories under different fault events, as illustrated in Figure 5. We also calculate and compare the trajectory RMSES in Figure 6. Under FT_1, the estimated active and reactive power trajectories under both CDI and JCDI closely match the actual trajectories. The mean trajectory RMSEs are approximately 0.001. Under FT_2, which involves electronic load tripping, the steady-state active power after the fault becomes smaller than its initial value due to incomplete load recovery. And under FT_3, which induces motor stalling, the absolute values of steady powers after the fault significantly increase. The estimated post-fault trajectories under CDI deviates substantially from the actual trajectories,\n$\\text{RMSE} = \\sqrt{\\frac{1}{T} \\sum_{t=1}^T (p_a(t) - p_o(t))^2 + \\frac{1}{T} \\sum_{t=1}^T (q_a(t) - q_o(t))^2}$\n(11)\nwhere $\\hat{p}_\\theta (t)$ and $\\hat{q}_\\theta(t)$ respectively represent the estimated active and reactive powers at time t, $p_0(t)$ and $q_0(t)$ are the actual active and reactive powers, T is the time instant."}, {"title": "5 Conclusion", "content": "In this work, we present JCDI, a novel probabilistic parameter estimation framework that addresses the key challenges of parameter degeneracy and cross-event generalization through the multi-event joint conditioning structure and transformer encoder-based denoise neural network. Successful verification of JCDI has been achieved for WECC CLM in power systems. Our comprehensive evaluation yields several key findings. First, the global sensitivity analysis reveals the sensitivity discrepancies under different fault events, particularly when power electronic load tripping or motor stalling occurs. Second, single-event parameterization produces multiple valid parameter sets that accurately model power trajectories - but only for the specific disturbance, demonstrating the degeneracy property in WECC CLM. Third, JCDI enhances parameter estimation accuracy in these degenerate cases. The parameters derived by JCDI accurately reproduce power trajectories under a group of fault events. Finally, comparative studies firmly establish JCDI's superiority over existing parameter estimation methods, including deep reinforcement learning and supervised neural networks. Beyond load modeling application, the proposed JCDI can also be extended to other electric systems, such as power electronic converters and energy storage systems, as well as other research fields."}, {"title": "Appendix A Mathematical model and characteristics of WECC CLM", "content": "This appendix introduces the mathematical model and electrical characteristics of each component in WECC CLM. Each component is specified by a certain fraction ($F_{ma}$, $F_{mb}$, $F_{mc}$, $F_{md}$, $F_{el}$, $F_{derA}$ for fractions of motors A, B, C, D, power electronic load, and DER, respectively, and the remainder for static load), and the individual responses are summed together to represent the overall performance of electric loads.\nThree-Phase Induction Motors: WECC CLM categories the three-phase induction motors into motors A, B and C. They are modeled by fifth-order differential-algebraic equations based on electromagnetic equations and electromechanical equations with different parameters [Ma et al., 2020]. Motor A characterizes the three-phase induction motors that drive low inertia constant torque loads, typical examples include positive displacement compressors and pumps [EPR, September 2020]. Motor B represent the high inertia variable torque loads, such as large fans, and motor C is used to model the low inertia variable torque loads, such as centrifugal pumps.\nSingle-Phase Induction Motor: Motor D represents the single-phase induction motor, such as residential air-conditioners and heat ventilation. In WECC CLM, it's developed as a \"performance model\" based on laboratory test. One important characteristic for the single-phase induction motor is the stalling behavior during voltage dips, that is, there is insufficient motor torque to overcome the load torque and therefore the motor stops. Figure 10 shows the power change of motor D during the stalling and recovering processes.\nWhen the voltage remains above the motor compressor breakdown voltage, i.e. $V > V_{brk}$, the motor works in run state, and the active and reactive powers are expressed by (12).\n$p = (p_0 + K_{p1} \\cdot (V - V_{brk})^{N_{p1}}) \\cdot (1 + CmpK_{pf} \\cdot \\Delta f)$\n$q = (q_0 + K_{q1} \\cdot (V - V_{brk})^{N_{q1}}) \\cdot (1 + CmpK_{qf} \\cdot \\Delta f)$\n(12)\nwhere $p$ and $q$ are the active and reactive powers of motor D, $p_0$ and $q_0$ are the initial powers. $V$ denotes the terminal voltage, $\\Delta f$ represents the frequency deviation. $K_{p1}$ and $K_{q1}$ are the voltage coefficients, $N_{p1}$ and $N_{q1}$ are the power exponents, $CmpK_{pf}$ and $CmpK_{qf}$ are the frequency coefficients.\nWhen the voltage reduces between $V_{brk}$ and the motor stalling voltage $V_{stall}$, motor D still works in run state, but the expressions of powers are formulated as (13).\n$p= (p_0 + K_{p2} \\cdot (V_{brk} - V)^{N_{p2}}) \\cdot (1 + CmpK_{pf} \\cdot \\Delta f)$\n$q = (q_0 + K_{q2} \\cdot (V_{brk} - V)^{N_{q2}}) \\cdot (1 + CmpK_{qf} \\cdot \\Delta f)$\n(13)\nwhere $K_{p2}$ and $K_{q2}$ are the voltage coefficients, $N_{p2}$ and $N_{q2}$ are the power exponents.\nWhen the voltage drops lower than $V_{stall}$ for a time duration of $T_{stall}$, motor D transitions to stall state, and the active and reactive powers are calculated by (14).\n$p = V^2/R_{stall}$\n$q = -V^2/X_{stall}$\n(14)\nwhere $R_{stall}$ and $X_{stall}$ respectively denote the stall resistance and reactance."}, {"title": "Static Load Model:", "content": "The ZIP model, which consists of constant impedance (Z), constant current (I) and constant power (P) components, is used to represent the static loads. The active and reactive powers are time independent, their relationship with voltage is expressed by (15).\n$p=p_{0} \\left(p_{1 c}\\left(\\frac{V}{V_{0}}\\right)^{2}+p_{2 c}\\left(\\frac{V}{V_{0}}\\right)+p_{3}\\right) \\cdot\\left(1+p_{f r q} \\cdot \\Delta f\\right)$\n$q=q_{0} \\left(q_{1 c}\\left(\\frac{V}{V_{0}}\\right)^{2}+q_{2 c}\\left(\\frac{V}{V_{0}}\\right)+q_{3}\\right) \\cdot\\left(1+q_{f r q} \\cdot \\Delta f\\right)$\n(15)\nwhere $p_0$ and $q_0$ are the initial active and reactive powers of static load, which are calculated by (16), $V_0$ is the initial terminal voltage. $p_{1c}$, $q_{1c}$, $p_{2c}$ and $q_{2c}$ are power coefficients. $p_3$ and $q_3$ are the percentages of constant power loads, as calculated by (17). $p_{frq}$ and $q_{frq}$ are the frequency sensitivities of active and reactive powers.\n$p_{0} = P_{\\text {load }} \\cdot\\left(1-F_{m a}-F_{m b}-F_{m c}-F_{m d}-F_{e l}-F_{d e r A}\\right)$\n$q_{0} = p_{0} \\tan \\left(a \\cos \\left(P F\\right)\\right)$\n(16)\nwhere $P_{load}$ represents the total active power of WECC CLM, $PF$ represents the power factor of static load.\n$p_{3}=1-\\frac{p_{1 c}-p_{2 c}}{p_{0}}$\n$q_{3}=1-\\frac{q_{1 c}-q_{2 c}}{q_{0}}$\n(17)"}, {"title": "Power Electronic Load:", "content": "The power electronic load in WECC CLM represents an aggregation of inverter-interfaced or electronic coupled loads, such as consumer electronic devices like computers. The power-voltage relationship of electronic load is shown in Figure 11. When the voltage maintains above $V_{d1}$, it consumes constant active and reactive power. When the voltage drops lower than $V_{d1}$, the electronic loads start to trip and the powers reduce linearly with voltage until 0. The active and reactive powers will also recover gradually with voltage but with a certain fraction."}, {"title": "DER:", "content": "Distributed energy resource model version A (DER_A) is newly developed in load modeling to represent the aggregation of inverter-based generation (e.g. photovoltaic) [CMP, Feburary 2015]. Its block diagram and specification refers to [DER, 2019]. Compared with the previous DER model pvd1, DER_A has more functionalities, such as various control modes, thus can represent various distributed generation (DG) models to be plugged into WECC CLM."}, {"title": "Appendix B Sobol global sensitivity analysis", "content": "In this work, sensitivity analysis based on Sobol's method [Sobol, 2001, Tosin et al., 2020, Saltelli, 2002, Iwanaga et al., 2022, Herman and Usher, 2017] is conducted for parameter reduction. Sobol's method is a variance-based global sensitivity analysis method, which decomposes the variance of output into contributions from individual input parameters and their interactions."}, {"title": "Appendix C Algorithm settings", "content": "JCDI The algorithm and training parameters for the proposed JCDI are listed in Table 4. The transformer encoder includes 3 layers and 4 attention heads. There are 2 Resnet blocks and 3 Stem blocks in the trajectory encoder. For the diffusion process, the diffusion step is 200, and a linear variance schedule is used to add noise. Adam Optimizer is used to minimize the loss function, with a learning rate of 1 \u00d7 10-4, and a batch size of 128. Training is implemented with Pytorch on a NVIDIA GeForce RTX 3090 graphics processing unit (GPU).\nSupervised learning The neural network structure for supervised learning is presented in Figure 14. The active and reactive power trajectories are input to the ResNet-based trajectory encoder, which extracts the features of the trajectories. The output features are then tokenized and input into the transformer encoder, based on which the model parameters are inferenced. The algorithm parameters are listed in Table 5.\nReinforcement learning Formulated the parameter calibration process for WECC CLM as a MDP, the agent starts from an initial parameter estimation state, modifies the parameters at each step, transfers to the next state of estimation, calculates the reward according to the change of RMSE, and finally learns a policy that is able to adjust the WECC CLM parameters in the direction of minimizing RMSEs of dynamic responses. Therefore, the state is defined as the current estimation of parameters, as expressed by (21).\n$s = \\theta$\n(21)\nAction is defined as the adjustment of parameters, as expressed by (22).\n$a = \\Delta \\theta$\n(22)\nState transition represents the parameter update, as represented by (23).\n$s' = \\theta' = \\theta + \\Delta \\theta$\n(23)\nReward is calculated according to the decrement of trajectory RMSE in (11). To motivate continuous accuracy improvement when RMSE is small, a reciprocal respresentation is used, as seen in (24).\n$r = \\frac{1}{\\text{RMSE}+0.1}$\n(24)"}]}