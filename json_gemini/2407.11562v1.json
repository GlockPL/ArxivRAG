{"title": "RobotKeyframing: Learning Locomotion with High-Level Objectives via Mixture of Dense and Sparse Rewards", "authors": ["Fatemeh Zargarbashi", "Jin Cheng", "Dongho Kang", "Robert Sumner", "Stelian Coros"], "abstract": "This paper presents a novel learning-based control framework that uses keyframing to incorporate high-level objectives in natural locomotion for legged robots. These high-level objectives are specified as a variable number of partial or complete pose targets that are spaced arbitrarily in time. Our proposed framework utilizes a multi-critic reinforcement learning algorithm to effectively handle the mixture of dense and sparse rewards. Additionally, it employs a transformer-based encoder to accommodate a variable number of input targets, each associated with specific time-to-arrivals. Throughout simulation and hardware experiments, we demonstrate that our framework can effectively satisfy the target keyframe sequence at the required times. In the experiments, the multi-critic method significantly reduces the effort of hyperparameter tuning compared to the standard single-critic alternative. Moreover, the proposed transformer-based architecture enables robots to anticipate future goals, which results in quantitative improvements in their ability to reach their targets.", "sections": [{"title": "1 Introduction", "content": "Legged robots hold a great promise for becoming household companions [1] or automated performers in the entertainment industry [2, 3]. In these applications, it is crucial for robot controllers to perform natural and directable behavior from simple high-level user command inputs beyond the common commands used in the robotic domain such as joystick velocity commands [4, 5] or target base position [6, 7].\nIn the character animation domain, a widely used technique for specifying character behavior from simple and sparse inputs is keyframing [8, 9]. It involves defining the target position or kinematic pose of the character at particular points in time, allowing animators to create smooth movements by interpolating between these keyframes. Despite its proven effectiveness within the kinematic animation pipeline, incorporating keyframing for achieving time-specific targets remains unexplored in the realm of physics-based robot control.\nInspired by character animation, we aim to equip legged robots with more refined control by incorporating sparse and temporal high-level objectives as keyframes. The primary goal of this work is to develop a locomotion controller that enables the robot to fulfill specified partial or full-pose targets while infilling natural behavior during the intermediate periods. This goal aligns with recent advancements in using reinforcement learning (RL) for legged robots due to their promising robustness and flexibility [10, 11]. However, learning a policy that accurately meets keyframes without imposing undesired constraints at intermediate periods presents challenges, particularly due to the need to handle sparsity in the keyframe objectives. Acquiring effective policies requires a meticulous reward design procedure that carefully balances these sparse rewards with other dense rewards which are crucial for regularizing and encouraging natural motion.\nIn this work, we present a novel framework that unifies timed high-level objectives with natural locomotion of legged robots through temporal keyframes. Along with the imitation objective similar to Peng et al. [12] for natural motion generation, our pipeline allows specifying full or partial high-level targets, including base position, orientation, and joint postures. We propose using a multi-critic RL framework to address the challenge of managing groups of sparse and dense rewards by learning distinct value functions. Our method also employs a novel transformer-based architecture to encode a variable number of goals with arbitrary time intervals. Unlike typical sequence-to-sequence transformers [13], we propose a lightweight sequence-to-token module that can be used autoregressively within a feedback control loop. We demonstrate the effectiveness of our framework through experiments both in simulation and on real-world hardware. Our policies successfully guides the robot to meet multiple keyframes at the required times, for both position and posture targets. Furthermore, the multi-critic approach showcases better convergence with less hyperparameter tuning compared to the conventional single-critic method. Our experiments also reveals that using a transformer-based encoder to anticipate future goals significantly enhances goal-reaching accuracy.\nThe contribution of this paper is threefold: (i) We introduce RobotKeyframing, a novel learning-based framework for integrating high-level objectives in natural locomotion of legged robots; (ii) We propose using multi-critic RL to handle the mixture of dense and sparse rewards and a novel sequence-to-token encoder to accommodate a variable number of keyframes; (iii) We validate the effectiveness of our method through extensive experiments in simulation and on hardware."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Reinforcement Learning for Legged Robots", "content": "Over the last decade, reinforcement learning has been increasingly applied to develop locomotion policies for legged robots [4, 14, 15]. The primary focus has been to achieve robust control policies that can accurately track velocity commands from joysticks [11, 16, 17]. More recently, researchers have attempted to enhance the versatility of legged robot controllers by incorporating high-level objectives, particularly through position- or orientation-based targets [18, 19, 20]. This high-level control is typically accomplished through hierarchical frameworks, where a high-level policy is learned to drive a low-level controller [7, 21, 22]. Conversely, end-to-end approaches aim to develop a unified policy for both high- and low-level control, allowing high-level objectives to directly influence low-level decisions [6, 18, 23]. However, the aforementioned methods typically urge the robot to reach a target as fast as possible, lacking refined control over the temporal profile of achieving the target. Inspired by keyframing in animation, this work aims to further expand control over robot motion by incorporating multiple keyframes as input to the control policy, thereby enabling robots to generate diverse behaviors in reaching targets. We further enhance this versatility by allowing partial or full targets, including base position, orientation, and joint postures."}, {"title": "2.2 Natural Motion for Characters and Robots", "content": "Synthesizing naturalistic behavior from existing motion datasets while fulfilling spatial or temporal conditions has been extensively studied in the character animation domain [24, 25, 26, 27]. Existing research for generating natural motions between keyframes [28, 29, 30] has mainly focused on the kinematic properties of characters and thus cannot be directly applied to physics-based characters or robots, whose dynamic interactions with the environment require consideration of both kinematics and dynamics. Various efforts have also been made to combine kinematic motion generation with physically controlled robots to achieve natural behavior on hardware [31, 32, 33, 34]. Another thread of research focuses on controlling characters in physically simulated environments, incorporating motion datasets as demonstrations [35, 36, 37, 38]. Some of these methods have also been successfully transferred to robot control for quadrupeds or humanoids [39, 40, 41, 42]. Among these works, Adversarial Motion Priors (AMP) [12] provides a flexible way to encourage the policy to have natural, expert-like behavior by connecting generative adversarial networks (GAN) [43] with RL given an offline motion dataset. We also incorporate an AMP-based imitation objective to encourage naturalistic motion for the policy and further extend it to infilling keyframes for robots."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Setup", "content": "To integrate high-level control objectives into the robotic control framework, we employ sparse keyframes that require a robot to achieve specific goals at predetermined times. Each keyframe contains a full or partial combination of a variety of targets such as global base position $\\hat{p} \\in \\mathbb{R}^{3}$, global base orientation $(\\phi, \\zeta, \\psi) \\in \\mathbb{R}^{3}$ where $\\phi, \\zeta, \\psi$ denote roll, pitch, and yaw angles respectively, and full posture specified by joint angles $\\theta_{j} \\in \\mathbb{R}^{N_{j}}$ where $N_{j}$ is the number of joints. Each keyframe is also assigned with a specific time $t \\in \\mathbb{R}$ in the future at which the robot is expected to meet the goals. In summary, the high-level objectives are specified through these keyframes $K = (k_{1},k_{2}, ..., k_{n_{k}})$, where $k_{i} = (\\hat{g}, t)$ and $\\hat{g} \\subset \\{p, \\phi, \\zeta, \\psi, \\theta_{j}\\}$. Here, $n_{k} < N_{k}$ where $n_{k}$ and $N_{k}$ denote the actual and the maximal number of keyframes, respectively. We aim to support an arbitrary number of keyframes, allowing for the flexible specification of high-level objectives only as needed. The main goal is to train a locomotion policy for legged robots that not only meets these keyframes but also maintains a natural style in the intervals between them. To avoid undesired restrictions on the intermediate periods, policy's task performance is evaluated exclusively at the designated times, making the keyframe objectives temporally sparse. However, relying solely on keyframes to train the control policy may result in undesirable motions. Thus, it is crucial to have additional rewards for regularizing and promoting a natural motion style. In this regard, we incorporate AMP [12] as a general style guide for the robot, encouraging the policy to behave naturally and similarly to an offline motion dataset from real animals [44]. The style and regularization rewards are evaluated at every step of the episode, making them temporally dense. The mixture of sparse and dense rewards presents a unique challenge that is difficult to manage effectively with standard RL frameworks. Further details on the observation, action, reward definitions, and training procedure can be found in Appendix A."}, {"title": "3.2 Multi-Critic RL for Dense-Sparse Reward Mixture", "content": "Modern RL algorithms [45, 46, 47] typically employ the actor-critic paradigm, where the actor decides the action to take, and the critic evaluates the action by estimating the value function. To effectively manage a complex mixture of temporally dense and sparse rewards, we employ a multi-critic (MuC) RL framework by Martinez-Piazuelo et al. [48] as shown in Fig. 2. It involves training a set of critic networks $\\{V_{\\phi_{i}}\\}_{i=0}^{n}$ to learn distinct value functions associated with different reward groups $\\{r_{i}\\}_{i=0}^{n}$. Similar concepts have been used to balance a set of dense rewards [49, 50]; however, we aim to adapt the multi-critic method to the context of dense and sparse reward combination. We design each reward group to contain either exclusively dense or sparse rewards. This division"}, {"title": "3.3 Transformer-based Keyframe Encoding", "content": "The transformer framework [51] has achieved great success in modeling sequential data not only in the natural language processing [52, 53] but also in other areas including robotics [54]. The attention mechanism, serving as the core of transformer networks, models the correlation between each element of the input sequence and reweights them accordingly. To handle a variable number of keyframes in our problem, we utilize a transformer-based encoder to process the sequence of goals for both the policy and critics. However, unlike the typical application of transformers in sequence-to-sequence tasks, we adapt the architecture to function in a sequence-to-token manner, as shown in Fig. 3. This adaptation makes it suitable for autoregressive feedback control in robotic systems.\nIn our system, each input token corresponds to a particular keyframe. At every time step $t$, each keyframe $k^{i}$ is transformed spatially and temporally into a robot-centric view, resulting in a goal error $\\Delta g^{i}$ and a calculated time to goal $\\hat{t}^{i} - t$. These are then concatenated with the robot state $s_{t}$ to form a single token. Additionally, we incorporate a self-goal keyframe, $\\ae$, as the first token in the sequence. This token represents a state with zero error and zero time to goal, which ensures that the control system remains operational despite the absence of active goals or after achieving all goals. The transformer encoder receives the sequence of tokens $X_{t} = (x^{1},...,x^{n_{k}})$, where $x^{<1>} = (s_{t}, 0, 0)$, and $x^{i} = (s_{t}, \\Delta g^{i}, \\hat{t}^{i} - t)$ for $i = 1, ..., n_{k}$.\nIn scenarios where the number of active keyframes is less than the maximum capacity of the system, we apply masking to ignore the surplus tokens and focus only on the relevant keyframes. Furthermore, we also apply masking to keyframes once their designated time is reached and surpassed by a few steps. This practice prevents past goals from inappropriately influencing the long-term behavior of the policy. The output from the transformer encoder is then forwarded to a max-pooling layer, which condenses the encoded goal features for delivery to the subsequent multilayer perceptrons (MLP). By leveraging transformer's ability to handle sequences of varying lengths, our architecture can effectively integrate multiple and arbitrary numbers of goals into the control process."}, {"title": "4 Results", "content": "The control policies are trained for quadruped robots with 12 degrees of freedom (DoF) using Isaac Gym [55]. At the start of each episode, the robot is either set to a default state or initialized according to a posture and height sampled from the dataset, a technique known as Reference State Initialization (RSI) [56]. We incorporate a learning curriculum, beginning with keyframes entirely sourced from"}, {"title": "4.1 Keyframe Tracking", "content": "We demonstrate that our trained policy effectively reaches keyframes at the designated times through several simulation experiments. Given keyframes consisting of position goals, our policy reaches its targets with notable precision, as illustrated in Fig. 4a by the horizontal trajectories for two example scenarios with different number of keyframes. Furthermore, our framework offers control over target reaching time and can generate diverse behaviors for the same targets by specifying different time profiles. This is depicted in Fig. 4b through snapshots of robot motion when provided with keyframes consisting of the same position goal, but different target times. Full posture targets are also supported along with position and orientation goals. Fig. 5 shows snapshots of the robot motion given different keyframe scenarios, highlighting that our policy accurately meets its full posture targets and maintains a natural style while reaching them."}, {"title": "4.2 Multi-Critic RL", "content": "In this section, we conduct a comparative analysis between multi-critic and single-critic approaches in the keyframing setup. Learning curves for both methods are presented in Fig. 6, with each method trained across three different ranges of sparsity ratios by sampling keyframes with varying time horizons. Initially, reward and advantage weights are tuned separately for single- and multi-critic according to the time horizon range [25, 50]. New policies are then trained using the same weights"}, {"title": "4.3 Future Goal Anticipation", "content": "An advantage of using a transformer-based encoder is that it enables the policy to incorporate multiple and a varying number of goals as input. If the goals are temporally close to each other, awareness of future goals influences the robot's motion to achieve all of them more accurately. The phenomenon of future goal anticipation is demonstrated in Fig. 7 where we compare a policy aware of all goals and a policy only aware of the immediate next goal, both trained with only position goals in the keyframe. The policy trained with multiple keyframes adopts a larger yaw angle at the first goal, leaning more towards the second one to be able to reach it with higher accuracy. Table 1 provides a quantitative comparison of the two policies across three different scenarios: straight, turn and slow turn, the latter featuring a longer time horizon for the second goal. The results indicate that future goal anticipation helps the policy to adjust its motion while approaching earlier goals to gain better accuracy for the subsequent targets. This is particularly important when keyframes are temporally close, resulting in higher accuracy gains in fast and dynamic movements, compared to slower ones."}, {"title": "4.4 Hardware Deployment", "content": "We validate our method through extensive hardware experiments using the Unitree Go2 [57], a 12-DoF commercial quadruped robot. Fig. 8 illustrates the outcomes of a policy that manages up to 5 positional goals arranged in different courses, and a policy trained for full pose targets that successfully drives the robot to achieve various posture keyframes. These experiments underscore"}, {"title": "5 Discussion", "content": "Conclusion: This paper presents RobotKeyframing, a learning-based control framework designed to incorporate high-level objectives into the natural locomotion of legged robots through a sequence of keyframes. Simulation and hardware experiments demonstrate the efficacy of our framework. The sparse reward imposed by keyframe objectives is effectively handled by a multi-critic PPO algorithm. In addition, the transformer-based architecture is adaptive to an arbitrary number of target keyframes and improves accuracy in reaching targets through future goal anticipation.\nLimitations and future work: First, if the timing values are infeasible for the specified goals, the robot may fail to meet the targets. However, it is worth noting that such cases do not result in uncontrolled behaviors, such as falling down. Second, our approach inherits the mode collapse issue from the AMP framework [12], which can be mitigated in future research through the integration of style embeddings. Third, the performance of our policy is currently limited by the motions in the dataset, restricting its ability to generalize to out of distribution motions or targets. Looking ahead, our method can be expanded to incorporate diverse types of goals in the keyframes, such as end-effector targets or more intuitive high-level inputs such as skill or text. Additionally, RobotKeyframing can be extended to more complex characters and potentially used for physics-based motion in-betweening in the character animation domain."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Observation and Action Space", "content": "The observation of the policy is composed of two main components: state observation and goal observation. State observation at time t include the linear velocity (v) and angular velocity ($\\omega$) of the base in local coordinates, current joint angles ($\\theta_{j}$), current joint velocities ($\\dot{\\theta_{j}}$), projected gravity in the base frame ($g_{proj}$), base height (h) and previous actions ($a_{prev}$),\n$s_{t} = \\{v, \\omega, \\theta_{j}, \\dot{\\theta_{j}}, g_{proj}, h, a_{prev}\\}_{t}$.\n$A variable number of keyframes $K = (k_{1},k_{2}, ..., k_{n_{k}})$ are specified as targets for the robot. At each time step t, each keyframe $k^{i}$ is transformed spatially and temporally into a robot-centric view. Then, the goal observation is prepared by calculating the remaining time to goal $\\hat{t}^{i} - t$ and the error to target goals ($\\Delta g^{i}$),\n$\\Delta g^{<i} \\subset \\{\\Delta p^{i}, \\Delta \\phi^{i}, \\Delta \\zeta^{i}, \\Delta \\psi^{i}, \\Delta \\theta_{j}^{i} \\}$.\nHere, $\\Delta p$ denotes the error between robot base position and keyframe position in the base coordinate frame, $\\Delta \\theta_{j}$ is the error in joint angles, and $\\Delta \\phi, \\Delta \\zeta$ and $\\Delta \\psi$ denote the errors in roll, pitch and yaw angles, respectively, which are wrapped to $(-\\pi, \\pi]$.\nThe policy receives the sequence of tokens $X_{t} = (x^{<1>},...,x^{n_{k}})$ as input to the encoder, where $x^{<1>} = (s_{t}, 0, 0)$, and $x^{i} = (s_{t}, \\Delta g^{i}, \\hat{t}^{i} - t)$ for $i = 1,..., n_{k}$. Thanks to the transformer-based keyframe encoding, the extra tokens can be masked to enable arbitrary number of goals. In addition, keyframes with a time over one second past the current time are also masked to avoid any long-term influence on reaching the future goals.\nThe action ($a_{t}$) space of the policy is set to target joint angles, which are tracked using a PD controller to compute the motor torques."}, {"title": "A.2 Reward Terms", "content": "We include three groups of rewards in this framework: regularization, style, and goal. For each reward group, the final reward is computed as a multiplication of individual reward terms,\n$r_{group} = \\prod_{i \\in group} r_{i}$.\nRegularization rewards are designed to provide a smooth output of the policy and consist of several terms defined in Table A1. Here, K is an exponential kernel function defined in Eq. 9 where $\\sigma$ and $\\delta$ are the sensitivity and tolerance of the kernel function, respectively.\n$K(x, \\sigma, \\delta) = exp\\left(-\\frac{\\left(max(0, ||x|| - \\delta)\\right)^{2}}{\\sigma}\\right)$\nTo generate natural motion between the keyframes, we use AMP proposed by Peng et al. [12], which involves training a discriminator D to identify motions that are similar to those of the offline expert"}, {"title": "A.3 Dataset Preparation", "content": "We use a database of motion capture from dogs introduced by Zhang et al. [44]. The motions are retargeted to the robot skeleton using inverse kinematics for the end-effectors' positions with some local offsets to compensate for the different proportions of the robot and dog. A subset of around 20 minutes of data was used, removing the undesired motions such as smelling the ground, walking on slopes, etc. We augment this dataset with other motion clips animated by artists to include more diversity in the dataset. The frame rate is adjusted to that of the simulation, i.e. 50 frames per second."}, {"title": "A.4 Training Procedure", "content": "We utilize Isaac Gym [55] for simulating the physical environment. At the start of each episode, the robot is either set to a default state or initialized according to a posture and height sampled from the dataset with Reference State Initialization (RSI). RSI plays a crucial role in capturing and learning the specific style of motion, as highlighted in previous studies such as Peng et al. [56]. Keyframes are derived either randomly or directly from a reference data trajectory. Our methodology incorporates a learning curriculum, beginning with keyframes entirely sourced from reference data and progressively increasing the proportion of randomly generated keyframes. To generate random keyframes, we start by selecting a time interval for each goal within a predetermined range. Subsequently, the distance and direction of the target position relative to the previous goal (or the initial position for the first goal) are sampled based on a specified range. The yaw angle is also chosen from a set range and adjusted relative to the previous goal. The robot's full posture is sampled from the dataset to ensure the target posture is feasible. The roll, pitch, and height of the keyframe are aligned with the corresponding attributes of the target posture frame.\nThe meticulous sampling of target keyframes is critical for ensuring their feasibility and preventing them from impeding effective policy learning.\nWe train the policy to handle a maximum number of keyframes, randomly selecting the actual number of keyframes for each episode. To avoid negative impacts on training, unused goals are masked when input into the transformer encoder. For stability, the episode does not terminate immediately after the last goal is reached; instead, it terminates approximately one second later. The training"}, {"title": "A.5 Hardware Implementation Details", "content": "Domain randomization is added during training to achieve a robust policy that can be executed on hardware. Similar to Kang et al. [58], we randomize friction coefficients, motor stiffness and damping gains and actuator latency. Furthermore, we add external pushes during training. Although joint limits are softly taken into account in the simulation, we found it crucial to terminate episodes when reaching joint limits to ensure a stable deployment on hardware. We use a motion capture system to receive the global position and orientation of the robot. These are used to compute the relative errors to the target goals and are then passed to the policy. Other observations are computed based on the outputs from the state estimator."}, {"title": "A.6 Future goal anticipation", "content": "Details of target keyframes used for Table 1 are given in Table A3."}, {"title": "A.7 Training Hyperparameters", "content": "Table A4 provides details of hyperparameters used for training."}]}