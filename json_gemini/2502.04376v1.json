{"title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf", "authors": ["Lingxiang Hu", "Shurun Yuan", "Xiaoting Qin", "Jue Zhang", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/40 maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.", "sections": [{"title": "1 Introduction", "content": "Nowadays, the nature of work has increasingly become more collaborative (Mugayar-Baldocchi et al., 2021), with meetings becoming an essential component (Spataro, 2020) to facilitate the exchange of ideas and information, fostering innovation and ensuring alignment among team members. Attending meetings, however, poses notable difficulties. Firstly, the rapid increase in the number of meetings can consume a substantial amount of time, diverting attention from core tasks and reducing overall productivity (Perlow et al., 2017; Kost, 2020). Secondly, scheduling conflicts often arise when multiple meetings are double-booked, forcing participants to prioritize or miss valuable discussions altogether. Thirdly, not all meetings require full attendance; participants may only need to contribute to specific topics, leading to inefficiencies when attendees are required for entire duration.\nIn this study, we investigate the feasibility of developing a meeting delegate system to represent individuals in meetings. This concept is becoming increasingly viable with the advancement of Large Language Models (LLMs). These LLMs, renowned for their remarkable capabilities in natural language understanding and generation (Ouyang et al., 2022; OpenAI, 2023; Google, 2024a), demonstrate potential to comprehend meeting context, participate in dynamic conversations, and provide informed responses.\nDeveloping LLM-powered meeting delegate systems faces several challenges. Firstly, such systems must navigate complex, context-rich conversations involving multiple participants, requiring them to discern opportune moments for engagement and restraint. Secondly, human conversations often contain ambiguities and uncertainties, such as queries directed ambiguously or pronunciation-related ambiguities, which challenge the system's ability to respond effectively. Thirdly, ensuring user privacy is crucial to prevent over-sharing of information and safeguard the user's personal image. Finally, these systems must operate in real-time, necessitating low-latency responsiveness.\nIn this work we aim to develop a prototype of an LLM-powered meeting delegate system to address the above challenges, focusing initially on the first two while leaving the last two in the future work. To assess its effectiveness across various LLMs, we conduct real-world testing in a few demo scenarios"}, {"title": "2 Related Work", "content": "Language Model Applications in Meetings. Considerable research has been dedicated to the summarization of meetings (Zhong et al., 2021) and other real-life dialogues (Mehdad et al., 2014; Tuggener et al., 2021). In the context of meetings, key tasks include meeting transcript summarization and action item identification (Cohen et al., 2021). MeetingQA (Prasad et al., 2023) investigated Q&A tasks based on meeting transcripts, highlighting the challenges faced by models such as RoBERTa in handling real-world meeting data. Recent advancements in LLMs have opened new avenues for enhancing these tasks. For instance, an LLM-based meeting recap system (Asthana et al., 2023) has demonstrated effectiveness in generating accurate and coherent summaries and action items.\nFacilitator in Multi-Participant Chat. MUCA (Mao et al., 2024) presents a framework that leverages LLMs to facilitate group chats by simulating users, demonstrating notable effectiveness in goal-oriented conversations. Similarly, approaches like GPT-40 demo for meetings (OpenAI, 2024a) are designed to serve as facilitators in group discussions. While these studies underscore LLMs' capabilities in managing group chats, they primarily focus on LLMs guiding the meeting process rather than representing individuals with different roles.\nRole-Playing with LLMs: Characters and Digital Twins. Role-play prompting (Kong et al., 2024) has been shown to be a more effective trigger for the chain-of-thought process in LLMs. Additionally, efforts to simulate famous personalities (Shao et al., 2023; Sun et al., 2024) have garnered interest, leading to research on maintaining character consistency and studying social interactions within agent-based group chat environments. Recently, Reid Hoffman (Hoffman, 2024) showcased an interview between himself and his digital twin built on GPT-4. Although this demonstration highlighted the potential of digital representations, it was confined to one-on-one interactions, leaving the complexities of group discussions unexplored. Unlike previous work, our work focus on LLMs as meeting participant delegates, delivering targeted engagement tailored to multi-participant, meeting-specific objectives. Our comprehensive evaluation and real-world deployment further demonstrate the system's potential to significantly reduce the burden of meetings on individuals, thereby advancing the application of LLMs in professional environments."}, {"title": "3 LLM-based Meeting Delegate System", "content": "Figure 1 illustrates the architecture of our proposed meeting delegate system, which comprises three main components:\n\u2022 Information Gathering: This component shown on the top-left collects meeting-related information to assist LLMs in participating in meetings. Users can manually provide topics of interest, background knowledge, and shareable materials prior to the meeting. Alternatively, if the user has a personal knowledge base or an intelligent personal assistant/agent, the system can query them in real-time, provided latency is manageable.\n\u2022 Meeting Engagement: The Meeting Engagement module actively monitors the meeting's status and uses LLMs to determine the appropriate timing and content for engagement. Engagement evaluation occurs after each participant's utterance, using in-context learning methods. The prompt for evaluation includes general instructions, user-provided meeting information, and the ongoing meeting context (see Table 14 in the Appendix for details). While various contextual data (e.g., transcript, screen sharing, audio) can be utilized, this work focuses on transcripts obtained via meeting software or speech-to-text tools. Figure 1 shows the three common response types: leading the discussion, responding to others, and chiming in. This work concentrates on the latter two, emphasizing the participant's role.\n\u2022 Voice Generation: After deciding on the content to be spoken, the Voice Generation module shown on the right produces a voice response mimicking the user's voice using text-to-speech (TTS) technology (Qin et al., 2023). To minimize latency, the system employs streaming modes for both LLM API calls and TTS.\nWe implemented a prototype of the above system on a widely-used meeting platform and conducted several demo case studies. Detailed insights and lessons learned from these real-world applications will be presented in Section 6.2. As an illustration, we present a real demo case in Figure 2. In this example, Bob uses his Meeting Delegate to participate in a meeting with Alice and others. Before the meeting, Bob provides topics of interest and relevant shareable information to the Meeting Delegate . This information, along with instructions, forms the prompt for the Meeting Delegate . The delegate then joins the meeting and determines, based on the ongoing meeting transcript, whether to engage . During the meeting, Alice discusses updates on the voice function, which aligns with Bob's goal to learn about its progress. The Meeting Delegate then chimes in , generating a text-based response (converted to speech ), asking for more"}, {"title": "4 Benchmark Dataset", "content": "While the proposed meeting delegate system demonstrates potential in a few sample demonstrations, more systematic and quantitative evaluation in diverse contexts is needed. Our evaluation goals are to determine whether the system can appropriately time its interventions and generate relevant spoken content. No existing benchmark datasets meet these objectives, prompting us to create one."}, {"title": "4.1 Dataset Construction", "content": "Our dataset construction strategy involves using real meeting transcripts and generating test cases by taking \u201csnapshots\u201d from these transcripts. A \u201csnapshot\u201d is defined as a truncation of the transcript after a participant's utterance. Then, by comparing the generated response according to this snapshot with the actual responses in the real script, we can determine how well the system performs. An illustration of this process in given in Figure 8.\nOur base meeting transcripts are taken from the ELITR Minuting Corpus (Nedoluzhko et al., 2022), comprising de-identified project meeting transcripts in English and Czech. 61 English meeting transcripts are used and the test cases are constructed as follows. Motivated by promising results from LLM annotation (Gilardi et al., 2023), we also leverages LLMs for preparing this dataset while conducting manual verification for quality assurance. Specifically, we first employ GPT-4 to progressively analyze each participant's utterances by taking a \u201csliding window\" on the original meeting transcript. This is to capture their meeting intents and the information that they can share during the meeting, serving as the critical input to the Meeting Engagement module for response generation. The shareable meeting information contains pairs of <Context> and <Information>, with <Context> specifying under which context the points in <Information> can be shared. Details of this intent and contextual information extraction prompt can be found in Table 21 in the Appendix.\nNext, we extract suitable snapshots from the transcripts as test cases. For each participant (excluding facilitators), we identify their utterances and use the preceding transcript as the ongoing meeting context. The ground-truth response is determined by considering several subsequent utterances. This"}, {"title": "4.2 Evaluation Metric", "content": "In our evaluation, we generate responses using LLMs with the same prompt as in our prototype. These responses are assessed using two categories of metrics: Response Rate / Silence Rate, which determines whether a response is generated, and quality-related metrics, Recall and Attribution.\nThe Recall metric evaluates if the generated response includes key points present in the ground-truth response. We define two recall rates: \u201cloose\u201d recall rate, which is 1 if at least one main point from the ground-truth is mentioned and 0 otherwise; and \"strict\" recall rate, which measures the percentage of main points from the ground-truth included in the generated response.\nAttribution assesses the origin of the main points in the generated response, classifying them into four categories: the expected ground-truth response"}, {"title": "4.3 Dataset Statistics", "content": "From the 61 original meeting transcripts, we extract 846 test cases for Matched Dataset, in which 54.5% belongs to Implicit Cue, followed by 30.9% for Explicit Cue and 14.7% for Chime In. The numbers of test cases for Mismatched Dataset and Noisy Name Dataset are 294 and 122, respectively. For Matched Dataset, we present various data statistics in Figure 3. Over 50% of test cases involve more than four participants and contain transcripts exceeding 50 utterances, highlighting the dataset's complexity and the involvement of multiple individuals. Additionally, approximately 40% of test cases include at least two main points in the ground-truth response, and in more than 50% of cases, participants contribute over ten main points. This indicates a substantial level of detail and interaction within the meetings, suggesting that the dataset captures rich and multifaceted discussions."}, {"title": "5 Experiment", "content": "Setup. In our experiment, we utilize three prominent series of LLMs: the GPT series (GPT-3.5-Turbo, GPT-4, GPT-40) (OpenAI, 2024c), the Gemini series (Gemini 1.5 Flash, Gemini 1.5 Pro) (Google, 2024b) and the Llama series (Llama3-8B, Llama3-70B) (Meta, 2024). For all LLMs, we set the temperature to 0 and use the default API settings for other parameters. Note that, due to model context window restriction, we remove test cases that exceed the 8K context window for Llama3 models (56.3% kept) and those exceeding the 16K context window for GPT-3.5-Turbo (94.3% kept), while keeping all for the other LLMs.\nResponse Rate Analysis. The Response and Silence Rates of the studied LLMs are obtained for Matched and Mismatched Datasets, respectively. Summarized results are presented in Figure 4, with further details (e.g., breaking down to different meeting scenes) provided in Tables 2 and 4 in the Appendix. Overall, GPT-4 and GPT-40 demonstrated balanced performance, with Response/Silence Rates between 0.7 and 0.8. Among the Gemini series models, Gemini 1.5 Pro achieved the highest Silence Rate of approximately 0.9, coupled with a low Response Rate, indicating a cautious engagement strategy. In contrast, the smaller Gemini 1.5 Flash model and the Llama series exhibited"}, {"title": "6", "content": "higher activity levels, suggesting a more proactive engagement approach; however, this also led to a tendency to engage when they should remain silent. These patterns persisted when all LLMs are tested using the same subset of cases as the Llama series.\nTo uncover the underlying causes of failures, we conduct an in-depth analysis of all failure cases in representative models: GPT-40 and Gemini 1.5 Pro for state-of-the-art LLMs, and Gemini 1.5 Flash and Llama3-8B representing more lightweight models. We manually analyze and categorize all error types, proposing corresponding directions for improvement. For instance, in the \"Explicit Cue\" scenario within the Matched Dataset, the meeting delegate may correctly identify the cue but fail to respond, indicating a need for enhanced reasoning capabilities in meeting contexts. Detailed analysis can be found in Table 12 and Figure 9 in Appendix. A summary of these results is presented in Figure 5. Our findings reveal that: 1) LLMs like GPT-40 and Gemini 1.5 Pro can improve performance or make functional advancements in meeting scenarios by enhancing reasoning in meeting-specific context, and 2) smaller models need to improve general instruction following and reasoning abilities before addressing meeting-specific issues.\nRecall Analysis. The recall results for both loose and strict metrics are similar; therefore, we only present the loose recall rate for all studied LLMs on Matched Dataset in Figure 6. Detailed results, including the strict recall rate, are available in Table 6 in the Appendix. Figure 6 shows that these LLMs achieve a loose recall rate of approximately 60%. This indicates that, for 60% of test cases, the generated response contains at least one key point from the ground-truth response. Such a result is promising, as it suggests that LLM-powered meeting delegates can typically respond with reasonable content, maintaining the overall meeting flow.\nPerformance differences among the LLMs reveal that GPT-40 achieves the highest performance across almost all categories, followed by GPT-4. The two Gemini models exhibit similar performance, excelling in \u201cExplicit Cue\" but lagging in \"Chime In\". The Llama series models perform comparably to the Gemini models but tend to be better in \"Chime In\" scenarios."}, {"title": "6 Discussion", "content": "6.1 Phased Deployment of Meeting Delegate\nThis study primarily explores the feasibility of using LLMs to represent users by generating meaningful content in meeting scenarios. However, deploying such a meeting delegate system in real-world settings requires addressing additional critical factors, such as responsible AI practices and ethical considerations (see further discussion in the Ethics Statement section). Key challenges include implementing strong privacy safeguards, such as secure data handling, consent mechanisms, user-defined boundaries, and audit trails. A review (Yan et al., 2024; Anwar et al., 2024) of current privacy-preserving methods for LLMs highlights the difficulty of creating a fully autonomous and unconstrained meeting delegate at present. Therefore, we propose a three-phase approach that incrementally enhances the AI's autonomy and responsibility, as detailed in Table 1. The phases are characterized by the evolution of data boundaries and limitations on the meeting delegate's roles in sharing information, collecting data, and making decisions.\nIn Phase I (Execute), the delegate operates strictly within user-defined data boundaries, sharing only explicitly approved information and collecting information from other meeting participants based on direct user instructions. There is no autonomous decision-making allowed, ensuring strong user control and minimal privacy risk. In Phase II (Assist), the system can reason over sensitive data while adhering to privacy guidelines. It infers context beyond explicit instructions and can propose actions, though user approval is still required for making decisions. This phase introduces controlled autonomy with dynamic data boundary management. In Phase III (Delegate), the delegate fully autonomously collects and shares information, making real-time decisions based on user-defined goals and preferences. Privacy filters, decision-making models, and audit logs ensure transparency"}, {"title": "6.2 Application in Practice", "content": "Our current implementation of the meeting delegate system indeed corresponds to Phase I, consistent with available technologies. To assess its practical performance, the system was tested in several demo scenarios. For example, as shown in Figure 2, we simulated a demo scenario of a daily project update scrum with three human participants and one LLM-powered delegate. All participants were aware of the delegate's presence and located in the same room. One participant acted as the moderator, while the others, including the delegate, provided project updates. Each human participant followed a script, requesting information from the delegate, which was preloaded with project-related topics via the Information Gathering module. The moderator guided the meeting, with responses cued or initiated by the participants. The demo lasted five minutes and was repeated to assess the delegate's consistency using different LLMs.\nWe evaluated three models: GPT-3.5-Turbo, GPT-4, and GPT-40. GPT-3.5-Turbo underperformed, proving inadequate for meeting delegation tasks, even at Phase I. GPT-4 and GPT-40 generally delivered relevant responses but occasionally repeated information from earlier transcripts. Response latency was another issue, with the fastest model, GPT-4o, taking ~5 seconds to respond.\nTo address issues of irrelevant and repetitive responses, future improvements may include utilizing advanced general LLMs or fine-tuning smaller models. Benchmark results indicate that the Llama3-8B model exhibits satisfactory base performance, and fine-tuning smaller models could potentially reduce latency. For instance, a recent implementation of Llama3-8B achieved a 500 ms latency in real-time communication (Cerebrium, 2024). Other improvements, such as incorporating windowed context management, advanced summarization techniques, or adopting multi-modal language models with direct speech input and output capabilities (OpenAI, 2024b), have the potential to not only further reduce latency but also maintain or improve performance. For example, the added information from speech, such as speed and tone, could lead to enhanced system performance."}, {"title": "7 Conclusion", "content": "This study introduces and evaluates an LLM-powered meeting delegate system designed to address contemporary challenges in collaborative work environments. By focusing on participant roles rather than facilitators, our prototype and comprehensive benchmark highlight the potential of LLMs to enhance meeting efficiency. Through real-world testing and rigorous assessment, we demonstrate varying performance levels among LLMs, with notable strengths and areas for improvement. Challenges include managing transcription errors and reducing irrelevant or repetitive responses. Future work will need to address these challenges and enhance the real-time responsiveness and privacy safeguards of such systems to fully realize their potential in collaborative work environments."}, {"title": "Limitations", "content": "We acknowledge several limitations in our study. First, the evaluation is restricted to a set of representative language models. While this provides valuable insights, future work should explore a broader range of LLMs, particularly models specifically fine-tuned for meeting-related tasks. Additionally, recent advancements such as OpenAI's Realtime API (OpenAI, 2024b), which supports direct voice input and output, could significantly enhance the relevance of our findings in multimodal contexts.\nSecond, our benchmark is largely based on limited experimental conditions. Future evaluations should incorporate more diverse and dynamic environments to provide a more comprehensive understanding of our system's capabilities.\nLastly, while our system shows promise in facilitating meeting participation, it represents an initial exploration of the possibility of using LLMs as meeting delegates. Specifically, it does not extensively address other key dimensions such as privacy, security, or user trust. In the following section, we share an initial discussion on responsible AI and ethics consideration to outline potential directions for further investigation."}, {"title": "Ethics Statement", "content": "This paper explores the potential use of LLMs as meeting delegates, raising several ethical considerations. We propose a phased approach to AI autonomy, starting with limited decision-making in earlier phases and building toward greater capabilities with accountability measures. Privacy-by-design principles should be central to the system's architecture, and educating users about the AI's limitations will ensure responsible use. Below, we outline key ethical dimensions (Bender et al., 2021; Kasneci et al., 2023; Wang et al., 2024; Kirk et al., 2024), including bias, privacy, transparency, human agency, security, and socio-economic impact, alongside suggested safeguards.\nBias and Fairness: LLMs may generate biased or inappropriate content, potentially affecting fairness in meeting outcomes. This risk requires bias detection and mitigation strategies, such as training on diverse datasets, bias audits, and user feedback loops. Fine-tuning models for meeting scenarios and ongoing bias monitoring could be crucial for ensuring fairness.\nPrivacy: Personalization is only possible by collecting user data. This applies to any technology that relies on personal information to deliver tailored benefits. The personalization of meeting delegates relies on sensitive user data, which risks over-sharing or misusing private information. To address this, we advocate for privacy-enhancing technologies like encryption and differential privacy, as well as user-defined data boundaries. Real-time voice capabilities also heighten the risk of identity misuse, necessitating strict privacy controls to ensure compliance with data protection standards.\nTransparency: Transparency is essential for responsible deployment. All participants must be informed when an AI is acting as a delegate. Clearly stating the AI's capabilities and limitations helps manage expectations, and audit logs should be available for users to track AI actions and decisions during meetings.\nHuman Agency: LLM-based delegates should support, not replace, human decision-making. In the early phases, the AI assists users without autonomy, and even in later phase like Phase III, human oversight must remain integral. Human-in-the-loop HITL systems are crucial for maintaining control and ensuring users can intervene as needed.\nSecurity and Fraud Risks: Unauthorized access to a meeting delegate could lead to fraud or impersonation. Security measures like multi-factor authentication, identity verification, and anomaly detection are essential. Federated learning could further protect sensitive data by minimizing centralized storage risks.\nEthical Governance and Mitigation: Ethical governance frameworks, including guidelines, audits, and interdisciplinary collaboration, must guide the system's development. User consent should be obtained at key stages, and continuous monitoring is essential to identify and address unintended consequences.\nSocio-Economic Impact: Automating meeting participation could lead to job displacement in roles that rely on meeting facilitation. While this risk is limited by current technology, future developments may amplify these concerns. It's essential to focus on augmenting human labor rather than replacing."}]}