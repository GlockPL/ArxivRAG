{"title": "ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise and Compute Resources", "authors": ["Jason Wu", "Kang Yang", "Lance Kaplan", "Mani Srivastava"], "abstract": "Multimodal deep learning systems are deployed in dynamic scenarios due to the robustness afforded by multiple sensing modalities. Nevertheless, they struggle with varying compute resource availability (due to multi-tenancy, device heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed corruption, environmental noise, etc.). Current multimodal systems employ static resource provisioning and cannot easily adapt when compute resources change over time. Additionally, their reliance on processing sensor data with fixed feature extractors is ill-equipped to handle variations in modality quality. Consequently, uninformative modalities, such as those with high noise, needlessly consume resources better allocated towards other modalities. We propose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of tackling both challenges - it adjusts the total number of active layers across all modalities to meet compute resource constraints, and continually reallocates layers across input modalities according to their modality quality. Our evaluations showcase ADMN can match the accuracy of state-of-the-art networks while reducing up to 75% of their floating-point operations.", "sections": [{"title": "1. Introduction", "content": "Background: Multimodal deep learning systems fusing sensory data from various modalities are the standard for accurate, robust sensing (Chen et al., 2022; Eitel et al., 2015). The robustness of multimodality arises from redundant information captured across modalities, which mitigates the effect of sensor failure, noise corruption, and adverse environmental conditions (Lin & Hu, 2023; Liu et al., 2022). Accordingly, these multimodal systems are invaluable in highly dynamic environments, where a given input modality's quality-of-information (QoI) can vary drastically across samples. Qol refers to the information content of the sensor data, where noise-corrupted modalities would be classified as low-QoI. Fluctuations in a modality's QoI can occur over long periods of time (e.g., lighting conditions over the day), or rapidly (e.g., battlefield settings or unstable sensor feeds).\nChallenges: Although multimodal robustness allows these deep learning systems to deal with highly variable QoI, the first challenge surrounds efficiency of such systems. State-of-the-art multimodal networks employ static provisioning in which inputs proceed through a fixed computational graph established by the architecture (Wang et al., 2024; Yin et al., 2024). Consequently, each modality's data is fully processed by the network with no regard to variable input QoI, and valuable compute resources may be wasted on low-QoI modalities. In particular, systems with considerable energy or latency constraints will suffer greatly from this misallocation. We hypothesize that flexibly allocating computational resources among modalities in accordance to each modality's Qol on a per-sample basis can greatly boost model performance in compute-limited settings.\nIn addition to its inability to adapt to the varying Qol of the input modalities, static provisioning also struggles with the second challenge of dynamic compute resources. The highly dynamic, real-world environments in which multimodal systems are particularly relevant tend to also suffer from variable computing resource availability over time. For instance, the deployment platforms can be affected by thermal throttling, energy fluctuations, or multi-tenancy. Unfortunately, statically provisioned models are unable to adjust their resource usage to meet dynamic compute resource constraints. A naive solution uses model selection in which several models of different sizes are trained and deployed for various levels of compute resource availability. Aside from the obvious drawback of requiring an unreasonable amount of training resources, it also complicates the standard practice of initializing multimodal networks with pretrained weights prior to finetuning (Manzoor et al., 2023). Publicly available pretrained weights only exist for a few configurations of model size, with fewer options for uncommon sensing modalities. We hypothesize that a single network, initialized with pretrained weights, which can dynamically adjust its resource usage, offers an effective solution to the challenge of fluctuating compute resources.\nProposed Solution: We propose ADMN, an Adaptive Depth Multimodal Network jointly tackling the challenges of adaptation to both dynamic compute resources and variable QoI inputs. While these challenges are agnostic to the multimodal fusion method (e.g., data-level (Kim et al., 2021), embedding-level (Jeong et al., 2024), and late (Samplawski et al., 2023)), we focus specifically on embedding-level fusion due to applicability and ease of implementation. Figure 1 provides a high-level depiction of ADMN. Following the standard for embedding-level fusion, each modality is processed with an independent backbone before undergoing fusion with a transformer encoder.\nFirst, ADMN addresses the challenge of dynamic compute resources with adaptive backbones containing adjustable layer configurations. With the same set of model weights, ADMN activates a subset of backbone layers according to the available compute resources. We accomplish this by training large backbones initialized with pretrained weights while stochastically dropping layers in every modality's backbone through the LayerDrop technique (Fan et al., 2019). We extend the unimodal, text-only LayerDrop technique to not only Vision Transformers, but also multimodal networks. Such a strategy produces a novel multimodal network whose backbones are resilient to missing layers at test-time, allowing for the usage of fewer layers during resource scarcity.\nSecond, given a total layer budget established by the available resources, ADMN addresses the challenge of dynamic Qol by adapting the choice of selected backbone layers according to each modality's QoI. Our approach trains a multimodal controller on top of the adaptive backbones. The controller learns an optimal layer allocation among the backbones, conditioned on the relative QoI across the modalities of a given input sample. We propose a simple training procedure leveraging Gumbel-Softmax Sampling (Maddison et al., 2016) and the straight-through estimator (Bengio et al., 2013) to enable end-to-end training without costly reinforcement learning.\nWe benchmark ADMN against several baselines to demonstrate its ability to preserve accuracy while simultaneously minimizing energy and latency costs. ADMN is tested on both multimodal localization and multimodal action recognition tasks, reinforcing its generality and applicability. ADMN can match the performance of larger state-of-the-art models while reducing FLOPS by up to 75% and latency by up to 60%. We release our code at https://anonymous. 4open.science/r/ADMN-15C9/. Our contributions are summarized as below:\n\u2022 We present an adaptive multimodal network where resource allocation among modality backbones is dictated by QoI characteristics and current computational resource availability at inference time for every sample.\n\u2022 We adapt the LayerDrop technique from its original domain of text transformers to multimodal visual networks by introducing full-modality dropout, and demonstrate that this technique can be generalized to diverse tasks such as localization and classification.\n\u2022 We design a multimodal controller that can be trained from end-to-end with a simple method to propagate gradients to the controller.\n\u2022 Experiments on multimodal localization and human activity recognition tasks across different modalities demonstrate ADMN's efficiency."}, {"title": "2. Related Work", "content": "Early Exiting in Unimodal Networks. Early Exiting has been explored extensively in unimodal networks to improve inference efficiency (Xin et al., 2020; Zhou et al., 2020; Meng et al., 2022). Methods like DeeBERT (Xin et al., 2020) and PABEE (Zhou et al., 2020) use confidence thresholds to halt computation for simpler inputs. However, these unimodal approaches fail to consider multimodal challenges such as Qol-aware resource allocation. Moreover, Early Exiting is incompatible with redistributing resources away from low-QoI modalities, as these low-confidence samples propagate through the entire network.\nDynamic Inference for Multimodal Systems. Multimodal networks traditionally rely on input-agnostic static provisioning, resulting in inefficiencies when modality QoI varies, and also incompatibility with dynamic computational resource availability. To enhance computational efficiency, dynamic networks have been proposed (Xue & Marculescu, 2023; Panda et al., 2021; Gao et al., 2020; Cai et al., 2024; Mullapudi et al., 2018). DynMM (Xue & Marculescu, 2023) trains a set of expert networks representing different modality combinations, processing simple inputs with a subset of available modalities. AdaMML (Panda et al., 2021) and Listen to Look (Gao et al., 2020) improve inference efficiency by leveraging multimodal information to eliminate temporal redundancy in videos. ACF (Cai et al., 2024)"}, {"title": "3. Methodology", "content": "3.1. Problem Description\nA typical multimodal system employing embedding-level fusion is illustrated on the left side of Figure 2. It first utilizes independent, modality-specific backbones to extract informative embeddings for each modality. These embeddings are fused via self-attention in a Transformer encoder, which condenses all modality embeddings into a single joint embedding. Finally, an output head converts the dense joint embedding into the desired final output specific to the target task. In this work, ADMN accomplishes two objectives. First, it contains dynamic backbones robust to various dropped layers, thus enabling adaptation to different total layer budgets. Second, it allocates the layer budget optimally among modality backbones (i.e., transformer layers) according to the fluctuating input QoI, greatly outperforming static models with the same layer budget.\n3.2. ADMN Architecture\nFigure 2 showcases the task-agnostic architecture of ADMN, which involves a two-stage training process.\nStage 1: LayerDrop Finetuning. We initialize each backbone with a set of weights pretrained with LayerDrop (Fan et al., 2019). Subsequently, the multimodal network is finetuned with LayerDrop on the desired task while freezing the majority of earlier backbone layers (shown in blue) to prevent overfitting. Stage 1's objective is to adjust the learnable weights for the specific target task, while also training the Fusion and Output layers to accept diverse embeddings arising from various backbone layer configurations.\nStage 2: Controller Training. We freeze all the Stage 1 network weights and train a controller that learns to allocate a fixed budget of L layers. The controller network accepts the multimodal inputs, and from the relative QoI, outputs a discrete sequence summing to L outlining the selection of backbone layers. Based on the task loss, the controller will readjust the layer allocation across the modalities.\n3.3. Stage 1: LayerDrop Finetuning\nWe adapt the LayerDrop (Fan et al., 2019) work, originally designed for text input, to support non-text multimodal input, while also showcasing how LayerDrop can be integrated into modality pretraining.\n3.3.1. VANILLA LAYERDROP\nLayerDrop trained text transformers with layer-wise dropout, enabling on-demand depth reduction during test-time. During training, each layer of the transformer is dropped out with a certain probability, thereby forcing the network to function with only a subset of its layers. At inference time, they proposed a dropout strategy alternating layers starting from the middle to meet a particular compute requirement, referred to as the \"every-other\" strategy. We refer readers to the original work for greater detail.\n3.3.2. MULTIMODAL LAYERDROP\nWe extend LayerDrop to Vision Transformers (ViTs) (Dosovitskiy, 2020) by first integrating them into ImageNet-1K (Russakovsky et al., 2015) pretraining. Rather than performing supervised training on ImageNet, which suffers heavily from convergence issues, we employ Masked Autoencoder (MAE) pretraining (He et al., 2022). We use a LayerDrop rate of 0.2 in each ViT layer and do not drop any layers in the decoder. By employing LayerDrop in MAE pretraining, the model learns to reason about the image in the presence of missing layers. Then, the MAE pretrained weights are loaded into each of the ViT backbones of a multimodal (e.g., vision-depth) neural network to be finetuned on a downstream task. The majority of the backbone layers are frozen during finetuning, with the last few layers left tunable to adjust to the new task. A LayerDrop rate of 0.2 is maintained during the finetuning process in all the backbones, allowing the remaining learnable layers to adapt to the countless different combinations of missing layers. This process ultimately creates a task-specific multimodal network containing several ViT backbones with adaptable layer configurations at inference time.\nFull-Backbone LayerDrop: One unique challenge that surfaces when applying LayerDrop in a multimodal context is the need to subject a backbone to extreme dropout conditions. In the unimodal case, dropping all layers of a backbone is avoided because it would leave the input unprocessed. For a multimodal network, however, one may wish to drop all layers of a modality that suffers from extreme noise. The typical LayerDrop training ratio of 0.2 in a 12 layer ViT is unlikely to drop out all layers of a given modality, resulting in unpredictable behavior when all layers are missing at test-time. Increasing the dropout rate is an option, but can hurt the full-layer performance. To remedy this, we employ full-backbone dropout during training time, under which there is a 10% chance all the layers of a given modality's backbone will be dropped out independently of the 0.2 LayerDrop rate. Through this procedure, the fusion network can observe embeddings that emerge when one backbone's layers are entirely bypassed, preventing them from potentially disrupting the self-attention process.\n3.4. Stage 2: Controller Training\nThe controller decides which backbone layers to activate within the frozen Stage 1 network depending on the relative input modality QoI, as shown in Figure 2. For instance, in a multimodal image and depth network with a layer budget L, the controller may choose to allocate all L layers to the image backbone if it detects that depth is severely corrupted, and vice versa. The controller performs this adaptation on a per-sample basis. In order to maximize the resources provided to the main network, the controller should also remain as lightweight as possible.\n3.4.1. CONTROLLER ARCHITECTURE\nFigure 3 reveals the structure of the controller. First, we downsample the multimodal inputs to 100 \u00d7 100 and pass them through a series of modality specific lightweight convolutional networks. The goal of these convolutional networks is to produce embeddings containing information solely regarding each modality's QoI, which can be accomplished from low-resolution data. With M input modalities, the convolutional networks produce M total noise embeddings, which are subsequently fused by a transformer encoder. The transformer encoder outputs a single embedding $e_{noise}$ containing the noise signature of every input modality. From this joint noise embedding, we can obtain a set of raw logits\n$\\pi= MLP(e_{noise}) where \\pi\\in R^C, C = \\sum_{i=0}^M b_i$ (1)\nand $b_i$ is the number of layers in the backbone of modality $m_i$. In essence, $\\pi$ represents an allocation of L available layers among C total backbone layers, with values dependent on the noise characteristics of every input sample.\nIdeally, the convolutional layers will automatically learn to extract each modality's noise signature. Failure to identify the modality QoI will result in high loss arising from improper layer allocation, which will be backpropagated to the convolution. In practice, however, we found that adding supervision by providing ground truth noise information greatly aided convergence. Thus, we introduce an additional MLP that predicts the noise standard deviation $\\hat{\\sigma}_{m_i}$ of every modality $m_i$ from $e_{noise}$. We optimize over the joint loss $L_{total} = L_{model} + L_{noise}$, where $L_{noise} = \\sum_{i=0}^M |\\hat{\\sigma}_{m_i} - \\sigma_{m_i}|$\n3.4.2. DIFFERENTIABLE LAYER SELECTION\nAlthough the logits $\\pi$ provide information regarding which layers to select, activating a given layer is a discrete, binary decision. One simple method is to perform top-L masking where the top-L largest logits are discretized to one and the rest are set to zero. The output of top-L masking is multiplied with the output of each backbone layer, zeroing out the contribution of dropped layers. While necessary for layer selection, the discretization process interrupts the flow of gradients during backpropagation. Consequently, the gradients from the frozen main network carrying vital information on the success of a layer configuration will not propagate to the learnable parameters of the controller.\nTraditional Unimodal Discretization: Gumbel-Softmax Sampling (Maddison et al., 2016) propagates gradients over discretization by approximating a categorical distribution while retaining differentiability, where\n$G = -log(-log(u)) ; u \\sim U(0, 1)$ (2)\ndefines the Gumbel distribution, and\n$y_i = \\frac{exp((log(\\pi_i) + g_i)/T)}{\\sum_{j=1}^C exp((log(\\pi_j) + g_j)/T)}$ (3)\nwhere y is the result of Gumbel-Softmax Sampling with i.i.d samples $g_0...g_C \\sim G$. As the value of temperature T decreases, y approximates a categorical distribution with a single $y_i = 1$ and the remaining $y_j = 0; j = 0...C, j \\neq i$.\nUnimodal Early-Exit methods rely upon Gumbel-Softmax Sampling (Meng et al., 2022). They employ decision networks at every layer of the model, outputting a single logit representing the likelihood of executing that particular layer, which then undergoes Gumbel-Softmax sampling.\nMultimodal Discretization: Unfortunately, simply applying Gumbel-Softmax Sampling is insufficient for ADMN. The key difference between ADMN and unimodal Early-Exit works is that ADMN predicts the entire layer configuration at the beginning of model execution, rather than forming a decision at every layer. ADMN must decide the layer allocation by selecting L total layers based off characteristics from all modalities, and utilizing decision networks at every layer of the unimodal backbones would omit crucial multimodal information. Given this, emulating discretization solely through low-temperature Gumbel-Softmax sampling is a poor choice, as it approximates a categorical distribution in which only a single layer is selected.\nWe devise a method for retaining gradients over top-L sampling. Initially, we perform standard Gumbel-Softmax sampling with a temperature of 1 instead of a low temperature. This allows multiple high-value logits to be represented in the resulting probability distribution, better accommodating the selection of L layers. This is accomplished at the expense of the highly desirable near-discrete behavior of the low-temperature Gumbel-Softmax. We introduce a subsequent top-L discretization stage to activate only L layers, and maintain gradient flow through the straight-through estimator (Bengio et al., 2013). Represented by $y + (discretize(y) - y).detach()$, the downstream layers receive the discretized value, while the gradients received are copied over to the continuous logits, allowing for gradient flow. Intuitively, standard Gumbel-Softmax sampling stochastically provides a probability distribution across layers, after which we sample the L-most likely layers via discretization, and copy the gradients across the discretization step. We provide justification in Appendix A.2."}, {"title": "4. Implementation Details", "content": "ADMN Application Domain: To showcase the generality of ADMN, we evaluate it on two highly diverse tasks - regression and classification. We select distributed multimodal localization as the regression task and multimodal human activity recognition for classification. For the task of distributed multimodal localization, we follow the approach in (Jeong et al., 2024). Assuming S sensor nodes each containing M modalities, we define M modality-specific backbones that process each node's data, resulting in S \u00d7 M backbone embeddings. The S \u00d7 M embeddings are provided as input tokens into the transformer encoder for fusion, and the output head converts the fused embedding into a prediction of target location. For multimodal human activity recognition, we employ the popular space-time encoder architecture (Woo et al., 2023). Given F frames of an activity from M modalities, we define M backbones that process each frame of data, resulting in F \u00d7 M embeddings. Once again, a stack of transformer encoders fuses the embeddings, and the output head predicts the activity class. In this work, we restrict the majority of our evaluations to image-like data (e.g., RGB, depth) that can utilize the same set of MAE ImageNet initializations. However, we hope that our work inspires future models to be pretrained with LayerDrop, removing the heavy pretraining burden and allowing the user to proceed directly to Stage 1 Training.\nMAE Pretraining: We pretrain Vision Transformers on the ImageNet-1K dataset with the Masked Autoencoders method. Additionally, we employ a LayerDrop probability of 0.2 in each layer of the encoder. We follow the standard pretraining process for a ViT-Base model with 12 layers and dimension 768. We refer to the original paper (He et al., 2022) for further implementation details.\nTraining Details: To accomplish the goal of adjusting to variable modality QoI, ADMN must understand how to process noisy multimodal input. In addition to employing LayerDrop, we also add varying amounts of noise to the multimodal inputs during Stage 1 Training. We simulate dynamic QoI by adding Gaussian Noise of different standard deviations to each modality, allowing the model to gain robustness to noisy inputs during finetuning. For each modality $m_i$ in a batch of input samples, we will draw $\\sigma_{m_i} \\sim [0, \\sigma_{imax}]$, and then add zero mean Gaussian Noise with standard deviation $\\sigma_{\\eta}; (i.e., N(0, \\sigma_{m_i}))$ to modality $m_i$. This ensures the controller can properly perform QoI-aware layer allocation during Stage 2 Training. Further details and hyperparameter settings are provided in Appendix A.3."}, {"title": "5. Evaluations", "content": "5.1. Experimental Setup\n5.1.1. DATASETS\nThe GDTM localization dataset (Jeong et al., 2024) contains multimodal data (RGB, depth, mmWave radar, multichannel audio) of a small remote-controlled car driving on an indoor track. The dataset is also distributed, containing 3 sensor nodes each with a full set of modalities. We specifically leverage the distributed vision and depth data to localize the car. For human activity recognition, we use the multimodal MM-Fi (Yang et al., 2024) dataset containing 40 subjects with 27 total activities captured by RGB cameras, depth cameras, mmWave radar, and WiFi. We focus on the visual modalities (RGB, depth) for classification. These datasets do not naturally contain varying modality QoI, so we synthetically add Gaussian Noise to each multimodal input and clip the values between 0 and 1 to simulate different QoI. Although we focus specifically on Gaussian Noise due to its simplicity and irreversible nature, we emphasize that ADMN's design is generalizable and not designed specifically for Gaussian Noise. We define three categories of Gaussian Noise employed during Stage 2 Training: binary, discrete, and continuous.\nBinary: Every modality has an 50% likelihood of either suffering extreme noise corruption, or being left unmodified. This is represented by adding either $N(0, 0)$ or $N(0, \\sigma_{imax})$ Gaussian Noise to a given modality, generating various combinations of noisy and clean modalities. The real-world analog involves flickering sensor feeds alternating between clean and severely degraded transmissions.\nDiscrete: We extend the binary case to a larger set of standard deviations encompassing noisy, yet still informative data. We add $N(0, \\sigma_{i_j})$ to every modality $i$'s input data. Each modality defines a finite set of $N_i$ standard deviations $\\{\\sigma_{i_1}, \\sigma_{i_2}, \\sigma_{i_3}...\\sigma_{i_{N_i}} \\}$ from which $\\sigma_{i_j}$ is drawn. This setting is representative of systems deployed in environments with a finite set of conditions (e.g., indoor lighting 25%, 50%, 100%), or even sensors that contain different settings such as ISO levels in cameras.\nContinuous: Instead of drawing $\\sigma_{i_j}$ from a finite set, we instead draw $\\sigma_{i_j}$ from a continuous range $[0, \\sigma_{imax}]$ defining the allowable range of standard deviations. The continuous case represents highly dynamic noise settings that are difficult to decompose into a finite set of scenarios.\n5.1.2. BASELINES\nWe present the following baselines:\nUpper Bound: We do not drop any layers, allocating the full 12 layers to each backbone. This represents the best performance with maximum layer budget.\nNaive Allocation: This baseline represents input-agnostic allocation. Given a layer budget L and M modalities, we naively allocate L layers to each backbone following every-other allocation.\nImage Only: All L layers are allocated to the Image modality, valid only for L < 12\nDepth Only: All L layers are allocated to the Depth modality, valid only for L < 12.\nNaive Scratch: We train a network from scratch without LayerDrop on the downstream task with each backbone containing L layers. As this model is statically provisioned, we train a new \"Naive Scratch\" model for every layer budget L. Although impractical due to the high training cost, it nonetheless serves a useful comparison.\n5.1.3. METRICS\nFor the localization task, we use the average localization error (measured in cm) as the evaluation metric. For the classification task, we evaluate performance using classification accuracy, a standard metric that measures the proportion of correctly predicted labels. In addition, we calculate the FLOPs (Floating Point Operations) of each network to assess computational efficiency, highlighting the trade-off between performance and resource consumption."}, {"title": "5.2. Localization Results", "content": "Localization Error: We compare ADMN's localization error to every baseline tested on the three noise categories in Table 1, shown in dark green. We observe the greatest boost in performance in the Binary case, where the model achieves localization performance competitive with the Upper Bound with only 6 layers. This demonstrates that the controller has correctly allocated resources towards high QoI modalities in every sample. In comparison, the high error of the baselines reveals how input-aware layer allocation is invaluable in low-compute settings. The Discrete case, which is an extension of the Binary case with various levels of noise, also benefits from introducing ADMN. The 6 layer allocation incurs higher error, but the error sharply drops as we move to 8 layers. Interestingly, the Naive Scratch baseline outperforms ADMN in the 6 and 8 layer Discrete case, but fails to maintain its advantage at larger layer allocations. We hypothesize that the Naive Scratch models are more competitive at smaller layer budgets as proper initialization with pretrained weights has a greater impact in deeper networks. We observe similar trends in the Continuous case.\nLocalization Compute: Figure 4 shows the meaningful reductions in floating-point operations and latency when employing ADMN. Although the controller accounts for a significant proportion of the latency at smaller layer budgets (~20%), the high throughput at these latencies (> 150 fps) surpasses most sensor sampling rates. Moreover, ADMN's controller utilizes a negligible amount of FLOPs. The controller consumes only 0.26 GFLOPs, which constitutes about 1% of the model's total operations at the fewest allocation of 6 layers. When viewing Table 1 in context of these metrics, we can observe the significance of ADMN. In the Binary case, ADMN localizes within 4 cm of the Upper Bound while reducing latency by ~60% and FLOPs by ~75%. With the more complex Discrete case, employing an 8 layer allocation incurs 8 cm of additional error, but cuts latency and FLOPs by ~50% and ~65%, respectively. Even in the difficult Continuous case, using a 12 layer allocation reduces latency and FLOPs by 35% and 50%, respectively, with only 5 cm of additional error."}, {"title": "5.3. Classification Results", "content": "Classification Accuracy: ADMN's classification accuracy on the MM-Fi dataset is shown in dark blue in Table 1. Similarly to its localization performance, ADMN exhibits significant gains in the Binary noise case over the baselines, outperforming the second best Depth-only baseline by 28% with a 6 layer budget. We observe similar trends in the Discrete and Continuous cases. Notably, one key difference in classification is the poor performance of the Naive Scratch model. Regardless of the number of layers, training the model purely from scratch did not converge, highlighting the importance of prior weight initialization. With ADMN, one can initialize a full-size model with pretrained weights, enabling easier convergence, and then simply trim the model size by disabling layers.\nClassification Compute: We observe in Figure 5 that the total GFLOPs are once again dominated by the main network, with the controller accounting for only 0.2% of the total GFLOPs at the smallest configuration of 6 layers, and 0.07% with 16 layers. The controller constitutes a larger proportion of total latency, but the accuracy gain of QoI-aware allocation far outweighs the small latency increase."}, {"title": "5.4. Additional Modalities", "content": "To ensure that ADMN can generalize beyond two vision-based modalities, we incorporate mmWave radar and perform localization on GDTM with RGB, depth, and mmWave modalities. Figure 6 shows a CDF of the error when tested on the 12-Layer Binary case, demonstrating that ADMN retains good performance with three modalities that contain significant heterogeneity."}, {"title": "5.5. Dynamic Compute Constraints", "content": "One drawback is that the ADMN controller is trained only for one particular layer budget L. Consequently, we must employ multiple controllers to adapt to dynamic test-time compute requirements induced by factors such as thermal throttling. Nevertheless, the training overhead for the controllers is low (Appendix A.1), enabling a controller to be trained for every layer budget. As the main network is frozen during controller training, all the controllers are compatible with the same set of main network weights. Additionally, the controller constitutes only 2.3% and 3.2% of the total network parameters in the localization and classification tasks, respectively, allowing for easy storage on disk. As a result, ADMN is highly capable of tackling fluctuating compute requirements. Imagine a scenario where due to thermal throttling, 25% of execution time lies in the 6 layer regime while the rest allows for 16 layers. Assuming that the system must function at all times, a statically provisioned model has two choices - use a six layer model at all times, or separately train and store a 6 layer model and 16 layer model. The first scenario sacrifices accuracy, while the second scenario not only requires significant investment in training time, but also must store two separate, large models in memory."}, {"title": "5.6. Ablation Study", "content": "Efficacy of LayerDrop: LayerDrop is integrated into two stages - initial MAE pretraining on ImageNet, and subsequent finetuning on the desired task. Figure 7 showcases the localization error on the GDTM dataset as the number of dropped layers increases in separate unimodal depth and image networks. We observe that adding LayerDrop during finetuning has the greatest impact, but benefit is observed in both cases, allowing us to drop a meaningful amount of layers with negligible degradation. These results confirm that LayerDrop is compatible with the ViT architecture, MAE pretraining, and is also effective when finetuned on another task. We present additional results in Appendix A.4."}, {"title": "6. Discussion", "content": "Batched Inference: In ADMN, the layer allocation is decided by the controller prior to backbone execution, enabling support for batched inference. Classically, adaptive models cannot support batched inference as different samples will proceed through different layers. However, since ADMN decides the layer allocation at the controller, we can group samples into sub-batches based on similar layer allocation. For instance, samples with high depth noise and low image noise will activate a similar set of layers, allowing them to be grouped together for batched execution.\nFusion with Early Exit: While ADMN and unimodal Early-Exit methods tackle fundamentally different problems, the two techniques can be combined for further computation efficiency. ADMN's controller always allocates L layers across all the modalities. However, on simple inputs, all L layers may not be necessary, allowing for Early-Exit techniques to be integrated for further performance gains."}, {"title": "7. Conclusion", "content": "This paper proposes ADMN, a multimodal network capable of dynamically adjusting the number of active Transformer layers across modalities according to the quality of each sample's input modalities. Through this continuous reallocation, ADMN can match the accuracy of far larger networks while utilizing a fraction of their operations. Additionally, the dynamic backbones of ADMN are also well suited for scenarios with adaptive compute, ranging from heterogeneous deployment devices to fluctuating energy availability. We demonstrate the superiority of ADMN compared to other baselines across both classification and localization tasks."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Appendix", "content": "A.1. Controller Training Overhead\nWe found it sufficient to train the controller for 10 and 15 epochs on the localization and classification tasks, respectively. We attribute this to the simple end-to-end training recipe in which we avoid complex reinforcement learning. On a Nvidia RTX 4090, training the localization controller took only 27 min.\nA.2. Justification of Gradient Propagation Technique in the Controller\nDirectly Employing the Straight-Through Estimator: ADMN utilizes the combination of standard temperature Gumbel-Softmax sampling and the straight-through estimator to propagate gradients over the discretization to the continuous logits. One natural question is whether Gumbel-Softmax Sampling is necessary, as one could theoretically apply discretization on the raw logits and propagate gradients with the straight-through estimator. In Table 2 and Table 3, we present the localization results on the GTDM dataset across different layer configurations and noise categories, with three seeds for each experiment. The results highlight that Gumbel-Softmax sampling plays an important role in model training.\nThis behavior can be attributed to several reasons. First, by applying the softmax function to the logits, we convert them into probability values where one logit's high probabilities come at the expense of the others. As a result, the softmax function encourages the controller to select only the L best performing layers for some value of noise and minimize the probability of the remaining layers. Additionally, utilizing the Gumbel distribution also introduces stochasticity into the sampling process. Instead of always selecting the top-L logits as the active layers, the stochasticity intuitively serves to encourage exploration of different layer configurations.\nProgressive Top-L Gumbel Softmax Sampling: Instead of employing the straight-through estimator, one can also utilize repeated Gumbel-Softmax Sampling to emulate discrete top-L sampling. (Xie & Ermon, 2019) proposed a method to emulate discrete top-L sampling by repeatedly applying the softmax function L times while adjusting the logits each\niteration. However, these methods may cause issues when applied to ADMN. First, methods utilizing Gumbel-Softmax Sampling to emulate discrete distributions typically have to undergo temperature annealing (Maddison et al., 2016), where the temperature is slowly decreased until the distribution is approximately categorical. Utilizing annealing can lead to a longer and more complicated training process for the controller. Additionally, the lack of explicit discretization during training may also result in a distribution shift at inference time, where the controller may learn to overrely upon partially activated layers during training."}, {"title": "A.3. Training Details:", "content": ""}, {"title": "A.4. Additional LayerDrop Results", "content": ""}, {"title": "A.5. Qualitative Results", "content": "In Figure 8, we visually showcase the noise corrupted multimodal inputs, and the corresponding layer allocation output by the ADMN controller with a budget of 6 layers. Given that the depth modality is naturally lower Qol, we can see that the controller gives preference towards the image modality. When Image Noise Standard Deviation is 3 and Depth Noise Standard Deviation is 0.25, the controller equally allocates layers among the two modalities. However, when the depth is entirely clean, the controller recognizes this and allocates all the layers towards the clean depth modality. These results reveal the intelligent allocation"}]}