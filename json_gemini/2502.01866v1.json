{"title": "Online Curvature-Aware Replay: Leveraging 2nd Order Information for Online Continual Learning", "authors": ["Edoardo Urettini", "Antonio Carta"], "abstract": "Online Continual Learning (OCL) models continuously adapt to nonstationary data streams, usually without task information. These settings are complex and many traditional CL methods fail, while online methods (mainly replay-based) suffer from instabilities after the task shift. To address this issue, we formalize replay-based OCL as a second-order online joint optimization with explicit KL-divergence constraints on replay data. We propose Online Curvature-Aware Replay (OCAR) to solve the problem: a method that leverages second-order information of the loss using a K-FAC approximation of the Fisher Information Matrix (FIM) to precondition the gradient. The FIM acts as a stabilizer to prevent forgetting while also accelerating the optimization in non-interfering directions. We show how to adapt the estimation of the FIM to a continual setting stabilizing second-order optimization for non-iid data, uncovering the role of the Tikhonov regularization in the stability-plasticity tradeoff. Empirical results show that OCAR outperforms state-of-the-art methods in continual metrics achieving higher average accuracy throughout the training process in three different benchmarks.", "sections": [{"title": "1. Introduction", "content": "Online Continual Learning (OCL) models are trained continuously on a nonstationary data stream. The goal is to obtain a model that is accurate at any point in time, quickly adapts to new data (i.e. plasticity), and does not forget old information (i.e. stability), without any information about task start, end, or identity. In this setting, many standard Continual Learning (CL) approaches, cannot be applied (Aljundi et al., 2019b; Mai et al., 2022b). Many specific OCL methods have been developed, nearly all replay-based (Yoo et al., 2024), but several shortcomings still exist: (Soutif-Cormerais et al., 2023) shows that most OCL methods have high forgetting and fail to beat a simple replay baseline on some metrics; (De Lange et al., 2023; Caccia et al., 2022) discovered the stability gap, a sudden drop in performance at task boundaries; (Dohare et al., 2024) even observed loss of plasticity, limiting the ability to learn in time.\nThese results suggest fundamental failures of the algorithms. Most methods focus on preventing forgetting at the end of a learning task, an approach that does not ensure stable optimization throughout the entire stream (stability gap); furthermore, stability is often optimized at the expense of plasticity. We argue that a proper CL optimizer should seek to maximize both stability and plasticity, assuming that the model is large enough, at every step in time, interpreting it as a continual filtering process (filter intended as in nonstationary time series literature (Durbin & Koopman, 2012)).\nThis paper proposes Online Curvature-Aware Replay (OCAR), a novel method designed for replay-based online continual learning, aiming at tackling the challenges in both plasticity and stability inherent in this scenario with a continual optimization approach at every step in time. We formalize OCL as the joint optimization on past and new data, with past data approximated using a limited replay buffer, and adding an explicit constraint on the variation of KL-divergence on previous information. The Fisher Information Matrix (FIM) is used to capture the loss function's curvature, providing both plasticity and stability constraints in the model distribution space. When the KL divergence is used as a metric, the FIM has the additional value of describing the curvature of the parameter space itself, being the Riemannian metric tensor of that space (Amari, 2016). We can directly adapt our gradient to the geometry of the space in stark difference with traditional CL methods that uses the FIM as a penalization term (Kirkpatrick et al., 2017). Kronecker-factored Approximate Curvature (K-FAC) (Martens & Grosse, 2015) is used to efficiently approximate the FIM, with some critical adjustments to make it work in OCL settings.\nThe main contributions of this paper are: the design of Online Curvature-Aware Replay (OCAR) as a combination of"}, {"title": "2. Related Work", "content": "Continual Stability and OCL: Most continual learning methods assume that stability must be kept at the expense of plasticity (Lange et al., 2022; Masana et al., 2023), the so called plasticity-stability tradeoff, and therefore are designed to preserve knowledge about previous tasks to mitigate catastrophic forgetting (French, 1999). However, recent evidence in (Lange et al., 2023; Caccia et al., 2022) suggest that even the methods with high \"stability\" measured at the end of tasks suffer from high instability and forgetting immediately after the task switch, and they recover the lost performance over time. (Kamath et al., 2024) found evidence of the stability gap even in incremental i.i.d. settings. In OCL settings, (Soutif-Cormerais et al., 2023) showed that some state-of-the-art methods are unable to outperform a simple reservoir sampling baselines on some fundamental stability metrics. Furthermore, CL methods also fail at keeping plasticity, and (Dohare et al., 2024) provides evidence of the loss of plasticity in deep continual networks. Overall, the literature suggests that CL methods fail at both stability and plasticity due to instabilities in the learning dynamics. Recently, some methods such as OnPro (Wei et al., 2023) and OCM (Guo et al., 2022) proposed novel self-supervised auxiliary losses and prototype-based classifiers, two approaches orthogonal to our optimization-based method.\nOptimization in Continual Learning: Most CL optimization algorithms are designed to prevent forgetting by removing interfering updates. GEM (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019a) models interference using the dot product of the task gradients and constrains the model updates to have positive dot products with the gradients of previous tasks. Subsequent work explored orthogonal projection methods (Saha et al., 2021; Farajtabar et al., 2020) that either extend the idea of interfering gradients or project in the null space of the latent activations. (Mirzadeh et al., 2020) discusses the relationship between the curvature of the first task and the forgetting, proposing a hyperparameter schedule that implicitly regularizes the curvature. More recently, LPR(Yoo et al., 2024) exploits proximal optimization in the L2 space of latent activations, and it is the only projection-based optimizer compatible with replay. (Hess et al., 2023) proposes a combination of GEM and replay as a potential mitigation for the stability gap.\nNatural Gradient and FIM in CL: Natural gradients can"}, {"title": "3. Online Continual Learning", "content": "In continual learning (CL), the model learns incrementally from nonstationary data. In most CL settings, we can identify a sequence of tasks $T_1, ..., T_v$, each one with its own distribution. For example, in class-incremental learning (van de Ven & Tolias, 2019), each tasks has different classes, and tasks are seen sequentially during training. The goal of the model is to learn all the tasks seen during training. Given the model parameters we learned"}, {"title": "4. Online Curvature-Aware Replay", "content": "We show the building process of our method, starting from the current ER optimization and expanding it to a second-order method, then approximated with FIM and making some final adjustments to improve its CL performance.\n4.1. The optimization problem\nOnline continual learning (OCL) is an online learning problem in a nonstationary setting. Common optimizers in machine learning implicitly assume stationarity, which justifies the gradient estimate from the minibatches (e.g.: ADAM (Kingma & Ba, 2015), SGD). Instead, in OCL the distribution can change at any point in time (non i.i.d.). At each step, the method can only use the current minibatch and, in replay-based methods, an additional minibatch sampled from a small buffer of old data.\nFirst-order optimization: We define our learning process as a sequence of local optimization problems solved at each step. Differently from stationary settings, these problems can be weakly dependent one on another, preventing the use of more \"global\" approaches (e.g, learning rate decay and momentum (LeCun et al., 2015)). Each single step must be meaningful by itself. What is forgotten or not learned could be lost forever.\nThe Kullback-Leibler (KL) divergence (Thomas & Joy, 2006; LeCun et al., 2015) is used as our objective, aiming to minimize the \"distance\" between the predicted and the real distribution. The KL is esti-"}, {"title": "4.2. Estimations and approximations", "content": "Approximating the Hessians: Computing two Hessians and one Fisher Information Matrix would be impractical and"}, {"title": "4.3. Nuts and Bolts for OCL", "content": "To make the method work in practice, we found some adjustments are needed.\nHyperparameter optimization and 7 scheduling: Usually, hyperparameter selection is done only on the first K tasks of the stream, but it must generalize to longer streams during training. Our method uses three hyperparameters: the learning rate a, the Tikhonov regularizer 7, and the parameter y for the EMA used for Kronecker factors estimation. In hyperparameter selection, we found it beneficial to search for a value for the increase of 7 instead of 7 itself. Tis then initialized at the same value of the learning rate and increased by the selected value at each optimization step improving long-term stability.\nEstimate of K-FAC factors at boundaries: In class-incremental settings, the shape of the classifier will grow over time as new classes are observed. This means the K-FAC factor G\u0131, of the last layer will change shape, also breaking the relations with the previously observed gra-"}, {"title": "5. Continual Stability in OCAR", "content": "In this section, we provide a qualitative analysis in a simplified setting, showing how OCAR results in a smoother continual optimization compared to ER and how \u03b1, \u03c4, and their ratio can be used to control stability and plasticity.\nLoss landscape and model trajectories: We can visualize the improvements in the optimization trajectory of OCAR in a simple continual learning setting. We train a small feedforward network with ER and OCAR on Split MNIST (5 Tasks). Given the small size of the model, we can store the entire training history of the model, which allows us to plot 2D projections of the model trajectory in the loss landscape (Figure 1) (details in Appendix F).\nLooking at their task-wise and joint loss surfaces in figures 1a and 1b, OCAR shows a much smoother model trajectory across all analyzed loss landscapes (Task 1, Task 2, and Average loss), which results in consistent improvements over time (plasticity) and mild forgetting (stability). Second-order information moves the optimization directly toward the next minimum (black stars in the plots), in fewer steps. On the other hand, ER always suffers from instability at the task boundaries (right after black stars in the plots) which results in an abrupt deviation from the optimal path. The learning curves on the first task (Fig. 10b) and the average of all tasks (Fig. 10a), available in Appendix G, confirm the result. OCAR maintains a smoother learning path without experiencing any stability gap on the first task and ending the stream with higher overall accuracy. ER instead, while still performing well in the basic MNIST setting, suffers from much more instability during training.\nRole of the Eigenvectors and Hyperparameters: One approach to understand OCAR effect is to study how the eigenvalues of the matrix $(F_N + (1 + \\lambda)F_B + \\tau I)^{-1}$ are related to the hyperparameters \u03b1, \u03bb, and \u03c4. We can diagonalize $F = F_N + (1 + \\lambda)F_B = Q\\Sigma Q^T$, where Q is a unitary matrix where the rows are the eigenvectors and \u03a3 a diagonal matrix with the eigenvalues \u03c3i. Therefore, we find that the eigenvalues of $\\alpha(F_N + (1 + \\lambda)F_B + \\tau I)^{-1}$ are $\\frac{\\alpha}{\\sigma_i + \\tau}$. In the new coordinate system defined by the eigenvectors Q, we can interpret OCAR as slowing or accelerating directions depending on their curvature. Directions with \u03c3 \u00ab 1 (\u03c3 \u00bb 1) correspond to directions with high (low) curvature for some tasks. The learning rate a and the Tikhonov regularization + rescale these eigenvalues (as shown in Figure 9 in Appendix). In particular, a learning rate a < 1 decreases the step size. Conversely, T mitigates the acceleration caused by small eigenvalues. When \u03c3\u03b5 \u2192 0, $\\frac{\\alpha}{\\sigma_i + \\tau} \\rightarrow \\frac{\\alpha}{\\tau}$, limiting the maximum acceleration in each direction. When \u03c3\u1f31 \u226b \u03c4, we have $\\frac{\\alpha}{\\sigma_i + \\tau} \\approx \\frac{\\alpha}{\\sigma_i}$, which is approximately independent of T.\nEmpirically, we find that the spectrum spans several orders of magnitude, with the smallest eigenvalues close to zero and a small set of very large values around $(10^4, 10^6)$ (consistently with the common intuition behind methods such as EWC, which expect few important parameters)."}, {"title": "6. Experiments", "content": "6.1. Comparison Between EWC, NGD, and OCAR in a Convex Setting\nFirst, we compare OCAR with alternative uses of the Fisher Information, that are not commonly considered for OCL, in a small-scale convex setting. In CL, EWC (Kirkpatrick et al., 2017) generated a cascade of derived methods (Chaudhry et al., 2018b; Liu et al., 2018; Husz\u00e1r, 2017) based on the idea to add a quadratic regularization term to the loss, penalizing the movement of the parameters from an optimal configuration, weighted by the FIM. This approach finds some limitations in OCL when no task boundaries are provided and it's not possible to select the previous task's \"best\" weights (Mai et al., 2022b), breaking the fundamental assumption behind the Laplace approximation. Additionally, regularizing the loss does not directly speed up non-important directions, a non-optimal approach in OCL. On the other hand, outside CL, the Natural Gradient Descent (NGD) (Amari, 1998) uses the FIM as a preconditioner for the gradient, slowing it down in the direction of high curvature and accelerating it in others. We believe NGD can be well-suited for OCL problems. The problem is that raw NGD is derived for i.i.d. settings. OCAR, on the other hand, is an adaptation for non-i.i.d. problems. To underline the differences, we tested the three approaches in combination with ER in an online stream of 10 small convex tasks (all details in Appendix D). We measure the cumulative loss experienced on single batches during the training $L_p = \\sum_t L(y_t, \\hat{y}_t)$ to measure the ability to adapt to current data and the cumulative loss experienced on all previous data $L_s = \\sum_t L(y_{0:t}, Y_{0:t})$ to measure the stability of the model. The results in figure 3 show that, while NG is much more adaptable than EWC, it is slightly less stable.\nEWC performance in OCL is very similar to the ones of basic ER, a result aligned with (Mai et al., 2022b). OCAR, thanks to its explicit memory constraint, and its dynamic hyperparameters is able to improve both on the speed and on the stability, showing a slight optimization superiority already in this very basic setting."}, {"title": "7. Conclusion", "content": "In this paper, we revisit replay-based OCL as second-order optimization with hard stability constraints and information geometry rooting. The resulting method OCAR shows consistent improvements in plasticity and continual stability with clear hyperparameters interpretation in the stability-plasticity tradeoff. Future research directions include the comparison of different approximations of the curvature (e.g. George et al. (2018b)), alternative derivations for the optimization problem (e.g. Benzing (2022b)), and feasible dynamic adaptation of a and T. We believe OCAR can be the starting point for further improvements towards a deeper understanding of continual learning dynamics."}, {"title": "A. Algorithm", "content": "Algorithm 1. Online Curvature-Aware Replay (OCAR)\nInput: network parameters w, learning rate a, per batch gradient steps count S, Tikhonov increase \u2206\u0442, EMA parameter\n\u03a3\u0395\u039c\u0391.\nOutput: trained network parameters w.\n1: Initialize replay buffer: B \u2190 {}.\n2: \u03c4 \u03b1\n3: \u03bb \u2190 1\n4: for t \u2208 {1, ..., \u221e} do\n5: Obtain new data batch Nt.\n6: Sample buffer data batch Bt\n7: for s \u2208 {1, ..., S} do\n8: Compute loss L(w) using Nt and Bt.\n9: Compute loss gradient VL(w).\n10: \u0442\u2190\u0442 + \u0410\u0442\n11: if Class Incremental then\n12: Update known classes list with Nt\n13: Increase X with new classes\n14: else\n15: \u03bb \u03bb + \u0394\u0442\n16: end if\n17: if s = 1 then\n18: Compute K-FAC factors A and G with Nt and Bt (Bt influence weighted by \u5165)\n19: for l\u2208 1, ..., L do\n20: \u0391\u0395\u039c\u0391,\u0406 \u2013 (1 \u2013 \u0425\u0415\u041c\u0410)\u0410\u0415\u041c\u0410,\u0406 + \u03b1\u0395\u039c\u0391\u0391\u03b9\n21: GEMA, \u2190 (1 \u2013 \u0425\u0415\u041c\u0410)GEMA,1 + QEMAGI\n22: if l = L and L changed shape then\n23: GEMA,IGI\n24: end if\n25: AEMA,GEMA\n26: FINV \u2190 (FEMA + TI)\u22121\n27: end if\n28: \u2207\u013d(w) \u2190 FINV\u2207L(w)\n29: w \u2190 w \u2013 a\u2207L(w)\n30: end for\n31: B \u2190 Reservoir.update(B, Nt, maxsize)\n32: end for\n33: end for\nThe K-FAC computations are done after weighting with X the data related to the buffer."}, {"title": "B. Experimental Setup", "content": "Code The entire code used to perform the full experiments (CIFAR100, TinyImageNet e Clear) can be found in the anonymous repository at https://anonymous.4open.science/r/CAR-8412. The anonymization process could have removed some important parts of the code but this is highly unlikely. The code was built on a combination of the repository for the OCL survey (Soutif-Cormerais et al., 2023) and the nngeometry repository (George, 2021).\nHardware The experiments were performed in a Linux cluster equipped with Nvidia Tesla V100 16GB GPUs and Intel Xeon Gold 6140M CPUs. The training was not parallelized between the GPUs."}, {"title": "C. Fisher Matrix Computations", "content": "The Fisher Information Matrix can be computed both as the variance of the score (the gradient of the log-likelihood) or as the negative expected value of the curvature of the log-likelihood. For a classification problem we assume the model fw is predicting a probability vector probability vector p = [P1,...,PK] where each pk is the probability that yk = 1, such that p = f(x; w). The distribution assumed is then a categorical distribution. On a single example x the FIM can be computed as:\nF(w) = Ey~Cat(y|x;w)[\u22072 logCat(y|x; w)] = Ey~Cat(y|x;w) [22 \u03a3yklogfw(x)k] =\n= Ey~Cat(y|xn;w) [\u03a3kYk\u2202fw(x) k\u2202w1fw(x)k\n:\u03a3kYk\u2202fw(x) k\u2202w1fw(x)k\n\u03a3kYk\u2202fw(x) k\u2202w2fw(x)k] =\n= Ey~Cat(y/2n;w) [\u03a3kYk\u2202fw(x) k\u2202w1\u2202w1\u03a3kYk\u2202fw(x) k\u2202w1\u2202w2:\u03a3kYk\u2202fw(x) k\u2202w2\u2202w2] =\nNote how each element is a sum of the contribution from each class, weighted by the inverse predicted probability for that class. The Fisher computed for a gradient composed of the sum (or mean) of gradients from multiple examples is the sum (or mean) of the individual Fishers. This is due to the required assumption to compute the Fisher: being at the optimum of the log-likelihood optimization, assuming the predicted parameters are the true ones. A consequence of this assumption is that the correlations between gradients of different examples will be zero: V[\u2207(yi|xi; w) + \u221a(yj|xj;w)] = V[\u2207(yi|xi; w)] + V[\u2207(yj|xj;w)]. Then, our A to give more weight to the buffer data is easily implemented. For the same assumption about the optimum, the variance of the score is computed as the expected value of the squared gradients: the squared expected value would be equal to zero.\nLast Layer FIM Focusing on the FIM of the last layer and ignoring all the correlations between the weights for a moment, we can compute the diagonal element of a classifier's weight. Assuming the loss L is a standard cross entropy loss:\nd\u13dddwj,i= hj(Pi - Yi)\nwhere pi is the prediction for the class i and y\u2081 an indicator function for the real value. The variance of the score for this weight is:\nE[(d\u13dddwj,i)2]= E[h}(pi \u2013 yi)\u00b2] = h3pi(1 \u2212 pi)\nIf we transform the partial derivative with the inverse variance:\nE[(d\u13dddwj,i)2]-1=1Pi (d\u13dddwj,i)/(hj Pi(1 - Pi))"}, {"title": "D. Qualitative Experiment", "content": "The code of the qualitative experiment is included in our repository. We used a basic linear model with 10 inputs and a single output trained to solve a simple linear regression problem with an MSE loss. 10 tasks with 1000 samples per task are randomly generated. Each task has a different multivariate normal from which input data are sampled and different real weights that should be estimated by the mode. The data are accessed as a stream, with a single pass per batch. The Standard Vitter algorithm is used to keep a buffer of old data. Each method uses a batch composed of 10 new data and 10 data sampled from the buffer.\nNatural Gradient is applied directly by estimating the full Fisher information of the model with an EMA of the FIM computed on the single batches. This FIM after the EMA is used to precondition the gradient.\nOCAR uses the exact same approach but uses a A to give more weight to old data and uses a scheduling of both A and 7 (the Tikhonov regularize) to increase them both in time. EWC is more tricky to implement in its basic form for online problems. Our approach is very similar to what is done in (Mai et al., 2022b), an online extension of the EWC++ strategy (Chaudhry et al., 2018a). The difference is that we use also replay data for computing the gradient. Namely, the loss is computed on both old and new data, but the Fisher (being a penalization) is computed only on old data (an EMA is kept also in this case for estimation). Given that no task boundaries are accessible, the penalization of the movement of the weights is done with respect to the weights at the previous step in time. In this way, a more regularized descent should be followed, penalizing the displacement from step to step using the Fisher Information of old data. The loss is a basic MSE but for analysis purposes, the figure D shows the cumulative loss (simply the loss experienced in each batch accumulated in time) and the cumulative loss on all previous data (at each step the loss on all previous data is computed after the optimization step and accumulated in time). The first measure shows a general capability of fast adaptation while the second an ability to find an optimum stable for the entire previous stream.\nA hyperparameter selection is performed to select the best setting on the sum of the final value of both these stability and"}, {"title": "E. Online CLEAR", "content": "Following the very recent work of (Yoo et al., 2024), we tested our method also on the Online CLEAR benchmark (Lin et al., 2021), a domain incremental learning scenario. This scenario is fundamentally different from the class-incremental, with the same classes that undergo some sort of evolution. The CL aspect is then less impactful, with much more forward/backward transfer and less catastrophic forgetting due to the similarity of the tasks in different domains. We followed the same settings as in LPR paper (Yoo et al., 2024), that are very similar to the standard approach used for our main experiments, but with some differences: the use of the full ResNet18 instead of the Slim version and 10 gradient steps per batch. Unfortunately, we encountered some bugs when their exact code was used in ours, requiring some slight modifications. Not being able to ensure the exact same conditions, we rerun all the experiments. We decided to compare the baseline of Experience Replay, the very recent LPR approach, and our method. ER-ACE is tailored for task-incremental settings, so we avoid its use. In this scenario, we increase X not with the number of classes in the buffer (as in class incremental), but we increase it in time as new batches arrive. LPR was tested using 100 samples from the buffer to estimate its preconditioner."}, {"title": "F. Learning Trajectory on Split MNIST", "content": "The model is a small feedforward network with two hidden layers of 100 units and ReLU activations. The model is trained online with 3 passes for each mini-batch with a small replay buffer of 100 elements, corresponding to 10 samples per class by the end of training.\nTo plot the 2D learning projections in Figure 5, we follow a procedure similar to (Mirzadeh et al., 2021)."}, {"title": "G. Additional Figures", "content": "We consider the 2D plane that intersects the model's initialization w\u1ed7, the model after the first task w\u2081, and the final model w. Many other possibilities were considered before this choice (e.g. using different tasks or random directions), but they all resulted in qualitatively similar plots. The coordinates system is obtained by orthonormalizing the directions u = w\u2081 \u2013 wo and v = \u03c9\u03be \u2013 wo, obtaining \u016b = u/||u|| and v = v-cos(u,v)u/||v-cos(u,v)u||. Given a model w, its coordinates in the 2D space are the unique (x, y) such that w = x\u016b + y\u016b + w\u1ed7. Notice that each method has different values for w\u2081 and w, and therefore different 2D planes and coordinate systems are chosen for each method. Since the coordinates are not meaningful and cannot be compared across plots, we do not show them in Figure 5."}]}