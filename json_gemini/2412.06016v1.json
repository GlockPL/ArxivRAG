{"title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation", "authors": ["Hyeonho Jeong", "Chun-Hao P. Huang", "Jong Chul Ye", "Niloy J. Mitra", "Duygu Ceylan"], "abstract": "While recent foundational video generators produce visually rich output, they still struggle with appearance drift, where objects gradually degrade or change inconsistently across frames, breaking visual coherence. We hypothesize that this is because there is no explicit supervision in terms of spatial tracking at the feature level. We propose Track4Gen, a spatially aware video generator that combines video diffusion loss with point tracking across frames, providing enhanced spatial supervision on the diffusion features. Track4Gen merges the video generation and point tracking tasks into a single network by making minimal changes to existing video generation architectures. Using Stable Video Diffusion [5] as a backbone, Track4Gen demonstrates that it is possible to unify video generation and point tracking, which are typically handled as separate tasks. Our extensive evaluations show that Track4Gen effectively reduces appearance drift, resulting in temporally stable and visually coherent video generation.", "sections": [{"title": "1. Introduction", "content": "Diffusion-based video generators [5, 7, 52] are making rapid strides in creating temporally consistent and visually rich video content. This progress marks a significant shift, as the unification of generation and control has the potential to transform the traditional workflow of first capturing and then digitally editing video.\nDespite impressive capabilities, video generators often suffer from appearance drift, where visual elements gradually change, mutate, or degrade over time, causing inconsistencies in the objects. For example, in Fig. 1, we observe the horns of the cow distorting and morphing unrealistically over time, breaking the plausibility of the generated content. This is in striking contrast to humans, who develop a sense of appearance constancy as early as infancy through observation and interaction with the world [77].\nUnfortunately, appearance drift remains a persistent issue in current video models, even with increased training data and more advanced architectures. We speculate that this limitation arises from supervision being based solely on video diffusion loss (i.e., denoising score matching [69]) in the pixel/latent space, without explicit spatial awareness guidance in the feature space. Hence, in this paper, we ask if and how we can empower video diffusion models with appearance constancy by providing additional supervision.\nWe present Track4Gen as a spatially aware video generator that receives supervision both in terms of the original diffusion-based objective as well as (dense) point correspondence across frames, which we refer to as tracks. We demonstrate that it is possible to provide such track-level supervision in the diffusion feature space by making minimal architecture changes. Our generated videos do not suffer from degradation of video quality (according to the usual video generation metrics), while being significantly more spatially coherent as the highlight cow in Fig. 1.\nWe train Track4Gen using the latest Stable Video Diffusion [5] as the backbone and evaluate on the publicly available VBench dataset [32, 33]. We report significant improvement in terms of appearance constancy of subjects, both in quantitative and qualitative (i.e., via user studies) evaluations. In summary, we demonstrate that it is possible to upgrade existing video generators, by supervising them with additional correspondence tracking loss, to produce videos without significant appearance drifts, a problem commonly encountered in diffusion-based video generators."}, {"title": "2. Related Work", "content": "Diffusion-based video generation. Building on the success of diffusion models in image synthesis [13, 56], diffusion-based video generators have seen significant advancements [5, 7, 31, 52]. A commonly adopted approach is to extend text-to-image models to the video domain by incorporating temporal layers to facilitate interactions across video frames [6, 24, 59]. While some works have adopted cascaded approaches to produce both spatially and temporally high-resolution videos [30, 52, 59, 72, 81, 85], others have utilized lower-dimensional latent space modeling to reduce computational demands [6, 10, 26, 87]. We build on top of one such approach, Stable Video Diffusion (SVD, [5]), which introduces a latent image-to-video diffusion model trained on a large-scale and curated video data.\nWith advances in generation, systematic evaluation of generation quality has become crucial. Traditionally, metrics such as Fr\u00e9chet Inception Distance (FID, [28]), Fr\u00e9chet Video Distance (FVD, [68]), and CLIPSIM [54] are used. Additionally, comprehensive benchmark suites [32, 33, 73] have been introduced to provide a more robust evaluation aligned with human perception. Inspired by such work, we thoroughly evaluate our approach and demonstrate improved video generation quality with respect to both conventional metrics and the recent VBench metrics [32].\nFoundational models as feature extractors. Various foundational models such as vision transformers [17] or diffusion-based generators [55] have been utilized as feature extractors for various tasks including semantic matching [18, 27, 46], classification [43], segmentation [71, 76], and editing [21, 23, 36, 66]. There have been efforts to boost their performance by post-processing the feature maps obtained from the pre-trained models, e.g., by upsampling [20, 63]. In a recent effort, Yue et al. [80] lift semantic per-frame features from a foundational model into a 3D Gaussian representation. They fine-tune the foundational model with such 3D-aware features resulting in improved performance in downstream tasks. Similarly, Sundaram et al. [62] fine-tune state-of-the-art foundational models on human similarity judgments yielding improved representations across downstream tasks. In a concurrent effort, Yu et al. [79] propose to align the internal features of an image generation model with external discriminative features [50], which results in more effective training of the generator.\nOur work also enhances the internal feature representation of a foundational generation model but with significant differences compared to previous literature. First, unlike most previous work that focus on image level foundational models, we exploit the power of recently emerging video models. Second, instead of post-processing, we enhance the spatial awareness of the intermediate features by training the generator to jointly perform an additional tracking task. We show that this joint training boosts the performance of intermediate features in correspondence tracking, leading to improved video generation quality.\nTracking any point in a video. The task involves following any arbitrary query point across a long video sequence. First introduced by PIPs [25] and later re-framed by TAP-Vid [14], several methods have emerged in recent years to tackle long-term point tracking. PIPs [25] revisits the classical particle-based representation [58] and introduces MLP-based networks that predict point tracks within an 8-frame window. Subsequent works have improved performance by capturing longer temporal context through advanced architectures [3, 15, 25, 37], as well as by enabling the simultaneous tracking of multiple queries [12, 37]. More recent training-based trackers [12, 38, 45, 75] have achieved remarkable performance by leveraging high-capacity neural networks to learn robust priors from large-scale training data. While high-quality data is crucial for accurate tracking, manually annotating point tracks is prohibitively expensive. Hence, synthetic videos [22] with automatic annotations, have become an alternative and have demonstrated effectiveness in real-world video tracking. An alternative approach is self-supervised adaptation at test time, where tracking is learned without ground-truth labels [35, 67, 70]. In a recent study, Aydemir et al. [2] evaluate the effectiveness of several image foundational model features for point tracking both in zero-shot setting as well as with supervised training using low-rank adapter layers. To the best of our knowledge, we are the first to exploit the features of a foundational video diffusion model for dense point tracking."}, {"title": "3. Method", "content": "In this section, we provide a comprehensive discussion of the Track4Gen framework. We begin with a concise overview of latent video diffusion models (Sec. 3.1). Next, we discuss how video diffusion features relate to temporal correspondences both for real and generated videos (Sec. 3.2). Finally, we detail the design of Track4Gen both in terms of network architecture and the employed supervision signals (Sec. 3.3).\n3.1. Background: Stable Video Diffusion\nStarting from random Gaussian noise, diffusion models aim to generate clean images or videos via an iterative denoising process [29, 60]. This process reverses a fixed, time-dependent diffusion forward process, which gradually corrupts the data by adding Gaussian noise. While our method is applicable to general video diffusion models, in this paper, we design our architecture based on Stable Video Diffusion (SVD), a latent video diffusion model which employs the EDM-framework [39]. The diffusion process operates in the lower-dimensional latent space of a pre-trained VAE [41], consisting of an encoder \\(E(\\cdot)\\) and a decoder \\(D(\\cdot)\\).\nGiven a clean sample \\(x_{1:N} \\sim P_{data}(x)\\) of an \\(N\\)-frame video sequence, the frames are first encoded into the latent space as \\(z_{1:N} = E(x_{1:N})\\). Gaussian noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) is then added to the latents to produce the intermediate noisy latents via the forward process \\(z_{1:N}^t = a_t z_{1:N} + \\sigma_t \\epsilon\\), where \\(t\\) represents the diffusion timestep, and \\(a_t, \\sigma_t\\) are the discretized noise scheduler parameters. The diffusion denoiser \\(f_\\theta\\) is trained by minimizing the \\(v\\)-prediction loss:\n\\[\\min_\\theta E_{\\epsilon \\sim \\mathcal{N}(0,I), t \\sim U[1,T]} [ || f_\\theta (z_{1:N}^t, t, c) - v ||^2 ],\\]\nwhere \\(v\\) is defined as \\(v = a_t \\epsilon - \\sigma_t z_{1:N}\\). In the image-to-video variant of SVD, the condition \\(c\\) refers to the CLIP image embedding [54], replacing the typical text embeddings. For the remainder of this paper, we will refer to Eq. 1 as the video diffusion loss \\(\\mathcal{L}_{diff}\\).\nOnce trained, the diffusion model generates videos by iteratively denoising a noisy latent \\(z_{1:N}^T\\) sequence sampled from pure Gaussian distribution. At each diffusion step, the model predicts the noise in the input latent. Once the clean latent \\(z_{1:N}^0\\) is obtained, the decoder \\(D\\) maps it to the higher-dimensional pixel space \\(x_{1:N} = D(z_{1:N}^0)\\). For further details, we refer to the Appendix D of [5].\n3.2. Video Diffusion Features\nPrevious studies have demonstrated that image diffusion models learn discriminative features in their hidden states that are effective for various analysis tasks and propose methods for improving the representation power of such features [11, 74, 78, 79]. Similarly, we argue that while also being powerful, internal representations of pre-trained video diffusion models may not be fully temporally consistent, resulting in appearance drift in generated videos.\nTo better investigate this hypothesis, we first evaluate the long-term video tracking capabilities of U-Net-based video diffusion models [5, 61, 85]. Specifically, we evaluate the effectiveness of the features from each block of the U-Net for the task of point tracking. Given a real-world video, we add a small amount of noise and extract feature maps from each layer in each block. We perform a cosine-similarity-based nearest-neighbor search [49, 64] over these feature maps for a given set of fixed query points on the first frame (we use a similarity threshold of 0.6 [67] in our experiments). We also perform a similar analysis for generated videos where we extract the feature maps corresponding to diffusion steps with small amount of noise.\nBased on this feature analysis, we make some important observations. Notably, regardless of the model (we analyze both Zeroscope T2V [61] and SVD I2V [5]), we find out that output features from the upsampler layer of the third decoder block consistently yield stronger temporal correspondences. Hence, we use this block when extracting features for the remainder of our experiments. Furthermore, when we analyze generated videos and point tracks estimated based on the feature maps (as shown in Fig. 4), we observe that there is a correlation between tracking failures that reveal feature-space inconsistencies and appearance drifts that reveal pixel-space inconsistencies.\n3.3. Track4Gen\nTrack4Gen aims to utilize point tracking as an additional supervision signal to enhance the spatial-awareness of video diffusion features. Given that we build on top of a pre-trained video generation model, to retain the prior knowledge and avoid tampering the original features directly, we propose a novel architecture change as shown in Fig. 2. Specifically, instead of directly using the raw diffusion features for correspondence estimation, we propose a trainable refiner module \\(R_\\theta\\), which is designed to refine the raw features by projecting them into a correspondence-rich feature space. The refined features, which are spatially-aware, are then both used to estimate point tracks with an explicit supervision as well as feeding back to the generation backbone. We empirically find out that this design is more effective compared to fine-tuning the original model with no refinement module (see Sec. 4.2).\nGiven an \\(N\\)-frame video sequence \\(x_{1:N}\\), its corresponding latent \\(z_{1:N}\\), and a diffusion timestep \\(t\\), in order to train Track4Gen we continue to utilize the standard diffusion training loss as defined in Eq. (1), where we adopt the velocity prediction objective [5, 39, 57] for \\(\\mathcal{L}_{diff}\\).\nTo enable tracking supervision, we assume access to a dense set of point trajectories \\(\\Omega = \\{(x^i_q, x^j_{trg})\\}\\) across frames where a point \\(x^i_q\\) in frame \\(i\\) corresponds to a matching point \\(x^j_{trg}\\) in frame \\(j\\) and vice versa. Given the corresponding noisy video latent sequence \\(z_{1:N}^t\\), we first extract raw diffusion features as the hidden states \\(h_{1:N} \\in \\mathbb{R}^{N \\times H \\times W \\times C}\\) from a specific block \\(b_k\\) within the U-Net, where \\(b_k\\) is set to the upsampler layer of the third decoder block (see Sec. 3.2). We then pass these features through the refiner module to obtain the refined feature map \\(\\hat{h}_{1:N} = R_\\theta(h_{1:N})\\).\nWe sample a query point \\(x^i_q\\) along with its ground-truth target point \\(x^j_{trg}\\) from the correspondence set \\(\\Omega\\). Given the query point feature \\(\\hat{h}^i(x_q) \\in \\mathbb{R}^{1 \\times 1 \\times C}\\) and the target feature map \\(\\hat{h}^j \\in \\mathbb{R}^{H \\times W \\times C}\\), we calculate the cost volume \\(S \\in \\mathbb{R}^{H \\times W \\times 1}\\) as follows:\n\\[S(p) = cos\\text{-}sim(\\hat{h}^i(x_q), \\hat{h}^j(p)),\\]\nwhere cos-sim denotes cosine similarity. The predicted target point \\(x_{trg}\\) is then determined using the differentiable soft-argmax operation:\n\\[x_{trg} = \\frac{\\sum_{p \\in \\Omega'} S(p) \\cdot x_p}{\\sum_{p \\in \\Omega'} S(p)},\\]\nwhere \\(\\Omega' = \\{p: ||x_p - x_{p_{max}}||_2 < R\\}\\). Thus, the target point prediction can be expressed as \\(x_{trg} = g(x_q^i, j, \\hat{h}^j)\\), and the predicted tracklet for \\(x_q^i\\) is given by \\(T_x = \\{\\hat{x}_n: \\)"}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nTo train Track4Gen, we construct a training dataset consisting of 567 video-trajectory pairs, with each video having a resolution of 320 \u00d7 576 and a duration of 24 frames. Since no real-world video with (dense) ground-truth trajectory annotations exist at the time of this work, we utilize optical flow to generate trajectory annotations. A key challenge is the need for accurate video segmentation maps to ensure a balanced distribution of trajectory points between foreground objects and the background [14]. To address this, we utilize public video datasets paired with ground-truth segmentation maps [8, 19, 44, 51, 53], where we split longer videos into 24-frame segments.\nWe use Stable Video Diffusion (SVD) image-to-video pretrained checkpoints\u00b2 as the base video generator. Our proposed refiner module consists of eight stacked 2D convolution layers and is attached to the third decoder block of the SVD UNet. The refiner module preserves the shape of the hidden states throughout and is initialized as the identity mapping. We finetune this enhanced video generator architecture for 20K steps with our joint loss \\(\\mathcal{L}_{diff} + \\lambda \\mathcal{L}_{corr}\\), where \\(\\lambda\\) is set to 8. Rather than finetuning the entire model, we finetune only the temporal transformer blocks, the refiner module \\(R_\\theta\\), and the zero convolution \\(\\psi\\). In each iteration, we sample 512 correspondence pairs from the precomputed trajectories. We use the AdamW optimizer [48] with a learning rate of 1e-5, \\(\\beta_1\\)=0.9, \\(\\beta_2\\)=0.999, and a weight decay of 1e-2. We train the model on 4\u00d7 H100 GPUs with a total batch size of 4. For sampling new videos, we apply the default settings using 30 steps with the EDM sampler [39], motion bucket id =127, and fps=7.\n4.2. Track4Gen for Video Generation\nWe evaluate Track4Gen for the image-to-video generation task via a series of experiments using multiple datasets, automated metrics, and human evaluations.\nEvaluation Setup. We compare Track4Gen against the original SVD (SVD*) [5], as well as a version of SVD that is finetuned on the same videos as Track4Gen (finetuned SVD). Furthermore, we train a variant of Track4Gen without the refiner module. For VBench metrics [32], evaluations are conducted on the VBench-I2V dataset, containing 355 diverse images. FID and FVD are measured using the DAVIS [53] dataset as reference. We generate 24-frame videos conditioned on each input image.\nAutomatic metrics. We first report five key metrics from VBench [32]: (1) Subject Consistency-assesses subject appearance consistency of the video by computing the similarity of DINO [50] features. (2) Temporal Flickering-detects temporal consistency by taking static frames and calculating the mean absolute difference across frames. (3) Motion Smoothness-measures smoothness of motion, and how well it adheres to real-world physics, using video frame interpolation priors [47]. (4) Image Quality-evaluates distortions (e.g., noise, blur) using a pretrained, multi-scale image quality predictor [40]. (5) Video-Image Alignment-measures alignment between the subject in the input image and in the generated video using DINO features. We additionally report FID [28] and FVD [68]. To further assess temporal consistency, CLIPSIM [54] and LPIPS [84] are reported in the Appendix.\nHuman evaluation. We further evaluate Track4Gen against baselines through a user study. We ask 64 participants to compare our results with a randomly selected baseline. We ask the users to evaluate how consistent main objects appear across the frames in a generated video as well as how natural the depicted motion is.\nQualitative results. Qualitative comparisons with the base SVD are shown in Fig. 5. As illustrated, Track4Gen generates videos with strong appearance consistency, avoiding issues of appearance drift. In contrast, videos produced by the original SVD exhibit noticeable inconsistencies: the sheep's head (row 1) mutates, the plane's wing (row 2) shows unnatural transitions, and the cars (row 3) disappear. Further comparisons with finetuned SVD and Track4Gen without the refiner module are shown in Fig. 6 and highlight the superior visual coherence of the proposed Track4Gen.\nQuantitative results. As shown in Tab. 1, our method achieves the highest scores across all 5 metrics from VBench, along with the lowest FID and second-lowest FVD values, outperforming the base SVD by substantial margins. Fig. 8 provides the user study results where the majority of the participants agreed that Track4Gen is superior both in terms of identity preservation and naturalness of motion.\n4.3. Track4Gen for Video Tracking\nWe evaluate Track4Gen's capability to track any point in real videos by adding a small amount of noise to the input video [64] and passing it through the video denoiser \\(f_\\theta\\) to extract feature maps. We first compare tracking results with such features against other raw features [5, 61, 65] in Sec. 4.3.1. In Sec. 4.3.2, we utilize Track4Gen's features in a test-time optimization method [67] and compare to both self-supervised and fully supervised video trackers."}, {"title": "5. Conclusion and Future Work", "content": "We have presented the first unified framework that bridges two distinct tasks: video generation and dense point tracking. We demonstrated that this produces temporally consistent feature representations and appearance-consistent videos. As for limitations, videos generated by Track4Gen tend to exhibit less dynamic motion compared to those from other video generators. Additionally, failure cases are included in the Appendix.\nFuture work. Recently, cutting-edge video trackers [12, 16, 38] have emerged, enabling dense, accurate, and long-term tracking, especially with better handling of occlusions. This opens up promising future directions for extending our work to utilize real-world videos, automatically annotated by these advanced trackers. Additionally, we aim to explore conditional video generation, using point tracks to guide or blend motions as a coarse authoring tool."}, {"title": "Appendix", "content": "This document is structured as follows: Sec. A provides additional implementation details for the experiments. In Sec. B, we report supplementary quantitative metrics for video generation assessment. Sec. C presents additional qualitative results for image-to-video generation, while Sec. D focuses on qualitative video tracking results. Following this, we discuss the potential limitations and failure cases of Track4Gen in Sec. E.\nA comprehensive view of results in the form of videos is available on our project page. Furthermore, an extensive video generation comparison against all baselines can be found on this page.\n\nA. Experimental Details\nA.1. Preprocessing Video Correspondence\nWe utilize RAFT optical flow [65] to compute dense point trajectories across video frames. RAFT has demonstrated robust point tracking performance across various input types [70], even compared to supervised trackers like TAP-Net [14]. Following previous tracking literature [67, 70], we first compute pairwise correspondences between all consecutive frames. Tracks are then formed by chaining the estimated flow fields and filtered using a cycle consistency constraint. Specifically, given a point \\(x^i\\) in frame \\(i\\) and optical flow between frames \\(i\\) and \\(i + 1\\) denoted as \\(f_{i \\rightarrow i+1}\\), the corresponding point in frame \\(i + 1\\) is estimated as \\(x^{i+1} = x^i + f_{i \\rightarrow i+1}(x^i)\\). We retain the pair (\\(x^i\\), \\(x^{i+1}\\)) only if it satisfies \\(||x^i - (x^{i+1} + f_{i+1}(x^{i+1}))|| \\leq 1.5\\).\nA.2. Refiner Network\nWhen training Track4Gen, we design a convolutional neural network for the refiner module \\(R_\\theta\\). The network comprises 8 layers, each with a fixed channel dimension of 640, a kernel size of 3, stride of 1, and padding of 1. The first 7 layers follow the structure Conv2d \\(\\rightarrow\\) BatchNorm2d \\(\\rightarrow\\) ReLU, except for the last layer which consists of Conv2d \\(\\rightarrow\\) ReLU.\nTo better demonstrate the architecture of the baseline Track4Gen without Refiner, we provide a visualization in Fig. 10. The figure compares the training schemes of this baseline with Track4Gen. In this variant, the correspondence loss \\(\\mathcal{L}_{corr}\\) is computed directly from the raw video diffusion features \\(h_{1:N}\\).\nA.3. User Study Details\nFig. 11 shows an example of our user evaluation page. The input image is displayed on the left, while the middle and right columns show two generated videos for comparison. One result is from Track4Gen, and the other is randomly selected from four baselines: pretrained Stable Video Diffusion [5], finetuned Stable Video Diffusion without correspondence supervision, and Track4Gen trained without the refiner module. Note that the order of Track4Gen and the baseline is randomly shuffled (i.e., Track4Gen may appear first or the baseline may appear first). Participants are asked to answer two questions: (i) Identity preservation: Which video better preserves the identity of the main object(s)? (ii) Motion naturalness: Which video has more natural motion?\nA.4. Encoding Long Videos with Video Diffusion Models\nMajority of video diffusion models struggle with flexibility in temporal resolution. Specifically, if a model is trained on a fixed temporal resolution of \\(N\\) frames (e.g., \\(N = 24\\)), the quality of generated videos significantly degrades when attempting to generate videos with a much larger number of frames. Similarly, when these models are used as video feature extractors, the extracted features are invalid if the input video contains significantly more frames than the model was trained to handle.\nThis limitation poses a challenge, as most videos in video tracking benchmarks contain more frames than the training resolution of video diffusion models. To address this, for a benchmark video with temporal resolution M,"}]}