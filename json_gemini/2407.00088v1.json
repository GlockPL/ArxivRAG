{"title": "T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge", "authors": ["Jianyu Wei", "Shijie Cao", "Ting Cao", "Lingxiao Ma", "Lei Wang", "Yanyong Zhang", "Mao Yang"], "abstract": "The deployment of Large Language Models (LLMs) on edge\ndevices is increasingly important to enhance on-device in-\ntelligence. Weight quantization is crucial for reducing the\nmemory footprint of LLMs on devices. However, low-bit\nLLMs necessitate mixed precision matrix multiplication\n(mpGEMM) of low precision weights and high precision ac-\ntivations during inference. Existing systems, lacking native\nsupport for mpGEMM, resort to dequantize weights for high\nprecision computation. Such an indirect way can lead to a\nsignificant inference overhead.\nIn this paper, we introduce T-MAC, an innovative lookup\ntable(LUT)-based method designed for efficient low-bit LLM\n(i.e., weight-quantized LLM) inference on CPUs. T-MAC di-\nrectly supports mpGEMM without dequantization, while si-\nmultaneously eliminating multiplications and reducing addi-\ntions required. Specifically, T-MAC transforms the traditional\ndata-type-centric multiplication to bit-wise table lookup, and\nenables a unified and scalable mpGEMM solution.\nOur LUT-based kernels scale linearly to the weight bit-\nwidth. Evaluated on low-bit Llama and BitNet models, T-\nMAC demonstrates up to 4\u00d7 increase in throughput and 70%\nreduction in energy consumption compared to llama.cpp. For\nBitNet-b1.58-3B, T-MAC delivers a token generation through-\nput of 30 tokens/s with a single core and 71 tokens/s with\neight cores on M2-Ultra, and 11 tokens/s on lower-end de-\nvices like Raspberry Pi 5, which significantly exceeds the\nadult average reading speed. T-MAC with LUT-based com-\nputing paradigm, paves the way for the practical deployment\nof low-bit LLMs on resource-constrained edge devices with-\nout compromising computational efficiency. The system is\nopen-sourced at https://github.com/microsoft/T-MAC.", "sections": [{"title": "1 Introduction", "content": "More and more large language models (LLMs) are deploying\non client devices, such as smartphones, desktops, and robot-\nics, to provide unprecedented intelligence services, real-time\ntask response and user data protection. Typical examples are\nPhi-3-mini-4bit deployed on iPhone [11], Llama-2-7B-4bit on\nPixel 5 and Llama-2-13B-4bit on Apple M2 Ultra [5], as well\nas the recent Microsoft Copilot+PC [3] that collaboratively\nrun the on-device LLM with the on-cloud LLMs.\nLow-bit weight quantization is a must-have for on-device\nLLM inference due to the hardware resource limitation. LLM\ninference quality is also robust to precision loss. Beyond 4-bit,\n3-bit, 2-bit and even 1-bit models are emerging [13, 18, 33, 35].\nBy comparison, activation quantization cannot follow the\ntrend due to outliers. The computing operands thus have\nasymmetric precision and bit width, such as W4A16, W2A16,\nor W1A81.\nOn the other hand, current commodity hardware still sup-\nports fixed bit-width and symmetric operands, and cannot\nsupport these various and mixed precision. Even some re-\nsearch papers support asymmetric bit-width operands. The\nbit-width is still fixed, such as W4A8 [22]. Computing ker-\nnels have to convert/dequantize low-bit weight to match the\nactivation, and compute on hardware.\nThis conversion raises two obvious issues. (i) Performance\nwise, the conversion cost offsets the gain from bit scaling\ndown. Our evaluation (ref Figure 6) shows that scaling down\nbits from 4 bit to 1 bit even increases latency cost for most of\nthe cases. (ii) Development wise, the data layout and kernels\nneed to be designed case by case for each mixed precision.\nFor example, the data layouts, as well as the interleaving or\nswizzling methods for W3 and W2 are totally different. The\nkernels need to be redesigned to match the layout. Therefore,\nto deploy LLM on devices, a fundamental problem is how to\ndirectly and efficiently support mpGEMM (mixed precision\nGeneral Matrix Multiplication) of low bit weights and high\nbit activations on devices.\nThis paper aims a mpGEMM kernel design, which is in-\ndependent of hardware data types and the bit width of the\nquantization algorithms, to achieve scalable speedup with bit\nscaling down. To realize that, our key concept is that rather\nthan the dominant data-type-centric calculation, we exploit\nthe bit-wise calculation of the standard algorithm of mul-\ntiplication. That is, the multiplication of two numbers can\nbe transformed into multiplying one number by each bit of\nthe other number, then shifting and adding the partial prod-\nucts. The mpGEMM between activation and weight matrices\nis decomposed into a series (= the bit-width of weight) of\nmpGEMM between activation and a one-bit matrix, and then\nadds the partial results up. This method can thus support\nany bit-width combination of activation and weight.\nA promising method to implement bit-wise calculation\nis by table lookups [29]. Since one bit can only represent\ntwo values, e.g., 1/-1, the bit patterns of a one-bit vector are"}, {"title": "2 Background and Motivation", "content": "The advent of Large Language Models (LLMs) has revolution-\nized the field of natural language processing, opening new\nhorizons for human-computer interaction and personalized\nassistant. The deployment of LLMs directly on edge devices,\nsuch as smartphones, desktop computers, and robotics, has\nemerged as a critical frontier in computing, promising to en-\nhance these devices with unprecedented levels of intelligence\nand autonomy.\nDeploying LLMs on edge devices\nbrings several compelling benefits. On-device processing\ndrastically reduces response latencies, which is critical for\ntime-sensitive applications such as autonomous vehicles and\ninteractive robotics. Furthermore, local data processing en-\nhances user privacy by keeping sensitive information con-\nfined to the device, thereby reducing the risk of data leakage.\nAnother significant advantage is the operational reliability\nthat comes with network independence, allowing for con-\nsistent functionality regardless of network availability or\nstability.\nThe primary challenge in de-\nploying LLMs on edge devices is the substantial memory\nrequirement to accomodate these models. LLMs often en-\ncompass billions of parameters. For example, LLAMA-2-7B\nwith FP16 precision requires at least 14GB of memory to"}, {"title": "2.2 Low-Bit (Weight-Quantized) LLM", "content": "The memory-intensive nature of LLM inference necessitates\nstrategies to reduce the model's memory footprint with-\nout significantly compromising model performance. Weight\nquantization emerges as a key technique to achieve this bal-\nance.\nWeight quantization involves reducing the precision of\nthe model's parameters, effectively allowing the model to\noccupy less memory and potentially speed up computa-\ntion by leveraging lower-precision arithmetic [17, 20, 25].\nNowadays, LLMs are increasingly being released with a 4-\nbit version specifically tailored for deployment on edge or\nother resource-constrained environments [32, 36]. Recent\nresearch has pushed the boundaries even further, investigat-\ning the feasibility of 2-bit and 1-bit weight representations\nin LLMs [18, 33, 35]. Fundamentally, the choice of bitwidth\nor precision in weight quantization represents a trade-off\nbetween computational efficiency and model accuracy."}, {"title": "2.3 Deployment Challenges of Low-Bit LLM", "content": "The adoption of low-bit LLMs has become a necessity for\nedge deployment. In fact, numerous LLM-on-Edge systems\nand implementations are actively employing low-bit tech-\nniques [2, 5]. However, deploying low-bit LLMs introduces\nunique computational challenges particularly in accommo-\ndating mixed-precision operations, which are not natively\nsupported by most hardware architectures, and managing\nthe diversity of bit widths and precision levels required by\nvarious deployment scenarios. Addressing these challenges\nis critical for full potential and seamless intergration of low-\nbit LLMs in edge computing."}, {"title": "Mixed-precison GEMM/GEMV", "content": "The utilization of Low-\nBit (weight-quantized) LLMs introduces a computational par-\nadigm where low precision weights are combined with rela-\ntively higher precision activations. This necessitates a shift\nfrom standard matrix multiplication operations (i.e., GEMM,\nGEMV) to mixed-precision ones (i.e., mpGEMM, mpGEMV).\nHowever, current hardware architectures, including CPUs,\nGPUs, and NPUs, do not natively support mixed-precision\noperations. These architectures are traditionally optimized\nfor standard operations where both operands share the same\ndata type and precision level.\nIn response to this limitation, existing systems implement\nan indirect approach by employing dequantization, which\ninvolves converting low-precision weights back to a higher\nprecision to align with activation precision. This process\nenables the use of high-precision GEMM for low-bit LLM\ninference. For instance, systems like those used in the Intel\nNeural Compressor and llama.cpp rely on this dequantization-\nbased technique. However, the efficacy of such methods is\ncontingent upon the assumption that dequantization does\nnot become a bottleneck and can be overlapped with memory\nloading. Since this indirect approach ultimately reverts to\nhigh-precision computation, it fails to fully capitalize on the\nbenefits of low-bit weights, such as reduced memory usage\nand potentially faster computation."}, {"title": "Bit-width/precision diversity", "content": "Beyond the challenge of\nfacilitating mixed-precision operations, the diversity of bit-\nwidths and precisions required by different deployment sce-\nnarios compounds the complexity. Depending on the task's\ndifficulty and the specific requirements of the deployment\nenvironment, a variety of bit-widths may be selected to opti-\nmize performance. No single bit-width or precision setting\ncan universally satisfy the diverse demands of all possible\nuse cases. Consequently, this necessitates computational ap-\nproaches capable of supporting a spectrum of low-bit widths,\nensuring adaptability to the wide-ranging needs of edge\ncomputing tasks."}, {"title": "2.4 LUT-based Computation for Quantized Model", "content": "A new trend in the computation of quantized models is the\nadoption of Lookup Table (LUT)-based methods. For quan-\ntized Convolutional Neural Networks (CNNs), where both\nweights and activations are quantized to levels such as 4-bit,\n2-bit, or 1-bit, DeepGEMM [21] precomputes all possible\nproducts of weights and activations, stores them in a lookup\ntable, and efficiently accesses them at inference time to avoid\ncostly multiply-accumulate operations. Another example is\nfor vector quantization, where activations are vector quan-\ntized, MADDNESS [12] and LUT-NN [31] also transform\nGEMM computation to table lookups.\nIn the context of low-bit LLMs, or weight-only quan-\ntized LLMs, the LUT-based approach has been explored on\nGPUs [26, 29]. These methods leverage the GPU's shared"}, {"title": "3 Design", "content": "Current implementations for mixed-precision GEMM vary\ncase by case. Each bit-width combination of activation and\nweight, such as W4A16 and W2A8, requires specific weight\nlayout and computing kernels. For example, the layout for\nW3 could pack 2 bits and the other 1 bit in separate, and\nleverages different interleaving or swizzling methods for\nmemory alignment or fast decoding. The corresponding com-\nputing kernel then needs to unpack this specific layout to a\nhardware-supported data type for execution.\nTo provide a unified and scalable solution for mixed-precision\nGEMM, this paper transforms the dominant data-type-centric\ncomputation to bit-wise computation, based on the linear\nequivalent transformation in Eq. 1. For mixed-precision GEMM,\nA and W are the activation and weight matrix, respectively.\nn is the bit-width of the weight. $W_i$ is each bit matrix of W.\n$A \\times W = A \\times (\\sum_{i=0}^{n-1} 2^iW_i) = \\sum_{i=0}^{n-1} 2^iA \\times W_i$ (1)\nIn this way, the diverse weight layouts are reduced to a\nunified one-bit matrix layout. The diverse computing kernels\nare reduced to unified multiplication of the activation matrix"}, {"title": "3.1 T-MAC Algorithm", "content": "Figure 2 and and Alg. 1 shows the T-MAC design. During\nthe offline preparation stage (line 29 to 35), a n-bit weight\nmatrix is decomposed into n one-bit matrices. Since one bit\ncan only represent two values, for a group with g bits, the\npossible permutations are only $2^g$. The permutations can be\nprecomputed with each g-width vector of the activation and\nsaved in a table. A g-bit group in the weight is thus an index\nto look up the table for the precomputed results. Therefore, a\ntable in T-MAC is defined to save the results of a [1, 9]\u00d7[$g$, $2^g$]\nsub-matrix multiplication, and the table size is [1, $2^g$]. During\nthe offline stage, a tile in the one-bit matrix will be saved\ncontinuously in memory to facilitate fast loading. Same as\nthe tiling of normal matrix multiplication, a tiling here is\nalso to improve data locality and cache utilization during\nLUT.\nFor the online stage, given the input activation of a GEMM,\nT-MAC traverses every [1, g] vector of the activation to mul-\ntiply with the [$g$, $2^g$] bit-pattern matrix and build up a table\n(line 16 to 27). During LUT, each index (i.e., group) of the\none-bit weight matrix is used to look up the tables for partial\nresults (line 6-9). The accumulation of the partial results will\nbe the final GEMM results (line 12-14)."}, {"title": "3.2 LUT-Centric Data Layout", "content": "As described in \u00a73.1, the LUT-based method for low-bit\nGEMM requires more memory to store the lookup table\nand the intermediate results. On the other hand, the table\nlook-ups usually lead to memory access inefficiency due to\nthe random memory accesses. To resolve the inefficiency\non memory storage and accesses, we design a LUT-centric\ndata layout for the LUT-based low-bit GEMM. Specifically,\nit stores the lookup table on on-chip memory like registers\nto accelerate table accesses, and designs axis reordering and\ndata tiling to enhance data reuses for reducing memory con-\nsumption. Furthermore, to improve the efficiency, we design"}, {"title": "3.3 Reduce LUT Storage", "content": "In the realm of LUT-based method for low-bit LLM inference,\nthe size of the lookup table is a crucial factor that impacts\nboth the storage requirements and the access latency, espe-\ncially when optimizing table look-ups with on-chip memory\nin \u00a73.2. A larger table size not only demands more memory\nspace, but also leads to slower table access speed. To address\nthis challenge, we introduce two optimization strategies:\nmirror consolidation and table quantization. As illustrated\nin Figure 5, mirror consolidation exploits the symmetrical\nproperties of table values to halve the length of the table,\nwhile table quantization applies quantization techniques to\nthe table values themselves to reduce the width of the table.\nCombined, these methods enable a significant reduction in\nthe storage footprint of the lookup table (up to a quarter of\nits original size) without accuracy loss in the LLM inference."}, {"title": "4 Implementation", "content": "Code generation through TVM. We employ TVM [14] +\nLLVM [24] for code generation. This allows us to generate\noptimal code for GEMM of varying shapes and for different\nhardware, and implement common optimizations such as\nloop unrolling, vectorization, and constant folding. We uti-\nlize TVM Tensorize to embed hardware intrinsics into the\ncode. AutoTVM [15] is used to automatically fine-tune the\ngenerated code for different hardware targets.\nAPI and integration. We provide a consistent API for\nboth C++ and Python. The GEMM functions are encapsulated\ninto TVM PackedFunc, and the tensors can be transferred\nthrough the DLPack [1] tensor structure. This facilitates easy\ninteroperability with other frameworks like PyTorch, Numpy,\nand etc. Additionally, we provide an additional wrapper for\nC++, where tensors can be passed through raw pointers and\nTVM runtime dependency is eliminated. This offers a more\nlightweight solution for integration into other C++ projects.\nParallelism. We utilize the TVM runtime threadpool to\ndynamically assign tasks to CPUs. However, when integrat-\ning T-MAC into llama.cpp, we notice an obvious conflict\nbetween the llama.cpp threadpool and the TVM threadpool.\nThis conflict arises as threads from different threadpools"}, {"title": "5 Evaluation", "content": "We evaluate T-MAC with real-world, low-bit LLMs, specif-\nically Llama and BitNet, across four distinct edge devices.\nOur benchmarking efforts are aimed at a direct comparison\nwith the existing state-of-the-art llama.cpp implementation.\nWe summarize our key findings as follows:\n\u2022 The mpGEMV and mpGEMM kernels of T-MAC show\na marked performance gain, significantly outperform-\ning the state-of-the-art dequantization-based kernels.\n\u2022 T-MAC enables an end-to-end model inference through-\nput improvement of 2-4x, while concurrently reducing\nthe energy consumption by 60%-70% relative to the\noriginal llama.cpp implementation."}, {"title": "5.2 mpGEMV/mpGEMM Performance Benchmark", "content": "We evaluate the kernels in Llama-2-7B/13B across all four\ndevices. As illustrated in Figure 6 with the bits decrease from\n4-bit to 2-bit, llama.cpp fails to gain any additional speedup,\nand even experiences a 15% slowdown at 3-bit compared to\n4-bit due to decoding overhead. Therefore, we can infer that\nthe 1-bit llama.cpp performance would be similar to 2-bit,\neven though llama.cpp does not provide a 1-bit implemen-\ntation. In contrast, T-MAC achieves linear speedup with bit\nreduction. For single-threaded GEMV, T-MAC achieves max-\nimum speedups of 11.2x, 5.8x, 4.7x, and 3.1x respectively for"}, {"title": "5.3 End-to-End Inference Throughput", "content": "After integrating into llama.cpp, we compare the end-to-\nend token generation throughput of llama.cpp with T-MAC.\nWe employ 2-bit to execute BitNet. As depicted in Figure 8,\nunder single-threading on Raspberry Pi 5, T-MAC achieves\nspeedups of 2.8x, 6.7x, and 5.8x for the three models respec-\ntively. Under multi-threading, due to memory constraints\nand operators other than mpGEMV/mpGEMM, the speedup"}, {"title": "5.4 Power and Energy Consumption", "content": "In addition to computational efficiency, energy efficiency is\nequally critical, especially for edge devices that rely on bat-\ntery power. To evaluate the power and energy consumption\nof T-MAC relative to llama.cpp, we conducted experiments\nusing multi-threaded implementations on the M2 Ultra de-\nvice. We selected three models for our analysis: Llama-2-\n7B-4bit, Llama-2-7B-2bit, and BitNet-3B. The power usage\nis measured using powermetrics on OSX, which can record\nthe average power usage over a specified sample interval."}, {"title": "5.5 Optimization Breakdown", "content": "To evaluate the effectiveness of the optimizations in \u00a73, we\nbreak down our optimization strategies. Most of these op-\ntimizations yield greater benefits for single-threading, but\ntiling requires multi-threading to be effective, hence this\nevaluation is conducted using multi-threading. While most"}, {"title": "5.6 Error Analysis", "content": "There are two sources of error compared to conventional\nmpGEMM implementation: (a) table quantization, which is\nan algorithmic approximation included in our method, and\n(b) fast aggregation, whose error is introduced during the\ninstruction execution within the fixed CPU architecture. We\nevaluate the impact of these two error sources at both kernel-\nlevel and model-level."}, {"title": "Kernel-level Evaluation", "content": "We use the unquantized\nWFP16AFP16 GEMV as the benchmark. The weights and ac-\ntivation of the GEMV are randomly generated FP16 values\nfollowing a Gaussian Distribution, which are then quantized"}, {"title": "5.7 Compared with GPU", "content": "GPUs are widely used in LLM deployments. We compare\nT-MAC on CPU with llama.cpp on GPU to illustrate the\nefficiency of T-MAC. llama.cpp is the state-of-the-art LLM\ninference framework on both CPU and GPU."}, {"title": "6 Related Works", "content": "LLM quantization has\nemerged as a crucial technique for the efficient deployment\nof LLMs in resource-constrained environments. A segment\nof the research has been dedicated to the dual quantiza-\ntion of both weights and activations. LLM.int8() [16] iso-\nlates outlier feature dimensions to 16-bit computations while\nprocessing majority dimensions in efficient 8-bit computa-\ntions. SmoothQuant [34] migrates the quantization difficulty\nfrom activations to weights with a mathematically equiva-\nlent transformation to enable an INT8 quantization of both\nweights and activations. Advancements in the field have\nled to a refined focus on the singular quantization of model\nweights, as weight storage accounts for the majority of mem-\nory footprint. Specific algorithms such as GPTQ [20] and\nAWQ [25] have demonstrated the feasibility of quantizing\nLLMs to just 4 bits using post-training quantization tech-\nniques. Furthermore, BitDistiller [18] pushes the boundary\nto 2 bits by leveraging quantization-aware training (QAT)\nand self-distillation. Meanwhile, BitNet [33] takes an even\nmore ambitious route by training 1-bit LLMs from scratch."}, {"title": "LLM Inference System", "content": "The significance of LLMs has\nspurred the development of various LLM inference systems,\ntailored to different platforms and optimized for specific\ngoals. vLLM [23] is a high-throughput and memory-efficient\ninference engine designed for LLMs, which excels in large\nbatch processing. llama.cpp [5] stands out with its plain\nC/C++ implementation, free of external dependencies, which\ndelivers superior performance on edge computing devices.\nTensorRT-LLM [9] incorporates a suite of state-of-the-art\noptimizations specifically for NVIDIA GPUs. Intel Neural\nCompressor [2] provides an open-source Python library for\nmodel compression techniques, tailored to enhance the ef-\nficiency of LLMs within the Intel ecosystem. All of these\ninference systems share a crucial capability for supporting\nlow-bit LLMs, which not only minimizes memory usage but\nalso enhances computational efficiency, thereby broadening\nthe accessibility of LLMs for diverse applications. To com-\nplement the landscape of end-to-end LLM inference systems,\nthere are also efforts concentrated on developing highly effi-\ncient computational kernels tailored for low-bit LLMs [7, 19]."}, {"title": "7 Conclusion", "content": "T-MAC transforms the data-type-centric multiplication to\nbit-wise table lookup, and provide a unified and scalable\nsolution for the increasingly popular mpGEMM. On the per-\nvasively available CPUs of edge devices, T-MAC kernels\nachieve up to 6.6\u00d7 speedup compared to llama.cpp, which\nmakes the CPU inference speed comparable or even higher\nthan the GPU on the same device. T-MAC thus provides a\npractical solution to deploy LLMs on edge devices without\nrelying on GPU, even on a Raspberry Pi. T-MAC also opens\nup the broad opportunity for novel LLM hardware accelera-\ntor design based on LUT, as LUT is much more efficient in\nhardware implementation than multiplications."}]}