{"title": "Radio U-Net: a convolutional neural network to detect diffuse radio sources in galaxy clusters and beyond", "authors": ["C. Stuardi", "C. Gheller", "F. Vazza", "A. Botteon"], "abstract": "The forthcoming generation of radio telescope arrays promises significant advancements in sensitivity and resolution, enabling the identification and characterization of many new faint and diffuse radio sources. Conventional manual cataloging methodologies are anticipated to be insufficient to exploit the capabilities of new radio surveys. Radio interferometric images of diffuse sources present a challenge for image segmentation tasks due to noise, artifacts, and embedded radio sources. In response to these challenges, we introduce Radio U-Net, a fully convolutional neural network based on the U-Net architecture. Radio U-Net is designed to detect faint and extended sources in radio surveys, such as radio halos, relics, and cosmic web filaments. Radio U-Net was trained on synthetic radio observations built upon cosmological simulations and then tested on a sample of galaxy clusters, where the detection of cluster diffuse radio sources relied on customized data reduction and visual inspection of LOFAR Two Metre Sky Survey (LoTSS) data. The 83% of clusters exhibiting diffuse radio emission were accurately identified, and the segmentation successfully recovered the morphology of the sources even in low-quality images. In a test sample comprising 246 galaxy clusters, we achieved a 73% accuracy rate in distinguishing between clusters with and without diffuse radio emission. Our results establish the applicability of Radio U-Net to extensive radio survey datasets, probing its efficiency on cutting-edge high-performance computing systems. This approach represents an advancement in optimizing the exploitation of forthcoming large radio surveys for scientific exploration.", "sections": [{"title": "1 INTRODUCTION", "content": "The backbone of the universe is the cosmic web: an intricate network of filaments interconnected with galaxy clusters. The thermal plasma that fills filaments (the Warm Hot Intergalactic Medium, WHIM) and galaxy clusters (the Intra Cluster Medium, ICM) is permeated by weak magnetic fields (0.01-10 \u03bcG) whose origin and evolution have attracted increasing attention in the scientific community in recent years (see e.g. Vazza et al. 2021a; Carretti et al. 2023). Structure formation processes inject shocks and turbulence into the thermal plasma, accelerating particles to ultra-relativistic energies. Cosmic ray electrons, spinning in the magnetic fields, emit radio synchrotron radiation in the form of diffuse radio sources which are observed in galaxy clusters (van Weeren et al. 2019), or in filaments and bridges within them (Govoni et al. 2019; Botteon et al. 2020c; Vernstrom et al. 2021a). Diffuse radio sources are primary probes for investigating cosmic magnetic fields, particle acceleration mechanisms, and the processes driving the formation of large-scale cosmic structures.\nDiffuse radio sources in galaxy clusters have historically been categorized into giant radio halos, mini-halos, and radio relics based on their location in the cluster, morphology, size, polarization, and spectral characteristics (Feretti et al. 2012) However, the boundaries between the different classes are not always clear-cut. For instance, the discovery of Mpc-size sources in clusters hosting mini-halos suggests the presence of an intermediate stage between mini-halos and giant radio halos (e.g., Kale et al. 2019; Savini et al. 2019; Biava et al. 2021), leading to the grouping of these two classes into the common category of radio halos. Furthermore, diffuse radio sources often have unique morphologies (e.g., de Gasperin et al. 2017; Botteon et al. 2020b), and projection effects may cause the overlap of different sources along the line-of-sight (e.g., Rajpurohit et al. 2020). Sometimes diffuse radio sources are connected with radio galaxies or revived fossil plasma sources, also known as phoenices (see Mandal et al. 2020 and Raja"}, {"title": "2 THE CONVOLUTIONAL NEURAL NETWORK: RADIO U-NET", "content": "We have adapted the original U-Net architecture (Ronneberger et al. 2015) to our problem. Fig. 1 shows an illustrative representation of the Radio U-Net network. Radio U-Net is an autoencoder consisting of a contractive and an expansive path. The contractive path is composed of a downsampling convolutional network which starts from an input layer, loading the input images, and ends with the deepest convolutional layers. The expansive path starts from this deepest level, followed by an upsampling convolutional network, specular to the contractive one. At each level, the feature maps are also summed to preserve spatial information. The final output layer returns the results. In our case, the results are represented by the segmented images, of the same size as the input ones, identifying the presence of diffuse radio emission.\nA 3 \u00d7 3 pixels window (receptive window) is used for the convolution, applying it starting from the input images:\n$s_{m,n}^f = \\sum_{i,j=-1}^{1} w_{i,j}^f x_{m+i,n+j} + b^f,$\nwhere $x_{m,n}$ is the (m, n) pixel of the input image, $w_{i,j}^f$, and $b^f$ are the weights and the biases of the convolutional kernel. The result of the convolution, the $s_{m,n}$ element, constitutes the f-th feature map. Starting from the random initialization of the weights and the biases several different feature maps are created. Non-linearity is introduced by further processing the $s_{m,n}$ elements with an activation function. We have adopted the ReLU activation function (Agarap 2018):\n$t_{mn} = max(0, s_{m,n}).$\nIn each level, the convolution plus activation steps are repeated two times. Batch normalization has been used after each convolution step to improve the convergence of the method. At this point, each feature map is downscaled through a max pooling function, which selects the maximum $t_{i,m,n}$ every 2\u00d72 pool of feature map elements. The resulting maps are 1/4 the original size. A dropout layer (with a rate of 0.5) is inserted after each max pooling function to prevent overfitting. The same convolution plus pooling procedure is repeated four times until the deepest layer is reached.\nTranspose convolutions with ReLU activation and upsampling layers are alternated in the expansive path, while dropout layers are not used. After five upsampling layers, specular to the contractive ones, the network produces 32 feature maps of the same size as the input image. A final convolution with softmax activation function provides two feature maps that represent, respectively, the probability of each pixel to be 0 or 1. The output is the probability map showing the probability of the (m, n) pixel to be part of the diffuse radio emission:\n$P_{m,n}=\\frac{e^{x_{m,n}^1}}{e^{x_{m,n}^0} + e^{x_{m,n}^1}}$\nDuring the training, weights and biases are optimized with an iterative procedure. Two sets of images are needed for the training: the input and the reference images. For the initial training of our network, we used as input synthetic observations of diffuse radio sources described in Sec. 4. The reference images are masks, displaying unity in pixels where emission from the diffuse radio source is present and zero otherwise. The input images are processed by the network and the segmented images are compared to the corresponding reference images using a loss function. We used the categorical cross-entropy loss function, defined as:\n$H(y_{m,n}, P_{m,n}) = -\\sum_{f=1}^{N_c} y_{m,n}log(p_{m,n}),$\nwhere $y_{m,n}$ is the pixel value in the reference image (0 or 1) and $N_c$ is the total number of classes to classify. For our application $N_c = 2$, that is, a pixel belongs or not to a diffuse radio source. The training aims at minimizing H by updating the weights and biases with the backpropagation of the cross-entropy error estimate. This is done using the RMSprop algorithm as an optimizer.\nIn addition to the trainable parameters (weights and biases), the network includes additional hyperparameters used to control the learning process. For our network, we selected the hyperparameters that demonstrated the best performance in (Sanvitale et al. 2022), ensuring their suitability for our purposes. They are:"}, {"title": "3 LOFAR OBSERVATIONS", "content": "We developed Radio U-Net to detect diffuse radio sources in the LOFAR Two-meter Sky Survey (LoTSS, Shimwell et al. 2017, 2019). This is an ongoing survey that aims to cover the entire northern sky with the LOFAR High-Band Antenna (HBA) at frequencies from 120 to 168 MHz (150 MHz central frequency). The second data release (DR2) of the LoTSS is fully described in Shimwell et al. (2022). The LoTSS DR2 covers 27% of the northern sky (5634 deg\u00b2) and consists of images at 6\" resolution with a median root-mean-square (rms) sensitivity of 83 \u00b5Jy/beam. Released low-resolution images have 20\" restoring beam and median rms sensitivity of 95 \u00b5Jy/beam.\nTo test Radio U-Net on real data, we applied the network to a sample of galaxy clusters for which the search and classification of diffuse radio sources was made by tailored data reduction and visual inspection. These are the 309 clusters in the second catalog of Planck Sunyaev Zel'dovich (PSZ2 Planck Collaboration et al. 2016) that lie within the LoTSS-DR2 area (hereafter, the LoTSS-DR2/PSZ2 sample), presented in Botteon et al. (2022a). We used their findings as a benchmark to test our approach (see Sec. 6).\nWe note, however, that we did not use the images produced by Botteon et al. (2022a). Instead, we used the public LoTSS DR2 data at 20\" resolution, directly downloaded from the archives. Our aim is to test the segmentation and detection capability of Radio U-Net on basic archival data. If good performance is achieved, this approach will significantly reduce the computational time required for detection, which is typically performed on post-processed data."}, {"title": "4 SYNTHETIC OBSERVATIONS", "content": "We trained Radio U-Net on synthetic observations of diffuse radio emission. Input images have been generated to be the closest possible to actual LOFAR HBA observations, using the same procedure described in Gheller & Vazza (2022b). The images are already publicly available (https://owncloud.ia2.inaf.it/index.php/s/IbFP1CCcPUresrr). Here we summarize the main steps of the procedure.\nWe used magneto-hydrodynamical (MHD) cosmological simulations, produced with the grid code Enzo (Bryan et al. 2014). The simulation box has a uniform and constant spatial cell resolution of 41.65 kpc (comoving) and covers a volume of 1003 Mpc\u00b3 simulated with 24003 dark matter particles and cells. A uniform primordial magnetic seed field of Bo = 0.1 nG (comoving) was initialized in all directions at zin = 45, and was evolved assuming ideal MHD, via adiabiatic compression/rarefaction and small-scale dynamo amplification in halos, until z = 0. The simulation assumed a standard ACDM cosmological model, with density parameters \u03a9\u044c = 0.0478, ODM = 0.2602, \u03a9\u039b = 0.692, and a Hubble parameter Ho = 67.8 km/s/Mpc.\nWe computed the emission at 150 MHz from relativistic electrons accelerated by cosmic shocks at various redshifts, assuming that only shocks can accelerate relativistic particles via diffusive shock acceleration, and produce synchrotron radio emission(e.g. Brown 2011). For simplicity, the synchrotron emission model by Hoeft & Br\u00fcggen (2007) has been assumed here, which requires the jump condition of each cell undergoing shocks (computed from the simulation), the local value of the magnetic field, and the electron acceleration efficiency as a function of Mach number (which is calibrated on shocks internal to galaxy clusters, as in Vazza et al. 2015 and Vazza et al. 2019). We did not include in our simulations radio emission produced by turbulence, as is expected from radio halos, bridges, or mega halos (Brunetti 2011; Brunetti & Vazza 2020; Nishiwaki et al. 2024), as well as by electrons re-accelerated by shocks. However, the morphology and emissivity of diffuse radio sources can loosely resemble even those of radio halos, despite the different particle acceleration mechanisms likely at work in radio halos (e.g., van Weeren et al. 2019; Lee et al. 2023). This same simulation and model for radio emission was successfully used to compare with the result of stacking attempts of the cosmic web with real radio data (Vernstrom et al. 2021b, 2023b), and therefore it can be considered as a fairly realistic representation of how the radio emitting cosmic web might look like.\nThe production of the training set of synthetic images for Radio U-Net, starting from the 3-dimensional emission model of the cosmological simulations, followed two main basic steps.\nFirst, we integrated the emission along the line of sight within four comoving volumes chosen at four different redshift snapshots (roughly equally spaced from z = 0.02 to z = 0.15). An example of the maps of some of the redshift snapshots used is given in Fig. 2 in Gheller et al. (2018). We applied cosmological corrections for"}, {"title": "5 TRAINING STRATEGY AND VALIDATION OF THE NETWORK", "content": "Training and validation of Radio U-Net were performed on synthetic observations. The primary objective of this training is to enable the network to identify a wide range of cosmological source morphologies and to distinguish real sources from imaging artifacts. Additionally, the reference masks used in the training were created with a threshold lower than the actual noise level of the input clean images, allowing the network to recognize faint patterns even below the noise.\nThe 2000 x 2000 pixels images are further divided into square tiles that become the actual training set of the network. The tile size is chosen to be the smallest possible, still representative of the features to be identified (both sources and artifacts, in particular, those due to the dirty beam, which can span large areas of the image). In this way we maximize the size of the training sets without losing the significance of each single input image, reducing, at the same time, the memory footprint of the training. An effective tile size for our images results to be 192\u00d7192 pixels so that from each image we can create 100 tiles.\nDue to GPU memory constraints, the training set comprises 100 sky images, i.e 10000 tiles. A fraction of them (5%, that is, 500 tiles) is kept as a validation set to verify the performance of the network during training. Another 10 randomly selected images (i.e. 100 tiles), never seen by the network, are used as a test set.\nThe workflow is implemented as follows:\n(i) the training program reads the input parameters, and the hyperparameters and sets up the network;\n(ii) training images (input clean images and reference masked images) are read from FITS files, and stored on disk;\n(iii) the data are transformed in logarithmic scale and normalized in a [0,1] range, using 10-10 Jy/pixel and 10-5 Jy/pixel as minimum and maximum data normalization values. Pixels outside this range were clipped to the boundaries of the normalization range;\n(iv) images are divided into tiles of 192\u00d7192 pixels;\n(v) tiles are serialized to feed the network;\n(vi) mini-batches of tiles are offloaded to the GPU and processed for the training;\n(vii) at each epoch, the loss function value is computed for the training and the validation sample;\n(viii) the trained network is saved in a file (in \"SavedModel\" Tensorflow format).\nWeights and biases of the trained network can be loaded by the evaluation program which performs the segmentation of new images and saves the result into a FITS file. The only significant limitation of the tiling procedure is the potential for minor misalignments at tile borders, which may compromise the accuracy of segmentation in the reconstructed images. To mitigate this issue, when Radio U-Net is used, tiles are created with overlapping boundaries, which are then discarded in the reconstruction. The tiles are overlapped by half their size (96 pixels) and only the central 48x48 pixels of each tile are then used to recompose the final image. An explicative scheme of this procedure is shown in Fig. 3. Residual boundary effects are still visible in the probability image but they do not affect the detection of the diffuse emission.\nExamples of the resulting segmented images from the test set are shown in Fig. 2, third column. This probability image has to be compared with the sky image, in the first column, where the contours show the reference mask on which the network was trained,"}, {"title": "6 RESULTS", "content": "Having trained Radio U-Net on a large sample of synthetic observations, we can now use it to process real LOFAR observations. We will use as a reference the LoTSS-DR2/PSZ2 cluster sample, introduced in Sec. 3, to test the classification performances of the network."}, {"title": "6.1 Application to the LoTSS-DR2/PSZ2 cluster sample", "content": "Low-resolution (20\") images of the 309 clusters in the sample were downloaded from the LoTSS archive. For each cluster, we created a cut-out of 960\u00d7960 pixels centered on the cluster position, ensuring the target was as close as possible to the pointing center of the mosaic. The cut-out size was selected to be as large as possible, ensuring that it does not exceed too much the boundary of the LOFAR primary beam, and it is also a multiple of 192, which remains the tile size for the network.\nThe pixel size is 4.5\", hence the cutout size is 1.2\u00b0 x 1.2\u00b0. Redshift, available for all but 28 clusters, spans between 0.016 and 0.9, with a median value of 0.28 (Botteon et al. 2022a). By adopting the same cosmology model (i.e., A cold dark matter, with \u03a9 = 0.7, \u03a9m = 0.3, and Ho = 70 km s\u00af\u00b9 Mpc\u00af\u00b9), 1 Mpc corresponds to 682 pixels at z = 0.016 and to 28 pixels at z = 0.9. This means that, if present, cluster-scale radio emission should be contained in the cut-out and it can span a broad range of angular extensions.\nWe processed the 309 images with Radio U-Net using a slightly modified version of the evaluation program which allows us to preserve the astrometric information contained in the header. As in step (iii) of the training procedure described in Sec. 5, we converted the data in logarithmic scale and we normalized between 10-7 and 10-2 Jy/beam. We thus obtained probability images for all the galaxy clusters.\nWe show some examples of LoTSS-DR2/PSZ2 clusters with the corresponding output generated by Radio U-Net in Fig. 4 and Fig. 5. In particular, Fig. 4 shows examples of clusters where the direct application of Radio U-Net gave very good segmentation. PSZ2 G055.80+32.90 is a galaxy cluster with no signs of diffuse radio emission; indeed, very few pixels show a probability greater than 0.5 of being part of a diffuse source. In contrast, PSZ2 G055.59+31.85 hosts a radio halo, and its shape is clearly recovered by the network. Notably, in many cases, although Radio U-Net is applied to images with the contaminant emission from compact sources and extended radio galaxies, it successfully recovers the morphology of the emission that was only identifiable in lower-resolution source-subtracted images in Botteon et al. (2022a). PSZ2 G057.61+34.93 hosts a radio relic (a.k.a. the Cornetto relic, Locatelli et al. 2020) and a candidate radio halo, which was classified as uncertain due to the presence of a bright radio source that generates residual artifacts in the halo region. Since Radio U-Net was trained to uncover diffuse radio emission even beneath residual imaging artifacts, the probability map obtained for this cluster allows us to confirm the presence of the central radio halo.\nFig. 5 shows examples of poorly segmented images. PSZ2 G056.14+28.06, which does not host diffuse radio emission according to Botteon et al. (2022a), appears to host a central radio halo in the Radio U-Net probability map. On the contrary, PSZ2 G081.02+50.57, which hosts a radio halo visible only in lower-resolution images, is completely missed by Radio U-Net. Additionally, the bright and extended radio galaxy at the center of PSZ2 G031.93+78.71 dominates the probability map of this cluster, obscuring the emission of the halo. The fact that Radio U-Net also detects extended radio galaxies indicates its ability to generalize and suggests its potential use in searching for such objects. However, this also highlights the need to develop better strategies to distinguish between different types of extended radio emissions."}, {"title": "6.2 Classification accuracy", "content": "Botteon et al. (2022a) classified the radio sources detected in each cluster as radio halo (RH), radio relic (RR), candidate radio halo or relic (CRH, cRR, if no X-ray information was available for the cluster), and uncertain (U) when the image was significantly affected by calibration artifacts or the radio emission did not show a morphology, size, and/or position consistent with the previous categories. If no diffuse emission was detected, the cluster is classified as NDE, while in case of poor data quality, the classification is not applicable (N/A). Hereafter, we will use the same classification scheme, but we will also use the umbrella term diffuse emission (DE) to encompass RH, RR, CRH, and cRR, since we are not interested in distinguishing different kinds of cluster diffuse emission at the moment. Clusters with both DE and/or uncertain sources are classified as DEU.\nFor testing purposes, we selected all NDE (114), DE (85), and DEU (47) clusters having redshift estimates. This will be our initial test set comprising a total of 246 galaxy clusters (see Tab. 1). This sub-sample is well balanced, having similar numbers of positive (DE and DEU) and negative (NDE) instances. To claim a detection, i.e. classify a cluster as positive or negative, we considered the sum probability of all the pixels within the cluster area which we defined as a circle with radius 2.2R500 (where R500 for each cluster is provided by Botteon et al. 2022a). This is the maximum distance at which radio relics were detected in this sample (Jones et al. 2023). We define R as the ratio of the sum probability and the total number of pixels (N) within the circle:\n$R = \\frac{1}{N} \\sum_{m,n} p_{m,n}$\nR is a proxy for the extension of the source within the cluster: it approaches 0 when no source is detected while it is 1 when a diffuse source occupies the entire circle and has all pixels detected with probability 1.\nIn Fig. 6 we show the distribution of R for NDE, DE, and DEU clusters and their median values. Clearly, the distributions are different for clusters with and without detections, with the peak of the distribution being lower for NDE clusters. We note that the distribution of DE and DEU clusters is instead similar and they can be drawn from the same distribution according to the two-sample Kolmogorov-Smirnov (KS) test with a confidence level of 95%. This indicates that Radio U-Net recognizes diffuse radio emission regardless of the specific classification. However, it is not straightforward to define a threshold value for R to separate NDE from DE and DEU clusters.\nIn binary classification problems, each object belongs to a class and is labeled as positive or negative. The model assigns a predicted label to each object, which can be true if the classification is correct or false if it is incorrect. In our case, the predicted label depends on the threshold set on R to distinguish between galaxy clusters with detected (positive) and non-detected (negative) diffuse radio sources. Consequently, the total number of true classifications (TP and true negative, TN) and false classifications (FP and FN) also depends on R. To choose the optimal threshold for R, we calculated various standard performance metrics as a function of R.\nThe accuracy is defined as the fraction of correct classifications to the total number of objects:\n$accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\nThis is an important metric but can be misleading in the case"}, {"title": "6.3 Fine-tuning and data augmentation", "content": "Considering that Radio U-Net was trained exclusively on synthetic observations, achieving a detection accuracy of 73% is a significant result. However, to enhance the network's performance, we have explored transfer learning using LoTSS observations. This procedure, known as fine-tuning, can be applied to various types of neural networks, including U-Net (Amiri et al. 2020). A widely used approach involves transferring the weights directly from the pre-"}, {"title": "7 DISCUSSION", "content": "In the previous Section, we applied Radio U-Net on a labeled sample of galaxy clusters and we discussed the detection statistics. Here, we focus on the investigation of the properties of wrong predictions, both false positives and false negatives, to understand in what circumstances the network fails. We then discuss the results. In the next Sections, we will refer to the results obtained on the initial test set, where we directly applied the network trained on synthetic observations."}, {"title": "7.1 False negative: undetected sources", "content": "Among the galaxy clusters hosting diffuse radio emission (DE and DEU), 23 over 132 are undetected. For the majority of them, R500 has a small angular extent, both due to the small mass and/or high redshift. An example is represented by the right panel in Fig. 5.\nMost false negative galaxy clusters show a region at the cluster's center with high detection probability, but the R-value is too low to classify them as detected. This suggests that the performance of the network can be increased by improving our classification method, for instance, by adding a classification layer to the network.\nIt is interesting to note that not even one radio relic hosting system was missed, the sources in the false negative clusters being cataloged as RH (6), cRH (9), and U (8). This suggests that, by including radio halos in our simulations, in particular in low mass and high redshift systems, could potentially allow us to improve the model accuracy."}, {"title": "7.2 False positive detections", "content": "Considering the initial test set, 43 galaxy clusters classified as NDE in Botteon et al. (2022a), host diffuse radio emission according to their R value. Clusters with false positive and true negative detections have similar distributions of redshift and angular extent of R500.\nMost of the false detections are connected with the presence of radio galaxies with diffuse emission within the cluster, or by the convolution of radio emission from different radio galaxies (see left and central panel of Fig. 5). This is probably a consequence of the fact that the radio emission arising from galaxies, both extended and compact, is not present in the synthetic observations used for the training. However, the fact that the network identifies different kinds of diffuse radio emission is also encouraging because it means that it can generalize to sources never seen before.\nAmong the false positive detections, we also noticed some galaxy clusters for which the interesting region found by Radio U-Net is not restricted to extended radio galaxies. Two examples are shown in the left and central panel of Fig. 8. In these cases, the regions spotted by the network are extended, covering a large fraction of the cluster area, and their morphology resembles that of radio halos. We speculate that a deeper observation with tailored data reduction and imaging procedures could lead to detect a diffuse radio emission in these systems. In particular, according to Cuciti et al. (2022), PSZ2 G073.97-27.82 is one of the galaxy clusters in the LoTSS-DR2/PSZ2 sample having the highest chance to host a mega-halo, and indeed the emission that we detect surpasses R500. Another interesting case is PSZ2 G080.16+57.65 (see right panel of Fig. 8), where Radio U-Net spotted both a relic and the central radio halo which was detected at lower resolution in van Weeren et al. (2021) but not confirmed in Botteon et al. (2022a). This confirms that Radio U-Net can recover diffuse radio emission below the sensitivity limits of the observations."}, {"title": "7.3 Low-quality images", "content": "Botteon et al. (2022a) excluded from their analysis 31 galaxy clusters (marked as N/A) due to strong calibration artifacts or very high rms noise levels. Since the synthetic images used for the training include high noise levels and artifacts, Radio U-Net is expected to be effective for these low-quality images.\nFig. 9 shows three examples in which Radio U-Net achieves a proper segmentation of these low-quality images. The two clusters in the left and central panels are known to host radio halos thanks to previous targeted observations (Giacintucci et al. 2014; Botteon et al. 2019) and are indeed detected by the network. The network gives instead a low probability of detection to pixels that are bright only due to imaging artifacts, such as in the right image, where the segmented map does not contain any relevant detection. These images provide additional evidence of the capability of Radio U-Net to quickly identify and select interesting targets, even in low-quality datasets, thereby identifying the fields that require more customized and time-intensive data analysis."}, {"title": "8 CONCLUSIONS", "content": "Radio interferometric images, derived from advanced image processing techniques, pose significant challenges for any data analysis solution due to the presence of both random noise and artifacts with vastly different statistical and morphological properties. Diffuse radio sources, such as radio halos and radio relics, are rare and particularly challenging to detect because of their low surface brightness and smooth boundaries. Their detection often requires computationally expensive procedures such as source subtraction. The anticipated massive data volumes in the coming decade from observatories like the SKA will necessitate software tools capable of effectively leveraging HPC solutions and achieving full automation, as interactive human intervention and supervision will become impractical. Machine learning solutions appear to be ideally suited to meet these requirements.\nWe have explored the use of a fully convolutional neural network to segment radio images and detect diffuse radio sources in large-area surveys. We trained a modified version of the classic U-Net architecture, named Radio U-Net, on synthetic LOFAR HBA observations of diffuse radio emission based on cosmological simulations. The network creates segmented images of the same size as the input image, assigning to each pixel a probability of being part of a diffuse radio source. The segmentation can be used to claim the"}]}