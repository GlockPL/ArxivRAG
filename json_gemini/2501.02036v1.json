{"title": "DEEP CLUSTERING VIA COMMUNITY DETECTION", "authors": ["Tianyu Cheng", "Qun Chen"], "abstract": "Deep clustering is an essential task in modern artificial intelligence, aiming to partition a set of data\nsamples into a given number of homogeneous groups (i.e., clusters). Even though many Deep Neural\nNetwork (DNN) backbones and clustering strategies have been proposed for the task, achieving\nincreasingly improved performance, deep clustering remains very challenging due to the lack of\naccurately labeled samples. In this paper, we propose a novel approach of deep clustering via\ncommunity detection. It initializes clustering by detecting many communities, and then gradually\nexpands clusters by community merging. Compared with the existing clustering strategies, community\ndetection factors in the new perspective of cluster network analysis. As a result, it has the inherent\nbenefit of high pseudo-label purity, which is critical to the performance of self-supervision. We\nhave validated the efficacy of the proposed approach on benchmark image datasets. Our extensive\nexperiments have shown that it can effectively improve the SOTA performance. Our ablation study\nalso demonstrates that the new network perspective can effectively improve community pseudo-label\npurity, resulting in improved clustering performance.", "sections": [{"title": "Introduction", "content": "Due to the success of deep learning, deep clustering has attracted extensive attention from the research community\nin recent years. The existing approaches for deep clustering can be broadly classified into two categories: two-stage\nsolutions[1][2] and single-stage ones [3][4][5][6][7]. The two-stage solutions typically alternate between a feature\nrepresentation learning stage and a clustering assignment stage. These two stages are usually designed to be separate but\ncomplementary. Most of recent work focused on designing increasingly advanced DNN backbones for representation\nlearning and their training strategies. In contrast, the single-stage solutions choose to jointly learn feature representation\nand clustering assignment within an end-to-end framework.\nIt has been well recognized that both two-stage and single-stage solutions have their own advantages and disadvantages.\nBy separating the stages of representation learning and cluster assignment, the two-stage solutions can easily leverage\na wide array of existing techniques on either of them. They also usually require less training cost and can be more\neasily implemented. In comparison, the single-stage solutions can more effectively align representation learning with\nclustering objectives, achieving improved performance in some applications. However, it is noteworthy that in both\ntwo-stage and single-stage solutions, the efficacy of self-supervision remains as the core challenge of deep clustering[8].\nWe have observed that the existing clustering strategies often rely on minimizing intra-cluster distance and maximizing\ninter-cluster disparity through specific objective functions [8]. In the two-stage solutions, the popular clustering\nobjectives include the K-means loss and maximum mutual information loss[9]. In the single-stage solutions, clustering\nstrategies are instead implicitly encoded as contrastive losses. These clustering strategies primarily focus on overall\ncluster distributions. Unfortunately, they often group negative samples into a cluster, whose aggregation can adversely\naffect the performance of self-supervised learning[10].\nIn this paper, we aim to improve the performance of self-supervised learning by enhancing pseudo-label purity. We\npropose a novel two-stage approach for deep clustering based on the idea of community detection, which has been"}, {"title": "2 Related Work", "content": "Traditional clustering algorithms are often designed for low-dimensional class vector data and may perform poorly\nfor complex high-dimensional data, such as images[15][16][17]. In recent years, deep clustering has emerged as a\npromising approach and attracted extensive research attention [8].\nThe typical roadmap for deep clustering is two-stage, alternating between feature representation learning and cluster\nassignment. It leverages the representation learning capabilities of deep neural networks to transform complex data\ninto low-dimensional feature representations, upon which clustering objectives can be applied to generate the final\nclusters [3, 18]. For instance, Van Gansbeke et al. [2] proposed the SCAN method, which first performs contrastive\nlearning to mine nearest neighbors and then optimizes learning and clustering in a second stage to obtain clustering\nresults. To extend SCAN, Dang et al. [1] proposed NNM, which matches samples with their nearest neighbors at both\nlocal and global levels.\nTo improve representation learning, some two-stage solutions have adopted an over-clustering strategy followed by\ngradual merging. For instance, the Deep Adaptive Clustering method proposed by Chang et al. [19] implements a more\nflexible cluster merging mechanism through dynamic adaptive adjustment of features and clusters, preserving more\nsemantic information in the feature space. Similarly, Huang et al. [20] proposed a method of deep semantic clustering\nby partition confidence maximization for image clustering, ensuring that semantically similar clusters are prioritized for\nmerging. These studies demonstrate the efficacy of over-clustering strategies on deep clustering.\nA lot of effort has also been devoted to designing increasingly advanced DNN backbones for single-stage deep clustering.\nFor instance, Yang et al. [21] proposed the Deep Clustering Network (DCN), which clusters latent features produced\nby an autoencoder using K-means, jointly minimizing reconstruction loss and clustering loss. Xie et al. [22] used a\npretrained autoencoder and iteratively improved clustering through high-confidence KL-divergence clustering loss\nassignments. Yang et al. [23] proposed JULE, which combines an agglomerative clustering process with deep learning\nin a recurrent framework. Guo et al. [6] developed the Improved Deep Embedded Clustering (IDEC) method, which\noptimizes both clustering label assignments and feature representations, considering the local structure of the data\ndistribution. Dizaji et al. [24] incorporated cross-entropy loss and regularization (based on prior knowledge about\ncluster size) into deep clustering. Huang et al. [20] proposed a deep clustering method called Partition Confidence\nMaximization (PICA), which seeks to maximize the global partition confidence of clustering solutions.\nOrthogonal to the two-stage and single-stage work, the research community has investigated graph-based deep clustering,\nwhich aims to leverage the underlying structure information of data for improved performance [25][26][27]. For\ninstance, Chiang et al. [25] proposed a fast and memory-efficient deep clustering method based on Graph Convolutional\nNetworks (GCNs). Bo and Wang [26] developed the Structural Deep Clustering Network (SDCN), combining GCNs\nwith the DEC framework to integrate structural information into deep clustering. Peng et al. [27] proposed the Attention-\ndriven Graph Clustering Network (AGCN), which dynamically aggregates node attribute features and topological\nfeatures, and adaptively fuses multi-scale features embedded in different layers. Huang et al. [28] proposed an\ninnovative deep image clustering framework, DeepCluE, which integrates feature representations from multiple network\nlayers, overcoming the limitations of traditional methods that mainly rely on single-level features, thus significantly\nenhancing clustering performance.\nIt is worthy to point out that the existing work on graph-based deep clustering primarily focus on leveraging\ngraph structure to improve representation learning; the existing work on over-clustering strategies primarily\nfocus on improving semantic representation capability in self-supervision. Unlike these existing work, our\nproposed approach of deep clustering via community detection introduces the new perspective of complex\nnetwork analysis into clustering strategies."}, {"title": "3 Methodology", "content": "As shown in Figure 1, our proposed solution of DCvCD uses the classical spectral clustering method, DivCluster[29],\nfor initial clustering. Leveraging the well-fine-tuned backbone, ResNet50, DivCluster uses a diversity control strategy\nto ensure feature richness and consistency, and integrates multiple clustering heads to capture sample structures. Initial\nclustering divides samples into n clusters, where n denotes the specified number of categories. After initial clustering,\neach cluster represents a category, and there are no isolated samples."}, {"title": "Community Detection.", "content": "To enable community detection, we construct a network of samples based on their representation\nsimilarity. Specifically, we connect two samples within a cluster by an edge if and only if their representation similarity\nexceeds a threshold. Then, we apply the classical Louvain community detection algorithm on each cluster to group\nsamples into approximate communities. It is noteworthy that the Louvain algorithm emphasizes community purity.\nTherefore, as visualized in Figure 2, unlike the previous over-clustering approaches, which usually construct only a\nfew more clusters than the specified categories, the result of the Louvain algorithm usually consists of only a few large\ncommunities and much more numerous small communities.\nTechnically speaking, the Louvain algorithm identifies communities by maximizing modularity. As a measure of\nnetwork partition quality, modularity reflects connection tightness within communities as well as connection sparseness\nbetween communities. Formally, the metric of modularity Q can be defined as:\n$Q = \\frac{1}{2m}\\sum_{i,j} [I_{ij} - \\frac{k_ik_j}{2m}]\\delta(c_i, c_j)$ (1)\nwhere m denotes the total number of edges in a network; $I_{ij}$ indicates the connection between node i and node j,\n$I_{ij}$=1 if there is an edge between them, and $I_{ij}$=0 otherwise; $k_i$ and $k_j$ represent the degrees of node i and node j,\nrespectively; $\\delta(c_i, c_j)$ serves as a community indicator, $\\delta(c_i, c_j)$=1 if nodes i and j belong to the same community, and\n$\\delta(c_i, c_j)$=0 otherwise."}, {"title": "Community Merging.", "content": "To effectively reduce the accumulation of pseudo-label errors during the iterative process of\ncommunity merging, we present an ensemble metric, consisting of both network structural indicators and traditional\ndistance indicators, to optimize the merging of isolated communities with main communities.\nIntuitively speaking, our solution aims to ensure that the merging operation maintains community structure consistency\nwhile minimizing the negative impact of misclassification. Towards this aim, our merging strategy not only considers\nthe representation similarity between communities, but also incorporates network structural metrics such as modularity"}, {"title": "increment and average degree change to ensure the structural coherence of merged communities. Specifically, we define\nthe guiding ensemble metric by", "content": "$L = \\frac{\\Delta Q}{max(\\Delta Q)} + \\frac{\\Delta k}{max(\\Delta k)} + \\frac{t}{max(t)},$ (2)\nwhere L denotes the ensemble score, $\\Delta Q$ denotes modularity increment after community merging, $\\Delta k$ denotes average\ndegree change, and t denotes the measured distance between two communities. We elaborate each measure as follows:"}, {"title": "Community Distance t.", "content": "As usual, our ensemble metric also uses the node distances between communities to\nassess positional similarity in the feature space, aiming to avoid the merging of communities that are far apart.\nSuppose d(xi, yj) represents the Euclidean distance between a node $x_i$ in an isolated community,a, and a\nnode yj in a main community, A, then t is defined as:\n$t= \\frac{1}{|a| \\cdot |A|} \\sum_{i \\in a} \\sum_{j \\in A} d(x_i, y_j),$ (3)\nwhere d(xi, yj) denotes the Euclidean distance between two nodes. It can be observed that a smaller t value\nindicates that the nodes are closer in feature space, which helps to avoid structural instability caused by\nlong-distance merging.\nIn each iteration, for each main community, A, we select an isolated community with the highest L metric value w.r.t\nA and merge it with A. After community merging, we re-train the RL backbone on the updated main communities,\nre-construct a network on the nodes in isolated communities, and re-execute the Louvain algorithm. Then, we invoke a\nnew round of community merging. Note that for modularity and average degree estimation, our solution adds an edge\nbetween a node in an isolated community and a node in a main community if their representation similarity exceeds a\nthreshold."}, {"title": "4 Empirical Evaluation", "content": "In this section, we empirically evaluate the performance of our proposed approach on benchmark image datasets.\nSubsection 5.1 describes the experimental setup. Subsection 5.2 presents the comparative evaluation results. Subsection\n5.3 presents the results of our ablation study. Finally, Subsection 5.4 presents the sensitivity evaluation results of key\nparameters."}, {"title": "4.1 Experimental Setup", "content": "We have conducted experiments using four benchmark image datasets: CIFAR-10[30], CIFAR-100[30], STL-10[31],\nImageNet-10[32], whose brief descriptions are as follows:\n\u2022 CIFAR-10: Containing 60,000 color images in 10 classes, CIFAR-10 is widely used for image clustering and\nclassification to evaluate the performance of deep learning solutions.\n\u2022 CIFAR-100: Containing 20 superclasses and 100 subclasses, CIFAR-100 is an extension of CIFAR-10. Due to\nits greater number of categories, CIFAR-100 is more challenging, often used for the evaluation of fine-grained\nclustering and classification.\n\u2022 STL-10: Containing 13,000 images across 10 classes, STL-10 is usually used for the evaluation on self-\nsupervised learning, semi-supervised learning and transfer learning due to its large number of unlabeled\nsamples.\n\u2022 ImageNet-10: As a subset of ImageNet, ImageNet-10 contains 13,000 color images from 10 classes. ImageNet-\n10 is usually used for quickly evaluating image classification performance without requiring full training on\nthe large ImageNet dataset."}, {"title": "4.2 Comparative Evaluation", "content": "The detailed comparative results have been reported in Table 1. It can be observed that our proposed approach of\nDCvCD achieves highly competitive performance across four datasets. Notably, compared to the recent DeepCluE,\nDCvCD shows considerable improvement across all metrics, particularly on the ARI metric, where DCvCD outperforms\nDeepCluE by more than 5% on both CIFAR-10 and CIFAR-100. Similarly, DCvCD consistently outperforms SeCu,\nwhich is the second best approach based on the reported results. On STL-10, the improvement margins of DCvCD over\nSeCu are considerable, more than 10% in terms of NMI and ARI and around 7% in terms of ACC.\nIt is interesting to point out that the overall improvement margins of DCvCD are smaller on CIFAR-10 and CIFAR-100\nthan on STL-10 and ImageNet-10. This is primarily due to the more subtle differences in images within the two CIFAR\ndatasets, which make accurate clustering more challenging. Overall, the evaluation results clearly demonstrate that\nDCvCD, which introduces the new perspective of network analysis into its clustering strategy, can effectively improve\nthe performance of image clustering."}, {"title": "4.3 Ablation Studies", "content": "In this section, we present the evaluation results of our ablation studies. We first evaluate the effect of different metrics\nused for community merging on clustering performance. Then, to demonstrate the efficacy of community detection, we\nalso evaluate the performance of DCvCD built with another clustering approach of SC [34], which is used for initial\nclustering."}, {"title": "4.3.1 Impact of Different Metrics in Community Merging", "content": "The detailed evaluation results have been presented in Table 2. It can be observed that:\n\u2022 DCvCD performs considerably better than the baseline DivClust;\n\u2022 If only using the distance metric, the performance of DCvCD is considerably worse than using the ensemble\nmetric;\n\u2022 Given the modularity metric, subsequently adding the metrics of average degree and distance can effectively\nimprove clustering performance. The ensemble score consisting of all the three metrics consistently achieves\nthe best performance on all the test datasets.\nThese observations clearly demonstrate that the new perspective of network analysis can effectively improve the\naccuracy of community merging, and the three metrics are complementary to each other."}, {"title": "4.3.2 Evaluation of DCvCD Using Different Clustering Baselines", "content": "It is noteworthy that the proposed approach of DCvCD begins with initial clustering; therefore, it can essentially\nleverage any clustering approach and serve as an enhancement plug-in. To demonstrate its efficacy with other clustering\nbaselines, we have also implemented DCvCD based on the open-sourced clustering approach of CC [7]. Note that CC\nuses ResNet34 instead of ResNet50 as its RL backbone. For the ablation purpose, we compare the performance of\nDCvCD with that of CC.\nThe detailed comparative evaluation results have been presented in Table 3. It can be observed that similar to the case\nof DivClust, the performance of DCvCD is considerably better than that of CC. It is noteworthy that the observed\nimprovement margins fluctuate across the datasets. This result can be attributed to different cluster purity during\ninitialization and its impact on clustering performance. The higher the cluster purity, the larger the main communities\nthat can be detected by the community detection algorithm; larger and purer main communities can consequently\nimprove the performance of subsequent self-supervised learning.\nOur experimental results clearly demonstrate that the proposed approach of community detection can work with other\nclustering approaches and RL backbones, and effectively improve their performance. It bodes well for its application in\nreal scenarios."}, {"title": "4.4 Sensitivity Evaluation w.r.t Similarity Threshold", "content": "In this study, we set the similarity threshold for cluster network construction within the reasonable range of [0.6,0.75],\nwith an increment of 0.05, and track the performance fluctuation of DCvCD.\nThe detailed performance results on the three metrics have been presented in Figure 3. It can be observed that on all the\ntest datasets, the performance of DCvCD does fluctuate as the similarity threshold varies. Its performance tops when the\nthreshold is set between 0.7 and 0.75. However, it is noteworthy that even with the threshold set at suboptimal values,\ne.g., [0.6, 0.7], the performance of DCvCD remains very competitive compared with the existing SOTA alternatives.\nThese results can be expected because setting the threshold at too low would result in a very dense network, blurring the\nboundaries between communities, while setting it at too high would instead result in an overly sparse network, making\neffective community detection harder.\nThese results demonstrate that the performance of DCvCD is generally robust w.r.t the key parameter of similarity\nthreshold provided that it is set within a reasonable range. In real applications, it is suggested that the similarity\nthreshold be set based on the desired density of constructed cluster network."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel approach of deep clustering via community detection, whose clustering strategy\nfactors in cluster network analysis. It alternates between RL backbone fine-tuning and community detection. In the\nphase of community detection, it employs an algorithm to divide unlabeled samples into many small communities,\nand then merges them with main communities. To improve the accuracy of pseudo-label purity within communities,\nwe introduce an ensemble metric consisting of both network structural measures and traditional distance measures to\nguide community merging. Our extensive experiments on benchmark image datasets have validated the efficacy of the\npropose approach. We will open-source our implementation as soon as possible provided that review policy allows.\nIt is noteworthy that the proposed DCvCD, as a clustering strategy, is a flexible approach. It is complementary to the\nexisting work on RL backbones and community detection. It can naturally works with any existing RL backbone.\nOn the other hand, even though we implement DCvCD using the Louvain algorithm in this paper, other advanced\ncommunity detection algorithms tailored to specific data distributions can also be easily implemented. The flexibility of\nDCVCD bodes well for its application in real scenarios.\nHowever, DCvCD has several limitations, which may merit future investigations:\n\u2022 The efficacy of community detection to a large extent depends on the structure of similarity network, which is\nsupposed to be constructed based on a specified similarity threshold. Our experiments on benchmark image\ndatasets show that the performance of DCvCD is generally robust w.r.t the threshold provided that it is set\nwithin a reasonable range, and the threshold can be optimized based on the density of constructed cluster\nnetwork. However, how to optimize this threshold for different types of datasets may be a tricky task requiring\nan in-depth future investigation.\n\u2022 The two-stage solutions for deep clustering usually require large initial clusters with high pseudo-label purity,\nwhich is critical to self-supervision, to achieve competitive performance. Our implementation leverages the\nLouvain algorithm for community detection, and shows promising results on image datasets. However, the\nLouvain may not perform satisfactorily on other types of datasets. Given a dataset, selecting an appropriate\ncommunity detection algorithm, or researching a new community detection algorithm for deep clustering, is\nan interesting task."}]}