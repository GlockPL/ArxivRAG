{"title": "Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image Prompts with Text Summarization", "authors": ["Portia Cooper", "Harshita Narnoli", "Mihai Surdeanu"], "abstract": "Text-to-image models are vulnerable to the stepwise \"Divide-and-Conquer Attack\" (DACA) that utilize a large language model to obfuscate inappropriate content in prompts by wrapping sensitive text in a benign narrative. To mitigate stepwise DACA attacks, we propose a two-layer method involving text summarization followed by binary classification. We assembled the Adversarial Text-to-Image Prompt (ATTIP) dataset (N = 940), which contained DACA-obfuscated and non-obfuscated prompts. From the ATTIP dataset, we created two summarized versions: one generated by a small encoder model and the other by a large language model. Then, we used an encoder classifier and a GPT-40 classifier to perform content moderation on the summarized and unsummarized prompts. When compared with a classifier that operated over the unsummarized data, our method improved F1 score performance by 31%. Further, the highest recorded F1 score achieved (98%) was produced by the encoder classifier on a summarized ATTIP variant. This study indicates that pre-classification text summarization can inoculate content detection models against stepwise DACA obfuscations.", "sections": [{"title": "Introduction", "content": "Disclaimer: This paper includes language that some readers might find offensive.\nText-to-image models can generate highly realistic images that represent the prompting text provided by users (Betker et al., 2024). Many publicly accessible text-to-image models utilize content moderation techniques to ensure generated images are appropriate. However, even state-of-the-art models such as DALL-E 3 are susceptible to adversarial prompting techniques. Deng and Chen (2024) found that \u201cdivide-and-conquer attack\" (DACA) obfuscations, which involve re-contextualizing and padding inappropriate image prompts with a narrative generated by a large language model (LLM), tricked DALL-E 3's content filter more than 85% of the time.\nVarious attacks on text-to-image models have been developed, such as reverse engineering a model's safety filter (Rando et al., 2022) or introducing character-level perturbations into prompts (Kou et al., 2023). However, DACA is significant as it is the first method which utilizes LLM-altered prompts to circumvent text-to-image model safety filters. The stepwise DACA method operates by instructing an LLM to identify an adversarial prompt's main components (e.g., characters, actions, properties, and scene descriptions). Then, the LLM is used to obfuscate the extracted content by re-contextualizing sensitive text.\nWe hypothesize that direct summarization could serve as an effective counter to DACA by removing linguistic obfuscations. In Figure 1 sub-figures (a) and (b), we show an appropriate prompt cleared for image generation and an inappropriate prompt flagged by the filter. Sub-figure (c) presents a successful DACA obfuscation in which the inappropriate prompt is muddied by LLM-powered alteration. We refer to this phenomenon as a \u201cwolf in sheep's clothing,\" as the inappropriate prompt is camouflaged in a thick coat of linguistic \"fluff.\u201d Finally, sub-figure (d) depicts the result of our proposed text summarization method. The obfuscated prompt is condensed into a single descriptive sentence. With the \"fluff\" removed, the content detection filter flagged the prompt and prevented image generation.\nThe key contributions of our work include:\n\u2022 Introducing the simple method of direct text summarization for removing DACA obfuscations in text-to-image prompts.\n\u2022 Assembling an evaluation baseline, the Adversarial Text-to-Image (ATTIP) dataset, which"}, {"title": "Related Work", "content": "Recent advancements in text-to-image generation models have spurred a wave of research highlighting their vulnerabilities. Rando et al. (2022) explored the limits of the Stable Diffusion model's safety filter and demonstrated that it could be easily bypassed to generate violent and gory imagery. Qu et al. (2023) studied the vulnerabilities of text-to-image models and found that unsafe images 19% were generated of the time. Liu et al. (2023) introduced reliable and imperceptible adversarial text-to-image generation and demonstrated that under white- and black-box settings, images based on adversarial prompts were generated. And Radharapu and Krishna (2023) proposed a novel taxonomy of attacks and found that ten of the thirteen attack styles were successful in evading model safety filters. Additionally, CharGrad (Kou et al., 2023), which used character-level perturbations in text-to-image prompts to trick black-box models, was proposed as a controllable prompt adversarial attacking framework. And the Groot framework (Liu et al., 2024) utilized semantic decomposition and a LLM to systematically refine adversarial prompts.\nFinally, previous work has studied techniques for neutralizing a variety of adversarial image generation methods. Yang et al. (2024) introduced the GuardT2I framework, which utilizes a LLM to convert text embeddings in text-to-image prompts into natural language to ensure the integrity of generated images. The universal prompt optimizer for safe text-to-image generation framework (Wu et al., 2024) used a LLM to process toxic prompts prior to image generation, which significantly decreased the probability of inappropriate image creation. And Schramowski et al. (2023) proposed a stable latent diffusion layer as a classifier-free method for suppressing inappropriate content in generated images."}, {"title": "Prompt Obfuscation Dataset", "content": "A key contribution of our paper is the assimilation of a baseline dataset containing a mixture of text-to-image prompts including non-obfuscated appropriate, obfuscated appropriate, and obfuscated inappropriate. To achieve this, we assembled an initial corpus of 1,000 prompts with a ground truth label composition of: (1) 100 inappropriate prompts"}, {"title": "Approach", "content": "Our proposed method for combating DACA obfuscations operates in two steps: text summarization and inappropriate prompt classification."}, {"title": "Text Summarization", "content": "Prompts are de-obfuscated using a direct text summarization approach. Two summarization methods were selected for extracting key information in prompts:\nEncoder summarizer: philschmid/ bart-large-cnn-samsum (philschmid, 2022), a variant of Facebook's BART transformer model (Lewis et al., 2019) fine-tuned on Samsung's SAMSum Dataset (N = 16,369) (Gliwa et al., 2019).\nGPT-40 summarizer: The current flagship model published by OpenAI et al. (2024), which was provided the two in-context learning examples from the hold-out set and instructed to summarize the obfuscated prompts in a style such that the resulting summary mirrored the original pre-obfuscated form of the prompt.\nWe applied both the encoder summarizer and the GPT-40 summarizer across the full ATTIP baseline dataset to create 940 encoder summaries and 940 GPT-40 summaries."}, {"title": "Inappropriate Prompt Classification", "content": "The second aspect of our solution to DACA obfuscation involves binary classification. We selected two methods for content detection:\nEncoder classifier: michellejie -li/ inappropriate_text_classifier (Li, 2022), a version of DistillBERT trained on a 19,604 subset of the Comprehensive Abusiveness Detection Dataset (Song et al., 2021).\nGPT-40 classifier: The current flagship model published by OpenAI et al. (2024).\nThe encoder classifier was trained on the encoder and GPT-40 summaries associated with the members of the pre-defined train set (N = 470), and GPT-40 was given two in-context learning examples: one ground truth appropriate & one ground truth inappropriate. Additionally, both the encoder classifier and GPT-40 classifier were tuned using the raw, unsummarized prompts of the ATTIP baseline dataset. Precision, recall, and F1 score on the inappropriate class and accuracy were calculated for both models using the designated test sets. And an error analysis (Appendix A.1) was conducted."}, {"title": "Explanations for the Encoder Classifier", "content": "Local interpretable model-agnostic explanations (LIME) were utilized to assess the quality of the encoder classifier. From the test subset of the ATTIP baseline dataset (n = 235) and corresponding encoder and GPT-40 summaries, a 10% sample of was randomly selected.\nTwo human annotators independently evaluated the explanations using a detailed codebook (Appendix A.2) on the generated LIME plots. Based on the ten highest ranked words in each plot, annotators assigned the labels of poor, fair, and high quality to each explanation. Intercoder agreement was 89%, and Cohen's Kappa was 0.82 (SE=0.06, 95% CI = [0.70, 0.94]). Coding disagreements were discussed and reconciled."}, {"title": "Results", "content": ""}, {"title": "Analysis of Inappropriate Prompt Classification", "content": "In Table 1, the accuracy, precision, recall, and F1 scores of the encoder-classifier on the baseline obfuscated texts, the encoder summaries, and the GPT-40 summaries are reported. The F1 score improved from 94% (when trained using the original obfuscated texts) to 98% (when trained using our the encoder summarization method).\nTable 2 shows the results of the parallel experiment, which involved using GPT-40 to perform the same inappropriate prompt classification. The highest achieved F1-score was 81% (when using the GPT-40 summarization method), which is lower than the comparable encoder-classifier F1-score on the GPT-40 summarized texts (94%)."}, {"title": "Analysis of Encoder Classifier Explanations", "content": "As shown in Figure 2, the ATTIP baseline dataset explanations accounted for the largest share of poor quality labels (47.37%). The encoder summary explanations were associated with the largest percentage of the fair quality labels (39.47%). And the GPT-40 summary explanations received the most high quality labels (40.00%). These results indicate that the classification of summarized prompts not only yields better performance but also better explanations. To further illustrate how obfuscation hurts interpretability, we provide four LIME"}, {"title": "Discussion", "content": "Both the encoder and GPT-40 classifiers experienced difficulty with labeling the unsummarized prompts. Most notably, the GPT-40 classifier's F1 score increased from 49% on the inappropriate-only test set from the ATTIP baseline dataset to 81% on the corresponding inappropriate-only GPT-40 summaries. The overall highest inappropriate-only F1 score (98%) was generated by the encoder classifier fine-tuned on the encoder summaries. Additionally, the LIME plots analyzed in the present study yielded higher quality explanations on summarized data with the encoder and GPT-40 summaries accounting for 88% of the total high quality labels."}, {"title": "Conclusion", "content": "Stepwise DACA text-to-image prompt obfuscations present a challenge for traditional content detection models. However, our study shows that encoder and large language-based text summarization models are viable methods for defending against adversarial DACA prompts or \u201cwolves\" masquerading as \"sheep.\" Further, our results indicate that text-to-image models can be inoculated against obfuscated prompts through training on their summarized variants. This suggests that integrating text summarization into content moderation pipelines of image generation models can enhance the robustness of detection systems."}, {"title": "Limitations", "content": "A significant limitation of the present study is its narrow focus on DACA obfuscations. However, DACA is a potent attack style at present and important to address. While our method relies on pre-existing text summarization tools, we are the first to directly combat DACA in this way. Despite the simplicity of our method, performance benefits were achieved."}, {"title": "Ethical Considerations", "content": "Due to the adversarial nature of the ATTIP dataset, we will share the data on a case-by-case basis with researchers in order to minimize the potential that the prompts will be used to attack text-to-image models. Additionally, it is important to acknowledge that offensiveness is a subjective construct. This study utilized labeled text-to-image prompts in existing datasets and did not independently categorize content as offensive or non-offensive."}, {"title": "Appendix", "content": ""}, {"title": "Error Analysis", "content": "For both the encoder and GPT-40 classifiers, we analyzed all instances where an obfuscated prompt was correctly labeled but a comparative summarized variant was misclassified.\nThree such cases were identified for the encoder classifier-all of which occurred on GPT-40 summary variants of ATTIP obfuscated prompts. Two of the three cases were misclassifications by the encoder classifier. However, the last case presented an unique issue-the ground truth label changed from appropriate to inappropriate after DACA obfuscation. This result is, of course, the opposite of the goal of DACA obfuscation. This particular instance involved a reference to \"It's a Wonderful Life,\" a 1947 Frank Capra film (Table 4). When generating the DACA-obfuscated prompt, the LLM introduced characters from the film not mentioned in the original prompt text (i.e., George Bailey and Clarence Odbody) and inserted them into an inappropriate situation (i.e., George Bailey pulling the trigger of a firearm). Thus, the ground truth label"}]}