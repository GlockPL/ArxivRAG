{"title": "Structural Optimization of Lightweight Bipedal Robot via SERL", "authors": ["Yi Cheng", "Chenxi Han", "Yuheng Min", "Linqi Ye", "Houde Liu", "Hang Liu"], "abstract": "Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance and hard to obtain optimal design results within vast design spaces, thus failing to full exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution Reinforcement Learning) algorithm, which combines reinforcement learning for locomotion tasks with evolution algorithms. The aim is to identify the optimal parameter combinations within a given multidimensional design space. Through the SERL algorithm, we successfully designed a bipedal robot named Wow Orin, where the optimal leg length are obtained through optimization based on body structure and motor torque. We have experimentally validated the effectiveness of the SERL algorithm, which is capable of optimizing the best structure within specified design space and task conditions. Additionally, to assess the performance gap between our designed robot and the current state-of-the-art robots, we compared Wow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of experimental results demonstrate the Outstanding energy efficiency and performance of Wow Orin, further validating the feasibility of applying the SERL algorithm to practical design.", "sections": [{"title": "I. INTRODUCTION", "content": "Designing a bipedal robot is a vast and intricate engineering task, primarily due to numerous parameters that require manual selection. These parameters encompass structural length, body weight, rotational inertia, among others, playing a crucial role in the motion performance and overall functionality of bipedal robots. However, researchers have long faced the challenge of making appropriate choices among numerous possible parameter combinations to design an outstanding bipedal robot.\nIn many cases, these parameters are subjectively chosen based on engineers' personal experiences. This empirical approach often demands engineers to possess extensive experience and undergo numerous trials and modifications to achieve satisfactory results. Additionally, some design methodologies have drawn inspiration from the structural characteristics of natural bipedal organisms[1], adopting bio-inspired method to attempt replication of these organisms' structural parameters within robotic designs. However, considering the essential differences in the overall structure between robots and their biological models, such approaches often fail to fully exploit the potential performance aspects of the robot's hardware.\nFurthermore, current research tends to overly emphasize either the control domain or the singular aspect of structural design, neglecting the inseparable relationship between the structure and control policy of bipedal robots. Isolated studies on either structural design or control policy may not yield optimal results. To address this research gap, collaborative design methods have emerged in recent years, treating structural design and control policy as an integral whole. However, most of these methods are currently predominantly applied to the design and simulation testing of soft-bodied robots, with relatively less application in the practical design of rigid-bodied robots.\nTo tackle these challenges, this paper proposes an innovative algorithm known as the Structure Evolution Reinforcement Learning Algorithm (SERL). This algorithm can end-to-end generate optimized design structures under given task goals and design requirements, while concurrently producing robust task completion policy. SERL algorithm achieves this by training the control policy of bipedal robots through reinforcement learning and implementing self-evolution of robot structural parameters via genetic algorithms. In comparison to methods relying on manual adjustments and bio-inspiration, the SERL algorithm significantly enhances design efficiency, reduces workload, and ensures the rational reliability of design results.\nThen we developed a bipedal robot with nine degrees of freedom, named Wow Orin. The design process initially involves defining the necessary motor parameters and transmission methods. We innovatively adopted a method utilizing innovative fishbone-barb line actuation and belt drive, which relocates the drive motors for the ankle and knee joints to the top of the robot, reducing the rotational inertia of leg movements. Simultaneously, we employed the SERL algorithm to optimize the leg length parameters within the specified design space, obtaining the optimal leg length for comprehensive motion tasks. We compared the optimized robot model with a manually specified leg length parameter model, confirming that the SERL algorithm optimized model performs significantly better in meeting task requirements.\nFurthermore, we conducted a comprehensive comparison between \"Wow Orin\" and advanced bipedal robots Cassie and Unitree H1, demonstrating the outstanding performance of Wow Orin in terms of energy efficiency and agility. The main contributions of this paper include:\n\u2022 Introduce a novel bipedal robot structural parameter design method\u2014the SERL algorithm, capable of finding the optimal parameter combination for specific tasks within a predefined design space.\n\u2022 Propose a two-stage optimization training framework, achieving integrated optimization of structural parameters and motion control policy.\n\u2022 Using the optimized structural parameters from the SERL algorithm, combined with innovative fishbone-barb line actuation and belt drive, we designed a flexible and lightweight bipedal robot, demonstrating its significant advantages over existing bipedal robots."}, {"title": "II. RELATER WORK", "content": "A. Evolution Algorithms for Robot Structure Design\nEvolution policy is a type of optimization algorithm inspired by the principles of evolution in the natural world, primarily employed to address optimization problems in continuous parameter spaces. Evolution policy possesses a broader capability to explore design spaces, especially adept at handling complex and multivariate design challenges. Researchers have successfully integrated evolution policy into the co-design of robots with simplified models for simple tasks, achieving commendable results[3], [4], [5]. In addition, [5], [6], and [7] share a common emphasis on combining evolutionary strategies with reinforcement learning techniques to address various challenges. Specifically, they focus on evolving agent morphologies and optimizing robot designs to enhance locomotion control and adaptability in complex environments. For instance, [5] introduces the Deep Evolutionary Reinforcement Learning (DERL) algorithm, which enables the evolution of diverse agent morphologies using low-level sensory inputs. Similarly, [6] utilizes meta reinforcement learning and genetic algorithms to optimize legged robot designs, achieving improved adaptability across different environmental conditions. Additionally, [7] presents Evolution Gym, a benchmark platform for co-optimizing soft robot design and control through co-evolution algorithms.\nIt is worth noting that existing research methods often focus on the design of soft robots or robots in virtual environments. However, we focus on the rigid bipedal robot and validates the designs through physical experiments, showcasing the feasibility of our optimization approach in more realistic scenarios.\nB. RL for Locomotion Control of Legged Robots\nThe locomotion control of legged robots is a complex engineering task. Traditional methods involve the physical dynamic modeling of legged robots and the application of control theory to address this challenge. However, such an approach requires designers to possess extensive expertise and practical experience.\nIn recent years, reinforcement learning methods have shown tremendous potential in the mobility tasks of legged robots, with many studies significantly enhancing the mobility robustness and agility of legged robots through the use of reinforcement learning. These methods have been particularly applied to quadruped robots, achieving robust outdoor walking, high-speed running [11], and stair climbing [8], [10], [12], among other remarkable feats such as parkour [13], [14]. Compared to quadrupeds, the design of motion controllers for bipedal robots is more challenging. Current approaches either optimize through reward design and feature engineering or incorporate prior reference motions (trajectories, gait parameters, etc.) to achieve robust walking, jumping, and running tasks for bipedal robots [15], [16], [17]. These efforts typically involve designing specialized controllers for robots after their physical systems have been fully developed.\nUnlike the aforementioned works, our approach begins from the structural design phase, considering the compatibility between structure and motion control, and proposes a hybrid optimization method that simultaneously optimizes structural design and learns optimal control.\nC. Co-Design of Legged Robots\nThe parameter design of robots is often deeply associated with specific tasks, particularly for complex nonlinear systems like legged robots. Structural parameters significantly impact the performance in various tasks. To achieve the structural design of legged robots for specific tasks, the scientific community has explored and attempted various methods. For instance, [19] uses the HZD gait generation framework to optimize for stable gait and leg length. [20], [21], [22]seek the optimal robot design through a bilevel optimization method. Additionally, other works such as [23], [24], [25] employ trajectory optimization and ADMM methods for co-design.\nUnlike these methods, our optimization procedure is based on reinforcement learning approach that does not require cumbersome task modeling and algorithm parameter design, making it more generalizable to different tasks. This approach simplifies the design process and allows for greater flexibility in adapting to various task requirements."}, {"title": "III. METHOD", "content": "A. Structural Evolution Reinforcement Learning\n1) Overview: In response to the black-box optimization problem regarding the impact of structural parameters on the comprehensive motion performance of biped robots, this study introduces a Structural Evolution Reinforcement Learning algorithm (SERL). SERL integrates the dynamic adjustment capabilities of reinforcement learning policy with the global search capacity of genetic algorithms. Without reliance on explicit model guidance, it explores the design parameter space (specified in this paper as the leg length of the bipedal robot), with the aim of maximizing task rewards and optimizing the best size design and control policy. This method effectively combines the strengths of both algorithms, not only finding the optimal structural parameters but also developing a highly robust motion control policy. Due to the limited information acquired from the robot's onboard sensors, training directly within the body's proprioceptive observation space makes it challenging to uncover the bipedal robot's limit performance. Therefore, we propose the implementation of a two-phase training architecture, we introduce additional observations in the first stage of training and distill them into a deployable policy in the second stage. Figure 2 and Figure 3 present the details.\n2) Basic Settings of RL: In this work, we treat all structural parameters of the biped robot interaction with the environment as a Markov transfer process(MDP). The MDP is defined by a tuple $(S, A, P, r, \\gamma, L)$, where S denotes the state space, encompassing the possible environmental states that the robot may encounter. The action space A includes the diverse actions available to the robot. The state transition probability function $P(S_{t+1}|S_t,a_t)$ characterizes the likelihood of the system transitioning from one state to another. The reward function $r(s_t, a_t)$ delineates the immediate reward received by the robot upon taking a specific action. The y stands for the discount factor, indicating the extent to which future rewards are discounted, typically ranging between 0 and 1. The goal is to find the optimal policy $\u03c0^*$ that maximize the discounted reward:\n$\u03c0^* = arg \\underset{\\pi}{max} E[\\sum_{t=0}^{\\infty} \u03b3^tR_t]$    (1)\nState Space: In the training phase of SERL (Structured Environment Reinforcement Learning), privileged information is utilized to facilitate the rapid adaptation of bipedal robots by quickly identifying optimal parameters. The state space $s_t$ encompasses proprioceptive information $o_t$, velocity information $v_t$ and privileged information $e_t$, where the privileged observational information $e_t$ includes terrain data, body mass, and friction coefficients. Moreover, the impact of leg length on the interaction between the robot and its environment presents a challenging modeling task. So we take the structural information $L_i$ as one of the observable parameters, and both $L_i$ and $e_t$ are mapped to a latent space to generate the latent variable $z_t$. This process aims to obtain an implicit representation of the interaction between the robot's structural parameters and the environment, enabling the robot to more rapidly adapt to changes in leg length and environmental conditions.\n$S_t = O_t + Z_t + U_t$    (2)\nAction Space: The action $a_t$ consists of the desired positions for the nine motors corresponding to the biped robot. The joint torque is obtained by converting the desired joint positions into joint torques using a PD controller.\nReward Design: The design of rewards is crucially aligned with our ultimate goal for the robot's design. We have established a comprehensive reward system for the humanoid robot's walking performance based on [18]. In addition, we added the task reward. For example, in a comprehensive locomotion task, the task reward is based on tracking a specified speed. For the achieving maximum velocity task, the task reward is based on attaining higher speeds. More details on reward settings and training can be found in the SERL-IROS2024.github.io appendix here.\n3) Method for Structural Evolution: The process of the structural evolution begins by initializing a population of individuals, each representing a unique set of leg length. The leg length, denoted as $l_t$; and $l_s$, are randomly selected for each individual, this initialization step creates an initial set of individuals $l_i$.\nEach individual within the population undergoes reinforcement learning training to assess its performance, resulting in the output of a reward value $R_i$. This training process focuses on optimizing the leg length based on the rewards obtained over a specified number of training episodes.\nGenetic operations, encompassing crossover and mutation, are then applied to individuals to generate new sets of leg length. Crossover involves combining the leg length of two individuals, while mutation introduces minor adjustments to an individual's leg length.\nThe newly generated individuals from crossover and mutation undergo RL training again. The optimization process iterates between RL training and genetic operations until a predetermined number of iterations is reached or when the variance of rewards among the population falls below a defined threshold. Eventually, the structural evolution process for the bipedal robot is completed, resulting in optimized leg length and control policy.\n4) Policy Training for Physical Deployment: After determining the optimal leg length through SERL in the first phase to obtain deployable adaptive motion control policy based on proprioceptive sensing, we distilled the RL policy from the first phase online. In the second phase, interactions are exclusively with robots using the optimal leg length. The state space $s_t$ combines proprioceptive information $o_t$ and estimated body linear velocity $\\hat{v}_t$. Unlike traditional high-dimensional state processing methods that use CNNs for dimensionality reduction[9], this study employs GRUs to transform high-dimensional historical state information into low-dimensional vectors $y_t$, showcasing GRUs' superior temporal sequence capturing ability compared to CNNs, the specific framework is shown in the Figure 3. The state space is defined as:\n$S_t = O_t + V_t + Y_t$    (3)\nThe adaptive motion policy based on the optimal leg length is iteratively trained under the supervision of the phase one policy.\n5) Train Details: As shown in the pseudocode 1, the algorithm initializes the weight parameters of the neural network and the population of the genetic algorithm. It randomly selects different combinations of leg length $L = (l_t, l_s)$ for each individual, with the population size set to pop_size = 250. Subsequently, each individual undergoes training in a simulation environment, aiming to maximize the reward obtained for completing locomotion tasks. After training, the algorithm evaluates the training results for each individual and selects the optimal individuals for crossover, mutation, and selection, generating a subpopulation of individuals. This subpopulation iteratively repeats the aforementioned process until the specified number of cycles is reached. Ultimately, the algorithm outputs optimal individual parameters, achieving leg joint length and robot motion control policy optimization.\nWe initially load robot models with varying pole length on the Isaac Gym platform and conduct walking training using a unified reward mechanism. To comply with physical laws, we set the unit of leg length to have a uniform mass density. Changes in length will result in an increase in both mass and moment of inertia. After completing 500 iterations of training, we further apply the SERL algorithm for optimization iterations. This step is designed to prevent the optimization goal of the SERL algorithm from biasing towards the combination of pole length that learns the fastest, neglecting the optimal combination for a specific task. In the application of the SERL algorithm, adjusting hyperparameters ensures the convergence speed of the policy network and the genetic algorithm is consistent, thus avoiding falling into local optima.\nMoreover, the concept of \"fair rules\" is introduced during the training process. The optimization goal of the genetic algorithm is the combination of pole length that achieves the maximum reward under the same task conditions. However, different command resampling, random thrust sizes, and other factors may introduce unfair criteria for evaluation. Therefore, it is necessary to ensure these parameters are consistent for each individual to guarantee the fairness of the assessment. The specific hyperparameters can be referred to in Table I.\nIn the first phase of the algorithm, SERL, we use a single NVIDIA RTX-4090 GPU to train on the Isaac Gym simulation platform, which takes approximately 12 hours. In the second phase, we use the same hardware configuration as in the first phase, with a training time of about 1 hour."}, {"title": "B. Bipedal Robot Platform Design", "content": "1) Overview: We introduce Wow Orin, an innovative bipedal robot featuring nine degrees of freedom, encompassing hip joints for internal/external rotation, hip flexion/extension, ankle flexion/extension, knee flexion/extension in each leg, and the abduction/adduction joints of the hip are driven by a single shared motor. All joints are powered by Unitree Al motors with a peak torque of 33.5Nm and a maximum speed of 21rad/s. Wow Orin is designed with a strong emphasis on lightweight principles, weighing a mere 10.5kg and standing at a height of 0.88m. The thigh and shin dimensions are optimized using the SERL algorithm, resulting in values of $l_t$ = 0.31m and $l_s$ = 0.36m respectively. To reduce leg rotational inertia and enhance motion performance, the knee joint incorporates a synchronou belt drive, while the ankle joint utilizes a Bowden cable drive. Wow Orin is equipped with a Jetson Orin NX (8GB) onboard PC, delivering an impressive AI performance of 40TOPS.\n2) Lightweight Leg Design: To reduce the rotational inertia of the legs and further enhance motion performance, besides the aforementioned ankle joint rope drive method, the knee joint adopts a synchronous belt drive. In addition, we used carbon fiber as the skeleton for the robot's lower and upper leg links, 3D printed PETG material for some connectors, and aluminum alloy for key load-bearing parts to achieve the ultimate reduction in weight without sacrificing structural strength. Through the above design, the weight of the robot's single leg was reduced to 1.3kg. We designed the joint parameters as follows in the Table II.\n3) Bionic Fishbone Bowden Cable Driven Ankle Joint: The design of the ankle joint is our main distinction from other robots, which use smaller mass motors in the ankle joint to reduce leg inertia, resulting in the maximum force generated by the ankle joint being significantly less than that of the thigh joint. According to research in literature, the torque generated by the human ankle joint during running is nearly equal to that of the thigh joint. Therefore, based on previous work, following the principle of mass concentration, and to meet the requirements of mass elevation in structural design, we adopted a Bowden cable to implement a series elastic actuation system, allowing the ankle actuator to be positioned anywhere outside the joint itself. This eliminates the need to consider the mass of the ankle joint motor affecting leg movement, significantly reducing leg inertia mass while greatly enhancing the torque of the ankle joint and significantly increasing the overall structural design flexibility. On our robot, the ankle joint power motor is placed in the upper body and transmits power to the ankle joint through the Bowden cable.\nWe adopted a bionic fishbone structure for the Bowden cable (as shown in the Figure 4), which uses multiple segments closely connected by spherical joints to achieve structural flexibility and adjustability. By adjusting the number of segments, the total length of the structure can be rapidly changed, thus providing high adaptability. The interlayer part is made of a complete oil tube filled with lubricant, significantly reducing the friction encountered by the steel wire during movement. Compared to traditional Bowden cable structures, this design not only significantly improves load-bearing capacity but also achieves extremely low friction."}, {"title": "IV. EXPERIMENTAL EVALUATIONS", "content": "A. Evaluation of the SERL Algorithm\n1) Effectiveness of the Algorithm Optimization Process: In order to demonstrate the effectiveness of SERL algorithm in enhancing robot motion performance, we first define two different task forms as specific design objectives: comprehensive locomotion under perturbations and achieving maximum velocity. The two different tasks are represented by different task reward forms, denoted as $r_t$. Through data analysis of the mean reward of individuals in the population and the maximum fitness in the population for both tasks, where population fitness is used to measure the maximum difference in individual performance within the population and can effectively demonstrate the convergence of the population, we can effectively showcase the convergence effect of the population. We also record the thigh and shin length during the population evolution process. The convergence process of the above data reflects the improvement of policy and performance and the stable variation of design parameters.\nFor the comprehensive locomotion under perturbations task, as the results shown in Figures 5, ref and 7, the leg length parameters converge to their optimal values at around 700k steps, with the population's mean reward reaching its maximum and the population's maximum fitness reaching its minimum. The population distribution of leg length also changed from scattered to gradually concentrated. Surprisingly, the convergence results show that longer shin length compared to thigh length lead to better task completion. This finding is different from previous methods based on biological inspiration or manual adjustment, where thigh and shin length are typically set to the same value in bipedal robot design.\nIn the results of the achieving maximum velocity task shown in Figures 8, 9 and 10, we found that the convergence is faster, reaching a good convergence effect after 200k steps. The results show that, compared to the comprehensive locomotion task, in the task of achieving maximum velocity, shin length increase while thigh length decrease slightly, and the length of the thigh and shin tend to be the same.\nThe convergence results of the two tasks indicate that for different task forms, the SERL algorithm can effectively provide different optimal design solutions, satisfying diverse requirements in bipedal robot design and maximizing robot performance and capabilities."}, {"title": "B. Performance Testing of Wow Orin Robot", "content": "To assess the superior performance of Wow Orin and demonstrate its high energy efficiency and agility, we employed the same algorithmic settings to train the control policy of Wow Orin, Cassie, and Unitree H1 in a flat simulation environment, followed by comparative tests of their Cost of Transport (COT), maximum velocity and Froude number[26].\n1) Energy Efficiency Comparison: The COT is a common metric used to gauge the efficiency of legged animals and robots, calculated as COT = $P/(mgv)$, where P denotes joint power, m represents the robot's weight, g is the acceleration due to gravity, and v is the robot's forward velocity.As shown in Table III, through extreme lightweight design, Wow Orin achieves the lowest weight among the three robots, at only 10.5 kg. Additionally, it employs motors matched to the body mass for propulsion, resulting in minimal joint torque. The COT results demonstrate that Wow Orin significantly outperforms the other two robots, indicating its superior energy efficiency. This allows the robot to achieve longer endurance with limited battery capacity, which is crucial for tasks requiring prolonged movement. Since the calculation process of COT excludes the influence of the robot's weight, it indicates to some extent that the optimization of our robot's leg length design plays a crucial role in achieving a lower COT.\n2) Maximum Velocity on Flat Ground: The Froude number is a dimensionless quantity used to compare the dynamic similarity of different-sized robots or organisms under similar motion conditions [26], especially in walking or running scenarios. Its calculation formula is Fr = $v\u00b2/gl$, where v is the speed of the robot's movement, g is the acceleration due to gravity, and I is the length of the robot's legs. We trained the three robots with the same parameters and environmental settings to achieve their maximum speed on flat ground. The results, as shown in the Table III, reveal that Wow Orin's maximum speed and Froude number exceeds that of Unitree H1 but falls short of Cassie's top speed. This difference may be attributed to Cassie's more explosive leg structure and the lower peak torque of the joint motors used in Wow Orin compared to Cassie's."}, {"title": "V. CONCLUSIONS", "content": "This study demonstrates the effectiveness of the SERL algorithm in the structural parameter design of bipedal robots, offering valuable insights for advancing this field. By combining reinforcement learning motion control policy with evolutionary algorithms, the SERL algorithm successfully identifies structural parameter values within the specified design space that best meet task requirements. Through comparison with other parameters within the design space, we validate the exceptional performance of the SERL algorithm in achieving theoretically optimal values. Furthermore, we apply the optimized results of the SERL algorithm to practical design, creatively developing the Wow Orin bipedal robot. In this paper, we primarily focus on optimizing the leg length as a starting point for the design. In the future, we will apply the SERL algorithm to other parameters and tasks, and improve the algorithm to achieve the co-optimization of multiple design parameters."}]}