{"title": "Structural Optimization of Lightweight Bipedal Robot via SERL", "authors": ["Yi Cheng", "Chenxi Han", "Yuheng Min", "Linqi Ye", "Houde Liu", "Hang Liu"], "abstract": "Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance and hard to obtain optimal design results within vast design spaces, thus failing to full exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution Reinforcement Learning) algorithm, which combines reinforcement learning for locomotion tasks with evolution algorithms. The aim is to identify the optimal parameter combinations within a given multidimensional design space. Through the SERL algorithm, we successfully designed a bipedal robot named Wow Orin, where the optimal leg length are obtained through optimization based on body structure and motor torque. We have experimentally validated the effectiveness of the SERL algorithm, which is capable of optimizing the best structure within specified design space and task conditions. Additionally, to assess the performance gap between our designed robot and the current state-of-the-art robots, we compared Wow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of experimental results demonstrate the Outstanding energy efficiency and performance of Wow Orin, further validating the feasibility of applying the SERL algorithm to practical design.", "sections": [{"title": "I. INTRODUCTION", "content": "Designing a bipedal robot is a vast and intricate engineering task, primarily due to numerous parameters that require manual selection. These parameters encompass structural length, body weight, rotational inertia, among others, playing a crucial role in the motion performance and overall functionality of bipedal robots. However, researchers have long faced the challenge of making appropriate choices"}, {"title": "II. RELATER WORK", "content": "Evolution policy is a type of optimization algorithm in-spired by the principles of evolution in the natural world, primarily employed to address optimization problems in continuous parameter spaces. Evolution policy possesses a broader capability to explore design spaces, especially adept at handling complex and multivariate design challenges. Researchers have successfully integrated evolution policy into the co-design of robots with simplified models for simple tasks, achieving commendable results. In addition,  share a common emphasis on com-bining evolutionary strategies with reinforcement learning techniques to address various challenges. Specifically, they focus on evolving agent morphologies and optimizing robot designs to enhance locomotion control and adaptability in complex environments. For instance,  introduces the Deep Evolutionary Reinforcement Learning (DERL) algorithm, which enables the evolution of diverse agent morphologies using low-level sensory inputs. Similarly, [6] utilizes meta reinforcement learning and genetic algorithms to optimize legged robot designs, achieving improved adaptability across different environmental conditions. Additionally, [7] presents Evolution Gym, a benchmark platform for co-optimizing soft robot design and control through co-evolution algorithms.\nIt is worth noting that existing research methods often focus on the design of soft robots or robots in virtual environments. However, we focus on the rigid bipedal robot and validates the designs through physical experiments, showcasing the feasibility of our optimization approach in more realistic scenarios."}, {"title": "B. RL for Locomotion Control of Legged Robots", "content": "The locomotion control of legged robots is a complex engineering task. Traditional methods involve the physical dynamic modeling of legged robots and the application of control theory to address this challenge. However, such an approach requires designers to possess extensive expertise and practical experience.\nIn recent years, reinforcement learning methods have shown tremendous potential in the mobility tasks of legged"}, {"title": "C. Co-Design of Legged Robots", "content": "The parameter design of robots is often deeply associated with specific tasks, particularly for complex nonlinear systems like legged robots. Structural parameters significantly impact the performance in various tasks. To achieve the structural design of legged robots for specific tasks, the scientific community has explored and attempted various methods. For instance,  uses the HZD gait generation framework to optimize for stable gait and leg length. [20], [21], [22]seek the optimal robot design through a bilevel optimization method. Additionally, other works such as [23], [24], [25] employ trajectory optimization and ADMM meth-ods for co-design.\nUnlike these methods, our optimization procedure is based on reinforcement learning approach that does not require cumbersome task modeling and algorithm parameter design, making it more generalizable to different tasks. This ap-proach simplifies the design process and allows for greater flexibility in adapting to various task requirements."}, {"title": "III. METHOD", "content": ""}, {"title": "A. Structural Evolution Reinforcement Learning", "content": ""}, {"title": "1) Overview", "content": "In response to the black-box optimization problem regarding the impact of structural parameters on the comprehensive motion performance of biped robots, this study introduces a Structural Evolution Reinforcement Learning algorithm (SERL). SERL integrates the dynamic adjustment capabilities of reinforcement learning policy with the global search capacity of genetic algorithms. Without reliance on explicit model guidance, it explores the design parameter space (specified in this paper as the leg length of the bipedal robot), with the aim of maximizing task rewards and optimizing the best size design and control policy. This method effectively combines the strengths of both algorithms, not only finding the optimal structural parameters but also developing a highly robust motion control policy. Due to the limited information acquired from the robot's onboard sensors, training directly within the body's proprioceptive observation space makes it challenging to uncover the bipedal robot's limit performance. Therefore, we propose the implementation of a two-phase training architecture, we introduce additional observations in the first stage of training and distill them into a deployable policy in the second stage. Figure 2 and Figure 3 present the details."}, {"title": "2) Basic Settings of RL", "content": "In this work, we treat all structural parameters of the biped robot interaction with the environment as a Markov transfer process(MDP). The MDP is defined by a tuple (S, A, P, r, \u03b3, L), where S denotes the state space, encompassing the possible environmental states that the robot may encounter. The action space A includes the diverse actions available to the robot. The state transition probability function P(St+1|St,at) characterizes the likelihood of the system transitioning from one state to another. The reward function r(st, at) delineates the imme-diate reward received by the robot upon taking a specific action. The y stands for the discount factor, indicating the extent to which future rewards are discounted, typically ranging between 0 and 1. The goal is to find the optimal policy \u03c0* that maximize the discounted reward:\n$\\pi^* = \\arg \\max_\\pi E [\\sum_{t=0}^{\\infty} \\gamma^t R_t]$\nState Space: In the training phase of SERL (Structured Environment Reinforcement Learning), privileged informa-tion is utilized to facilitate the rapid adaptation of bipedal robots by quickly identifying optimal parameters. The state space st encompasses proprioceptive information ot, veloc-ity information vt and privileged information et, where the"}, {"title": "3) Method for Structural Evolution", "content": "The process of the structural evolution begins by initializing a population of individuals, each representing a unique set of leg length. The leg length, denoted as lt; and ls,, are randomly selected for each individual, this initialization step creates an initial set of individuals li.\nEach individual within the population undergoes reinforce-ment learning training to assess its performance, resulting in the output of a reward value Ri. This training process focuses on optimizing the leg length based on the rewards obtained over a specified number of training episodes.\nGenetic operations, encompassing crossover and mutation, are then applied to individuals to generate new sets of leg length. Crossover involves combining the leg length of two individuals, while mutation introduces minor adjustments to an individual's leg length.\nThe newly generated individuals from crossover and mu-tation undergo RL training again. The optimization process iterates between RL training and genetic operations until a predetermined number of iterations is reached or when the variance of rewards among the population falls below a defined threshold. Eventually, the structural evolution process for the bipedal robot is completed, resulting in optimized leg length and control policy."}, {"title": "4) Policy Training for Physical Deployment", "content": "After deter-mining the optimal leg length through SERL in the first phase to obtain deployable adaptive motion control policy based on proprioceptive sensing, we distilled the RL policy from the first phase online. In the second phase, interactions are exclusively with robots using the optimal leg length. The state space st combines proprioceptive information ot and estimated body linear velocity \u00fbt. Unlike traditional high-dimensional state processing methods that use CNNs for dimensionality reduction[9], this study employs GRUs to transform high-dimensional historical state information into low-dimensional vectors yt, showcasing GRUs' superior temporal sequence capturing ability compared to CNNs, the specific framework is shown in the Figure 3. The state space is defined as:\n$St = Ot + Vt + Yt$\nThe adaptive motion policy based on the optimal leg length is iteratively trained under the supervision of the phase one policy."}, {"title": "5) Train Details", "content": "As shown in the pseudocode 1, the algo-rithm initializes the weight parameters of the neural network and the population of the genetic algorithm. It randomly selects different combinations of leg length L = (lt,ls) for each individual, with the population size set to pop_size = 250. Subsequently, each individual undergoes training in a simulation environment, aiming to maximize the reward obtained for completing locomotion tasks. After training, the algorithm evaluates the training results for each individual and selects the optimal individuals for crossover, mutation, and selection, generating a subpopulation of individuals. This subpopulation iteratively repeats the aforementioned process until the specified number of cycles is reached. Ulti-mately, the algorithm outputs optimal individual parameters, achieving leg joint length and robot motion control policy optimization.\nWe initially load robot models with varying pole length on the Isaac Gym platform and conduct walking training using a unified reward mechanism. To comply with physical laws, we set the unit of leg length to have a uniform mass density. Changes in length will result in an increase in both mass and moment of inertia. After completing 500 iterations of training, we further apply the SERL algorithm for optimization iterations. This step is designed to prevent the optimization goal of the SERL algorithm from biasing towards the combination of pole length that learns the fastest,"}, {"title": "B. Bipedal Robot Platform Design", "content": ""}, {"title": "1) Overview", "content": "We introduce Wow Orin, an innovative bipedal robot featuring nine degrees of freedom, encom-passing hip joints for internal/external rotation, hip flex-ion/extension, ankle flexion/extension, knee flexion/extension in each leg, and the abduction/adduction joints of the hip are driven by a single shared motor. All joints are powered by Unitree Al motors with a peak torque of 33.5Nm and a maximum speed of 21rad/s. Wow Orin is designed with a strong emphasis on lightweight principles, weighing a mere 10.5kg and standing at a height of 0.88m. The thigh and shin dimensions are optimized using the SERL algorithm, resulting in values of lt = 0.31m and ls = 0.36m respec-tively. To reduce leg rotational inertia and enhance motion performance, the knee joint incorporates a synchronou belt drive, while the ankle joint utilizes a Bowden cable drive. Wow Orin is equipped with a Jetson Orin NX (8GB) onboard PC, delivering an impressive AI performance of 40TOPS."}, {"title": "2) Lightweight Leg Design", "content": "To reduce the rotational in-ertia of the legs and further enhance motion performance, besides the aforementioned ankle joint rope drive method, the knee joint adopts a synchronous belt drive. In addition, we used carbon fiber as the skeleton for the robot's lower and upper leg links, 3D printed PETG material for some connectors, and aluminum alloy for key load-bearing parts to achieve the ultimate reduction in weight without sacrificing structural strength. Through the above design, the weight of the robot's single leg was reduced to 1.3kg. We designed the joint parameters as follows in the Table II."}, {"title": "3) Bionic Fishbone Bowden Cable Driven Ankle Joint", "content": "The design of the ankle joint is our main distinction from other robots, which use smaller mass motors in the ankle joint to reduce leg inertia, resulting in the maximum force generated by the ankle joint being significantly less than that of the thigh joint. According to research in literature, the torque generated by the human ankle joint during running is nearly equal to that of the thigh joint. Therefore, based on previous work, following the principle of mass concentration, and to meet the requirements of mass elevation in structural design, we adopted a Bowden cable to implement a series elastic actuation system, allowing the ankle actuator to be positioned anywhere outside the joint itself. This eliminates the need to consider the mass of the ankle joint motor affecting leg movement, significantly reducing leg inertia mass while greatly enhancing the torque of the ankle joint and significantly increasing the overall structural design flexibility. On our robot, the ankle joint power motor is placed in the upper body and transmits power to the ankle joint through the Bowden cable.\nWe adopted a bionic fishbone structure for the Bowden ca-ble (as shown in the Figure 4), which uses multiple segments closely connected by spherical joints to achieve structural flexibility and adjustability. By adjusting the number of segments, the total length of the structure can be rapidly changed, thus providing high adaptability. The interlayer part is made of a complete oil tube filled with lubricant, signif-icantly reducing the friction encountered by the steel wire during movement. Compared to traditional Bowden cable structures, this design not only significantly improves load-bearing capacity but also achieves extremely low friction."}, {"title": "IV. EXPERIMENTAL EVALUATIONS", "content": ""}, {"title": "A. Evaluation of the SERL Algorithm", "content": ""}, {"title": "1) Effectiveness of the Algorithm Optimization Process", "content": "In order to demonstrate the effectiveness of SERL algorithm in enhancing robot motion performance, we first define two different task forms as specific design objectives: comprehen-sive locomotion under perturbations and achieving maximum velocity. The two different tasks are represented by different task reward forms, denoted as rt. Through data analysis of the mean reward of individuals in the population and the maximum fitness in the population for both tasks, where population fitness is used to measure the maximum differ-ence in individual performance within the population and can effectively demonstrate the convergence of the population, we can effectively showcase the convergence effect of the population. We also record the thigh and shin length during the population evolution process. The convergence process of the above data reflects the improvement of policy and performance and the stable variation of design parameters."}, {"title": "B. Performance Testing of Wow Orin Robot", "content": "To assess the superior performance of Wow Orin and demonstrate its high energy efficiency and agility, we em-ployed the same algorithmic settings to train the control policy of Wow Orin, Cassie, and Unitree H1 in a flat simu-lation environment, followed by comparative tests of their Cost of Transport (COT), maximum velocity and Froude number[26]."}, {"title": "1) Energy Efficiency Comparison", "content": "The COT is a common metric used to gauge the efficiency of legged animals and robots, calculated as COT = P/(mgv), where P denotes joint power, m represents the robot's weight, g is the acceler-ation due to gravity, and v is the robot's forward velocity.As shown in Table III, through extreme lightweight design, Wow Orin achieves the lowest weight among the three robots, at only 10.5 kg. Additionally, it employs motors matched to the body mass for propulsion, resulting in minimal joint torque. The COT results demonstrate that Wow Orin significantly outperforms the other two robots, indicating its superior energy efficiency. This allows the robot to achieve longer endurance with limited battery capacity, which is crucial for tasks requiring prolonged movement. Since the calculation process of COT excludes the influence of the robot's weight, it indicates to some extent that the optimization of our robot's leg length design plays a crucial role in achieving a lower COT."}, {"title": "2) Maximum Velocity on Flat Ground", "content": "The Froude num-ber is a dimensionless quantity used to compare the dynamic similarity of different-sized robots or organisms under similar motion conditions [26], especially in walking or running scenarios. Its calculation formula is $Fr = v^2/gl$, where v is the speed of the robot's movement, g is the acceleration due to gravity, and I is the length of the robot's legs. We trained the three robots with the same parameters and environmental settings to achieve their maximum speed on flat ground. The results, as shown in the Table III, reveal that Wow Orin's maximum speed and Froude number exceeds that of Unitree H1 but falls short of Cassie's top speed. This difference may be attributed to Cassie's more explosive leg structure and the lower peak torque of the joint motors used in Wow Orin compared to Cassie's."}, {"title": "V. CONCLUSIONS", "content": "This study demonstrates the effectiveness of the SERL algorithm in the structural parameter design of bipedal robots, offering valuable insights for advancing this field. By combining reinforcement learning motion control policy with evolutionary algorithms, the SERL algorithm successfully identifies structural parameter values within the specified design space that best meet task requirements. Through comparison with other parameters within the design space, we validate the exceptional performance of the SERL algorithm in achieving theoretically optimal values. Furthermore, we apply the optimized results of the SERL algorithm to practical design, creatively developing the Wow Orin bipedal robot. In this paper, we primarily focus on optimizing the leg length as a starting point for the design. In the future, we will apply the SERL algorithm to other parameters and tasks, and improve the algorithm to achieve the co-optimization of multiple design parameters."}]}