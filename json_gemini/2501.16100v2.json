{"title": "Automated Detection of Sport Highlights from Audio and Video Sources", "authors": ["Francesco Della Santa", "Morgana Lalli"], "abstract": "This study presents a novel Deep Learning-based and lightweight approach for the automated detection of sports highlights (HLs) from audio and video sources. HL detection is a key task in sports video analysis, traditionally requiring significant human effort. Our solution leverages Deep Learning (DL) models trained on relatively small datasets of audio Mel-spectrograms and grayscale video frames, achieving promising accuracy rates of 89% and 83% for audio and video detection, respectively. The use of small datasets, combined with simple architectures, demonstrates the practicality of our method for fast and cost-effective deployment. Furthermore, an ensemble model combining both modalities shows improved robustness against false positives and false negatives. The proposed methodology offers a scalable solution for automated HL detection across various types of sports video content, reducing the need for manual intervention. Future work will focus on enhancing model architectures and extending this approach to broader scene-detection tasks in media analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "The growing volume of sports content across various platforms has increased the demand for automated tools to detect key moments, such as goals, penalties, or other notable events, and compile them into highlights (HLs). This task is essential for creating game summaries, enhancing viewer engagement, and improving content recommendation systems. However, manually annotating sports videos is time-consuming and labor-intensive, making efficient and scalable automated solutions crucial for the sports media industry.\nTraditionally, sports HL detection has relied on handcrafted features and rule-based approaches. For example, Xie et al. [1] proposed a framework combining domain knowledge and Hidden Markov Models (HMMs) to identify important events in soccer broadcasts. Similarly, Rui et al. [2] utilized game-specific heuristics, such as referee gestures and scoreboard overlays, to automatically extract potential HLs from baseball videos. These methods, while innovative for their time, required extensive manual tuning and lacked generalizability across different sports and video formats [3.\nRecent advancements in Deep Learning (DL) have revolutionized sports video analysis by enabling systems to learn meaningful patterns directly from data. For instance, Tran et al. [4] demonstrated the effectiveness of 3D Convolutional Neural Networks (3D CNNs) in capturing spatiotemporal features in video clips, which significantly improved action recognition tasks. Similarly, in [5], Rongved et al. developed a 3D CNN model for real-time sports event detection. However, these approaches are computationally expensive and often require large, labeled datasets to achieve optimal performance. Alternative methods, such as Channel-Separated Convolutional Networks (CSNs), attempt to reduce computational complexity by separating channel interactions from spatiotemporal interactions [6].\nIn parallel, the integration of audio data has been explored to enhance sports action detection. Vanderplaetse and Dupont [7] demonstrated that combining audio and video streams improved action spotting accuracy by capturing crowd reactions and commentator speech. While their work highlights the importance of multi-modal data fusion, their method processes raw audio streams, potentially missing finer nuances such as tonal variations in human voices.\nIn this context, we propose a dual-stream approach that leverages both video and in-stadium audio to improve sports highlight detection. Our method takes a simplified, lightweight approach to video analysis by using 2D Convolutional Neural Networks (CNNs) to focus on spatial feature extraction from video frames, reducing model complexity compared to 3D CNNs. The audio stream is transformed into Mel-spectrograms, a technique that captures human voice frequencies, allowing the model to detect commentators' and fans' reactions, which are strong indicators of key moments in sports events.\nUnlike previous work, our approach aims to achieve high accuracy with small datasets while maintaining computational efficiency. The proposed ensemble model combines predictions from both the video and audio streams, resulting in improved robustness against false positives and false negatives, making it adaptable to various sports and real-world scenarios. \nOur contributions can be summarized as follows:\n\u2022 We propose an efficient and lightweight video analysis method using 2D CNNs, achieving accurate highlight detection without the need for computationally expensive 3D models.\n\u2022 We utilize Mel-spectrograms to process game audio, focusing on human voice frequencies to capture commentators' and fans' reactions, which are strong indicators of key moments.\n\u2022 We introduce a multi-modal ensemble model that combines predictions from both audio and video streams, improving detection accuracy and robustness."}, {"title": "II. PROBLEM FORMULATION AND DATASETS CREATION", "content": "For developing Deep Learning (DL) models able to detect a chosen type of scene, from a given audio or video source, it is important to define the mathematical formulation of the scene-detection problem. Indeed, the problem formulation characterizes both the criteria for the dataset creation (see the next subsections), the model architecture (see Section III), and the model applications (see Section V).\nIn the following, we assume that the audio and video sources are recordings of professional football matches, while the type of scene we want to detect is the match highlights (HLs); of course, the same formulation can be generalized to any other kind of audio/video source (other sports, movies, etc.) and any other type of scene."}, {"title": "A. Problem formulation", "content": "Let A and V denote the sets of all the audio and video recordings of football matches, respectively. For each audio record $a \\in A$ and each video record $v \\in V$, we denote by $H(a)$ and $H(v)$ the sets of time intervals (in seconds) that identify the match highlights contained into a and v, respectively. More precisely:\n$H(s): = \\{[t^{(1)}_{start}, t^{(1)}_{end}],..., [t^{(N_s)}_{start}, t^{(N_s)}_{end}]\\} \\subset \\mathbb{N}$,\nwhere s is the type of recording (i.e., s = a ors = \u03c5), and where $t^{(i)}_{start} \\in \\mathbb{N}$ denote the starting second and the ending second of the i-th highlight, respectively; $N_s \\in \\mathbb{N}$ denotes the total number of highlights in the recording s.\nMoreover, we have $t^{(i)}_{start} < t^{(i)}_{end}$, for each i = 1,..., $N_s$, and $t^{(i)}_{end} < t^{(i+1)}_{start}$ for each i = 1, ..., $N_s$ - 1; indeed, we assume that highlights cannot have a length less than one second and we assume that they do not overlap.\nNow, let us define a highlight detection function for audio chunks of k seconds; i.e., we define the function $A_k : A_k \\subset A \\rightarrow \\{0, 1\\}$, for a fixed $k \\in \\mathbb{N}$, such that\n$A_k (a) = \\begin{cases}\n0 & \\text{if } H(a) = \\emptyset \\\\\n1 & \\text{otherwise}\n\\end{cases}$,\nwhere $A_k := \\{a \\in A \\mid a's \\text{ length is k seconds}\\}$.\nAnalogously, we define a highlight detection function for video chunks $V_k : V|k \\subset V \\rightarrow \\{0,1\\}$, for a fixed $k \\in \\mathbb{N}$, such that\n$V_k(v) = \\begin{cases}\n0 & \\text{if } H(v) = \\emptyset \\\\\n1 & \\text{otherwise}\n\\end{cases}$,\nwhere $V_k := \\{v \\in V \\mid v's \\text{ length is k seconds}\\}$.\nIt is easy to observe that the functions $A_k$ and $V_k$ can be correctly evaluated for a chunk only if the chunk's seconds are labeled to know if a highlight is happening or not. However, in real applications, we want to detect highlights inside unlabeled audio/video sources; for this reason, we train DL models with the task of approximating (1) and (2), using sets of labeled source chunks of length k.\nIn the next two subsections, we will describe how we build the audio and video datasets necessary for training the DL models, given a fixed size $k \\in \\mathbb{N}$ for the source chunks.\nRemark 2.1 (On the choice of k for building the datasets):\nWe observe that the smaller k, the more precise $A_k$ and $V_k$ are in detecting the highlights; i.e., if k is small, more probably we have only one highlight in the chunk and, if k is very small, the chunk can even coincide with a highlight-chunk. Then, the shorter the labeled chunks, the more precise the datasets. Nonetheless, the smaller k, the harder the training of a DL model in learning the detection task (because of less input information). Therefore, a careful choice of k is important for finding a good trade-off between the detection precision of the target function and the difficulty in training the DL model for learning it."}, {"title": "B. Audio Source Dataset", "content": "In DL, a good approach for working with audio sources is to transform them into Mel-spectrograms (e.g., see [8], [9], [10], [11], [12]). These spectrograms are computed with respect to the Mel scale[13], a scale useful to approximate the non-linear frequency response of the human ear; therefore, they are highly effective for analyzing human voices. In particular, the Mel-spectrograms are processed as images (see Figure 1), exploiting two-dimensional Convolutional Layers (CLs) [14].\nFor this reason, concerning the highlight detection task for audio sources, we build a dataset\n$D_{A_k} := \\{(X_1,Y_1), ..., (X_D, Y_D)\\}$\nsuch that $X_i \\in \\mathbb{R}^{p \\times q}$ is the Mel-spectrogram of $a_i \\in A_k$ and $Y_i = A_k(a_i)$, for each i = 1, ..., D.\nThe $D \\in \\mathbb{N}$ samples of audio chunks $a_i \\in A_k$ are taken by splitting a set of available audio recordings a \u2208 A with known H(a) and are selected in order to obtain a balanced dataset; i.e., 50% of data have label $y_i = 1$ and the other 50% have label $y_i = 0$, i \u2208 {1,..., D}."}, {"title": "C. Video Source Dataset", "content": "In DL, the typical approach for working with colored video inputs is to stack the video frames (RGB images, i.e., three-dimensional tensors) into four-dimensional tensors and process them through three-dimensional CLs [15], [16]. Such an approach is efficient but also very expensive, both in terms of computational costs and computational resources for storing the DL model and the data.\nFor performing our task (highlight detection) from video sources, we observe that colors are not so meaningful. Then, we can work with video chunks made of black-and-white frames (i.e., matrices of grey-scale pixels) stacked into three-dimensional tensors, representing the (black-and-white) video as an image with one channel per frame (see Figure 2); therefore, we can process these video chunks through two-dimensional CLs. This approach permits the reduction of the memory dedicated to the data and to work with simpler DL models, made of two-dimensional CLs instead of three-dimensional ones. Given this approach, we build a dataset\n$D_{V_k} := \\{(X_1, z_1), ..., (X_E, z_E)\\}$\nsuch that $X_i \\in \\mathbb{R}^{u \\times w \\times \\varphi k}$ is the tensor representation of the black-and-white video chunk $v_i \\in V_k$, made of $ \\varphi k$ frames (\u03c6\u2208 N is the frame-rate) of size u-by-w pixels. The labels are such that $z_i = V_k(v_i)$, for each i = 1, . . ., E.\nThe $E \\in \\mathbb{N}$ samples of video chunks $v_i \\in V_k$ are taken by splitting a set of available video recordings v \u2208 V with known H(v) and are selected in order to obtain a balanced dataset; i.e., 50% of data have label $z_i = 1$ and the other 50% have label $z_i = 0$, i \u2208 {1, ..., E}."}, {"title": "III. MODEL ARCHITECTURES", "content": "In this section, we describe the general idea behind the architectures used for the two DL models trained for learning the highlights detection task from audio chunks and video chunks, respectively, of length k seconds.\nRemark 3.1 (NDA and limited description of NN architectures):\nThe models described in this paper have been developed inside the Innovation Lab of the company Deltatre S.p.A., and the details of the NN architectures cannot be shared, because they are protected by a non-disclosure agreement (NDA). Nonetheless, we can report a general description of the models, to understand the fundamental characteristics of the NNs."}, {"title": "A. Model for Audio Sources", "content": "The architecture for the DL model that is dedicated to learning the highlight detection task from audio chunks is conceptually simple. Indeed, since the audio chunks have been encoded into Mel-spectrograms (see Section II-B), we can build the model only using two-dimensional CLs, Pooling Layers (PLs), and Fully-Connected (FC) layers. More clearly, we can divide the architecture into two main blocks: a first block consisting of a series of CLs and PLs and a second block that is a series of FC layers. At the end of the second block, we have a last FC layer made of just one unit with sigmoid activation function, because our target is a binary classification task (see (1))."}, {"title": "B. Model for Video Sources", "content": "The architecture for the DL model dedicated to learning the highlight detection task from video chunks is strongly based on transfer learning techniques [17]. Indeed, for this model, we can exploit the large number of high-performance trained models for image classification that are available in the literature or accessible in private repositories (e.g., in the case of a company).\nWe point the reader's attention to the fact that we base our model on an architecture used for image classification tasks, not on an architecture for video classification tasks. The reasons for this choice are two:\n1) Our video chunks are encoded as u-by-w images of \u03c6k channels, where each channel denotes a black-and-white video frame (see Section II-C). On the contrary, video classification models typically work with 4-dimensional tensors where the fourth axis denotes the video frames (that can be either black-and-white or RGB images);\n2) Image classification models, usually based on 2-dimensional CLs, PLs, and FC layers only, typically are lighter than video classification models, which can involve also 3-dimensional CLs.\nLet us denote with M the trained model for image classification that we use for transfer learning; we assume that the model is built for working with RGB images of the same height and width as our video frames (i.e., u-by-w-by-3 input tensors) and that it performs a multi-class classification task.\nThe operations that we apply to the model M for obtaining a model suitable for our video chunk classification task are listed in the following:\n\u2022 Adaptation to input video chunks: since M has been built for working with RGB images, we must modify its first 2-dimensional CL for working with our video chunks, without wasting the information contained in its kernel. Therefore, for each filter $K^{(1)},...,K^{(L)} \\in \\mathbb{R}^{m \\times n \\times 3}$ of this CL, we compute L new filters $\\tilde{K}^{(1)},...,\\tilde{K}^{(L)} \\in \\mathbb{R}^{m \\times n \\times \\varphi k}$ such that the weights in each channel of the new filter $\\tilde{K}^{(l)}$ are the same, and equal to the mean of the weights of $K^{(l)}$ along its depth; i.e., for each l = 1,..., L, and each \u03ba = 1,..., \u03c6k, we have that\n$(\\tilde{K}^{(l)}_{ij\\kappa}) := \\frac{1}{3} \\sum_{h=1}^{3} (K^{(l)}_{ijh})$\nThe meaning of (3) is that, for each fixed \u03ba and l, we have a one-channel filter that aggregates the information of the original filter for working with black-and-white images. Therefore, building the filter $\\tilde{K}^{(l)}$ by stacking \u03c6k times these one-channel filters, we obtain a filter that applies the knowledge learned by M to each frame of the input video chunk.\n\u2022 Adaptation to binary classification task: since M has been built for a multi-class classification task, we substitute its output FC layer (with softmax activation function) with a FC layer made of one unit and with sigmoid activation function.\n\u2022 Setting trainable and fixed weights: after the modification of the first CL layer and the substitution of the output layer, we must specify which layer weights are trainable and which are not. In a model like M, at the end of the training, the block of CLs and PLs typically results in an encoder for images, that returns an encoded version of the information inside the pictures; then, the block of FC layers process the encoded images for classifying them.\nFor these reasons, we will preserve the image encoding abilities of the block of CLs and PLs by freezing its layer weights (i.e., they are not trainable); on the other hand, since we perform a new classification task, we forget the values of the FC layers' weights, training them on our data from scratch."}, {"title": "IV. TRAINING AND PERFORMANCE EVALUATION OF THE MODELS", "content": "In this section, we briefly describe the training procedure of both models introduced in Section III. Then, we report their performance with respect to a test set.\nRemark 4.1 (NDA and limited description of NN training options): As already discussed in Remark 3.1, the models described in this paper have been developed inside the Innovation Lab of the company Deltatre S.p.A. and they are protected by an NDA. Therefore, we cannot describe the details of the training procedures executed for obtaining the final, trained models. In particular, we will anonymize any quantity to hide their original values without losing the possibility of describing the procedure and the results."}, {"title": "A. Training of the Models", "content": "Given a dataset $D_{A_k}$ of labeled audio chunks of k seconds (encoded as Mel-spectrograms, see Section II-B) and a dataset $D_{V_k}$ of labeled video chunks of k seconds (encoded as stacked black-and-white images, see Section II-C), k = 5 fixed, we randomly split each of them into training set, validation set, and test set. We recall that the chunks have been selected from highlight-labeled football match videos and are such that the datasets are balanced with respect to the labels; therefore, we can assume that the training, validation, and test sets are balanced, too. In order to give an idea of the size of the datasets used, we have that the cardinality of $D_{A_k}$ is made of approximately 700 audio chunk samples, while $D_{V_k}$ is made of of approximately 1000 video chunk samples.\nConcerning the training of the models, we do not adopt strategies for searching optimal hyper-parameters and/or training options. We focus the training of each model on a single configuration of hyper-parameters and training options; these configurations have been selected after a brief preliminary investigation. We postpone to future work a deeper study, intending to further improve the performances reported in Section IV-B.\nIn the following list, we report a general description of the training configurations of the models:\n\u2022 Audio model. Dataset split: ~50% training, ~10% validation, ~40% test; Minibatch size: ~4.5% of the training set's cardinality; Regularization: early-stopping method.\n\u2022 Video model. Dataset split: ~70% training, ~10% validation, ~20% test; Minibatch size: ~1.2% of the training set's cardinality; Regularization: early-stopping method."}, {"title": "B. Performance of the Models", "content": "The performances observed on the test sets are promising for both the audio model and the video model, with an accuracy of approximately 89% and 83%, respectively. We also observe that the models have not only a good accuracy (that in these cases coincides with the recall index) but also good precision in detecting the chunks that intersect a highlight action (i.e., a low rate of false positives/negatives among predictions). Moreover, these performances are not unbalanced with respect to the positive or negative labels."}, {"title": "V. APPLICATION OF THE MODELS FOR AUTOMATED HIGHLIGHTS DETECTION", "content": "In this section, we illustrate the pipeline to automatically detect highlights in a video file using the models described in the previous sections. We recall that the advantage of using such kind of procedure, based on our DL models, is the notable reduction of human efforts required for performing a selection of highlight moments in a football match (see Section I).\nIn a nutshell, given a video chunk of length h seconds, h > k, we apply the models on a sliding window of k seconds, estimating the possibility of having an intersection between the window and a highlight (the audio model is applied only to the audio file extracted from the video). Then, for each second of the video, we compute the average possibility of being part of a highlight, given all the estimates that the second received for each window that includes it. The detailed pipeline is illustrated in Subsection V-A (items 1-4); nonetheless, here we point the attention of the reader to the following details:\n1) Continuous values for predictions: we are not classifying the windows (and the seconds, as a consequence) with a label 1 or 0, but we use the rough output of the last layer of the model, characterized by a sigmoid activation function (i.e., values in [0, 1] \u2282 R). Then, the average predicted score $H_s(\\tau) \\in [0, 1]$ for the second \u03c4 of the video file represents the possibility that \u03c4 belongs to a highlight time interval with respect to source s, $s \\in \\{\u03b1, \u03c5\\}$.\n2) Explanation of the predicted score's meaning: as written in the item above, the average predicted score for the second \u03c4 represents the possibility for that second of belonging to a highlight time interval. Therefore, we can observe some \u201cunexpected-but-correct\u201d scores in certain cases. For example, we can observe cases where a second \u03c4 with a large score corresponds to a moment of silence in the audio source, but only because after a few seconds (in the range of the k seconds of the model's time window), a team suddenly scores a goal and the audio source \u201cexplodes\u201d. A similar example can be observed for the video sources; e.g., when we have large scores for a second \u03c4 corresponding to an apparently harmless action of a team that unexpectedly scores a goal a few seconds later.\nWe want to emphasize that this kind of prediction is an advantage and not a disadvantage for the applications of the model. Indeed, a typical highlight does not focus on the single instant that characterized the scene (e.g., the instant of the goal), but it is a chunk that comprehends the start of the action that originated the key event and that continues with the reactions of players, commentators, and supporters to it.\nWith a preliminary application of the models to the video files, using the pipeline described at the beginning of this section, we observed that each model may suffer from false positive/negative detections in some specific contexts. For example:\n\u2022 Audio false positives/negatives. The audio model often recognizes as probable highlights those audio chunks where the spectators at the stadium and/or the commentators are particularly noisy. Therefore, for example, when chants last for a long period after a goal, the model can consider these chunks as chunks containing part of a highlight (false positive). On the other hand, the model can fail to detect chunks containing a highlight moment that is characterized by a not-so-loud noise; for example, when most of the spectators and the commentators are supporters of the team that concedes a goal.\n\u2022 Video false positives/negatives. The video model often recognizes as probable intersections with highlights those video chunks where the camera is near the goal of a team, while rarely recognizes intersections with highlights for chunks where the camera is focused on spectators (or similar behaviors). Therefore, the video model may consider as a probable highlight intersection a chunk where the goalkeeper is alone and is kicking the ball (false positive example); on the other hand, the model may not consider as a highlight intersection a chunk where supporters exult after a goal of their team (false negative example).\nFrom these observations, we noted that false positive/negative audio-based detections are typically compensated by true positive/negative video-based detections, and vice-versa. Therefore, we decided to build an ensemble model, based both on audio and video sources and both models for notably improving the prediction performances. In particular, the final estimate of highlight possibility for a second \u03c4 is the average between the audio average score and the video average score; in this way, the new ensemble model can reduce both false positive and false negative detections, with respect to the models that compose it (see 5 in Subsection V-A, below)."}, {"title": "A. Detailed Pipeline", "content": "Here, we illustrate in detail the pipeline used to automatically detect highlights from a video file, using the audio model, the video model, or the ensemble model (i.e., audio and video models together). Let us denote by $M_a$, $M_v$, and $M_e$ the audio, video, and ensemble models, respectively. Then, the pipeline consists of the following steps:\n1) Separate the audio source and the \"only-video\" source in the given video file; then, transform the video source into a black-and-white video (if the original file was in RGB colors) and adapt the frame-rate to \u03c6 (frame-rate of Mv).\n2) Split the sources in time windows of k seconds and compute the Mel-spectrograms of each audio window. The overlap between the windows is arbitrary; obviously, the larger the overlap, the more precise the detection, and the more memory is occupied by data.\n3) For each source s = a, v and for each k-seconds window w, estimate the highlight possibility rate with the model Ms (i.e., the raw output in [0, 1] \u2282 R). We denote the highlight possibility rate of w, with respect to the sources, as $M_s(s(w))$; where s(w) denotes the k-seconds interval of source s corresponding to w and properly encoded for being a model's input.\n4) For each second \u03c4 in the time length of the video file, evaluate its average estimated highlight possibility rate assigned from the models Ma and Mv; i.e., for each s = a, v, evaluate the quantity\n$H_s(\\tau):= \\frac{1}{T} \\sum_{i=1}^{T} M_s(s(w^{(i)}))$,\nwhere $w^{(1)},..., w^{(T)}$ are all and only the time windows that include the second \u03c4.\n5) For each second T in the time length of the video file, evaluate its estimated highlight possibility rate assigned from the model Me; i.e., compute the quantity\n$H_e(\\tau) := \\frac{1}{2} (H_a(\\tau) + H_v(\\tau))$\n6) Classify as a highlight scene with respect to Ms, each second of the video that has value $H_s(\u03c4)$ greater that a chosen threshold \u2208 \u2208 (0, 1), for each s = a, v, e."}, {"title": "B. Application Examples", "content": "We conclude the section by illustrating two examples, where the models Ma, Mv, and Me (trained with respect to a time window of k = 5 seconds) are applied to two video clips, with a highlight detection threshold \u2208 = 0.5. The two video clips have been taken from the Soccernet's public repository (see [18], [19]). In particular, we want to show the efficiency of the ensemble model Me.\nThe first clip is characterized by one goal action. This action is fast (~ 3 seconds), starting with a cross far from the goalpost and ending with a volley. Nonetheless, the ensemble model Me successfully identifies a highlight that starts with the cross and lasts until the end of the celebrations. In Figure 3 we illustrate the predictions of all the models, emphasizing the predictions of the ensemble model and adding video frames to explain the analysis better. In particular, looking at Figure 3, we can see that the audio and video predictions are almost always coherent, except after the goal and before the zoom-in on the celebrating players. In this situation, it is thank to the enthusiasm of the commentator (and the choirs of spectators) that the audio model keeps its predictions high and, therefore, the ensemble model identifies a better highlight (i.e., not only focused on the action, but also on the celebrations).\nThe second clip we consider is characterized by two consecutive actions (of the same team): the first action terminates with a failure, but they maintain control of the game and start a second action, scoring a goal. Also in this case, the ensemble model Me performs very efficiently, even if a very brief false highlight (~ 2 seconds) is detected during a throw-in of the Goal-Keeper at the beginning of the clip (see \"GK throw-in\" in Figure 4). This kind of false positive can be easily avoided automatically; e.g., by introducing a criterion of \u201cminimum length\u201d for the highlights. Focusing on the two actions contained in this clip, we observe that the ensemble model identifies a unique highlight because there is not a true break between the two. Nonetheless, it is thanks to the \"roar of the crowd\" if the two actions are predicted as one unique highlight; indeed, this sound maintains high the predictions of the audio model, permitting the ensemble model to identify one \"exciting\" highlight (see the rightmost part of the blue line in Figure 4). On the other hand, the video model, ignoring the sounds, identifies two actions (see the spike of the orange line in Figure 4); in particular, the video model is responsible for a good choice of the start of the true highlight (see \"Action Start\" in Figure 4)."}, {"title": "VI. CONCLUSION", "content": "In this work, we presented an approach for developing Deep Learning models able to automatically detect chosen types of scenes in videos. In particular, we focused on the case of highlight detection in recordings of professional football matches. We worked with separated audio and video sources and we formalized the highlight detection problem for both of them.\nWe approached the detection from audio sources by developing from scratch a Convolutional NN model that, for each audio chunk of k seconds, predicts if it intersects a highlight time interval or not; specifically, the model receives as inputs the Mel-spectrograms of the audio chunks.\nFor detecting highlights from video sources, we proposed to work with greyscale video frames and stack them into tensors, obtaining \u201cmulti-channel\u201d images that describe video chunks of k second. Then, we developed a Convolutional NN model strongly based on a pre-trained NN model for image classification tasks.\nBoth the trained models have good performances on the test sets, but they may suffer from false positive/negative predictions, depending on the situation. Therefore, we developed a third ensemble model, with predictions that are the average of the audio model predictions and the video model predictions.\nWe concluded the work with two experiments. We applied the trained models to two video clips. The results obtained are extremely promising, even if there is still potential for performance improvements.\nIn conclusion, we presented a methodology for developing efficient and reliable Deep Learning models for fast, cheap, and automated highlight detection in recordings of professional football matches.\nFuture work may focus on improving the NN architectures and on increasing the amount of training data, for improving the detection performances. Moreover, the development of an automated tool for highlight detection based on these models will be of critical interest; analogously, it would be interesting to extend this methodology to other scene-detection applications."}]}