{"title": "Answering real-world clinical questions using large language model based systems", "authors": ["Yen Sia Low", "Michael L. Jackson", "Rebecca J. Hyde", "Robert E. Brown", "Neil M. Sanghavi", "Julian D. Baldwin", "C. William Pike", "Jananee Muralidharan", "Gavin Hui", "Natasha Alexander", "Hadeel Hassan", "Rahul V. Nene", "Morgan Pike", "Courtney J. Pokrzywa", "Shivam Vedak", "Adam Paul Yan", "Dong-han Yao", "Amy R. Zipursky", "Christina Dinh", "Philip Ballentine", "Dan C. Derieg", "Vladimir Polony", "Rehan N. Chawdry", "Jordan Davies", "Brigham B. Hyde", "Nigam H. Shah", "Saurabh Gombar"], "abstract": "Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2% - 10%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs. 0-9%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.", "sections": [{"title": "Introduction", "content": "Evidence based medicine, the conscientious, explicit, and judicious use of current best evidence in making decisions about the care of individual patients, has been the standard for the last three decades\u00b9. However, in some specialties less than 20% of daily medical decisions are supported by quality evidence\u00b2. The gap between the need for evidence and the availability of evidence in care decisions is driven by two issues. First, the clinical trials often lack generalizability\u00b3, to complex patients who often fail to qualify for trials4,5. Such evidence gaps between trials and real-world settings motivate the need for timely and relevant real world evidence (RWE) to guide care and treatment decisions 6,7. Second, even when studies exist, they frequently have conflicting findings or are so numerous it is difficult to summarize them for a given patient8,9. As a result physicians are often in the situation where they require either summarized evidence from reliable sources or custom evidence generated directly taking into account the patient in front of them.\nLLMs have increasingly been looked upon as potential sources of such evidence, summarizing from prior literature learned during LLM training10\u201312. While LLMs have displayed impressive performance in responding to natural language queries in various medical domains 13,14, they are prone to hallucinating reference materials or treatment guidelines 15,16 and may produce \u201crecommendations\u201d that are satirical or inappropriate 17.\nOne approach to adapt LLMs for reliable evidence summarization is the use of a retrieval augmented generation (RAG), where an LLM is used to compile information retrieved from curated knowledge sources. 18 A clinician could submit a clinical question to a LLM and receive summaries of relevant publication (trial reports and observational studies) and practice guidelines retrieved from the knowledge base. This approach is used by OpenEvidence (https://www.openevidence.com) to answer clinical queries via RAG using a LLM. However, in such a RAG system, the answers are limited to pre-existing evidence sources.\nAlternatively, an approach that would enable an LLM to generate on-demand evidence is to use a LLM with an agent, combining a natural language interface with an evidence generation platform with access to medical record data. In such an agentic system, the LLM would serve as a co-pilot to match clinical intent to an underlying purpose-built agent for generating evidence7,19. ChatRWD \u2122\u2122 (https://www.atroposhealth.com/chatrwd) uses this approach for answering clinical queries via an LLM chat interface. This LLM-driven user interface translates clinical queries into a structured"}, {"title": "Methods", "content": "Figure 1 outlines the evaluation process. First, we selected 50 questions as the basis for evaluation (Question Selection) then submitted them to ChatGPT-4, Claude 3 Opus, Gemini 1.5 Pro, OpenEvidence, and ChatRWD. All of the LLM-based systems except ChatRWD provided supporting citations that were then checked for hallucinations (Citation Review). Because ChatRWD performs a new study on demand, the intermediate code generated by ChatRWD for patient cohorting was reviewed by trained clinical informaticians (Study Integrity Review). The output of each system was graded according to a standard medical rubric by a panel of clinicians (Clinical Review)."}, {"title": "Question Selection", "content": "We selected 50 questions (Table S1) that were either received from physicians requesting additional evidence for clinical decisions or inspired by such questions. All questions met the following following criteria:\n\u2022 Study is a comparative cohort study;\n\u2022 All four PICO components can be fully defined;\n\u2022 Control group is an active comparator rather than absence of treatment;\n\u2022 Question is not evaluating treatment route, dosage, duration, or line of therapy;\n\u2022 Intervention does not require washout from prior intervention.\nEach question was also assessed for its novelty. This was determined by a consensus vote among three independent reviewers who searched the medical literature for keywords from each question. A question was considered to be novel if the search did not yield any obvious matches for combination of disease indication, treatment, control, and outcome."}, {"title": "Response Generation", "content": "LLMs\nWe tested three general-purpose LLMs (ChatGPT-4, Claude 3 Opus, and Gemini Pro version 1.5) to produce answers to the selected clinical questions. OpenAl's ChatGPT21 is an instruction-tuned pre-trained transformer model designed to produce human-like text in response to natural language instructions. Anthropic's Claude22 is a suite of Al language and image models trained within a governance framework of pre-specified rules and principles for generative output. Gemini23 from Google is a suite of natively multimodal Al models designed to interact with and generate multiple data types including text, audio, and images.\nWe submitted questions to ChatGPT, Claude, and Gemini via their REST APIs. We provided a standardized prompt followed by the clinical question (Fig. S1). The prompt, inspired by previous research24, directed the model to serve as a helpful assistant with medical expertise. Additionally, in order to facilitate evaluation and understand its reasoning, we specifically asked the models to cite any referenced studies and respond with \"I do not know the answer\" when that was the case.\nOpenEvidence\nOpenEvidence uses an LLM for RAG with different types of medical literature, including PubMed articles and FDA drug labels to answer clinical questions submitted via the web or its API25. Using RAG on existing medical literature in this manner reduces the likelihood of hallucinating information, because OpenEvidence can summarize the relevant literature retrieved and present the conclusions to the requester. The final"}, {"title": "ChatRWD", "content": "At its core, ChatRWD includes an agent for cohort selection coupled with statistical analysis to ensure results are not hallucinated. ChatRWD adds four steps to the agent: 1) Chain-of-Thought prompting to convert plain English questions into PICO format, classify the study design, and perform named entity recognition (NER), 2) semantic search of a curated phenotype library, 3) generation of Temporal Query Language (TQL)26 code to do the cohort-selection and invoke an underlying purpose-built platform for statistical analyses, and 4) summarization of findings from statistical analyses (https://www.atroposhealth.com/chatrwd). Users can confirm and modify the inferred PICO as well as the retrieved phenotypes via a web interface (Fig. S2) before the study is executed. The data source used with ChatRWD (Eversana's Electronic Health Record Integrated Database) consisted of electronic health records of 63 million patients from outpatient and inpatient providers in the United States, including structured medication, laboratory, procedure, and diagnosis data."}, {"title": "Evaluation of Al-generated responses", "content": "Citation Review\nFour of the LLM systems tested (ChatGPT, Claude, Gemini, and OpenEvidence) can cite from the medical literature. We checked the validity of these citations before passing the responses on to our clinical reviewers (Clinical Review). Since OpenEvidence provides links for each citation, we were able to verify the citations by confirming that each link directed to the appropriate study. For the general LLMs, we verified citations by determining if the relevant article could be located on PubMed. We first searched for combinations of author, title, journal, and year of the citation via the PubMed API. For citations without matches, we manually searched PubMed using either the article title alone or the whole reference from the LLM responses. For any citations that were still unmatched, we checked the provided URL from the LLM, if any. Citations still unmatched at that point were considered to be hallucinations.\nStudy Integrity Review\nChatRWD does not return citations because the study is run on-demand using RWD. Therefore, we conducted a Study Integrity Review instead. ChatRWD involves rule-based generation of TQL code 26 defining a study cohort. For each of the 50 questions, two medical informaticists reviewed the phenotypes and the TQL code for each PICO element to ensure their appropriateness, selecting from one of three grades:"}, {"title": "Clinical Review", "content": "Nine physicians across several specialties graded the responses from all five LLM systems. Before grading, all reviewers underwent training on the use of the standardized rubric (Table S2) using several case studies.\nFirst, they graded along three primary dimensions (response generation, relevance, and evidence quality) to which reviewers had to rate: \u201cyes\u201d, \u201cno\u201d or optionally \"mixed\u201d for the relevance and evidence quality dimensions (Table S2). We also asked reviewers to evaluate the actionability of each response by selecting \u201cyes\u201d or \u201cno\u201d to indicate whether the response was of sufficient quality to justify or change their practice. Additionally, after reviewing all five responses to each question, reviewers were asked which was the best response.\nBecause the various LLM systems all have easily identifiable response structures, the physicians were not blinded to the system that generated the answer. The physicians reviewed the content independently and were not able to see the responses of their peers."}, {"title": "Data analysis", "content": "We first aggregated the ratings along the three primary dimensions from the nine clinical reviewers based on a majority vote. From the combination of these aggregated ratings, we then binned each response into one of five exhaustive and mutually exclusive response categories according to the logic shown in Table 1a.\nWe considered a given response to be actionable if at least five reviewers classified it as high enough quality to justify or change practice. The response receiving the highest number of votes from the nine reviewers, was deemed the 'best', with ties being allocated to both equivalent LLM systems.\nWe calculated the inter-rater agreement on response category using Krippendorff's Alpha27 and on the best and actionable metrics using Fleiss' Kappa 28.\nFor each of the five LLM systems, we assessed an LLM-specific inter-rater reliability of the nine clinical reviewers across 50 questions. We also computed overall inter-rater agreement across all 250 combinations of the 50 clinical questions and five LLM systems."}, {"title": "Results", "content": "Relevance, Reliability, and Actionability\nThe five LLM systems differed widely in their ability to produce relevant, evidence-based results (Table 1a, Fig S3). The LLMs (ChatGPT, Claude, and Gemini) only produced an answer for 58% to 78% of the questions. In comparison, the RAG and agentic systems produced answers for 86% (OpenEvidence) and 94% (ChatRWD) of the questions. When LLMs did produce answers, they rarely gave responses judged to be relevant and evidence-based, meeting this standard for 2% to 10% of the responses. OpenEvidence and ChatRWD, in contrast, respectively produced relevant, evidence-based answers for 24% and 58% of the questions.\nAdditionally, when considering the stricter criterion for actionability (i.e., of sufficient quality to justify or change clinical practice), the reviewers rarely found the LLM responses (2-4%) to be actionable, while responses from OpenEvidence (30%) and ChatRWD (44%) were more often judged to be actionable (Table 1b). Reviewers most often rated ChatRWD (60%) as providing the best answer, followed by OpenEvidence (46%). None of the LLMs provided the consensus best answer across reviewers.\nFailure Analysis\nTo understand the causes of poor relevance and poor evidence, we tallied the reasons selected by the clinical reviewers when grading the clinical rubric (Table 2). A major reason for poor relevance was study design mis-specification by ChatRWD (44.7%) and ChatGPT (37.9%, Table 2b). Mis-specification by ChatRWD almost always stemmed from ill-defined phenotypes and sometimes logical errors (e.g., misinterpreting \u201cand\u201d for \u201cor\u201d logic in drug combinations). Some examples of phenotyping errors were when migraine medications included antibiotics (question 9) and when surgery for lower extremities included procedures for upper extremities (question 1).\nIn contrast, the key reason for the general-purpose LLMs' failure was their frequency of including hallucinated or irrelevant citations, which reviewers identified in 40-80% of relevant responses (Table 2c). Over 40% of all citations from Claude and Gemini could not be located on PubMed, as well as 25.5% of ChatGPT's citations (Table S3). When"}, {"title": "The role of question novelty", "content": "To test our hypothesis that ChatRWD would outperform OpenEvidence particularly on novel questions for which ChatRWD can generate new studies, we stratified questions by their novelty and compared the relative performance of ChatRWD and OpenEvidence (Fig. 2, Table 3). Among the novel questions, ChatRWD could produce answers that were actionable (52.2%) as well as answers that were relevant and evidence-based (65.2%). When faced with novel questions, OpenEvidence was rarely able to produce actionable answers (8.7%) or answers that were relevant and evidence based (8.7%).\nConversely, on questions that have existing literature, the comparative gap narrowed (37% relevant and evidence-based, 48.2% actionable by OpenEvidence vs 51.9% relevant and evidence-based, 37.0% actionable by ChatRWD). ChatRWD was more likely to generate answers of varying quality while OpenEvidence would at worst provide partially relevant and partially evidence-based answers (Fig. 2, Table 3, Fig. S4).\nThis was echoed by clinical reviewers who preferred OpenEvidence when there was existing literature to draw on but noted that ChatRWD was superior when existing"}, {"title": "Discussion", "content": "To practice evidence-based medicine it is essential that physicians have rapid access to reliable summarization of trusted published literature as well as have a way to generate on-demand evidence when existing literature does not address the decision at hand. We demonstrated that special-purpose LLM systems when augmented with specialized knowledge (24%, OpenEvidence) or agents (58%, ChatRWD) far outperform off-the-shelf LLMs (2-10%) in producing relevant and evidence-based answers for the clinical questions examined.\nThe poor performance of general LLMs in this setting of generating RWE is due to several reasons. LLMs hallucinate 29 and use non-credible sources. LLMs cannot appraise sources for relevance, quality and trustworthiness, a critical task for RWE. Particularly harmful is when LLMs are so adept at mimicking the corpora they have been trained on that it becomes difficult to distinguish fact from fiction. Indeed, we observed that probable authors, journals and article titles were often composed together into non-existent citations, making up nearly 40% of citations reported. Further, general-purpose LLMs are not designed for the complex tasks required for RWE generation: study design classification, PICO extraction and clinical NER. Even LLMs that perform well on medical benchmarks such as the USMLE 30 may not generalize across all medical tasks 31. Finally, LLMs cannot provide responses to novel medical questions whose answer is in content created after the LLMs have been trained. One solution is using RAG to augment LLM with external data sources like the PubMed knowledge base to provide recent evidence sources to draw from, as OpenEvidence has demonstrated.\nHowever, because there is a considerable time lag between asking a clinical question and publishing a comparative study to answer the question, relying only on past studies is inadequate. Given that almost half of our selected questions are novel, there is a huge need to rapidly conduct new studies on demand. These new studies are motivated by patients with specific preexisting conditions and medications who do not qualify for clinical trials. It is precisely for such real-world patients that clinicians struggle to find relevant and high quality evidence that studies have to be performed on demand 32.\nThus, it is unsurprising that ChatRWD, specifically designed to conduct comparative studies on demand, outperformed the general LLMs and the RAG-based OpenEvidence particularly for novel questions. This makes OpenEvidence and ChatRWD as complementary tools. OpenEvidence can provide relevant, evidence-based responses to questions that have existing literature, often making use of high-quality sources such as randomized controlled trials and meta-analyses. On the other hand, ChatRWD can generate new evidence for questions that have not previously been studied in the published literature. In combination, these two tools provided relevant, evidence-based answers to 66% of the questions and were deemed actionable 60% of the time."}, {"title": "Conclusions", "content": "Pertinent evidence remains difficult to obtain for many patient care decisions. Challenges in obtaining evidence stem from two sources: 1) nearly 80% of care decisions lack high quality evidence due to no specific study being available\u00b2 and 2) difficulty in contextualizing available studies for the specific intricacies of the patient at hand. While LLMs excel at summarizing and contextualizing existing literature, either internalized during training or retrieved from external RAG sources, they cannot perform new real-world studies for the exact question at hand unless integrated with an agent to do so. By evaluating the response of five LLMs and having independent reviewers evaluate them for relevance, reliability, and actionability, we demonstrated general-purpose LLMs are not fit for the task of providing evidence for clinical decisions. However, a combination of purpose-built literature retrieval and agent based system to perform on-demand studies can do a fair job of surfacing relevant, reliable, and actionable evidence. As these systems continue to improve, it is likely they can be integrated into the physician workflow to enable true evidence-based practice."}, {"title": "Author contributions", "content": "SG, NHS, BH, YL, NS conceived of the study, defined the main outcomes and measures. YL, MJ, RH, and RB drafted the manuscript. NS, JB, SG selected the questions and ran them through ChatRWD. CD ran the questions through OpenEvidence. RH designed the LLM prompts and ran the questions through the general LLMs. RH, SG, medical reviewers searched the literature to corroborate the responses from the LLM systems. SG, JM and NS reviewed the inferred PICO and TQL code. The PICO and TQL evaluation criteria were designed by SG and NS. The medical review rubric was designed by SG, RH, NS, JB and YL. SG trained the clinical"}, {"title": "Competing Interests", "content": "ChatRWD, the LLM system evaluated in this study, is developed by Atropos Health where many of the authors are employed. Nigam H Shah is not an Atropos Health employee but sits on its board. OpenEvidence, another LLM system evaluated here, is provided by OpenEvidence whom we consulted during the writing of this manuscript. Non-Atropos employees, Natasha Alexander, Hadeel Hassan, Rahul V Nene, Morgan Pike, Courtney J. Pokrzywa, Shivam Vedak, Adam Paul Yan, Dong-han Yao, and Amy R Zipursk, have nothing to disclose."}]}