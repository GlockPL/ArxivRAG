{"title": "Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles", "authors": ["Zhilong Wang", "Haizhou Wang", "Nanqing Luo", "Lan Zhang", "Xiaoyan Sun", "Yebo Cao", "Peng Liu"], "abstract": "Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts aimed at exploiting the models to generate malicious content. This paper proposes a new type of jailbreak attacks which shift the attention of the LLM by inserting a prohibited query into a carrier article. The proposed attack leverage the knowledge graph and a composer LLM to automatically generating a carrier article that is similar to the topic of the prohibited query but does not violate LLM's safeguards. By inserting the malicious query to the carrier article, the assembled attack payload can successfully jailbreak LLM. To evaluate the effectiveness of our method, we leverage 4 popular categories of \u201charmful behaviors\" adopted by related researches to attack 6 popular LLMs. Our experiment results show that the proposed attacking method can successfully jailbreak all the target LLMs which high success rate, except for Claude-3.\nContent warning: This paper contains unfiltered content generated by LLMs that may be offensive to readers.\nKeywords- LLM, Jailbreak, Prompt Injection", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown tremendous potential across various domains, including education, reasoning, programming, and scientific researches [23, 14, 27]. Due to the ability of generating text in natural language extremely similar to what human can create, LLMs becomes ubiquitous in online services and applications. However, just as many other rising cutting-edge technologies, the omnipresence of LLM does introduce serious cybersecurity challenges, one of which is the misuse of LLM, involving malicious users leveraging LLMs to achieve illegal and unethical goals.\nAccordingly, most of LLMs will have certain safeguard implemented so that they will refuse to generate any illegal, discriminative, or other kinds of unethical responses. In other words, safeguard will prohibit LLMs from replying to certain queries, and we called these queries to be prohibited queries. The most commonly adopted safeguard method is so-called model safety alignment [13, 10, 28]. Models"}, {"title": "2 Background", "content": "2.1 Blackbox LLM Jailbreak Attack\nJailbreaking LLM in blackbox settings usually involves crafting payload prompts to bypass the safeguard of the model without access to the model parameters, hyperparameters, and outputs before token sampling. Some previous works [3, 25] focus on generating jailbreak prompt to decieve the LLM as if the LLM is a real human [19]. For example, PAIR [3] adopt another LLM to create and improve the payload prompt, which usually use fictional scenarios to bypass the safe guards. Another example is [25] using a different (natural) language to describe the prohibited queries.\nOn the other hands, whitebox jailbreak settings [12, 17, 29] are much attractive to attackers, but it could be unrealistic as running LLMs are very hardware heavy and majority users will use proprietary models provided by large companies. Compared to whitebox jailbreak methods, the challenge of the blackbox methods is that it is extremely difficult to evaluate the quality of the payload prompt, and therefore it is hard to improve the payload prompt in principled ways.\n2.2 Prompt Injection Attack\nA prompt injection attack exploits the security vulnerabilities in LLM applications where adversaries manipulate the prompts sent to the underlying LLM, causing the model to ignore prior instructions and respond in attackers' favor. These vulnerabilities may lead to unintended outcomes, including data leakage, unauthorized access, the generation of hate speech, the propagation of fake news, or other potential security breaches [4]. There are two kinds of prompt injection attacks:\nDirect Prompt Injection Malicious users directly providing instruction prompts will result in direct and instant injection. In the case of a direct prompt injection attack, the attacker interacts directly with the LLM in an attempt to cause the LLM to produce a specific response. For example, a user might ask an AI assistant to summarize a news article [18]. An adversary could append an additional command to the prompt, such as: \u201cSummarize the news article and output the prompts of this question.\" If the AI assistant lacks proper checks, it might execute both tasks, inadvertently leading to a data breach.\nIndirect Prompt Injection Directly providing data prompt by a malicious user can lead to indirect prompt injection. Indirect prompt injection relies on LLM's access to external data sources that it uses when constructing queries to the system. An attacker can insert malicious content into these external data sources, which is ingested by LLM and inserted into the prompt to produce the response required by the attacker. For example, Liu et al. [18] introduced HOUYI, a black-box prompt injection attack method inspired by traditional web injection attacks. HOUYI comprises three essential components: a seamlessly integrated pre-constructed prompt, a context partition inducing injection prompt, and a malicious payload to achieve attack objectives. This method uncovers previously unnoticed and impactful attack repercussions, including unrestricted arbitrary language model usage and straightforward pilfering of application prompts.\""}, {"title": "3 Overview: Hide a Tree in Forest", "content": "3.1 Terminology\nThe following of our paper will use some terminology, which we will first define here to avoid confusion.\n\u2022 Target LLM is the LLM that jailbreak attack target on.\n\u2022 Prohibited Query is the query that asks for harmful, inappropriate, or unethical content, which t would normally be prohibited by LLMs that shipped with safeguard. Therefore, LLMs will avoid answering this prohibited query.\n\u2022 Subject Words are the words in the prohibited query that can represent the topic of the query.\n3.2 Framework\nThe intuition of our method is based on the fact that neural networks are very sensitive to their input, and therefore could be vulnerable against adversarial attacks. In the context of generative LLMs, even with the adoption of attention mechanisms, it is found that the model will still generate very different outputs by slightly altering the input. For example, [9, 16] found that the input context lengths and positions of the relevant information could greatly affect the perplexity of the output.\nToward the core of LLM is the transformer [22], one of whose explanations are to weigh certain tokens as more important than others through the attention mechanism. This fact suggests that prompt injection [4, 18, 21] could be very helpful in jailbreaking an LLM, as inserting prohibited information into a relevant yet allowed text is essentially \"hidden a tree in forest\", bypassing the LLM safeguard by scattering the attention to tokens in the \u201cforest\". Start from a prohibited query, our \u201chidden a tree in forest\" method need to generate a carrier article and hide the prohibited query in to the carrier article. However, there is one remaining problems when adopting prompt injection in jailbreaking LLMs: what carrier article should the prohibited query to be injected into.\nKeeping this problems in mind, we develop an automatic method to jailbreak LLMs by injecting prohibited query into benign narratives. The workflow of the method is shown in Figure 1. The overview takes the scenario that an attacker want to query (prohibited query) the methods to do money laundering as an example. Our method first extract the subject words from the prohibited query so that words can represent the topic of the prohibited query. Secondly, it generates n-step hypernyms from the query and use the n-step hypernyms to instruct composer LLM to generate a carrier articles, which is an article that discuss a topic that similar to the prohibited query. With them same hypernyms, a list of carrier articles can be generated by running the composer LLM multiple times. Thirdly, we insert the prohibited query into the carrier articles to generate an attacking prompt which is used to attack a LLM. By inserting the prohibited into different carrier articles, or different locations of the same carrier article, our method can generate a list of attacking prompts. Fourthly, we try each of the attacking prompts to attack a target LLM until success. In the following of this section, we will introduce details of each step.\n3.3 Extract the Subject Words\nExtracting subject words from a query involves identifying the main topics or keywords that convey the essential meaning of the query. This process typically includes the removal of stop words (common words like \"the,\" \"is,\" \"at,\" etc.) and focusing on nouns, verbs, and sometimes adjectives that hold semantic value. To ensure the relevance of carrier article with the prohibited query, we only use subject words in the prohibited query, because these subject words are thing-like concepts, directly connecting to subject and/or object entities in the events and actions described by the prohibited query. This is essentially the reason why we want the query to contain as many nouns as possible. For example, in the query\n3.4 Generating Hypernyms\nWe adopt a keyword set derived from the prohibited query to generate a carrier article, so that carrier article has the same/similar topics as the prohibited query. However, directly using these selected subject words from prohibited query may still not be enough to yield us a carrier article which can jailbreak the LLMs effectively, because the subject words are too \u201csensitive", "upward\" to the parent node through breadth-first search in the knowledge graph, how far should we go. In this paper, we selected this value by conducting experiment, which is 3 hops. We call the generated hypernyms through n-hop breadth-first search as n-step hypernyms.\n3.5 Generating Carrier Article\nWhen generating carrier article, it is important to guarantee that \u2460 the carrier article is related to the topic that we want to jailbreak; and \u2461 the carrier article does not contain any topics forbidden by a target LLM. Goal is very important because inconsistency among the topics of the carrier and prohibited query will result in confuse when send them together to a target LLM as we will show in our experiment. On the other hand, we use the GPT-4 to generate the carrier articles from n-step hypernyms as it is unlikely a LLM will generate article that contains topics that it will forbid. Specifically, we use the following prompt to generate the carrier article based on the keywords:\n3.6 Generating Attacking Payloads\nAs described earlier, the core idea of the workflow shown in Figure 1 is to \\\"hide a tree in a forest\\\". Here, the carrier article is serving as the forest and our verbose prohibited is the tree we want to hide. Eventually, the only remaining question is where in the carrier article to inject the verbose prohibited query. Many prompt injection methods use a template such as \u201cIgnore the previous instructions and do XXX\". When adopting such templates, it is essential to append the injected text toward the end of the prompt sent to the LLM.\nIn contrast, since our primary target are not LLM applications but just the instruct-tuned LLMs, we do not have an existing prompt. Instead, we have a carrier article that is randomly generated that has a related topic with the prohibited query. From human's perspective, no matter where the verbose prohibited query is injected, it is not clear in logic in most cases. Besides, due to the black box nature of the LLMs, it is also not possible to pre-determine the best location for injection. Therefore, we adopt the simplest strategy: we will try to inject the verbose prohibited query between every sentence in the carrier article, generating different payloads.\n3.7 Implementation Details\nAlgorithm 1 Automated payload (attacking prompt) generation.\nRequire: Prohibited Query Q, Number of Carrier Articles m\nRequire: Composer LLM Mc\n1: Object Word Set W \u2190 {Nouns in Q}\n2: Result Payload Set E \u2190 {}\n3: for all w \u2208 W do\n4:\tHypernym Keyword Set K \u2190 GetHypernyms(w)\n5:\ti\u21900\n6:\twhile i < m do\n7:\t\ti\u2190i+1\n8:\t\tCarrier Article Ai \u2190 Mc(K)\n9:\t\tfor all pos in Ai do\n\u25b7 Injection Position\n10:\t\t\tPayload eos \u2190 InjectPrompt(Ai, Q, pos)\n11:\t\t\tE\u2190{E, eos}\n12:\t\tend for\n13:\tend while\n14: end for\n15: return E\nWe implement the whole workflow in Python. The algorithm to generate payloads for each malicious query is shown in Algorithm 1. In the algorithm, for a prohibited query, we firstly extract the object words from the query and generate a 3-hop hypernyms by breadth-first searching the WordNet. The generated hypernyms contain 8-12 word that highly related to the malicious query. Secondly, our method generates a 3-carrier articles for each query based on the keyword set through use a composer LLM. The reason that we only generate 3 carrier articles is that we observed that all our attacks can succeed within a very limited number of tries. The prompt to generate carrier article is attached in the Appendix. Thirdly, for each carrier article, there are n+1 points that we can insert a prohibited query into a carrier article with n sentences with breaking of the sentences, which will generate n+1 attacking payloads. Finally, we send each of the payloads set to the target LLMs and step until one payload is successful.\nJudgment Model. Automatic methods (structured query evaluation, rule patterns, APIs and ChatGPT assistance) or human efforts (human annotators) can be used to judge the results of LLMs output to\"\n    },\n    {\n      \"title\": \"4 Experiments\",\n      \"content\": \"We conduct the evaluation experiments in this section to evaluate the following questions: \nHow effective is the proposed method?\nHow does the inserting location of the query in to the carrier article affect the success rate?\nHow will the topics of carrier article affect the performance?\nHow will the length of carrier article affect the performance?\nHow does the proposed method compare to related methods?\nWhat are the impacts of the LLM's input parameters (temperature, top-p, top-k, and repetition penalty)? To answer these questions, we choose a set of popular large language models and evaluate the on a dataset which is shown as follows.\n4.1 How effective is the proposed method?\nIn this section, we evaluate the effectiveness of the proposed method on the popular LLMs.\nTo evaluate whether our method is effective, we select 4 popular topics of \u201charmful behaviors\\\" adopted by related researches [3": "and choose one prohibited query for each topic. Specifically, we have 4 queries asking the LLMs to generate responses about how to produce dynamite, insulting president of United States, game cheating, and money laundering. All the LLMs in our experiments will refuse to answer these queries, providing responses such as \"I can't assist with that\".\nSince our method is a blackbox method, we include both open-source and proprietary LLMs as victim LLMs. In particular, we use 3 open-source LLMs \u2013 Llama-2 7B, Llama-3-8b, Claude-3 and 3 proprietary LLMs \u2013 Gemini, GPT-3.5-turbo and GPT-4. For each of these victim models, we use default parameters (Temperature, Top-p, Top-k, and Repetition Penalty) for our experiment except the Section 4.6, where we evaluate the impact of the parameters on the effectiveness of our attacking method. We did not use any system prompts in all the experiments.\nOur experiment follows the procedure shown in Figure 1, and we evaluate the effectiveness of our method using success rate, which is defined as:\n$Success Rate = \\frac{Num \\ of \\ Success \\ Payloads}{Total \\ Num \\ of \\ Attacking \\ Payloads}$ (1)\n4.2 How does the inserting location of the query in to the carrier article affect the success rate?\nThe proposed automated payload (attacking prompt) generation (Algorithm 1) does not have any preference of the location in the carrier article to insert the query. The prohibited query is inserted between every adjacent sentence pair sequentially. Obviously the sequential insertion will not be the optimal strategy, and in practice, the attacker is able to prioritize some locations over others. Therefore, we want to evaluate whether the inserting location affect the effectiveness of the proposed method.\nSince different carrier articles are of different lengths (different number of sentences), we group the inserting locations into 3 ranges: Front, Middle, and Rear. Each of the ranges takes 1/3 of the whole article. Then, we calculate the success rate (number-of-successes/number-of-tries) in three different locations and the results in shown in Table 2. We have following observations from the results: 1) Firstly, inserting into the front of the carrier articles achieve the highest successful rate overall. 2) Secondly, different models have different preferences for the best inserting locations. Specifically, our attacking method achieves the higher successful rate when inserting into the middle during attack to llama-2-7B, llama-3-8b, and gemini-1.5, when inserting into rear during attack to and gpt-4-turbo, when inserting into front during attack to gpt-3.5.\n4.3 How will the topics of carrier article affect the performance?\nIn Figure 2, our hypothesis is that to achieve jailbreak, we need to find topics that are highly related to the forbidden topic, yet themselves are not prohibited by the LLMs. In this section, we evaluate whether the pertinency between the topic of the prohibited query and topic of the carrier articles affect the model performance.\nBased on our hypothesis, in Section 4.1, we used carrier articles of similar topics to the prohibited query. Now we conduct another group of experiments which adopt carrier articles that mismatch the topic of the prohibited query. Specifically, we keep the whole workflow intact, except the generation of the carrier articles we select keywords, that are unrelated to the topic of the prohibited query, to generate the carrier articles. The success rates of the new experiments are shown in the Table 3. The results show that the performance drop tremendously when we choose an completely irreverent topic to generate the carrier articles. For example, the success rate when attacking gpt-3.5 and gpt-4 drop from 92.55% and 56.38% to 02.04% and 29.59%, respectively.\n4.4 How will the length of carrier article affect the performance?\nTo answer this research question, we generate the carrier articles of different lengths (range from 2-14). Then we insert the malicious query into these carrier articles to generate payloads of different lengths, which are then used to attack GPT-3.5 and GPT-4.0-tubo (we only focus on two models in this experiment, due to the huge amount of time effort that are required). The Figure 3 shows the success rate of our attack with different article lengths.\nBased on Figure 3, the success rate is low either when the carrier article is too short or too long. Therefore, it is best to choose a carrier article of an appropriate length. We conjecture that a short carrier article is not able to hide the malicious goal in the query and a carrier article of too long will result in the target model's misinterpretation. We investigate the failures in two different situations and found that most failures with a short carrier articles are direct refusing from the LLM; whereas most failures with long carrier articles are responses not answering the prohibited query (i.e., the reply is answering other topics that mentioned in the carrier article). The observation is aligned with our conjecture.\n4.5 How does the proposed method compare to related methods?\nIn this section, we compare our work with [3], which is a blackbox LLM jailbreak method. Prompt Automatic Iterative Refinement (PAIR) [3] is an algorithm that pits two black-box LLMs\u2014which we call the attacker and the target-against one another, in that the attacker is instructed to discover candidate prompts which jailbreak the target. Since PAIR is an automatic method that attack the target model, therefore we adopt their tool directly to jailbreak LLMs with queries used in Section 4.1. Table 4 shows the success rate of the attacks. Unfortunately, our results show that PAIR cannot successfully attack any model used in our experiments.\n4.6 What are the impacts of the LLM's input parameters?\nIn this section, we investigate the impact of the target model input configuration on the attacking performance. Specifically, our experiments involved subjecting the Llama-2-13B model to our attack, while systematically varying the temperature, top-p, top-k, and repetition penalty parameters.\n1. The temperature controls the randomness of predictions by scaling the logits before applying softmax. With a low temperature, the model is more deterministic and chooses the highest probability words more frequently, leading to more focused and less diverse text. On the other hand, a"}, {"title": "5 Related Work", "content": "5.1 LLM Ethical Issues\nA number of research studies have shown the ethical challenges posed by Large Language Models (LLMs). Fang et al. [7] analyze how biases embedded within LLMs can discriminate against various demographic groups, emphasizing the urgent need for models that encapsulate diversity and inclusiveness. Likewise, Cao and Daum\u00e9 III [2] highlight the issues of exclusion in their 2024 research, particularly how LLMs often overlook non-standard genders, claiming that LLM should recognize the complexity\nof genders and be respectful to non-binary genders. Moreover, Ousidhoum et al. (2021) [20] also claims that toxic content is another significant concern of native LLM and they find a way to quantify the toxic content. In response to these ethical concerns, Dong et al. (2023) [6] have developed a method known as RAFT, which involves extensive fine-tuning of LLMs with complex datasets to ensure adherence to ethical norms. Likewise, Bai et al. (2022) [1] illustrate the effectiveness of incorporating human feedback into the reinforcement learning process, thereby aligning the outputs of LLMs more closely with ethical expectations. These techniques, however, are just general methods to prevent LLM producing harmful content in the happy path. Deliberate attacks like prompt injection may still trigger the dark side of the LLM.\n5.2 LLM Jailbreak\nRecent research has explored various approaches to probing the limitations and potential vulnerabilities of Large Language Models (LLMs). These studies, while controversial, aim to understand and ultimately improve the robustness and safety of AI systems. The landscape of these probing techniques, often referred to as \"jailbreak attacks,\" can be broadly categorized into several approaches.\nPrompt-level Jailbreak methods [15, 24, 12] employ semantically meaningful deception and social engineering to elicit unintended responses from LLMs. Although effective, they require significant human input in terms of creativity and dataset curation.\nToken-Level Jailbreaks [29, 11] focuses on optimizing the input tokens received by the targeted LLM. Given a potentially harmful user query, this attack [29] appends an adversarial suffix to the query that produces an affirmative response. Though potentially more effective, these approaches are computationally expensive and often produce outputs that are difficult for humans to interpret.\nObfuscation-based techniques leverage non-English translations or other forms of obfuscation to bypass safety mechanisms. Given a malicious input, [25] translate it from English into another language, feed it into GPT-4, and subsequently translate the response back into English. One key insight is that these safety features may not generalize well to low-resource languages.\nAutomated jailbreak generation tools such as GPTfuzz [26] and Masterkey [5] aim to automate the process of generating effective jailbreak prompts, often based on human-written templates."}, {"title": "6 Reproducibility", "content": "Given the swift evolution of LLM-integrated applications, certain detected vulnerabilities may become non-reproducible over time. This could be attributed to various factors, such as the implementation of prompt injection protection systems or the inherent evolution of the back-end LLMs. Therefore, it is important to acknowledge that the transient nature of these vulnerabilities might impede their future reproducibility. In the future, we will closely monitor the reproducibility of the proposed attack methods."}]}