{"title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments", "authors": ["Yunzhe Xu", "Yiyuan Pan", "Zhe Liu", "Hesheng Wang"], "abstract": "Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for trajectory summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion rate on Touchdown dataset. This work showcases the potential of Multimodal LLMS (MLLMs) in complex navigation tasks, representing an advancement towards practical applications of MLLMs in embodied AI. Project page: https://flame-sjtu.github.io", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Achiam et al. 2023; Touvron et al. 2023) have revolutionized the field of embodied intelligence. Vision-and-Language Navigation (VLN) (Anderson et al. 2018), a fundamental task in embodied AI, challenges agents to navigate to a goal following human instructions in indoor or outdoor environments. This task demands sophisticated abilities in instruction comprehension, environmental understanding, and decision-making, which can be effectively managed by LLMs. Recent approaches have integrated LLMs into VLN methods, either by translating visual data into language (Zhou, Hong, and Wu 2024; Pan et al. 2024; Qiao et al. 2023; Lin et al. 2024a; Chen et al. 2024) or by employing Multimodal LLMs (MLLMs) (Zhang et al. 2024; Zhou et al. 2024) for environmental perception. However, the incorporation of general-purpose LLMs in VLN still faces critical challenges, primarily stemming from their inherent limitations in navigation-specific scenarios, as Figure 1(a) depicts. For text-only LLMs, translating visual data into language using visual foundation models can lead to information loss (Zhou, Hong, and Wu 2024), resulting in a huge performance gap compared to VLN-specialized models. For Multimodal LLMs, while partially addressing the limitations of text-only LLMs, they often struggle to interact within navigation-specific scenarios. Recent attempts to incorporate MLLMs for navigation focus on integration with down-stream models (Zhou et al. 2024) or intensive training on navigation videos (Zhang et al. 2024), rather than making efficient adaptations to navigation-specific scenario that would harness the MLLMs' inherent capabilities in handling interleaved textual and visual inputs, leading to suboptimal performance. Moreover, the application of MLLM in urban VLN remains unexplored, despite its importance alongside indoor VLN. Outdoor navigation presents unique challenges for introducing MLLM, including longer trajectory lengths (up to 55 iterations) and increased difficulty (40% lower success rate compared to indoor navigation tasks). To address these challenges, we introduce FLAME (FLAMingo-Architected Embodied Agent), the first MLLM-based agent designed for urban VLN tasks, as shown in Figure 1(b). Based on Flamingo (Alayrac et al. 2022), FLAME operates autoregressively and efficiently handles multiple perceptions without increasing context length, ensuring efficiency in end-to-end training and inference. We propose a three-phase tuning technique to adapt Flamingo model (Alayrac et al. 2022) to navigation tasks using augmented data: 1) Single perception tuning: Learning to describe street views. 2) Multiple perception tuning: Learning to summarize agent trajectories. 3) End-to-End training and evaluation on VLN datasets. To support the first two tuning phases, we utilize GPT-4 (Achiam et al. 2023) to synthesize captions and route summaries for the Touchdown environment (Chen et al. 2019). Additionally, we synthesize navigation rationales for urban VLN datasets (Chen et al. 2019; Schumann and Riezler 2021) to validate FLAME's reasoning capability (Wei et al. 2022). Experimental results demonstrate FLAME's superiority over existing methods on two urban VLN datasets: Touchdown (Chen et al. 2019) and Map2seq (Schumann and Riezler 2021). Our approach significantly outperforms current state-of-the-art (SOTA) methods by 7.3% TC in Touchdown and 3.74% TC in Map2seq, establishing new standards for urban VLN tasks. Our work not only benefits the field of urban VLN but also showcases the potential of MLLMs in navigation within complex environments and tasks. In summary, our contributions are threefold: \u2022 We introduce FLAME, to our knowledge, the first agent based on Multimodal LLM (MLLM) for urban Vision-and-Language Navigation (VLN) tasks. \u2022 We propose a tailored three-phase tuning technique for adapting Flamingo into navigation scenarios using synthetic data, fully unleashing MLLM's power. \u2022 Experiments show the superiority of FLAME over current SOTAS. FLAME's performance proves that MLLMs can significantly outperform specialized models, opening new avenues for research in embodied AI."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Vision-and-Language Navigation", "content": "Vision-and-Language Navigation (VLN) (Anderson et al. 2018) encompasses indoor (Qi et al. 2020; Krantz et al. 2020) and outdoor scenarios (Chen et al. 2019; Schumann and Riezler 2021), with advancements in multimodal understanding (Tan, Yu, and Bansal 2019; Hong et al. 2021; Zhu et al. 2020) and action strategies (Qi et al. 2020; Chen et al. 2022b), primarily for indoor environments. Traditional VLN agents often lack advanced decision-making skills, prompting the integration of Large Language Models (LLMs) (Lin et al. 2024b,a; Zhan et al. 2024b; Schumann et al. 2023), leveraging the reasoning (Pan et al. 2024; Chen et al. 2024; Lin et al. 2024a) and dialogue capabilities (Qiao et al. 2023; Long et al. 2024) of LLMs. These approaches either convert visual data to text (Zhou, Hong, and Wu 2024) or employ Multimodal LLMs (MLLMs) with intensive training (Zhang et al. 2024). Given the relative underexploration of outdoor VLN, our work addresses this gap by introducing an effectively adapted MLLM-based agent for urban VLN tasks."}, {"title": "2.2 Multimodal Large Language Models", "content": "The emergence of Multimodal Large Language Models (MLLMs) (Liu et al. 2024; Wang et al. 2023a; Zhu et al. 2024; Alayrac et al. 2022; Awadalla et al. 2023) has expanded their application to various tasks, including captioning (Li et al. 2023b), grounding (Peng et al. 2023), and general instruction following (Dai et al. 2023; Bai et al. 2023). These models demonstrate multimodal reasoning skills (Lu et al. 2022; Zeng et al. 2023; Lu et al. 2024) in chat conversations, handling single-turn, in-context (Sun et al. 2024), and interleaved text and image inputs (Lauren\u00e7on et al. 2024a; Jiang et al. 2024). MLLMs have benefited from vision-and-language tuning (Lauren\u00e7on et al. 2024b), enabling them to process diverse modalities (Zhan et al. 2024a; Wu et al. 2023). However, the general pretraining is insufficient for expert navigation tasks. Our work addresses this gap by adapting an MLLM from general scenarios to navigation through a specialized tuning approach."}, {"title": "2.3 Data Augmentation in Vision-and-Language Navigation", "content": "To overcome data scarcity in navigation, various data augmentation techniques (Chen et al. 2022a; Ku et al. 2020; Zhao et al. 2021; Huang et al. 2019b,a) have been proposed. These include utilizing a speaker module (Fried et al. 2018; Dou and Peng 2022) to generate synthetic instructions, leveraging multilingual data (Li, Tan, and Bansal 2022a), incorporating counterfactual information (Wang et al. 2022; Parvaneh et al. 2020; Fu et al. 2020), and altering environments (Li, Tan, and Bansal 2022b; Liu et al. 2021). For outdoor VLN, previous studies have explored training agents on instructions with different styles (Zhu et al. 2021), pre-training on auxiliary tasks (Armitage, Impett, and Sennrich 2023) and using driving videos (Li et al. 2024). However, the use of auxiliary training data to tailor MLLMs for outdoor navigation remains largely unexplored. Our work addresses this gap by using synthetic data to adapt MLLMs for urban VLN tasks."}, {"title": "3 Method", "content": "We present FLAME (Flamingo-Architected Embodied Agent), an agent for urban Vision-and-Language Navigation (VLN). Our method comprises three key components: 1) An architecture for urban navigation. 2) A three-phase tuning technique. 3) A synthetic data creation process."}, {"title": "Task Formulation", "content": "We formalize urban VLN as follows: Given a navigation instruction $I = \\{W_1, W_2, ..., w_n\\}$, an agent starts in an initial state $S_0$. At each timestep t, the agent selects an action"}, {"title": "3.1 FLAME Architecture", "content": "FLAME builds upon the Flamingo architecture (Alayrac et al. 2022), leveraging cross-attention for processing of visual and textual inputs, without extending context length and improves efficiency. We introduce two key adaptations to Flamingo for urban VLN:\nStrided Cross-Attention To handle the large number of observations in urban VLN, we implement strided cross-attention (Child et al. 2019) in the cross attention layer, as depicted in Figure 2(b). The Perceiver Resampler module transforms CLIP (Radford et al. 2021) features into a compact set of visual tokens, with $N_1$ tokens per observation. Let $X \\in \\mathbb{R}^{N\\times d}$ represent flattened visual tokens up to timestep t, where $N = N_1 t$. The LM block outputs a word embedding matrix $X_t$, with $X_t$ as the the segment of context corresponds to t, and l as the stride length. Pattern $S = \\{S_1, ..., S_t\\}$ defines visual token indices for attention at t, with $S_t = \\{k, k+1, ..., t\\cdot N\\}$ and $k = max(0, (t-1)\\cdot N)$. The strided cross-attention score is given by:\n$A(X_t, S_t) = CMA(X_tW_Q, (x_jW_K)_{j\\in S_t}, (x_jW_V)_{j\\in S_t}),\\quad\\quad\\quad\\quad (1)$\nwhere CMA denotes cross-modal attention, $W_Q, W_K, W_V$ are learnable parameters, and $x_j$ refers to the j-th visual token. This approach is aimed at prioritizing recent observations, thereby augmenting the system's proficiency in identifying and responding to significant features in the environment dynamically.\nAction Prediction As Figure 2(c) shows, FLAME operates autoregressively, predicting actions based on the initial instruction I, current observation $O_t$, history of observations $O_{<t-1}$ and previous actions. By default, at each timestep t, the next action $a_t$ is obtained by:\n$a_t = MLLM(I, O_1, a_1, ..., O_{t-1}, a_{t-1}, O_t).\\quad\\quad (2)$\nOptionally, to make agent's thought process transparent and understandable, FLAME generate rationales at key locations (e.g., intersections) before action prediction, similar to ReAct (Yao et al. 2023). Let $R_t$ denote the generated rationale, the next action can be obtained by:\n$a_t = MLLM(I, O_1, a_1, ..., O_{t-1}, a_{t-1}, O_t, R_t).\\quad (3)$"}, {"title": "3.2 Three-Phase Tuning for Navigation", "content": "To adapt Flamingo for urban VLN tasks, we propose a three-phase tuning paradigm, as depicted in Figure 3(a). This approach progressively builds the model's capabilities from understanding environment to complex decision-making.\nSingle Perception Tuning In the first phase, we train FLAME on a street view captioning task to develop its feature recognition abilities. Given a dataset $D_{p1} = \\{(\\iota^{(i)})\\}_{i=1}^N$, where each instance $\\iota^{(i)} = \\{(P^{(i)}, O^{(i)}, c^{(i)})\\}$ consists of a task prompt $P^{(i)}$, an observation $O^{(i)}$, and a ground truth caption $c^{(i)}$, the training objective is defined as:\n$\\mathcal{L}_{p1} = - \\sum_{i=1}^N log\\,p(c^{(i)}| P^{(i)}, O^{(i)}; \\theta),\\quad\\quad\\quad\\quad (4)$\nwhere $\\theta$ represents learnable parameters of the model.\nMultiple Perception Tuning The second phase focuses on synthesizing information from sequential observations, crucial for navigation understanding. Using a trajectory summary dataset $D_{p2} = \\{(\\iota^{(i)})\\}_{i=1}^N$, where $\\iota^{(i)} = \\{(P^{(i)}, O^{(i)}_{1}, ..., O^{(i)}_{T^{(i)}}, s^{(i)})\\}$, we minimize:\n$\\mathcal{L}_{p2} = - \\sum_{i=1}^N log\\,p(s^{(i)}| P^{(i)}, O^{(i)}_{1}, ..., O^{(i)}_{T^{(i)}}; \\theta).\\quad\\quad (5)$\nHere, $s^{(i)}$ is the ground truth summary, and $T^{(i)}$ is the number of images in the i-th instance.\nEnd-to-End Navigation Tuning The final phase finetunes FLAME on the VLN dataset $D_{vln} = \\{(\\iota^{(i)})\\}_{i=1}^N$, where each instance $\\iota^{(i)} = \\{(I^{(i)}, O^{(i)}_{<t}, a^{(i)}_{<t}), ..., (O^{(i)}_{T^{(i)}}, a^{(i)}_{T^{(i)}})\\}$ contains an instruction $I^{(i)}$, observations $O^{(i)}_{t}$, and ground truth actions $a^{(i)}_{t}$. We minimize:\n$\\mathcal{L}_{vln} = - \\sum_{i=1}^N \\sum_{t=1}^{T^{(i)}} log\\,p(a^{(i)}_{t} | I^{(i)}, O^{(i)}_{<t}, a^{(i)}_{<t}; \\theta).\\quad (6)$\nThis multi-phase approach progressively builds FLAME's capabilities, from basic urban environment understanding to complex decision-making, enabling it to effectively process navigational instructions and environmental cues."}, {"title": "3.3 Synthetic Data Generation", "content": "To support the finetuning of our agent, we leverage LLMs to automatically synthesize street view captions, route summaries (Figure 3(b)), and navigation rationales (Figure 3(c)).\nStreet View Caption Generation We focus on generating captions for street views at key locations, which are crucial for successful navigation. These images are processed by GPT-4V (Achiam et al. 2023) to produce corresponding descriptions. To ensure diversity, we randomly select meta-prompts from a curated list, guiding the caption generation process.\nTrajectory Summary Generation We construct a comprehensive knowledge graph of landmarks by combining places of interests from OpenStreetMap with additional map information, following the methodology of (Schumann and Riezler 2021). Using this graph as input, we employ GPT-4 to generate detailed route summaries between key locations.\nRationale Generation for VLN Datasets To validate reasoning capabilities of our model, we generate synthetic rationales for the VLN datasets. The process involves segmenting each VLN trajectory into sub-routes between key locations, then retrieving the corresponding image caption and sub-route summary for each key location. We then utilize GPT-4 to generate a synthetic rationale at each key location, based on the retrieved information. To maintain data quality, we discard instances containing rationales that contradict ground truth actions. This results in a altered subset of the VLN dataset augmented with synthetic rationales, which facilitates end-to-end training with reasoning capabilities."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setup", "content": "Datasets We evaluate our approach on two urban Vision-and-Language Navigation (VLN) datasets: Touchdown (Chen et al. 2019) and Map2seq (Schumann and Riezler 2021), both set in the StreetLearn environment (Mirowski et al. 2018). Touchdown contains 9,326 instruction-trajectory pairs, while Map2seq comprises 7,672 pairs. The augmented dataset for the first two training phases contains 2,354 and 4,674 instances, respectively. Our agent is benchmarked against others on the original datasets. We collect 6,518 (out of 9,326) and 6,291 (out of 7,672) pairs grounded with rationales at key locations using GPT-4 for Touchdown and Map2seq, respectively. We formulate synthetic pairs to create a dataset for evaluating reasoning capabilities. The reasoning performance is evaluated exclusively on the subset containing synthetic rationales to ensure a fair comparison. Metrics For the original VLN datasets, we employ three metrics for performance evaluation: Task Completion (TC), Shortest-Path Distance (SPD) and Normalized Dynamic Time Warping (nDTW). Specifically, TC represents the percentage of successful navigation. SPD calculates the minimum distance from the stop location to the goal. nDTW assesses the overlap between the agent's and the ground truth trajectories. To further evaluate the agent's reasoning capabilities with synthetic rationales, we introduce two new metrics: \u2022 Rationale Coherence (RC):\n$RC = \\frac{\\sum_{i=1}^N \\sum_{j=1}^{M_i} CFR_{rc}(I_i, \\gamma(R), R)}{\\sum_{i=1}^N M_i} \\quad\\quad\\quad\\quad (7)$\n\u2022 Rationale-Action Alignment (RA):\n$RA = \\frac{\\sum_{i=1}^N \\sum_{j=1}^{K_i} CFR_{ra}(R, A)}{\\sum_{i=1}^N K_i} \\quad\\quad\\quad\\quad (8)$\nHere, $I_i$ is the i-th instruction, $R$ is the agent's rationale, $A$ is the action, and $\\gamma(R)$ is the ground truth rationale at the j-th key location. $M_i$ and $K_i$ are the number of key location overlaps and visited key locations, respectively. N is the total number of instances. Functions $CFR_{rc}(\\cdot)$ and $CFR_{ra}(\\cdot)$ use GPT-4 (Achiam et al. 2023) to evaluate rationale consistency and action alignment (true or false), respectively. Implementation Details Our agent is built upon Otter and OpenFlamingo (Li et al. 2023a; Awadalla et al. 2023), integrating CLIP (Radford et al. 2021) and LLaMA (Touvron et al. 2023), trained on a single A100 GPU. To accommodate CLIP's input size, we center-crop and resize panoramas, which differs from the broader visual context used in existing models but aligns better with MLLM's nature."}, {"title": "4.2 Comparison with SOTAS", "content": "In the section, we compare FLAME with previous state-of-the-art (SOTA) approaches. As shown in Table 1, FLAME establishes new SOTA performance on both Touchdown and Map2seq datasets. On Touchdown test split, FLAME surpasses the previous SOTA, Loc4Plan (Tian et al. 2024), by 7.3% in TC and %1.97 in SPD. This demonstrates the superiority of our MLLM-based approach in comprehending navigational instructions and environmental cues, resulting in higher success rates and better path adherence. For Map2seq, FLAME outperforms VELMA (Schumann et al. 2023), an LLM-based method, with a 3.74% increase in TC and a 5.35% improvement in nDTW. This highlights the advantage of Multimodal LLMs in capturing comprehensive information compared to text-only LLM, which may introduce information loss when translating visual data. Furthermore, we evaluated open-sourced methods in our visual setting, which features a restricted field of view similar to human vision. The results, marked with an asterisk (*) in Table 1, show a significant performance drop for baseline models, especially on the Touchdown dataset. This underscores the dependence of baseline methods on panoramic environmental perception. In contrast, FLAME excels at navigating with limited visual input, more closely approximating human navigation capabilities."}, {"title": "4.3 Reasoning Performance", "content": "We evaluated FLAME's reasoning capabilities during navigation using the self-consistency approach (Wang et al. 2023b), exploring various decoding paths and temperatures. The results are presented in Table 2. Rationale Coherence (RC) and Rationale-Action Alignment (RA) consistently remained above 80% and 95% respectively, indicating robust rationale generation and strong consistency. Higher temperatures led to performance fluctuations, especially with fewer decoding paths. However, when we increase the number of decoding paths to 8, we see more pronounced improvements in both TC and RC. On Touchdown, FLAME achieved optimal TC performance with a temperature of 1.0 and 8 decoding paths, outperforming greedy decoding (T=0.0, P=1) by 1.72% in TC and 1.75% in RC. For Map2seq, with 8 decoding paths, FLAME surpassed greedy decoding by 3.91% in TC and 3.62% in RC on the test set. These results demonstrate that increased sampling diversity and a larger decoding budget enable FLAME to generate diverse rationales and effectively ensemble reasoning results, leading to improved decision-making. This provides strong evidence for the reasoning capabilities developed through synthetic rationale tuning and the advanced decision-making capacity of FLAME's architecture.\nMetric Calculation To validate the reliability of our automatic metric calculations (Eqs. 7 and 8), we conducted human evaluations on 50 instances each from the Touchdown and Map2seq datasets. The results, presented in Table 3, revealed discrepancies between the vanilla method and human assessments. However, our calibrated approach significantly reduced these disparities, demonstrating the human-comparable reliability of the automatic evaluation method. Consequently, we employ this calibrated evaluation technique as default. Specifically, We calibrate the outputs of $CFR_{rc}$ and $CFR_{ra}$ as follows: \u2022 When action at key location is incorrect: If $CFR_{ra} = 1$ but $CFR_{rc} = 1$, we force $CFR_{rc}$ to 0. \u2022 When action at key location is correct: If $CFR_{rc} = 1$ but $CFR_{ra} = 0$, we force $CFR_{ra}$ to 1."}, {"title": "4.4 Analyses", "content": "This section presents a comprehensive analysis of our approach, evaluating the strided cross-attention module, the three-phase tuning technique, and providing qualitative insights of navigation details.\nEffect of Strided Cross Attention To investigate the effect of the strided cross attention module, we conduct experiments on different stride sizes on both original datasets and subsets in Figure 4. We observe that increasing stride size generally correlates with a decrease in task completion rates, highlighting the importance of prioritizing current observations over longer history in decision-making processes. Paying full attention to numerous observations appears to overwhelm the agent, negatively impacting performance. Based on these findings, we set the default stride size to 1 in our implementation. However, nDTW scores for Map2seq fluctuate rather than decline consistently. This suggests stride size may be less critical for path-related performance in tasks less"}, {"title": "Qualitative Analysis of Navigation", "content": "To illustrate the navigation capabilities of FLAME and compare it with the baseline method (Schumann and Riezler 2022), we present a qualitative example in Figure 5. In this instance, ORAR fails at the fourth intersection, while FLAME successfully completes the navigation task. The agent's responses, accompanied by rationales, accurately identify key landmarks such as \"traffic light\" and \"scaffolding\", which align closely with the given instructions. FLAME demonstrates proficiency in following instructions and capturing salient environmental details. This example highlights the MLLM-based approach's effectiveness in correlating specific environmental features with verbal navigation instructions."}, {"title": "5 Conclusion", "content": "In this paper, we introduced FLAME, a Multimodal LLM-based agent for urban Vision-and-Language Navigation tasks. By adapting the architecture through a novel three-phase tuning technique and synthetic data, FLAME achieves state-of-the-art performance in urban VLN. The comparison results and reasoning performance demonstrate FLAME's superior ability to integrate verbal and environmental cues for decision-making. The effectiveness of our proposed tuning technique and other components is validated through comprehensive analyses. These findings highlight the potential of Multimodal LLMs in complex navigation tasks."}]}