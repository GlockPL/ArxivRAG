{"title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments", "authors": ["Yunzhe Xu", "Yiyuan Pan", "Zhe Liu", "Hesheng Wang"], "abstract": "Large Language Models (LLMs) have demonstrated potential\nin Vision-and-Language Navigation (VLN) tasks, yet current\napplications face challenges. While LLMs excel in general\nconversation scenarios, they struggle with specialized naviga-\ntion tasks, yielding suboptimal performance compared to spe-\ncialized VLN models. We introduce FLAME (FLAMingo-\nArchitected Embodied Agent), a novel Multimodal LLM-\nbased agent and architecture designed for urban VLN tasks\nthat efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adap-\ntation to navigation tasks, including single perception tun-\ning for street view description, multiple perception tuning for\ntrajectory summarization, and end-to-end training on VLN\ndatasets. The augmented datasets are synthesized automati-\ncally. Experimental results demonstrate FLAME's superior-\nity over existing methods, surpassing state-of-the-art methods\nby a 7.3% increase in task completion rate on Touchdown\ndataset. This work showcases the potential of Multimodal\nLLMS (MLLMs) in complex navigation tasks, representing\nan advancement towards practical applications of MLLMs in\nembodied AI. Project page: https://flame-sjtu.github.io", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (Achiam et al. 2023; Tou-\nvron et al. 2023) have revolutionized the field of embodied\nintelligence. Vision-and-Language Navigation (VLN) (An-\nderson et al. 2018), a fundamental task in embodied AI,\nchallenges agents to navigate to a goal following human in-\nstructions in indoor or outdoor environments. This task de-\nmands sophisticated abilities in instruction comprehension,\nenvironmental understanding, and decision-making, which\ncan be effectively managed by LLMs. Recent approaches\nhave integrated LLMs into VLN methods, either by trans-\nlating visual data into language (Zhou, Hong, and Wu 2024;\nPan et al. 2024; Qiao et al. 2023; Lin et al. 2024a; Chen et al.\n2024) or by employing Multimodal LLMs (MLLMs) (Zhang\net al. 2024; Zhou et al. 2024) for environmental perception.\nHowever, the incorporation of general-purpose LLMs in\nVLN still faces critical challenges, primarily stemming from\ntheir inherent limitations in navigation-specific scenarios, as\nFigure 1(a) depicts. For text-only LLMs, translating visual\ndata into language using visual foundation models can lead\nto information loss (Zhou, Hong, and Wu 2024), resulting in\na huge performance gap compared to VLN-specialized mod-\nels. For Multimodal LLMs, while partially addressing the\nlimitations of text-only LLMs, they often struggle to inter-\nact within navigation-specific scenarios. Recent attempts to\nincorporate MLLMs for navigation focus on integration with\ndown-stream models (Zhou et al. 2024) or intensive training\non navigation videos (Zhang et al. 2024), rather than mak-\ning efficient adaptations to navigation-specific scenario that\nwould harness the MLLMs' inherent capabilities in handling\ninterleaved textual and visual inputs, leading to suboptimal\nperformance.\nMoreover, the application of MLLM in urban VLN re-\nmains unexplored, despite its importance alongside indoor\nVLN. Outdoor navigation presents unique challenges for in-\ntroducing MLLM, including longer trajectory lengths (up to\n55 iterations) and increased difficulty (40% lower success\nrate compared to indoor navigation tasks).\nTo address these challenges, we introduce FLAME\n(FLAMingo-Architected Embodied Agent), the first\nMLLM-based agent designed for urban VLN tasks, as\nshown in Figure 1(b). Based on Flamingo (Alayrac et al.\n2022), FLAME operates autoregressively and efficiently\nhandles multiple perceptions without increasing context\nlength, ensuring efficiency in end-to-end training and\ninference. We propose a three-phase tuning technique to\nadapt Flamingo model (Alayrac et al. 2022) to navigation\ntasks using augmented data: 1) Single perception tuning:\nLearning to describe street views. 2) Multiple perception\ntuning: Learning to summarize agent trajectories. 3) End-\nto-End training and evaluation on VLN datasets. To support\nthe first two tuning phases, we utilize GPT-4 (Achiam et al.\n2023) to synthesize captions and route summaries for the\nTouchdown environment (Chen et al. 2019). Additionally,\nwe synthesize navigation rationales for urban VLN datasets\n(Chen et al. 2019; Schumann and Riezler 2021) to validate\nFLAME's reasoning capability (Wei et al. 2022).\nExperimental results demonstrate FLAME's superiority\nover existing methods on two urban VLN datasets: Touch-\ndown (Chen et al. 2019) and Map2seq (Schumann and Rie-\nzler 2021). Our approach significantly outperforms current\nstate-of-the-art (SOTA) methods by 7.3% TC in Touchdown\nand 3.74% TC in Map2seq, establishing new standards for\nurban VLN tasks. Our work not only benefits the field of\nurban VLN but also showcases the potential of MLLMs in\nnavigation within complex environments and tasks.\nIn summary, our contributions are threefold:\n\u2022 We introduce FLAME, to our knowledge, the first agent\nbased on Multimodal LLM (MLLM) for urban Vision-\nand-Language Navigation (VLN) tasks.\n\u2022 We propose a tailored three-phase tuning technique for\nadapting Flamingo into navigation scenarios using syn-\nthetic data, fully unleashing MLLM's power.\n\u2022 Experiments show the superiority of FLAME over cur-\nrent SOTAS. FLAME's performance proves that MLLMs\ncan significantly outperform specialized models, opening\nnew avenues for research in embodied AI."}, {"title": "Related Works", "content": ""}, {"title": "Vision-and-Language Navigation", "content": "Vision-and-Language Navigation (VLN) (Anderson et al.\n2018) encompasses indoor (Qi et al. 2020; Krantz et al.\n2020) and outdoor scenarios (Chen et al. 2019; Schumann\nand Riezler 2021), with advancements in multimodal under-\nstanding (Tan, Yu, and Bansal 2019; Hong et al. 2021; Zhu\net al. 2020) and action strategies (Qi et al. 2020; Chen et al.\n2022b), primarily for indoor environments. Traditional VLN\nagents often lack advanced decision-making skills, prompt-\ning the integration of Large Language Models (LLMs) (Lin\net al. 2024b,a; Zhan et al. 2024b; Schumann et al. 2023),\nleveraging the reasoning (Pan et al. 2024; Chen et al. 2024;\nLin et al. 2024a) and dialogue capabilities (Qiao et al. 2023;\nLong et al. 2024) of LLMs. These approaches either convert\nvisual data to text (Zhou, Hong, and Wu 2024) or employ\nMultimodal LLMs (MLLMs) with intensive training (Zhang\net al. 2024). Given the relative underexploration of outdoor\nVLN, our work addresses this gap by introducing an effec-\ntively adapted MLLM-based agent for urban VLN tasks."}, {"title": "Multimodal Large Language Models", "content": "The emergence of Multimodal Large Language Models\n(MLLMs) (Liu et al. 2024; Wang et al. 2023a; Zhu et al.\n2024; Alayrac et al. 2022; Awadalla et al. 2023) has ex-\npanded their application to various tasks, including caption-\ning (Li et al. 2023b), grounding (Peng et al. 2023), and gen-\neral instruction following (Dai et al. 2023; Bai et al. 2023).\nThese models demonstrate multimodal reasoning skills (Lu\net al. 2022; Zeng et al. 2023; Lu et al. 2024) in chat conversa-\ntions, handling single-turn, in-context (Sun et al. 2024), and\ninterleaved text and image inputs (Lauren\u00e7on et al. 2024a;\nJiang et al. 2024). MLLMs have benefited from vision-and-\nlanguage tuning (Lauren\u00e7on et al. 2024b), enabling them\nto process diverse modalities (Zhan et al. 2024a; Wu et al.\n2023). However, the general pretraining is insufficient for\nexpert navigation tasks. Our work addresses this gap by\nadapting an MLLM from general scenarios to navigation\nthrough a specialized tuning approach."}, {"title": "Data Augmentation in Vision-and-Language Navigation", "content": "To overcome data scarcity in navigation, various data aug-\nmentation techniques (Chen et al. 2022a; Ku et al. 2020;\nZhao et al. 2021; Huang et al. 2019b,a) have been proposed.\nThese include utilizing a speaker module (Fried et al. 2018;\nDou and Peng 2022) to generate synthetic instructions,\nleveraging multilingual data (Li, Tan, and Bansal 2022a),\nincorporating counterfactual information (Wang et al. 2022;\nParvaneh et al. 2020; Fu et al. 2020), and altering environ-\nments (Li, Tan, and Bansal 2022b; Liu et al. 2021). For out-\ndoor VLN, previous studies have explored training agents\non instructions with different styles (Zhu et al. 2021), pre-\ntraining on auxiliary tasks (Armitage, Impett, and Sennrich\n2023) and using driving videos (Li et al. 2024). However, the\nuse of auxiliary training data to tailor MLLMs for outdoor\nnavigation remains largely unexplored. Our work addresses\nthis gap by using synthetic data to adapt MLLMs for urban\nVLN tasks."}, {"title": "Method", "content": "We present FLAME (Flamingo-Architected Embodied\nAgent), an agent for urban Vision-and-Language Navigation\n(VLN). Our method comprises three key components: 1) An\narchitecture for urban navigation. 2) A three-phase tuning\ntechnique. 3) A synthetic data creation process."}, {"title": "Task Formulation", "content": "We formalize urban VLN as follows: Given a navigation in-\nstruction $I = {W_1, W_2, ..., w_n}$, an agent starts in an ini-\ntial state $S_0$. At each timestep t, the agent selects an action"}, {"title": "FLAME Architecture", "content": "FLAME builds upon the Flamingo architecture (Alayrac\net al. 2022), leveraging cross-attention for processing of vi-\nsual and textual inputs, without extending context length and\nimproves efficiency. We introduce two key adaptations to\nFlamingo for urban VLN:\nStrided Cross-Attention To handle the large number of\nobservations in urban VLN, we implement strided cross-\nattention (Child et al. 2019) in the cross attention layer, as\ndepicted in Figure 2(b). The Perceiver Resampler module\ntransforms CLIP (Radford et al. 2021) features into a com-\npact set of visual tokens, with $N_1$ tokens per observation.\nLet $X \\in R^{N \\times d}$ represent flattened visual tokens up to\ntimestep t, where $N = N_1t$. The LM block outputs a\nword embedding matrix $X_t$, with $X_t$ as the the segment of\ncontext corresponds to t, and l as the stride length. Pattern\n$S = {S_1, ..., S_t}$ defines visual token indices for attention at\nt, with $S_t = {k, k+1, ..., t\\cdot N_l}$ and $k = max(0, (t-1)\\cdot N_l)$.\nThe strided cross-attention score is given by:\n$A(X_t, S_t) = CMA(X_tW_Q, (x_jW_K)_{j \\in S_t}, (x_jW_V)_{j \\in S_t})$\nAction Prediction As Figure 2(c) shows, FLAME oper-\nates autoregressively, predicting actions based on the initial\ninstruction I, current observation $O_t$, history of observations\n$O0)$, and a ground truth\ncaption $c^{(i)}$, the training objective is defined as:\n$L_{p1} = -\\sum_{i=1}^{N}log p(c^{(i)}|p^{(i)}, O^{(i)}; \\theta)$,\nMultiple Perception Tuning The second phase focuses\non synthesizing information from sequential observations,\ncrucial for navigation understanding. Using a trajectory\nsummary dataset $D_{p2} = {\\tau^{(i)}}_{i=1}^{N}$, where $\\tau^{(i)}$\n$=\\{(P^{(i)}, O_1^{(i)}, ..., O_{T(i)}^{(i)}, s^{(i)})\\}$, we minimize:\n$L_{p2} = -\\sum_{i=1}^{N}logp(s^{(i)}|p^{(i)}, O_1^{(i)}, ..., O_{T(i)}^{(i)}; \\theta)$.\nEnd-to-End Navigation Tuning The final phase finetunes\nFLAME on the VLN dataset $D_{vln} = {\\gamma^{(i)}}_{i=1}^{N}$, where each\ninstance $\\gamma^{(i)} = \\{(I^{(i)}, O_0^{(i)}, a_0^{(i)}), ..., (O_{T(i)}^{(i)}, a_{T(i)}^{(i)})\\}$ contains\nan instruction $I^{(i)}$, observations $O^{(t)}$, and ground truth ac-\ntions $a^{(t)}$. We minimize:\n$L_{vln} = -\\sum_{i=1}^{N}\\sum_{t=1}^{T(i)}logp(a_t^{(i)}|I^{(i)},O_{0:t-1}^{(i)}; \\theta)$.\nThis multi-phase approach progressively builds FLAME's\ncapabilities, from basic urban environment understanding to\ncomplex decision-making, enabling it to effectively process\nnavigational instructions and environmental cues."}, {"title": "Synthetic Data Generation", "content": "To support the finetuning of our agent, we leverage LLMs\nto automatically synthesize street view captions, route sum-\nmaries (Figure 3(b)), and navigation rationales (Figure 3(c)).\nStreet View Caption Generation We focus on generat-\ning captions for street views at key locations, which are cru-\ncial for successful navigation. These images are processed\nby GPT-4V (Achiam et al. 2023) to produce corresponding\ndescriptions. To ensure diversity, we randomly select meta-\nprompts from a curated list, guiding the caption generation\nprocess.\nTrajectory Summary Generation We construct a com-\nprehensive knowledge graph of landmarks by combining\nplaces of interests from OpenStreetMap with additional map\ninformation, following the methodology of (Schumann and\nRiezler 2021). Using this graph as input, we employ GPT-4\nto generate detailed route summaries between key locations.\nRationale Generation for VLN Datasets To validate rea-\nsoning capabilities of our model, we generate synthetic ra-\ntionales for the VLN datasets. The process involves seg-\nmenting each VLN trajectory into sub-routes between key\nlocations, then retrieving the corresponding image caption\nand sub-route summary for each key location. We then uti-\nlize GPT-4 to generate a synthetic rationale at each key lo-\ncation, based on the retrieved information. To maintain data\nquality, we discard instances containing rationales that con-\ntradict ground truth actions. This results in a altered subset of\nthe VLN dataset augmented with synthetic rationales, which\nfacilitates end-to-end training with reasoning capabilities."}, {"title": "Experiments", "content": ""}, {"title": "Experiment Setup", "content": "Datasets We evaluate our approach on two urban Vision-\nand-Language Navigation (VLN) datasets: Touchdown\n(Chen et al. 2019) and Map2seq (Schumann and Riezler\n2021), both set in the StreetLearn environment (Mirowski\net al. 2018). Touchdown contains 9,326 instruction-\ntrajectory pairs, while Map2seq comprises 7,672 pairs. The\naugmented dataset for the first two training phases contains\n2,354 and 4,674 instances, respectively. Our agent is bench-\nmarked against others on the original datasets.\nWe collect 6,518 (out of 9,326) and 6,291 (out of 7,672)\npairs grounded with rationales at key locations using GPT-\n4 for Touchdown and Map2seq, respectively. We formulate\nsynthetic pairs to create a dataset for evaluating reasoning\ncapabilities. The reasoning performance is evaluated exclu-\nsively on the subset containing synthetic rationales to ensure\na fair comparison.\nMetrics For the original VLN datasets, we employ three\nmetrics for performance evaluation: Task Completion (TC),\nShortest-Path Distance (SPD) and Normalized Dynamic\nTime Warping (nDTW). Specifically, TC represents the per-\ncentage of successful navigation. SPD calculates the mini-\nmum distance from the stop location to the goal. nDTW as-\nsesses the overlap between the agent's and the ground truth\ntrajectories.\nTo further evaluate the agent's reasoning capabilities with\nsynthetic rationales, we introduce two new metrics:\n\u2022 Rationale Coherence (RC):\n$RC = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{M_i} CFR_{rc}(I_i, y(R), R)}{\\sum_{i=1}^{N} M_i}$\n\u2022 Rationale-Action Alignment (RA):\n$RA = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{K_i} CFR_{ra}(R, A)}{\\sum_{i=1}^{N} K_i}$"}, {"title": "Comparison with SOTAS", "content": "In the section, we compare FLAME with previous state-of-\nthe-art (SOTA) approaches. As shown in Table 1, FLAME\nestablishes new SOTA performance on both Touchdown and\nMap2seq datasets. On Touchdown test split, FLAME sur-\npasses the previous SOTA, Loc4Plan (Tian et al. 2024),\nby 7.3% in TC and %1.97 in SPD. This demonstrates the\nsuperiority of our MLLM-based approach in comprehend-\ning navigational instructions and environmental cues, result-\ning in higher success rates and better path adherence. For\nMap2seq, FLAME outperforms VELMA (Schumann et al.\n2023), an LLM-based method, with a 3.74% increase in TC\nand a 5.35% improvement in nDTW. This highlights the ad-\nvantage of Multimodal LLMs in capturing comprehensive\ninformation compared to text-only LLM, which may intro-\nduce information loss when translating visual data.\nFurthermore, we evaluated open-sourced methods in our\nvisual setting, which features a restricted field of view simi-\nlar to human vision. The results, marked with an asterisk (*)"}]}