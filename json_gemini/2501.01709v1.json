{"title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders", "authors": ["Jiajun Cao", "Yuan Zhang", "Tao Huang", "Ming Lu", "Qizhe Zhang", "Ruichuan An", "Ningning Ma", "Shanghang Zhang"], "abstract": "Visual encoders are fundamental components in vision-language models (VLMs), each showcasing unique strengths derived from various pre-trained visual foundation models. To leverage the various capabilities of these encoders, recent studies incorporate multiple encoders within a single VLM, leading to a considerable increase in computational cost. In this paper, we present Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework that distills the unique proficiencies of multiple vision encoders into a single, efficient encoder model. Specifically, to mitigate conflicts and retain the unique characteristics of each teacher encoder, we employ low-rank adaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate specialized knowledge based on input features, enhancing both adaptability and efficiency. To regularize the KD process and enhance performance, we propose an attention-based distillation strategy that adaptively weighs the different visual encoders and emphasizes valuable visual tokens, reducing the burden of replicating comprehensive but distinct features from multiple teachers. Comprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT, validate the effectiveness of our method. The code will be released.", "sections": [{"title": "1. Introduction", "content": "The rapid development of large vision-language models (VLMs) has significantly advanced artificial intelligence, particularly in tasks requiring integrated visual and linguistic understanding. At the core of these models, the vision encoder is essential for visual perception, forming the foundation for interpreting visual inputs and enabling the effective execution of vision-language tasks. Recent studies [36, 40] highlight the distinct strengths of various vision encoders, such as CLIP [33], EVA [10], and ConvNeXt [29], each excelling in specific vision-language applications. This diversity makes the optimization and integration of visual encoders a key area of research.\nTo harness the diverse proficiencies of various vision encoders, current methods [22, 36, 40] often employ multiple encoders in a vision-language model via feature concatenation or attention mechanisms. However, compared to VLMs with a single vision encoder, using multiple encoders unavoidably increases computational costs and model complexity, diminishing efficiency and scalability. To address this, we explore a critical question in this paper: can we distill the unique proficiencies of various encoders into a single vision encoder, capturing their collective advantages while improving overall efficiency?\nTo unify multiple encoders into one, knowledge distillation (KD) [15] presents a promising approach, as it effectively transfers knowledge from a teacher model to a student model. However, classical KD methods primarily focus on one-to-one distillation, and the simultaneous distillation from multiple models, each with distinct pre-training datasets and objectives, remains relatively under-explored. Although AM-RADIO [34] proposes using multiple heads within a single model to replicate the predictions of various"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Vision-language models", "content": "With the impressive success of large language models (LLMs) [1, 4, 6, 41, 45, 48], recent studies work on generative large vision-language models (VLMs) [5, 7, 22, 24, 25, 31, 39, 46] to improve multimodal comprehension and generation through utilizing the strong generality of LLMs. Built upon the CLIP [33] image encoder which is somewhat aligned with the language modality, current VLMs typically utilize vast image-text pairs to connect the vision encoder and LLM, enabling LLM to receive and understand visual content. For instance, Flamingo [2] integrates visual features into LLM through gated attention. LLaVA [25] directly connects the vision encoder and LLM with MLPs, showing proficiency in multi-modal dialogues. Besides, recent works boost the representation of the vision encoder [22, 35, 50] to further enhance the perception of VLMs. For example, Mini-Gemini [22] employs an additional vision encoder for high-resolution refinement, and S2 [35] introduces multiple visual branches by scaling up the image scale. However, the above methods are usually fed with higher-resolution image inputs or designed with extra modules, which require more computational resources. In our paper, we propose to improve the vision modality of VLMs through the adaptive supervision of a mixture of visual experts, where higher-quality training data is optional."}, {"title": "2.2. Knowledge distillation", "content": "Knowledge distillation (KD) [15] is proposed to transfer dark knowledge in the teacher model to a student model, to boost the student without more parameters. Since the large vision-language models have become popular, how to enhance VLMs via KD is a notable research direction.\nKD on vision encoder. In vision-language models, the vision encoder is essential for extracting high-level features from visual signals, and provides perception and visual understanding ability. Before that, DINOv2 [32] employs self-distillation to train their smaller variants from the larger teacher, while [42] distills their model from a CLIP teacher,"}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "We propose MoVE-KD, a novel knowledge distillation method with multiple visual encoders for visual-language Models (VLM). The pipeline of MoVE-KD is shown in Fig. 3. Specifically, we first employ encoder adapters to project the outputs of multiple teacher encoders into a unified representation space. Based on the [CLS] attention from the pre-trained CLIP model, weights are dynamically assigned to both the teacher encoders and the visual tokens. Then, the KD loss is calculated based on the weighted sum of the teacher weights and token weights. To mitigate potential conflicts from learning multiple sources of knowledge, we incorporate a mixture-of-LoRA-experts (MoLE) structure within the student encoder. Our final training objective is to minimize the text loss and the KD loss."}, {"title": "3.2. Learning from multiple encoders", "content": "Encoder adapter. Given a visual input, it is processed by different visual encoder teachers to obtain visual tokens. Due to the inconsistent representation spaces of visual encoders from different sources, these visual tokens cannot be directly aligned with the student visual token. Meanwhile, the commonly-used linear interpolation in KD methods is challenging to bridge the distinct tokens from different encoders to a unified and student-friendly token space, as discussed by LLaVA-1.5 [25]. Therefore, to match the dimensions and align the token spaces, we introduce encoder adapters for each teacher encoder. Each adapter, implemented as a two-layer MLP tailored to the output of its respective teacher, is independently utilized and optimized using the knowledge distillation loss.\nMixture-of-LoRA-experts (MoLE). After token alignment, the student encoder, initialized from the pre-trained CLIP visual encoder, is fine-tuned to learn teacher tokens. However, we find that directly fine-tuning the student encoder poses certain challenges: Fine-tuning directly on the target dataset leads to issues like overfitting and catastrophic forgetting, which affect both model accuracy and generaliz-"}, {"title": "3.3. Attention-guided KD regularization", "content": "The key of distilling knowledge from multiple teachers into one model is to guide the student in which features should be focused on. Since different visual encoders have different understanding towards one image, and some representations are useless or redundant for the visual-language recognition, paying too much attention on those representations would weaken the learning of the real important and unique features. Therefore, a proper way is to find an appropriate constraint to regularize the distillation.\nIn such constraint, the student should be guided by distillation loss, which discriminates the valuable and redundant regions in the teacher tokens on both fine-grained token level and coarse-grained teacher level. Therefore, an ideal distillation loss on visual tokens can be formulated as\n$\\Lkd = \\frac{1}{m}\\sum_{i=1}^{m} \\text{W}_{i}^{(\\text{tea})}\\sum_{j=1}^{n} (\\text{W}_{ij}^{(\\text{tok})} + \\varepsilon) \\text{MSE}(V_{i}^{(s)}, V_{j}^{(t)}),$ \nwhere m denotes the number of teacher encoders, n is the sequence length of visual tokens, $V^{(t)} \\in R^{mxnxc}$ and $V^{(s)} \\in R^{nxc}$ represent visual tokens of teacher and student, and $W^{(tok)}$ and $W^{(tea)}$ denote the token-level and teacher-level weight vectors.\nGiven the above motivation, we now elaborate on our method for deriving the weights $W^{(tok)}$ and $W^{(tea)}$. Basically, a good visual encoder should have a strong perception and focusing ability for the key information in the image. In this paper, instead of using the commonly-used learnable tokens [17] to capture the weights, we adopt a more efficient and generalizable way by using the [CLS] token in CLIP. As shown in Fig. 2, the cross-attention between the [CLS] token and other visual tokens in CLIP reveals the key regions in the image and shows less interest in repeated and unimportant information (see Sec. 5 for detailed discussion). This focusing characteristic, as well as the influential regions, would be beneficial for the student to learn from. Therefore, we design our KD regularization using the weights provided by the [CLS] attention of CLIP.\nToken weight. As mentioned above, we hope that the student encoder can focus on key visual tokens just like the pre-trained CLIP. Therefore, we calculate the [CLS] attention between the [CLS] token $V^{(cls)} \\in R^{d}$ and other visual tokens $V^{(res)} \\in R^{nxd}$ of CLIP, and use the normalization as the weight of each token, which is formulated as:\n$W^{(tok)} = Softmax(\\frac{(V^{(cls)}W^{(Q)}) \\cdot (V^{(res)}W^{(V)})^{T}}{\\sqrt{d}}),$ \nwhere $W^{(tok)} \\in [0, 1]^{n}$ represents the token weight, while $W^{(Q)}$ and $W^{(V)}$ are the transformation matrices for queries"}, {"title": "3.4. Overall loss.", "content": "The overall loss consists of two parts, namely the $L_{text}$ and the $L_{kd}$, representing the conventional log-likelihood loss in VLMs and distillation loss proposed in Eq. 2 of this paper, respectively. As a result, the total loss is formulated as:\n$L_{total} = L_{text} + A_{kd} \\cdot L_{kd},$ \nwhere $A_{kd}$ is the weight of knowledge distillation."}, {"title": "4. Experiments", "content": "In this section, we validate our method within various VLM architectures on comprehensive multimodal benchmarks to assess its effectiveness on image understanding tasks."}, {"title": "4.1. Experimental settings", "content": "Models. We verify the proposed MoVE-KD on two popular VLM frameworks: LLaVA [25] and LLaVA-NeXT [26]. LLaVA-1.5 employs CLIP-pretrained [33] ViT-L as the visual tower. For resolution scaling, LLaVA-NeXT employs an adaptive image cropping strategy, encodes each image, and concatenates them in one single sequence. For LLaVA-1.5 and LLaVA-NeXT 7/13B, we follow the same training and inference setting as the original paper as it is available. LLaVA-1.5 1.7b is built on MobileLLaMA [8] 1.4b and is trained in the same way as LLaVA-1.5 7b. For the teacher encoder, we retain CLIP [42] and additionally select EVA-02 [10] and ConvNeXT [29], which are mentioned in Eagle [36] as the top performers on vision-language tasks.\nDataset. In the pre-training stage, we take LLaVA Visual Instruct Pretrain LCS-558K as the dataset. In the fine-tuning stage, the fine-tuning datasets of LLaVA-1.5 and LLaVA-NeXT are used respectively. For fairness, we do not introduce additional datasets for training.\nBenchmarks. To validate the effectiveness of our method, we conduct comprehensive experiments on eight widely adopted benchmarks including VQAV2 (VQA V2) [13],"}, {"title": "4.2. Main results", "content": "To the best of our knowledge, MoVE-KD is the first KD method for visual encoders in VLMs, and there is no directly comparable baseline. When evaluating various benchmarks, we use the original model as the baseline. Additionally, RADIO [34] is a known multi-encoder distillation method. However, it distills on DataComp-1B [12] with 1.4 billion image-text pairs and then replaces CLIP in LLaVA-1.5 for training. Although the additional dataset is a bit unfair to our method, we still include it for reference.\nOur results compared with the previous method are summarized in Tab. 1, MoVE-KD achieves state-of-the-art (SOTA) performance on mainstream VLM frameworks like LLaVA-1.5 and LLaVA-NeXT. The RADIO [34] exhibits obvious knowledge forgetting issues on VQAV2 and VQA Text, while our approach overcomes this problem and achieves comprehensive improvements on Viunca-7b [51]. Besides, the LLaVA-1.5 equipped with our method even surpasses LLaVA-NeXT on some tasks, like MME and MMB, and it validates the effectiveness of MOVE-KD. Although we observe minimal degradation for MOVE-KD on VQA Text, where a large number of questions are not related to vision. Our enhancement of vision may have a counter-productive effect on the model."}, {"title": "4.3. Ablation study", "content": "We present the results of the ablation study for each design in our method in Tab. 2. For simplicity, we use the LLaVA1.5-7b model as the baseline and incrementally add each method design from MOVE-KD.\nEncoder adapter. Directly mapping the teacher's representation space to the student's through interpolation neglects feature continuity, which can easily lead to information loss. This approach can even result in average performance falling below the baseline. Through comparison, we demonstrated that utilizing learnable encoder adapters enables continuous feature mapping, ensuring effective knowledge transfer.\nMixture-of-LoRA-experts. The introduction of MoLE allows the student encoder to dynamically select activated parameters based on the input, avoiding the knowledge confusion that arises during multi-teacher and multi-domain learning. This has led to significant performance improvements across various benchmarks, particularly in POPE and MMB. MoLE has mitigated the substantial performance degradation typically caused by knowledge distillation. Since the experts selected in MoLE are LoRAs, the parameters we introduce account for only 0.3% of the total parameters. To eliminate the possibility that the performance improvement is solely due to the increase in parameters, we also conducted a control experiment where MoLE was introduced without knowledge distillation and the MoLE parameters were tuned during training. The results, as shown in Tab. 3, indicate that the introduction of MoLE does not lead to a performance improvement. On the contrary, some benchmarks show signs of degradation."}, {"title": "4.4. Visualization", "content": "In Fig. 5, we show the visualization results of the [CLS] attention of both the pre-trained CLIP and the distilled student model. In the upper left figure, the student encoder focuses more attention on the crowd and banner, reducing attention to the background above compared to CLIP. In the upper right figure, the student encoder maintains additional attention on the \"Headline\" and ignores blank areas. In the lower left figure, the student encoder disregards the blank area in the lower right, concentrating attention on the flowers and text. In the lower right figure, unlike CLIP, the student encoder clearly outlines the plane and the airport runway, without excessively focusing on the sky as CLIP does."}, {"title": "5. Discussion", "content": "In this section, we first explore the definitions of foreground and background in knowledge distillation of visual encoders, and then discuss the practical implications of visual tokens in the background with high [CLS] attention."}, {"title": "5.1. Foreground and background definitions", "content": "In vision-language tasks, the image's foreground refers to the visual tokens most relevant to the text. SparseVLM [49] reduces the number of visual tokens by pruning low-similarity background visual tokens, calculated based on text-token and visual-token similarity. We also experimented with using this similarity as the weight for visual tokens during distillation to focus attention on the foreground. However, we found that this approach led to performance degradation. Although the text provides clear foreground information, the definition of foreground and background varies with different questions. For instance, in Fig. 5, the buildings behind a crowd may also become the foreground depending on the question context. Therefore, the definition of foreground and background in distillation should be fixed. When observing, humans tend to focus on dynamic, complex, semantically rich elements, while being less sensitive to repetitive items (e.g., sky, water, and grasslands)."}, {"title": "5.2. The high [CLS] attention tokens in background", "content": "In Fig. 5, we observe some tokens in the background with significantly high [CLS] attention, which are referred to as \"artifacts\" in the literature [9]. The study suggests that exposing these artifacts can lead to more interpretable attention maps and improve performance in dense prediction tasks. However, as previously discussed, the background is not unimportant; rather, it can be summarized with fewer tokens to capture repetitive information, similar to peripheral vision in the human eye. We believe these so-called artifacts essentially carry rich global information, which is crucial for VLMs to generate accurate responses. CLIP condenses background regions with repetitive information using few tokens with relatively high attention. Therefore, we retain the [CLS] attention weight of these tokens."}, {"title": "6. Conclusion", "content": "In this paper, we introduce a novel framework called Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD) aimed at fusing the unique proficiencies of multiple visual encoders into a single efficient encoder model, marking the first approach to integrate different encoders for large vision-language models from a knowledge distillation perspective. Through the use of low-rank adaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate specialized knowledge based on input features, we successfully mitigate conflicts and preserve the distinctive characteristics of each teacher encoder. Our attention-based distillation strategy further enhances performance by adaptively weighing different visual encoders and emphasizing valuable visual tokens. Comprehensive experiments on popular VLMs like LLaVA and LLaVA-NeXT have validated the efficacy of our approach. Furthermore, as the scale of large language models increases, we notice diminishing marginal returns from knowledge distillation, suggesting that the performance bottleneck of large vision-language models may lie in the projector that bridges the visual encoder and the large language model (LLM). This indicates that further research into developing more efficient methods to seamlessly and losslessly project visual and text tokens into a unified representation space should be a key focus for future advancements in VLM research."}]}