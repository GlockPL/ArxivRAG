{"title": "Towards Multi-dimensional Explanation Alignment\nfor Medical Classification", "authors": ["Lijie Hu", "Songning Lai", "Wenshuo Chen", "Hongru Xiao", "Hongbin Lin", "Lu Yu", "Jingfeng Zhang", "Di Wang"], "abstract": "The lack of interpretability in the field of medical image analysis has significant eth-\nical and legal implications. Existing interpretable methods in this domain encounter\nseveral challenges, including dependency on specific models, difficulties in under-\nstanding and visualization, as well as issues related to efficiency. To address these\nlimitations, we propose a novel framework called Med-MICN (Medical Multi-\ndimensional Interpretable Concept Network). Med-MICN provides interpretability\nalignment for various angles, including neural symbolic reasoning, concept se-\nmantics, and saliency maps, which are superior to current interpretable methods.\nIts advantages include high prediction accuracy, interpretability across multiple\ndimensions, and automation through an end-to-end concept labeling process that re-\nduces the need for extensive human training effort when working with new datasets.\nTo demonstrate the effectiveness and interpretability of Med-MICN, we apply it\nto four benchmark datasets and compare it with baselines. The results clearly\ndemonstrate the superior performance and interpretability of our Med-MICN.", "sections": [{"title": "Introduction", "content": "The field of medical image analysis has witnessed remarkable advancements, especially for the deep\nlearning models. Deep learning models have exhibited exceptional performance in various tasks,\nsuch as image recognition and disease diagnosis [31, 46, 1], with an opaque decision process and\nintricate network. However, this lack of transparency is particularly problematic in the medical\ndomain, making it challenging for physicians and clinical professionals to trust the predictions made\nby these deep models. Thus, there is an urgent need for the interpretability of model decisions in the\nmedical domain [43, 13, 57].\nThe medical field has strict trust requirements. It not only demands high-performing models but\nalso emphasizes comprehensibility and earning the trust of practitioners [20]. Thus, Explainable\nArtificial Intelligence (XAI) has emerged as a prominent research area in this field. It aims to enhance\nthe transparency and comprehensibility of decision-making processes in deep learning models and\nlarge language models by incorporating interpretability [65, 21, 22, 64, 63, 25, 8]. Various methods\nhave been proposed to achieve interpretability, including attention mechanisms [56, 61, 27, 26, 24],\nsaliency maps [70, 16], DeepLIFT and Shapley values [38, 4], influence functions [34, 55]. These\nmethods strive to provide users with visual explanations that shed light on the decision-making\nprocess of the model. However, while these post-hoc explanatory methods offer valuable information,\nthere is still a gap between their explanations and the model decisions [42]. Moreover, these post-hoc\nexplanations are generated after the model training and cannot actively contribute to the model\nfine-tuning process, hindering them from being a faithful explanation tool.\nThus, there is increasing interest among researchers in developing self-explanatory methods. Among\nthese, concept-based methods have garnered significant attention [50, 2, 35]. Concept Bottleneck\nModel (CBM) [35] initially predicts a set of pre-determined intermediate concepts and subsequently\nutilizes these concepts to make predictions for the final output, which are easily understandable to\nhumans. Concept-based explanations provided by inherently interpretable methods are generally\nmore comprehensible than post-hoc approaches. However, most existing methods treat concept\nfeatures alone as the determination of the predictions. This approach overlooks the intrinsic feature\nembeddings present within medical images, thus degrading accuracy [44]. Moreover, while these\nconcepts are human-understandable, they lack semantic meanings, thus questioning the faithfulness\nof their interpretability [39]. To improve further human trust, several recent works [3] aim to leverage\nsyntactic rule structures to concept embeddings. However, there are still several potential issues.\nFirst, unlike CBMs, current concept models with logical rules mainly focus on the supervised concept\ncase, which is quite strict for biomedical images as concept annotation is expensive. Second, while\ncurrent concept models (with logical rules) provide interpretations via concepts, we found that the\nimportance of these concepts is misaligned with other explanations, especially the explanation given\nby saliency maps [70, 16]. This will lead to a possible reduction in human trust when using these\nmodels.\nTo address these challenges, we introduce a new and innovative end-to-end concept-based framework\ncalled the Med-MICN (Medical Multi-dimensional Interpretable Concept Network), as illustrated\nin Figure 3. As shown in Figure 2, Med-MICN is an end-to-end framework that leverages Large\nMultimodals (LMMs) to generate concept sets and perform auto-annotation for medical images,\nthereby aligning concept labels with images to overcome the high cost associated with medical\nconcepts annotation. In contrast to typical concept-based models, our interpretation is notably more\ndiverse and precise (shown in Figure 1). Specifically, we map the image features extracted by the\nbackbone through a concept encoder to obtain concept prediction and concept embeddings, which are\nthen input into the neural symbolic layers for interpretation. This also establishes alignment between\nimage information and concept embeddings by utilizing a concept encoder, leading to the derivation\nof predictive concept scores. Furthermore, we align concept semantics with concept embeddings\nby incorporating neural symbolic layers. Thus, we effectively align image information with con-\ncept semantics and concept saliency maps, achieving comprehensive multidimensional alignment.\nAdditionally, unlike most concept-based methods, we use concept embeddings to complement the\noriginal image features, which enhances classification accuracy without any post-process. Our main\ncontributions can be summarized as follows:"}, {"title": "Related Work", "content": "Concept Bottleneck Model. The Concept Bottleneck Model (CBM) [35] has emerged as an in-\nnovative deep-learning approach for image classification and visual reasoning by incorporating a\nconcept bottleneck layer into deep neural networks. However, CBM faces two significant chal-\nlenges. Firstly, its performance often falls short of the original models without the concept bottleneck\nlayer, attributed to incomplete information extraction from the original data to bottleneck features.\nSecondly, CBM extensively depends on meticulous dataset annotation. To solve these problems,\nresearchers have delved into potential solutions. For example, [7] have extended CBM into interactive\nprediction settings by introducing an interaction policy to determine which concepts to label, ulti-\nmately improving the final predictions. Additionally, [41] has addressed the limitations of CBM by\nproposing a novel framework called Label-free CBM, which offers promising alternatives. Post-hoc\nConcept Bottleneck models [66] can be applied to various neural networks without compromising\nmodel performance, preserving interpretability advantages. Despite much research in the image field\n[18, 33, 32, 45, 47, 36, 28, 23, 37], concept-based method for the medical field remains less explored,\nwhich requires more precise results and faithful interpretation. [9] used a conceptual alignment\ndeep autoencoder to analyze tongue images representing different body constituent types based on\ntraditional Chinese medicine principles. [35] introduced CBM for osteoarthritis grading and used ten\nclinical concepts such as joint space narrowing, bone spurs, calcification, etc.\nHowever, previous research heavily relies on expert annotation datasets or often focuses solely\non concept features to make predictions while overlooking the intrinsic feature embeddings within\nimages. Furthermore, while the concept neural-symbolic model has been explored in the graph domain\n[3], its application to images, particularly in the medical domain, has been largely absent. Additionally,\nour work addresses these gaps by proposing an end-to-end framework with an alignment strategy that\nleverages various explainable methods, including concept-based models, neural-symbolic methods,\nsaliency maps, and concept semantics, to provide comprehensive solutions to these challenges.\nExplanation in Medical Image. Research on the interpretability of deep learning in medical\nimage processing provides an effective and interactive approach to enhancing medical knowledge\nand assisting in disease diagnosis. User studies involving physicians have revealed that doctors\noften seek explanations to understand AI results, especially when the outcomes are related to their\nown hypotheses or differential diagnoses [60]. They also turn to explanations to resolve conflicts\nwhen their judgments differ from those of AI [6], thereby enhancing the intelligence of medical\nmodels. Previous studies have visualized lesion areas through methods such as heatmaps [59]\nand attention visualization [12], aiding in the identification of lesion regions and providing visual\nevidence. Additionally, utilizing language model-based methods like LLM or LMM to generate\nmedical reports complements the interpretation of model results (ChatCAD [58], XrayGPT [52],\nMed-PaLM [49]). Saliency maps have emerged as the most common and clinically user-friendly\nexplanation for medical imaging tasks [53, 68, 69]. Recent research underscores the importance\nof understanding the pivotal features influencing AI predictions, particularly when clinicians must\ncompare AI decisions with their own clinical assessments in cases of decision incongruity [54]. In\naddition to the image's intrinsic feature recognition, assisted discrimination methods based on concept\ninjection are widely employed in assisted medical diagnosis [35, 7]. Compared to relying solely\non self-supervised training, conceptual feature-based supplementation integrates expert knowledge,\noffering more accurate assistance for interpreting detection results."}, {"title": "Preliminaries", "content": "Concept Bottleneck Models. To introduce the original Concept Bottleneck Models, we adopt\nthe notations used by [35]. We consider a classification task with a concept set denoted as $C =$\n$C_1, C_2,..., C_N$ and a training dataset represented as $\\{(x_i, Y_i, C_i)\\}_{i=1}^M$. Here, for $i \\in [M]$, $x_i \\in R^d$\nrepresents the feature vector, $y_i \\in R^{d_z}$ denotes the label (with $d_z$ corresponding to the number of\nclasses), and $c_i \\in R^{d_c}$ represents the concept vector. In this context, the $j$-th entry of $c_i$ represents\nthe weight of the concept $p_j$. In CBMs, our goal is to learn two representations: one that transforms\nthe input space to the concept space, denoted as $g : R^d \\rightarrow R^{d_c}$, and another that maps the concept\nspace to the prediction space, denoted as $f : R^{d_c} \\rightarrow R^{d_z}$. For any input x, we aim to ensure that\nits predicted concept vector $\\hat{c} = g(x)$ and prediction $\\hat{y} = f(g(x))$ are close to their underlying\ncounterparts, thus capturing the essence of the original CBMs.\nFuzzy Logic Rules. As described by [17, 3], continuous fuzzy logic extends upon traditional Boolean\nlogic by introducing a more nuanced approach to truth values. Rather than being confined to the\ndiscrete values of either 0 or 1, truth values are represented as degrees within the continuous range\nof {0, 1}. Conventional Boolean connectives including t-norm $\\wedge : [0, 1] \\times [0, 1] \\rightarrow [0, 1]$, t-conorm\n$\\vee : [0, 1] \\times [0, 1] \\rightarrow [0, 1]$, negation $\\neg x = 1-x$. The logical connectives, including $\\neg, \\vee, \\wedge, \\Rightarrow, \\Leftarrow, \\Leftrightarrow,$\nare utilized to convey the logical relationships between concepts and their representations. For\nexample, consider the problem of deciding whether an X-ray lung image has COVID, given the\nvocabulary of concepts \"ground-glass opacities (GO),\" \"Localized or diffuse presentation (LDP),\"\nand \"lobar consolidation (LC).\" A simple decision rule can be $y \\Leftrightarrow CGO > \\neg \\vee CLC$. From this rule, we\ncan deduce that (1) Having both \"no LC\" and \"GO\" is relevant to having COVID. (2) Having LDP is\nirrelevant to deciding whether COVID exists."}, {"title": "Medical Multi-dimensional Interpretable Concept Network", "content": "Here, we present Med-MICN (Figure 3), a novel framework that constructs a model in an automated,\ninterpretable, and efficient manner. (i) In traditional CBMs, the concept set is typically generated\nthrough annotations by human experts. When there is no concept set and concept labels, we first\nintroduce the automated concept labeling alignment process (Figure 2). (ii) Then, the concept set\n(output by LLMs such as GPT4-V) is fed into the text encoder to obtain word embedding vectors.\nOur method utilizes Vision-Language Models (VLMs) to encode the images and calculate cosine\ndistances to generate heatmaps. We apply average pooling to these heatmaps to obtain a similarity\nscore aligned with the concept set through a threshold to obtain concept labels. (iii) Next, we extract\nimage features using a feature extraction network and then map them through a concept encoder to\nobtain concept embeddings. (iv) We finally use these concept embeddings as input into the neural\nsymbolic layers to generate concept reasoning rules and incorporate them as complementary features\nto the intrinsic spatial features for predictions, proving multi-dimensional interpretation alignment.\nWe provide details for each component of Med-MICN as follows."}, {"title": "Concept Set Generation and Filtering", "content": "Given $M_c$ classes of target diseases or pathology, the first step of our paradigm is to acquire a set of\nuseful concepts related to the classes. A typical workflow in the medical domain is to seek help from\nexperts. Inspired by the recent work, which suggests that instruction-following large language models\npresent a new alternative to automatically generate concepts throughout the entire process [41, 40].\nWe propose to generate a concept set using LMMs, such as GPT-4V, which has extensive domain\nknowledge in both visual and language, to identify the crucial concepts for medical classification.\nFigure 2 (a) illustrates the framework for concept set generation. Details are in Appendix A."}, {"title": "VLMs-Med-based Concept Alignment", "content": "Generating Concept Heatmaps. Suppose we have a set of $N$ useful concepts $C =$\n$\\{C_1, C_2,..., C_N\\}$ obtained from GPT-4V. Then, the next step is to generate pseudo-(concept)"}, {"title": "Multi-dimensional Alignment", "content": "In Section 4.2, we get the concept labels for all input images. However, as we mentioned in the\nintroduction, such representation might significantly degrade task accuracy [39, 67]. To overcome\nthis issue, recently [67] propose using concept embeddings, which increase the task accuracy of\nconcept-based models while weakening their interpretability. Motivated by this, we use these concept\nembeddings to increase the accuracy and leverage our concept label to enhance interpretability. In the\nfollowing, we provide details."}, {"title": "Concept Embeddings", "content": "For the training data $X = \\{(x_m, Y_m)\\}_{m=1}^M$, we use a backbone network\n(e.g., ResNet50) to extract features $F = \\{f(x_m)\\}_{m=1}^M$. Then, for each feature, it passes through a\nconcept encoder [67] to obtain feature embeddings $f_c(x_m)$ and concept embeddings $\\hat{c}_m$. The specific\nprocess can be represented by the following expression:\n$f(x_m) = \\Theta_b(x_m), f_c(x_m), \\hat{c}_m = \\Theta_c(f(x_m))$ for $m \\in [M]$,\nwhere $\\Theta_b$ and $\\Theta_c$ represent the backbone and concept encoder, respectively.\nTo enhance the interpretability of concept embeddings, we utilize binary cross-entropy to optimize\nthe accuracy of concept extraction by computing $L_c$ based on $\\hat{c} = \\{\\hat{c}_m\\}_{m=1}^M$ and concept labels $c$ in\nSection 4.2:\n$L_c = BCE(\\hat{c}, c)$.  (1)"}, {"title": "Neural-Symbolic Layer", "content": "Our next goal is to use concept embedding to learn concept rules for\nprediction, which is motivated by [3]. We aim to generate rules involving the utilization of two sets\nof feed-forward neural networks: $\\Phi(\\cdot)$ and $\\Psi(\\cdot)$. The output of $\\Phi(\\cdot)$ signifies the assigned role of\neach concept, determining whether it is positive or negative (such as \u201cno LC\u201d and \u201cGO\u201d). On the"}, {"title": "Final Objective", "content": "In this section, we will discuss how we derive the class of medical images and the process of\nnetwork optimization. First, we have the loss $L_c$ in (1) for enhancing the interpretability of concept\nembeddings. Also, as the concept embeddings are input into the neural-symbolic layer to output\nlogical reasoning rules of the concept and prediction $\\hat{y}_{neural,m}$ in (2) for $x_m$, we also have a\nloss between the predictions given by concept rule and ground truth, which corresponds to the\ninterpretability of our neural-symbolic layers. In the context of binary classification tasks, we employ\nbinary cross-entropy (BCE) as our loss function. For multi-class classification tasks, we use cross-\nentropy as the measure. Using binary classification as an example, we calculate the loss $L_{neural}$ by\ncomparing the output $\\hat{y}$ from the neural-symbolic layer to the label y as follows:\n$L_{neural} = BCE(\\hat{y}_{neural}, y)$, (3)\nClassification Loss. Note that as $L_{neural}$ in (3) is purely dependent on the concept rules rather than\nfeature embeddings, we still need a loss for final prediction performance. In a typical classification\nnetwork, the process involves obtaining the feature $f(x_m)$ and passing it through a classification\nhead to generate the classification results. What sets our approach apart is that we fuse the previously\nextracted $f_c(x_m)$ with the $f(x_m)$ using a fusion module as input to the classification ahead. This can\nbe expressed using the following formula:\n$y_m = W_F \\space Concat(f(x_m), f_c(x_m))$,\nNote that $W_F$ represents a fully connected neural network. For training our classification model, we\nuse categorical cross-entropy loss, which is defined as follows:\n$L_{task} = CE(\\tilde{y}, Y)$,\nFormally, the overall loss function of our approach can be formulated as:\n$L = L_{task} + \\lambda_1 \\cdot L_c + \\lambda_2 \\cdot L_{neural}$,\nwhere $\\lambda_1, \\lambda_2$ are hyperparameters for the trade-off between interpretability and accuracy."}, {"title": "Experiment", "content": "In this section, we introduce the experimental settings, present our superior performance, and\nshowcase the interpretability of our network. Due to the space limit, additional experimental details\nand results are in the appendix C."}, {"title": "Experimental Setting", "content": "Datasets. We consider four benchmark medical datasets: COVID-CT [29] for CT images, DDI [10]\nfor dermatology images, Chest X-Ray [14], and Fitzpatrick17k [15] for a dermatological dataset with\nskin colors.\nBaselines. We compared our model with other state-of-the-art interpretable models, such as Label-free\nCBM[41] and DCR[3], to highlight the robustness of our interpretability capabilities. Furthermore,\nwe conducted comparisons with black-box models, such as SSSD-COVID [51]."}, {"title": "Model Utility Analysis", "content": "Med-MICN delivers superior performance. In Table 1, our method achieved different improve-\nments on various backbones for the COVID-CT dataset. Taking Acc as an example, it increased by\n3.39% with ResNet50 and by 3.45% with VGG compared to backbones. Compared to SSSD-COVID,\nour method outperforms in terms of Acc and F1, with improvements of 4.68% and 7.15%, demonstrat-\ning the superiority of our method in enhancing model accuracy. Additionally, our method possesses\ninterpretability, which is not achievable by SSSD-COVID. On the other hand, our method achieved\nsignificant improvements on different backbones for the DDI dataset. For instance, Acc increased by\n6.01% with VGG. Despite the significant differences between the two datasets in terms of modality,\nour method demonstrated significant effects on both datasets, indicating its good generalizability.\nDetails are shown in Table 4, 5, 6, and 7 in Appendix.\nMeanwhile, when compared to existing well-performing interpretable models, our approach demon-\nstrates significant advantages in terms of accuracy and other metrics across different datasets. This\nindicates that our joint prediction of image categories using both concept and image feature spaces\noutperforms predictions based solely on a limited concept space."}, {"title": "Model Interpretability Analysis", "content": "Explanation across multiple dimensions. In our approach, we discover and generate concept rea-\nsoning rules based on the neural-symbolic layer. Analyzing these rules enhances the interpretability of\nour network. In addition, our approach derives concept prediction scores through the concept encoder,\nand during evaluation, it also produces saliency maps. Through multi-dimensional explanations, we\ncan observe the basis for the model decision from different perspectives. In concept score prediction,\nwe can observe how the model maps data to concept dimensions, allowing doctors to observe and\ncorrect concept results, thereby rectifying prediction errors caused by incorrect concept predictions.\nIn concept reasoning rules, we can deduce the model classification criteria, further explaining the\nmodel decisions. Additionally, during the evaluation process, we can generate saliency maps for the"}, {"title": "Ablation Study", "content": "The ablation experiments presented in Table 2, conducted with Resnet50 as the backbone, reveal sig-\nnificant contributions from both $L_c$ and $L_{neural}$ to the classification result. To illustrate, considering\nthe comprehensive index AUC in the DDI dataset, utilizing only $L_c$ yeilds a 3.79% improvement,\nwhile relying solely on $L_{neural}$ does not notably enhance performance. However, employing both"}, {"title": "Conclusion", "content": "This paper proposes a novel end-to-end interpretable concept-based model called Med-MICN. Com-\nbining medical image classification, neural symbolic solving, and concept semantics, Med-MICN\nachieves superior accuracy and multi-dimensional interpretability. Our comprehensive experiments\ndemonstrate consistent enhancements over other baselines, highlighting its potential as a generalized\nand faithful interpretation model for medical images."}, {"title": "A Concept Set Generation and Filtering", "content": null}, {"title": "Concept Generation with GPT-4V", "content": "In this section, we will explain our process for generating concept sets using GPT-4V. Specifically,\nwe ask GPT-4V the following:"}, {"title": "Concept Set Filtering", "content": "After concept set generation, a concept set with some noise can be obtained. The following filters are\nset to enhance the quality of the concept :"}, {"title": "The length of concept", "content": "To keep concepts simple and avoid unnecessary complication, We\nremove concepts longer than 30 characters in length."}, {"title": "Similarity", "content": "We measure this with cosine similarity in a text embedding space. We removed\nconcepts too similar to classes or each other. The former conflicts with our interpretability\ngoals, while the latter leads to redundancy in concepts. We set the thresholds for these two\nfilters at 0.85 and 0.9, respectively, ensuring that their similarities are below our threshold."}, {"title": "Remove concepts we cannot project accurately", "content": "Remove neurons that are not interpretable\nfrom the BioViL [5]. This step is actually described in section 4.2."}, {"title": "B Neural-symbolic Layer", "content": "We give the details with examples for neural-symbolic layer [3]."}, {"title": "Concept Polarity", "content": "For each prediction class j, there exists a neural network $\\Phi_j(\\cdot)$. This network\ntakes each concept embedding as input and produces a soft indicator, a scalar value in the [0, 1]\nrange. This soft indicator represents the role of the concept within the formula. As an illustration,\nconsider a specific concept like \"Crazy-paving pattern\". If its value after passing through $\\Phi_j(\\cdot)$ is 0.8,\nit indicates that the \u201cCrazy-paving pattern\" has a positive role with a score of 0.8 for class j. We use\nthe notation $I_{o,i,j}$ to represent the soft indicator for concept $c_i$."}, {"title": "Concept Relevance", "content": "For each prediction class j, a neural network $\\Psi_j(\\cdot)$ is utilized. This network\ntakes each concept embedding as input and produces a soft indicator, a scalar value within the range\nof [0, 1], representing the relevance score within the formula. To illustrate, let us consider a specific\nconcept such as \u201cMultilobar distribution\u201d. If its value after passing through $\\Psi_j(\\cdot)$ is 0.2, it implies\nthat the relevance score of \"Multilobar distribution\" in the inference of class j is 0.2. We denote the\nsoft indicator for concept $c_i$ as $I_{r,i,j}$."}, {"title": "Prediction via Concept Rules", "content": "Finally, for each class j, we combine the previous concept polarity\nvector $I_{o,j}$ and the concept relevance vector $I_{r,j}$ to obtain the logical inference output. This is"}, {"title": "More Experimental Setup", "content": null}, {"title": "Training Setting", "content": "Our model exhibits remarkable efficiency in its training process. We utilized only a single GeForce\nRTX 4090 GPU, and the training duration did not exceed half an hour. We configured the model to\nrun for 100 epochs with a learning rate set at 5e-5. Additionally, all images were resized to a uniform\ndimension of (256, 256)."}, {"title": "Datasets", "content": "COVID-CT. The COVID-CT dataset was obtained from [29] and comprises 746 CT images,\nconsisting of two classes (349 COVID and 397 NonCOVID). We divided this dataset into a training\nset and a test set with an 8:2 ratio, and the data were split accordingly.\nDDI. DDI [10] is a dataset comprising diverse dermatology images designed to assess the model's\nability to correctly identify skin diseases. It consists of a total of 656 images, including 485 benign\nlesion images and 171 malignant lesion images. These images are divided into training and test sets,\nwith an 80% and 20% split, respectively.\nChest X-Ray. The Chest X-Ray [14] dataset comprises 2D chest X-ray images of both healthy\nand infected populations. It aims to support researchers in developing artificial intelligence models\ncapable of distinguishing between chest X-ray images of healthy individuals and those with infections.\nThe dataset includes 5933 images, divided into 5309 training images and 624 testing images.\nFitzpatrick17k. Fitzpatrick17k [15] dataset is a dermatological dataset that includes a wide range\nof skin colors. In order to better compare the model performance, we filtered the malignant and\nnonmalignant classes in 3230 images that have been relabeled by SkinCon [11]. And we divided the\ntraining and test set according to an 8:2 ratio."}, {"title": "Baseline Models", "content": "SSSD-COVID. SSSD-COVID incorporates a Masked Autoencoder (MAE) for direct pre-training\nand fine-tuning on a small-scale target dataset. It leverages self-supervised learning and self-\ndistillation techniques for COVID-19 medical image classification, achieving performance levels\nsurpassing many baseline models. Our method is trained exclusively on the COVID-CT dataset\nwithout considering the effects of introducing knowledge from other datasets. Notably, our approach\noutperforms SSSD-COVID in terms of performance. Furthermore, SSSD-COVID falls under the\ncategory of black-box models, indicating that our method, to some extent, overcomes the accuracy\ndegradation issue introduced by the incorporation of concepts.\nLabel-free CBM. Label-free CBM is a fully automated and scalable method for generating concept\nbottleneck models. It has demonstrated outstanding performance on datasets like ImageNet. In\nour comparisons, our model outperforms this model significantly in terms of accuracy. Regarding\ninterpretability, our model not only possesses the same level of interpretability as this model but also"}, {"title": "Definition of Metrics", "content": "Accuracy is the ratio of the number of correctly categorized samples to the total number of samples:\n$Acc = \\frac{TP + TN}{TP + TN + FP + FN}$\nWhere TP denotes true positive, TN denotes true negative, FP represents false negative, and FN\nrepresents false negative.\nThe precision is the proportion of all samples classified as positive categories that are actually positive\ncategories:\n$Precision = \\frac{TP}{TP + FP}$\nRecall is the proportion of samples that are correctly categorized as positive out of all samples that\nare actually positive categories:\n$Recall = \\frac{TP}{TP + FN}$\nThe F1 score is the reconciled mean of precision and recall:\n$F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$\nAUC denotes the area under the ROC curve, which is a curve with True Positive Rate (Recall Rate)\nas the vertical axis and False Positive Rate (False Positive Rate) as the horizontal axis."}, {"title": "More Experimental Results", "content": null}, {"title": "Utility Evaluation", "content": "We provide our detailed utility evaluation for four datasets in Table 4, 5, 6, and 7."}, {"title": "Ablation Study: Effect of Concept Filters", "content": "In this section, we will discuss how each step in our proposed concept filtering affects the results of our\nmethod. In general, our utilization of filters has two main goals: First, improving the interpretability\nof our models. Second, improving computational efficiency and complexity by reducing the number\nof concepts.\nAlthough our initial goal was not to improve model accuracy (a model with more concepts is generally\nlarger and more powerful [41]), the more precise concepts after filtering make the concatenated\nfeatures more effective for classification and slightly improve the accuracy. To evaluate the impact\nof each filter, we trained our models separately on COVID-CT and DDI while removing one filter\nat a time and one without using any filter at all. The results are shown in the Table 8. From Table\n8, it is noticeable that our model's accuracy does not exhibit high sensitivity to the choice of filters.\nOn the COVID-CT dataset, the accuracy of our model remains largely unaffected by the choice of\nfilters. Furthermore, employing filters on the DDI dataset results in improved model accuracy. This\nphenomenon arises due to the relatively sparse nature of concepts within the DDI dataset images,\nwhere increasing the number of concepts did not yield superior solutions."}, {"title": "Sensitivity Analysis", "content": "We also performed a sensitivity analysis for baselines and Med-MICN in the DDI dataset. we\nset various attacks under $\\delta \\in \\{4/255, 6/255, 8/255, 10/255, 12/255\\}$ and attack radius $p_a \\in\n\\{0, 2/255, 4/255, 6/255, 8/255, 10/255\\}$. in Figure 14, the results show that our model consists of\nstronger robustness against perturbation. This is evidenced by the marginal decrease in test accuracy\nas the attack radius increases. The model's detection performance fluctuates slightly at $p_a$ of 4/255,\nwith this variability diminishing as the perturbation level rises, underscoring the robustness of our\nmodel against significant disturbances."}, {"title": "Computational Cost", "content": "We conducted a computational cost analysis for Med-MICN and the baseline models. By inputting a\nrandom tensor of size (1, 3, 244, 224) into the model and computing the FLOPs and parameters, the\nresults are presented in Table 9. Experimental evidence demonstrates that Med-MICN incurs only\nnegligible computational cost compared to the baseline models while it achieves improvements in\naccuracy and interpretability."}, {"title": "Limitation", "content": "In the case of the \"neural symbolic\" method, additional acceleration techniques may be required\nwhen dealing with large sample sizes. However, it is worth noting that medical datasets tend to be\nrelatively small."}]}