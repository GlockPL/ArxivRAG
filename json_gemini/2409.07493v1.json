{"title": "Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review", "authors": ["Javad Hassannataj Joloudari", "Mohammad Maftoun", "Bahareh Nakisa", "Roohallah Alizadehsani", "Meisam Yadollahzadeh-Tabari"], "abstract": "The Complex Emotion Recognition System (CERS) deciphers complex emotional states by examining combinations of basic emotions expressed, their interconnections, and the dynamic variations. Through the utilization of advanced algorithms, CERS provides profound insights into emotional dynamics, facilitating a nuanced understanding and customized responses. The attainment of such a level of emotional recognition in machines necessitates the knowledge distillation and the comprehension of novel concepts akin to human cognition. The development of AI systems for discerning complex emotions poses a substantial challenge with significant implications for affective computing. Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task due to the intricacies involved in capturing subtle emotions, necessitating specialized methods for data collection and processing. Incorporating physiological signals such as Electrocardiogram (ECG) and Electroencephalogram (EEG) can notably enhance CERS by furnishing valuable insights into the user's emotional state, enhancing the quality of datasets, and fortifying system dependability. A comprehensive literature review was conducted in this study to assess the efficacy of machine learning, deep learning, and meta-learning approaches in both basic and complex emotion recognition utilizing EEG, ECG signals, and facial expression datasets. The chosen research papers offer perspectives on potential applications, clinical implications, and results of CERSs, with the objective of promoting their acceptance and integration into clinical decision-making processes. This study highlights research gaps and challenges in understanding CERSs, encouraging further investigation by relevant studies and organizations. Lastly, the significance of meta-learning approaches in improving CERS performance and guiding future research endeavors is underscored.", "sections": [{"title": "1. Introduction", "content": "Affective computing is an interdisciplinary research domain that amalgamates the fundamental tenets of psychology, computer science, and cognitive science.\nThe automatic recognition of emotions is a crucial component of affective computing, whereby the recognition of emotional states serves as the foundation for the computer's comprehension of emotions and subsequent reactions [1]. The field of emotion recognition, which is highly appealing, has garnered significant attention from both industry and academia. Emotion recognition has numerous potential applications, including but not limited to human-computer interaction, video gaming, and continuous monitoring of infants and patients with medical conditions such as Parkinson's disease, Alzheimer's disease, depression, falls, and so on [2].\nBasic emotions are primary, universal emotional states that are widely acknowledged in various societies and typically consist of happiness, sadness, fear, anger, surprise, and disgust [3]. These emotions are believed to be innate and have distinct facial expressions and physiological reactions [4]. In contrast, complex emotions are more intricate and frequently entail a blend of basic emotions alongside higher cognitive operations, such as jealousy,\npride, and guilt [5]. Differing from basic emotions, complex emotions are shaped by personal encounters, social engagements, and societal standards [6].\nIn fact, emotion is a complex psychological phenomenon, characterized by distinct emotional states that manifest through various physical and physiological cues. These cues can be broadly categorized into physical expressions and physiological signals. Physical expressions include facial expressions, voice intonation, gait, and body posture.Physiological signals encompass indicators such as Electroencephalogram (EEG) and Electrocardiogram (ECG) readings, Galvanic Skin Response (GSR), and other biofeedback mechanisms. Additionally, emotions can be conveyed through written text and other communication methods, acting as mediators of our internal emotional states [1,7,8]. The recognition of emotions through physiological signals is an essential aspect of scrutinizing psychological states and advancing biofeedback-based applications. The advent of the metaverse concept and the integration of physiological signal trackers into smart devices have significantly advanced this area of investigation, making it both essential and intriguing for researchers.\nStudies have shown that changes in emotional states can directly influence physiological signals, providing a valuable means of emotion detection.\nThe Autonomic Nervous System (ANS) and Central Nervous System (CNS) are essential in overseeing various physiological responses to emotional stimuli, like heart rate, skin conductance, and brain activity. Monitoring this regulation effectively can be done using techniques such as EEG and ECG. EEG allows for precise capture of dynamic changes in brain activity in response to emotional stimuli due to its high temporal resolution [9, 10]. In contrast, ECG provides a reliable measurement of heart rate variability, which is closely associated with emotional arousal and autonomic regulation [11, 12]). These methods together offer a comprehensive understanding of the physiological basis of emotional states, enhancing the analysis of facial expressions .\nThis understanding is vital for developing accurate and reliable methods for detecting and interpreting emotions through physiological signals [13, 14]. The objective of the researchers is to develop emotion recognition systems that exhibit equitable levels of accuracy and responsiveness.\nThe present difficulty in recognizing complex emotions stems from the limited array of physiological signals considered, which could obstruct a comprehensive understanding of the subtle emotional states that individuals undergo. Although physiological signals are crucial for emotion detection, the emphasis has frequently been confined to a restricted range of measurements that may fail to encapsulate the complexities of intricate emotions. Broadening the focus to include signals such as EEG, indicative of central nervous system activity, and ECG, which tracks heart activity influenced by the autonomic nervous system and sensitive to emotional intensity and stress, can offer a more comprehensive viewpoint. By merging these varied signals, it becomes feasible to reveal the detailed interactions between different physiological systems and complex emotional reactions, thereby improving both the precision and richness of emotion recognition while overcoming the constraints imposed by a limited signal emphasis [15].\nHowever, given that these signals are non-linear and non-stationary, it is crucial to meticulously select appropriate features to significantly enhance the accuracy of the system [16, 17]. Another type of emotion recognition as the most effective, natural, and universal signal used by human beings to communicate their emotional states and objectives is facial expression [18]. From 1974, 55% of messages relating to emotions and beliefs are conveyed by facial expression, 7% through spoken words, and the remaining 40% are paralinguistic [19]. Numerous facial expression recognition (FER) approaches have been investigated in the fields of computer vision and machine learning to encode expression information from face representations.\nMachine learning methodologies, such as Support Vector Machine (SVM), Multilayer Perceptron (MLP), Random Forest (RF), and K-Nearest Neighbor (KNN), are frequently utilized for the purpose of determining the presence of emotional content in facial expressions [20]. Nevertheless, the application of these methodologies extends beyond facial expressions alone, encompassing the analysis of physiological signals as well. This broader scope allows for a more comprehensive approach to the recognition of emotions. Through the integration of machine learning techniques with physiological signals (e.g., heart rate, skin conductance, brain activity) and physical cues (e.g., facial expressions), it becomes feasible to attain heightened precision and resilience in detecting emotions. This all-encompassing strategy capitalizes on the advantages offered by diverse data sources, ultimately facilitating a more profound comprehension of emotional states [21, 22, 23].\nAnother variety of techniques that utilize deep architectures are known as deep learning methods, and they are outperforming traditional machine learning techniques in terms of accuracy and productivity. Deep Learning approaches, especially convolutional neural networks (CNN), which primarily depend on supervised learning using manually labeled data, have been inspected for facial emotion recognition [24]. A fixed learning algorithm that is manually created is usually used to train contemporary machine learning models from scratch for a specific task. Additionally, approaches based on deep learning have demonstrated remarkable success in numerous domains. There are, however, evident disadvantages. For instance, areas possessing massive computational power and the ability to accumulate and simulate massive quantities of data have seen the most success. Many applications where data is inherently expensive or rare or where computational resources are not available are disqualified by this [25, 26]."}, {"title": "1.1 Publication analysis and search results", "content": "In this review paper, we examined 891 studies from various databases including IEEE, Science Direct, Springer, Wiley, and Google Scholar. This collection comprised 82 papers from IEEE, 60 from Springer, 158 from Science Direct, 139 from Wiley, and 450 from Google Scholar. In the initial phase, 801 records were eliminated due to duplication and irrelevance. Consequently, 90 records proceeded to the screening stage. During screening, 29 papers were excluded for being off-topic, leaving 61 publications for further review. At the eligibility stage, 23 more papers were removed. Ultimately, 38 research papers were selected as the final set of featured studies. The process of obtaining them based on PRISMA guidelines is shown in Fig. 1."}, {"title": "1.2 Data mapping of included studies", "content": "In this section, we provide the research papers selected from various databases. The trend chart for the number of publications on complex emotion recognition from 2017 to 2024 reveals a dynamic pattern as shown in Fig 2."}, {"title": "2. Type of emotions, differences and emotion models", "content": "The next step of this study is structured as follows: Section 2 describes the type of emotions, the differences between them and emotion models In section 3, the complex emotion databases are reviewed and analyzed. Section 4 the preprocessing steps of developing a complex emotion recognition system are presented. Section 5 focuses on the concepts and technologies of emotion recognition systems and also limitations of traditional methods. In section 6, a review of related works are conducted. Section 7, the discussion and evaluation metrics are delivered.In Section 8, and last section are addressed into Open Research Challenges, Conclusions and future research directions."}, {"title": "2.1 Basic Emotions", "content": "Emotions that are basic are brief emotional reactions that often occur automatically and have counterparts in other vertebrates. They are generally recognized worldwide and are commonly linked to specific physical reactions and facial expressions. Fear, anger, disgust, sadness, joy, and surprise are among the most frequently acknowledged basic emotions [39]. Key features of basic emotions comprise:\n1. Automaticity: Basic emotions are activated automatically in reaction to particular stimuli without conscious deliberation. For instance, the sight of a predator may trigger fear, while the presence of a loved one may trigger joy.\n2. Distinctive Facial Expressions: Each basic emotion is tied to a distinct facial expression that is easily recognized across diverse cultures. For instance, a frown and squinting eyes are typically associated with anger, while a grin signifies happiness. Alongside facial signals, physiological metrics like EEG and ECG offer a more profound insight into emotional experiences. The EEG method captures the brain's activities tied to various emotional states, while the ECG technique evaluates heart rate variability and autonomic nervous system responses, both being notably affected by emotional excitement and stress. Collectively, these indicators present a more comprehensive view of emotion, merging external signals with internal physiological reactions.\n3. Physiological Responses: Basic emotions come with specific physiological alterations in the body, such as variations in heart rate, breathing, and hormone levels. Fear, for example, initiates the \"fight or flight\" response, resulting in heightened heart rate and adrenaline release.\n4. Evolutionary Roots: Basic emotions are thought to have distinct evolutionary functions tied to survival and procreation. Fear, for instance, aids individuals in reacting to dangers, while joy strengthens social connections and motivates actions that enhance overall well-being."}, {"title": "2.2 Complex Emotions", "content": "Complex emotions are intricate and multifaceted emotional experiences that encompass a blend of basic emotions, cognitive processes, and social influences. Unlike basic emotions, which are generally widespread, complex emotions can vary significantly among individuals and societies [40]. Here are the primary characteristics:\n1. Cognitive Components: Complex emotions entail substantial cognitive assessment and understanding of situations. Individuals may have to evaluate the context, their personal beliefs and values, and the perspectives of others to fully understand and experience intricate feelings. For instance, feelings of jealousy may emerge from interpreting a situation as a threat to a valued relationship.\n2. Cultural and Individual Variation: In contrast to basic emotions, which are predominantly uniform, complex emotions can differ greatly among individuals and cultures. Cultural norms, values, and personal experiences have a substantial impact on shaping the experience and expression of complex emotions. For example, the way pride is felt and exhibited may vary across cultures.\n3. Extended Duration: Complex emotions typically endure longer than basic emotions and may transform over time as new information and experiences are assimilated. For example, feelings of guilt may persist as individuals contemplate their actions and their repercussions.\n4. Interpersonal Functions: Complex emotions often serve a vital role in managing social connections and navigating social standards. They assist individuals in comprehending and responding to others' emotions, negotiating social hierarchies, and upholding social ties. For instance, expressions of romantic love may entail a complex interplay of emotions like affection, desire, and commitment."}, {"title": "2.3 Differences Between Basic and Complex Emotions", "content": "To further clarify the differences between basic and complex emotions, it is crucial to explore their cognitive and social implications more deeply . Basic emotions like joy, fear, anger, sadness, disgust, and surprise are often seen as universal and innate, arising automatically in response to stimuli with little conscious thought. These emotions are believed to have developed to fulfill specific survival purposes, such as fear activating fight-or-flight reactions to possible dangers, and disgust aiding individuals in avoiding harmful substances. Studies by [3] and others have shown that basic emotions are expressed and understood similarly across various cultures, hinting at a common evolutionary origin.\nIn contrast, complex emotions such as guilt, shame, envy, pride, and love involve more intricate cognitive processes, often requiring self-reflection, comparison with others, and an understanding of societal norms and expectations. These emotions are not only enduring but also dependent on context, influenced by personal encounters, cultural upbringing, and the particular social setting. For example, feeling shame involves evaluating one's actions in relation to societal standards and anticipating others' judgments, which can differ significantly among cultures [44, 45]. Research indicates that while the fundamental elements of these emotions might be universal, their expression, interpretation, and importance are molded by cultural context and individual distinctions [46].\nFurthermore, complex emotions frequently result from combinations or interactions of basic emotions [47] and are more likely to involve mixed sentiments or uncertainty. For instance, jealousy could blend fear (of losing a\nrelationship) with anger (towards a perceived rival) and sadness (due to feeling unappreciated). This complex interplay of emotions showcases the sophisticated cognitive processes that underlie complex emotions, making them more challenging to investigate and comprehend compared to basic emotions [48]. Additionally, the influence of language in expressing and shaping complex emotions should not be underestimated; different societies possess specific terms and ideas that capture nuanced emotional states, emphasizing further the diversity in how these emotions are felt and conveyed [49].\nBasic emotions and complex emotions differ in several key aspects. Firstly, regarding duration, basic emotions are typically short-lived and immediate, whereas complex emotions tend to persist for extended periods. Secondly, in terms of cognitive involvement, basic emotions often arise automatically with minimal cognitive processing, while complex emotions require significant cognitive appraisal and interpretation of situations. Thirdly, basic emotions have clear evolutionary functions primarily related to survival, whereas complex emotions have evolved to manage more sophisticated social interactions and relationships. Finally, basic emotions are universally recognized and expressed similarly across cultures, whereas complex emotions can vary significantly based on cultural and individual differences in expression and recognition [50,51,52,53,54]."}, {"title": "2.4 Emotion models", "content": "Defining emotion, or affect, is essential in setting criteria in affective computing. The understanding of emotions was initially presented by Ekman in the 1970s. Despite psychologists' various attempts to classify emotions in fields like neuroscience, philosophy, and computer science, there is no universally agreed-upon model of emotions. However, the two main types of emotion models commonly utilized are the discrete (or categorical) emotion model and the dimensional (or continuous) emotion model [55].\nIn the discrete emotion modelas which illustrated by the wheel of emotions depicted in Fig 4, individuals typically choose one emotion from a set of predefined emotions that best represents their feelings. This model categorizes emotions clearly and is based on Ekman's early research.\nConversely as shown in Fig 5, the dimensional model portrays emotions using quantitative measures across various dimensions. This method often utilizes tools such as the Self-Assessment Manikin (SAM) or Feeltrace. SAM uses images of SAMmanikins to evaluate the static level of a dimension at a specific moment, while Feeltrace monitors emotional data continuously over time [56]."}, {"title": "3. Datasets used in scientific studies", "content": "Datasets are fundamental for developing every Al-based system, encompassing the essential attributes necessary for facial emotion recognition, as well as for the analysis of EEG and ECG signals. These datasets facilitate the extraction and interpretation of results, and they are combined in advance to support the development of algorithms for recognizing facial emotions and analyzing physiological signals. They include a variety of data types such as static images, videos, or a combination of both [57, 58]. Additionally, some datasets include pictures taken in staged settings using laboratory-controlled environments or are provided by psychology centers. Moreover, datasets derived organically from real-world environments are also available."}, {"title": "3.1 Basic and complex Emotion Datasets", "content": "CMED\nBased on existing spontaneous micro-expression datasets such as CASME I, CASME II, CAS(ME), and SAMM, researchers created the Compound Micro-expression Database (CMED). These datasets mainly gather facial expression data rather than physiological signals. Due to their subtle features, it is challenging to analyze the motion track and characteristics of micro-expressions. Consequently, generating compound micro-expression images presents numerous obstacles and limitations. The data in these datasets is captured using high-speed cameras that record slight facial movements at a high frame rate, typically ranging from 100 to 200 frames per second, enabling the detection of micro-expressions lasting between 1/25th and 1/5th of a second. Participants in these datasets engage in various tasks or are exposed to specific stimuli to evoke spontaneous emotional reactions, which are then documented [59,60, 61, 62, 63]. The number of participants varies among the datasets. For example, the CASME II dataset contains recordings from 26 individuals, while the SAMM dataset includes data from 32 subjects. The exact number of participants in the CMED may differ, but it usually encompasses a diverse group to ensure a wide array of facial expressions and emotions. These datasets cover a range of both basic and complex emotions. Common basic emotions captured include happiness, sadness, anger, fear, surprise, and disgust, which are universally understood and easily recognizable. Additionally, complex emotions like contempt, embarrassment, pride, and guilt are also recorded to offer a more thorough insight into human emotional expression."}, {"title": "CASME", "content": "The first dataset established by the Chinese Academy of Sciences (CAS) was CASME. It was recorded at 60 frames per second and includes 180 videos from 19 subjects. Thirteen females and twenty-two males, with an average age of 22.03 years (\u03c3 = 1.60) were enlisted for the study. The eight micro-expressions that have been recognized are: amusement, sadness, disgust, surprise, contempt, fear, repression, and tense. The experiments were limited to the top five classes (tense, disgust, happiness, surprise, and repression) with the greatest number of samples[64]."}, {"title": "CFEE", "content": "This dataset contains 5,060 images of faces from 230 people which have been labeled with 15 complex emotions and 7 basic emotions.[65] There are 21 different emotion categories in the CFEE dataset. Compound emotions, like delightfully surprised and angry surprised, comprise multiple basic component categories. The Facial Action Coding System was utilized for the analysis of the obtained images. These 21 categories have distinct production processes that are nevertheless in line with the subordinate categories they represent[66]."}, {"title": "FER-2013", "content": "This dataset was introduced at the 2013 International Conference on Machine Learning (ICML) by Pierre-Luc Carrier and Aaron Courvill, the 2013 Facial Expression Recognition dataset (FER-2013) is available in the Kaggle dataset. The 35,887 grayscale 48x48-pixel images in the FER-2013 dataset remain in a spreadsheet with the pixel values of each image listed in a row of cells. After using Google to source images, they were categorized into various emotional classes including surprise, anger, disgust, fear, happiness, neutral, and sadness. Upon the conclusion of the challenge, 3,589 images designated for private testing were added to the dataset, which originally contained 28,709 images for training and 3,589 images for public testing. Public test images are employed for a range of purposes in published research projects, each using the FER-2013 dataset distribution for individual training, validation, or test sets[67,68]."}, {"title": "3.2 Basic emotions", "content": "DEAP\nThe main emphasis of the DEAP dataset (Database for Emotion Analysis using Physiological Signals) lies in basic emotions. The DEAP dataset [69] has been split into two sections. The first contains the ratings from 120 one-minute music video excerpts that participants reviewed online using three criteria: arousal, valence, and dominance. The participants ranged in age from 14 to 16. The second accumulation of data comprises participant ratings, physiological recordings, and face films from an experiment in which 32 participants viewed a selection of the 40 music videos mentioned above. Every participant reviewed the videos as above, and physiological and EEG signals were recorded. Oval face videos were also captured for 22 subjects.\nBP4D+\nThe main focus of the BP4D+ dataset is on basic emotions. A large-scale, multimodal emotion dataset is known as BP4D+. The FERA challenge of 2017 made use of it. There are 140 participants total, aged 18\u201366, including 58 male and 82 female participants. Eight physiological signals are present in total: heart rate, respiration (rate and voltage), blood pressure (diastolic, systolic, mean, and raw), and electrodermal (EDA). Ten target emotions including happiness, sadness, anger, disgust, embarrassment, astonished skepticism, fear, pain, and surprise are represented in the data for each subject [70]."}, {"title": "3.3 Complex Emotion Datasets", "content": "RAF-DB\n29,672 real-world images of faces that were extracted from Flickr are included in RAF-DB. 315 talented annotators have labeled each of the RAF-DB's images, with roughly 40 independent annotators labeling each image. The single-label subset and the multi-label subset are the two distinct subsets found in RAF-DB[71]."}, {"title": "CEED", "content": "The CEED (Compound Emotion Expression Database) is mainly concentrated on capturing intricate emotions. Unlike datasets that center on fundamental emotions, CEED is tailored to document and examine emotional expressions that merge multiple basic emotions or encompass more subtle and socially influenced emotional states [72]. 480 images of eight young adult actors emulating nine complicated and six basic social-sexual emotional expressions are available in the complicated Emotion Expression Database (CEED). There is some racial variety among the actors, who are both male and female. Almost 800 individuals independently scored images to confirm how the expression was perceived [73]."}, {"title": "4. Preprocessing in CEMRS", "content": "In this section, the challenges of preprocessing in complex emotion recognition systems (CEMRS) will be discussed. This section is divided into two subsection: a) Physical Cues Preprocessing b) Physiological Cues Preprocessing"}, {"title": "a) Physical Cues Preprocessing", "content": "This section focuses on the preprocessing difficulties related to physical cues, specifically facial expressions obtained via image processing. It delves into the intricacies of preparing facial images to derive significant features that represent various emotional states. Major challenges encompass managing fluctuations in lighting conditions, facial angles, and the range of facial expressions. Furthermore, it highlights the necessity for sophisticated techniques in feature extraction to effectively capture the subtleties of intricate emotions [81].\nInitial stages involve detecting and aligning faces to ensure consistent positioning and orientation of facial features in images. Following face detection, normalization methods are utilized to counteract lighting variations by adjusting brightness, contrast, and color balance for standardization. Next, noise reduction techniques like Gaussian blurring or median filtering are applied to minimize unwanted image artifacts that could disrupt emotional cue recognition. Feature extraction is then carried out to capture important facial attributes crucial for emotion recognition, including identifying facial landmarks and extracting texture descriptors from facial regions. Given that feature vectors often consist of intricate data, dimensionality reduction techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) are applied to decrease computational complexity and prevent overfitting. Through these preprocessing steps, facial expression recognition systems can enhance their accuracy, resilience, and ability to generalize in emotion classification tasks. This thorough preprocessing process ensures that the recognition system is well-prepared to provide consistent and precise results across various input images, enabling applications in affective computing, human-computer interaction, and physical research [82,83]."}, {"title": "b) Physiological Cues Preprocessing", "content": "This section will address the difficulties linked with preprocessing physiological cues, such as those derived from electroencephalography (EEG) and electrocardiography (ECG) signals through signal processing. It will encompass the preprocessing procedures needed to cleanse and filter the physiological signals, extract relevant features, and alleviate artifacts to precisely capture the underlying emotional states. Challenges in this field may involve noise reduction, artifact elimination, feature extraction from intricate physiological signals, and ensuring the dependability and precision of the processed data for emotion recognition [84].\nThe preprocessing workflow involves several essential steps tailored to optimize the EEG and ECG signals for effective analysis. Firstly, artifact removal is imperative to eliminate noise and unwanted interference from the signals. EEG signals, for instance, are susceptible to various artifacts like eye blinks, muscle movements, and environmental electrical noise, while ECG signals can be affected by muscle activity and movement artifacts. Techniques such as independent component analysis (ICA) and adaptive filtering are commonly used to mitigate these artifacts, ensuring that the extracted signals accurately represent the underlying neural and cardiac activity. Following artifact removal, signal segmentation is performed to divide the continuous EEG and ECG recordings into shorter, temporally meaningful epochs, often aligned with specific stimuli or events that elicit emotional responses. This segmentation allows for the analysis of emotion-related signal patterns within defined time windows. After segmentation, feature extraction is conducted to capture relevant characteristics from the EEG and ECG signals indicative of different emotional states. For EEG signals, features may include spectral power, coherence, and asymmetry in specific frequency bands, while ECG features may encompass heart rate variability (HRV), amplitude variations, and timing intervals such as RR intervals. Dimensionality reduction techniques are then used to minimize the complexity of the feature space while keeping important information, such as principal component analysis (PCA) and wavelet processing. By implementing these preprocessing steps, EEG and ECG-based emotion recognition systems can significantly improve their accuracy, robustness, and generalization capabilities, allowing for the development of more reliable and versatile emotion classification models for use in affective computing, human-computer interaction, and psychological research [85,86,87]."}, {"title": "5. Machine Learning and deep learning Models in Emotion Recognition: Limitations and Challenges", "content": "Most studies in emotion recognition have primarily focused on utilizing machine learning, including deep learning techniques, to recognize emotions based on basic features. However, emotions are inherently complex, and traditional machine learning models, including deep learning models, often struggle to capture this complexity effectively. While these models may perform reasonably well in recognizing basic emotions, they face several limitations when it comes to understanding and categorizing complex emotional states."}, {"title": "5.1 Limitations of Machine/Deep Learning Models", "content": "1. Inability to Capture Complex Emotional States: Traditional machine learning models, including deep learning models, often rely on predefined features and lack the flexibility to capture the nuances and subtleties of complex emotional states. As a result, they may struggle to distinguish between similar emotional expressions or interpret the context-dependent nature of complex emotions [88].\n2. Dependency on Handcrafted Features: Many traditional machine learning and deep learning approaches require handcrafted features to be extracted from the data. These features may not fully represent the multidimensional nature of complex emotions, leading to a limited ability to generalize across different emotional contexts [89,90].\n3. Limited Adaptability to Individual Differences: Traditional machine learning and deep learning models may not be adaptable enough to account for individual differences in expressing and experiencing emotions. They often rely on generic models that do not adequately capture the variability and subjectivity of emotional responses among different individuals [91].\n4. Overfitting and Generalization Issues: Traditional machine learning and deep learning models may be prone to overfitting when trained on limited datasets, resulting in poor generalization to unseen data. This is particularly problematic in the case of complex emotions, where the variability and diversity of emotional expressions can be significant [92]."}, {"title": "5.2 The Role of Meta-Learning", "content": "In the face of these challenges, meta-learning offers promising approaches to enhance the capabilities of complex emotion recognition systems. Meta-learning, which focuses on learning how to learn, can provide a framework for adapting traditional machine learning and deep learning models to better handle complex emotional states. By leveraging meta-knowledge acquired from a diverse range of emotional contexts, meta-learning algorithms can enhance the adaptability and generalization capabilities of emotion recognition systems. This approach enables\nmodels to learn from previous experiences and rapidly adapt to new emotional contexts, thereby improving their performance in recognizing complex emotions [93,94].\nMost studies have presented machine learning and deep learning techniques for recognizing emotion based on basic features but emotions are always complex. Few studies have worked on complex emotions based on basic features for recognition. In this scenario, another approach called meta-learning could play a vital role in these types of tasks."}, {"title": "5.3 Complex and basic emotion recognition system (methods and technologies)", "content": "The following sections are divided into basic and complex for introducing emotion recognizer systems based on machine learning, deep learning, and meta-learning methods."}, {"title": "5.4 Machine learning, Deep learning in Basic emotion Recognition", "content": "Machine learning approaches such as SVM, Random forest, KNN, Naive Bayes, etc in some studies utilized as feature selection or prediction tasks for emotion recognition[95]. Deep learning methods, such as recurrent neural networks (RNNs) and CNNs, have made significant strides in the field of computer vision in recent decades. These techniques based on deep learning have been used for problems related to recognition, classification, and feature extraction. By enabling \"end-to-end\" learning directly from input images, a CNN's primary benefit is to eliminate or greatly reduce reliance on physics-based models and/or other pre-processing approaches. Due to these factors, CNN has produced cutting-edge outcomes in several domains, such as emotion recognition tasks based on face expression, ECG, and EEG signals[96,97]."}, {"title": "5.5 Meta-learning in complex emotion recognition", "content": "Meta-learning, commonly referred to as \"learning to learn,\" is the process through which Al models acquire the ability to swiftly adjust to new environments or tasks while working with limited data. This capability is especially beneficial in the realm of complex emotion recognition, where challenges such as data scarcity and variability across different contexts frequently arise. Meta-learning comprises various methodologies: optimization-based, model-based, and metric-learning-based techniques [98].\nOptimization-based techniques, including Model-Agnostic Meta-Learning (MAML), Reptile, and Almost No Inner Loop (ANIL), focus on identifying optimal initialization parameters that allow models to quickly converge on new tasks with minimal modifications. These techniques offer flexibility and adaptability across a variety of emotion recognition scenarios by effectively learning from a handful of examples [99,100] .\nModel-based techniques, such as recurrent and convolutional neural networks, integrate meta-learning principles directly into their architecture. These models are crafted to internally adjust to new tasks; however, they may encounter difficulties in generalizing to more complex or diverse emotional contexts due to their streamlined optimization processes.\nMetric-learning-based techniques, like ProtoNet, RelationNet, and MatchingNet, depend on learning embedding functions that transform data into a space where classification can be performed using similarity metrics. These non-parametric approaches prove effective for emotion recognition as they facilitate rapid learning from sparse data by exploiting the relationships among data points [101]."}, {"title": "5.6 Continual learning and Few-shot learning", "content": "Continual learning involves compiling research and methods to tackle the challenge of learning in scenarios where knowledge integration across endless streams of data needs to be considered, especially when the data distribution fluctuates over time [103,104]. In the realm of continual learning, models are crafted to acquire knowledge from new data while preserving previously gained knowledge, a task made difficult by catastrophic forgetting, a phenomenon where new learning can disrupt and overwrite previously absorbed information. To combat this issue, continual learning strategies often incorporate replay techniques, where a model is regularly trained on a combination of fresh data and selected samples of past data. These replay methods play a vital role in achieving the perfect equilibrium between stability\u2014maintaining existing knowledge and adaptability-effectively integrating new information. Furthermore, additional approaches like regularization techniques, which discourage alterations to crucial weights, and parameter isolation methods, which designate specific parts of the model for different tasks, are utilized to alleviate catastrophic forgetting and boost the performance of continual learning systems [105].\nAccording to Fig 8, few-shot learning is the concept for developing an expanding algorithm from an insignificant sample set. Few-shot learning for facial emotion recognition has been established to decrease the intraclass distance and enhance the interclass distance[106]."}, {"title": "5.7 Label noise", "content": "Label noise is a prevalent issue in real-world datasets caused by several factors, including the expense of the labeling process and the challenge of accurately classifying data. In the field of affective computing for complex emotion recognition, employing noisy labels addresses several challenges inherent in interpreting facial expressions. Concerning the subjective nature of human emotions, different annotators frequently provide diverse interpretations, leading to problems in labeling. Noisy labels help manage this ambiguity by allowing models to learn from a distribution of possible labels instead of a single, potentially erroneous one, thereby fostering more robust representations. Moreover, Fig 9 demonstrates that integrating label noise during training also enhances model robustness by leveraging techniques like label distribution learning, which improves generalization and mitigates the impact of incorrect labels. This approach is particularly beneficial for handling the complexity of real-world facial expressions, which often involve mixtures of basic emotions. Noisy labels mirror this variability and help models distinguish subtle emotional nuances. Additionally, methods like Face-Specific Label Distribution Learning (FSLDL) create augmented training samples with label distributions, broadening the range of expressions and viewpoints captured, thus enhancing the model's ability to generalize to new data. To prevent overfitting to noisy samples, techniques such as rank regularization and discriminative loss functions are employed, ensuring that the model focuses on more reliable samples and maintains overall performance. By addressing ambiguity and subjectivity, improving robustness, handling complex expressions, enhancing training with augmented data, and reducing overfitting, noisy labels significantly contribute to the advancement of complex emotion recognition systems in affective computing [107,108,109,110,111,112,113,114]."}, {"title": "5.8 Reinforcement learning", "content": "In affective computing for complex emotion recognition, employing reinforcement learning (RL) offers significant advantages by addressing key challenges. One major issue is handling unlabeled data, as complex emotions are often not explicitly labeled in datasets. RL, particularly through Deep Q-Networks (DQN), can learn from the environment via rewards and penalties, thus improving performance without the need for labeled data. Another challenge is identifying emotionally relevant intervals within data streams, such as video or physiological signals. RL-based segmentation dynamically learns to highlight these intervals, refining its strategy over time through rewards for accurate emotion recognition. It's also complex to integrate multimodal data, including facial expressions and physiological signals. RL excels in this by adaptively learning which signals are more indicative of specific emotions in varying contexts, enhancing the system's overall recognition capability.The system employs both facial expressions and physiological signals for emotion recognition. From facial expressions, it extracts confidence scores of seven basic emotions, valence-arousal (VA) levels, and ten action units (AUs). For physiological signals, it utilizes remote photoplethysmography (rPPG) to derive heart rate (HR) and heart rate variability (HRV) indices, along with EEG and ECG signals. This combination of facial and physiological data provides a comprehensive approach to recognizing complex"}]}