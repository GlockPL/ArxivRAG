{"title": "A Decision-Based Heterogenous Graph Attention Network for Multi-Class Fake News Detection", "authors": ["Batool Lakzaei", "Mostafa Haghir Chehreghani", "Alireza Bagheri"], "abstract": "A promising tool for addressing fake news detection is Graph Neural Networks (GNNs). However, most existing GNN-based methods rely on binary classification, categorizing news as either real or fake. Additionally, traditional GNN models use a static neighborhood for each node, making them susceptible to issues like over-squashing. In this paper, we introduce a novel model named Decision-based Heterogeneous Graph Attention Network (DHGAT) for fake news detection in a semi-supervised setting. DHGAT effectively addresses the limitations of traditional GNNs by dynamically optimizing and selecting the neighborhood type for each node in every layer. It represents news data as a heterogeneous graph where nodes (news items) are connected by various types of edges. The architecture of DHGAT consists of a decision network that determines the optimal neighborhood type and a representation network that updates node embeddings based on this selection. As a result, each node learns an optimal and task-specific computational graph, enhancing both the accuracy and efficiency of the fake news detection process. We evaluate DHGAT on the LIAR dataset, a large and challenging dataset for multi-class fake news detection, which includes news items categorized into six classes. Our results demonstrate that DHGAT outperforms existing methods, improving accuracy by approximately 4% and showing robustness with limited labeled data.", "sections": [{"title": "1 Introduction", "content": "The rapid expansion of the internet has enabled global connections, with platforms like Facebook, Instagram, and Twitter facilitating communication regardless of location [1]. These platforms allow users to quickly and efficiently share information, ideas, and opinions with wide audiences [2], which has changed how people access information in their daily lives [3]. Today, many prefer online resources and social networks over traditional media for communication, entertainment, and news [2]. However, the reliability of information on these platforms is often uncertain [4], leading to a rise in fake news due to the speed, accessibility, and low cost of social media [5]. Research shows that fake news spreads faster and wider than real news [6]. For instance, 50% of Facebook traffic involves fake news referrals, compared to 20% from reputable sources [7]. The spread of fake news can have harmful effects, such as influencing voter perceptions during elections [8] or causing stock market fluctuations [9]. The challenge of distinguishing real from fake news worsens these effects [10], making it crucial to develop strategies for automatically detecting and preventing fake news dissemination.\nIn recent years, researchers have proposed various methods for detecting fake news. Early approaches used traditional machine learning algorithms like Support Vector Machine (SVM) [11]"}, {"title": "2 Related work", "content": "There are two main strategies for combating fake news [36]: intervention and detection. Detection approaches concentrate on identifying and distinguishing fake news from legitimate information. In contrast, intervention strategies seek to limit the spread of fake news by identifying malicious sources [37, 38] or fraudulent users [39, 40]. The method proposed in this paper falls under the detection category, so this section will focus on detection methods.\nDifferent methods for detecting fake news have been extensively explored in the literature, each employing distinct features, models, and approaches to differentiate between fake and real news. The majority of these methods are grounded in a supervised learning approach. Yadav et al. [41] introduce FNED, a deep neural network model designed for the rapid identification of fake news on social media. By integrating user profile analysis with textual data analysis techniques like CNNs and attention mechanisms, FNED detects fake news across various content formats. Qu et al. [42] introduce QMFND, a quantum model for fake news detection that combines text and image analysis using quantum convolutional neural networks (QCNN). The NSEP framework [43] detects fake news by analyzing both macro and micro semantic environments. It first categorizes news into these environments and then uses GCNs and attention mechanisms to identify semantic inconsistencies between news content and associated posts.\nOne of the most significant challenges in detecting fake news using supervised learning approaches is the need for large and high-quality labeled datasets, which requires substantial time and financial and human resources. To address this challenge, researchers have turned to semi-supervised approaches. DEFD-SSL [44] is a semi-supervised method for detecting fake news that integrates various deep learning models, data augmentations, and distribution-aware pseudo-labeling. It uses a hybrid loss function that combines ensemble learning with distribution alignment to achieve balanced accuracy, especially in imbalanced datasets. Shaeri and Katanforoush [45] introduce a semi-supervised approach for fake news detection, which combines LSTM with self-attention layers. To handle data scarcity, they suggest a pseudo-labeling algorithm and utilized transfer learning from sentiment analysis through pre-trained ROBERTa pipelines to further enhance accuracy. Canh et al. [46] propose MCFC, a fake news detection method using multi-view fuzzy clustering on data from various sources. It extracts features like title and social media engagement, applying fuzzy clustering and semi-supervised learning. MCFC improves accuracy but faces challenges with parameter complexity and computational inefficiency. WeSTeC [47] automates labeling by generating functions from content features or limited labeled data, applying them to unlabeled data with weak labels aggregated via majority vote or probabilistic models. It uses a ROBERTa classifier fine-tuned for the dataset's domain, handling semi-supervision and domain adaptation by incorporating labeled data from various domains.\nDespite the fact that multi-class classification of fake news can offer a more accurate and nuanced analysis compared to binary classification, many of the presented methods (both supervised and semi-supervised) still classify news into just two classes: fake or real, and few researchers have explored multi-class classification in depth. Pritzkau [48] present a method for detecting fake news by fine-tuning RoBERTa, a Transformer-based model, for a 4-class classification task. The approach involves training on concatenated titles and bodies of articles, employing word-level embeddings and multi-head attention for feature processing. Rezaei et al. [49] propose a 5-class fake news detection model using a stacking ensemble of five classifiers-random forest, SVM, decision tree, LightGBM, and XGBoost-combined with AdaBoost for improved accuracy. The model is trained on PolitiFact data and leverages features like sentiment and semantic analysis. Majumdar et al. [50] present a deep learning approach for 4-class fake news detection using LSTM networks. Data preprocessing steps include removing stop words, correcting spelling errors, and eliminating punctuation.\nIn this paper, we focus on the LIAR dataset [18], which is one of the largest and most"}, {"title": "3 Problem definition", "content": "Let $D = \\{d_1, d_2,..., d_n\\}$ be the news set with n news items, where each news item $d_i$ contains text content $t_i$ and a contextual information set $S_i$. The contextual information $S_i$ is drawn from a set $S = \\{s_1, s_2, ..., s_m\\}$, representing m types of side information available in the dataset. We formulate the problem of fake news detection as a multi-class classification task utilizing a semi-supervised approach. The class labels are represented by $y \\in \\{Y_1, Y_2,..., Y_c\\}$, where C indicates the number of classes. For instance, in the LIAR dataset, which we focus on in this study, $C = 6$. To approach this problem, we first construct a heterogeneous graph $HG(V, E, R)$ using the contextual information, where V is the set of nodes, E is the set of edges, and Ris a set of edge types. In this graph, $|V| = n$, meaning that each news item is represented as a node, and $E = \\{(v_i, v_j, etype = r)|v_i \\in V, v_j \\in V, r \\in R\\}$. Thus, all nodes are of the same type, but the edges can vary in type, represented by the set R. The edge types are determined based on the contextual information S, implying $|R| \\leq |S|$. Different subsets or all of the contextual features can be selected to define various edge types in the graph.\nFollowing the semi-supervised approach, labels for a small subset of news, denoted as $D_L \\subset D$, are observed. Our goal is to learn the model $F$ for predicting labels for all unlabeled data $D_U = D - D_L$, utilizing the labeled news:\n$\\hat{y} = F(d_i)$,\n(1)"}, {"title": "4 Proposed method", "content": "In this paper, we propose a Decision-based Heterogeneous Graph Attention Network (DHGAT) for detecting fake news using a semi-supervised approach. Figure 1 illustrates the structure of our proposed model, which includes three main components: 1) heterogeneous graph construction, 2) Decision-based Heterogeneous Graph Attention Network (DHGAT), and 3) multi-class classification.\nWe use the FastText [61] model to generate textual embeddings for the news. FastText improves upon Word2Vec [62] by incorporating subword information, representing each word as a collection of character n-grams. This approach enables FastText to handle rare words, misspellings, and morphologically rich languages more effectively, generating meaningful word vectors even for unseen words. After generating text embeddings, we construct a heterogeneous graph comprising nodes of the same type but connected by different edge types, representing various relationships between the news items. The decision-based deterogeneous graph attention network processes this graph, where each node in each layer independently selects the most effective type of neighborhood to update its embedding vector. The final embedding vectors are fed into a Multi-Layer Perceptron (MLP) classifier, which uses a custom loss function to not only predict the labels but also minimize semantic errors between predicted and ground-truth labels. Detailed explanations of these components are provided in the subsequent sections."}, {"title": "4.1 Heterogeneous graph construction", "content": "The LIAR dataset contains a number of speaker profiles, such as speaker name, party affiliation, job title, and more. We utilize these features to construct a heterogeneous graph with uniform node types but different edge types. Each news item is considered a node. Then, two or more of these speaker profile features are selected, and for each one, a different type of edge is created"}, {"title": "4.2 Decision-based Heterogeneous Graph Attention Network (DHGAT)", "content": "In the heterogeneous graph HG, each node can connect to neighbors through various edge types, representing different relationships between news items. This results in a diverse set of neighborhood types for each node, with each type potentially transmitting unique information. These varying neighborhood types can differently influence the generation of the node's embedding. While existing GNNs for heterogeneous graphs use methods like multi-level attention mechanisms [64, 65] or distinct weight matrices [66] to capture and differentiate the effects of these neighborhood types, they overlook an important aspect: the need for different nodes to have different neighborhood requirements. This means that two nodes v and u in the same layer might need different types of neighborhoods, or a single node v might require different neighborhood types across different layers. Current GNNs cannot accommodate this scenario because they lack"}, {"title": "4.3 Multi-class classification", "content": "To classify news, the embeddings HL generated by DHGAT in the final layer (Layer L) are fed into a Multi-Layer Perceptron (MLP) network. To enhance classification performance, we propose a customized loss function defined as:\n$L(\\Theta) = \\frac{\\lambda_1}{N_L} \\sum_{i=1}^{N_L} \\sum_{c=1}^{C}(-y_{i,c}log(\\hat{y}_{i,c})) + \\frac{\\lambda_2}{N_L}\\sum_{i=1}^{N_L} |y_i - \\hat{y}_i|$ (18)\nwhere NL = |DL| is the number of labeled samples, C is the number of classes, $y_{i,c}$ is a binary indicator (0 or 1) if class label c is the correct classification for sample i, $\\hat{y}_{i,c}$ is the predicted probability that sample i belongs to class c, yi is the true class label for sample i, and $\\hat{y}_i$ is the predicted label for sample i. The parameters $\\lambda_1$ and $\\lambda_2$ control the contribution of each term in the loss function. The left term in the loss function aims to predict the true labels for unlabeled samples, while the right term is designed to decrease the semantic distance between the predicted labels and the ground-truth labels.\nIn the LIAR dataset, news items are classified into six categories: {false: 0, half-true: 1, mostly-true: 2, true: 3, barely-true: 4, pants-fire: 5}. We relabel the dataset to sort these labels from \"pants-fire\" to \"true\" with numeric values from 0 to 5 as: {pants-fire: 0, false: 1, barely-true: 2, half-true: 3, mostly-true: 4, true: 5}. This relabeling treats the labels as scores indicating the accuracy of news, where a score of 0 indicates the least accuracy (pants-fire) and a score of 5 indicates the highest accuracy (true). Consequently, the right term of the"}, {"title": "5 Model properties", "content": "In this section, we explore the key features of the DHGAT model. This model introduces the concept of \"optimal\" neighborhoods and dynamic computation graphs, addressing the limitations of standard GNNs. With the ability to learn and optimize the computation graph at each layer, DHGAT not only mitigates issues like over-smoothing and over-squashing but also generates embedding vectors tailored to the specific target task.\n\u2022 GNNs generate embeddings through a computational graph defined based on local neighborhoods. In this computational graph, node identifiers are not important; only nodes' features matter. In other words, if two nodes have computational graphs with the same local structure and neighborhood and identical node features, the embedding vectors for these two nodes will be the same. In DHGAT, each node learns to use an optimal neighborhood, which can lead to the creation of different optimal computational graphs for two nodes with the same neighborhood and features.\n\u2022 In standard GNNs, the computational graph of each node is determined based on the initial graph structure and remains fixed throughout the training process. However, in DHGAT, each node learns to interact only with optimal neighbors at each layer. As a result, the computational graph for each node is dynamic and can vary across different layers, with its optimal structure being learned during the training process.\n\u2022 In GNNs, node embeddings are updated based on their local neighborhoods and are independent of the target task defined in the prediction head. In DHGAT, each node learns an optimal neighborhood for updating its embedding, resulting in a task-specific and optimal computational graph. For instance, if a specific task requires each node to use only a particular type of neighborhood, the DHGAT model can learn to use only that specific type of neighborhood for updating the embeddings.\n\u2022 Over-squashing in the context of GNNs refers to a phenomenon where information from distant nodes in a graph gets \"squashed\" or compressed as it propagates through the network layers. This leads to a loss of important details and hampers the network's ability to capture the relationships between distant nodes effectively [68, 69]. Our model, which allows each node in each layer to choose the optimal type of neighborhood can help in mitigating the over-squashing problem. The optimal selection of information propagation paths and the effective allocation of resources to nodes allow for more efficient information propagation, reducing the problems associated with excessive compression. To argue how DHGAT improves over-squashing, we can consider the following points:\nBy allowing each node to select the optimal neighborhood type at each layer, DHGAT can optimally receive information from different types of edges during each learning step. This means that important and useful information is transferred more effectively and precisely to the nodes, preventing it from being overly compressed.\nOver-squashing typically occurs on long paths in graphs [68, 69]. By choosing the optimal type of neighborhood in DHGAT, nodes can select shorter and more efficient paths for information propagation, which can help reduce the negative effects of over-squashing.\nBy allowing each node to learn independently its neighborhood type, the model can be specifically optimized for each node's local structure. This specialization can help mitigate issues related to over-squashing."}, {"title": "6 Experiments", "content": "In this section, first we provide an overview of the LIAR dataset, the baseline models and the evaluation metrics used in our empirical evaluations. Then, we report the results of our extensive experiments, encompassing diverse settings and conditions to comprehensively assess our model\n6.1 Dataset\nWe evaluate the performance of our proposed model on the LIAR [18] dataset, one of the largest public benchmarks for real-world fake news detection. The dataset comprises 12,836 short labeled news from 3,318 public speakers, with an average of 17.9 tokens per news item. These news are labeled into six categories: pants-fire, false, barely-true, half-true, mostly-true, and true. The LIAR dataset is created in 2017 and sourced from PolitiFact's public fake news data. It includes extensive speaker profiles, such as speaker name, party affiliations, job title, home state, context, subject, and credit history.\n6.2 Baseline Models\nTo evaluate the performance of multi-class fake news detection on the LIAR dataset, we compare the DHGAT model against seven fake news detection methods:"}, {"title": "6.3 Experimental setup", "content": "We use the pre-trained FastText model to extract the feature vectors of the textual content of news articles. The 300-dimensional embedding vectors are considered as the textual feature vectors for each news item. To construct a heterogeneous graph, we consider various types of neighborhoods. In the LIAR dataset, we define 9 types of edges: speaker, context, subject, party-affiliation, job-title, state, KNN-5, KNN-6, and KNN-7. The first six are side information available in the dataset, and the last three are based on the similarity of textual embedding vectors using the K-nearest neighbors (KNN) algorithm. We explore different combinations of subsets (pairs, triplets, etc.) from these edge types to construct the heterogeneous graph. For both the decision network (\u03a6) and representation network (\u03a8), we use a 2-layer GATv2 network. The hidden units are set to [256, 128], and the learning rate is 0.001. The dropout rate is set to 0.5. We use the Adam optimizer with a weight decay strategy to train all parameters over 200"}, {"title": "6.4 Results", "content": "Table 4 shows the results for 6-class fake news classification on the LIAR dataset for various models. Our proposed DHGAT model, along with baseline models like GATv2, GCN, and MGCN, utilizes different types of contextual information to construct the graph. We have reported the best result obtained for each model in Table 4. The TextCNN model, which only uses the textual content of news for classification, generally performs worse than the graph-based models. This suggests the positive impact of modeling the dataset as a graph and learning the structural relationships between different data points. The HGNNR4D model is originally proposed for binary classification of fake news on the LIAR dataset. We adapted it with slight modifications to its classification layer for multi-class classification. Despite achieving relatively good performance in binary classification on the LIAR dataset (accuracy of 57.53, as stated by the original authors), it shows very poor performance in the 6-class classification task. The LSTM-CP-GCN and TextGCN models use a graph-based modeling approach. However, the textual content of each news item is modeled separately and independently as an individual graph, and the task is defined as a graph classification problem. In these models, no relationships between different news items are considered, and each news article is processed independently. Finally, the GAT, GCN, and MGCN models utilize contextual information to model the dataset as a graph, where each news item is treated as a node and the edges represent relationships between different data points. These models generally outperform the other evaluated methods. GATv2 performs better than GCN because, unlike GCN, which treats the influence of all neighboring nodes equally, GATv2 assigns different weights to each neighbor, and these weights are learned during the training process. None of these models have a control mechanism to select optimal neighborhood connections. The DHGAT model, with its ability to learn and select the optimal neighborhood for each node, achieves better performance than the aforementioned methods, improving accuracy by approximately 4%.\nTo understand the impact of each speaker profile attribute (speaker, context, subject, party affiliation, job title, state) and the textual embedding vectors (KNN-5, KNN-6, KNN-7), we present the results of graph construction based on each of these features. These graphs are then processed using the GATv2 and GCN models, as shown in Figure 3. As expected, constructing the graph using the speaker feature delivers the best performance, consistent with reports from existing methods. Following that, the context and subject features rank second and third,"}, {"title": "7 Conclusion", "content": "In this paper, we introduced the Decision-based Heterogeneous Graph Attention Network (DHGAT), a novel semi-supervised model for multi-class fake news detection. DHGAT effectively addresses several limitations inherent in traditional graph neural networks by dynamically optimizing neighborhood types at each layer for every node. It represents news data as a heterogeneous graph where nodes (news items) are connected through multiple types of edges that capture various relationships. The DHGAT architecture, comprising a decision network and a representation network, allows nodes to independently choose their optimal neighborhood type in each layer. This dynamic and task-specific approach to neighborhood selection significantly enhances the model's ability to generate accurate and informative embeddings, tailored to the classification task. Our extensive experiments on the LIAR dataset demonstrated the high accuracy and performance of DHGAT."}]}