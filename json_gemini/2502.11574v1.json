{"title": "Large Language Models and Mathematical Reasoning Failures", "authors": ["Johan Boye", "Birger Mo\u00ebll"], "abstract": "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies focusing solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models-including Mixtral, Llama, Gemini, GPT-40, and OpenAl's ol variants-we find that while newer models (e.g., 03-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers via flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and inability to translate physical intuition into mathematical steps. Manual scrutiny reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.", "sections": [{"title": "1 Introduction", "content": "How good are large language models (LLMs) at mathematical reasoning? This question has been addressed by several authors, who have constructed data sets in order to evaluate the mathematical capabilities of LLMs, e.g. (Hendrycks et al., 2020, 2021; Cobbe et al., 2021; Chernyshev et al., 2024; Li et al., 2024). In most of these studies, only the final answer produced by the LLM on a given problem was checked for correctness \u2013 the questions were either multiple-choice, or the answer consisted of a single number, both cases facilitating automatic evaluation. However, as it is possible to arrive at a correct answer by means of shallow heuristics rather than a watertight argument, it is important to also study the full solution provided by the model, much in the same way a teacher would assess a student exam. Of course, this method requires manual scrutiny and is therefore more time-consuming, but we argue that it is indispensable for to get a proper picture of the mathematical prowess of LLMs.\nIn this paper, we present a small dataset\u00b9 of 50 newly constructed mathematical problems intended for LLM evaluation, and use it to evaluate X models: mixtral8x7b (Albert Q. Jiang et al., 2024), llama3.3-70B-versatile (Hugo Touvron et al., 2023), Gemini-2.0-pro-exp (Google, 2024), GPT40 (OpenAI, 2024a), o1-preview, o1, and 03-mini (OpenAI, 2024b). Our problems are all formulated in natural language (\u201cword problems\") and require no more than high-school level mathematical knowledge: basic principles of counting and divisibility, some algebra, arithmetic, probability and geometry, and some real-world knowledge, e.g. that it is impossible to walk on water, how many minutes there are in an hour, how the dots are placed on dice (e.g. the is opposite the ), and so on. We purposely excluded complicated sums or integrals written in pure mathematical notation, since there are already computer algebra systems like Mathematica that can solve large classes of such problems in a precise way. Our goal was rather to focus on natural language word problems.\nSuch mathematical word problems provide an excellent testbed for evaluating the reasoning capabilities of LLMs. Early LLMs were not explicitly trained to perform reasoning but rather to do next-token prediction, possibly with additional training based on techniques like RLHF (Ouyang et al., 2022). Still, these models seemed capable of per-\""}, {"title": "2 Related work", "content": "A number of researchers have created datasets to evaluate the mathematical abilities of LLMs. MATH (Hendrycks et al., 2021) contains a large collection of mathematical problems from different domains, with 7 different levels of difficulty. The answer is always a number. Also the MMLU (Hendrycks et al., 2020) contains a mathematics section consisting of multiple-choice questions.\nGSM8K (Cobbe et al., 2021) contains word problems on a grade-school level solvable by simple arithmetic. The answer is always a number. GSM-Plus (Li et al., 2024) and GSM-symbolic (Mirzadeh et al., 2024) are both extensions of GSM8k with adversarial examples. In the latter case, the authors showed that it was possible to confuse the models by adding irrelevant numerical information to the problem formulation. In some cases, the models worked this irrelevant informa-"}, {"title": "3 Method", "content": "We constructed a set of 50 problems. Four problems were taken from a Swedish book of mathematical puzzles (Vaderlind, 1996), the rest were invented by the authors. The selection/design criterion for the problems was that they should be solvable with only high-school mathematics, although the questions themselves might be of a different nature than those posed to high-school students (depending on country). Some problems had a specific numerical answer, some asked for a statement of the type \u201cIt is possible/impossible to do X?\u201d, and some asked for a concrete method, algorithm, or strategy to obtain some particular goal. All problems are listed in the appendix.\nEach question was posed once to each model through their respective APIs. This led to 400 answers from the models, which were assessed manually by the first author (who is also an experienced teacher), checking both the answer and the solution for correctness. If the solution was incorrect, we also wrote a brief note describing the nature of the problem."}, {"title": "4 Results", "content": "4.1 Quantitative results\nTable 1 summarizes the results of the various models. \"Correct\" means that the model has given the correct answer and a correct solution, whereas \"Ans\" means that the model has given the right answer but an erroneous solution. This could happen as some questions have the structure \"Is it possible to...\", where the model might answer \u201cNo\u201d while providing the wrong motivation. All in all, 21 questions (5%) were answered in this way, suggesting that it is essential not just to look at the final answer when evaluating the reasoning capabilities of models. There are also a few \"Sol\" instances where the reasoning is correct and model has found the key idea, but makes a small calculation error leading to the wrong answer.\nWe see from table 1 that Mixtral8x7b is the worst-performing model, getting no solutions right, followed by Llama3.370B-versatile (10/50) and gpt-4o (14/50). The later models that have been trained with an explicit problem-solving objective Google (2024), OpenAI (2024b), Guo et al. (2025) fare much better, although there is still some variation.\n4.2 Spatial reasoning problems\nThis is a problem that confounded every model:\n(Problem 11): A dog is on an automatically retractable leash. If the owner is standing at (0,0) and the dog runs to (5,0), the extended part of the leach is 5 metres long, but when the dog returns to its owner at (0,0), the leach is rewinded and is 0 metres long again. However, if there is a lamppost at (1,3) and the dog runs from (0,0) to (5,0), then to (0,5) and then back to (0,0) again, the leash will loop around the lamppost so the extended part of the leash is now 2*sqrt(10), i.e. the distance from (0,0) to the lamppost and back again. Suppose now that there are lampposts at (1,3), (3,1), (6,3), (3,6), (9,7), and (7,9). The dog runs the following trail: (0,0) to (6,0) to (0,6) to (6,12) to (12,6) to (6,0) to (0,6) to (6,12) to (12,6) to (6,0) to (0,0). What is the length of the extended part of the leash when the dog has finished its run? Round the answer upwards to the closest integer.\nFigure 1 shows the dog's trail (left), and how the leash will wrap around the lampposts (right). This is an example of a problem which is easy to solve for a human (if allowed to use pen and paper to draw a figure), since the mathematics involved is just repeated use of the distance formula. Most adults would have intuitive idea of how a piece of string behaves when looped around some lampposts and then tightened, which makes it easy to come up with the picture in Figure 1.\nThe reasoning errors committed by the models suggest that they cannot grasp the physics of the situation. 01 seemed to seize on the example in the question, and assumed that it should add the Euclidean distances from (0,0) to (some of) the lampposts and back again. deepseek-r1 comes to the same conclusion, even though in its reasoning printout (which is accessible for the user, unlike in the 01 and 03 models), deepseek seems to realize that the leash is wrapped twice around the diamond created by the four furthest lampposts, but fails to draw the right conclusion from that observation.\n03-mini explains (wrongly) that the dog is running one leap clockwise around the four furthest lampposts, and then counter-clockwise, meaning that \"the two windings cancel each other\". Somehow its conclusion is that the extented part of the leash is $2\\sqrt{10}$, just as in the example in the question. The remaining models have non-sensical explanations.\nAnother problem where humans are helped by mental imagery is the following:\n(Problem 19): Suppose you have two ordinary six-sided dice which you want to place on a wooden table so as few dots as possible are visible. The best way of doing this is placing them next to each other with the six dots facing downwards and the five dots facing each other. This way 2*(1+2+3+4)=20 dots will be visible altogether (the observer is allowed to walk around the table). We define v(n) to be the minimal number of dots visible on n dice placed on a table. You are given v(1)=15, v(2)=20, v(3)=26. What is v(37)?\nThe correct answer is 95. The optimal placement is first to arrange 36 of the dice in a 6 \u00d7 6 square, with \"1\" facing upwards on each die, \"2\" and \"3\" facing outwards on the dice in the corners, and \"2\" facing outwards on the dice along the edges, making 88 dots visible. The 37th die is placed with \"1\" facing upwards, and its \"5\" pressed against one of the \"3\"s in the square of dice. The 37th die now exposes 1\u20134, but covers a \"3\" which was previously visible. All in all, adding the 37th die will contribute an additional 7 visible dots, so v(37) = 88+7 = 95.\ndeepseek-r1 actually nailed this problem, giving essentially the explanation above, after an extensive chain-of-thought process (>22,000 tokens). 03-mini realized the 6 \u00d7 6 configuration, but then goes astray when placing the 37th die. o1 and gemini instead suggested putting the dice in a line (which is sub-optimal), and also failed to correctly count the number of visible dots for that configuration. The remaining models tried to fit a numerical formula (e.g. a quadratic formula) based on the three examples given in the question, without considering the actual physics of the problem. These attempts all ended in failure.\nFinally, we mention the following problem, which resulted in the largest number of incorrect solutions but correct answers:\n(Problem 26): We want to assign a number in {1... 12} to each of the edges on a cube so that (1) each edge is assigned a different number, and (2) the sum of the four edges on one face of the cube will be the same for all faces. Determine whether this is possible or not. If it is possible, determine which number the edges on one face should add up to.\nA correct solution would first point out that each number would appear on two faces, meaning that the total number of numbers visible on the six faces is 2(1 + ... + 12) = 156, which entails that the sum of each face is 156/6 = 26. All models except mixtral came this far. However, the second part of the solution is to show that there is a concrete assignment of numbers to edges that result in each face having the sum 26. deepseek-r1 tried to do this but came up with an erronous assignment. Only 03-mini managed to get the solution completely correct.\nSeveral models concluded that assigning numbers to the edges as described in the question is possible just because twice the sum of 1..12 is divisible by 6, or equivalently that 1 + . . . + 12 is divisible by 3. But there are many sets of 12 numbers whose sum is divisible by 3 but which cannot be assigned to the edges of a cube in the way described in the question. The failure to realize this might have been due to the model having seen the problem in its training data (and knowing it to be solvable), or simply a failure to consider the physical constraints of the problem.\n4.3 Strategy problems\nMost LLMs were struggling with problems of a strategic nature. An example was the following:\n(Problem 4): An ordinary tic-tac-toe board has 9 squares: (1,1) - (3,3). Now consider fric-frac-froe, which is played on an extended board where the top row has four squares (1,1)-(1,4), and the other two rows have three squares as before. The objective of fric-frac-froe is to have three markers in a row, just as in ordinary tic-tac-toe. Either find a winning strategy for the fric-frac-froe player who goes first, or explain why the game is a draw.\nThat is, the board looks like Figure 2. It does not take a human observer long to discover a winning"}, {"title": "5 Discussion", "content": "Throughout the erroneous answers to the 50 example problems, we can see many traits we also see in many human math and engineering students failing to solve similar problems:\n\u2022 Making arithmetic errors\n\u2022 Disregarding constraints in the question formulation (as several models did for problem 47 above)\n\u2022 Adding unwarranted assumptions (as in problem 7 above)\n\u2022 Over-reliance on preliminary numerical evidence, as in problem 17\n\u2022 Trying to shoe-horn a problem into a known solution method, as in the \"fric-frac-froe\" game, where several models seemed to assume the game to be a draw, just like tic-tac-toe.\n\u2022 Failure to find a key idea (the problem is just too difficult).\nHowever, the failure to add unstated but common-sense assumptions is rather unique to models, e.g. that it is impossible for the runner to run in a swimming pool (this running strategy was suggested by 01-preview in problem 30).\nOn the other hand, in particular the latest models 01, 03-mini, deepseek-r1 and gemini seem to have a large base of mathematical knowledge. Throughout the solutions, we could see the models make reference to Pick's theorem, the Frobenius coin problem, and Eulerian circuits, among others. Even though we have focused on faulty reasoning in this paper, state-of-the-art models (in particular the 01, 03, and deepseek models) have impressive reasoning capabilities and can solve quite difficult problems. The problem, as always with LLMs, is that also the erroneous solutions can look good at a cursory inspection, in particular if the reader has limited mathematical knowledge."}, {"title": "6 Limitations", "content": "The results presented in this article provide a snapshot of the mathematical abilities of some state-of-the-art large language models (LLMs) in early 2025. The article provide some insights to the blind spots and shortcomings of LLMs when it comes to mathematical reasoning, but is unclear how much one can generalize from the results, due to the following:\n\u2022 LLM technology is developing rapidly, and it is perfectly possible that state-of-the-art models can solve more problems than described here just a few months from now.\n\u2022 Each model was just queried once, due to time constraints (each solution was assessed manually, which took considerable amounts of time). It is possible that in some cases, a model might have produced a better answer in a second or third try.\n\u2022 We do not have access to the internals of the systems, in particular, we could not scrutinize the chain-of-thought printouts from the 01, 01-preview, 03-mini, and gemini-2.0-pro-exp models.\n\u2022 8 state-of-the-art models were tested, but there are of course more models than these, and the models also exist in several versions. Due to time contraints, we could not try all of them.\n\u2022 The 50 problems in the problem set only covered certain sub-areas of high school mathematics. Notably, trigonometry and calculus were missing.\n\u2022 Though we strived to invent original problems which would not appear in the training set of any model, our imagination is limited, and it is perfectly possible that some model had seen some problem (or something very similar) in its training phase."}, {"title": "A Appendix: All 50 problems", "content": "1. Let $n_i$ be the numeral obtained by writing the number 97 in base i. Then interpret $n_2,..., n_9$ as decimal numbers, and let s be the sum of those numbers. What is s modulo 97 (in base 10)?\n2. We have a calculator that respects the ordinary laws of arithmetic precedence (e.g., 2+3*4 will result in 14). We now randomly press (with a uniform probability) one of the digits 0-9, then either '+' or '*', then another random digit, then then either '+' or '*' again, and then another random digit. Finally, we press '=', and note down the answer. If we keep repeating this experiment over and over, what is the expected average result?\n3. On a black-and-white computer screen, digits and numbers are displayed as bitmaps with 7 rows and 5 columns. For instance, an \"I\" is displayed like this:\n01110\n00100\n00100\n00100\n00100\n00100\n01110\nThe bitmap for \"I\" contains 3 ones on the first row, 1 one on the second row, etc., that is, [3,1,1,1,1,1,3] ones, counting from the first row to the last. Which letter in A-Z has this number of ones: [4,2,2,4,2,2,4], counting from first row to the last?\n4. An ordinary tic-tac-toe board has 9 squares: (1,1) - (3,3). Now consider fric-frac-froe, which is played on an extended board where the top row has four squares (1,1)-(1,4), and the other two rows have three squares as before. The objective of fric-frac-froe is to have three markers in a row, just as in ordinary tic-tac-toe. Either find a winning strategy for the fric-frac-froe player who goes first, or explain why the game is a draw.\n5. We will call a binary tree with numbers at each node a 'labeled binary tree'. Either give an example of a labeled binary tree of depth 3 whose pre-order traversal and post-order traversal yields the same sequence of numbers, or explain why no such tree can exist.\n6. A rectangle has sides with non-zero integer lengths. Adding the length of the perimeter and the area of the rectangle yields 9793. How long are the sides?\n7. We have two disjoint sets of numbers: A, with n members, and B with n+1 members. We want to construct a sequence of numbers which is 2n+1 numbers long, and every second number is selected from A and every second number from B, and the sequence has to start and end with a number from A. Either suggest a method for doing this, or explain why such a method cannot exist.\n8. We have 4 points in the plane: p1, p2, p3, p4, and construct a polygon by drawing a line from p1 to p2, from p2 to p3, from p3 to p4, and from p4 back to p0 again. Suppose p1=(3,4), p2=(7,7), and p3=(10,3). If we want the polygon to be a square, where should p4 lie? Either give the coordinates of p4, or explain why no such point can exist.\n9. Let $a_0$ be the factorial of $1000^{1000}$, and let $a_k$ be the sum of digits in $a_{k-1}$, for k > 0. After i steps, $a_j$, $A_{i+1}$, $A_{i+2}$,\u00b7\u00b7 . will be same number, which is a single digit. Which digit?\n10. Let x be a positive integer and define the following rule f: f(x) = x/3 if x is divisible by 3, otherwise f(x) = 2x + 1. We are interested in how many times we must apply this rule before we reach the number 1. For x = 4, we need 3 applications: f(4) = 9, f(9) = 3, f(3) = 1. Let us use the notation g(x) to denote the smallest i such that i applications of f starting from x results in 1. As we saw, g(4) = 3. If no such i exists, we let g(x) = -1. What is g(1) + g(2) + . . . + g(100)?\n11. A dog is on an automatically retractable leash. If the owner is standing at (0,0) and the dog runs to (5,0), the extended part of the leach is 5 metres long, but when the dog returns to"}, {"title": "B Appendix: Solution to problem 22", "content": "The answer is that 6 questions are sufficient and also necessary in the worst case. Careful analysis of the problem reveals that there are 8 different possible configurations, called A-H (see Figure 4)):\nIn addition, the triangles might have different ID numbers, e.g. (1,8,171) might be any of T1 \u2013 T5, so the total number of possibilities are 5! 8 = 960.\nThe stategy for querying the oracle is described visually in Figure 4. First ask the oracle about about 1 and 8. If 1 and 8 are in the same triangle, ask about 4 and 6. If they are in the same triangle,"}]}