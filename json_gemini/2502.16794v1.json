{"title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding", "authors": ["Xilin Jiang", "Sukru Samet Dindar", "Vishal Choudhari", "Stephan Bickel", "Ashesh Mehta", "Guy M McKhann", "Adeen Flinker", "Daniel Friedman", "Nima Mesgarani"], "abstract": "Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available\u00b9.", "sections": [{"title": "1 Introduction", "content": "The human auditory system does not process all sounds equally but selectively amplifies relevant elements while suppressing others based on the listener's intent (Mesgarani and Chang, 2012). In a multi-speaker setting, a listener may focus on a single speaker, tune into background music, or ignore speech entirely (Cherry, 1953; Osgood, 1959; Shinn-Cunningham and Best, 2008). While modern auditory foundation models, including auditory large language models (LLMs), are designed for general-purpose auditory understanding, they do not inherently account for listener intent. In applications such as assistive hearing devices, however, listener-aware processing is critical. These systems must prioritize the content most relevant to the user to improve usability in complex acoustic environments. Existing models, such as LTU (Gong et al., 2024), SALMONN (Tang et al., 2024), and Qwen2-Audio (Chu et al., 2024), process all incoming audio equally, making them ineffective in scenarios where distinguishing between attended and unattended speech is essential.\nAlthough these models excel at general auditory scene understanding (Sakshi et al., 2024; Wang et al., 2025), they lack mechanisms to selectively process speech based on listener perception. In multi-speaker environments, they transcribe and analyze all speech sources indiscriminately, failing to separate what the user is actually attending to from background conversations (Wu et al., 2024). To address this, a listener-aware auditory AI must move beyond passive transcription and actively adapt its processing to reflect user intent.\nStudies have shown that the auditory cortex encodes speech features from the attended talker (Mesgarani and Chang, 2012), allowing neural decoding methods to reconstruct or enhance the target speaker by comparing brain signal-derived representations with competing speech (O'sullivan et al., 2015). This line of research, also known as auditory attention decoding (AAD), has sought to infer a listener's auditory focus from neural signals. Both invasive (Han et al., 2019; Ceolini et al., 2020; Choudhari et al., 2024) and non-invasive methods (Ciccarelli et al., 2019; Geirnaert et al., 2021; Vandecappelle et al., 2021; Pan et al., 2024, 2023) have been explored for attention-controlled speech extraction in hearing devices. However, AAD is primarily used for signal enhancement rather than guiding an Al's interpretation of an auditory scene. While it could improve speech intelligibility, it does not enable models to reason about the attended content, such as summarizing speech or answering user queries based on what they perceive.\nPrior studies have attempted to integrate neural signals into large language models models, incorporating neural signals to enhance multimodal processing. Some efforts have used neural data for EEG-based text generation (Jiang et al., 2024a; Kim et al., 2024) or fMRI-informed representations (Zheng and Sun, 2024), while others have integrated brain signals to improve LLM semantic understanding (Toneva and Wehbe, 2019; Moussa et al., 2025). However, these approaches focus primarily on language comprehension and semantic alignment, rather than using neural signals to refine auditory scene interpretation or speaker selection.\nThis paper introduces Intention-Informed Auditory Scene Understanding (II-ASU), a framework where models align their interpretation of sound with listener intent. Instead of modifying the auditory signal like AAD, we integrate attentional signals into a language model to guide how it processes and responds to auditory scenes. This approach enables reasoning beyond speech separation, allowing models to adapt responses based on listener focus rather than treating all input speech equally. To implement this, we present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that extends an auditory LLM with neural attention decoding. The model processes intracranial EEG (iEEG) to determine which speaker the listener is attending to, extracts speech representations while retaining both attended and ignored sources for contextual processing, and integrates the decoded attentional state into the LLM to generate responses that reflect listener perception.\nThis work makes several contributions. It introduces II-ASU as a paradigm shift from passive auditory processing to listener-aligned interpretation; It proposes AAD-LLM as the first system to integrate brain signals into an auditory LLM for attention-driven scene understanding; Finally, it evaluates AAD-LLM across multiple auditory tasks, demonstrating improved alignment with listener perception. While this study focuses on selective attention, the broader II-ASU framework could extend to other intent-detection methods, including gaze tracking, head orientation, posture shifts, or explicit user input. By incorporating such attentional signals, future intention-aware auditory AI could dynamically adapt to user perception in a wide range of applications."}, {"title": "2 Intention-Informed Auditory Scene Understanding", "content": "Consider the scenario illustrated in Figure 1: both a human and a machine listener are exposed to the same auditory scene, denoted as S. The human listener applies selective auditory attention forming an internal representation based on their intent I. When asked a question Q, an intention-informed auditory model, such as AAD-LMM, should generate an answer A that depends on Q, S, and I:\nA = MachineListener(Q, S, I)                                                                                                               (1)\nHowever, existing auditory LLMs (Gong et al., 2024; Hu et al., 2024; Chu et al., 2024) are not intention-informed. They function as:\nA\u00b0 = AuditoryLLM(Q, S)                                                                                                             (2)"}, {"title": "3 AAD-LLM", "content": "We introduce AAD-LLM as a prototype system designed to address the Intention-Informed Auditory Scene Understanding (II-ASU) problem. AAD-LLM is a multimodal auditory large language model (LLM) that takes three distinct inputs: a textual question Q, a speech mixture S, and a listener's brain signal Z. The model's output A is conditioned on all three inputs:\nA = AAD-LLM(Q, S, Z)                                                                                                    (3)\nAs illustrated in Figure 2, AAD-LLM integrates neural attention decoding with auditory language processing. Unlike standard auditory models, AAD-LLM must process acoustic, linguistic and neural signals simultaneously. This introduces two key challenges. First, most neural recordings provide only a few minutes of data per participant, making it impractical to train a full end-to-end system jointly. Second, existing auditory attention decoding (AAD) methods (Geirnaert et al., 2021) reconstruct a temporal representation of speech from neural activity. However, these representations are continuous, noisy, and not directly compatible with discrete token-based LLMs. AAD-LLM solves these challenges by first extracting a discrete speaker identity token from brain signals, and second, using the speaker identity to condition the LLM's response generation. By decoupling brain decoding from language modeling, AAD-LLM allows intention alignment to be trained independently on large-scale speech data, while brain decoding is trained on limited neural data."}, {"title": "3.1 Intention Decoding", "content": "AAD-LLM introduces a speaker-based approach for decoding auditory attention from intracranial EEG signals. Instead of reconstructing speech features, we classify which speaker the listener is attending to using a discrete token representation. The process consists of two steps. First, we perform speaker clustering by applying K-means clustering to x-vectors (Snyder et al., 2018) extracted from a large corpus of thousands of speakers, ensuring no overlap with test speakers. The x-vectors, commonly used in speaker verification, are 512-dimensional embeddings, which remain frozen during training. The number of clusters K is set to 8. Next, we perform speaker prediction from neural signals. A bidirectional LSTM maps the neural signal Z \u2208 \\mathbb{R}^{C\\times T} to a predicted cluster index \\u03af \u2208 {0, 1, ..., K \u2013 1}. The ground-truth label i is determined by finding the closest centroid to the attended speaker's x-vector. Further details about the model is included in Appendix B."}, {"title": "3.2 Intention Alignment", "content": "Once the listener's attended speaker identity is decoded, the next challenge is to align the auditory LLM's response with listener perception. This is achieved in three steps:\n1. Embedding the intention token: The speaker identity token v (or \u00fb during inference) is projected into the LLM's embedding space via a linear speaker projector. It is then concatenated with the acoustic embeddings from the speech encoder and the textual embeddings from the question. Given two candidate speeches, s\u2081 and s\u2082, presented in random order, the LLM must determine which one to prioritize as the foreground and which to relegate to the background. However, simply embedding the intention into the LLM's input does not automatically enable the auditory LLM for such selective speech processing. Two more steps are needed:\n2. Intention-informed Training: Since real human attentional data is limited, we simulate attention during training by randomly assigning one of two mixed speech sources as the foreground (attended) speaker. We then assign the corresponding speaker identity token v as the intention input. Finally, we train the model with intention-aware tasks such as speaker-focused transcription, selective speech summarization, and foreground/background question answering. The solution is derived from the specific speaker or speech referenced in the question, whether attended or ignored. Further details are included in Appendix A.\n3. Chain-of-Thought Prompting: Empirical experiments revealed that even after training with intention-aware answers with a large rank (512) with low-rank approximation method (LoRA) (Hu et al., 2022), the LLM often ignored the attention token. To enforce attention usage, we introduce a structured prompt format adding chain-of-thought (CoT) (Wei et al., 2022) prefix of this form:\n\u201cAttention:;<att_spk_label>;\nSpk1:<spk1_label>;\nSpk2:<spk2_label>;\u201d\nThis CoT prompting explicitly directs the model to extract the speaker labels of the input speeches and the label of the attended speaker, all in {0, 1, ..., K \u2013 1}. We compare with the performance of AAD-LLM trained without the CoT prefix in Appendix D.4."}, {"title": "3.3 Auxiliary Module", "content": "To further enhance speaker differentiation, AAD-LLM incorporates a speech separator based on Mamba-TasNet (Jiang et al., 2024d). This module pre-processes the speech mixture, outputting two separated streams s\u2081 and s\u2082, which are then processed by the LLM. Importantly, the speech separator is intention-uninformed, meaning that it does not inherently prioritize the attended speaker. Instead, the LLM must selectively process the correct speech stream based on brain-decoded attention signals. We compare with the performance of AAD-LLM without the separator in Appendix D.4."}, {"title": "3.4 Training Objective", "content": "AAD-LLM is trained to generate a response sequence O = Cat(CoT, A), combining the CoT prompt and final answer. Each output token Oi is conditioned on the intention I decoded from the brain, the two speech inputs s\u2081 and s\u2082, the question Q, and all preceding tokens O1:i\u22121. This leads to the following loss function L\n\\sum_{i=2}^{N} - log P(O_i | SP(I), SE(s_1), SE(s_2), Q, O_{1:i-1})\nwhere SP and SE represent the speaker projector and speech encoder, respectively, as illustrated in Figure 2. Both the speech encoder and the LLM were finetuned using LoRA. The speech separator was trained to maximize the signal-to-noise ratio (SNR) of the separated speeches, and the speaker"}, {"title": "4 Experiments", "content": "We evaluated AAD-LLM using intracranial (iEEG) data with neural signals collected from human listeners as they attended to one conversation while another interfering conversation and background noise were present. We present only the key findings in this section (more results can be found in the appendix, including dataset curation (Appendix A) and task specification (Appendix C), model implementation (Appendix B), and additional results (Appendix D) and analyses (Appendix E)."}, {"title": "4.1 Datasets", "content": "The iEEG Clinical Dataset includes six epilepsy patients implanted with intracranial electrodes as part of their medical care for epilepsy surgery. Electrode placement, clinically determined, consisted of subdural electrocorticography (ECOG) grids and/or stereo-electroencephalography (sEEG) depth electrodes. Neural signals were bandpass-filtered (0.5-30 Hz) and concatenated across participants to maximize electrode coverage. The study was approved by local IRBs, and informed consent was obtained. Subjects listened to overlapping conversations masked with either pedestrian or babble noise. They attended to one of two simultaneous conversations, each containing two speakers taking turns. Conversations were aligned and segmented into sentences, yielding 280 training, 30 validation, and 50 testing utterances. To mitigate potential biases in the LLM's response to speaker order, we expanded the test set by training the speaker predictor five times with different random initializations and reversing speaker order, resulting in 500 test samples.\nThe Speech-Only Dataset was collected primarily to train AAD-LLM, which requires significantly more data than the iEEG dataset. To simulate the iEEG recording conditions, we mixed two random speakers from TextrolSpeech (Ji et al., 2024), a subset of LibriTTS (Zen et al., 2019), with one background noise sample from DEMAND (Thiemann et al., 2013). This resulted in approximately 54,000 (85.3h) training, 1,000 (1.6h) validation, and 3,000 (4.8h) testing utterances. Notably, there is no overlap in speakers, spoken content, or background noise between the iEEG and speech-only datasets, ensuring no information leakage during testing. Further details on both datasets are provided in Appendix A."}, {"title": "4.2 Tasks and Metrics", "content": "We trained and evaluated AAD-LLM and other models on the following tasks and metrics:\nAuditory Attention Decoding (AAD) measures how accurate AAD-LLM identifies the attended speaker, a crucial step for downstream tasks. We also evaluated Target Speech Extraction (TSE) with signal-to-noise ratio (SNR), scale-invariant signal-to-distortion ratio (SI-SDR) (Le Roux et al., 2019), word error rate (WER), and speaker similarity (SIM) against the clean attended speech.\nWe further considered four tasks for both the foreground (attended) and background (ignored) speaker or speech, covering different levels of speech and language processing:\nSpeaker Description (acoustic) evaluates the accuracy of identifying the gender (G), pitch (P), and tempo (T) of the target speaker.\nSpeech Transcription (phonetic & syntactic) measures transcription quality using word error rate (WER) and BLEU (Papineni et al., 2002) against the target speaker's actual speech.\nSpeech Summarization (semantic) assesses summary quality with ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), using three GPT-40 mini (OpenAI, 2024) reference summaries. The highest-scoring reference is reported.\nFree Q&A (semantic & pragmatic) evaluates responses to questions about the target speech, such as sentiment analysis, fact-checking, and named entity recognition. Three questions and reference answers were generated by GPT-40 mini, with performance measured by ROUGE-L and METEOR.\nPlease refer to Appendix C for specific questions and metric computation details. While AAD-LLM was trained exclusively on these five tasks, the ac-"}, {"title": "4.3 Results", "content": "We present the performance of AAD-LLM and other models on the iEEG clinical dataset. Results on the speech-only dataset are provided in Appendix D.3. By default, AAD-LLM, except for the speaker predictor, was trained on the speech-only dataset, which differs in both speakers and content from the iEEG dataset. Additionally, we report results for AAD-LLM trained with an extra 15 minutes of in-domain clinical data (\u201cclinical-15m\u201d). In all tables and figures, \u201cbrain-decoded attention\u201d represents the realistic BCI use case, where the speaker predictor infers the attended speaker label from neural signals. \"Oracle attention\" serves as an upper bound, using the ground-truth speaker label from the dataset."}, {"title": "4.3.1 Objective Evaluation", "content": "Objective metrics are reported in Table 1 for AAD&TSE tasks and Table 2 for all other tasks. A baseline for all tasks is evaluating the speech mixture or a randomly selected speech (first six rows and the lower bound in Table 2). These models, which lack attentional state, include blind speech separation and existing auditory LLMs. Their performance is close to random guessing for both foreground and background speech understanding.\nAdditionally, we compared AAD-LLM with standard auditory attention decoding (AAD) methods designed to separate the attended talker. We reproduced conventional AAD approaches that reconstruct the Mel spectrogram or speech envelope from brain activity to identify the attended speaker by similarity (O'Sullivan et al., 2017; Han et al., 2019; Geirnaert et al., 2021). We also implemented an ad-hoc target speech extractor similar to (Ceolini et al., 2020; Pan et al., 2024, 2023), optimizing SNR with the same speaker decoding method. While our model slightly underperforms the target speech extractor, it surpasses standard AAD methods and outperforms the extractor when trained with an additional 15 minutes of clinical data.\nFor other speech tasks, AAD-LLM outperforms all intention-uninformed auditory LLMs and a cascaded speech extractor and Qwen2-Audio (fine-tuned on the same data) on most metrics, particularly in transcription and summarization of the attended speaker. Notably, AAD-LLM with decoded"}, {"title": "4.3.2 Psychophysics Evaluation", "content": "Subjective ratings were collected from 40 participants in psychophysics experiments replicating the auditory scenes with the same attended speakers as done in the clinical setting. Participants rated responses from five models for Summarization and Free Q&A tasks, presented in random order, using a 5-point Likert scale. The evaluated models included AAD-LLM with brain-decoded and oracle attention, and Qwen2-Audio finetuned on single sources assessed with either a random speaker (lower bound) or the oracle speaker (upper bound). Average ratings are shown in Figure 3 and Table 4. Kruskal-Wallis H tests revealed significant differences in ratings between the groups for both Summarization and Free Q&A tasks (p-values < 0.001). These tests were then followed up with post-hoc pairwise Bonferroni-corrected Mann-Whitney U tests. The results of these tests show that AAD-LLM's responses were rated significantly higher than those from the mixture and random speaker baselines across both the tasks. While mean performance increased from brain-decoded to oracle attention with ground-truth speaker labels, this improvement was not statistically significant, indicating that the neural decoding is close to its best capacity. Furthermore, both AAD-LLM models (brain-decoded and oracle attention) approach the \"oracle speaker\" upper bound, suggesting that AAD-LLM's responses closely mirror human perception. Please see Appendix F for more details on the evaluation methods, Tables 11 and 12 for p-values."}, {"title": "4.3.3 Attention Validation", "content": "To ensure AAD-LLM's performance improvement stems from its attentional state rather than generating balanced responses for both speakers, we conducted additional analyses. Specifically, we measured the percentage of responses closer to the target speaker than the other for both foreground and background speakers across all tasks. As shown in Table 3, over 80% to 90% of responses aligned more with the target speaker, confirming AAD-LLM's effectiveness in speaker selection.\nAdditionally, we observed that AAD-LLM achieved similar performance in Free Q&A tasks when using brain-decoded attention or oracle attention (Figure 3 and Table 2). Also, the Free Q&A accuracy (96.9%, Table 3) surpassed the extraction accuracy (94.4%, Table 1). These results suggest that AAD-LLM might infer the target speaker based on question content rather than attentional state, especially when speakers discuss different topics. To address this, we designed a more challenging evaluation by replacing the background speech with another speech on a similar topic that could yield a different answer. In these conditions, AAD-LLM still achieved a ROUGE-L score of 62.0 and a METEOR score of 64.2 (Table 7), only slightly lower than the original scores (63.1 and 64.6), demonstrating that AAD-LLM effectively relies on attentional state to filter out the distracting speaker. More details about attention validation are included in Appendix D.2, with other results in Appendix D and analyses in Appendix E."}, {"title": "5 Conclusion", "content": "This work introduces intention-informed auditory scene understanding (II-ASU) as a new paradigm for aligning machine listening with human perception. We present AAD-LLM, a prototype system that integrates brain signals into an auditory large language model (LLM) to decode listener attention and generate responses that align with human perception. Experimental results demonstrate that incorporating attentional state improves model performance across multiple auditory tasks, including speaker description, speech transcription, and freeform question answering. Beyond improving speech-processing capabilities, this work represents an early step toward listener-centered auditory AI, where models do not merely process sound passively but interpret auditory scenes based on what the listener perceives, which has implications for assistive hearing technologies, adaptive voice assistants, and human-computer interaction. AAD-LLM lays the groundwork for future systems that process sound in alignment with human cognitive and perceptual priorities."}, {"title": "Limitations", "content": "Several limitations and challenges remain. While attention is a fundamental aspect of auditory intent, future work should explore broader cognitive signals, including task goals, semantic relevance, and perceived emotional significance of the scene. AAD-LLM also relies on intracranial EEG, which limits its current practical use, although invasive neural recordings are increasingly used more in various speech brain computer interfaces (BCI) (Akbari et al., 2019; Moses et al., 2021; Metzger et al., 2023; Willett et al., 2023). While non-invasive neural recording methods such as EEG or fNIRS is desired, they presents challenges in signal quality which limits their applicability. Finally, our experiments focus on controlled two-speaker scenarios, whereas real-world auditory scenes are more complex, involving multiple speakers and environmental noise. Expanding to these settings requires further neural data collection and improved adaptation techniques."}, {"title": "Ethical Statement", "content": "The development of AAD-LLM introduces exciting new possibilities for auditory scene understanding by integrating brain signals to align machine listening with human perception. This innovation has the potential to enhance communication for individuals with hearing impairments, improve virtual assistants, and advance human-computer interaction. While the model only decodes the attended speaker without accessing sensitive cognitive information, we need to remain vigilant about privacy by implementing robust safeguards and ensuring responsible data handling practices.\nWithin this study, approval of all ethical and experimental procedures and protocols was granted by the university's Institutional Review Board (IRB). The iEEG participants provided informed consent as per the local IRB regulations (IRB protocol number AAAD5482). The human raters evaluating model outputs also provided informed consent (IRB protocol number AAAR8655)."}, {"title": "A Dataset Details", "content": "We curated two datasets for this work. The first is a clinical dataset collected in a hospital setting from epilepsy patients with implanted intracranial electrodes. This dataset includes both neural and speech signals and was used to train the speaker predictor and evaluate AAD-LLM. The second is a synthetic speech-only dataset generated using publicly available speech and noise corpora, which was primarily used to train the AAD-LLM."}, {"title": "A.1 Clinical Dataset", "content": "This study involved six human participants, recruited from three medical centers: two from North Shore University Hospital (NSUH), two from Columbia University Irving Medical Center (CUIMC), and two from NYU Langone Health. All participants were undergoing clinical treatment for epilepsy and were implanted with intracranial electrodes for monitoring.\nEach participant had electrode implants tailored to their clinical needs. Some participants were implanted with both subdural electrocorticography (ECOG) grids and stereo-electroencephalography (sEEG) depth electrodes, while others had only sEEG depth electrodes. Electrode coverage across subjects in the clinical dataset can be seen in Figure 4, with each subject represented by a different color.\nNeural recordings were bandpass-filtered to extract low-frequency components in the 0.5\u201330 Hz range. Electrodes that were visually identified as disconnected from anatomical tissues were excluded from the analysis. To maximize brain coverage, electrode recordings from each participant were concatenated.\nAll participants listened to 28 trials, with the average duration of 44.2 s (standard deviation = 2.0 s) each. The trials consisted of two concurrent and independent conversations that were equally loud and spatially separated. Diotic background noise (either \"pedestrian\" or \"speech babble\") was also mixed along with the conversations at power either 9 or 12 dB below the power of a conversation stream. The subjects were instructed to follow (attend to) the conversation that started first. The to-be-unattended conversation started 3 seconds later. The trials were spatialized using head-related transfer functions (HRTFs) and delivered to the subjects via earphones.\nThe talkers in both the conversations intention-"}, {"title": "A.2 Speech-Only Dataset", "content": "The speech-only dataset was constructed using speech utterances from the train-clean-100 and train-clean-360 subsets of LibriTTS\u00b2 (Zen et al., 2019) along with noise samples from the DEMAND\u00b3 (Thiemann et al., 2013). We filtered out utterances shorter than 0.5 seconds or longer than 15 seconds and randomly combined two speech utterances of similar duration but different speakers with one of 18 environmental noise types, such as park, office, and metro station. The speech sources were normalized to have equal energy and then mixed with background noise at SNR levels of 9 or 12 dB. This process aimed to replicate the auditory conditions of the clinical dataset while introducing a more diverse set of speakers and noise types to enhance model generalizability.\nIn total, we generated 57,963 speeches-and-noise mixtures of 1146 speakers, which were randomly split into 53,963 (85.3 hours) for training, 1,000 (1.6 hours) for validation, and 3,000 (4.8 hours) for testing. The speech-only dataset has a distinct set of speakers and sentences from the clinical dataset. The validation and testing set were primarily used for model development and in-domain evaluation.\nThe gender of each speaker and the transcription of the speech were obtained from the LibriTTS corpus. Additionally, we retrieved the \u201cpitch\u201d and \"tempo\" labels for the LibriTTS utterances from TextrolSpeech (Ji et al., 2024). Both pitch and tempo were quantized into three levels. For pitch, utterances with a fundamental frequency below 136.6 Hz were labeled as \"low\", above 196.1 Hz as \"high\", and those in between as \u201cnormal\u201d. For tempo, utterances with an average speaking rate slower than 0.39 seconds per word were labeled as \"low\", faster than 0.25 seconds per word as \u201chigh\u201d, and those in between as \u201cnormal\u201d. We applied these same pitch and tempo thresholds to annotate speech utterances in the clinical dataset, serving as"}, {"title": "B Model and Training Details", "content": "B.1 Auditory Large Language Model\nAAD-LLM adopts the backbone of Qwen2-Audio (Chu et al., 2024). The pretrained checkpoint Qwen2-Audio-7B-Instruct is publicly available\u2075. We further finetuned both the LLM and the speech encoder on the speech-only dataset using low-rank approximation, LoRA (Hu et al., 2022), on the key, query, and value matrices of attention layers and the weight matrices of multilayer perceptrons. We used a rank of 512, an \u03b1 of 512, and a dropout of 0.05 by default, which adds around 16.5% of trainable parameters.\nWe added a special token <ATT> as the placeholder for the listener's attention (speaker vector). In reality, <ATT> is <|extra_124|> from Qwen2-Audio's reserved special token sets. Then, the entire input to the LLM looks like the following:\nsystem: You are a helpful\nassistant.\nuser: Attention: <ATT>\nAudio 1: <speech1>\nAudio 2: <speech2>\nQuestion: <question>\nSolution:\nAttention:<att_spk_label>;\nSpk1:<spk1_label>;\nSpk2:<spk2_label>;\n<solution>\n<att_spk_label>,  , and  are ground-truth labels of the attended speaker, the first input speaker, and the second input speaker. All are integers from 0 to K-1, with K=8 by default. (<att_spk_label> \u2260 <ATT>. The former is an integer; The latter is the projected speaker vector.) The tokens of pink and red parts corresponding to the chain-of-thought prefix and the actual solution were optimized. The maximum number of allowed tokens is 1024.\nSpeaker Projector is a linear layer from 512 to 4096, from the dimension of x-vector to the dimension of the LLM. It was optimized jointly with the audio encoder and the LLM. The projected speaker vector replaces the embedding at the <ATT> token."}, {"title": "B.2 Speaker Clusters", "content": "K=8 speaker clusters were obtained by K-Means. We randomly sampled 10,000 utterances of 1137 speakers from LibriTTS and extracted their x-vectors using a pretrained speaker verification model. The x-vector extractor has an output dimension of 512 and was kept frozen. Please see the right subfigure of Figure 5 for the distribution of 10,000 x-vectors colored by the nearest cluster."}, {"title": "B.3 Speaker Predictor", "content": "The speaker predictor classifies the neural signal Z into one of K = 8 discrete speaker labels. The predictor consists of a bidirectional recurrent neural network (RNN) followed by a temporal pooling layer and a fully connected classification head.\nBefore passing the neural signals into the recurrent layer, layer normalization is applied. The recurrent module is a bidirectional long short-term memory (LSTM) network with a hidden state size of S = 64, resulting in an output dimension of 2S = 128 due to bidirectionality. The LSTM output is processed by a mean pooling layer, followed by a fully connected layer with 128 hidden units and a final softmax activation for classification.\nWe trained the predictor on 280 samples with aligned neural and speech signals in the clinical training set. We obtained the ground-truth speaker label from the speech signal. The predictor was then optimized by cross-entropy loss, with the predicted speaker labels against ground-truth labels.\nWe optimized the predictor with an Adam optimizer of a constant learning rate of 1 \u00d7 10\u22124, with a batch size of 1, for a total of 30 epochs, which took fewer than 10 minutes on an GPU."}, {"title": "B.4 Speech Separator", "content": "The speech separator reproduces Mamba-TasNet (M) (Jiang et al., 2024c,d) which has a linear waveform encoder & decoder and 32 Mamba (Gu and Dao, 2024) layers with a dimension of 256. The model is tiny (15.6M parameters) and intention-uninformed. It was trained separately on the speech-only dataset. The model was trained using an Adam optimizer (Kingma and Ba, 2015) with a learning rate of 2 \u00d7 10\u22124. A cosine learning rate schedule with 20,000 warmup steps and a ReduceLROnPlateau scheduler was applied, halving the learning rate if performance plateaus after"}, {"title": "C Tasks, Prompts, and Metrics", "content": "C.1 Questions and Solutions\nWe wrote eight different questions for each task except Free Q&A, for which GPT-40 mini generates three questions and reference solutions uniquely for each utterance.\nHere are three example questions for foreground speaker description:\nQ1: \"Describe the attended speaker.\"\nQ2: \"Please write a description of\nthe attended speaker.\"\nQ3: \"Can you identify the person the\nsubject is listening to?\"\nThe solution to the description question is formatted as:\nA: \"A  speaker\nwith  pitch\nand  tempo.\"\nThe pitch and tempo labels are \u201clow\u201d, \u201cnormal\u201d, or \"high\" (Appendix A.2).\nHere are three example questions for background speech summarization:\nQ1: \"What is the background speaker\ntalking about?\",\nQ2: \"Can you summarize the speech\nof the speaker being ignored?\",\nQ3: \"What topic is the background\nspeaker discussing?\",\nWe gave GPT-40 mini the transcription of the speech to generate three candidate summaries as the solutions.\nIn each training epoch, a random question from a random speaker was sampled for every utterance."}, {"title": "C.2 Free Q&A Generation", "content": "Three pairs of freeform questions and answers were generated for either speaker of each utterance with the following prompt to GPT-40 mini:\n\"You are listening to"}]}