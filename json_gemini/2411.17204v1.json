{"title": "Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks", "authors": ["Ratnesh Kumar Joshi", "Priyanshu Priya", "Vishesh Desai", "Saurav Dudhate", "Asif Ekbal", "Roshni Ramnani", "Anutosh Maitra"], "abstract": "Given the advancements in conversational artificial intelligence, the evaluation and assessment of Large Language Models (LLMs) play a crucial role in ensuring optimal performance across various conversational tasks. In this paper, we present a comprehensive study that thoroughly evaluates the capabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon, Alpaca, and MPT. The study encompasses various conversational tasks, including reservation, empathetic response generation, mental health and legal counseling, persuasion, and negotiation. To conduct the evaluation, an extensive test setup is employed, utilizing multiple evaluation criteria that span from automatic to human evaluation. This includes using generic and task-specific metrics to gauge the LMs' performance accurately. From our evaluation, no single model emerges as universally optimal for all tasks. Instead, their performance varies significantly depending on the specific requirements of each task. While some models excel in certain tasks, they may demonstrate comparatively poorer performance in others. These findings emphasize the importance of considering task-specific requirements and characteristics when selecting the most suitable LM for conversational applications.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (a.k.a generative AI) are language models with parameter sizes over a hundred billion. Examples of LLMs include Megatron Shoeybi et al. (2019), GPT-3 Brown et al. (2020), Gopher Rae et al. (2021), GPT-Jurassic Lieber et al. (2021), OPT-175B Zhang et al. (2022), Falcon ZXhang et al. (2023), Llama Touvron et al. (2023), InstructGPT Ouyang et al. (2022), ChatGPT\u00b9, etc. In recent times, there has been a significant surge in the prominence of LLMs. With the release of ChatGPT, they have gained widespread recognition and use across various domains, including research and industry; perhaps most notably, they have become more accessible and well-known to the general public. These unsupervised autoregressive models can forecast the next tokens, whether characters, words, or strings, by leveraging contextual information from previous data. To enable the LLMs to demonstrate their abilities, sophisticated prompt engineering NeuralMagic (2023) is required. Prompts are used to probe the LLMs to generate the target outcome by sampling the language distribution.\nThe rapid advancements and impressive achievements of LLMs have triggered a revolutionary shift in Natural Language Processing (NLP). These models have showcased substantial improvements across a variety of traditional tasks in NLP, including natural language understanding tasks like emotion recognition Zhang et al. (2023b); Zhao et al. (2023a), hate speech detection Huang et al. (2023); Oliveira et al. (2023), to name a few, to generative tasks such as summarization Liang et al. (2022); Pu & Demberg (2023), dialogue Lin & Chen (2023), code generation Liu et al. (2023), and more.\nLarge language models have many applications, including language generation for articles, essays, and creative writing Swanson et al. (2021), as well as"}, {"title": "2. Related Work", "content": "In recent years, Large Language Models (LLM) have emerged as a focal point of extensive research and development within natural language processing (NLP) Zhao et al. (2023b); Chang et al. (2023). Google's T5 Raffel et al. (2020), introduced in 2019, marked a pivotal moment with its text-to-text approach, departing from the reliance on task-specific architectures. This innovative approach offered unparalleled flexibility and adaptability across various NLP tasks. Shortly thereafter, OpenAI introduced GPT-3 Brown et al. (2020), which not only showcased remarkable scale and performance but also set new benchmarks, propelling the field into uncharted territory and sparking tremendous interest and investment in LLM research.\nSubsequent years have witnessed a remarkable boom in the development and release of numerous other LLMs, each contributing to the diversification and enhancement of NLP capabilities. Among these models, ChatGPT2, a conversational model, has gained prominence for its ability to engage in human-like interactions and generate coherent dialogues. Additionally, Facebook's text model Llama Touvron et al. (2023) has made significant contributions to the NLP ecosystem, leading to comprehensive models such as Falcon ZXhang et al. (2023) and Alpaca Taori et al. (2023), offering robust language understanding capabilities and addressing complex language comprehension tasks. These models can be used in a wide variety of tasks such as machine translation Gao et al. (2024), code simplification Patsakis et al. (2024), medical diagnosis Caruccio et al. (2024), and social media analysis Najafi & Varol (2024). These models continue to shape the NLP landscape, pushing the boundaries of research and application possibilities in natural language processing and opening new horizons for innovation and discovery.\nThe training of large-scale language models Li et al. (2023); Nijkamp et al. (2023) is a multifaceted process that unfolds in two distinct phases: pretraining"}, {"title": "3. Methodology and Experimental Setup", "content": ""}, {"title": "3.1. Tasks/Datasets", "content": "In this section, we provide an overview of the datasets used in our evaluation of various Large Language Models (LLMs) through zero-shot, one-shot, and two-shot learning prompts. These datasets have been carefully selected to assess the capabilities of LLMs in different linguistic and conversational domains. Each dataset is a unique benchmark for evaluating the model's performance in a specific context. The 5 following datasets were chosen for our evaluation work:\nMulti-Domain Wizard-of-Oz or MultiWOZ Zang et al. (2020) is a multi-domain dialogue dataset. In total, the dataset consists of 7 domains - Attraction, Hospital, Police, Hotel, Restaurant, Taxi, and Train. The latter four are extended domains, which include the sub-task Booking. The dataset has 10438 dialogues, with 8438 dialogues for training and 1000 dialogues each for evaluation and testing.\nThe Craigslist Bargain He et al. (2018) dataset comprises textual content extracted from Craigslist listings, a platform where individuals advertise items for sale or trade. The dataset consists of 6682 conversations between people"}, {"title": "3.2. Prompting", "content": "We have created prompts that we provide to the LLMs to get the responses. The structure of the prompt is as follows:\nSystem Prompt: Here, the system prompt is the generalized part of the prompt, and this has been used in all the prompts for all the tasks. It guides the Language Model to generate the responses in a Conversational Setting. The prompt structure is as follows, and the missing details are filled with instructions specific to the task.\n\"[SYSTEM] Your task is to generate coherent and contextually relevant responses based on the given input. Your responses should aim to The goal is to... Please ensure the response is grounded to the last user input and context.\"\nFew-Shot: We have added few-shot examples for each context response pair in the test set. For each test sample in the test set, we sample 4 context-response pairs. All context-response pairs correspond to a similar type of Generated Response. For each test sample, 4 prompts have been created as follows: Zero-shot = only context used to generate the response, One-shot = 1 example along with the context used to generate the response, Two-shot = 2 examples along with the context used to generate the response.\nContext: The context is a sequence of user and system utterances corresponding to the dialogue. The last utterance in this sequence is the user utterance, and the Language Model predicts the next System utterance considering the few-shot examples and the context provided regarding dialogue history.\nGold Response: For each test sample, the next System utterance after the last utterance of the context, according to the dialogue in the data set, has been taken as the Gold Response. We have used this Gold Response to evaluate some of the generic automatic metrics (token-based similarity like BLEU, METEOR, and embedding-based similarity like BERT Score, Greedy Matching, etc.; more details are mentioned in Evaluation Setup).\nPrompt specific to an Evaluation Metric: In this paper, apart from the general format of prompts mentioned above, a slightly different format of prompt has also been used to evaluate Context Consistency for MultiWOZ dialogue dataset.\nFor each test sample, the entire Context remains the same, but the last User"}, {"title": "3.3. Large Language Models", "content": "In this research study, we delve into the evaluation of five state-of-the-art Large Language Models (LLMs) within the context of generating text in response to zero-shot, one-shot, and two-shot prompts. The evaluated LLMs are Falcon, OPT, MPT, Llama, and Alpaca. We aim to assess their capabilities in understanding, generalizing, and responding to various prompts in diverse datasets, shedding light on their performance across various tasks and domains.\nFalcon ZXhang et al. (2023) Falcon LLM has been trained on 1 trillion tokens and is available in various versions having 180B, 40B, 7.5B, and 1.3B parameters, respectively. We utilize the Falcon 7B Instruct version\u00b3 for our experiments.\nThe Open Pre-trained Transformer Language Models Zhang et al. (2022) or OPT model was proposed in Open Pre-trained Transformer Language Models by Meta AI. OPT is a series of open-sourced large causal language models that perform similarly to GPT-3. Currently, six models are available on Hugging Face: 125M, 350M, 1.3B, 2.7B, 6.7B, and 30B parameters. OPT was trained on publicly available data sets to allow for more community engagement in understanding this foundational new technology. We utilize the OPT 6.7B Instruct version for our experiments.\nThe MosaicML Pretrained Transformer Team (2023) or MPT models are GPT-style decoder-only transformers that come with many improvements, such as performance-optimized layer implementations, greater training stability due to architecture changes and no context length limitations. MPT-7B is a transformer model trained from scratch using 1T tokens of text and code. It was"}, {"title": "4. Evaluation Setup", "content": "For the comprehensive evaluation of LLMs, we have used a set of Generic Metrics, which evaluates the general aspects of any conversational agent (like fluency context relevance). We have also used Task-specific Metrics, which evaluate the ability of LLMs to perform a particular task (like negotiation, empathy, etc.).\nIn this paper, we present the two approaches for evaluation: 1. Automatic Evaluation and 2. Human Evaluation."}, {"title": "4.1. Automatic Evaluation", "content": "The metrics presented in this paper to perform automatic evaluation are divided into two segments: 1. Generic Metrics 2. Task-specific Metrics"}, {"title": "4.1.1. Generic Metrics", "content": "The generic metrics for automatic evaluation presented in this paper can be further divided into following buckets:\n1. Lexical-based similarity between generated responses and gold responses: BLEU Papineni et al. (2002) (BLEU1, BLEU2, BLEU3, BLEU4), METEOR Banerjee & Lavie (2005)\n2. Embedding-based similarity between generated responses and gold responses: Greedy Matching Rus & Lintean (2012), Embedding Average Landauer & Dumais (1997), Vector Extrema Forgues et al. (2014), BertScore Zhang et al. (2019)"}, {"title": "4.1.2. Task-specific Metrics", "content": "1. MultiWOZ Dialogue Dataset: This dataset evaluates the ability of a LLMs to capture the requested information mentioned in the user query and provide relevant and useful information in the generated response. In Context Consistency, we compute the similarity among the three alternate queries as follows: $(similarity(q_1,q_2) + similarity(q_2, q_3) + similarity(q_3,q_1))/3$, where $q_1,q_2$, and $q_3$ are the generated responses for the queries, and $similarity (q_i,q_j)$ computes the cosine similarity between the Bert Devlin et al. (2018) embeddings for the responses $q_i$ and $q_j$. INFORM evaluates whether the slot values informed by the user appear in the system-generated response or not. More weights are given to newly added slot values compared to existing slot values. SUCCESS evaluates whether the slot values requested by the user appear in the system-generated response. This gives us the SUCCESS metric score; the higher the score, the higher the deviation, and worse the performance."}, {"title": "4.2. Human Evaluation", "content": "To assess the responses generated by the LLMs regarding response quality and task success, we also conduct a human evaluation of the generated responses"}, {"title": "4.2.1. Generic Metrics", "content": "Given below are the evaluation metrics to evaluate the general aspects of a conversational agent:\n1. Fluency: It refers to the correctness of the generated response concerning grammar and word choice, including spelling. The score in the range of 1 to 5 has been given to each test sample (generated response). The scoring is 1 (Poor), 2 (Fair), 3 (Average), 4 (Good), and 5 (Excellent).\n2. Context Relevance: The response should correctly address the query and be consistent with the contents of the context provided. If the response follows the flow of the given context, the response should be logically sound for the user query. Here also, we have given scores in the range of 1 to 5. The scoring criteria are: 1 - response completely inconsistent with the context and provide no useful information, 2 response provides some random information that is not at all asked for in the context, 3 - response provides only some of the information asked for in the context with some extra irrelevant information, 4 - response provides only some of the information asked for in the context without adding any extra irrelevant information, 5 response provides all the relevant information asked for in the context without adding any extra irrelevant information.\n3. Non-Repetitiveness: to check the non-repetitiveness of the generated utterance. The metrics score higher in case the response does not include repeated words."}, {"title": "4.2.2. Task-specific Metrics", "content": "Here, for each dataset, we present dedicated human evaluation metrics to evaluate the Language Model for the tasks associated with these datasets.\n1. MHLCD. For human evaluation, we employ three task-specific metrics to evaluate the performance of the LLMs, namely Counseling strategy correctness (Con), Politeness (Pol), and Empathy (Emp). The human evaluators rate the"}, {"title": "4.2.3. Metrics Details", "content": "1. MultiWOZ Dialogue Dataset: This dataset evaluates the ability of a Language Model to capture the requested information mentioned in the user query and provide relevant and useful information in the generated response. Given below are the metrics used for the evaluation of the MultiWOZ dataset:\na. Context Consistency: User queries in goal-oriented dialogues are less predictable. Different users can request the same information (mentioned in the context) and can ask to perform the same task but can ask in different ways. To simulate this human behavior, for each test sample, we create three alternate user queries providing the same context. Example:\nContext: USER: I am looking for a train to Boston from New York that leaves at 9 in the morning.\nSYSTEM: Two trains are leaving from New York for Boston, suitable for your timings, first at 9:15 a.m. and second at 9:40 a.m. Which train would you like to book ?\nUSER1: Book one ticket for the 9:15 train\nUSER2: Finalize my reservation for the one departing at 9:15\nUSER3: Yes, the 9:15 one\nFor each test sample, we compute the similarity among the three alternate queries as follows: $(similarity(q_1,q_2)+similarity(q_2, q_3)+similarity(q_3, q_1))/3$.\nHere, q1, q2, and q3 are the generated responses for the queries, and similarity(qi, qj) computes the cosine similarity between the Bert embeddings for the responses qi and qj. We compute the average of these similarity scores over all the test samples, and this is the Context Consistency score. The closer this score to 1, the higher the similarity among the alternate query responses and the more consistent the Language Model performance concerning the context.\nb. INFORM: In the MultiWOZ dataset, for each dialogue, there are annotations for the informed slots. These informed slots contain the information the"}, {"title": "5. Results and Analysis", "content": ""}, {"title": "5.1. Results", "content": "For our evaluation, we give much higher importance to the task-specific metrics and the human evaluation metrics over the Generic Automatic evaluation metrics 5.2. This is primarily due to our focus being on the task performance of the models and also the lack of certainty of the gold response as the only valid response. There could be multiple valid responses for a context, so we give importance to the task-based and human evaluation metrics during evaluation.\nThe task-specific metrics and the human evaluation metrics results are shown"}, {"title": "5.2. Results: Generic Automatic Evaluation", "content": "The results for generic automatic evaluation of the LLMs are shown in Tablel, Table2, Table3, Table4 and Table5"}, {"title": "5.3. Results for task oriented evaluation and analysis", "content": "MHLCD 61: LLaMA outperforms the others with the best performance, while Falcon performs the worst. LLaMA excels in two-shot settings, while OPT achieves the highest scores in one-shot settings. Its performance decreases in two-shot settings. Falcon, on the other hand, performs slightly better in one-shot settings compared to zero-shot and two-shot settings for all automatic evaluation metrics. OPT achieves the best scores in a one-shot setting. In two-shot settings, OPT's performance degrades.\nCraigslist Bargain73: For this dataset, the evaluation primarily focused on negotiation capability and some degree of logical understanding of the product when suggesting the price. The Falcon model performed the best in this case, with the Falcon one-shot model outperforming other variants in all automatic metrics except for NegStr, where Llama performs better. For our combined"}, {"title": "5.4. Good Examples", "content": ""}, {"title": "5.4.1. Empathetic Persuasions", "content": "\u2022 Alpaca Zero-shot :\nContext:\nuser: So the charity helped the Syrian children?\nbot: Oh yes and others all over the globe. Would you donate to this charity today?\nuser: Yeah, maybe so. Can i donate part of the $0.30 from this HIT or do I have to go to their website?"}, {"title": "5.5. Bad Examples", "content": ""}, {"title": "5.5.1. Empathetic Persuasions", "content": "\u2022 Falcon Zero-shot :\nContext:\nbot: There is a very important charity called Save the Children. Do you know much about it?\nuser: I do not, can you tell me some?\nbot: They help children all over who are in need. They have programs to help with help, literacy, and disaster relief.\nuser: I would like to donate\nGold response: How much of the money you earn from this task would you be willing to donate?\nGenerated response : Thank you for your donation.\n\u2022 OPT One-shot :\nContext:\nbot: It's called Save the Children. Have you heard of it?\nuser: I think so. Does it have commercials on tv?\nbot: Yes, they do. I wish they didn't have to spend their monies on commercials but it is the only way to get their name out there.\nuser: Does this charity help children in the United States, or do they deal mostly with children in African nations?\nbot: It is a global charity. They help out children in the US and around the world. You should check out their website: URL s/what-we-do/us-programs\nbot: There is a very important charity called Save the Children. Do you know much about it?\nuser: I do not, can you tell me some?\nbot: They help children all over who are in need. They have programs to help with help, literacy, and disaster relief.\nuser: I would like to donate\nGold response: How much of the money you earn from this task would you be willing to donate?\nGenerated response : Hey you wanna donate some money today?"}, {"title": "6. Conclusion", "content": "In this work, we evaluated 5 prevalent LLMs Llama Touvron et al. (2023), OPT Zhang et al. (2022), Falcon ZXhang et al. (2023), Alpaca Taori et al. (2023) and MPT Team (2023) across a diverse range of tasks, namely reservation, empathetic response generation, mental health and legal counseling, persuasion, and negotiation. We selected datasets corresponding to the tasks in focus to evaluate the capabilities of LLMs in a conversational setting. Through a rigorous evaluation process, we conclude that no LLM is optimal for all the tasks, and the choice of LLM depends on the task. Some models were better for one task but performed relatively poorly in other specific tasks.\nIn conclusion, our study provides a comprehensive evaluation of five prevalent LLMs across a diverse range of conversational tasks. We found that no LLM is overall optimal for all tasks, highlighting the importance of considering task-specific requirements when selecting an LLM. Additionally, we identified drawbacks in the performance of LLMs, including the inconsistency across tasks and the reliance on the validity of gold responses. To ensure optimal functional performance, it is crucial to evaluate LMs based on the specific capabilities they demonstrate for each distinct task. By understanding their strengths and limitations within different conversational domains, researchers and practitioners can make informed decisions when choosing the most appropriate LM for specific applications."}]}