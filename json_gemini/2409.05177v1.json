{"title": "Insights from Benchmarking Frontier Language Models on Web App Code Generation", "authors": ["Yi Cui"], "abstract": "This paper presents insights from evaluating 16 frontier large language models (LLMs) on the WebApp1K benchmark, a test suite designed to assess the ability of LLMs to generate web application code. The results reveal that while all models possess similar underlying knowledge, their performance is differentiated by the frequency of mistakes they make. By analyzing lines of code (LOC) and failure distributions, we find that writing correct code is more complex than generating incorrect code. Furthermore, prompt engineering shows limited efficacy in reducing errors beyond specific cases. These findings suggest that further advancements in coding LLM should emphasize on model reliability and mistake minimization.", "sections": [{"title": "Introduction", "content": "In [Cui, 2024], we introduced WebApp1K, a benchmark to evaluate web app code generation performance of frontier LLMs. The performance results of these 16 models are summarized in Tab. 1.\nIn this report, we share insights gained from the code written by these 16 models To prevent benchmark contamination, we do not reveal the actual code, but their outcome aggregated by certain measures. The artifacts are on GitHub and Huggingface: the dataset containing all 1000 problems of WebApp1K[Lab, 2024a], the script[Lab, 2024c] to run WebApp1K, and the leaderboard[Lab, 2024b]."}, {"title": "Benchmark Difficulty", "content": "We begin by examining the difficulty of the benchmark. We made each model solve each coding problem for 10 times, which gives us 160 solutions per problem. If a solution passes the tests, it is considered a succes, otherwise a failure. Fig. 1 shows number of failures per problem. The more failures a problem collects, the more difficult it is.\nAs the figure shows, the majority of the problems are situated on the left side of the graph, characterized by low failure rates, indicating that these problems were relatively easy, especially for top-ranked models. Conversely, a small cluster of problems on the far right exhibit extremely high failure rates. The most difficult 38 problems have never been solved by any model. This pattern still holds after we break down Fig. 1 by applications. More details are available in Appendix A."}, {"title": "Failure Distributions", "content": "2.2 Does the Code Build?\nOf the total 160,000 solutions produced (10 runs by each of the 16 models for 1000 problems), only 172 have syntax errors, i.e. the build failure rate is 0.1%. In particular, the solutions by Claude 3.5 and Mistral Large 2 have no syntax errors.\nThis means all models are able to write high-quality and compilable code. Yet some of them did not manage to meet test expectations, some explicit and others implicit, therefore causing the failures."}, {"title": "Line-of-Code (LOC) Analysis", "content": "Thanks to the modularized design of the React framework, the solutions output by all models universally follow the template outlined in Tab. 2, with no need for any explicit prompting. As such, we seek to another proxy signal, LOC (line-of-code), to gain insights."}, {"title": "LOC Distribution by Models", "content": "In Tab. 3, we rank models by their median LOC alongside their respective pass@1 scores. Picking one pass@k is sufficient because all scores produced basically the same model rankings as shown in Tab. 1.\nWe observe that the median LOCs across all models stay close, ranging from 35 to 46. We believe this narrow range is largely enforced by the conciseness and expressiveness of the React framework itself. Also there is no strong correlation between the conciseness (median LOC) and correctness (pass@1). For example, mixtral-8x7b-instruct, which has the shortest median LOC, ranks quite low on pass@1 (0.1269). Conversely, stronger models like claude-3.5-sonnet and gpt-40-2024-08-06, generate longer code. Other models, e.g. deepseek-coder-v2-instruct and gemini-1.5-pro, strike a balance between median.\nNext, we use violin charts to visualize LOC distribution of each model. The distributions are either bimodal or unimodal, and they are collected in Fig. 2 and Fig. 3 respectively.\nNotably, all high-performing models with high pass@1 scores are located in Fig. 2. These models, such as the gpt-40 variants and deepseek-coder series, demonstrate higher variability in their LOC distributions, i.e. bimodal. The two distinct peaks in these models' distributions suggests that they generate both shorter and longer code lengths, depending on the task. Importantly, the median LOC values for these bimodal models consistently fall between the two peaks, highlighting a balance in their code generation. Also the higher of the two peaks often corresponds to smaller LOC. This suggests that while these models can produce longer code when necessary, they tend to generate shorter, more optimized code in most cases.\nIn contrast, Fig. 3 contains smaller models. Some exhibit near-perfect normal distributions, e.g. mixtral-8x7b-instruct and llama-v3-8b-instruct. These models generate LOC distributions that are tightly centered around their medians, indicating more consistent and predictable behavior. The lack of bimodal characteristics in these distributions reflects a more stable output across tasks, but with lower complexity compared to the larger models in Fig. 2."}, {"title": "Impact of Success/Failure", "content": "To get more insights, we search for statistical distinction between successful model outputs and failed outputs. In Fig. 4 and 5, we visualize the LOC distribution separately for succssful outputs and failed ones, for each model. The graphs are ranked by pass@1, where higher pass@1 means bigger success sample set and smaller failure sample set. We normalize the width of each violin chart by its sample set size, hence resulting in the thinnest failure graph for the model with the highest pass@1. The graph gradually grows wider as the model performance degrades. The opposite pattern is observed for the success violin chart.\nAn important finding here is that the success distribution is always more complex than its failure counterpart, with more peaks and deviations. Fig. 5 groups lower performing models whose failure sample set dominates the success sample set. The failure LOC distributions are unimodal, in contrast with the multimodal distributions of top models in Fig. 4. This implies the inherent complexity involved in writing correct code even when the mean LOC is less than 50.\nThe success/failure LOC distributions of remaining 8 models are collected in Appendix C. Also Appendix D shows LOC distributions sharded by applications."}, {"title": "Error Analysis", "content": "When testing model-generated code, the failed solutions end up with error logs, one log file for each JS file. In this section, we study these logs and share our findings."}, {"title": "Error Types", "content": "There are seven types of errors, which we code them to A through G. They are summarized in Tab. 4.\nThe verbatim errors are the original error messages or codes captured by the log. Each of them is broadly scoped to contain a wide array of behaviors. However, in the context of our benchmark, we find all verbatim errors are projected to narrowband of behaviors attributed to the same root causes."}, {"title": "Singular and Twin Errors", "content": "An error log can contain a combination of many error types, indicating the code is poorly implemented. But this is not the dominant pattern. 93% of error logs contain either a singular error or twin errors. Fig. 7 shows the distribution of singular and twin errors.\nSingular error means the log contains only one error pointing to a single line. Twin errors are two errors of the same type, preeminently pointing to the same error line. Since the code needs to pass two unit tests, often times the same bug offends both tests. This means that even upon failures, all models produce quality code, but with only one bug."}, {"title": "Error Distribution by Models", "content": "In Fig. 8, we show the error distribution separately for each model. The most important finding here is that no model is immune to any of the seven error types, even when the raw error counts differ by one order of magnitude bewteen two extremes.\nThis means that all models possess the same knowledge and capabilities to write high-quality code which meets test expectations, and same inherent vulnerabilities resulting in the same types of errors. But top models distinguish themselves at lower error rates, i.e. ability to make fewer bugs.\nWe also study error distribution sharded by applications, whose results can be found in Appendix E."}, {"title": "Targeted Prompt Optimization", "content": "Prompt engineering is a common practice to improve model outputs by paying additional efforts to refine model inputs. After root causing each error type, we apply targeted prompt optimization to see if it can help weaker models to reduce bugs in their outputs.\nSpecifically, following the standard prompt below, we add a sentence reminding the model to avoid a specific type of errors.\nGenerate {file_name} to pass the tests below:\n{success_test_code}{failure_test_code}. RETURN CODE ONLY.\nType A is the only error type we manage to achieve significant improvement. To give more background, useHistory is a commonly used framework function deprecated in React v6, 2021. As such, type A error is triggered each time useHistory appears in the code.\nWe choose llama-v3p1-70b-instruct because it makes the most type A errors (Fig. 8) among all models. We use different prompts to emphasize to the model not to call useHistory. As shown in Tab. 5, the more specific the prompt for the model to follow, the better the error reduction."}, {"title": "Related Works", "content": "The development and evaluation of large language models (LLMs) for code generation have been an area of significant research interest in recent years."}, {"title": "Benchmarks", "content": "Benchmarks are essential for evaluating the effectiveness and generalizability of models across various software engineering tasks. CodeSearchNet[Husain et al., 2020] is a benchmark to evaluate semantic code search performance. HumanEvalPack[Muennighoff et al., 2024] assesses the ability of models like Codex to generate correct Python code from natural language prompts. Defects4J[Just et al., 2014] is widely used to assess LLMs' ability to handle bug localization and repair in Java. XL-CoST[Zhu et al., 2022] evaluates how well LLMs can work across different programming languages. BugsJS[Gyimesi et al., 2019] collects real-world JavaScript bugs and is used to evaluate LLM ability to detect and fix bugs in web applications. ClassEval[Du et al., 2023] evaluates class-level code generation."}, {"title": "Error Analysis", "content": "Error analysis is a critical area of research that focuses on understanding and improving the LLM weaknesses on software tasks. BugAID[Hanam et al., 2016] is a system to discover JavaScript bug patterns in JavaScript and web applications. DeepFix[Gupta et al., 2017] is a deep learning-based system repairing errors in C programs. TSSB-3M[Richter and Wehrheim, 2022] is a large dataset of single-statement bugs across multiple languages. ManySStuBs4J[Karampatsis and Sutton, 2020] is a dataset of bug-fix pairs commonly used to train models to detect subtle errors in Java."}, {"title": "Prompt Engineering", "content": "Quite a few studies focus on prompt engineering to improve LLM performance on coding tasks. In the era of GPT-3[Brown et al., 2020] and Codex[Chen et al., 2021], prompt engineering has been used for code translation tasks. Chain-of-Code[Li et al., 2024] expands on Chain-of-Thought by way of pseudocode. DotPrompts[Agrawal et al., 2023] leverages prompts for code summarization. APE (Automatic Prompt Engineer)[Zhou et al., 2023] automates prompt creation by exploring different configurations to LLM code repair performance."}, {"title": "Code Complexity", "content": "Code complexity research focus on understanding how models handles complex code and generates efficient code. CoCoNut[Lutellier et al., 2020] is a syntax-guided neural machine translation system for automatic program repair. AST-T5[Gong et al., 2024] incorporates Abstract Syntax Trees (AST) into T5 to understand the structure of complex code. InCoder[Fried et al., 2023] is a model designed for code generation and infilling tasks."}, {"title": "Error Reduction", "content": "Error reduction strategies focus on minimizing the number of mistakes made by models during code generation, completion, and repair tasks. CYCLE[Ding et al., 2024] is a self-refining model designed to reduce error rates by iterating outputs through code evaluation. CodeRL[Le et al., 2022] use reinforcement learning to reduces syntax errors via immediate feedback at training time. AlphaRepair[Xia and Zhang, 2022] incorporates static analysis feedback into zero-shot learning."}, {"title": "Conclusions and Future Works", "content": "In this report, we study WebApp1K results on 16 frontier LLMs, particularly failure rates, LOC distributions, and error types. Here are some tentative insights.\n1. A failed solution is often one bug away from a correct one. This suggests that all models possess the necessary knowledge and capabilities, but mistake minimization is the key differentiator between top and weak models.\n2. Success code outputs exhibit more complex patterns (LOC distribution) than failed code outputs, implifying more factors influencing the model output.\n3. Prompt optimization is only effective when errors can be described (and hence avoided) in an exact and specific way.\nWe hope these insights are useful to the LLM community, especially model trainers. Below are some future tasks.\n1. We will make the benchmark more challenging, forcing LLMs to write more lines of code to cover more scenarios.\n2. We will incorporate more frameworks (e.g. Vue) and languages (e.g. Python) to increase the benchmark coverage.\n3. We will continue to explore and evaluate new prompting techniques since they are crucial to LLM practitioners."}, {"title": "Appendix: Benchmark Difficulty per Application", "content": "Fig. 9 shows the failure pattern broken down by applications.\n1. Consistency Across Applications: All applications exhibit the same general shape-a large concentration of easier problems on the left side and a few harder problems on the right side. This consistency suggests that across different domains, there are always a few particularly challenging problems that models struggle with.\n2. Variations in Skewness: Some applications, such as Fitness Tracking and Music Streaming, show a more pronounced skew with a sharp rise in failure rates for a few problems, indicating a steeper difficulty curve. Others have a more gradual increase, indicating a more even distribution of problem difficulty.\n3. Extreme Difficulty in Certain Applications: Applications like Customer Support and Pet Care have a sharper increase towards the right, implying that these domains have a subset of problems that are especially challenging.\n4. Easier Applications: In applications like Weather and Photo Gallery, the overall number of failures seems lower compared to other appli cations, suggesting that the problems in these areas were generally easier."}, {"title": "Appendix: LOC Distribution by Applications", "content": "In Tab. 7, we rank median LOC for each application. Consistent with the case for model ranking (Tab. 3), the median values stay within a narrow range (37 to 46). This suggests that all models consistently produce solutions of similar length, irrespective of the task complexity or domain.\nFig. 10 collects violin charts of 14 applications following unimodal distribution, where the model outputs are centered around a common length, with less variation between extremes. The remaining 6 applications are in Fig. 11, following multimodal distribution. In both cases, the median LOC is always positioned centrally in each distribution, which suggests that the code generation is stable across applications. Applications in Fig. 11 exhibit more complex patterns, but the distributions remain balanced with the median value positioned at the center of the distribution."}, {"title": "Appendix: LOC Distribution by Models: Success vs Failure", "content": "Continuing to Fig. 4 and 5, Fig. 12 shows the success/fail LOC distribution of remaining 8 models."}, {"title": "Appendix: LOC Distribution by Applications: Success vs Failure", "content": "We conduct the same study described in Sec. 3.2, except we shard the LOC distribution across applications instead of models. The results are collected in Fig. 13."}, {"title": "Appendix: Error Distribution by Applications", "content": "Fig. 14 shows error distribution by applications. Since each application assembles outputs from all models, the raw error counts are at the same scale for all applications. We do not find any distinctive patterns. There is neither special error nor special application."}]}