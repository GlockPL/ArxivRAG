{"title": "CausalStock: Deep End-to-end Causal Discovery\nfor News-driven Stock Movement Prediction", "authors": ["Shuqi Li", "Yuebo Sun", "Yuxin Lin", "Xin Gao", "Shuo Shang", "Rui Yan"], "abstract": "There are two issues in news-driven multi-stock movement prediction tasks that are\nnot well solved in the existing works. On the one hand, \"relation discovery\" is a\npivotal part when leveraging the price information of other stocks to achieve accu-\nrate stock movement prediction. Given that stock relations are often unidirectional,\nsuch as the \"supplier-consumer\u201d relationship, causal relations are more appropriate\nto capture the impact between stocks. On the other hand, there is substantial\nnoise existing in the news data leading to extracting effective information with\ndifficulty. With these two issues in mind, we propose a novel framework called\nCausalStock for news-driven multi-stock movement prediction, which discovers\nthe temporal causal relations between stocks. We design a lag-dependent tempo-\nral causal discovery mechanism to model the temporal causal graph distribution.\nThen a Functional Causal Model is employed to encapsulate the discovered causal\nrelations and predict the stock movements. Additionally, we propose a Denoised\nNews Encoder by taking advantage of the excellent text evaluation ability of large\nlanguage models (LLMs) to extract useful information from massive news data.\nThe experiment results show that CausalStock outperforms the strong baselines\nfor both news-driven multi-stock movement prediction and multi-stock movement\nprediction tasks on six real-world datasets collected from the US, China, Japan,\nand UK markets. Moreover, getting benefit from the causal relations, CausalStock\ncould offer a clear prediction mechanism with good explainability.", "sections": [{"title": "1 Introduction", "content": "The financial services industry has maintained a leading position in embracing data science method-\nologies to inform investment determinations. Within this domain, quantitative trading has garnered\nsubstantial attention from both academia and industry. Researchers have consistently worked on ex-\nploring different approaches to predict the stock movement (rise or fall of stock price) for many years,\nsuch as uni-stock movement prediction [21], multi-stock movement prediction [44, 23], news-driven\nstock movement prediction [42, 19] and so on, which have shown significant success. These methods\nusually model the stock movement prediction task as a time series classification problem.\nIn this paper, we focus on the news-driven multi-stock movement prediction task. A prevalent\nmodel paradigm for this task often takes the historical price features and the stock-related news\nof multiple stocks as inputs and then leverages the well-designed neural networks to make stock\nmovement predictions. There are two key modeling points for tackling this task: modeling the stock"}, {"title": "2 Related work", "content": "Stock prices prediction In traditional trading practices, there are two analysis paradigms commonly\nused to make stock movement predictions: technical analysis and fundamental analysis. With\ntechnical analysis, investors and traders tend to forecast stock prices relying on historical price\npatterns. Fundamental analysis aims to assess the intrinsic value of a stock by considering other factors\nbesides historical prices, such as financial statements, industry trends, and economic conditions.\nSince stock movement prediction involves sequential data, RNN-based networks are applied in many\nworks. ALSTM [28] integrated a dual-stage attention mechanism with LSTM. Adv-ALSTM [8]\nfurther employed adversarial training by adding perturbations to simulate the stochastic and unstable"}, {"title": "3 Preliminary & problem formulation", "content": "In CausalStock, we integrate the model inputs with causal relations into FCM for prediction. In this\nsection, we introduce the fundamental concepts of FCM and the temporal causal graph.\nConsider a multivariate time series ${X_t}_{t=1}^{T}$ with $D$ variables, the\ntemporal causal graph $\\mathcal{G}$ [46] is commonly defined as a series of directed acyclic graph $\\mathcal{G} =$\n$[\\mathcal{G}_1, \\mathcal{G}_2, ..., \\mathcal{G}_L] = {\\mathcal{G}_l}_{l=1}^L \\in \\mathbb{R}^{L \\times D \\times D}$ with maximum time lag $L$. Each $\\mathcal{G}_l \\in \\mathbb{R}^{D \\times D}$ specifies\nthe lagged causal relationships between $X_{t-l}$ and $X_t$, the element $\\mathcal{G}_{l,ji} = 1$ if there exists a causal\nlink $X_{t-l}^i \\rightarrow X_t^j$ and $\\mathcal{G}_{l,ji} = 0$ otherwise.\nFCM represents a set of generative functions that incorporate the\ninput features based on causal knowledge (structured as a causal graph) to produce a final prediction.\nOptimizing the prediction accuracy concurrently refines the underlying causal graph. The theoretical\ndemonstration presented in [14, 9] indicates that if the prediction is accurate, the causal graph can\nbe considered a reliable approximation of real causal relations. Given the temporal causal graph $\\mathcal{G}$\ndefined as before, a temporal FCM is defined as follows:\n$X_t^i = F_i(\\text{Pa}_{\\mathcal{G}}^i(<t), z_t^i)$,"}, {"title": "3.2 Problem formulation", "content": "In this paper, we focus on tackling the news-driven multi-stock\nmovement prediction task. For the target trading day $T$, we\ndenote the model inputs as the past $L$ time lag information\nof $D$ stocks as $X_{<T} = {X_t}_{t=T-DL:T-1} = [\\mathcal{C}_{<T}, P_{<T}] =$\n${[\\mathcal{C}_t^i, P_t^i]}_{i=1}^{D}\\t=T-DL:T-1$, where $\\mathcal{C}_t^i$ and $P_t^i$ represent the news\ncorpora representation and the historical price features rep-\nresentation of $i$-th stock at time step $t$ respectively. The ob-\njective is to predict the movement of adjusted close prices\n$\\mathcal{Y}_T = {y_T^i}_{i=1}^{D} \\in \\mathbb{R}^{D\\times 1}$ on $T$-th trading day of all stocks si-\nmultaneously, where $y_T^i \\in \\{0,1\\}$ representing the $i$-th stock\nprice will fall or rise at trading day $T$, i.e., stock movement.\nIn a theoretical way, this task could be trained by maximiz-\ning the log-likelihood of conditional probability distribution\n$p(\\mathcal{Y}_T | X_{<T})$, so that the most likely $\\mathcal{Y}_T$ are generated."}, {"title": "4 CausalStock", "content": "The conditional probability distribution could be further factorized as follows:\n$p(\\mathcal{Y}_T | X_{<T}) = \\int_{\\mathcal{G}} p(\\mathcal{Y}_T, \\mathcal{G} | X_{<T}) d\\mathcal{G} = \\int_{\\mathcal{G}} p(\\mathcal{Y}_T | X_{<T}, \\mathcal{G}) p(\\mathcal{G} | X_{<T}) d\\mathcal{G}.$\nThe overall process is taken as two joint training parts: temporal causal graph discovery $p(\\mathcal{G} | X_{<T})$\nand the prediction process given the causal relations $p(\\mathcal{Y}_T | X_{<T}, \\mathcal{G})$. The probabilistic graphic\nrepresentation of this modeling process is shown in Figure 1. In CausalStock, we develop a lag-\ndependent causal discovery module, according to which we could take another step by modeling\n$p(\\mathcal{G}|X_{<T})$ as a lag-dependent format:\n$p(\\mathcal{G} | X_{<T}) = p(\\mathcal{G}_1 | X_{T-1}) \\prod_{l=2}^{L} p(\\mathcal{G}_l | \\mathcal{G}_{l-1}, X_{T-l}).$\nFor the prediction part $p(\\mathcal{Y}_T | X_{<T}, \\mathcal{G})$, we design an FCM as shown in Equation 9 to predict the\nfuture movement based on the past information $X_{<T}$ and the discovered temporal causal graph $\\mathcal{G}$.\nIn a nutshell, CausalStock comprises three primary components as shown in Figure 2:\n1. Market Information Encoder (MIE) encodes the news text and price features. In this part, an\nLLM-based Denoised News Encoder is proposed;\n2. Lag-dependent Temporal Causal Discovery (Lag-dependent TCD) module leverages varia-\ntional inference to mine the causal relationship based on the given market information of\nstocks, i.e., modeling $p(\\mathcal{G} | X_{<T})$;\n3. Functional Causal Model (FCM) generates the prediction of future price movements accord-\ning to the discovered causal graph, i.e., modeling $p(\\mathcal{Y}_T | X_{<T}, \\mathcal{G})$."}, {"title": "4.2 Market information encoder (MIE)", "content": "Market Information Encoder (MIE) takes news corpora and numerical stock price features as\ninputs, and outputs the historical market information representations $X_{<T} = [\\mathcal{C}_{<T}, P_{<T}] =$\n${[\\mathcal{C}_t^i, P_t^i]}_{i=1}^{D}\\t=T-DL:T-1 = {X_t^i}_{i=1}^{D}\\t=T-DL:T-1$ for $D$ stocks with time lag $L$. For $i$-th stock, each time\nstep representation $X_t^i$ is the combination of the text representation $\\mathcal{C}_t^i$ generated by the news encoder\nand the historical price features representation $P_t^i$ generated by the price encoder."}, {"title": "4.3 Lag-dependent temporal causal discovery (Lag-dependent TCD)", "content": "In this section, we propose Lag-dependent Temporal Causal Discovery module. Inspired by [14], our\nmodel takes a Bayesian view for modeling the distribution of temporal causal graph, which aims to\nlearn the posterior distribution $p(\\mathcal{G} | X_{<T})$. Unfortunately, the exact graph posterior is intractable"}, {"title": "4.4 Functional causal model (FCM)", "content": "In this section, we design an FCM to model $p_{\\theta}(\\mathcal{Y}_T | X_{<T}, \\mathcal{G})$, where $\\theta$ denotes the parameter set of\nFCM. We focus on additive noise FCM [18] to generate $\\mathcal{Y}_T = {y_T^i}_{i=1}^{D} \\in \\mathbb{R}^{D\\times 1}$:\n$\\mathcal{Y}_T^i = F_i(\\text{Pa}_{\\mathcal{G}}^i(<T), z_i) = f_i(\\text{Pa}_{\\mathcal{G}}^i(<T)) + z_i,$\nwhere $z_i$ represents mutually and serially independent dynamical noise, and $f_i : \\mathbb{R}^{D \\times L} \\rightarrow \\mathbb{R}^{1}$ are\ngeneral differentiable non-linear function that satisfies the relations specified by the temporal causal\ngraph $\\mathcal{G}$ strictly, namely, if $X_l^j \\notin \\text{Pa}_{\\mathcal{G}}^i(<T)$, then $df_i/dX_l^j = 0."}, {"title": "4.5 Training objective", "content": "We train our model by maximizing the conditional log-likelihood $\\log p_{\\theta}(\\mathcal{Y}_T | X_{<T})$. The variational\nevidence lower bound (ELBO) of the model objective is derived as follows:\n$\\log p_{\\theta}(\\mathcal{Y}_T | X_{<T}) = \\log \\int_{\\mathcal{G}} p_{\\theta}(\\mathcal{Y}_T | X_{<T}, \\mathcal{G}) p(\\mathcal{G}) d\\mathcal{G}$"}, {"content": "$\\geq \\int_{\\mathcal{G}} \\frac{q_{\\phi}(\\mathcal{G})}{q_{\\phi}(\\mathcal{G})} \\log \\frac{p_{\\theta}(\\mathcal{Y}_T | X_{<T}, \\mathcal{G}) p(\\mathcal{G})}{q_{\\phi}(\\mathcal{G})} q_{\\phi}(\\mathcal{G}) d\\mathcal{G} + H(q_{\\phi}(\\mathcal{G}))$"}, {"content": "$\\geq \\mathbb{E}_{q(\\mathcal{G})} [\\log p_{\\theta}(\\mathcal{Y}_T | X_{<T}, \\mathcal{G}) + \\log p(\\mathcal{G})] + H(q_{\\phi}(\\mathcal{G}))$"}, {"content": "$\\geq \\mathbb{E}_{q(\\mathcal{G})} [\\sum_{i=1}^D \\log p_{z_i}(z_i) + \\log p(\\mathcal{G})] + H(q_{\\phi}(\\mathcal{G})).$\nHere, $p(\\mathcal{G})$ represents the prior of causal graph, and $H(q_{\\phi}(\\mathcal{G}))$ is the entropy of the posterior\napproximator. $\\log p_{\\theta}(\\mathcal{Y}_T | X_{<T}, \\mathcal{G}) = \\log p_{z_i}(z_i)$ is the log-likelihood of the target distribution, in\nwhich $z_i$ is calculated by Equation 9 at training stage.\nBesides, we further adopt the binary cross entropy loss as another objective $BCE(g_T, \\mathcal{Y}_T)$ to improve\nthe learning performance, where $g_T$ is the ground truth movement at target trading day $T$. Overall,\nthe final training loss $\\mathcal{L}$ is as follows,\n$BCE(\\mathcal{Y}_T, y_T) = -\\sum_{i=1}^D(g_T^i \\log(y_T^i) + (1-g_T^i)\\log(1-y_T^i))$\n$\\mathcal{L} = ( -ELBO + \\lambda BCE(g_T, \\mathcal{Y}_T)).$\nwhere $\\lambda$ is the scalar weight to balance loss terms. We note that the required assumptions and the\ntheoretical guarantees are summarized in Appendix B."}, {"title": "5 Experiments", "content": "Except for the news-driven multi-stock movement prediction task, our model could also handle the\nmulti-stock movement prediction task without news by removing the Denoised News Encoder. Thus,\nwe do the experiments for both two tasks."}, {"title": "5.3 Ablation study", "content": "For the ablation study, we conduct several model variants on ACL18, CMIN-CN and CMIN-US to\nexplore the contributions of different settings in CausalStock. For the main framework, we have\nthe following five variants. CausalStock w/o TCD: removing the causal discovery module from\nCausalStock; CausalStock w/o News: removing the news encoder from CausalStock and just taking\nprices data as input; CausalStock w/o link non-existence modeling: only model the causal link exis-\ntence likelihood and leverage Sigmoid function to obtain the link existence probability; CausalStock\nw/o Lag-dependent TCD: replacing the Lag-dependent Temporal Causal Discovery module with\nthe Lag-independent Temporal Causal Discovery module; CausalStock with Variable-dependent\nTCD: we add a variable-dependent causal mechanism that explicitly captures the dependencies\namong different stock edges. Specifically, each edge's probability is conditioned on the states of all\nother edges at the same time step, and the conditional function is the same as the function in the\nlag-dependent mechanism (Equation 6). Furthermore, we explore the performance of six different\nTraditional News Encoders by replacing the denoised news encoder, which outputs the news embed-\ndings as representations. CausalStock with Glove + Bi-GRU: leveraging the Glove word embedding"}, {"title": "5.4 Results of explainability", "content": "Here, we present many cases detailing the interpretability of CausalStock from two perspectives:\nthe news representation from the Denoised News Encoder, and the causal graph discovered by the\nLag-dependent TCD module.\nFirstly, regarding the Denoised News Encoder module, three cases are selected as shown in Figure\n3(c). A piece of news about APPL suggests a potential delay in its 5G iPhone launch, with Denoised\nNews Encoder giving it a negative sentiment score of -0.7 and an impact score of 9. Similarly, a\nnews about TSLA hints at surpassing a significant delivery milestone, receiving a positive sentiment\nscore of 0.7. In contrast, a news piece showing no discernible connection to GOOG is scored with\nnegligible impact. These cases indicate the Denoised News Encoder's efficacy in discerning and\nquantifying the potential influence of news on respective stock prices.\nSecondly, concerning the causal graph discovered by Lag-dependent TCD, we denote the causal\nstrength graph as the dot product of the causal graph $\\mathcal{G}$ and the causal weight graph $\\hat{\\mathcal{G}}$. Every item\nof causal strength graph indicates not only the causality of two stocks but also the degree of causality.\nThe visualized causal strength matrix for ACL18 is shown in Figure 3(b) with a heatmap. From"}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel news-driven multi-stock movement prediction framework called\nCausalStock. We design a lag-dependent temporal causal discovery mechanism to uncover the\ncausal relations among the stocks. Then the functional causal model is employed to encapsulate\ncausal relations and predict future movements. The effectiveness of CausalStock is demonstrated by\nexperiments on multiple real-world datasets. Moreover, CausalStock could offer a clear prediction\nprocess with explainability."}]}