{"title": "PAR-ADVGAN: IMPROVING ADVERSARIAL ATTACK\nCAPABILITY WITH PROGRESSIVE AUTO-REGRESSION\nADVGAN", "authors": ["Jiayu Zhang", "Zhiyu Zhu", "Xinyi Wang", "Silin Liao", "Zhibo Jin", "Flora D. Salim", "Huaming Chen"], "abstract": "Deep neural networks have demonstrated remarkable performance across various\ndomains. However, they are vulnerable to adversarial examples, which can lead to\nerroneous predictions. Generative Adversarial Networks (GANs) can leverage the\ngenerators and discriminators model to quickly produce high-quality adversarial\nexamples. Since both modules train in a competitive and simultaneous manner,\nGAN-based algorithms like AdvGAN can generate adversarial examples with\nbetter transferability compared to traditional methods. However, the generation of\nperturbations is usually limited to a single iteration, preventing these examples from\nfully exploiting the potential of the methods. To tackle this issue, we introduce a\nnovel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It\nincorporates an auto-regressive iteration mechanism within a progressive gener-\nation network to craft adversarial examples with enhanced attack capability. We\nthoroughly evaluate our PAR-AdvGAN method with a large-scale experiment,\ndemonstrating its superior performance over various state-of-the-art black-box\nadversarial attacks, as well as the original AdvGAN. Moreover, PAR-AdvGAN\nsignificantly accelerates the adversarial example generation, i.e., achieving the\nspeeds of up to 335.5 frames per second on Inception-v3 model, outperform-\ning the gradient-based transferable attack algorithms. Our code is available at:\nhttps://anonymous.4open.science/r/PAR-01BF/", "sections": [{"title": "INTRODUCTION", "content": "Deep neural networks (DNNs) are widely used in different real-world applications, i.e., image\nclassification (Li et al., 2020), emotional analysis (Qiang et al., 2020), and item recommendations (Pan\net al., 2020). DNNs demonstrate human-surpassed performance when properly trained. However,\nDNNs can be vulnerable to adversarial examples crafted by attackers (Szegedy et al., 2013; Ma et al.,\n2021; Deng et al., 2020), which is a concern in safety-critical scenarios. Thus, a practical approach\nis to develop effective attack algorithms that can assess the robustness of DNNs against adversarial\nattacks at an early stage, ultimately enhancing model safety.\nCurrently, both white-box and black-box attack algorithms, such as gradient-based methods like\nFGSM (Goodfellow et al., 2014b), NAA (Zhang et al., 2022), SSA (Long et al., 2022), and\noptimization-based approaches such as PGD (Madry et al., 2017) and C&W (Carlini & Wagner, 2017),\nrequire continuous computation of the model's gradient information throughout the attack process.\nHowever, they all require extensive running time. Generative Adversarial Networks (GANs) (Goodfel-\nlow et al., 2014a) have demonstrated promising results for realistic sample generation by leveraging\nboth generator and discriminator for training (Karras et al., 2021; Chang et al., 2022; Haidar &\nRezagholizadeh, 2019; Croce et al., 2020). While the generator constructs high-quality examples,\na discriminator learn to distinguish the original and generated examples. Furthermore, once the\ngenerator is trained, there will be no additional gradient computation for input examples."}, {"title": "RELATED WORK", "content": null}, {"title": "ADVERSARIAL ATTACKS", "content": "While numerous adversarial algorithms are dedicated to generating high-quality and robust adversarial\nsamples, gradient-based attack algorithms constitute a main type. FGSM (Goodfellow et al., 2014b)\nwas the first to utilise the model's gradients, which adds a small perturbation to the input data in the\ndirection of the gradient, thereby maximising the loss function through gradient ascent to achieve\noptimal attack performance. MI-FGSM (Dong et al., 2018) incorporates a momentum factor in each\niteration to mitigate the impact of local optima on the attack success rate. TI-FGSM (Dong et al.,"}, {"title": "ADVERSARIAL DEFENSES", "content": "Adversarial defense represents an effective approach to mitigate the impact of attacks on DNNs.\nCommonly used adversarial defense techniques include denoising and adversarial training. The\ndenoising technique employs preprocessing mechanisms to filter out adversarial examples, thereby\npreventing the poisoning of training data and reducing the likelihood of subsequent attacks on the\nmodel. Other notable works include HRGD (Liao et al., 2018), R&P (Xie et al., 2017) and so\non (Dziugaite et al., 2016; Cohen et al., 2019).\nAdversarial training enhances model robustness by incorporating adversarial examples into the\ntraining process. Ensemble adversarial training (Hang et al., 2020) works by decoupling the target\nmodel from adversarial examples generated by other black-box models, thereby defending against\ntransferable attacks. To enhance the robustness of our algorithm against adversarial defenses, we\nvalidated the attack effectiveness of PAR-AdvGAN on the target model subjected to ensemble\nadversarial training."}, {"title": "METHODOLOGY", "content": "In this section, we first provide the problem definition of adversarial attacks. Then, we discuss the issue\nof perturbation escalation in AdvGAN and propose three strategies to optimise the generator, aiming\nto generate highly transferable adversarial samples. Finally, we provide a detailed implementation for\nthe proposed PAR-AdvGAN method."}, {"title": "PROBLEM DEFINITION OF ADVERSARIAL ATTACKS", "content": "Consider a clean data distribution $P_{data}$ in which benign samples are represented by $X \\subseteq P_{data}$.\nIn an untargeted attack, the network $f$ is misled by the manipulated sample $X_{adv}$. For the original\nsample $x \\in X$, with the original label denoted as $m$, the adversarial goal can be defined as:\n$f(x_{adv}) \\neq m$ \t (1)\n$||X_{adv} - X||_n \\leq \\epsilon$\t(2)\nwhere $||\\cdot||$ represents the n-order norm (e.g., L2 norm), and $\\epsilon$ denotes the maximum perturbation."}, {"title": "PERTURBATION ESCALATION IN ADVGAN", "content": "AdvGAN adopts a non-repetitive iterative approach to improve attack performance. However, as\niterations progress, the perturbation magnitude of the adversarial example increases rapidly. This\nissue arises because it treats each iterated example as an independent instance, neglecting its relation\nto the initial sample $x_0$. Additionally, AdvGAN fails to impose constraints on the distance between\nthe generated samples and the initial samples. This suggests that the perturbations generated in each\niteration will have significant magnitudes. To address this issue, we introduce three propositions:\nProposition 1 The generator should obtain information about the original sample $x_0$."}, {"title": "PROGRESSIVE AUTO-REGRESSION ADVGAN", "content": "In this section, we first introduce the solutions for three propositions, namely progressive generator\nnetwork, auto-regression iterative method, and generator constraints. Next, we explain the training\nprocesses for both the discriminator and the generator. Finally, as shown in Algorithm. 1, we provided\nthe pseudo-code for the PAR-AdvGAN approach."}, {"title": "PROGRESSIVE GENERATOR NETWORK", "content": "For Proposition 1, we adjust the generator to include initial example $x_0$ as an input, resulting in a\nrevised generator $G(x_{adv}^{t-1}, x_0)$. To do this, we expand the channel dimension of the generator's first\nlayer, and employ a concat operator to merge $x_{adv}^{t-1}$ and $x_0$ along the channel dimension. This design\nenables the generator to leverage information from both the current adversarial example $x_{adv}^{t-1}$ and\ninitial input $x_0$, thus facilitating the generation of incremental adversarial perturbations during the\niterative process (refer to line 5 in Alg. 1)."}, {"title": "AUTO-REGRESSION ITERATIVE METHOD", "content": "In Proposition 2, during each training iteration, we utilise a hyperparameter $T$ to regulate the number\nof interactions for $x_{adv}^{t-1}$ instances (refer to line 4). We iteratively generate $x_{adv}^{t}$ by adding perturbations\n$G(x_{adv}^{t-1}, x_0)$ to the preceding $x_{adv}^{t-1}$ (see line 6), then use the resulting gradient progression to update\nand train the generator.\n$V_{\\Theta} = \\frac{\\partial L_{adv}}{\\partial x_{t-1}^{adv} + G(x)} \\frac{\\partial G(x)}{\\partial \\Theta}$\t(3)"}, {"title": "Training of the Discriminator", "content": "We train the discriminator to accurately distinguish adversarial\nsamples generated by the progressive generator and actual samples from the data distribution $P_{data}$.\nSpecifically, we fix the parameters related to the progressive generator and trained the discriminator\n$S_d$ times (line 7). The loss function $L_D$ can be written as:\n$L_D = (1 - D(x))^2 + D(x_{adv})^2$ \t(7)\nIt is worth noting that we did not choose to calculate $L_D$ in the form of $log(1 - D(x))$ as in AdvGAN.\nThis is because we find that the gradient of $log(1 - D(x))$ for D will be very large and not smooth\nwhen $D(x)$ is close to 1, and gradient explosion will occur during iteration. We employ a squared\nform in the Eq. 10 to help mitigate this issue.\nHence, the gradient of the discriminator with respect to the parameters $\\Theta_D$ can be expressed using\nEq. 8 (line 8):\n$\\nabla_{\\Theta_D} = \\nabla_{\\Theta_D} L_D$ \t(8)\nBy updating $\\Theta_D$ through gradient descent, we finally obtain the optimal parameters for the discrimi-\nnator (line 9-10):\n$\\Theta_D = \\Theta_D - \\eta_1 \\nabla_{\\Theta_D}$ \t(9)\nHere $\\eta_1$ is the learning rate in discriminator training."}, {"title": "Constraints on the Generator", "content": "We propose the use of $L_{adv}$ to measure whether the adversarial\nsamples are generated in a direction more conducive to the attack (refer to line 13).\n$L_{adv} = -Cross Entropy(x_{adv}^{t}, m)$ \t(10)\nIt is worth noting that, in untargeted attacks, a larger value of $Cross Entropy(x_{adv}^{t}, m)$ indicates a\nmore effective adversarial example. Consequently, to enhance the adversarial nature of $x_{adv}^{t}$ during\ngradient descent on $L_{adv}$, we prepend a negative sign to $Cross Entropy(x_{adv}^{t}, m)$. It is also feasible\nto replace Cross Entropy with the loss function used in C&W (Carlini & Wagner, 2017).\nTo prevent the issue of perturbation explosion in the auto-regression iterative process, we introduce\n$L_p$ to constrain the magnitude of perturbation (refer to line 14), where $||\\cdot||_2$ stands for the 12 norm:\n$L_p = ||P_t||_2 = ||G(x_{adv}^{t-1}, x_0)||_2$ \t(11)\nTo fulfill Proposition 3, we introduce an additional loss function $L_d$ that enforces the generated\nadversarial examples to remain close to the initial example. This constraint ensures that the iterative\nprogress of generating adversarial perturbations does not deviate significantly from the original input,\nthus maintaining the adversarial samples' proximity to the initial data (see line 15).\n$L_d = ||x_{adv}^{t} - x_0||_2$ \t(12)"}, {"title": "Training of the Progressive Generator", "content": "We train the progressive generator to generate adversarial\nsamples with high transferability and low distortions from original samples while attacking the target\nneural network N. Specifically, we fixed the parameters related to the discriminator and trained the\nprogressive generator $S_g$ times (refer to line 12).\nAs shown in Eq. 14, we computed the gradient of the progressive generator with respect to the\nparameters $\\Theta_G$ (line 16):\n$L_G = (1 - D(x_{adv}))^2$ \t(13)\n$\\nabla_{\\Theta_G} = \\nabla_{\\Theta_G} (L_G + \\lambda_1 L_p + \\lambda_2 L_d + \\lambda_3 L_{adv})$ \t(14)\nNote that $L_G$ is the loss function to deceive the discriminator and $\\lambda_1, \\lambda_2, \\lambda_3$ are the weight hyper-\nparameters that control the balance between loss functions. By updating $\\Theta_G$ through gradient descent,\nwe ultimately obtain the optimal parameters for the progressive generator (line 17-18):\n$\\Theta_G = \\Theta_G - \\eta_2 \\nabla_{\\Theta_G}$ \t(15)\nHere $\\eta_2$ is the learning rate in discriminator training."}, {"title": "EXPERIMENTS", "content": "In this section, we present the experiments conducted to evaluate the performance of our method. To\nguide the analysis, we address the following research questions.\n\u2022 What is the attack success rate of PAR-AdvGAN compared to the baseline AdvGAN? (RQ1)\n\u2022 How does PAR-AdvGAN's performance in attack transferability and attack speed compare\nto state-of-the-art methods in adversarial attacks? Is it effective? (RQ2)\n\u2022 Why does PAR-AdvGAN work effectively? (RQ3)"}, {"title": "EXPERIMENT SETUP", "content": null}, {"title": "DATASET AND MODELS", "content": "We conducted the experiments on the ImageNet-compatible dataset consisting of 1000 images with\na resolution of 299\u00d7299\u00d73 (Papernot et al., 2018) 1. The dataset generation process follows the\nliterature (Dong et al., 2018; 2019; Gao et al., 2021; 2020)\nHere we refer to the typical and state-of-the-art transferable adversarial attack methods (Xiao et al.,\n2018; Xie et al., 2019; Dong et al., 2019; 2018; Lin et al., 2019; Zhang et al., 2022). To ensure\nexperiment fairness, we selected representative models from two types: normally-trained and defense-\ntrained models. The normally trained models include Inceptionv3 (Inc-v3) (Szegedy et al., 2016),\nInception-v4 (Inc-v4) (Szegedy et al., 2017), Inception-ResNet-v2 (IncRes-v2) (Szegedy et al., 2017),\nResNet-v2-50 (Res-50) (He et al., 2016a;b), ResNet-v2-101 (Res-101) (He et al., 2016a;b), and\nResNet-v2-152 (Res-152) (He et al., 2016a;b). As for the defense-trained models through ensemble\nadversarial training, we selected Inc-v3ens3 (Tram\u00e8r et al., 2017), Inc-v3ens4 (Tram\u00e8r et al., 2017),\nand IncResv2ens (Tram\u00e8r et al., 2017)."}, {"title": "BASELINE METHODS", "content": "We employ the original AdvGAN (Xiao et al., 2018) algorithm as our baseline to validate the\ntransferability performance by incorporating self-regressive iteration in PAR-AdvGAN. Meanwhile, to\nevaluate our proposed PAR-AdvGAN, we selected seven state-of-the-art black-box adversarial attack\nmethods as our competitive baselines, including FGSM (Goodfellow et al., 2014b), BIM (Kurakin\net al., 2018), PGD (Madry et al., 2017), DI-FGSM (Xie et al., 2019), TI-FGSM (Dong et al., 2019),\nMI-FGSM (Dong et al., 2018), and SINI-FGSM (Lin et al., 2019)."}, {"title": "PARAMETER SETTINGS", "content": "All experiments in this study are conducted using the Nvidia RTX 6000 Ada 48GB. In all experiments,\nwe set the following fixed parameters for each algorithm according to the settings in Kim (2020). For\nAdvGAN and PAR-AdvGAN, the training epochs are set to 60. The initial learning rate for both the\nGenerator and Discriminator is set to 0.001, which is then reduced to 0.0001 at the 50th epoch. For\nDI-FGSM, we set the decay to 0, the resize_rate to 0.9, and the diversity_prob to 0.5. For TI-FGSM,\ndecay is set to 0, kernel_name is set to \"gaussian,\" len_kernel is set to 15, resize rate is set to 0.9, and\nthe diversity_prob is set to 0.5. For MI-FGSM, decay is set to 1. For SINI-FGSM, decay is set to 1,\nand m is set to 5."}, {"title": "\u039cETRICS", "content": "Attack success rate (ASR) is a metric to evaluate the transferability of attacks. It quantifies the\naverage proportion of mislabeled samples among all generated samples after the attack. Thus, a\nhigher attack success rate signifies better transferability. Additionally, we use Frames Per Second\n(FPS) to assess the attack speed. Another crucial measure, the perturbation rate, is utilised to ensure\nthat the adversarial images do not largely diverge from the original images in visual perception. A\nlow value of this rate suggests that the adversarial examples maintain close visual fidelity to their\noriginals. Detailed formulas are in the Appendix A.4."}, {"title": "RQ1: ATTACKING PERFORMANCE", "content": "As shown in Table. 1, we compare the attack success rates of the original AdvGAN and our proposed\nPAR-AdvGAN at three different perturbation rates of 8, 9, and 10. The comparisons are conducted\nusing Inc-v3 and Inc-v4 as surrogate models and attacks are lunched on IncRes-v2. The results\nindicate that in most cases, our algorithm outperforms AdvGAN in terms of attack success rate.\nWe can see that compared to the most represen-\ntative AdvGAN algorithm, PAR-AdvGAN has\nmade significant improvements for attacking per-\nformance at a low perturbation rate. Specifically,\nsimilar to AdvGAN, PAR-AdvGAN, as a gen-\nerative model, does not require additional gra-\ndient calculations based on different input data\nafter training the generator. Compared to tra-\nditional gradient-based black-box transferable\nattack methods, it possesses faster attack speed.\nTherefore, we consider the PAR-AdvGAN algo-\nrithm feasible and suitable for attack scenarios\nthat demand high transferability and fast generation of adversarial samples."}, {"title": "EFFECTIVENESS EXPERIMENT FOR RQ2: TRANSFERABILITY AND ATTACK SPEED", "content": "To validate the transferability and attack speed of PAR-AdvGAN compared to other SOTA methods,\nwe conduct the experiments using various attack methods on Inc-v3, Inc-v4, and IncRes-v2 as source\nmodels to generate adversarial samples. We then conduct transferable attacks on different target\nmodels and use ASR and FPS as the main metrics, to validate the effectiveness of our algorithm."}, {"title": "EXPERIMENTS ON INC-V3", "content": "As shown in Table. 2, we conduct attacks using Inc-v3 as the source model with three different\nperturbation rates on target models of Inc-v4, Res-50, Res-101, Res-152, Inc-v3ens3, Inc-v3ens4,\nand IncRes-v2. We can see that our PAR-AdvGAN algorithm has achieved an average increase of\n30.3% in attack success rate compared to other baselines. Moreover, despite DI-FGSM achieving\nbetter performance than PAR-AdvGAN on Inc-v4, which may be attributed to the randomness in\nmodel training, a comprehensive comparison across all models reveals that the attack success rate of\nPAR-AdvGAN is elevated by 24.6% compared to the best-performing competing baseline, DI-FGSM."}, {"title": "EXPERIMENTS ON INC-V4", "content": "As shown in Table. 3, we conduct attacks using Inc-v4 as the source model with three different\nperturbation rates on target models of Inc-v3, Res-50, Res-101, Res-152, Inc-v3ens3, Inc-v3ens4, and\nIncRes-v2. We can see that our PAR-AdvGAN algorithm has achieved an average increase of 20.13%\nin attack success rate compared to other baselines. Furthermore, compared to the best performing\nSINI-FGSM among competitive baselines, PAR-AdvGAN achieved an increase of 7.48% in ASR."}, {"title": "EXPERIMENTS ON INCRES-V2", "content": "In this section, we conduct transferability tests on Inc-v3, Inc-v4, Res-50, Res-101, Res-152, Inc-\nv3ens3, Inc-v3ens4, and IncRes-v2 as target models with three different perturbation rates using\nIncRes-v2 as the source model. We have included the results in the Table 4. The results demonstrate\nthat PAR-AdvGAN achieves an average increase of 14.96% in ASR compared to other baselines.\nWe can see that although PAR-AdvGAN achieves a lower ASR of 0.02% than the best performing\nSINI-FGSM among competitive baselines, it outperforms AdvGAN by 6.31%."}, {"title": "EXPERIMENTS ON RESNET-50", "content": "As shown in Table 5, we conduct attacks using ResNet-50 as the source model with three different\nperturbation rates on target models of Inc-v3, Inc-v4, Res-101, Res-152, Inc-v3ens3, Inc-v3ens4,\nand IncRes-v2. The experimental results demonstrate that PAR-AdvGAN consistently achieves\nsuperior performance across all target models compared to other baseline methods. Specifically,\nPAR-AdvGAN achieves the highest mASR of 48.2%, 55.64%, and 63.81% for the three perturbation\nrates, outperforming the best-performing baseline DI-FGSM by 5.21%, 10.43%, and 15.45%, respec-\ntively. Furthermore, PAR-AdvGAN shows significant improvements over AdvGAN, with an average\nincrease in attack success rate of 25.37%. Although DI-FGSM achieves competitive performance\non certain models such as Inc-v4 and Res-101, the overall effectiveness of PAR-AdvGAN across all\nmodels underscores its robustness and transferability."}, {"title": "EXPERIMENTS ON VIT-B/16", "content": "In Table 6, we present the results of attacks using ViT-B/16 as the source model with three different\nperturbation rates on several target models, including Inc-v3, Inc-v4, Res-50, Res-101, Res-152,\nInc-v3ens3, Inc-v3ens4, and IncRes-v2. Unlike traditional convolutional neural networks (CNNs),\nVision Transformers (ViTs) adopt a fundamentally different architecture for image classification tasks.\nOur experimental findings show that the proposed PAR-AdvGAN algorithm performs exceptionally\nwell when transferred to ViT-based models, achieving an average increase of 6.85% in attack success\nrate (ASR) compared to AdvGAN across all target models. Specifically, PAR-AdvGAN consistently\ndelivers the highest mean ASR values of 73.38%, 78.56%, and 81.63% across the three perturbation\nrates, surpassing all baseline methods, including DI-FGSM, which was the best performer in certain\ncases. These results underscore the robustness and transferability of PAR-AdvGAN, demonstrating"}, {"title": "ATTACK TRANSFERABILITY RESULT ANALYSIS", "content": "With the results from Tables 2-6, it can be observed that in most cases, our adversarial attack\nalgorithm shows significantly improved transferability compared to the original AdvGAN, especially\nat lower perturbation rates. Additionally, compared to other competitive baselines, PAR-AdvGAN\nexhibits the best transferability. Notably, to ensure the fairness of the experiments, our algorithm\nwas consistently compared with other methods for a lowest perturbation rate. In instances where\nthe perturbation rates were higher, some algorithms did not exhibit a proportional increase in attack\ntransferability. However, the transferability is overall improved."}, {"title": "ATTACK SPEED ANALYSIS", "content": "As shown in Table. 7, we evaluated the computational efficiency of PAR-AdvGAN and seven\ncompetitive baselines using Inc-v3, Inc-v4, and IncRes-v2 as source models. We use FPS as the\nmetric for measuring attack speed. It can be observed that across Inc-v3, Inc-v4, and IncRes-v2, PAR-\nAdvGAN exhibits speed improvements of 61, 88.3, and 158.5 times over the slowest-performing PGD\nalgorithm among the competitive baselines. Furthermore, in comparison to the fastest-performing\nFGSM algorithm among the competitive baselines, PAR-AdvGAN achieves speed enhancements of\n1.9, 2.5, and 4.4 times, respectively. We assert that PAR-AdvGAN demonstrates significantly higher\nattack speed in comparison to traditional gradient-based transferable methods, while simultaneously\nachieving state-of-the-art transferability performance."}, {"title": "ABLATION EXPERIMENT FOR RQ3", "content": "We investigate the effects of parameter $\\lambda$ on the\nattack transferability as it is an important pa-\nrameter to control the perturbation range. Fig. 1\nshows the performance of PAR-AdvGAN with\nInc-v3 as the source model, with a fixed $\\epsilon$ of\n20, and $\\lambda$ set to 0.5, 0.8, and 1 for different\ntarget models. At $\\lambda$ of 0.5, the specific pertur-\nbation rates are 11.75, 12.56, and 12.89. At $\\lambda$\nof 0.8, the specific perturbation rates are 11.55,\n12.59, and 12.90. At $\\lambda$ of 1, the specific per-\nturbation rates are 11.66, 12.45, and 12.81. We\ncan see that at higher perturbation rate intervals,\nsetting $\\lambda$ to 0.8 achieves best transferability per-\nformance."}, {"title": "CONCLUSION", "content": "In this paper, we present a novel Progressive Auto-Regression AdvGAN (PAR-AdvGAN) algorithm\nto boost adversarial attack capability through iterative perturbations. Specifically, to address the pertur-\nbation escalation issue in AdvGAN, we first adopt a progressive generator network to incorporate the\ninitial sample $x_0$ in the perturbation generation process. An auto-regression iterative method is then\nproposed to include non-initial sample information in generator training. Furthermore, we constrain\nthe distance between initial samples and subsequent samples. Our extensive experimental results\nexhibit the superior attack transferability of our method. Moreover, compared with the state-of-the-art\ngradient-based transferable attacks, our method achieves an accelerated attack efficiency."}, {"title": "CODE OF ETHICS AND ETHICS STATEMENT", "content": "All authors of this paper have read and adhered to the ICLR Code of Ethics, as outlined at https:\n//iclr.cc/public/CodeOfEthics. We ensure that our research follows the principles of\nintegrity, fairness, and respect. The methodologies proposed in this work do not involve human\nsubjects, and no personal data is used in our experiments. The datasets and models employed\nare open-source, and the outcomes of the experiments are aimed at advancing adversarial attack\nresearch without promoting any harmful applications. There are no conflicts of interest or external\nsponsorship affecting this work. We have ensured compliance with ethical standards and legal\nregulations throughout the research process."}, {"title": "REPRODUCIBILITY", "content": "In order to ensure the reproducibility of our results, we have provided detailed descriptions of the\nexperimental setup in the main text. Our proposed PAR-AdvGAN method is described in Section 3,\nand the algorithms used are thoroughly outlined, including pseudo-code (Algorithm 1). We have also\nincluded hyperparameter settings and model architectures in Section 4.1.3. Additional details about\nthe datasets, metrics, and baseline methods are available in the supplementary materials. Moreover,\nwe have made our code and trained models available in an anonymous repository at https://\nanonymous.4open.science/r/PAR-01BF/, ensuring easy access for future research and\nreproduction of our results."}, {"title": "LIMITATIONS", "content": "We notice a slight difference in the attack success rate of PAR-AdvGAN on Inc-v3ens3 and In-\ncResv2ens compared to the best baseline TI-FGSM in Tab. 3, although for overall mASR our method\nachieves the best performance. This inconsistency suggests a possible dependency of our approach\non model selection. The main reason for the inconsistency is related to whether the method requires\nfurther interaction with the model after training the Generator regarding the generation process. Our\nmethod PAR-AdvGAN doesn't require the interaction, while TI-FGSM needs to continuously interact\nwith the model for the generation process to obtain gradient information. It is always be feasible for\ndifferent models and we will consider reducing the dependence on model selection in the future to\ngenerate more transferable adversarial examples."}, {"title": "SCHEMATIC DIAGRAM OF PERTURBATION COMPARISON", "content": null}, {"title": "OPTIMIZATION OBJECTIVES FOR ADVGAN", "content": "As previously mentioned, in AdvGAN (Xiao et al., 2018), the adversarial sample $X_{adv}$ is synthesized\nfrom the original input x and perturbation G(x). By employing a process of iterative and competitive\ntraining between the generator G and the discriminator D, AdvGAN can generate high-quality\nadversarial samples. This training approach enables AdvGAN to enhance the generator's capabil-\nity to produce perturbations that effectively deceive the discriminator, leading it to classify these\nperturbations as real samples."}, {"title": "OPTIMIZATION OBJECTIVE OF GENERATOR", "content": "For optimal attack performance, particularly in deceiving the target model $f$ to classify the sample as\nthe target label $l$, AdvGAN utilises the loss function $L_{adv}$ to estimate the likelihood of successfully\nmisleading $f$. In mathematical terms:\n$L_{adv} = E_x (J_f(x + G(x),l))$ \t(16)\nHere, $E_x$ denotes the expected value of the input data x, in accordance with the distribution $P_{data}$.\n$J_f$ represents the loss function employed in training the target model $f$."}, {"title": "OPTIMIZATION OBJECTIVE OF DISCRIMINATOR", "content": "The discriminator's role is to distinguish between adversarial samples and real samples. AdvGAN\nemploys the loss function $L_{GAN}$ to maximize the distinction between manipulated data and the real\ndata in discriminator D. The mathematical expression of $L_{GAN}$ is as follows:\n$L_{GAN} = E_x (log (1 - D(x))) + E_x (logD (x + G(x)))$ \t(17)"}, {"title": "OPTIMIZATION OBJECTIVE OF PERTURBATIONS", "content": "Drawing on the findings from (Carlini & Wagner, 2017; Liu et al., 2016; Isola et al., 2017), AdvGAN\nincorporates the $L_{hinge}$ loss function to regulate the magnitude of the perturbations. With the\nincorporating of $L_{hinge}$, AdvGAN effectively limits the perturbation magnitude, ensuring that the\ngenerated adversarial samples remain imperceptible and closely resemble the original samples.\n$L_{hinge}$ is shown as:\n$L_{hinge} = E_x (max(0, ||G(x)||_2 - \u0441))$ \t(18)\n$||\\cdot||_2$ denotes the L2 norm, and $c$ is a user-specified bound."}, {"title": "DETAILED PROOF OF THE OMISSION IN EQ. 3", "content": "Through the chain rule of gradients, we can split $\\frac{\\partial L_{adv}}{\\partial G(x)}$ into $\\frac{\\partial L_{adv}}{\\partial x + G(x)} \\frac{\\partial x+G(x)}{\\partial G(x)}$. (2). However, in"}]}