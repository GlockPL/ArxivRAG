{"title": "Parkinson's Disease Detection from Resting State EEG using Multi-Head Graph Structure Learning with Gradient Weighted Graph Attention Explanations", "authors": ["Christopher Neves", "Yong Zeng", "Yiming Xiao"], "abstract": "Parkinson's disease (PD) is a debilitating neurodegenerative disease that has severe impacts on an individual's quality of life. Compared with structural and functional MRI-based biomarkers for the disease, electroencephalography (EEG) can provide more accessible alternatives for clinical insights. While deep learning (DL) techniques have provided excellent outcomes, many techniques fail to model spatial information and dynamic brain connectivity, and face challenges in robust feature learning, limited data sizes, and poor explainability. To address these issues, we proposed a novel graph neural network (GNN) technique for explainable PD detection using resting state EEG. Specifically, we employ structured global convolutions with contrastive learning to better model complex features with limited data, a novel multi-head graph structure learner to capture the non-Euclidean structure of EEG data, and a head-wise gradient-weighted graph attention explainer to offer neural connectivity insights. We developed and evaluated our method using the UC San Diego Parkinson's disease EEG dataset, and achieved 69.40% detection accuracy in subject-wise leave-one-out cross-validation while generating intuitive explanations for the learnt graph topology.", "sections": [{"title": "1 Introduction", "content": "Parkinson's Disease (PD) is the second most common neurodegenerative disorder worldwide [24]. Primarily characterized by motor symptoms, the complex disease can also include psychiatric and cognitive issues. MRI-based biomarkers have attracted major attention, including biochemical alteration shown in quantitative MRI and structural/functional connectivity changes revealed by diffusion"}, {"title": "2 Related Works", "content": "To date, several GNN-based methods [11] have been explored for EEG analysis, particularly for seizure detection in epilepsy. Traditionally, manually defined"}, {"title": "3 Methods and Materials", "content": "Figure 1 outlines an overview our proposed DL architecture, which is composed of a feature encoder (LongConv feature encoder), a multi-head graph structure learner (MH-GSL), a Chebyshev GNN, and a classifier made of fully connected layers for PD vs. Healthy classification."}, {"title": "3.1 Feature Encoder with Contrastive Learning", "content": "Following the success of Structured Global Convolutions (SGConv) [14] for modeling long sequential data in deep learning tasks, we incorporate it into our EEG feature encoder design, which encodes the input EEG signal to $X_e \\in \\mathbb{R}^{C \\times d_m}$ (C is the number of channels and $d_m$ is model dimension). Specifically, we follow the feature extraction network setup in the work of Vetter et al. [27], who modify the Structured Global Convolution layer from its original formulation to have more fine-grained control over its kernel size (referred to as SLConv in Fig. 1). The feature extraction network (called LongConv) consists of interleaved masked 1D convolutions, which project the input channels to a set of hidden ones while SLConv layers extract long-range temporal information from each hidden channel. Each masked 1D convolution is followed by a batch normalization layer and"}, {"title": "3.2 Multi-Head Graph Structure Learner", "content": "Graph topology of EEG signals obtained from stationary connectivity measures and/or the physical distance between electrodes for GNN learning can be misleading and sub-optimal. To tackle this, we proposed a novel graph structure learner (GSL) using multi-head attention. Based on the graph structure layer by Tang et al. [23], which adopts the self-attention mechanism [25] to learn edge weights, we extended this approach to include multiple attention heads. Thus, the resulting graph structure learner can attend to different graph representations (adjacency matrices) in parallel, with each attention head providing the edge weights for its paired graph representation. Then, each head-wise learnt graph representation, together with the encoded EEG features are passed to a Chebyshev GNN, updating the features with the learnt spatial relationships. The output of the Chebyshev GNN for each head is then concatenated and projected back to the model dimension $d_m$ using a linear layer. The adjacency matrix $A_h \\in \\mathbb{R}^{C \\times C}$ for a single attention head $h$ out of $H$ heads is given by:\n\n$Q_h = X_eW_{qh}, K_h = X_eW_{kh}$\n\n$A_h = softmax(\\frac{Q_hK_h^T}{\\sqrt{d_k}})$\n\nwhere $X_e \\in \\mathbb{R}^{C \\times d_m}$ are the feature embeddings, and $W_{qh}$ and $W_{kh}$ are the parameter matrices projecting $X_e$ to query $Q_h$ and key $K_h$, respectively."}, {"title": "3.3 Graph-based EEG Classification", "content": "As shown in Fig. 1, the final EEG classification is achieved by first adding the head-wise aggregated output from the Chebyshev GNN and EEG feature embeddings from the temporal feature encoder, and average pooling the result along"}, {"title": "3.4 Head-wise Gradient-Weighted Graph Attention Explainer", "content": "In multi-head self-attention networks, the average or maximum of the head-wise attention scores [26] are often used to provide graph explainations, but this could be insufficient as some heads may carry greater contributions for decision-making. Inspired by the work of Rasoulian et al. [19], where head-wise gradient-weighted self-attention maps were used to improve the specificity of the attention map, we adapted the core idea for GNN-based EEG analysis. Specifically, we obtain a graph explanation by first weighing the head-wise graph representation $A_h$ with the norm of its gradient based on the class activation. Then, the final adjacency matrix $A \\in \\mathbb{R}^{C \\times C}$ is generated as:\n\n$A = \\frac{1}{H}\\sum_{h=1}^{H} |\\frac{\\partial Y}{\\partial A_h}| A_h$\n\nwhere $H$ is the number of attention heads and $Y$ is the target class to generate a graph representation for. Finally, $A$ is thresholded to keep the attention scores within two standard deviations from the mean, and then are normalized to [0,1]."}, {"title": "3.5 Dataset and Preprocessing", "content": "We used the UC San Diego Parkinson's disease resting-state EEG (rs-EEG) dataset [20] for our study. The dataset contains the resting-state data of 15 PD patients (63.2\u00b18.2 years, 8 females) and 16 healthy controls (63.5\u00b19.6 years, 9 females). All PD patients had mild to moderate disease severity. Each participant had at least 3 minutes of resting state data recorded using a 32-channel Biosemi ActiveTwo EEG system (sampling rate = 512 Hz). We minimally preprocessed each subject's EEG by first setting the reference to the mean of the EXG7 and"}, {"title": "3.6 Experimental Setup and Ablation Studies", "content": "To assess the classification performance of our proposed framework, we compared it against a variety of DL models and configurations. With CNN methods dominating EEG analysis, as a baseline, we re-implemented the method by Dose et al. [7] that showed great success on small datasets. To further validate the benefits of each design component of our method, we performed a series of ablation studies. First, to confirm the contribution of the Chebyshev GNN, we compared the full version of our method (CL-Encoder+Freeze) against PD detection only based on the temporal feature encoder (LongConv Encoder). Second, to verify whether our multi-head GSL had a positive impact on the network performance, we replaced the learnt graph structure input to the Chebyshev GNN with a static graph based on PCC, and evaluate the classification accuracy against the original design (\"Full Model w/o MH-GSL vs. Full Model with MH-GSL\", both without CL). Third, to quantify the performance gain from the SimCLR framework, we compared the proposed frameworks with and without self-supervised pre-training (\u201cCL-Encoder+Freeze vs. Full Model with MH-GSL\"). Finally, as some studies demonstrated the benefit of finetuning pre-trained feature encoder, we further tested our proposed method by finetuning the feature encoder weights that were pre-trained using the SimCLR framework, and compared the outcome to freezing the feature encoder weights after SimCLR pre-training (\"CL-Encoder+Finetune vs. CL-Encoder+Freeze\"). We computed classification accuracy, precision and recall, macro F1-score, and AUC metrics for all experimental setups over 3 random seeds (i.e., model weight initialization). We trained and evaluated all configurations using a leave-one-out cross-validation, where a single subject was used for testing and the rest for training to avoid data leakage. For each fold, two subjects (one healthy and one PD) were randomly selected from the training data as a validation set. Unlike the more common sample-wise cross-validation in EEG-related DL algorithms, our subject-wise strategy can better assess the generalizability of the proposed framework to unseen subjects. Each model was trained with a batch size of 8 with a MultiStep learning rate (LR) scheduler at an initial LR of 1E-4 and a gamma of 0.1. The MH-GSL model was trained using 2 attention heads and the Chebyshev GNN used a single layer with K=5 and a dropout rate of 0.2."}, {"title": "4 Results", "content": "We present the PD vs. Healthy classification performance of all experiments in Table 1, and with an accuracy of 69.40\u00b11.59%, our proposed method (CL-Encoder+Freeze) outperformed the CNN baseline [7] (accuracy=62.99\u00b14.07%) and the other model configurations. For the ablation studies, we confirmed"}, {"title": "5 Discussion", "content": "Our novel multi-head graph structure learner presents a more dynamic approach that establishes task-driven graphs with improved performance in comparison"}, {"title": "6 Conclusion", "content": "We have developed a novel GNN technique for PD detection from resting state EEG based on dynamic graph structure learning, with a head-wise gradient-"}]}