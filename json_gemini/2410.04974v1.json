{"title": "6DGS: ENHANCED DIRECTION-AWARE GAUSSIAN SPLATTING FOR VOLUMETRIC RENDERING", "authors": ["Zhongpai Gao", "Benjamin Planche", "Meng Zheng", "Anwesa Choudhuri", "Terrence Chen", "Ziyan Wu"], "abstract": "Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based ray tracing with view-dependent effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D spatial-angular representation to better incorporate view-dependent effects, but the Gaussian representation and control scheme are sub-optimal. In this paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. Experiments demonstrate that 6DGS significantly outperforms 3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction of 66.5% Gaussian points compared to 3DGS.", "sections": [{"title": "1 INTRODUCTION", "content": "Novel view synthesis enables the generation of new viewpoints of a scene from limited images, underpinning applications in virtual reality, augmented reality, and realistic rendering for films and games. A significant milestone was the development of neural radiance fields (NeRF) (Mildenhall et al., 2020), representing scenes as continuous volumetric functions that map 3D coordinates and viewing directions to color and density values. While NeRF captures intricate details and complex lighting effects, its reliance on computationally intensive neural networks makes real-time rendering challenging. To address this, 3D Gaussian splatting (3DGS) (Kerbl et al., 2023) was introduced, using 3D Gaussians to represent scenes. By projecting these Gaussians onto the image plane and aggregating"}, {"title": "2 RELATED WORK", "content": "The field of novel view synthesis has seen significant advancements in recent years, with various techniques developed to enhance rendering quality and efficiency. This section reviews the most relevant works on 3D Gaussian splatting, N-dimensional Gaussians, and Gaussian-based ray tracing."}, {"title": "3D Gaussian Splatting", "content": "3D Gaussian splatting (3DGS) (Kerbl et al., 2023) has emerged as a significant advancement in computer graphics and 3D vision, achieving high-fidelity rendering quality while maintaining real-time performance. Numerous works have been proposed to improve rendering quality (Yu et al., 2024; Lu et al., 2024), rendering efficiency (Lee et al., 2024; Bagdasarian et al., 2024), and training optimization (Kheradmand et al., 2024; H\u00f6llein et al., 2024), as well as to explore applications (Kocabas et al., 2024; Zhou et al., 2024b; Niedermayr et al., 2024) and extensions (Charatan et al., 2024; Luiten et al., 2024; Wu et al., 2024; Tang et al., 2024) of 3DGS. For example, Mip-Splatting (Yu et al., 2024) introduces a Gaussian low-pass filter based on Nyquist's theorem to address aliasing and dilation artifacts by matching the maximal sampling rate across all observed samples. Compact3D (Lee et al., 2024) applies vector quantization to compress different attributes into corresponding codebooks, storing the index of each Gaussian to reduce storage overhead. 3DGS-MCMC (Kheradmand et al., 2024) proposes densification and pruning strategies in 3DGS as deterministic state transitions of Markov Chain Monte Carlo (MCMC) samples instead of using heuristics. Many of these advancements and applications of 3DGS can potentially be applied to our 6DGS, which extends 3DGS by incorporating an additional directional component."}, {"title": "N-dimensional Gaussians", "content": "To enhance the 3D Gaussian representation, researchers have introduced other Gaussian representations for rendering. 2DGS (Huang et al., 2024) introduces a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization to enhance geometry reconstruction. 3D-HGS (Li et al., 2024) proposes a 3D half-Gaussian kernel to improve performance without compromising rendering speed. 4DGS (Yang et al., 2024) proposes to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives. N-DG (Diolatzis et al., 2024) introduces N-dimensional Gaussians (N-DG) along with a high-dimensional culling scheme inspired by locality-sensitive hashing. Specifically, N-DG introduces a 10-dimensional Gaussian (10-DG) that incorporates geometry and material information such as world position, view direction, albedo, and roughness, as well as a 6-dimensional Gaussian (6-DG) that includes world position and view direction. Our 6DGS combines the strengths of 3DGS and N-DG for better representation and better adaptive control of Gaussians."}, {"title": "Gaussian-based Ray Tracing", "content": "Unlike most 3DGS methods that render Gaussians via rasterization, some approaches have proposed Gaussian-based ray tracing, generally at the cost of slower rendering speeds or even lower quality. For instance, Condor et al. (2024) models scattering and emissive media using mixtures of simple kernel-based volumetric primitives but achieve lower quality and slower speeds compared to 3DGS. Blanc et al. (2024) enables differentiable ray casting of irregularly distributed Gaussians using a BVH structure, but their rendering is slower than rasterization-based methods. Moenne-Loccoz et al. (2024) performs ray tracing with BVH for secondary lighting effects such as shadows and reflections, but this approach is approximately three times slower than rasterization. Zhou et al. (2024a) proposes a unified rendering primitive based on 3D Gaussian distributions, enabling physically based scattering for accurate global illumination but do not achieve real-time performance. In contrast, our 6DGS method slices 6D Gaussians into conditional 3D Gaussians and renders via rasterization, approximating physically based ray-traced images with high fidelity while achieving real-time performance."}, {"title": "3 PRELIMINARY", "content": null}, {"title": "3.1 6D GAUSSIAN REPRESENTATION", "content": "N-dimensional Gaussian (N-DG) (Diolatzis et al., 2024) extends the 3DGS (Kerbl et al., 2023) approach by introducing a 6D Gaussian representation, which includes both position and directional information. Specifically, each Gaussian is defined by the following parameters: position ($\\mu_p \\in \\mathbb{R}^3$), direction ($\\mu_\\alpha \\in \\mathbb{R}^3$), covariance matrix ($\\Sigma \\in \\mathbb{R}^{6\\times 6}$), opacity ($a \\in \\mathbb{R}^1$), and color ($c \\in \\mathbb{R}^3$).\nThe covariance matrix $\\Sigma$ is modeled as a 6D matrix that includes both spatial and directional variances. To ensure stability and positive definiteness, we utilize a Cholesky decomposition, parameterizing $\\Sigma$ with a lower triangular matrix $L$ as $\\Sigma = LL^T$. Diagonal elements are ensured to be positive using an exponential activation function, while the off-diagonal elements are constrained within [-1, 1] using a sigmoid function."}, {"title": "3.2 SLICE 6D GAUSSIAN TO CONDITIONAL 3DGS", "content": "The slicing Gaussians technique is adopted to render 6D Gaussians efficiently. For a given viewing direction d, we compute a conditional 3D Gaussian that represents the slice of our 6D Gaussian in that direction. Let $X = [X_p, X_d]$ be the 6D Gaussian random variable, where $X_p$ represents the position and $X_d$ represents the direction. The joint distribution is:\n$X \\sim \\mathcal{N}\\left(\\begin{array}{c}\\mu_p \\\\ \\mu_d\\end{array},\\left[\\begin{array}{cc}\\Sigma_p & \\Sigma_{pd} \\\\ \\Sigma_{pd}^T & \\Sigma_d\\end{array}\\right]\\right)$\nwhere $\\Sigma_{pd}$ represents the cross-covariance between position and direction.\nFor rendering, we compute the conditional 3D Gaussian distribution $p(X_p|X_d=d)$, using the properties of multivariate Gaussians, as follows:\n$p(X_p|X_d = d) = \\mathcal{N}(\\mu_{cond}, \\Sigma_{cond})$,\nwhere\n$\\mu_{cond} = \\mu_p + \\Sigma_{pd} \\Sigma_d^{-1}(d - \\mu_d)$,\n$\\Sigma_{cond} = \\Sigma_p - \\Sigma_{pd} \\Sigma_d^{-1} \\Sigma_{pd}^T$.\nThe opacity of each Gaussian also depends on the view direction as:\n$f_{cond} = exp(-(d - \\mu_d)^T \\Sigma_d^{-1} (d - \\mu_d))$,\n$\\alpha_{cond} = \\alpha \\cdot f_{cond}$,\nwhere $f_{cond}$ represents the conditional probability density function (PDF) of the directional component, evaluated at the current viewing direction $d$, and $\\alpha_{cond}$ is the modulated opacity based on $f_{cond}$."}, {"title": "4 METHOD", "content": null}, {"title": "4.1 THEORETICAL ANALYSIS OF CONDITIONAL GAUSSIAN", "content": "This section provides a theoretical analysis of the conditional Gaussian parameters derived from the 6D Gaussian representation, highlighting their physical meanings in Gaussian splatting.\nConditional Mean ($\\mu_{cond}$). The conditional mean $\\mu_{cond}$ can be interpreted as the Best Linear Unbiased Estimator (BLUE) for the position component $X_p$, as in Equation 3. A linear estimator $\\delta$ is BLUE if it is unbiased ($E[\\delta] = \\delta$) and has the smallest variance among all linear unbiased estimators. The expression for $\\mu_{cond}$ given earlier ensures that the position mean is unbiased and minimizes variance according to the Schur complement. The conditional mean $\\mu_{cond}$ represents the expected position of the Gaussian splat in 3D space, adjusting dynamically based on the viewing direction to capture non-planar geometry and parallax effects."}, {"title": "Conditional Covariance ($\\Sigma_{cond}$)", "content": "The conditional covariance $\\Sigma_{cond}$ is derived as the Schur complement of $\\Sigma_d$ in the joint covariance matrix $\\Sigma$, as in Equation 4. This covariance matrix represents the residual uncertainty in $X_p$ after accounting for the correlation with $X_d$. Notably, $\\Sigma_{cond}$ remains constant regardless of the viewing direction $d$. The conditional covariance $\\Sigma_{cond}$ describes the shape and orientation of the Gaussian, encoding the local surface geometry and the uncertainty in $X_p$ after accounting for the correlation with $X_d$."}, {"title": "Conditional Opacity ($\\alpha_{cond}$)", "content": "The function $f_{cond}$ is derived from the conditional probability density function (PDF) of the directional component in the 6D Gaussian. Specifically, $f_{cond}$ represents the likelihood of the viewing direction $d$ aligning with the mean direction $\\mu_d$ given the directional variance $\\Sigma_d$. Equation 5 is a standard form of the exponent in a Gaussian PDF, reflecting the Mahalanobis distance. The opacity $\\alpha_{cond}$ is scaled by $f_{cond}$ in Equation 6 to introduce view-dependency, based on the principle that visibility may vary with viewing direction due to anisotropic properties. By modulating the base opacity $\\alpha$ with $f_{cond}$, we ensure that $\\alpha_{cond}$ dynamically adjusts to reflect the directional characteristics of the Gaussian splat. This approach leverages the mathematical relationship between conditional probabilities and Gaussian distributions, enabling realistic and physically plausible rendering of view-dependent effects.\nTogether, these parameters form a robust framework that integrates view-dependent and view-independent characteristics, enabling efficient and realistic rendering, particularly effective for scenes with varied geometries and materials."}, {"title": "4.2 ENHANCED 6D GAUSSIAN REPRESENTATION", "content": "In N-DG (Diolatzis et al., 2024), the color c is defined by learnable RGB values. To introduce view-dependent effects, we adopt the spherical harmonics representation $\\varphi_\\beta(d) : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$, as used in 3DGS. This representation captures the variation in color based on the viewing direction. The spherical harmonics functions $Y_l^m(d)$ of order l = 3 are parameterized by the coefficients $\\beta \\in \\mathbb{R}^{48}$. The view-dependent spherical harmonics representation can be expressed as:\n$\\varphi_\\beta(d) = f\\left(\\sum_{l=0}^{l_{max}} \\sum_{m=-l}^{l} \\beta_l^m Y_l^m(d)\\right)$,\nwhere f is the sigmoid function used to normalize the colors.\nTo better control the effect of the view direction on opacity, we refine the conditional PDF of the directional component as follows:\n$f_{cond} = exp(-\\lambda_{opa} (d - \\mu_d)^T \\Sigma_d^{-1} (d - \\mu_d))$,"}, {"title": "4.3 IMPROVED CONTROL OF GAUSSIANS", "content": "To enhance the control of Gaussians, we adapt the explicit adaptive control mechanism from 3DGS, leveraging the additional directional information available in our 6D Gaussian representation. Instead of relying on the high-dimensional culling scheme used in N-DG, our approach focuses on refining Gaussian placement and density based on the scene's geometry.\nIn 3DGS, the adaptive Gaussian densification scheme involves two primary operations: cloning and splitting. Cloning is employed when small-scale geometry is insufficiently covered by existing Gaussians, while splitting is used to divide a large Gaussian into smaller ones when it encompasses fine details of the geometry. This scheme requires the scale and rotation of each Gaussian, which are not directly provided in our 6D Gaussian representation.\nTo extract the necessary scale and rotation information from our 6D representation, we utilize the conditional covariance matrix $\\Sigma_{cond}$. By performing Singular Value Decomposition (SVD) on $\\Sigma_{cond}$, we can decompose the matrix into its principal components, revealing both the rotation and scale of the Gaussian. The decomposition is given by $\\Sigma_{cond} = USV^T$, where U and V are orthogonal matrices, and S is a diagonal matrix containing the singular values. From this decomposition: the rotation matrix R is derived from the left singular vectors as $R = U$, and the scale vector s is obtained by taking the square root of the diagonal elements of S as $s = \\sqrt{diag(S)}$. To ensure that the rotation matrix R forms a right-handed coordinate system, we adjust its last column based on the sign of its determinant: $R_{:,3} = R_{:,3} \\cdot sign(det(R))$.\nThis SVD-based decomposition allows us to represent the conditional Gaussian as an oriented ellipsoid, with clearly defined rotation and scale. By extracting these components, we can directly apply the adaptive Gaussian densification scheme from 3DGS (Kerbl et al., 2023), thereby improving the coverage of small-scale geometry and enhancing the overall quality of the rendered scene.\nIn 3DGS, Gaussians are pruned when their opacity falls below a minimum threshold $\\tau_{min}$ (e.g., $\\tau_{min} = 0.005$ by default) or when they become excessively large. In our 6DGS approach, the conditional opacity $\\alpha_{cond}$ is also influenced by the opacity parameter $\\lambda_{opa}$. By incorporating $\\lambda_{opa}$, we can fine-tune the density of Gaussians with greater precision, allowing for more granular control over which Gaussians are retained or pruned based on their opacity."}, {"title": "4.4 COMPATIBILITY WITH 3DGS", "content": "Algorithm 1 outlines the implementation details for converting (i.e., slicing) our 6DGS representation into a 3DGS-compatible format in a single function. Once this slicing operation is performed, the subsequent implementation remains identical to that of 3DGS.\nThe proposed 6DGS seamlessly integrates with the existing training framework of 3DGS, including the use of the same loss functions, optimizers, and training hyperparameters (except for the minimum opacity threshold where we set $\\tau_{min} = 0.01$). By slicing 6DGS into a conditional 3DGS format, we can directly utilize the adaptive density control and the differentiable rasterization method employed in 3DGS. This compatibility ensures that many downstream applications can effortlessly switch from 3DGS to our 6DGS, resulting in enhanced performance without the need for extensive modifications.\nNote that the scale s and rotation R are only required during refinement iterations (e.g., every 100 iterations) for the adaptive density control. Additionally, during inference, $\\Sigma_{cond}$ can be pre-computed. Therefore, only $\\mu_{cond}$ and $\\alpha_{cond}$ need to be computed for each rendering. To further enhance rendering efficiency, the slice operation can be implemented in CUDA."}, {"title": "5 EXPERIMENTS", "content": null}, {"title": "5.1 EXPERIMENTAL PROTOCOL", "content": "Datasets. We evaluate 6DGS on two datasets: the public Synthetic NeRF dataset (Mildenhall et al., 2020) and a custom dataset rendered using physically-based ray tracing (PBRT), which we refer to as the 6DGS-PBRT dataset. The 6DGS-PBRT dataset consists of six scenes: 1) cloud from the Walt Disney Animation Studios volumetric cloud dataset; 2) bunny-cloud, explosion, and smoke from OpenVDB volumetric models; 3) suzanne, the standard Blender test mesh, with a \"Glasss BSDF\" translucent material applied to it; 4) ct-scan, prepared from a real CT scan.\nWe rendered these scenes in Blender using its PBRT engine \"Cycle\". At the image sizes (with width equal to height) as listed in Table 2 on an NVIDIA Tesla V100 GPU, the rendering times per view were as follows: bunny-cloud took 504.0 seconds per view, cloud 838.6 seconds, explosion 35.5 seconds, smoke 71.5 seconds, suzanne 9.1 seconds, and ct-scan 28.5 seconds. For the ct-scan object, we generated 360 views with corresponding camera poses and randomly selected 324 images for training and 36 for testing. For each of the other objects, we rendered 150 views, randomly selecting 100 images for training and 50 for testing. We will make our 6DGS-PBRT dataset publicly available to the community.\nEvaluation Metrics. We evaluate our method's performance using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) for image quality, number of Gaussian points (# point) for size, and Frames Per Second (FPS) for rendering speed.\nImplementation. In our experiments, we set $\\lambda_{opa} = 0.35$ and the minimum opacity threshold $\\tau = 0.01$. For learnable $\\lambda_{opa}$, we initialize $\\lambda_{opa} = 0.35$ and make it trainable only during the iterations of 150,000 - 280,000. All other parameters are set to their default values as in 3DGS (Kerbl et al., 2023). For the ct-scan object, we initialize the point cloud using the marching cubes algorithm as in DDGS (Gao et al., 2024). For the other objects and the Synthetic NeRF dataset (Mildenhall et al., 2020), we randomly initialize the point cloud with 100,000 points within a cube encompassing the scene. Training is performed on a single NVIDIA Tesla V100 GPU with 16 GB of memory, using the Adam optimizer (Kingma & Ba, 2014). We set the learning rate to $1 \\times 10^{-2}$ for the 6D covariance parameters and $1 \\times 10^{-3}$ for the direction component $\\mu_d$. The default learning rates from 3DGS are applied to the remaining parameters."}, {"title": "5.2 COMPARISON WITH STATE-OF-THE-ART", "content": "Table 1 compares our 6DGS method with 3DGS and N-DG on the 6DGS-PBRT dataset. Our 6DGS achieves significantly better image quality, with an average improvement of +10.08 dB in PSNR,"}, {"title": "5.3 ABLATION STUDY", "content": "We conduct ablation experiments on both the 6DGS-PBRT dataset and Synthetic NeRF dataset (see Appendix Table 5) to investigate the effects of various components in our 6DGS method. Specifically, we examine the impact of the parameter $\\lambda_{opa}$ in the conditional probability density function (PDF) of the directional component, defined as $f_{cond} = exp(-\\lambda_{opa} \\cdot D)$, where $D = (d - \\mu_d)^T \\Sigma_d^{-1} (d - \\mu_d)$ represents the Mahalanobis distance between the viewing direction d and the Gaussian mean direction $\\mu_\\alpha$, and $\\lambda_{opa}$ is a scalar between 0 and 1 that controls the influence of the view direction on opacity. Our ablation experiments include several settings related to $\\lambda_{opa}$:\n*   No-fcond: Setting $\\lambda_{opa} = 0$ results in $f_{cond} = exp(0) = 1$, meaning opacity is not modulated by the view direction.\n*   No-$\\lambda_{opa}$: Setting $\\lambda_{opa} = 1$ uses the default $f_{cond}$ as in N-DG, i.e., $f_{cond} = exp(-D)$.\n*   $\\lambda_{opa} = 0.35$: We set $\\lambda_{opa} = 0.35$, so $f_{cond} = exp(-0.35 \\cdot D)$, providing a balance between no modulation and full modulation."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce 6D Gaussian splatting (6DGS) that builds on the foundations laid by 3DGS and N-dimensional Gaussians (N-DG). By improving the handling of color and opacity within the 6D spatial-angular framework and optimizing the adaptive control of Gaussians using additional directional information, we develop a method that not only maintains compatibility with the 3DGS framework but also offers superior rendering capabilities, particularly in modeling complex view-dependent effects. Our extensive experiments on the custom physically-based ray tracing dataset (6DGS-PBRT) demonstrate the effectiveness of 6DGS in achieving higher image quality and faster rendering speed compared to 3DGS and N-DG.\nLooking ahead, 6DGS opens avenues for more accurate and efficient real-time volumetric rendering in virtual and augmented reality, gaming, and film production. Future research will explore further optimizations and extensions to enhance the scalability and robustness of the 6DGS framework, as well as its application to dynamic scenes and integration with advanced lighting models."}, {"title": "A APPENDIX", "content": "Figure 4 illustrates the effect of adjusting the parameter $\\lambda_{opa}$ on the conditional probability density function (PDF) $f_{cond}$ of the directional component, plotted as a function of the Mahalanobis distance D between the viewing direction d and the Gaussian mean direction $\\mu_d$. The Mahalanobis distance is defined as:\n$D = (d - \\mu_d)^T \\Sigma_d^{-1} (d - \\mu_d)$,\nand the conditional PDF is given by:\n$f_{cond} = exp(-\\lambda_{opa} \\cdot D)$.\nWhen $\\lambda_{opa} = 1$, the Mahalanobis distance D has the maximum influence on the opacity, meaning that even small deviations in the viewing direction d from the Gaussian mean direction $\\mu_d$ significantly reduce $f_{cond}$. Conversely, when $\\lambda_{opa} = 0$, the opacity remains unaffected by the Mahalanobis distance D, resulting in a constant $f_{cond} = 1$ regardless of the viewing direction. When $\\lambda_{opa} = 0.35$, the Mahalanobis distance D has a moderate influence on the opacity, providing a balance between sensitivity to the viewing direction and maintaining opacity over a wider range of directions.\nTable 5 presents the results of our ablation studies on the Synthetic NeRF dataset (Mildenhall et al., 2020). Since this dataset lacks strong view-dependent effects, the influence of $\\lambda_{opa}$ is less significant compared to the 6DGS-PBRT dataset (see Table 4). Compared to our model with $\\lambda_{opa} = 0.35$, setting No-fcond (i.e., $\\lambda_{opa} = 0$) slightly degrades image quality and increases the number of Gaussian points. Conversely, when we set No-$\\lambda_{opa}$ (i.e., $\\lambda_{opa} = 1$), both the image quality and the number of Gaussians remain comparable. Notably, when we make $\\lambda_{opa}$ a learnable parameter, the model achieves improved image quality with a comparable number of Gaussian points. Furthermore, the No-SH setting results"}]}