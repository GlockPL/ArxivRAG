{"title": "Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic Segmentation", "authors": ["Jintao Tong", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "abstract": "Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train the model on a large-scale source-domain dataset, and then transfer the model to data-scarce target-domain datasets for pixel-level segmentation. The significant domain gap between the source and target datasets leads to a sharp decline in the performance of existing few-shot segmentation (FSS) methods in cross-domain scenarios. In this work, we discover an intriguing phenomenon: simply filtering different frequency components for target domains can lead to a significant performance improvement, sometimes even as high as 14% mIoU. Then, we delve into this phenomenon for an interpretation, and find such improvements stem from the reduced inter-channel correlation in feature maps, which benefits CD-FSS with enhanced robustness against domain gaps and larger activated regions for segmentation. Based on this, we propose a lightweight frequency masker, which further reduces channel correlations by an amplitude-phase-masker (APM) module and an Adaptive Channel Phase Attention (ACPA) module. Notably, APM introduces only 0.01% additional parameters but improves the average performance by over 10%, and ACPA imports only 2.5% parameters but further improves the performance by over 1.5%, which significantly surpasses the state-of-the-art CD-FSS methods.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in semantic segmentation have been driven by large-scale annotated datasets and developments in deep neural networks [8, 29, 50, 44]. Nevertheless, the requirement for extensive labeled data remains a significant challenge, particularly for dense prediction tasks like semantic segmentation. Hence, few-shot semantic segmentation (FSS) [35, 11, 49] has been proposed to meet this challenge, aiming to produce predictions for the unseen categories with only limited annotated data. However, these FSS methods perform poorly when confronted with domain shifts, particularly when there is a significant gap between the novel class (target domain) and the base class (source domain). This issue has spurred the development of the cross-domain few-shot semantic segmentation (CD-FSS) task [24]. Despite various efforts in CD-FSS, the outcomes remain sub-optimal.\nTo handle the domain shift problem, efforts have been made to study the generalization of neural networks. Recently, some works [42, 40, 7] have explored this from the perspective of the frequency domain, achieving theoretical breakthroughs. Compared to humans, neural networks exhibit heightened sensitivity to different frequency components. Additionally, amplitude and phase exhibit distinct properties and effects on neural network performance. Inspired by these works, we study the domain shift problem from the perspective of the frequency domain and discover an intriguing phenomenon shown in Figure 1: for a model already trained on the source domain, simply filtering frequency components of images during testing can lead to significant performance improvements, sometimes even as high as 14% mIoU."}, {"title": "2 Interpreting Enhanced Performance from Frequency Filtering", "content": "In this section, we delve into why filtering certain frequency components can significantly improve CD-FSS performance in certain target domains for interpretation."}, {"title": "2.1 Preliminaries", "content": "Cross-domain few-shot semantic segmentation (CD-FSS) aims to generalize knowledge acquired from source domains with ample training labels to unseen target domains. Given a source domain Ds = (Xs, Vs) and a target domain Dt = (Xt, Yt), where X represents input data distribution and Y represents label space. The model will be trained on the training set from the Ds, then applied to perform segmentation on novel classes in the Dt. Notably, Ds and Dt exhibit distinct input data distribution, with their respective label spaces having no intersection, i.e., Xs \u2260 Xt, Vs \u2229 Yt = \u2205.\nIn this work, we adopt the episodic training manner. Specifically, both the training set sampled from Ds and the testing set sampled from Dt are composed of several episodes, each episode is constructed"}, {"title": "2.2 Enhanced Performance Stem from Reduced Inter-Channel Correlation", "content": "Existing research indicates inter-channel relationships are crucial for performance, as different feature channels can represent distinct features [3, 30]. Therefore, we study the change of channel correlations brought by the frequency filtering. Specifically, we measured the mean mutual information (MI) [4] between channels from the last layer of the backbone network. The measured cases include the combination of phase and amplitude for the highest and lowest performance in Fig. 1. We report the 1-shot mIoU and MI in Tab. 1, where we can see that for frequency combinations with improved mIoU, their MI consistently decreases, indicating reduced inter-channel correlation in the feature maps. Conversely, for frequency combinations with decreased mIoU, their MI consistently increases.\nThe higher the mutual information value, the larger the correlation between channels, while a low MI indicates more independent semantic information captured by different channels. Therefore, the experimental results demonstrate that improved performance is associated with decreased inter-channel correlation in the feature map."}, {"title": "2.3 Why Lower Inter-Channel Correlation is Better?", "content": "The reduced inter-channel correlation benefits our model in two aspects:\n(1) Cross-domain generalization. Previous works [2, 51] indicate that a lower correlation between features implies reduced redundancy and enhanced generalizability. Intuitively, a lower inter-channel correlation demonstrates that channels capture patterns more independently, therefore each channel will capture patterns in the input image more uniformly, which means the mean magnitude of each channel across all images will be more uniform. Consistent with our intuition, [30] shows the channel bias problem affects the generalizability of few-shot methods, and it utilizes the Mean Magnitude of Channels (MMC) to visualize and measure the channel response in features, where effective few-shot methods might have a more uniform MMC curve in the testing set. Therefore, this means the reduced correlation also benefits our model by addressing the channel bias problem, as studied in [30].\nInspired by this, we visualize the MMC before and after applying the mask to filter frequency components on four target datasets. As shown in Figure 2, for FSS-1000, performance degrades after masking, with the curve steeper than the baseline. Conversely, for the other three target datasets, performance improves with the curve more uniform than the baseline after masking. This indicates the channel bias problem is also handled by frequency filtering, which benefits the model with more independent and diverse semantic patterns to represent target domains.\n(2) Exploring larger activated regions for segmentation. To study why reducing inter-channel correlation through frequency filtering benefits the segmentation task, we visualize the heatmap of feature maps before and after filtering out specific frequency components. As shown in Figure 3(a), after filtering certain frequency components, the heatmap demonstrates expanded activation regions,"}, {"title": "2.4 Why Feature Disentanglement in the Frequency Domain?", "content": "In this subsection, we illustrate why the feature disentanglement is carried in the frequency domain.\nFourier Transform (FT) FT transforms finite signals into complex-valued functions of frequency. For a single channel f \u2208 Rh\u00d7w of the feature map, the Fourier transform is formulated as:\nF(u, v) = \\frac{1}{w h} \\sum_{x=0}^{w-1} \\sum_{y=0}^{h-1} f(x,y)e^{-i2\\pi(\\frac{ux}{w} + \\frac{vy}{h})}  (1)\nwhere i is imaginary unit, h and w are the height and the width of f. f(x, y) is an element of f at spatial pixel (x, y), and F(u, v) represents the Fourier coefficient at frequency component (u, v).\nThe process in which the spatial feature f is decomposed into the amplitude a and phase p is called spectral decomposition. The corresponding frequency feature F can be reassembled from amplitude a and phase p (|F| is the modulus of F):\nF = a \\cos(p) + i a \\sin(p) = a \\cdot e^{ip} (2)\n|F| = \\sqrt{ a^2(\\cos^2(p) + \\sin^2(p))} = \\sqrt{a^2} = \\alpha (3)\nMathematical Derivation. To intuitively prove the correlation between phase differences and channel correlation within a feature map. For different channels of a feature map, representing distinct features, F1(m, n) = a1e^{p1} and F2(m,n) = a2e^{p2} are defined as the frequency domain representations of the same location in different channels. The correlation coefficient is calculated based on the following formula in the frequency domain:\nr=\\sum_{m=0}^{h-1} \\sum_{n=0}^{w-1} \\frac{F_{1}(m, n)F_{2}^{*}(m, n)}{\\sqrt{|F_{1}(m,n)|^{2}|F_{2}(m, n)|^{2}}} = \\sum_{m=0}^{h-1} \\sum_{n=0}^{w-1}r(m,n) (4)\nThe F_{2}^{*}(m, n) is the complex conjugate of F2(m, n), can be computed as:\nF_{2}^{*}(m, n) = a2 \\cos(p2) \u2013 ia2 \\sin(p2) = a2e^{-ip2} (5)\nSubstituting equations (2), (3), and (5) into equation (4) yields:\nF_{1}(m, n)F_{2}^{*}(m, n) = \\alpha_{1}\\alpha_{2}e^{i(p_{1}-p_{2})} (6)\n|F_{1}(m, n)|^{2}|F_{2}(m, n)|^{2} = a_{1}^{2}a_{2}^{2} (7)\nfrom which we can further derive:\nr(m,n) = \\frac{\\alpha_{1}\\alpha_{2}e^{i(p_{1}-p_{2})}}{\\sqrt{a_{1}^{2}a_{2}^{2}}} = e^{i(p_{1}-p_{2})}, p_{1} - p_{2} = \\Delta p \\in [0, \\pi] (8)\nAccording to Euler's formula, we know that e^{i\\pi} = \u22121 and ei0 = 1. From this derivation, we have proved that the correlation between features in the spatial domain can be translated into phase differences and amplitudes in the frequency domain. When the frequency components are identical, the following can be inferred: 1) \u0394\u03c1 = 0, r(m,n) = 1 indicates perfect positive correlation; 2) \u0394\u03c1 = \u03c0, r(m,n) = \u22121 indicates a perfect negative correlation. Therefore, when the phase differences of more corresponding points from different channels in the frequency domain aggregate around 0 or \u03c0, there is a higher correlation between the channels. For amplitude, when the phase differences are the same, the closer the amplitudes, the more similar the waveforms are, thus indicating a higher correlation between the channels."}, {"title": "2.5 Conclusion and Discussion", "content": "Through mathematical derivation and experiments, we demonstrate that manipulating the frequency can reduce inter-channel correlation in feature maps. Each channel of the feature map represents a distinct pattern, and a lower inter-channel correlation implies a higher degree of channel disentangle ment, leading to more independent and diverse semantic patterns for each feature. This benefits the model with 1) alleviation of channel bias, boosting model robustness on target domains; and 2) larger activated regions for segmentation, therefore a simple frequency filtering operation can significantly improve performance for the CD-FSS task.\nBased on the above analysis, we can draw the following insights: 1) The aforementioned mask operates at the input level, but fundamentally affects the feature map\u2019s channel correlation. Therefore, we can directly apply mask operations to the frequency domain of each channel in the feature map; 2) Different domains require filtering different components. The aforementioned mask manually filters different frequencies based on the target domain, but the mask can be made adaptive; 3) The aforementioned mask does not perform well on FSS-1000. We believe this may be due to the overly coarse high-low frequency division. A finer frequency division can be designed, dividing the frequency into h \u00d7 w parts (where h and w are the spatial dimensions of the feature map)."}, {"title": "3 Method", "content": "Our method consists of two major steps, i.e., 1) amplitude-phase masker is proposed to reduce feature correlation, and obtain more accurate and generalized feature maps; 2) adaptive channel phase attention is proposed to select features that benefit the current instance and align the feature spaces of support and query. Our modules do not require source-domain training and can be directly integrated into during target-domain fine-tuning. The overall framework of our approach is shown in Figure 4."}, {"title": "3.1 Amplitude-Phase Masker", "content": "Amplitude-Phase Masker(APM) is a model-agnostic module that filters out negative frequency components at the feature level within feature maps. Through mathematical derivation, it is shown that APM accomplishes feature disentanglement. Consequently, this leads to a feature map that is more robust, generalizable, and provides broader and more accurate representations.\nIn our work, we utilize a fixed encoder, trained on the source domain, to extract feature maps F \u2208 Rc\u00d7h\u00d7w, where c, h and w represent channels, height, and width. We then apply the Fast"}, {"title": "3.2 Adaptive Channel Phase Attention", "content": "Adaptive Channel Phase Attention(ACPA) can be seen as a process of feature selection. Building on the APM-optimized feature map, ACPA encourages the model to focus on more effective channels (features) while aligning the feature spaces of the support and query. Its underlying insight is that amplitude and phase are considered as style and content, respectively. Therefore, the phase can be seen as an invariant representation, with consistent phase elements across both support and query.\nFor the enhanced support feature maps Fsupenh, the mask average pooling(MAP) is performed first:\nFsupmap = MAP(Fsupenh)  (13)\nSubsequent operations input to ACPA are consistent with those applied to the query feature maps. Therefore, we uniformly refer to the feature map fed into ACPA as Fenh. We adopt a SE block following SENet [19] as our channel attention module, denoted as SE:\nWphase = SE(Penh) (14)"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "We utilize the benchmark established by PATNet [24] and adopt the same data preprocessing methods. For training, our source domain is the PASCAL-5i dataset [35], an extended version of PASCAL VOC 2012 [13] enhanced with additional annotations from the SDS dataset. For evaluation, our target domains include FSS-1000 [26], Deepglobe [10], ISIC2018 [9, 38], and the Chest X-ray datasets [6, 20]. See Appendix A.1 for more details about datasets."}, {"title": "4.2 Implementation Details", "content": "We employ ResNet-50 [18] as our encoder, initialized with weights pre-trained on ImageNet [34]. The training manner is consistent with our baseline model HSNet [31]. To optimize memory usage and speed up training, the spatial sizes of both support and query images are set to 400 \u00d7 400. The model is trained using the Adam [21] optimizer with a learning rate of 1e-3.\nDuring the adaptation stage, the model initially predicts the support mask and then uses the corre sponding label to optimize the APM and ACPA through CE loss. The adaption stage of the APM and the ACPA leverage feature maps from conv5_x whose channel dimensions are 2048 and spatial size is 13\u00d713, is performed using the Adam optimizer, with learning rates set at 0.1 for Chest X-ray, 0.01 for FSS-1000 and ISIC, and 1e-5 for Deepglobe. Each task undergoes a total of 60 iterations."}, {"title": "4.3 Comparison with State-of-the-Art Works", "content": "In Table 2, we compare our method with several state-of-the-art few-shot semantic segmentation approaches on the benchmark introduced by PATNet [24]. Our results show a significant improvement in cross-domain semantic segmentation for both 1-shot and 5-shot tasks. Specifically, APM-S exceeds the performance of the state-of-the-art PATNet, based on ResNet-50, by 2.87% and 0.87% in average"}, {"title": "4.4 Ablation Study", "content": "Effectiveness of each module. We evaluated each proposed module in both 1-shot and 5-shot settings to assess their effectiveness. As detailed in Table 3, introducing APM-S increased the average mIoU by 10.09% for 1-shot and 11.29% for 5-shot. Adding ACPA further enhanced the mIoU by 1.16% and 1.47%, respectively. Additionally, APM-M, a variant of APM with more parameters, when combined with ACPA, increased the average mIoU by 12.35% for 1-shot and 15.08% for 5-shot.\nModel-agnostic method. We implemented our method for FPTrans [47], which employs the Vision Transformer (ViT) [12] as its encoder. As shown in Table 4, our approach can also effectively enhance the performance of models based on the transformer architecture. Moreover, the new large-scale SAM [22] model has significantly advanced image segmentation, showcasing impressive zero-shot capabilities. However, SAM is not suited for cross-domain few-shot segmentation. Thus, we evaluate PerSAM [48] to compare our method with the SAM-based approach. The result shows that our method performs much better than PerSAM in cross-domain few-shot segmentation.\nComparison with other method. To demonstrate the effectiveness of our method, we compared it with full parameter fine-tuning and feature disentangling method, as shown in Table 5. Compared to full parameter fine-tuning, our method uses fewer parameters and achieves better performance. For spatial domain feature disentangling, we added a mutual information loss to the baseline model during training to encourage each channel of the feature map to learn independent representations. Our method significantly outperforms this approach. Intuitively, feature disentanglement in the frequency domain offers finer granularity and global representation, which is more beneficial for segmentation tasks compared to the local representation in the spatial domain."}, {"title": "4.5 APM: Feature Disentanglement via Frequency Operations", "content": "Reduce inter-channel correlation. As shown in Table 6, we validated APM's ability to reduce inter-feature correlation and improve generalization performance by calculating the mutual information between feature map channels. Compared to Table 1, this result demonstrates that the adaptive feature-level approach is more effective than the input-level masking, further reducing inter-feature correlation. Furthermore, we plotted the cumulative distribution function (CDF) of inter-channel correlations in the feature maps, as shown in Figure 6. It can be observed that with the inclusion of APM, the CDF curve shifts to the left, indicating a decrease in inter-channel correlations."}, {"title": "4.6 ACPA: Aligning Task-Relevant Features and Feature Spaces", "content": "ACPA can be seen as feature selection, enabling the model to focus on features that are more effective for the current task while aligning the feature spaces of the support and query feature maps. As shown in Table 7 (Left), after APM disentangles the features and produces a more broadly represented feature map, ACPA selects features that are more effective and discriminative for the current task. For example, in the first row, ACPA selects the swan's wings and head. In the second column, it selects the bird's head, tail, and feet. Furthermore, we measured the CKA (Centered Kernel Alignment) to calculate the distance between the support feature map and the query feature map, validating that ACPA aligns the support and query feature spaces. CKA is proposed to measure both intra-domain and inter-domain distances [52]; the smaller the CKA value, the closer the feature spaces. As shown in Table 7 (Right), after applying APM, the distance between support and query is reduced, and with the addition of ACPA, the support and query feature spaces are further aligned."}, {"title": "4.7 Comparison with Domain Transfer Methods", "content": "We compare our method against traditional frequency-based and correlation-based approaches to validate our method's effectiveness. For a fair comparison, all methods are implemented on the baseline HSNet [31] and then evaluated under the 1-shot setting on the CD-FSS benchmark.\nFrequency-based method DFF [27] preserves frequency information beneficial for generalization. GFNet [32] replaces self-attention layer with global frequency filter layer. ARP [7] introduces Amplitude-Phase Recombination via amplitude transformation. DAC [23] proposes a normalization method that removes style (amplitude) while preserving content (phase) through spectral decompo sition. Although these methods improve generalization, they fall short in addressing large domain gaps. Our method requires no source domain training. It adaptively masks harmful components for the target domain at the feature level. By treating amplitude and phase separately, we exploit phase invariance to design a channel attention module that handles intra-class variations. As shown in Table 8 our method outperforms existing frequency-based approaches on the CD-FSS task."}, {"title": "5 Related Work", "content": "Few-shot learning Few-shot learning aims to build robust representations for new concepts with limited annotated examples. Existing approaches typically fall into three categories: metric learning [36, 39], optimization-based methods [14, 33] and graph-based methods [15, 28]. Recently, cross domain few-shot learning has gained attention due to disparities in both data distribution and label space between meta-testing and meta-training stages. BSCD-FSL [16] introduces a challenging benchmark for cross-domain few-shot learning, featuring a substantial domain gap between the source and target domains. It covers several target domains with varying similarities to natural images.\nFew-shot semantic segmentation Few-shot semantic segmentation aims to segment unseen classes in query images with only a few annotated samples. OSLSM [35] is the first two-branch FSS model. Following this, PL [11] introduces a prototype learning paradigm utilizing cosine similarity between pixels and prototypes. SG-One [49] adopts masked average pooling (MAP) to optimize the extraction of support features. Recently, many FSS methods have emerged in the research community, such as RPMMs [43], PFENet [37], ASGNet [25], and HSNet [31]. HSNet employs efficient 4D convolutions on multi-level feature correlations, serving as the baseline for our work. However, these methods primarily address segmenting novel classes within the same domain and struggle with generalization across disparate domains due to significant feature distribution disparities. Bridging this substantial domain gap, particularly with limited labeled data, remains a formidable challenge."}, {"title": "6 Conclusion", "content": "In this paper, we delve into the phenomenon that filtering specific frequency components based on different domains significantly improves performance, providing an interpretation through experiment and mathematical derivation. Building on our interpretation, we propose the APM, a feature-level frequency component mask designed to enhance the generalization of feature map representations. Further, we introduced ACPA. Based on the APM-optimized feature map, the ACPA encourages the model to focus on more effective features while aligning the feature spaces of the support and query. Experimental results demonstrate the approach\u2019s effectiveness in reducing domain gaps."}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 More Dataset Details", "content": "Our experimental setup is grounded in the benchmark established by PATNet [24]. Fig. 7 presents an example of segmentation for four target datasets. Further details follow:\nPASCAL-52 [35] extends the PASCAL VOC 2012 [13] by integrating additional annotations from the SDS dataset [17]. Utilizing PASCAL-5\u00b2 as our source domain for training, we then evaluate the models' performance across four target datasets.\nFSS-1000 [26] is a few-shot segmentation dataset comprising 1000 natural image categories, with each category containing 10 samples. In our experiment, we adhere to the official split for semantic segmentation and report results on the designated testing set, which encompasses 240 classes and 2,400 images. We consider FSS-1000 as our designated target domain for testing.\nDeepglobe [10] consists of satellite images annotated densely at the pixel level across 7 categories: urban, agriculture, rangeland, forest, water, barren, and unknown. Since ground-truth labels are available only in the training set, we utilize the official training dataset comprising 803 images to showcase our results. We designate it as our testing target domain and follow the same processing approach as PATNet.\nISIC2018 [9, 38] is designed specifically for skin cancer screening and comprises images of lesions, with each image depicting exactly one primary lesion. Adhering to the guidelines established by PATNet, we process and utilize the dataset, considering ISIC2018 as our target domain for testing.\nChest X-ray [6, 20] is tailored for Tuberculosis diagnosis, comprising 566 images with a resolution of 4020 x 4892 pixels. These images depict cases from 58 Tuberculosis patients and 80 individuals with normal conditions. A common approach to handling large image sizes involves resizing them to 1024 x 1024 pixels."}, {"title": "A.2 APM Reduces Inter-Channel Correlation by Frequency Domain Mask", "content": "As shown in Figure 8, we present histograms of the phase differences between channels in the feature maps (weighted by amplitude) before and after adding the APM module. After applying APM, the phase differences between channels concentrated around 0 and \u3160 are reduced, which aligns with our mathematical derivation in the main text. The APM decreases the correlation between feature map channels by altering phase and amplitude, thereby enhancing the independence of their semantic representations. Additionally, due to the use of finer-grained frequency domain partitioning, APM performs well on FSS-1000 compared to the simple high-low frequency partitioning at the input level."}, {"title": "A.3 Analyze the Frequency Components Filtered by the APM", "content": "The visualization results in Figure 9 show the average frequency components filtered by the amplitude masker and phase masker across different domains. The center represents low frequencies, while the periphery represents high frequencies. White indicates a value of 1, meaning the frequency component passes through, and black indicates a value of 0, meaning the frequency component is filtered out. For FSS, the amplitude masker (AM) primarily filters out mid-to-high-frequency components, while the phase masker (PM) mainly retains mid-frequency components. For DeepGlobe, both AM and PM retain more mid-to-high frequencies. For ISIC, AM filters out more mid-to-high frequencies, retaining low frequencies, whereas PM retains relatively more mid frequencies. For ChestX, AM mainly retains low-frequency components, while PM filters out frequencies across the spectrum, retaining relatively more low-to-mid frequencies. These results align well with the patterns observed in Figure 1 of our main text. It is evident that for different targets, AM and PM dynamically and adaptively filter different frequency components, selecting those more beneficial for the current domain. Additionally, the advantageous amplitude and phase frequency components vary across different target domains, underscoring the necessity of considering amplitude and phase separately."}, {"title": "A.4 Detailed Ablation Study Results", "content": "In the main text, we demonstrate the effectiveness of our various designs by presenting the average mIoU. Here, as shown in Table 10, we provide detailed results on each target dataset."}, {"title": "A.5 Broader Impact", "content": "Our research delves into the phenomenon that filtering specific frequency components based on different domains significantly improves performance, providing an interpretation. Furthermore, we demonstrated the relationship between frequency components and inter-channel correlation through mathematical derivation. Building on our interpretation and derivation, we propose the APM, a feature-level frequency component mask designed to enhance the generalization of feature map representations. Further, we introduced Adaptive Channel Phase Attention (ACPA). Based on the APM-optimized feature map, the ACPA encourages the model to focus on more effective features while aligning the feature spaces of the support and query. Experimental results demonstrate the effectiveness of our approach significantly enhances the model\u2019s cross-domain transferability. This work is applicable not only to CDFSS but also to other areas like domain generalization and domain adaptation. Future research will aim to broaden our evaluations to encompass a wider range of target domains, enhancing our understanding of their performance in various real-world scenarios."}]}