{"title": "TimeCNN: Refining Cross-Variable Interaction on Time Point for Time Series Forecasting", "authors": ["Ao Hu", "Dongkai Wang", "Yong Dai", "Shiyi Qi", "Liangjian Wen", "Jun Wang", "Zhi Chen", "Xun Zhou", "Zenglin Xu", "Jiang Duan"], "abstract": "Time series forecasting is extensively applied across diverse domains. Transformer-based models demonstrate significant potential in modeling cross-time and cross-variable interaction. However, we notice that the cross-variable correlation of multivariate time series demonstrates multifaceted (positive and negative correlations) and dynamic progression over time, which is not well captured by existing Transformer-based models. To address this issue, we propose a TimeCNN model to refine cross-variable interactions to enhance time series forecasting. Its key innovation is timepoint-independent, where each time point has an independent convolution kernel, allowing each time point to have its independent model to capture relationships among variables. This approach effectively handles both positive and negative correlations and adapts to the evolving nature of variable relationships over time. Extensive experiments conducted on 12 real-world datasets demonstrate that TimeCNN consistently outperforms state-of-the-art models. Notably, our model achieves significant reductions in computational requirements (approximately 60.46%) and parameter count (about 57.50%), while delivering inference speeds 3 to 4 times faster than the benchmark i Transformer model.", "sections": [{"title": "1 INTRODUCTION", "content": "TIME series forecasting is extensively applied across various fields, including traffic [7, 21], finance [17, 29], weather [15, 6], and energy consumption [4], etc. Deep learn-ing [27, 13, 24, 14] methods have exhibited exceptional perfor-mance in time series forecasting. Among them, Transformer-based models [11, 22, 30, 18, 25, 9] demonstrate significant potential in modeling long-term temporal dependency for cross-time interaction.\nIn addition to cross-time interaction, recent research increasingly underscores the significance of cross-variable interaction to achieve accurate time series forecasting. Some prior works [26, 24] have leveraged Graph Neural Networks (GNNs) to explicitly capture cross-variable dependency. Be-sides, Informer [11], Autoformer [22], and FEDformer [30] only implicitly utilize this dependency. These Transformer-based models embed multiple variates of the same timestamp as temporal tokens to capture the dependencies among time points. This interaction of time-unaligned tokens from different multivariate introduces unnecessary noise into forecasting processes. Hence, cross-variable dependency is not captured well. To explicitly capture multivariate correlations, Crossformer [28] introduces two-stage atten-tion on variables and segments of time points to enhance prediction performance. Recently, iTransformer [14] embeds independent time series as tokens to capture multivariate correlations by self-attention, thereby achieving state-of-the-art (SOTA) performance.\nHowever, iTransformer overlooks the intrinsic dynamic nature of variable relationships over time, as it encodes inde-pendent entire time series into variable tokens and captures multivariate correlations among these tokens. As shown in Figure 1, it is notable that the cross-variable correlation of multivariate time series demonstrates a multifaceted and dynamic progression over time. This phenomenon reflects the complex and evolving nature of the underlying processes and interactions among the variables. Attention among variable tokens in iTransformer cannot portray complex and dynamic multivariate correlation. Consequently, this constraint reduces its capacity and generalization ability on diverse time series data.\nAdditionally, negative correlations are observed in Fig-ure 1, indicated by a shift from red to blue. We further illustrate negative correlations in Figure 2. The purple dashed box indicates that the two variables are positively correlated, while the red dashed box denotes the negative correlation. We notice the variable relationships as they dynamically change over time, exhibiting instances of both positive and negative correlation. More notably, it demonstrates that the relationship of variables between adjacent time points may also change. In iTransformer, the learned pre-Softmax maps of the self-attention module also demonstrate similar negative correlations among variables of the entire time series. However, negative correlations are transformed into particularly small similarities through the softmax function. Consequently, the attention mechanism cannot utilize negative correlations to improve time forecasting.\nBuilding on the preceding analysis, refining cross-variable interactions for complex dynamic multivariate correlations remains a challenging task. To mitigate this issue, we propose a TimeCNN model that contains the key com-ponent: Timepoint-independent. Each time point has an independent convolution kernel. Specifically, it utilizes a large convolution kernel to capture the relationships within all variables at each time point. In this manner, TimeCNN is capable of dynamically capturing the relationships among variables, even when those of adjacent time points differ. Furthermore, the convolution of all variables at each time point can capture both positive and negative correlations. A comparison between the iTransformer and TimeCNN is illustrated in Figure 3. The iTransformer leverages the self-attention mechanism to capture cross-variable interactions among variable tokens. However, it overlooks the dynamic nature of these relationships. In contrast, our TimeCNN captures cross-variable interactions at each individual time point.\nAfter cross-variable interaction, TimeCNN embeds the complete time series for each variable and utilizes a feed-forward network to learn time series representation. Experi-mentally, the proposed TimeCNN demonstrates state-of-the-art performance on real-world forecasting benchmarks, as illustrated in Figure 4. Remarkably, it tackles the primary challenges associated with refining cross-variable interactions in the context of complex dynamic multivariate correlations. Moreover, compared to iTransformer, TimeCNN achieves a reduction in computational demand by approximately 60.46% and in parameter conunt by 57.50%, while enhancing inference speed 3 to 4 times faster. This efficiency can be attributed to the parallel computation of convolutions across all time points. The main contributions are as follows:\n\u2022 We observe that Transformer-based models struggle to effectively capture complex dynamic multivariate correlations in time series forecasting. This limits their generalization capabilities when applied to various time-series datasets.\n\u2022 We propose TimeCNN to refine cross-variable interac-tions for complex dynamic multivariate correlations to enhance time series forecasting. Its key component is timepoint-independent, where each time point has an independent convolution kernel.\n\u2022 Extensive experimental results on 12 real-world datasets demonstrate that our model outperforms state-of-the-art models. Moreover, compared to iTrans-former, TimeCNN achieves a reduction in computa-tional demand by approximately 60.46% and param-eter conunt by 57.50%, while delivering inference speeds 3 to 4 times faster."}, {"title": "2 RELATED WORK", "content": "Deep learning models have demonstrated remarkable perfor-mance in time series forecasting. These models can be broadly divided into Transformer-based models [20, 22, 30, 18, 28, 14], MLP-based models [27, 3, 12], GNN-based models, and CNN-based models [13, 23, 16]. These models primarily enhance predictive accuracy by modeling temporal dimensions (cross-time) and variable dimensions (cross-variable)."}, {"title": "2.1 Cross-Time Interaction Models", "content": "Cross-Time Interaction Models primarily capture the de-pendencies between time points or patches of time points. Among these models, Transformer-based models have shown advancements in time series forecasting owing to their capacity to capture long-range dependencies. Informer [11], Autoformer [22], and FEDformer [30] aim to capture tempo-ral dependencies and reduce the quadratic computational cost brought by attention mechanisms. However, their effec-tiveness is challenged by a simple linear model [27]. Addi-tionally, RLinear [12] verifies that employing the Multilayer Perceptron (MLP) along the temporal dimension can effec-tively capture periodic features in time series. Subsequently, PatchTST [18] relies on a channel-independent strategy to capture dependencies between patches, demonstrating supe-rior performance. In addition, CNN-based models [1] capture the information of temporal dynamics by convolutional kernels sliding along the time dimension but face challenges in capturing global dependencies due to the limited receptive field of the convolutional kernels. To overcome this issue, TimesNet [23] transforms one-dimensional time series into a set of two-dimensional tensors based on multiple periods and applies two-dimensional convolution kernel modeling to extract complex temporal variations effectively."}, {"title": "2.2 Cross-Variable Interaction Models", "content": "The increasing emphasis on cross-variable interaction model-ing in recent research aims to improve the accuracy of time se-ries forecasting. Previous studies have extensively leveraged Graph Neural Networks (GNNs) to capture cross-variable dependencies. For instance, STGCN [26] effectively models multi-scale traffic networks, thereby capturing both spatial and temporal correlations. Similarly, MTGNN [24] employs GNNs to extract uni-directed relationships among variables. In contrast, the CNN-based model ModernTCN [16] uses ConvFFN1 to independently learn new feature represen-tations for each variable, while ConvFFN2 independently captures cross-variable dependencies for each feature. Ad-ditionally, Transformer-based models have demonstrated success in precise time series forecasting by capturing inter-variable interactions. Crossformer [8] introduces a two-stage attention mechanism to capture both cross-time and cross-variable dependencies, significantly improving prediction accuracy. More recently, iTransformer embeds entire time series into variable tokens and employs explicit attention mechanisms to capture cross-variable dependencies, achiev-ing state-of-the-art performance. However, the attention mechanism used in iTransformer [14] fails to effectively capture complex and dynamic multivariate correlations and struggles to learn negative correlations between variables. To address these limitations, we propose a convolutional operation based on a timepoint-independent strategy that captures dynamic relationships, including both positive and negative correlations among variables."}, {"title": "3 PROPOSED METHOD", "content": ""}, {"title": "3.1 Problem Definition", "content": "In multivariate time series forecasting tasks, given a his-torical input sequence $X = [x^{(1)}, x^{(2)},...,x^{(L)}] \\in \\mathbb{R}^{L \\times N}$,\nthe goal is to predict the future output sequence $Y =\n[x^{(L+1)}, x^{(L+2)},...,x^{(L+T)}] \\in \\mathbb{R}^{T \\times N}$, where $N$ represents\nthe number of variables, $L$ and $T$ respectively indicate the\nlengths of the input and output sequences. For convenience,\n$x^{(i)} \\in \\mathbb{R}^{1 \\times N}$ represents the values of $N$ variables at the $i$-th\ntime point, where $x_j^{(i)} \\in \\mathbb{R}^{1 \\times 1}$ denotes the $j$-th variable at\nthe $i$-th time point."}, {"title": "3.2 The General Structure", "content": "The cross-variable correlation of multivariate time series demonstrates multifaceted (positive and negative correla-tion) and dynamic progression over time. Considering this intrinsic nature of multivariate time series, we propose a TimeCNN model to refine cross-variable interaction on time points to enhance prediction performance. Its key component is timepoint-independent, where each time point"}, {"title": "3.3 Model Architecture", "content": "CrossCNN. It adopts timepoint-independent, where each time point has an independent convolution kernel. The independent handling of time points enables CrossCNN to ef-fectively capture dynamically evolving relationships among variables. Moreover, this configuration ensures the modeling of all variables at each time point while minimizing the number of parameters to just $L \\times N$ for all time points. The detailed structure of CrossCNN is shown in the Figure 6. For the i-th time point input $x^{(i)} \\in \\mathbb{R}^{1\\times N} = [x_1^{(i)}, x_2^{(i)},...,x_N^{(i)}]$, we pad it by taking the last N \u2013 1 variables and finally obtain $\\tilde{x}^{(i)} = [x_1^{(i)},x_2^{(i)},...,x_N^{(i)}, x_1^{(i)}, x_2^{(i)},...,x_{N-1}^{(i)}]$, as described in Figure 6. After the padding operation, the sliding window mechanism of the convolution ensures that each convolu-tional operation covers all variables.\nSubsequently, the convolutional operation is performed to capture multivariate correlations at the i-th time point. We set the convolutional kernel of size to the number of variables N for enlarging the local receptive field. The convolutional ker-nel parameters are assumed as $w^{(i)} = [w_1^{(i)}, w_2^{(i)}, ..., w_N^{(i)}]$. The j-th variable of the output, $c_j^{(i)}$, is obtained as follows:\n$c_j^{(i)} = \\sum_{k=1}^{N} w_k^{(i)} \\tilde{x}_{(j+k)}^{(i)}, j = 1,2,..., N, i = 1, 2, ..., L$\nAs demonstrated in Figure 6, by sliding convolutional operation we can obtain the output of the i-th time point $c^{(i)} = [c_1^{(i)}, c_2^{(i)},...,c_N^{(i)}] \\in \\mathbb{R}^{1\\times N}$. We notice that the signs of the convolutional kernel parameters indicate the positive and negative correlations between variables at each point, which attention mechanisms fail to capture.\nFinally, dropout is applied to the output via the convolu-tional operation to mitigate overfitting and promote model generalizability. Furthermore, the production of CrossCNN incorporates a skip connection to preserve the integrity of the input data throughout the processing pipeline.\nFeed-Forward Networks (FFN). In light of the advance-ments presented by DLinear [27] and RLinear [12], we employ FFN to learn variable representations for time series forecasting. The acquired output $X_{crosscnn}$ of CrossCNN is transposed and the entire time series of each variable is subsequently embedded into individual variable tokens. Then, we obtain the input $X_{emb}$ of FFN. After cross-variable interaction, the time series of each variable is processed by sharing the same embedding and FFN.\nAs depicted in Figure 5, to ensure consistent model train-ing and improve the network's convergence properties, layer normalization is implemented at the input stage. Constituting a multi-layer perceptron (MLP) with a singular hidden layer, FFN employs a Gaussian Error Linear Unit (GELU) activation function to facilitate non-linear transformations. Furthermore, it incorporates a skip connection, characterized by its fully linear nature, to preserve the integrity of the input data throughout the processing pipeline. To mitigate overfitting and promote model generalizability, dropout is applied to the linear mapping between the hidden layer and the output. FFN consists of M above MLP units and its formula is as follows:\n$X_h = Dropout(Gelu(Dense(LayerNorm(X_{m-1}))))$,\n$X_m = Dropout(Dense(X_h)) + X_{m-1}$.\nHere, $X_{m-1}$ and $X_m \\in \\mathbb{R}^{N \\times D}$ are the input and output of the m-th layer, $X_h \\in \\mathbb{R}^{N \\times H}$, where H represents the size of the hidden layer. We define $X_o = X_{emb}$ as the input of FFN.\nProjection Prediction. A linear layer is employed to ob-tain the final prediction $\\hat{Y} = [x^{(L+1)}, x^{(L+2)},..., x^{(L+T)}] \\in \\mathbb{R}^{T \\times N}$. We use Mean Squared Error (MSE) to measure the difference between the predicted values $\\hat{Y}$ and the ground truth Y. MSE is calculated within T time steps. The formula is as follows:\n$L = \\frac{1}{T} \\sum_{j=1}^{T} \\sum_{i} |\\hat{x}_j^{(L+i)} - x_j^{(L+i)}|^2$."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experiment Settings", "content": ""}, {"title": "4.1.1 Datasets", "content": "We evaluate the performance of TimeCNN on 12 real-world datasets, including Weather, ECL, Traffic, ETT (ETTh1, ETTh2, ETTm1, ETTm2) used by AutoFormer [22], Solar-Energy used by LST-Net [10] and PEMS datasets (PEMS03, PEMS04, PEMS07, PEMS08) adopted by SCINet [13]. These are widely used multivariate time series datasets, and we handle the datasets the same way as iTransformer [14]. The details of datasets are as follows:\n\u2022 ETT (Electricity Transformer Temperature) [11] includes four subsets. ETTh1 and ETTh2 collect hourly data on 7 different factors from two distinct electricity transformers from July 2016 to July 2018. ETTm1 and ETTm2 record the same factors at a higher resolution of every 15 minutes.\n\u2022 Traffic [22] collects hourly data from the California Department of Transportation, describing road occu-pancy rates measured by 862 sensors on San Francisco Bay Area freeways.\n\u2022 ECL(Electricity) [22] captures the hourly electricity consumption of 321 clients from 2012 to 2014.\n\u2022 Weather [22] records 21 meteorological factors such as air temperature and humidity every 10 minutes throughout the year 2020.\n\u2022 Solar-Energy [10] includes data from 137 PV plants in Alabama State, with solar power production sam-pled every 10 minutes during 2006.\n\u2022 PEMS [13] includes traffic network data from Califor-nia, sampled every 5 minutes, focusing on four public subsets: PEMS03, PEMS04, PEMS07, and PEMS08."}, {"title": "4.1.2 Baselines", "content": "We select the state-of-the-art model iTransformer [14] and 9 commonly recognized multivariate time series models as our baselines. Among them, Transformer-based mod-els include iTransformer, PatchTST [18], Crossformer [28], FEDformer [30], and Autoformer [22]. Linear-based models include RLinear [12], DLinear [27], and TiDE [3]. The CNN-based models include SCINet [13] and TimesNet [23]. Each model adheres to a uniform experimental framework, characterized by a prediction interval, T, which is defined within the set {96, 192, 336, 720}. The lookback window is configured to 96. We aggregate the results of baselines as reported by iTransformer [14], and the result of PatchTST and TimeCNN are from our experiments."}, {"title": "4.1.3 Pseudocode of TimeCNN", "content": "In the pseudocode presented in Algorithm 1, we outline the architecture of TimeCNN, which processes an input time series through CrossCNN and a series of feed-forward neural network (FFN) layers to generate future predictions."}, {"title": "4.2 Main Result", "content": "The experimental results are delineated in Table 2, where lower MSE and MAE indicate better prediction accuracy. Our proposed TimeCNN outperforms all baselines on almost all datasets. Particularly, on datasets with a large number of variables, such as Traffic, Solar-Energy, and PEMS, TimeCNN exhibits significantly higher prediction accuracy compared to the cross-variable models iTransformer and Crossformer. This is mainly attributed to our strategy timepoint-independent, which enables TimeCNN to capture variable relationships at each time point using a large convolutional kernel. Consequently, TimeCNN obtains the best performance by capturing the dynamically changing variable relationships over time. In contrast, iTransformer embeds entire time series into variable tokens, making it difficult to capture such dy-namic relationships. Additionally, the attention mechanism of iTransformer and Crossformer fails to extract negative correlations between variables. On the other hand, compared to cross-time interaction models such as PatchTST, TimesNet, and DLinear, TimeCNN demonstrates significantly superior performance, alongside robust generalization capabilities across diverse datasets. Finally, our proposed TimeCNN can address real-world prediction challenges effectively.\nIn addition, we extended our experiments by utiliz-ing a longer lookback window setting to further evaluate the performance of our model. For comparison, we se-lected several state-of-the-art models as baselines, including iTransformer [14], ModernTCN [16], PatchTST [18], RLin-ear [12], Crossformer [28], DLinear [27], FEDformer [30], and SCINet [13]. During the evaluation process, we corrected a bug related to the setting 'dropout_last = True' in Mod-ernTCN, PatchTST, and RLinear, which could have otherwise led to erroneous prediction performance, especially when utilizing a larger batch size as a hyperparameter. Our look-back window setting is aligned with ModernTCN, following its configuration to optimize prediction performance. The results for Crossformer, DLinear, FEDformer, and SCINet were obtained from the ModernTCN paper, while the results for the other models were generated through our own experimental runs. As presented in Table 3, the experimental results demonstrate that, even with an extended lookback window, our proposed TimeCNN consistently outperforms these baseline models. We attribute this superior performance to the strong scalability of our CrossCNN block, which is highly effective in extracting cross-variable dependencies at each time point over a longer lookback window, significantly enhancing prediction accuracy."}, {"title": "4.3 Efficiency Analysis", "content": ""}, {"title": "4.3.1 With Fixed Setting", "content": "We conducted experiments to evaluate the efficiency metrics of TimeCNN, including Multiply-Accumulate Operations (MACs) [2], the number of parameters (Params), and in-ference time (Time). We evaluated these metrics on the Weather and ECL datasets using prediction lengths of 96, 192, 336, and 720 time steps, and on the PEMS03 and PEMS07 datasets using prediction lengths of 12, 24, 48, and 96 time steps, with a lookback window of 96 time steps and a batch size of 1 across all experiments. Results are averaged from all prediction lengths. All experiments are conducted on a GeForce RTX 4090 24GB. Notably, during inference time testing, we conducted 300 iterations of GPU preheating and average over 10,000 trials to ensure precision. The methodology of calculating inference time is detailed at the site. And we selected iTransformer [14], PatchTST [18] and ModernTCN [16] as baseline models for comparison.\nThe results are shown in the Table 4. TimeCNN provides a better balance of efficiency and performance. Across the four datasets used in our experiments, TimeCNN achieves a 60.46% reduction in MACs and a 57.50% reduction in the number of parameters compared to iTransformer, while achieves a speed that is 3 to 4 times faster than iTransformer. This demonstrates that TimeCNN surpasses iTransformer in both prediction accuracy and computational efficiency."}, {"title": "4.3.2 With Various Setting", "content": "We configured various lookback windows and adjusted the number of variables to assess the efficiency of our TimeCNN model in comparison to other baselines. The results are presented in Figure 8. The left panel illustrates the changes in Multiply-Accumulate Operations (MACs) and Inference Time under different lookback window configurations, while the right panel shows the corresponding metrics for different numbers of variables. Across all scenarios, our TimeCNN consistently achieves lower computational overhead and faster inference times. This improvement arises primarily due to the computational complexity of PatchTST growing quadratically with the length of the lookback window (with a fixed patch size), while iTransformer's Computational complexity scales quadratically with the number of variables."}, {"title": "4.4 Model Analysis", "content": ""}, {"title": "4.4.1 Increasing Lookback Window", "content": "Previous work [27] has indicated that most Transformer-based models such as Informer [11], Autoformer [22], and FEDformer [30] struggle to attain more precise future predic-tions from increasing lookback windows. However, longer lookback windows imply larger receptive fields. We also con-ducted experiments to test the performance of our proposed TimeCNN across different historical windows. The experi-mental results are illustrated in the Figure 7. On datasets with a larger number of variables such as Weather, ECL, and Traffic, TimeCNN's predictive performance surpasses these models across almost all benchmarks. Furthermore, TimeCNN can capture dynamic variable relationships with longer lookback windows to enhance predictive capabilities."}, {"title": "4.4.2 Module Ablation", "content": "We conducted ablation experiments to assess the contri-butions of the CrossCNN and FFN components within TimeCNN. In these experiments, we either replaced Cross-CNN with alternative CNN variants or removed it entirely to evaluate its effectiveness. Specifically, the 'CrossLinear' replaces CrossCNN with a linear layer for variable mixing, following the approach used in TSMixer [5], where fully connected layers capture dependencies across both variables and time. The 'OneCNN' employs a single convolutional kernel applied uniformly across all time points, using the same padding and operations as in TimeCNN. Additionally, the '2DCNN_3' and '2DCNN_7' substitute CrossCNN with (3, 3) and (7, 7) convolutional kernels, respectively, designed to capture temporal relationships and variable dependencies. Furthermore, we introduced a 'w/o CrossCNN' model, where the CrossCNN block is removed entirely. In all cases, the number of layers is kept constant, with the CrossCNN block replaced. We also added PatchTST [18] as a baseline in our ablation study. The lookback window is configured to 96, with prediction lengths set to 96, 192, 336, and 720, respectively.\nThe experimental results, as presented in Table 9, demon-strate that TimeCNN consistently outperforms other mod-els across all datasets. Specifically, TimeCNN surpasses CrossLinear in terms of MSE, showing an average reduction of 25.1%, and MAE, with a decrease of 16.5%. This improve-ment is primarily attributed to CrossLinear's suboptimal performance on datasets with a larger number of variables, where the quadratic increase in parameters as variables grow leads to overfitting. In contrast, our proposed CrossCNN employs a shared convolutional kernel across all variables at each time step, resulting in a linear increase in parameters, which significantly reduces the risk of overfitting and en-hances predictive accuracy. Furthermore, OneCNN applies a single convolution across all time points, overlooking the intrinsic dynamic relationships between variables over time, similar to the limitation observed in iTransformer. As a result, its performance is inferior to that of TimeCNN. Although 2D convolutional kernels can capture variable relationships to a certain extent, their limited receptive fields hinder their ability to fully model inter-variable dependencies, which further explains their lower performance compared to TimeCNN. Notably, even without the CrossCNN block, the model achieves superior predictive accuracy compared to PatchTST across all datasets, indicating that the FFN component is effective in modeling cross-time interactions and learning variable representations."}, {"title": "4.4.3 Performance promotion", "content": "The CrossCNN block is a modular component designed to capture dynamic relationships between variables and can be seamlessly integrated into any time series forecasting models. To assess its effectiveness as a supplementary module, we incorporated CrossCNN into recent baseline models, including iTransformer [14], PatchTST [18], and RMLP [12]. Specifically, we insert the CrossCNN block immediately after instance normalization to capture cross-variable relationships over time, while keeping all other components unchanged. The lookback window was set to 96, and the prediction lengths were set to 96, 192, 336 and 720. The experimental results, as shown in Table 5, indicate performance improvements across all datasets when CrossCNN is added to the baselines, particularly for datasets with a large number of variables. RMLP and PatchTST, which are variable-independent models, do not inherently capture inter-variable relationships. However, the inclusion of CrossCNN significantly enhances their performance by capturing these dynamic relationships. Similarly, in the case of iTransformer, using CrossCNN to capture inter-variable dependencies leads to performance gains. We attribute this modest improvement to the fact that the attention mechanism in iTransformer, to some extent, mitigates the dynamic relationships captured by CrossCNN."}, {"title": "4.5 Robustness study", "content": ""}, {"title": "4.5.1 Random Seeds", "content": "We conducted a series of experiments with different random seeds to assess the robustness of the TimeCNN model. In both the main experiments and the other ablation study, we initially used random seed 2023. To further evaluate robustness, we performed multiple trials with random seeds 2021, 2022, 2023, 2024, and 2025, using the ECL, Traffic, Weather, ETTm1 and PEMS03 datasets. For all experiments, the input lengths were set to 96 and the prediction lengths were set to the same as the main experiment. We calculate the standard deviation of TimeCNN performance under five runs. The results are shown in Figure 6, the results indicate that the performance of TimeCNN remains stable across different random seeds, demonstrating its robustness in handling variations in initialization."}, {"title": "4.5.2 Variable Noise", "content": "We conducted experiments on the PEMS04 dataset to assess the robustness of TimeCNN in handling noisy input data. Specifically, we selected a single variable from the input data and introduced Gaussian noise with a mean of 0 and a standard deviation of Noise_strength, applied at various intensities. After training under normal conditions, we calculated the mean squared error (MSE) during testing, focusing only on the predictions of the selected variable as the experimental result. The lookback window and prediction length were both set to 96. We used iTransformer [14] and PatchTST [18] as baseline models for comparison. The experimental results are presented in Figure 10. The results indicate that as the noise intensity increases, PatchTST's MSE rises sharply. This is because the strategy of variable-independent processing makes it difficult for PatchTST to capture the inherent patterns in noisy time series. Al-though iTransformer outperforms PatchTST, its performance becomes unstable as noise intensifies, likely due to the susceptibility of attention scores to noise interference. In contrast, our proposed TimeCNN demonstrates better and more stable performance as noise intensity increases. This improvement can be attributed to the dynamic interactions between variables, which significantly mitigate the impact of noise."}, {"title": "4.6 Instance Visualization", "content": "Our CrossCNN is designed to capture both positive and neg-ative relationships among variables over time. Specifically, our CrossCNN can enhance the existing correlations between variables. For instance, variables that initially exhibit positive correlation demonstrate stronger positive correlation after information extraction through CrossCNN. To demonstrate this effect, before and after applying CrossCNN we calculate the rolling correlation coefficients between two variables over time. We select the latter half of the input time series (with a length of 48) and set a rolling window of 4. The resulting scatter plot of correlation coefficients is presented in Figure 11. The scatter plot reveals that the majority of points fall within the shaded regions, indicating that originally positively correlated variables show strengthened positive correlations, while originally negatively correlated variables exhibit enhanced negative correlations. This observation validates CrossCNN's capability to capture dynamically changing variable relationships over time."}, {"title": "5 CONCLUSION AND Future WORK", "content": "Our research demonstrates that TimeCNN effectively ad-dresses the limitations of current Transformer-based models in capturing the intricate and dynamic interactions among variable correlations in multivariate time series forecast-ing. The innovative architecture of TimeCNN, featuring timepoint-independent convolutional kernels, allows the model to independently learn cross-variable dependencies at each time point, adn capture both positive and negative corre-lations among variables. A comprehensive evaluation across 12 real-world datasets confirms that TimeCNN outperforms existing state-of-the-art models in predictive performance while achieving substantial improvements in computational efficiency. These results highlight the potential of TimeCNN as a robust and efficient advancement in the field of time series forecasting."}]}