{"title": "Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement", "authors": ["Junyu Lu", "Kai Ma", "Kaichun Wang", "Kelaiti Xiao", "Roy Ka-Wei Lee", "Bo Xu", "Liang Yang", "Hongfei Lin"], "abstract": "Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.", "sections": [{"title": "1 Introduction", "content": "Motivation. A fundamental challenge in offensive language detection is annotation disagreement-cases where human annotators provide conflicting labels for the same text. Disagreement arises due to differences in individual perception, cultural context, and linguistic ambiguity, making offensive language detection inherently subjective (Aroyo et al., 2019; Basile, 2020; Uma et al., 2021b). However, prior research predominantly treats this task as a binary classification problem, assuming consensus among annotators and failing to account for the inherent subjectivity in offensive language perception.\nWhile large language models (LLMs) have been extensively applied to offensive language detection (Kumar et al., 2024; Huang et al., 2023), existing studies primarily evaluate their performance on datasets with binary labels, overlooking their ability to handle cases where annotators disagree. This oversimplification limits our understanding of how well LLMs align with human judgment in ambiguous cases. Moreover, models may exhibit overconfidence in cases where human annotators themselves are uncertain, raising concerns about their reliability for real-world moderation.\nResearch Objectives. To bridge this gap, we systematically evaluate LLMs' ability to process disagreement samples, analyzing both classification accuracy and model confidence. Through this, we seek to determine whether LLMs can effectively navigate subjective offensive language judgments and align with human reasoning. An ideal model should express high confidence for unanimously labeled cases and lower confidence for ambiguous samples, reflecting their inherent uncertainty (Weerasooriya et al., 2023; Baumler et al., 2023; Uma et al., 2021a; Leonardelli et al., 2023). Our study provides insights into whether LLMs capture these nuances or exhibit overconfidence in disagreement cases, which could undermine their trustworthiness in content moderation.\nThis paper systematically investigates how LLMs handle annotation disagreement in offensive language detection. Specifically, we address the following research questions: (RQ1) To what extent can LLMs accurately detect offensive language in cases of human annotation disagreement? (RQ2) How do disagreement samples shape LLM learning and influence decision-making?"}, {"title": "2 Preliminary", "content": "Since annotation disagreements can stem from both intrinsic linguistic ambiguity and labeling error, selecting an appropriate benchmark dataset requires meeting two key criteria: (1) high annotation quality to ensure reliability, and (2) open access to unaggregated annotations to facilitate fine-grained analysis. To ensure a robust evaluation, we employ the MD-Agreement dataset (Leonardelli et al., 2021), a high-quality corpus for offensive language detection. It contains 10,753 tweets, each labeled by five trained human annotators, ensuring a reliable annotation process.\nThe dataset provides both hard labels (majority-voted labels) and soft labels, which indicate the level of agreement among annotators. The soft labels are categorized into three levels:\n\nEach sample in the dataset is also classified as either offensive or non-offensive, following the same agreement-level framework:\n\nThus, the agreement notation (++, +, 0) applies uniformly across both offensive and non-offensive categories, ensuring consistency in the dataset's annotation schema. To facilitate subsequent research, we convert the soft labels into floating-point numbers in the range [0, 1] by averaging the hard labels from five annotators (offensive as 1, non-offensive as 0) for each sample."}, {"title": "3 RQ1: Evaluating LLMs on Offensive Language with Annotation Disagreement", "content": "In this section, we evaluate the ability of LLMs to detect offensive language in a zero-shot setting. We focus on two key aspects: (1) binary classification accuracy, assessing how effectively models distinguish offensive from non-offensive language across varying annotation agreement levels, and (2) model confidence, analyzing whether LLMs exhibit appropriate uncertainty in ambiguous cases. These aspects are essential for determining whether LLMs can reliably perform offensive language detection in real-world scenarios, where human annotators often disagree."}, {"title": "3.1 Evaluation of Binary Classification Performance", "content": "We assess binary classification performance by evaluating LLMs in a zero-shot setting without additional fine-tuning. To ensure deterministic predictions, we set the temperature coefficient of the LLMs to 0, forcing the model to select the most probable category. We use accuracy and F1 score as evaluation metrics to measure classification performance. The prompt template used for offensive language detection is provided in Appendix A.3. LLM outputs are converted into hard predictions, where 1 indicates offensive and 0 indicates non-offensive. We utilize all the samples from MD-Agreement for a comprehensive evaluation. Based on the results, we observe the following key findings:\n(1) LLMs achieve high accuracy for unanimous agreement (A++) samples. In the zero-shot setting, LLMs consistently accurately classify unanimously agreed-upon (A++) samples, achieving 88.28% accuracy for closed-source models and 86.07% for open-source models. Notably, LLaMa3-70B now performs comparably to proprietary models. These results suggest that LLMs perform well on clear-cut cases, driven by their background knowledge and reasoning capabilities.\n(2) LLM performance declines sharply for ambiguous cases. As annotation agreement decreases, LLMs struggle to classify offensive language consistently. GPT-40's F1 score drops from 85.24% on A++ samples to 74.6% on A+ and 57.06% on A\u00ba. Similarly, all models score below 65% on A\u00ba samples. This highlights LLMs' inability to resolve subjective cases in the real world, where human disagreement often stems from cultural, contextual, or linguistic nuances that models fail to capture.\n(3) Larger models improve accuracy but do not resolve annotation disagreement. While larger models generally perform better, their improvement shrinks for ambiguous cases. For example,"}, {"title": "3.2 Evaluation of Relationship between Agreement Degree and LLM Confidence", "content": "We analyze how well LLM confidence aligns with human annotation agreement, as a well-calibrated model should exhibit high confidence for clear cases and lower confidence for ambiguous cases. If LLMs assign high confidence to disagreement samples, this may indicate overconfidence, limiting their ability to reflect human-like uncertainty. To evaluate this, we apply the self-consistency method (Chen and Mueller, 2024; Wang et al., 2023b), which resamples model outputs under varying temperature settings to estimate confidence.\nTo measure confidence, we evaluate models under five temperature settings: 0, 0.25, 0.5, 0.75, and 1. Higher temperatures introduce more randomness in predictions, helping assess the model's certainty across varying conditions. The final confidence score is computed by averaging the hard predictions across these temperature settings.\nWe use Mean Squared Error (MSE) to measure the alignment between LLM confidence and annotation agreement, where a smaller MSE indicates closer alignment (Uma et al., 2021a; Leonardelli et al., 2023). Additionally, we employ Spearman's Rank Correlation Coefficient (p) to assess statistical correlation. Based on these results:\n(1) As annotation agreement decreases, the alignment between model confidence and human agreement weakens. As annotation agreement decreases, LLMs become less reliable in assessing their own uncertainty. GPT-40, which performs best overall, has an MSE of 0.05 for A++ samples but sees this error rise to 0.2 for A\u00ba samples. Additionally, Spearman's correlation (p) between confidence and agreement weakens from above 0.7 for unanimous samples to below 0.3 for disagreement cases. This suggests that LLMs do not effectively recognize uncertainty in ambiguous cases. In real-world moderation, this could lead to overconfident misclassifications, where the model assigns a high confidence score to an incorrect label, making it harder to detect errors and apply human oversight.\n(2) LLMs demonstrate high self-consistency but may be overconfident in disagreement cases. We assess self-consistency using Cohen's Kappa (K), measuring how stable LLM outputs remain across multiple sampling attempts. As shown , self-consistency decreases for lower agreement samples but remains above 0.75 even for A\u00ba cases, indicating strong internal agreement. While high self-consistency is desirable for clear-cut cases, it becomes problematic in ambiguous cases, as it suggests that LLMs remain overconfident even when human annotators disagree. This rigidity limits the model's ability to adjust for nuanced linguistic or contextual differences.\n(3) Even high-performing models exhibit overconfidence, limiting their ability to reflect human-like uncertainty. We construct a confusion matrix of GPT-40 to visually analyze the relationship between the model's confidence score and the soft labels of samples. The result reveals that even GPT-40, the best-performing model, assigns high confidence to its predictions regardless of annotation agreement, indicating a lack of adaptability to disagreement cases. This overconfidence highlights a critical flaw in LLM-based moderation: their inability to reflect the diversity of human judgment. Overconfident models are more likely to make systematic errors in handling subjective content, leading to unreliable moderation outcomes. Instead of relying on LLMs as sole decision-makers, future research should explore ensemble methods, uncertainty-aware training, or human-AI collaboration to mitigate biases and improve disagreement resolution."}, {"title": "4 RQ2: Impact of Disagreement Samples on LLM Learning", "content": "In this section, we examine how samples with varying annotation agreements influence LLM performance during the learning phase. We focus on two key learning paradigms: few-shot learning and instruction fine-tuning. Specifically, we explore the impact of both single-category agreement samples and different agreement-level combinations on model performance."}, {"title": "4.1 Impact of Disagreement Samples on Few-Shot Learning", "content": "We evaluate the effect of disagreement samples on GPT-40's binary classification accuracy and its confidence alignment with human annotations during few-shot learning.\nFew-Shot Learning Setup. We evaluate both single-category agreement samples and combinations of agreement levels in few-shot learning, following (Leonardelli et al., 2021). We first construct prompts using positive and negative sample pairs randomly drawn from the MD-Agreement training set, with each prompt including pairs corresponding to the respective agreement level. For example, the simplest setup w/ A++ consists of only unanimous agreement (A++) samples, containing one offensive and one non-offensive example. Furthermore, we examine mixed setups with different agreement configurations, consisting of sample pairs from their respective single categories for reliable evaluation. For instance, w/ A++/0 and w/ A++/+ combine unanimous agreement samples with one level of disagreement, respectively. Additionally, we assess a broader configuration, w/ A++/+/0, which includes samples from all three agreement levels.\nWe evaluate model performance on the MD-Agreement test set, analyzing both overall results and performance across different agreement levels. summarizes the key findings."}, {"title": "4.2 Impact of Disagreement Samples on Instruction Fine-tuning", "content": "We analyze how instruction fine-tuning with different annotation agreement levels affects model performance, using LLaMa3-7B as the backbone.\nInstruction Fine-tuning Setup. We fine-tune an equal number of instances from each agreement level in the MD-Agreement dataset. Specifically, we extract 1,800 samples each from A++, A+, and A\u00ba, based on the least-represented A\u00ba category. The instruction template remains consistent with that used in the zero-shot setting . We also evaluate combinations of multiple agreement levels, using the same experimental markers as in Section 4.1. presents the results, leading to the following conclusions:\n(1) Medium-agreement (A+) samples yield the best balance in fine-tuning. Fine-tuning with high-agreement (A++) samples improves classification accuracy, while low-agreement (A\u00ba) samples enhance confidence alignment with human annotations, reducing MSE. However, exclusive reliance on A\u00ba samples may lead to catastrophic forgetting, where the model becomes overly attuned to ambiguous cases at the cost of general classification accuracy. A+ samples offer the best trade-off, allowing the model to capture nuanced decision boundaries while maintaining robust performance.\n(2) Combining multiple agreement levels further enhances performance. Fine-tuning with all three agreement levels (w/ A++/+/0) achieves the best overall results, yielding performance comparable to GPT-40 in few-shot learning . Among two-category combinations, mixing disagreement samples (w/ A+/0) provides the most improvement, reinforcing the importance of disagreement-aware learning.\nThese results confirm that strategically selecting disagreement samples is essential for instruction fine-tuning. A well-balanced combination enhances both classification performance and confidence calibration, ensuring better alignment with human judgments.\nWe replicate the instruction fine-tuning experiment with Qwen2.5-7B using the same training and test data. The results closely align with those of LLaMa3-7B, confirming that these insights generalize across different model architectures."}, {"title": "5 Related Work", "content": "Large Language Model. In recent years, large language models (LLMs) have rapidly emerged, showcasing extensive world knowledge and strong reasoning capabilities (Kojima et al., 2022; Ouyang et al., 2022; OpenAI, 2023). Many researchers have proposed diverse tasks to deeply analyze the relationship between the model's outputs and human judgments (Xu et al., 2024; Fan et al., 2024). In addition, the confidence of LLMs in their outputs has also attracted attention from researchers, which is often used to assess the reliability and robustness of the generated content (Jiang et al., 2021). Various methods for estimating confidence have been proposed (Zhang et al., 2020; Wang et al., 2023b; Tian et al., 2023; Lin et al., 2022). In this study, we employ the most straightforward approach, self-consistency, to estimate the model's confidence.\nOffensive Language Detection. Researchers have developed various methods for detecting offensive language (Founta et al., 2018; Davidson et al., 2017; Mathew et al., 2021). As research advances, many studies argue that treating offensive language detection as a binary classification is an idealized assumption (Basile et al., 2021; Basile, 2020; Plank, 2022), as annotation disagreement are inherent in datasets for such subjective task (Pavlick and Kwiatkowski, 2019; Uma et al., 2021b). Using majority voting for annotation agreement leads to information loss (Davani et al., 2022), as these disagreements arise from the subtlety of the samples, not labeling errors (Uma et al., 2022). Leonardelli et al. (2023) emphasizes that detection models should recognize this disagreement, rather than just improving classification performance. Recently, several studies have begun evaluating the potential of LLMs for detecting offensive language (Kumar et al., 2024; Roy et al., 2023), and designing detection methods based on them (Park et al., 2024; Wen et al., 2023). Some studies (Wang et al., 2023a; Huang et al., 2023) leverage the generative capabilities of LLMs to provide explanations for offensive language, assisting human annotation. Furthermore, Giorgi et al. (2024); Zhang et al. (2024) assess the sensitivity of LLMs to demographic information in the context of offensive language. Though great efforts have been made, these studies lack focus on the phenomenon of offensive language with annotation disagreement. In this paper, we aim to fill this research gap."}, {"title": "6 Conclusion", "content": "This study examines how LLMs handle annotation disagreement in offensive language detection, a critical challenge in real-world moderation. We evaluate multiple LLMs in a zero-shot setting and find that while they perform well on unanimously agreed-upon samples, their accuracy drops significantly for disagreement cases. Moreover, their overconfidence leads to rigid predictions, misaligning them with human annotations.\nTo address this, we investigate the impact of disagreement samples in few-shot learning and instruction fine-tuning. Our results show that incorporating these samples improves detection accuracy and human alignment, enabling LLMs to better capture the subjective nature of offensive language. We further find that balancing agreement levels in training data prevents overfitting to ambiguous cases, ensuring model robustness.\nKey findings of this work include: (1) a systematic evaluation of LLMs on annotation disagreement, (2) insights into how disagreement samples improve learning, and (3) guidelines for leveraging disagreement-aware training strategies. These results emphasize the need for model calibration techniques to mitigate overconfidence and for training strategies that incorporate disagreement to improve generalization. Future research should explore dynamic fine-tuning approaches and confidence-aware moderation systems to bridge the gap between LLM decisions and human subjectivity."}, {"title": "Limitations", "content": "(1) Due to the scarcity of high-quality offensive language datasets with unaggregated labels, we only utilize the MD-Agreement dataset for experiments, which has been widely used in the field. Considering that relying on a single dataset may introduce bias or randomness, we mitigate this by conducting experiments with multiple closed-source and open-source LLMs to ensure the consistency and reliability of our findings, reducing the impact of bias. In future work, we plan to further explore the performance of LLMs in other subjective text analysis tasks, such as humor detection and misogyny detection, particularly in understanding samples with annotation disagreement.\n(2) Due to usage restrictions, we are unable to evaluate the detection performance of several emerging LLMs, such as GPT-03. We plan to further assess these more advanced models as soon as experimental conditions allow. Additionally, due to space limitations, the potential of certain techniques for detecting offensive language with annotation disagreement, such as reinforcement learning methods, are not discussed. We plan to explore these methods in future work and investigate effective strategies for enabling LLMs to fully leverage disagreement samples, thereby enhancing their detection capabilities.\n(3) In evaluating the confidence of LLMs, we adopt a straightforward approach based on temperature resampling. We have noted another common method, the Logit-based approach (Guo et al., 2017; Zhang et al., 2020), which involves using the logits of category-specific tokens to compute statistical probabilities within the model's output. This method may provide deeper insights into the decision-making mechanisms of LLMs when handling disagreement samples. We plan to explore and evaluate this method in future work."}, {"title": "Ethics Statement", "content": "The opinions and findings contained in the samples of this paper should not be interpreted as representing the views expressed or implied by the authors. Accessing the MD-Agreement dataset requires users to agree to the creators' usage agreements. The usage of these samples in this study fully complies with these agreements."}, {"title": "A Experimental Details", "content": "In this section, we provide a detailed introduction to the annotation quality control process of our used MD-Agreement dataset (Leonardelli et al., 2021). The researchers implemented a two-stage annotation process: First, three linguists annotated a subset of the samples, and those with unanimous agreement were used as the gold standard for the annotation process. Following this, trained annotators from Amazon Mechanical Turk were employed to annotate the complete samples based on the established gold standard. After the task was completed, annotations from workers who did not achieve at least 70% accuracy were discarded. Additionally, it was ensured that each sample in the final dataset received five annotations. These measures help ensure the accuracy of the annotations. Sandri et al. (2023) further manually reviewed a random selection of 2,570 samples with annotation disagreement from the MD-Agreement dataset. The results showed that only 12 samples contained annotation errors, accounting for less than 0.5%, demonstrating the high quality and reliability of the dataset."}, {"title": "A.2 Description of Metrics", "content": "This section introduces the metrics used to assess the relationship between LLM confidence and the degree of human annotation agreement.\nMean Squared Error (MSE): The MSE is a widely used evaluation metric in regression tasks, measuring the difference between predicted and actual values. In this study, we adopt MSE for alignment estimation, as described by Leonardelli et al. (2023), where a smaller MSE indicates closer alignment between LLM confidence and agreement degree. We first obtain soft labels y and soft predictions \u0177 of samples by averaging their discrete 0-1 annotation sequences Y and the LLM outputs Y across different samplings, as follows:\n$Yi = \\frac{1}{n} \\sum_{i=1}^{n} Yi, Yi = \\frac{1}{n} \\sum_{i=1}^{n} Yi,$\nwhere n is the number of observations, set to n = 5 in this paper, representing the number of annotators and LLM outputs. Then, the MSE is calculated as:\n$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (Yi-Yi)^2,$\nSpearman's Rank Correlation Coefficient (p):\nThe Spearman's Rank Correlation Coefficient is a non-parametric test that quantifies the degree of monotonic relationship between two variables. Unlike Pearson correlation, which assumes normally distributed variables, Spearman's correlation does not require this assumption and can be applied to discrete data. This makes it an ideal choice for assessing the statistical correlation between annotation agreement and LLM confidence, which is computed as follows:\n$\\rho=1-\\frac{6 \\sum d_i^2}{n(n^2 - 1)},$\nwhere di is the difference between the ranks of corresponding values of soft labels y and predictions \u0177.\nCohen's Kappa (\u043a): Cohen's Kappa is widely used to evaluate the consistency between annotators, especially in classification tasks. Compared to simple percentage agreement, which directly calculates the proportion of agreement between two evaluators, Cohen's Kappa provides a more precise measure as it accounts for the influence of random agreement. In this paper, we use Cohen's Kappa to estimate the consistency of LLM outputs Y. We first compute the kappa value between the i-th and the j-th LLM output of samples, i.e., Yi and Yj, using the following formula:"}, {"title": "A.3 Design of Prompt Template", "content": "To enhance the reproducibility of our study, we avoided conducting complex prompt engineering. Instead, we directly referenced (Roy et al., 2023) to design a straightforward prompt template, as in Table A3. The template includes three parts: first, the definition of offensive language, which aligns with that used in the MD-Agreement dataset (Leonardelli et al., 2021) to ensure the accuracy of the evaluation; second, examples of varying degrees of disagreement in a few-shot scenario; and finally, the sample to be detected."}, {"title": "A.4 Other Experimental Settings", "content": "We access closed-source LLMs via their official APIs and deploy open-source LLMs with parameters downloaded from Hugging Face. To ensure a fair comparison, we use model versions released around the same time, as detailed in Table A4. Since GPT-01 only has a default temperature of 1 and does not allow adjustments, we present its binary performance in this setting. Except for the temperature coefficient, other hyperparameters, such as top-p and top-k, are set to their default values for each model. For instruction fine-tuning, we adopt the efficient Qlora fine-tuning method. The learning rate is set to 2e-4, with a per-device batch size of 36. We train the model for 15 epochs using the AdamW optimizer, applying an early stopping mechanism. We reserve the parameters of best-performing models based on the development set and evaluate their performance on the test set. The models are trained on two NVIDIA H100 80GB GPUs. All the few-shot learning and instruction fine-tuning experiments are repeated five times with different random seeds to minimize error, and the average results are reported."}, {"title": "A.5 Handling of Refusal Behavior", "content": "Handling offensive language can trigger the refusal behavior of LLMs, as they are designed with ethical and safety considerations (Kumar et al., 2024). Nevertheless, in our experiments, refusal occurred only in the zero-shot evaluation setting, where Claude-3.5, with a temperature coefficient set to 1, failed to generate responses for 23 samples. When the experiment was repeated with the same settings, the model successfully provided predictions for these samples. This phenomenon also highlights the model's sensitivity to offensive language."}, {"title": "B Supplementary Experiments", "content": null}, {"title": "B.1 Impact of Temperature Sampling on Detection Performance of LLMs", "content": "In this section, we analyze the impact of temperature sampling on the accuracy of detecting offensive language by LLMs. We select four representative models for comparison: the closed-source models GPT-4 and Claude-3.5, as well as the open-source models LLaMa3-70B and Qwen2.5-72B. The experimental results are shown in Figure B1. Based on these results, we conclude that after adjusting the temperature coefficient, the detection accuracy of each LLM remains generally stable, although some fluctuations are observed, with varying degrees of sensitivity to the temperature coefficient across different models. As the temperature increases, the accuracy of most models shows a declining trend, with the sole exception being Qwen2.5-72B, which exhibits an increase in accuracy. This may be due to differences in the models' training mechanisms. Nevertheless, the performance ranking between the models remains stable, indicating that changes in the temperature coefficient do not notably affect the performance differences among the models."}, {"title": "B.2 Consistency Analysis Across Different LLMS", "content": "Building upon Section 3.2, we further explore the consistency of hard predictions across different LLMs when processing samples with varying degrees of annotation agreement. We select six representative models, including the close-source models GPT-40, Claude-3.5, and Gemini-1.5, as well as the open-source models Mixtral-8x22B, LLaMa3-70B, and Qwen2.5-72B. Cohen's Kappa is used as the metric. Based on the results, we can observe that:\nAs annotation agreement decreases, cross-model consistency in detecting offensive language declines more significantly compared to each model's self-consistency. For unanimous agreement samples (A++), cross-model consistency generally exhibits good agreement, with \u043a > 0.6. However, for low agreement samples A\u00ba, consistency drops explicitly, with many models showing poor agreement (\u043a < 0.4), despite many of these models exhibiting similar overall performance in terms of both binary classification accuracy and alignment with human annotations . Notably, the lowest prediction consistency Kappa is"}, {"title": "B.3 Few-shot Learning with Qwen2.5-72B", "content": "We replicate the few-shot learning experiment from Section 4.1 using the open-source LLM Qwen2.5-72B, employing the same sample pairs in the prompts. The results are shown in Table B1. Based on the results, we observe the following:\nIn the few-shot learning with samples of varying annotation agreement degrees, the results of Qwen2.5-72B align closely with the trends of GPT-40 . Whether introducing samples with a single annotation agreement degree or combinations of different agreement categories, the detection performance of the model shows notable improvement compared to the zero-shot scenario. Additionally, the benefit to model performance varies explicitly depending on the annotation agreement degree and the combinations used as prompts.\nFurthermore, compared to GPT-40, Qwen2.5-72B demonstrates two distinct differences: (1) On the subset of low-agreement samples, the introduction of few-shot learning also results in a noticeable improvement in the alignment between LLM confidence and annotation agreement. This is primarily because Qwen2.5-72B performs less effectively than GPT-40 in detecting offensive language under zero-shot scenarios, making it more susceptible to performance enhancements through few-shot learning. (2) For Qwen2.5-72B, the combination of disagreement samples, i.e., w/ A+/0, achieves the best performance across most subsets and metrics. In contrast, GPT-40 performs better with combinations such as w/ A++/+ and w/ A++/0. This highlights that the effects of learning from disagreement samples differ between different LLMs, which is closely related to the ambiguous characteristics of these samples."}, {"title": "B.4 Fine-tuning with Qwen2.5-72B", "content": "We replicate the instruction fine-tuning experiment from Section 4.2 using Qwen2.5-7B, training with the same instruction data. Based on the results shown , we observe conclusions that are largely consistent with those on LLaMa3-8B. Specifically, Qwen2.5-7B performs best with"}, {"title": "B.5 Error Analysis", "content": "To gain deeper insight into the challenge posed by offensive language with annotation disagreement, we manually inspect the set of samples misclassified by the models. The following two main types of errors are identified, with samples and predictions from GPT-40 and LLaMa3-72B shown in Table B3 for illustration:\nType I error refers to samples that are labeled as non-offensive but are detected as offensive. This error primarily arises from subtle linguistic features such as sarcasm and metaphor, which make the judgment of the sample ambiguous. For instance, in Example (a), the term \u201cAmerikkka\u201d is a variant of \"America\" used to intensify emotional expression. Due to insufficient context, most annotators do not consider it offensive. However, GPT-40 and LLaMa3, due to their sensitivity to the hashtag #blacklivesmatter, consistently classify it as offensive language. Similarly, in Example (b), a sarcastic rhetorical question leads to a misclassification by GPT-40. This phenomenon highlights the complexity that human annotators face in determining offensive language and also reveals the issue of over-sensitivity in existing LLMs to certain linguistic expressions, resulting in decisions that do not align with human standards. In future work, we will perform a more detailed analysis of expressions in samples with disagreement annotation and explore how different types of expressions affect model detection performance.\nType II error refers to sentences labeled as offensive but classified as non-offensive by the models. This error primarily arises from the models lacking or failing to effectively integrate the necessary background knowledge for detecting offensive content, leading to an inaccurate understanding of the sample's true meaning. For example, in Example (c), the comparison between the mayor of Minneapolis and Justin Trudeau uses \u201cSoy\" as an adjective, which implies weakness and is intended to belittle the mayor. Both human annotators and GPT-40 capture the offensive nature of the sample, but LLaMa3 fails to correctly identify its offensiveness due to insufficient relevant knowledge. In Example (d), the phrase \"Ye Generation of Vipers\", a religiously charged expression, is used to strongly criticize police brutality against black people. However, the model fails to integrate the context, leading to a missed detection. We plan to introduce more comprehensive background knowledge to enhance the understanding capability of LLMs and explore the performance of knowledge-enhanced models in detecting disagreement samples."}]}