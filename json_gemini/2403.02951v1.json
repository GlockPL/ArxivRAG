{"title": "Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation", "authors": ["Bin Zhang", "Yuxiao Ye", "Guoqing Du", "Xiaoru Hu", "Zhishuai Li", "Sun Yang", "Chi Harold Liu", "Rui Zhao", "Ziyue Li", "Hangyu Mao"], "abstract": "Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process. Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.", "sections": [{"title": "Introduction", "content": "Text-to-SQL, which involves the automatic transformation of natural language (NL) questions into structured SQL statements, is a pivotal component in facilitating seamless user interaction with databases [35]. Previous approaches to this task primarily focus on pattern matching between natural language and SQL statements, utilizing machine learning models to acquire the mapping between the two [60, 21]. However, the introduction and rapid advancement of Large Language Models (LLMs) have brought about a substantial transformation in this field [9, 33]. LLMs have emerged as powerful tools [38, 18, 43], showcasing tremendous potential in comprehending complex NL questions and generating accurate SQL statements. By combining advanced reasoning techniques and in-context learning capabilities, LLMs have significantly pushed the boundaries of the state-of-the-art in this domain, outperforming traditional methods by a considerable margin.\nDespite the continuous improvement of LLM-based methods in various benchmarks such as Spider [56] and BIRD [24], there remains a critical gap in the systematic benchmarking of these solutions [17, 35, 19]. The absence of a comprehensive framework for evaluating LLMs in Text-to-SQL complicates the design and assessment of effective systems. The risk of overfitting, particularly for LLMs trained on coding tasks and open-source datasets, poses a significant challenge to the reliability of benchmark evaluations [37]. Additionally, the optimal prompt engineering strategies that play a crucial role in guiding LLMs to generate accurate SQL queries are yet to be determined. Although various design schemes are explored in different methods [13], there is no consensus on the most effective prompt template. Furthermore, the current benchmarks, while comprehensive in their assessment of end-to-end Text-to-SQL tasks, have not yet provided a detailed exploration of the models' performance across the various sub-tasks and components of the Text-to-SQL process [32, 27]. A detailed exploration of these sub-tasks is crucial for a thorough evaluation of LLMs' cognitive capabilities and their role in facilitating the Text-to-SQL process. Therefore, it is necessary to develop a more granular benchmarking approach that can accurately reflect the multifaceted nature of Text-to-SQL and inform the creation of more effective LLM-based solutions.\nTo address the aforementioned challenges and fill the gap in the systematic benchmarking of LLMs in Text-to-SQL, we construct a comprehensive testing benchmark that provides a holistic assessment of LLM capabilities in this domain. Our approach begins with the construction of a Text-to-SQL dataset, designed to mitigate the risk of overfitting by considering question complexities, database sizes, and prerequisite knowledge. Formally, we devise five distinct tasks\u2014Text-to-SQL, SQL Debugging, SQL Optimization, Schema Linking and SQL-to-Text\u2014to comprehensively evaluate the capabilities of LLMs across the full spectrum of the Text-to-SQL process (see Figure 1). Subsequently, we perform an extensive analysis of various techniques that are essential for improving the in-context learning abilities of LLMs and their precision in generating SQL queries. Specifically, our evaluations are summarized as follows:\n\u2022 To determine the optimal prompt template, we partition the prompt text into distinct compo-nents and perform thorough testing of LLMs' performance on end-to-end Text-to-SQL tasks across all possible combination patterns.\n\u2022 Our benchmarking approach encompasses a range of LLMs, including both general-purpose and coding-specific models with varying parameter sizes. We determine the performance boundaries of these models and identify their performance disparities (see Figure 2).\n\u2022 For each task, we systematically assess the impact of information granularity on model performance and identify the optimal context learning strategies, such as zero-shot and few-shot, to maximize the performance of the models."}, {"title": "Related Work", "content": "Numerous traditional learning-based Text-to-SQL methods existed before the emergence of LLMs [60, 52, 29]. These methods can be essentially divided into non-seq2seq and seq2seq methods according to their network architecture. In Non-seq2seq methods, representative works [46, 2, 1, 15] typically employed a relation-aware self-attention mechanism as encoder to learn representations of questions and schemas, and then used a grammar-based decoder to generate the SQL as an abstract syntax tree [54], or utilized a sketch-based decoder to obtain the SQL via slot-filling [8, 16, 5]. These methods can further benefit from leveraging pre-trained language models like BERT and its extensions [7, 28, 55] for input embedding initialization. As another line of research, seq2seq methods [36, 26, 34, 23] directly translate NL questions into SQL queries through transformer-based [45] architectures in an end-to-end manner. These methods obtained competitive performance"}, {"title": "LLM-based Text-to-SQL Methods", "content": "With the advancements in LLMs, researchers are increasingly interested in creating a natural language interface for relational databases through the powerful linguistic and coding capabilities of LLMs, forge a new trend of LLM-based Text-to-SQL. Given the Powerful zero-shot reasoning and domain generalization abilities of LLMs, these methods successively refreshed the record on the cross-domain Spider leaderboard. C3 [9], a zero-shot Text-to-SQL method based on ChatGPT, provided treatment in terms of model input, bias and output, reaching an 82.3% execution accuracy on Spider leaderboard. DIN-SQL [33] proposed decomposing the Text-to-SQL task into smaller sub-tasks effectively and achieved an accuracy of 85.3% on Spider. DAIL-SQL [13] refreshes the accuracy of Spider with 86.6% through both supervised fine-tuning and a systematic study of in-context learning. It explored how to select the most helpful example and organize them properly in prompts in a few-shot scenario. Similarly, other studies investigated the selection of few-shot demonstrations by synthesising in-domain examples [3] and retrieving question skeletons [14]. MAC-SQL [47] utilized multi-agent collaboration for Text-to-SQL tasks and reached an accuracy of 59.6% on more challenging BIRD.\nSeveral other studies focused on special yet non-trivial scenarios, aiming to expand the scope of the Text-to-SQL task. DBCopilot [48] considered a more realistic problem, wherein it dealt with large-scale schemas, characterized by massive databases and a large number of tables. It proposed the use of a lightweight seq2seq copilot model for schema-routing, increasing scalability in comparison to traditional schema-linking. ACT-SQL [58] designed a chain-of-thought (CoT) [49] prompt to improve the reasoing ability when generating SQLs, and extended to the multi-turn Text-to-SQL task [22].\nIn summary, these methods primarily focused on improving the performance of the overall Text-to-SQL task through several aspects of the sub-tasks of interest in this paper. However, these methods 1) did not individually and systematically study the performance of sub-tasks, and 2) typically relied on experiments using only OpenAI LLMs (gpt-3.5-turbo, gpt4, etc.), without verifying the robustness of their approaches when applied to other open-source LLMs."}, {"title": "Datasets and Evaluation Metrics", "content": "WikiSQL [60] is considered the first large-scale dataset enabling the training and evaluation of learning-based Text-to-SQL methods, as well as offering a standardized benchmark for straight-forward comparison across various methods. It is also known as a cross-domain dataset, featuring over 25,000 tables and 80,000 question-SQL pairs comprising a variety of domains derived from Wikipedia.\nSubsequently, most of the recent Text-to-SQL works are done on Spider [56] because it is widely acknowledged as the most challenging cross-domain benchmark. It comprises 10,181 queries covering 138 different domains, involves multi-table queries (embodied by JOIN), complex SQL clauses (ORDER BY, GROUP BY, and HAVING, etc.) and nested SQLs. Several variants of Spider have been designed to evaluate the adaptability of Text-to-SQL methods [40, 12]. Spider-Realistic [6] removes the explicit mention of column names in the NL questions while keeping the SQL queries unchanged, which is more aligned with the use-case of Text-to-SQL in practice. Spider-Syn [10] replaces some schema words in the NL question with their synonyms that reflect real-world question paraphrases. Eliminating such explicit correspondence between NL questions and table schemas poses a greater challenge of Text-to-SQL methods. Spider-DK [11] integrates artificial domain knowledge, aiming to investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge.\nGiven that Spider was curated for benchmarking Text-to-SQL methods while diverging from real-world scenarios, KaggleDBQA [20] constructed a small-scale cross-domain dataset from Kaggle, highlighting the practicality and diversity of actual web-sourced databases. KaggleDBQA firstly comprises documentation and metadata of databases, raising an question of how this extra information could be used to improve the performance.\nThe newest seminal benchmark is BIRD [24], comprising 12,751 Text-to-SQL pairs and 95 databases with a size of 33.4 GB. Different from previous datasets, it is primarily designed to evaluate 1) the SQL execution efficiency in large-scale databases and 2) the ability of using external knowledge evidence. In this paper, we construct a novel dataset built upon BIRD and use it in the evaluation (see Section 3.3).\nTwo primary evaluation metrics for assessing the accuracy of SQLs on Spider are Exact Matching(EM) and Execution Accuracy(EX). EM measures whether the predicted query as a whole is equivalent to the gold query. It is possible to encounter false negative evaluations since a question might be solvable by multiple syntactically different but semantically identical SQL statements. EX is a more widely used metric, measures whether the result of executing the predicted query matches the gold value. We use EX to evaluate the accuracy of SQLs in this paper. Additionally, BIRD further proposed Valid Efficiency Score (VES), an integrated metric assessing both accuracy of execution results (i.e., EX) and the execution efficiency of SQL queries. To increase the VES, a Text-to-SQL method requires enhancing execution efficiency of SQL queries without sacrificing EX. We use this metric to assess the SQL optimization in Section 4.3. Refer to Appendix C.1 for detailed definitions of evaluation metrics."}, {"title": "Settings", "content": "In an LLM-based Text-to-SQL system, LLMs are employed to facilitate the transformation of natural language questions into executable SQL queries. Specifically, Let Q be a natural language question and S be the database schema. S is defined by a tuple $S = (T,C,K)$, where $T = {t_1,...,t_m}$ represents multiple tables, $C = {C_1,...,C_n}$ represents columns, and K represents foreign key relationships. The goal is to produce a SQL query Y such that Y is executable and accurately represents the intent of Q. Given the prompt template P(Q, S), the generation process of the SQL query Y by an LLM M can be formally defined as a conditional probability distribution:\n$P_M(Y|P(Q,S)) = \\prod_{i=1}^{|Y|}P_M(V_i|P(Q,S), V_{1:i-1}).$ (1)\nHere, LLM autoregressively generates each token, $V_i$ denotes the i-th token of the SQL query Y, and |Y| denotes the length of the query V."}, {"title": "Evaluation Models", "content": "Our benchmarking study evaluates the performance of two distinct categories of LLMs with varying parameter sizes: general-purpose and coding-specific. General-purpose LLMs are designed for versatile text generation and comprehension across diverse domains, trained on extensive internet text datasets. Specifically, ChatGPT (gpt-35-turbo-16k), LLaMa2-Chat-70B [44], InternLM-70B and InternLM2-20B are selected as the main baseline models. Coding-specific LLMs are fine-tuned and optimized for programming scenarios, excelling in code generation and technical language understanding. In this paper, the performance analysis of Codellama-34B [37] and SQLCoder-34B are provided."}, {"title": "Dataset Construction", "content": "We conduct a preliminary assessment of the performance of various LLMs on multiple open-source datasets. As depicted in Table 1, the performance of LLMs varies inconsistently across differ-ent datasets. Specifically, on the Spider dataset, Codellama-34B outperforms InternLM-70B and SQLCoder-34B, while on the Bird dataset, SQLCoder-34B surpasses InternLM-70B and Codellama-34B. On the one hand, there may be differences in the problem types that different LLMs excel at handling. On the other hand, considering that LLMs learn and train from large corpora, these findings suggest that the performance discrepancies observed could be attributed to the potential utilization of open source datasets during the fine-tuning process for coding-specific LLMs. This poses challenges in ensuring the reliability of evaluation results obtained on these datasets.\nTo address the potential overfitting of LLMs, particularly those specialized in coding tasks, and to ensure a reliable and accurate assessment of their capabilities, we construct a novel dataset, termed \"BigTable-0.2k\". This dataset is an extension and augmentation of the BIRD dataset, which is a recently released and widely acknowledged benchmark for evaluating Text-to-SQL parsing.\nSpecifically, our construction process involves a systematic analysis of the original BIRD dataset, identifying queries of varying difficulty levels and involving different numbers of tables (1, 2, 3, and more than 3). We modify and expand these queries by altering table and column names, as well as filtering conditions, to create a more diverse set of challenges. In cases where the original dataset lacks sufficient examples with four or more tables (there are only 20 instances in BIRD-Dev dataset), queries that involved three tables are expanded to four. As shown in Table 2, this process generates 50 new instances for each category, resulting in the \"BigTable-0.2k\u201d dataset. Moreover, each instance in the dataset underwent mutual verification by at least two individuals to ensure the accuracy of the data."}, {"title": "Evaluation", "content": "In this section, we formally evaluate the different sub-tasks within the Text-to-SQL process to determine the performance differences among various LLMs and provide recommendations for addressing specific task requirements."}, {"title": "Text2SQL", "content": "Unlike previous learning-based studies, the primary challenge in LLM-based Text-to-SQL is the design of an effective prompt template P (as introduced in Section 3.1) for LLMs to generate accurate SQL queries, known as prompt engineering. Researchers have evaluated a variety of prompt templates [13]. However, these representations lack uniformity in their structure, making it difficult to find out how a specific feature within a prompt template impact performance.\nTo address this issue, we investigate a more unified series of prompt templates. As shown in Listing 1- 3, these templates differ across three features:\n\u2022 DDL/SimpleDDL prefix affects the representation of the database schema S.\n- \"DDL\" (Data Definition Language) encompasses the standardized language that in-cludes commands for defining the structure and properties of a database, providing detailed information necessary for database creation, including column types and primary/foreign keys.\n- \"SimpleDDL\u201d is simplified by only supplying table and column names.\n\u2022 MD/HTML/Coding infix wraps the entire prompt template with Markdown syntax, HTML snippets and code comment blocks.\n\u2022 Complete/Chat postfix indicates the task of either completing SQL statements based on the \"SELECT\" clause or directly answering questions."}, {"title": "End-to-End Text2SQL evaluation.", "content": "This research conducts an end-to-end evaluation of the Text-to-SQL capabilities of various LLMs using the \"SimpleDDL-MD-Chat\" prompt template on the \"BigTable-0.2k\" dataset, with results depicted in Table 4.\nComparison of Performance Across Different Models. The results demonstrate a clear performance hierarchy among the models, with SQLCoder, CodeLlama, InternLM, and InternLM2 consistently outperforming Llama2-Chat. This finding highlights the effectiveness of coding-specific models, such as SQLCoder and CodeLlama, in the Text-to-SQL domain. Additionally, certain general-purpose models, like InternLM and InternLM2, can achieve performance levels comparable to specialized models, even without fine-tuning for coding tasks.\nDifficulty Comparison Across Different Numbers of GT Tables. We examine the query difficulty based on the number of GT tables involved. The results reveal a decrease in EX as the number of GT tables increases. Notably, the EX for models on queries with two GT tables is unexpectedly lower compared to those with three or more GT tables. This observation can be attributed to the fact that queries involving two GT tables have the highest average number of columns (see Table 2).\nCore Conclusion 2. As the number of tables and columns involved in user queries increases, the Text-to-SQL challenge for LLMs significantly escalates."}, {"title": "SQL Debugging", "content": "In recent studies, researchers have demonstrated that LLMs possess self-reflection and self-correction capabilities similar to those of humans [53, 41, 57]. Additionally, previous studies also investigate the potential of LLMs to debug the code they generate [4, 33]. In this section, we provide a comprehensive analysis of the performance of numerous SQL debugging methods across LLMs."}, {"title": "Debugging Dataset", "content": "We collect incorrect SQL queries generated by various LLMs in Section 4.1.2 and visualize the distribution of their error information, as shown in Figure 3. The error information can be divided into two categories:\n\u2022 System Error refers to syntax errors in the SQL statement, and the detailed system error information is generated by the Database Management System (DBMS), e.g., \"syntax error\" and \"no such column\".\n\u2022 Result Error indicates that the syntax of the SQL statement is correct, but the execution result does not match the ground truth.\nThe word cloud distribution reveals that \u201cno such column\" and Result Error are the primary areas of error concentration for all models. Additionally, more advanced models exhibit a greater proportion of the Result Error. This aligns with expectations, as powerful models are less prone to low-level System Errors. However, the concise nature of the error information in the Result Error category significantly hampers the debugging performance. Therefore, we propose a further detailed classification method.\nSpecifically, in addressing the Result Error category, we categorize these errors based on the logical construction of SQL statements. This classification is prioritized according to the logical structure within the SQL query. It is delineated in order into the following five subcategories:\n(1) Table Query Error pertains to issues related to the selection of tables in the SQL query. It is further subdivided into three types: Excessive/Missing/Incorrect Tables, which respectively address scenarios where unnecessary tables are included, required tables are omitted, or the wrong tables are referenced.\n(2) Column Selection Error focuses on the appropriateness of column selection. Similar to the Table Query Error, it is broken down into Excessive/Missing/Incorrect Columns.\n(3) Join Columns Error examines the errors associated with JOIN operations.\n(4) Condition Filter Error encompasses errors that occur in the conditions used to filter the data, including incorrect comparisons or misapplied filters.\n(5) Data Processing Error pertains to errors in the data processing stage, which includes aggregations, calculations, etc. applied to the data within the SQL query.\nDuring the categorization process, we employ rules to determine the first three error types and utilize LLMs to perform binary classification for the last two error types (see Appendix A.2). The distribution of different subcategories within Result Error is shown in the bottom section of Figure 3."}, {"title": "Debug Evaluation", "content": "To assess the impact of different levels of information granularity on performance, we propose 5 distinct strategies for self-debugging, progressively incorporating more detailed information:\n\u2022 Regenerate. Simply regenerate the SQL query with the same prompt in Section 4.1. This setting acts as a baseline to eliminate the impact of model randomness.\n\u2022 w/ Wrong SQL. Let LLMs generate a new SQL query based on the wrong SQL statement.\n\u2022 w/ Wrong SQL + System_error_info. Provide the wrong SQL statement, the corresponding System Error information and the rough Result Error information.\n\u2022 w/ Wrong SQL + All_error_info. Add detailed Result Error information for those SQL queries that are syntactically correct but semantically wrong.\n\u2022 w/ Wrong SQL + All_error_info + Comment. Add manual annotations for all error information. See Appendix A.2 for a detailed prompt template.\nWhat is the most powerful information organization of self debug? As shown in Figure 4, it is evident that the self-debugging performance of LLMs exhibits an upward trend with the introduction of more granular error information. In the absence of additional information, LLM does not possess the capability to regenerate correct answers. However, all models are able to comprehend fine-grained error information, whether it includes comments or not, and rectify their own mistakes.\nCore Conclusion 3. Detailed error information and corresponding annotations greatly enhance the capabilities of LLMs, enabling them to effectively correct errors."}, {"title": "SQL Optimization", "content": "Execution efficiency of SQL queries is a critical aspect, particularly in real-time systems that utilize large-scale databases. In this section, we further explore whether LLMs are able to enhance the execution efficiency of correct SQL queries. Formally, the SQL optimization process [31] involves transforming the initial SQL queries Y into an optimized form, denoted as $Y^\\circ$, with the goal of improving efficiency while maintaining identical results:\n$Y^\\circ = f_M(P_O(Y, I)),$ (2)\nwhere $P_O$ and I are the corresponding prompt template and the additional information used for SQL optimization, $f_M(\\cdot)$ represents the mapping function of the LLM M.\nVES is commonly employed to evaluate the efficiency of SQL query execution. However, in practice, LLMs can sometimes rewrite a correct SQL query into an incorrect one, making it challenging to figure out if the main reason for the decline in VES is due to these incorrect rewrites or a decrease in the SQL execution efficiency. Therefore, we propose a complementary metric C-VES (Correct-VES):\nC-VES = $\\frac{\\sum_{n \\in N_c} R(Y, \\hat{Y}_n)}{N_c}$ , (3)\nwhere $N_c = \\{n | I(\\hat{Y}_n, Y_n) = 1\\}.$ (4)\nHere, $N_c$ represent the set of accurate SQLs (see Appendix C.1 for detailed notations). C-VES is designed exclusively to validate the capability of LLMs to generate more efficient SQL queries, regardless of the potential drawback of rewriting correct SQLs into erroneous ones.\nDo LLMs have the capability for SQL self-optimization? To the best of our knowledge, we are the first to consider utilizing LLMs for SQL optimization. Specifically, we devise an extensive suite of prompts $P_O$ curated to SQL optimization:\n\u2022 with $Y$: In this basic form, only original SQL statements are provided.\n\u2022 w/ $Y + S + Q$: Further incorporates the database schema S and the user question Q.\n\u2022 w/ demo: Introduce few-shot demonstrations without explanations. Demonstrations are intu-itively designed, incorporating common optimization rules, such as substituting \u201cCOUNT(*)\u201d with \"COUNT(<column_name>)\".\n\u2022 w/ demo + comments: Add an explanation for the few-shot demonstrations. See Ap-pendix A.3 for a detailed prompt template.\n\u2022 SimpleDDL-MD-Chat-Efficiency: To avoid the accumulation of errors caused by multiple generations, this prompt template require LLMs to directly generate the most efficient SQL query statement based on user query.\nThe effectiveness of these SQL optimization methods are demonstrated in Table 6. Almost all two-stage methods experience a significant decrease in VES. It can be attributed to the possibility of LLMs optimizing the correct SQL statements into incorrect ones, thereby resulting in a further decrease in accuracy. Even when considering only the correct results, the performance improvement in terms of execution efficiency brought by the optimized SQL statements is almost negligible. Furthermore, it is intriguing to note that directly instructing the LLM to generate efficient SQL statements appears to achieve improved accuracy. This suggests that placing higher demands on the LLM could yield surprisingly positive outcomes.\nCore Conclusion 6. In-context learning methods present challenges in achieving effective SQL optimization with LLMs."}, {"title": "SQL-to-Text", "content": "The goal of SQL-to-Text is to transform the SQL query back into its original natural language question [51, 30, 42]. While it seems that SQL-to-Text cannot serve as a sub-task within the Text-to-SQL pipeline to improve the performance of End-to-End Text-to-SQL systems, employing this conversion as a supplementary step within the pipeline can indeed provide valuable insights. By converting the generated SQL statements back into text and juxtaposing these with the semantics of the original user questions, we can assess the accuracy of the SQL statements produced. In addition, it can assist researchers in evaluating the semantic comprehension capabilities of different LLMs, thus facilitating the development of more effective Text-to-SQL methodologies.\nTo this end, we assess the performance of SQL-to-Text across different LLMS (See Appendix A.4 for prompt templates). The selected metrics for evaluation encompass the F1 values of Rouge-1/2/L and BertScore, along with the application of LLM to assess the semantic coherence between the two texts. The evaluation results are depicted in Table 7. InternLM2 demonstrates the highest performance,"}, {"title": "Schema Linking", "content": "Schema linking is recognized as a crucial prerequisite of generating correct SQL queries. It involves aligning entity references in the question with the corresponding schema tables or columns, requiring the model to understand both structure and value of the database, as well as the the semantics of user questions. In LLM-based Text-to-SQL, prior studies [9, 33] design prompt instructions with in-context learning examples to enable LLMs to retrieve linked tables and columns, which are then used for the downstream Text-to-SQL task. However, none of these methods individually evaluate the performance of schema linking, and explicit evaluation metrics have yet to be established. Moreover, despite considerable advancements in semantic comprehension and generalization brought by LLMs, the performance of schema linking is still far from promising.\nIn this section, we aim to bridge these gaps by: (1) introducing a elaborately designed metrics to assess schema linking methods, (2) presenting a novel schema linking method \u201cPreSQL\u201d, which demonstrates superior performance, (3) conducting a comprehensive evaluation of a range of schema linking methods across various LLMs."}, {"title": "Evaluation Metric for Schema Linking: RES", "content": "What schema linking method is considered good? A straightforward goal of schema linking is that the GT tables should be retrieved as much as possible. However, due to the ambiguity inherent in natural language and the potential semantic similarities between candidate tables, retrieving more GT tables typically comes at the cost of a higher redundancy, known as the precision-recall trade-off. As discussed in [33], excessive table retrieval can introduce redundant joins between tables, potentially impairing the EX of Text-to-SQL generation.\nTherefore, the objective of schema linking is to retrieve all GT tables while avoiding the retrieval of excessive tables (with minimal redundancy). To evaluate this, we design a comprehensive metric called Retrieval Efficiency Score (RES), defined as:\nRES = $\\frac{\\sum_{n=1}^N 1(T_n, \\hat{T}_n) \\cdot R(T_n, \\hat{T}_n)}{N}$, (5)\nwhere $1(T_n, \\hat{T}_n) = \\{\\begin{array}{ll}1, & \\text{if } T_n \\subseteq \\hat{T}_n \\\\0, & \\text{if } T_n \\nsubseteq \\hat{T}_n\\end{array}$, R(T_n, \\hat{T}_n) = $\\frac{|T_n|}{|\\hat{T}_n|}$ (6)\nwhere $T_n$ and $\\hat{T}_n$ denote the set of GT tables and retrieved tables for the n-th instance, respectively, | refers to the scale of a set.\nWe emphasize that the RES serves as a more appropriate metric for evaluating schema linking than the F1-score. This is because it aligns the principle that recalling all GT tables is more important than increasing the precision of retrieval, as the former constitutes a prerequisite for generating correct SQL queries."}, {"title": "Schema Linking Evaluation", "content": "The methods we evaluated are as follows (see Appendix A.5 for the full prompt of these methods):\n\u2022 Zero Shot: A schema linking prompt proposed in C3 [9], which instructs the LLM to rank all the tables from relevant to irrelevant in a zero-shot manner.\n\u2022 Few Shot: Instructing the LLM to retrieve only the most important tables with few-shot demonstration, presented in DIN-SQL [33].\n\u2022 PreSQL: First, we employ the zero-shot Text-to-SQL described in Section 4.1 to generate a preliminary SQL query. From this preliminary SQL, table and column entities are parsed to serve as the retrieval results for schema linking.\n\u2022 Few Shot + PreSQL: This approach takes the union of the retrieval results from both the Few Shot and PreSQL methods, aiming to leverage the strengths of each.\nNote that we organize the information of the database schema in a way similar to \"SimpleDDL-\" in the prompt of all the methods mentioned above, which ignores the information about foreign keys. However, as argued in [46], foreign keys embody features of known schema relations and provide importance clues for understanding the database structure. To this end, we conduct experiments under both settings, w/ and w/o foreign keys, to investigate how incorporating foreign keys in the prompt influences the performance of schema linking. Results are demonstrated in Table 8 (refer to Appendix B for results on detailed metrics like Exact Match & Subset Match).\nWhich method achieved the best performance in schema linking? It can be seen that code-specific models excel in performance when utilizing the PreSQL approach, whereas general-purpose models yield optimal results through the Few-Shot + PreSQL method. This aligns with our expectations, as these two types of models excel in coding tasks (see Section 4.1) and semantic understanding tasks (see Section 4.4), respectively.\nCan foreign key information facilitate schema linking? The introduction of foreign key information yield improved performance across all methods and all LLMs. This is evident since a valid JOIN operation in SQL queries is typically based on foreign keys. The foreign key information helps the model retrieve more GT tables by indicating all potential table pairs involved in a JOIN operation.\nCore Conclusion 8. Foreign key information is capable of advance the performance of schema linking. PreSQL yields the highest performance on coding-specific models, and integrating the results from Few Shot can further enhance performance on general-purpose models."}, {"title": "Conclusion", "content": "In this study, we conduct a systematic benchmarking of the various sub-tasks within the Text-to-SQL pipeline, encompassing Text-to-SQL, SQL-Debugging, SQL-Optimization, SQL-to-Text, and Schema-Linking. Our comprehensive evaluation involves six distinct LLMs, spanning both general-purpose and coding-specific models. We focus on determining the optimal prompt templates for each task, assessing performance variations among different approaches, and identifying the distinct capabilities and limitations of each LLM. The results of the study demonstrate notable performance variations across the LLMs, underscoring the significance of careful model selection and prompt engineering in attaining optimal outcomes in text-to-SQL tasks. Our benchmarking provides a meticulous perspective on the pipeline, equipping the research community with strategies to improve the semantic understanding and computational performance of LLMs. This advancement contributes to the development of more reliable Text-to-SQL systems."}, {"title": "Details for Sub-tasks", "content": "In this section, we present the detailed definition of evaluation metrics in this paper. EX is defined as:\nEX = $\\frac{\\sum_{n=1}^N 1(Y_n, \\hat{Y}_n)}{N}$ , (7)\nwhere $1(Y_n, \\hat{Y}_n) = \\begin{cases}1, & \\text{if } Y_n = \\hat{Y}_n \\\\0, & \\text{if } Y_n \\neq \\hat{Y}_n\\end{cases}$ (8)\nwhere $Y_n$ and $\\hat{Y}_n$ denote execution results of the predicted SQL and the GT SQL for the n-th instance in the evaluation set, respectively. The evaluation metric VES used for assessing the performance of SQL optimization is defined as:\nVES = $\\frac{\\sum_{n=1}^N 1(\\hat{Y}_n, Y_n) \\cdot R(\\hat{Y}_n, Y_n)}{N}$ , (9)\nwhere R($\\hat{Y}_n$, $Y_n$) = $\\frac{E(\\hat{Y}_n)}{E(Y_n)}$, (10)\nwhere $\\hat{Y}_n$ and $Y_n$ denote the optimized predicted SQL and GT SQL for the n-th instance, respectively. E() serves as a function that quantifies the absolute execution efficiency (e.g., the reciprocal of execution time) of each SQL within a specified environment."}, {"title": "Details for SQL Debugging", "content": "Detailed comments for all Result Error are shown in Table 14."}, {"title": "Details for SQL Optimization", "content": "We suspect the reasons LLMs are challenging to achieve effective SQL optimization are as follows:\n\u2022 Lack of in-context learning guidance. For self-debug, models can benefit from the error messages given by DBMS (see the last two rows of Figure 4). However, SQL optimization tasks lack explicit feedback from the system and guidance through In-context learning, despite the comprehensively designed prompt templates.\n\u2022 Lack of pre-training data. During the pre-training phase, models can access massive correct SQL statements as training data, yet they may lack information on what kind of SQL statements are efficient.\n\u2022 System-level optimization is inherently challenging. The efficiency of a SQL query can only be assessed by executing it within a database system. This is a system-level optimization beyond the scope of syntactic-level and semantic-level language modeling areas in which LLMs excel. It is challenging for models to capture the mapping between SQLs and their execution efficiency."}, {"title": "Details for SQL-to-Text", "content": "Detailed description of evaluation metrics used for SQL-to-Text task are as follows:\n\u2022 Rouge [25"}]}