{"title": "FactFlow: Automatic Fact Sheet Generation and Customization from Tabular Dataset via Al Chain Design & Implementation", "authors": ["Minh Duc Vu", "Jieshan Chen", "Zhenchang Xing", "Qinghua Lu", "Xiwei Xu", "Qian Fu"], "abstract": "With the proliferation of data across various domains, there is a critical demand for tools that enable non-experts to derive meaningful insights without deep data analysis skills. To address this need, existing automatic fact sheet generation tools offer heuristic-based solutions to extract facts and generate stories. However, they inadequately grasp the semantics of data and struggle to generate narratives that fully capture the semantics of the dataset or align the fact sheet with specific user needs. Addressing these shortcomings, this paper introduces FactFlow, a novel tool designed for the automatic generation and customisation of fact sheets. FactFlow applies the concept of collaborative Al workers to transform raw tabular dataset into comprehensive, visually compelling fact sheets. We define effective taxonomy to profile Al worker for specialised tasks. Furthermore, FactFlow empowers users to refine these fact sheets through intuitive natural language commands, ensuring the final outputs align closely with individual preferences and requirements. Our user evaluation with 18 participants confirms that FactFlow not only surpasses state-of-the-art baselines in automated fact sheet production but also provides a positive user experience during customization tasks.", "sections": [{"title": "INTRODUCTION", "content": "Visual data storytelling is a powerful method for conveying a series of interconnected data facts and visualizations through a narrative flow. Among data-driven storytelling techniques, fact sheets are particularly effective. They integrate various data visualizations extracted from a dataset and weave them into a coherent narrative that effectively conveys key findings [47]. Fact sheets present information in a concise, single-page format, enhancing viewer retention and memorization [57]. With the digital transformation leading to an overwhelming influx of data, fact sheets have become increasingly vital for summarizing and presenting information.\nCreating a fact sheet involves intricate tasks, from identifying com-pelling data points to designing visualizations and crafting a persuasive narrative [44]. Despite advancements in commercial tools like Excel and Power BI, which suggest appropriate visual encodings for datasets [4], substantial manual effort is still required. Tasks like identifying crucial data points and designing fact sheets demand considerable expertise in data analysis and graphic design. To address these chal-lenges, research has introduced tools that streamline various aspects of fact sheet creation [60]. These tools automate the generation of visual-izations from tabular datasets, simplifying the creation of compelling visuals [17]. Additionally, they facilitate fact sheet layout design, ensur-ing clear and effective information presentation [15,57]. By assisting in narrative composition, these tools reduce the learning curve for users without extensive data analysis expertise [47]. DataShot [57], intro-duced in 2020, was the first tool to automatically generate fact sheets from tabular data, applying heuristics to rank and generate relevant facts. Calliope [47] further enhanced this by improving the relation-ships between facts for better content cohesion and logical flow.\nHowever, existing approaches to automatic fact sheet generation struggle to fully capture the semantics of dataset and present them effectively. At the fact component level, these methods often fail to grasp both the meaning of individual data columns and the relationships between them, leading to missed insights and the generation of irrele-vant conclusions. For instance, when analyzing the Tourism dataset (as shown in Fig. 1), which includes columns for country names, GDP, and tourism revenue, these methods might inappropriately aggregate GDP values across countries, rather than correctly attributing the contribution of tourism to each specific country's economy. At the fact sheet level, these approaches struggle to establish relevance between facts and form a coherent narrative, resulting in a lack of interconnection and a dis-jointed flow of information. Additionally, the reliance on slot-filling, template-based methods for text generation leads to content that can be difficult for general readers to comprehend especially without pro-viding related context of the dataset, such as unnatural statements like \"receipts_in_billions of France is 35.96\". These issues negatively impact the effectiveness of fact sheets, highlighting the need for more advanced techniques that make fact sheets accessible to a broader audience.\nThe rapid progress in generative AI has driven the development of Large Language Models (LLMs), such as GPT [40] and Llama [54]. These models, with in-context learning [37], have demonstrated re-markable proficiency in performing logical reasoning tasks [9]. Their inherent efficiency in handling diverse tasks without extensive retrain-ing represents a promising approach to providing AI solutions across various domains. In data visualization, existing research has explored the utility of LLMs at the level of individual visualizations [35, 60]. While these studies have shown improvements in segregated data anal-ysis tasks, generating a fact sheet is a complex task involving multiple sub-steps, each requiring distinct expertise\u2014from planning facts, ex-tracting data, to drawing visualizations and composing the fact sheet layout. Recently, to enhance the capability of LLMs for complex tasks, researchers have explored AI chain design [24], where it involves mul-tiple AI-based worker with specialized expertise to excel in its modular task. This design has improved output quality across domains, includ-ing online discussions [18] and social simulations [42]. Building on previous research, our primary objective is to investigate how AI-chain systems can improve the quality of generated fact sheets. Additionally, our research examines how these AI advancements can be integrated into data tasks, aiding not only data scientists but also novice users.\nWe introduce FactFlow, an automated fact sheet generation and customization tool designed to streamline the creation of fact sheets from raw datasets. FactFlow employs a novel approach featuring the collaboration of five specialized AI workers, each with distinct roles in processing and transforming the data. Upon receiving an input dataset and user requirements, FactFlow generates a dataset representation and passes it to the AI workers, where each worker executes its designated task, sequentially passing the output to the next worker in the pipeline. The result is a comprehensive fact sheet comprising interconnected facts. To enhance human-AI interaction, FactFlow includes a customization platform that allows users to review and modify the generated fact sheet using natural language commands, ensuring the final output aligns with specific user preferences.\nTo demonstrate the usability of the fact sheets generated by FactFlow, we present two fact sheets based on real-world datasets. Furthermore, we assessed the quality of our automatically generated fact sheets by conducting a user study with 18 participants. The results showed a preference for FactFlow's fact sheets over those from two state-of-the-art baselines [40,47], citing more comprehensive content and superior visual appeal. Additionally, we validated the customisa-tion interface by asking users to experiment FactFlow to customise a fact sheet and rate the quality of their customised fact sheets alongside their user experience. Participants confirmed that the customised fact sheets aligned with the dataset and their specific requests and praised the visual design. They also reported a positive user experience through the System Usability Scale questionnaire.\nTo summarise, the contributions of this paper include:\n\u2022 We proposed a taxonomy for configuring AI chain workers and introduced a tool, FactFlow, that leverages this taxonomy for fact sheet generation. This demonstrates the applicability of AI chains in managing complex, multi-step workflows within the domain of data storytelling. We hosted FactFlow as a public web-based automatic fact sheet generation and customisation platform for data community\u00b9.\n\u2022 We investigated the dataset representation aims for scalability and secure dataset communication to LLMs. Our data anomysation techniques propose the first step towards responsible AI usages for data works.\n\u2022 We used real-world datasets to illustrate the capability of our pro-posed system and validated FactFlow through a user study with 18 participants, which confirms its effectiveness and applicability across different expertise levels. The feedback provides future directions for human-AI collaboration on data tasks."}, {"title": "RELATED WORKS", "content": "Data analysis and visualization tools have seen significant advance-ments in recent years. Earlier tools like Excel and R required sub-stantial manual effort and expertise, but there has been a notable shift towards automation. This transformation is driven by research efforts that have progressively automated various aspects of the data analy-sis process [60]. Initial research focused on providing visualization recommendations from datasets [15, 29, 59], aiding users in the vi-sualization process. Other work improved existing visualizations by focusing on aspects such as visual style searching [25], assessing vi-sualizations [19], and annotation placement [7]. Industrial tools like Tableau and Power BI have begun integrating AI to streamline data management for business applications [26,39]. Although these systems have proven useful for data workers, novice users still face challenges in creating visualizations due to a lack of expertise.\nRecent research has shifted towards automatically generating charts from datasets. Data2Vis [17] and VizSmith [3] demonstrated the feasi-bility of generating structured code in Vega-Lite and Python to produce appropriate graphs. Despite these advancements, a common limitation has been the inadequate integration of the human perspective in the outputs. The generated visualizations may not fully align with user in-tentions, limiting the usability of these systems. NL4DV [38] addressed this by enabling chart generation based on natural language requests, aligning outputs more closely with user preferences. This capability was further enhanced by NL2Viz [63], which incorporated program context to better capture user intentions. However, heuristic-based se-mantic matching in these tools still struggled with linguistic challenges, such as synonyms. For instance, terms like \"yearly\" and \"annual\" were not recognized as equivalent. Recently, LLMs have been applied to data visualization tasks, solving the problem of natural language semantic understanding and accommodating greater flexibility in user input [35]. While this research has shown positive results with vari-ous prompt techniques, we aim to explore how specialized AI worker definitions can further improve the quality of generated charts."}, {"title": "Automatic Story-Telling and Fact Sheet Generations", "content": "In scenarios where isolated visualizations fail to convey complex datasets effectively, narrative visualizations play a crucial role in pre-senting data with an intuitive and coherent flow, thereby enhancing accessibility for general audiences [43]. Narrative visualizations en-compass a range of representations, including data videos [56], data comics, storylines [33], and infographics [14]. Early technologies pro-vided platforms for users to manually design story flows. For example, Infonice [58] assisted users in creating infographics, bridging the gap between data exploration and presentation. Similarly, DataSelfie [28] enabled users to create personalized data visuals. However, these tools required a deep understanding of data and experience in composing data storytelling artifacts, limiting their accessibility to general users.\nRecent technological advancements have led to the development of automated storytelling approaches [10], offering a more accessible method for composing storytelling artifacts. These tools capture infor-mation from datasets and craft narratives around them. AutoClip [46], for example, automatically generates data videos to showcase fact find-ings. News Views [21] provides an automated pipeline that extracts topics and creates corresponding geographic visualizations using con-textual information from articles. Focusing on a more concise and data-driven format, recent advancements have also targeted the auto-mated generation of fact sheets. DataShot [57] was the first tool to automatically generate fact sheets from tabular datasets, focusing on deriving quality facts from the data. However, this approach overlooked the overall story flow due to a lack of semantic relationship identifica-tion between different visualizations. Calliope [47], an improvement upon DataShot, applied a logic-oriented Monte Carlo tree search al-gorithm to construct the story from facts. Despite this enhancement, Calliope still faces challenges in creating a seamless narrative across different visualizations due to limited understanding of data seman-tics. Additionally, previous fact sheet generation approaches did not consider user input and struggled with generating engaging textual components, such as chart titles and descriptions, due to their reliance on template-based methods [47,57]. Building on this evolving research trajectory, FactFlow aims to enhance the integration and natural flow of charts with more precise and natural captions."}, {"title": "Large Language Models for Data Tools", "content": "Recent advancements in generative AI have driven the development of Large Language Models (LLMs), designed to understand natural lan-guage and generate human-like responses. Through prompt engineering techniques [37], LLMs have improved the accuracy of heuristic-based solutions while reducing the need for extensive data and complex model training. Research on the applicability of LLMs spans various fields, from healthcare [8] and finance [61] to IT sectors like testing [16], automation [45], and smart devices [30]. However, LLMs struggle with complex multi-step tasks, necessitating further research to enhance their usability. To address this, Wu et al. introduced AI chains [62], which break down complex tasks into manageable subtasks, each handled by a specific prompt. PromptSapper builds on this concept by defining the roles and collaboration of workers within the chain [13].\nIn the domain of data tools, pioneering research has explored the capabilities of LLMs in generating visualizations and aiding other visualization-related tasks. LLM4Vis, for instance, conducted experi-ments using a ChatGPT-based approach for visualization recommen-dations [55], demonstrating the model's applicability for data tasks. Additionally, Chat2VIS [35] conducted a comparative study on the abil-ities of ChatGPT, Codex, and GPT-3 in rendering single visualizations from language queries. Other studies have also explored enhancing user interaction with visualizations by integrating LLMs to develop chatbot assistants for datasets [27]. Recently, ChartGPT [53] performed fine-tuning of LLMs on a specialized dataset of visualizations to gen-erate accurate and expressive charts from abstract natural language inputs. Additionally, DataTales [51] utilized LLMs' natural language capabilities to generate textual narratives accompanying charts in data-driven articles. This research has paved the way for advanced LLM applications in creating robust, user-centric data tools. Building on this, FactFlow adopts a holistic approach, leveraging LLMs to generate complete fact sheets from tabular datasets. Our goal is to apply AI chain techniques to tackle complex data tasks and validate their feasibility."}, {"title": "SYSTEM DESIGN", "content": "We introduce FactFlow, an automated fact sheet generation system, as shown in Fig. 2. FactFlow applies the concept of an AI chain to generate fact sheets based on data and user requests. Users can further customize the fact sheet, such as by adding new facts or rearranging content. In this section, we first highlight the AI worker profiling and workflow, followed by a detailed description of the fact sheet generation process. Lastly, we present the FactFlow 's interface for generating and customizing fact sheets."}, {"title": "Al Worker Profiling & Workflow", "content": "While simple prompting techniques can effectively handle straight-forward, isolated tasks, such as visualization code generation, they face challenges when applied to more complex, integrated tasks. First, LLMs often lack domain-specific knowledge, which can result in out-puts that do not meet the nuanced requirements of specialized tasks. Second, providing an abstract task without thorough step-by-step plan-ning can disrupt the overall process, as LLMs may fail to propose the necessary sequence of actions to achieve the desired outcome. Finally, there can be incompatibilities between subtasks, where outputs from one stage may not align with the inputs required by the next, leading to pipeline failures and hindering task completion. To address these issues, we adopted the AI chain approach for generating fact sheets from tabular datasets. This approach leverages specialized AI work-ers, known as agents, each designed to handle specific subtasks more efficiently. These agents possess unique knowledge and capabilities tailored to their specific tasks, allowing them to focus on their assigned roles with greater precision, thereby improving the overall quality of the output. We define each worker with the following attributes:\nOverall Goal & Specific Role. To simulate the positive impact on work outcomes and productivity, we provide overall goals before detailing individual tasks, mirroring real-world scenarios [2]. The workers share common objectives while working on specific parts of the pipeline, stimulating goal-directed behaviors [49]. The specific role of each worker in the AI chain is then defined to achieve the overall objective.\nPersona. Persona refers to how each individual behaves and thinks [5], which has been shown to be crucial in optimizing the ef-fectiveness of AI workers [52]. While LLMs do not have a default persona, we manually define the persona of each worker, including the characteristic traits, motivation, and expertise necessary for suc-cessfully completing assigned tasks. For example, the Fact Composer is specified to be creative and analytical, while the Data Extractor is meticulous and knowledgeable. Further details of the AI workers we defined can be found in Section 3.2.2.\nInput & Output. We provide a list of input and output variables with their descriptions, formats, and data types that the agent receives or re-turns. This improves the reliability of the overall pipeline. Additionally, we include few-shot examples following this input and output format, which further enhances the reliability and alignment of the output [50].\nKnowledge Base. This refers to specialized domain knowledge that supplements the general capabilities of LLMs. For each task assigned to a worker, we compile and refine relevant datasets and prior research to build a tailored Knowledge Base. This allows the agent to perform optimally in its specialized task, avoiding the hallucination effects of LLMs when dealing with complex user requests [36].\nInstructions. Research has shown the importance of sub-step guid-ance over abstract instruction for task performance accuracy [34]. Therefore, we include step-by-step procedures that each worker follows to achieve its objectives.\nOur AI workers utilize a decentralized communication model, where interaction occurs directly between related agents [13]. Upon complet-ing a task, an agent forwards its output to the next designated agent in the workflow. To maintain consistency, all input and output data are standardized in JSON format, adhering to REST API protocols. For transmitting object files, such as images or data files, these are stored as block objects, with the directory path provided for file retrieval."}, {"title": "Fact Sheet Generation", "content": "Upon receiving the dataset and, optionally, a user request, the fact sheet generation process begins. FactFlow first analyzes the dataset to create a dataset representation and then utilizes the AI workers described in Sec. 3.2.2 to generate the fact sheet structure and visual components. Following this, we arrange the fact sheet contents on the canvas and export the initial fact sheet."}, {"title": "Dataset Representation", "content": "Given that LLMs process input as textual prompts, our goal is to identify the optimal method for capturing and conveying the essential details of the input dataset. Traditional approaches often involve feeding the entire dataset to the LLM, but this method faces significant limitations. Firstly, LLMs have token constraints, making it impractical to handle large datasets. Additionally, directly exposing the entire dataset can compromise privacy and intellectual property. Therefore, our approach focuses on incorporating sufficient dataset information into the prompts to guide LLMs in making accurate decisions without directly providing the dataset. We achieve this by presenting the dataset using SQL table creation commands, column statistics, and example rows. An example of dataset representation for the Global Population dataset with 3 columns (Country, Population, Continent) is shown in Fig. 2.\nSQL Table Creation Command. To capture the table name and column names along with their corresponding data types, we use SQL table creation command syntax. This approach also aids our AI workers in better formulating and generating SQL query syntax in subsequent processes.\nColumn Statistics. For each column, we generate statistics that summarize the values it contains. For numerical columns, this includes the maximum, minimum, median, average, and the 25th and 75th percentiles. For string columns, we count the number of unique values and identify the top five most frequent occurrences. These statistics provide LLMs with a comprehensive overview of the data range.\nAnonymized Data Examples. We include anonymized sample rows from the dataset to help LLMs understand the expected data values in each column. The number of rows presented depends on the token lim-its, with more columns requiring more tokens. Given that the input data may contain private information or intellectual property, anonymization is crucial to protect the confidentiality of the data [32]. We employ a format-preserving encryption technique that maintains semantic in-tegrity, preserving the semantic meaning and ordinal relationships of the data during anonymization. This allows LLMs to comprehend the underlying data structure without directly accessing the original dataset. The anonymization process involves classifying each column by its data type-nominal, ordinal, discrete, or continuous and applying a tailored anonymization method.\n\u2022 Nominal values are qualitative and lack a specific order or com-parability. To anonymize these values while preserving meaning, we substitute them with semantically equivalent terms. For in-stance, if the column refers to country names, \u201cFrance\u201d might be replaced by \u201cItaly.\u201d We use the column\u2019s entity type to guide this substitution, generating random values for each entity type and mapping them to unique values in the data.\n\u2022 Ordinal values have a specific order, and maintaining this order during substitution is crucial to avoid inconsistencies in later anal-ysis. For example, in a letter grading system where grades are A, B, C, D, and F, substituting these grades with random values could lead to confusion, such as when querying how many students re-ceived a grade better than C. To prevent this, we first extract all valid values from the dataset and ensure that the anonymized values are selected from this pool.\n\u2022 Discrete numerical values are anonymized using a mathematical transformation that maintains the ordinal relationships between values. For example, if the original values are integers repre-senting counts, the transformed values will still reflect the same relative magnitudes, ensuring consistency in any subsequent anal-ysis.\n\u2022 Continuous values such as those found in time series data, are treated with a different approach. We anonymize these by gen-erating new random values within the original range, ensuring that the data remains realistic and usable while masking the exact figures."}, {"title": "Fact Sheet Content Generation", "content": "After anonymizing the dataset, we feed the data into our AI chain to generate the fact sheet content. Users can provide additional requests in natural language to specify their requirements for the dataset. This feature is particularly useful when users are interested in covering only a specific portion of the dataset rather than the entire dataset. For example, a user may request data within a limited time range or focus on specific columns. This section discusses how each agent handles its tasks in the pipeline.\nFact Idea Composer. The primary task of this agent is to generate fact ideas from the dataset. To enhance the agent's capability, we incor-porated expert knowledge from a prior study that categorizes facts into eleven distinct types such as Distribution, Categorization, Trend and Rank [57]. Additionally, we conducted a design session with three se-nior data scientists, providing each with ten distinct datasets and asking them to suggest interesting facts for a fact sheet. We refined these suggestions by removing duplicates and resolving ambiguities, integrating the refined list of fact ideas into the Fact Idea Composer's knowledge base2. This ensures that the generated fact ideas align closely with expert expectations and are grounded in the actual data context. The agent is instructed to generate all potential facts and rank them based on relevance and significance, filtering out less important or redun-dant facts. The Fact Composer Agent employs the self-consistency prompting technique [12], using multi-turn communication to ensure output accuracy. The agent's output consists of a list of fact ideas, each categorized by fact type and content.\nData Extractor. Since each fact represents only a portion of the original dataset, FactFlow involves the Data Extractor to retrieve the data relevant to the fact. This agent receives a fact idea, additional user requests, and the dataset representation to generate an appropriate SQL command to query the original dataset. To improve the accuracy of converting natural language to SQL (NL2SQL), we added into the agent's knowledge base the Spider dataset [65], which is a well-known dataset containing the natural language questions and its corresponding SQL command. Given the challenges LLMs face in code generation, particularly in terms of robustness and accuracy [64], we apply the Code Generation with Advising and Validation approach [11]. The agent first lists recommendations for code generation, considering data types and required facts. It then generates an executable SQL command to query the data locally. In the validation step, the agent presents the data representation obtained from the initial query, along with the SQL command, to the LLMs to refine any syntactical errors. Upon completion, the agent outputs the filtered dataset relevant to the fact idea.\nData Visualizer. After filtering the data, each fact idea is presented through a visualization. The Data Visualizer uses the dimensions of the filtered dataset provided by the previous agent to recommend the most appropriate visualisation type and visual encoding channels To ensure the fact sheet's accessibility to a wide range of users, FactFlow supports widely recognized visualization types such as line charts, bar charts, scatter plots, pie charts, and area plots. We limit the number of encoded dimensions to three, resulting in a 2D chart with color as the maximum additional encoding. Unlike previous approaches that relied on direct code generation for creating graphs, we implemented a parameter-driven approach. In this method, a function generates different types of visualizations based on input parameters provided by the agent. These parameters may include chart type, data dimensions, axis labels, color schemes, and more. To enhance the Data Visual-izer's capability in generating accurate and relevant visualizations, we documented the function's input and output parameters. This documen-tation serves as a knowledge base, guiding the agent in appropriately configuring the parameters to produce the desired visual outputs.\nFact Writer. The Fact Writer leverages the processed dataset and visualizations from previous agents to generate factual statements that are well-aligned with the context provided by the charts. Additionally, the agent aims to clarify the underlying rationale of the presented data. While some inquiries related to the data can be directly addressed using the processed data from previous steps, others require a broader knowledge base for comprehensive explanation. Therefore, we instruct the agent to generate questions regarding the causal relationships in the presented data. The Fact Writer uses LLMs to create additional questions that explore the causation effects of external contexts, such as related historical events or geographical information. These questions help explain the key data points highlighted in the chart or explain what the fact implies. For example, if the dataset shows that France is the most visited country for tourism, the agent might ask, \"Why is France the most visited country for tourism?\" to gain deeper insights and present them. This enriched content significantly enhances the viewer's understanding and interpretation of the charts, especially when dealing with complex data that might be unfamiliar to the audience.\nFactsheet Organizer. The primary goal of this agent is to structure the fact sheet in the most logical order to enhance the learnability of the presented information. Our fact sheet contains different sections, each presenting a key topic with a list of relevant charts to support the idea. Using the list of facts and generated material from other workers, the Fact Sheet Organizer finds relevancy between facts to form the story flow with several key topics. After that, the agent sorts the list of facts into these topics based on the relevancy and outputs the structure of the fact sheet in JSON format for fact sheet generation."}, {"title": "Fact Sheet Layout", "content": "After establishing the structure of the fact sheet, we proceed to design its layout. We advocate for a dual-column layout to enhance readability and minimize eye movement [48]. The layout consists of multiple sections within each column, arranged vertically. Our objectives in developing the layout are twofold: First, to preserve the original logical order of the sections as outlined in the fact sheet structure, thereby enhancing its storytelling attributes. Second, to achieve a balanced distribution of content between the two columns. This balance not only contributes to the aesthetic appeal of the fact sheet but also en-sures content readability. A key challenge is dividing sections into two columns, given the varying sizes of each section due to differing numbers of facts. To address this, we assign a fixed height to each fact and calculate the cumulative height of each section. We then apply an optimization algorithm to distribute sections across the columns, as described in Algorithm 1. This algorithm processes an array of section heights in logical order and outputs a boolean array, assigning sections to the left column as \u201cTrue\u201d and to the right column as \u201cFalse\u201d. The Introduction section is always placed at the top left, so the first array value is set to \u201cTrue\u201d by default. FactFlow then uses this layout structure to iteratively assign sections to the columns, generating the complete fact sheet."}, {"title": "Fact Sheet Customization", "content": "In developing our automated pipeline for fact sheet generation, we emphasize the synergy between human input and AI automation, aiming to enhance human-AI collaboration with Al as an assistant in various tasks. To support this, we provide an interface that allows users to create and customize AI-generated content within the fact sheet. Users start by uploading their dataset on the Upload page (Fig. 3a) and can specify their interests. We also offer two sample datasets for experimentation. Upon clicking Begin, FactFlow automates the fact sheet generation and directs users to the Editor page (Fig. 3b), where they can dynam-ically tailor the content to their needs. The interface allows users to add, remove, and rearrange sections according to their preferences. For each visualisation, users can delete, rearrange or move them across sections. FactFlow supports adding new charts based on natural lan-guage request, processing this as new fact ideas through the AI chain to generate corresponding charts and textual content. This new fact will be presented in appropriate section automatically. After customization, users can export the fact sheet as a PDF. The generated content will be stored with the unique ID, where user can further continue to modify the same fact sheet in the future."}, {"title": "Implementation", "content": "FactFlow is implemented in Python 3.8, utilizing the Matplotlib library for generating visualizations and the FPDF library for creating and storing fact sheets. To facilitate seamless communication with the GPT-4 model from OpenAI, we establish a REST API interface. For public accessibility, FactFlow is deployed as a web application using Flask."}, {"title": "EVALUATION", "content": "We evaluate the usefulness of FactFlow through (1) two data stories generated by FactFlow to showcase the quality of the fact sheets, and (2) a comprehensive user study to validate both the quality of the generated fact sheets and the usability of the customization module."}, {"title": "Use Cases", "content": "To demonstrate the capabilities of our automatic generation tool, we present two illustrative fact sheets in Sec. 3.4. These fact sheets distill complex datasets into clear, insightful narratives using visualizations and storytelling.\nFigure 4a presents a dataset about Startup Failures, featuring 1,234 rows and 6 columns that record the details of companies that closed during or after the \"new economics\" boom in China from 2010 to 2019. This dataset covers various aspects of startup profiles, including the broken year (e.g., 1/1/2019), industry (e.g., Finance), location (e.g., Shanghai), funding status (e.g., Series B), survival time in days (e.g., 1,730), and cause of failure (e.g., supervision). The initial request was \"Analyze the reasons for startup failures across different loca-tions and industries.\u201d FactFlow structured the analysis into sections: industry distribution, startup trends and funding, and geographical dis-tribution. The Industry Distribution section reveals the dominance of the E-Commerce sector, while noting that the Education industry has the highest average survival time. The Startup Trends and Funding section highlights a peak in startups in 2017 and notes that most lack funding, underscoring investment challenges in various industries. The Geographical Distribution section identifies Beijing as a major innova-tion hub, leading with 519 startups, reflecting its significance in China\u2019s startup ecosystem.\nFigure 4b features a dataset on university majors and employment opportunities, encompassing 173 rows and 12 columns. This dataset in-cludes statistics on various university majors, such as the major\u2019s name (e.g., Accounting), its broader category (e.g., Business), total student enrollment (e.g., 1,778,219), employability statistics (e.g., Employed: 1,335,825; Unemployed: 75,379; Unemployment Rate: 0.07), and the median pay (e.g., $65,000) associated with each major. The initial request was \u201cFacts about university majors and employment status.\u201d FactFlow structured the analysis into four sections: Education Majors Overview, Top Majors, Proportion of Students in Each Major Cate-gory, and Top 5 Majors with the Highest Unemployment Rates. The Education Majors Overview notes that the Engineering category has the highest number of distinct fields, with 29 different majors. It then identifies the most popular majors, with Business Management and Ad-ministration leading in student enrollment among the top five. In terms of earning potential, Petroleum Engineering majors stand out with the highest median salary of $125,000. The analysis continues with the distribution of students across various major categories, highlighting that Business majors constitute the largest proportion at 24.75%. Lastly, the section on unemployment rates reveals that Miscellaneous Fine Arts has the highest unemployment rate at 15.6% and displays the correlation between the median salary and unemployment rate."}, {"title": "User Evaluation", "content": "We conducted a user study to assess the quality of the fact sheets generated by FactFlow and the usability of our fact sheet customiza-tion interface. First, to evaluate the quality of the fact sheets created by FactFlow, we involved data workers from industry and non-data workers to determine the tool\u2019s versatility in catering to both data professionals and general audiences. We presented the automatically generated fact sheets from FactFlow and two state-of-the-art baselines to the participants and collected their feedback. Second, we assessed the quality of customized fact sheets and the user experience during the customization process using FactFlow. This involved a user study where participants actively customized their fact sheets with FactFlow, followed by feedback collection."}, {"title": "Research Questions", "content": "This study aimed to answer the following research questions:\n\u2022 RQ1: How does the content and visual presentation of fact sheets generated by FactFlow compare to those produced by existing approaches?\n\u2022 RQ2: How does a user's experience level with data analytics influence their evaluation of fact sheets?\n\u2022 RQ3: What is the quality of customized fact sheets created using FactFlow, and how do users perceive the experience of customiz-ing fact sheets with this tool?"}, {"title": "Experimental Setup", "content": "Participants. We recruited eighteen participants (8 males", "visualizations": "Group 1 (N1-N6) with no prior experience with data work, Group 2 (J1-J6) with 1 to 5 years of work experience, and Group 3 (S1-S6) with over 5 years of experience. This division aimed to analyze the impact of work experience on expectations of fact sheet quality and the user experience with FactFlow. Each participant received a AU$50 Amazon gift card for their participation.\nDatasets. We used two different datasets to generate fact sheets for this experiment. First, we selected the CarSales dataset (as used in previous research [47", "47": "is a state-of-the-art tool for producing high-quality narrative flows in fact sheets. We used Calliope to benchmark improvements in fact sheet generation. The second baseline, ChatGPT Plus with the latest GPT-4 model, is recognized for its capability to interpret data files and generate graph-ical visualizations [40"}]}