{"title": "Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling", "authors": ["Junn Yong Loo", "Michelle Adeline", "Arghya Pal", "Vishnu Monn Baskaran", "Chee-Ming Ting", "Rapha\u00ebl C.-W. Phan"], "abstract": "Energy based models (EBMs) are appealing for their generality and simplicity\nin data likelihood modeling, but have conventionally been difficult to train due\nto the unstable and time-consuming implicit MCMC sampling during contrastive\ndivergence training. In this paper, we present a novel energy-based generative\nframework, Variational Potential Flow (VAPO), that entirely dispenses with im-\nplicit MCMC sampling and does not rely on complementary latent models or\ncooperative training. The VAPO framework aims to learn a potential energy func-\ntion whose gradient (flow) guides the prior samples, so that their density evolution\nclosely follows an approximate data likelihood homotopy. An energy loss function\nis then formulated to minimize the Kullback-Leibler divergence between density\nevolution of the flow-driven prior and the data likelihood homotopy. Images can\nbe generated after training the potential energy, by initializing the samples from\nGaussian prior and solving the ODE governing the potential flow on a fixed time\ninterval using generic ODE solvers. Experiment results show that the proposed\nVAPO framework is capable of generating realistic images on various image\ndatasets. In particular, our proposed framework achieves competitive FID scores\nfor unconditional image generation on the CIFAR-10 and CelebA datasets.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep generative modeling has garnered significant attention for unsupervised learning\nof complex, high-dimensional data distributions [1]. In particular, probabilistic generative models\nsuch as variational autoencoders [2], normalizing flows [3], score-matching or diffusion models\n[4-6]. Poisson flow [7, 8], and energy-based models (EBMs) [9, 10] aim to maximize the likelihood\n(probability density) underlying the data. By design, these probabilistic frameworks enhance training\nstability, accelerate model convergence, and reduce mode collapse compared to generative adversarial\nnetworks [11], albeit at the cost of a slow sampling procedure and poor model scalability [12].\nAmong these frameworks, EBMs have emerged as a flexible and expressive class of probabilistic\ngenerative models [9, 12\u201317, 10, 18]. EBMs model high-dimensional data space with a network-\nparameterized energy potential function that assigns data regions with energy that is directly (or\ninversely) proportional to the unnormalized data likelihood [19]. This provides a natural interpretation\nof the network model in the form of an energy landscape, thereby endowing EBMs with inherent\ninterpretability.\nDeep EBMs are particularly appealing since they impose no restrictions on the network architecture,\npotentially resulting in high expressiveness [1]. Moreover, they are more robust and generalize\nwell to out-of-distribution samples [9, 10] as regions with high probability under the model but low"}, {"title": "2 Background and Related Works", "content": "In this section, we provide an overview of EBMs, particle flow, and the deep Ritz method, collectively\nforming the cornerstone of our proposed VAPO framework."}, {"title": "2.1 Energy-Based Models (EBMs)", "content": "Denote $x \\in \\Omega \\subseteq \\mathbb{R}^n$ as the training data, EBMs approximate the data likelihood $P_{data}(x)$ via defining\na Boltzmann distribution, as follows:\n$P_\\theta(x) = \\frac{e^{\\Phi_\\theta(x)}}{\\int_\\Omega e^{\\Phi_\\theta(x)} dx}$   (1)\nwhere $\\Phi$ is an energy function modelled by deep neural networks. Given that the denominator of (1),\ni.e., the partition function, is analytically intractable for high-dimensional data, EBMs perform the"}, {"title": "2.2 Particle Flow", "content": "Particle flow, initially introduced by the series of papers [45], is a class of nonlinear Bayesian\nfiltering (sequential inference) methods that aim to approximate the posterior distribution $p(x_t|x_{0:t})$\nof the state of system given the observations. While particle flow methods are closely related to\nnormalizing flows [3] and neural ordinary differential equations [46], these latter frameworks do\nnot explicitly accommodate a Bayes update. In particular, particle flow performs the Bayes update\n$p(x_t|x_{0:t}) \\propto p(x_t|x_{0:t-1}) p(x_t|x_t, x_{0:t-1})$ by subjecting prior samples $x_t \\sim p(x_t|x_{0:t-1})$ to a series\nof infinitesimal transformations through the ordinary differential equation $\\frac{dx}{d\\tau} = v(x, \\tau)$\nparameterized by a flow velocity (field) function $v(x, \\tau)$, in a pseudo-time interval $\\tau\\in [0, 1]$ in\nbetween sampling time steps. The flow velocity is designed such that the driven Kolmogorov forward\npath evolution (Fokker-Planck dynamics, see (13)) of the sample particles, coincides with a data\nlog-homotopy (smooth path) that inherently perform the Bayes update. Despite its efficacy in time-\nseries inference [47\u201349] and resilience to the curse of dimensionality [50], particle flow has yet to be\nexplored in generative modelling for high-dimensional data."}, {"title": "2.3 Deep Ritz Method", "content": "The deep Ritz method is a deep learning-based variational numerical approach, originally proposed\nin [51], for solving scalar elliptic partial differential equations (PDEs) in high dimensions. Consider\nthe following Poisson's equation, fundamental to many physical models:\n$\\Delta u(x) = f(x), x\\in \\Omega$    (3)\nsubject to boundary condition\n$u(x) = 0, x \\in \\partial \\Omega$    (4)\nwhere $\\Delta$ is the Laplace operator, and $\\partial \\Omega$ denotes the boundary of $\\Omega$. For a Sobolev function\n$u \\in H_0^1(\\Omega)$ (see Proposition 2 for definition) and square-integrable $f \\in L^2(\\Omega)$, the variational\nprinciple ensures that a weak solution $u^*$ of the Euler-Lagrange boundary value equation (3)-(4) is\nequivalent to the variational problem of minimizing the Dirichlet energy [52], as follows:\n$u^* = arg \\min_{v} \\int_\\Omega (||\\nabla v(x)||^2 - f(x)v(x)) dx$  (5)\nwhere $\\nabla$ denotes the Del operator (gradient). In particular, the deep Ritz method parameterizes\nthe trial energy function $v$ using neural networks, and performs the optimization (5) via stochastic\ngradient descent. Due to its versatility and effectiveness in handling high-dimensional PDE systems,\nthe deep Ritz method is predominantly applied for finite element analysis [53]. In [54], the deep Ritz\nmethod is used to solve the probabilistic Poisson's equation resulting from the feedback particle filter\n[55]. Nonetheless, the method has not been explored for generative modelling."}, {"title": "3 Variational Energy-Based Potential Flow", "content": "In this section, we introduce a novel generative modelling framework, Variational Energy-Based\nPotential Flow (VAPO), drawing inspiration from both particle flow and the calculus of variations.\nFirst, we establish a homotopy that transforms a prior to the data likelihood and derive the evolution\nof the prior in time. Then, we design an energy-generated potential flow and a weighted Poisson's\nequation that aligns the evolving density distribution of transported particles with the homotopy-\ndriven prior. Subsequently, we formulate a variational loss function where its optimization with\nrespect to the flow-generating potential energy is equivalent to solving the Poisson's equation. Finally,\nwe describe the model architecture that is used to parameterize the potential energy function and the\nbackward ODE integration for generative sampling."}, {"title": "3.1 Bridging Prior and Data Likelihood: Log-Homotopy Transformation", "content": "Let $\\{x\\}_{i=1}^N \\in \\mathcal{N}$ denote the training data, $p_{data}(x)$ be the data likelihood, $x \\in \\Omega$ denote the approximate data\nsamples. To achieve generative modelling, our objective is to closely approximate the training data $x$\nwith the data samples $\\hat{x}$. On this account, we define a conditional data likelihood $p(x|\\hat{x}) = \\mathcal{N}(x;\\hat{x}, \\Pi)$\nwith isotropic Gaussian noise with covariance $\\Pi = diag(\\sigma^2)$ and standard deviation $\\sigma \\in \\Omega$. This is\nequivalent to considering a state space model $\\hat{x} = x + v$, where $v \\in \\Omega \\sim \\mathcal{N}(v; 0, \\Pi)$. Here, we set a\nsmall $\\sigma$ so that $\\hat{x}$ closely resembles the training data $x$.\nSubsequently, consider a conditonal (data-conditioned) density function $\\rho : \\Omega^2 \\times [0, 1] \\rightarrow \\mathbb{R}$, as\nfollows:\n$\\rho(x; \\hat{x}, t) = \\frac{e^{f(x; \\hat{x}, t)}}{\\int_\\Omega e^{f(x; \\hat{x}, t)} dx}$  (6)\nwhere $f: \\Omega^2 \\times [0, 1] \\rightarrow \\mathbb{R}$ is a log-linear function:\n$f(x; \\hat{x}, t) = log q(x) + t log p(x|\\hat{x})$  (7)\nparameterized by the auxiliary time variable $t \\in [0, 1]$, and we let $q(x) = \\mathcal{N}(x; 0, \\Lambda)$ be a isotropic\nGaussian prior density with covariance $\\Lambda = diag(\\omega^2)$ and standard deviation $\\omega \\in \\Omega$. Here, diag($\\cdot$)\ndenotes the diagonal function. By construction, we have $\\rho(x; \\hat{x}, 0) = q(x)$ at $t = 0$, and $\\rho(x; \\hat{x}, 1) = p(x|\\hat{x})$ at $t = 1$ since we have\n$\\rho(x; \\hat{x}, 1) = \\frac{e^{f(x; \\hat{x}, 1)}}{\\int_\\Omega e^{f(x; \\hat{x}, 1)} dx} = \\frac{p(x|\\hat{x})q(x)}{p_{data}(\\hat{x})} = \\frac{p(x|\\hat{x})}{\\frac{p_{data}(\\hat{x})}{q(x)}} = p(x|\\hat{x})$\n(8)\nwhere we have used the fact that $p_{data}(\\hat{x}) = \\int_\\Omega e^{f(x; \\hat{x}, 1)} dx = \\int_\\Omega p(x|\\hat{x}) q(x) dx$. Therefore, the condi-\ntional density function $\\rho(x; \\hat{x}, t)$ here (6) essentially represents a density homotopy between the prior\n$q(x)$ and the posterior $p(x|\\hat{x})$.\nIn particular, the density function $\\rho(x; \\hat{x}, t)$ also defines a conditional (data-conditioned) homotopy\nbetween the prior $q(x)$ and the exact posterior $p(x|\\hat{x})$, the latter of which gives a maximum a\nposteriori (Bayesian) estimate of the approximate data samples after observing true training data.\nTo obtain an estimate of the intractable data likelihood for generative sampling, we then consider a\n(approximate) data likelihood homotopy $p : \\Omega \\times [0, 1] \\rightarrow \\mathbb{R}$ as follows:\n$p(x; t) = \\int_\\Omega p_{data}(\\hat{x}) \\rho(x; \\hat{x}, t) d\\hat{x}$  (9)\nConsidering this, it remains that $p(x; 0) = q(x)$ at $t = 0$. Furthermore, given that we have\n$p(x; 1) = \\int_\\Omega p_{data}(\\hat{x}) p(x|\\hat{x}) d\\hat{x} = p(x)$ at $t = 1$, the data likelihood homotopy $p(x; t)$ here\ninherently performs a kernel density approximation of the true data likelihood, using the normalized\nkernel $p(\\hat{x})$ obtained from the conditional homotopy $\\rho(x; \\hat{x}, 1)$ at $t = 1$. Therefore, the approximate\ndata likelihood $p(x)$ acts as a continuous interpolation of the data likelihood $p_{data}(x)$, represented by\nDirac delta function $\\delta(x - \\hat{x})$ centered on the discrete training data $\\hat{x}$.\nNevertheless, the conditional homotopy (8) is intractable due to the normalizing constant in the\ndenominator. This intractability rules out a close-form solution of the data likelihood homotopy (9),\nthus it is not possible to sample directly from the data likelihood estimate. Taking this into account,\nwe introduce the potential flow method in the following section, where we model the evolution of the\nprior samples (particles) instead, such that their distribution adheres to the data likelihood homotopy."}, {"title": "3.2 Modelling Potential Flow in a Homotopy Landscape", "content": "Our aim is to model the flow of the prior particles in order for their distribution to follow the data\nlikelihood homotopy and converge to the data likelihood. To accomplish this, we first derive the\nevolution of the latent prior density with respect to time in the following proposition.\nProposition 1. Consider the data likelihood homotopy $p(x; t)$ in (9) with Gaussian conditional data\nlikelihood $p(x|\\hat{x}) = \\mathcal{N}(x; \\hat{x}, \\Pi)$. Then, its evolution in time $t \\in [0, 1]$ is given by the following PDE:\n$\\frac{\\partial p(x; t)}{\\partial t} = \\frac{1}{2} \\mathbb{E}_{p_{data}(\\hat{x})} [\\rho(x; \\hat{x}, t) (\\gamma(x, \\hat{x}) - \\overline{\\gamma}(x, \\hat{x}))]$   (10)"}, {"title": "3.3 Variational Energy Loss Function Formulation: Deep Ritz Approach", "content": "In this section, we introduce an energy method which presents a variational formulation of the\nprobabilistic Poisson's equation. Given that the aim is to minimize the divergence between the\ndata likelihood homotopy and the flow-driven prior and directly solving the probabilistic Poisson's\nequation is difficult, we first consider a weak formulation of (14) as follows:\n$\\int_\\Omega (\\frac{1}{2} \\mathbb{E}_{p_{data}(\\hat{x})} [\\rho(x; \\hat{x}, t) (\\gamma(x, \\hat{x}) - \\overline{\\gamma}(x, \\hat{x}))] - \\nabla . (p(x; t) \\nabla \\Phi(x))) \\Psi(x) dx = 0$  (15)\nwhere the equation must hold for all differentiable trial functions $\\Psi$. In the following proposition,\nwe introduce an energy loss objective that is equivalent to solving this weak formulation of the\nprobabilistic Poisson's equation.\nProposition 3. The variational problem of minimizing the following loss function:\n$\\mathcal{L}(\\Phi; t) = \\frac{1}{2} Cov_{p(x, t), p_{data}(\\hat{x})} [\\Phi(x), \\gamma(x, \\hat{x})] + \\frac{\\lambda}{2} \\mathbb{E}_{p(x; t)} [||\\nabla \\Phi(x)||^2]$    (16)\nwith respect to the potential energy $\\Phi$, is equivalent to solving the weak formulation (15) of the\nprobabilistic Poisson's equation (14). Here, $||\\cdot||$ denotes the Euclidean norm, and $Cov$ denotes the\ncovariance.\nFurthermore, the variational problem (16) has a unique solution if for all energy functions $\\Phi \\in\nH_0^1(\\Omega; p)$, the data likelihood homotopy $p$ satisfy the Poincar\u00e9 inequality:\n$\\mathbb{E}_{p(x; t)} [||\\nabla \\Phi(x)||^2] \\geq \\lambda \\mathbb{E}_{p(x; t)} [||\\Phi(x)||^2]$  (17)\nfor some positive scalar constant $\\lambda > 0$ (spectral gap).\nIn sum, leveraging Propositions 2 and 3, we reformulate the intractable task of minimizing the KLD\nbetween flow-driven prior and data likelihood homotopy equivalently as a variational problem with\nenergy loss function (16). By optimizing the potential energy function $\\Phi$ with respect to the energy\nloss and transporting the prior samples through the potential flow ODE (12), the prior particles follow\na trajectory that accurately approximates the data likelihood homotopy. In doing so, the potential\nflow $\\nabla \\Phi$ drives the prior samples to posterior regions densely populated with data, thus enabling us\nto perform generative modelling.\nThe minimum covariance objective in (16) plays an important role by ensuring that the normalized\ninnovation is inversely proportional to the potential energy. As a result, the potential-generated\nvelocity field $\\nabla \\Phi$ consistently points in the direction of greatest potential ascent, thereby driving the\nflow of prior particles towards high likelihood regions of the true posterior, as illustrated in Figure 1.\nIn other words, the potential energy is conjugate to the approximate data likelihood $p(x)$, analogous\nto Hamiltonian fluid mechanics [58]. It is worth noticing that instead of being an ad hoc addition,\nthe $L^2$ regularization term $\\mathbb{E}_{p(x; t)} [||\\nabla \\Phi(x)||^2]$ on the velocity field in (16) arises, from first-principle\nderivation, as a direct consequence of considering the data likelihood homotopy."}, {"title": "3.4 Energy Parameterization and ODE Sampling", "content": "To implement stochastic gradient descent on top of the energy loss function (18), we adopt the deep\nRitz approach and in particular, we model the potential energy function $\\Phi_\\theta$ as deep neural networks"}]}