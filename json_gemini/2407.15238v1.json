{"title": "Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling", "authors": ["Junn Yong Loo", "Michelle Adeline", "Arghya Pal", "Vishnu Monn Baskaran", "Chee-Ming Ting", "Rapha\u00ebl C.-W. Phan"], "abstract": "Energy based models (EBMs) are appealing for their generality and simplicity\nin data likelihood modeling, but have conventionally been difficult to train due\nto the unstable and time-consuming implicit MCMC sampling during contrastive\ndivergence training. In this paper, we present a novel energy-based generative\nframework, Variational Potential Flow (VAPO), that entirely dispenses with im-\nplicit MCMC sampling and does not rely on complementary latent models or\ncooperative training. The VAPO framework aims to learn a potential energy func-\ntion whose gradient (flow) guides the prior samples, so that their density evolution\nclosely follows an approximate data likelihood homotopy. An energy loss function\nis then formulated to minimize the Kullback-Leibler divergence between density\nevolution of the flow-driven prior and the data likelihood homotopy. Images can\nbe generated after training the potential energy, by initializing the samples from\nGaussian prior and solving the ODE governing the potential flow on a fixed time\ninterval using generic ODE solvers. Experiment results show that the proposed\nVAPO framework is capable of generating realistic images on various image\ndatasets. In particular, our proposed framework achieves competitive FID scores\nfor unconditional image generation on the CIFAR-10 and CelebA datasets.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep generative modeling has garnered significant attention for unsupervised learning\nof complex, high-dimensional data distributions [1]. In particular, probabilistic generative models\nsuch as variational autoencoders [2], normalizing flows [3], score-matching or diffusion models\n[4-6]. Poisson flow [7, 8], and energy-based models (EBMs) [9, 10] aim to maximize the likelihood\n(probability density) underlying the data. By design, these probabilistic frameworks enhance training\nstability, accelerate model convergence, and reduce mode collapse compared to generative adversarial\nnetworks [11], albeit at the cost of a slow sampling procedure and poor model scalability [12].\nAmong these frameworks, EBMs have emerged as a flexible and expressive class of probabilistic\ngenerative models [9, 12\u201317, 10, 18]. EBMs model high-dimensional data space with a network-\nparameterized energy potential function that assigns data regions with energy that is directly (or\ninversely) proportional to the unnormalized data likelihood [19]. This provides a natural interpretation\nof the network model in the form of an energy landscape, thereby endowing EBMs with inherent\ninterpretability.\nDeep EBMs are particularly appealing since they impose no restrictions on the network architecture,\npotentially resulting in high expressiveness [1]. Moreover, they are more robust and generalize\nwell to out-of-distribution samples [9, 10] as regions with high probability under the model but low\nprobability under the data distribution are explicitly penalized during training. Additionally, EBMs,\nwhich trace back to Boltzmann machines [20], have strong ties to physics models and can thus borrow\ninsights and techniques from statistical physics for their development and analysis [21]. On these\ngrounds, EBMs have been applied across a diverse array of applications apart from image modelling,\nincluding text generation [22, 23], point cloud synthesis [24], scene graph generation [25], anomaly\ndetection [26, 27], earth observation [28], robot learning [29, 30], trajectory prediction [31, 32], and\nmolecular design [33, 34].\nDespite a number of desirable properties, deep EBMs require implicit Langevin Markov Chain\nMonte Carlo (MCMC) sampling during the contrastive divergence training. MCMC sampling in\na high-dimensional setting, however, has shown to be challenging due to poor mode mixing and\nexcessively long mixing time [1, 9, 14, 15, 10, 35]. As result, energy potential functions learned with\nnon-convergent MCMC do not have valid steady-states, in the sense that MCMC samples can differ\ngreatly from data samples [12]. Current deep EBMs are thus plagued by high variance training and\nhigh computational complexity due to MCMC sampling. In view of this, recent works have explored\nlearning complementary latent model to amortize away the challenging MCMC sampling [36-40], or\ncooperative learning where model-generated samples serve as initial points for subsequent MCMC\nrevision in the latent space [41, 42]. While such approaches alleviate the burden of MCMC sampling,\nit comes at the expense of the inherent flexibility and composability of EBMs [13]. Moreover,\nco-optimizing multiple models adds complexity [43, 44] to the implementation of these approaches.\nIn this paper, we introduce Variational Potential Flow (VAPO), a novel energy-based generative\nframework that eliminates the need for implicit MCMC sampling and complementary models. At the\ncore of VAPO lies the construction of a homotopy (smooth path) that bridges the prior distribution\nwith the data likelihood. Subsequently, a potential flow with model-parameterized potential energy\nfunction is designed to guide the evolution of prior sample densities along this approximate data\nlikelihood homotopy. Applying a variational approach to this path-matching strategy ultimately\nyields a probabilistic Poisson's equation, where the weak solution corresponds to minimizing the\nenergy loss function of our proposed VAPO.\nOur contributions are summarized as follows:\n\u2022 We introduce VAPO, a novel energy-based generative framework that entirely dispenses\nwith the unstable and inefficient implicit MCMC sampling. Our proposed framework learns\na potential energy function whose gradient (flow) guides the prior samples, ensuring that\ntheir density evolution path closely follows the approximate data likelihood homotopy.\n\u2022 We derive an energy loss function for VAPO by constructing a variational formulation of\nthe intractable homotopy path-matching problem. Solving this energy loss objective is\nequivalent to minimizing the Kullback-Leibler divergence between density evolution of the\nflow-driven prior and the approximate data likelihood homotopy.\n\u2022 To assess the effectiveness of our proposed VAPO for image generation, we conduct\nexperiments on the CIFAR-10 and CelebA datasets and benchmark the performances against\nstate-of-the-art generative models. Our proposed framework achieves competitive FID scores\nof 0.0 and 0.0 for unconditional image generation on CIFAR-10 and CelebA, respectively."}, {"title": "2 Background and Related Works", "content": "In this section, we provide an overview of EBMs, particle flow, and the deep Ritz method, collectively\nforming the cornerstone of our proposed VAPO framework."}, {"title": "2.1 Energy-Based Models (EBMs)", "content": "Denote $x \\in \\Omega \\subseteq \\mathbb{R}^n$ as the training data, EBMs approximate the data likelihood $P_{data}$($\\mathcal{P}$) via defining\na Boltzmann distribution, as follows:\n$P_\\theta(x) = \\frac{e^{\\Phi_\\theta(x)}}{\\int_{\\Omega} e^{\\Phi_\\theta(x)} dx}$\nwhere $\\Phi$ is an energy function modelled by deep neural networks. Given that the denominator of (1),\ni.e., the partition function, is analytically intractable for high-dimensional data, EBMs perform the"}, {"title": "2.2 Particle Flow", "content": "Particle flow, initially introduced by the series of papers [45], is a class of nonlinear Bayesian\nfiltering (sequential inference) methods that aim to approximate the posterior distribution $p(x_t|\\mathcal{T}_{0:t})$\nof the state of system given the observations. While particle flow methods are closely related to\nnormalizing flows [3] and neural ordinary differential equations [46], these latter frameworks do\nnot explicitly accommodate a Bayes update. In particular, particle flow performs the Bayes update\n$p(x_t|\\mathcal{T}_{0:t}) \\propto p(x_t|\\mathcal{T}_{0:t-1}) p(\\mathcal{T}_{t}|x_t, \\mathcal{T}_{0:t-1})$ by subjecting prior samples $x_t \\sim p(x_t|\\mathcal{T}_{0:t-1})$ to a series\nof infinitesimal transformations through the ordinary differential equation (ODE) $\\frac{dx}{d\\tau} = v(x, \\tau)$\nparameterized by a flow velocity (field) function $v(x, \\tau)$, in a pseudo-time interval $\\tau \\in [0, 1]$ in\nbetween sampling time steps. The flow velocity is designed such that the driven Kolmogorov forward\npath evolution (Fokker-Planck dynamics, see (13)) of the sample particles, coincides with a data\nlog-homotopy (smooth path) that inherently perform the Bayes update. Despite its efficacy in time-\nseries inference [47\u201349] and resilience to the curse of dimensionality [50], particle flow has yet to be\nexplored in generative modelling for high-dimensional data."}, {"title": "2.3 Deep Ritz Method", "content": "The deep Ritz method is a deep learning-based variational numerical approach, originally proposed\nin [51], for solving scalar elliptic partial differential equations (PDEs) in high dimensions. Consider\nthe following Poisson's equation, fundamental to many physical models:\n$\\Delta u(x) = f(x), \\quad x\\in \\Omega$\nsubject to boundary condition\n$u(x) = 0, \\quad x\\in \\partial \\Omega$\nwhere $\\Delta$ is the Laplace operator, and $\\partial\\Omega$ denotes the boundary of $\\Omega$. For a Sobolev function\n$u \\in H^1(\\Omega)$ (see Proposition 2 for definition) and square-integrable $f \\in L^2(\\Omega)$, the variational\nprinciple ensures that a weak solution $u^*$ of the Euler-Lagrange boundary value equation (3)-(4) is\nequivalent to the variational problem of minimizing the Dirichlet energy [52], as follows:\n$u^* = \\underset{v}{\\text{arg min}} \\int_{\\Omega} (\\frac{1}{2}||\\nabla v(x)||^2 - f(x)v(x)) dx$\nwhere $\\nabla$ denotes the Del operator (gradient). In particular, the deep Ritz method parameterizes\nthe trial energy function $v$ using neural networks, and performs the optimization (5) via stochastic\ngradient descent. Due to its versatility and effectiveness in handling high-dimensional PDE systems,\nthe deep Ritz method is predominantly applied for finite element analysis [53]. In [54], the deep Ritz\nmethod is used to solve the probabilistic Poisson's equation resulting from the feedback particle filter\n[55]. Nonetheless, the method has not been explored for generative modelling."}, {"title": "3 Variational Energy-Based Potential Flow", "content": "In this section, we introduce a novel generative modelling framework, Variational Energy-Based\nPotential Flow (VAPO), drawing inspiration from both particle flow and the calculus of variations.\nFirst, we establish a homotopy that transforms a prior to the data likelihood and derive the evolution\nof the prior in time. Then, we design an energy-generated potential flow and a weighted Poisson's\nequation that aligns the evolving density distribution of transported particles with the homotopy-\ndriven prior. Subsequently, we formulate a variational loss function where its optimization with\nrespect to the flow-generating potential energy is equivalent to solving the Poisson's equation. Finally,\nwe describe the model architecture that is used to parameterize the potential energy function and the\nbackward ODE integration for generative sampling."}, {"title": "3.1 Bridging Prior and Data Likelihood: Log-Homotopy Transformation", "content": "Let $\\mathcal{X} \\in \\mathbb{R}^N$ denote the training data, $p_{data}(x)$ be the data likelihood, $x \\in \\Omega$ denote the approximate data\nsamples. To achieve generative modelling, our objective is to closely approximate the training data $\\mathcal{X}$\nwith the data samples $x$. On this account, we define a conditional data likelihood $p(x|\\bar{x}) = \\mathcal{N}(x;\\bar{x}, \\Pi)$\nwith isotropic Gaussian noise with covariance $\\Pi = diag(\\sigma^2)$ and standard deviation $\\sigma \\in \\Omega$. This is\nequivalent to considering a state space model $\\bar{x} = x + \\nu$, where $\\nu \\in \\Omega \\sim \\mathcal{N}(\\nu; 0, \\Pi)$. Here, we set a\nsmall $\\sigma$ so that $\\bar{x}$ closely resembles the training data $\\mathcal{X}$.\nSubsequently, consider a conditonal (data-conditioned) density function $\\rho : \\Omega^2 \\times [0,1] \\rightarrow \\mathbb{R}$, as\nfollows:\n$\\rho(x; \\bar{x}, t) = \\frac{e^{f(x;\\bar{x},t)}}{\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx}$\nwhere $f: \\Omega^2 \\times [0, 1] \\rightarrow \\mathbb{R}$ is a log-linear function:\n$f(x; \\bar{x}, t) = log \\quad q(x) + t \\quad log \\quad p(x|\\bar{x})$\nparameterized by the auxiliary time variable $t \\in [0, 1]$, and we let $q(x) = \\mathcal{N}(x; 0, \\Lambda)$ be a isotropic\nGaussian prior density with covariance $\\Lambda = diag(\\omega^2)$ and standard deviation $\\omega \\in \\Omega$. Here, $diag(\\cdot)$\ndenotes the diagonal function. By construction, we have $\\rho(x; \\bar{x}, 0) = q(x)$ at $t = 0$, and $\\rho(x; \\bar{x}, 1) =$\n$p(x|\\bar{x})$ at $t = 1$ since we have\n$\\rho(x; \\bar{x}, 1) = \\frac{e^{f(x;\\bar{x},1)}}{\\int_{\\Omega} e^{f(x;\\bar{x},1)} dx} = \\frac{p(x|\\bar{x})q(x)}{p_{data}(\\bar{x})} = \\frac{p(x|\\bar{x})\\frac{p_{data}(\\bar{x})}{p(x|\\bar{x})}}{p_{data}(\\bar{x})} = p(x|\\bar{x})$\nwhere we have used the fact that $p_{data}(\\bar{x}) = \\int_{\\Omega} e^{f(x;\\bar{x},1)} dx = \\int_{\\Omega} p(x|\\bar{x}) dx$. Therefore, the condi-\ntional density function $\\rho(x; \\bar{x}, t)$ here (6) essentially represents a density homotopy between the prior\n$q(x)$ and the posterior $p(x|\\bar{x})$.\nIn particular, the density function $\\rho(x; \\bar{x}, t)$ also defines a conditional (data-conditioned) homotopy\nbetween the prior $q(x)$ and the exact posterior $p(x|\\bar{x})$, the latter of which gives a maximum a\nposteriori (Bayesian) estimate of the approximate data samples after observing true training data.\nTo obtain an estimate of the intractable data likelihood for generative sampling, we then consider a\n(approximate) data likelihood homotopy $p : \\Omega \\times [0, 1] \\rightarrow \\mathbb{R}$ as follows:\n$p(x; t) = \\int_{\\Omega} p_{data}(\\bar{x}) \\rho(x; \\bar{x}, t) d\\bar{x}$\nConsidering this, it remains that $p(x;0) = q(x)$ at $t = 0$. Furthermore, given that we have\n$p(x; 1) = \\int_{\\Omega} p_{data}(\\bar{x})p(x|\\bar{x}) d\\bar{x} = p(\\bar{x})$ at $t = 1$, the data likelihood homotopy $p(x; t)$ here\ninherently performs a kernel density approximation of the true data likelihood, using the normalized\nkernel $p(x)$ obtained from the conditional homotopy $\\rho(x; \\bar{x}, 1)$ at $t = 1$. Therefore, the approximate\ndata likelihood $p(\\bar{x})$ acts as a continuous interpolation of the data likelihood $p_{data}(x)$, represented by\nDirac delta function $\\delta(x \u2013 \\bar{x})$ centered on the discrete training data $\\bar{x}$.\nNevertheless, the conditional homotopy (8) is intractable due to the normalizing constant in the\ndenominator. This intractability rules out a close-form solution of the data likelihood homotopy (9),\nthus it is not possible to sample directly from the data likelihood estimate. Taking this into account,\nwe introduce the potential flow method in the following section, where we model the evolution of the\nprior samples (particles) instead, such that their distribution adheres to the data likelihood homotopy."}, {"title": "3.2 Modelling Potential Flow in a Homotopy Landscape", "content": "Our aim is to model the flow of the prior particles in order for their distribution to follow the data\nlikelihood homotopy and converge to the data likelihood. To accomplish this, we first derive the\nevolution of the latent prior density with respect to time in the following proposition.\nProposition 1. Consider the data likelihood homotopy $p(x;t)$ in (9) with Gaussian conditional data\nlikelihood $p(x|\\bar{x}) = \\mathcal{N}(x; \\bar{x}, \\Pi)$. Then, its evolution in time $t \\in [0, 1]$ is given by the following PDE:\n$\\frac{\\partial p(x; t)}{\\partial t} = \\frac{1}{2} \\mathbb{E}_{p_{data}(\\bar{x})} [\\rho(x; \\bar{x}, t) \\gamma(x, \\bar{x}) (\\mathbb{E} \\gamma(x, \\bar{x}) - \\gamma(x, \\bar{x}))] $"}, {"title": "3.3 Variational Energy Loss Function Formulation: Deep Ritz Approach", "content": "In this section, we introduce an energy method which presents a variational formulation of the\nprobabilistic Poisson's equation. Given that the aim is to minimize the divergence between the\ndata likelihood homotopy and the flow-driven prior and directly solving the probabilistic Poisson's\nequation is difficult, we first consider a weak formulation of (14) as follows:\n$\\int_{\\Omega} (\\frac{1}{2} \\mathbb{E}_{p_{data}(\\bar{x})} [\\rho(x; \\bar{x}, t) \\gamma(x, \\bar{x}) (\\mathbb{E} \\gamma(x, \\bar{x}) - \\gamma(x, \\bar{x}))] - \\nabla \\cdot (\\rho(x;t) \\nabla\\Phi(x))) \\Psi(x) dx = 0$\nwhere the equation must hold for all differentiable trial functions $\\Psi$. In the following proposition,\nwe introduce an energy loss objective that is equivalent to solving this weak formulation of the\nprobabilistic Poisson's equation.\nProposition 3. The variational problem of minimizing the following loss function:\n$\\mathcal{L}(\\Phi; t) = \\frac{1}{2} Cov_{\\rho(x,t) \\rho_{data}(\\bar{x})} [\\Phi(x), \\gamma(x, \\bar{x})] + \\frac{1}{2} \\mathbb{E}_{\\rho(x;t)} [||\\nabla\\Phi(x)||^2]$\nwith respect to the potential energy $\\Phi$, is equivalent to solving the weak formulation (15) of the\nprobabilistic Poisson's equation (14). Here, $|| \\cdot ||$ denotes the Euclidean norm, and $Cov$ denotes the\ncovariance.\nFurthermore, the variational problem (16) has a unique solution if for all energy functions $\\Phi \\in$\n$H(\\Omega; \\rho)$, the data likelihood homotopy $p$ satisfy the Poincar\u00e9 inequality:\n$\\mathbb{E}_{\\rho(x;t)} [||\\nabla\\Phi(x)||^2] \\geq \\lambda \\mathbb{E}_{\\rho(x;t)} [||\\Phi(x)||^2]$\nfor some positive scalar constant $\\lambda > 0$ (spectral gap).\nIn sum, leveraging Propositions 2 and 3, we reformulate the intractable task of minimizing the KLD\nbetween flow-driven prior and data likelihood homotopy equivalently as a variational problem with\nenergy loss function (16). By optimizing the potential energy function $\\Phi$ with respect to the energy\nloss and transporting the prior samples through the potential flow ODE (12), the prior particles follow\na trajectory that accurately approximates the data likelihood homotopy. In doing so, the potential\nflow $\\nabla\\Phi$ drives the prior samples to posterior regions densely populated with data, thus enabling us\nto perform generative modelling.\nThe minimum covariance objective in (16) plays an important role by ensuring that the normalized\ninnovation is inversely proportional to the potential energy. As a result, the potential-generated\nvelocity field $\\nabla\\Phi$ consistently points in the direction of greatest potential ascent, thereby driving the\nflow of prior particles towards high likelihood regions of the true posterior, as illustrated in Figure 1.\nIn other words, the potential energy is conjugate to the approximate data likelihood $p(\\bar{x})$, analogous\nto Hamiltonian fluid mechanics [58]. It is worth noticing that instead of being an ad hoc addition,\nthe $L2$ regularization term $\\mathbb{E}_{\\rho(x;t)} [||\\nabla\\Phi(x)||^2]$ on the velocity field in (16) arises, from first-principle\nderivation, as a direct consequence of considering the data likelihood homotopy."}, {"title": "3.4 Energy Parameterization and ODE Sampling", "content": "To implement stochastic gradient descent on top of the energy loss function (18), we adopt the deep\nRitz approach and in particular, we model the potential energy function $\\Phi_\\theta$ as deep neural networks"}, {"title": "4 Experiments", "content": "In this section, we show that VAPO is an effective generative model for images. In particular,\nSection 4.1 demonstrates that VAPO is capable of generating realistic unconditional images on the\nwell-known CIFAR-10 and CelebA datasets. Section 4.2 demonstrates that VAPO is capable of\nperforming smooth interpolation between two generated samples. Implementation details, including\nmodel architecture and training, numerical ODE solver, datasets and FID evaluation are provided\nin Appendix B. Apart from that, we also show that VAPO exhibits extensive mode coverage and\nrobustness to anomalous data, as well as generalizing well to unseen data without over-fitting.\nSpecifically, Appendix C.1 evaluates model over-fitting and generalization based on the energy\nhistogram of CIFAR-10 train and test sets and the nearest neighbors of generated samples. Appendix\nC.2 examines robustness to anomalous data by assessing its performance on out-of-distribution\n(OOD) detection on various image datasets."}, {"title": "4.1 Unconditional Image Generation", "content": "Figure 2 shows the uncurated and unconditional image samples generated from the learned energy\nmodel on the CIFAR-10 and CelebA 64 \u00d7 64 datasets. More generated samples are provided in\nAppendix D. The samples are of decent quality and resemble the original datasets despite not having\nthe highest fidelity as achieved by state-of-the-art models. Tables 1 and 2 summarize the quantitative"}, {"title": "4.2 Image Interpolation", "content": "Figure 3 shows the interpolation results between pairs of generated CelebA samples, where it\ndemonstrates that VAPO is capable of smooth and semantically meaningful image interpolation.\nTo perform interpolation for two samples $\\bar{x}_1(1)$ and $\\bar{x}_2(1)$, we construct a spherical interpolation\nbetween the initial Gaussian noise $\\bar{x}_1(0)$ and $\\bar{x}_2(0)$, and subject them to sampling over the potential\nflow ODE. More interpolation results on CIFAR-10 and CelebA are provided in Appendix D."}, {"title": "5 Limitations and Future Work", "content": "We propose VAPO, a novel energy-based generative modelling framework without the need for\nexpensive and unstable MCMC runs amidst training. Despite the improvement over the majority of\nexisting EBMs, there is still a large performance gap between VAPO and the state-of-the-art score-\nbased (or diffusion) and Poisson flow models [4, 5, 7]. To close this gap, diffusion recovery likelihood\n[16, 17], which is shown to be more tractable than marginal likelihood, can be incorporated into the\nVAPO framework for a more controlled diffusion-guided energy optimization. The dimensionality\naugmentation technique of [7, 8] can also be integrated given that fundamentally, both Poisson\nflow and VAPO aim to model potential field governed by a Poisson's equation. On top of that, the\nscalability of VAPO to higher resolution images and its generalizability to other data modalities have\nyet to be validated. In addition, the current VAPO framework does not allow for class-conditional\ngeneration. Moreover, the training of VAPO requires a large number of iterations to converge and\nthus warrants improvement. These important aspects are earmarked for future extensions of our work."}, {"title": "A Proofs and Derivations", "content": ""}, {"title": "A.1 Proof of Proposition 1", "content": "Proof. Differentiating the conditional homotopy $\\rho(x; \\bar{x}, t)$ in (6) with respect to t, we have\n$\\frac{\\partial \\rho(x; \\bar{x}, t)}{\\partial t} = \\frac{\\partial}{\\partial t} \\frac{e^{f(x;\\bar{x},t)}}{\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx} = \\frac{ \\frac{\\partial e^{f(x;\\bar{x},t)}}{\\partial t} \\int_{\\Omega} e^{f(x;\\bar{x},t)} dx - e^{f(x;\\bar{x},t)} \\frac{\\partial \\int_{\\Omega} e^{f(x;\\bar{x},t)} dx}{\\partial t} }{(\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx)^2} = \\frac{ \\frac{\\partial e^{f(x;\\bar{x},t)}}{\\partial t} \\int_{\\Omega} e^{f(x;\\bar{x},t)} dx - e^{f(x;\\bar{x},t)} \\int_{\\Omega} \\frac{\\partial e^{f(x;\\bar{x},t)}}{\\partial t} dx }{(\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx)^2} = \\frac{ e^{f(x;\\bar{x},t)} \\frac{\\partial f(x;\\bar{x}, t)}{\\partial t} \\int_{\\Omega} e^{f(x;\\bar{x},t)} dx - e^{f(x;\\bar{x},t)} \\int_{\\Omega} e^{f(x;\\bar{x},t)} \\frac{\\partial f(x;\\bar{x}, t)}{\\partial t} dx }{(\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx)^2} = \\frac{e^{f(x;\\bar{x},t)}}{\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx} (\\frac{\\partial f(x;\\bar{x}, t)}{\\partial t} - \\frac{\\int_{\\Omega} e^{f(x;\\bar{x},t)} \\frac{\\partial f(x;\\bar{x}, t)}{\\partial t} dx}{\\int_{\\Omega} e^{f(x;\\bar{x},t)} dx}) = \\rho(x; \\bar{x}, t) (\\frac{\\partial f(x;\\bar{x}, t)}{\\partial t} - \\int_{\\Omega} \\rho(x; \\bar{x}, t) \\frac{\\partial f(x;\\bar{x}, t)}{\\partial t} dx) = \\frac{1}{2} \\rho(x; \\bar{x}, t) ( (x - \\bar{x})^T \\Pi^{-1} (x - \\bar{x}) - \\int_{\\Omega} \\rho(x; \\bar{x}, t) (x - \\bar{x})^T \\Pi^{-1} (x - \\bar{x}) dx)$\nwhere we used the quotient rule in the first equation, and chain rule in the second equation.\nLet $\\gamma(x,\\bar{x}) = (x - \\bar{x})^T \\Pi^{-1} (x - \\bar{x})$ and using the fact that\n$\\frac{\\partial p(x; t)}{\\partial t} = \\int_{\\Omega} \\frac{\\partial \\rho(x; \\bar{x}, t)}{\\partial t} p_{data}(\\bar{x}) d\\bar{x}$\nwe can substitute (22) into (23) to get\n$\\frac{\\partial p(x; t)}{\\partial t} = \\frac{1}{2} \\int_{\\Omega} p_{data}(\\bar{x}) (\\frac{1}{2} \\rho(x; \\bar{x}, t) ( (x - \\bar{x})^T \\Pi^{-1} (x - \\bar{x}) - \\int_{\\Omega} \\rho(x; \\bar{x}, t) (x - \\bar{x})^T \\Pi^{-1} (x - \\bar{x}) dx) ) d\\bar{x} = \\frac{1}{2} \\int_{\\Omega} p_{data}(\\bar{x}) \\rho(x; \\bar{x}, t) (\\gamma(x,\\bar{x}) - \\int_{\\Omega} \\rho(x; \\bar{x}, t) \\gamma(x,\\bar{x}) dx) d\\bar{x}$\nGiven that both $\\rho(x; \\bar{x}, t)$ and $p_{data}(\\bar{x})$ are normalized (proper) density functions, writing (24) in\nterms of expectations yields the PDE in (10)."}, {"title": "A.2 Proof of Proposition 2", "content": "Here, we used the Einstein tensor notation interchangeably with the conventional notation for vector\ndot product and matrix-vector multiplication in PDE.\nGiven that the context is clear, we write $g_t(\\cdot)$ in place of time-varying functions $g(\\cdot, t)$. For brevity,\nwe will also omit the time index $t$, and write $g$ in place of $g_t(x)$.\nProof. Applying the forward Euler method to the particle flow ODE (12) using step size $\\Delta t$, we\nobtain:\n$x_{t+\\Delta t} = \\alpha_t(x_t) = x_t + \\Delta t u(x_t)$\nwhere\n$u(x_t) = \\nabla \\Phi(x_t)$\nwhere we denote $x_t$ as the discretizations random variables $x(t)$.\nAssuming that the $\\alpha_t : \\Omega \\rightarrow \\Omega$ is a diffeomorphism (bijective function with differentiable inverse),\nthe push-forward operator $\\alpha_{t\\#} : \\mathbb{R} \\rightarrow \\mathbb{R}$ on density function $p_t \\rightarrow p_{t+\\Delta t} := \\alpha_{t\\#}p_t$ is defined by:\n$\\int_{\\Omega} p_{t+\\Delta t}(x) g(x) dx = \\int_{\\Omega} \\alpha_{t\\#}p_t(x) g(x) dx = \\int_{\\Omega} p_t(x) g(\\alpha_t(x)) dx$\nfor any measurable function $g$.\nAssociated with the change-of-variables formula (27) is the following density transformation:\n$p_{t+\\Delta t}(\\alpha_t(x)) = \\frac{1}{|D \\alpha_t|} p_t(x)$"}, {"title": "A.3 Proof of Proposition 3", "content": "Proof. The energy loss function in (16) can generally be written as follows:\n$\\mathcal{L"}, "Phi, t) = \\frac{1}{2} \\mathbb{E}_{\\rho(x,t)} p_{data} (\\bar{x}) [\\Phi(x) (\\gamma(x,\\bar{x}) - \\gamma(x, \\bar{x}))"], "condition": "n$I(\\Phi, \\Psi) = \\frac{d}{d\\epsilon} \\mathcal{L}(\\Phi(x) + \\epsilon\\Psi(x), t)|_{\\epsilon=0} = 0$\nwhich must hold for all trial function $\\Psi$.\nTaking the variational derivative of the particle flow objective (51) with respect to $\\epsilon$, we have\n$I(\\Phi, \\Psi) = \\frac{d}{d\\epsilon} \\mathcal{L}(\\Phi + \\epsilon \\Psi)|_{\\epsilon=0} = \\frac{d}{d\\epsilon} \\frac{1}{2} \\int_{\\Omega\\times\\Omega} p_{data}(\\bar{x}) p(x;x) (\\gamma(x;x) - \\gamma(x, \\bar{x})) \\frac{d}{d\\epsilon} \\Psi (\\Phi + \\epsilon \\Psi) dx dx + \\frac{1}{2} \\int_{\\Omega} \\frac{d}{d\\epsilon} (\\nabla\\Phi(x))^2 + \\epsilon (\\nabla\\Psi) dx| = \\frac{1}{2} \\int_{\\Omega\\times"}