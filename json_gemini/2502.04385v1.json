{"title": "TexLiDAR: Automated Text Understanding for Panoramic LiDAR Data", "authors": ["Naor Cohen", "Roy Orfaig", "Ben-Zion Bobrovsky"], "abstract": "Efforts to connect LiDAR data with text, such as LidarCLIP, have primarily focused on embedding 3D point clouds into CLIP's text-image space. However, these approaches rely on 3D point clouds, which present challenges in encoding efficiency and neural network processing. With the advent of advanced LiDAR sensors like Ouster's OS1, which, in addition to 3D point clouds, produce fixed-resolution depth, signal, and ambient panoramic 2D images, new opportunities emerge for LiDAR-based tasks.\nIn this work, we propose an alternative approach to connect LiDAR data with text by leveraging 2D imagery generated by the OS1 sensor instead of 3D point clouds. Using the Florence 2 large model in a zero-shot setting, we perform image captioning and object detection. Our experiments demonstrate that Florence 2 generates more informative captions and achieves superior performance in object detection tasks compared to existing methods like CLIP. By combining advanced LiDAR sensor data with a large pre-trained model, our approach provides a robust and accurate solution for challenging detection scenarios, including real-time applications requiring high accuracy and robustness.", "sections": [{"title": "1. Introduction", "content": "The integration of LiDAR technology with deep learning has advanced significantly, driven by innovations in both sensor hardware and machine learning models. Traditional LiDAR systems produce 3D point clouds that offer rich spatial information but are computationally intensive to process. Recent work, such as LidarCLIP [1], has attempted to bridge the gap between LiDAR data and textual descriptions by embedding 3D point clouds into the CLIP framework. However, this approach has limitations: it requires aligning point clouds with a camera's field of view, discarding the broader 360-degree spatial context, and relies on CLIP [2], which provides abstract and less detailed outputs for tasks like image captioning.\nThe Ouster OS1 LiDAR sensor addresses some of these challenges by outputting high-resolution depth, signal, and ambient images that are spatially coherent and cover the full 360-degree field of view [3]. These structured 2D images retain the spatial richness of LiDAR data while enabling direct processing with advanced deep learning models. Compared to 3D point clouds, they offer better compatibility with existing neural network architectures, allowing for efficient and scalable solutions.\nIn this paper, we propose leveraging the Florence 2 large model [4] in a zero-shot setting to perform image captioning and object detection directly on the 2D images generated by the OS1 sensor. Florence 2's ability to handle diverse visual tasks and generate detailed, contextually rich outputs makes it an ideal choice for lidar-based applications. By bypassing the complexities of 3D point cloud processing and fully utilizing the 360-degree data, our approach achieves more informative and accurate results compared to traditional methods.\nThis paper makes the following contributions:\n\u2022 We introduce a framework for leveraging advanced lidar sensor data with the Florence 2 model, bypassing the need for 3D point cloud processing.\n\u2022 We demonstrate the effectiveness of our approach in performing image captioning and object detection tasks, achieving more detailed and accurate results compared to existing methods.\n\u2022 We highlight the ability of our method to utilize the full 360-degree field of view provided by Ouster sensors, overcoming the limitations of alignment-based approaches like LidarCLIP [1].\n\u2022 In addition to captioning and detection, we also demonstrate the capability of estimating the angle and distance of objects relative to the sensor, using the paired point cloud data for distance calculations."}, {"title": "2. Related Work", "content": "Lidar technology plays an essential role in autonomous systems by providing high-resolution depth and spatial information. Traditional lidar data processing primarily focuses on 3D point clouds, which deliver detailed environmental data but are computationally expensive and require complex preprocessing. More recent approaches have integrated lidar data with deep learning models, such as LidarCLIP [1], which embed 3D point clouds into a shared space alongside images and text. However, these methods are constrained by the need to align point clouds with the camera's field of view, limiting their ability to fully leverage lidar's 360-degree spatial context.\nThe Ouster OS1 lidar sensor [3] offers high-resolution 2D images that are spatially correlated and cover the entire 360-degree field of view. These structured images align more naturally with deep learning models, overcoming the challenges associated with processing 3D point clouds. By directly processing 2D lidar images, new possibilities arise for lidar-based tasks, such as image captioning and object detection, without the need for alignment or transformation to the camera's perspective.\nRecent advances in large-scale multimodal models, such as Florence 2 [4], have significantly expanded the potential applications of lidar data. Florence 2's zero-shot capabilities enable it to perform a wide range of visual tasks-image captioning, object detection, and scene understanding-without additional training. This makes Florence 2 particularly well-suited for processing 2D lidar images, yielding more detailed and contextually relevant results compared to earlier models like CLIP [2].\nIn contrast to LidarCLIP, which operates primarily on 3D point clouds and requires alignment with images, our approach utilizes the full 360-degree field of view provided by the Ouster OS1 sensor. By leveraging Florence 2 for zero-shot image captioning and object detection on 2D lidar images, we eliminate the need for alignment and preprocessing, yielding more accurate and insightful results for lidar-based tasks. Unlike LidarCLIP, which decodes lidar points into vectors similar to its handling of paired images [1], our approach facilitates richer, more nuanced interpretations of the data. Furthermore, future models like CLIP-Cap [5], which combine CLIP with GPT-2 [6] for image captioning, often produce less informative captions. This is due to CLIP's more limited image captioning capabilities, in contrast to the more robust and contextually aware performance of Florence 2 on lidar data."}, {"title": "3. Methodology", "content": "In this work, we focus on the Drular dataset [7], specifically on ambient images, which provide detailed grayscale representations of the environment. These ambient images, with a resolution of 2048x128 pixels, capture the full 360-degree field of view from the Ouster OS1 lidar sensor [3]. These images contain complex environmental features but are not free from noise, as they stem directly from lidar sensor measurements.\nThe Ouster OS1 lidar sensor [3] generates structured 2D images depth, signal, and ambient that are spatially coherent and cover the full 360-degree field of view. These images are computationally efficient and highly compatible with modern deep learning models, such as Florence 2 [4], which are designed for 2D image data. Unlike traditional 3D point clouds, which require complex preprocessing (e.g., transforming point clouds into a camera coordinate system or filtering out irrelevant points), 2D lidar images can be processed directly by deep learning models. This eliminates the need for extensive data transformation and reduces computational overhead, making them ideal for real-time applications with improved scalability.\nIn order to leverage the full potential of the 360-degree lidar data, it is crucial to feed the images into Florence 2 [4] in a way that preserves spatial coherence. Simply inputting the full 2048x128 image would cause Florence 2 to treat it as one large image, potentially leading to a loss of spatial understanding of the entire 360-degree scene. To address this, we divide the 360-degree image into four segments: right, left, front, and back. Each segment covers a 90-degree section of the original image, preserving the spatial layout of the scene.\nFlorence 2's zero-shot capabilities [4] make it particularly well-suited for processing structured 2D image inputs, such as the segmented images derived from 360-degree lidar data. Each segmented image, representing a 90-degree field of view, is processed independently by Florence 2, which identifies key features, objects, and their relationships within each segment. Leveraging its design for diverse visual tasks without additional training, Florence 2 performs image captioning and object detection on these inputs. The predictions from all four segments are then merged to form a comprehensive understanding of the entire 360-degree scene.\nIn addition to image captioning and object detection, we leverage point cloud data paired with each ambient image to estimate the angle and distance of detected objects relative to the sensor. The point cloud provides precise distance measurements, while the 360-degree nature of lidar data enables accurate angular localization. This enhances scene understanding and provides richer, more informative outputs for real-world applications.\nTo achieve this, we use the Florence v2 model to detect objects in ambient images and generate bounding boxes (BBs). Each bounding box provides the pixel coordinates of the detected object, and we use its center as the reference point for our calculations.\nUsing the distance image derived from LiDAR data, where each pixel represents the computed distance based on the horizontal and vertical angles of the LiDAR points, we calculate the distance for each LiDAR point as:\ndistance = $\\sqrt{x^2 + y^2}$\nwhere x and y are the real-world coordinates of the LiDAR point. The distance image is then constructed by mapping each LiDAR point to its corresponding pixel in the image, where each pixel holds the computed distance value.\nGiven the center of the bounding box at pixel coordinates (UBB, VBB), the object's distance from the sensor is directly obtained by extracting the corresponding pixel value from the distance image:\ndistance = distanceimage (UBB, VBB)\nwhere distanceimage (UBB, VBB) represents the value of the distance at the pixel corresponding to the bounding box center in the distance image.\nTo estimate the angular position of the object relative to the sensor, we compute:\nangle = 360 \u00d7$\\frac{UBB - W/2}{W}$\nwhere u\u00df\u00df is the horizontal coordinate of the bounding box center in the image, and W is the image width.\nAn example of a distance image, extracted from the 3D points in the point cloud, is shown below. Given the coordinates of the bounding box center in the ambient image, we can extract the corresponding matching coordinates in the distance image. Using camera-LiDAR calibration, we map the 2D image coordinates to the 3D point cloud and estimate the distance from the sensor.\nThe image has a width of W pixels and covers a 360-degree field of view. The center of the image corresponds to 0\u00b0, with the leftmost edge representing -180\u00b0 and the rightmost edge representing +180\u00b0. Using the formula above, we can map the horizontal pixel coordinate \u0438\u0432\u0432 to its corresponding angle.\nBy combining object detection with point cloud data, we obtain accurate spatial localization of objects, significantly enhancing contextual awareness and scene interpretation.\nThis approach highlights the strength of Florence 2 in directly processing 2D lidar images to generate rich, context-aware outputs. Unlike methods such as LidarCLIP [1], which require alignment or complex preprocessing of lidar data, Florence 2 bypasses these steps, providing more accurate and informative results. This makes it an effective tool for lidar-based tasks, including image captioning and object detection.\nThe following key points summarize the advantages of combining lidar sensor data with large multimodal models like Florence 2:\nKey Points\n\u2022 Zero-Shot Capability: Florence 2 excels in zero-shot tasks, enabling effective interpretation of lidar data without specific training.\n\u2022 Spatial Coherence: Florence 2 processes spatially coherent 2D lidar images more effectively than unstructured 3D point clouds.\n\u2022 Efficient Processing: Dividing the 360-degree lidar image into smaller sections allows Florence 2 to retain global context while processing each segment independently.\n\u2022 Object Angle and Distance Estimation: Leveraging the paired point cloud data with ambient images, and using object detection from Florence 2, we estimate the angle and distance of objects relative to the lidar sensor, adding another layer of contextual information."}, {"title": "4. Results", "content": "The proposed approach, leveraging the Florence 2 model for image captioning and object detection on 2D lidar images, was evaluated using data from the Ouster OS1 sensor. The sensor's high-resolution depth, signal, and ambient images were processed to generate captions and detect objects within the scene. Figure 3 illustrates these results, showcasing both tasks on 90\u00b0 imagery. The left side presents the captioning output, where the model describes key elements such as vehicles, pedestrians, and background structures. On the right, object detection results demonstrate accurate identification and localization.\nFurther analysis, shown in Figure 2, incorporates object distance and angle estimation, enhancing spatial awareness. By segmenting the 360-degree lidar image into four 90-degree views, the model processes each independently before merging the outputs into a unified scene description. This capability is particularly valuable for applications such as autonomous navigation, robotic perception, and smart city monitoring, where precise environmental understanding is crucial.\nBy integrating captioning and detection within a single framework, Florence 2 enables comprehensive scene interpretation. Its robust performance across diverse environments demonstrates its potential for real-world deployment, advancing perception capabilities beyond traditional 2D image analysis."}, {"title": "5. Discussion", "content": "The results obtained using ambient images highlight the potential of lidar-generated 2D images for tasks such as image captioning and object detection. However, the Drular dataset [7] includes additional image modalities, such as reflectivity images, which capture the strength of the reflected lidar signal. These reflectivity images can provide unique insights into material properties and surface characteristics, offering a complementary perspective to ambient images.\nAs shown in Figure 4, the reflectivity images tend to reveal hidden or obscured objects that are not easily visible in the ambient versions. In the first pair, the reflectivity image highlights a house partially obscured by trees, which is barely visible in the ambient version. In the second pair, while the ambient image captures broader context, including people in the scene, the reflectivity image does not provide any additional advantages. In general, due to the black-and-white nature of the reflectivity images, the model often interprets them as night-time scenes, which can affect the model's interpretation of the data. These examples demonstrate how reflectivity and ambient images complement each other by revealing different elements of the scene.\nExploring reflectivity images using the same Florence 2-based methodology [4] could further enhance performance in lidar-based tasks. For instance, the reflectivity data could improve object detection in scenarios involving low-visibility conditions or materials with distinct reflectance properties, such as metallic or glass surfaces.\nMoreover, combining all available lidar modalities, including ambient, reflectivity, intensity, and range images, could lead to a powerful multimodal system. Each modality contributes unique information:\n\u2022 Ambient images: grayscale ambient imagery at 850 nm offers a high signal-to-noise ratio, improving usability in low-light conditions like dawn, dusk, or cloudy days.\n\u2022 Reflectivity images: highlights material properties and surface textures based on the detected surface reflectivity.\n\u2022 Intensity images: represents lidar intensity data, highlighting anomalies and strong signal returns based on detected photon counts.\n\u2022 Range images: point distance is calculated using the laser pulse's time of flight, providing accurate 3D measurements and enabling precise object localization in a scene.\nA multimodal approach could leverage these complementary strengths to deliver more robust and context-aware results. Florence 2's versatility in processing diverse image types [4] makes it an excellent candidate for implementing such a system.\nFuture research could focus on developing methods to efficiently fuse these modalities, exploring the potential of multimodal systems in applications such as autonomous driving, environmental monitoring, and robotics."}, {"title": "6. Conclusion", "content": "In this paper, we have introduced a novel methodology for leveraging advanced lidar sensor data, particularly the 2D images generated by the Ouster OS1 sensor [3], in conjunction with the Florence 2 large model [4] for tasks such as image captioning and object detection. By focusing on the structured, high-resolution 2D images, we bypass the complexities associated with 3D point cloud processing and demonstrate the potential of zero-shot inference for lidar-based tasks.\nOur approach effectively utilizes the full 360-degree spatial context provided by the Ouster OS1 sensor, dividing the panoramic ambient images into manageable segments for processing with Florence 2. The results highlight the advantages of this method, showing detailed and contextually rich outputs that surpass traditional methods such as Lidar-CLIP [1]."}]}