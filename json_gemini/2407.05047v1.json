{"title": "MFE-ETP: A Comprehensive Evaluation Benchmark\nfor Multi-modal Foundation Models on Embodied\nTask Planning", "authors": ["Min Zhang", "Jianye Hao", "Xian Fu", "Peilong Han", "Hao Zhang", "Lei Shi", "Hongyao Tang", "Yan Zheng"], "abstract": "In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial\nIntelligence (EAI) have been advancing side by side at an unprecedented pace.\nThe integration of the two has garnered significant attention from the AI research\ncommunity. In this work, we attempt to provide an in-depth and comprehensive\nevaluation of the performance of MFMs on embodied task planning, aiming to\nshed light on their capabilities and limitations in this domain. To this end, based\non the characteristics of embodied task planning, we first develop a systematic\nevaluation framework, which encapsulates four crucial capabilities of MFMs: ob-\nject understanding, spatio-temporal perception, task understanding, and embodied\nreasoning. Following this, we propose a new benchmark, named MFE-ETP, char-\nacterized its complex and variable task scenarios, typical yet diverse task types,\ntask instances of varying difficulties, and rich test case types ranging from multiple\nembodied question answering to embodied task reasoning. Finally, we offer a\nsimple and easy-to-use automatic evaluation platform that enables the automated\ntesting of multiple MFMs on the proposed benchmark. Using the benchmark\nand evaluation platform, we evaluated several state-of-the-art MFMs and found\nthat they significantly lag behind human-level performance. The MFE-ETP is a\nhigh-quality, large-scale, and challenging benchmark relevant to real-world tasks.", "sections": [{"title": "1 Introduction", "content": "The impressive progress of Multi-modal Foundation Models (MFMs) [1, 2, 3, 4, 5, 6] has sparked a\nsurge of interest in promoting the application of MFMs in Embodied Artificial Intelligence (EAI).\nCurrently, the mainstream approaches can be divided into two branches, one of which is to train\nspecific models for low-level robotic control based on particular robotics data [7, 8, 9, 10, 11, 12, 13].\nThe other is to employ off-the-shelf models such as GPT-4V [4] as a high-level task planner and then\nuse pre-trained skills to achieve all sub-task goals from high-level task planner, adapting flexibly to\nvarious robotics scenarios in a zero-shot manner [14, 15, 16, 17, 18, 19]. The main difference between\nthese approaches lies in the design details of the task planning pipeline, e.g., whether it integrates an\naffordance analyzer [14], or incorporates perceptual information and visual feedback [15], etc. Rooted\nin data-driven algorithms and specific scenarios, the former only characterizes abstract features of\nlimited acquisition data, often failing to derive nuanced scenario understanding and effective causal\nreasoning for robotics manipulation. In contrast, the latter approach avoids the high costs of data\ncollection and leverages the remarkable reasoning and generalization capabilities of MFMs to enhance\nmodel generalization in robotics applications. However, blindly applying off-the-shelf MFMs without\nunderstanding their capabilities may prevent these methods from achieving optimal performance.\nActually, since the release of GPT-4V[4], a large number of performance evaluation reports on\nMFMs in different domains have been published, such as commonsense tasks[20, 21, 22], vision\ntasks [20, 23, 24, 25], autonomous driving [26, 27], and robotics [15, 16, 28], etc. However, a\ncomprehensive performance evaluation of MFMs for embodied task planning has not been thoroughly\nexplored.\nTo better utilize MFMs for embodied task planning, based on the characteristics of embodied task\nplanning, we first propose a systematic evaluation framework, which encapsulates four crucial\ncapabilities: object understanding, spatio-temporal perception, task understanding, and embodied\nreasoning, aiming to answer what factors constrain MFMs to output accurate task plans. Then,\nwe propose a comprehensive evaluation benchmark (MFE-ETP) that aligns with this framework\nto quantitatively and qualitatively access the performance limitations of MFMs on embodied task\nplanning. In addition, we develop an automated evaluation platform to facilitate the performance\nevaluation of various embodied task planners using MFMs on our proposed benchmark. Compared\nto existing evaluation efforts, our work has four key advantages: \u2460 Our evaluation framework and\nbenchmark are specifically designed for embodied task planning, ensuring accurate performance\nevaluation of MFMs on task planning; \u2461 We provide a diverse set of scenarios and tasks, allowing\nfor an extensive evaluation of MFMs' task planning capabilities across different settings; \u2462 The\ncapability dimensions we evaluate are more comprehensive and tightly aligned with embodied task\nplanning, covering a wide range of test case types from embodied question answering to embodied\ntask planning; \u2463 We offer an easy-to-use evaluation platform that facilitates the automatic evaluation\nof multiple MFMs on our benchmark.\nIn summary, the main contributions of our work are as follows:\n\u2022 We propose a systematic evaluation framework tailored for embodied task planning for the\nfirst time. This framework offers effective guidance for the improvement of MFMs-based\ntask planners.\n\u2022 We propose a benchmark, MFE-ETP containing over 1100 high-quality test cases carefully\nannotated by human, covering 100 embodied tasks.\n\u2022 We develop an evaluation platform and evaluate six advanced MFMs. The evaluation results\nshow that object type recognition and spatial perception are the main constraints for MFMs\nto generate correct task planning results.\n\u2022 We will make this benchmark and platform open source\u00b2 to foster future research on\nembodied task planning and inspire more focused research directions."}, {"title": "2 Evaluation Framework", "content": "In this section, we introduce our evaluation framework designed for embodied task planning, which\nconsists of four levels of crucial capabilities: object understanding, spatio-temporal perception, task\nunderstanding, and embodied reasoning. Each capability dimension is further decomposed into\nseveral sub-aspects. The overall framework is shown in Fig.1.\nObject Understanding (OU) Object understanding involves recognizing the Type and Property\nof objects. The object properties include physical properties (e.g., color, shape, etc.) and functional\nproperties (e.g., openable, grabbable, etc.). Accurate recognition of object types and properties is a\nfundamental requirement for correct task planning. For example, in the task of preserving food shown\nin Fig.1, if the food on the dinner table and its properties (e.g., the apple and pie that can be grasped)\nare incorrectly or incompletely recognized by MFMs, their output task plans may fail to enable the\nrobot to complete the task. In addition, this could raise safety concerns due to inappropriate actions\nagainst misidentified objects. Tab.4 in Appendix lists the definitions of each considered property\nalong with representative objects.\nSpatio-Temporal Perception (STP) Building upon the understanding of static objects, the spatial-\ntemporal perception capability of MFMs is vital for solving embodied task planning problems.\nIn our proposed framework, for Spatial perception, we evaluate the ability of MFMs to judge\nspatial attributes such as distance, geometry, direction, and relative position between objects. Our\nbenchmark covers five types of spatial relations: \"Inside\", \"OnTop\", \"NextTo\", and \"Under\", with\nfurther subdivisions of \"NextTo\" into \"Front\", \"Back\", \"Left\", and \"Right\" to accommodate more\ntasks. For Temporal perception, we access MFMs' ability to recognize the chronological sequence\nof task progress, and predict the effects of actions on the environment. As illustrated in Figure 1,\nMFMs need to perceive the spatial positions of the apple and pie from a first-person perspective.\nThis understanding enables them to plan navigation actions and subsequent steps by observing task\nprogress and changes in the environment, such as the apple being placed in the freezer and the\nfreezer being opened. Accurate spatial and temporal perception enhances MFMs' effectiveness in\ntask planning.\nTask Understanding (TU) Compared to object understanding and spatio-temporal perception,\ntask understanding is a higher-level and more abstract capability for MFMs. It requires MFMs\nto recognize task-relevant objects and understand the operation knowledge of these objects, the\nsequence of task steps, and the conditions for task goal completion. To this end, we evaluate the\ntask understanding capability of MFMs from the above four aspects, which we name as Relevant\nObject, Operation Knowledge, Step Sequence, and Goal, respectively. For the food preserving task in\nFig.1, MFMs first need to accurately select the food on the dinner table and the appropriate storage\nlocation, i.e., the freezer, from various object types within the field of view. Then, MFMs determine\nthe operation knowledge (e.g., open/place_inside/close) and the sequence of steps (e.g., open the\nfreezer first, then put the food in the freezer, and finally close the freezer) based on the usage of\nthe freezer. Finally, MFMs need to identify whether the task goal has been achieved. If the task\nunderstanding capability of MFMs is insufficient, it may result in planning errors, such as improper\naction functions or sequences of steps.\nEmbodied Reasoning (ER) Towards more realistic, long-horizon, and complex embodied tasks\nbased on general descriptions, it is evident that relying solely on the first three capabilities is\ninsufficient for MFMs. Indispensably, MFMs need to further integrate and reason based on perceived\nobject information, spatio-temporal information, and task knowledge to complete task planning. Thus,"}, {"title": "3 Evaluation Benchmark", "content": "In this section, we describe in detail our benchmark, MFE-ETP, a dataset comprising video-derived\nimages and task instructions designed to evaluate MFMs on embodied task planning. This section\nintroduces the data collection (Sec.3.1), benchmark structure (Sec.3.2), and automatic evaluation\nmethod (Sec.3.3) in sequence, respectively.\n3.1 Data Collection\nThe household environment represents a common scenario for robot-human interaction, where the\ndiversity and complexity of household tasks can thoroughly test the robot's perception, reasoning,\nplanning, and action capabilities. Therefore, in this paper, we select typical household tasks to build\na benchmark for embodied task planning. Specifically, we collect raw data from representative\nhousehold task platforms BEHAVIOR-100 [29] and VirtualHome [30]. More information about these\nplatforms can be found in Appendix C.1.\nTo avoid redundancy of similar tasks while ensuring task diversity, we abstracted 20 typical household\ntask types from all tasks provided by BEHAVIOR-100 [29] and VirtualHome [30], and generate 100"}, {"title": "3.2 Benchmark Structure", "content": "In order to comprehensively evaluate the performance of MFMs on embodied task planning, we\ndesigned six different task instruction forms based on our proposed evaluation framework: (1) \u041e\u0440\u0435\u043f-\nvocabulary Q&A requires MFMs to identify and answer questions about object types, based on\ngeneral or task-related objects. (2) Single-Choice Q&A evaluates MFMs' spatial perception ability\nand task-related object operation knowledge, requiring MFMs to select the correct answer from\nmultiple options. (3) Multi-Choice Q&A means that there may not be a single correct answer to the\nquestion, considering that objects can have multiple properties and there may be multiple reasonable\ntask step sequences. (4) Sorting Q&A tasks MFMs with sorting visual frames from different moments\nbased on time or spatial distance to evaluate their spatio-temporal perception. (5) Yes/No Q&A\nThis is a simple judgment question, mainly used to determine whether the task goal is achieved\nand the changes in the embodied environment. (6) Planning Q&A challenges MFMs to generate\ntask plans based on a general task description to access MFMs' end-to-end embodied task planning\nperformance."}, {"title": "3.3 Automatic Evaluation", "content": "As illustrated in Fig.3, the complete evaluation process of MFMs on our automatic evaluation platform\nincludes data preparation, model deployment, and evaluation metric calculation.\nData Preparation The main purpose of this phase is to standardize the data format of the evaluation\ncases based on diverse task instructions in the proposed benchmark to enhance the accuracy and\nstability of MFMs. First, we organize all the evaluation cases (with different instruction formats)\nunder each capability dimension into the same JSON file. Each evaluation case in this file has a\nunified format and contains keys such as \"sample_id\", \"task_name\". We then carefully customize\nmultiple fixed prompt templates for each capability dimension to reduce the impact of prompts on the\nperformance of MFMs. Finally, we structure each evaluation case based on the JSON structure and\nthe corresponding prompt template as the input for MFMs.\nModel Delpoyment For model deployment, our platform provides a unified interface for multiple\ndifferent models, through which they can receive input prompts and return results in the same\nway. This interface enhances scalability, enabling the easy integration of new models by simply\nimplementing this interface.\nMetric Calculation The typical evaluation methods of MFMs fall into two categories: automatic\nevaluation and human evaluation[31]. In this paper, we use the human evaluation method for cases\ninvolving planning Q&A instructions, while other evaluation cases under the form of embodied Q&A\ninstructions are evaluated automatically using GPT-3.5[32]. There are two main reasons for this: \u2460\nCompared with embodied Q&A, the output results of planning Q&A are more uncertain and complex,\nwith results often not being uniquely correct. This makes it difficult for automatic evaluation to judge\nthe quality of the outputs accurately. \u2461 GPT-3.5 is capable of performing efficient and accurate\nevaluation when dealing with cases where the output results are relatively stable. This characteristic\nis very beneficial for large-scale benchmarks as it enhances both the efficiency and accuracy of the"}, {"title": "4 Experiments", "content": "In this section, we report the experimental results of MFMs on embodied Q&A tasks (Sec. 4.1) and\nPlanning Q&A (Sec. 4.2) tasks according to different evaluation metrics.\n4.1 Results on Embodied Q&A Tasks\nFor the embodied Q&A tasks (object understanding, spatio-temporal perception, and task under-\nstanding), we evaluate the performance of GPT-4V [4] and five other open-source MFMs, namely\nBLIP-2 [1], MiniGPT-4 [2], InstructBLIP [3], LLaVA-1.5 [5], and MiniCPM [6]. The experimental\nresults are presented in Tab. 1 and radar chart (Fig. 5). In Tab. 1, we report the average scores for\neach capability dimension across each aspect, the average scores for all aspects of each capability\ndimension (denoted OU./STP./TU.Agg.), as well as the overall average score for all embodied Q&A\ntasks (denoted EQA.Agg.). From Tab. 1 and Fig. 5, we can draw the following general conclusions:\n\u25cf GPT4V [4] and MiniCPM [6]significantly outperform the other four MFMs, with a performance\nimprovement of over 100% compared to the worst-performing LLaVA-1.5[5]. \u25cf The overall average\nscore differences among the four MFMs, BLIP-2 [1], MiniGPT-4 [2], InstructBLIP [3], LLaVA-1.5\n[5] are relatively minor, indicating comparable performance levels on the embodied Q&A tasks. \u25cf\nMiniCPM [6] performs close to GPT-4V [4] and surpasses the other open-source MFMs in all aspects. \u25cf\nCompared with higher-level and more abstract task understanding capabilities, MFMs generally\nperform worse in low-level perception capabilities such as object understanding and spatio-temporal\nperception. Therefore, we recommend augmenting MFMs with object detection modules, such as\nGroundingDINO [33], and integrating 3D information [34], instead of relying solely on MFMs.\nNote that all models occasionally generate output that does not conform to the specified format\nrequirements, with MiniGPT-4 exhibiting this issue most frequently. By specifically analyzing the\nresults of the six MFMs in each capability dimension, we further obtain the following conclusions:\nObject Understanding MiniCPM [6] excels the other models in recognizing object types with an\naverage score of 44.0%. In terms of identifying object properties, GPT-4V [4] is far ahead with an\naverage score of 58.1%, followed by MiniCPM[6] with 53.6%. Overall, the object understanding\ncapability of MiniCPM [6] and GPT-4V [4] is more impressive compared to other MFMs. MiniGPT-4\n[2] and LLaVA-1.5 [5] have an accuracy of 29.6% and 25.9%, respectively, while InstructBLIP [3]\nand BLIP-2 [1] can only succeed in about 11% cases."}, {"title": "4.2 Results on Planning Q&A Tasks", "content": "Based on the proposed evaluation framework, Planning Q&A corresponds to the highest-level\nembodied reasoning capability dimension, which includes two types of tasks: General Description-\nbased embodied reasoning (GD-ER) and object understanding (OU), spatio-temporal perception\n(STP), and task understanding (TU)-oriented embodied reasoning (OU/STP/TU-ER). We evaluate\nthe performance of MFMs on the two types of tasks, respectively. Specifically, we first utilize\nthe proposed automatic evaluation platform to obtain the evaluation result files for all evaluation\ncases. Then, experienced AI researchers manually score all the result files, based on whether the task\nplanning results were reasonable and could guide the completion of the task goals, assigning a score\nof 1 for satisfactory task plans and 0 otherwise.\nGiven that GPT-4V [4] and MiniCPM [6] show state-of-the-art performance on embodied Q&A tasks\nand significantly outperform other MFMs, this experiment evaluates the performance of GPT-4V [4]\nand MiniCPM [6] on planning Q&A tasks. However, none of the 270 evaluation cases for MiniCPM\n[6] in the benchmark yielded correct task planning results, and its outputs contained extraneous\ninformation to the answers. To ensure reliability, we also evaluated the output results of MiniCPM [6]\non planning Q&A tasks on its official website demo. The results are consistent with those obtained\non our platform, which shows that MiniCPM [6] has serious limitations in end-to-end embodied task\nplanning. Therefore, we only show the evaluation results of GPT-4V. From Fig. 6, we can draw"}, {"title": "5 Conclusion", "content": "In this paper, we propose a systematic evaluation framework tailored for embodied task planning\nfor the first time. Utilizing this framework, we have developed an evaluation benchmark, called\nMFE-ETP, striving to explore the performance limitations of MFMs. Empirical results on our\nbenchmark consistently indicate that even advanced GPT-4V fails to effectively complete embodied\nplanning tasks. Notably, object type recognition and spatiao-temporal perception are critical factors\nthat affect the accuracy of MFMs' task planning capabilities. We conduct an in-depth analysis\nof the experimental results and put forward many useful insights to promote the application and\ndevelopment of MFMs in the field of embodied task planning.\nresearch.\nLimitations and future work. \u2460 We utilized virtual environments as data sources for easier data\ncollection, which may raise concerns about the generalizability of our findings to more realistic\nscenarios. \u2461 Besides, evaluating a broader range of models remains a promising direction for future\nresearch. \u2462 While the Embodied Reasoning task remains highly challenging for current models,\nplanning all future steps in advance remains advantageous. Utilizing closed-loop control systems\nlike ViLa [15] for testing task planning could offer a viable compromise. \u2463 Furthermore, exploring\nadditional QA formats such as correcting past behavior and predicting subsequent actions could\nfurther enrich the research."}, {"title": "A Appendix", "content": null}, {"title": "B Appendix Overview", "content": "The appendix includes the following content:\n1. Detailed information about the proposed MFE-ETP benchmark. (Sec. C)\n2. Additional experimental results. (Sec. D)\n3. Detailed information of MFMs (Sec. E).\n4. Accessibility and Statement(Sec. F)"}, {"title": "C Details of MFE-ETP Benchmark", "content": null}, {"title": "C.1 BEHAVIOR-100 and VirtualHome", "content": "BEHAVIOR-100 [29] simulates 100 everyday household tasks for embodied AI, with a distribution\nsimilar to the full space of simulatable tasks in the American Time Use Survey (ATUS). It aims to\ncreate realistic and complex scenarios for AI testing. It introduces a predicate logic-based language\n(BDDL) for defining tasks and possesses simulator-agnostic features for versatile applications. Due\nto the reality, diversity, and complexity of home tasks, state-of-the-art AI still struggles with the\nbenchmark's challenges. More information can be found in BEHAVIOR-100[29].\nVirtualHome [30] utilizes \"programs\" to model complex household activities, that can be implemented\nmanually through a game-like interface. Specifically, VirtualHome [30] uses the Unity3D platform to\nenable human agents to perform tasks in a simulated environment. This helps create video datasets of\nhousehold activities, each of which is a series of actions and interactions executed by agents. More\ninformation can be found in VirtualHome [30].\nTo avoid redundancy of similar tasks while maintaining task diversity, we illustrate the mapping\nrelationships between our abstracted 20 typical household task types and specific task instances in\nBEHAVIOR-100 [29], VirtualHome[30], and our benchmark In Tab. 2 and Tab. 3."}, {"title": "C.2 MFMs Prompt Texts", "content": "As indicated in other research, the success of embodied tasks with MFMs is heavily influenced by\nthe design of prompts. Therefore, we have elaborately designed a series of standardized prompt\ntemplates to ensure that the evaluation process uses the same question format and language style,\nthereby reducing the impact of differences in prompt quality. Fig.8 and Fig.9 present examples of\nprompts on embodied Q&A and planning Q&A, respectively."}, {"title": "C.3 Full Object Properties and Action List", "content": "Tab.4 lists the definitions of each considered functional property and its representative objects. In\naddition, we illustrate in Tab.5 the definition of all actions that can satisfy embodied reasoning for the\ntypical household tasks we consider."}, {"title": "D Additional Experimental Results", "content": null}, {"title": "D.1 Results of Qualitative Analysis", "content": "Based on our proposed framework, we further qualitatively analyze the evaluation results of the\nstate-of-the-art GPT-4V[4].\nObject Understanding Through manual analysis of specific failed cases of GPT-4V, we attribute\nthe main causes of incorrect answers to limitations inherent in GPT-4V, poor perspective (even under\nthe best perspective we can obtain, the objects are still difficult to recognize), and low resolution. Fig.\n(10) shows a few representative failed cases.\nSpatio-temporal Perception Previous works have mentioned that GPT-4V lacks spatio-temporal\nunderstanding ability [26, 35], which is consistent with our evaluation results. By analyzing specific\nfailed cases of spatio-temporal perception Q&A tasks, we found that GPT-4V often fails to perceive\ndistance and direction of 2D images, especially the left and right directions. (as in Fig. 11) Also, we\nfound that GPT-4V performs poorly in terms of temporal perception as well, which is reflected in its\ninability to accurately sort images according to the task completion process.\nTask Understanding Fig. 12 shows a few representative failed cases. Combined with Tab. 1, the\nmain limitations of GPT-4V in task understanding are mainly in the inability to accurately select\ntask-related objects and select reasonable action execution sequences, which are essential knowledge\nfor task planning.\nEmbodied Reasoning We analyzed the corresponding results of planning Q&A and found some\ncommon issues: firstly, task related object recognition faces challenges, including insufficient\nrecognition, incorrect recognition, and chaotic spatial relationships when there are a large number\nof objects. Secondly, the MFMs' spatiotemporal perception ability is not strong, manifested in"}, {"title": "D.2 Linear Regression Analysis of The Impact of Each Aspect on Task Planning", "content": "In this section, we will use linear regression coefficients as proxies for the impact of various factors\non embodied reasoning. Specifically, for each robotic task, we use the binary result of embodied\nreasoning as the dependent variable and mark successful cases as 1 and failed cases as 0. Take the\nbasic capabilities corresponding to all aspects as a multivariate independent variable, and use their\naccuracy as the value. The obtained coefficients are shown in the Tab. 6. It can be concluded that\nspatial perception is the most important for embodied reasoning, with a coefficient of 0.4, followed"}, {"title": "D.3 Validity Analysis of LLM-based Metrics", "content": "For the larger scale evaluation benchmark we propose, it is a challenge to efficiently evaluate\nembodied Q&As. In essence, we would like MFMs to give reasonable results rather than results that\nare exactly consistent with the ground truth. While human evaluation remains the gold standard, it\nis also expensive and time-consuming. Thus, we use an automatic LLM-Based evaluation metric\nin this work as described in Sec 3.3. To show the effectiveness of LLM-Based evaluation metric,\nwe manually score the output of state-of-the-art GPT-4V and compare them with the evaluation\nresults of GPT 3.5, and the experimental results are shown in Tab. 7. ICC1 (Intraclass Correlation\nCoefficient 1) is primarily used to evaluate the absolute consistency among different evaluators in\na single measurement scenario. This type of ICC focuses on the consistency of each evaluator's\nindividual rating of the same object or subject. We calculated the ICC1 value to be 0.95, indicating"}, {"title": "E Detailed Information of MFMS", "content": "Tab. 8 shows the versions of each MFM. In particular, we access GPT-4V through OpenAI API, and\nfor other open-source models, we utilize a single NVIDIA GeForce RTX 4090 GPU."}, {"title": "F Accessibility and Statement", "content": "URL to dataset Both the code and data are slated for release at our repositorys:\nhttps://github.com/TJURLLAB-EAI/MFE-ETP.\nAuthor Statement We accept complete accountability for any infringement of rights that may arise\nduring the utilization or dissemination of the data presented in this work. We commit to undertaking\nnecessary measures, including the modification or deletion of any data involved in such violations, to\naddress these concerns promptly.\nWe commit to maintaining the data and the codes. Updates to any future versions of the dataset and\ncode will be made available at this location. The data provided is designed for academic purposes.\nLicense MFE-ETP is released under the MIT License."}]}