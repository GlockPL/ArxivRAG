{"title": "MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation Models on Embodied Task Planning", "authors": ["Min Zhang", "Jianye Hao", "Xian Fu", "Peilong Han", "Hao Zhang", "Lei Shi", "Hongyao Tang", "Yan Zheng"], "abstract": "In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial Intelligence (EAI) have been advancing side by side at an unprecedented pace. The integration of the two has garnered significant attention from the AI research community. In this work, we attempt to provide an in-depth and comprehensive evaluation of the performance of MFM s on embodied task planning, aiming to shed light on their capabilities and limitations in this domain. To this end, based on the characteristics of embodied task planning, we first develop a systematic evaluation framework, which encapsulates four crucial capabilities of MFMs: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. Following this, we propose a new benchmark, named MFE-ETP, characterized its complex and variable task scenarios, typical yet diverse task types, task instances of varying difficulties, and rich test case types ranging from multiple embodied question answering to embodied task reasoning. Finally, we offer a simple and easy-to-use automatic evaluation platform that enables the automated testing of multiple MFMs on the proposed benchmark. Using the benchmark and evaluation platform, we evaluated several state-of-the-art MFMs and found that they significantly lag behind human-level performance. The MFE-ETP is a high-quality, large-scale, and challenging benchmark relevant to real-world tasks.", "sections": [{"title": "Introduction", "content": "The impressive progress of Multi-modal Foundation Models (MFMs) [1, 2, 3, 4, 5, 6] has sparked a surge of interest in promoting the application of MFMs in Embodied Artificial Intelligence (EAI). Currently, the mainstream approaches can be divided into two branches, one of which is to train specific models for low-level robotic control based on particular robotics data [7, 8, 9, 10, 11, 12, 13]. The other is to employ off-the-shelf models such as GPT-4V [4] as a high-level task planner and then use pre-trained skills to achieve all sub-task goals from high-level task planner, adapting flexibly to various robotics scenarios in a zero-shot manner [14, 15, 16, 17, 18, 19]. The main difference between these approaches lies in the design details of the task planning pipeline, e.g., whether it integrates an affordance analyzer [14], or incorporates perceptual information and visual feedback [15], etc. Rooted in data-driven algorithms and specific scenarios, the former only characterizes abstract features of limited acquisition data, often failing to derive nuanced scenario understanding and effective causal reasoning for robotics manipulation. In contrast, the latter approach avoids the high costs of data collection and leverages the remarkable reasoning and generalization capabilities of MFMs to enhance model generalization in robotics applications. However, blindly applying off-the-shelf MFMs without understanding their capabilities may prevent these methods from achieving optimal performance. Actually, since the release of GPT-4V[4], a large number of performance evaluation reports on MFMs in different domains have been published, such as commonsense tasks[20, 21, 22], vision tasks [20, 23, 24, 25], autonomous driving [26, 27], and robotics [15, 16, 28], etc. However, a comprehensive performance evaluation of MFMs for embodied task planning has not been thoroughly explored.\nTo better utilize MFMs for embodied task planning, based on the characteristics of embodied task planning, we first propose a systematic evaluation framework, which encapsulates four crucial capabilities: object understanding, spatio-temporal perception, task understanding, and embodied reasoning, aiming to answer what factors constrain MFMs to output accurate task plans. Then, we propose a comprehensive evaluation benchmark (MFE-ETP) that aligns with this framework to quantitatively and qualitatively access the performance limitations of MFMs on embodied task planning. In addition, we develop an automated evaluation platform to facilitate the performance evaluation of various embodied task planners using MFMs on our proposed benchmark. Compared to existing evaluation efforts, our work has four key advantages: \u2460 Our evaluation framework and benchmark are specifically designed for embodied task planning, ensuring accurate performance evaluation of MFMs on task planning; \u2461 We provide a diverse set of scenarios and tasks, allowing for an extensive evaluation of MFMs' task planning capabilities across different settings; \u2462 The capability dimensions we evaluate are more comprehensive and tightly aligned with embodied task planning, covering a wide range of test case types from embodied question answering to embodied task planning; \u2463 We offer an easy-to-use evaluation platform that facilitates the automatic evaluation of multiple MFMs on our benchmark.\nIn summary, the main contributions of our work are as follows:\n\u2022 We propose a systematic evaluation framework tailored for embodied task planning for the first time. This framework offers effective guidance for the improvement of MFMs-based task planners.\n\u2022 We propose a benchmark, MFE-ETP containing over 1100 high-quality test cases carefully annotated by human, covering 100 embodied tasks.\n\u2022 We develop an evaluation platform and evaluate six advanced MFMs. The evaluation results show that object type recognition and spatial perception are the main constraints for MFMs to generate correct task planning results.\n\u2022 We will make this benchmark and platform open source\u00b2 to foster future research on embodied task planning and inspire more focused research directions."}, {"title": "Evaluation Framework", "content": "In this section, we introduce our evaluation framework designed for embodied task planning, which consists of four levels of crucial capabilities: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. Each capability dimension is further decomposed into several sub-aspects. The overall framework is shown in Fig.1.\nObject Understanding (OU) Object understanding involves recognizing the Type and Property of objects. The object properties include physical properties (e.g., color, shape, etc.) and functional properties (e.g., openable, grabbable, etc.). Accurate recognition of object types and properties is a fundamental requirement for correct task planning. For example, in the task of preserving food shown in Fig.1, if the food on the dinner table and its properties (e.g., the apple and pie that can be grasped) are incorrectly or incompletely recognized by MFMs, their output task plans may fail to enable the robot to complete the task. In addition, this could raise safety concerns due to inappropriate actions against misidentified objects. Tab.4 in Appendix lists the definitions of each considered property along with representative objects.\nSpatio-Temporal Perception (STP) Building upon the understanding of static objects, the spatial-temporal perception capability of MFMs is vital for solving embodied task planning problems. In our proposed framework, for Spatial perception, we evaluate the ability of MFMs to judge spatial attributes such as distance, geometry, direction, and relative position between objects. Our benchmark covers five types of spatial relations: \"Inside\", \"OnTop\", \"NextTo\", and \"Under\", with further subdivisions of \"NextTo\" into \"Front\", \"Back\", \"Left\", and \"Right\" to accommodate more tasks. For Temporal perception, we access MFMs' ability to recognize the chronological sequence of task progress, and predict the effects of actions on the environment. As illustrated in Figure 1, MFMs need to perceive the spatial positions of the apple and pie from a first-person perspective. This understanding enables them to plan navigation actions and subsequent steps by observing task progress and changes in the environment, such as the apple being placed in the freezer and the freezer being opened. Accurate spatial and temporal perception enhances MFMs' effectiveness in task planning.\nTask Understanding (TU) Compared to object understanding and spatio-temporal perception, task understanding is a higher-level and more abstract capability for MFMs. It requires MFMs to recognize task-relevant objects and understand the operation knowledge of these objects, the sequence of task steps, and the conditions for task goal completion. To this end, we evaluate the task understanding capability of MFMs from the above four aspects, which we name as Relevant Object, Operation Knowledge, Step Sequence, and Goal, respectively. For the food preserving task in Fig.1, MFMs first need to accurately select the food on the dinner table and the appropriate storage location, i.e., the freezer, from various object types within the field of view. Then, MFMs determine the operation knowledge (e.g., open/place_inside/close) and the sequence of steps (e.g., open the freezer first, then put the food in the freezer, and finally close the freezer) based on the usage of the freezer. Finally, MFMs need to identify whether the task goal has been achieved. If the task understanding capability of MFMs is insufficient, it may result in planning errors, such as improper action functions or sequences of steps.\nEmbodied Reasoning (ER) Towards more realistic, long-horizon, and complex embodied tasks based on general descriptions, it is evident that relying solely on the first three capabilities is insufficient for MFMs. Indispensably, MFMs need to further integrate and reason based on perceived object information, spatio-temporal information, and task knowledge to complete task planning. Thus,"}, {"title": "Evaluation Benchmark", "content": "In this section, we describe in detail our benchmark, MFE-ETP, a dataset comprising video-derived images and task instructions designed to evaluate MFMs on embodied task planning. This section introduces the data collection (Sec.3.1), benchmark structure (Sec.3.2), and automatic evaluation method (Sec.3.3) in sequence, respectively.\nThe household environment represents a common scenario for robot-human interaction, where the diversity and complexity of household tasks can thoroughly test the robot's perception, reasoning, planning, and action capabilities. Therefore, in this paper, we select typical household tasks to build a benchmark for embodied task planning. Specifically, we collect raw data from representative household task platforms BEHAVIOR-100 [29] and VirtualHome [30]. More information about these platforms can be found in Appendix C.1.\nTo avoid redundancy of similar tasks while ensuring task diversity, we abstracted 20 typical household task types from all tasks provided by BEHAVIOR-100 [29] and VirtualHome [30], and generate 100"}, {"title": "Benchmark Structure", "content": "In order to comprehensively evaluate the performance of MFMs on embodied task planning, we designed six different task instruction forms based on our proposed evaluation framework: (1) \u041e\u0440\u0435\u043f-vocabulary Q&A requires MFMs to identify and answer questions about object types, based on general or task-related objects. (2) Single-Choice Q&A evaluates MFMs' spatial perception ability and task-related object operation knowledge, requiring MFMs to select the correct answer from multiple options. (3) Multi-Choice Q&A means that there may not be a single correct answer to the question, considering that objects can have multiple properties and there may be multiple reasonable task step sequences. (4) Sorting Q&A tasks MFMs with sorting visual frames from different moments based on time or spatial distance to evaluate their spatio-temporal perception. (5) Yes/No Q&A This is a simple judgment question, mainly used to determine whether the task goal is achieved and the changes in the embodied environment. (6) Planning Q&A challenges MFMs to generate task plans based on a general task description to access MFMs' end-to-end embodied task planning performance."}, {"title": "Automatic Evaluation", "content": "As illustrated in Fig.3, the complete evaluation process of MFMs on our automatic evaluation platform includes data preparation, model deployment, and evaluation metric calculation.\nThe main purpose of this phase is to standardize the data format of the evaluation cases based on diverse task instructions in the proposed benchmark to enhance the accuracy and stability of MFMs. First, we organize all the evaluation cases (with different instruction formats) under each capability dimension into the same JSON file. Each evaluation case in this file has a unified format and contains keys such as \"sample_id\", \"task_name\". We then carefully customize multiple fixed prompt templates for each capability dimension to reduce the impact of prompts on the performance of MFMs. Finally, we structure each evaluation case based on the JSON structure and the corresponding prompt template as the input for MFMs.\nFor model deployment, our platform provides a unified interface for multiple different models, through which they can receive input prompts and return results in the same way. This interface enhances scalability, enabling the easy integration of new models by simply implementing this interface.\nThe typical evaluation methods of MFMs fall into two categories: automatic evaluation and human evaluation[31]. In this paper, we use the human evaluation method for cases involving planning Q&A instructions, while other evaluation cases under the form of embodied Q&A instructions are evaluated automatically using GPT-3.5[32]. There are two main reasons for this: \u2460 Compared with embodied Q&A, the output results of planning Q&A are more uncertain and complex, with results often not being uniquely correct. This makes it difficult for automatic evaluation to judge the quality of the outputs accurately. \u2461 GPT-3.5 is capable of performing efficient and accurate evaluation when dealing with cases where the output results are relatively stable. This characteristic is very beneficial for large-scale benchmarks as it enhances both the efficiency and accuracy of the"}, {"title": "Experiments", "content": "In this section, we report the experimental results of MFMs on embodied Q&A tasks (Sec. 4.1) and Planning Q&A (Sec. 4.2) tasks according to different evaluation metrics."}, {"title": "Results on Embodied Q&A Tasks", "content": "For the embodied Q&A tasks (object understanding, spatio-temporal perception, and task under-standing), we evaluate the performance of GPT-4V [4] and five other open-source MFMs, namely BLIP-2 [1], MiniGPT-4 [2], InstructBLIP [3], LLaVA-1.5 [5], and MiniCPM [6]. The experimental results are presented in Tab. 1 and radar chart (Fig. 5). In Tab. 1, we report the average scores for each capability dimension across each aspect, the average scores for all aspects of each capability dimension (denoted OU./STP./TU.Agg.), as well as the overall average score for all embodied Q&A tasks (denoted EQA.Agg.). From Tab. 1 and Fig. 5, we can draw the following general conclusions: \u25cf GPT4V [4] and MiniCPM [6]significantly outperform the other four MFMs, with a performance improvement of over 100% compared to the worst-performing LLaVA-1.5[5]. \u25cf The overall average score differences among the four MFMs, BLIP-2 [1], MiniGPT-4 [2], InstructBLIP [3], LLaVA-1.5 [5] are relatively minor, indicating comparable performance levels on the embodied Q&A tasks. \u25cf MiniCPM [6] performs close to GPT-4V [4] and surpasses the other open-source MFMs in all aspects. \u25cf Compared with higher-level and more abstract task understanding capabilities, MFMs generally perform worse in low-level perception capabilities such as object understanding and spatio-temporal perception. Therefore, we recommend augmenting MFMs with object detection modules, such as GroundingDINO [33], and integrating 3D information [34], instead of relying solely on MFMs. Note that all models occasionally generate output that does not conform to the specified format requirements, with MiniGPT-4 exhibiting this issue most frequently. By specifically analyzing the results of the six MFMs in each capability dimension, we further obtain the following conclusions:\nObject Understanding MiniCPM [6] excels the other models in recognizing object types with an average score of 44.0%. In terms of identifying object properties, GPT-4V [4] is far ahead with an average score of 58.1%, followed by MiniCPM[6] with 53.6%. Overall, the object understanding capability of MiniCPM [6] and GPT-4V [4] is more impressive compared to other MFMs. MiniGPT-4 [2] and LLaVA-1.5 [5] have an accuracy of 29.6% and 25.9%, respectively, while InstructBLIP [3] and BLIP-2 [1] can only succeed in about 11% cases."}, {"title": "Results on Planning Q&A Tasks", "content": "Based on the proposed evaluation framework, Planning Q&A corresponds to the highest-level embodied reasoning capability dimension, which includes two types of tasks: General Description-based embodied reasoning (GD-ER) and object understanding (OU), spatio-temporal perception (STP), and task understanding (TU)-oriented embodied reasoning (OU/STP/TU-ER). We evaluate the performance of MFMs on the two types of tasks, respectively. Specifically, we first utilize the proposed automatic evaluation platform to obtain the evaluation result files for all evaluation cases. Then, experienced AI researchers manually score all the result files, based on whether the task planning results were reasonable and could guide the completion of the task goals, assigning a score of 1 for satisfactory task plans and 0 otherwise.\nGiven that GPT-4V [4] and MiniCPM [6] show state-of-the-art performance on embodied Q&A tasks and significantly outperform other MFMs, this experiment evaluates the performance of GPT-4V [4] and MiniCPM [6] on planning Q&A tasks. However, none of the 270 evaluation cases for MiniCPM [6] in the benchmark yielded correct task planning results, and its outputs contained extraneous information to the answers. To ensure reliability, we also evaluated the output results of MiniCPM [6] on planning Q&A tasks on its official website demo. The results are consistent with those obtained on our platform, which shows that MiniCPM [6] has serious limitations in end-to-end embodied task planning. Therefore, we only show the evaluation results of GPT-4V. From Fig. 6, we can draw the following conclusion: Under the general task description, the success rate of the most advanced GPT-4V is only 19%. In other words, among the 100 typical household tasks we proposed, GPT-4V correctly planned only 19 tasks. Obviously, this performance falls short of human expectations. Similarly, GPT-4V's results on OU-ER, STP-ER, and TU-ER tasks were also unsatisfactory.\nFurthermore, we explore the effect of providing additional information in embodied reasoning. As shown in Fig. 7, we select 50 typical tasks from the original set. For each task, we gradually inject information related to some supporting capabilities into GPT-4V [4] via prompts (e.g., informing GPT-4V that the food on the table consists of an apple and a pie) and detect the impact of corresponding capabilities on task planning through changes in success rate. We obtain the following conclusions: \u2460 Injecting object information, spatio-temporal information, and task knowledge can have a positive effect on task planning; \u2461 compared to settings where only object information (+o.) or object, spatial, and task knowledge (+o.+s.t.+k.) are injected, injecting object and spatial information (+o.+s.t.) notably enhances the success rate of task planning. Due to space constraints, we have placed the other additional experiments in Appendix D."}, {"title": "Conclusion", "content": "In this paper, we propose a systematic evaluation framework tailored for embodied task planning for the first time. Utilizing this framework, we have developed an evaluation benchmark, called MFE-ETP, striving to explore the performance limitations of MFMs. Empirical results on our benchmark consistently indicate that even advanced GPT-4V fails to effectively complete embodied planning tasks. Notably, object type recognition and spatiao-temporal perception are critical factors that affect the accuracy of MFMs' task planning capabilities. We conduct an in-depth analysis of the experimental results and put forward many useful insights to promote the application and development of MFMs in the field of embodied task planning.\nLimitations and future work. \u2460 We utilized virtual environments as data sources for easier data collection, which may raise concerns about the generalizability of our findings to more realistic scenarios. \u2461 Besides, evaluating a broader range of models remains a promising direction for future research. \u2462 While the Embodied Reasoning task remains highly challenging for current models, planning all future steps in advance remains advantageous. Utilizing closed-loop control systems like ViLa [15] for testing task planning could offer a viable compromise. \u2463 Furthermore, exploring additional QA formats such as correcting past behavior and predicting subsequent actions could further enrich the research."}]}