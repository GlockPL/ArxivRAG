{"title": "SplitSEE: A Splittable Self-supervised Framework for Single-Channel EEG Representation Learning", "authors": ["Rikuto Kotoge", "Zheng Chen", "Tasuku Kimura", "Yasuko Matsubara", "Takufumi Yanagisawa", "Haruhiko Kishima", "Yasushi Sakurai"], "abstract": "While end-to-end multi-channel electroencephalography (EEG) learning approaches have shown significant promise, their applicability is often constrained in neurological diagnostics, such as intracranial EEG resources. When provided with a single-channel EEG, how can we learn representations that are robust to multi-channels and scalable across varied tasks, such as seizure prediction? In this paper, we present SplitSEE, a structurally splittable framework designed for effective temporal-frequency representation learning in single-channel EEG. The key concept of SplitSEE is a self-supervised framework incorporating a deep clustering task. Given an EEG, we argue that the time and frequency domains are two distinct perspectives, and hence, learned representations should share the same cluster assignment. To this end, we first propose two domain-specific modules that independently learn domain-specific representation and address the temporal-frequency trade-off issue in conventional spectrogram-based methods. Then, we introduce a novel clustering loss to measure the information similarity. This encourages representations from both domains to coherently describe the same input by assigning them a consistent cluster. SplitSEE leverages a pre-training-to-fine-tuning framework within a splittable architecture and has following properties: (a) Effectiveness: it learns representations solely from single-channel EEG but has even outperformed multi-channel baselines. (b) Robustness: it shows the capacity to adapt across different channels with low performance variance. Superior performance is also achieved with our collected clinical dataset. (c) Scalability: With just one fine-tuning epoch, SplitSEE achieves high and stable performance using partial model layers.", "sections": [{"title": "I. INTRODUCTION", "content": "ELECTROENCEPHALOGRAPHY (EEG) reveals the activities of millions of neurons in multi-channel recordings. As a staple brain imaging tool, it is now widely used in various healthcare domains, including sleep medicine [1]\u2013[3] and seizure epilepsy detection [4]. Deep learning models have shown impressive success in automating EEG analysis. Since meaningful features have region-specific activation effects, successful methods typically involve a multi-channel modeling framework [5]\u2013[7]. Their main objective is to learn holistic topographic brain information for localizing eventful features [8]. Learning such spatial or spatio-temporal information has shown great potential in EEG analyses [9]\u2013[11]. Despite their success, there is a need in neurological diagnostics [12], [13], particularly in disease detection and portable monitoring, for developing accurate and disruption-reduced methods using fewer or even single channels, specifically,\n\u2022 Multi-channel methods are often inapplicable to some data resources, such as intracranial EEG, which is limited to specific brain regions, lacking holistic multi-channel recordings, but important for pre-surgical analysis [14].\n\u2022 Continuous wear is essential for diagnostics but is impractical with multi-channel EEG because it often disrupts natural conditions, such as sleep [15].\nIn this paper, we aim to devise a flexible training strategy that can effectively capture meaningful features solely from single-channel EEG for varying channels and tasks. Recently, several single-channel EEG methods have been proposed [16]\u2013[18]. Since lacking spatial information, these methods aim to learn temporal-frequency features for specific tasks; however, some limitations remain.\n1) Inadequate feature encoding and trade-off issue in temporal-frequency learning. Existing works can generally be categorized into two approaches: using deep learning"}, {"title": "II. RELATED WORK", "content": "A. Single-Channel EEG Studies\nDeploying too many channels can potentially increase the risk of skin irritation and discomfort [24]. Some studies are considering the use of fewer channels to minimize noise and computational expenses [4], [7], [16]. To be more effective, some studies [25]\u2013[27] consider modeling a single-channel EEG; for example, AttnSleep [13] deploys causal convolutions to model the temporal relations of the single-channel input for sleep staging. MS-HNN [24] proposes a multi-scale learning framework that uses CNN and RNN to extract more beneficial features from single-channel EEGs across various frequencies. A seizure detection work [18] refines the temporal-spatial features and models the correlations between them with the Transformer model.\nB. Self-Supervised Learning in EEGs\nEarly attempts in SSL adopted on the unsupervised pre-trained sequential model, such as BERT [28], to model the long-term temporal dependencies of EEG data for fine-tuning [29]. Most methods focus primarily on modeling multi-channel EEGs [4], [5]. For single-channel EEG, TS-TCC [1], a contrastive learning (CL) framework (one of method in SSL) for time-series representation, introduces novel data augmentation techniques and cross-view prediction tasks tailored for time-series data, including EEGs. However, this method does not incorporate frequency domain features for EEG representation. In response, BTSF [30] and TF-C [31] have addressed this gap and emphasize temporal-frequency fusion. Recently, [6] has explored a temporal-frequency method and reported superior results in two EEG tasks. However, the SOTA works [6], [7], [32] are typically multi-channel SSL frameworks; moreover, SplitSEE learns meaningful temporal-frequency representation by formulating a deep clustering in an SSL fashion."}, {"title": "III. PROBLEM SETTING", "content": "A. Single-Channel EEG Data\nEEG data consist of multi-channel recordings, where each channel captures a series of neuronal activities from a specific brain region over time. Formally, let $X = {X^{(m)}}_{m=1}^C$ denote EEG data composed of C channels. Here, $X^{(m)} \u2208 R^{N\u00d7L}$ indicates a single-channel data that collects N subjects over a duration of L time points. For clarity, each $X^{(m)} \u2208 R^{N\u00d7L}$ can be expressed as a sequence of vectors: $(x_1, x_2,...,x_n)$. Let"}, {"title": "B. Problem Formulation", "content": "Our primary objective is to build a well-performing model $f_\u03b8(\u00b7)$, which functions with a single-channel EEG $x_i$, where i = 1,2,..., N. The aim is to learn a lower dimensional vector z that represents all essential information across all C channels for $X_i = (x_1^{(1)}, x_2^{(1)},...,x_2^{(C)})$. That is, using $z_i$ and $x_i$ is equivalent for any downstream task. For even greater effectiveness, $z_i$ should ideally exhibit performance that is competitive with the multi-channel learning methods in ${X}_{m=1}^C$. Practically, z is produced by $z = f_\u03b8(x)$, where the function is parameterized by \u03b8. To obtain reliable z, we have three expectations for $f_\u03b8(\u00b7)$:\nEXPECTATION 1: Operate on single-channel EEG data, with the capacity to perform competitively with multi-channel or multi-data source modeling.\nEXPECTATION 2: Learn high-quality representations z from x without label supervision, which is applicable to various EEG tasks across different channels.\nEXPECTATION 3: Develop a flexible framework to overcome the constraints of end-to-end model re-training and enhance its practical utility in real-world clinical settings with limited computing resources.\nWe confirm the realization of the above expectations in Sections VI."}, {"title": "IV. PROPOSED METHOD", "content": "A. Forward Process Overview\nWe develop a deep clustering framework to train $f_\u03b8(\u00b7)$, employing a two-step process to obtain a representative vector z for a single-channel EEG x. Fig. 2 shows workflow of SplitSEE. Fundamentally, SplitSEE consists of three key modules: two dedicated to learning in the temporal and frequency domains, and a deep clustering module designed to align the learned features. Initially, \u00e6 is processed independently by the domain-specific modules to extract temporal ($c_t$) and frequency ($c_{tF}$) representations (see Sections IV-B and IV-C).\nThese representations are then input into the deep clustering module. This module optimizes the representations to ensure they share the same cluster assignment, which integrates them into a unified feature space (see Section IV-D).\nRemark. Two independent domain-specific modules aim to address the limitations of conventional methods, such as the trade-offs associated with spectrogram usage and inadequate temporal-frequency encoding. Each module is designed in a CL manner. This ensures the learned features are disentangled and more relevant to their respective domains. Further clustering is an information metric, making the temporal-frequency representations coherently describe the same input data.\nB. Multi-Granularity Temporal Independent Learning\nMotivation. In general, EEG data lack recognizable patterns, e.g., trend and seasonality. The eventful features are transient and temporally unpredictable. For instance, a spontaneous K-complex waveform often exhibits bursts within 0.5~1.5 seconds [18]. Therefore, this module involves multi-granularity learning that encodes different levels of temporal dependencies.\n1) Data augmentation: We initially generate two noisy inputs of the single-channel EEG data. Given \u00e6, we introduce random noises to the signal and adjust its amplitude for weak augmentation. For strong augmentation, we divided the input EEG data into a random set of segments, capped at a total of L. We randomly shuffle these segments. The objective is to increase robustness for capturing millisecond-level key features that appear randomly. We limit the number of cut points to a maximum of 4 to 11, ensuring that most of EEG waveform within the entire time points is preserved. These augmentations are formulated as:\n$x^{weak} = \u03b1(x + \u03f5_w), x^{strong} = {X_{\u03b2(1)}, X_{\u03b2(2)}, ..., X_{\u03b2(L)}} + \u03f5_s$,                                                                                                        (1)\nwhere $\u03f5_w$ and $\u03f5_s$ denote the respective random noises, \u03b1 is a scaling factor and \u03b2(\u00b7) is a random permutation function.\n2) Multi-granularity feature extraction: The backbone is a temporal convolutional network (TCN) [33] with dilated convolutional layers. By introducing a fixed step between successive filter taps, we expand the receptive field of different EEG scales of EEG data points as:\n$Z^W, Z^S = TCN(x^{weak}, x^{strong})$                                       (2)\nwhere each $Z^W, Z^S \u2208 R^{Lx|z|}$ is characterized as $[z_1, z_2,...z_L]$, L is the number of timesteps. |z| represents feature embedding dimensions in different granularity.\nUnlike previous works [1], [6] that fuse and preserve filtered features as a vector, our method expands ($x \u2192 Z$) by preserving a landscape of local features at various granularities, such as 0.5-second and 1-second intervals, each characterized by distinct receptive fields.\n3) Temporal semantic information summarization: We employ the Transformer encoder [18] with an additional class token to extrapolate the global long-term dependencies in EEGs. We first segment the $Z^W$ and $Z^S$ sequence into T patches,"}, {"title": "C. Multi-Granularity Frequency Independent Learning", "content": "Motivation. To learn high-quality frequency representation, we propose this module that independently learns eventful features from the full frequency spectrum with precise resolution.\n1) Frequency decomposition: We apply the Fast Fourier Transform (FFT) to the normalized signal a to decompose it into its frequency spectrum. We then extract the absolute values from the mirrored first half of the frequency bands, denoted as $x_{FFT} = |FFT(Norm(x))|$.\n2) Frequency local feature extraction: We use a 1D convolution block to learn the intricate bi-directional correlations between narrow frequency bands, given by:\n$Z^f = CNN1d(x_{FFT}), Z^f \u2208 R^{B\u00d7|z|}.$ (6)\nwhere B is the CNN down-sampled dimensions of frequency bands. We create the feature vector $Z^h$ by reversing the order of elements in the vector $Z^f$.\n$c_h, c_f = Trans_F ([Z_1, Z_2,...Z_F], [\\widehat{Z}_1, \\widehat{Z}_2,...\\widehat{Z}_F])$ (7)\nNotably, Fis randomly chosen for each batch training to diversify the learning of different frequency wave activation.\n3) Frequency Contrastive Learning: We introduce cross-band CL to unify the low-band and high-band representations. This CL is also a dual prediction pretext. The positive samples are $Z_{F+K}^f$ and $Z_{F+K}^h$ for $c_f$ and $c_h$, respectively. The module employs linear projection $W(\u00b7)$ following Transformer encoders to learn the frequency order information.\n$L'_{FC} := E_X \\Big[ -log \\frac{exp \\Big( (W^f(c_f)) \\cdot Z_{F+K}^f /T \\Big)}{\\sum_n exp \\Big((W^f(c_f)) \\cdot z_n^f /T \\Big)} \\Big] ,$ (8)\n$L''_{FC}:= E_X \\Big[ -log \\frac{exp \\Big( (W^f(c_h)) \\cdot Z_{F+K}^h /T \\Big)}{\\sum_n exp \\Big((W^f(c_h)) \\cdot z_n^h /T \\Big)} \\Big] ,$ (9)"}, {"title": "D. Deep Clustering for Temporal-Frequency Alignment", "content": "We introduce a deep clustering learning to conduct an integration loss function, $L_{dc}$, for further optimization of temporal-frequency representation. This loss function is an information metric with two expectations: (1) to align the two latent spaces. ensuring their carried information is coherently related to the same input, i.e., same cluster assignment and (2) to ensure the learned representations are task-relevant by promoting distinct cluster assignments for other input samples.\nFormally, we initially concatenate representations as $c_{tr}$ = Concat($c_w$, $c_s$) and $c_{tF}$ = Concat($c_f$, $c_h$). Inspired by SwAV [23], we approach the clustering of $c_{tr}$ and $c_{tF}$ as a problem of similarity measures, aiming to maximize the mutual information between them. Since $c_{tr}$ and $c_{tF}$ are from the same given \u00e6, they inherently carry high informational similarity and conceptually belong to the same cluster. We treat $c_{tr}$ and $c_{tF}$ as two distinct views of \u00e6, denoted $z^{\u03c5_1}$ and $z^{\u03c5_2}$, respectively. Therefore, high informational similarity between these views implies that they can predict each another. The clustering task thus involves aligning a set of J learnable cluster centroids {$c_1, ..., c_J$} to the paired views ($z^{\u03c5_1}$, $z^{\u03c5_2}$). If $z^{\u03c5_1}$ and $z^{\u03c5_2}$ can predict each other effectively, they are likely to share the same cluster centroids $c_j$. This process is given by:\n$L_{dc} := l(z^{\u03c5_1}, q^{\u03c5_2}) + l(z^{\u03c5_2}, q^{\u03c5_1})$ (10)\nwhere $q^{\u03c5_1}$, $q^{\u03c5_2}$ are trainable code that determines how to assign $z^{\u03c5_1}$ $z^{\u03c5_2}$, represented a mirror pair of a patient to a cluster centroid $c_j$. In a nutshell, $l(z^{\u03c5_1}, q^{\u03c5_2})$ tells how close or aligned the latent variable $z^{\u03c5_1}$ is to all centroids, weighted by transformed vector $q^{\u03c5_2}$ of $z^{\u03c5_2}$, given by:\n$l(z^{\u03c5_1}, q^{\u03c5_2}) := - \\sum_j q_j^{\u03c5_2} log \\frac{exp (z^{\u03c5_1}c_j/T)}{\\sum_{j'} exp (z^{\u03c5_1}c_{j'}/T)}$ (11)\nThe trainable code generalizes the Euclidean distance metric. It can be seen that Eq.(11) is a special case of InfoNCE in that the expectation is over the code. Let $Z = [z_1,..., z_b]$ and $Q = [q_1,\u2026, q_J]$ denote the matrices respectively collecting the representation vectors, where b refers to the batch size. The code is trained by maximizing their similarity subject to the constraint of belonging to the transportation polytope [23]: maxQ Tr(QCZ) + H(Q). H(Q) denotes the Shannon entropy of the code matrix in an element-wise manner to avoid deterministic solutions. For each paired $c_{tr}$ and $c_{tF}$, this cluster assignment maximizes the similarity between the latent variables, ensuring the temporal and frequency representations are more related to the input."}, {"title": "E. Two-Phase Training", "content": "SplitSEE is designed in combination with a two-phase learning strategy to enhance its practical applicability.\ni) Pre-training phase operates as an end-to-end training process. We conduct this phase in a fully self-supervised manner to obtain informative representations that are relevant to the data itself and can be transferred to various tasks. The loss function is:\n$L_{Pretrain} = (L'_{TC} + L''_{TC}) + (L'_{FC} + L''_{FC}) + L_{dc}$ (12)\nii) Fine-tuning Phase: After task-free pre-training, we implement task-oriented fine-tuning. Notably, the networks are partitioned, and all subsequent networks following the class token extraction are discarded. . This representation diverges from traditional approaches, as it is not simply the output of the features from a final layer in the model. Both temporal and frequency Transformers are pre-added class tokens, and we employ the intermediate class tokens $c_{tt}$ and $c_{tf}$, and concatenate them, denoted as $c_t := Concat(c_{tt}, c_{tf})$. A task-specific training loss is applied with only single linear classifier. We perform fine-tuning from the \"input-to-intermediate-task\" network on the representation, mapping it to the label information y, such as the classification task."}, {"title": "V. EXPERIMENTAL SETTINGS", "content": "We assess the performance of SplitSEE to determine if it meets with the EXPECTATIONS (introduced in Section III). Our evaluation uses experiments to answer the following questions.\nQ1. Effectiveness: How effectively does it perform various downstream tasks with only single-channel EEG?\nQ2. Robustness: How well does SplitSEE learn meaningful representations without label supervision, and how adaptable is it to different channels?\nQ3. Scalability: How can knowledge be transferred across different datasets, and how can SplitSEE be adapted for real-world scenarios?\nA. Tasks and Datasets\nWe conducted extensive experiments on four datasets with two EEG tasks: seizure prediction and sleep stage classification (a brief introduction can be found in Figure 3). The datasets are described in TABLE II.\nSeizure prediction. We focus on a more challenging task, that is, noticing early signs before the onset of a seizure. We used two public databases. The CHB-MIT dataset consists of 844 hours of 22-channel scalp EEG (SEEG) for 22 patients with 163 episodes of seizure. The pre-seizure state was considered to be 5 minutes before seizure onset. The HUH dataset consists of 21-channel sEEG of 79 patients. The pre-seizure state was considered to be 30 seconds before seizure onset.\nSleep stage classification. This task classifies sleep into five stages, i.e., wake, N1, N2, N3, and rapid eye movement"}, {"title": "VI. RESULTS", "content": "A. Effectiveness\nSeizure prediction. TABLE III shows the main results for the seizure prediction task performed on three datasets. Overall, our proposed SplitSEE outperforms all baselines on all metrics. SplitSEE demonstrates superior performance compared with two multi-channel EEG baselines for the CHB-MIT dataset. This promising performance can also be found in our real clinical datasets with 0.930, 0.833, and 0.879 of Spec, Sens, and Acc, respectively. Our SplitSEE exhibits superior stability, with a notably small variance of 2.6 compared to the second-best method, with a variance of 19.2 for the CHB-MIT dataset. In seizure prediction, accuracy is not the only critical metric.\nSleep stage classification. TABLE IV shows that the proposed method achieved superior results to various single-channel EEG-only methods (marked by *) for SHHS datasets across all stage classifications. Compared with the multi-data source baselines, SplitSEE also accurately classifies most of the classes, resulting in a high F1 with 0.94, 0.90, and 0.88 for Wake, N1, and N2, respectively. A similar conclusion can be found in the results for the Sleep-EDF dataset. The proposed method achieved higher staging performance even compared with other multi-channel methods [6], [38].\nB. Robustness\nVariance. Fig. 1 shows comparison results on the average accuracy among channels and its variance. A smaller variance implies that the accuracies are closely grouped around the mean. Here, our method not only achieves the highest average accuracy but also shows the least variance. This means that the errors and uncertainties in our method are minor, and we can achieve high classification accuracy no matter which channel we choose. This result proves the effectiveness of extending SplitSEE to different EEG installations and settings.\nC. Scalability\nLoss and accuracy curves. Fig. 5 presents the self-supervised learning loss curve over 200 epochs alongside the fine-tuning accuracy curve for a few epochs. The loss curve demonstrates a consistent decline, highlighting the effectiveness of the loss function in guiding pre-training for both tasks. On the right, in Fig. 5 (a) and (b), we also show results from supervised training on top of the self-learned features. Significant improvements are evident after just the first epoch of fine-tuning. Notably, this fine-tuning is performed only on selected networks, suggesting that superior performance can be achieved by using a few networks and minimal re-training."}, {"title": "VII. CONCLUSION", "content": "This paper presents SplitSEE, which is a splittable self-supervised learning framework that aims to extract representative and eventful features solely from single-channel EEG data. Our method has the following desirable properties:\n(a) Effectiveness: SplitSEE learns informative features solely from single-channel EEG but has even outperformed multi-channel and multi-data source baselines.\n(b) Robustness: SplitSEE has the capacity to adapt across different channels with low performance variance. Superior performance is also achieved on four public databases and our real clinical dataset.\n(c) Scalability: Our experiments show that with just one fine-tuning epoch, SplitSEE achieves high and stable performance using partial model layers. The federated experiments mirror these results with only one-layer local deployment, showing its great potential in real-world clinical scenarios. Future directions include verifying the effectiveness of the proposed method in more fundamental neurological problems."}]}