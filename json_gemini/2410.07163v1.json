{"title": "SIMPLICITY PREVAILS: RETHINKING NEGATIVE PREFERENCE\nOPTIMIZATION FOR LLM UNLEARNING", "authors": ["Chongyu Fant", "Jiancheng Liu", "Licong Lin", "Jinghan Jia", "Ruiqi Zhang", "Song Mei", "Sijia Liu"], "abstract": "In this work, we address the problem of large language model (LLM) unlearning, aiming to remove\nunwanted data influences and associated model capabilities (e.g., copyrighted data or harmful content\ngeneration) while preserving essential model utilities, without the need for retraining from scratch.\nDespite the growing need for LLM unlearning, a principled optimization framework remains lacking.\nTo this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and\nidentify the issue of reference model bias, which could undermine NPO's effectiveness, particularly\nwhen unlearning forget data of varying difficulty. Given that, we propose a simple yet effective\nunlearning optimization framework, called SimNPO, showing that 'simplicity' in removing the\nreliance on a reference model (through the lens of simple preference optimization) benefits unlearning.\nWe also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of\nMarkov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over\nexisting unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning\nattacks. Codes are available at https://github.com/OPTML-Group/Unlearn-Simple.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language\nmodels (LLMs) has raised security and safety\nconcerns, including issues related to copy-\nright violations and sociotechnical harms\n(Huang et al., 2024; Wang et al., 2023; Li\net al., 2024; Shi et al., 2024). However, re-\ntraining these models to remove undesirable\ndata influences is often impractical due to\nthe substantial costs and time required for\nsuch processes. This gives rise to the prob-\nlem of LLM unlearning, which aims to ef-\nfectively remove undesired data influences\nand/or model behaviors while preserving the\nutility for essential, unrelated knowledge gen-\neration, and maintaining efficiency without\nthe need for retraining (Eldan & Russinovich,\n2023; Yao et al., 2023; Liu et al., 2024b;\nBlanco-Justicia et al., 2024).\nTo trace its origins, the concept of machine unlearning was initially developed for data removal to comply with privacy\nregulations such as the \"right to be forgotten\" (Rosen, 2011; Hoofnagle et al., 2019), with early studies focusing on\nvision models (Cao & Yang, 2015; Warnecke et al., 2021; Bourtoule et al., 2021; Thudi et al., 2022; Kurmanji et al.,"}, {"title": "2 Related work", "content": "Machine unlearning. The commonly accepted gold standard for machine unlearning is retraining the model from\nscratch, excluding the data points to be forgotten from the training set. Such a method (referred to as 'Retrain' in\nour work) is also known as exact unlearning (Cao & Yang, 2015; Thudi et al., 2022; Fan et al., 2024a). However,\nexact unlearning is challenging in practice due to the need for access to the full training set, accurately attributing\nand identifying the forget data, and the high computational cost of retraining. To address these challenges, various\napproximate unlearning methods have been developed (Nguyen et al., 2022; Bourtoule et al., 2021; Triantafillou et al.,\n2024). These approaches typically involve model fine-tuning or editing, applied to the pre-trained model, based on the\nunlearning request. Their effectiveness has been shown in different application domains, including image classification\n(Liu et al., 2022b; Jia et al., 2023; Kurmanji et al., 2023; Fan et al., 2024a), image generation (Gandikota et al., 2023;\nFan et al., 2024b; Zhang et al., 2024b), federated learning (Liu et al., 2022c; Halimi et al., 2022; Jin et al., 2023), and\ngraph neural networks (Chen et al., 2022; Chien et al., 2022; Wu et al., 2023a).\nLLM unlearning. There has also been a growing body of research focusing on machine unlearning for LLMs (Lu\net al., 2022; Jang et al., 2022; Kumar et al., 2022; Zhang et al., 2023; Pawelczyk et al., 2023; Eldan & Russinovich,\n2023; Ishibashi & Shimodaira, 2023; Yao et al., 2023; Maini et al., 2024; Zhang et al., 2024a; Li et al., 2024; Wang\net al., 2024; Jia et al., 2024; Liu et al., 2024b;a; Thaker et al., 2024; Kadhe et al., 2024). Applications of unlearning in\nLLMs are diverse, from safeguarding copyrighted and personally identifiable information (Jang et al., 2022; Eldan &\nRussinovich, 2023; Wu et al., 2023b), to preventing LLMs from creating cyberattacks or bioweapons (Barrett et al.,"}, {"title": "3 A Primer on LLM Unlearning", "content": "Problem formulation of LLM unlearning. Unlearning tasks can take various forms and are typically associated\nwith a specific set of data points to be removed, known as the forget set (Df). In addition, these tasks often require a\ncomplementary set of non-forgotten data points, known as the retain set (Dr), to preserve model utility by penalizing the\ndivergence caused by unlearning. As a result, the problem of LLM unlearning can be cast as a regularized optimization\nproblem that balances the forget and retain objectives (Liu et al., 2024b; Yao et al., 2023; Zhang et al., 2024a):\nminimize \u03b8 E(x,y)\u2208D\u00a3 [lf(y|x; 0)] + \u03bb E(x,y)\u2208D\u2081 [lr(y|x; 0)], (1)\nForget loss\nRetain loss\nwhere \u03b8 represents the model parameters to be updated during unlearning, \u03bb \u2265 0 is a regularization parameter to\npenalize the 'divergence' of unlearning, and lf and lr represent forget and retain losses incurred when using model\nparameters \u03b8 to generate the desired response (y) given the input x.\nSubstantial research has focused on designing and analyzing appropriate forget and retain loss functions to solve\nproblem (1) (Liu et al., 2024b; Yao et al., 2023; Zhang et al., 2024a; Maini et al., 2024; Shi et al., 2024; Eldan &\nRussinovich, 2023; Jia et al., 2024). For instance, let \u03c0\u03b8(y|x) represent the prediction probability of the model \u03b8 given\nthe input-response pair (x, y). The retain loss is typically chosen as the cross-entropy-based sequence prediction loss,\nlr(y|x, 0) = log \u03c0\u03b8(y|x), whose minimization encourages the model to perform well on the retain data (x, y) \u2208 Dr.\nIf we specify the forget loss as the negative token prediction loss lf(y|x, 0) = log \u03c0\u03b8(y|x), whose minimization then\ndiscourages the model from learning the forget data (x, y) \u2208 Df. Minimizing such a forget loss is known as the gradient\nascent (GA) method (Maini et al., 2024; Thudi et al., 2022). Similarly, minimizing the regularized loss that integrates\nGA with the retain loss is known as the gradient difference (GradDiff) method (Liu et al., 2022a; Maini et al., 2024;\nYao et al., 2023).\nNegative preference optimization (NPO). A popular optimization framework for solving problem (1) is NPO (Zhang\net al., 2024a). It treats the forget data as negative examples in DPO (Rafailov et al., 2024), transforming the unbounded\nGA-based forget loss into a \u2460 bounded loss from below, which helps prevent catastrophic collapse, and an \u2461 adaptive\nweight smoothing applied to the forget loss gradients, allowing for more controlled and stable unlearning. These benefits"}, {"title": "4 Uncovering Reference Model Bias: A Limitation of NPO", "content": "In this section, we illustrate the key weakness of NPO, which we term 'reference model bias'. As illustrated in\n(2)-(3), the reference model \u03c0ref is used in NPO to measure and control the divergence speed required for unlearning.\nSpecifically, since the NPO loss (2) is bounded below by 0, minimizing it drives the prediction probability \u03c0\u03b8(y|x)\nto decrease, widening the gap between the prediction probability and the reference model on the forget set, i.e.,"}, {"title": "5 SimNPO: Advancing NPO by Simple Preference Optimization", "content": "In the following, we address the reference model bias in NPO by using a reference-free optimization method, SimPO\n(simple preference optimization) (Meng et al., 2024). We refer to the NPO alternative derived from SimPO as SimNPO,\nsimple negative preference optimization.\nMotivation of SimNPO and its forget objective. The simplest solution to mitigating NPO's reference model bias is\nto directly remove Tref from the gredient in (3), setting #ref = 0. However, this variant would be ineffective, as the\nreference-free gradient reduces to GA, with we(x, y) = 1. This negates NPO's advantages.\nTo develop a better solution for improving NPO, we address the reference model issue by revisiting the context of pref-\nerence optimization and investigating whether the reference model can be excluded while still retaining the unlearning\nbenefits provided by NPO. Our idea parallels how NPO was originally inspired by DPO (Rafailov et al., 2024). We\nadopt SimPO, a reference-free alternative to DPO, as the optimization framework for unlearning, leading to the SimNPO\nmethod. The key difference between SimPO and DPO lies in their reward formulation for preference optimization. In\nDPO, the reward formulation is given by the comparison with the reference model, i.e., \u03b2log(\u03c0\u03b8(y|x)/\u03c0ref(y|x)). This\nformulation was used by NPO. In contrast, SimPO takes a reference-free but length-normalized reward formulation:\n(\u03b2/|y|) log \u03c0\u03bf(y|x), where |y| denotes the response length.\nTaking the inspiration of SimPO, we can mitigate the reference model bias in NPO by replacing its reward formulation\n\u03b2log(\u03c0\u03bf(y|x)/\u03c0ref(y|x)) in (2) with the SimPO-based reward formulation (\u03b2/|y|) log(\u03c0\u04e9(y|x)). This modification\ntransforms (2) into the SimNPO loss:\nlSimNPO (0) = E(x,y)\u2208Df 2 logo (-  Y log \u03c0\u03bf (yx) \u2013 \u03b3 )], (4)\nwhere y \u2265 0 is the reward margin parameter, inherited from SimPO, which defines the margin of preference for a\ndesired response over a dispreferred one. However, unless otherwise specified, we set y = 0 to align with the NPO loss\n(2). This is also desired because y introduces a constant shift to the prediction loss \u2013 (\u03b2/|y|) log \u03c0\u04e9(y|x). Consequently,\na larger y requires greater compensation to further suppress token prediction, enforcing a stricter unlearning condition.\nThis can accelerate the utility drop during unlearning. See Fig. Al for an empirical justification. The SimNPO loss (4),\nwhen integrated with the regularized optimization in (1), forms the SimNPO method.\nInsights into SimNPO. Similar to NPO, the SimNPO loss (4) is bounded from below, with a minimum value of 0.\nApproaching this minimum drives the unlearning. However, the key distinction of SimNPO from NPO is its forget\ndata-aware, length-normalized reward formulation, (\u03b2/|y|) log \u03c0\u04e9(y|x) in (4). This eliminates the reference model bias\nand results in an improved gradient smoothing scheme. Specifically, the gradient of the SimNPO loss (with \u03b3 = 0)\nyields (as derived in Appendix A):\nVolSimNPO (0) = E(x,y)\u2208Df 2(\u03c0o(y|x))B//y Velog To (yx) := w(x, y) (5)\n1+ (To(y/x))/y Y\nSimilar to NPO in (3), the gradient in (5) can be divided into two components: weight smoothing (w) and GA.\nHowever, in SimNPO, the weight smoothing is no longer influenced by the reference model and is instead normalized\nby the length |y|. This introduces two key advantages (a)-(b) below, in response to NPO's limitations (L1)-(L2)."}, {"title": "6 Experiments", "content": "6.1 Experiment setups\nDatasets, tasks, and models. Our experiments cover unlearning tasks across three benchmark datasets: TOFU (Maini\net al., 2024), MUSE (Shi et al., 2024), and WMDP (Li et al., 2024), as summarized in Table 1. For TOFU, we focus on\ntwo unlearning scenarios, termed 'Forget05' and 'Forget10', which refer to forget set sizes of 5% and 10%, respectively.\nIn MUSE, we also explore two unlearning scenarios: forgetting the Harry Potter books (termed \u2018Books') and news\narticles (termed 'News'), respectively. WMDP, on the other hand, is designed for knowledge-based unlearning, with\nthe forget texts representing hazardous knowledge in biosecurity and cybersecurity. The LLM models used for each\nunlearning benchmark are listed in Table 1.\nLLM unlearning methods and evaluation. First, we refer to the model prior to unlearning as Original, which is either\nfine-tuned on the unlearning tasks (TOFU or MUSE) or the pre-trained model after alignment for WMDP. Starting from\nthe original model, we then apply the following unlearning methods to a given forget set and/or retain set to achieve the\nunlearning objective, as outlined in (1). Specifically, Retrain refers to retraining an LLM by excluding the forget set\nand is considered as the gold standard of unlearning when available. Retrain is provided in both the TOFU and MUSE\nbenchmarks. As introduced in Sec. 3, we also include GA (gradient ascent) and GradDiff (the retain-regularized GA\nvariant) as unlearning baseline methods, following the implementations in TOFU and MUSE benchmarks. For other\nbaseline methods such as the rejection-based unlearning method (IDK) in TOFU, and the Task Vector unlearning\nmethod in MUSE, we adhere to the original implementations specified in their respective benchmarks. NPO with\nthe retain regularization in (1) serves as the primary baseline. Note that its implementation on TOFU follows the\noriginal NPO study (Zhang et al., 2024a), while its implementation on MUSE aligns with the MUSE benchmark. For\nNPO on WMDP, due to the absence of open-source implementation, we adapt the TOFU codebase to WMDP. More\nimplementation details can be found in Appendix C.1. To implement the proposed method SimNPO, we adopt a setting\nsimilar to NPO but adjust the temperature parameter B. Due to the presence of length normalization in (4), a larger\nvalue for \u1e9e is preferred compared to that in NPO. See the specific choices in Appendix C.2.\nTo assess unlearning effectiveness and model utility, we use the evaluation metrics summarized in Table 1 under each\nunlearning benchmark. In addition, we evaluate the robustness of an unlearned model using relearning-based attacks\n(Hu et al., 2024), which aim to recover the forgotten information by fine-tuning the unlearned models on a small subset\nof the forget set after unlearning. We select 20% of the TOFU forget05 set as the relearning set over three epochs."}, {"title": "7 Conclusion", "content": "We revisited the current unlearning optimization framework, negative preference optimization (NPO), and identified\nits reference model bias issue, which compromises unlearning effectiveness, particularly for forget data of varying\ndifficulty. To address this, we introduced SimNPO, a simple yet effective framework that eliminates reliance on a\nreference model by leveraging simple preference optimization. We provided deep insights into SimNPO's advantages\nthrough both synthetic data analysis and evaluations on existing unlearning benchmarks such as TOFU, MUSE, WMDP,\nand relearning attacks. In future work, we will further investigate the limitations of SimNPO and enhance it for tasks\ninvolving model capability removal. See further discussions in Appendix E-F."}, {"title": "Appendix", "content": "A Gradient Analysis of SimNPO\nFollowing is the detailed derivation of (5). First, let R =\n2 log \u03c0\u03bf(y/x)+|y|/B. We then have the following steps:\nVelSimNPO (0) = E(x,y)\u2208Df Ve -logo(-\u03b2R) (A1)\n= E(x,y)\u2208D Vo 2 log \u03c3(1 + exp(3R)) (A2)\n= E(x,y)ED B1+ exp(BR)VOR (A3)\n= E(x,y)\u2208Df Bexp(BR)VOR Velog to (y) (A4)\n2 exp(Blog \u03c0\u03bf (y|x)+|y|/\u03b2)\nWhen y = 0, the gradient simplifies to the following, which matches (5):\nVelSimNPO(0) = E(x,y)\u2208Df Velog \u03c0\u03bf (yx) (A5)\n= E(x,y) \u2208Df 1 (A6)\n2(To(y/x))/ B 1+(To(y/x))/ y\nB Additional Details on the Synthetic Study\nSynthetic experiment setup. In the synthetic experiment, we study the unlearning problem in a scenario where the\ndata are generated from a mixture of Markov chains. Namely, we assume the Markov chains have a shared state space\nof size 10 (denoted by s = 1, 2, ..., 10), and the retain distribution and the forget distribution have the formulas as\nfollows:\n\u2022 Retain distribution: Markov chain with initial distribution \u03c0, \u2208 R10 and transition matrix T \u2208 R10\u00d710, where\n17\u20ac for j \u2264 3, Tr,j for j \u2265 4.\n3\nTr,i. = \u03c0r for i \u2264 3, Tr,i. = 0.1110 for i\u2265 4.\n\u2022 Forget distribution: a mixture of two Markov chains (denoted by Forget1 and Forget2) with equal probability. Let\n(\u03c0f1, Tf1) and (\u03c0f2, Tf2) denote the initial distribution and transition matrix for Forget1 and Forget2. We assume"}]}