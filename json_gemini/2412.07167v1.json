{"title": "Reinforcement Learning Policy as Macro Regulator\nRather than Macro Placer", "authors": ["Ke Xue", "Ruo-Tong Chen", "Xi Lin", "Yunqi Shi", "Shixiong Kai", "Siyuan Xu", "Chao Qian"], "abstract": "In modern chip design, placement aims at placing millions of circuit modules,\nwhich is an essential step that significantly influences power, performance, and\narea (PPA) metrics. Recently, reinforcement learning (RL) has emerged as a\npromising technique for improving placement quality, especially macro placement.\nHowever, current RL-based placement methods suffer from long training times,\nlow generalization ability, and inability to guarantee PPA results. A key issue lies\nin the problem formulation, i.e., using RL to place from scratch, which results in\nlimits useful information and inaccurate rewards during the training process. In\nthis work, we propose an approach that utilizes RL for the refinement stage, which\nallows the RL policy to learn how to adjust existing placement layouts, thereby\nreceiving sufficient information for the policy to act and obtain relatively dense\nand precise rewards. Additionally, we introduce the concept of regularity during\ntraining, which is considered an important metric in the chip design industry but is\noften overlooked in current RL placement methods. We evaluate our approach on\nthe ISPD 2005 and ICCAD 2015 benchmark, comparing the global half-perimeter\nwirelength and regularity of our proposed method against several competitive\napproaches. Besides, we test the PPA performance using commercial software,\nshowing that RL as a regulator can achieve significant PPA improvements. Our RL\nregulator can fine-tune placements from any method and enhance their quality. Our\nwork opens up new possibilities for the application of RL in placement, providing\na more effective and efficient approach to optimizing chip design. Our code is\navailable at https://github.com/lamda-bbo/macro-regulator.", "sections": [{"title": "1 Introduction", "content": "In the complex and evolving landscape of modern chip design, placement is a pivotal process that\nsignificantly influences the power, performance, and area (PPA) metrics of the final chip [21; 22].\nA modern chip typically comprises thousands of macros (i.e., individual building blocks such as\nmemories) and millions of standard cells (i.e., smaller basic components like logic gates). The\nmacro placement result provides a fundamental solution for the subsequent processes (e.g., standard\ncells placement and routing), thus playing an important role [32]. For example, macro placement\ninfluences the placement of standard cells, and poor macro placement might make it challenging\nto place these cells optimally, leading to an unsatisfactory chip performance [33]. Moreover, an\ninappropriate macro placement can result in macro blockage in the core center, which harms the\noverall chip performance by causing unwanted effects such as routing congestion, inferior wirelength,\nand timing performance issues [26]."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Placement", "content": "The circuit in the placement stage is considered as a graph where vertices model gates. The main\ninput information is the netlist N = (V, E), where V denotes the information (i.e., height and\nwidth) about all macros designated for placement on the chip, and E is a hyper-graph comprised\nof nets $e_i \\in E$, which encompasses multiple cells (including both macros and standard cells) and\ndenotes their inter-connectivity in the routing stage. Given a netlist, a fixed canvas layout and a\nstandard cell library, a placement method is expected to determine the appropriate physical locations\nof movable macros such that the total wirelength can be minimized. A macro placement solution\n$s = \\{(x_1,y_1), ..., (x_k, Y_k)\\}$ consists of the positions of all the macros $\\{v_i\\}_{i=1}^{k}$, where k denotes the\ntotal number of macros. One popular objective of macro placement is to minimize the total HPWL of\nall the nets while satisfying the cell density constraint, which is formulated as,\n\n$\\min HPWL(s) = \\min \\sum_{e\\in E} HPWL_e(s), s.t. D(s) \\leq \\epsilon,$\n\nwhere D denotes the density, $\\epsilon$ is a threshold, and $HPWL_e$ is the HPWL of net e, which is defined\nas: $HPWL_e(s) = (\\max_{v_i \\in e} x_i - \\min_{v_i \\in e} x_i) + (\\max_{v_i \\in e} Y_i - \\min_{v_i \\in e} Y_i)$.\n\nThere are three mainstream placement methods, i.e., analytical methods, black-box optimization\nmethods, and learning-based methods. Analytical methods [4] place macros and standard cells\nsimultaneously, which can be roughly categorized into quadratic placement and nonlinear placement.\nQuadratic placement [11; 18] iterates between an unconstrained quadratic programming phase to\nminimize wirelength and a heuristic spreading phase to remove overlaps. Nonlinear placement [6;\n20; 7] formulates a nonlinear optimization problem and tries to directly solve it with gradient descent\nmethods. Generally speaking, nonlinear placement can achieve better solution quality, while quadratic\nplacement is more efficient. Recently, there has been extensive attention on GPU-accelerated non-\nlinear placement methods. For example, DREAMPlace [19; 17] transforms the non-linear placement\nproblem in Eq. (1) into a neural network training problem, solves it by classical gradient descent\nand leverages GPU, enabling ultra-high parallelism and acceleration and producing state-of-the-art\nanalytical placement quality."}, {"title": "2.2 RL for Macro Placement", "content": "Researchers recently leverage RL-based methods for better placement quality to meet the demands of\nmodern chip design. GraphPlace [23] first models macro placement as a RL problem. It divides the\nchip canvas into discrete grids, with each macro assigned discrete coordinates of grids, wherein the\nagent decides the placement of the current macro at each step. However, no reward is given until\nall the macros are placed, making the reward sparse and hard to learn. DeepPR [9] and PRNet [8]\nincorporate macro placement, standard cells placement, and routing to achieve better performance\nthan GraphPlace, but may violate the non-overlap constraint. To address this issue, MaskPlace [16]\nintroduces a dense reward and uses a pixel-level visual representation for circuit modules, which\ncan comprehensively capture the configurations of thousands of pins, enabling fast placement in\na full action space on a large canvas size. MaskPlace has many attractive benefits that previous\nmethods do not have, e.g., 0% overlap, dense reward, and high training efficiency. ChiPFormer [15]\nincorporates an offline learning decision transformer and focuses on improving the generalizability of\nplacer. EfficientPlace [10] integrates a global tree search algorithm to guide the optimization process,\nachieving remarkable placement quality within a short time.\n\nHowever, current RL methods exhibit several shortcomings: 1) Placing from scratch provides\ninsufficient state information and inaccurate reward signals; 2) Most methods focus on minimizing\nwirelength, which may bring macro blockages and thus harm the final PPA metrics. In this work, we\npropose a novel RL approach for macro placement: an RL policy acts as a macro regulator rather than\na macro placer. Specifically, our learned RL policy is designed to adjust macros based on an existing\nplacement result, rather than placing all macros from scratch. This approach aims to refine and\noptimize pre-existing layouts, addressing the limitations of traditional RL-based placement methods."}, {"title": "3 Method", "content": "We present our proposed MaskRegulate here. Section 3.1 introduces our problem formulation and\npolicy architecture, and Section 3.2 describes how to integrate regularity into the method."}, {"title": "3.1 MaskRegulate Framework", "content": "Problem formulation of RL regulator. In the Markov Decision Process (MDP) formulation of\ntraditional RL placer, a macro is placed at each step [23; 9; 16; 15]. The placement order of macros\nis determined based on some pre-defined rules, such as the number of nets, the size of macros, and\nthe number of connected modules that have been placed. An episode ends after all macros have been\nplaced. Typically, the state representation includes information about the chip canvas, the macros that\nhave already been placed, and the macro currently being placed. In GraphPlace [23], the reward is\ndetermined only after all macros have been placed, resulting in a sparse reward signal that complicates\nthe training process. Recent works have introduced various methods to densify the reward signal. For\ninstance, WireMask [16] provides a more continuous reward based on the macros already placed. In\ncontrast to RL placers, our RL regulator focuses on refining an existing placement by adjusting the\nlocation of one macro at each step. Unlike the placer, which initiates the placement process from\nscratch, the regulator benefits from additional information when adjusting each macro. Specifically,\nthe regulator considers not only the macros that have already been placed but also the positions of\nall other macros. Furthermore, it enhances accuracy by taking into account all macros, even while\nemploying a reward function similar to WireMask.\n\nDue to the advantages mentioned above in the MDP problem formulation, even without considering\nadditional factors (e.g., regularity), RL regulator is able to achieve better results compared to RL\nplacer, as shown in our experiments in Appendix B.1. Furthermore, our main experimental results\ndemonstrate superior performance not only in proxy metrics but also in PPA metrics measured by\ncommercial tools, as shown in Section 4.2. The regulator also exhibits better generalization abilities,\nas shown in Section 4.3. Intuitively, adjusting an unseen chip is easier for the regulator compared to"}, {"title": "3.2 Integration of Regularity", "content": "Why does regularity matters? Macro placement has significant impact on subsequent chip design\nprocesses, including standard cell placement and routing. If only focusing on minimizing wirelength\n(which is the case for most current RL placers), certain macros may end up positioned in the middle"}, {"title": "4 Experiment", "content": "In this section, we first introduce the basic experimental settings, including the tasks and evaluation\nmetrics in Section 4.1. Then, we try to answer the following three research questions (RQs) in\nSections 4.2 to 4.4: 1) How does MaskRegulate perform compared to other methods? 2) How is the\ngeneralization ability of MaskRegulate? 3) How do the different parts of MaskRegulate affect the\nperformance? Finally, we provide the visualization of placement results and congestion in Section 4.5."}, {"title": "4.1 Experimental Settings", "content": "Tasks. We mainly use the ICCAD 2015 benchmark [14] as our test-bed, which includes sufficient\nadvanced chip information and is currently one of the largest open-source benchmarks that allows us\nto evaluate congestion, timing and other PPA metrics. The benchmark statistics are listed in Table 3\nin Appendix A.1. Although ICCAD 2015 is the benchmark we have found that closely reflects the\ncurrent practices in the EDA industry, it still has some shortcomings. For example, it allows for a\nlarge placement area, resulting in loose placement results that do not adhere to the design principles\nof advanced modern chips. Note that the \"A\" in PPA denotes \u201cArea\u201d, which is a core metric of\nchip design and should be minimized [3; 32]. Therefore, we scale down the chip's placement area,\npresenting further challenges for the compared methods. Besides, we also conduct experiments on\nISPD 2005 benchmark [25], which is also a popular benchmark in AI for chip design but does not\nhave sufficient information for PPA evaluation. Detailed results can be found in Appendix B.\n\nProxy evaluation metrics. We use the following two popular proxy metrics for a quick comparison\nof different algorithms: 1) Global HPWL. After determining the locations of all the macros, we use\nDREAMPlace [19] to place standard cells to obtain the global placement result, and then report the\nglobal HPWL (i.e., full HPWL involving both macros and standard cells). Compared to macro HPWL,\nglobal HPWL considers the total wirelength, typically on a scale that is two orders of magnitude\nlarger, providing a better estimation of the final real performance of the chip. 2) Regularity: We\ncompute the regularity values for all macros, which serve as a measurement of the overall regularity\nof the placement result. We run each algorithm for five times and report their mean and variance. We\ndo not consider the rectangular uniform wire density (RUDY) metric [30] for congestion proxy, as\nthis approximation is sometimes positively correlated with the HPWL metric and is not accurate [29].\nInstead, we will evaluate congestion within our PPA evaluation."}, {"title": "4.2 RQ1: How does MaskRegulate perform compared to other methods?", "content": "We consider the following methods to be compared: DREAMPlace [19]: A state-of-the-art analytical\nplacer; AutoDMP [1]: A method that improves DREAMPlace by exploring its configuration space\niteratively; WireMask-EA [29]: A state-of-the-art black-box macro placement method with EA as\nthe optimizer; MaskPlace [16]: A representative online RL methods, which shares similar policy\narchitecture, state, HPWL reward with our MaskRegulate.\n\nFor the same components, MaskPlace and MaskRegulate use the same settings, e.g., the number of\ngrids, and the learning rate. Detailed information is provided in Appendix A.3. Additionally, in order\nto demonstrate that the regulator has higher training efficiency than the placer, MaskRegulate and\nMaskPlace are trained for 1000 and 2000 episodes, respectively. For each chip, MaskRegulate uses\nDREAMPlace to obtain an initial macro placement result to be adjusted, which takes within few\nminutes and has relatively low quality.\n\nThe overall evaluation results are shown in Table 1. MaskRegulate achieves the best average rank on\nboth proxy and PPA metrics. DREAMPlace has the worst average ranking on wirelength, congestion,\nand timing. However, after adjustment by MaskRegulate, the obtained placements achieve the\nbest average rank. Compared to MaskPlace, MaskRegulate leads to significant improvements in\nmultiple PPA indicators: improves 17.08% on routing wirelength, 73.08% and 38.81 % on routed\nhorizontal and vertical congestion overflow respectively, 18.35% on worst negative slack, 37.89%\non total negative slack, and 46.17% on the number of violation points. By incorporating regularity,\nMaskRegulate achieves the highest regularity on all the eight chips. We can observe a certain\ncorrelation between the proxy metric (global HPWL) and the real metric (rWL), but there still exists\na gap, indicating the challenges involved in the placement task. Furthermore, we provide detailed\nvisualizations of placement results in Figure 5, where MaskRegulate shows significant improvements\non congestion metrics. Besides, the final placement layouts of MaskRegulate are much regular than\nall the other methods."}, {"title": "4.3 RQ2: How is the generalization ability of MaskRegulate?", "content": "The generalization ability of RL policies is an important question to be investigated. In this section,\nwe pre-train MaskRegulate and MaskPlace on the first four chips (i.e., superblue1, superblue3,\nsuperblue4, and superblue5) and test on the remaining four chips. To further validate the ability of\nMaskRegulate to adjust different initial placement results, we use it to adjust the results obtained by\ndifferent initial placements on the test chips.\n\nThe results are shown in Table 2. On both the global HPWL and regularity metrics, MaskRegulate\nconsistently outperforms MaskPlace, showcasing its stronger generalization capability. An interesting\nfinding is that MaskRegulate performs better on unseen chips than on the chips it was trained on,\nspecifically in terms of global HPWL, such as with superblue16. This may suggest that MaskRegulate\nhas learned some general knowledge during the pre-training process, enabling it to overcome local\noptima that may arise from direct learning on the target chip."}, {"title": "4.4 RQ3: How do the different parts of MaskRegulate affect the performance?", "content": "We investigate the influence of different parts and provide additional analysis in this section.\n\nHyperparameters sensitivity analysis: different trade-off coefficient a leads to different multi-\nobjective preferences. One hyperparameter of RegularMask is the coefficient $\\alpha$ between HPWL\nreward $r_{wire}$ and regularity reward $r_{reg}$, where a higher $\\alpha$ indicates a preference for optimizing"}, {"title": "5 Final Remarks", "content": "Conclusion. In this paper, we present a novel RL problem formulation for macro placement, focusing\non the development of a macro regulator rather than a placer. Our proposed method, MaskRegulate,\ndemonstrates substantial improvements in chip placement quality by refining existing layouts instead\nof generating them from scratch. By integrating dense reward signals and emphasizing regularity, our\napproach effectively addresses the limitations of traditional RL-based placement methods, resulting\nin superior performance in PPA metrics across various chips. This advancement paves the way for\nmore efficient and effective chip design through RL.\n\nLimitations and future work. This study has several primary limitations: it does not consider the\nimpact of module aspect ratio and area factors on placement; it overlooks global wirelength and timing\nmetrics during the training process; and it does not employ advanced transformer architectures [15] to\nenhance the generalization of the regulator. Chip design inherently involves different preferences, such\nas the need for compact size in mobile phone chips and larger sizes for computer chips. Therefore,\nfuture research should address these challenges and explore efficient methods to obtain a set of chip\nplacements that accommodate different preferences using multi-objective optimization."}]}