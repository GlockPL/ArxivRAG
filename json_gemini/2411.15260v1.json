{"title": "VIVID-10M: A Dataset and Baseline for Versatile and Interactive Video Local Editing", "authors": ["Jiahao Hu", "Tianxiong Zhong", "Xuebo Wang", "Boyuan Jiang", "Xingye Tian", "Fei Yang", "Pengfei Wan", "Di Zhang"], "abstract": "Diffusion-based image editing models have made remarkable progress in recent years. However, achieving high-quality video editing remains a significant challenge. One major hurdle is the absence of open-source, large-scale video editing datasets based on real-world data, as constructing such datasets is both time-consuming and costly. Moreover, video data requires a significantly larger number of tokens for representation, which substantially increases the training costs for video editing models. Lastly, current video editing models offer limited interactivity, often making it difficult for users to express their editing requirements effectively in a single attempt. To address these challenges, this paper introduces a dataset VIVID-10M and a baseline model VIVID. VIVID-10M is the first large-scale hybrid image-video local editing dataset aimed at reducing data construction and model training costs, which comprises 9.7M samples that encompass a wide range of video editing tasks. VIVID is a Versatile and Interactive VIdeo local eDiting model trained on VIVID-10M, which supports entity addition, modification, and deletion. At its core, a keyframe-guided interactive video editing mechanism is proposed, enabling users to iteratively edit keyframes and propagate it to other frames, thereby reducing latency in achieving desired outcomes. Extensive experimental evaluations show that our approach achieves state-of-the-art performance in video local editing, surpassing baseline methods in both automated metrics and user studies. The VIVID-10M dataset and the VIVID editing model will be available at https://inkosizhong.github.io/VIVID/.", "sections": [{"title": "1. Introduction", "content": "Image and video editing based on diffusion models [11, 26, 28] have achieved great progress in recent years. Video editing algorithms, which generate edits based on a reference video and a provided description, can generally be classified into two categories: training-free [1, 6, 15, 22] and training-based [5, 18, 21, 23, 30\u201332, 36, 40]. Training-based algorithms typically achieve superior text alignment and temporal consistency. To enable more precise and controllable video edits, local editing methods [18, 36, 40] utilize mask sequences to define the editing regions, enhancing the ability to preserve background, i.e., maintaining non-editable areas unchanged.\nHowever, achieving high-performance video local editing faces several challenges. C1. Lack of large-scale video editing datasets. Training-based algorithms require extensive high-quality paired data. Some algorithms [5, 23] leverage large language models and training-free approaches to construct synthetic video datasets. However, this approach is unable to generate local editing data, thereby constraining the performance of training-based models to the limitations of training-free approaches. Video local editing algorithms [36, 40] extract mask sequences from video frames via visual perception algorithms [17, 19, 35] and mask the original videos to generate paired data. Despite using high-quality real-world video data, there is still no open-source large-scale dataset for video local editing tasks. Constructing such a dataset is challenging due to the time- and resource-intensive demands of the data processing pipeline. C2. High training overhead. Video editing models typically add temporal attention layers [18, 31, 36, 40] to image editing [2] or generation models [25]. Video data also require more tokens to represent than image data, reducing the training efficiency of video editing models compared to image editing models. C3. Limited interactivity. Users often find it challenging to represent their editing requirements in a single attempt. This necessitates iterative adjustments and feedback cycles to refine the edits, leading to prolonged inference times during the video editing process. This lack of seamless interactivity prolongs the time to achieve desired results.\nWe address challenges C1 and C2 by leveraging a large volume of easily constructed image data to optimize the model's spatial modeling capabilities, while using video data to enhance spatio-temporal modeling. To this end, we introduce VIVID-10M, a high-quality video local editing dataset, consisting of 9.7M samples derived from 73.7K videos and 672.7K images. Each video and image meets a resolution above 720p, with video clips spanning at least 5 seconds in duration. VIVID-10M is constructed through an automated pipeline that cascades various visual perception models [19, 24, 29, 35] and a multi-modality large language model [4]. Each sample includes ground truth, masks, masked data and local captions for addition, deletion, and modification tasks. To evaluate VIVID-10M, we propose VIVID, a versatile and interactive video local editing model that supports entity addition, deletion, and modification (Fig. 1). VIVID is jointly trained on image and video data to reduce training overhead, achieving state-of-the-art performance compared to existing methods [31, 38, 40].\nTo address challenge C3, we propose a Keyframe-guided Interactive Video Editing mechanism (KIVE), enabling users to quickly achieve editing results for keyframes using an image editing model and propagate satisfactory results to the remaining frames. Additionally, since VIVID employs mixed image and video training, it is also applicable during the keyframe editing phase. Experiments demonstrate that the KIVE mechanism significantly enhances user interactivity, leading to more efficient workflows and high-quality video editing outcomes. Furthermore, the KIVE mechanism supports local editing of long videos by using the last frame of one edited clip as the keyframe for the next.\nIn summary, we highlight the main contributions:\n1. We introduce VIVID-10M, the first large-scale high-quality dataset for video local editing.\n2. We present VIVID, a robust video local editing model that supports entity addition, modification, and deletion.\n3. We propose a Keyframe-guided Interactive Video Editing (KIVE) mechanism that enhances user experience by enabling iterative keyframe edits."}, {"title": "2. Related Work", "content": "Open-source image editing datasets have significantly contributed to advancing image editing models [2, 7, 8, 13, 14, 16, 20, 34, 37, 39]. Tab. 1 summarizes the existing image and video editing datasets. For instance, InstructPix2Pix [2] and HQ-Edit [13] use large language models (LLMs) to generate paired captions and editing instructions, with image generation models creating the corresponding images. MagicBrush [34] relies on human annotators to manually label data from image generation models. UltraEdit [37] uses the prompt-to-prompt [9] mechanism and a modified image inpainting pipeline to generate free-form and region-based (local) editing samples, respectively. In contrast, only one public video editing dataset, InsV2V [23], is currently available, and it does not support local editing. InsV2V synthesizes videos based on the captions generated by the LLM, and produces corresponding editing data through the prompt-to-prompt [9] mechanism. The lack of large-scale high-quality video editing datasets is a primary obstacle to the advancement of video editing."}, {"title": "2.2. Training-free Video Editing", "content": "Training-free video editing algorithms use pretrained image or video generation models [1, 6, 15, 22] to implement video editing in a training-free manner. These algorithms apply DDIM inversion [27] and incorporate additional mechanisms to ensure controllable, continuous and stable video editing. For example, FateZero [22] blends the self-attention maps with masks to stabilize non-editing areas. FLATTEN [6] extracts inter-frame optical flow to guide self-attention calculations and improve temporal consistency. RAVE [15] shuffles latents across frames and concatenates them together as a large image for denoising to ensure temporal consistency. UniEdit [1] maintains separate reconstruction and motion branches, injecting attention maps or value features into the main branch. Although these algorithms do not require the construction of data or training models, the quality of the edits often falls short in terms of temporal consistency, text alignment, and background preservation, among other factors."}, {"title": "2.3. Training-based Video Editing", "content": "Training-based approahces [5, 23, 30\u201332, 36, 40] often achieve better editing editing quality. Several algorithms [30, 32] extend the text-to-image model to a text-to-video model, employing one-shot learning to extract motion information into the model parameters, thereby enabling video editing in a similar way as image editing. Other algorithms [5, 23] generate synthetic datasets based on training-free or one-shot approaches, which are then used to train models. However, the editing quality is constrained by the generation quality of the data generator. Recently, video local editing algorithms [31, 36, 40] introduce automated data construction pipelines and train the model on real-world data. These algorithms mask entities in videos and use LLMs to generate local captions for the masked regions. The masked video serves as the model input, while the original video is used as the ground truth during training."}, {"title": "3. VIVID-10M Dataset", "content": "In this section, we introduce VIVID-10M, which, to the best of our knowledge, is the first open-source large-scale video local editing dataset. It covers a range of tasks including addition, modification, and deletion (Fig. 2). Each training sample is a tuple $(x, m, \\tilde{x}, y)$, where $x = \\{x^i\\}$ represents a video or image. i denotes the i-th frame of the video, and we consider an image as a video with only one frame. $m = \\{m^i\\}$ denotes the corresponding binary masks of the editing area, $\\tilde{x} = \\{\\tilde{x}^i\\}$ is the masked video or image, and y is a caption of the editing area. In the masked video or image, the editing regions are erased, while the non-editing regions are preserved, so $\\tilde{x}^i = x^i \\odot (1 - m^i)$.\nVIVID-10M contains two subsets, VIVID-10M-Video and VIVID-10M-Image, both derived from the publicly available PANDA-70M dataset [3]. The video subset includes 73.7K videos, each at least 5 seconds in length. The image subset contains the first frame extracted from 672.7K videos. Subsequent sections detail the dataset construction methods for various tasks (Sec. 3.1 and Sec. 3.2). We also proposed a data augmentation method in Sec. 3.3, designed to diversify the original mask into six distinct types, varying in shape and scale. Finally, in Sec. 3.4, we provides statistics for the dataset and introduces VIVID-10M-Eval, a high-quality validation dataset."}, {"title": "3.1. Addition&Modification Data Pipeline", "content": "The addition task adds new entities to the video, while the modification task changes the type or attributes of existing entities. The goal of both tasks is to draw entities in the mask area within the video. To unify the training data formats for both tasks, we select entities from images and videos and generate corresponding local captions. As shown in Fig. 3(a), the pipeline for VIVID-10M-Video consists of three stages: entity selection, mask propagation, and local caption generation, while the pipeline for VIVID-10M-Image omits the mask propagation stage.\nEntity Selection. In this stage, editable entities are selected from the image or the first frame of the video, followed by mask and caption generations. Specifically, we first apply RAM [35] to extract entity labels from the frame and filter the labels using a predefined vocabulary (see Appendix). Then, we use Grounding DINO [19] to detect the bounding boxes corresponding to the labels. Finally, each box serves as a prompt for SAM2 [24] to generate the mask.\nMask Propagation. For video data, the editable area must track the movement of the entity across the frames. Therefore, we use SAM2 [24] to propagate the mask from the first frame to the subsequent frames.\nLocal Caption Generation In this stage, we generate local captions for the editing areas. First, we use x and m to crop entities from the video or image $\\{x^i m^i\\}$, where the non-editing areas are erased. These cropped inputs, denoted as $\\hat{x}$ are then fed into InternVL2 [4] to generate local captions of three different lengths. This process augments the addition and modification samples by a factor of three. The prompt we used for InternVL2 is detailed in the Appendix."}, {"title": "3.2. Deletion Data Pipeline", "content": "The deletion task involves removing existing entities from the video and inpainting these areas with background pixels. Unlike the addition and modification tasks, paired data for the deletion task cannot be generated simply by masking existing entities, as this task requires ground truth background pixels for effective training. To address this, we construct the deletion dataset by adding entity masks from other videos to the background areas. Fig. 3(b) illustrates the pipeline of the deletion task. The deletion pipeline consists of three stages: background positioning, mask pasting, and mask propagation. The local caption for deletion task is fixed: \u201cRemove objects and generate areas that blend with the background.\u201d\nBackground Positioning. Similar to the Entity Selection stage in the addition&modification pipeline, we use RAM [35], Grounding DINO [19] and SAM2 [24] to identify the background areas in the first frame. The only difference is that the vocabulary is replaced with the background vocabulary (see Appendix).\nMask Pasting. To align with inference, we paste entity masks from other videos to the background areas. Specifically, we randomly select a mask sequence from the addition&modification samples and paste the first mask into the background area of the keyframe.\nMask Propagation. There are two possible scenarios for the deletion task: 1) deleting a foreground entity (e.g., removing a car on the road) and 2) deleting a background entity (e.g., removing a picture frame from the wall). In the first case, the foreground entity follows its motion trajectory, so we directly copy the subsequent masks and paste them into the subsequent frames. In contrast, for the second case, the entity's motion trajectory is aligned with the background. Therefore, we use RAFT [29] to calculate the optical flow of the background pixels and propagate the mask on keyframe to the subsequent frames. Both propagation methods are applied for each video, thereby expanding the deletion samples with 2x."}, {"title": "3.3. Mask Augmentation", "content": "The pipelines described in Sec. 3.1 and Sec. 3.2 generate masks that strictly match the entity shapes, which may leak semantic information and reduce the robustness of the editing model. To address this issue and expand the dataset, we apply data augmentation. Three operators are employed: expand, hull, and box. The expand operator randomly enlarges the mask while preserving its original shape. The hull operator calculates the convex hull of the mask, and the box operator determines the bounding box. By combining these operators, we derive five new masks: 1) expand, 2) hull, 3) box, 4) hull+expand, 5) box+expand. These masks are visualized in Fig. 3(c). Through data augmentation, the amount of samples is increased by 6\u00d7. To maintain data quality, we filter out masks that are too large or too small."}, {"title": "3.4. Statistics", "content": "The statistics of VIVID-10M are shown in Tab. 2. Since the pipeline of VIVID-10M-Video is more complicated, it not only consumes more computation, but also introduces more noised data than VIVID-10M-Image. To evaluate the quality of the datasets, we measure the quality from three dimensions using user study: Mask Generation (MG), Mask Propagation (MP) and Text Alignment (TA). Tab. 2 shows that the two subsets perform similarly in terms of MG and TA metrics, while VIVID-10M-Video introduces additional noise in the MP process, which ultimately leads to lower ratio of high-quality data (HQ) than VIVID-10M-Image. This demonstrates that using image data to expand video data can effectively reduce the construction cost of high-quality data.\nLastly, to accurately evaluate the editing performance of the model, we manually construct a high-quality validation dataset, VIVID-10M-Eval, aligning with the real-world scenes (detailed in Appendix)."}, {"title": "4. VIVID Model", "content": "To validate VIVID-10M, this section outlines a versatile and interactive video local editing model. Specifically, Sec. 4.1 covers the foundational principles, Sec. 4.2 introduces the VIVID architecture, Sec. 4.3 presents the keyframe-guided interactive video editing for efficient video editing, and Sec. 4.4 discusses our multi-task joint training."}, {"title": "4.1. Preliminaries", "content": "Video editing model can be framed as a conditional diffusion model, where the model $\\epsilon_\\theta$ is trained to predict noise based on given conditional information. The optimization objective of the video editing model is defined as Eq. (1).\n$L(\\Theta) = E_{\\epsilon \\sim N(0,1)} [||\\epsilon - \\epsilon_\\theta (x_t, t, c) ||^2]$, (1)\nwhere $t \\in \\{1, ..., T\\}$ represents the number of diffusion steps, $x_t$ denotes the noised video, and c represents the conditional inputs (e.g., the caption and masks)."}, {"title": "4.2. Architecture", "content": "We introduce VIVID, a versatile and interactive video editing model, that supports adding, modifying, and deleting entities within specific region. Given a video x, VIVID generates high quality, harmonious contents within the mask sequence m, guided by the semantics of local caption embedding $T_\\theta(y)$. Using the optimization target defined in Eq. (1), we set the condition $c = (\\tilde{x}, m, T_\\theta(y))$. VIVID builds upon CogVideoX [33] to leverage its pretrained video generation capabilities. Fig. 4 highlights the trainable components, including LoRA [12] and the patch encoder. Specifically, we concatenate the mask sequence m and the masked video $\\tilde{x}$ with the noise, converting them into visual latent $z_{vision}$. Since the input dimensions of the patch embedder are different with text-to-video generation [33], it is also trained. Meanwhile, we obtain the textual latent $z_{text} = T_\\varphi(y)$ from the local caption y using a text encoder $T_\\varphi$. Finally, $z_{vision}$ and $z_{text}$ are concatenated and input to the DiT to generate the edited video."}, {"title": "4.3. Keyframe-guided Interactive Video Editing", "content": "In practical video editing scenarios, users often cannot fully express their requirements in a single attempt, leading to iterative adjustments of the local caption based on model feedback. This process requires multiple model runs to achieve satisfactory results, increasing both time and resource demands and potentially compromising user experience. To address this, we propose the Keyframe-guided Interactive Video Editing (KIVE) mechanism, shown in Fig. 5, which enables users to quickly edit keyframes using an image editing model and propagate these edits across the remaining frames. Assume that we have both an image editing model and a video editing model with comparable generative capabilities and respective inference costs $C_{im}$ and $C_{vid}$. If users need an average of N edits to reach a satisfactory result, the cost for direct video editing would be $N C_{vid}$, whereas the cost using KIVE is only $N \\cdot C_{im} + C_{vid}$. As N grows, the advantages become pronounced. To enable VIVID to support KIVE, we train it by replacing the first frame of masked video with the original video, and the first mask with the all-black frame 0. Thus, the conditional input can be represented as $\\bar{c} = (\\bar{x}, \\bar{m}, T_\\theta(y))$, where $\\bar{x} = \\{x_0\\} \\cup \\{x^i\\}_{i>0}$ and $\\bar{m} = \\{0\\} \\cup \\{m^i\\}_{i>0}$ represent the masked video and mask sequence with the first frame replaced. Additionally, selecting the last frame of an edited clip as the keyframe for the next enables VIVID to edit long videos of through the KIVE mechanism (see Appendix)."}, {"title": "4.4. Multi-Task Joint Training", "content": "To reduce training overhead and accelerate convergence, we incorporate both image and video data during training. As noted in Sec. 3.4, image data offers greater diversity and a higher proportion of high-quality samples. Given a fixed training time, leveraging this broader image dataset improves the model's generalization capability for edits. Our default configuration uses an image-to-video ratio of 10:1. At each training step, we set the batch to consist entirely of either images or videos based on this proportion, maximizing training efficiency. In addition, to support the KIVE mechanism, we randomly replace the conditional input $\\bar{c}$ in video editing with c with 50% probability during training. Recognizing that addition and modification tasks are more challenging than deletion task, since they require generating clear foreground information, we adjust the data ratio of different tasks to addition&modification:deletion=3:1."}, {"title": "5. Experiments", "content": "Our approach builds on the CogVideoX 5B model [33]. We train VIVID on VIVID-10M using LoRA [12] at a resolution of 480 \u00d7 720 for the original video frames, with a LoRA rank of 32.\nBaselines. We evaluate VIVID alongside other video editing models, VideoComposer [31] and COCOCO [40], on VIVID-10M-Eval, which comprises three editing tasks: 1) addition, 2) modification, and 3) deletion. We modify the local caption for VideoComposer to a global caption to match its training setup. Considering that existing video editing models [31, 40] do not support deletion tasks, we also include the video inpainting model ProPainter [38]."}, {"title": "A. VIVID-10M Dataset", "content": "Fig. 9 illustrates visual examples of addition&modification and deletion samples in the VIVID-10M dataset. The key differences between these two types of samples are as follows: 1) Mask Placement. In addition&modification samples, the mask overlays an existing entity in the video, while in deletion samples, the mask is positioned over a background area. 2) Local Caption Content. The local caption for addition&modification samples provides a description of the masked entity, whereas for the deletion samples, it is a fixed prompt: Remove objects and generate areas that blend with the background."}, {"title": "A.2. VIVID-10M-Eval", "content": "We provide VIVID-10M-Eval to fit the actual editing tasks. Specifically, the pipelines mentioned in Sec. 3.1 and Sec. 3.2 essentially flip the original task to generate data, i.e., remove entities from the original video to generate the input for the addition and modification tasks, and add entities for the deletion task. In addition, previous work [40] uses random masks and prompts for evaluation, which also has a gap with the user's editing in real scenes. Therefore, we manually construct the high-quality validation dataset. First, we selected 64 videos from the VIVID-10M-Video according to diversity and aesthetics. Then, for each video, we create at least one sample for each task. For each mask, we use 3 augmentation methods: 1) expand, 2) hull, 3) box, resulting 852 validation samples."}, {"title": "A.3. Foreground Vocabulary", "content": "We filter out verbs, adjectives, colors, and repeated character descriptions from RAM's [35] output labels. That is, the foreground vocabulary is the difference between RAM's vocabulary [35] and the following vocabulary:\nAdjectives. ancient, athletic, beautiful, classic, clean, clear, close-up, crowded, decorative, dark, empty, fresh, healthy, high, indoor, light, long, modern, narrow, new, old, outdoor, peaceful, quiet, rainy, remote, romantic, sharp, shiny, short, silent, single, small, smooth, soft, spicy, square, strong, stunning, sweet, tall, tiny, traditional, warm, wet, wide, wooden.\nVerbs. act, add, adjust, aid, appear, applause, approach, archery, arrest, assemble, attach, attend, auction, back, baking, balance, bend, blow, boil, bounce, build, burn, buy, call, carry, carve, catch, celebrate, cheer, climb, close, cook, cool, cover, create, crochet, crush, cry, cut, dance, decorate, deliver, dive, dribble, drift, drink, drive, drop, eat, exercise, feed, fight, fill, find, fit, float, fly, fold, freeze, fry, gather, give, glow, glue, go, graze, greet, grow, guard, guide, hang, harvest, hide, hike, hit, hold, hug, hunt, illuminate, install, jog, jump, kick, knit, laugh, launch, lay, lead, lean, learn, leave, lie, lift, load, locate, lock, look, lose, make, measure, milk, mix, move, open, pack, paint, peel, perform, pick, plant, play, plow, pour, practice, prepare, press, print, pull, punch, push, put, read, receive, reflect, relax, release, remove, repair, rescue, reveal, ride, rise, roll, rub, run, sail, scatter, see, sell, send, serve, sew, shake, shape, shear, shine, shoot, shout, show, shovel, sing, sip, sit, skate, ski, sleep, slice, slide, smell, smile, smoke, snap, snow, soak, sort, sow, speak, spill, spin, splash, split, spread, spring, sprinkle, squeeze, stab, stand, start, stare, steam, stir, stitch, stop, store, stretch, strike, stroll, study, stuff, swirl, swing, take, talk, teach, tear, tell, think, throw, tick, tie, toast, touch, tour, tow, train, trim, trip, type, unlock, use, vacuum, walk, wash, watch, wave, wear, weave, weld, widen, wipe, write, zip, kiss.\nColors. aqua, amber, beige, black, blue, bronze, brown, gold, gray, green, lilac, orange, pink, purple, red, silver, teal, violet, white, yellow.\nRepeated character descriptions. person man, person woman, person girl, person boy, man person, woman person, girl person, boy person, girl woman, boy man, woman girl, boy man."}, {"title": "A.4. Background Vocabulary", "content": "We maintain the following RAM's [35] background labels:\nsky, water, ocean, sea, river, lake, forest, mountain, desert, field, city, cityscape, city skyline, night sky, evening sky, snow, snowfield, iceberg, beach, sand, grassland, grassy, meadow, garden, park, jungle, island, cave, mountain range, valley, dune, hill, hillside, horizon, skyline, background, scenery, landscape, countryside, farmland, village, road, road trip, street, street corner, street scene, path, trail, outdoor, outcrop, rocky, coast, coastline, shore, shoreline, riverbank, river valley, mountain lake, riverbed, mountain stream, mountain pass, mountain village, mountaineer, mountain view, mountain snowy, waterfall, cascade."}, {"title": "A.5. Prompt for Local Caption Generation", "content": "The following prompt is used to generate the local caption for addition and modification samples, where tag is replaced by the entity label. (Section 3.1)\nNow you are the 'text prompt creator'. Let's think step by step, from simple to complex to describe the center object tag, your response should return three answers. Use \\n as a separator in the answerThe first answer is to describe the center object tag in two or three phrases. This answer must start with 'The video shows' and must contain tag. The second answer is to describe the center object tag in a clear and concise manner using two or three sentences. This answer must start with 'The video shows' and must contain tag. The third answer is to describe the center object tag in detail. This answer must start with 'The video shows' and must contain tag."}, {"title": "B. Training Details of VIVID", "content": "VIVID adopts v-prediction and zero SNR, following LDM's noise schedule [25]. It is trained on 7 nodes, each equipped with 8 Nvidia A100 GPUs, using a batch size of 56 for 102K steps. We employ the Adam optimizer with exponential moving average (EMA), setting the learning rate to le-3, betas to 0.9 and 0.95, weight decay to le-4, and EMA decay to 0.9999."}, {"title": "C. Temporal Consistency for 30fps Edits", "content": "Since other models are often train at low frame rates [31, 40], we downsample VIVID edits from 30fps to 7.5fps in Sec. 5.2. To demonstrate the coherence of the high frame rate videos generated by VIVID, we additionally show the TC metrics of VIVID's native 30fps editing in Tab. 6. The experimental results illustrate that VIVID achieves the best TC among all tasks. Moreover, existing methods [31, 40] for generating low frame rate videos are mainly aimed at editing longer videos under limited resources. For example, VideoComposer [31] and COCOCO [40] use 16 frames to depict about 2 seconds of video. In contrast, the KIVE mechanism enables VIVID's long video editing, see Sec. D, allowing VIVID to focus on editing smooth 30fps clips."}, {"title": "D. KIVE-based Long Video Editing", "content": "Fig. 10 shows the principle of VIVID to achieve long video editing by leveraging the Keyframe-guided Interactive Video Editing (KIVE) mechanism. Given the ith edited video clip, we use the last frame to replace the keyframe of the i + 1th input clip and propagate the edit to subsequent frames. In particular, users are free to choose the KIVE mechanism or direct video editing to edit the first clip. The Fig. 1 in the manuscript come from long video editing. More results are shown in Fig. 11, where we display the keyframe of each clip (49 frames). Experimental results show that the edits remain stable across clips and can be reasonably deformed according to the mask and content. The long video editing application further proves that the KIVE mechanism can stably propagate edits."}, {"title": "E. More Quantitative Comparisions", "content": "Fig. 12, Fig. 13, and Fig. 14 show more comparison results between VIVID and baseline approaches [31, 38, 40] on addition, modification and deletion tasks, respectively. Our approach achieves more aesthetic and semantically correct edits across all tasks. In particular, in Fig. 14, VIVID correctly inpaints the background according to the semantics of the video. For example, VIVID generates the open car door and the turning truck in the second and third examples, respectively. This demonstrates that VIVID has a better understanding of the real world."}]}