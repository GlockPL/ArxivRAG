{"title": "Transforming Precision: A Comparative Analysis of Vision Transformers, CNNs, and Traditional ML for Knee Osteoarthritis Severity Diagnosis", "authors": ["Tasnim Sakib Apon", "Md.Fahim-Ul-Islam", "Nafiz Imtiaz Rafin", "Joya Akter", "Md. Golam Rabiul Alam"], "abstract": "Knee osteoarthritis is a degenerative joint disease\nthat can cause severe pain and impairment. With increased preva-\nlence, precise diagnosis by medical imaging analytics is crucial\nfor appropriate illness management. This research investigates\na comparative analysis between traditional machine learning\ntechniques and new deep learning models for diagnosing knee\nosteoarthritis severity from X-ray pictures. This study does not\nintroduce new architectural innovations but rather illuminates\nthe robust applicability and comparative effectiveness of pre-\nexisting ViT models in a medical imaging context, specifically\nfor knee osteoarthritis severity diagnosis. The insights garnered\nfrom this comparative analysis advocate for the integration of\nadvanced ViT models in clinical diagnostic workflows, potentially\nrevolutionizing the precision and reliability of knee osteoarthritis\nassessments. This study does not introduce new architectural\ninnovations but rather illuminates the robust applicability and\ncomparative effectiveness of pre-existing ViT models in a medical\nimaging context, specifically for knee osteoarthritis severity\ndiagnosis. The insights garnered from this comparative analysis\nadvocate for the integration of advanced ViT models in clinical\ndiagnostic workflows, potentially revolutionizing the precision\nand reliability of knee osteoarthritis assessments. The study\nutilizes an osteoarthritis dataset from the Osteoarthritis Initiative\n(OAI) comprising images with 5 severity categories and uneven\nclass distribution. While classic machine learning models like\nGaussianNB and KNN struggle in feature extraction, Convo-\nlutional Neural Networks such as Inception-V3 and VGG-19\nachieve better accuracy between 55-65% by learning hierarchical\nvisual patterns. However, Vision Transformer architectures like\nDa-VIT, GCViT and MaxViT emerge as indisputable champions,\ndisplaying 66.14% accuracy, 0.703 precision, 0.614 recall, and\nAUC exceeding 0.835 thanks to self-attention processes. This\nanalysis strongly promotes the deployment of sophisticated vision\ntransformers over CNNs and traditional ML for increased\nprecision in knee osteoarthritis diagnosis using X-ray picture\ncategorization.", "sections": [{"title": "I. INTRODUCTION", "content": "Knee osteoarthritis (OA) is one of the most prevalent and\ndisabling chronic joint diseases, affecting over 250 million\npeople worldwide. It involves the degradation of articular knee\ncartilage and underlying bone, resulting in joint pain, stiff-\nness, and impaired mobility. Osteoarthritis, a neurologically\nincurable disease that progresses and affects roughly 15A\nsignificant technological barrier to the advancement of knee\nOA prevention and treatment is the lack of effective imaging\ncharacteristics to detect OA recurrence. [12]. The primary\ndiagnostic method employed in clinical research appears to be\nmedical imaging. By calculating the distance between the tibia\nand femur bones, the cartilage can be indirectly determined\nusing standard X-ray imaging. Thus, cartilage cannot be ac-\ncurately assessed with an X-ray. [20]. Due to this, a substantial\namount of research has been done using various conventional\nmachine-learning techniques to predict early osteoarthritis.\nTwo examples of computer-aided techniques that have been\nintroduced are dynamic contours and B-splines. [21]. Poor\nprecision and dependability, as well as an inability to recognize\nminute cartilage changes, are some of these technologies'\ndrawbacks. These methods may easily overlook the minute\nchanges that take place in the early stages of knee osteoarthritis\nbecause osteoarthritis is a chronic condition that progresses\nslowly, on average by 2% per year. Therefore, it is still\ndifficult to create a reliable quantitative technique that is valid,\nreproducible, and change-sensitive. The outdated method may\nidentify a cartilage thickness growth of only 2-3 percent per\nyear. [13]. In osteoarthritis medical studies, body mass index,\nage, and gender are routinely used to identify people who\nare more likely to develop knee osteoarthritis. [15]. Since the\neffects and linkages of these markers are not fully understood,"}, {"title": "II. RELATED WORKS", "content": "In persons over the age of 60, Osteoarthritis disease is a\nchronic cause of impairment. Osteoarthritis of the knee is a\ncommon condition marked by cartilage deterioration. A study\naims at establishing a new automatic segmentation method for\nMRI-based examination of human knee cartilage thickness.\nThe exam consisted of a double echo steady state (DESS)\npattern, which compares cartilage and soft tissues, such as the\nsynovial fluid, utilizing a 3T scanner and a knee coil. The\napproach was constructed using MRI 3-D scans in which an\nautonomous segmentation method divided the bone-cartilage\ninteraction for the femur and tibia, yielding a descriptive area\nof the interface. The MR images are first resampled in the\nvicinity of the surface of the bone. Second, the cartilage is\ndistinguished as a bright and uniform tissue utilizing texture-\nanalysis techniques enhanced by filtering. The exterior limit of\nthe cartilage can be detected using this procedure of omitting\nsoft tissues. Third, a Bayesian decision criterion-based tech-\nnology allows for the automatic separation of cartilage and\nsynovial fluid. Finally, the developed technology was used to\nassess the cartilage thickness and variations in thickness for\nan individual across sessions.\nThe slow degradation of articular cartilage is a symptom\nof osteoarthritis (OA). In a research method, the cartilage\nis segmented using a pixel-based segmentation process. The\nstudy uses MATLAB R2013a to analyze 15 images, including\nregular and OA-affected ones. The ambient noise is minimized\nusing a crude mask, and GLCM feature extraction is applied\nfor segmentation.The accuracy of the classification of the\nindividual into regular and OA-affected was determined to be\n86.66 percent.\nSubsequently, learning-based Computer-Aided Diagnosis\n(CAD) added various features can be potential for enhancing\nknee OA diagnostic performance. Learning discriminative fea-\ntures, on the other hand, can be difficult, when working with\ncomplicated data like X-ray pictures, which are commonly\nused to diagnose knee OA. One work presents a Discriminative\nRegularized Auto Encoder (DRAE) that enables to discover\nsimultaneously relevant and discriminative features that cheval\nclassification accuracy. Very particularly, the usual Auto-\nEncoder learning objective is paired with a penalty concept\ncalled discriminative loss. This supplementary phrase seeks\nto drive discriminatory features into the learned model. The\npurpose of this study was to differentiate among definitive\nabsence (KL grade 0) and premature OA presence (KL-G1 and\nKL-G2). The study evaluated five learning algorithms: SVM-\nRBF, LDA, SMC, and KNN, using 3900 knee joint pictures.\nThe SVM-RBF classifier outperformed the others with a max-\nvoting efficiency of 82.53 percent, F-measure efficacy of 83.48\npercent, and 88.23 percent precision and 79.22 percent recall,\ndemonstrating superior performance.\nA study using T2Map MRI and Density-weighted Protons\nsequence identified osteoarthritis in knee articular structures.\nHowever, these diagnostics have poor responsiveness. A ran-\ndom forest approach was used to classify osteoarthritis into\nthree severity classifications: A, B, and C. The model accu-\nrately predicted osteoarthritis data by 86.96 percent. Results\nfrom 33 patients at Indonesia's Cipto Mangunkusumo Na-\ntional Hospital showed the multiclass classification strategy\nwas effective.Following the determination of the classification\nmethod, the model classifier is trained utilizing training data.\nThe confusion matrix is used to evaluate and validate this\nclassifier model.\nThe study introduces a machine learning-based diagnostics\nmethod for knee osteoarthritis severity, classifying patients\ninto distinct stages based on cartilage thickness. The method\noutperforms conventional methods with 97% accuracy.\nA new method for detecting radiographic osteoarthritis in\nknee X-ray pictures uses Kellgren-Lawrence classification\nratings. The system uses subjectively graded X-rays to identify\ndifferent phases of OA intensity. Over 95% of mild OA\npatients were correctly diagnosed, but due to the significant\ndifficulties that accompanied KL grades 4 (severe Osteoarthri-\ntis) and 5 (knee replacement), these stages were left from this\nresearch. Furthermore, this research was carried out as part of a\nlongitudinal aging experiment that will allow imaging findings\nto be compared not only to medical Osteoarthritis aspects\nsuch as pain, but also to physiological measurements related"}, {"title": "III. METHODOLOGY", "content": "Traditional approaches have limitations, necessitating a\ncomparison of traditional machine learning with deep learning,\nnotably vision transformers. The study recommends vision\ntransformers (Da-VIT, GCViT, and MaxViT) for accurate OA\nseverity assessment after analyzing 1526 knee radiographs as\nshown in Figure 1. The paper examines past methodologies,\ndescribes the process, and assesses the suggested strategy\nusing criteria such as accuracy and precision.\n1) Data Acquisition: According to radiographic data, more\nthan half of American people over 65 had osteoarthritis disease\naround one or both joints in 2000. The osteoarthritis syndrome\nwill be more common in 20 percent or more of US citizens by\n2030, which would have a significant socioeconomic impact\n[16], [17]. By the year 2050, Osteoarthritis and other reactive\narthritis illnesses are predicted to affect at least 130 million\npeople worldwide [14]. We have trained our model with \u201cKnee\nformulated in 2018. The Osteoarthritis Initiative (OAI), a multi-\ncenter, longitudinal, prospective observational research of knee\nosteoarthritis (OA) with the goal of identifying biomarkers for\nOA development and progression, provided the knee X-ray\nimages used in our dataset.\n2) Data Description: Degeneration of the articular carti-\nlage, a flexible, slick substance that typically shields bones\nfrom joint friction and impact, is what constitutes knee os-\nteoarthritis. The disorder can also damage neighboring soft\ntissues and results in alterations to the bone that lies beneath\nthe cartilage. By far the most prevalent form of arthritis\nto result in knee discomfort is knee osteoarthritis, which\nis frequently referred to as just knee arthritis. Rheumatoid\narthritis, reactive arthritis, and many other uncommon kinds\nof arthritis can also hurt the knees. Numerous individuals who\nhave computed tomography corroboration of Osteoarthritis do\nnot demonstrate any symptoms, and the level of radiological\nalteration differs from person to person, making it challenging\nto identify Osteoarthritis in its initial stages. Measurement of\nhyaline cartilage alteration, that is also used to determine the\nfeasibility of clinical therapies, is the primary technique for\ndetermining the structural development of Osteoarthritis. In\nthis dataset, there are 4796 people whose ages range from 45\nto 79. Additionally, there are 8260 knee joints in 4130 X-ray\nphotos. This dataset contains 9786 images. The dataset was"}, {"title": "A. Data Acquisition and Preparation"}, {"title": "B. Model Specification", "content": "A wide range of machine learning techniques for image cat-\negorization are included in the model specifications. To make\nuse of classical techniques, five classic machine learning mod-\nels-GaussianNB, AdaBoost, K Neighbors, Gradient Boost-\ning, and Random Forest are used. Six Convolutional Neural\nNetworks (CNNs) that illustrate the application of deep learn-\ning techniques are also included in the model repertoire. These\ninclude Inception-V3, MobileNet, VGG-19, DenseNet-121,\nXception, and VGG-16. Moreover, the model specifications\npresent the incorporation of the most advanced image classi-\nfication transformers, namely Da-VIT, GCViT, and MaxViT,\nwhich are prime examples of the utilization of state-of-the-art vision transformers to improve efficiency and precision\nfor classifying diseased knee images. The GCViT improves\npicture classification by using global context self-attention\nefficiently, reducing inductive bias, and obtaining cutting-edge\nresults without pre-training. The DaViT here resides in its in-\nnovative use of dual self-attention mechanisms\u2014spatial tokens\nand channel tokens effectively capturing both global context\nand local interactions, resulting in state-of-the-art performance\nwith efficient calculations.Besiedes, MaxViT overcomes the\nscalability limitation of self-attention in vision transformers\nby introducing an efficient and scalable multi-axis attention"}, {"title": "3) Image Preprocesing"}, {"title": "C. Models of Vision Transformers", "content": "Configurations of Da-VIT, GCVIT, and MaxViT:\n\u2022 Da-VIT: The configuration has 12 layers, a hidden size\nof 768, 12 attention heads, and a patch size of 16. The\ndecision was driven by the dual attention mechanism's\nability to effectively capture subtle characteristics in knee\nX-ray images across both spatial and channel dimensions.\n\u2022 GCVIT: The model has 24 layers, a dimension of 1024,\n16 attention heads, and a global context window size\nof 14. The global context-enhancing self-attention mech-\nanism developed by GCViT is well-suited for medical\ndatasets such as OAI, which consist of diverse visual\ncharacteristics and artifacts.\n\u2022 MaxViT: The model incorporates a multi-axis attention\nmechanism, with 32 transformer blocks and a model\nwidth of 512. MaxViT was selected for its efficiency in\nprocessing high-resolution pictures and maintaining both\nlocal and global image features, making it perfect for\nextensive knee image studies.\n\u2022 Learning Rate: A learning rate of 1e-4 was established\nfor all models, based on grid search optimization findings\nfor optimal convergence.\n\u2022 Batch Size: Chosen as 16 to balance computational\nefficiency and model performance within GPU memory\nlimits.\n\u2022 Optimizer: AdamW optimizer was utilized with a weight\ndecay of 0.01 for its effectiveness in sparse gradient\nmanagement.\nCNN Architectures: Inception-V3 and VGG-19 were con-\nfigured with standard parameters, chosen for their demon-\nstrated performance in image classification, giving a bench-\nmark for ViT's evaluation.\nTraditional ML Models: Models like Random Forest\nand Gradient Boosting were utilized with default scikit-learn\nparameters to provide a complete comparison across machine\nlearning approaches, showcasing the gains enabled by deep\nlearning in medical imaging."}, {"title": "Hyperparameters"}, {"title": "D. Comparative Analysis using CNNs and Traditional ML", "content": "The selection of deep learning architectures was informed\nby their demonstrated effectiveness in various image identifi-\ncation challenges. Each model's particular approach to image\nprocessing is vital for negotiating the complexity inherent in\nmedical images, which are characterized by small variances\nand significant variability. The combination of classic ML"}, {"title": "E. Rationale Behind Model Selection"}, {"title": "IV. RESULT ANALYSIS", "content": "The full performance evaluation in Table III provides a\ndeep understanding of several picture classification models.\nTraditional machine learning models such as GaussianNB,\nAdaBoost, K Neighbors, Gradient Boosting, and Random\nForest perform poorly in terms of accuracy, with GaussianNB\ntrailing at 21.00%, highlighting challenges in capturing de-\ntailed visual data. Convolutional Neural Networks (CNNs)\nsuch as Inception-V3, MobileNet, VGG-19, DenseNet-121,\nXception, and VGG-16, on the other hand, show significant\nincreases, with accuracy rates ranging from 55.81% to 65.25%\nwhen exploiting deep hierarchical features.\nThe standout performances, however, are the cutting-\nedge Vision Transformer (ViT) models Da-VIT, GCVIT,\nand MaxViT. Da-VIT outperforms CNNs like VGG-19 and\nDenseNet-121 with an astounding accuracy of 65.35%. GCVIT\nand MaxViT significantly improve performance with accura-\ncies of 66.14%, demonstrating the constancy of ViT models in\noutperforming classical and CNN models.ViT models consis-\ntently outperform other models in terms of precision, recall,\nand F-1. Da-VIT, for example, has a precision of 0.706 and a\nrecall of 0.606, demonstrating its ability to correctly classify\npositive cases. Precision-recall balances are demonstrated by\nGCViT and MaxViT, proving their powerful categorization\ncapabilities.\nThe superiority of the ViT models is further highlighted\nby the AUC statistic, where Da-VIT, GCViT, and MaxViT\nroutinely score above 0.835. This demonstrates their ability\nto discern across different classes and cements their status\nas leaders in picture classification challenges. Finally, the nu-\nmerical analysis demonstrates that Vision Transformer models\noutperform classic machine learning models and CNNs in\nterms of not only accuracy but also precision, recall, and AUC\nvalues."}, {"title": "V. CONCLUSION", "content": "This investigation of detecting knee osteoarthritis severity\nfrom X-ray pictures shows compelling evidence for vision\ntransformer models over traditional machine learning and\nconvolutional neural networks. Utilizing a multi-center dataset\nof 1526 knee radiographs categorized into 5 progressive dis-\nse categories, advanced ViT architectures including DaVIT,\nGCViT and MaxViT achieve exceptional 66% grading accu-\nracy, 0.7 precision, 0.6 recall and AUC beyond 0.83. In com-\nparison, conventional models like as GaussianNB obtain only\n21% accuracy, hampered by problems in informative visual\nfeature extraction from scans. While CNNs like Inception-\nV3 achieve advances exceeding 65% accuracy via hierarchical\nfeature learning, their performance lags behind the latest vision\ntransformers tuned for both local and global context modeling.\nWe strongly encourage priority application of ViT models in\nreal-world automated systems for robust knee osteoarthritis\nevaluation to guide suitable clinical decisions. Their advanced\nself-attention systems can record small visual patterns to\nadvance diagnostics and monitoring. Further enhancements\nto transformer model generality, efficiency and clinical inte-\ngration can provide a pathway for AI-assisted severity rating\nthat improves conservative and surgical care of this crippling,\nchronic illness affecting over 250 million patients worldwide.\nIntelligent knee OA image categorization with transformers\nhas the potential to radically improve orthopedic therapy. Table\nIII depicts the evaluation scores of different models."}]}