{"title": "Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation", "authors": ["Xue Liu", "Senyu Li", "Zipeng Sun", "Jiayi Wang", "Pontus Stenetorp", "Siva Reddy", "David Ifeoluwa Adelani"], "abstract": "Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence tasks often train models to directly generate the target output. Recent work has shown that guiding models with intermediate steps-such as keywords, outlines, or reasoning chains-can significantly improve performance, coherence, and interpretability. However, these methods often depend on predefined intermediate formats and annotated data, limiting their scalability and generalizability. In this work, we introduce a task-agnostic framework that enables models to generate intermediate \"warmup\" sequences. These warmup sequences, serving as an initial state for subsequent generation, are optimized to enhance the probability of generating the target sequence without relying on external supervision or human-designed structures. Drawing inspiration from reinforcement learning principles, our method iteratively refines these intermediate steps to maximize their contribution to the final output, similar to reward-driven optimization in reinforcement learning with human feedback. Experimental results across tasks such as translation, summarization, and multi-choice question answering for logical reasoning show that our approach outperforms traditional SFT methods, and offers a scalable and flexible solution for sequence-to-sequence tasks.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large-scale pre-trained language models (LLMs), such as T5 (Raffel et al., 2019), GPT (Brown et al., 2020), and LLaMA (Touvron et al., 2023), have significantly improved performance on both predictive tasks (e.g., multi-choice question answering) and generative tasks (e.g., machine translation and summarization). These models have demonstrated exceptional capabilities in generating coherent and contextually relevant outputs by modelling dependencies across long sequences of data. Despite their successes, traditional supervised fine-tuning (SFT) methods for such models often focus on directly generating the target output without leveraging the benefits of intermediate steps or initial guidance (Sutskever et al., 2014).\nResearch has shown that guiding models with intermediate steps, such as outlines, keywords, or reasoning chains, can significantly improve performance, coherence, and interpretability across tasks (Wang et al., 2022; Creswell and Shanahan, 2022). For instance, hierarchical frameworks for tasks such as story generation (Fan et al., 2019) and summarization (Amplayo et al., 2020) often first generate high-level structures, such as outlines or reasoning steps, before producing detailed outputs. These approaches highlight the utility of intermediate guidance in organizing complex tasks. Similarly, chain-of-thought (COT) (Wei et al., 2023) reasoning for predictive tasks extends this concept by demonstrating the value of logical steps by decomposing complex predictive tasks into explicit logical steps, demonstrating how structured reasoning between inputs and outputs can improve model performance. However, these approaches heavily rely on predefined intermediate formats and annotated data, which are costly to create due to human annotation efforts and often task-specific, thus limiting their scalability and adaptability to broader applications.\nIn this work, we address these limitations by introducing a framework that enables models to generate an initial state, which we refer to as \"warmup sequence.\u201d These warmup sequences act as preparatory steps, priming the model for the main generation task. Drawing inspiration from reinforcement learning (RL) principles, our method treats these steps as actions within a reward-driven framework, optimizing them to maximize their utility in improving the quality and coherence of the final target output. Importantly, this process operates without relying on predefined formats or external annotations, making it adaptable to a wide range of tasks and model architectures. This approach eliminates dependence on annotated data for intermediate steps, achieves generalization across tasks, and unifies the optimization of intermediate and final outputs, leading to improved final performance of the models.\nThrough experiments, we demonstrate that our method improves output quality across translation, summarization, and multi-choice question answering for logical reasoning, and is compatible with various model architectures, including encoder-decoder models like T5 (Raffel et al., 2020) and mT5 (Xue et al., 2021), as well as decoder-only models like Llama (Touvron et al., 2023). In addition, our method is simple to implement, requiring only about 10 additional lines of codes, without modifications to existing model architectures or reliance on task-specific annotations, and is grounded in a solid theoretical framework. These contributions establish an approach where models can autonomously discover and leverage a helpful initial state that increases the probability of the target sequence across diverse tasks to enhance the quality of the final generation."}, {"title": "2 Related work", "content": "Guiding generative models with intermediate steps has been widely explored to enhance coherence, interpretability, and task performance. Existing approaches can be categorized into explicit human-readable intermediate steps, and structured weakly supervised intermediate steps.\nHuman readable intermediate steps Plan-and-Write (Yao et al., 2019) introduces storyline-based planning, where a model first generates a structured sequence of key events before expanding them into a full story, improving coherence and creativity.\nAmplayo et al. (2020) employ content planning, explicitly modelling aspect and sentiment distributions to guide summary generation, thereby enhancing readability and informativeness.\nSimilarly, Wolfson et al. (2022) propose Question Decomposition Meaning Representations (QDMR), which break down complex questions into sequences of reasoning steps. These decompositions serve as an explicit intermediate representation, improving interpretability and guiding Text-to-SQL parsing by systematically mapping natural language queries to SQL. Baziotis et al. (2019) introduce a sequence-to-sequence-to-sequence model (SEQ3), where the intermediate step is a compressed version of the input sentence, explicitly represented in natural language.\nStructured intermediate steps Some models introduce structured but weakly supervised intermediate steps, where the intermediate representations are partially interpretable but not explicitly labelled during training. Cheng et al. (2017) generate predicate-argument structures, which serve as an intermediate step in semantic parsing. Unlike explicit intermediate representations, these structures are learned through optimization-based search rather than direct supervision. Similarly, Jambor and Bahdanau (2022) propose Label Aligned Graphs (LAGr), where models predict node and edge labels to construct structured meaning representations aligned with input text, improving systematic generalization in semantic parsing. These representations enhance compositional generalization but still depend on predefined structural mappings. Herzig et al. (2021) introduce intermediate representations that transform meaning representations (e.g., SPARQL or SQL queries) into structured forms that improve compositional generalization while maintaining reversibility. While these methods balance interpretability and generalization, they still rely on task-specific constraints rather than fully flexible intermediate representations.\nReinforcement learning in NLP RL has also been applied in NLP to optimize model generation beyond traditional supervised learning for text summarization (Paulus et al., 2018), dialogue generation (Li et al., 2016) and machine translation (Wu et al., 2018). More recently, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) has been instrumental in aligning large-scale language models with human preferences, demonstrating the effectiveness of RL-based fine-"}, {"title": "3 Formulation and Derivation", "content": "We reformulate the process of text generation by assuming that given a specific input x and the target text Ytarget, there exists an intermediate sequence, or the initial state Cinit preceding Ytarget, where length(Cinit) \u2265 0. To be more specific, Cinit = {C1, C2,...,Ck} is a sequence of tokens, where k \u2265 0. The intermediate sequence Cinit serves as a latent variable that conditions the generation of Ytarget. When k = 0, Cinit is an empty sequence, reducing the framework to the traditional sequence-to-sequence paradigm:\n$P(Y_{target}|x) = \\sum_{C_{init}} P(C_{init}, Y_{target}|x)$\nUsing the chain rule, this can be decomposed as:\n$P(Y_{target}|x) = \\sum_{C_{init}} P(y_{target}|C_{init}, x)P(C_{init}|x)$\nWhich can be rewritten in the form:\n$P(y_{target}| x) = E_{C_{init} \\sim P(C_{init}|x)} [P(y_{target}|C_{init}, X)]$\nOur objective is to maximize the probability of the target sequence Ytarget given the input x. Traditionally, the loss for maximizing P(ytarget|x) is:\n$L_{ytarget} = -log(P(ytarget|x))$\nWhich is equivalent to:\n$L_{ytarget}= -log(E_{C_{init} \\sim P(C_{init}|x)} [P(Ytarget|C_{init}, x)])\nwhere Cinit represents any possible initial state conditioning Ytarget. This expectation implies maximizing P(ytarget) across all initial states, weighted by their probability P(Cinit|x)."}, {"title": "Reward-Based Initial State Optimization", "content": "Since we lack labels for Cinit, we train the model to generate Cinit using reward-based optimization. A good initial state Cinit increases the probability of Ytarget, while a poor Cinit reduces it. The reward R(Cinit) quantifies the quality of Cinit in terms of its contribution to generating Ytarget given x:\n$R(Cinit) = P(ytarget|Cinit, X)$\nUnder a RL framework, we aim to maximize:\n$E_{C_{init} \\sim P(C_{init} |x)} [R(Cinit)]$\nwhich is equivalent to:\n$E_{C_{init} \\sim P(C_{init} |x)} [P(ytarget|Cinit, x)]$\nThe loss for training Cinit generation is defined as the negative log of the expected reward:\n$LCimit = -log (E_{C_{init} \\sim P(C_{init} |x)} [R(Cinit)])= -log (E_{C_{init} \\sim P(C_{init}|x)} [P(Ytarget|Cinit, x)])$\nAs we can observe, this formulation aligns the optimization of Ytarget and Cinit under the same loss function.\n$Lcinit = Lytarget$\nDirectly minimizing Lcinit or Lytarget is computationally infeasible due to numerical underflow for long sequences. Instead, we minimize the expected cross-entropy loss based on Jensen's inequality:"}, {"title": "4 Warmup Generations Approach", "content": "A general overview of our method is illustrated in Figure 2. Unlike traditional SFT methods, where the loss is computed solely based on the input, our approach introduces an intermediate generation step. After receiving the input, the model first generates n warmup sequences. The final loss is then computed as the average of n individual losses,\n$L_{final} = E_{C_{init} \\sim P(C_{init}|x)}[-log(P(y_{target}|C_{init}, x))] < E_{C_{init} \\sim P(C_{init}|x)} [-log(P(y_{target}|C_{init}, x))]$\nMinimizing the expected cross-entropy loss indirectly minimizes an upper bound on -log(P(ytarget|x)), bringing us closer to maximizing P(Ytarget|x).\nTo approximate the expected value, we use Monte Carlo sampling with n samples of Cinit:\n$L_{final} = E_{C_{init} \\sim P(C_{init}|x)}[-log(P(y_{target}|C_{init}, x))] \\approx \\frac{1}{n} \\sum_{i=1}^{n} -log(P(Y_{target}| C_{init, x}))$\nThus, minimizing the expected cross-entropy loss over sampled contexts is an effective approach to optimize text generation tasks."}, {"title": "4.1 Implementation for Encoder-Decoder Models", "content": "Encoder-decoder structured models have a clear separation between input and output. For this type of model, the input is processed through the encoder, and then n Cinit are generated using beam search with sampling. Each generated Cinit is followed by a separator and fed into the decoder. Subsequently, the cross-entropy loss of Ytarget is calculated n times, conditioned on the input and each of the n generated Cinit. Finally, the average of these n losses is taken as the final loss.\nThe inference process follows the same logic shown in Figure Figure 2, given an input sequence, the model first generates a warmup sequence. This sequence is then concatenated with a separator and fed back into the beginning of the decoder. The model then generates the target sequence conditioning on both the original input and the generated warmup sequence. The final output consists of the tokens generated after the concatenated separator."}, {"title": "4.2 Implementation for Decoder-Only Models", "content": "Similar to encoder-decoder models, the input is first fed into the model, and n Cinit are sampled using beam search. The input is then concatenated with n Cinit sequences, followed by a separator, and fed back into the model. The final loss is the average cross-entropy loss of Ytarget conditioned on the n Cinit sequences and the input.\nDuring inference, similar to encoder-decoder models, the warmup sequence is first generated and then appended to the input sequence, followed by a separator. The combined sequence is then fed back into the model to generate the target sequence. The final output consists of the tokens produced after the concatenated separator."}, {"title": "4.3 Rationale Behind Using a Separator Between Cinit and Ytarget", "content": "The inclusion of separators helps the model to distinguish the boundary between Cinit and Ytarget, preventing Ytarget from being treated as a continuation of Cinit. This enhances both the stability and efficiency of training. Since the separators are deterministically appended to the end of each Cinit, the probability distributions of Cinit and Cinit"}, {"title": "5 Experiments", "content": "In this section, we present the tasks and corresponding datasets used, the models selected for the experiments and the results obtained."}, {"title": "5.1 Tasks and Datasets", "content": "We evaluated our approach on three datasets spanning three tasks: FLORES (Team et al., 2022) for testing, WMT for training for the translation task; LogiQA2 (Liu et al., 2023) for logical reasoning multi-choice QA; and XSum (Narayan et al., 2018) for summarization; Specifically, we used WMT19 datasets (Barrault et al., 2019) for the fine-tuning of de-en (en-de), ru-en (en-ru), and zh-en (en-zh) and the fr-en (en-fr) data from the WMT14 dataset (Bojar et al., 2014)."}, {"title": "5.2 Models", "content": "We used T5-base (223M) and T5-large (738M) for summarization, T5-base, T5-large, and Llama-3.2-1B (1.24B) for multiple-choice logical reasoning, and mT5-base (582M) and mT5-large (1.23B) for translation. These models, covering both encoder-decoder and decoder-only architectures, serve as well-established benchmarks in their respective categories and are widely recognized for their effectiveness."}, {"title": "5.3 Metrics", "content": "We used the BLEU score (Papineni et al., 2002), COMET score 2 (Rei et al., 2020), and ChrF++ score (Popovi\u0107, 2015) for translation. For logical reasoning multiple-choice, we used macro F1 and accuracy, and for summarization, we employed"}, {"title": "5.4 Experimental Settings", "content": "For fine-tuning all tasks, we used a learning rate of 2e-5, with the warmup sequence's maximum sampled length capped at 8 tokens. Models were trained for 10 epochs. Due to computational constraints, we randomly selected 50,000 samples from the training set for fine-tuning in translation and summarization tasks. During fine-tuning, warmup sequences were generated using a beam size of 4, with 4 warmup sequences sampled per training sample for each loss calculation.\nFor each task, we selected the checkpoint that achieved the highest metric score on the validation set and reported its performance on the test set. Specifically, for translation, checkpoints were selected based on the COMET score; for summarization, based on BERTScore; and for logical reasoning multiple-choice, based on the macro F1 score. For LogiQA2, each model was fine-tuned 3 times with different random seeds, and we report the average performance of the selected checkpoints."}, {"title": "5.5 Results and Discussions", "content": "We put our experiment results in Table 1, 2, and 3.\nWarmup generations consistently enhance performance across tasks Across all three tasks, models utilizing warmup generations outperform those employing traditional SFT methods. The most significant gains are observed in translation tasks, where mT5-base achieves an average improvement of 1.57 BLEU, 1.32 COMET, and 1.60 ChrF++ scores across 8 language pairs. For multiple-choice logical reasoning, the T5-base model trained with warmup generations achieves 0.84 higher macro F1 and 0.87 higher accuracy compared to models using traditional SFT. A similar trend is observed in summarization, where the T5-base model with warmup generations yields gains of 0.45 ROUGE-1, 0.32 R2, 0.36 RL, and 0.1 BERTScore.\nPerformance gains are robust to increases in model size When scaling the models from base to large, we observe similar or even greater performance gains. In translation, mT5-large exhibits a greater average improvement than mT5-base, with a higher BLEU gain of 1.69, a COMET increase"}, {"title": "5.6 Ablation Studies", "content": "5.6.1 Number of Samples\nTo assess the impact of the number of sampled warmup sequences during training, we analyze both training loss trends and test set performance across different sample numbers.\nAs shown in Figure 3, increasing the number of sampled warmup sequences generally accelerates convergence and reduces final training loss. However, the differences between 4 and 6 sequences for LogiQA2, and 4, 6, and 8 sequences for de-en translation are relatively small, suggesting that beyond a certain threshold, additional samples do not significantly reduce training loss further.\nThe results for LogiQA2 in Table 6 indicate that increasing the number of samples does not lead to strictly monotonic improvements. The default"}, {"title": "5.7 Qualitative Analysis", "content": "We performed a qualitative analysis of the translation task to investigate the role of warmup sequences. As the results shown in Table 4, we found that these warmup sequences can be primarily categorized into two types:\n\u2022 Direct Core Phrases: These warmup sequences can be directly identified in both the labels and the generated outputs.\n\u2022 Similar Phrases: Expressions that are semantically similar to important components in the labels and outputs.\nFor example, \u201ca British traveller in\u201d and \u201ca series of events that\" can be directly found in both the output and ground-truth labels. This indicates that for certain scenarios, initial states function as core-information extractors, guiding the model to generate outputs focusing on these core concepts.\nMeanwhile, in other cases, the warmup sequences are more semantically related rather than exact phrase matches. For instance, \u201cwhen they are in danger\u201d is semantically related to \"they perceive a threat\" in the label, and \"Please contact us directly\" aligns with \"book by phone directly with the airline.\" In such scenarios, the initial states serve as semantic guides, enabling the model to generate outputs that capture the intended meaning without relying on exact phrase matching.\nTo further evaluate this dual role, we calculated the overlapping rate of the words in the initial states that also appear in the labels in Table 5. As we can see, \"X-eng\u201d all achieve a overlapping rate over 40%, with \"de-en\" reaching 56.57% at the most. On \u201cEng-X\u201d, the overlapping rate also reach at least 32.71% on \"en-de\", and get to the highest on \"en-zh\" with the rate of 63.53%. This means that generally, over 40% of the words in the warmup sequence could be found in the ground-truth, indicating a strong alignment between the initial states and the ground-truth data. By acting as both direct extractors and semantic interpreters, the initial states ensure the generated outputs remain closely aligned with the intended semantics and structure of the target language."}, {"title": "6 Conclusions", "content": "In this work, we introduced a task-agnostic framework with theoretical proof and derivation, for improving sequence-to-sequence learning through warmup generations, where models learn to generate intermediate sequences to enhance final output quality. Unlike traditional approaches, our method learns intermediate steps in an unsupervised manner, improving performance across diverse tasks without requiring task-specific annotations. Experiments demonstrate that warmup sequences consistently benefit both encoder-decoder and decoder-only models across different sizes. Analysis reveals that warmup sequences aid generation by extracting key phrases and providing semantically related guidance, resulting in more fluent and contextually accurate outputs. Additionally, increasing the number of sampled warmup sequences accelerates convergence and enhances test-time performance, though gains diminish beyond a certain threshold. However, the performance gains vary across tasks and architectures, highlighting the need for further investigation into how different task types and model structures influence warmup effectiveness. Overall, by introducing and demonstrating the effectiveness of warmup sequences across multiple seq2seq tasks, this work lays the groundwork for further research into leveraging intermediate generations to enhance model training and generation."}, {"title": "Limitations", "content": "While our proposed framework demonstrates improvements across various tasks, there are several limitations to address. The first is increased training time. The framework relies on sampling multiple initial states during training, introducing computational overhead compared to traditional supervised fine-tuning methods. This can make training more resource-intensive, particularly for large-scale datasets or deployment in constrained environments. Future work could explore more efficient sampling strategies or adaptive selection methods to mitigate this cost. Another limitation is that warmup sequences primarily enhance the model's lexical-level understanding rather than deeper reasoning or structural-level improvements. As shown in our experiments, warmup generation aids the model in selecting key phrases and improving word choice, but it does not explicitly introduce or infer new knowledge beyond what is present in the input. Future research could explore how warmup sequences might be adapted to facilitate higher-level abstraction or knowledge augmentation, potentially bridging gaps in implicit reasoning. Finally, our framework has not been tested on decoder-only models for generative tasks. While experiments on LogiQA2 demonstrate improvements for decoder-only architectures, the application of warmup sequences to open-ended text generation (e.g., summarization or translation) in decoder-only models remains unexplored. This poses potential challenges, as decoder-only models lack explicit input-output alignments found in sequence-to-sequence tasks, making it unclear whether warmup sequences would be equally effective. Investigating warmup generation within causal language models is an important direction for future work."}, {"title": "A Selection of Seperator", "content": "The separator token for T5 and mT5 was set to \" || \", as this symbol is rarely used in natural text, making it an ideal choice for separating different parts of the sequence."}, {"title": "B Warmup Sequence for Summarization and Logical Reasoning", "content": "The warmup sequences for summarization follow a similar pattern to those in translation, predominantly consisting of either Direct Core Phrases or Similar Phrases. For logical reasoning, the warmup sequence is identical to the target sequence, representing only the final letter choice that indicates the predicted answer."}]}