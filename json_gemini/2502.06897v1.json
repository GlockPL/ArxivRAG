{"title": "PyPotteryInk: One-Step Diffusion Model for Sketch to Publication-Ready Archaeological Drawings", "authors": ["Lorenzo Cardarelli"], "abstract": "Archaeological pottery documentation traditionally requires a time-consuming manual process of converting pencil sketches into publication-ready inked drawings. I present PyPotteryInk, an open-source automated pipeline that transforms archaeological pottery sketches into standardised publication-ready drawings using a one-step diffusion model. Built on a modified img2img-turbo architecture, the system processes drawings in a single forward pass while preserving crucial morphological details and maintaining archaeologic documentation standards and analytical value. The model employs an efficient patch-based approach with dynamic overlap, enabling high-resolution output regardless of input drawing size. I demonstrate the effectiveness of the approach on a dataset of Italian protohistoric pottery drawings, where it successfully captures both fine details like decorative patterns and structural elements like vessel profiles or handling elements. Expert evaluation confirms that the generated drawings meet publication standards while significantly reducing processing time from hours to seconds per drawing. The model can be fine-tuned to adapt to different archaeological contexts with minimal training data, making it versatile across various pottery documentation styles. The pre-trained models, the Python library and comprehensive documentation are provided to facilitate adoption within the archaeological research community.", "sections": [{"title": "Introduction", "content": "Archaeological ceramics are a valuable source of information for reconstructing the customs, exchanges and social relationships of ancient populations, as well as for dating archaeological contexts (Sinopoli 1991; Peroni 1994; Steiner and Allason-Jones 2005; Vidale 2007; Orton and Hughes 2013; Hunt 2016). However, in order to turn a ceramic fragment into a rich source of scientific information, a long process of study and elaboration is required: once recovered in an excavation, the ceramic fragment is washed, catalogued, drawn and made ready for publication through the preparation of tables and figures that allow its correct interpretation and comparison with other archaeological contexts. Archaeological drawing is a fundamental and well-established tool in archaeological practice, and new technologies and methods are emerging to automate, standardise and speed up this process as much as possible. An example of this is the LAD (Laser Aided Profiler - Demj\u00e1n, Pav\u00fak, and Roosevelt 2023), a tool that allows ceramic fragments to be 'drawn' quickly and accurately using a laser beam. Over time, however, many drawings were made by hand using traditional tools such as pencils and then had to be 'inked' and made ready for publication. Traditionally, this post-process was done by hand with Indian ink, and nowadays digital drawing programmes are used. This process is however extremely time-consuming and can often discourage the publication of new contexts due to the difficulties in terms of time and resources needed for inking.\nGenerative AI can help to achieve this task, using complex image translation operation. Today, AI is permeating business, creativity and everyday life (Elliott 2019; Le et al. 2020; Varghese, Raj, and Venkatesh 2022; Azatbekova"}, {"title": "Research Aims", "content": "This research addresses the need for efficient and accurate documentation methods in archaeological pottery studies. PyPotteryInk aims to transform traditional documentation workflows through application of DL and generative methods, specifically targeting the labour-intensive process of converting sketches to publication-ready illustrations. The project pursues six interconnected objectives:\n1. Develop a one-step diffusion model for converting archaeological sketches to publication-ready drawings.\n2. Create an efficient patch-based system for processing drawings of any size.\n3. Enable model adaptation to different pottery styles through fine-tuning.\n4. Validate output quality through expert archaeological assessment.\n5. Provide an accessible tool for the archaeological community through open-source software and documentation.\n6. Demonstrate significant time savings in the documentation workflow while maintaining quality standards."}, {"title": "Materials and Methods", "content": null}, {"title": "Archaeological drawings and digital inking", "content": "Archaeological drawing is a fundamental part of the documentation and publication of archaeological finds (Griffiths, Jenner, and Wilson 2002; Steiner and Allason-Jones 2005). It makes it possible to produce a standardised, two-dimensional representation of an artefact according to a set of conventions that are followed everywhere, with some minor variations. Within various traditions, this work fits within the Italian tradition of protohistory, which favours the following standardised conventions.\nThe drawing consists of several components (1) the profile section on the left end; (2) the exterior view (or prospectus) on the right; (3) the exterior profile on the right end; (4) a series of lines defining the axis of symmetry or the diameter line. The profile section reveals the vessel's internal structure through a cross-sectional cut, including wall thickness, rims, and any applied features such as handles or decorative elements. The exterior view displays decorative patterns and the overall shape of the fragment. The horizontal distance between the profile and exterior profile corresponds directly to the vessel's maximum diameter or width. Some scholars also include detailed rendering of surface textures and finishing treatments in the exterior view. Traditionally, the drawing is done in pencil and the shadows on the prospectus are quickly rendered by a play of chiaroscuro. When the artefact is published, the drawing is 'inked', i.e. re-drawn by hand with Indian ink or using a graphics program such as Adobe Illustrator or Inkscape. This polishing process results in a cleaner and more defined drawing, with clean and homogeneous lines, while shadows are represented by a fine dotting. Inking is an artistic process and requires a certain amount of time and expertise, and a single drawing can take from minutes to several hours of work. The inking process is also essential for the interpretation of the artefact: especially in the case of morphological analysis (both traditional and digital - L. Cardarelli 2023), the inking lead to a more standardised result, which makes it easier to compare the artefact's characteristics between different assemblages."}, {"title": "Image-to-image translation and diffusion models", "content": "The task for the proposed model is to transform a sketch into a ready-to-publish drawing. In AI, this task fall within image-to-image translation. Among the first methods proposed to handle this task are Isola et al. (2018), where the job is tackled with the use of Generative Adversarial Networks (GANs). GANs are a type of artificial neural network used to generate new synthetic data from real data. Specifically, they consist of two neural networks, a generator \\(G(x)\\) and a discriminator \\(D(x)\\), which compete in a zero-sum game (Goodfellow et al. 2014; Gui et al. 2020). The generator tries to produce synthetic data that is indistinguishable from real data, while the discriminator tries to distinguish real data from synthetic data. GANs have enjoyed considerable success, even in archaeological applications, where their generative power has been mainly used to restore fragmented artefacts (Navarro et al. 2022; Altaweel, Khelifi, and Zafar 2024). However, training a GAN model can be difficult due to problems such as training instability and mode collapse (Saad, O'Reilly, and Rehmani 2023)."}, {"title": "Input Encoding", "content": "An input image x is encoded into a lower-dimensional latent representation:\n\\(Z_{in}=E(x)\\)\nwhere x \u2208 RH\u00d7W\u00d73 is the input image and E is the VAE encoder augmented with LoRA adapters (see below)."}, {"title": "Latent Space Processing", "content": "Unlike traditional diffusion models that require multiple denoising iterations (Sohl-Dickstein et al. 2015), Parmar et al. (2024) propose modified U-Net (Ronneberger, Fischer, and Brox 2015) that performs the entire denoising process in a single forward pass:\n\\(Z_{out} = U(z_{in}, c)\\)\nwhere c represents the text embedding condition, processed with CLIP's text encoder (Radford et al. 2021) (see below). U denotes the U-Net architecture enhanced with targeted modifications for efficient single-step processing."}, {"title": "Decoding", "content": "The denoised latent representation is transformed back to image space:\n\\(\\hat x = D(z_{out})\\)\nwhere D represents the VAE decoder with skip connections for preserving high-detail fidelity."}, {"title": null, "content": "Parmar et al. (2024) propose several losses for training the model paired translation task:"}, {"title": "Reconstruction Loss (\\(L_{rec}\\)):", "content": "\\(L_{rec} = ||x-\\hat x||_2 + \\lambda_{LPIPS} L_{lpips}(X, Y)\\)\nwhere \\(||x-\\hat x||_2\\) is the L2 distance (Euclidean distance) between x: the target (ground truth) image and the generated image \\(\\hat x\\); \\(L_{lpips}\\) is the LPIPS perceptual loss that measures similarity in feature space and \\(\\lambda_{LPIPS}\\) is a weight parameter to balance the two terms. The L2 term ensures pixel-level accuracy, while LPIPS ensures perceptual similarity. LPIPS uses a pretrained neural network (typically VGG - Simonyan and Zisserman 2015) to compare images in feature space rather than pixel space, which better matches human perception of image similarity (R. Zhang et al. 2018)."}, {"title": "Adversarial Loss (\\(L_{GAN}\\)):", "content": "\\(L_{GAN} = E_x[log D(x)] + E_{\\hat x}[log(1 \u2013 D(\\hat x))]\\)\nThis is the standard GAN loss, where discriminator (D) tries to distinguish between real images x and generated images \\(\\hat x\\), while the generator (G) tries to fool the discriminator."}, {"title": null, "content": "In conclusion, the training objective is to minimise the reconstruction loss \\(L_{rec}\\), the adversarial loss \\(L_{GAN}\\), and the CLIP text-image alignment loss \\(L_{CLIP}\\):\n\\(L = L_{rec} + \\lambda_{GAN}L_{GAN} + \\lambda_{CLIP} L_{CLIP}\\)\nwhere \\(\\lambda_{GAN}\\) and \\(\\lambda_{CLIP}\\) are hyperparameters that balance the different losses. For this implementation, all training parameters are available in the project repository.\nAlthough the model accepts a textual prompt as a condition, this is irrelevant for our purposes, as the need is to produce a single style of output. Therefore, the model is trained with a fixed prompt (\"make it ready for publication\") to ensure a consistent output style.\nDuring the encoding process, the model uses LoRA (Low-Rank Adaptation) to reduce the number of parameters and computational overhead (Hu et al. 2021). In other words, LoRA allows the model to adapt to new tasks with minimal additional parameters, making it ideal for fine-tuning on new examples. The key innovation of LoRA lies in decomposing the weight updates into low-rank matrices:\nInstead of updating the full weight matrix W, LORA decomposes the update into:\n\\(\\Delta W = BA\\)\nwhere \\(B \u2208 R^{d\u00d7r}\\) and \\(A \u2208 R^{r\u00d7k}\\), r is the adaptation rank (typically much smaller than d and k) while the original weight matrix \\(W \u2208 R^{d\u00d7k}\\) remains frozen\nThe capacity of the model to adapt to new tasks is considered crucial for archaeological applications, where the model must be able to learn from a small dataset and then be fine-tuned on new contexts or styles."}, {"title": "Archaeological dataset used and general training process", "content": "As a paired image translation task, a pencil sketch and an inked version of the same drawing are required to train the model. Simplifying, the model can learn the relationship between the two styles (or domains) and reproduce it in inference. The dataset therefore consists of pairs of 492 pots from the Casinalbo (Andrea Cardarelli 2014), Montale (Andrea Cardarelli 2009), Monte Croce Guardia (Andrea Cardarelli et al. 2017) and Monte Cimino (Barbaro et al. 2011; A. Cardarelli and Trucco 2014) contexts. Casinalbo and Montale are MBA-RBA contexts, while Monte Croce Guardia and Monte Cimino are FBA contexts. The drawings were provided by chair of European Protohistory, Sapienza University of Rome and Museo Civico di Modena (Section 7).\nThe pottery assemblage used in this experiment consists of protohistoric vessels manufactured using the characteristic impasto technique of the period. These vessels are handmade with coarse-tempered clay fired to produce distinctive brownish surfaces (Figure 1). The decorative elements include both incised geometric patterns and applied clay elements such as cordons and lugs, representing typical stylistic features of Italian north-central protohistoric pottery production (Levi 2010, 194-200).\nTo train the model, a dataset was used in which the drawings were scaled and resized within a square of 512 512 pixels, without any augmentation (Figure 1). The main objective of this phase is to create a solid model suitable for general use (called the '10k' model - see below), which can then be adapted to specific contexts through fine-tuning. The dataset was divided into a training set of 440 images and a validation set of 52 images. The training last for 10,000 steps. While all training parameters are available in the code repository, the training dataset is not included due to copyright restrictions as most of the drawings are unpublished."}, {"title": "Fine-tuning and inference", "content": "To ensure optimal resolution and maintain the high fidelity required for archaeological documentation, a patch-based processing methodology is then implemented. This approach divides each drawing into 512 512 pixel segments, allowing the model to capture fine-grained details. The 10k model was specifically fine-tuned on these high-resolution patches to preserve essential archaeological features such as shadows, decorative patterns, surface treatments, and precise vessel profiles. 9 pairs of drawings from the Monte Croce Guardia site were used as fine-tuning training dataset. To compensate for the limited dataset size, extensive data augmentation is implemented including the creation of random patches from the source images and applying rotations, translations, and reflections (Figure 2). The augmentation strategy effectively expanded the small dataset, while also testing the model's ability to adapt to specific archaeological contexts with limited examples. The training lasts for 600 steps, resulting in the fine-tuned model called \u20186h-MCG\u2019.\nThis fine-tuning procedure preludes the inference process, as illustrated in the figure below (Figure 3):"}, {"title": "Code Repository and the PyPotteryInk package", "content": "This research introduces a modified implementation of the img2img-turbo model adapted for archaeological documentation purposes. The implementation incorporates several enhancements, particularly the integration of checkpoint-based training resumption through LoRA configuration management. This is fundamental for the fine-tuning process and further development, as a previously trained models can be used as a starting point. Unfortunately, an initial explorations of CPU compatibility revealed significant performance constraints, establishing GPU acceleration as a prerequisite for the application of the model. The development also focuses on delivering PyPotteryInk as a Python library to facilitate adoption within the archaeological research community. The package implements ready-to-use functions for model application, incorporating diagnostic tools, batch processing capabilities and post-processing steps. The"}, {"title": "Training results", "content": "Analysis of the training metrics demonstrates successful model convergence and learning progression. The discriminator loss (lossD) exhibits three distinct learning phases before stabilizing around 2.6, indicating the model successfully learned to distinguish between real and generated drawings. This is complemented by the generator loss (lossG) showing stepped decreases before settling at approximately 0.78, reflecting the model's improving ability to produce convincing archaeological drawings. The perceptual quality metrics show particularly encouraging results. The LPIPS perceptual loss demonstrates substantial improvement, decreasing from 1.25 to 0.65, indicating the model learned to generate drawings that better match human perception of visual similarity. This improvement is further validated by the clean-FID score (Parmar, Zhang, and Zhu 2022), which showed dramatic enhancement from 350 to 60, confirming increased fidelity between generated drawings and their targets. While the model tracks several text-related metrics like CLIP similarity (improving from 23.5 to 25.0), these are less relevant for our archaeological documentation purpose since we employ a fixed prompt. Instead, the most significant indicators are the clean-FID and LPIPS scores, as they directly measure the visual quality and accuracy of the generated drawings. The model appears to reach convergence around step 8000, with the stepped nature of the loss curves reflecting discrete improvements in generation quality. Notably, these improvements occurred without implementing learning rate decay.\nThe visual comparison of the model's output during training within the original sketch and ink versions is also proposed, making it easier to understand the evolution of the model (Figure 6):\nWhile the first epoch doesn't produce any results, a macro-division in 3 phases is evident: (1) Steps 1000-3000: Shows light cyan coloured, somewhat noisy or sketchy lines (2) Steps 3000-5000: Transitions to purple-tinted lines, starting to stabilise the drawing (3) Steps 5000-10000: Gradually converges to clean, black line drawings. Double lines and noise are gradually eliminated, and artifacts are reduced."}, {"title": "The case study: Montale pottery", "content": "To test the model's performance, a case study is conducted using ceramic assemblages from the Montale site (Section 3.3). The test dataset consisted of 72 previously unseen pencil sketches of ceramic vessels, deliberately excluded from the training corpus to ensure the model's generalisation capabilities. Some results are shown in Figure 8."}, {"title": "Expert validation", "content": "If the visual analysis of the output seems to indicate a positive outcome for the training of the model, it is necessary to compare the results with the knowledge-domain of experts in the field. This is a crucial step in the validation of the model, as it allows to understand if the model is able to produce results that are consistent with the archaeological standards and the analytical requirements of the field. The results are based on two steps. Firstly, a single-blind discrimination test is carried out. In this test, a group of archaeological experts were tasked with distinguishing between traditionally inked illustrations and model-generated outputs. Secondly, some key technical attributes are qualitatively evaluated based on a visual comparison between the original pencil drawings and the model output."}, {"title": "Single-Blind Discrimination Test", "content": "A single-blind discrimination test is conducted where four archaeological experts evaluated a mixed set of traditional and AI-generated drawings. The experts were aware of the test's purpose but were blind to the origin of each individual drawing. Their ability to correctly discriminate between AI-generated and traditional drawings was measured using accuracy, recall, precision, and Jaccard metrics. To replicate authentic publication conditions, all test images were scaled to 1:4 or 1:3 ratio, corresponding to standard publication dimensions in archaeological literature."}, {"title": "Qualitative evaluation", "content": "The qualitative evaluation of the model's output is based on a visual comparison between the original pencil drawings and the model output. The experts were asked to evaluate the quality of the output in terms the following criteria:\n1. Archaeological consistency (AC): Does the output respect the original characteristics of the artefact?\n2. Line quality (LQ): Are the lines clean and homogeneous?\n3. Shading quality (SQ): Is the shading consistent and appropriate?\n4. Overall quality (OQ): Does the output look like a publication-ready drawing?\n5. Features recognizability (FR): Are the features (handles, decorations) of the artefact clearly represented?"}, {"title": "Hardware and software requirements, scalability", "content": "Each experiment was performed in a Python (3.10) environment. A NVIDIA L4 GPU with 24 GB VRAM was used to train the model (10k and 6h-MCG). You can find the training parameters in the GitHub project repository. The use of a GPU is mandatory for processing: the inference process is performed using a RTX 3070Ti with 8 GB VRAM. Regarding scalability, it is pointless to hide that this process is highly computational expansive: benchmarks in the project repository that show the processing time of a test image, so that the applicability of the model to different hardware can be assessed. For further considerations on the scalability and limitations of the model, please refer to the discussion."}, {"title": "Discussion", "content": null}, {"title": "Training results and expert validation", "content": "The results of our single-blind discrimination test present an intriguing pattern that deserves careful analysis (Table 1). Most notably, two highly experienced archaeological illustrators (FE and EP1) showed drastically different abilities to distinguish AI-generated drawings from traditional ones, with accuracy scores of 0.85 and 0.05 respectively. FE's high accuracy and perfect precision (1.0) demonstrate they consistently identified distinguishing characteristics in AI-generated drawings. In contrast, EP1's extremely low accuracy (0.05), well below random chance, reveals they consistently misclassified AI-generated drawings as traditional ones. The intermediate scores of EF (0.48) and LP (0.32) suggest varying levels of ability to detect AI-generated content. The dramatic variance in expert performance highlights the inherent subjectivity in evaluating archaeological drawings. Different experts prioritise different aspects of the drawings and apply varying criteria for quality assessment. This subjectivity is particularly evident in EP1's evaluations - while they correctly identified many AI-generated drawings, they classified them as traditional due to their perceived high quality and cleanliness (EP1 - personal communication). This suggests that preconceptions about AI capabilities might lead experts to attribute high-quality outputs to human craftsmanship, rather than artificial intelligence.\nThe qualitative evaluation scores provide additional context to understand the model's performance (Table 2). Archaeological consistency (AC) received high ratings across all experts (CP: 4.94, EP2: 4.27, ADR: 4.98, AC: 4.96), strongly indicating the model's reliability in preserving crucial archaeological information. Line Quality (LQ) shows more variation among experts (CP: 4.87, EP2: 3.41, ADR: 4.93, AC: 4.17), reflecting different professional standards and expectations in archaeological illustration. The assessment of Shading Quality (SQ) also varies considerably (CP: 3.51, EP2: 3.78, ADR: 4.74, AC: 4.36), suggesting that the rendering of shadows and textures is interpreted differently by various experts. Overall Quality (OQ) ratings span from satisfactory to excellent (CP: 3.86, EP2: 3.73, ADR: 5.00, AC: 4.30), while Feature Recognizability (FR) maintains consistently good scores (CP: 3.53, EP2: 4.04, ADR: 4.88, AC: 4.38). The evaluation of Further Details (FD) shows the widest range of scores (CP: 2.74, EP2: 3.82, ADR: 4.98, AC: 4.01), suggesting varying perspectives on the need for manual intervention or enhancement. These scores - combined with the discrimination test results - suggest two major findings. Firstly, the model produces drawings of consistently high quality that meet archaeological standards, as evidenced by the strong AC scores across all experts and the difficulty most evaluators had in distinguishing between AI-generated and traditional drawings in the single-blind test. Secondly, the evaluation reveals significant professional subjectivity in assessing archaeological illustrations, with experts often providing notably different scores for the same criteria, particularly in areas like Line Quality and Shading Quality. In terms of practical benefits, the model's ability to produce drawings in seconds rather than the hours required for manual inking (as noted by CP and EP2) represents a significant advancement in archaeological documentation efficiency, while still allowing for manual refinement when needed.\nMoving to reliability, as shown by the 6h-MCG example, the model can be fine-tuned on a small dataset to adapt to specific archaeological contexts. I cannot define a fixed or minimum number of examples needed for fine-tuning, as this depends on the complexity of the style and the variability of the new dataset, as well as the differences between the pottery's styles or morphologies. If the target dataset is so different in terms of style and morphology, maybe more training examples are required. As a general rule of thumb, I suggest using at least 10-20 examples for fine-tuning, trying to include as much variability (especially decorations) as possible."}, {"title": "Limitations", "content": "Currently, the model has only been tested on protohistoric Italian pottery, which means that the model cannot correctly handle painted decoration that is not attested in the training dataset. Furthermore, the model may have difficulty with drawing styles that are very different from those in the training dataset. In this respect, the relationship between"}, {"title": "Ethical Considerations", "content": "I obtained explicit permissions from the original drawers for using their work in training this model. I acknowledge the potential impact of automation on the professional practice of archaeological illustrators and address these implications directly.\nThe application of generative AI in artistic creation remains an active subject of debate (Amanbay 2023; Wang 2023; Zhou and Lee 2024). The model specifically generates digitally inked archaeological drawings, and a clear disclosure of AI assistance in any resulting illustrations is mandatory. This transparency ensures that viewers can distinguish between traditional and AI-assisted works, maintaining the scholarly integrity of archaeological documentation.\nThe model is designed as a complementary tool rather than a replacement for human expertise. It requires hand-drawn pencil sketches as input, preserving the critical interpretative role of archaeological illustrators. This dependency ensures that the foundational work - the identification, interpretation, and initial documentation of archaeological artifacts remains firmly in human hands. The model simply automates the inking process, a traditionally time-consuming but technically straightforward task. The model's output can be further edited or supplemented manually, allowing illustrators to add detail, correct errors, or enhance the drawing as needed."}, {"title": "Conclusion and Future Work", "content": "PyPotteryInk represents a significant advancement in archaeological documentation, automating the time-consuming process of inking pencil sketches to produce publication-ready drawings. In this way, the model helps to increase the amount of data available for research. The results of the model training and expert evaluation demonstrate its effectiveness in generating high-quality illustrations suitable for academic publication. For archaeologists, this tool offers a substantial reduction in processing time, enabling the rapid production of publication-grade drawings for multiple artifacts. Within the future work, the model needs to be applied to other class of materials, like lithics or metal objects. The open-source nature of the model and the project encourages the community to contribute to the development of the model, and to adapt it to different contexts, styles and materials."}]}