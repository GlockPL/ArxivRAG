{"title": "AIMA at SemEval-2024 Task 10:\nHistory-Based Emotion Recognition in Hindi-English Code-Mixed\nConversations", "authors": ["Mohammad Mahdi Abootorabi", "Nona Ghazizadeh", "Seyed Arshan Dalili", "Alireza Ghahramani Kure", "Mahshid Dehghani", "Ehsaneddin Asgari"], "abstract": "In this study, we introduce a solution to the Se-\nmEval 2024 Task 10 on subtask 1, dedicated to\nEmotion Recognition in Conversation (ERC) in\ncode-mixed Hindi-English conversations. ERC\nin code-mixed conversations presents unique\nchallenges, as existing models are typically\ntrained on monolingual datasets and may not\nperform well on code-mixed data. To address\nthis, we propose a series of models that incorpo-\nrate both the previous and future context of the\ncurrent utterance, as well as the sequential in-\nformation of the conversation. To facilitate the\nprocessing of code-mixed data, we developed\na Hinglish-to-English translation pipeline to\ntranslate the code-mixed conversations into En-\nglish. We designed four different base models,\neach utilizing powerful pre-trained encoders to\nextract features from the input but with vary-\ning architectures. By ensembling all of these\nmodels, we developed a final model that out-\nperforms all other baselines.", "sections": [{"title": "1 Introduction", "content": "The first subtask of SemEval 2024 Task 10 focuses\non Emotion Recognition in Conversation (ERC)\n(Kumar et al., 2023). This subtask requires the de-\nsign of a model capable of predicting an emotion\nfor each utterance. Our final system is an ensemble\nof four high-performing models we developed in\nthis paper. Our primary strategy involves leverag-\ning powerful pre-trained models and utilizing the\ncontext of preceding and succeeding utterances in\nthe conversation. We also consider the sequential\ninformation of the conversation to accurately pre-\ndict emotions. The final system is designed to work\nwith Hindi-English code-mixed conversations. A\ndetailed description of the task is available in (Ku-\nmar et al., 2024).\nERC is an emerging research frontier in Natu-\nral Language Processing (NLP), that aims to iden-\ntify emotions in conversational data. The ability\nto accurately recognize emotions in conversation\nis crucial for a variety of applications, including\nopinion mining from social media platforms (Poria\net al., 2019). ERC is also extremely important for\ngenerating emotion-aware dialogues that require an\nunderstanding of the user's emotions. It is useful in\nvarious sectors, such as healthcare for psychologi-\ncal analysis and education to aid in understanding\nstudent frustration (Antony et al., 2021).\nERC presents several research challenges due\nto the complexity and rapid changeability of emo-\ntions in conversation. The same words can con-\nvey different emotions depending on the context,\nadding a layer of complexity to the task (Kumar\net al., 2023). This complexity is further ampli-\nfied in code-mixed conversations, a common phe-\nnomenon in multilingual societies and online social\nmedia platforms where two or more languages are\nused interchangeably. The challenges in ERC for\ncode-mixed conversations include (i) Linguistic\nComplexity, due to complex linguistic structures\nand sentence or word-level language switches; (ii)\nInsufficient Training Data, as the scarcity of an-\nnotated datasets hampers the training of deep learn-\ning models; (iii) Cultural Nuances, since emo-\ntions can be expressed differently across cultures\nand languages; and (iv) Ambiguity and Context-\nDependence, as word meaning and emotions vary\nbased on context and language."}, {"title": "2 Background", "content": "The official dataset for this task is the MaSaC\ndataset (Bedi et al., 2023), a mixed Hindi-English\nlanguage dataset relevant to our study of emotion\nrecognition in code-mixed dialogues (Kumar et al.,\n2023). This dataset consists of approximately 8506\ntrain, 1354 validation, and 1580 test sentences.\nERC has garnered significant attention in the NLP\ncommunity due to its potential applications.\nRecent research in ERC has attempted to ad-"}, {"title": "3 System Overview", "content": "In the preprocessing stage, we implement a two-\nstep translation process due to the unique nature\nof our data, which comprises Hindi-English mixed\nconversations. At present, there are no robust mod-\nels trained specifically in this mixed language, nor\nare there translators capable of directly translating\nHindi-English mixed text to English with accept-\nable performance. As a result, we first need to\ntranslate our data to English. In the first step, we\ntransform our Hindi-English mixed conversations\ninto Hindi using the indic-trans transliteration mod-\nule (Bhat et al., 2015), a tool proficient in cross-\ntransliteration among all Indian languages. Follow-\ning this, we employ SeamlessM4T Medium (Com-\nmunication et al., 2023) to translate these Hindi\nconversations into English. The English conversa-\ntions obtained from this process serve as our pre-\nprocessed data."}, {"title": "3.2 Model Architecture", "content": "In this section, we propose the model architec-\ntures that were used to construct our final ensemble\nmodel. We designed three distinct architectures,\nand the final model is an ensemble of four models\ntrained based on these architectures. The second\nmodel follows the same architecture as the first, but\nit is trained on an augmented dataset. Our system\npredicts the emotion of the current sentence using\nmajority voting based on the predicted emotions\nfrom four base models.\nGiven the specific domain of the task and the lim-\nited number of samples in the dataset, it is crucial\nto strike a balance between model complexity and\nperformance. Overly complex models may lead to\noverfitting, especially given the unique distribution\nof our dataset. Conversely, overly simple models\nmay not capture the complexity of this particular\ntask. Therefore, we aimed to find a balance, ensur-\ning adequate model complexity to learn effectively\nfrom the data without leading to overfitting. Fur-\nthermore, due to the limited dataset for this task\nand the special domain and emotions that are used,\nsuch as contempt, we leveraged the encoder compo-\nnent of a pre-trained RoBERTa-based model (Liu\net al., 2019) for the emotion recognition task in\nsentences, and fine-tuned it for our specific task\nand domain. This model was trained on the GoE-\nmotions dataset (Demszky et al., 2020), allowing\nus to employ the capabilities of pre-trained models\nfor our task. This encoder was incorporated into\nall of our base models for sentence encoding. In\nthe following parts, each of our base models is ex-\nplained in detail. An overview of architectures is\nshown in Figure 1."}, {"title": "3.2.1 Simple History-Based Model", "content": "This model leverages both the current sentence, for\nwhich we aim to predict the emotion, and the pre-\nceding sentence along with its associated emotion\nas historical information to enhance the model's\nprediction. Both the current and previous sentences\nare processed through our pre-trained encoder to"}, {"title": "3.2.2 Simple History-Based Model + Data\nAugmentation", "content": "This model architecture is identical to the base\nmodel described earlier. The key difference lies\nin the training data. We used a Pegasus paraphrase\nmodel (Zhang et al., 2019) to augment our dataset\nand increase its size. We expanded our dataset by\nrandomly selecting three sentences from the first\nten paraphrases of each original sentence. Given\nthe limited size of the original dataset, this augmen-\ntation method should enhance the model's learning\ncapability by exposing it to a wider range of data."}, {"title": "3.2.3 Full History-Based Model", "content": "This model, which is an extension of the Simple\nHistory-Based model, aims to leverage more his-\ntorical information for improved performance. In\naddition to the current sentence, previous sentence,\nand previous emotion, we also incorporated the\nconcatenated string of all previous sentences in the\nconversation into our model. The rationale behind\nthis is to enable the model to access additional in-\nformation and gain a better understanding of the\ncontext of the current sentence within the conversa-\ntion. The concatenated string of all previous sen-\ntences is processed through our pre-trained encoder\nto obtain the history embedding. This encoding is\nthen passed through a simple feed-forward neu-\nral network, which consists of two linear layers,\na batch normalization layer, a dropout layer, and\na LeakyReLU activation function. This network\ntransforms the 768-dimensional input into a 128-\ndimensional space."}, {"title": "3.2.4\nContext-Aware GRU-Based Model", "content": "This model, more complex than its predecessors,\nintroduces several key modifications. Firstly, it in-\ncorporates information from both the preceding and\nsucceeding sentences in a conversation, allowing\nthe model to leverage both past and future con-\ntexts. Secondly, in contrast to previous architec-\ntures that use the emotion of the previous sentence,\nthis model omits this feature to prevent error prop-\nagation during the inference phase. If a model\nincorrectly predicts the emotion of one sentence,\nit could potentially use this incorrect information\nwhen predicting the emotion of the next sentence,\nleading to further errors. Lastly, this model em-\nploys a Gated Recurrent Unit (GRU) (Chung et al.,\n2014; Cho et al., 2014), enabling it to leverage the\nsequential information in the conversation.\nThe model processes all sentences up to and\nincluding the current one (for which we want to\npredict the emotion) and the next sentence through\nour pre-trained encoder to obtain their embeddings.\nIf the current sentence in the conversation has more\nthan three previous sentences, only the last three are\nconsidered, making the model focus on the most\nrecent context. These embeddings are then passed\nthrough a stacked GRU, consisting of two GRUS\nwith a hidden dimension of 256 and a dropout rate\nof 0.25. Both the current and next sentences went\nthrough a transformation via a linear layer and\na dropout layer to generate output encodings in\na common 256-dimensional space. The last two\nhidden layers of the GRU are concatenated and\npassed through a multi-head self-attention mecha-\nnism, similar to our previous models.\nThe output of the last layer of the GRU, the\noutput of the attention mechanism, and the trans-"}, {"title": "4 Experimental Setup", "content": "We utilized the official dataset provided for the\ntask as the only data source for our system. The\ndefault split provided for the task was also used.\nDuring the development phase, the validation set\nwas exclusively used for evaluating various steps\nand experimental configurations. For the final sub-\nmission, models were fine-tuned on both the train-\ning and validation splits. For evaluation purposes,\nour primary metric was Weighted F1. However, to\nprovide a more comprehensive analysis, we also\nreported three additional metrics, as detailed in Ta-\nble 1. Our training process primarily employed the\nPyTorch and Transformers libraries. All base mod-\nels were trained using the early stopping method\nand the AdamW (Loshchilov and Hutter, 2019) op-\ntimizer. A learning rate scheduler was used, with a\nlower learning rate set for the pre-trained encoder\n(5e-6) compared to other parameters (1e-4). The\nbatch size was set to 1 for the Context-Aware GRU-\nBased model and 4 for other models during training.\nThe cross-entropy loss function was used for the\ntraining."}, {"title": "5 Results", "content": "Table 1 presents SubTask 1 results. We compare\nour approach with four baseline models. The first\nbaseline is GPT 3.5 Turbo, for which we used its\nAPI key to input the entire conversation and pre-\ndict the emotion for each sentence. The results of\nthis baseline model illustrate that this task is much\nmore challenging than general sentence emotion\nrecognition because it is domain-specific. The next\ntwo models are traditional ones, namely Linear\nRegression and Decision Tree, that utilize embed-\ndings extracted from the LaBSE sentence encoder\n(Feng et al., 2022). The LaBSE model serves as a\npowerful encoder for our text data, enabling us to\nachieve comprehensive and multilingual text em-\nbeddings. The final baseline model is similar to a\nSimple History-Based model. It employs our pre-\ntrained encoder but does not use any context, such\nas the previous sentence, and relies solely on the\ncurrent sentence.\nMoving on to the comparison of our models, we\nfirst consider the Simple History-Based model. By\ncomparing its results with the Full History-Based\nmodel, we find that most of the information for pre-\ndicting the emotion is contained in the current and\nthe previous sentence. Therefore, information from\nall of the previous sentences is not as useful for\npredicting emotion. Our second model, which uses\ndata augmentation, does not perform well. This is\nlikely due to overfitting and the domain-specific\nnature of the conversations, making data augmen-\ntation less effective. As can be seen in our models,\nthe Context-Aware GRU-Based model outperforms\nthe others. This is because it incorporates infor-\nmation from both the preceding and succeeding\nsentences and the GRU can leverage the sequential\ninformation in the conversation. The closeness of"}, {"title": "6 Conclusion", "content": "In this paper, we proposed a novel method to ad-\ndress the Code-Mixed Emotion Recognition in\nConversations (ERC) challenge. Our approach\nleverages the power of pre-trained large models\nand incorporates both previous and future context\ninformation of the current utterance, as well as se-\nquential information of the conversation up to that\npoint, to recognize each utterance's emotion. In\naddition to our primary model, we utilized other\nbase models with different architectures based on\nvarious Deep Learning components to tackle this\nproblem. By ensembling all of these models, we\ndeveloped a final system that outperforms previous\nmodels.\nDespite these advancements, Code-Mixed ERC\nremains a challenging task with significant poten-\ntial for further investigation. Future research direc-\ntions could include designing robust encoders ca-\npable of processing code-mixed dialogues and pre-\ndicting emotions in an end-to-end manner. More-\nover, collecting more data on these code-mixed\ndialogues is necessary to improve the performance\nof models. Furthermore, we can explore more\ncomplex models that incorporate different infor-\nmation from various modalities to achieve better\nperformance. This work serves as a stepping stone\ntowards more sophisticated emotion recognition\nsystems for code-mixed dialogues."}]}