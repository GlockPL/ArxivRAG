{"title": "Anatomy of Machines for Markowitz: Decision-Focused Learning for Mean-Variance Portfolio Optimization", "authors": ["Junhyeong Lee", "Inwoo Tae", "Yongjae Lee"], "abstract": "Markowitz laid the foundation of portfolio theory through the mean-variance optimization (MVO) framework. However, the effectiveness of MVO is contingent on the precise estimation of expected returns, variances, and covariances of asset returns, which are typically uncertain. Machine learning models are becoming useful in estimating uncertain parameters, and such models are trained to minimize prediction errors, such as mean squared errors (MSE), which treat prediction errors uniformly across assets. Recent studies have pointed out that this approach would lead to suboptimal decisions and proposed Decision-Focused Learning (DFL) as a solution, integrating prediction and optimization to improve decision-making outcomes. While studies have shown DFL's potential to enhance portfolio performance, the detailed mechanisms of how DFL modifies prediction models for MVO remain unexplored. This study aims to investigate how DFL adjusts stock return prediction models to optimize decisions in MVO, addressing the question: \"MSE treats the errors of all assets equally, but how does DFL reduce errors of different assets differently?\" Answering this will provide crucial insights into optimal stock return prediction for constructing efficient portfolios.", "sections": [{"title": "1 INTRODUCTION", "content": "Decision making is a crucial aspect across various fields, where optimization is often employed to quantitatively guide us toward the best possible outcome. In ideal scenarios, where the values of all parameters in the optimization formula are known, a mathematically optimal solution can be determined with precision. However, in most real-world situations, parameters are often uncertain. The quality of the decisions derived from optimization heavily depends on how accurately these uncertain parameters are estimated. Therefore, it is essential to ensure accurate estimation of these parameters to achieve high-quality decision-making outcomes.\nThis is particularly evident in the field of portfolio optimization, where financial decisions are made under uncertainty. Harry Markowitz laid the foundation of portfolio theory based on the mean-variance optimization (MVO) [22]. The objective of MVO is to construct an investment portfolio that maximizes return for a given level of risk or minimizes risk for a given level of expected return. While the MVO has been fundamental to investment management [17], its effectiveness depends on the accurate estimates of the expected return, variance, and covariance of asset returns, which are often uncertain in practice. Despite this uncertainty, decision makers who utilize the MVO should have their own estimates of expected returns and risks of financial assets. Thus, many researchers and practitioners have questioned how the input parameters of MVO should be estimated. To this, Markowitz is said to have responded with wit and grace, \u201cThat's your job, not mine.\u201d [28]\nIn relation to \"your job\u201d, many studies have been conducted to investigate the impact of estimation errors in input parameters on mean-variance optimization. [23] showed that mean-variance optimization can maximize the effect of input parameter estimation errors, which can lead to inferior results compared to an equally weighted portfolio. Other studies, such as [3, 4, 13], have discussed the importance of input parameter settings in the MVO framework. In addition, some researchers have analyzed how the MVO optimal portfolio or the distribution of all possible portfolios change as the input parameters change (e.g., [3, 4, 7, 8, 13]). While these studies reveal how the degree of estimation errors affect the MVO framework, they do not go further into how the shape of estimation errors affect the MVO framework.\nIn practice, machine learning has become very useful in the estimation of parameters and decisions are made through optimization based on the machine learning estimates as inputs [19]. Hence, so-called Predict-then-Optimize method can be seen as a two-stage method. The prediction and optimization stages are separated, and thus, the prediction stage is solely concerned with enhancing prediction accuracy such as the mean squared errors (MSE). Recent studies have argued that a prediction model that minimizes traditional prediction losses, such as MSE, may not be optimal for decision-making in the subsequent optimization stage. To overcome this issue, a framework called Decision-Focused Learning (DFL) has been proposed (e.g., [10, 11, 20, 26, 29, 31]). DFL has been studied in various fields, and portfolio optimization is no exception. A couple of studies (e.g., [6, 9]) have showed that DFL can be implemented for portfolio optimization and it can enhance investment performance. However, they have not analyzed the detailed characteristics of the DFL prediction model.\nThe objective of this study is to gain insight into how the prediction model changes when DFL is applied to the MVO. To be more specific, we wish to answer the following question:"}, {"title": "2 BACKGROUND", "content": "2.1 Decision-Focused Learning (DFL)\n$w^* (c) = \\arg \\min_w f (w, c)$\n$s.t. g(w, c) \\leq 0$\n$h(w, c) = 0$\n(1)\nPredict-then-Optimize. Following the notations of [21], the general objective of an optimization problem is to find a solution $w^* (c)$, where w represents the decision variables and c represents the parameters. The solution $w^* (c)$ minimizes the objective function $f(w, c)$ while satisfying the inequality constraint $g(w, c) \\leq 0$ and equality constraint $h(w, c) = 0$. So-called \"Predict-then-Optimize\" framework proceeds with prediction before optimization, also known as two-stage learning. As the term suggests, it is divided into two phases. Initially, a machine learning model $F_\\theta$ generates $\\hat{c} = F_\\theta(x)$, where $\\theta$ represents the parameters of model F, and x represents the features that can be used to predict $\\hat{c}$. Subsequently, optimization is performed using the predicted parameter $\\hat{c}$. In this case, traditional ML training methods are used to accurately predict the ground truth $c^*$. Commonly, the mean squared error (MSE) or cross entropy is used to minimize the difference between the ground truth $c^*$ and the predicted parameter $\\hat{c}$, thus training the model.\nDecision-Focused Learning. Many studies (e.g., [10, 11, 31]) suggested that predict-then-optimize framework often results in suboptimal outcomes, because the prediction and optimization stages are separated. Minimizing prediction errors measured by MSE or cross entropy is not necessarily beneficial to the subsequent decision making stage. To overcome this limitation, DFL framework has been proposed, which can be seen as new model training methodologies that consider both prediction and optimization stages holistically.\n$Regret(w^* (c), c) = f (w^*(\\hat{c}), c) \u2013 f(w^* (c), c)$\n(2)\nIn DFL, a machine learning model is trained to minimize a loss function that reduces the decision making error when the actual decision is realized through $w^*(c)$. To be more specific, $Regret(w^* (c), c)$, which measures the suboptimality of the decision made via $w^* (\\hat{c})$, is considered in most cases. The prediction model is trained to predict $\\hat{c}$ that is most helpful for optimal decision making.\nWhile the concept of DFL is straightforward, there are some obstacles when implementing them. The major issue in DFL implementation is the difficulty of calculating gradients for model training. Let L be the DFL loss, analogous to the concept of regret. In order to proceed with gradient-based learning, it is necessary to differentiate L with respect to the model parameter $\\theta$. The gradient can be expressed as follows based on the chain rule:"}, {"title": null, "content": "$\\frac{dL(w^*(c), c)}{d\\theta} = \\frac{dL(w^*(\\hat{c}), c)}{dw^* (\\hat{c})} \\frac{dw^*(\\hat{c})}{d\\hat{c}} \\frac{d\\hat{c}}{d\\theta}$\n(3)\nThe first term on the right hand side should not be a problem, because the DFL loss consists of the objective function of optimization problem, and thus, it should be mostly differentiated with respect to w* analytically. The third term can be computed in the same way as for usual gradient-based learning of most prediction models. However, the second term is the gradient of the optimal solution of an optimization problem, making differentiation extremely tricky. Even if the solution is continuous, the second gradient must be calculated through the argmin or argmax operation[31].\nA number of studies have tried to overcome this computational issue of DFL. [1, 2] proposed methodologies for integrating optimization problems into neural networks. Recent studies including [11, 20, 24, 29] proposed to use surrogate functions to avoid direct calculation of the second gradient. Note that this study aims to analyze how DFL affects the prediction model, and thus, the efficiency of training is not an important issue. Hence, we calculate loss function values by directly solving optimization problems.\n$w^* (\\mu) = arg \\max_w w^T\\mu - \\lambda w^T \\Sigma w$\ns.t.\n$\\sum_{i=1}^N w_i = 1,$\n$0 \\leq w_i \\leq 1 \\ \\ for \\ i \\in \\{1,..., N\\}$\n(4)\n2.2 Mean-Variance Optimization (MVO)\nModel Formulation. A typical formulation for mean-variance optimization developed by Markowitz [22] is as follows:\nHere, w represents the portfolio weights of the N risky assets, which is constrained to have sum equal to 1 and to be between 0 and 1. \u00b5 is the expected return of the assets, A is the risk aversion, and \u2211 is the covariance of asset returns. This optimization problem allows us to maximize the portfolio returns, $w^T\\mu$, while considering a risk penalty, $\\lambda w^T \\Sigma w$. Note that the optimal portfolio weight $w^*$ is represented as a function of expected return \u00b5, because the focus of this study is on how the optimal portfolio $w^*$ changes with respect to the predition of \u00b5.\nAs mentioned in Section 1, accurately estimating uncertain parameters for optimization is challenging, especially because the returns of financial assets are known to be highly volatile. To address this, different approaches have been proposed by many researchers and practitioners. Examples include robust optimization [16], Black-Litterman model [5], Bayesian approach [12], and risk factor models [27].\nEstimation of Returns and Covariances. As mentioned in Section 1, there has been extensive research on the estimation errors of input parameter of the MVO framework (i.e., \u00b5 and 2). One of the earliest studies to highlight the relative importance of estimation errors in MVO parameters is by Chopra and Ziemba [7]. They conducted simple perturbations on mean, variance, and covariance, finding that errors in u have a relatively greater impact on the optimal objective compared to errors in \u03a3. [25] quantified how sensitive optimal portfolios are to estimation errors in \u00b5 and \u2211, theoretically showing that the relative impact of covariance matrix errors mainly"}, {"title": "3 METHODOLOGY", "content": "3.1 DFL Loss for MVO\nIn this study, we focus on how the prediction of expected returns \u00b5 changes when DFL is implemented for MVO. That is, for covariances, we use sample covariance matrix \u00ca calculated using historical data. We define the DFL loss for MVO based on the regret loss defined in Eq. 2 as follows:\n$L_{MVO} = Regret(w^* (\\mu), \\mu^*)$\n$= f(w^* (\\hat{\\mu}), \\mu^*) \u2212 f(w^* (\\mu^*), \\mu^*)$\n$= (\\lambda w^* (\\hat{\\mu})^T \\Sigma w^*(\\hat{\\mu}) \u2212 {\\mu^*}^T w^* (\\hat{\\mu}))$\n$\u2212 (\\lambda w^* ({\\mu^*})^T \\Sigma w^*({\\mu^*}) \u2212 {\\mu^*}^T w^* ({\\mu^*}))$\n(5)\nA machine learning model $F_\\theta$ generates \u00fb, the \u2018predicted' expected return. The goal is to train the model $F_\\theta$ in such a way that it minimizes the difference between the objective value obtained with the prediction \u00fb and the objective value obtained with the ground truth \u03bc*.\n3.2 Combined Loss\nWe do not simply compare a prediction model trained to minimize prediction error and a prediction model trained with DFL. Instead, we analyze the changes in the prediction model as it gradually becomes more decision-focused. In this regard, we define a combined loss as the weighted sum of MVO loss Eq. 5 and mean squared error (MSE), which is the most common loss function for prediction models.\n$L_{Combined} = \\alpha L_{MVO} + (1 \u2212 \\alpha) L_{MSE}$\n(6)"}, {"title": "4 THEORETICAL ANALYSIS", "content": "Let us first present a theoretical analysis of how predictions should change when DFL is implemented for MVO. For simplicity, we consider the Sharpe ratio maximization problem, which is a special case of MVO. The Sharpe ratio [30] is the most common measure of how much reward an investment provides relative to the risk it takes.\n$Sharpe \\ Ratio = \\frac{r_p - r_f}{\\sigma_p} = \\frac{{\\mu}^T w}{\\sqrt{w^T \\Sigma w}}$\n(7)\nHere, $r_p$ is the return of a portfolio and $\\sigma_p$ is the risk of the portfolio. $r_f$ is the risk-free rate. For a portfolio w, expected return \u03bc, and covariance matrix \u2211, the Sharpe ratio with zero risk-free rate can be written as the last expression in Eq. 7.\n$w^* (\\mu) = arg \\max_W \\frac{w^T \\mu}{\\sqrt{w^T \\Sigma w}} = \\Sigma^{-1} \\mu$\n(8)\nAs shown in Eq. 8, there is an analytical solution for the unconstrained Sharpe ratio maximization. Bsed on this expression, the Sharpe ratio of a portfolio $w^* (\\hat{\\mu})$ evaluated using the ground truth expected return \u03bc* is as follows:\n$SR({\\mu^*}, \\hat{\\mu}) = \\frac{w^* (\\hat{\\mu})^T {\\mu^*}}{\\sqrt{w^*(\\hat{\\mu})^T \\Sigma w^*(\\hat{\\mu})}}$\n$= \\frac{(\\Sigma^{-1} \\hat{\\mu})^T {\\mu^*}}{\\sqrt{(\\Sigma^{-1} \\hat{\\mu})^T \\Sigma (\\Sigma^{-1} \\hat{\\mu})}}$\n$= \\frac{(\\Sigma^{-1} \\hat{\\mu})^T {\\mu^*}}{\\sqrt{{\\hat{\\mu}}^T \\Sigma^{-1} \\hat{\\mu}}}$\n(9)\nThen, an DFL model would be trained using the gradient of the Sharpe ratio $SR({\\mu^*}, \\hat{\\mu})$ with respect to the model prediction \u00fb, which can be calculated as"}, {"title": null, "content": "$\\frac{\\partial SR({\\mu^*}, \\hat{\\mu})}{\\partial \\hat{\\mu}} = \\frac{{\\mu^*}^T \\Sigma^{-1}}{\\sqrt{{\\hat{\\mu}}^T \\Sigma^{-1} \\hat{\\mu}}} - \\frac{{\\hat{\\mu}}^T \\Sigma^{-1} {\\mu^*} \\Sigma^{-1} \\hat{\\mu}}{({\\hat{\\mu}}^T \\Sigma^{-1} \\hat{\\mu})^{3/2}}$\n$= \\frac{\\Sigma^{-1} {\\mu^*}}{\\sqrt{{\\hat{\\mu}}^T \\Sigma^{-1} \\hat{\\mu}}} - \\frac{{\\hat{\\mu}}^T \\Sigma^{-1} {\\mu^*} \\Sigma^{-1} \\hat{\\mu}}{{\\hat{\\mu}}^T \\Sigma^{-1} \\hat{\\mu}} \\frac{1}{\\sqrt{{\\hat{\\mu}}^T \\Sigma^{-1} \\hat{\\mu}}}$\n$= \\frac{\\Sigma^{-1} {\\mu^*} - SR({\\mu^*}, \\hat{\\mu}) \\Sigma^{-1} \\hat{\\mu}}{SR(\\hat{\\mu}, \\hat{\\mu})}$\n$= \\frac{\\Sigma^{-1} ({\\mu^*} - SR({\\mu^*}, \\hat{\\mu}) \\hat{\\mu})}{SR(\\hat{\\mu}, \\hat{\\mu})}$\n(10)\nThe i-th element of the gradient is\n$\\frac{\\partial SR({\\mu^*}, \\hat{\\mu})}{\\partial \\hat{\\mu}_i} = \\frac{{\\Sigma_i}^{-1} ({\\mu^*} - SR({\\mu^*}, \\hat{\\mu}) \\hat{\\mu})}{SR(\\hat{\\mu}, \\hat{\\mu})}$\n(11)\nHere, ${\\Sigma_i}^{-1}$ denotes the i-th row of $\\Sigma^{-1}$. Note that in Eq. (11), $SR({\\mu^*}, \\hat{\\mu})$ and $SR(\\hat{\\mu}, \\hat{\\mu})$ are scalar values, and thus, they do not change even if we look at the gradient with respect to different i's.\nThe gradient of MSE with respect to $\\mu_i$ is\n$\\frac{\\partial MSE(\\hat{\\mu}, {\\mu^*})}{\\partial \\hat{\\mu}_i} = \\frac{\\partial \\sum_{i=1}^N (\\hat{\\mu}_i - {\\mu^*}_i)^2}{\\partial \\hat{\\mu}_i} = 2(\\hat{\\mu}_i - {\\mu^*}_i)$\n(12)\nHence, the gradient of the DFL model $\\frac{\\partial SR({\\mu^*}, \\hat{\\mu})}{\\partial \\hat{\\mu}}$ can be seen as the gradient of MSE loss $\\frac{\\partial MSE(\\hat{\\mu}, {\\mu^*})}{\\partial \\hat{\\mu}}$ tilted by $\\Sigma^{-1}$. That is, while a conventional machine learning model that minimized MSE loss does not consider covariance \u2211 as shown in Eq. 12, an DFL model would incorporate the covaraince information through its inverse $\\Sigma^{-1}$ as shown in Eq. 11. In our numerical experiments, we empirically confirm that $\\Sigma^{-1}$ plays an important role in training of DFL for MVO."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "5.1 Experiments Settings\nThe overall experimental setup is similar to those of the portfolio optimization example in [29]. In this study, all experiments were"}, {"title": "5.2 Evaluation Metrics", "content": "Along with the MVO and MSE loss functions introduced in Section 3, we evaluate the performance of models in terms of three metrics: normalized decision quality, Sharpe ratio, and cumulative return.\nWe follow the definition of decision quality and normalized decision quality from [9]. For our MVO problem, the decision quality (DQ) can be calculated as follows:\n$DQ(\\hat{\\mu}) = {\\mu^*}^T w^* (\\hat{\\mu}) \u2212 \\lambda w^*(\\hat{\\mu})^T \\Sigma w^*(\\hat{\\mu})$\n(13)\nAs can be seen from the definition, DQ(\\hat{\\mu}) is simply the objective value of the decision made based on the prediction \u00fb evaluated under the ground truth \u03bc*. The normalized decision quality (NDQ) can be calculated as follows:\n$NDQ_{Model} = NDQ(\\hat{\\mu}) = \\frac{DQ_{Model} - DQ_{Random}}{DQ_{Optimal} - DQ_{Random}}$\n(14)\nHere, $DQ_{Model} = DQ(\\hat{\\mu})$, $DQ_{Random}$ is the average of the DQ values evaluated using a number of random predictions, and $DQ_{Optimal} = DQ({\\mu^*})$. Note that $NDQ_{Random} = 0$ and $NDQ_{Optimal} = NDQ({\\mu^*}) = 1$. Thus, $NDQ_{Model}$ should be between 0 and 1. The resulting comparison allows for a nuanced assessment of the relative performance of the models in question.\nWhile NDQ directly measures whether the model is trained to make good decisions or not, these values may not be intuitive to practitioners. Therefore, we use additional metrics to see the investment performance of various models. In this regard, we use"}, {"title": "5.3 Model Performance", "content": "We first present the normalized decision quality (NDQ), MVO loss and MSE loss for models trained with $\u03b1 \\in \\{0, 0.25, 0.5, 0.75, 1\\}$ and $\u03bb \\in \\{1, 3, 5, 10\\}$. The results are summarized in Table 1, where each value represents an average of five experiments with different random seeds, and each value in parenthesis represents the standard deviation. For most A values, NDQ increases as a increases, but the trend is less pronounced when \u03bb = 1. However, the standard deviation becomes smaller for larger a, which means that DFL makes the model more reliable in terms of decision quality.\nIt is also interesting to see that the deviation in NDQ across different a becomes smaller as A becomes larger. We suspect that this could be because there is less room for change and improvement as the problem requires more risk-averse decisions.\nMVO and MSE Loss were evaluated in the test set. We can clearly see that MVO Loss decreases as a increases. For MSE Loss, it is interesting to see that it becomes the smallest when a = 0.75. Note that MSE is supposed to be the smallest when a = 0, because the model would be trained to minimize MSE only. We guess that it might be because DFL makes the prediction more robust. This could be investigated in more detail in future research. For a ="}, {"title": "5.4 Cosine Similarity Between Optimal and Model Portfolios", "content": "In the previous subsection, we could see that DFL improves decision qualities and investment performances, and here, we examine if the decisions made using DFL prediction actually become close to the optimal decisions. We measure it using the cosine similarity between the optimal decision $w^* ({\\mu^*})$ based on the ground truth \u03bc* and the model portfolios $w^* (\\hat{\\mu})$ based on the prediction \u00fb. It can be calculated as follows.\n$Cos(w^* ({\\mu^*}), w(\\hat{\\mu})) = \\frac{\\sum_{i=1}^N w_i({\\mu^*}) \\times w_i(\\hat{\\mu})}{\\sqrt{\\sum_{i=1}^N (w_i({\\mu^*}))^2} \\times \\sqrt{\\sum_{i=1}^N (w_i(\\hat{\\mu}))^2}}$\n(15)\nTable 2 shows the results. It is evident that as a increases, cosine similarity increases, indicating that as the model becomes more"}, {"title": "5.5 Relationship Between Gradient Direction and 2-1", "content": "So far, we confirmed that DFL makes prediction models to produce predictions that could lead to better decisions. Now let us turn to our main research question: \"what is the difference between decision-focused prediction models and conventional prediction models?\"\nFirst, relationship between \u03bc* and \u00fb. Figure 3 shows the box plot of correlations between u* and \u00fb with different values of A and a. It is interesting to see that the correlations show general upward trend as a increases. It is quite surprising given that MSE values were very large when a = 1. Hence, what DFL really cares is not the individual errors, but the order of predicted values.\nSecond, relationship between DFL and \u2211\u00af\u00b9. We have analytically shown in Section 4 that the training of DFL is closely related to 2-1. To empirically examine this, we investigate various aspects. Table 3 and 4 shows the average correlation between 2\u00af\u00b9 and \u00fb, MSE and \u00fb, and 2-1 and w*(\u00fb). We can see that when a = 1, all correlation values exhibit large absolute values. It is evident that"}, {"title": "6 CONCLUSION", "content": "We analized how the return prediction model changes as we make the model more decision-focused for the MVO problem. Our findings are threefolds. First, we found that more decision-focused prediction models tend to perform better in terms of decision quality and Sharpe ratio. Second, our experiments showed that more decision-focused models tend to produce prediction mu that is more highly correlated with the ground truth \u03bc*. That is, in MVO, the order between predicted values matters more than the errors measured in MSE. Third, we theoretically found that the training of DFL for MVO should be related to the inverse of the covariance matrix 2-1. Indeed, our numerical experiments confirmed this finding empirically. In specific, if the average of 2\u00a1\u00b9 is large, the prediction \u00fbi tend to have smaller values with large errors, and if the average of \u00a1\u00b9 is small, the prediction \u017f\u00fbi tend to have larger values with small errors. In other words, DFL tries to reduce the prediction error for good assets, while it does not care much about the prediction error for bad assets.\nAlthough this study was conducted on the basic MVO formulation, it will be interesting in the future to see how the predictive model changes when DFL is applied to other portfolio optimization models, such as portfolio optimization based on other risk measures such as Conditional Value-at-Risk (CVaR) and Mean Semi-Absolute Deviation (MSAD), or other types of portfolio optimizations such as robust optimization and the Black-Litterman model. It is also important to note that while we do present theoretical findings in this study, it is important to be cautious in interpreting our empirical findings. In order for the empirical findings to be more generalized, they should be tested on various machine learning models and datasets."}]}