{"title": "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "authors": ["Hawau Olamide Toyin", "Hao Li", "Hanan Aldarmaki"], "abstract": "Speech recognition and speech synthesis models are typically trained separately, each with its own set of learning objectives, training data, and model parameters, resulting in two distinct large networks. We propose a parameter-efficient approach to learning ASR and TTS jointly via a multi-task learning objective and shared parameters. Our evaluation demonstrates that the performance of our multi-task model is comparable to that of individually trained models while significantly saving computational and memory costs (~50% reduction in the total number of parameters required for the two tasks combined). We experiment with English as a resource-rich language, and Arabic as a relatively low-resource language due to shortage of TTS data. Our models are trained with publicly available data, and both the training code and model checkpoints are openly available for further research.", "sections": [{"title": "Introduction", "content": "Fundamentally, text and speech are different representations of similar information, with text being a far more condensed form of the linguistic content of speech. Conversion between text and speech modalities in the form Automatic Speech Recognition (ASR), text-to-speech synthesis (TTS), or Voice Conversion (VC), are traditionally achieved by training separate ASR, TTS, and VC models as the input and output modalities and training objectives differ significantly. However, considering the recent developments in self-supervised and multi-modal pre-training, a more integrated approach can now be promising. A unified model trained simultaneously for multiple speech/text to speech/text, improves on generalization to new data, cross-task knowledge transfer, simplifies maintenance, and also reduces the computational and memory requirements for training, storage, and inference. Recent studies seek to achieve a smooth fusion of text and audio by developing unified audio-text models capable of addressing diverse tasks both within and across these modalities. While these models are considered multi-modal if they can process different input modalities, for our purposes we group audio-text models into two categories based on their output modality: uni-modal and cross-modal. We describe uni-modal approaches as models capable of generating output in a single modality only, such as Whisper (Radford et al., 2022), and Google USM (Zhang et al., 2023b), which only generate text outputs. Cross-modal approaches, on the other hand, are capable of generating outputs in both speech and text modalities, such as Viola (Wang et al., 2023), SpeechT5 (Ao et al., 2022), and SpeechGPT (Zhang et al., 2023a). Some of these models (Zhang et al., 2023a; Maiti et al., 2024) use discrete representations for audio tasks, merging text and audio tokens in a shared vocabulary while jointly training multiple tasks; other models, such as SpeechT5 (Ao et al., 2022) use continuous representations for audio. While SpeechT5 is pre-trained with a cross-modal objective, handling both text and speech as input and output modalities, the model is fine-tuned individually for downstream tasks such as ASR and TTS.\nThis work builds on these developments, particularly SpeechT5 (Ao et al., 2022), by achieving a truly cross-modal speech and text conversion in a single architecture. Unlike previous work, our approach does not separate the speech/text-to-speech/text tasks based on input/output modalities; instead, we fine-tune these components concurrently using a unified model and loss function with the help of a simple MLP-based task fusion module. The resulting model is a single encoder-decoder that can handle both modalities at the input and output, depending on the desired task. We summarize our contributions below:"}, {"title": "Related Work", "content": ""}, {"title": "Multi-Task Speech Models", "content": "Radford et al. introduced Whisper, an encoder-decoder model trained on vast amount of speech-text (680K hours) data. Whisper a multilingual model with multitasking capabilities. However, it only uses speech as input and can not generate speech output. In SLAM (Bapna et al., 2021), the authors unified speech and text pre-training within a single model using a single encoder with the combined BERT (Devlin et al., 2019) and W2V-BERT (Chung et al., 2021) objectives on unlabeled text and speech. To align their model's representations across modalities, they used Translation Language Modeling (TLM) and Speech Text Matching (STM) alignment losses that use supervised speech-text recognition data. They show that joint pre-training improves model performance on downstream speech translation and recognition tasks. However, these models are not jointly trained for multi-task purposes and require dedicated fine tuning for each task."}, {"title": "Unified Speech-Text Models", "content": "SpeechT5 (Ao et al., 2022) introduces a multi-modal encoder-decoder pre-training approach for spoken language processing. The authors attempted a joint pre-training approach of speech and text to improve the model's performance on downstream speech/text tasks like ASR, TTS, speaker identification, speech enhancement, and voice conversion. They built on the transformer architecture (Vaswani et al., 2017), adding modal-specific pre-nets and post-nets to handle latent feature extraction/conversion for different modalities. Although their model is pre-trained jointly with speech and text data in a self-supervised manner, the supervised downstream models (e.g. ASR, TTS) were trained individually for each task. The model was trained and evaluated on English only. A subsequent work followed the same architecture and training paradigm for building an Arabic version of the model, named ArTST (Toyin et al., 2023), which also requires task-specific training.\nSome recent methods employ a decoder-only framework post-conversion of continuous audio into discrete tokens, subsequently combining text and audio tokens into a unified vocabulary (Maiti et al., 2024; Zhang et al., 2023a; Wang et al., 2023). These models can generate both text and speech output from speech/text input. Some models (Maiti et al., 2024) discretize speech using k-means on features extracted from speech-text pre-trained models like HuBert (Hsu et al., 2021). However, their method can suffer from information loss caused by quantizing speech signals into discrete tokens and its performance highly depends on the value of k used for feature extraction. Moreover, combining text and discrete speech tokens into a vocabulary can lead to a large vocabulary size for multilingual training."}, {"title": "Our Method", "content": "In this section, we describe a unified Speech-To-Text And Text-To-Speech model, STTATTS, our proposed architecture for jointly training ASR and TTS. After unified self-supervised training as described in Ao et al. (2022), we propose utilizing a multi-task loss objective to optimize our model parameters for multiple tasks, along with a task fusion module to handle the different tasks. Unlike the fine-tuning methodology followed in SpeechT5 (Ao et al., 2022; Toyin et al., 2023), which results in a completely disjoint copy of the model for each task (see Figure 2: Right), STTATTS utilizes a task fusion module with negligible number of additional parameters to handle multiple tasks using the same encoder-decoder backbone (See Figure 1). This results in a ~50% reduction in the total number of parameters required for the two tasks combined."}, {"title": "Underlying Architecture", "content": "The model is based on the SpeechT5 architecture, which comprises transformer encoder-decoder blocks and some auxiliary modules for modality-specific feature extraction and decoding. Specifically, the modal-specific encoder prenets, and modal-specific decoder pre/postnet modules, are used to handle the text and speech modalities at the input and output, while the main encoder/decoder network processes the unified representations."}, {"title": "Unified Encoder-Decoder", "content": "The unified encoder-decoder follows the transformer architecture (Vaswani et al., 2017). The transformer encoder in our base model has 12 blocks, with a model dimension of 768 and an inner feed-forward network dimension of 3072. The decoder comprises 6 transformer decoder blocks with a model dimension of 768 and an inner dimension of 3072. The unified encoder and decoder can process either latent representation of text or speech. We use the pre-trained encoder weights from SpeechT5 and ArTST for English and Arabic experiments, respectively."}, {"title": "Text Encoder and Decoder Pre/Post-Nets", "content": "The text encoder pre-net and text decoder pre/post-nets use shared embeddings. The pre-nets transform a token in the sequence to a 768 embedding vector. The text decoder post-net projects the decoder's hidden state into the probability distribution of tokens, normalized by the softmax function."}, {"title": "Speech Encoder and Decoder Pre/Post-Nets", "content": "The speech encoder pre-net is a convolutional feature extraction model with 6 1-dimensional convolutional layers, and GELU activation following wav2vec2.0 (Baevski et al., 2020). The output of the convolutional network is normalized before passing to a linear layer to up-sample from 512 to 768. The speech decoder pre-net upsamples the mel spectrogram to a 768-dimensional vector. It consists of two sequential layers of Linear transformations with ReLU activations, followed by an additional Linear layer that upsamples the output from the previous layers. To enable multi-speaker TTS, the speaker embedding vector of the target speaker extracted using x-vectors (Snyder et al., 2018) is concatenated to the output of the speech decoder pre-net before being down-sampled with a linear layer to the decoder hidden size followed by RELU activation.\nThe speech decoder post-net utilizes a linear layer to predict the log mel-filterbank from the decoder output and five 1-dimensional convolutional layers to generate a residual for enhancing the predicted mel (Ao et al., 2022). Another linear module is incorporated to transform the decoder output into a scalar to predict the stop token. Ao et al.'s (2022) HiFi-GAN vocoder is used to synthesize speech from the generated mel spectrogram."}, {"title": "Task Fusion Module", "content": "Each task is represented with a 128-dimensional vector, which is concatenated with the encoder's output, followed by a fully connected layer (See Figure 1) to project the learned representation back to the encoder embedding size. The output of this module is used as input to the decoder."}, {"title": "Multi-Task Loss Objective", "content": "For ASR, we use $L_{ce}$: the cross-entropy loss of the decoder and $L_{ctc}$: the standard CTC loss from ESPNet (Watanabe et al., 2018).\n$L_{asr} = L_{ce} + L_{ctc}$   (1)\nFor optimizing speech synthesis, we use the $L_1$ loss to minimize the distance between the target and generated mel spectrograms; the binary cross entropy $L_{bce}$ loss is used to predict the stop token for generation; and the guided attention loss is used to speed up training convergence for speech synthesis as described by Tachibana et al. (2018). The latter was added to speed up the training of TTS (Ao et al., 2022) which is typically a lot slower"}, {"title": "Experimental Settings", "content": "We conducted experiments across two languages: English and Arabic using open-source datasets. English was used as a resource-rich language, and trained using the benchmark LibriSpeech dataset; this enables the utilization of pre-trained models from SpeechT5 (Ao et al., 2022), and a direct comparison with their downstream models. Arabic served as a relatively low-resource language, mainly because of the shortage of clean speech data that can be utilized for training high-quality TTS systems (Kulkarni et al., 2023). This also enables using the pre-trained checkpoints from ArTST (Toyin et al., 2023)."}, {"title": "Datasets", "content": "English. We used LibriSpeech (Panayotov et al., 2015), referred to as LS for ASR. For TTS, we use LibriTTS (Zen et al., 2019), referred to as Ltts, along with LJSpeech (Ito and Johnson, 2017). We conducted experiments with varying training data sizes to evaluate the impact of data size on performance. \nArabic. We combined quality data for speech synthesis from two publicly available datasets: Arabic Speech Corpus (ASC) (Halabi, 2016), and Classical Arabic Text-to-Speech Corpus (ClArTTS) (Kulkarni et al., 2023) for both ASR and TTS. Since TTS quality is highly sensitive to the quality and consistency of the audios and annotations used for training, we did not utilize other large speech data for joint training . This also serves as a good"}, {"title": "Data Preparation", "content": "All punctuation marks were removed for both English and Arabic texts and all English characters were converted to lowercase letters. The standardized sampling rate for speech data across all collected datasets was 16 kHz. For Arabic, we trained the model for TTS with and without diacritics."}, {"title": "Training Details", "content": "The pre-trained checkpoints from SpeechT53 and ArTST4 were used as initialization weights for our experiments on English and Arabic, respectively. For reproducibility, we summarize training details per experiment in Table 2. All experiments were carried out on A100 GPUs with 80GB memory. Our training code and checkpoints are available."}, {"title": "Warm Fine-tuning", "content": "Toyin et al. (2023) discuss that in the low-resource TTS setting for Arabic, fine-tuning with larger ASR datasets first, followed by continual fine-tuning with high-quality TTS data improved synthesized speech quality. The resulting model achieved higher intelligibility even without"}, {"title": "Evaluation Metrics", "content": "ASR. We report the Word and Character Error rates (WER & CER) for evaluating our model's performance for ASR task. For Arabic, the texts are normalized before evaluation by removing diacritics. Diacritics are mostly useful as input for TTS models, but most ASR models are trained and evaluated without diacritics. We transcribe speech using CTC weight of 0.5.\nTTS. We employ objective metrics to evaluate our model's synthesized speech intelligibility. We"}, {"title": "Results", "content": "In this section, we report our model's performance, comparison against SpeechT5 single-task models (see Figure 2) and comparison against a baseline joint-task model, VoxLM. Table 3 shows the results for Arabic and English with various scales of training data. The performance improves with additional training data, approaching state-of-the-art results on the LibriSpeech test set. On Arabic, the high WER is attributed to the low-resource setting as it was trained with only 16 hours for ASR. Examples of ASR predictions from the Arabic model are shown in Figure 3."}, {"title": "Comparison with Single-Task Models", "content": "In Table 4, we compare our model with single task models from ArTST (Toyin et al., 2023) and SpeechT5 (Ao et al., 2022), which have the same pre-trained checkpoint and fine-tuning data, so they are directly comparable: we used the ens described in Table 1 for English and ar. STTATTS achieves comparable performance to the single-task models, demonstrating the effectiveness of the proposed multi-task methodology. Compared to the large-scale multi-lingual model, Whisper, STTATTS performs better than Whisper small, in spite of having smaller number of parameters and being trained on a much smaller data set. In English, Whisperlarge performs marginally better than STTATTS trained on ens. For Arabic, STTATTS performs substantially better than both small and large variants."}, {"title": "Comparison with Joint-Task Models", "content": "We compare our model's performance with reported joint models that perform both TTS and ASR in Table 5. In particular, we use VoxtLM (Maiti et al., 2024) as it is publicly available for direct comparison, while other models are not currently available to perform comparable evaluation.\nUsing the same training data, STTATTS performs better in both tasks, even when compared with the large VoxtLM model of 1.3B parameters. Compared to the VoxtLM base, which has a comparable number of parameters, STTATTS performs on a par with a fraction of the training data and is far better when an almost equal amount of data is used."}, {"title": "Ablations & Analysis", "content": ""}, {"title": "Task Scaling", "content": "To evaluate whether the model can be scaled to perform additional speech/text to speech/text tasks, we experiment with adding Voice Conversion in the enm setting. We use the CMU Arctic (Kominek and Black, 2004) dataset, and optimize voice conversion using the same loss function used for TTS (see Equation 3.3). We evaluate VC performance using CER and Speaker Similarity (SS) using the"}, {"title": "Architectural Variations", "content": "Y-Decoder. Yin Y-decoder stands for the desired output: either text or speech. Here, we use a similar approach to STTATTS but with modal-specific decoders, i.e., a separate text decoder and a speech decoder, while sharing the same encoder. In this model, no task fusion module is used since each task is parameterized by its own standalone decoder. This results in a larger model than STTATTS but still smaller than the disjoint ASR and TTS SpeechT5 models.\nMulti-Stage Training. We also experimented with multi-stage training, where we alternate updating the weights for a specific task with some model components frozen/unfrozen. We performed two experiments using STTATTS, where we update the parameters of the ASR first, followed by TTS, or vice versa, with the respective single-task loss function in each stage. For example, if we follow an ASR-first schedule, we first update the auxiliary weights, the encoder, and the decoder for the ASR task, then in the second stage, we keep the encoder and decoder frozen and update the auxiliary weights for the TTS task.\nTask-Specific Adapters. We experimented with a shared transformer encoder-decoder backbone using pre-trained weights with adapters (Houlsby et al., 2019) as an alternative architecture. We tried using the base model architecture offline and only updating the weights of an adapter for each task. We added a 64 inner dimension adapter to the decoder, but this approach didn't yield good results. For ASR, the WER was 150% and the TTS loss converged at a high value (\u2248 0.8) relative to STTATTS (\u2248 0.4).\nResults. Evaluation results are shown in Table 7. While performance on ASR is comparable across models and slightly better with the Y-decoder architecture, STTATTS is the best overall in terms of balancing performance across multiple tasks while maintaining a lower number of parameters. With multi-stage training, we find that it works better when we update the encoder weights for the ASR task first. We observe TTS performance is good whether the transformer encoder is optimized for ASR or TTS first, but the performance for ASR significantly degrades with \u00d710 difference between the WER for ASR-first and TTS-first approaches. This may be attributed to the dimensionality of the input features for each task, as ASR requires more computations to process the input in the encoder, while TTS works with discrete text input."}, {"title": "Effect of Data Imbalance", "content": "For ens, we first fine-tuned with LS-100 and Ltts-100 (which contains \u2248 58 hours of speech). This data combination resulted in less intelligible and robotic synthesized speech (MOS of 1.5). Interestingly, the ASR performance is not affected when we have more TTS data, as shown for ens in Table 3; on the contrary, the results improved from 5.61 to 4.84. As a result, and since TTS data are generally more scarce, our main experimental settings are all conducted with ASR data downsampled to match the size of the TTS training set."}, {"title": "Effect of Warm Fine-Tuning", "content": "We compare STTATTS's performance with and without the warm fine-tuning approach introduced described in section 6.4. Except for the Arabic ASR performance that is degraded by \u2248 2% absolute WER, we find that this approach results in improved performance."}, {"title": "Diacritization in Arabic", "content": "TTS models for Arabic are typically trained with full diacritics (Kulkarni et al., 2023). This is because diacritics contain essential information about most vowels, without which the text is highly ambiguous. However, Toyin et al. (2023) demonstrated good TTS performance without the inclusion of diacritics, which is mainly attributed to the warm fine-tuning they perform with ASR data. As the TTS data include diacritics, we performed experiments where we train models with and without diacritics. We evaluate ASR on normalized text where all diacritics are removed. The results are reported in Table 9. We notice that performance in TTS is in fact better without diacritics using this model. This surprising observation may be attributed to the fact that ArTST(Toyin et al., 2023) was pre-trained without diacritics, so adding diacritics in the fine-tuning stage with small data size may not be sufficient. However, as shown in the examples in Figure 3, we note that ASR transcription with diacritics (first line) is in fact correct, even though it does not match exactly the reference, which is mainly a result of the sukoon diacritic that is often omitted in the reference."}, {"title": "Effect of Task-Fusion Module Position", "content": "We experimented with having the task fusion module before the encoder with the aim of guiding latent feature extraction based on the given output. This approach performs fairly well for ASR (CER 4%) but fails for TTS with CER of 80%."}, {"title": "Effect of Pre-Trained Weights", "content": "The results above are all reported with pre-trained weights from SpeechT5 and ArTST pre-trained checkpoints. We examined the effect on performance with and without starting with these pre-trained weights. See Figure 4 for the learning curve in terms of loss reduction during training. We see that using pre-trained weights is beneficial for maximizing performance. Pre-training is particularly crucial for TTS tasks, as starting from scratch results in 10-fold degradation in CER. On the other hand, the performance for ASR is far less affected (only +1% increase in CER value for STTATTS, and a larger increase for Y-decoder). Overall, starting with pre-trained weights is crucial in downstream tasks for all model variations."}, {"title": "Discussion", "content": "We described our experiments of jointly learning Speech-to-Text and Text-to-Speech models based on the SpeechT5 architecture, resulting in a truly multi-modal and functional model, that accepts both speech and text as input and output. We experimented with both Arabic and English languages, with Arabic being a relatively low-resource language due to limited amounts of data available for TTS training. Our results show that while it is possible to train models using our framework with less than 20 hours of speech in total, more data is always better for maximizing performance in both ASR and TTS tasks. Our results are the first to report multi-task ASR/TTS in the Arabic language, showing promising results in low-resource settings, and potential for improvement with additional data.\nFor English, a few other models have been recently proposed; our comparative analysis with the only publicly available model of this variety, named VoxtLM (Maiti et al., 2024), favors STTATTS in both performance and parameter efficiency. We experimented with different parameter-efficient approaches to jointly learn ASR and TTS tasks and we find using the task-fusion module strikes a perfect balance in performance between both tasks with the least amount of parameters. It's also worth noting that the task-fusion module makes incorporating more tasks and output modalities feasible as the module aligns latent representation to match the desired output modality. Furthermore, the integrated multi-task approach, in addition to being more efficient in model size, is more efficient in training as it requires fewer updates in total. Future work will explore the possibility of integrating additional text output tasks within the framework and improving synthesized speech naturalness.\nWe believe that the proposed model, being trained on publicly available data with the code and checkpoints publicly available, can serve as a strong baseline for multi-task speech processing.\nLimitations. Our experiments focus on three key aspects: language generalization, scalability (in terms of training data requirements and tasks), and parameter efficiency. Although we explored two languages separately, we did not experiment with a joint multilingual models. Additionally, we used VoxLM as our only baseline for multi-task models. While other models have recently been proposed, their code and models are not yet publicly available for comparison. We did not explore increasing model size, as it would require pre-training from scratch, which is computationally expensive. However, slightly larger models could potentially enhance performance in the multi-task setting by better embedding the diverse input and output modalities. Finally, we found that subjective MOS evaluation was rather difficult to conduct as most outputs were intelligible but somewhat noisy and unnatural. For Arabic, the small data size and lack of diacritics does result in degraded intelligibility due to mismatched pronunciation of short vowels. Therefore, the reported TTS results provide some signal of quality, but may not be informative of the actual quality of the synthesized speech."}]}