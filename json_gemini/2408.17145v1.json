{"title": "TOWARDS HYPER-PARAMETER-FREE FEDERATED LEARNING", "authors": ["Geetika", "Drishya Uniyal", "Bapi Chatterjee"], "abstract": "The adaptive synchronization techniques in federated learning (FL) for scaled global model updates show superior performance over the vanilla federated averaging (FEDAVG) scheme. However, existing methods employ additional tunable hyperparameters on the server to determine the scaling factor. A contrasting approach is automated scaling analogous to tuning-free step-size schemes in stochastic gradient descent (SGD) methods, which offer competitive convergence rates and exhibit good empirical performance. In this work, we introduce two algorithms for automated scaling of global model updates. In our first algorithm, we establish that a descent-ensuring step-size regime at the clients ensures descent for the server objective. We show that such a scheme enables linear convergence for strongly convex federated objectives. Our second algorithm shows that the average of objective values of sampled clients is a practical and effective substitute for the objective function value at the server required for computing the scaling factor, whose computation is otherwise not permitted. Our extensive empirical results show that the proposed methods perform at par or better than the popular federated learning algorithms for both convex and non-convex problems. Our work takes a step towards designing hyper-parameter-free federated learning.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) refers to a framework for training machine learning (ML) models on a distributed system without exchanging data (McMahan et al., 2017). In the age of constraints on data centralization, it has gained popu-larity as a paradigm even for training large models (Jianyi Zhang et al., 2024). Often, the distributed system includes a node designated as a server which stores the global model \u2013 a synchronized state of the local models trained at peer nodes termed as clients. To reduce the cost of communication, it is standard that the clients perform local training for several gradient update steps before communicating with the server.\nFEDAVG, a basic FL scheme (McMahan et al., 2017), updates the global model to the average of the local models received from the available clients. The clients train their local models executing stochastic gradient descent (SGD) (Robbins and Monro, 1951) updates. FEDAVG suffers from heterogeneity in data distribution (T. Li et al., 2020), in addition to that in participation frequency of clients. It also underperforms in training deep models, such as attention models (Jingzhao Zhang et al., 2020), wherein SGD shows similar trends.\nMitigating the effects of heterogeneity primarily depends on synchronization between the optimization dynamics of clients and the trajectory of the global model. For this, FEDPROX (T. Li et al., 2020) introduces a proximal term in clients' objectives with respect to the global model. Similarly, SCAFFOLD (Karimireddy et al., 2020) introduces control variates at server and clients to check the client drifts. FEDDYN (Durmus et al., 2021) proposes an additional regularization term for clients' objectives similar to FEDPROX. However, beyond a modified local objective, FED-PROX, SCAFFOLD, FEDDYN, update the global model to an average of the local models received at a synchronization round.\nAdaptive scaling approaches, by contrast, conceptualize the server's model update \u2013 the difference between the model communicated to and the average of models received from available clients as a pseudo-gradient, and use it to run a step of first-order optimization on the global model. Exemplars include FedAdAGRAD, FedAdam, FEDYOGI (Reddi et al., 2021) methods, who use this pseudo-gradient for one-step of ADAGRAD (Duchi, Hazan, and Singer, 2011), ADAM (Kingma and Ba, 2015), and YOGI, (Zaheer et al., 2018), respectively. FEDAVG can be interpreted as a gradient descent process on the global model, utilizing the pseudo-gradient with a unit step-size. Essentially, the step-size of the one-step pseudo-gradient descent at the server is the scaling factor for the scaled global model update.\nThe proliferation of hyperparameters is an inherent characteristic of adaptive federated algorithms. For instance, (Reddi et al., 2021) (a) includes two momentum hyperparameters, which are generally robust across applications, (b) needs to tune the local and global step-sizes periodically, and (c) introduces an adaptivity hyperparameter, which they effortfully tune via extensive grid search and show that the convergence behavior heavily relies on it.\nThe practical performance of SGD heavily depends on its step-size (Schaul, S. Zhang, and LeCun, 2013). In fact, in many cases, SGD with well-tuned step-sizes generalizes better than ADAM for deep models (Wilson et al., 2017; Zhou et al., 2020). This observation serves as a key motivation for designing automated step-size tuning schemes for SGD as an alternative to the celebrated adaptive methods such as ADAM, ADAGRAD, and YOGI.\nTuning-free step-size schemes find an essential place in the journey of gradient-based optimization. Not long ago, Vaswani et al. (2019) proposed a line-search for SGD step-size applying a stochastic variant of classical Armijo scheme (Armijo, 1966). They proved that under an interpolation condition generally satisfied by models such as deep neural networks (C. Zhang et al., 2016), boosting (Bartlett et al., 1998), etc., SGD achieves the convergence rate of full batch gradient descent. Contemporarily, Berrada, Zisserman, and Kumar (2018) designed a scheme for setting up the SGD step-size based on an insight that an iteration of SGD with regularization can be formulated as minimization of a linear objective, which could be solved using Frank-Wolfe algorithm (Frank and Wolfe, 1956) in dual; they named it DEEP FRANK-WOLFE (DFW). Though DFW does not have a convergence theory, their performance and generalization on both convex and non-convex models are impressive.\nServer's step-size plays a significant role in determining the optimization trajectory of the federated model as seen in Malinovsky, Mishchenko, and Richt\u00e1rik (2023). They suggested that small step-sizes on clients reduce their drifts, and larger step-sizes on the server can offset the resulting slowdown. FEDEXP (Jhunjhunwala, S. Wang, and Joshi, 2023b) subsequently identified limitations in arbitrarily large server step-sizes and excessively small client step-sizes, introducing a global model update extrapolation method derived from projected convex optimization (Pierra, 1984). H. Li, Acharya, and Richtarik (2024) extended FEDEXP to incorporate proximal objectives on clients.\nAutomated scaling of global model updates clearly holds significant promise, particularly in light of recent find-ings on how server step-size influences federated learning processes (Malinovsky, Mishchenko, and Richt\u00e1rik, 2023). However, a key challenge in developing such a scheme lies in evaluating the global objective function. The federated learning paradigm, with its emphasis on client data privacy, precludes the server from directly computing the global objective. Consequently, we must seek a substitute, potentially utilizing the local objectives.\nThis work introduces new automated scaling techniques for the global model updates on the server of a federated learning system. We present the following two algorithms:\n1. Federated Line-search (FEDLI-LS): We establish that the tuning-free ARMIJO line-search on clients is directly translated into an automated scaled global model update on the orchestrating server. To elaborate, clients execute SGD updates using ARMIJO line search, whereas, the server performs an update on the global model using a weighted average of the differences in clients' model states. With that, we show that the global model will have a guaranteed descent in convex and strongly-convex cases.\nOften the clients also communicate their step-size together with the model. Inspired by (Malinovsky, Mishchenko, and Richt\u00e1rik, 2023), that a larger server-side step-size offsets the small client-side step-size, we heuristically selects the maximum value from the set of local step sizes to scale the global model update at the server.\n2. Federated Linearized Updates (FEDLI-LU): An optimal scaling factor for global model updates is computed by minimization of a loss-preserving linearization of federated objective. More specifically, clients perform SGD updates and communicate the objective function value along with the model. The server uses a weighted average of model state differences as a pseudo-gradient and weighted average of clients' function values as the pseudo-objective value to perform a DFW-like update to the global model.\nThe FEDLI algorithms communicate only one extra word, thus, incurring negligible extra communication overhead. FEDLI-LS ensures a global objective descent as a consequence of the guaranteed descents in local loss values. FEDLI-LU, conceptually, at every synchronization round, solves a pseudo linear approximation of the global objective using FRANK-WOLFE algorithm in dual. We exhibit the efficacy of both FEDLI methods on extensive image classification and language tasks with heterogeneity in clients' data."}, {"title": "Our contributions can be summed up as the following:", "content": "\u2022 We introduce a framework for automating the scaling factor for global model updates in federated learning. In this framework, we propose two practically efficient algorithms. (Section 3)\n\u2022 We prove linear convergence for strongly convex objectives and the standard sub-linear rates for convex and general non-convex problems for FEDLI-LS algorithm. (Section 5)\n\u2022 The proposed method FEDLI-LU introduces a formulation for the global model updates based on the solution of a new pseudo-linearization of the global objective, which results in effective empirical convergence across deep learning tasks. (Section 6)\n\u2022 We extensively demonstrate the efficacy of FEDLI methods on various deep learning tasks. (Section 7)"}, {"title": "2 System Model and Optimization Algorithm", "content": "We consider a federated learning system with a server and N clients/devices to train a model w \u2208 Rd, on which T number of synchronization rounds take place. We consider that St C [N] is the subset of devices sampled at the synchronization round t\u2208 [T] to perform local optimization. For simplicity, we take |St| = S \u2200 t\u2208 [T]. On this system, we aim to solve the following optimization problem\n$$min_{w\\in \\mathbb{R}^d} f(w) := \\frac{1}{N} \\Sigma_{i=1}^N f_i(w),$$\nwhere fi denotes the objective function of the i-th client for i \u2208 [N] and f is referred to as the global objective function.\nThe clients run an iterative stochastic gradient-based optimization (2) At the beginning of the tth round, each partici-pating client i \u2208 St stores identical local copies of the model wi\u2030 = wt and performs K local optimization steps to update it to \u03c9, as the following iteration:\n$$w_{i,k+1} = w_{i,k} \u2013 \u03b7_{i,k}g_{i,k}, for k \u2208 [K \u2212 1],$$\nwhere \u03b7ik is the step-size in the k-th local step. The stochastic gradient gik (w) is an unbiased estimator of the gradient of local objective \u2207 fi(w), i.e.\n$$E[g_{i,k}(w)] = \\nabla f_i(w)$$\nfor all t \u2208 [T], k \u2208 [K \u2212 1], and w\u2208 Rd."}, {"title": "2.1 Analytical Assumptions", "content": "Assumption 1 (Smoothness) The functions fi are L-smooth, i.e., for all x, y \u2208 Rd, it holds that\n$$f_i(y) \\le f_i(x) + \\nabla f_i(x)^T (y - x) + \\frac{L}{2} ||y - x||^2.$$\nIt is straightforward to prove that f as a sum of L-smooth functions is also L-smooth.\nAssumption 2 (Convexity) When needed, we specify that the functions fi are convex, i.e., for all x, y \u2208 Rd, it holds that\n$$f_i(y) \\ge f_i(x) + \\nabla f_i(x)^T (y \u2212 x).$$\nTherein, is straightforward to prove that f is also convex as the sum of convex functions.\nAssumption 3 (Strong- Convexity) When needed, we specify that the functions fi are \u00b5- strongly convex, i.e., for all x, y \u2208 Rd, it holds that\n$$f_i(y) \\ge f_i(x) + \\nabla f_i(x)^T (y - x) + \\frac{\\mu}{2} ||y - x||^2.$$\nTherein, is straightforward to prove that f is also \u00b5- strongly convex as the sum of \u00b5- strongly convex functions.\nAssumption 4 (Bounded Variance) We assume that the variance of gi.k(w) is bounded by a constant G, given as\n$$E[||g_{i,k}(w) \u2013 \\nabla f_i(w)||^2] \\le G.$$\nFor the result in non-convex cases, we assume that clients' objective functions are Lipschitz."}, {"title": "Assumption 5 (\u03b2- Lipschitz)", "content": "The functions fi are \u03b2-Lipschitz, i.e., for all x, y \u2208 Rd, it holds that\n$$||f_i(y) - f_i(x)|| \\le \u03b2||y \u2013 x||^2.$$\nMoreover, fi are \u03b2-Lipschitz for all x \u2208 Rd, it holds that ||\u2207fi|| < \u03b2. Using the sum of functions, f is also B-Lipschitz."}, {"title": "3 FEDLI Algorithms", "content": "Algorithm 1: A framework for FEDLI methods for a Federated Learning Server.\n1: initialize wo\n2: for each round t = 1, 2, ... do\n3: Server sends wt to all clients\n4: St\u2190 (random set of S clients);\n5: for each client i \u2208 St in parallel do\n6: (\u03c9,\u03ba,,\u03ba, \u03b4\u03b9,\u03ba) \u2190 CLIENTUPDATE(wt, initial-constants,client-algo);\n7: \u2206\u2190 wt \u2013 w\u0390, \u03ba; \u2206t = $\u2211ies, \u0394\u03af;\n8: fi \u2190 FUNCTIONSYNC(f,K,Server-algo);\n9: \u03b7 \u2190 STEPSIZESYNC(n,K,server-algo);\n10: Wt+1 \u2190 SERVERUPDATE(wt, fi, \u2206t, ni, server-algo);\nThe interface for the FEDLI algorithms is given as the pseudo-code in Algorithm 1. We start with selecting a random subset of clients and call CLIENTUPDATE method on them. The CLIENTUPDATE method runs SGD or it variant such as SGD with ARMIJO line-search scheme depending on the variable client-algo, see line 6. CLIENTUPDATE method returns the evaluated client's objective and its step-size at the last iteration in addition to the local model. The interface allows us to implement any model training optimization algorithm on the clients.\nTo implement FEDLI-LS, we set client-algo as Armijo-sgd, whereas to implement FEDLI-LU, we set it sgd. For a self-contained reading we have included Armi jo-sgd in Appendix A in the supplementary material.\nAfter a call to CLIENTUPDATE method, the server computes the global model state difference At, the pseudo-gradient, as in line 7 similar to (Reddi et al., 2021), (Jhunjhunwala, S. Wang, and Joshi, 2023b), (H. Li, Acharya, and Richtarik, 2024), etc. Additionally, the objective values and the step-sizes of the clients are synchronized at lines 8 and 9, respec-tively.\nStep-size and Objective Synchronization. Before calling SERVERUPDATE, the framework provides synchronization methods for the objective function values and step-sizes received from the clients. The method FUNCTIONSYNC and STEPSIZESYNC returns a global pseudo-objective and step-size depending on the method and objective class. For example, for FEDLI-LU algorithm, we do a weighted averaging of clients' objective values in FUNCTIONSYNC. For FEDLI-LS algorithm, a call to FUNCTIONSYNC is not required.\nFor convex problems, FEDLI-LS guarantees descent in the server's objective when we use a unit n, which is accord-ingly implemented in STEPSIZESYNC. For non-convex problems the server's step size remains tunable in FEDLI-LS, where we apply the heuristic in a call to STEPSIZESYNC to select the maximum of the clients' step-sizes. FEDLI-LU algorithm does not require STEPSIZESYNC as the step-size is computed in a SERVERUPDATE call.\nAlgorithm 2: The SERVERUPDATE method for FEDLI-LU.\nRequire: proximal coefficient \u03b7, weight-decay \u03bb, pseudo-gradient At, pseudo-objective f, model-state wt.\n1: rt = \u03bb\u03c9\u03c4\n2: \u03b3t = clipped to [0, 1]\n3: Return Wt+1 = Wt - n (rt + Ytt)"}, {"title": "SERVERUPDATE method implements one step of a gradient-based iterative optimization.", "content": "On passing GD as server-algo for implementing FEDLI-LS, it simply calls one step of gradient descent with step-size as nf and the pseudo-gradient \u2206t. We pass DFW as server-algo for implementing FEDLI-LU. The SERVERUPDATE method for FEDLI-LU is given in Algorithm 2, which we discuss further in Appendix A in the supplementary material."}, {"title": "4 Related Work", "content": "We discussed several federated learning algorithms in Section 1. Indeed, the landscape of FL algorithms is now rich, which also includes second-order model updates: FEDDANE (Tian Li et al., 2019) and FEDNEW (Elgabli et al., 2022); still, the first-order methods are popular for their low per-iteration costs. The implementation strategy our work is close to FEDOPT framework of (Reddi et al., 2021). MOON (Q. Li, B. He, and Song, 2021) and FEDPROTO (Tan et al., 2022) also communicate extra information in addition to the model from clients to the server though their objectives for this communication are different.\nIn (Malinovsky, Mishchenko, and Richt\u00e1rik, 2023), authors proposed incorporating a server step-size as a scaling factor, partial participation and client reshuffling. They derived convergence guarantees for strongly-convex, general convex and non-convex setting obtaining theoretical bounds on server step-size. A key insight of this work is that small client step-size and a large server step-size gives better convergence.\nIn recent work, (Jhunjhunwala, S. Wang, and Joshi, 2023a) presented both the theoretical and practical aspects of utilizing a scaled model update on the server. They applied a generalized gradient descent and derived the step-size by drawing an analogy between the over-parametrized federated setting and the process of finding projections on convex sets, using the adaptive relaxation coefficient in the Projections Onto Convex Sets (POCS) algorithm (Combettes, 1997). They assume an approximate projection in the federated learning context and motivate their server step-size based on the aggregated model state differences among clients. Their analysis shows that the distance between global iterates generated by FEDEXP and the global optimum is monotonically decreasing; however, this does not necessarily imply descent. For partial participation of devices in FEDEXP, computation of global step-size requires approximation of model state difference.\nBy contrast, FEDLI-LS, ensures descent for global objectives in the context of convex functions. Unlike FEDEXP, whose theoretical convergence has been examined in a deterministic full gradient setting, our analysis of FEDLI-LS incorporates both the stochastic nature of local gradient descent steps and the partial participation of clients. Empirically, the performance of FEDLI-LS and FEDLI-LU is on par with that of FEDEXP.\n(H. Li, Acharya, and Richtarik, 2024) further examine extrapolation with FEDPROX in Federated setting, using con-stant and adaptive extrapolation under partial participation. They explore two variants of adaptive extrapolation as a server step-size based on gradient diversity and stochastic Polyak step-size using the proximal operator and Moreau envelope, respectively. However, the theoretical guarantees are laid out for convex objectives under the interpolation regime, where \u2207 fi(x*) = 0, \u2200 i\u2208 [N].\nIn terms of theoretical guarantees, before this paper, two existing works offer linear convergence rates for strongly convex objectives: the FEDLIN algorithm (Mitra et al., 2021) and FEDEXPROX of (H. Li, Acharya, and Richtarik, 2024). FEDLIN achieves linear ergodic convergence \u2013 convergence of function of averaged model over iterates \u2013 for smooth and strongly convex objectives in the deterministic setting. In the stochastic setting, FEDLIN maintains a standard sublinear convergence even for strongly convex objectives. By contrast, our work demonstrates a linear convergence even in the stochastic setting with partial client participation. Moreover, our convergence is stronger convergence of squared norm of iterates' distance from the optimal. The convergence behaviour and rate for strongly convex objectives in FEDEXPROX (H. Li, Acharya, and Richtarik, 2024), by virtue of constant extrapolation for the server-side step size is linear. However, they do it under the stricter conditions of the interpolation regime with full participation. By contrast, our result in Theorem 2 is established under partial participation."}, {"title": "5 Convergence of FEDLI-LS", "content": "We denote the global model state after the t-th global round as wt, the local model states at client i for k-th local round is denoted by w\u00b2,k+1, where k \u2208 1, 2, . . ., K. Rewriting equation (2), the local SGD update is given by\n$$w_{i,k}^{t,k} = w_{i,k-1}^{t,k} - \u03b7_{i.k}g_i(w_{i,k-1}^{t,k}),$$\nwhere nk is tuned using ARMIJO line-search for local SGD updates. The Armijo search at local steps translates to a line search that minimizes the global model as shown in Lemma 6."}, {"title": "5.1 Convergence Theory for Convex Objectives", "content": "We now describe the convergence for convex functions.\nTheorem 1 Under the Assumption 1, 2 and 4, FEDLI-LS with $$max\\{\\frac{L\\eta_{lmax} (\u03b7_g+2K\u03c1)}{2\u03c1(2+L\\eta_{lmax} K)}, \\frac{(\u03b7_g + N_{max} LK)}{2\u03c1}\\} < c < 1$$ achieves the convergence rate\n$$E[f(\\overline{w}_T) - f(w^*)] \\le max \\{\\frac{1}{RT}, \\frac{1}{KT}\\} \\|w_0 - w^*\\|^2 \u2013 min \\{\\frac{1}{R'}, \\frac{1}{K'}\\} \\frac{1 - \u03c1^2}{\u03c1} \u03b7_{max}^2 K^2G,$$\nwhere $$R := \u03b7_g N_{lmax} K (2 - \\frac{L\u03b7_g N_{lmax}K}{2(1-c)\u03c1c}) and K := \u03b7_g N_{lmax} K (2 - \\frac{\u03b7_g}{\u03c1} - \\frac{N_{lmax} LK}{c}).$$\nThe proof of Theorem 1 is included in Appendix B. Theorem 1 shows a sublinear convergence for convex problems."}, {"title": "5.2 Convergence Theory for Strongly-convex Objectives", "content": "Theorem 2 Under the Assumption 1, 3 and 4, FEDLI-LS with $$max\\{\\frac{L\\eta_{lmax} (\u03b7_g+2K\u03c1)}{2\u03c1(2+L\\eta_{lmax} K)}, \\frac{(\u03b7_g + N_{max} LK)}{2\u03c1}\\} < c < 1$$ achieves the convergence rate\n$$E[\\|\u03c9\u03c4 \u2013 w^*\\|^2] < (1 \u2212 ngNlmax\u00b5K)T+1\\|wo \u2013 w^*\\|^2,$$\nwhere $$R := ngNlmax K (2 - \\frac{L\u03b7_g N_{lmax}K}{2(1-c)\u03c1c}) and K := ngNlmax K (2 - \\frac{\u03b7_g}{\u03c1} - \\frac{N_{lmax} LK}{c}).$$\nThe proof of Theorem 2 is included in the supplementary in Appendix B. Theorem 2 shows a linear convergence for strongly-convex problems."}, {"title": "5.3 Convergence Theory for Non-convex Objectives", "content": "Theorem 3 Under the Assumption 1, 4 and 5, FEDLI-LS achieves the convergence rate\n$$min_{t=0,...,T-1} E[\\|\u2207f(wt) \\|^2] < \\frac{2}{\u03b7_g N_{lmax} KT} E[f(w_0) \u2212 f(w_\u03c4)] + \\frac{LN_{lmax} K}{2} (\\frac{2K+1}{K^2}) (\\frac{G}{LM_{lmax}} + \u03b7_g)$$\nThe proof of Theorem 3 is included in Appendix B in the supplementary. Theorem 3 shows a sub-linear convergence for non-convex problems where the gradient is bounded."}, {"title": "6 Deep Frank-Wolfe for Global Model Update", "content": "Here we discuss the formation of the FEDLI-LU method. We provide this discussion for a self-contained reading. A reader can refer to (Berrada", "problem": "n$$w_{t+1"}, "underset{w\\in\\mathbb{R}^d}{argmin} \\{\\frac{1}{2\u03b7}\\|w \u2013 w_{t}\\|^2 + T(r(w_{t})) + T(f(O(w_{t}))) \\},$$\nwhere T(.) denotes Taylor's first order approximation and n is the proximal coefficient. Now, to preserve the geometry of the loss function, we formulate a loss-preserving linearization as the following:\n$$w_{t+1} = \\underset{w\\in\\mathbb{R}^d}{argmin} \\{\\frac{1}{2\u03b7}\\|w \u2013 w_{t}\\|^2 + T(r(w_{t})) + f(T(O(w_{t}))) \\},$$\nConsidering a convex and piecewise-linear loss function, such as multi-class hinge loss, the dual of the optimization problem 19 can be solved using the Frank-Wolfe method, which Berrada, Zisserman, and Kumar (2018) applied. With that, an optimal step size of the dual can be obtained as t \u2208 [0, 1"], "by": "n$$f_{hinge}(x, y) = \\underset{\u0177 \u2208 Y}{max} \\{x + (\u0177, y) \u2013 X_y\\}$$"}