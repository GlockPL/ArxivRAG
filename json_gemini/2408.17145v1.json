{"title": "TOWARDS HYPER-PARAMETER-FREE FEDERATED LEARNING", "authors": ["Geetika", "Drishya Uniyal", "Bapi Chatterjee"], "abstract": "The adaptive synchronization techniques in federated learning (FL) for scaled global model updates show superior performance over the vanilla federated averaging (FEDAVG) scheme. However, existing methods employ additional tunable hyperparameters on the server to determine the scaling factor. A contrasting approach is automated scaling analogous to tuning-free step-size schemes in stochastic gradient descent (SGD) methods, which offer competitive convergence rates and exhibit good empirical performance. In this work, we introduce two algorithms for automated scaling of global model updates. In our first algorithm, we establish that a descent-ensuring step-size regime at the clients ensures descent for the server objective. We show that such a scheme enables linear convergence for strongly convex federated objectives. Our second algorithm shows that the average of objective values of sampled clients is a practical and effective substitute for the objective function value at the server required for computing the scaling factor, whose computation is otherwise not permitted. Our extensive empirical results show that the proposed methods perform at par or better than the popular federated learning algorithms for both convex and non-convex problems. Our work takes a step towards designing hyper-parameter-free federated learning.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) refers to a framework for training machine learning (ML) models on a distributed system without exchanging data (McMahan et al., 2017). In the age of constraints on data centralization, it has gained popularity as a paradigm even for training large models (Jianyi Zhang et al., 2024). Often, the distributed system includes a node designated as a server which stores the global model \u2013 a synchronized state of the local models trained at peer nodes termed as clients. To reduce the cost of communication, it is standard that the clients perform local training for several gradient update steps before communicating with the server.\nFEDAVG, a basic FL scheme (McMahan et al., 2017), updates the global model to the average of the local models received from the available clients. The clients train their local models executing stochastic gradient descent (SGD) (Robbins and Monro, 1951) updates. FEDAVG suffers from heterogeneity in data distribution (T. Li et al., 2020), in addition to that in participation frequency of clients. It also underperforms in training deep models, such as attention models (Jingzhao Zhang et al., 2020), wherein SGD shows similar trends.\nMitigating the effects of heterogeneity primarily depends on synchronization between the optimization dynamics of clients and the trajectory of the global model. For this, FEDPROX (T. Li et al., 2020) introduces a proximal term in clients' objectives with respect to the global model. Similarly, SCAFFOLD (Karimireddy et al., 2020) introduces control variates at server and clients to check the client drifts. FEDDYN (Durmus et al., 2021) proposes an additional regularization term for clients' objectives similar to FEDPROX. However, beyond a modified local objective, FED-PROX, SCAFFOLD, FEDDYN, update the global model to an average of the local models received at a synchronization round."}, {"title": "2 System Model and Optimization Algorithm", "content": "We consider a federated learning system with a server and N clients/devices to train a model w \u2208 Rd, on which T number of synchronization rounds take place. We consider that St C [N] is the subset of devices sampled at the synchronization round t\u2208 [T] to perform local optimization. For simplicity, we take |St| = S \u2200 t\u2208 [T]. On this system, we aim to solve the following optimization problem\n$\\min _{w \\in \\mathbb{R}^{d}} f(w):=\\frac{1}{N} \\sum_{i=1}^{N} f_{i}(w),$                                                                               (1)\nwhere fi denotes the objective function of the i-th client for i \u2208 [N] and f is referred to as the global objective function.\nThe clients run an iterative stochastic gradient-based optimization (2) At the beginning of the tth round, each participating client i \u2208 St stores identical local copies of the model wi\u2030 = wt and performs K local optimization steps to update it to \\omega, t,K as the following iteration:\n$w_{i, k+1}=w_{i, k}-\\eta_{i, k} g_{i, k}, \\quad \\text { for } k \\in[K-1],$\n(2)\nwhere nik is the step-size in the k-th local step. The stochastic gradient gik (w) is an unbiased estimator of the gradient of local objective \u2207 fi(w), i.e.\n$\\mathbb{E}[g_{i, k}(w)]=\\nabla f_{i}(w)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (3)\nfor all t \u2208 [T], k \u2208 [K \u2212 1], and w\u2208 Rd.\nAssumption 1 (Smoothness) The functions fi are L-smooth, i.e., for all x, y \u2208 Rd, it holds that\n$f_{i}(y) \\leq f_{i}(x)+\\nabla f_{i}(x)^{T}(y-x)+\\frac{L}{2}\\|y-x\\|^{2}$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (4)\nIt is straightforward to prove that f as a sum of L-smooth functions is also L-smooth.\nAssumption 2 (Convexity) When needed, we specify that the functions fi are convex, i.e., for all x, y \u2208 Rd, it holds that\n$f_{i}(y) \\geq f_{i}(x)+\\nabla f_{i}(x)^{T}(y-x)$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        (5)\nTherein, is straightforward to prove that f is also convex as the sum of convex functions.\nAssumption 3 (Strong- Convexity) When needed, we specify that the functions fi are \u00b5- strongly convex, i.e., for all x, y \u2208 Rd, it holds that\n$f_{i}(y) \\geq f_{i}(x)+\\nabla f_{i}(x)^{T}(y-x)+\\frac{\\mu}{2}\\|y-x\\|^{2}$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (6)\nTherein, is straightforward to prove that f is also \u00b5- strongly convex as the sum of \u00b5- strongly convex functions.\nAssumption 4 (Bounded Variance) We assume that the variance of gi.k(w) is bounded by a constant G, given as\n$\\mathbb{E}[\\|g_{i, k}(w)-\\nabla f_{i}(w)\\|^{2}] \\leq G$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (7)\nFor the result in non-convex cases, we assume that clients' objective functions are Lipschitz."}, {"title": "3 FEDLI Algorithms", "content": "The interface for the FEDLI algorithms is given as the pseudo-code in Algorithm 1. We start with selecting a random subset of clients and call CLIENTUPDATE method on them. The CLIENTUPDATE method runs SGD or it variant such as SGD with ARMIJO line-search scheme depending on the variable client-algo, see line 6. CLIENTUPDATE method returns the evaluated client's objective and its step-size at the last iteration in addition to the local model. The interface allows us to implement any model training optimization algorithm on the clients.\nTo implement FEDLI-LS, we set client-algo as Armijo-sgd, whereas to implement FEDLI-LU, we set it sgd. For a self-contained reading we have included Armijo-sgd in Appendix A in the supplementary material.\nAfter a call to CLIENTUPDATE method, the server computes the global model state difference At, the pseudo-gradient, as in line 7 similar to (Reddi et al., 2021), (Jhunjhunwala, S. Wang, and Joshi, 2023b), (H. Li, Acharya, and Richtarik, 2024), etc. Additionally, the objective values and the step-sizes of the clients are synchronized at lines 8 and 9, respectively.\nStep-size and Objective Synchronization. Before calling SERVERUPDATE, the framework provides synchronization methods for the objective function values and step-sizes received from the clients. The method FUNCTIONSYNC and STEPSIZESYNC returns a global pseudo-objective and step-size depending on the method and objective class. For example, for FEDLI-LU algorithm, we do a weighted averaging of clients' objective values in FUNCTIONSYNC. For FEDLI-LS algorithm, a call to FUNCTIONSYNC is not required.\nFor convex problems, FEDLI-LS guarantees descent in the server's objective when we use a unit n, which is accordingly implemented in STEPSIZESYNC. For non-convex problems the server's step size remains tunable in FEDLI-LS, where we apply the heuristic in a call to STEPSIZESYNC to select the maximum of the clients' step-sizes. FEDLI-LU algorithm does not require STEPSIZESYNC as the step-size is computed in a SERVERUPDATE call."}, {"title": "4 Related Work", "content": "We discussed several federated learning algorithms in Section 1. Indeed, the landscape of FL algorithms is now rich, which also includes second-order model updates: FEDDANE (Tian Li et al., 2019) and FEDNEW (Elgabli et al., 2022); still, the first-order methods are popular for their low per-iteration costs. The implementation strategy our work is close to FEDOPT framework of (Reddi et al., 2021). MOON (Q. Li, B. He, and Song, 2021) and FEDPROTO (Tan et al., 2022) also communicate extra information in addition to the model from clients to the server though their objectives for this communication are different.\nIn (Malinovsky, Mishchenko, and Richt\u00e1rik, 2023), authors proposed incorporating a server step-size as a scaling factor, partial participation and client reshuffling. They derived convergence guarantees for strongly-convex, general convex and non-convex setting obtaining theoretical bounds on server step-size. A key insight of this work is that small client step-size and a large server step-size gives better convergence.\nIn recent work, (Jhunjhunwala, S. Wang, and Joshi, 2023a) presented both the theoretical and practical aspects of utilizing a scaled model update on the server. They applied a generalized gradient descent and derived the step-size by drawing an analogy between the over-parametrized federated setting and the process of finding projections on convex sets, using the adaptive relaxation coefficient in the Projections Onto Convex Sets (POCS) algorithm (Combettes, 1997). They assume an approximate projection in the federated learning context and motivate their server step-size based on the aggregated model state differences among clients. Their analysis shows that the distance between global iterates generated by FEDEXP and the global optimum is monotonically decreasing; however, this does not necessarily imply descent. For partial participation of devices in FEDEXP, computation of global step-size requires approximation of model state difference.\nBy contrast, FEDLI-LS, ensures descent for global objectives in the context of convex functions. Unlike FEDEXP, whose theoretical convergence has been examined in a deterministic full gradient setting, our analysis of FEDLI-LS incorporates both the stochastic nature of local gradient descent steps and the partial participation of clients. Empirically, the performance of FEDLI-LS and FEDLI-LU is on par with that of FEDEXP.\n(H. Li, Acharya, and Richtarik, 2024) further examine extrapolation with FEDPROX in Federated setting, using constant and adaptive extrapolation under partial participation. They explore two variants of adaptive extrapolation as a server step-size based on gradient diversity and stochastic Polyak step-size using the proximal operator and Moreau envelope, respectively. However, the theoretical guarantees are laid out for convex objectives under the interpolation regime, where \u2207 fi(x*) = 0, \u2200 i\u2208 [N].\nIn terms of theoretical guarantees, before this paper, two existing works offer linear convergence rates for strongly convex objectives: the FEDLIN algorithm (Mitra et al., 2021) and FEDEXPROX of (H. Li, Acharya, and Richtarik, 2024). FEDLIN achieves linear ergodic convergence \u2013 convergence of function of averaged model over iterates \u2013 for smooth and strongly convex objectives in the deterministic setting. In the stochastic setting, FEDLIN maintains a standard sublinear convergence even for strongly convex objectives. By contrast, our work demonstrates a linear convergence even in the stochastic setting with partial client participation. Moreover, our convergence is stronger \u2013 convergence of squared norm of iterates' distance from the optimal. The convergence behaviour and rate for strongly convex objectives in FEDEXPROX (H. Li, Acharya, and Richtarik, 2024), by virtue of constant extrapolation for the server-side step size is linear. However, they do it under the stricter conditions of the interpolation regime with full participation. By contrast, our result in Theorem 2 is established under partial participation."}, {"title": "5 Convergence of FEDLI-LS", "content": "We denote the global model state after the t-th global round as wt, the local model states at client i for k-th local round is denoted by w\u00b2,k+1, where k \u2208 1, 2, . . ., K. Rewriting equation (2), the local SGD update is given by\n$\\omega_{i, k}^{t}=\\omega_{i, k-1}^{t}-\\eta_{i, k} g_{i}(\\omega_{i, k-1}^{t}),$\nwhere nk is tuned using ARMIJO line-search for local SGD updates. The Armijo search at local steps translates to a line search that minimizes the global model as shown in Lemma 6."}, {"title": "6 Deep Frank-Wolfe for Global Model Update", "content": "Here we discuss the formation of the FEDLI-LU method. We provide this discussion for a self-contained reading. A reader can refer to (Berrada, Zisserman, and Kumar, 2018) for details. Consider a supervised learning task implemented in this federated setting with clients running SGD locally. For simplicity, we assume that only one local step is performed by each client at every synchronization round. Consider a sample j picked randomly by a client i \u2208 St in the tth synchronization round. Let the output of the deep learning model be denoted by O(wt) over which the loss function f(.) is applied. Consider a regularizer r(w) applied to the objective. Then we can see a global model update step for regularized objective as\n$w_{t+1}=w_{t}-\\eta\\left(\\nabla r\\left(w_{t}\\right)+\\nabla f\\left(O\\left(w_{t}\\right)\\right)\\right)$.\n(18)\nAs in (Berrada, Zisserman, and Kumar, 2018), using (Bubeck et al., 2015), equation 18 can be written as the following proximal problem:\n$w_{t+1}=\\underset{w \\in \\mathbb{R}^{d}}{\\operatorname{argmin}}\\left{\\frac{1}{2 \\eta}\\|w-w_{t}\\|^{2}+T\\left(r\\left(w_{t}\\right)\\right)+T\\left(f\\left(O\\left(w_{t}\\right)\\right)\\right)\\right},$\nwhere T(.) denotes Taylor's first order approximation and n is the proximal coefficient. Now, to preserve the geometry of the loss function, we formulate a loss-preserving linearization as the following:\n$w_{t+1}=\\underset{w \\in \\mathbb{R}^{d}}{\\operatorname{argmin}}\\left{\\frac{1}{2 \\eta}\\|w-w_{t}\\|^{2}+T\\left(r\\left(w_{t}\\right)\\right)+f\\left(T\\left(O\\left(w_{t}\\right)\\right)\\right)\\right},$\n(19)\nConsidering a convex and piecewise-linear loss function, such as multi-class hinge loss, the dual of the optimization problem 19 can be solved using the Frank-Wolfe method, which Berrada, Zisserman, and Kumar (2018) applied. With that, an optimal step size of the dual can be obtained as t \u2208 [0, 1]. Using that, the update rule can be formulated as\n$w_{t+1}=w_{t}-\\eta\\left(\\nabla r\\left(w_{t}\\right)+\\gamma_{t} \\nabla f\\left(O\\left(w_{t}\\right)\\right)\\right)$.\nNotice that, with the standard cross-entropy loss over neural networks, convergence of DFW algorithm is not established, though the empirical results establish an impressive convergence behaviour.\nHere, we include a discussion for the case where the step-size Yt Berrada, Zisserman, and Kumar (2018) is calculated during each iteration multi-class hinge loss function f. The multi-class hinge loss is given by:\n$f_{\\text {hinge }}(x, y)=\\max _{\\hat{y} \\in \\mathcal{Y}}\\left\\langle x+(\\hat{y}, y)-X_{y}\\right\\rangle$\n(20)"}, {"title": "7 Experiments and Numerical Results", "content": "Implementation: We implemented Algorithm 1 on FedML federated Learning framework (C. He et al., 2020). We compare FEDLI-LS and FEDLI-LU algorithms with FEDAVG (McMahan et al., 2017), FEDPROX (T. Li et al., 2020), SCAFFOLD (Karimireddy et al., 2020), and FEDADAM (Reddi et al., 2021) and FedExP (Jhunjhunwala, S. Wang, and Joshi, 2023a) (incorporated in FedML). The default hyperparameters as mentioned in papers were taken for competing algorithms. We compute mini-batch gradients on clients with a fixed batch size of 32 per task. We sample 10 clients per round out of a total of 100 clients. Clients are sampled uniformly at random without replacement in each round"}, {"title": "8 Conclusion", "content": "This paper presents a framework FEDLI for federated learning where clients communicate the step-size and local objective values in addition to the model at every synchronization round. The communicated values are synchronized alongside the model and used to efficiently scale the global model updates. An interesting result presented in this work shows that if the local optimizers on the clients ensure descent, the global updates will follow that for convex objectives. This result enables various line search techniques to be incorporated in our framework without losing their convergence guarantees in the federated setting. Note that, for such schemes where descent is guaranteed, extra computation comes as an inherent character of the algorithm, which is then translated in to the federated algorithm as well; this can be seen in our FEDLI-LS algorithm.\nWe presented the convergence rates for convex, strongly-convex, and non-convex objectives and showed that for strongly-convex objectives it achieve a linear rate, which is available in only a couple of previous works."}, {"title": "APPENDIX", "content": ""}, {"title": "A Model Update Algorithms", "content": ""}, {"title": "A.1 ARMIJO Line Search", "content": "For a self-contained reading, here we include the stochastic gradient descent algorithm with stochastic ARMIJO line-search (Vaswani et al., 2019). Algorithm 3 gives pseudo-code for SGD with ARMIJO line-search where c is the ARMIJO condition constant, \u03b2 and b are multiplicative factors for decreasing the step size, d is the factor used for scaling the step size at each line-search step, Nlmax is an upper bound on the step size. The stochasticity in the line-search scheme is due to the fact that at every step it has to satisfy the descent condition (22), where the function and its gradient are based on a sample minibatch (potentially could be of size 1) of data.\nDefinition 2 (Armijo Condition Restatement) For the k-th step in the t-th communication round, the Armijo condition for the local objective functions fi with a constant c > 0 is given by\n$f_{i}(\\omega_{i, k}^{t})-f_{i}(\\omega_{i, k-1}^{t}) \\leq-c \\eta_{i, k}\\|g_{i}(\\omega_{i, k-1}^{t})\\|^{2}$.\n(22)"}, {"title": "A.2 DEEP FRANK-WOLFE Algorithm", "content": "Dual direction in FEDLI-LU FEDLI-LU extends the DFW optimization scheme (Berrada, Zisserman, and Kumar, 2018) to a federated learning context. Berrada, Zisserman, and Kumar (2018) discuss computation of an optimal step-size Yt in closed-form using a feasible direction in dual st. More specifically, since the linearized objective is given as:\n$\\omega_{t+1}=\\underset{w \\in \\mathbb{R}^{d}}{\\operatorname{argmin}}\\left{\\frac{1}{2 \\eta}\\|w-w_{t}\\|^{2}+T\\left(r\\left(w_{t}\\right)\\right)+f\\left(T\\left(O\\left(w_{t}\\right)\\right)\\right)\\right}$.\nThe dual of the above equation is given by:\n$\\max _{\\alpha \\in \\mathcal{P}}\\left{\\frac{1}{2 \\eta}\\left\\|A \\alpha\\right\\|^{2}+b \\alpha\\right}$,\nwhere A = (na\u1ef9) \u2208 Rp\u00d7|Y| s.t. \u0177 \u2208 Y, P = {\u03b1 \u2208 RP|\u03a3\u03c8\u03b5\u03bd \u03b1\u1ef9 = 1}, a\u1ef9 = \u2207r(w)|wo + \u2207 fhinge, (W)|wo , \u2207 fhinge,y (W)|wo and b = fhinge\",\u300f(wo) - fhinger,y (wo) + \u2206(\u1ef9, y). Recall that Y is the space of labels of the training dataset of the supervised learning task. Taking s \u2208 P as a dual direction enables computing an optimal step-size of the DFW algorithm as discussed in Appendix A.2 and A.3 of (Berrada, Zisserman, and Kumar, 2018). We adopted the derivation to our server-side model update in FEDLI-LU as\n$\\gamma_{t}=\\frac{-\\eta \\Delta r_{t}+T_{s_{t}} \\dagger_{0}}{\\eta\\left\\|\\Delta_{t}\\right\\|^{2}}$.\nWhen the loss is cross-entropy, Berrada, Zisserman, and Kumar (2018) propose that the gradient of cross-entropy loss in the primal g gives the feasible directions \u2208 P in dual such that g = -As; see Appendix A.6 in (Berrada, Zisserman, and Kumar, 2018). Computing the softmax of the vector of scores gives the feasible direction in the dual as Scross-entropy \u2208 P by the property of softmax function, as all its components are non-negative and add up to 1, fulfilling the simplex constraints inherent in P."}, {"title": "B Proofs", "content": "Lemma 5 Restatement of Lemma 2 Armijo line search, when applied at each client i, determines the step-size nt,k constrained to lie in (0, Nlmax], that is bounded below as follows:\n$\\eta_{t, k}^{i} \\geq \\min \\left{\\frac{2(1-c)}{L}, \\eta_{\\max }^{i}\\right},$\nwhere 0 < c < 1 is a constant associated with the Armijo condition.\nThe proof below assumes that the function evaluation at a data point is L-smooth. We have considered a constant L instead of a different smoothness constant for different data points at each client for simplicity."}]}