{"title": "DeepGate3: Towards Scalable Circuit Representation Learning", "authors": ["Zhengyuan Shi", "Ziyang Zheng", "Sadaf Khan", "Jianyuan Zhong", "Min Li", "Qiang Xu"], "abstract": "Circuit representation learning has shown promising results in\nadvancing the field of Electronic Design Automation (EDA). Ex-\nisting models, such as DeepGate Family, primarily utilize Graph\nNeural Networks (GNNs) to encode circuit netlists into gate-level\nembeddings. However, the scalability of GNN-based models is fun-\ndamentally constrained by architectural limitations, impacting their\nability to generalize across diverse and complex circuit designs. To\naddress these challenges, we introduce DeepGate3, an enhanced\narchitecture that integrates Transformer modules following the\ninitial GNN processing. This novel architecture not only retains\nthe robust gate-level representation capabilities of its predecessor,\nDeepGate2, but also enhances them with the ability to model subcir-\ncuits through a novel pooling transformer mechanism. DeepGate3\nis further refined with multiple innovative supervision tasks, sig-\nnificantly enhancing its learning process and enabling superior\nrepresentation of both gate-level and subcircuit structures. Our\nexperiments demonstrate marked improvements in scalability and\ngeneralizability over traditional GNN-based approaches, establish-\ning a significant step forward in circuit representation learning\ntechnology.", "sections": [{"title": "1 INTRODUCTION", "content": "Pre-training scalable representation learning models and subse-\nquently fine-tuning them for diverse downstream tasks has emerged\nas a transformative paradigm in artificial intelligence (AI). This evo-\nlution is facilitated by the expansion of training data [1], allowing\nthese models not only to enhance performance on existing tasks\nbut also to demonstrate exceptional capabilities in novel applica-\ntions. For instance, large language models like GPT-4[2], T5[3] and\nRoberta [4] demonstrate near-human proficiency in text compre-\nhension and generation, inspiring similar methodologies in other\ndomains such as computer vision [5] and graph analysis [6].\nIn the realm of Electronic Design Automation (EDA), the quest\nfor effective circuit representation learning has also garnered sig-\nnificant attention. Established approaches such as the DeepGate\nfamily [7-9], Gamora [10], and HOGA [11] employ Graph Neural\nNetworks (GNNs) to translate circuit netlists into gate-level em-\nbeddings, successfully supporting a variety of downstream EDA\ntasks. Despite their notable successes, the scalability of GNN-based\nmodels is fundamentally constrained by architectural limitations.\nThat is, GNN-based models, while adept at handling structured\ndata, fail to align with traditional scaling laws [12]. Merely increas-\ning the size of training datasets does not proportionately enhance\nperformance, a limitation substantiated by recent findings [13].\nMoreover, the discriminative capacity of GNNs is often insufficient;\ntheir inherent message-passing mechanism can result in informa-\ntion distortion as messages traverse long paths within large graphs,\nleading to challenges in distinguishing between similar graph struc-\ntures [14, 15]. This limitation is highlighted by the difficulties faced\nby common GNN architectures such as GCN [16] and GraphSAGE\n[17], which struggle to differentiate similar graph structures [18].\nThis introduces a critical challenge: How can we scale circuit repre-\nsentation models to effectively leverage large amounts of training data\nwhile continuing to improve model performance and generalization\ncapabilities?\nInspired by the success of Transformer\u00b9 -based graph learning\nmodels [21-24], we propose DeepGate3, an enhancement over the\nprevious circuit representation learning model, DeepGate2, by inte-\ngrating Transformer blocks with GNN. This integration marries the\nflexible representation capabilities of GNNs with the scalability and\nrobustness of Transformers, offering a scalable solution for netlist\nrepresentation learning.\n\u2022 Architecture: DeepGate3 utilizes the pre-trained DeepGate2\nas the backbone, extracting initial gate-level embeddings as\ntokens that capture both the relative position and global\nfunctionality of logic gates. To further enhance these em-\nbeddings, DeepGate3 employs a component known as the\nRefine Transformer (RT), which refines the initial embeddings\nand captures the intricate long-term correlations between\ntokens. This process ensures a deeper and more nuanced\nunderstanding of circuit dynamics compared to DeepGate2.\n\u2022 Pooling Mechanism: In contrast to other circuit learning\nmodels like Gamora [10] and HOGA [11], which use average\npooling functions to obtain circuit-level embeddings, Deep-\nGate3 introduces a so-called Pooling Transformer (PT) block.\nSpecifically, we use a special [CLS] token that aggregates\ninformation between the inputs and outputs of a circuit, serv-\ning as the graph-level embedding of the circuit and allowing\nDeepGate3 to detect and emphasize minor yet significant\ndifferences across the circuit more effectively than its prede-\ncessors.\n\u2022 Pre-training Strategy: DeepGate3 is pre-trained using a\ncomprehensive set of tasks that operate at multiple levels\nof circuit analysis. At the gate level, it predicts the logic-1\nprobability under random simulation and assesses pair-wise\ntruth table similarity, consistent with DeepGate2 [8]. At the\ncircuit level, the model selects gates randomly and extracts\nfan-in cones as separate circuit graphs. It then predicts inher-\nent features of individual cones and quantifies similarities\nbetween pairs of cones based on their graph-level embed-\ndings. Importantly, all labels used in these pre-training tasks\nare generated through logic simulation and circuit analy-\nsis, without relying on artificial annotations, ensuring the\nauthenticity and applicability of the training process."}, {"title": "2 RELATED WORK", "content": "Circuit representation learning is increasingly recognized as a cru-\ncial area in the EDA field, reflecting broader trends in AI that empha-\nsize learning general representations for a variety of downstream\ntasks. Within this context, the DeepGate family [7, 8] emerges as\na pioneering approach, employing GNNs to map circuit netlists\ninto graph forms and learn gate-level embeddings. The initial ver-\nsion, DeepGate [7], focuses on converting arbitrary circuit netlists\ninto And-Inverter Graphs (AIGs), using the logic-1 probability de-\nrived from random simulation for model supervision. An evolved\niteration, DeepGate2 [8], enhances this model by learning disen-\ntangled structural and functional embeddings, enabling it to refine\ngate-level representation by distinguishing between structural and\nfunctional similarities (see Figure 1), thereby supporting diverse\nEDA tasks including testability analysis [25], power estimation [9],\nand SAT solving [26, 27]. Recently, the Gamora model [10] has\nfurther advanced the field by showcasing an enhanced reasoning\ncapability through the representation of both logic gates and cones."}, {"title": "2.1 Circuit Representation Learning", "content": "Circuit representation learning is increasingly recognized as a cru-\ncial area in the EDA field, reflecting broader trends in AI that empha-\nsize learning general representations for a variety of downstream\ntasks. Within this context, the DeepGate family [7, 8] emerges as\na pioneering approach, employing GNNs to map circuit netlists\ninto graph forms and learn gate-level embeddings. The initial ver-\nsion, DeepGate [7], focuses on converting arbitrary circuit netlists\ninto And-Inverter Graphs (AIGs), using the logic-1 probability de-\nrived from random simulation for model supervision. An evolved\niteration, DeepGate2 [8], enhances this model by learning disen-\ntangled structural and functional embeddings, enabling it to refine\ngate-level representation by distinguishing between structural and\nfunctional similarities (see Figure 1), thereby supporting diverse\nEDA tasks including testability analysis [25], power estimation [9],\nand SAT solving [26, 27]. Recently, the Gamora model [10] has\nfurther advanced the field by showcasing an enhanced reasoning\ncapability through the representation of both logic gates and cones.\nDespite these advancements, the application of GNNs in circuit\nrepresentation poses notable challenges, primarily concerning scal-\nability and generalizability [13]. The HOGA model [11] attempts to\naddress these issues by introducing a novel message-passing mech-\nanism and an efficient training strategy. However, the inherent\nlimitations of the GNN framework, such as difficulty in managing\nlong-range dependencies and susceptibility to over-smoothing [28]\nand over-squashing [14], continue to restrict the scalability and\nadaptability of these models [18]."}, {"title": "2.2 Transformer-Based Graph Learning", "content": "Addressing the limitations of GNNs [16, 17, 29], recent research\nhas pivoted towards incorporating Transformer architectures into\ngraph learning, capitalizing on their renowned ability for handling\nlong-range dependencies through the self-attention mechanism.\nGraph-BERT [21] replaces traditional aggregation operators with\nself-attention to enhance information fidelity across nodes. Simi-\nlarly, Graphormer [22] extends the standard Transformer architec-\nture to graph data, demonstrating superior performance across a\nvariety of graph-level prediction tasks. Moreover, GROVER [23]\nillustrates the potential of Transformers to capture deep structural\nand semantic details in molecular graphs. Recent initiatives like\nOpenGraph [24] aim to develop scalable graph foundation mod-\nels that excel in zero-shot learning tasks across diverse datasets,\nshowcasing the broad applicability of this approach.\nInspired by these advancements, our work explores the integra-\ntion of Transformer technology into circuit representation learning.\nDeepGate3 harnesses the strengths of both GNNs and Transform-\ners, aiming to create a robust framework capable of overcoming the\ntraditional limitations of graph learning in the context of circuit\nnetlists. This approach promises not only enhanced scalability and\ngeneralization but also superior performance in capturing complex\ninterdependencies within circuit data."}, {"title": "3 METHODOLOGY", "content": "The framework of DeepGate3 is illustrated in Figure 2. Initially, the\ninput circuit in And-Inverter Graph (AIG) format is processed using\nthe pre-trained DeepGate2 model, capturing context and structural\ninformation for each logic gate (see Section 3.2). Next, the gate-level\nembeddings serving as tokens are processed through the Refine\nTransformers (RTs). RT improves the embedding sequences by cap-\nturing the pair-wise interactions, which is elaborated in Section 3.3.\nSubsequently, we introduce a Transformer-based pooling function\nin Section 3.4, named Pooling Transformer (PT). PT aggregates\ninformation from the refined gate-level embeddings to learn graph-\nlevel embeddings. Section 3.5 provides details on the pre-training\ntasks. Moreover, to handle large practical circuits, we provide an\nefficient fine-tuning strategy for DeepGate3 in Section 3.6."}, {"title": "3.1 Overview of DeepGate3", "content": "The framework of DeepGate3 is illustrated in Figure 2. Initially, the\ninput circuit in And-Inverter Graph (AIG) format is processed using\nthe pre-trained DeepGate2 model, capturing context and structural\ninformation for each logic gate (see Section 3.2). Next, the gate-level\nembeddings serving as tokens are processed through the Refine\nTransformers (RTs). RT improves the embedding sequences by cap-\nturing the pair-wise interactions, which is elaborated in Section 3.3.\nSubsequently, we introduce a Transformer-based pooling function\nin Section 3.4, named Pooling Transformer (PT). PT aggregates\ninformation from the refined gate-level embeddings to learn graph-\nlevel embeddings. Section 3.5 provides details on the pre-training\ntasks. Moreover, to handle large practical circuits, we provide an\nefficient fine-tuning strategy for DeepGate3 in Section 3.6."}, {"title": "3.2 DeepGate2 as Tokenzier", "content": "The Transformer-based circuit learning model requires the tok-\nenization of the circuit graph into a sequence. However, directly\nconverting the circuit into a sequence of gates solely based on their\ngate type and topological order leads to a loss of valuable infor-\nmation. This limitation is particularly evident in AIG, where there"}, {"title": "3.3 Refine Transformer", "content": "Positional embedding. Given initialized functional embedding\nsequence HF = [hf1, hf2, ..., hfn] and structural embedding HS =\n[hs1, hs2, ..., hsn] sequence acquired by DeepGate2, we propose to\nuse two independent Transformers to get refined functional embed-\nding and structural embedding, respectively. However, a problem\nlies: How to design a positional embedding to represent the order\ninformation of the gate embedding sequence?\nDifferent from data like natural language, which are inherently\nsequential, the gates in AIGs does not have a clear order. Existing\ngraph sequentialization methods like topological order and canoni-\ncal order cannot differentiate the isomorphic graphs [30], making\nit impractical to rely on explicit order as positional embeddings.\nTo tackle this problem, we propose to use implicit embedding as\npositional embedding.\n\u2022 For functional embedding, we use corresponding structural\nembedding as positional embedding to identify AIGs with\nthe same function but different structure, e.g. Circuit 1 and\nCircuit 2 in Figure 1.\n\u2022 Similarly, for structural embedding, we use corresponding\nfunctional embedding as positional embedding. This can\nalso help identify AIGs with similar structure but greatly\ndifferent function, as illustrated by the example of Circuit 1\nand Circuit 3 in Figure 1.\nFanin-fanout Cone. While the Transformer model has the ca-\npability to aggregate global information from an AIG, it should\nbe denoted that not all the information is equally useful for the\nrepresentation learning. As shown in Figure 3, the functionality of\nGate 4 is directly impacted by its predecessors, namely Gate 1 and\nGate 3, while Gates 6 to 9 are influenced by Gate 4. Conversely, the\nremaining Gate 0, 3, 5 neither affect nor are affected by the gate\nunder consideration. Subsequently, we identify the fanin and fanout\ncones for each gate. For Gate 4, we create the Attention Mask matrix\nin Figure 3, following the masking approach in Transformer [19].\nWithin this matrix MASK, \"Available\" denotes the corresponding"}, {"title": "3.4 Pooling Transformer", "content": "DeepGate3 aims to learn not only gate level embeddings but also\nsub-graph level embeddings. To obtain sub-graph level embeddings,\nwe utilize a lightweight Transformer, referred to as the Pooling\nTransformer (PT).\nWe first extract a set of sub-graphs S from the circuits based on\nl-hop cones. The construction of a sub-graph involves randomly\nselecting a logic gate within the circuit and extracting its l-order\npredecessors to form the sub-graph. The resulting 1-hop sub-graph\nencompasses a maximum of 2\u00b9\u22121 PIs and 2\u00b9 \u2013 1 gates. In our default\nsettings, we assign number of hops as 1 = 4. It is worth noting that\nthe order of the PIs can influence the truth table of a sub-graph. To\nensure uniformity, we reorder the indexes of nodes based on the\ncanonical labels generated by Nauty tool [31].\nFunctional Pooling Transformer. For AIGs, the function of a\nl-hop cone is directly linked to its truth table. Therefore, we design\na pre-training task that predicts the truth table of a sub-graph\nusing only its PIs and PO. However, the variable number of PIs\nin a sub-graph and the fixed length of the truth table remain a\nchallenge. During the model pre-training, we fix the truth table\nlength to 64, which corresponds to a sub-circuit with 6 inputs and\n1 output. During the model inference, PT can process circuits with\nany number of PIs and POs.\nTo handle sub-graphs with different numbers of inputs, we in-\ntroduce three special learnable tokens: [Don't care], [Zero], and\n[One]. These tokens help adjust the length of the truth table as\nneeded. As illustrated in Figure 4, adjustments are as follows:\n\u2022 Case 1. When there are fewer than 6 PIs, [Don't care]\ntokens are added to increase the count to 6 and expand the\ntruth table accordingly.\n\u2022 Case 2. When there are more than 6 PIs, some PIs are replaced\nrandomly with [Zero] or [One] to adjust the truth table.\nSimilar to NLP techniques, we introduce a special learnable token\n[CLS] at the beginning of the embedding sequence to aggregate\ninformation from a sub-graph. The order of PIs is crucial for pre-\ndicting the truth table, so we use learnable positional embeddings,\nfollowing the method used by BERT [32]."}, {"title": "3.5 Model Pre-training", "content": "Gate-level Pre-training Tasks. DeepGate3 model is pre-trained\nwith a series of tasks in both gate-level and graph-level. To disen-\ntangle the functional and structural embeddings, we employ pre-\ntraining tasks with different labels to supervise the corresponding\ncomponents.\nRegarding function-related tasks at the gate-level, we incorporate\nthe pre-training tasks from DeepGate2, which involve predicting\nthe logic-1 probability of gates and predicting the pair-wise truth\ntable distance. We sample the gate pairs Ngate_tt and record the\ncorresponding simulation response as incomplete truth table Ti.\nThe pair-wise truth table distance $D^{gate}_{tt}$ is calculated using the\nfollowing formula:\n$D^{gate}_{tt} = \\frac{HammingDistance(T_i, T_j)}{length(T_i)}$  (i, j) \u2208 Ngate_tt\nThe loss functions for gate-level functional pre-training tasks are\ndepicted as below. In DeepGate3, multiple Multi-Layer Perceptron\n(MLP) heads are utilized to readout embeddings.\n$L^{gate}_{prob} = L1Loss(p_k, MLP_{prob}(h^f_k))$, k \u2208 V\n$L^{gate}_{tt\\_pair} = L1Loss(D^{gate}_{tt}, MLP^{gate}_{tt}(h^f_i, h^f_j))$, (i, j) \u2208 Ngate_tt\nIn addition, we incorporate supervisions for structural learning\nby predicting logic levels and the pair-wise connections. The pre-\ndiction of pairwise connections is treated as a classification task,\nwhere a sampled gate pair (i, j) \u2208 Ngate_con can be classified into\nthree categories: gate i can propagate to gate j across edges, gate j\ncan propagate to gate i across edges, or gate i cannot reach gate j\nat any time. We list the loss functions as below, where Leuk is the\nlogic level of gate k.\n$L^{gate}_{lev} = L1Loss(Lev_k, MLP_{lev}(h^s_k))$, k \u2208 V\n$L^{gate}_{con} = BCELoss(MLP_{con}(h^s_i, h^s_j))$, (i, j) \u2208 Ngate_con\nGraph-level Pre-training Tasks. To ensure an adequate quantity\nof high-quality training samples, we construct graph-level supervi-\nsions using the sub-graphs extracted from the original netlists, i.e.,\nthe extracted 1-hop sub-graphs elaborated in Section 3.4."}, {"title": "3.6 Fine-tuning on Large AIGs", "content": "Since the computational complexity of self-attention mechanism\ngrows quadratically with the sequence length, there is a practical\nlimitation on the maximum sequence length in Transformer block.\nHowever, real circuits typically contain a large number of gates,\nfar exceeding the maximum sequence length of 512 in our Refine\nTransformer. One approach to address this limitation is to spar-\nsify the attention matrix by restricting the field of view to fixed,\npredefined patterns such as local windows and block patterns of\nfixed strides [34-39]. Similarly, we introduce a window-shifting\nmethod to facilitate the fine-tuning of the model on the large AIG.\nSpecifically, RT calculates the self-attention of the nodes within a\nshifting window. This shifting window slides by one step at each\niteration, allowing the RT to capture the entire large AIG.\nLarge Circuit Partition. The partitioned circuits covered by the\nshifting window are shown in Figure 5. Initially, we focus on gather-\ning all the l-hop cones that terminate in logic level 1. These cones are\nsubsequently merged, forming an area denoted as area\u2081. Moving\nforward, we continue collecting and merging cones with output\ngates situated in level 1 + 8. Here, & represents the level gap between\ntwo distinct areas. It is important to note that the chosen value of\n8 must be smaller than I in order to guarantee an overlap between\nthe two areas. The aforementioned process is repeated iteratively\nuntil the partitioned areas cover the entire circuit. Furthermore, we\nimpose a constraint wherein the maximum number of gates within\nan area is limited to 512. If the number of gates in any given area\nexceeds the limitation, it will be further divided into small areas.\nWindow-Shifting Pipeline. As depicted in Algorithm 1, given an\nAIG with the partitioned area {area1, area2, ..., aream}, we first\ntokenize the entire AIG into embedding sequences HF and HS.\nThen, we acquire the embedding sequences {Hf1, Hf2, ..., Hfm}\nand {Hs1, Hs2,..., Hsm} for each area. The embeddings in area\nareat covering by the shifting window are then fed into the pre-trained Refine Transformer to obtain the refined embeddings H ft\nand Hst. Subsequently, the window move forward to the next area\nareat+1. As there is overlap among the areas, we adopt an average\nupdating strategy, computing the embedding of an overlapping\ngate as the average of the corresponding gate embeddings. Finally,\nthese embeddings are passed to the fine-tuning task-specific heads\nfor various downstream tasks."}, {"title": "4 EXPERIMENTS", "content": "We collect the circuits for pre-training from various sources, in-cluding benchmark netlists in ITC99 [40], IWLS05 [41], EPFL [42],\nand synthesizable register-transfer level (RTL) designs from Open-Core [43] and Github [44]. All designs are transformed into AIG\nnetlists by ABC tool [45]. If the original netlists are too large, we ran-\ndom partition them into small circuits, ensuring that the maximum\nnumber of nodes in each circuit remains below 512. Furthermore,\nwe augment the dataset by synthesizing netlists using various logic\nsynthesis recipes. The statistical details of our pre-training dataset\ncan be found in Table 2, which comprises a total of 67, 905 circuits.\nNote that we pre-train DeepGate3 on these small circuits, and for\ndownstream tasks, we transfer DeepGate3 to large circuits."}, {"title": "4.1 Data Preparation", "content": "We collect the circuits for pre-training from various sources, in-cluding benchmark netlists in ITC99 [40], IWLS05 [41], EPFL [42],\nand synthesizable register-transfer level (RTL) designs from Open-Core [43] and Github [44]. All designs are transformed into AIG\nnetlists by ABC tool [45]. If the original netlists are too large, we ran-\ndom partition them into small circuits, ensuring that the maximum\nnumber of nodes in each circuit remains below 512. Furthermore,\nwe augment the dataset by synthesizing netlists using various logic\nsynthesis recipes. The statistical details of our pre-training dataset\ncan be found in Table 2, which comprises a total of 67, 905 circuits.\nNote that we pre-train DeepGate3 on these small circuits, and for\ndownstream tasks, we transfer DeepGate3 to large circuits."}, {"title": "4.2 Implementation Details", "content": "The dimensions of both the structural embedding hs and the func-tional embedding hf are set to 128. The depth of Refine Transformer\nis 12 and the depth of Pooling Transformer is 3. All pre-training\ntask heads are 3-layer multilayer perceptrons (MLPs). For pair-wise\ntasks in Section 3.5, we concatenate their embeddings as the input\nto the pre-training task head.\nWe pre-train our model with tasks described in Section 3.5 for\n500 epochs to ensure convergence. This pre-training is performed\nwith a batch size of 128 on 8 Nvidia A800 GPUs. We utilize the\nAdam optimizer with a learning rate of 10-4."}, {"title": "4.3 Evaluation Metric", "content": "To better assess the performance of our model, we utilize several\nmetrics as follows.\nOverall Loss. To calculate the overall loss Lall, we simply sum up\nall the losses as Eq. (9).\nError of Truth Table Prediction. Given the 6-input sub-graph s\nin test dataset S' with special token in Section 3.4, we predict the\n64-bit truth table based on the graph-level functional embedding\nhfs. The prediction error is calculated by the Hamming distance\nbetween the prediction and ground truth.\n$p^{tt} = \\frac{1}{len(S')} \\Sigma_{S'} HammingDistance(T_s, MLP_{tt} (h^f_s))$\nAccuracy of Gate Connection Prediction. Given the structural\nembedding of the gate pair (i, j) in test dataset Neon and binary\nlabel ycon \u2208 {0, 1, 2}, we predict the gate connection and define\nthe accuracy as follows:\n$p^{con} = \\frac{1}{len(N^{'}con)} \\Sigma_{N^{'}_{con}} 1 (y^{con}_{(i,j)}, MLP_{con} (h^s_i, h^s_j))$\nAccuracy of Gate-in-Graph Prediction. Given the gate-graph\npair (k, s) in test dataset Nin, we predict whether the gate is in-\ncluded by the sub-graph with the gate structural embedding hsk\nand the sub-graph structural embedding hss. The binary label is\nnoted as yhs) uin = {0,1}.\n$p^{in} = \\frac{1}{len (N^{'}in)} \\Sigma_{N^{'}_{in}} 1(MLP_{in} (h^s_k, h_s), y^{in}_{(k,s)})$"}, {"title": "4.4 Data Scalability of DeepGate3", "content": "The data scalability, indicating that the model performance contin-ues to increase as the training data expands, has been demonstrated"}, {"title": "4.5 Ablation Study", "content": "In this section, we perform ablation studies on the primary com-ponents of DeepGate3, namely the Refine Transformer (RT) and\nthe Pooling Transformer (PT), following the metrics outlined in\nSection 4.3. For DeepGate3 without the RT (DG3 w/o RT), we use\nthe embedding of DeepGate2 and PT to perform graph-level pre-training tasks. For DeepGate3 without the PT (DG3 w/o PT), we\nreplace PT with average pooling. For DeepGate3 without both RT\nand PT (DG3 w/o RT & PT), we only use the embedding of Deep-Gate2 and average pooling.\nEffectiveness of Refine Transformer. We evaluated the perfor-mance difference between DeepGate3 and DeepGate2, focusing on\nthe inclusion of the PT. As demonstrated in Table 3, when compar-ing to DG3 w/o RT, DeepGate3 shows enhancements in Pcon and\npin by 7.56% and 12.79% respectively, and a reduction in overall\nloss by 0.4501, while maintaining comparable performance in Ptt.\nSimilarly, DG3 w/o PT outperforms DG3 w/o RT & PT in terms of\nLall, Ptt, Pcon, and Pin, with improvements of 0.7182, 0.167, 6.46%,\nand 9.57% respectively. These results highlight the critical role of\nthe Refine Transformer in refining initial embeddings and capturing\ncomplex, long-term correlations between gates.\nEffectiveness of Pooling function. Prior work on graph-level\ntasks primarily utilized average or max pooling, such as Gamora [10]\nand HOGA [11], which often fails to capture circuit-specific infor-mation and struggles to perform well under varying conditions.\nOur proposed Pooling Transformer addresses this by incorporating\nspecial tokens to account for diverse circumstances. According to\nthe results in Table 3, the inclusion of the Pooling Transformer\nsignificantly enhances Lall, Ptt, and pin in DeepGate3 by 0.6480,0.0494, and 7.04% respectively. Note that the Pooling Transformer"}, {"title": "4.6 Downstream Task: SAT Solving", "content": "The Boolean Satisfiability (SAT) problem is a fundamental problem\nthat determines whether a Boolean formula can output logic-1 with\nat least one variable assignment. It is the first proven NP-complete\nproblem [50] and serves as a foundational problem applicable to\nvarious fields, including scheduling, planning and verification. The\nprevalent SAT solvers opt for the conflict-driven clause learning\n(CDCL) as the backbone solving algorithm. CDCL efficiently han-dles searching path conflicts and effectively explores additional\nconstraints to reduce searching space. Over the past decades, nu-merous heuristic designs [51-54] have been proposed to accelerate\nCDCL process in SAT solvers.\nSolving heuristic Design. [51] introduces a variable decision\nheuristic that assigns the opposite value to a pair of correlated\nvariables, aiming to intentionally cause conflicts rapidly. While\nthis approach achieves notable speed improvements compared to\ntraditional SAT solvers, it still relies on time-consuming logic sim-ulation to acquire the necessary functional relation between vari-ables. DeepGate2 [8] proposes an alternative solution by utilizing\nfunctional embeddings to measure the similarity between vari-ables based on their functionality. Similarly, we can leverage the\ngate-level embeddings to predict functional similarity and identify\ncorrelated variables.\nThe variable decision heuristic pipeline is shown in Figure 7.\nGiven a SAT instance, the first step is to obtain gate-level functional\nembeddings prior to solving. During the variable decision process,\na decision value di is assigned to variable vi. If another variable\nvj with an assigned value dj is identified as correlated to vi, the\nreversed value d'; is assigned to vi, i.e., d\u2081 = 0 if dj = 1 or di =\n1 if dj = 0. The determination of correlated variables relies on their\nfunctional similarity, as defined in Eq. (13), where the similarity\nSim(vi, vj) exceeding the threshold e indicates correlation.\n$Sim(v_i,v_j) = MLP_{gate\\_tt} (h^f_{v_i}, h^f_{v_j})$\nExperiment Settings. We utilize the CaDiCal [55] SAT solver\nas the backbone solver and modify the variable decision heuris-tic based on it. In the Baseline setting, SAT problems are directly\nsolved using the backbone SAT solver. In the DeepGate3 setting,our model is further fine-tuned efficiently on large-scale circuits inour proposed shifting-window manner (see Section 3.6). We finetune DeepGate3 for 200 epochs only with the pair-wise truth tabledistance prediction task, i.e., optimizing the loss function $L^{gate}_{tt\\_pair}as Eq. (4). Then, we enhance the variable decision heuristic byincorporating gate-level embeddings produced by fine-tuned Deep-Gate3. Additionally, we also employ the embeddings obtained byDeepGate2 for comparison. The test cases, denoted as C1-C5, are col-lected from industrial logic equivalence checking (LEC) problems.These cases exhibit diverse levels of solving complexity, showcasinga range of challenges encountered in practical scenarios.Results. Table 4 presents a runtime comparison among the Base-line, DeepGate2, and DeepGate3 settings, where the runtime re-duction compared to the Baseline setting is denoted as Red. andthe number of gates in each circuit case is represented as # Gates.Based on the observations from the table, we can draw three mainconclusions. First, both DeepGate2 and DeepGate3 capture thefunctional correlation of variables to guide variable decision, resulting in reduced solving time compared to the Baseline setting.On average, DeepGate2 achieves a runtime reduction of 14.92%,and DeepGate3 achieves average reduction of 50.46%. However,it is worth noting that the heuristic-based solving strategy is less\neffective for easier cases, as the model inference process accountsfor a significant portion of the total runtime. Secondly, DeepGate3demonstrates its capability to handle large-scale circuits, as the LECtest cases contain more than 10K gates, significantly surpassingthe size of circuits in the pre-training dataset (with an average of225.29 gates). Thirdly, DeepGate3 exhibits superior performancecompared to DeepGate2 in this task, indicating that DeepGate3captures more informative gate-level functional embeddings. Inconclusion, our model demonstrates effective generalization abilityto solve practical SAT solving problems."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced DeepGate3, a pioneering framework\nthat leverages the synergy between GNNs and Transformers to ad-\ndress the scalability of circuit representation learning. DeepGate3's\ninnovative architecture, featuring Refine Transformer and Pooling\nTransformer mechanisms, significantly enhances the scalability\nand generalization capabilities of circuit representation learning.\nMoreover, the introduction of multiple novel supervision tasks hasenriched the learning process, allowing DeepGate3 to capture abroader range of circuit behaviors with higher fidelity. Our experi-mental results demonstrate that DeepGate3 not only outperformsits predecessors DeepGate2 but also sets new benchmarks in han-dling complex and large-scale circuit designs efficiently. Futurework will focus on incorporating additional data types, such astemporal and operational conditions, to enrich model insights andexpanding DeepGate3's applications within EDA tasks."}]}