{"title": "DeepGate3: Towards Scalable Circuit Representation Learning", "authors": ["Zhengyuan Shi", "Ziyang Zheng", "Sadaf Khan", "Jianyuan Zhong", "Min Li", "Qiang Xu"], "abstract": "Circuit representation learning has shown promising results in advancing the field of Electronic Design Automation (EDA). Existing models, such as DeepGate Family, primarily utilize Graph Neural Networks (GNNs) to encode circuit netlists into gate-level embeddings. However, the scalability of GNN-based models is fundamentally constrained by architectural limitations, impacting their ability to generalize across diverse and complex circuit designs. To address these challenges, we introduce DeepGate3, an enhanced architecture that integrates Transformer modules following the initial GNN processing. This novel architecture not only retains the robust gate-level representation capabilities of its predecessor, DeepGate2, but also enhances them with the ability to model subcircuits through a novel pooling transformer mechanism. DeepGate3 is further refined with multiple innovative supervision tasks, significantly enhancing its learning process and enabling superior representation of both gate-level and subcircuit structures. Our experiments demonstrate marked improvements in scalability and generalizability over traditional GNN-based approaches, establishing a significant step forward in circuit representation learning.", "sections": [{"title": "1 INTRODUCTION", "content": "Pre-training scalable representation learning models and subsequently fine-tuning them for diverse downstream tasks has emerged as a transformative paradigm in artificial intelligence (AI). This evolution is facilitated by the expansion of training data [1], allowing these models not only to enhance performance on existing tasks but also to demonstrate exceptional capabilities in novel applications. For instance, large language models like GPT-4[2], T5[3] and Roberta [4] demonstrate near-human proficiency in text comprehension and generation, inspiring similar methodologies in other domains such as computer vision [5] and graph analysis [6].\nIn the realm of Electronic Design Automation (EDA), the quest for effective circuit representation learning has also garnered significant attention. Established approaches such as the DeepGate family [7-9], Gamora [10], and HOGA [11] employ Graph Neural Networks (GNNs) to translate circuit netlists into gate-level embeddings, successfully supporting a variety of downstream EDA tasks. Despite their notable successes, the scalability of GNN-based models is fundamentally constrained by architectural limitations.\nThat is, GNN-based models, while adept at handling structured data, fail to align with traditional scaling laws [12]. Merely increasing the size of training datasets does not proportionately enhance performance, a limitation substantiated by recent findings [13]. Moreover, the discriminative capacity of GNNs is often insufficient; their inherent message-passing mechanism can result in information distortion as messages traverse long paths within large graphs, leading to challenges in distinguishing between similar graph structures [14, 15]. This limitation is highlighted by the difficulties faced by common GNN architectures such as GCN [16] and GraphSAGE [17], which struggle to differentiate similar graph structures [18].\nThis introduces a critical challenge: How can we scale circuit representation models to effectively leverage large amounts of training data while continuing to improve model performance and generalization capabilities?\nInspired by the success of Transformer\u00b9 -based graph learning models [21-24], we propose DeepGate3, an enhancement over the previous circuit representation learning model, DeepGate2, by integrating Transformer blocks with GNN. This integration marries the flexible representation capabilities of GNNs with the scalability and robustness of Transformers, offering a scalable solution for netlist representation learning.\n\u2022 Architecture: DeepGate3 utilizes the pre-trained DeepGate2 as the backbone, extracting initial gate-level embeddings as tokens that capture both the relative position and global functionality of logic gates. To further enhance these embeddings, DeepGate3 employs a component known as the Refine Transformer (RT), which refines the initial embeddings and captures the intricate long-term correlations between tokens. This process ensures a deeper and more nuanced understanding of circuit dynamics compared to DeepGate2.\n\u2022 Pooling Mechanism: In contrast to other circuit learning models like Gamora [10] and HOGA [11], which use average pooling functions to obtain circuit-level embeddings, DeepGate3 introduces a so-called Pooling Transformer (PT) block. Specifically, we use a special [CLS] token that aggregates information between the inputs and outputs of a circuit, serving as the graph-level embedding of the circuit and allowing DeepGate3 to detect and emphasize minor yet significant differences across the circuit more effectively than its predecessor.\n\u2022 Pre-training Strategy: DeepGate3 is pre-trained using a comprehensive set of tasks that operate at multiple levels of circuit analysis. At the gate level, it predicts the logic-1 probability under random simulation and assesses pair-wise truth table similarity, consistent with DeepGate2 [8]. At the circuit level, the model selects gates randomly and extracts fan-in cones as separate circuit graphs. It then predicts inherent features of individual cones and quantifies similarities between pairs of cones based on their graph-level embeddings. Importantly, all labels used in these pre-training tasks are generated through logic simulation and circuit analysis, without relying on artificial annotations, ensuring the authenticity and applicability of the training process."}, {"title": "2 RELATED WORK", "content": "2.1 Circuit Representation Learning\nCircuit representation learning is increasingly recognized as a crucial area in the EDA field, reflecting broader trends in AI that emphasize learning general representations for a variety of downstream tasks. Within this context, the DeepGate family [7, 8] emerges as a pioneering approach, employing GNNs to map circuit netlists into graph forms and learn gate-level embeddings. The initial version, DeepGate [7], focuses on converting arbitrary circuit netlists into And-Inverter Graphs (AIGs), using the logic-1 probability derived from random simulation for model supervision. An evolved iteration, DeepGate2 [8], enhances this model by learning disentangled structural and functional embeddings, enabling it to refine gate-level representation by distinguishing between structural and functional similarities (see Figure 1), thereby supporting diverse EDA tasks including testability analysis [25], power estimation [9], and SAT solving [26, 27]. Recently, the Gamora model [10] has further advanced the field by showcasing an enhanced reasoning capability through the representation of both logic gates and cones.\nDespite these advancements, the application of GNNs in circuit representation poses notable challenges, primarily concerning scalability and generalizability [13]. The HOGA model [11] attempts to address these issues by introducing a novel message-passing mechanism and an efficient training strategy. However, the inherent limitations of the GNN framework, such as difficulty in managing long-range dependencies and susceptibility to over-smoothing [28] and over-squashing [14], continue to restrict the scalability and adaptability of these models [18].\n2.2 Transformer-Based Graph Learning\nAddressing the limitations of GNNs [16, 17, 29], recent research has pivoted towards incorporating Transformer architectures into graph learning, capitalizing on their renowned ability for handling long-range dependencies through the self-attention mechanism. Graph-BERT [21] replaces traditional aggregation operators with self-attention to enhance information fidelity across nodes. Similarly, Graphormer [22] extends the standard Transformer architecture to graph data, demonstrating superior performance across a variety of graph-level prediction tasks. Moreover, GROVER [23] illustrates the potential of Transformers to capture deep structural and semantic details in molecular graphs. Recent initiatives like OpenGraph [24] aim to develop scalable graph foundation models that excel in zero-shot learning tasks across diverse datasets, showcasing the broad applicability of this approach.\nInspired by these advancements, our work explores the integration of Transformer technology into circuit representation learning. DeepGate3 harnesses the strengths of both GNNs and Transformers, aiming to create a robust framework capable of overcoming the traditional limitations of graph learning in the context of circuit netlists. This approach promises not only enhanced scalability and generalization but also superior performance in capturing complex interdependencies within circuit data."}, {"title": "3 METHODOLOGY", "content": "3.1 Overview of DeepGate3\nThe framework of DeepGate3 is illustrated in Figure 2. Initially, the input circuit in And-Inverter Graph (AIG) format is processed using the pre-trained DeepGate2 model, capturing context and structural information for each logic gate (see Section 3.2). Next, the gate-level embeddings serving as tokens are processed through the Refine Transformers (RTs). RT improves the embedding sequences by capturing the pair-wise interactions, which is elaborated in Section 3.3. Subsequently, we introduce a Transformer-based pooling function in Section 3.4, named Pooling Transformer (PT). PT aggregates information from the refined gate-level embeddings to learn graph-level embeddings. Section 3.5 provides details on the pre-training tasks. Moreover, to handle large practical circuits, we provide an efficient fine-tuning strategy for DeepGate3 in Section 3.6.\n3.2 DeepGate2 as Tokenzier\nThe Transformer-based circuit learning model requires the tokenization of the circuit graph into a sequence. However, directly converting the circuit into a sequence of gates solely based on their gate type and topological order leads to a loss of valuable information. This limitation is particularly evident in AIG, where there are only two gate types. Despite the limited diversity of gate types, each gate holds valuable contextual information, highlighting the need for a domain-specific tokenization approach.\nWe propose to utilize DeepGate2 as a tokenizer, which embeds distinct spatial and functional features of logic gates. However, the original DeepGate2 assumes that each PI has a 0.5 probability of being logic-1, which is inadequate for handling circuits under arbitrary workloads. We re-train the DeepGate2 model with specific workloads, i.e., we randomly initialize the probability $p_i$ of each PI $i$. Therefore, such mechanism allows model to learn workload-aware embeddings, enhancing its ability to provide more informative and adaptable representations. Aside from this modification, DeepGate2 retains its original setting.\nThe input circuit graph is denoted as $G = (V, \\mathcal{E})$, where $V = [v_1, v_2, ..., v_n]$ represents the set of vertices, and $\\mathcal{E}$ represents the set of edges. To initialize the functional embeddings of gate $i$, where $i$ belongs to PI $I$, we assign a repeated value of $p_i$ to each dimension for a total of $d$ times, i.e., $h_{fi} = \\text{Repeat}(p_i, d)$, $i \\in I$.\nNext, the structural embeddings of PIs are assigned a set of randomly generated but orthogonal vectors. The remaining gate embeddings are all initialized to zero. Formally, DeepGate2 tokenizes the circuit graph $G$ into two sequences of gate-level token embeddings: structural embeddings HS and functional embeddings HF as follows:\n$HS, HF = \\text{DeepGate2}(G, p)$,\n$HS = [h_{s1}, h_{s2}, ..., h_{sn}]$\n$HF = [h_{f1}, h_{f2}, ..., h_{fn}]$     (1)\n3.3 Refine Transformer\nPositional embedding. Given initialized functional embedding sequence $HF = [h_{f1}, h_{f2}, ..., h_{fn}]$ and structural embedding $HS = [h_{s1}, h_{s2}, ..., h_{sn}]$ sequence acquired by DeepGate2, we propose to use two independent Transformers to get refined functional embedding and structural embedding, respectively. However, a problem lies: How to design a positional embedding to represent the order information of the gate embedding sequence?\nDifferent from data like natural language, which are inherently sequential, the gates in AIGs does not have a clear order. Existing graph sequentialization methods like topological order and canonical order cannot differentiate the isomorphic graphs [30], making it impractical to rely on explicit order as positional embeddings. To tackle this problem, we propose to use implicit embedding as positional embedding.\n\u2022 For functional embedding, we use corresponding structural embedding as positional embedding to identify AIGs with the same function but different structure, e.g. Circuit 1 and Circuit 2 in Figure 1.\n\u2022 Similarly, for structural embedding, we use corresponding functional embedding as positional embedding. This can also help identify AIGs with similar structure but greatly different function, as illustrated by the example of Circuit 1 and Circuit 3 in Figure 1.\nFanin-fanout Cone. While the Transformer model has the capability to aggregate global information from an AIG, it should be denoted that not all the information is equally useful for the representation learning. As shown in Figure 3, the functionality of Gate 4 is directly impacted by its predecessors, namely Gate 1 and Gate 3, while Gates 6 to 9 are influenced by Gate 4. Conversely, the remaining Gate 0, 3, 5 neither affect nor are affected by the gate under consideration. Subsequently, we identify the fanin and fanout cones for each gate. For Gate 4, we create the Attention Mask matrix in Figure 3, following the masking approach in Transformer [19]. Within this matrix MASK, \"Available\" denotes the corresponding tokens will participate in attention computation, whereas \"Masked\" means the opposite. In other words, only the gates within the fanin or fanout cones are involved in the attention computation process.\nTransformer Architecture. As shown in Figure 2, DeepGate3 includes two independent Refine Transformers (denoted as RTs and RTF) for learning functional and structural information, respectively. As described before, we use HS and HF as the positional embedding. Therefore, we get the refined embedding as follows:\n$H_F = RT_f(H_F + H_S, MASK)$\n$H_S = RT_S(H_S + H_F, MASK)$  (2)\n3.4 Pooling Transformer\nDeepGate3 aims to learn not only gate level embeddings but also sub-graph level embeddings. To obtain sub-graph level embeddings, we utilize a lightweight Transformer, referred to as the Pooling Transformer (PT).\nWe first extract a set of sub-graphs S from the circuits based on l-hop cones. The construction of a sub-graph involves randomly selecting a logic gate within the circuit and extracting its l-order predecessors to form the sub-graph. The resulting 1-hop sub-graph encompasses a maximum of $2^{l-1}$ PIs and $2^l - 1$ gates. In our default settings, we assign number of hops as $l = 4$. It is worth noting that the order of the PIs can influence the truth table of a sub-graph. To ensure uniformity, we reorder the indexes of nodes based on the canonical labels generated by Nauty tool [31].\nFunctional Pooling Transformer. For AIGs, the function of a l-hop cone is directly linked to its truth table. Therefore, we design a pre-training task that predicts the truth table of a sub-graph using only its PIs and PO. However, the variable number of PIs in a sub-graph and the fixed length of the truth table remain a challenge. During the model pre-training, we fix the truth table length to 64, which corresponds to a sub-circuit with 6 inputs and 1 output. During the model inference, PT can process circuits with any number of PIs and POs.\nTo handle sub-graphs with different numbers of inputs, we introduce three special learnable tokens: [Don't care], [Zero], and [One]. These tokens help adjust the length of the truth table as needed. As illustrated in Figure 4, adjustments are as follows:\n\u2022 Case 1. When there are fewer than 6 PIs, [Don't care] tokens are added to increase the count to 6 and expand the truth table accordingly.\n\u2022 Case 2. When there are more than 6 PIs, some PIs are replaced randomly with [Zero] or [One] to adjust the truth table.\nSimilar to NLP techniques, we introduce a special learnable token [CLS] at the beginning of the embedding sequence to aggregate information from a sub-graph. The order of PIs is crucial for predicting the truth table, so we use learnable positional embeddings, following the method used by BERT [32].\nStructural Pooling Transformer. Unlike the functional counterpart, the Structural Pooling Transformer focuses on the structural information which is related to all gates within a sub-graph. Therefore, structural Pooling Transformer leverages all gates in a sub-graph, along with the [CLS] token, to obtain a structural pooling embedding. Similar to functional pooling Transformer, the structural version also uses learnable positional embeddings.\n3.5 Model Pre-training\nGate-level Pre-training Tasks. DeepGate3 model is pre-trained with a series of tasks in both gate-level and graph-level. To disentangle the functional and structural embeddings, we employ pre-training tasks with different labels to supervise the corresponding components.\nRegarding function-related tasks at the gate-level, we incorporate the pre-training tasks from DeepGate2, which involve predicting the logic-1 probability of gates and predicting the pair-wise truth table distance. We sample the gate pairs $N_{gate\\_tt}$ and record the corresponding simulation response as incomplete truth table $T_i$. The pair-wise truth table distance $D_{gate\\_tt}$ is calculated using the following formula:\n$D_{gate\\_tt}^{i,j} = \\frac{\\text{HammingDistance}(T_i, T_j)}{\\text{length}(T_i)} (i, j) \\in N_{gate\\_tt} (3)$\nThe loss functions for gate-level functional pre-training tasks are depicted as below. In DeepGate3, multiple Multi-Layer Perceptron (MLP) heads are utilized to readout embeddings.\n$L^{gate}_{prob} = L1Loss(p_k, MLP_{prob} (h_{fk})), k \\in V$\n$L^{gate}_{tt\\_pair} = L1Loss(D_{gate\\_tt}^{i,j}, MLP_{gate\\_tt} (h_{fi}, h_{fj})), (i, j) \\in N_{gate\\_tt}$  (4)\nIn addition, we incorporate supervisions for structural learning by predicting logic levels and the pair-wise connections. The prediction of pairwise connections is treated as a classification task, where a sampled gate pair $(i, j) \\in N_{gate\\_con}$ can be classified into three categories: gate i can propagate to gate j across edges, gate j can propagate to gate i across edges, or gate i cannot reach gate j at any time. We list the loss functions as below, where $Lev_k$ is the logic level of gate k.\n$L^{gate}_{lev} = L1Loss(Lev_k, MLP_{lev}(h_{sk})), k \\in V$\n$L^{gate}_{conn} = BCELoss(MLP_{con}(h_{si}, h_{sj})), (i, j) \\in N_{gate\\_con}$  (5)\nGraph-level Pre-training Tasks. To ensure an adequate quantity of high-quality training samples, we construct graph-level supervisions using the sub-graphs extracted from the original netlists, i.e., the extracted 1-hop sub-graphs elaborated in Section 3.4."}, {"title": "4 EXPERIMENTS", "content": "4.1 Data Preparation\nWe collect the circuits for pre-training from various sources, including benchmark netlists in ITC99 [40], IWLS05 [41], EPFL [42], and synthesizable register-transfer level (RTL) designs from OpenCore [43] and Github [44]. All designs are transformed into AIG netlists by ABC tool [45]. If the original netlists are too large, we randomly partition them into small circuits, ensuring that the maximum number of nodes in each circuit remains below 512. Furthermore, we augment the dataset by synthesizing netlists using various logic synthesis recipes. The statistical details of our pre-training dataset can be found in Table 2, which comprises a total of 67, 905 circuits. Note that we pre-train DeepGate3 on these small circuits, and for downstream tasks, we transfer DeepGate3 to large circuits.\n4.2 Implementation Details\nThe dimensions of both the structural embedding $h_s$ and the functional embedding $h_f$ are set to 128. The depth of Refine Transformer is 12 and the depth of Pooling Transformer is 3. All pre-training task heads are 3-layer multilayer perceptrons (MLPs). For pair-wise tasks in Section 3.5, we concatenate their embeddings as the input to the pre-training task head.\nWe pre-train our model with tasks described in Section 3.5 for 500 epochs to ensure convergence. This pre-training is performed with a batch size of 128 on 8 Nvidia A800 GPUs. We utilize the Adam optimizer with a learning rate of $10^{-4}$.\n4.3 Evaluation Metric\nTo better assess the performance of our model, we utilize several metrics as follows.\nOverall Loss. To calculate the overall loss $L_{all}$, we simply sum up all the losses as Eq. (9).\nError of Truth Table Prediction. Given the 6-input sub-graph s in test dataset S' with special token in Section 3.4, we predict the 64-bit truth table based on the graph-level functional embedding $h_{fs}$. The prediction error is calculated by the Hamming distance between the prediction and ground truth.\n$p^{tt} = \\frac{1}{\\text{len}(S')} \\sum_{S'} \\text{HammingDistance}(T_s, MLP_{tt} (h_{fs}))$     (10)\nAccuracy of Gate Connection Prediction. Given the structural embedding of the gate pair (i, j) in test dataset $N'_{con}$ and binary label $y^{con}_{(i,j)} \\in {0, 1, 2}$, we predict the gate connection and define the accuracy as follows:\n$p^{con} = \\frac{1}{\\text{len}(N'_{con})} \\sum_{N'_{con}} \\mathbb{1} (y^{con}_{(i,j)}, MLP_{con} (h_{si}, h_{sj}))$     (11)\nAccuracy of Gate-in-Graph Prediction. Given the gate-graph pair (k, s) in test dataset $N'_{in}$, we predict whether the gate is included by the sub-graph with the gate structural embedding $h_{sk}$ and the sub-graph structural embedding $h_{ss}$. The binary label is noted as $y^{in}_{(k,s)} \\in {0,1}$.\n$p^{in} = \\frac{1}{\\text{len}(N'_{in})} \\sum_{N'_{in}} \\mathbb{1}(MLP_{in} (h_{sk}, h_{s}), y^{in}_{(k,s)})$    (12)\n4.4 Data Scalability of DeepGate3\nThe data scalability, indicating that the model performance continues to increase as the training data expands, has been demonstrated across various models [46-49]. In this section, we conduct experiments to investigate the data scalability of DeepGate3. We partition our original training dataset, consisting of 67, 905 netlists, into subsets of 1%, 5%, 10%, 30%, and 50% of the original size (100%). We train multiple models with the same settings on each scaled training dataset and train them for 500 epochs to ensure the loss converges. All the models are validated with the same testing dataset to ensure the fairness. The experimental results with all loss values and evaluation metrics are shown in Table 1.\nTo further investigate the data scalability of DeepGate3, we train DeepGate2 on these subsets of train datasets with the same hyperparameters. The corresponding results are presented in Table 1. Furthermore, to provide a clear visualization of the results, we depict the performance metrics in Figure 6, including the overall loss, the error of truth table prediction, the accuracy of gate connection prediction, and the accuracy of gate-in-graph prediction. It is worth noting that the 1% subset of the training dataset contains only 679 netlists, resulting in ineffective performance for all models trained with this subset. Therefore, we exclude this setting in Figure 6.\nWe conclude three observations from the Table 1. First, DeepGate2 shows limited scalability when facing large dataset. In Table 1, for DeepGate3, most of the pre-training tasks achieve their best performance with 100% of the data. In contrast, for DeepGate2, optimal performance across these tasks occurs at 5%, 30%, 50%, and 100%. Furthermore, as illustrated in Figure 6, the overall loss and $P^{tt}$ for DeepGate2 no longer decrease when the data fraction increases from 50% to 100%. However, for DeepGate3, both the overall loss and $P^{tt}$ continue to decrease. In terms of $P^{con}$ and $P^{in}$, while DeepGate2 shows minimal improvement with additional data, DeepGate3 consistently enhances performance when trained with more data.\nSecond, the loss reduction rate of DeepGate3 is also superior to that of DeepGate2. As the data increases from 5% to 100%, these two methods exhibit different growth rates. For DeepGate2, the overall loss reduction ratios between 5% data and 100% data are 15.06%, 8.45%, 10.56%, and -3.37%; for DeepGate3, the reduction ratios are 13.67%, 27.23%, 10.67%, and 20.02%.\nLast, comparing the best performances of each model, i.e., DeepGate2 with 50% data and DeepGate3 with 100% data, DeepGate3 consistently demonstrates superior performance. For example, considering the evaluation metrics in Section 4.3, the improvements in $L_{all}$, $P^{tt}$, $p^{con}$, and $p^{in}$ for DeepGate3 are 1.2938, 0.0661, 7.74%, and 16.61% respectively, showcasing the promising capabilities of DeepGate3.\n4.5 Ablation Study\nIn this section, we perform ablation studies on the primary components of DeepGate3, namely the Refine Transformer (RT) and the Pooling Transformer (PT), following the metrics outlined in Section 4.3. For DeepGate3 without the RT (DG3 w/o RT), we use the embedding of DeepGate2 and PT to perform graph-level pre-training tasks. For DeepGate3 without the PT (DG3 w/o PT), we replace PT with average pooling. For DeepGate3 without both RT and PT (DG3 w/o RT & PT), we only use the embedding of DeepGate2 and average pooling.\nEffectiveness of Refine Transformer. We evaluated the performance difference between DeepGate3 and DeepGate2, focusing on the inclusion of the PT. As demonstrated in Table 3, when comparing to DG3 w/o RT, DeepGate3 shows enhancements in $p^{con}$ and $p^{in}$ by 7.56% and 12.79% respectively, and a reduction in overall loss by 0.4501, while maintaining comparable performance in $P^{tt}$. Similarly, DG3 w/o PT outperforms DG3 w/o RT & PT in terms of $L_{all}$, $P^{tt}$, $p^{con}$, and $p^{in}$, with improvements of 0.7182, 0.167, 6.46%, and 9.57% respectively. These results highlight the critical role of the Refine Transformer in refining initial embeddings and capturing complex, long-term correlations between gates.\nEffectiveness of Pooling function. Prior work on graph-level tasks primarily utilized average or max pooling, such as Gamora [10] and HOGA [11], which often fails to capture circuit-specific information and struggles to perform well under varying conditions. Our proposed Pooling Transformer addresses this by incorporating special tokens to account for diverse circumstances. According to the results in Table 3, the inclusion of the Pooling Transformer significantly enhances $L_{all}$, $P^{tt}$, and $p^{in}$ in DeepGate3 by 0.6480, 0.0494, and 7.04% respectively. Note that the Pooling Transformer is primarily related to graph-level tasks. Consequently, DeepGate3 and DG3 w/o PT exhibit similar performance on gate-level tasks, such as $P^{con}$. Moreover, when comparing the performance of DG3 w/o RT to that of DG3 w/o RT & PT, we observe better performance in $P^{tt}$ and $p^{in}$ by 0.0672 and 3.82%, respectively. Since $p^{con}$ is a gate-level task unrelated to the Pooling Transformer, the performance between DG3 w/o RT and DG3 w/o RT & PT remains almost unchanged.\n4.6 Downstream Task: SAT Solving\nThe Boolean Satisfiability (SAT) problem is a fundamental problem that determines whether a Boolean formula can output logic-1 with at least one variable assignment. It is the first proven NP-complete problem [50] and serves as a foundational problem applicable to various fields, including scheduling, planning and verification. The prevalent SAT solvers opt for the conflict-driven clause learning (CDCL) as the backbone solving algorithm. CDCL efficiently handles searching path conflicts and effectively explores additional constraints to reduce searching space. Over the past decades, numerous heuristic designs [51-54] have been proposed to accelerate CDCL process in SAT solvers.\nSolving heuristic Design. [51] introduces a variable decision heuristic that assigns the opposite value to a pair of correlated variables, aiming to intentionally cause conflicts rapidly. While this approach achieves notable speed improvements compared to traditional SAT solvers, it still relies on time-consuming logic simulation to acquire the necessary functional relation between variables. DeepGate2 [8] proposes an alternative solution by utilizing functional embeddings to measure the similarity between variables based on their functionality. Similarly, we can leverage the gate-level embeddings to predict functional similarity and identify correlated variables.\nThe variable decision heuristic pipeline is shown in Figure 7. Given a SAT instance, the first step is to obtain gate-level functional embeddings prior to solving. During the variable decision process, a decision value $d_i$ is assigned to variable $v_i$. If another variable $v_j$ with an assigned value $d_j$ is identified as correlated to $v_i$, the reversed value $d'_i$ is assigned to $v_i$, i.e., $d_i = 0$ if $d_j = 1$ or $d_i = 1$ if $d_j = 0$. The determination of correlated variables relies on their functional similarity, as defined in Eq. (13), where the similarity $Sim(v_i, v_j)$ exceeding the threshold $e$ indicates correlation.\n$Sim(v_i,v_j) = MLP_{gate\\_tt} (h_{fv_i}, h_{fv_j})$  (13)\nExperiment Settings. We utilize the CaDiCal [55] SAT solver as the backbone solver and modify the variable decision heuristic based on it. In the Baseline setting, SAT problems are directly solved using the backbone SAT solver. In the DeepGate3 setting, our model is further fine-tuned efficiently on large-scale circuits in our proposed shifting-window manner (see Section 3.6). We finetune DeepGate3 for 200 epochs only with the pair-wise truth table distance prediction task, i.e., optimizing the loss function $L_{tt\\_pair}^{gate}$ as Eq. (4). Then, we enhance the variable decision heuristic by incorporating gate-level embeddings produced by fine-tuned DeepGate3. Additionally, we also employ the embeddings obtained by DeepGate2 for comparison. The test cases, denoted as C1-C5, are collected from industrial logic equivalence checking (LEC) problems. These cases exhibit diverse levels of solving complexity, showcasing a range of challenges encountered in practical scenarios.\nResults. Table 4 presents a runtime comparison among the Baseline, DeepGate2, and DeepGate3 settings, where the runtime reduction compared to the Baseline setting is denoted as Red. and the number of gates in each circuit case is represented as # Gates. Based on the observations from the table, we can draw three main conclusions. First, both DeepGate2 and DeepGate3 capture the functional correlation of variables to guide variable decision, resulting in reduced solving time compared to the Baseline setting. On average, DeepGate2 achieves a runtime reduction of 14.92%, and DeepGate3 achieves average reduction of 50.46%. However, it is worth noting that the heuristic-based solving strategy is less effective for easier cases, as the model inference process accounts for a significant portion of the total runtime. Secondly, DeepGate3 demonstrates its capability to handle large-scale circuits, as the LEC test cases contain more than 10K gates, significantly surpassing the size of circuits in the pre-training dataset (with an average of 225.29 gates). Thirdly, DeepGate3 exhibits superior performance compared to DeepGate2 in this task, indicating that DeepGate3 captures more informative gate-level functional embeddings. In conclusion, our model demonstrates effective generalization ability to solve practical SAT solving problems."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced DeepGate3, a pioneering framework that leverages the synergy between GNNs and Transformers to address the scalability of circuit representation learning. DeepGate3's innovative architecture, featuring Refine Transformer and Pooling Transformer mechanisms, significantly enhances the scalability and generalization capabilities of circuit representation learning. Moreover, the introduction of multiple novel supervision tasks has enriched the learning process, allowing DeepGate3 to capture a broader range of circuit behaviors with higher fidelity. Our experimental results demonstrate that DeepGate3 not only outperforms its predecessors DeepGate2 but also sets new benchmarks in handling complex and large-scale circuit designs efficiently. Future work will focus on incorporating additional data types, such as temporal and operational conditions, to enrich model insights and expanding DeepGate3's applications within EDA tasks."}]}