{"title": "Watson: A Cognitive Observability Framework for the Reasoning of Foundation Model-Powered Agents", "authors": ["Benjamin Rombaut", "Sogol Masoumzadeh", "Kirill Vasilevski", "Dayi Lin", "Ahmed E. Hassan"], "abstract": "As foundation models (FMs) play an increasingly prominent role in complex software systems, such as FM-powered agentic software (i.e., Agentware), they introduce significant challenges for developers regarding observability. Unlike traditional software, agents operate autonomously, using extensive data and opaque implicit reasoning, making it difficult to observe and understand their behavior during runtime, especially when they take unexpected actions or encounter errors. In this paper, we highlight the limitations of traditional operational observability in the context of FM-powered software, and introduce cognitive observability as a new type of required observability that has emerged for such innovative systems. We then propose a novel framework that provides cognitive observability into the implicit reasoning processes of agents (a.k.a. reasoning observability), and demonstrate the effectiveness of our framework in boosting the debuggability of Agentware and, in turn, the abilities of an Agentware through a case study on AutoCodeRover, a cutting-edge Agentware for autonomous program improvement.", "sections": [{"title": "I. INTRODUCTION", "content": "Agentic software (i.e., Agentware [23]), powered by foundation models (FMs) with reasoning capabilities [6, 28, 46], are gaining significant traction in a variety of domains such as autonomous software development [31, 74], customer support [20], and data analytics [19]. However, such agentic software present new challenges compared to traditional software [24], especially in the realm of observability, as they operate with high levels of autonomy and unpredictability, and make decisions through implicit reasoning that is often opaque [68]. This opacity complicates the ability to monitor, debug, and ensure the reliability of such software. For instance, an FM-powered applicant recommendation system has been found to manifest biases towards certain applicants based on their specific traits [11]. Despite significant efforts, engineers are often not able to identify the root causes of these inherited biases due to the \u201cblack-box\u201d nature of the reasoning behind such agentic software.\nObservability techniques applied to traditional software typically involve principles related to operational observability-monitoring metrics and system status through logs, traces, and performance counters [40, 57]. Such methods are effective due to the direct traceability between the system's deterministic behavior and the development assets that are being executed, such as code snippets and predefined workflows. Developers can easily instrument such development assets (e.g., code) to track execution flows, identify bottlenecks, and detect anomalies. Issues can be traced back to and addressed at the respective assets.\nHowever, while operational observability has also shown its value for developers and operators of Agentware by tracing model inference calls, tool usage, token consumption, and intermediate outputs at a system level, it falls short of providing adequate insights for diagnosing, debugging, and addressing issues in the implicit reasoning process of agents. Agents often operate with a high degree of independence, making decisions based on evolving and dynamically retrieved contexts, learned experiences, and interactions with other agents or environments. The underlying logic driving agent behavior is not traceable through conventional logs or performance counters, as the behavior of agents is not specified in code, but rather instructed through prompts and reasoned and planned by FMs that have extremely limited inherent explainability. As prompts and FMs are not instrumentable and do not produce deterministic or reproducible execution results, it is extremely challenging to pinpoint failures or understand why an agent behaves in a certain way. As such, new observability concepts and approaches are required that are capable of capturing not just system outputs, but also the cognitive-like reasoning processes of agents.\nFor this purpose, we propose a new type of observability called cognitive observability for Agentware, which enables the rationale of agent actions and decisions within a broader environment to be observed, providing insight not only into what agents do, but also why they do it. This type of observability is crucial for debugging agentic software, as it helps track reasoning pathways, evaluate how knowledge and contexts influence agent behaviors, and optimize the alignment between agent decisions and desired outcomes.\nThe need for cognitive observability becomes more prominent with the latest Agentware, which adopts enhanced reasoning capabilities provided by proprietary novel reasoning models that do not reveal their implicit reasoning paths to agent developers. For example, OpenAI's 01 model series [32] are designed to spend more time reasoning before they respond, which allows them to articulate their reasoning processes more clearly while working through complex tasks [5, 64, 69, 72, 73]. However, despite these improvements, the developers of 01 opted not to reveal its reasoning process due to policy compliance or competitive advantage [38]. Additionally, it is not always possible to include the reasoning in the response of reasoning models, as downstream systems might be tightly coupled to the model outputs and require a strictly defined structured output format from such models.\nFurthermore, in the context of Agentware, effective observability is not only essential for human operators to understand how these systems function, but also for the systems themselves. Because Agentware systems include both human operators and autonomous agents, they can leverage observability techniques to enhance their system-understanding by providing insights into the internal states and processes of the whole software and even themselves. This capability for system-awareness means that effective observability enables Agentware to adapt and improve over time, which is critical in developing more capable and resilient systems. In fact, agents debating and reflecting on each other's decisions has been shown to significantly improve system performance [67], and by allowing one agent's reasoning process to be observed by other agents, such cross-checking has the potential to further boost system performance.\nIn this paper, we extend the current observability taxonomy of Agentware by integrating the concepts around cognitive observability. We then introduce a novel framework, called Watson, to observe the reasoning path of agents, a primary consideration of cognitive observability. We demonstrate the effectiveness of Watson on AutoCodeRover [74], a state-of-the-art Agentware for autonomous program improvement. Our case study shows that Watson enables developers to identify defective reasonings in agents, which in turn can help optimize the capabilities of the Agentware, by debugging and addressing two failed instances of AutoCodeRover on SWE-bench-lite [58]. The main contributions of this paper are as follows:\n\u2022 We propose a taxonomy of observability for Agentware, highlighting the unique challenges and opportunities, with a focus on the limitations of operational observability and the need for cognitive (in particular reasoning) observability.\n\u2022 We propose a novel framework, Watson, for observing an agent's implicit reasoning processes, contributing to more effective monitoring and debugging of Agentware.\n\u2022 We demonstrate the effectiveness of our framework on a state-of-the-art Agentware, AutoCodeRover, showcasing the effectiveness of cognitive observability in improving agent capabilities and debuggability.\nThe remainder of the paper is structured as follows: Section II discusses observability for Agentware. Section III introduces Watson, our proposed framework. Section IV demonstrates Watson on AutoCodeRover. Section V examines important points from our case study. Section VI discusses the threats to the validity of our study. Finally, Section VII concludes the paper."}, {"title": "II. OBSERVABILITY FOR AGENTWARE", "content": "In this section, we first introduce our proposed extended taxonomy for the observability of Agentware. We then discuss related work in operational (Section II-B) and cognitive (Section II-C) observability, as well as highlight the challenges of both types of observability as applied to Agentware, and FMware in general. Our taxonomy and their associated challenges were identified based on not only in-depth discussions with industrial and academic leaders (e.g., during SEMLA 2023 and 2024 [35], FM+SE Vision 2030 [22], FM+SE Summit 2024 [21], and SE 2030 workshop\u2014FSE 2024 [52, 53] events), but also our practical experience with the research and development of FMware [23, 41], and our close interactions with over 40 leading industrial partners (e.g., Intel, AMD, RedHat, HuggingFace, and SAP) as part of the Open Platform for Enterprise AI (OPEA) alliance [48].\nAgentware observability (Table I) encompasses two key categories: operational observability and cognitive observability, which together provide a comprehensive view of software behavior and insights. Operational observability focuses on monitoring software performance and resource consumption, with sub-categories like logs & program execution traces (e.g., capturing detailed events), volume metrics (e.g., measuring data or task throughput), latency metrics (e.g., tracking response times), token metrics (e.g., monitoring token usage in models), resource utilization (e.g., evaluating cost and computational resource consumption), and FM tool instrumentation (e.g., gathering metrics from FM tools such as document retrievers [43]). Cognitive observability refers to understanding the behavior and quality of Agentware at a cognitive level that often cannot be quantified or monitored easily by operational observability, such as reasoning paths (i.e., understanding the implicit reasoning path of an agent), semantic feedback (e.g., gathering explicit and implicit user responses to the system output, such as modifications to the output), and output integrity (e.g., analyzing model outputs for qualitative insights and hallucinations [29]).\nOperational observability remains crucial for any FMware, including Agentware, just as it is for traditional software [40]. It provides vital insights into software performance and behavior, ensuring that potential issues can be identified and addressed before they impact users. This is especially important when FMware involves complex workflows that integrate multiple model inferences and legacy system calls, as operational observability enables a comprehensive view of inter-system and inter-component interactions.\nEven when treating the FM as a \"black box\", we can still gain observability into their behaviors by analyzing logs and execution traces of FMware, which reveal data flow between components and help detect anomalies in execution paths [9]. Volume metrics offer a way to track the number of calls to specific FMs, while latency metrics measure delays in responses across FMware, helping to identify potential bottlenecks and improve the user experience. Token metrics provide insight into model behavior, revealing patterns of prompt and completion usage that can impact overall performance and cost, while tracking resource utilization ensures that compute, memory, and storage capacities are used by FMs efficiently [1]. Additionally, monitoring the tools that FMs use is crucial for understanding how they interact with other FMware components, as this enables us to assess their capabilities and effectiveness in the overall architecture of the FMware. By gaining observability into these categories, we can optimize the capabilities of FMware, enhance collaboration among FMware components, and ensure that the FMware operates smoothly and efficiently, ultimately leading to a better experience for all FMware users.\nState-of-the-art FM observability platforms like LangSmith [37], TraceLoop [60], Weights & Biases [65], and others [15, 25, 30, 36, 45, 54, 55, 66] have already begun to offer support for tracking operational observability metrics for FMware. For example, the open-source OpenLLMetry [61] package from TraceLoop is designed to offer standard OpenTelemetry [49] instrumentation for FM providers and vector databases. This enables developers to gain basic observability over their FMware, allowing them to monitor prompts, token usage, and grounding accuracy over time. While these products provide valuable insights into operational metrics, they often overlook the contextual factors and implicit reasoning paths of FMware, falling short of offering adequate insights to help developers assess the user-perceived quality and debug issues within reasoning paths.\nThe key to truly enhancing observability for FMware lies in implementing cognitive observability, which shifts the focus to higher-level cognitive and linguistic aspects. To address these complexities, cognitive observability focuses on three critical sub-categories: reasoning path, semantic feedback, and output integrity. Reasoning path tracks the reasoning process of FMs, offering insights into how they generate conclusions based on their often opaque decisions. Semantic feedback gathers and interprets user feedback on the FMware to improve system behavior. Finally, output integrity analyzes model outputs to derive qualitative insights and identify and mitigate malicious prompts, sensitive data, toxic responses, problematic topics, and hallucinations. These categories provide a comprehensive view of FMware performance, essential for managing complex, multi-agent systems and addressing challenges that traditional observability alone cannot solve.\nReasoning Path: This sub-category focuses on tracking FM reasoning paths, but faces several challenges. One significant challenge is the need for higher-level abstractions that go beyond simply logging inputs and outputs. Understanding how models arrive at specific conclusions requires insight into their implicit reasoning paths, which often involve complex algorithms, knowledge selection, and decision-making processes that are not easily interpretable. For example, in the case of retrieval-augmented generation (RAG) [39], although we can evaluate the retriever's performance independent of the rest of FMware through operational observability (i.e., monitoring the component output), there is no guarantee that the retrieved context is used correctly, or even used at all, during the reasoning process. Knowing this is essential for debugging decisions made by agents, particularly in complex FMware where potentially many agents interact and coordinate with both legacy systems and other agents. Without this higher-level abstraction, it becomes difficult to pinpoint why an agent took a specific action or made a particular decision, and understanding the reasoning process or inference chain that led to an erroneous decision is crucial for helping FMware developers understand how to correct potential errors. Still, current tools offer limited visibility into these reasoning pathways, which become even more important in observing as FMware agent reasoning capabilities improve [56].\nHowever, we cannot simply instruct agents in FMware to output their reasoning, because doing so inherently alters the completion [64]. This is similar to the observer effect in quantum physics, where observing a system changes its state [12]. When an agent is asked to explain its reasoning, it is no longer just completing its original task; it now has to simultaneously generate an explanation, which can shift its focus or introduce new biases into its responses. This alteration complicates the ability to assess the agent's original reasoning process in a pure, unmodified form, making it challenging to diagnose issues or understand the unobserved logic behind its actions. Additionally, integrating reasoning into the output of FMs may not always be feasible, as downstream systems in FMware can be tightly coupled to the FM's outputs and may demand a strictly defined and structured output format, limiting the agent's ability to provide comprehensive explanations without compromising its primary functions.\nWhen errors in FMware occur, it is not trivial to localize where the fault originated in the system. In traditional software systems, logs and stack traces provide clear insights into the source of an issue, allowing developers to trace errors to specific lines of code or API calls. However, in FMware, errors often arise from more abstract or emergent behaviors that are not easily observable because there is no explicit code driving the behavior of FM agents. This lack of traditional code structure means that errors can't always be pinpointed through conventional debugging techniques, further complicating understanding why agents behave the way they do. These errors are also harder to trace because they don't always stem from a single point of failure but rather from complex reasoning chains or interactions across FMware. There is no clear logic to inspect or trace because much of the decision-making is based on latent knowledge embedded within the model. Additionally, in multi-agent FMware systems, errors made by early agents in FMware systems may only manifest later in the workflow, further complicating the ability to trace back the source of a problem. For instance, an incorrect assumption or action by one agent can lead to a cascade of misaligned actions by downstream agents, resulting in unpredictable outcomes\nPrevious attempts have been suggested to observe the reasoning paths of agents in FMware, in which the cause-and-effect actions (i.e., when x happens, then y follows) of agents are tracked as a proxy to show their implicit reasoning paths [27, 44, 51]. Additionally, the benefits of enabling \u201cself-explainability\" in FMware agents, for the purpose of enhancing their observability [26], transparency [63, 71], and building trust with end-users [50, 76], have also been investigated. Zelikman et al. [70] are particularly relevant as they focus on generating reasonings that guide the FM towards the correct answer by reasoning backward from the solution, ultimately enhancing the model's accuracy. Our focus on observing reasoning paths diverges significantly in two key areas. First, our goal is not to ensure the correctness of the answer, but rather to understand how agents reason, irrespective of whether their completion is correct. Second, the methodologies differ: since their work only aims to generate reasoning that leads to the correct answer, it is not necessary to verify whether the generated reasoning aligns with the model's actual reasoning process. In contrast, our approach introduces verification mechanisms, including EOM token verification (Section III-A) and prompt attribution validation (Section IV-B), to ensure that the generated reasoning reflects the model's true reasoning process.\nThe tracking of cognitive observability includes another crucial component: capturing semantic feedback in the form of explicit, implicit, and free-form feedback from users and systems [47]. Explicit feedback includes direct actions on rating FMware-generated content. For example, OpenAI's ChatGPT [7], as well as existing FMware observability tools [25, 30, 37, 54], allow users to assign a \"thumbs up/down\" rating to FM completions. Implicit feedback is drawn from user behaviors, such as whether the user \"copied\", \"saved\", or \"dismissed\" the completion. For example, GitHub Copilot [17] checks whether a code suggestion that was accepted by the developer is still present in the code at different time intervals after the insertion, as well as the extent to which the user edited the code suggestion [10], to evaluate the utility of the suggestion. Free-form feedback consists of corrections or explanations provided by users regarding the generated output of FMware [30]. These three types of feedback metrics help developers gauge the efficacy of FMware output, optimize the user experience of FMware agentic applications, and identify trends that inform decisions about model training or fine-tuning.\nHowever, gathering semantic feedback presents its own challenges, particularly in the context of the limited observability of FMware. While user interactions provide valuable explicit and implicit responses, interpreting these signals can become complex as interactions in FMware grow more intricate, especially in multi-agent systems. Agents may dynamically adapt their responses based on not only their learning but also the interactions of other agents in the system, and this rising complexity can make it difficult to pinpoint specific areas for improvement or obscure the relationship between user feedback and agent behavior.\nThe focus of output integrity is on analyzing the output of FM agents to gain qualitative insights into their behavior. This involves techniques like sentiment analysis to understand emotional tone [45, 66], as well as detecting hallucinations\u2014instances where the model generates information that is factually incorrect or irrelevant [29]. These signals provide critical feedback on the reliability and coherence of the model's output, helping to identify inconsistencies, biases, or errors that may not be evident from traditional metrics alone.\nAnalyzing FM outputs for qualitative insights also comes with challenges, particularly in environments with rising complexity and dynamic interactions. As FMs commonly generate text outputs, understanding the nuances in these outputs requires sophisticated methods to assess their relevance and coherence. The dynamic nature of multi-agent systems can further complicate this analysis, as the context in which outputs are generated may change rapidly due to interactions with other agents, which can lead to situations where outputs are difficult to evaluate effectively, limiting observability and insight. Additionally, the interplay between language generation and reasoning paths complicates deriving actionable insights from output integrity, as the reasons behind certain outputs may not be immediately apparent."}, {"title": "III. WATSON: A REASONING PATH FRAMEWORK", "content": "To address the aforementioned challenges, specifically those related to reasoning path, we introduce Watson, a framework to observe the reasoning process of agents without affecting their behavior. In this framework, a \"surrogate agent\" [42] operates in parallel with an original agent of interest (a.k.a the \u201cprimary agent\u201d) for observing its implicit reasoning process. The goal of the surrogate agent is not to arrive at a \"correct\" completion, but rather to replicate the primary agent's completion, while also reasoning verbosely about its thought process for arriving at that outcome. As such, we can observe the implicit reasoning path used by the primary agent without affecting the original completion as a result of asking the agent to output its reasoning process. Agentware developers benefit from this reasoning, gaining insight into why the primary agent responded as it did and using the reasoning as \u201chints\u201d to debug or enhance the performance of the primary agent without altering its natural behavior.\nWatson is structured around three primary considerations. First, the surrogate agent must mirror the configuration of the primary agent, allowing for the replication of the primary agent's behavior and ensuring similar performance among the two agents. Next, Watson employs a mechanism for generating reasoning. Finally, Watson ensures the generated reasonings are aligned with the targeted completion. We explain each consideration in detail below.\nTo ensure the surrogate agent precisely replicates the primary agent's implicit reasoning process, it must maintain a mirrored configuration, including the FM architecture and key decoding parameters. By using the same FM, the surrogate agent interprets the inputs with the same underlying capabilities as the primary agent. Furthermore, decoding parameters, such as temperature and top_p, must be identical between the two agents, as these parameters govern the determinism and creativity of the model's outputs, where even slight variations could lead to divergence in agent behavior. It is worth noting that identical configurations do not guarantee identical or reproducible model output [8]. We address the concern in the final consideration of Watson.\nWith the goal of Watson being to generate an implicit reasoning path that ties an input prompt to the completion of a primary agent, the surrogate agent must access the primary agent's input prompt as the prefix and the primary agent's completion as the suffix, and generate completion tokens to bridge their gap with a reasoning path. However, this poses a novel challenge for designing Watson, as autoregressive FMs like the popular OpenAI's GPT [2] and Meta's LLaMa [59] are built using causal decoder-only transformer architectures [62] and are trained to predict the next token in a left-to-right fashion. In other words, the generation of the next completion token only depends on the given tokens in the input prompt and the already-generated tokens in the completion.\nTo solve this problem, a fill-in-the-middle (FIM) technique [4] is proposed for enabling causal decoder-based FMs to use both the prefix and suffix to infill a middle region of a prompt, primarily motivated for code completion applications [16, 33]. The FIM approach (Figure 3) divides a prompt into a prefix, middle, and suffix identified with <PRE>, <MID> <EOM> (i.e., start-of-middle and end-of-middle), and <SUF> special tokens, respectively. During training, the FM uses the prefix and suffix content as the input prompt, and attempts to generate the content in the middle.\nTo implement FIM, we arrange the input prompt as prefix-suffix-middle [4, 18], meaning the surrogate agent uses the primary agent's prompt as the prefix and the primary agent's completion as the suffix, followed by the <MID> token. The surrogate agent then generates the reasoning, with the <EOM> token marking its end. To ensure the proper conduct of the FIM task, we explicitly check for the generation of the <EOM> token. As such, the generated reasoning by the surrogate agent is a verified implicit reasoning path for linking the primary agent's input prompt and its corresponding completion.\nWhile FIM is ideal for our goal of generating reasoning paths, the approach is constrained to FMs that inherently support the FIM technique and can generate infilled completions. Consequently, it would prove infeasible for the surrogate agent to use the FIM technique if the primary agent does not inherently support the FIM technique. In such cases, we propose an alternative technique named \u201cRepetitive Chain-of-Thought\u201d for reasoning generation, which is not specific to FMs with FIM capability. This technique involves prompting the surrogate agent with the same input as the primary agent but adds an instruction to explicitly generate chain-of-thought reasoning [64] in its completion. To ensure alignment, any reasoning generated by the surrogate agent leading to an answer different from the primary agent's final completion is discarded. This alignment step is unnecessary in the FIM technique as the surrogate agent's completion is guided by the primary agent's output, which is provided as the suffix of the prompt. However, this step is crucial in Repetitive Chain-of-Thought as it guarantees that the surrogate agent's reasoning remains consistent with the primary agent's implicit reasoning process, ensuring that we capture only the most relevant reasoning paths. This filtering process minimizes discrepancies and ensures that the reasoning we analyze is closely tied to the primary agent's behavior. Using this approach, it is possible to capture the reasoning paths of the surrogate agent, and consequently the implicit reasoning of the primary agent, without the necessity for their underlying FMs supporting FIM. However, depending on the rounds of inferences that need to be filtered out due to inconsistent answers, such a technique may result in higher inference costs.\nIt is necessary to ensure that the reasoning generated by the surrogate agent is aligned with the primary agent's completion so that the surrogate's reasoning reflects the implicit reasoning process of the primary agent. For this purpose, we task the surrogate agent with generating multiple potential reasoning paths so that we can extract the mutual threads and recurring ideas [72]. Doing so also allows for capturing different angles from which the primary agent might have addressed the input prompt, providing a more comprehensive view of its implicit reasoning process.\nAfter extracting the consistent threads from the generated reasonings, we validate their alignment with the input prompt using PromptExp [14], a cross-granularity prompt explanation technique that calculates the importance of different components in the input prompt (a.k.a its attribution) in influencing the agent's completion. In other words, we confirm the ideas represented by the components of the prompt with the highest attributions are emphasized within the generated reasoning. For calculating these attributions, we tokenize the input prompt and create a perturbed version of the input by removing one token at a time, effectively masking it [75]. This perturbed prompt is then passed to the primary agent for a new completion. To evaluate the importance of the masked input token, we calculate probability differences among analogous tokens of the new completion and the completion corresponding to the original input prompt. A higher arithmetic mean of these differences results in a higher attribution value for the masked input token, which in turn indicates its greater importance for the agent completion. We then aggregate the token attributions to different granularities of the prompt using the method proposed in PromptExp."}, {"title": "IV. DEBUGGING AUTOCODEROVER WITH WATSON", "content": "In this section, we conduct a case study of using Watson for debugging AutoCodeRover, a state-of-the-art Agentware for autonomous program improvement, with 30.67% efficacy (pass@1) [3] on SWE-bench-lite, a subset of 300 issue instances from the SWE-bench [34] evaluation framework.\nAutoCodeRover operates in two phases. In the initial phase, an agent is tasked with analyzing an issue report and identifying corresponding defective code locations in the associated software project. The locations are identified by leveraging a predefined set of APIs that allows for the agent to examine the software project associated with the issue report and gather context of components in the project that might be relevant to resolving the issue. The retrieved context is then learned by the agent, refining its understanding of the code structure and issue report. If needed, the agent can re-invoke the same collection of API calls to gather additional context. This iterative process continues until the agent determines that it has sufficient context to identify the appropriate code locations for applying a solution patch, at which point the agent returns a blank completion in response to the input prompt. Once the defective code locations are decided, they are passed to a second agent, which is tasked with generating the solution patch. However, this is an opaque decision made by the first agent-it is not known why the agent decided it had enough context, or which parts of the already retrieved context it considers sufficient in making this decision. Therefore, the Agentware developers are not able to effectively debug the decisions of the agent.\nWe apply Watson to AutoCodeRover to understand the implicit reasoning process leading to the agent's decision on the defective code locations. We use GPT-3.5-turbo-instruct as the underlying FM in AutoCodeRover (i.e., the primary agent) and in Watson (i.e., the surrogate agent), as this FM supports FIM. We select the following two SWE-bench-lite instances for which AutoCodeRover has made an attempt to solve. For both instances, incorrect code files were identified by the first agent as buggy locations, while the agent's reasoning for this decision is not immediately clear."}, {"title": "C. Results", "content": "In this section, we discuss the results of running Watson on the two instances described in Section IV-B. For each instance, we discuss the output of reasoning from Watson, the validation of this reasoning against the prompt attributions of the primary agent, and the outcome of re-running AutoCodeRover with corresponding hints to the aforementioned instances.\nThe reasoning generated from Watson for django_django-13401, shown in Listing 1, suggests the agent is focusing on the test files, specifically model_inheritance/tests.py, which is not the code location for where we expect the solution patch to be applied. Following this incorrect decision pathway, AutoCodeRover attempts to modify subclasses B and C, which are contained in model_inheritance/tests.py, as the fix for the encountered issue.\nWith the inspection of PromptExp for the current instance shown in Figure 4, it becomes evident that, compared to the rest of the input prompt, model_inheritance/tests.py has a higher attribution value . The same applies for subclasses B and C, confirming that the AutoCodeRover agent aims to write a solution patch by modifying model_inheritance/tests.py and consequently subclasses B and C .\nBased on the reasonings, we add the following hint at the end of the issue report and rerun AutoCodeRover.\nWith this hint, AutoCodeRover arrives at the correct file location to be patched, while specifically acknowledging that the fix is not located in any test files (Listing 2).\nThe reasoning generated from Watson for django_django-14238, shown in Figure 3, suggests that the agent is placing its focus on the files in the example directory. However, upon closer inspection of the original issue report, it is explicitly mentioned that the fix can be applied to the_subclasscheck_method, which does not show up at all in the reasonings, indicating that the agent is ignoring this information.\nUpon closer inspection of PromptExp for the current instance, illustrated in Figure 5, it is evident that the attribution value of _subclasscheck__ is lower compared to other components of the prompt-AutoCodeRover agent does not consider the method _subclasscheck_ while aiming to solve the instance.\nBased on the reasonings, we decide to add the following hint at the end of the issue report and rerun AutoCodeRover.\nWith this hint, AutoCodeRover retrieves the correct contextual information related to _ subclasscheck\nand arrives at the correct file location to be patched"}, {"title": "V. IMPLICATIONS", "content": "In our case study, we demonstrate how Watson's observed reasoning helps us identify where AutoCodeRover's implicit reasoning path went wrong. Furthermore, we show how these reasonings enable us to guide and correct AutoCodeRover's reasoning process through formulated hints. This observation is important because Agentware systems involve both Agentware developers and autonomous agents, both of which can leverage observability techniques to gain insights into the internal states and processes of Agentware. Reasoning path is crucial for Agentware developers to identify and address issues by refining meta prompts, redesigning agent interactions, and implementing other enhancements. Meanwhile, reasoning path enables the agents themselves to develop a deeper system-understanding. Allowing agents to observe and reflect on each other's reasoning paths can lead to improved quality of Agentware, as this cross-checking process enables them to refine their decision-making collaboratively. In turn, this fosters continuous improvement in Agentware's implicit reasoning capabilities and overall performance, leading to more robust and effective systems."}, {"title": "VI. THREATS TO VALIDITY", "content": "In this section, we discuss the threats to the validity of our study.\nThreats to internal validity concern factors that could have influenced our analysis and findings. One such threat in Watson arises from the fact that the surrogate agent and the primary agent are two different entities. Although both agents share the same FM architecture and decoder parameter settings, their reasoning paths may still differ. This discrepancy occurs because even with identical configurations, subtle variations in token generation or contextual interpretation can lead to different reasoning paths and consequently, divergent completions, and so the possibility of the surrogate agent producing reasoning that deviates from that of the implicit reasonings of the primary agent.\nAnother potential threat lies in our approach for ensuring reasoning consistency (Section III-A). Extracting recurring ideas from the reasonings generated by the surrogate agent does not guarantee the inclusion of all possible reasoning paths for the primary agent. This process may miss nuanced, less frequent reasoning paths that are still valid, or it may be biased towards paths that appear more often but are not necessarily the most accurate. So, while the most common reasoning paths can emerge, their consistency cannot be fully ensured. However, this threat is partially mitigated by our approach to reasoning validation, where we compare the prompt attribution values against the reasoning to ensure their alignment with each other (Section III-A)."}, {"title": "VII. CONCLUSION", "content": "As Agentware become more prominent in powering complex systems, they introduce new challenges for developers, particularly in terms of observability. Unlike traditional software, where behavior can often be traced directly to explicit code assets using operational observability techniques, Agentware operate with a high degree of autonomy and unpredictability, making decisions based on evolving and dynamically retrieved contexts, learned experiences, and implicit reasoning paths that are often opaque. This opacity makes it difficult for developers to fully understand how this software function during runtime, especially when dealing with unexpected behaviors or errors, emphasizing the limitation of applying traditional operational observability techniques for Agentware.\nIn this work, we introduce a taxonomy of observability for Agentware, addressing the unique challenges and opportunities they present. Our taxonomy emphasizes the limitations of traditional operational observability and underscores the critical need for cognitive observability, particularly in capturing agent implicit reasoning paths. To begin to address these challenges, we propose a novel framework called Watson, which focuses on observing an agent's implicit reasoning, providing deeper insights for more effective monitoring and debugging of Agentware. We validate the efficacy of our approach using the state-of-the-art Agentware, AutoCodeRover, demonstrating how cognitive observability enhances agent performance and significantly improves debuggability, while also highlighting the ability for Agentware to leverage these observability insights for multi-agent debates and reflections and in turn correct potential errors in runtime. Together, these contributions pave the way for more resilient, transparent, and better performing Agentware."}]}