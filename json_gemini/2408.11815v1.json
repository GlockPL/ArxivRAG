{"title": "Great Memory, Shallow Reasoning: Limits of kNN-LMS", "authors": ["Shangyi Geng", "Wenting Zhao", "Alexander M Rush"], "abstract": "K-nearest neighbor language models (kNN-\nLMs), which integrate retrieval with next-word\nprediction, have demonstrated strong perfor-\nmance in language modeling as well as down-\nstream NLP benchmarks. These results have\nled researchers to argue that models trained on\npoor quality or outdated data could perform\nwell by employing a kNN extension that has\naccess to a higher-quality datastore. In this\nwork, we ask whether this improved ability to\nrecall information really translates into down-\nstream abilities. We extensively evaluate kNN-\nLMs on a diverse set of tasks, ranging from\nsentiment classification and commonsense rea-\nsoning to multi-hop reasoning. Results show\nthat kNN-LMs excel at memory-intensive tasks,\nwhere utilizing the patterns in the input is suf-\nficient for determining the output, but struggle\nwith reasoning tasks that require integrating\nmultiple pieces of information to derive new\nknowledge. We further demonstrate through\noracle experiments and qualitative analysis that\neven with perfect retrieval, kNN-LMs still fail\nto determine the correct answers, placing an\nupper bound on their reasoning performance.\nCode and datastores are released at https:\n//github.com/GSYfate/knnlm-limits/.", "sections": [{"title": "1 Introduction", "content": "A foundational property of pretrained language\nmodeling (Peters et al., 2018; Devlin et al., 2019)\nhas been that improvements to the perplexity of\nthe model lead to improvements on downstream\ntasks. This property is central to the scaling of large\nlanguage models (LLMs) where researchers focus\nnearly exclusively on perplexity as a proxy met-\nric for improved general purpose abilities (Kaplan\net al., 2020). In recent years, this research has cen-\ntered primarily on high-quality text data at greater\nand greater quantities as the limiting component\nfor producing better language models (Hoffmann\net al., 2022).\nThis increasing need for data to train language\nmodels has led to significant challenges. On one\nhand, including as much high-quality data as possi-\nble results in improved downstream performance.\nOn the other hand, this data is often protected by\nlicenses or copyright, which means training on\nsuch data brings legal issues. For example, the re-\ncent high-profile lawsuit from the New York Times\nnotes the clear use of their data in OpenAI mod-\nels (Grynbaum and Mac, 2023).\nIt would be ideal to circumvent this issue en-\ntirely with alternative approaches. If a model could\nbe trained on lower-quality data but adapted to per-\nform well on real tasks, it might provide a technical\nworkaround. Non-parametric Language Models\n(NPLMs), such as kNN-LMs, have emerged as\na promising approach in this space (Khandelwal\net al., 2020). kNN-LMs extend neural LMs by lin-\nearly interpolating with simple k-nearest neighbor\nLMs. This approach can improve language model-\ning with its memory over a massive collection of\ntexts, usually referred to as a datastore. Khandelwal\net al. (2021) and Shi et al. (2022) validate that kNN-\nLMs achieve better performance on downstream\ntasks compared to standard LMs. The SILO model\nof Min et al. (2024) applies this approach further\nby training a LM exclusively on license-permissive\ndata, and using a non-parametric datastore to im-\nprove the models during inference.\nIn this work, we study the limits of how kNN-\nLMs can be used to improve LLMs. Specifically,\nwe are interested in whether the improvements in\nperplexity seen with kNN-LMs are equivalent to\nother improvements in LM ability, or if improve-\nments in non-parametric memory are orthogonal to\nstandard language modeling. This question relates\nto debates about whether memory is separable from\nother language abilities and how they interact in\nNLP benchmarks.\nTo study this question, we implement large-scale\nkNN-LMs on top of modern open LLMs with two"}, {"title": "2 Related Work", "content": "Retrieval Models Although Large Language\nModels (LLMs) achieve superhuman performance\non a wide range of natural language processing\ntasks, they often produce hallucinations, strug-\ngle with integrating new knowledge, and expose\nprivate information present in the training data.\nRecently, research interest has shifted towards\nretrieval-based LMs, which combine a parametric\nneural model and a non-parametric external data-\nstore (Guu et al., 2020; Karpukhin et al., 2020).\nThese retrieval-based LMs naturally incorporate\nnew knowledge, enhance the factuality of gener-\nated texts, and reduce privacy concerns (Asai et al.,\n2024). Furthermore, studies (Borgeaud et al., 2022)\nhave demonstrated that employing retrieval aug-\nmentation during large-scale pre-training can out-\nperform standard LMs while requiring fewer pa-\nrameters.\nAmong retrieval-based LMs, kNN-LMs (Khan-\ndelwal et al., 2020) emerge as a popular choice\n(Min et al., 2024). Unlike other retrieval models\nthat encode and retrieve documents, kNN-LMs en-\ncode and retrieve tokens. At every token, kNN-\nLMs search for the k most similar tokens from\nthe datastore based on contextualized token em-\nbeddings, which are then turned into a next-token\ndistribution. kNN-LMs linearly interpolate the re-\ntrieved kNN distribution with the output of a base\nLM. They do not require additional training but\nintroduce computational and memory overhead.\nReasoning Retrieval. Little research has been\nconducted on constructing retrieval models for rea-\nsoning tasks. Leandojo (Yang et al., 2023) investi-"}, {"title": "3 k-Nearest Neighbor Large Language\nModels", "content": "Non-parametric language models are variants of\nstandard language models that give the model the\nability to utilize an additional datastore D during\ninference to determine the next word prediction,\np(xt+1|x1...t; D). This datastore may be part of the\noriginal training data, data for adaptation to a new\ndomain, or be used to incorporate continual updates\nor protected data. As these datastores are typically\nquite large, this process requires a retrieval com-\nponent in the loop to find the sparse subset of the\ndatastore that can best inform the current predic-\ntion. Several popular approaches exist including\nDPR (Karpukhin et al., 2020) and REALM (Guu\net al., 2020).\nIn this work, we focus on kNN-LMs due to their\npopularity as an approach to directly improve LM\nperplexity on fixed models without a need for re-\ntraining. As noted in the intro, this approach has\nalso been put forward as a method for circumvent-"}, {"title": "Experimental Setup", "content": "The hyperparameters in-\nclude A, k, and \u03c3. A determines the weight of the\ndatastore, and we consider \u03bb \u2208 {0.1, 0.2, 0.3}. Ad-\nditionally, we retrieve k \u2208 {1600, 2048} neighbors\nand smooth the kNN distribution with a tempera-\nture \u03c3\u2208 {1,3,5, 10}.\nFor each inference model, we use Math and\nWiki datastores for language modeling on the cor-\nresponding evaluation datasets: wikitext and math\ntextbooks. Each datastore represents a specific do-\nmain, and we evaluate the performance of KNN-\nLM on a domain by measuring the perplexity of\neach evaluation dataset. We conduct a grid search\nto find the hyperparameters that yield the lowest\nPPL for each datastore. The optimal hyperparame-\nters for each datastore are later applied across all\ndownstream tasks in our experiments.\nWe provide eight demonstrations for GSM8K\nand three demonstrations for BBH. For the other\ndatasets, we all perform zero-shot inference. We\npresent full details of the experiments in the Ap-\npendix A.\nInference and Retrieval Models. We use\nLlama-2-7b (Touvron et al., 2023), Llama-3-8B\n(AI@Meta, 2024), and Mistral-7B (Jiang et al.,\n2023) as our inference models. For each inference\nmodel, we build the corresponding datastores. The\nkeys are the 4096-dimensional hidden representa-\ntions before the final MLP which predicts the token"}, {"title": "4 kNN-LMs Help In-Domain Perplexity", "content": "To explore how different sources of external knowl-\nedge impact downstream task performance, we ex-\nperiment with two datastores. First, we follow the\nchoice made by Shi et al. (2022), where they iden-\ntify heterogeneous data sources that are broadly\nrelevant to common downstream NLP tasks. In par-\nticular, they mix Wikitext103 (Merity et al., 2017),\nwith other sources including the English portion of\nAmazon Review (He and McAuley, 2016), and CC-\nNEWS (Hamborg et al., 2017) and IMDB (Maas\net al., 2011). We call this datastore Wiki.\nThen, we hypothesize that the commonly ex-\nplored corpora for building datastores do not con-\ntain relevant knowledge to assist with math rea-\nsoning tasks. To maximize the performance gain\non these tasks, we construct a datastore compris-\ning 3.94K mathematical textbooks, sourced from\n(Wang et al., 2023b). These textbooks contain both\ntheorems and practice questions, from which hu-\nmans acquire mathematical knowledge. This datas-\ntore consists of 200M tokens. We will refer to this\ndatastore as Math. We summarize the statistics of\neach datastore in Table 1.\nWe begin by validating past results of kNN-LMs\non language modeling. We present results in Ta-\nble 2. To facilitate meaningful comparisons be-\ntween models with different tokenizers and vocabu-\nlary sizes, we report word-level perplexities. These\nresults show that having access to a non-parametric\ndatastore leads to lower perplexity compared to\nusing a standalone LM across all datasets. This\nimprovement in perplexity is observed when the\ncorpus used to construct the datastore and the one\nused for inference share the same data source. For\ninstance, since the training split of Wikitext103 is\nin Wiki, the LM+Wiki setting achieves the lowest\nperplexity on Wikitext103's validation set. Utiliz-\ning the other datastore results in performance worse\nthan that of the standalone LM."}, {"title": "5 kNN-LMs Can Help Memory-Intensive\nTasks", "content": "We begin by looking at a set of memory-intensive\ntasks, which we believe can be solved by pattern\nmatching at scale without complex reasoning. We\nincorporate three types of tasks: sentiment classi-\nfication, which aims to predict whether the senti-\nment of a text is positive or negative; textual entail-\nment, which assesses the relationship between two\nsentences, determining if it constitutes entailment,\ncontradiction, or neutrality; and topic classification,\nwhich involves identifying the main topic of a text.\nThe datasets included for these tasks are as follows:\n\u2022 For sentiment classification, we include SST-2\n(Socher et al., 2013), movie review (MR) (Pang\nand Lee, 2005), customer review (CR) (Hu and\nLiu, 2004), Rotten Tomatoes (RT), and a variant\nof hyperpartisan news detection (HYP) (Kiesel\net al., 2019).\n\u2022 For textual entailment, we use CommitmentBank\n(CB) (De Marneffe et al., 2019) and Recognizing\nTextual Entailment (RTE) (Dagan et al., 2010).\n\u2022 For topic classification, our datasets are AG News\n(AGN) (Zhang et al., 2015) and Yahoo! Answers\n(Yahoo) (Zhang et al., 2015)."}, {"title": "6 kNN-LMs Hurt Reasoning Performance", "content": "For reasoning tasks, we consider three types:\nknowledge-intensive reasoning, which focuses on\nutilizing world knowledge for making (potential)\nmulti-hop inferences; commonsense reasoning,\nwhich involves leveraging commonsense knowl-\nedge to understand social and physical interactions;\nand mathematical reasoning, which includes arith-\nmetic, logical, and discrete reasoning abilities. The\ndatasets selected for these categories are as follows:\n\u2022 For knowledge-intensive reasoning, we explore\nNatural Questions (NQ) (Kwiatkowski et al.,\n2019), HotpotQA (Yang et al., 2018), ARC Easy\nand Challenge (Clark et al., 2018), OpenbookQA\n(OBQA) (Mihaylov et al., 2018), and MMLU\n(Hendrycks et al., 2020) to assess the model's\nability to apply extensive world knowledge.\n\u2022 For commonsense reasoning, we examine Hel-\nlaSwag (Zellers et al., 2019) and Winogrande\n(Sakaguchi et al., 2021), which test the model's\nunderstanding of social norms and physical laws.\n\u2022 For mathematical reasoning, we utilize DROP\n(Dua et al., 2019), GSM8K (Cobbe et al., 2021),\nand Big Bench Hard (BBH) (Suzgun et al., 2022)\nto evaluate the model's capacity for complex\narithmetic, logical deductions, and handling of\ndiscrete concepts.\nWe present the results for knowledge-intensive\ntasks in Table 6. In stark contrast to the earlier\nfindings, using a standalone LM consistently out-\nperforms kNN-LMs on these tasks. Most surpris-\ningly, on Natural Questions and HotpotQA, which\nconsist of QA pairs constructed from Wikipedia\ndocuments, performance does not improve even\nthough Wiki contains several million Wikipedia\ntokens. Retrieving from Wiki leads to a three-point\ndecrease in performance.\nResults for commonsense reasoning and mathe-\nmatical reasoning tasks are shown in Table 5. The\nstandalone LM once again outperforms kNN-LMs\nmodels on four out of the five datasets. The most\nsignificant differences in performance occur on\nGSM8K. Although incorporating an external data\nstore results in a slight performance increase on\nMistral, this does not demonstrate the effectiveness\nof kNN-LMs on GSM8K. Under Mistral's parame-\nter settings,kNN-LMs has minimal changes on the\npredictions of the standalone LM, merely introduc-\ning some randomness. Finally, although kNN-LMs\ndo not improve GSM8K and Drop over standard\nLMs, we find that retrieving from Math improves\nover retrieving from Wiki."}, {"title": "7 Analysis", "content": "The results of this work show that kNN-LMs gen-\nerally hurt reasoning of models, despite helping\nperplexity and other simpler tasks. In this section,\nwe investigate the cause of this further.\nQualitative Analysis. We conduct qualitative\nanalysis to understand the failures of kNN-LMs\nbetter. In the qualitative analysis, we inspect ex-\namples of knowledge-intensive and mathematical\nreasoning datasets and show the retrieved tokens as\nwell as the proceeding context. Through these ex-"}, {"title": "Is the problem a failure of model weighting?", "content": "Let (ki, vi) be the ith (key, value) pair in D, f(\u00b7)\nmaps a token sequence to its contextual representa-\ntion, and d() measures the distance between two\nvectors.\npkNN(xt+1 | x1:t; D)\n\u03a3 1x+1=vi \u00d7 exp(\u2212d(ki, f(x1:t))).\n(ki,vi) ED\nWhen using a Transformer language model, we\ndefine the distance metric d(\u00b7) as the squared l2\ndistance. To assemble the datastore we run the\nlanguage model over all the documents to collect\nthe necessary hidden states and corresponding next\nword.\nWe investigate whether degraded reasoning capa-\nbilities of kNN-LMs stem from a failure in choos-\ning a good weighting \u5165. This experiment aims to\nanalyze kNN-LMs' behaviors when A is optimal\nfor the downstream task. Specifically, we directly\nsearch for A that maximizes the log probabilities\nof a small set of labeled downstream task exam-\nples. We conduct this experiment on OpenbookQA\nand HotpotQA. We enumerate through retrieving\nk\u2208 {16, 32, 64, 128, 256, 512, 1024, 2048} neigh-\nbors and setting temperature \u03c3\u2208 {1,2,5, 10}. We\nretrieve from Wiki. We initialize \u5165 at 0.5, and as\nthe optimization proceeds, we find that smaller \u5165\nvalues correlate with lower loss. Ultimately, we\narrive at the minimum loss when A is close to 0.\nThis process suggests that without any interpola-\ntion of the kNN distribution, the correct labels of\nthe provided demonstrations receive the highest\nlog probability. Therefore, OpenbookQA and Hot-\npotQA are unlikely to benefit from having simple\nKNN access to Wiki.\n\u2022 kNN-LMs struggle with multi-hop reasoning\nquestions. When the task requires extracting\nmultiple pieces of sentences from the corpus and\nthen combining the information to infer the an-\nswer, kNN-LMs often retrieve tokens that are\ncontextually appropriate and relevant to part of\nthe question, rather than the correct answer. As\nshown in Table 7, for the multi-hop reasoning\nquestion from HotpotQA, the model needs to\nidentify an actor who both starred in Stargate\nSG-1 and guest-starred in Twin Peaks. While the\nrequired information is available in Wikipedia, it\nis distributed across two paragraphs. kNN-LMs\nretrieve only the actors from Stargate SG-1, fail-\ning to combine information from two sources to\nperform accurate multi-hop reasoning.\n\u2022 kNN-LMs are sensitive to the syntax but not\nthe semantics of the question. While kNN-LM\nretrieves the next token that fits the context, it\ncannot distinguish subtle semantic differences\n\u2022 kNN-LMs tend to retrieve high-frequency en-\ntities in the corpus. The entities are often proper\nnouns like person names and locations. If part\nof the answer overlaps with these high-frequency\nproper nouns, kNN-LMs will retrieve them and\nmake wrong predictions, as shown in Table 9 and\nTable 14.\n\u2022 kNN-LMs fail at mathematical reasoning\ntasks. For instance, in the object counting task\nfrom the BBH dataset, even though kNN-LM\nunderstands the context that it needs to retrieve\na number as the next token, it cannot solve the\ncomplex task of first identifying which objects\nare musical instruments and then counting them,"}, {"title": "Is the problem a failure of retrieval?", "content": "We in-\nvestigate whether degraded reasoning capabilities\nof kNN-LMs stem from a failure in retrieval. We\nexamine kNN-LMs' behaviors when retrieval is\nperfect. To achieve perfect retrieval, we include\nthe correct answer among the k nearest neighbors.\nSpecifically, we construct a datastore for Open-\nbookQA, NQ, and HotpotQA, respectively, includ-\ning their train and test examples. We then exam-\nine both perplexity and accuracy. The results, pre-\nsented in Table 6, indicate that while kNN-LMs can\nsignificantly reduce the perplexity, the model does\nnot always derive the correct answer, even when\nthe correct answer is explicitly given as one of the\nk neighbors. Therefore, the failure of reasoning\ncannot be fully attributed to the failure of retrieval.\nHowever, perfect retrieval does improve LM by a\nlarge margin, suggesting that better retrieval is ben-\neficial. Currently, retrieval is performed by finding\nsimilar hidden representations. A training-based\napproach such as RAG (Lewis et al., 2020) has the\npotential to improve retrieval substantially."}, {"title": "8 Conclusions", "content": "We investigate whether the improved perplexity\nobserved in kNN-LMs models can be translated\ninto enhanced reasoning capabilities. We con-\nduct extensive evaluation across 22 datasets. Our\nfindings indicate that while kNN-LMs improve\nperplexity and can achieve better performance\non memory-intensive tasks, they struggle with\nreasoning-intensive tasks, showing a disconnect\nbetween LM ability and task ability. Further qual-\nitative analysis reveals that even when kNN-LMs\nproduce correct answers, these are often the result\nof spurious correlations rather than actual reason-\ning. We believe this places an upper bound on the\nusefulness of these approaches compared to results\nfrom parametric models."}, {"title": "Limitations", "content": "As we are limited by computing budget, we only\nbuild datastores up to 610 million tokens. It is un-\nlikely although not impossible that larger datastores\nbuilt on general web corpus like C4 will lead to"}, {"title": "A More Implementation Details", "content": "Table 11 presents the data sources of the Wiki data-\nstore. Table 12 shows hyperparameters we use for\ndifferent tasks."}, {"title": "B More Qualitative Analysis", "content": "We explain why retrieving from Math improves\nLMs on sentiment analysis. First, we consider a\nsentiment analysis example in Table 13. In this\ntask, given a sentence, a model is required to pre-\ndict whether the sentiment expressed is positive or\nnegative. The sentence in the example expresses\na positive sentiment; however, Llama-2 predicts\nthe sentiment to be negative. kNN-LMs, when re-\ntrieving from Wiki, fail to find sentiment-related\ntokens, and hence also predict a negative senti-\nment. Performing retrieval from Math produced\nthe correct sentiment. However, this is more coin-\ncidental rather than reflective of the model's capa-\nbility, because, although the retrieved tokens dis-\nplay a positive sentiment, the retrieved contexts are\nnot relevant to the test example. we observe that\nsentiment-related content is ubiquitous, regardless\nof the source we use to build the datastore. Even\nin math textbooks, we find many sentences that\nexpress sentiment."}]}