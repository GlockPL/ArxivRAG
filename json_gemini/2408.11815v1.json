{"title": "Great Memory, Shallow Reasoning: Limits of kNN-LMs", "authors": ["Shangyi Geng", "Wenting Zhao", "Alexander M Rush"], "abstract": "K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. In this work, we ask whether this improved ability to recall information really translates into downstream abilities. We extensively evaluate kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning. Results show that kNN-LMs excel at memory-intensive tasks, where utilizing the patterns in the input is sufficient for determining the output, but struggle with reasoning tasks that require integrating multiple pieces of information to derive new knowledge. We further demonstrate through oracle experiments and qualitative analysis that even with perfect retrieval, kNN-LMs still fail to determine the correct answers, placing an upper bound on their reasoning performance. Code and datastores are released at https://github.com/GSYfate/knnlm-limits/.", "sections": [{"title": "1 Introduction", "content": "A foundational property of pretrained language modeling (Peters et al., 2018; Devlin et al., 2019) has been that improvements to the perplexity of the model lead to improvements on downstream tasks. This property is central to the scaling of large language models (LLMs) where researchers focus nearly exclusively on perplexity as a proxy metric for improved general purpose abilities (Kaplan et al., 2020). In recent years, this research has centered primarily on high-quality text data at greater and greater quantities as the limiting component for producing better language models (Hoffmann et al., 2022).\nThis increasing need for data to train language models has led to significant challenges. On one hand, including as much high-quality data as possible results in improved downstream performance. On the other hand, this data is often protected by licenses or copyright, which means training on such data brings legal issues. For example, the recent high-profile lawsuit from the New York Times notes the clear use of their data in OpenAI models (Grynbaum and Mac, 2023).\nIt would be ideal to circumvent this issue entirely with alternative approaches. If a model could be trained on lower-quality data but adapted to perform well on real tasks, it might provide a technical workaround. Non-parametric Language Models (NPLMs), such as kNN-LMs, have emerged as a promising approach in this space (Khandelwal et al., 2020). kNN-LMs extend neural LMs by linearly interpolating with simple k-nearest neighbor LMs. This approach can improve language modeling with its memory over a massive collection of texts, usually referred to as a datastore. Khandelwal et al. (2021) and Shi et al. (2022) validate that kNN-LMs achieve better performance on downstream tasks compared to standard LMs. The SILO model of Min et al. (2024) applies this approach further by training a LM exclusively on license-permissive data, and using a non-parametric datastore to improve the models during inference.\nIn this work, we study the limits of how kNN-LMs can be used to improve LLMs. Specifically, we are interested in whether the improvements in perplexity seen with kNN-LMs are equivalent to other improvements in LM ability, or if improvements in non-parametric memory are orthogonal to standard language modeling. This question relates to debates about whether memory is separable from other language abilities and how they interact in NLP benchmarks.\nTo study this question, we implement large-scale kNN-LMs on top of modern open LLMs with two"}, {"title": "2 Related Work", "content": "Retrieval Models Although Large Language Models (LLMs) achieve superhuman performance on a wide range of natural language processing tasks, they often produce hallucinations, struggle with integrating new knowledge, and expose private information present in the training data. Recently, research interest has shifted towards retrieval-based LMs, which combine a parametric neural model and a non-parametric external datastore (Guu et al., 2020; Karpukhin et al., 2020). These retrieval-based LMs naturally incorporate new knowledge, enhance the factuality of generated texts, and reduce privacy concerns (Asai et al., 2024). Furthermore, studies (Borgeaud et al., 2022) have demonstrated that employing retrieval augmentation during large-scale pre-training can outperform standard LMs while requiring fewer parameters.\nAmong retrieval-based LMs, kNN-LMs (Khandelwal et al., 2020) emerge as a popular choice (Min et al., 2024). Unlike other retrieval models that encode and retrieve documents, kNN-LMs encode and retrieve tokens. At every token, kNN-LMs search for the k most similar tokens from the datastore based on contextualized token embeddings, which are then turned into a next-token distribution. kNN-LMs linearly interpolate the retrieved kNN distribution with the output of a base LM. They do not require additional training but introduce computational and memory overhead.\nReasoning Retrieval. Little research has been conducted on constructing retrieval models for reasoning tasks. Leandojo (Yang et al., 2023) investi-"}, {"title": "3 k-Nearest Neighbor Large Language Models", "content": "Non-parametric language models are variants of standard language models that give the model the ability to utilize an additional datastore $D$ during inference to determine the next word prediction, $p(x_{t+1}|x_{1:t}; D)$. This datastore may be part of the original training data, data for adaptation to a new domain, or be used to incorporate continual updates or protected data. As these datastores are typically quite large, this process requires a retrieval component in the loop to find the sparse subset of the datastore that can best inform the current prediction. Several popular approaches exist including DPR (Karpukhin et al., 2020) and REALM (Guu et al., 2020).\nIn this work, we focus on kNN-LMs due to their popularity as an approach to directly improve LM perplexity on fixed models without a need for retraining. As noted in the intro, this approach has also been put forward as a method for circumvent-\ning the need for high-quality licensed training data in LLMs. Formally kNN-LMs are defined as\n$p(x_{1:T}; D) = \\prod_{t} p(x_{t+1} | x_{1:t}; D)$ \n$= \\prod_{t} (\\lambda P_{kNN}(x_{t+1}|x_{1:t}; D)+(1 - \\lambda)p(x_{t+1}|x_{1:t}))$\nLet $(k_i, v_i)$ be the $i$th (key, value) pair in $D$, $f(\u00b7)$ maps a token sequence to its contextual representation, and $d(\u00b7)$ measures the distance between two vectors.\n$P_{kNN}(x_{t+1} | x_{1:t}; D) \\propto \\sum_{(k_i,v_i) \\in D} 1_{x_{t+1}=v_i} \\times exp(-d(k_i, f(x_{1:t})))$.\nWhen using a Transformer language model, we define the distance metric $d(\u00b7)$ as the squared $l_2$ distance. To assemble the datastore we run the language model over all the documents to collect the necessary hidden states and corresponding next word.\nExperimental Setup. The hyperparameters include $\\lambda$, $k$, and $\\sigma$. $\\lambda$ determines the weight of the datastore, and we consider $\\lambda \\in \\{0.1, 0.2, 0.3\\}$. Additionally, we retrieve $k \\in \\{1600, 2048\\}$ neighbors and smooth the kNN distribution with a temperature $\\sigma \\in \\{1, 3, 5, 10\\}$.\nFor each inference model, we use Math and Wiki datastores for language modeling on the corresponding evaluation datasets: wikitext and math textbooks. Each datastore represents a specific domain, and we evaluate the performance of KNN-LM on a domain by measuring the perplexity of each evaluation dataset. We conduct a grid search to find the hyperparameters that yield the lowest PPL for each datastore. The optimal hyperparameters for each datastore are later applied across all downstream tasks in our experiments.\nWe provide eight demonstrations for GSM8K and three demonstrations for BBH. For the other datasets, we all perform zero-shot inference. We present full details of the experiments in the Appendix A.\nInference and Retrieval Models. We use Llama-2-7b (Touvron et al., 2023), Llama-3-8B (AI@Meta, 2024), and Mistral-7B (Jiang et al., 2023) as our inference models. For each inference model, we build the corresponding datastores. The keys are the 4096-dimensional hidden representations before the final MLP which predicts the token"}, {"title": "4 kNN-LMs Help In-Domain Perplexity", "content": "To explore how different sources of external knowledge impact downstream task performance, we experiment with two datastores. First, we follow the choice made by Shi et al. (2022), where they identify heterogeneous data sources that are broadly relevant to common downstream NLP tasks. In particular, they mix Wikitext103 (Merity et al., 2017), with other sources including the English portion of Amazon Review (He and McAuley, 2016), and CC-NEWS (Hamborg et al., 2017) and IMDB (Maas et al., 2011). We call this datastore Wiki.\nThen, we hypothesize that the commonly explored corpora for building datastores do not contain relevant knowledge to assist with math reasoning tasks. To maximize the performance gain on these tasks, we construct a datastore comprising 3.94K mathematical textbooks, sourced from (Wang et al., 2023b). These textbooks contain both theorems and practice questions, from which humans acquire mathematical knowledge. This datastore consists of 200M tokens. We will refer to this datastore as Math. We summarize the statistics of each datastore in Table 1.\nWe begin by validating past results of kNN-LMs on language modeling. We present results in Table 2. To facilitate meaningful comparisons between models with different tokenizers and vocabulary sizes, we report word-level perplexities. These results show that having access to a non-parametric datastore leads to lower perplexity compared to using a standalone LM across all datasets. This improvement in perplexity is observed when the corpus used to construct the datastore and the one used for inference share the same data source. For instance, since the training split of Wikitext103 is in Wiki, the LM+Wiki setting achieves the lowest perplexity on Wikitext103's validation set. Utilizing the other datastore results in performance worse than that of the standalone LM."}, {"title": "5 kNN-LMs Can Help Memory-Intensive Tasks", "content": "We begin by looking at a set of memory-intensive tasks, which we believe can be solved by pattern matching at scale without complex reasoning. We incorporate three types of tasks: sentiment classification, which aims to predict whether the sentiment of a text is positive or negative; textual entailment, which assesses the relationship between two sentences, determining if it constitutes entailment, contradiction, or neutrality; and topic classification, which involves identifying the main topic of a text. The datasets included for these tasks are as follows:\n\u2022 For sentiment classification, we include SST-2 (Socher et al., 2013), movie review (MR) (Pang and Lee, 2005), customer review (CR) (Hu and Liu, 2004), Rotten Tomatoes (RT), and a variant of hyperpartisan news detection (HYP) (Kiesel et al., 2019).\n\u2022 For textual entailment, we use CommitmentBank (CB) (De Marneffe et al., 2019) and Recognizing Textual Entailment (RTE) (Dagan et al., 2010).\n\u2022 For topic classification, our datasets are AG News (AGN) (Zhang et al., 2015) and Yahoo! Answers (Yahoo) (Zhang et al., 2015)."}, {"title": "6 kNN-LMs Hurt Reasoning Performance", "content": "For reasoning tasks, we consider three types: knowledge-intensive reasoning, which focuses on utilizing world knowledge for making (potential) multi-hop inferences; commonsense reasoning, which involves leveraging commonsense knowledge to understand social and physical interactions; and mathematical reasoning, which includes arithmetic, logical, and discrete reasoning abilities. The datasets selected for these categories are as follows:\n\u2022 For knowledge-intensive reasoning, we explore Natural Questions (NQ) (Kwiatkowski et al., 2019), HotpotQA (Yang et al., 2018), ARC Easy and Challenge (Clark et al., 2018), OpenbookQA (OBQA) (Mihaylov et al., 2018), and MMLU (Hendrycks et al., 2020) to assess the model's ability to apply extensive world knowledge.\n\u2022 For commonsense reasoning, we examine HellaSwag (Zellers et al., 2019) and Winogrande (Sakaguchi et al., 2021), which test the model's understanding of social norms and physical laws.\n\u2022 For mathematical reasoning, we utilize DROP (Dua et al., 2019), GSM8K (Cobbe et al., 2021), and Big Bench Hard (BBH) (Suzgun et al., 2022) to evaluate the model's capacity for complex arithmetic, logical deductions, and handling of discrete concepts.\nWe present the results for knowledge-intensive tasks in Table 6. In stark contrast to the earlier findings, using a standalone LM consistently outperforms kNN-LMs on these tasks. Most surprisingly, on Natural Questions and HotpotQA, which consist of QA pairs constructed from Wikipedia documents, performance does not improve even though Wiki contains several million Wikipedia tokens. Retrieving from Wiki leads to a three-point decrease in performance.\nResults for commonsense reasoning and mathematical reasoning tasks are shown in Table 5. The standalone LM once again outperforms kNN-LMs models on four out of the five datasets. The most significant differences in performance occur on GSM8K. Although incorporating an external data store results in a slight performance increase on Mistral, this does not demonstrate the effectiveness of kNN-LMs on GSM8K. Under Mistral's parameter settings,kNN-LMs has minimal changes on the predictions of the standalone LM, merely introducing some randomness. Finally, although kNN-LMs do not improve GSM8K and Drop over standard LMs, we find that retrieving from Math improves over retrieving from Wiki."}, {"title": "7 Analysis", "content": "The results of this work show that kNN-LMs generally hurt reasoning of models, despite helping perplexity and other simpler tasks. In this section, we investigate the cause of this further.\nQualitative Analysis. We conduct qualitative analysis to understand the failures of kNN-LMs better. In the qualitative analysis, we inspect examples of knowledge-intensive and mathematical reasoning datasets and show the retrieved tokens as well as the proceeding context. Through these examples, we find the following patterns that prevent KNN-LM from retrieving the correct token.\n\u2022 kNN-LMs struggle with multi-hop reasoning questions. When the task requires extracting multiple pieces of sentences from the corpus and then combining the information to infer the answer, kNN-LMs often retrieve tokens that are contextually appropriate and relevant to part of the question, rather than the correct answer. As shown in Table 7, for the multi-hop reasoning question from HotpotQA, the model needs to identify an actor who both starred in Stargate SG-1 and guest-starred in Twin Peaks. While the required information is available in Wikipedia, it is distributed across two paragraphs. kNN-LMs retrieve only the actors from Stargate SG-1, failing to combine information from two sources to perform accurate multi-hop reasoning.\n\u2022 kNN-LMs are sensitive to the syntax but not the semantics of the question. While kNN-LM retrieves the next token that fits the context, it cannot distinguish subtle semantic differences"}, {"title": "8 Conclusions", "content": "We investigate whether the improved perplexity observed in kNN-LMs models can be translated into enhanced reasoning capabilities. We conduct extensive evaluation across 22 datasets. Our findings indicate that while kNN-LMs improve perplexity and can achieve better performance on memory-intensive tasks, they struggle with reasoning-intensive tasks, showing a disconnect between LM ability and task ability. Further qualitative analysis reveals that even when kNN-LMs produce correct answers, these are often the result of spurious correlations rather than actual reasoning. We believe this places an upper bound on the usefulness of these approaches compared to results from parametric models.\nLimitations\nAs we are limited by computing budget, we only build datastores up to 610 million tokens. It is unlikely although not impossible that larger datastores built on general web corpus like C4 will lead to"}]}