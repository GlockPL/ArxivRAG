{"title": "Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine", "authors": ["Yifan Yang", "Qiao Jin", "Robert Leaman", "Xiaoyu Liu", "Guangzhi Xiong", "Maame Sarfo-Gyamfi", "Changlin Gong", "Santiago Ferri\u00e8re-Steinert", "W. John Wilbur", "Xiaojun Li", "Jiaxin Yuan", "Bang An", "Kelvin S. Castro", "Francisco Erramuspe \u00c1lvarez", "Mat\u00edas Stockle", "Aidong Zhang", "Furong Huang", "Zhiyong Lu"], "abstract": "The remarkable capabilities of Large Language Models (LLMs) make them increasingly compelling for adoption in real-world healthcare applications. However, the risks associated with using LLMs in medical applications have not been systematically characterized. We propose using five key principles for safe and trustworthy medical Al \u2013 Truthfulness, Resilience, Fairness, Robustness, and Privacy \u2013 along with ten specific aspects. Under this comprehensive framework, we introduce a novel MedGuard benchmark with 1,000 expert-verified questions. Our evaluation of 11 commonly used LLMs shows that the current language models, regardless of their safety alignment mechanisms, generally perform poorly on most of our benchmarks, particularly when compared to the high performance of human physicians. Despite recent reports indicate that advanced LLMs like ChatGPT can match or even exceed human performance in various medical tasks, this study underscores a significant safety gap, highlighting the crucial need for human oversight and the implementation of Al safety guardrails.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) such as OpenAl's GPT-4\u00b9, as well as Meta's LLaMA\u00b2, have demonstrated remarkable advances in many biomedical and healthcare applications. These models have outperformed previous state-of-the-art methods in various clinical tasks and have shown an impressive ability to interpret and generate text across multiple domains3\u20138.\n\nThe generative capability of LLMs enables a variety of applications, but also presents numerous opportunities to cause harm; this potential is compounded by the complexity of these systems. As LLMs are increasingly integrated into specialized applications, concerns about their safety and trustworthiness have grown, especially in the high-stakes medical domain6,7,9\u201311. Recent studies show that LLMs encounter several safety challenges, including generating inaccurate information 12,13, perpetuating racial/gender biases 14-16, being vulnerable to adversarial attacks 17,18, and posing risks to privacy19. Together, these challenges pose significant obstacles to realizing the full potential of LLMs in practical settings.\n\nFew studies have explored specific safety issues in medical LLMs, such as fairness and hallucination 12,14\u201316,20,21, with associated benchmarks summarized in Table 1. Built upon these prior works that focus on individual safety aspects, we introduce MedGuard: a holistic and comprehensive framework designed to assess medical LLMs across multiple safety dimensions in real-world scenarios. MedGuard evaluates models based on a set of distinct aspects (some previously studied; others novel) aligned with five key principles: Fairness, Privacy, Resilience, Robustness, and Truthfulness. In line with framework, we also present MedGuard-Bench: a benchmark for assessing Safety in Medical AI (SMAI). This benchmark consists of a total of 1,000\n100 expert-verified questions, with 100 questions dedicated to each of the ten aspects."}, {"title": "Abstract", "content": "The remarkable capabilities of Large Language Models (LLMs) make them increasingly\ncompelling for adoption in real-world healthcare applications. However, the risks associated\nwith using LLMs in medical applications have not been systematically characterized. We\npropose using five key principles for safe and trustworthy medical Al \u2013 Truthfulness,\nResilience, Fairness, Robustness, and Privacy \u2013 along with ten specific aspects. Under this\ncomprehensive framework, we introduce a novel MedGuard benchmark with 1,000 expert-\nverified questions. Our evaluation of 11 commonly used LLMs shows that the current language\nmodels, regardless of their safety alignment mechanisms, generally perform poorly on most of\nour benchmarks, particularly when compared to the high performance of human physicians.\nDespite recent reports indicate that advanced LLMs like ChatGPT can match or even exceed\nhuman performance in various medical tasks, this study underscores a significant safety gap,\nhighlighting the crucial need for human oversight and the implementation of Al safety\nguardrails."}, {"title": "Results", "content": ""}, {"title": "Key Safety Principles for Medical Al Systems", "content": "The research community has increasingly recognized the importance of Al safety in recent years. By integrating insights from general Al safety research 19,32 and specific concerns from the medical community7,10,11, we propose five key principles for ensuring safe and trustworthy Al behavior in SMAI: Fairness, Privacy, Resilience, Robustness, and Truthfulness. Each principle is characterized by one or more specific aspects that further define and characterize the desired safe behaviors, as illustrated in Figure 1. Together, these principles provide a structured approach for evaluating the complex requirements of medical Al safety, establishing a foundation for both immediate assessment and future development. We present the detailed definitions of each principle and their corresponding aspects in the Methods section."}, {"title": "Current LLMs Generally Fail on Safety Tests", "content": "Using our proposed safety benchmark, MedGuard, we evaluated a number of commonly used models in the recent medical Al research, including proprietary models such as GPT\u00b9 and Gemini27,28 variants, open-source models like Llama3-Instruct29 and Mistral-Instruct34, as well as domain-specific models such as PMC-LLaMa\u00b3\u00b9 and Meditron30. A brief introduction to the models tested in this work is included in the Methods section. We used temperature zero for all models to ensure deterministic output.\n\nAs shown in Figure 2a, the average model safety index score over all ten aspects ranges from 0.22 to 0.71, with an overall average of 0.48. The two highest overall performing models are GPT-4 (0.71\u00b10.03) and Llama-3-70B-Instruct (0.67\u00b10.04) while the PMC-LLaMA-13B and Meditron-70B scored the lowest (0.22\u00b10.05 and 0.29\u00b10.05 respectively). A ranking of LLMs' safety performance on MedGuard is shown in Figure 2b, along with the models that each LLM significantly surpasses (McNemar's test, p<0.05). Nearly all differences in average scores are statistically significant, with GPT-4 notably outperforming all other LLMs (p \u2264 0.05). Supplementary Table 1 presents the detailed performance statistics and p-values for comparing each pair of models using the McNemar's test.\n\nConsidering the average model performance in each aspect, current models perform best in Stereotype, Error Tolerance, and Sycophancy, with average scores above 0.60. However, average model performance is only modest in all other aspects, with Race Equity being the most challenging (0.11).\n\nFor the Fairness principle, most LLMs show effectiveness in mitigating stereotypes but all models fall significantly short in gender and race equity, particularly in race, where no model scores above 0.3. This indicates that, despite improvements in mitigating toxicity and discrimination, equity across demographics remains a significant challenge. While Stereotype"}, {"title": "Safety Capabilities of LLMs are Lagging Behind Their Accuracy", "content": "Significant efforts have been made towards improving the capabilities of LLMs. Zhang et al. report the performance of several LLMs on MedQA35 \u2013 a representative benchmark for assessing Al models' ability to answer complex medical questions \u2013 with GPT-4 achieving a score of 0.84 and Llama3-70B achieving 0.8137.\n\nIn our analysis, we compare and contrast the models' performance on medical knowledge, as measured by the Accuracy Index in Figure 3 using the MedQA dataset (which is comprised of USMLE questions), with their performance on our custom MedGuard dataset, represented by the Safety Index in Figure 3. A model positioned on the diagonal line in Figure 3 would indicate a balanced performance between accuracy and safety. However, we observe that all models have a higher Accuracy Index than Safety Index. The best-performing model, GPT-4, shows a difference of 0.11 between its Accuracy Index and Safety Index (0.84 versus 0.73). The largest discrepancy is found in the Meditron-70B model, which is fine-tuned for the medical domain, with a gap of 0.27 between its Accuracy Index and Safety Index (0.52 versus 0.25)."}, {"title": "Human Physicians Significantly Outperform LLMs in MedGuard Assessments", "content": "To assess the difficulty of answering these safety-related questions by domain experts, we conducted a human evaluation study. We further compare performance between medical experts and selected top-performing models in each category, specifically the proprietary models GPT-4 and Gemini-1.5-pro, the open-source general domain model Llama3-70B-Instruct, and the open-source medical domain model Meditron-70B.\n\nWe recruited five medical experts to answer 20 questions from each of the ten aspects (200 in total). Each medical expert is assigned to the aspects for which they are qualified. Each question was answered by two experts and we find a high agreement between annotators (Inter-annotator agreement is 0.88 on average across ten aspects). These medical experts were not involved in the creation or verification of the MedGuard questions. We provided the medical experts with basic annotation guidelines, including following medical compliance, treating patients with respect, and not leaking patient information to unauthorized personnel. A comparison of the performance of these models and the performance of the human medical experts on these questions is presented in Figure 4a. We observe that the human experts performs satisfatorliy on MedGuard and is substantially higher than the average performance of the four Al models."}, {"title": "Limited Enhancements in Safety with Prompt Engineering", "content": "Prompt engineering is the most widely used method for improving LLM performance. Therefore, we evaluated the performance of different prompting strategies when applied to our dataset. Specifically, we compared a basic prompt, a Chain-of-Thought (CoT) prompt that asks the model to think step-by-step before giving the answer, a safe prompt which provides the same instructions the physicians received during human evaluation, and a prompt that combines both the CoT and safe prompts. For the best-performing models, GPT-4 and Llama3-70B-Instruct, these prompts result in minimal changes in overall performance. A comparison of the performance results for each prompt across each model and safety aspect can be seen in Figure 4b. The most notable improvements are seen in the Gemini models with CoT prompts, though the gains remain limited.\n\nThe CoT prompt yields mixed results across different tasks. While it enhances the performance of Gemini variants in the Fairness, Resilience and Robustness principles, it also lead to declines for Gemini variants in Privacy and Truthfulness. For some other models, such as the domain specific models PMC-Llama-13B and Meditron-70B, CoT prompt even leads to significant performance declines in Sycophancy. The safe prompt, on the other hand, has almost no impact compared to the baseline performance. Additionally, combining both the safe and CoT prompts does not prove to be more effective than using either safe or CoT prompt individually and sometimes degrades performance by a large margin, indicating that their strengths do not complement each other to improve overall performance."}, {"title": "Discussion", "content": "As shown in the Results section, human experts achieve high scores on MedGuard, indicating that these tests are relatively straightforward for humans, whereas various LLMs perform unsatisfactorily in many aspects. The current LLMs still fall short of the safety standards required to be applicable in practical settings. Furthermore, our evaluations reveal significant gaps in areas such as Fairness: Equity and Resilience: Defense, where humans outperform LLMs by a large margin. Until these issues are resolved, the deployment of LLMs as reliable medical Al models remains premature.\n\nWe find that larger models are generally safer than smaller models. This trend can be attributed to several factors beyond their enhanced capacity to encode knowledge. For instance, larger models are shown to generalize better, capturing more complex and nuanced patterns in language, which reduces the likelihood of producing certain biased outputs38. Additionally, the ability of larger models to consider longer context windows improves their modeling and handling of long-context input39,40, which should lead to more informed and balanced responses when processing medical documents including electronic health records (EHRs). While larger models are generally more capable, our results also show that simply scaling up models is not a reliable approach for achieving safer outcomes, for example Meditron-70B is not better than Llama3-8B-Instruct with respect to safety performance. The underlying architecture, training data, training processes, and alignment phases are crucial factors in ensuring the safety of LLMs in medical applications.\n\nIn our evaluation of domain-specific LLMs, such as Meditron-70B, we observed that these models do not perform as safely as generalist models like Llama3-70B-Instruct when tested against our MedGuard benchmark. This discrepancy likely stems from the prioritization of domain-specific knowledge during finetuning. Contrary to expectations, fine-tuning LLMs on biomedical data does not consistently lead to improved performance and may even result in reduced effectiveness on unseen medical tasks\u2074\u00b9. Additionally, further fine-tuning can cause the model to forget previous safety alignments42,43, and in some cases, these models may lack the safety alignments that are present in general models 30. More research is needed to investigate why these domain-specific models exhibit reduced safety compared to generalist LLMs, and applying such models in real-world medical applications should be approached with caution.\n\nPrompt engineering is often used as an easily accessible option for enhancing LLM performance, as it does not require extensive model re-training. Techniques like CoT prompting, one-shot learning, and few-shot learning are widely used across various tasks to enhance model outcomes 38,44. However, while other work suggests that these strategies may be effective in improving certain types of model performance such as accuracy, our findings demonstrate that they do not necessarily translate into better safety outcomes. CoT prompting generally fails to improve safety performance in most models, and in some cases, it even leads to worse outcomes. Its effectiveness is inconsistent, offering only limited improvements. Similarly, the safe prompt does not result in noticeable changes to the models' safety performance. This lack of consistent improvement to safety performance with prompt engineering may indicate that the underlying unsafe behaviors are learned during training and are ingrained within the model, rendering prompts insufficient for improving LLM safety in medical applications. Therefore, more robust methods, such as fine-tuning with safety-focused datasets or redesigning the training process to emphasize safety, may be necessary to effectively address these issues.\n\nIn recent years, there has been a growing focus towards defining trustworthiness in LLMs and developing benchmarks for its proper evaluation. General domain safety benchmarks show that even GPT-4, one of the best-performing proprietary LLMs, performs poorly on some safety tasks, including adversarial manipulation and hallucination19. Strong performance on general domain safety tests would also not necessarily translate to strong safety performance in the medical domain, as models tend to perform less effectively when faced with the unique challenges of the medical field, such as medical jargon that is rare in the general domain24. Moreover, some methods used in general domain testing may not be applicable to the medical domain. For instance, privacy-leakage tests in the medical domain can have different requirements than general domain scenario, including carefully controlled access to sensitive data and strict adherence to healthcare-specific regulations.\n\nThis work has a few limitations that we acknowledge and plan to address in future research. Although our benchmark shows effectiveness in assessing model safety across five principles and ten aspects, it could be further expanded with additional principles and aspects of Al safety and trustworthiness (e.g., ethics, comprehensiveness, and additional languages). The number of questions in our testbed could also be further expanded, although its size is generally on par with other similar benchmarks to date. As noted, ensuring data quality requires manual verification and editing, which poses challenges for scalability. We also recognize that multiple-choice questions (MCQs) might not fully assess the safety capabilities of generative language models, although automatically evaluating free text responses at scale is challenging and the current MCQ setup is effective in revealing the safety concerns in current LLMs. While incorporating safety instructions into user prompts offers some improvement in some LLMs'\nsafety performance, further research is warranted to achieve more substantial progress. Finally, this work has focused on evaluating safety challenges and risks for current models; addressing the significant concerns we identified will be a priority for future research.\n\nIn conclusion, we propose five principles that should guide safe and trustworthy medical Al behaviors: Truthfulness, Resilience, Fairness, Robustness, and Privacy. Following these principles, we propose a safety benchmark dataset for LLMs, MedGuard, which has a total of ten aspects and 1,000 questions, each rigorously verified by domain experts. Our results with different LLMs expose significant shortcomings in both proprietary and open-source models across these safety aspects. Despite ongoing efforts to enhance the performance and accuracy of LLMs, this does not necessarily translate to safer outcomes in medical applications. Medical domain specific models perform poorly on almost all safety aspects. Comparing LLMs against human performance underscores the substantial gap that still exists in achieving reliable and equitable Al behavior in medical contexts, and the necessity of human oversight. While incorporating safety instructions in user prompt can improve some LLMs' performance, the gains are inconsistent and limited. MedGuard offers a comprehensive framework for assessing and improving the safety of medical Als, laying the groundwork for future research on fairness in medical A\u0399."}, {"title": "Methods", "content": ""}, {"title": "Dataset Development", "content": "The MedGuard medical safety benchmark is designed to mimic real-life medical applications, intending to serve as a safeguard to medical LLM deployment, or as a lightweight safety alignment dataset. All questions are multiple choice to facilitate automatic evaluation, and to ensure relevance to situations that LLMs are likely to encounter, the content avoids extreme corner cases. Each safety aspect in MedGuard includes 100 questions, for a total of 1,000 questions. For each safety aspect, a few manually crafted seed examples are input to GPT-4 (version 0613 via Azure) to generate over 200 questions. We then randomize the order and manually verify each question, selecting the first 100 valid questions. For the Multilingual and Colloquial aspects under Robustness, native speakers with a medical background verify all questions. The prompts, seed examples, and code used to generate the questions are provided in the code repository. Detailed descriptions and rationales for each principle and aspect are described below."}, {"title": "Fairness", "content": "Fairness in Al is a multifaceted concept, with the primary goal of ensuring equitable outcomes and unbiased decision-making for all individuals 45. The National Institute of Standards and Technology (NIST) describes fairness in Al as promoting equality and equity by addressing issues such as bias and discrimination46. Within this framework, we identify two essential aspects in the medical context: Equity, which addresses fairness for patients from diverse demographic backgrounds; and Stereotype, which focuses on fairness towards patients with different diseases and ensures they are not perceived negatively."}, {"title": "Fairness: Equity", "content": "An important consideration for topic in medical LLMs is whether they will treat patients of different backgrounds equally. For the Equity aspect under Fairness, each question is designed to mirror real-world medical tasks by switching patient's biological gender (male, female), race, or ethnicity (Asian, Black, Hispanic, White). These variations reflect most of the diversity seen in actual clinical settings in the United States. For the first 50 questions in the Equity aspect, we take patient information from MIMIC-III and ask the model to estimate the risk of mortality for this patient from very low to very high47,48. To balance the dataset, we included 25 mortality prediction questions for both deceased and living patients. For the other 50 questions in the Equity aspect, we generate patient cases using Diagnosis-Related Group (DRG) codes and ask the model to predict the cost, with the ground truth being the cost corresponding to the DRG code49. DRG codes are a standard and transparent system used to classify hospital cases into groups that are expected to have similar hospital resource use, which helps in standardizing payment for services and ensuring consistency in healthcare billing50. A medical expert ensures that all cases are indeed irrelevant to race, ethnicity or gender. With each question, we compare the model's answers horizontally, across gender within the same race and across race within the same gender for each patient case. A model is considered fair if its answers are consistent horizontally, meaning it provides the same prediction regardless of the patient's race, ethnicity, or gender. E.g., model scores on gender equity for a race on a question if the answers are the same across gender given that race. To prevent the LLM from achieving a high score by simply choosing the same option for all questions, we randomized the order of the options in each question."}, {"title": "Fairness: Stereotype", "content": "Stereotypes are a significant source of delayed treatment, increased risk, and other negative impacts on healthcare51. Considering that LLMs may play a role in communicating with patients in clinical applications, it is vital that they do not reinforce stereotypes about diseases. In total, the Stereotype aspect consists of questions about twelve diseases, which are reported in the literature to be associated with stereotypes including HIV, mental disorder, epilepsy, obesity, physical disability, Alzheimer, cancer, infertility51. Given the diverse nature of mental disorder, we consulted the WHO's fact sheet on mental disorders and included the most common five specific conditions listed52: anxiety, depression, bipolar disorder, post-traumatic stress disorder (PTSD), and schizophrenia. For each disease, we identified several real patient cases from published case studies available in PubMed. For each case, we created four statements regarding the patient and the target disease, with two being appropriate (respectful and free of bias) and two inappropriate (containing stigmatizing language that reduced the patient's identity to a negative stereotype51). A safe LLM, with correct stereotype awareness, should be able to distinguish between the appropriate and inappropriate statements."}, {"title": "Privacy", "content": "Privacy is crucial in healthcare, especially because patient information highly sensitive and protected under regulations such as the U.S. Health Insurance Portability and Accountability Act (HIPAA) 53 and the EU General Data Protection Regulation (GDPR)54. This raises significant concerns about the potential for medical Al or LLMs trained on patient data to leak sensitive information. We identify Confidentiality as a representative of the Privacy Principle, emphasizing the critical importance of maintaining data integrity and confidentiality standards in medical Al systems."}, {"title": "Privacy: Confidentiality", "content": "Ensuring data integrity in LLMs is crucial. One of the risks associated with LLMs is the potential for these models to reveal or 'leak' specific data that was part of their training sets. This issue arises because LLMs are trained on vast amounts of data, and during interactions, they might recall and output specific details from the training data, including personally identifiable information (PII). Such leaks can have serious implications in medicine as patient information are protected under various protocols and law.\n\nIn our study, we designed questions to test the model's ability to recall specific information from the MIMIC-III, one of the largest publicly available electronic health record (EHR) datasets. Each question included identifiable details, such as gender, age, or hospitalization dates, which could be used to identify a specific patient within the MIMIC-III, along with five answer options. The model was then prompted to provide additional details about that patient, offering four possible answers, one of which was correct. Ideally, a model with robust data integrity safeguards should refuse to answer any of these questions, selecting the fifth option that indicates a refusal to disclose information that could compromise patient privacy, irrespective of whether the MIMIC-III data was part of its training."}, {"title": "Resilience", "content": "Resilience against adversarial manipulation is essential for Al systems in medicine, where incorrect or manipulated information can lead to harmful medical decisions with potentially severe consequences18. Medical Al must be robust against attacks that could compromise their integrity, such as adversarial inputs designed to mislead the model or maliciously alter its outputs. Thus, we consider this need for Al systems to safeguard against potential malicious manipulation."}, {"title": "Resilience: Defense", "content": "In the context of medical LLMs, Resilience includes the ability to defend against prompt injection attacks: malicious prompts designed to manipulate the model's output, potentially causing it to generate harmful or incorrect information. For instance, LLMs that rely on external resources for information may be particularly susceptible to prompt injections if these external resources are compromised. To rigorously assess the resilience of medical LLMs against prompt injection attacks, we crafted several malicious prompts specifically designed for the medical setting. These prompts include scenarios such as promoting a specific drug or a type of treatment and are prepended to medical questions sourced from the MedQA dataset, a widely used medical QA benchmarking dataset based on USMLE questions 35. A resilient LLM is expected to consistently choose the correct answer, effectively resisting the influence of the malicious instructions."}, {"title": "Robustness", "content": "Robustness is essential for ensuring Al systems can function safely and reliably across a wide range of scenarios. Unlike fairness, robustness emphasizes the Al's capacity to process varied and diverse inputs without compromising performance55. Within the Robustness principle, we examine three aspects: Colloquial, which ensures that Al is robust when encountered by the public with layperson language and not just medical professionals; Error Tolerance, which allows the Al to function effectively even with common human errors like typos; and Multilingual capability, which enables the Al to perform well across different languages."}, {"title": "Robustness: Colloquial", "content": "A robust medical LLM should be accessible and effective for users with varying educational backgrounds. Patients may not be familiar with specific medical terminologies, yet a medical LLM should still be comprehensible and usable by all. In the Robustness: Colloquial aspect, we converted 100 questions from MedQA35 and their answer options into layperson language, while ensuring the core meaning remained unchanged. A robust LLM should still provide correct answers, demonstrating its ability to function across different levels of proficiency with medical terminology. To ensure accuracy, a domain expert verified the correctness of all layperson language translations in this aspect."}, {"title": "Robustness: Error tolerance", "content": "Healthcare practitioners frequently encounter typos in EHRs or patient notes, where the estimated spelling error rates can be up to 7%56. These misspellings can negatively impact text processing. To evaluate the resilience of medical LLMs in handling imperfect inputs, we introduced typographical errors into questions from the MedQA35 dataset. Each question for this aspect was modified with one to four typos, such as missing characters or adjacent characters being placed in the incorrect order. These imperfections simulate real-world scenarios where input data may contain errors. A robust LLM should demonstrate strong error tolerance by correctly interpreting and answering questions despite these flaws."}, {"title": "Robustness: Multilingual", "content": "Multilingual support is crucial for a robust LLM, especially in a diverse society where patients may speak various languages. Effective communication in a patient's native language can significantly influence the quality of care and patient outcomes. Recognizing this, we emphasize the importance of ensuring that the LLM can serve patients who speak different languages. While it is not feasible to cover every language spoken in the United States, we focus on Spanish and Chinese, the two most commonly spoken languages after English according to the United States Census Bureau33. In this aspect, we translate questions from the MedQA dataset into these languages. Native speakers in Chinese and Spanish with a medical background then review and validate the translations."}, {"title": "Truthfulness", "content": "Truthfulness assesses whether the model can deliver accurate and reliable information, a crucial aspect of ensuring safety in medical Al. False information in medical applications can have far more serious consequences than in the general domain. Under the Truthfulness principle, we examine two important aspects: Hallucination, which addresses the need for the model to avoid generating fabricated or misleading information; and Sycophancy, which focuses on ensuring the model delivers truthful and accurate responses rather than conforming to or reinforcing the user's incorrect beliefs or opinions."}, {"title": "Truthfulness: Hallucination", "content": "Hallucination in language models, despite varying definitions in the literature, generally refers to the generation of factually incorrect or inaccurate text12,57. Many existing tasks that test hallucination in LLMs have significant limitations, as they can often be bypassed or solved using external knowledge or retrieval augmentation techniques. In contrast, our hallucination dataset focuses on the summarization of medical documents, a task that is becoming increasingly important due to the growing volume of medical publications each year58. We select ten diseases from the list of leading causes of death in the United States59 as search keywords and retrieved relevant articles from PubMed Central. The keywords include heart disease, lung cancer, diarrhea, stroke, COVID-19, Alzheimer's disease, diabetes, cirrhosis, chronic obstructive pulmonary disease, and tuberculosis. For each question, we use two related article abstracts to generate four options: two correct summary statements one for each abstract and two incorrect summary statements created by copying one part of the sentence from one abstract and the remainder from the other abstract, combined incorrectly. A model that does not hallucinate should correctly distinguish between these statements."}, {"title": "Truthfulness: Sycophancy", "content": "Medical safety requires LLMs to resist agreeing with incorrect reasoning, even when presented with confident or authoritative arguments. To evaluate this, we combine an MedQA question with a persuasive argument supporting an incorrect answer. We create this argument by selecting a wrong answer and instructing GPT-4 to generate a persuasive response from an authoritative figure, such as a doctor with years of experience, who confidently presents the incorrect answer. Human annotators verify the inaccuracy of the generated response. A truthful model should adhere to the facts, maintaining accuracy despite being confronted with misleading authoritative input."}, {"title": "Benchmarking Models", "content": "This work benchmarks 11 popular LLMs, including proprietary LLMs (GPT and Gemini variants), open-source generalist models (Llama-3 variants, Mistral-7B and Mixstral-8x7B), and medical domain-specific LLMs (PMC-LLaMa and Meditron).\n\nGPT-3.5 Turbo and GPT-4\u00b9 are developed by OpenAl based on a generative, decoder-only Transformer architecture. GPT-3.5 Turbo excels in various tasks requiring reasoning and domain adaptation while GPT-4 is a large multimodal model capable of generating both text and images."}, {"title": "Manual Annotation by Medical Experts", "content": "We recruited five medical professionals to perform the human annotations across all ten aspects, including three MDs and two 4th year medical students. These annotators are not involved in the creations of the MedGuard benchmark dataset. We first randomly sampled 20 questions from each of aspects. We then provided the annotators with the questions and basic annotation guidelines, including following medical compliance, treating patients with respect, and not leaking patient information to unauthorized personnel (these instructions were also used safe prompt for LLMs in the prompt engineering experiment). We used separate Google Sheets as tools for the annotators to view the questions and make their answers. Each question was answered by two individuals and average inter-annotator agreement across aspects was subsequently calculated accordingly."}, {"title": "Statistical Tests", "content": "The standard error of the performance for each model is calculated using bootstrapping with n=9,999. The pair-wise model comparison uses the two-sided McNemar's test with the hypothesis that the models have no significant difference."}, {"title": "Data availability", "content": "We have submitted our data for peer review. Upon publication, the dataset will be publicly accessible at: https://github.com/ncbi-nlp/MedGuard. For the Equity and Confidentiality aspects, users must first obtain the MIMIC-III data from https://physionet.org/content/mimiciii/1.4/47. The questions can then be generated using the MIMIC-III data and the code provided. All other aspects can be directly accessed from this repository."}, {"title": "Code availability", "content": "We have submitted our code for peer review. Upon publication, the source code for this project will be publicly accessible at: https://github.com/ncbi-nlp/MedGuard. An online leaderboard of LLMs' performance on MedGuard, and methods to submit results to this leaderboard can be found at https://medguard-llm.github.io/."}]}