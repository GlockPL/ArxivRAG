{"title": "Generative AI Agents in Autonomous Machines: A Safety Perspective", "authors": ["Jason Jabbour", "Vijay Janapa Reddi"], "abstract": "The integration of Generative Artificial Intelligence (AI) into autonomous machines represents a major paradigm shift in how these systems operate and unlocks new solutions to problems once deemed intractable. Although generative AI agents provide unparalleled capabilities, they also have unique safety concerns. These challenges require robust safeguards, especially for autonomous machines that operate in high-stakes environments. This work investigates the evolving safety requirements when generative models are integrated as agents into physical autonomous machines, comparing these to safety considerations in less critical AI applications.\nWe explore the challenges and opportunities to ensure the safe deployment of generative AI-driven autonomous machines. Furthermore, we provide a forward-looking perspective on the future of AI-driven autonomous systems and emphasize the importance of evaluating and communicating safety risks. As an important step towards addressing these concerns, we recommend the development and implementation of comprehensive safety scorecards for the use of generative AI technologies in autonomous machines.", "sections": [{"title": "1 Introduction", "content": "Generative Artificial Intelligence (AI) refers to models capable of generating new data by learning patterns and distributions from existing datasets. Powered by neural network architectures such as transformers [187] and diffusion [64, 127, 167], these models have shown remarkable proficiency in generating realistic, high-quality data. Examples include natural language processing (NLP) systems such as text generators and image synthesis models, both of which are capable of producing outputs that closely mimic human-like interactions [25] and visuals [149, 152]. Models, such as GPT-4 [4] and DALL-E [134, 135], expand the potential of generative AI with capabilities such as contextual understanding, cross-domain generalization, and the ability to handle various multimodal inputs. Generative Al models have become valuable in autonomous systems (e.g. self-driving cars, robotic manipulation), where their context-aware reasoning and semantic understanding drive breakthroughs in navigation, perception, and task planning [14, 30, 36, 212].\nGenerative AI has seen widespread adoption in virtual settings, which comes with safety challenges such as generating false information, often called \u201challucinations\" [79, 100], or producing inappropriate visuals [132]. Although these issues are significant, they generally do not pose physical safety risks since they occur within virtual environments. When generative AI is applied to systems with physical agency, such as self-driving cars, the safety stakes are significantly higher. Such systems operate in the physical world and directly affect human lives, property, and critical infrastructure. This not only amplifies the existing safety challenges faced by virtual agents but also introduces new ones. As illustrated in Figure 1, the transition of generative models to systems with physical agency introduces additional challenges such as catastrophic forgetting, real-time processing constraints, increased resource requirements, lack of formal guarantees, and the need for better real-world grounding. Moreover, existing problems like hallucinations and harmful content generation become more critical in physically-embodied systems, making safety-critical considerations more pronounced.\nIn this paper, we first provide an introduction to generative AI, outlining its fundamental principles and the types of models that have driven its success. We then survey and categorize the most recent applications of generative models in autonomous systems. Following this taxonomy, we explore the unique safety challenges posed by integrating generative Al into autonomous machines and present the latest opportunities to address these challenges. We recommend implementing a safety scorecard to enable an easy high-level assessment of generative AI models in autonomous machines.\""}, {"title": "2 Generative AI Models Background", "content": "We provide background on the development of generative AI models that have enabled deeper contextual understanding and reasoning capabilities. This lays the groundwork for their use cases."}, {"title": "2.1 Early Image Generating Models", "content": "The rise of generative models began with autoencoders [11, 13, 111, 137], which compress data into a latent space and reconstruct inputs by minimizing reconstruction error. While useful for dimensionality reduction and denoising, they struggled with generating realistic data. Variational Autoencoders (VAEs) [84] and their conditional variants [166] introduced probabilistic latent spaces, enabling data generation through sampling. Despite improving realism, VAEs often produced images that lacked sharpness and detail.\nA major breakthrough came with Generative Adversarial Networks (GANs) [56, 82] and their conditional variant [113], which pit a generator and discriminator against each other in a zero-sum game to learn how to generate realistic data. GANs have excelled in tasks like image synthesis [42, 74, 82, 223], super-resolution [90], and text-to-image generation [138], often using U-Nets [148] for detail preservation. Despite successes, GANs suffer from unstable training and inconsistent quality [9, 21]. These challenges paved the way for the development of diffusion models, which offered a more robust approach to image and data generation."}, {"title": "2.2 Early Sequence Generating Models", "content": "In parallel to advancements in generative models for image data, sequence models underwent their own evolution. Recurrent Neural Networks (RNNs) [145, 150, 155] were among the first models to handle sequential data by utilizing hidden states, but they struggled with long-term dependencies [15] and gradient issues [126]. This led to Long-Short-Term Memory (LSTM) networks [66, 170], which introduced gating mechanisms to better handle information flow across long sequences. LSTMs significantly improved handling long-term dependencies, excelling in tasks such as language translation [176, 200], time-series forecasting [163, 175], and music generation [198]. However, they struggled with scaling to very long sequences and large datasets due to efficiency limitations [87, 160]. This led to the development of transformers, which have since become the backbone of modern NLP and other sequence-based tasks."}, {"title": "2.3 Diffusion Models", "content": "Unlike GANs, which rely on adversarial training, diffusion models iteratively refine noisy data to produce high-fidelity outputs [64]. Diffusion models operate in two stages: the forward (noising) process and the backward (denoising) process, both modeled as Markov chains. In the forward process, Gaussian noise is added step-by-step until the data become indistinguishable from random noise. During training, the model learns to predict the noise present and the uncertainty (variance) in this prediction at each step. Once trained, diffusion models can take random noise-either unconditioned or guided by input conditions-and progressively refine it through the denoising process to generate highly realistic outputs. This iterative refinement enables the model to transform noise into structured data, such as high-resolution images [45, 67, 128, 146], 3D models [69, 80, 108, 179], or videos [20, 63, 65]. The result is superior in stability and quality compared to previous models."}, {"title": "2.4 Transformer Models", "content": "The transformer architecture [187] solves the limitations of LSTMs, such as difficulties with long-term dependencies and lack of parallelization. Transformers process entire sequences in parallel for greater efficiency, adding positional embeddings to maintain the order of tokens without the need to process them sequentially. The self-attention mechanism in transformers captures relationships between tokens by calculating how much focus each token should have on others, updating their representations based on these interactions. This process helps the model understand the significance of each token, and the decoder uses this information to predict the most likely next token. Transformers have redefined sequence modeling and now serve as the backbone of cutting-edge systems in text generation [60, 102], language translation [105, 205], and code generation [16, 89, 92, 177], thanks to their ability to capture long-range dependencies and process sequences in parallel."}, {"title": "2.5 Multi-Modal Language Models", "content": "As transformer models proved highly effective in handling sequential data, their capabilities soon extended beyond text to multimodal applications, integrating different data types such as images, text, and speech. A key milestone in the shift towards multimodal models was the introduction of the Vision Transformer (ViT) [49], which applied transformers to images by splitting them into patches treated as tokens, much like words in language models. Building on this, CLIP (Contrastive Language-Image Pretraining) [133] aligned vision and language by training on image-text pairs, associating visual data with textual descriptions. This alignment became foundational for Vision-Language Models (VLMs) [26], which integrate image and text inputs to generate text outputs, driving applications such as image captioning and visual question answering [6, 27, 97, 190]. Multimodal capabilities have since expanded beyond vision and language, incorporating other modalities such as audio [44]."}, {"title": "2.6 Foundation Models", "content": "The evolution of generative models has led to the rise of foundation models, large-scale models trained on vast datasets, often across diverse modalities [10]. These models are notable for their immense size: GPT-3, for example, has 175 billion parameters and was trained on 300 billion tokens, equivalent to 45TB of compressed plaintext and requiring 3,640 petaflop-days of compute during pre-training [24]. This massive scale enables foundation models to capture intricate patterns and relationships that smaller models cannot. Fine-tuned using Reinforcement Learning from Human Feedback (RLHF) [124], they generalize between tasks and better align with human expectations. As a result, foundation models exhibit not only impressive generative capabilities, but also advanced contextual understanding and reasoning [25, 193], allowing them to grasp nuanced prompts and produce outputs that reflect deep comprehension of visual and textual inputs [4]. This ability to reason and adapt has spurred a new wave of advancements in fields such as robotics, unlocking capabilities previously unattainable."}, {"title": "3 Environments", "content": "As shown in Figure 1 (under \"Environment\"), the aforementioned generative Al models can serve in two different agentic forms: virtual and physical. In virtual agency, the generative AI agents act in digital environments, where the action space is limited to tasks such as generating text, images, or music. These agents have control over virtual elements, but their actions do not directly influence the physical world. Although virtual agents can pose challenges like sycophancy or leaking copyright content, the impact remains confined to the digital realm. Although significant, the risks posed by virtual agency are generally confined to non-physical harms.\nIn contrast, physical agency has a larger and more complex action space. Generative models in this domain have physical influence, such as moving a robotic arm, navigating a vehicle, or training an agent in simulation that will later be deployed in real-world environments. The consequences are more severe: errors can lead to physical damage, injury, or critical failures.\nMoving from virtual to physical environments greatly increases the need for safety due to direct real-world interactions. This paper focuses on physical agency, where generative Al requires higher and special standards of safety than in virtual settings."}, {"title": "4 Generative Al in Autonomous Machines", "content": "We explore the latest applications of generative AI in robotics, highlighting how models such as diffusion, transformers, and foundation models are advancing autonomous systems. Each category begins with a discussion of the pressing challenges in the area, followed by a description of generative methods that offer effective solutions."}, {"title": "4.1 Simulator Scene Generation", "content": "A key pressing challenge in the development of autonomous machines is the creation of scalable, realistic, and diverse simulation environments. Developing for the real world requires significant capital and labor and poses safety risks, making scaling impractical. So robotics turns to simulation, but one of the major challenges is the labor intensive process of creating realistic and diverse simulation environments. Simulations are difficult to control, as designing visual assets, configuring physics parameters, and defining task-specific details are time-consuming tasks that limit scalability. The manual process of identifying and modeling real-world scenes can produce high-quality results, but it often lacks the complexity and variability required for robust policy transfer, frequently leading to a sim-to-real gap. Scalable simulation content must meet three key criteria: it must be realistic enough to ensure that machine learning models trained within the simulation can transfer effectively to the real world; it must offer diversity in scenes, assets, and tasks to enable generalizable learning; and it must be easily controllable, allowing for the targeted generation of specific scenarios.\nGenerative models offer a scalable solution to many of the challenges in creating realistic and diverse simulation environments. By automating the generation of 3D assets, textures, and physics configurations from high-level inputs, these models significantly reduce manual effort, enabling the creation of diverse and scalable environments [35, 83, 122]. Language models have enhanced controllability by transforming high-level descriptions into specific simulation scenarios, making it possible to generate complex simulations from simple text inputs [95, 178, 221]. Generative models also streamline sim-to-real transfer by automating the design of reward functions and simulation parameters, reducing the need for manual adjustments and improving adaptability [109, 208]."}, {"title": "4.2 Data Generation and Sensor Refinement", "content": "Another challenge is the acquisition of large and diverse datasets necessary for generalizable robot learning. Robot learning has great potential for generalizing across a wide range of tasks, environments, and objects, but achieving this requires large and diverse datasets, which are costly and difficult to collect in real-world settings. Unlike the abundance of text data readily available on the web, robotics depends on specialized data sources like tactile sensing for identifying and grasping objects. Collecting this data is often limited by expensive techniques like on-robot teleoperation. Furthermore, the rigidity of most autonomous machine setups makes it difficult to collect diverse data across various scenarios, leaving robotics datasets constrained to single setups with only a few hours of data. Even with access to sensors like LiDAR or RGB cameras, hardware limitations often lead to low resolution, sparse data.\nGenerative models help scale robotic datasets using intelligent labeling and next-generation data augmentation techniques. Text-guided diffusion models can generate realistic images by enriching existing datasets with new objects, scenes, and annotations, while modifying textures, shapes, and backgrounds in ways that are physically consistent with real-world tasks. Pre-trained VLMs expand these datasets further by adding detailed semantic concepts, transforming basic descriptions like \u201cpick apple\u201d into more nuanced variations such as \u201cthe red-colored fruit\", enriching the data for downstream tasks [34, 86, 202, 211]. Generative models also augment sensor-rich datasets, such as tactile sensory data for grasping, and expand grasp datasets with a wider variety of objects [189, 220]. For more common sensory inputs, such as cameras or LiDAR, diffusion models enhance low-resolution data by upscaling it and creating higher-quality representations, improving perception in autonomous systems [62, 121, 143, 222].\""}, {"title": "4.3 Context-Aware Navigation & Task Planning", "content": "General-purpose robots has long been a goal of the robotics community, yet current systems remain brittle, often failing when confronted with unfamiliar environments. Unlike humans, who navigate the physical world using prior knowledge, robots lack the common sense understanding needed to handle everyday tasks and adapt to new settings. Humans excel at building cognitive maps, enabling them to locate landmarks, infer spatial layouts, and use semantic knowledge to navigate unfamiliar spaces. For robots to perform human-like exploration and task planning, they need to grasp how environments are organized semantically, such as recognizing that books are typically found near bookshelves in a living room. Tasks like \"cleaning the kitchen\" require not just an understanding of objects, such as knowing that a sponge is used for wiping surfaces, but also the logical sequences of actions, such as placing soap on the sponge before wiping the surface, and the ability to adapt to user preferences.\nVLMs and large language models (LLMs) allow robots to integrate common sense reasoning and semantic understanding, helping them associate language commands with physical spaces and objects, enabling them to navigate and perform tasks in new environments [31, 58, 72, 76, 164, 173, 206, 216]. These models also facilitate the creation of semantic maps that help robots build a structured understanding of their surroundings for efficient exploration, object localization, and memory of past experiences [30, 70, 101, 136, 139, 156, 196]. Robots can now combine multimodal sensors, to enhance physical reasoning for tasks that involve material properties, such as identifying soft objects by touch [210]. Generative AI allows robots to navigate based on learned semantic knowledge, inferring object relations, and spatial layouts (e.g., recognizing that certain objects are located together) [17, 159, 209]."}, {"title": "4.4 Visuomotor & Instruction-Aware Motion Planning", "content": "Humans can easily use a wide range of tools, from hammers to joysticks, with minimal instruction by understanding both the physical and cognitive affordances of objects. Robots, however, struggle with this adaptability, especially when it comes to handling diverse, articulated objects like home appliances or furniture. Manipulating these objects requires understanding both their physical structure and functional uses, which is complicated when an object's appearance doesn't align with its function. For example, turning on a stove by rotating a knob requires recognizing it as a control mechanism, not as an object to push or pull. This mismatch between structure and function challenges robots to bridge the gap between high-level understanding and low-level motion planning. Multimodal data, such as visual and linguistic inputs, are key to advancing their ability to adapt to human-like, lifelong learning when handling manipulation tasks, yet mapping these high-dimensional observations to low-level actions has been a long-standing challenge in robotics.\nGenerative models, particularly diffusion models and transformers, have demonstrated remarkable capabilities in generating complex motion trajectories, helping robots handle dynamic tasks like reorienting objects, planning precise paths, and executing 7-DoF manipulator trajectories in 3D [28, 77, 104, 112, 114, 115, 120, 129, 192, 201, 207, 214]. Beyond motion planning, these models have bridged the gap between high-level language inputs and low-level motor control, providing robotics with instruction-aware manipulation [32, 33, 54, 103, 195, 212]. Generative models enable flexible conditioning, allowing robots to integrate visual perception with control policies and handle highly multimodal data by modeling the full distribution via generative models. They can augment classic algorithms like Model Predictive Control (MPC) by giving them the \"eyes\" of VLMs, enhancing their ability to perceive and respond to complex environments [36, 46, 162, 168, 188, 219]. LLMs have transformed how robots reason about affordances and tool use, allowing them to infer an object's function from its appearance and adapt to new tools in a given context [19, 71, 96, 141]."}, {"title": "4.5 Human-Language Semantic Understanding and Communication", "content": "Natural language offers a rich interface for humans to interact with robots, enabling even those with minimal training to direct behaviors, express preferences, and provide feedback. However, interpreting and following language commands has been a significant challenge in robotics. Humans can communicate through direct instructions (e.g. \"Move the box to the corner\") or more ambiguous commands (e.g. \"We're getting ready for a big event!\"), and robots must learn to navigate this range of specificity. To be effective in human environments, robots need to understand the semantics of language and learn from human feedback, which can include adjusting goals (e.g., \"Clean the bedroom instead\"), adding constraints (e.g., \"Avoid stepping on the rug\u201d), or offering hints when the robot is stuck. They must also learn from observations and demonstrations, much like infants learning through interaction. While natural language plays a central role in communication, human-robot collaboration also relies on non-verbal cues such as gestures. Gestures like pointing provide an efficient way to express intent, but robots must be able to accurately infer the meaning within the context of a task-a challenge that further complicates human-robot interaction.\nUsing LLMs, VLMs, and transformers, robots can follow natural language instructions or learn from minimal human demonstrations, significantly improving the interface between humans and robots by allowing more natural communication [12, 57, 75, 81, 144]. Beyond language-based instructions, LLMs and transformers have also expanded nonverbal communication, using gestures and sketches as input to help robots infer human intent and collaborate more naturally [59, 94]. Building on these capabilities, generative models allow robots to interpret simple and vague instructions while grounding commands in visual and contextual cues, improving their ability to handle diverse and complex environments [14, 99, 171, 174, 180]. Also, robots are able to adapt to feedback in real time, refining their actions based on corrections and even asking for clarifications when instructions are ambiguous, making them more accessible to nonexperts by facilitating more clear and interactive human-robot collaborations [61, 93, 110, 158]."}, {"title": "4.6 Generative Machine Design", "content": "Generative design is emerging in robotics, where models optimize and accelerate the design process by efficiently exploring the vast design space. Unlike time-consuming manual methods, generative models enable rapid iteration and the exploration of multiple configurations to identify optimal solutions. This is particularly useful in designing modular robots, where a set of components such as bodies, legs, and wheels must be configured for specific tasks or terrains [68]. Generative design also addresses complex tasks in manipulator and soft robot design, generating custom geometries that are highly adapted to their tasks [29, 204]. Recent advances have developed physics-informed diffusion models, enabling the generation of designs that not only meet aesthetic or structural criteria, but also optimize performance based on physical simulations, such as minimizing drag coefficients in vehicle design [8, 191]."}, {"title": "4.7 Safety-Aware Machines", "content": "Safety challenges are inherent in autonomous machines, where these robots must correct potential failures and explain their decisions to earn human trust. Ideally, a truly robust system would integrate multiple sensory inputs-visual, auditory, and tactile-so it can effectively detect failures, as certain cues are best identified by specific sensors. For example, auditory cues may detect a door slamming that visual sensors miss if they are obstructed. Like human drivers prioritizing important objects, robots must also extract key information from dense data while ignoring irrelevant details. It is also important that robotic systems are immune to the risks of out-of-distribution (OOD) input that may not be captured during simulation, as these can trick the system. For example, semantic anomalies (e.g., billboard stop signs) have been known to cause failures like autopilot disengagement or phantom braking [51]. \u03a4\u03bf enhance safety, robots can incorporate various feedback mechanisms depending on the task. Human feedback through natural language corrections allows for real-time adjustments during long-horizon tasks where small errors may accumulate, while contact feedback is crucial in manipulation tasks to differentiate between desired contact (e.g., grasping) and harmful contact (e.g., collisions).\nDiffusion models have been used to generate OOD scenarios, enabling robots to train on rare and nuanced failure modes like compounding errors in long-horizon tasks [153, 218]. Risk-aware planning has improved with contact-awareness and future trajectory generation, allowing robots to navigate high-risk environments more reliably [39, 41, 123, 203]. Multimodal models, which fuse data from visual, auditory, and tactile inputs alongside control signals, enhance safety by offering comprehensive explanations for robot actions and failures [107, 142, 157, 213]. Generative models also enable real-time human feedback, allowing robots to refine their actions and even request assistance when necessary [140, 161]. Finally, LLMs have strengthened anomaly detection, allowing robots to recognize and reason about semantic anomalies [51, 165]."}, {"title": "4.8 Generalist Agents", "content": "The pursuit of generalist agents marks a new frontier in robotics, where machines must seamlessly adapt to diverse environments, tasks, and hardware platforms. This adaptability addresses the limitations of current systems, which struggle to generalize across conditions [224]. Robots are now building vast skill libraries, with LLM-guided bootstrapping enabling agents to expand their repertoires over time with minimal supervision [217]. Another key example is the Robotics Transformer (RT-1), a 35 million-parameter model trained on 700+ tasks and 130k demonstrations collected by 13 robots over 17 months, capable of generalizing to new tasks and scenes [23]. Building on this, RT-2 pushes generalization further by combing web knowledge and robotic data in a 55 billion-parameter model [22]. Octo further exemplifies versatility, trained on 800k trajectories across 9 platforms, and allows users to fine-tune within hours to new sensory inputs or action spaces [183]. At an even larger scale, PaLM-E, a 562 billion-parameter model, integrates sensor modalities with language models, unlocking new possibilities for real-time interaction with the physical world [50]. Finally, RT-X, trained on data from 22 robots and 160,000+ tasks, shows improvements in generalization and highlights the impact of large-scale data collection [125]. Together, these advancements signal the dawn of adaptable, general-purpose agents capable of transitioning seamlessly across tasks, environments, and platforms."}, {"title": "5 Safety Challenges and Opportunities", "content": "In virtual domains, chatbots and image generators are among the most common applications of generative AI, with ChatGPT alone serving over 180.5 million users [43]. Much of the safety focus has understandably centered on these use cases. Common challenges include generating inappropriate content, harmful outputs, and amplifying social biases [18, 37]. Additionally, interpretability issues arise, where models lack transparency in their outputs [52]. LLMs can also respond to harmful requests [130], generate deep-fakes that compromise authenticity [147], and inadvertently violate copyright laws [38]. Privacy risks arise from models that leak sensitive information [194], while hallucinations may lead to false or misleading information, particularly in fields such as law and medicine [194]. Additional concerns include sycophancy, where models reinforce user misconceptions, and miscalibration, where they exhibit undue confidence in incorrect or outdated information [106].\nIn autonomous machines, many of these challenges persist and are often amplified due to the physical nature of tasks, while other challenges are unique to this domain. The following sections explore the key safety challenges of applying generative AI to autonomous machines, categorized into Model and System challenges."}, {"title": "5.1 Model", "content": null}, {"title": "5.1.1 Hallucinations", "content": "Challenge: Hallucinations are a major challenge with LLMs currently deployed in the virtual domain, where models confidently generate outputs that seem plausible but are incorrect and untethered from reality. Such false outputs pose amplified safety risks compared to virtual applications. For instance, an LLM-based planner [140] tasked with cooking could hallucinate that the stove is off when it is actually on, leading the robot to incorrectly handle a hot surface, potentially causing burns or even a fire hazard. These hallucinations can result in dangerous outcomes, highlighting the need for more robust generative models in safety-critical systems.\nOpportunities: Developing systems that ask for help instead of hallucinating can greatly improve safety. In the earlier stove scenario, this would require the language model to recognize its uncertainty [181] and seek clarification (e.g., asking if the stove is on or off) [140]. Chain-of-thought (CoT) prompting, which encourages step-by-step task breakdowns, is another promising approach for improving reasoning [193]. However, it is important that CoT explanations faithfully reflect the model's true reasoning. Studies have shown that while LLMs can generate human-appealing explanations, these often misrepresent the true reason for a model's prediction [88, 182, 186], leading to further safety concerns. Other promising techniques are also being developed that can help reduce hallucinations in LLM-based systems, such as real-time verification and rectification which validates outputs on the fly making sure that the output is grounding in reality [184]."}, {"title": "5.1.2 Catastrophic Forgetting", "content": "Challenge: In robotic systems that fine-tune LLMs for specific tasks, catastrophic forgetting can occur, where improvements on the fine-tuned task come at the cost of forgetting broader knowledge learned during pretraining. Since fine-tuning datasets are smaller and less diverse, models risk losing critical capabilities from pretraining, and it may not always be clear what the model has forgotten, posing a safety risk as the system might unknowingly be incapable of reasoning about certain situations [215]. Alternatively, another approach to teach LLMs new tasks is through in-context learning, which occurs during inference without updating model weights, allowing for fast adaptation to language instructions. However, this method is constrained by limited context windows, such as LLaMA2's 4096 token limit [185], which restricts in-context learning to short-term interactions. In robotics, where human feedback is beginning to play a critical role in guiding behavior, this limitation poses a safety risk. If human instructions accumulate over long multi-step tasks and fall outside the context horizon, critical commands can be forgotten, leading to unsafe actions [93].\nOpportunities: Current work is focused on reducing the amount of pretrained capabilities lost during fine-tuning, such as Conjugate Prompting, which artificially makes the task appear farther from the fine-tuning distribution [85]. Additionally, efforts are underway to better understand the capabilities of in-context learning, as this emergent paradigm remains not fully understood [53, 197]. Techniques like Rotary Positional Embeddings (RoPE) [172] and LongROPE [47] are being developed to extend context windows and help models retain longer sequences. However, these methods increase system memory usage, which could pose challenges for edge devices, as discussed in Section 5.2.2."}, {"title": "5.1.3 Lack of Formal Guarantees", "content": "Challenge: Generative models lack formal guarantees, meaning they provide no mathematically-proven assurances about stability, safety, or performance. In contrast, traditional robotics algorithms, such as PID controllers, LQR, and MPC, offer formal guarantees of stability (e.g., the system will converge to a desired state or maintain a trajectory) and performance (e.g., minimizing error over time). Similarly, motion planning methods like RRT and A* provide guarantees of completeness (if a solution exists, the algorithm will find it) or optimality (the solution is the best possible under the given constraints). These approaches ensure predictable behavior.\nGenerative models, however, rely on training data and probabilistic reasoning, which can lead to unpredictable outcomes, especially in unfamiliar or OOD scenarios. This probabilistic nature makes it challenging to provide deterministic safety or stability when replacing traditional algorithms with generative models.\nOpportunities: There is potential in hybrid approaches that integrate control theory methods like Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs) to enforce safety and stability in generative Al systems. CBFs construct safe control constraints to prevent unsafe states, while CLFs ensure stable goal convergence. Recent work applies these methods in areas like constraint-aware diffusion models [91], CBF-inspired interventions for safe LLM outputs [117], and diffusion models leveraging both CBFs and CLFs to enforce safety and stability properties [118]."}, {"title": "5.1.4 Lack of Real-World Grounding", "content": "Challenge: A significant weakness of language models is their lack of real-world grounding, meaning they were not trained to directly perceive or interact with the physical world, making it difficult to apply them for decision-making in specific robotic embodiments [136]. For example, asking a language model to describe how to clean a spill might generate a reasonable narrative, but it may not be applicable to a particular robot morphology [5]. This lack of physical experience is a key reason why techniques developed in areas like visual question answering (VQA) and view-based navigation have not transitioned smoothly into embodied agents [17]. Bridging this gap requires extensive robotics data, which is costly and time-consuming. For instance, while ChatGPT-3 was trained on 300 billion tokens of web text, collecting the data for RT-1 required 13 robots and 17 months to gather just 130k episodes-a significant amount, but far smaller in scale than the data used for large language models [23]. Without sufficient data, robots are more prone to OOD scenarios, increasing the risk of unsafe behavior.\nOpportunities: The field is advancing toward building generalist models by pooling diverse robotics data across platforms. RT-X is at the forefront, aggregating data from 34 labs across 22 embodiments, including single-arm robots, bi-manual systems, and quadrupeds [125]. While this marks a significant step forward, much more work is needed to expand data collection across an even wider range of robotic platforms and environments. These efforts lay the groundwork for the creation of Large Robot Models (LRMs)-the next major leap in robotics."}, {"title": "5.2 System", "content": null}, {"title": "5.2.1 Real-Time Processing", "content": "Challenge: Diffusion models are gaining popularity as a powerful tool for motion planning [169], but their inherent latency poses a significant challenge. The high latency is due to the repeated inference steps required for denoising. For example, 2D path planning using diffusion operates at 2.5 Hz [98], while autonomous vehicles typically require path planning at 20 Hz [151], or even faster in dense, safety-critical environments. Such decision frequencies fall short of real-world requirements, raising safety concerns due to delayed response times in critical situations.\nThe growing scale of LLMs also hinders real-time, reactive reasoning in mobile robots, particularly when they are deployed on edge devices. For instance, common edge devices for robots, like the NVIDIA Jetson AGX Orin, offer drastically lower compute power compared to server-grade GPUs such as the NVIDIA H100, which are used for virtual applications like chatbots. The Orin's performance is significantly slower, underscoring the challenge of even running small language models on edge devices, as these values come from benchmarking a relatively small language model-GPT-J with only 6 billion parameters. Larger models are often too resource-intensive to deploy on edge devices, leading to reduced reasoning capabilities. Large generalist models for motion planning, like RT-2, can only run at 1-3 Hz [23]. This low frequency is exacerbated by the need to run on edge devices, since alternative deployment options such as running inference on the cloud typically introduces communication latency of more than 100 ms- far exceeding the 10-100 ms required for many robotic applications [131].\nOpportunities: Efforts like distilling student networks (e.g., Consistency Policy [129]) and simplifying models during denoising [48] have shown promise in speeding up diffusion models. Language models are also being optimized through better parameter utilization, KV cache management, and parallel decoding [73]. However, many systems have poor Machine FLOPs Utilization (MFU), typically 50% or less. Significant opportunities lie in maximizing MFU to fully exploit computational resources, alongside domain-specific optimizations that leverage the unique constraints of robotics, beyond what's achievable in virtual environments."}, {"title": "5.2.2 Resource Requirements", "content": "Challenge: Deploying generative models demands significant resources, especially in memory and power consumption. Server-grade setups like 8 NVIDIA H100 GPUs consume up to 5600 W, whereas edge devices such as the NVIDIA Jetson AGX Orin use just 60 W . Edge devices are more power-efficient"}]}