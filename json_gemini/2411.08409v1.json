{"title": "DiVR: incorporating context from diverse VR scenes for human trajectory prediction", "authors": ["Franz Franco Gallo", "Hui-Yin Wu", "Lucile Sassatelli"], "abstract": "Virtual environments provide a rich and controlled setting for collecting detailed data on human behavior, offering unique opportunities for predicting human trajectories in dynamic scenes. However, most existing approaches have overlooked the potential of these environments, focusing instead on static contexts without considering user-specific factors. Employing the CREATTIVE3D dataset, our work models trajectories recorded in virtual reality (VR) scenes for diverse situations including road-crossing tasks with user interactions and simulated visual impairments. We propose Diverse Context VR Human Motion Prediction (DiVR), a cross-modal transformer based on the Perceiver architecture that integrates both static and dynamic scene context using a heterogeneous graph convolution network. We conduct extensive experiments comparing DiVR against existing architectures including MLP, LSTM, and transformers with gaze and point cloud context. Additionally, we also stress test our model's generalizability across different users, tasks, and scenes. Results show that DiVR achieves higher accuracy and adaptability compared to other models and to static graphs. This work highlights the advantages of using VR datasets for context-aware human trajectory modeling, with potential applications in enhancing user experiences in the metaverse. Our source code is publicly available at https://gitlab.inria.fr/ffrancog/creattive3d-divr-model.", "sections": [{"title": "1 Introduction", "content": "Capturing dynamic interactions between individuals and their environments is crucial for human motion prediction, and including visual scene context enhances the accuracy of these predictions. However, existing approaches primarily rely on frame-by-frame video analysis, and have difficulty to adequately account for these complex interactions [19]. Scene point clouds generated from 3D sensors provide spatial data but can't capture temporal changes nor the human intentions [11,30]. These data capture and representation methods, though useful in controlled and simple scenarios, fall short in offering a nuanced understanding of individual intentions and interactions within dynamic environments. This limitation significantly impacts prediction accuracy, particularly in complex navigation scenarios or for individuals with invisible conditions such as visual impairments.\nVR provides a secure and controlled media to replicate real-world scenarios to study human behavior. The CREATTIVE3D dataset [23] 1, integral to this research, offers ontology-based VR environments, with annotations of 3D scenes, objects, and interactive tasks. Additionally, it includes scenes simulating low-vision conditions with gaze tracking. This allows a deeper investigation into how well models can capture individual behavior, or generalize across behaviors in varied contexts, addressing the limits of existing computer vision approaches and datasets.\nRecognizing these limitations, our research introduces DiVR, a novel model that uses heterogeneous graph representations [2] to effectively capture the dynamic nature of human environments, as shown in Figure 1. We leverage the annotations in the CREATTIVE3D dataset to test and refine DiVR for human motion prediction. By incorporating both static and dynamic variables, DiVR captures interactions and environmental factors, making it capable of generalizing across varying users, task complexities, and scene layouts, a key contribution to this work. Such versatility is crucial to applications like autonomous driving where the accuracy of predicting pedestrian movements can significantly impact safety and operational efficiency. We discuss DiVR's adaptability to diverse scenarios including those involving simulated low-vision conditions \u2013 vision loss that cannot be fully corrected - and complex navigation tasks.\nOur contributions are thus threefold:\n1. Propose DiVR, a novel approach using dynamic heterogeneous graphs that capture both the static and dynamic scene context to improve human motion prediction.\n2. Evaluate graph context to improve adaptability to low-vision and task complexity.\n3. Explore how scene graph context helps the model generalize better across different user groups, tasks, and scenes."}, {"title": "2 Related Work", "content": "Human trajectory prediction. Human trajectory prediction is a computer vision task, originally developed using surveillance video data from datasets like ETH and UCY. It is formulated as a sequence-to-sequence prediction task: given past position data to predict future sequences. Up until recently, Recurrent Neural Networks (RNNs) were dominantly used for trajectory prediction, including human-robot interaction scenarios [28]. Corona et al. included context through simple graph representations [4], and Rond\u00f3n et al. [16] with dedicated Long Short-Term Memory (LSTM) units fed with estimated saliency maps to modulate the importance of context over inertia depending on the prediction horizon.\nTransformer models [21] marked a significant shift, introducing self-attention that captures temporal relations more effectively than RNNs, and setting new standards for motion prediction [1,27,30]. However, transformers encounter challenges with the quadratic complexity in computational, particularly for high-dimensional inputs like images and videos, leading to significant computational costs during both training and testing phases. The Perceiver architecture [10] was conceived to address this by avoiding costly self-attention operations in the input space, instead restricting them to a low-dimensional latent space, with input and output dimensions handled only through cross-attention operations.\nCross-modality learning. Foundational cross-modal learning has been conceived to effectively fuse information from audio-visual [15] and audio-lingual [13] tasks, and are well-adapted to motion prediction tasks. Cross-modal representations enable the inclusion of human behavior and environmental interaction [25] information. GIMO [30] is representative of this and has strongly inspired our work, including motion capture, 3D point clouds, and gaze tracking in a cross-modal transformer architecture for human motion prediction. Zaier et al. [27] introduce a cross-attention model CMAPT to fuse pedestrian trajectory data with kinematic and visual features extracted from camera surveillance videos. They employ a bi-modal transformer to capture spatio-temporal interactions. Finally, Gallo et al. [6] were the first to benchmark trajectory prediction in fully VR using the CREATTIVE3D dataset including motion, gaze, and scene point cloud data.\nGraph-based techniques. Graph Neural Networks (GNN) have been recognized early on for their potential to model molecular structures [5,12] and are now applied to video tasks such as action recognition [7]. In pedestrian trajectory precition, graphs can appropriately capture the interactions between people and other scene elements such as obstacles and pathways to improve accuracy [20, 24, 32]. Graph models can involve static and dynamic scene elements, which help model continuous changes within interaction networks [14]. Notably, Zhou et al. [31] introduced a hybrid static-dynamic graph approach to represent relations between pedestrians, significantly improving multi-pedestrian trajectory prediction. Sun et al. [20] studied how road graphs improved trajectory prediction on the autonomous driving dataset nuScenes. Spatial Temporal GNN [3] effectively combines RNN mechanisms with GNNs for trajectory prediction in complex real-world scenarios such as urban and parking environments, leveraging temporal dynamics to refine predictions and align with rapidly changing settings. To the best of our knowledge, our approach is the first to leverage heterogeneous graph representations to investigate trajectory prediction of real humans in fully interactive virtual environments, incorporating simulated low-vision conditions. Drawing inspiration from methodologies in autonomous driving, such as those by Carrasco et al. [2] and Zipfl et al. [33], we incorporate temporal heterogeneous graphs to effectively capture and model the dynamic interactions and temporal evolution within urban pedestrian environments."}, {"title": "3 Approach", "content": "Our work addresses human trajectory prediction on the CREATTIVE3D dataset, modeled on the 2D head position from past positions and context data. The human model comprises, at a given time t (in frames), the head position $p_t \\in \\mathbb{R}^2$ in meters, representing the user's absolute position within a 10 by 4 meters tracked space. The problem involves predicting a full motion sequence over a future horizon H, defined as $M_{t+1:t+H} = \\{(p_{t+1}), ..., (p_{t+H})\\}$ from a given time t. We use 3 seconds of past motion data sampled at 2 frames per second to predict the trajectory for the subsequent 5 seconds.\nHere, we present the graph representation for scene context, and the DiVR model which decodes this representation and other kinematic information into coordinate prediction."}, {"title": "3.1 Context Representation", "content": "While road graph representations are usually obtained from pre-extraction steps for autonomous driving datasets, the CREATTIVE3D dataset offers detailed annotations for entities like pedestrians, vehicles, traffic lights, and movable objects, as well as navigable spaces: house, sidewalks, roads, and crossings. Using these detailed annotations, we generate scene graphs where nodes represent individuals, entities, and spaces, and the edges represent their relations such as proximity and interactions."}, {"title": "Heterogeneous Graph Representation", "content": "We model the scene context as a directed heterogeneous graph, denoted by $G = (V,E,T, R)$. Each node $v \\in \\nu$ and edge $e \\in E$ is classified by type, with $\\tau(v) : V \\rightarrow T$ mapping nodes to node types $T = \\{location, pedestrian, vehicle, button, traffic light\\}$, and $\\$(e) : E \\rightarrow R$ mapping edges to relation types $R = \\{approach, adjacent, interaction\\}$. These types represent typical urban interactions, such as a pedestrian or vehicle approaching a location, adjacency between locations, and interactions between entities like pedestrians and traffic lights.\nNode Representation: At a given time t, each node represents urban entity attributes within the graph. Node attributes are detailed in Table 1, and include a unique identifier, interaction capability, mobility, presence status, and spatial positioning. This modeling captures both dynamic and static properties of entities, essential for representing urban interactions in our trajectory prediction task."}, {"title": "Edge Representation", "content": "Edges in the heterogeneous graph indicate connectivity and are annotated with features that describe the nature of interactions, such as active or inactive status, type of interaction (e.g., crossing, waiting), and the physical or temporal distance between entities. This attributed edge characterization allows us to distinguish, for example, a pedestrian waiting at a traffic light from one actively crossing the street, providing deeper insights into interactive dynamics. Table 2 outlines the edge attributes.\nGraph Embedding ($f_c$) To extract graph embeddings from the temporal heterogeneous graphs, we employ a Temporal Graph Convolutional Network (TemporalGCN) [8,18], as illustrated in Fig. 2. This model uses edge-conditioned convolution operations to transform edge features into dynamic weights for graph convolution layers. This adaptation allows the model to effectively differentiate and represent diverse interaction patterns within the graph. Next, global mean pooling aggregates these features across all nodes, creating a graph-level representation for each timestamp. The temporal embedding obtained from these timestamps are then concatenated, providing a representation of the dynamic graph context. Finally, a linear transformation is applied to the concatenated embedding, resulting in the final output graph embedding that integrates both spatial and temporal information."}, {"title": "Homogeneous Graph Representation", "content": "We model the scene context as a homogenous graph denoted by $G_h = (V_h, E_h)$, where all nodes $v \\in V_h$ and edges $e \\in E_h$ are treated uniformly without distinct types. In this representation, nodes do not differentiate between specific types of entities, such as pedestrians, vehicles, or traffic infrastructure. Each node is associated with attributes that encompass both the type of entity and its current state."}, {"title": "3.2 The DiVR Model", "content": "We present the DiVR model (Figure 2), a new architecture that leverages cross-modal attention over three modalities of data: (1) gaze-interpolated scene point cloud, (2) past motion data, and (3) the human-scene interaction context through heterogeneous graph representations. Each of these modalities is processed by an individual branch with the PerceiverIO architecture. The first branch employs PointNet++ to extract and encode features from the gaze data interpolated with the scene point cloud into a latent vector $f_{gaze}$. Concurrently, the second branch transforms raw motion data into a latent representation $f_{motion}$. The third branch utilizes the TemporalGCN introduced in the previous section to handle temporal heterogeneous graphs, producing a latent graph vector $f_{context}$.\nAt the core of DiVR's architecture is a cross-modal attention mechanism that fuses $f_{motion}$ and $f_{gaze}$, enhancing the model's sensitivity to the interplay between gaze direction and movements. This mechanism also integrates $f_{context}$, combining motion, gaze, and environmental graph data via a predictive cross-modal transformer. As shown in Sec. 4, this fusion of diverse modalities, along with a structured representation of the environment and interactions using heterogeneous graphs, substantially improves the accuracy of future trajectory predictions."}, {"title": "Training", "content": "To effectively train the motion prediction models under various conditions, the loss function $L_{total}$ integrates components to capture key aspects of prediction accuracy: $L_{total} = L_{trans} + L_{rec} + L_{des\\_trans}$. The translation loss, $L_{trans}$, represents the mean of the L1 losses between the predicted and ground-truth trajectories. The reconstruction loss, $L_{rec}$, penalizes discrepancies between the model's outputs and the inputs during the encoding phase, ensuring temporal consistency. The destination loss, $L_{des\\_trans}$, specifically targets the accuracy of the final step predictions."}, {"title": "4 Experimental evaluation", "content": "This section evaluates the DiVR model's performance in predicting human trajectories using the CREATTIVE3D dataset. We assess the model's adaptability comparing it against established benchmarks. The goal is to demonstrate DiVR's robustness and accuracy across different user conditions, scene complexities, and task variations, highlighting its reduced trajectory prediction error compared to conventional models. We aim to show that DiVR integrates diverse contextual data effectively, using advanced graph-based techniques."}, {"title": "4.1 Experimental Setup", "content": "Dataset. We use the CREATTIVE3D dataset [22] to analyze human interactions and navigation in VR environments, focusing on road crossing scenarios. This dataset, which records user behaviors in urban settings, is structured around six scenarios, the dataset includes tasks of varying complexity: from simple tasks (ST) requiring navigation to complex tasks (CT) involving interactions, such as activating a traffic light before crossing. These scenarios are tested under two visual conditions: Normal Vision (NV) and Simulated Low Vision (LV). The Low Vision condition specifically simulates a scotoma, a central blind spot. We split the dataset with 70% for training, 15% for validation, and 15% for testing.\nExperiments. Multiple experiments are conducted to assess model performance under varying conditions. The Context Data Evaluation assesses the impact of different visual conditions and task complexities using the balanced dataset. The Low Vision and Complex Task Generalization experiment tests model adaptability through three configurations, beginning with a baseline model trained on NV and ST, and progressively including more complex tasks and varied visual conditions. The User Generalization examines the model's ability to generalize across different users by selecting a test group of 10 users at random from a total of 40. The Scene Generalization trains models on single-lane road scenarios and tests them on two-lane scenarios to evaluate generalization to different environments. Finally, the Task Generalization assesses models on crossing tasks and their adaptability to tasks requiring a return to the starting point.\nEvaluation metrics. Our evaluation adopts two metrics that are endorsed and utilized in existing literature [17,26,27,29]: the Average Displacement Error (ADE) and the Final Displacement Error (FDE). ADE measures the average L2 distance (in meters) between predicted and actual trajectories at each time step from $T_{obs} + 1$ to $T_{pred}$, indicating overall prediction consistency. FDE calculates the Euclidean distance (in meters) between the predicted and actual final positions at the last time step, $T_{pred}$, reflecting the accuracy of endpoint prediction.\nImplementation details. Our model uses the PerceiverIO architecture for motion prediction. It consists of 6 encoding layers, each with a dimension of 256 and 8 attention heads, identical to the original PerceiverIO. The implementation was done using PyTorch on an Ubuntu server, supported by an Nvidia V100 GPU with 32 GB RAM. We trained the DiVR model end-to-end for 100 epochs with a batch size of 16. The Adam optimizer was used with an initial learning rate of 0.0001 and a weight decay of 0.0001. The learning rate was adjusted using an exponential decay method with a gamma value of 0.99."}, {"title": "4.2 Results", "content": "Context Data Evaluation\nBaseline Models. We compare three prediction models with increasing context integration on the CREATTIVE3D dataset: (1) MLP [9] baseline, using only historical position data, (2) TRACK [16], incorporating real-time gaze data for user attention, and (3) GIMO [30], which includes gaze and 3D point cloud data and inspired our model."}, {"title": "DiVR Models", "content": "As shown in Table 3, DiVR-Het not only outperforms the MLP baseline, reducing the ADE and FDE by 31.2% and 44.3%, respectively, but also demonstrates the effectiveness of integrating high-level context as compared to GIMO. This context includes elements such as scene layout, and human interactions, which significantly influence trajectory predictions. Similarly, DiVR-Hom shows significant improvement over the baseline with a 27.3% reduction in ADE and 34.3% in FDE. We show in Fig. 1 qualitative results of the DiVR-Het model.\nLow Vision and Complex Task Generalization\nBaseline Table 4 presents the baseline performance and generalization capabilities of various models trained under Normal Vision (NV) with Simple Tasks (ST), and their generalizability to complex tasks and low vision conditions. In the NV+ST test, the GIMO model achieves an ADE of 0.509 and an FDE of 0.745, slightly outperforming the TRACK model. Both models initially demonstrate better ADE values than DiVR."}, {"title": "Adaptation to Complex Tasks", "content": "Differently from the previous setup, where models were trained only on simple tasks, the models are now trained on both simple and complex tasks with normal vision (Diverse Task Training). Table 5(a) shows the performance under these conditions, demonstrating that DiVR models benefit significantly from this training strategy. DiVR-Hom reduces its ADE from 0.592 to 0.470 and its FDE from 0.908 to 0.816, marking a 20.6% improvement in ADE and a 10.1% improvement in FDE, outperforming TRACK and GIMO under the same conditions. Although DiVR-Het's improvement in ADE is less pronounced than that of DiVR-Hom, it still demonstrates strong generalizability and adaptability."}, {"title": "Adaptation to low vision conditions", "content": "Table 5(b) shows the performance when models are trained on normal and low vision (Diverse Vision Training) conditions, targeting adaptability to low vision scenarios. In the low vision test, both DiVR-Het and DiVR-Hom achieve an ADE of 0.560, with FDEs of 0.791 and 0.767, respectively. These scores are lower than those of the TRACK and GIMO models.\nUser, Scene, and Task Generalization\nThis section analyzes the models' adaptability to user variations, scene complexity, and task challenges, as shown in Table 6. For user generalization, 10 randomly selected users formed the test group, while others were used for training and validation. Scene generalization involved training on single-lane roads and testing on two-lane crossings. Task generalization assessed models trained on simple crossing tasks and tested on scenarios requiring a return to the starting point."}, {"title": "User Generalization", "content": "The models show good adaptability to user variations, with DiVR-Hom achieving the best ADE of 0.513, a 17.4% improvement over the baseline GIMO model. DiVR-Het also excels in FDE, with a 16.7% improvement, indicating both DiVR models effectively generalize across diverse user behaviors. Scene Generalization. Adapting to increased scene complexity from single-lane to two-lane crossings, DiVR models show increases in ADE and FDE to around 1 and 1.4, respectively, demonstrating reasonable resilience given the additional 3.5m complexity in two-lane roads. Task Generalization. This presents the most significant challenge, with FDE values around 2 across all models. However, DiVR-Hom achieves the best ADE, suggesting the model can infer some directional or trajectory characteristics despite the increased task difficulty.\nAblation Tests\nWe conducted additional ablation studies on the DiVR-Het model. Table 7 shows the results, focusing on the impact of graph and gaze data on trajectory prediction accuracy, measured by ADE and FDE. The study evaluates the DiVR-Het model by replacing either graph or gaze inputs with a tensor of zeros.\nGraph Ablation Study. The graph ablation study reveals a significant decline in performance when graph data is omitted from the DiVR-Het model as shown in Table 7. Under NV+ST test, the graph-ablated model's ADE and FDE increase to 0.622 and 0.949, respectively, marking increases of 16.3% in ADE and 34.6% in FDE. The impact is even more pronounced under complex task conditions, with the ADE and FDE increasing to 0.944 and 1.767, jumps of 35.8% and 45.1%, respectively. In low-vision tests, the increases are 13.0% in ADE and 24.2% in FDE, showing again the usefulness of graph data for maintaining lower error rates across vision conditions.\nGaze Ablation Study. The gaze ablation study shows decreased performance, but with a slightly lesser impact than graph ablation, indicating that while gaze data is important, it is less critical than graph data in the DiVR-Het model. In NV+ST test, removing gaze data results in an increase in ADE and FDE to 0.590 and 0.802, up by 10.3% and 13.8% compared to the non-ablated model. For complex task tests, ADE and FDE rise by 14.1% and 14.2% to 0.793 and 1.391. In low vision conditions, ADE and FDE increase by 15.4% and 15.7%, demonstrating that gaze data significantly contributes to the model's accuracy in predicting human trajectories, especially in challenging scenarios.\nQualitative Results\nContext Data Evaluation. We compare GIMO and DiVR-Het models across three urban scenes. In the first scene, \"pressing Traffic Button\" (left column, Fig. 3), the observed trajectory (red line) shows a pedestrian disposing of trash and then interacting with a traffic button (blue line). GIMO (top row) incorrectly predicts immediate crossing, while DiVR (bottom row) accurately predicts the path toward the button, closely matching the ground truth. In the second scene, \"Waiting for Traffic Light\" (middle column), GIMO predicts the pedestrian begins crossing, but DiVR captures the stationary behavior, closely aligning with the ground truth. In the final scene, \"crossing\" (right column), DiVR's predictions better capture the actual crossing path. These comparisons highlight DiVR's superior performance in predicting complex pedestrian behaviors in urban environments.\nUser Generalization In the user generalization section, we visually demonstrate DiVR's adaptability across different users in the test set. Fig. 4 illustrate scenarios where DiVR has successfully predicted diverse user movements within the CREATTIVE3D dataset, reflecting its robustness in handling variations in pedestrian behavior. Despite varied actions and intentions of users, DiVR consistently aligns closely with the ground truth, showcasing its capacity to accurately generalize across a broad spectrum of user interactions. These qualitative results complement our quantitative findings, where DiVR exhibited robust values on ADE and FDE for user generalization."}, {"title": "5 Discussion and Conclusion", "content": "We presented DiVR, a multimodal transformer for human trajectory prediction that leverages heterogeneous graphs from rich VR contextual data. To our knowledge, this is the first work to investigate trajectory prediction of real humans in fully interactive virtual environments. Our experiments demonstrated DiVR's robustness across varied conditions, highlighted by extensive generalization tests. A key strength of this study is the detailed evaluation across a wide range of scenarios, showcasing DiVR's effectiveness in handling complex tasks and low-vision conditions through graph-based and temporal modeling techniques.\nHowever, a notable limitation is the model's reliance on high-quality datasets for accurate scene graph creation, which are not widely available. To mitigate this, future work could explore data from smart city infrastructures and autonomous vehicle sensors, offering real-time traffic and pedestrian data. This would enhance the model's applicability and performance in real-world scenarios, aiding in the development of more adaptive urban traffic systems. Additionally, VR's advantages in incorporating diverse scenes and populations into training highlight its potential for important real-life applications."}]}