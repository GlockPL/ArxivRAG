{"title": "Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge", "authors": ["Beidi Dong", "Jin R. Lee", "Ziwei Zhu", "Balassubramanian Srinivasan"], "abstract": "The United States has experienced a significant increase in violent extremism, prompting the need for automated tools to detect and limit the spread of extremist ideology online. This study evaluates the performance of Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-Trained Transformers (GPT) in detecting and classifying online domestic extremist posts. We collected social media posts containing \"far-right\" and \"far-left\" ideological keywords and manually labeled them as extremist or non-extremist. Extremist posts were further classified into one or more of five contributing elements of extremism based on a working definitional framework. The BERT model's performance was evaluated based on training data size and knowledge transfer between categories. We also compared the performance of GPT 3.5 and GPT 4 models using different prompts: na\u00efve, layperson-definition, role-playing, and professional-definition. Results showed that the best performing GPT models outperformed the best performing BERT models, with more detailed prompts generally yielding better results. However, overly complex prompts may impair performance. Different versions of GPT have unique sensitives to what they consider extremist. GPT 3.5 performed better at classifying far-left extremist posts, while GPT 4 performed better at classifying far-right extremist posts. Large language models, represented by GPT models, hold significant potential for online extremism classification tasks, surpassing traditional BERT models in a zero-shot setting. Future research should explore human-computer interactions in optimizing GPT models for extremist detection and classification tasks to develop more efficient (e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes) methods for identifying extremist content.\nKeywords: Online extremism; violent extremism; social media; large language model; artificial intelligence; natural language processing", "sections": [{"title": "Introduction", "content": "The United States (U.S.) has observed a sizeable surge in violent extremism in recent years. According to the Department of Homeland Security (DHS) Office of Intelligence and Analysis, a total of 231 violent extremist incidents (i.e., successful attacks and attempted plots) occurred in the U.S. between 2010 and 2021, indicating a 357% increase from the previous ten-year estimate (see Government Accountability Office, 2023a; 2023b). Violent extremist incidents are motivated by various ideologies, including anti-government extremism (e.g., Oath Keepers; Three Percenters), environmental extremism and eco-terrorism (e.g., Sea Shepard Conservation Society; Animal Liberation Front), and extreme nationalism and white supremacism (e.g., American Freedom Party; Proud Boys). All types of violent extremism pose significant risks as they can lead to various forms of harm to both those directly involved (e.g., victims or targets, law enforcement) and indirectly or vicariously exposed (e.g., bystanders, non-involved civilians). In effect, domestic violent extremism has been identified as a greater threat to national security than foreign terrorism (Chermak et al., 2012; Parkin, Freilich, & Chermak, 2015).\nRecent studies have highlighted the increasing role of online communications platforms in promoting violent extremism, which includes: (1) propaganda dissemination; (2) fundraising; (3) recruitment and radicalization; (4) planning and coordination; (5) online indoctrination; and (6) psychological warfare (Conway, 2017; Holt, Freilich, & Chermak, 2017; 2022; Holt et al., 2019; Klein, 2019; Scrivens, Gill, & Conway, 2020). Since online platforms increase individuals' exposure to radical ideas and provide networking spaces that enable people to connect with like-minded others, they allow for ideologies and motivations to be shared, cultivated, and embraced (Hamm & Spaaij, 2017). Research has found that differential exposure to online extremist content produces changes in attitude (Drevon, 2016; Magdy et al., 2016),"}, {"title": null, "content": "emotion (Baines et al., 2010; Sikorskaya, 2017), and behavior (Gill et al., 2017; Pauwels & Schils, 2016), such that individuals who are more frequently exposed to online extremist material are more likely to internalize those messages and form stronger bonds and social networks with like-minded individuals, increasing their likelihood of exhibiting affective, emotional, and behavioral change (Conway, 2017; Scrivens, Gill, & Conway, 2020).\nGovernment agencies and regulators recognize the pressing imperative to address the threat of online extremism through both tactical and strategic measures. For instance, central governments in various countries have reinforced legal frameworks and encouraged major technology companies and Internet Service Providers (ISPs) to actively monitor, regulate, and remove such content (Aldera et al., 2021; Guhl et al., 2020; Gunton, 2022; Miller, 2017; Wakefield, 2021). However, these regulations are still evolving and, to some extent, may clash with the financial motivations of technology companies and ISPs, which thrive on user engagement and attention. Unfortunately, extremist-promoted mis-information (i.e., false information), dis-information (i.e., false information deliberately created to discredit an entity), or mal-information (i.e., reality-based information that ignites hatred or inflicts harm) often garners significant visibility, inadvertently benefiting these corporate entities. As of now, a comprehensive counter-extremism strategy that fully incorporates the distinctive attributes of online communications and social media platforms has yet to be developed across the various stakeholders involved in addressing this issue (Amble, 2012; Droogan, Waldek, & Blackhall, 2018). This gap places regulators at a significant tactical disadvantage, especially given the growing evidence-base around social media's substantial role and impact in predicting various crime problems (Burnap & Williams, 2015; 2016; Waseem & Hovy, 2016; Williams, Burnap, & Sloan, 2017). As a result, there is an urgent need to improve our ability to confront online"}, {"title": null, "content": "extremism in an emerging era of advanced technology, including the proliferation of various artificial intelligence (AI) tools such as GPT (Achiam et al., 2023; Brown et al., 2020).\nWhile there have been anecdotal accounts of extremists utilizing AI tools for propaganda and radicalization purposes (Gilbert, 2023; Siegel & Doty, 2023), criminological investigation into effectively leveraging these AI technologies to counteract online extremism remains limited. Specifically, although various AI approaches have been employed to examine online extremism, particularly for detection and classification tasks, there has been a scarcity of studies that compare these AI approaches to evaluate their respective strengths and weaknesses (see Aldera et al., 2021 for discussion). Knowing the specific advantages and limitations of various AI tools is crucial for selecting the most appropriate method when addressing diverse challenges in tackling online extremism in real-world settings. Further, most studies exploring online extremism were concerned with understanding Islamic extremism with minimal attention directed toward online domestic extremism.\u00b9\nGiven the dearth of research in these areas, the current study aims to evaluate the effectiveness of two AI approaches, namely a traditional language model (LM)\u2014the Bidirectional Encoder Representations from Transformers (BERT) model (Devlin, Chang, Lee, & Toutanova, 2019)\u2014and a large language model (LLM)\u2014Generative Pre-Trained Transformers (GPT) (Achiam et al., 2023; Brown et al., 2020)\u2014in detecting and classifying online domestic extremist posts. In addition, the study seeks to investigate various prompt engineering techniques to determine whether certain prompts are more effective than others in"}, {"title": null, "content": "the detection and classification of online extremist content using GPT. We contend that online extremist content is multifaceted and complex, necessitating an approach that goes beyond a simple binary detection between extremist and non-extremist content. It is essential to also comprehend the contributing elements that determine whether a post is extremist, as this understanding can provide deeper insights into the nuances of online extremism and inform effective strategies for detection and intervention."}, {"title": "Characteristics of Online Extremism", "content": "Extremist groups and individuals frequently use online communications and social media platforms to engage with a broader audience that would have been challenging to reach through traditional methods (e.g., face-to-face interactions, print media). Compared to face-to-face interactions, online extremism is not restricted by spatiotemporal boundaries, as anyone with access to the Internet can potentially be exposed to online extremist content. In addition, while the intensity of extremist emotions may reach a natural peak and then subside in face-to-face interactions, aggression and hostilities may intensify indefinitely on social media channels (Gaudette et al., 2021; Perry & Scrivens, 2016; Scrivens, Gill, & Conway, 2020).\nSeveral characteristics of online communications and social media platforms can be exploited by extremists for propaganda and radicalization purposes. Online communications platforms operate within the realms of an \u201cattention economy\u201d where sensational or emotionally charged content outperforms accurate or nuanced information in the competition for users' attention. When advancing their ideological agenda, extremist groups purposely construct and disseminate mis-information, dis-information, and mal-information and employ sensational headlines or emotional triggers to capture users' attention and promote the sharing of such content (Klein, 2019; Weimann & Masri, 2023). The ability to share information anonymously"}, {"title": null, "content": "or under pseudonyms further facilitates the spread of such information, as it reduces the repercussions for disseminating false content (Ma, Hancock, & Naaman, 2016). While estimates of extremists' dissemination of false information are scarce, a recent 2020 report found that 38.2% of adult social media users in the U.S. shared some kind of false information through social media platforms (Statista, 2023).\nConsequently, online platforms tend to create echo chambers and filter bubbles where users are predominantly exposed to information that aligns with their existing beliefs and interests. Coupled with cognitive biases like confirmation bias, which leads people to favor information that confirms their pre-existing views, users become more entrenched in their beliefs and more susceptible to mis-information, dis-information, and mal-information, as they are less likely to encounter opposing viewpoints. Moreover, the use of fake accounts (e.g., bots) by extremists can rapidly amplify the spread of malicious information and make it seem more popular and credible than it is (Alrhmoun, Winter, & Kertesz, 2023; Patel, Agrahari, & Srivastava, 2020).\nEmpirical evidence demonstrates the effectiveness of extremist groups in leveraging online communications and social media platforms to further their agenda. For instance, Twitter accounts associated with extremist groups have significantly more followers than the average Twitter user, and Twitter networks of users who share extremist content are more densely interconnected than those who do not share such content (Berger & Morgan, 2015; Faris et al., 2016). Additionally, users who typically consume conspiracy-related content are more likely to interact with and share information from other conspiracy pages (Del Vicario et al., 2016). The average duration of radicalization (i.e., from initial exposure to extremist beliefs to participation in extremist acts) decreased from approximately 18-months in 2005, when social media was first"}, {"title": null, "content": "emerging as a factor in the radicalization of U.S. extremists, to 13-months in 2016 (Jensen et al., 2018). Furthermore, the use of social media in extremist movements accelerated the formation of consensus on radical viewpoints and increased commitment to the movement's objectives (Carley, 2017)."}, {"title": "Artificial Intelligence Tools and Online Extremism", "content": "The growing presence of online extremist content and user accounts on various social media platforms has led global governments to prioritize the detection of online extremism and improve their online counter-extremism efforts, with many electing to adopt content removal measures to reduce the spread and influence of online extremism (Aldera et al., 2021; Guhl et al., 2020; Gunton, 2022). For instance, the German government enacted a law in 2017 that imposed fines of up to 50-million euros to social media companies that failed to remove extremist images and propaganda from their sites (Miller, 2017). Similarly, the U.K. government drafted an Online Safety bill that permitted fines of up to 18-million pounds to social media companies that neglected to remove extremist images and propaganda from their platforms (Wakefield, 2021).\nContained within many of these laws is the mandate for social media platforms to both detect and remove extremist content from their sites within a short period of time to reduce users' interaction with and exposure to extremist ideology (Gorwa et al., 2020; Gunton, 2022).\nThough many methods can be adopted to accomplish these tasks, the growing use and influence of social media platforms has necessitated the development of automated tools to both improve the detection of online extremism and minimize the spread of extremist ideology (Gaikwad et al., 2021). While earlier methods of detecting online extremism were conducted manually by expert officials and researchers in counterterrorism units, manually filtering through the vast amounts of online data transmitted across social media networks (e.g., volume of social"}, {"title": null, "content": "media traffic) has become an increasingly improbable task (Aldera et al., 2021; Borum & Neer, 2018; Gaikwad et al., 2021). As a result, there is a critical need for automated AI solutions (e.g., predictive machine learning, automated hash-matching) to detect harmful content and remove them from online platforms (see Agarwal & Sureka, 2015a; Correa & Sureka, 2013; Fernandez & Alani, 2021; Gorwa et al., 2020; Llanso et al., 2020).\nGiven the cross-disciplinary nature of the issue, involving both social and computer scientists in developing automated AI solutions is crucial. While research in computer science can enhance our understanding of machine learning and data aspects related to online extremism, including the development of more efficient methods for modeling, detecting, and predicting online extremism and radicalization (see Ferrara et al., 2016), social science research can provide valuable insights into individual and group behaviors associated with online extremism and radicalization. More specifically, research in this area can be segmented into several categories based on their primary focus and objective, including those that adopt AI tools to conduct large-scale analyses of online radicalization (e.g., examining communication processes and analyzing influence and information spread; see Badawy & Ferrara, 2018; Carter et al., 2014; Chatfield et al., 2015; Klausen, 2015; Rowe & Saif, 2016), those focusing on the automatic detection of extremism (e.g., detection of extremist content and user accounts), as well as those that focus on the automatic prediction of radicalization (e.g., adoption of extremist content and interaction with extremist accounts; see Fernandez & Alani, 2021).\nResearch exploring individuals' transmission of online extremist content (i.e., information spread, influence transmission) have examined the online behaviors of extremist users through proxy parameters such as posting frequency and user mentions (see Carter, Maher, & Neumann, 2014; Chatfield, Reddick, & Brajawidagda, 2015; Klausen, 2015). Specifically, these studies"}, {"title": null, "content": "examined the ways in which extremists communicate with their followers, the terms and phrases they use in their communication, and the high relevance of social homophily on the diffusion of pro-extremist terminology (Vergani & Bliuc, 2015; Rowe & Saif, 2016). Using Natural Language Processing (NLP) techniques to filter their social media data (e.g., Twitter) for topically relevant content, Badawy and Ferrara (2018) found that extremist propaganda often revolves around four types of messaging (i.e., theological, violence, sectarianism, influential actors/events). Though these findings serve as a starting point for online extremism detection, these studies mainly focus on understanding the online radicalization process with minimal attention directed toward automatically detecting online extremism (Aldera et al., 2021).\nStudies that focus on automatically detecting online extremism have been increasing in recent years. For instance, Lara-Cabrera and colleagues (2019) used a set of keywords derived from social science theories of radicalization to automatically extract online extremist content, noting that while the proposed metrics reveal promising results, more refined metrics are needed given the inherent limitations of relying solely on keywords. In fact, many studies exploring the automatic detection of online extremist content are based on various textual features and frequently adopt machine learning techniques to identify extremism (Agarwal & Sureka, 2015a; Ashcroft et al., 2015; Kaati et al., 2015; Magdy et al., 2016). For instance, Agarwal and Sureka (2015b) explored semi-supervised learning approaches to detect extremist posts on Twitter based on a list of extremist hashtags to filter content related to foreign extremism, finding that religious war-related terms and offensive words containing negative emotions were strong indicators of online extremist tweets. Relatedly, both Ashcroft and colleagues (2015) and Kaati and colleagues (2015) used data-dependent (e.g., common hashtags, word bigrams, frequent words) and data-independent (e.g., stylometric and time markers) features to detect online extremist messages,"}, {"title": null, "content": "revealing the benefits of combining both features to enhance classifier performance (see also Agarwal & Sureka, 2015b; Magdy et al., 2016).\nThough many advancements have been made, several challenges with automated online extremism detection still exist. One significant issue is the absence of a uniform definition of online extremism, leading to varied interpretations across different studies and extremism detection algorithms. This lack of consensus results in a fragmented research landscape, with no standardized collection of online extremism data for analysis and algorithm training (Housen-Couriel et al., 2019). For example, one study may have identified a particular post as extremist, whereas another study may not classify that same post as extremist given contrasting or differing definitions of what constitutes extremism. This leads to multiple findings and datasets of online extremism without a consistent marker of the construct. This issue is exacerbated by the constant evolution of behaviors associated with online extremism, including changes in terminology and extremist beliefs (Fernandez & Alani, 2021). Relatedly, the diversity of content that is within the same sphere of extremist ideology may pose challenges to the automatic detection of extremist content and user accounts. That is, while distinguishing domestic far-right extremism from domestic far-left or Jihadist extremism may be a simpler task, there are many extremist groups who espouse different extremist attitudes and actions, or have differing interpretations of extremist concepts, despite sharing large portions of the main extremist ideology (Fernandez & Alani, 2021).\nAnother limitation associated with many automated extremist detection studies is their overreliance on a set of expressions at the expense of understanding the full context behind those words and/or phrases (Fernandez & Alani, 2021). For instance, while many AI-involved studies search for keywords and phrases to determine if online content is extremist in nature, they are"}, {"title": null, "content": "unable to fully grasp the context in which these words and phrases are used. Similarly, classification algorithms may encounter difficulties with correctly identifying an extremist post from one that is based on sarcastic rhetoric (Barnes, 2022). As a result, these findings may include non-extremist content despite containment of relevant keywords and phrases (see Fernandez & Alani, 2021).\nMoreover, many online extremism data used for research purposes may contain biases that do not reflect the larger population of interest, such as terminology and time-period bias (Fernandez & Alani, 2021). Terminology bias occurs when data is compiled based on a select number of terms and expressions (i.e., restricted lexicons) that encompass only a subsection of the topics discussed by extremist groups or individuals (Fernandez & Alani, 2021). For instance, data may be collected using lexicon that overrepresents a particular subgroup of extremists or those speaking only one language (e.g., Arabic), thus failing to capture a representative sample of extremist individuals, or even the larger group of extremists that fall within similar ideologies (Fernandez & Alani, 2021). Similarly, time-period bias occurs when data collection is restricted to a specified week or month(s) where notable world events or irregular activities (i.e., terrorist attacks, political and religious demonstrations) are taking place that skew the data (Fernandez & Alani, 2021). If algorithms are trained using data containing time-period bias, they may not be able to account for extremist content that appear across different time periods since those classifiers may evolve or change over time (Fernandez & Alani, 2021).\nThough not unique to automated detection studies, extremism datasets derived from social media samples are also prone to false positives (i.e., falsely categorized as extremist when it is not) given these collected data are not verified or only partially verified. For instance, an online post that reads \u201cIslamic State hacks Swedish radio station\u201d may be processed as extremist during"}, {"title": null, "content": "data collection even though the actual post is not extremist in nature. Other posts may be erroneously classified as extremist for simply containing religious rhetoric, regardless of its extremist nature (e.g., \u201cif you want to talk to Allah, pray. If you want Allah to talk to you, read the Qur'an\"). Counternarratives can also be identified as extremist content by various AI-solutions given their likeness to actual extremist posts (e.g., \u201carmed Jihad is for defense of Muslim nation, not for establishment of the Khilafah\u201d). In essence, various instances involving false positives may arise because data collection algorithms are trained to detect relevant keywords and sentiments (Fernandez & Alani, 2021).\nIt is important to note that the automatic detection of extremist content and user accounts are sensitive inquiries, as inaccurately labeling a post or user as extremist may result in censorship or unwarranted surveillance and investigation of an innocent person (Fernandez & Alani, 2021; Olteanu et al., 2017). The private and restricted nature of these datasets and their associated algorithms further limits it from being verified by others to determine its accuracy (Fernandez & Alani, 2021). In fact, very few datasets involving online extremism for research purposes are publicly shared for others to verify or validate (see Kaggle, 2019 for exception). Since most datasets are not made publicly available, there is uncertainty as to how much online content is actually extremist since reported findings can be highly skewed by irrelevant accounts or misleading classifications (see Parekh et al., 2018 for discussion). Given the consequences associated with erroneously categorizing an individual as being an extremist or engaging in online extremism (e.g., enhanced surveillance, censorship, restricted access to platform), it is important to consider the potential sources of inaccuracy with automated AI detection approaches and regularly reflect on the continuously changing patterns of extremism to reduce the potential negative impact of AI solutions and developments (Fernandez & Alani, 2021; Harford, 2014)."}, {"title": "Current Study", "content": "Considering the general background and limitations in existing research on AI tools and online extremism, the current study has three main objectives. First, the current study seeks to determine the effectiveness of two AI approaches (i.e., BERT and GPT) in identifying domestic online extremist posts. Before the recent emergence of LLMs, BERT was widely recognized as an advanced machine learning tool for detection and classification tasks. Unlike the traditional \"bag of words\" approach in NLP for text representation, which focuses solely on the frequency of words, the BERT model considers the context and relationships between words in a sentence. BERT generates contextualized word embeddings, meaning that the representation of a word changes based on the surrounding words in the sentence. Stated differently, the same word can have different representations depending on its context, enabling BERT to understand nuances in meaning. This advancement has at least partially addressed the issue of overreliance on a set of expressions or keywords while neglecting the broader context in which those words and/or phrases are used. Though the BERT model has generally demonstrated positive results in identifying online extremism, it is inherently a supervised machine learning approach when applied to downstream tasks, heavily reliant on both the quantity and quality of input labeled data. The training data directly influences the identification outcome, making the model's performance contingent on the comprehensiveness and accuracy of the data it is trained on.\nIn contrast, LLM methods, such as the representative model GPT, offer a zero-shot approach for downstream tasks. These models leverage their pre-trained knowledge and understanding of language to make predictions or classifications, enabling them to tackle new tasks without the need for explicit training on labeled data specific to those tasks. If an LLM"}, {"title": null, "content": "approach to identifying online extremism proves to be accurate and efficacious, it could offer significant advantages over BERT and other supervised machine learning approaches. Given that one of the goals of extremism research is to accurately identify extremist content and intervene before it spreads to wider audiences, LLMs provide a potentially more effective and practical means of achieving this goal. To date, no known study has compared the efficacy of both a BERT and GPT model within the same study (i.e., using the same data) to determine which method is more accurate and effective at accomplishing the specified task.\nSecond, the current study seeks to explore various prompt engineering techniques to determine whether certain prompts are more effective than others in identifying online domestic extremist content using GPT. In brief, prompt engineering refers to the process of designing and refining input instructions to effectively guide LLMs in performing specific tasks. For example, a prompt that simply asks the models to classify an online post as \u201cextremist\u201d or \u201cnon-extremist\" might not provide enough context for the LLM to make an accurate judgment. However, a more carefully engineered prompt (e.g., including specific examples of extremist language; instructing the LLM to assume a certain role; explicitly offering a definitional framework) could result in more accurate results. Additionally, prompt engineering can help address the challenge of evolving language and terminology used by extremist groups. By employing prompts that reflect current trends and language patterns, LLMs can maintain their effectiveness in identifying extremist content, adapting to the evolving language used by these groups. Moreover, LLMs can be prompted to provide the reasoning process through which they reach their decisions or conclusions. Unlike traditional \u201cblack box\u201d models, we can analyze the explanations or justifications provided by LLMs to understand why they arrived at correct or incorrect conclusions (e.g., compared to human-labeled gold standards). The insights gained from"}, {"title": null, "content": "examining the reasoning process itself have the potential to significantly advance online extremism research.\nLastly, the current study explores the capabilities of AI tools to classify online extremist content beyond simple dichotomies of extremism versus non-extremism. This more challenging task involves identifying and categorizing the nuanced elements and variations within extremist content, requiring a deeper understanding and more sophisticated analysis by the AI models. For example, many extremist posts are nuanced and infused with esoteric rhetoric (e.g., sarcasm). While previous studies have found that BERT models are fairly effective at distinguishing between extremist and non-extremist content, they often struggle with more complicated classifications of online extremism (e.g., far-right v. far-left; posts containing extremist key terms but are not inherently extremist posts) (see Fernandez & Alani, 2021). This challenge is linked to BERT's reliance on training data and the inability of researchers to compile a sufficiently large training dataset containing the many distinct layers and elements of extremism. As of now, there is limited knowledge regarding LLMs' ability to perform more refined classification tasks involving online extremism."}, {"title": "Methods", "content": "We collected Twitter posts (i.e., tweets) using Twitter's Application Programming Interface (API) version 2 on May 28th, 2023. Our data collection adhered to Twitter's terms and conditions at that time. Prior to gathering data, we ensured that no significant global events occurred in the previous month which could have influenced the typical presence and discussion of potential domestic extremist content on general-purpose social media platforms. In line with"}, {"title": null, "content": "our research objectives, we created two distinct sets of keywords, targeting \"far-right\" and \"far-left\" ideologies, to retrieve posts potentially relevant to domestic extremism. Our selection of search keywords was guided by two criteria. First, we based our choices on keywords used in prior studies on extremism, thus ensuring their relevance and validity (Scrivens, 2021; Scrivens, Davies, & Frank, 2018; Scrivens, 2017). Second, we aimed for the keywords to be indicative but not definitive of extremism, meaning that a post containing such a keyword is not necessarily an extremist post. The complete list of keywords used in this research can be seen in Appendix A.\nOur study focused exclusively on Twitter due to data availability constraints. Twitter's user base represents between one-fifth and one-quarter of the U.S. population and tends to be younger, more educated, and more affluent, with a greater likelihood of identifying as Democrats compared to the overall U.S. adult population (Wojcik & Hughes, 2019). Thus, the findings should be interpreted with caution due to the non-coverage of other general-purpose social media platforms, which might also host extremist content (e.g., Facebook, Instagram, YouTube, or TikTok), and the specific demographics of Twitter's users."}, {"title": "The Working Definitional Framework", "content": "Given that not every post containing one or more of our search keywords necessarily represents domestic extremism, we established a data labeling strategy tailored to our research objectives. Extremism, especially in online contexts, lacks a universally accepted definition (see Fernandez & Alani, 2021). Consequently, after a thorough review of existing definitions in empirical studies, we developed a working definitional framework for identifying extremism within our research. For a Tweet to qualify as \"extremist,\" we determined that it must exhibit at least one of the following two thematic components:"}, {"title": null, "content": "(a) Posts that explicitly or implicitly incite violence against specific individuals or groups. This includes content that celebrates, justifies, or advocates any form of harm towards an individual or group based on their identity or beliefs.\n(b) Content that promotes or justifies prejudice or hostility based on inherent attributes (e.g., race, religion, nationality, sexual orientation) potentially leading to conflict or actions (e.g., verbal, relational, social) aligned with socio-political beliefs or interests. Examples include posts advocating for the supremacy/inferiority of particular groups or individuals, dissemination of false information to sow discord and animosity, and content related to recruiting, supporting, admiring, or expressing allegiance with known extremist groups or individuals.\nFor the purposes of subsequent coding and analysis, the aforementioned thematic components were further divided into five elements: (1) direct incitements and threats of violence; (2) advocacy or glorification of violence (i.e., indirect violence); (3) content fostering or justifying prejudice or hostility based on inherent attributes and political affiliations; (4) dissemination of fabricated mis-information; and (5) affiliation with and recruitment for recognized extremist entities or ideologies."}, {"title": "Data Labeling", "content": "The research team, consisting of both criminologists and computer scientists, manually labeled all tweets collected for analysis. Before labeling, we filtered out tweets that were explicitly irrelevant to the topic of domestic extremism. For instance, although the keyword \"coon\" can be employed in a derogatory, racist context to describe a Black person, numerous tweets in our dataset employed the term in reference to the \u201cMaine Coon,\u201d a breed of cat, with no offensive or derogatory connotation. This preliminary screening of explicitly irrelevant tweets"}, {"title": null, "content": "was useful for our research, as it enabled BERT and GPT to focus on online posts that presented more complex classification challenges. We categorized these tweets into two distinct categories: \"yes\" for domestic extremist content, and \u201cno, but relevant\u201d for those not deemed extremist but still pertinent to our study, including discussion of political topics, societal issues, non-extremist commentary on race or religion, and debates on public policies. Further, we classified \u201cyes\u201d tweets into one or more of the five identified elements.\nThree authors (BD, JL, and ZZ) independently labeled the tweets. Discrepancies were resolved through two rounds of discussion, focusing on reaching consensus rather than simply relying on a majority vote. Ultimately, the research team assembled a dataset of 542 labeled tweets for subsequent analysis. This dataset included 151 \u201cyes\u201d and 99 \u201cno, but relevant\u201d tweets associated with \u201cfar-right\u201d ideological keywords, and an additional 188 \u201cyes\u201d and 104 \u201cno, but relevant\u201d tweets linked to \u201cfar-left\" ideological keywords.\u00b2"}, {"title": "Data Analysis: Binary Extremist Post Classification", "content": "We first conducted experiments for the binary extremist post classification task, which was to develop a method capable of determining whether a given post is extremist (positive label \u201cyes,\u201d negative label \u201cno\u201d). This task was performed separately for \"far-right\" and \"far-left\" datasets. For the far-right dataset (250 posts in total), we randomly selected 100 posts to serve as the test data, with the remaining posts allocated as training data. Similarly, for the far-left dataset (292 posts in total), 100 posts were randomly chosen as the test data, and the remaining posts were used as training data."}, {"title": "Traditional supervised learning method with BERT", "content": "Adopting BERT models for classifying extremist posts necessitates a supervised learning paradigm. That is", "sizes": {"experiments": "one where the model was trained on far-left data and then tested on far-right data, and another where the training was done with far-right data, followed by testing on far-left data. Moreover, we conducted experiments where the BERT model was trained using the combined data from both the far-right and far-left categories, subsequently testing the model separately on far-right and far-left test datasets. By merging the training data from both categories, we compiled a list of training datasets comprising {28, 56, 94, 112, 140, 168, 196"}}]}