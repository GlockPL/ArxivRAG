{"title": "Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge", "authors": ["Beidi Dong", "Jin R. Lee", "Ziwei Zhu", "Balassubramanian Srinivasan"], "abstract": "The United States has experienced a significant increase in violent extremism, prompting the need for automated tools to detect and limit the spread of extremist ideology online. This study evaluates the performance of Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-Trained Transformers (GPT) in detecting and classifying online domestic extremist posts. We collected social media posts containing \"far-right\" and \"far-left\" ideological keywords and manually labeled them as extremist or non-extremist. Extremist posts were further classified into one or more of five contributing elements of extremism based on a working definitional framework. The BERT model's performance was evaluated based on training data size and knowledge transfer between categories. We also compared the performance of GPT 3.5 and GPT 4 models using different prompts: na\u00efve, layperson-definition, role-playing, and professional-definition. Results showed that the best performing GPT models outperformed the best performing BERT models, with more detailed prompts generally yielding better results. However, overly complex prompts may impair performance. Different versions of GPT have unique sensitives to what they consider extremist. GPT 3.5 performed better at classifying far-left extremist posts, while GPT 4 performed better at classifying far-right extremist posts. Large language models, represented by GPT models, hold significant potential for online extremism classification tasks, surpassing traditional BERT models in a zero-shot setting. Future research should explore human-computer interactions in optimizing GPT models for extremist detection and classification tasks to develop more efficient (e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes) methods for identifying extremist content.", "sections": [{"title": "Introduction", "content": "The United States (U.S.) has observed a sizeable surge in violent extremism in recent years. According to the Department of Homeland Security (DHS) Office of Intelligence and Analysis, a total of 231 violent extremist incidents (i.e., successful attacks and attempted plots) occurred in the U.S. between 2010 and 2021, indicating a 357% increase from the previous ten-year estimate (see Government Accountability Office, 2023a; 2023b). Violent extremist incidents are motivated by various ideologies, including anti-government extremism (e.g., Oath Keepers; Three Percenters), environmental extremism and eco-terrorism (e.g., Sea Shepard Conservation Society; Animal Liberation Front), and extreme nationalism and white supremacism (e.g., American Freedom Party; Proud Boys). All types of violent extremism pose significant risks as they can lead to various forms of harm to both those directly involved (e.g., victims or targets, law enforcement) and indirectly or vicariously exposed (e.g., bystanders, non-involved civilians). In effect, domestic violent extremism has been identified as a greater threat to national security than foreign terrorism (Chermak et al., 2012; Parkin, Freilich, & Chermak, 2015).\nRecent studies have highlighted the increasing role of online communications platforms in promoting violent extremism, which includes: (1) propaganda dissemination; (2) fundraising; (3) recruitment and radicalization; (4) planning and coordination; (5) online indoctrination; and (6) psychological warfare (Conway, 2017; Holt, Freilich, & Chermak, 2017; 2022; Holt et al., 2019; Klein, 2019; Scrivens, Gill, & Conway, 2020). Since online platforms increase individuals' exposure to radical ideas and provide networking spaces that enable people to connect with like-minded others, they allow for ideologies and motivations to be shared, cultivated, and embraced (Hamm & Spaaij, 2017). Research has found that differential exposure to online extremist content produces changes in attitude (Drevon, 2016; Magdy et al., 2016),"}, {"title": "3", "content": "emotion (Baines et al., 2010; Sikorskaya, 2017), and behavior (Gill et al., 2017; Pauwels &\nSchils, 2016), such that individuals who are more frequently exposed to online extremist material\nare more likely to internalize those messages and form stronger bonds and social networks with\nlike-minded individuals, increasing their likelihood of exhibiting affective, emotional, and\nbehavioral change (Conway, 2017; Scrivens, Gill, & Conway, 2020).\nGovernment agencies and regulators recognize the pressing imperative to address the\nthreat of online extremism through both tactical and strategic measures. For instance, central\ngovernments in various countries have reinforced legal frameworks and encouraged major\ntechnology companies and Internet Service Providers (ISPs) to actively monitor, regulate, and\nremove such content (Aldera et al., 2021; Guhl et al., 2020; Gunton, 2022; Miller, 2017;\nWakefield, 2021). However, these regulations are still evolving and, to some extent, may clash\nwith the financial motivations of technology companies and ISPs, which thrive on user\nengagement and attention. Unfortunately, extremist-promoted mis-information (i.e., false\ninformation), dis-information (i.e., false information deliberately created to discredit an entity), or\nmal-information (i.e., reality-based information that ignites hatred or inflicts harm) often garners\nsignificant visibility, inadvertently benefiting these corporate entities. As of now, a\ncomprehensive counter-extremism strategy that fully incorporates the distinctive attributes of\nonline communications and social media platforms has yet to be developed across the various\nstakeholders involved in addressing this issue (Amble, 2012; Droogan, Waldek, & Blackhall,\n2018). This gap places regulators at a significant tactical disadvantage, especially given the\ngrowing evidence-base around social media's substantial role and impact in predicting various\ncrime problems (Burnap & Williams, 2015; 2016; Waseem & Hovy, 2016; Williams, Burnap, &\nSloan, 2017). As a result, there is an urgent need to improve our ability to confront online"}, {"title": "4", "content": "extremism in an emerging era of advanced technology, including the proliferation of various\nartificial intelligence (AI) tools such as GPT (Achiam et al., 2023; Brown et al., 2020).\nWhile there have been anecdotal accounts of extremists utilizing AI tools for propaganda\nand radicalization purposes (Gilbert, 2023; Siegel & Doty, 2023), criminological investigation\ninto effectively leveraging these AI technologies to counteract online extremism remains limited.\nSpecifically, although various AI approaches have been employed to examine online extremism,\nparticularly for detection and classification tasks, there has been a scarcity of studies that\ncompare these AI approaches to evaluate their respective strengths and weaknesses (see Aldera et\nal., 2021 for discussion). Knowing the specific advantages and limitations of various AI tools is\ncrucial for selecting the most appropriate method when addressing diverse challenges in tackling\nonline extremism in real-world settings. Further, most studies exploring online extremism were\nconcerned with understanding Islamic extremism with minimal attention directed toward online\ndomestic extremism.\u00b9\nGiven the dearth of research in these areas, the current study aims to evaluate the\neffectiveness of two AI approaches, namely a traditional language model (LM)\u2014the\nBidirectional Encoder Representations from Transformers (BERT) model (Devlin, Chang, Lee, \n& Toutanova, 2019)\u2014and a large language model (LLM)\u2014Generative Pre-Trained\nTransformers (GPT) (Achiam et al., 2023; Brown et al., 2020)\u2014in detecting and classifying\nonline domestic extremist posts. In addition, the study seeks to investigate various prompt\nengineering techniques to determine whether certain prompts are more effective than others in"}, {"title": "5", "content": "the detection and classification of online extremist content using GPT. We contend that online\nextremist content is multifaceted and complex, necessitating an approach that goes beyond a\nsimple binary detection between extremist and non-extremist content. It is essential to also\ncomprehend the contributing elements that determine whether a post is extremist, as this\nunderstanding can provide deeper insights into the nuances of online extremism and inform\neffective strategies for detection and intervention.\nCharacteristics of Online Extremism\nExtremist groups and individuals frequently use online communications and social media\nplatforms to engage with a broader audience that would have been challenging to reach through\ntraditional methods (e.g., face-to-face interactions, print media). Compared to face-to-face\ninteractions, online extremism is not restricted by spatiotemporal boundaries, as anyone with\naccess to the Internet can potentially be exposed to online extremist content. In addition, while\nthe intensity of extremist emotions may reach a natural peak and then subside in face-to-face\ninteractions, aggression and hostilities may intensify indefinitely on social media channels\n(Gaudette et al., 2021; Perry & Scrivens, 2016; Scrivens, Gill, & Conway, 2020).\nSeveral characteristics of online communications and social media platforms can be\nexploited by extremists for propaganda and radicalization purposes. Online communications\nplatforms operate within the realms of an \u201cattention economy\u201d where sensational or emotionally\ncharged content outperforms accurate or nuanced information in the competition for users'\nattention. When advancing their ideological agenda, extremist groups purposely construct and\ndisseminate mis-information, dis-information, and mal-information and employ sensational\nheadlines or emotional triggers to capture users' attention and promote the sharing of such\ncontent (Klein, 2019; Weimann & Masri, 2023). The ability to share information anonymously"}, {"title": "6", "content": "or under pseudonyms further facilitates the spread of such information, as it reduces the\nrepercussions for disseminating false content (Ma, Hancock, & Naaman, 2016). While estimates\nof extremists' dissemination of false information are scarce, a recent 2020 report found that\n38.2% of adult social media users in the U.S. shared some kind of false information through\nsocial media platforms (Statista, 2023).\nConsequently, online platforms tend to create echo chambers and filter bubbles where\nusers are predominantly exposed to information that aligns with their existing beliefs and\ninterests. Coupled with cognitive biases like confirmation bias, which leads people to favor\ninformation that confirms their pre-existing views, users become more entrenched in their beliefs\nand more susceptible to mis-information, dis-information, and mal-information, as they are less\nlikely to encounter opposing viewpoints. Moreover, the use of fake accounts (e.g., bots) by\nextremists can rapidly amplify the spread of malicious information and make it seem more\npopular and credible than it is (Alrhmoun, Winter, & Kertesz, 2023; Patel, Agrahari, &\nSrivastava, 2020).\nEmpirical evidence demonstrates the effectiveness of extremist groups in leveraging\nonline communications and social media platforms to further their agenda. For instance, Twitter\naccounts associated with extremist groups have significantly more followers than the average\nTwitter user, and Twitter networks of users who share extremist content are more densely\ninterconnected than those who do not share such content (Berger & Morgan, 2015; Faris et al.,\n2016). Additionally, users who typically consume conspiracy-related content are more likely to\ninteract with and share information from other conspiracy pages (Del Vicario et al., 2016). The\naverage duration of radicalization (i.e., from initial exposure to extremist beliefs to participation\nin extremist acts) decreased from approximately 18-months in 2005, when social media was first"}, {"title": "7", "content": "emerging as a factor in the radicalization of U.S. extremists, to 13-months in 2016 (Jensen et al.,\n2018). Furthermore, the use of social media in extremist movements accelerated the formation of\nconsensus on radical viewpoints and increased commitment to the movement's objectives\n(Carley, 2017).\nArtificial Intelligence Tools and Online Extremism\nThe growing presence of online extremist content and user accounts on various social\nmedia platforms has led global governments to prioritize the detection of online extremism and\nimprove their online counter-extremism efforts, with many electing to adopt content removal\nmeasures to reduce the spread and influence of online extremism (Aldera et al., 2021; Guhl et al.,\n2020; Gunton, 2022). For instance, the German government enacted a law in 2017 that imposed\nfines of up to 50-million euros to social media companies that failed to remove extremist images\nand propaganda from their sites (Miller, 2017). Similarly, the U.K. government drafted an Online\nSafety bill that permitted fines of up to 18-million pounds to social media companies that\nneglected to remove extremist images and propaganda from their platforms (Wakefield, 2021).\nContained within many of these laws is the mandate for social media platforms to both detect and\nremove extremist content from their sites within a short period of time to reduce users'\ninteraction with and exposure to extremist ideology (Gorwa et al., 2020; Gunton, 2022).\nThough many methods can be adopted to accomplish these tasks, the growing use and\ninfluence of social media platforms has necessitated the development of automated tools to both\nimprove the detection of online extremism and minimize the spread of extremist ideology\n(Gaikwad et al., 2021). While earlier methods of detecting online extremism were conducted\nmanually by expert officials and researchers in counterterrorism units, manually filtering through\nthe vast amounts of online data transmitted across social media networks (e.g., volume of social"}, {"title": "8", "content": "media traffic) has become an increasingly improbable task (Aldera et al., 2021; Borum & Neer,\n2018; Gaikwad et al., 2021). As a result, there is a critical need for automated AI solutions (e.g.,\npredictive machine learning, automated hash-matching) to detect harmful content and remove\nthem from online platforms (see Agarwal & Sureka, 2015a; Correa & Sureka, 2013; Fernandez &\nAlani, 2021; Gorwa et al., 2020; Llanso et al., 2020).\nGiven the cross-disciplinary nature of the issue, involving both social and computer\nscientists in developing automated AI solutions is crucial. While research in computer science\ncan enhance our understanding of machine learning and data aspects related to online extremism,\nincluding the development of more efficient methods for modeling, detecting, and predicting\nonline extremism and radicalization (see Ferrara et al., 2016), social science research can provide\nvaluable insights into individual and group behaviors associated with online extremism and\nradicalization. More specifically, research in this area can be segmented into several categories\nbased on their primary focus and objective, including those that adopt AI tools to conduct large-\nscale analyses of online radicalization (e.g., examining communication processes and analyzing\ninfluence and information spread; see Badawy & Ferrara, 2018; Carter et al., 2014; Chatfield et\nal., 2015; Klausen, 2015; Rowe & Saif, 2016), those focusing on the automatic detection of\nextremism (e.g., detection of extremist content and user accounts), as well as those that focus on\nthe automatic prediction of radicalization (e.g., adoption of extremist content and interaction with\nextremist accounts; see Fernandez & Alani, 2021).\nResearch exploring individuals' transmission of online extremist content (i.e., information\nspread, influence transmission) have examined the online behaviors of extremist users through\nproxy parameters such as posting frequency and user mentions (see Carter, Maher, & Neumann,\n2014; Chatfield, Reddick, & Brajawidagda, 2015; Klausen, 2015). Specifically, these studies"}, {"title": "9", "content": "examined the ways in which extremists communicate with their followers, the terms and phrases\nthey use in their communication, and the high relevance of social homophily on the diffusion of\npro-extremist terminology (Vergani & Bliuc, 2015; Rowe & Saif, 2016). Using Natural\nLanguage Processing (NLP) techniques to filter their social media data (e.g., Twitter) for\ntopically relevant content, Badawy and Ferrara (2018) found that extremist propaganda often\nrevolves around four types of messaging (i.e., theological, violence, sectarianism, influential\nactors/events). Though these findings serve as a starting point for online extremism detection,\nthese studies mainly focus on understanding the online radicalization process with minimal\nattention directed toward automatically detecting online extremism (Aldera et al., 2021).\nStudies that focus on automatically detecting online extremism have been increasing in\nrecent years. For instance, Lara-Cabrera and colleagues (2019) used a set of keywords derived\nfrom social science theories of radicalization to automatically extract online extremist content,\nnoting that while the proposed metrics reveal promising results, more refined metrics are needed\ngiven the inherent limitations of relying solely on keywords. In fact, many studies exploring the\nautomatic detection of online extremist content are based on various textual features and\nfrequently adopt machine learning techniques to identify extremism (Agarwal & Sureka, 2015a;\nAshcroft et al., 2015; Kaati et al., 2015; Magdy et al., 2016). For instance, Agarwal and Sureka\n(2015b) explored semi-supervised learning approaches to detect extremist posts on Twitter based\non a list of extremist hashtags to filter content related to foreign extremism, finding that religious\nwar-related terms and offensive words containing negative emotions were strong indicators of\nonline extremist tweets. Relatedly, both Ashcroft and colleagues (2015) and Kaati and colleagues\n(2015) used data-dependent (e.g., common hashtags, word bigrams, frequent words) and data-\nindependent (e.g., stylometric and time markers) features to detect online extremist messages,"}, {"title": "10", "content": "revealing the benefits of combining both features to enhance classifier performance (see also\nAgarwal & Sureka, 2015b; Magdy et al., 2016).\nThough many advancements have been made, several challenges with automated online\nextremism detection still exist. One significant issue is the absence of a uniform definition of\nonline extremism, leading to varied interpretations across different studies and extremism\ndetection algorithms. This lack of consensus results in a fragmented research landscape, with no\nstandardized collection of online extremism data for analysis and algorithm training (Housen-\nCouriel et al., 2019). For example, one study may have identified a particular post as extremist,\nwhereas another study may not classify that same post as extremist given contrasting or differing\ndefinitions of what constitutes extremism. This leads to multiple findings and datasets of online\nextremism without a consistent marker of the construct. This issue is exacerbated by the constant\nevolution of behaviors associated with online extremism, including changes in terminology and\nextremist beliefs (Fernandez & Alani, 2021). Relatedly, the diversity of content that is within the\nsame sphere of extremist ideology may pose challenges to the automatic detection of extremist\ncontent and user accounts. That is, while distinguishing domestic far-right extremism from\ndomestic far-left or Jihadist extremism may be a simpler task, there are many extremist groups\nwho espouse different extremist attitudes and actions, or have differing interpretations of\nextremist concepts, despite sharing large portions of the main extremist ideology (Fernandez &\nAlani, 2021).\nAnother limitation associated with many automated extremist detection studies is their\noverreliance on a set of expressions at the expense of understanding the full context behind those\nwords and/or phrases (Fernandez & Alani, 2021). For instance, while many AI-involved studies\nsearch for keywords and phrases to determine if online content is extremist in nature, they are"}, {"title": "11", "content": "unable to fully grasp the context in which these words and phrases are used. Similarly,\nclassification algorithms may encounter difficulties with correctly identifying an extremist post\nfrom one that is based on sarcastic rhetoric (Barnes, 2022). As a result, these findings may\ninclude non-extremist content despite containment of relevant keywords and phrases (see\nFernandez & Alani, 2021).\nMoreover, many online extremism data used for research purposes may contain biases\nthat do not reflect the larger population of interest, such as terminology and time-period bias\n(Fernandez & Alani, 2021). Terminology bias occurs when data is compiled based on a select\nnumber of terms and expressions (i.e., restricted lexicons) that encompass only a subsection of\nthe topics discussed by extremist groups or individuals (Fernandez & Alani, 2021). For instance,\ndata may be collected using lexicon that overrepresents a particular subgroup of extremists or\nthose speaking only one language (e.g., Arabic), thus failing to capture a representative sample of\nextremist individuals, or even the larger group of extremists that fall within similar ideologies\n(Fernandez & Alani, 2021). Similarly, time-period bias occurs when data collection is restricted\nto a specified week or month(s) where notable world events or irregular activities (i.e., terrorist\nattacks, political and religious demonstrations) are taking place that skew the data (Fernandez &\nAlani, 2021). If algorithms are trained using data containing time-period bias, they may not be\nable to account for extremist content that appear across different time periods since those\nclassifiers may evolve or change over time (Fernandez & Alani, 2021).\nThough not unique to automated detection studies, extremism datasets derived from social\nmedia samples are also prone to false positives (i.e., falsely categorized as extremist when it is\nnot) given these collected data are not verified or only partially verified. For instance, an online\npost that reads \u201cIslamic State hacks Swedish radio station\u201d may be processed as extremist during"}, {"title": "12", "content": "data collection even though the actual post is not extremist in nature. Other posts may be\nerroneously classified as extremist for simply containing religious rhetoric, regardless of its\nextremist nature (e.g., \u201cif you want to talk to Allah, pray. If you want Allah to talk to you, read\nthe Qur'an\"). Counternarratives can also be identified as extremist content by various AI-\nsolutions given their likeness to actual extremist posts (e.g., \u201carmed Jihad is for defense of\nMuslim nation, not for establishment of the Khilafah\u201d). In essence, various instances involving\nfalse positives may arise because data collection algorithms are trained to detect relevant\nkeywords and sentiments (Fernandez & Alani, 2021).\nIt is important to note that the automatic detection of extremist content and user accounts\nare sensitive inquiries, as inaccurately labeling a post or user as extremist may result in\ncensorship or unwarranted surveillance and investigation of an innocent person (Fernandez &\nAlani, 2021; Olteanu et al., 2017). The private and restricted nature of these datasets and their\nassociated algorithms further limits it from being verified by others to determine its accuracy\n(Fernandez & Alani, 2021). In fact, very few datasets involving online extremism for research\npurposes are publicly shared for others to verify or validate (see Kaggle, 2019 for exception).\nSince most datasets are not made publicly available, there is uncertainty as to how much online\ncontent is actually extremist since reported findings can be highly skewed by irrelevant accounts\nor misleading classifications (see Parekh et al., 2018 for discussion). Given the consequences\nassociated with erroneously categorizing an individual as being an extremist or engaging in\nonline extremism (e.g., enhanced surveillance, censorship, restricted access to platform), it is\nimportant to consider the potential sources of inaccuracy with automated AI detection approaches\nand regularly reflect on the continuously changing patterns of extremism to reduce the potential\nnegative impact of AI solutions and developments (Fernandez & Alani, 2021; Harford, 2014).\""}, {"title": "13", "content": "Current Study\nConsidering the general background and limitations in existing research on AI tools and\nonline extremism, the current study has three main objectives. First, the current study seeks to\ndetermine the effectiveness of two AI approaches (i.e., BERT and GPT) in identifying domestic\nonline extremist posts. Before the recent emergence of LLMs, BERT was widely recognized as\nan advanced machine learning tool for detection and classification tasks. Unlike the traditional\n\"bag of words\" approach in NLP for text representation, which focuses solely on the frequency of\nwords, the BERT model considers the context and relationships between words in a sentence.\nBERT generates contextualized word embeddings, meaning that the representation of a word\nchanges based on the surrounding words in the sentence. Stated differently, the same word can\nhave different representations depending on its context, enabling BERT to understand nuances in\nmeaning. This advancement has at least partially addressed the issue of overreliance on a set of\nexpressions or keywords while neglecting the broader context in which those words and/or\nphrases are used. Though the BERT model has generally demonstrated positive results in\nidentifying online extremism, it is inherently a supervised machine learning approach when\napplied to downstream tasks, heavily reliant on both the quantity and quality of input labeled\ndata. The training data directly influences the identification outcome, making the model's\nperformance contingent on the comprehensiveness and accuracy of the data it is trained on.\nIn contrast, LLM methods, such as the representative model GPT, offer a zero-shot\napproach for downstream tasks. These models leverage their pre-trained knowledge and\nunderstanding of language to make predictions or classifications, enabling them to tackle new\ntasks without the need for explicit training on labeled data specific to those tasks. If an LLM"}, {"title": "14", "content": "approach to identifying online extremism proves to be accurate and efficacious, it could offer\nsignificant advantages over BERT and other supervised machine learning approaches. Given that\none of the goals of extremism research is to accurately identify extremist content and intervene\nbefore it spreads to wider audiences, LLMs provide a potentially more effective and practical\nmeans of achieving this goal. To date, no known study has compared the efficacy of both a\nBERT and GPT model within the same study (i.e., using the same data) to determine which\nmethod is more accurate and effective at accomplishing the specified task.\nSecond, the current study seeks to explore various prompt engineering techniques to\ndetermine whether certain prompts are more effective than others in identifying online domestic\nextremist content using GPT. In brief, prompt engineering refers to the process of designing and\nrefining input instructions to effectively guide LLMs in performing specific tasks. For example, a\nprompt that simply asks the models to classify an online post as \u201cextremist\u201d or \u201cnon-extremist\"\nmight not provide enough context for the LLM to make an accurate judgment. However, a more\ncarefully engineered prompt (e.g., including specific examples of extremist language; instructing\nthe LLM to assume a certain role; explicitly offering a definitional framework) could result in\nmore accurate results. Additionally, prompt engineering can help address the challenge of\nevolving language and terminology used by extremist groups. By employing prompts that reflect\ncurrent trends and language patterns, LLMs can maintain their effectiveness in identifying\nextremist content, adapting to the evolving language used by these groups. Moreover, LLMs can\nbe prompted to provide the reasoning process through which they reach their decisions or\nconclusions. Unlike traditional \u201cblack box\u201d models, we can analyze the explanations or\njustifications provided by LLMs to understand why they arrived at correct or incorrect\nconclusions (e.g., compared to human-labeled gold standards). The insights gained from"}, {"title": "15", "content": "examining the reasoning process itself have the potential to significantly advance online\nextremism research.\nLastly, the current study explores the capabilities of AI tools to classify online extremist\ncontent beyond simple dichotomies of extremism versus non-extremism. This more challenging\ntask involves identifying and categorizing the nuanced elements and variations within extremist\ncontent, requiring a deeper understanding and more sophisticated analysis by the AI models. For\nexample, many extremist posts are nuanced and infused with esoteric rhetoric (e.g., sarcasm).\nWhile previous studies have found that BERT models are fairly effective at distinguishing\nbetween extremist and non-extremist content, they often struggle with more complicated\nclassifications of online extremism (e.g., far-right v. far-left; posts containing extremist key terms\nbut are not inherently extremist posts) (see Fernandez & Alani, 2021). This challenge is linked to\nBERT's reliance on training data and the inability of researchers to compile a sufficiently large\ntraining dataset containing the many distinct layers and elements of extremism. As of now, there\nis limited knowledge regarding LLMs' ability to perform more refined classification tasks\ninvolving online extremism.\nMethods\nData and Sample\nWe collected Twitter posts (i.e., tweets) using Twitter's Application Programming\nInterface (API) version 2 on May 28th, 2023. Our data collection adhered to Twitter's terms and\nconditions at that time. Prior to gathering data, we ensured that no significant global events\noccurred in the previous month which could have influenced the typical presence and discussion\nof potential domestic extremist content on general-purpose social media platforms. In line with"}, {"title": "16", "content": "our research objectives, we created two distinct sets of keywords, targeting \"far-right\" and \"far-\nleft\" ideologies, to retrieve posts potentially relevant to domestic extremism. Our selection of\nsearch keywords was guided by two criteria. First, we based our choices on keywords used in\nprior studies on extremism, thus ensuring their relevance and validity (Scrivens, 2021; Scrivens,\nDavies, & Frank, 2018; Scrivens, 2017). Second, we aimed for the keywords to be indicative but\nnot definitive of extremism, meaning that a post containing such a keyword is not necessarily an\nextremist post. The complete list of keywords used in this research can be seen in Appendix A.\nOur study focused exclusively on Twitter due to data availability constraints. Twitter's\nuser base represents between one-fifth and one-quarter of the U.S. population and tends to be\nyounger, more educated, and more affluent, with a greater likelihood of identifying as Democrats\ncompared to the overall U.S. adult population (Wojcik & Hughes, 2019). Thus, the findings\nshould be interpreted with caution due to the non-coverage of other general-purpose social media\nplatforms, which might also host extremist content (e.g., Facebook, Instagram, YouTube, or\nTikTok), and the specific demographics of Twitter's users.\nThe Working Definitional Framework\nGiven that not every post containing one or more of our search keywords necessarily\nrepresents domestic extremism, we established a data labeling strategy tailored to our research\nobjectives. Extremism, especially in online contexts, lacks a universally accepted definition (see\nFernandez & Alani, 2021). Consequently, after a thorough review of existing definitions in\nempirical studies, we developed a working definitional framework for identifying extremism\nwithin our research. For a Tweet to qualify as \"extremist,\" we determined that it must exhibit at\nleast one of the following two thematic components:"}, {"title": "17", "content": "(a) Posts that explicitly or implicitly incite violence against specific individuals or\ngroups. This includes content that celebrates, justifies, or advocates any form of harm towards an\nindividual or group based on their identity or beliefs.\n(b) Content that promotes or justifies prejudice or hostility based on inherent attributes\n(e.g., race, religion, nationality, sexual orientation) potentially leading to conflict or actions (e.g.,\nverbal, relational, social) aligned with socio-political beliefs or interests. Examples include posts\nadvocating for the supremacy/inferiority of particular groups or individuals, dissemination of\nfalse information to sow discord and animosity, and content related to recruiting, supporting,\nadmiring, or expressing allegiance with known extremist groups or individuals.\nFor the purposes of subsequent coding and analysis, the aforementioned thematic\ncomponents were further divided into five elements: (1) direct incitements and threats of\nviolence; (2) advocacy or glorification of violence (i.e., indirect violence); (3) content fostering\nor justifying prejudice or hostility based on inherent attributes and political affiliations; (4)\ndissemination of fabricated mis-information; and (5) affiliation with and recruitment for\nrecognized extremist entities or ideologies.\nData Labeling\nThe research team, consisting of both criminologists and computer scientists, manually\nlabeled all tweets collected for analysis. Before labeling, we filtered out tweets that were\nexplicitly irrelevant to the topic of domestic extremism. For instance, although the keyword\n\"coon\" can be employed in a derogatory, racist context to describe a Black person, numerous\ntweets in our dataset employed the term in reference to the \u201cMaine Coon,\u201d a breed of cat, with no\noffensive or derogatory connotation. This preliminary screening of explicitly irrelevant tweets"}, {"title": "18", "content": "was useful for our research", "categories": "n\"yes\" for domestic extremist content", "no, but relevant": "or those not deemed extremist but\nstill pertinent to our study", "yes": "ntweets into one or more of the five identified elements.\nThree authors (BD"}, {"yes": "nd 99 \u201cno", "far-right": "deological keywords"}, {"yes": "nd 104 \u201cno", "far-left\\\" ideological keywords.\u00b2\nData Analysis: Binary Extremist Post Classification\nWe first conducted experiments for the binary extremist post classification task, which\nwas to develop a method capable of determining whether a given post is extremist (positive label\n\u201cyes,": "egative label \u201cno\u201d). This task was performed separately for \"far-right\" and \"far-left\"\ndatasets. For the far-right dataset (250 posts in total), we randomly selected 100 posts to serve as\nt"}]}