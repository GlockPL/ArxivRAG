{"title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems", "authors": ["Hashmath Shaik", "Alex Doboli"], "abstract": "Large Language Models offer new opportunities to devise automated implementation generation methods that can tackle problem solving activities beyond traditional methods, which require algorithmic specifications and can use only static domain knowledge, like performance metrics and libraries of basic building blocks. Large Language Models could support creating new methods to support problem solving activities for open-ended problems, like problem framing, exploring possible solving approaches, feature elaboration and combination, more advanced implementation assessment, and handling unexpected situations. This report summarized the current work on Large Language Models, including model prompting, Reinforcement Learning, and Retrieval-Augmented Generation. Future research requirements were also discussed.", "sections": [{"title": "I. INTRODUCTION", "content": "Problem solving is the process of creating a solution for a problem description [1]-[4]. The solution can be an explanation for a set of properties exhibited by a static or dynamic situation, e.g., a mathematical proof, or an implementation (realization), which is the construction of a new materialization (e.g., design) that exhibits the required properties as a result of their operation (functioning, execution). This report focuses on the implementation (realization) of problem solving.\nCreating an implementation can pertain to the three general-purpose problem-solving situations: well-defined problems, ill-defined problems, and open-ended problems [5]-[7]:\n1) Well-defined problem solving for implementation construction describes situations in which an existing solution can be reused with some incremental changes to solve a new problem. For example, textbook algorithms are utilized to solve a new problem by selecting proper data structures and customizing the algorithm parameters, like the conditions of conditional statements and the iterations of loops. Using parameterized templates for circuit design [8]-[11] belongs to this category too.\n2) Ill-defined problem solving for implementation construction represents cases in which the existing implementations cannot solve all requirements, i.e. they satisfy some but not others [12], [13]. Changing the parameters of the implementation does not address the issue. Problem solving includes options, like producing a description of the implementation trade-offs by parameter sampling and selecting the best compromise, exploring implementation alternatives for specific fragments of the implementation, so that better trade-offs result for the overall solution, and selecting a different approach (principle) for an implementation, including situations when a new implementation must be built, similar to open-ended solving for building a new implementation.\n3) Open-ended problem solving for implementation generation requires devising new solutions with a significant departure and characteristics from previous implementations. The understanding of this process is still limited [14], [15]. Also, there are insufficient metrics to describe the degree to which the process is systematically progressing towards success, e.g., building a new implementation. Typical activities include problem framing and problem understanding, identifying and selecting the solving approach, divide and conquer (e.g., problem partitioning into sub-problems), implementation elaboration through trial-end-error, feature combination, adjustment, abstraction and insight gaining, implementation analysis to find pros and cons and the impact of features on the implementation operation, implementation modification, error correction, and handling unexpected situations.\nAs summarized in the next section, traditional automated implementation generation focuses mainly on elaboration and parameter trade-off exploration, for which the domain knowledge of the implementation is captured by customized metrics [16] or in a library of basic building blocks [16], [17]. The library is static and does not evolve to incorporate new knowledge either from external sources or as a byproduct of implementation generation. Moreover, traditional methods assume the existence of a problem specification expressing at least functional and performance requirements, but more often"}, {"title": "II. OVERVIEW OF TRADITIONAL AUTOMATED IMPLEMENTATION GENERATION", "content": "Traditional approaches to automatically generate implementations can be grouped into four broad categories: (i) approaches based on high-level specifications, (ii) methods using evolutionary algorithms, (iii) agent-based methods, and (iv) cognitive architectures. The four categories are summarized next.\n(i) Approaches based on high-level specifications: These approaches include traditional compiling methods to generate executable code [16] and high-level synthesis methods [18]-[21] and template-based synthesis [10], [11] to create electronic circuits and systems. They use high-level specifications described using a programming language. Conceptually, specifications serve as parameterized descriptions of the target implementation architecture. Specifically, internal representations are built using a set of predefined rules (e.g., language grammar) applied to the specifications and then used to create an optimized hardware design by exploring different optimization possibilities. Prediction models or simulation tools are integrated to evaluate the performance of possible implementation alternatives.\nThese methods address the problem-solving activities in the following ways: The specification gives an unambiguous, complete description of the parameterized architecture. Thus, there is no problem framing step and problem understanding is fully addressed during specification creation. Divide and conquer is defined by the structuring of the specification. Also, there is no step of exploring possible implementation alternatives, as the specification explicitly describes the data processing steps, including the connections between the sequences of processing steps, i.e. using the processing outputs as inputs for the next processing steps. Hence, feature combination during elaboration only connects predefined operators which do not change their function based on the connections. From the point of view of cognitive psychology, these combinations are relation-based combinations but do not reflect feature-based combinations, in which features of a concept are transferred to another concept [22]. Hence, there are no unexpected situations, including emerging features. Implementation analysis uses performance models and simulation, even though the pros and cons of an implementation are rarely causally linked to the implementation fragments responsible for them. Hence, the insight gain is limited. Trial-and-error (possibly guided by priority functions), implementation modification, and adjustment are only at the level of optimizing the architecture parameters. There is no abstraction or summarization during the process. Error correction requires to modify the specification and then repeat the problem-solving process.\n(ii) Methods using evolutionary algorithms: These methods create a dynamic process, in which large populations of"}, {"title": "III. OVERVIEW OF LARGE LANGUAGE MODELS AND DIFFUSION MODELS", "content": "A. Large Language Models\nLarge Language Models (LLMs), primarily those built on transformer architectures, have made significant strides in producing coherent, contextually relevant text [33]. They excel at pattern recognition and can generate fluent natural language by leveraging billions of parameters trained on massive corpora [34]. However, their computational principle-self-attention over sequential data imposes fundamental limitations that hinder their ability to perform the rich, open-ended problem-solving tasks described in the previous sections.\nAt the core of these limitations is the reliance on statistical correlations rather than genuine logical or conceptual understanding. While self-attention excels at identifying relevant tokens in a sequence, it does not inherently encode hierarchical structures, domain-specific causal rules, or strict logical constraints. This stands in contrast to open-ended problem solving, where the concept space can be segmented into three main categories\u2014hierarchical concepts, alternative concepts, and fundamental concepts\u2014and the action space encompasses complex operations, such as feature combination, dynamic adjustment, abstraction, insight generation, and summarization [35]. LLMs struggle to engage these conceptual spaces in a principled way because they are not grounded in mechanisms that ensure hierarchical reasoning, strategic problem decomposition, or the flexible reuse of insights and intermediate representations [36].\nAnother critical shortcoming is that LLMs tend to produce generalized answers aligned with the statistical patterns seen in their training data [37]. They are not inherently equipped to execute a true divide-and-conquer approach to complex tasks, nor can they systematically apply trial-and-error strategies. For example, while open-ended problem solving may demand iterative refinement\u2014where a solver explores a space of possible solutions, backtracks as necessary, and learns from failed attempts\u2014an LLM's output is typically a single forward"}, {"title": "B. Prompting Engineering", "content": "Prompting techniques utilize carefully constructed input prompts to guide the model's response generation process. Techniques can be grouped into five categories dicussed next.\na) Single-stage prompting (SSP): SSP methods directly instruct the model without iterative refinement. Meanwhile, Basic + Annotation Guideline-Based Prompting + Error Analysis-Based Prompting [43] uses formally defined entity annotation guidelines to specify how clinical terms should be identified and categorized, ensuring clarity in entity recognition. In addition, it incorporates instructions derived from analyzing common model errors, such as addressing ambiguous entity boundaries or redefining prompts for overlapping terms. This strategy significantly improves clinical Named Entity Recognition, with relaxed F1 scores reported as 0.794 for GPT-3.5 and 0.861 for GPT-4 on the MTSamples dataset [44] and 0.676 for GPT-3.5 and0.736 for GPT-4 on the VAERS dataset [45], demonstrating its effectiveness.\nb) Reasoning strategies: These methods are of three types: linear, branching, and iterative reasoning.\nLinear reasoning methods such as Chain-of-Thought (CoT) [46], Complex CoT [47], Thread-of-Thought"}, {"title": "C. Knowledge Retrieval", "content": "Retrieval-Augmented Generation (RAG) addresses one of the major issues of LLMs, which are their lack of a persistent, reliable memory and factual grounding [78]. RAG methods integrate external knowledge sources into the generation process. Instead of relying solely on learned representations within the model's parameters, the system retrieves relevant documents, facts, or structured data at inference time and incorporates this information into its output. This grounding reduces hallucinations, ensures that the model's reasoning steps reference accurate and up-to-date information, and can improve the alignment of the solution with real-world constraints [79]. The versatility of RAG has led to significant advancements in various domains, such as healthcare, finance, education, and scientific research facilitated by novel frameworks tailored to address challenges in reasoning, problem-solving, and knowledge integration. This review categorized these advancements into four areas: task-specific and schema-based techniques, self-aware and adaptive mechanisms, long-term"}, {"title": "D. Reinforcement Learning", "content": "Reinforcement Learning (RL) provides a systematic framework for refining LLM behavior by guiding models toward desired objectives through iterative feedback and carefully designed reward signals [105]. There are six main components: agent, environment, state, action, reward, and policy [106]. To apply RL for fine-tuning LLMs, the first step maps the six components to the LLM framework: the LLM represents the policy, the current textual sequence is the state, and based on this state, the LLM generates an action, the next token. This action updates the state, creating a new state that incorporates the newly added token. After generating a complete textual sequence, a reward is determined by assessing the quality of the LLM output. This reward can be used to train a pre-trained reward model or can be directly integrated into the alignment process to guide the behavior of the model.\nThe RL methods adopted by these models can be divided into two main categories, model-based RL approaches and model-free approaches, which were discussed next.\na) Model-based RL Approaches:The methods in this category can be grouped into three categories, RLHF, RLAIF and exploration, which are discussed next.\nReinforcement Learning from Human Feedback (RLHF): RL from Human Feedback (RLHF) re-train LLMs by incorporating a reward signal derived from human evaluations. RLHFs perform three fundamental stages: They initially perform supervised fine-tuning (SFT) using labeled datasets, followed by training a reward model (RM) based on human-evaluated outputs, and finally use this reward signal to inform the model's policy fine-tuning using the Proximal Policy Optimization (PPO) algorithm [107]."}, {"title": "IV. SIMILARITIES OF LLMS AND TRADITIONAL AUTOMATED IMPLEMENTATION GENERATION METHODS AND RELATED RESEARCH NEEDS", "content": "A broad analogy can be identified between using Genetic Algorithms (GAs) and LLMs for implementation creation.\n1) Selection: GA selection chooses the fittest individuals to pass their genes to the next generation. In fine-tuning or training data selection, LLMs prioritize coherence and relevance when generating textS, similar to selecting relevant context for responses. Like choosing the best seeds from a harvest, LLMs select the most relevant words or sentences to continue a conversation [156].\n2) Crossover (Recombination): GA crossover combines the genomes of two parents to create a new individual. This is similar to blending knowledge from different domains during text generation. For example, merging insights from literature and science in a single response. Crossover is like an LLM writing poetry about quantum physics, e.g., combining Shakespearean elegance with scientific rigor [157].\n3) Mutation: GA mutation introduces random changes in a genome to explore new possibilities. Similar to the slight randomness added during sampling techniques like top-k or temperature settings, which allow LLMs to produce diverse responses. Mutation in GAs is like LLMs occasionally breaking patterns to say something unexpected or creative [158].\n4) Inversion: GA inversion reverses a segment of the genome to explore new configurations. This parallels rephrasing or reordering sentences during text generation while preserving the original meaning. Like flipping a playlist order for a new vibe, LLMs rephrase \"The car is fast\" into \"A fast car it is\" [159].\n5) Elitism: GA ensures the best solutions carry over unchanged to the next generation. Similar to checkpointing the best-performing weights during training or favoring high-confidence outputs in decoding strategies. Like archiving the best answers during an essay edit, LLMs retain their most confident responses for the final output [160].\n6) Replacement: GA decides how much of the old population to keep versus the new one. Similar to parameter updates during fine-tuning, where new knowledge replaces older information incrementally. Replacement is like LLMs balancing old facts while integrating new updates, ensuring a model doesn't \u201cforget\u201d but adapts to current knowledge [161].\n7) Fitness Evaluation: GA scores individuals based on quality to determine their survival. Similar to evaluating model outputs using metrics like BLEU, ROUGE, or user feedback in RLHF. Fitness evaluation is like an LLM receiving human feedback to improve its responses based on relevance, coherence, or creativity [162].\n8) Exploration vs. Exploitation: GA balances trying new possibilities (exploration) and refining known solutions (exploitation). Balancing randomness and coherence during response generation. Parameters like temperature encourage exploration, while context relevance drives exploitation. Just as genetic algorithms search for novel solutions, LLMs strike a balance between playful creativity and logical reasoning in ambiguous prompts\n\u2022 Strategy 1 describes the elaboration process in which each kernel is elaborated without changing the kernel. A set of detailing alternatives can be used for each elaboration step to produce an implementation envelope. The envelopes are incrementally elaborated until the final implementation is created.\n\u2022 Strategy 2 represents the process, which in addition to the elaboration steps of Strategy 1 also uses elaboration results corresponding to a different implementation cluster. Figure 1 shows the using of features from Implementation cluster 1 (red arrow in the figure) to build the implementations of Implementation cluster 2. Hence, the subsequent solution include elaboration of all kernel features and the features adopted from another cluster.\n\u2022 Strategy 3 uses a kernel that combines kernel features from two different implementation clusters. The blue arrows in Figure 1 illustrates the combination.\n\u2022 Strategy 4 presents an elaboration process in which the selected detailing alternatives are excluded from the elaboration steps used for building other implementation clusters. It represents the excluded niche in Figure 1 (green arrow).\n\u2022 Strategy 5 creates a kernel bottom-up by identifying and generalizing the features of individual implementations. The individual implementations were produced through less-structured methods, like, for example, through experimental trial-and-error.\nWhile the five strategies provide templates for the implementation elaboration process, automated implementation generation requires the following additional activities:\n1) Divide and conquer: The activity partitions a problem into sub-problems and then provides ways to integrate the implementation for the sub-problems. Task decomposition methods in LLM prompting [73], [74] can produce certain decompositions, especially in situations for which the sb-problems are less coupled. However, design problems are often strongly coupled, so that even though there are specialized modules to implement a certain function, their operation and performance are tightly related. Decomposition requires not only a static problem partitioning based on the items in the prompts (i.e. words) but also the interpretation of a sub-problem within the context set-up by the interpretations of other sub-problems. LLM fine tuning through RL is likely infeasible due to the huge space of possible decompositions possible in real-life. A mechanism is also needed to track the analyzed decompositions, so that the information can be used to improve future decompositions. This capability is absent in current methods.\n2) Kernel creation: The method creates kernels either by assembling the features likely to address the problem requirements and then elaborating them top-down or in a bottom-up process as detailed in Strategy 5. Separate kernels can be created for different sub-problems followed by integrating them into a single kernel and its elaboration or separately elaborating each kernel and integrating their implementations. Ideas on LLM self-reflection and focus on the main information [71] can help identify the features to be included in a kernel. However, finding kernels, e.g., the invariant features present in all implementations pertaining to a cluster, remains mostly a manual process. Methods similar to RLHF [107] can help retrieving similar features, but their scalability is likely low. Moreover, combining features from different kernels to generate a new kernel (Strategy 3) has not been studied by current LLM methods. The combination of features needs a way to predict the expected performance at a high level (possibly a qualitative evaluation), which can be offered to some degree by LLM, similar to the use of LLM to solve ambiguities [82], [83]. However, it is likely that the current methods are insufficient for this purpose.\n3) Elaboration: Executing the five strategies requires devising additional methods for detailing alternatives, predicting the effectiveness of each alternative in the context of a partial implementation, assigning a priority (preference) to each alternative, and incorporating the alternative into the partial implementation. A possible approach is to use schema for elaboration, similar to RAG methods for LLMs [80], [81]. Schema matching can benefit from LLMs to clarify certain ambiguities, such as in [82], [83]. However, schema are static structures, useful in analogical reasoning, even though problem solving often requires performing new sequences of decisions beyond a static schema.\n4) Implementation assessment: LLMs can be used for two kinds of performance assessments. Qualitative assessment, including comparing implementations, such as pairs of circuit designs, can be obtained by prompting traditional LLMs. CoT prompting can be used to obtain performance assessment at a finer granularity. RLHF can fine tune assessment by adding human feedback about the quality of the implementations [112]. Moreover, self-critique methods could be used to improve the correctness and completeness of the LLM responses, like self-consistency and cross-referencing methods in VE [104]. A second approach uses datasets of characterized implementations to train an LLM, similar to exploration techniques in RL [133], [134]. Then, the generalization capacity of LLMs are used to quantitatively predict the performance parameters of a new implementation. Nevertheless, the two approaches do not scale beyond the samples used in training an LLM, including situations in which a new implementation uses a nonlinear combination of the features of different implementation. There is no mechanism similar to setting-up precise physical models of an implementation, so that the models can be solved to produce quantitative performance assessment, like in traditional automated implementation creation methods.\n5) Memory and learning: Similar to using long-term memory for knowledge retrieval in RAG, memory systems are needed to for learning to store associations, like kernel features, their most relevant implementations fragments, and their performance values or between high-level features and their detailed elaborations, the causal relationships of main features and performance attributes, and elaboration sequences that produced high-quality implementations. Similar to schema-based retrieval, memory cueing must solve semantic ambiguities.\n6) Adaptive process: It includes the sequence of automated activities performed to create an implementation. It requires devising new means to predict the expected outcomes of the available activities, selecting and adapting an activity to the current context, under-\nstanding the degree to which the sequence advances towards creating an implementation, and learning new knowledge available during the process. Also, when addressing collaboration between humans and LLMs to tackle unexpected challenges, such as handling zero-day attacks, the process necessitates reasoning, under-\nstanding of prior instructions, and intuitive decision-making within the context of new parameters and constraints. To automate this process, exploring reasoning techniques, including deductive reasoning, inductive reasoning, analogical reasoning, common sense reasoning, tree-of-thoughts, multiple chains of thought, causal reasoning, heuristic reasoning, and symbolic reasoning, is required. Among these, the primary human thought process often involves mapping the current problem to a previously encountered one or identifying similarities with analogous problems, like in analogical reasoning. Consequently, an effective approach to problem modeling could involve neuro-symbolic representations that allow LLMs to dynamically learn and adapt in real-time. Techniques such as grokking, which enable models to discover relationships and patterns through iterative refinement, and masked LLMs are promising methods to achieve this goal. These approaches empower the model to derive connections on the fly, effectively merging learned representations with reasoning capabilities."}, {"title": "V. CONCLUSIONS", "content": "Recent advances in Large Language Models (LLMs) offer the opportunity to extend automated implementation generation techniques beyond the current methods that require algorithmic specifications as input and can use only statically domain knowledge. LLMs can process multi-modal descriptions, including ideas communicated in natural language and using images and with certain degrees of specification completeness, unknowns, and ambiguity. LLMs learn a broad range of associations and for diverse contexts. These new capabilities might offer intriguing paths beyond traditional implementation generation, such as support problem framing and exploration of possible solution approaches, improved implementation assessment across abstraction levels by comprehensive comparison to similar, externally available implementations, collective feedback and preferences, and enhanced elaboration by incorporating continuously updated domain knowledge. These features are critical in solving open-ended problem, currently hard to address with existing methods. Summarizing the state-of-the-art on LLMs and their related improvements is a first step towards devising nocel LLM-based methods for implementation generation.\nThis report offers a comprehensive overview of existing LLM techniques and studied the degree to which they can model the activities needed for implementation generation for open-ended problem solving. The overview presents LLM enhancements, like prompting, Reinforcement Learning (RL) and Retrieval-Augmented Generation (RAG). Then the report discusses the possibility of using LLMs to realize problem solving activities that are not available in traditional automated implementation generation methods. New research requirements are also presented, e.g., support for problem framing, creating an implementation approach, effective elaboration control, robust qualitative and quantitative assessment across abstraction levels, knowledge memorizing during learning, and managing the problem solving process."}]}