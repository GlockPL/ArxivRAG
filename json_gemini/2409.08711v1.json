{"title": "Text-To-Speech Synthesis In The Wild", "authors": ["Jee-weon Jung", "Wangyou Zhang", "Soumi Maiti", "Yihan Wu", "Xin Wang", "Ji-Hoon Kim", "Yuta Matsunaga", "Seyun Um", "Jinchuan Tian", "Hye-jin Shim", "Nicholas Evans", "Joon Son Chung", "Shinnosuke Takamichi", "Shinji Watanabe"], "abstract": "Text-to-speech (TTS) systems are traditionally trained using modest databases of studio-quality, prompted or read speech collected in benign acoustic environments such as anechoic rooms. The recent literature nonetheless shows efforts to train TTS systems using data collected in the wild. While this approach allows for the use of massive quantities of natural speech, until now, there are no common datasets. We introduce the TTS In the Wild (TITW) dataset, the result of a fully automated pipeline, in this case, applied to the VoxCeleb1 dataset commonly used for speaker recognition. We further propose two training sets. TITW-Hard is derived from the transcription, segmentation, and selection of VoxCeleb1 source data. TITW-Easy is derived from the additional application of enhancement and additional data selection based on DNSMOS. We show that a number of recent TTS models can be trained successfully using TITW-Easy, but that it remains extremely challenging to produce similar results using TITW-Hard. Both the dataset and protocols are publicly available and support the benchmarking of TTS systems trained using TITW data.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative speech technology is evolving rapidly, driven in part by advances in diffusion models, speech codecs, and speech-language modeling methodologies [1]-[5]. Numerous ever-more advanced Text-To-Speech (TTS) algorithms con-tinue to emerge. In contrast to state-of-the-art systems from only a few years ago, today's TTS systems can generate speech samples that are largely indistinguishable from real human speech in terms of both intelligibility and naturalness.\nWhile minutes of target speaker data, usually studio-quality collected in controlled conditions (e.g. anechoic rooms), was once necessary, today's systems often need only a few seconds of target speaker data [6]-[8]. Some systems even perform well when using clean, though not necessarily studio-quality speech. Nonetheless, TTS models themselves are almost ex-clusively trained using relatively clean speech data. The need for high-quality training data can hinder the training of TTS systems using more plentiful speech data collected in the wild, implying that synthetic speech might lack the characteristics of natural, spontaneous speech [9]. The lack of high-quality data in sufficient quantities is also a barrier to the development of TTS systems for some under-resourced languages.\nLow-quality data was found to be challenging for training the legacy hidden Markov model-based TTS systems [10], [11]. The recent literature shows a more thrust to train TTS models using lower-quality data, even data collected in the wild (called noisy TTS in some cases) [12]-[16]. Results from the fifth edition of the ASVspoof challenge [17] confirm the potential to generate high-quality synthetic speech when TTS systems are trained using non-studio-quality data sourced from a database of audiobooks [18]. Even if there is scarce evidence that TTS models can be trained using even lower quality training data [19], most attempts have been made using proprietary rather than publicly available data resources or data where artificial noise was added.\nIn this paper, we introduce a new public data resource to support research in this direction the Text-To-Speech Synthesis In The Wild (TITW) database. Our goal is to overcome data constraints and to promote the research and development of TTS systems which can be trained with data collected in the wild, which can synthesize speech with a more natural and spontaneous style. The TITW database is constructed using the VoxCelebl source database [20], a large collection of speech data collected from celebrity speakers and extracted from videos posted to social media.\nWe report the use of a fully automated pipeline involving au-tomatic transcription, segmentation, and speech enhancement, which largely mitigates the need for any manual processing. We describe its use to generate two TTS training sets, each involving different levels of automatic processing and speech quality. TITW-Hard is the result of automatic transcription, segmentation, and data selection based on heuristically-defined rules. TITW-Easy is a subset derived from TITW-Hard using additional speech enhancement and data selection. We also propose common protocols for the evaluation and bench-marking of TTS solutions built using the TITW database. Last, we report the successful training of contemporary TTS models using the TITW dataset. The full data and protocols are available at https://jungjee.github.io/titw under a Creative Commons Attribution 4.0 International License."}, {"title": "II. RELATED WORKS", "content": "Numerous databases have been used for the training of TTS models. Legacy databases such as CMU ARCTIC [21] and VCTK [22] were very carefully designed and curated. They contain phonetically-balanced utterances, all recorded in highly controlled benign acoustic environments. Because of the high cost of recording, these and similar databases"}, {"title": "III. TITW", "content": "For multiple reasons, we selected VoxCeleb1 [20], speech data from entirely 1, 251 speakers, as source data for the TITW database. First, VoxCeleb1 is itself sourced from the wild, and it spans diverse acoustic environments. Second, as a dataset for automatic speaker recognition, each utterance contains data collected from a single speaker. Last, its use will help facilitate future research in speech deepfake detection and spoofing-robust automatic speaker verification, especially important to safeguard the speech generation technology from malicious usage.\n\nA. Transcription and segmentation\nTranscription. Since TTS training typically requires paired speech and text data, we generated transcriptions for the entire VoxCeleb1 corpus. Given the large number of utterances and both the time and cost involved in generating manual transcriptions, we generated transcription automatically using pretrained automatic speech recognition (ASR) models. We used the WhisperX [30] toolkit to generate transcriptions with word-level timestamps. WhisperX incorporates the OpenAI Whisper Large v2 model [31] for transcription and a phoneme-based another ASR model for word-level alignment. We addi-tionally employ the OSWMv3 speech foundation model [32] and transcribe the data in parallel. Transcriptions driven with the OWSMv3 model are used for the manual inspection of transcription accuracy.\nSegmentation. We adopt the default WhisperX pipeline to divide each sample into isolated segments via voice activity detection (VAD), and transcribe each segment separately. Practically, whenever a non-speech persists longer than 500ms,"}, {"title": "B. Data selection", "content": "An initial round of data selection is then applied using four heuristically defined rules, the result of months of iterative efforts to train TTS models with selected data. If any of the following conditions are met, the data is removed and discarded from further consideration.\n\nThe language is not English. To simplify TTS training, we use Whisper's language recognition capability to detect and remove utterances in languages other than English. Multilingual extensions are left for future work.\nThe segment duration is shorter than 1 s or longer than 8 s. Like many others, we found empirically that the use of utterances of such a semi-consistent duration to be beneficial to training stability.\nThe per-word duration is longer than 500 ms. The typical speaking rate is in the order of 2 words per second. Outliers often correspond to emotional or pathological speech, or long intervals of non-speech, all of which can destabilise TTS training and are hence removed.\nThe automatic transcription is empty. Such cases indicate a non-speech segment or ASR failure. In either case, they cannot be used for TTS training and are removed.\nThe application of transcription, segmentation and data selection results in the TITW-Hard database. Since the raw data is collected from videos posted to social media, utterances in the TITW-Hard database still contain background noise or low-quality speech. Preliminary experiments revealed that the training of TTS models using TITW-Hard data is extremely challenging; most attempts failed to converge."}, {"title": "C. Enhancement and DNSMOS-based further data selection", "content": "Given the difficulty of training TTS models using the TITW-Hard database, we produced a second, relatively less challenging database named TITW-Easy. First, we apply a pre-trained speech enhancement model, DEMUCS [33]4 to attenuate additive, background noise. We then apply a second round of data selection, this time to the enhanced data. This is done by estimating DNSMOS [34], [35] scores for each utterance and then by removing all utterances for which the DNSMOS 'BAK' score is below a threshold of 3.0 except when the segment belongs to speakers whose data is included in the generation protocol (Section IV)."}, {"title": "IV. EVALUATION AND BENCHMARKING", "content": "Once a TTS model is trained using the TITW database, it can be used with one of two protocols to generate new, synthetic speech.\nTITW-KSKT (Known Speaker, Known Text) can be used to generate synthetic speech for speakers and text which are both seen in TITW-Hard and TITW-Easy datasets. The sets of speakers and text are both extracted arbitrarily from those contained in the VoxCeleb1-O automatic speaker verification evaluation protocol. The number of speakers is hence the same, 40, as that for the VoxCeleb1-O evaluation protocol. However, due to the data preparation processes described in Section III, the number of segments is dissimilar (9, 113, not 4, 708).\nTITW-KSUT (Known Speaker, Unknown Text) can be used to generate synthetic speech using text which is unseen in either the TITW-Hard or TITW-Easy datasets. We use two sources of text. The first is the Rainbow Passage [36] which covers many English sounds and their combinations and which has been used widely in other data collection efforts, for example the VCTK corpus [22]. The second is a set of Semantically Unpredictable Sentences (SUS) [37] selected from past Blizzard challenges [38]. There are totally 200 different text samples (31 from The Rainbow Passage and"}, {"title": "V. EXPERIMENTS", "content": "We now describe the automated evaluation pipeline which can be used to benchmark competing TTS models.\n\nA. Metrics\nWe adopt four metrics to assess the quality of generated synthetic speech: Mel Cepstral Distortion [39] (MCD); UT-MOS [40]; DNSMOS [35]; the ASR WER. We use all four metrics as different proxies for speech quality. The WER is evaluated using the OpenAI Whisper-Large model [41]. In practice, we use the VERSA toolkit to compute all four metrics [42].\n\nB. TTS training data\nAs a reference, we first assessed the two TITW datasets among others commonly used for TTS training. Results shown in Table II show that the TITW-Easy dataset is of greater quality than the TITW-Hard dataset. As expected, the quality of data in both TITW datasets is less than that used typically for TTS training. By way of example, the DNOSMOS for the VCTK database [43] is 3.20. That for the MLS database [18] is 3.33 while that for EMILIA database [29] is 3.22.\n\nC. Baseline TTS systems\nWe also show results of four different TTS systems all trained using TITW datasets: (i) TransformerTTS [44] with ParallelWaveGAN [45]; (ii) GradTTS-DiffWave [46], [47]; (iii) VITS [48]; (iv) MQTTS [19]. These four systems provide a reasonably diverse mix of contemporary TTS systems. Table III shows results for the four TTS models trained using the TITW-Easy dataset and reports the results on the two protocols. Results show that all four systems are successfully trained, yet suffer from the challenging nature of the TITW data. UTMOS and DNSMOS of the synthesized speech remain"}, {"title": "VI. CONCLUSION AND REMARKS", "content": "We introduce TITW, a new dataset for the training, eval-uation and benchmarking of TTS systems using data col-lected in the wild. The dataset was designed in response to current trends in TTS research which show a drive to develop resources and techniques to train TTS systems without studio-quality data. We devised a fully automated processing pipeline and, while it is readily extendible to a larger scale, we describe its application to the VoxCelebl database. We show that four contemporary TTS systems trained using the TITW-Easy dataset produce synthetic speech of quality close to that of the training data itself and not far from the quality of some studio-quality training databases. Nonetheless, this is achieved only with the application of speech enhancement and data selection. Use of the original data sourced from videos posted to social media remains challenging, even if results show potential.\nResults from two years of work (not reported in this paper) show that only recent deep-learning-based TTS systems can be trained using the TITW database; our attempts to train legacy TTS systems were largely unsuccessful. Training is also sensitive to data preparation which might explain why attempts to train TTS systems with data collected in the wild remain sparse in the literature. We hope that the release of the TITW database into the public domain will fuel greater interest and research effort in the future, that it might help others to develop TTS systems for under-resourced languages for which studio-quality training data is lacking, and that it might also contribute to the development of systems with the capacity to generate more natural and expressive synthetic speech more typical of that encountered in the wild."}]}