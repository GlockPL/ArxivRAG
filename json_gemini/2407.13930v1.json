{"title": "RT-Pose: A 4D Radar Tensor-based 3D Human\nPose Estimation and Localization Benchmark", "authors": ["Yuan-Hao Ho", "Jen-Hao Cheng", "Sheng Yao Kuan", "Zhongyu Jiang", "Wenhao Chai", "Hsiang-Wei Huang", "Chih-Lung Lin", "Jenq-Neng Hwang"], "abstract": "Traditional methods for human localization and pose esti-\nmation (HPE), which mainly rely on RGB images as an input modal-\nity, confront substantial limitations in real-world applications due to\nprivacy concerns. In contrast, radar-based HPE methods emerge as a\npromising alternative, characterized by distinctive attributes such as\nthrough-wall recognition and privacy-preserving, rendering the method\nmore conducive to practical deployments. This paper presents a Radar\nTensor-based human pose (RT-Pose) dataset and an open-source bench-\nmarking framework. RT-Pose dataset comprises 4D radar tensors, Li-\nDAR point clouds, and RGB images, and is collected for a total of 72k\nframes across 240 sequences with six different complexity level actions.\nThe 4D radar tensor provides raw spatio-temporal information, differ-\nentiating it from other radar point cloud-based datasets. We develop an\nannotation process, which uses RGB images and LiDAR point clouds to\naccurately label 3D human skeletons. In addition, we propose HRRadar-\nPose, the first single-stage architecture that extracts the high-resolution\nrepresentation of 4D radar tensors in 3D space to aid human keypoint\nestimation. HRRadarPose outperforms previous radar-based HPE work\non the RT-Pose benchmark. The overall HRRadar Pose performance on\nthe RT-Pose dataset, as reflected in a mean per joint position error\n(MPJPE) of 9.91cm, indicates the persistent challenges in achieving\naccurate HPE in complex real-world scenarios. RT-Pose is available at\nhttps://huggingface.co/datasets/uwipl/RT-Pose.", "sections": [{"title": "1 Introduction", "content": "Human localization and 3D pose estimation [6, 20-22, 31, 49, 53, 59] are indis-\npensable in Augmented/Virtual Reality [5,9,15,30], human-computer interac-\ntion [18,51], healthcare [3,8,29]. However, capturing human poses across diverse\nscenarios and generating comprehensive 3D representations for individuals in\nvarying poses present significant challenges. Camera based motion capture sys-\ntems are often used to collect human body posture [19, 28, 42]. However, these\nsystems are sensitive to light and require careful multi-camera calibration, mak-\ning them unreliable for outdoor scenarios and unable to provide overall pose\nestimation [7]. Furthermore, due to privacy concerns, deploying its application\nin home or long-term care center scenarios poses challenges.\nRadar-based method emerges as a promising alternative, characterized by\ndistinctive attributes such as through-wall recognition [54,58]. In addition, radar\nis robust to lighting conditions and resilient to various weather conditions and\nocclusion [1,47]. These characteristics make it especially suitable for safety- and\nprivacy-critical applications [2,39]. In smart automotive applications, radar, as a\ncomplementary sensing modality, can complement adverse scenarios such as low-\nlighting environments and adverse weather [25], challenging RGB-based sensing.\nIn healthcare applications, RGB-based Human Pose Estimation (HPE)poses pri-\nvacy risks and is hindered by occlusion, prompting a demand for radar in indoor\ncare environments. Considering the advantages and prospects of radar technol-\nogy, it is indispensable to provide a dataset that contains divwerse scenarios for\nfostering radar-based HPE research.\nThe previous radar-based HPE methods [14, 37, 48] often utilize Constant\nFalse Alarm Rate (CFAR) techniques [13,45,46] to extract radar point clouds.\nHowever, the effectiveness of CFAR-based point cloud extraction can be eas-\nily influenced by variations in radar module types and hardware parameters.\nFurthermore, different human body reflections in radar signals across diverse\nenvironments may require specific adjustments to CFAR parameters, making it\nchallenging to generalize their applications. To address this issue, some works de-\ncompose the 4D radar tensor into vertical and horizontal directions as the input\nof the model to estimate human pose [26, 47, 50]. This approach computes the\nmagnitude of radar signals for both directions, which effectively reduces data loss\nduring the conversion to point cloud. However, a 4D radar tensor with Doppler\nvalues contains the velocity information, which is more informative than a de-\ncomposed radar tensor [10,34]. Therefore, this work focuses on exploring the use\nof raw 4D radar tensors for HPE and establishing a benchmark based on this\ndata format.\nThe presented radar tensor-based human pose (RT-Pose) dataset is the first\nbenchmark to integrate calibrated 4D radar tensors, RGB images, and LiDAR\npoint clouds data in the field of HPE. Releasing a multi-modality dataset marks a\nsignificant advancement, particularly in the domain of human behavior analyses.\nThere are a total of 72k frames in 240 sequences in the dataset. The actions are\norganized into sequences of increasing complexity, providing a realistic motion\ndistribution. All experiments are conducted in 5 environmental conditions in 8\nscenarios, which ensures the variety of scenarios for better comprehension of the\nmodel. Along with the release of this comprehensive RT-Pose dataset, we also\nbuild upon the High-Resolution Network (HRNet) [44] to propose HRRadarPose\nmodel, a robust baseline for 3D HPE utilizing 4D radar tensor data. Further-"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 3D Human Pose Estimation Datasets", "content": "As one of the fundamental tasks in the computer vision field, there are lots of\nworks introducing datasets and benchmarks for 3D HPE. Human3.6M [19] is the first large-scale dataset for 3D human sensing in lab\nenvironments, which contains 3.6-million frames of corresponding 2D and 3D\nhuman poses from mocap captured videos of 5 female and 6 male subjects.\nCompared to Human3.6M, MPI-INF-3DHP [32] is a more challenging 3D human\npose dataset captured in the wild. There are 8 subjects with 8 actions captured\nby 14 cameras covering a greater diversity of poses. 3DPW [41], the first one\nto include video footage taken from a moving phone camera, includes 60 video\nsequences. Recently, several works have explored the possibility of HPE from\nother modalities like radar and LiDAR. mmBody [7] consists of synchronized and\ncalibrated mmWave radar point clouds and RGB-D images in different scenes,\nas well as skeleton/mesh annotations for humans in the scenes.\nmRI [3], a multi-modal 3D HPE dataset using mmWave, RGB-D, and inertial\nsensors, generates a radar point cloud dataset based on the CFAR method. There\nare 12 rehabilitation exercises selected to evaluate radar application in home-\nbased health monitoring. HuPR [26] collects 235 sequences of data by calibrated\nradar and camera for 2D HPE in an indoor environment.\nAll open datasets for radar-based human pose estimation are collected within\na limited detection area, which contradicts the characteristics of radar applica-\ntions for tracking. Although detecting within a small area may ensure system\naccuracy, it limits the system's practical application. Furthermore, to the best\nof our knowledge, our dataset is the first to include radar, LiDAR, and RGB\nsequence information with 3D pose annotations. This allows our dataset to pro-\nvide pose annotations for both indoor and outdoor environments, distinguishing\nit from existing datasets."}, {"title": "2.2 Radar-based Human Pose Estimation", "content": "In spite of the popularity of RGB-based HPE applications, privacy concerns\nhave driven the exploration of radio or radar modalities for HPE. RF-Pose [54]\nis a pioneering application in 2D HPE utilizing radio frequency signals. RF-\nPose demonstrates the feasibility of pose detection in through-wall scenarios\nand provide the possibility of predicting the 3D pose using radio signals.\nOn the other hand, frequency-modulated continuous wave (FMCW) radar,\nwhich operates in the millimeter-wave band (30-300GHz), has been shown to\nbe suitable for detection and HPE tasks. The radar point cloud is widely used\nto represent the radar signal in HPE [3,4,7,36,37, 48, 52]. mmMesh [48] intro-\nduces a temporal model structure that concurrently captures global and local 3D\npoint cloud structures in spatial dimensions, ensuring precise localization per-\nformance for pose estimation. Nonetheless, in cluttered environments, the prob-\nability and intensity of radar signal reflections from the human body decrease.\nConsequently, implementing point cloud-based methods in real-world applica-\ntions becomes challenging.\nCompared to the radar point cloud methods, utilizing 4D tensor radar signal\nproves to be more informative and reliable [26, 47, 55, 56]. RF-pose 3D [55] is\nthe first work using the 4D RF tensor to predict 3D skeletons in 22 different\nlocations. However, the used RF system is not commercially available, which is\nhard to reproduce. HuPR [26] utilizes two pieces of well-calibrated TI mmWave\nradar modules to capture vertical and horizontal radar signals, enhancing the\nresolution of radar elevation signals to improve pose estimation performance.\nRPM2.0 [47] adopts a similar hardware setup to HuPR but utilizes the HR-\nNet [44] as the backbone and includes an attention model to reconstruct miss-\ning keypoints. However, their two-stage model is trained by only walking pose,\nrestricting the actions' complexity for achieving better pose estimation perfor-\nmance. The methods proposed in HuPR and RPM2.0 fuse information from both"}, {"title": "3 RT-Pose Dataset", "content": "To construct the dataset, we recruit\n10 participants to perform 6 types\nof actions. The experiments are di-\nvided into two categories. In the first\ncategory, participants are required to\nperform actions while standing, in-\ncluding waving, lifting legs, and ran-\ndom poses to increase the complex-\nity of the actions. A video sequence of\nrandom poses involves a subject ran-\ndomly performing one possible move-\nment at a time, including stretching,\nbending, twisting, etc., each lasting 3\nto 5 seconds. The second category fo-\ncuses on walking, with additional ac-\ntions added during the process, such\nas walking and waving or sitting down\nafter walking. Each type of action is inherited from previous radar-based datasets\n[4,35,48]. Each sequence contains one action for 30 seconds, providing a variety\nof different difficulty levels. As a result, people can thoroughly evaluate their\n3D HPE methods on a broad distribution of actions with diverse difficulties.\nFurthermore, we provide the dataset development kit to facilitate the usage and\nevaluation of future methods."}, {"title": "3.1 Sensors", "content": "The data collection hardware system comprises two RGB cameras, a non-repetitive\nhorizontal scanning LiDAR, and a cascade imaging radar module. To extract human pose information from radar signals, the radar mod-\nule's parameters have been specifically configured. The radar operates at ten\nframes per second in this work. The radar module is equipped with 12 transmit\n(TX) and 16 receive (RX) antenna elements and operates in a multiple-input\nmultiple-output (MIMO) mode [23]. This radar module provides the azimuth\nand elevation angle resolutions of 1.4 degrees and 18 degrees, respectively. Fur-\nthermore, this work implements a high-frequency slope with a long ramp time\nto efficiently utilize the hardware's sweep bandwidth, ranging from 77 GHz to\nalmost 81 GHz. The longer sweep bandwidth of a chirp increases the radar range\nresolution [24,37], which is 4.5 cm in this work. Each frame consists of 64 chirps\nused for calculating radar Doppler signals, providing a velocity resolution of 3.9\ncm/s. These carefully chosen configurations ensure the system's accuracy and\nreliability in capturing human poses."}, {"title": "3.2 Data Collection", "content": "We collect the dataset in 40 scenes with indoor and outdoor environments.\nure 3 shows some of the collected data. Indoor environments include clean and\ncluttered conditions, while outdoor ones include normal, rainy weather, and low-\nlighting conditions. Each experimental condition includes 8 scenarios, which are\nnonocclusive, and three occlusive conditions (cardboard, cloth, and plastic pad).\nEach nonocclusive and occlusive condition is recorded in single- and multi-person\nsettings. To consider the different fields of view (FoV) of all sensors, the sensor\nsuite is situated 109 cm above ground, and an appropriate detection area is\nplanned based on different scenarios."}, {"title": "3.3 Data Processing", "content": "The captured radar signal, referred to as 4D tensor data, is processed by a series\nof steps [16,33,45]. On the transmitter side, the frequency\nof the transmitted signals, or chirps, periodically increases based on the set fre-\nquency slope. Consequently, the received signal exhibits a different frequency\nthan the transmitted signal. The frequency difference between transmitted and\nreceived signals can be used to estimate the range of the object reflecting the\nsignal to the radar sensor by the Fast Fourier Transform (FFT). At the begin-\nning of the data processing, each radar frame consists of 64 chirps, resulting\nin 64 range estimation results. Utilizing this range information, the velocity of\nthe responding object can be measured through FFT, a phenomenon known as\nthe Doppler effect. Second, the radar signal data is re-modulated according to\nthe antenna position. This step is essential for the calculation of the angle of ar-\nrival (AoA), enabling the generation of highangular resolution results. Third, the"}, {"title": "3.4 Annotation Workflow", "content": "To achieve accurate human subject detection in a larger area with high-quality\nground truth, our system integrates LiDAR and RGB camera data. The images provided by the camera are processed using the HR-\nNet [38] to extract 2D human poses. Then, a 3D HPE model, called ZeDO [22],\nutilizes the 2D poses to estimate the 3D poses as the initial 3D pseudo ground\ntruth. However, estimating 3D poses using a monocular camera often lacks pre-\ncision and stability in depth. Therefore, this work incorporates LiDAR for depth\nestimation of the human subject center. The LiDAR sensor captures environ-\nmental information and the subject's point cloud data, ensuring errors are under\n2 cm inside the detection range of 20 m. The human subject is manually labeled\nwith a 3D bounding box, which can provide an accurate 3D position.\nZeDO, a state-of-the-art (SOTA) method for cross-dataset HPE, implements\nan optimization-based pipeline for 3D pose estimation using 2D poses. This\nmethod requires 2D poses as input, and ZeDO is able to estimate multiple-\nhypothesis 3D poses. In this research, the center of the 3D bounding box pro-\nvided by LiDAR is utilized as the pelvis of the initial pose, which enhances the\naccuracy of depth estimation compared to relying solely on the monocular cam-\nera data, and improves the performance of ZeDO. With the imported 2D pose\nfrom HRNet [43], ZeDO iteratively refines the 3D poses. To further improve\nannotation accuracy, an annotation tool is developed for manual selection and\noptimization of each frame to retain correct 3D human poses. The 3D HPE re-\nsults from ZeDO will be put in the LiDAR point cloud for visualization, which\nis beneficial for humans to filter and modify the error pose annotation results.\nAbout 30% of the data is removed or optimized after human correction, and this\nprocess ensures the quality of keypoints annotation. More details can be found\nin the supplementary material."}, {"title": "4 HRRadarPose", "content": "Inspired by HRNet [43], renowned for its adeptness in extracting high-resolution\nrepresentations, we build an architecture, HRRadarPose, with fully 3D convolu-\ntional layers. By treating the Doppler axis, D, as the input channels, our model\nextracts volumetric spatial features along the Z, Y, and X axes, representing ver-\ntical, horizontal, and depth directions in the Cartesian coordinate system. The\nHRRadarPose architecture is designed to preserve spatial resolution and assimi-"}, {"title": "5 Experiments", "content": "15 obvious keypoints of a human body are selected, aligning with the body skele-\nton model of the Human3.6M dataset [19]. Each frame's 3D pose annotation in\nthe sequence is manually validated to ensure the quality of the dataset. ure 7 demonstrates the testing results across various levels of action complexity,\npresenting the feasibility of 3D pose estimation using the proposed baseline.\nEvaluation metrics We employ joint position error (JPE), which measures the\nmean of the Euclidean distance for every joint's predicted keypoints and the\nground truth keypoints. Localization results are reported by the mean of the\nroot position error (MRPE) [42], which is the JPE of the pelvis keypoint. For\nthe evaluation of 3D HPE, we use Mean per joint position error (MPJPE) [57]\nand absolute-MPJPE (Abs-MPJPE) [40].\nBaselines Comparison We evaluate the performance of the HRRadar Pose in\nglobal localization and 3D HPE on the RT-Pose dataset. We compare our method\nwith three baseline methods, as shown in Table 2. mm-Pose [37] and mmMesh\n[48], utilizing radar point clouds for 3D HPE, are re-implemented to test on\nthe RT-Pose dataset. Notably, mmMesh introduces a global localization model,\ncombined with an RNN HPE module, showing more accurate estimation results\nin both MRPE and MPJPE compared to mm-Pose, which employs a simpler\nCNN model. RF-Pose 3D [55] uses the 4D radar tensor with a region proposal\nnetwork to locate the subjects and estimate the 3D pose. The experimental\nresults show that our HRRadarPose outperforms other baseline methods. On\nthe RT-Pose dataset, we obtain 9.93 cm in MPJPE and 9.91 cm in MRPE. We\nfind that radar-tensor-based methods reach less error than the methods using\nradar point cloud. We attribute this performance gap to complicated scenarios\nsince it is difficult for point cloud-based methods to discriminate different key"}, {"title": "6 Limitation and Future Work", "content": "First, to leverage the input data types for training the 3D HPE model, the\ncomputational resource is a crucial part. 4D radar tensor in Cartesian coordinate\nconsumes up to 100 MB per frame, which decreases the training speed. Second,\nwe limit the experimental data collection scope from 2m to 8 m because the\ncamera is hard to clearly capture 2D pose skeletons at a large distance and\nLiDAR can not easily collect completed point clouds in near vision due to the\nconstraints of FoV. Third, the proposed baseline HRRadarPose lacks robustness\nin accurately estimating poses during complex activities. Capturing human pose\naccurately on 4D radar tensor remains an unresolved challenge."}, {"title": "7 Conclusion", "content": "In this paper, we propose RT-Pose, the first dataset for human pose estima-\ntion (HPE) with synchronized and calibrated 4D radar tensors, LiDAR point\nclouds, and RGB images. RT-Pose provides cluttered scenarios and human ac-\ntivities of different complexity levels to enhance the difficulty of pose estimation\nfor practical applications. In addition, having various modalities is beneficial\nfor our semi-automatic 3D pose annotation optimization process. The proposed\nHRRadarPose is the first single-stage architecture designed for estimating hu-\nman pose using 4D radar tensors as the input. The results indicate that the pro-\nposed HRradarPose outperforms previous works, in which datasets are collected\nwith simpler actions and cleaner scenarios. Our work contributes to offering a\nthree-modality dataset with 4D radar tensors for HPE, which is promising to be\napplied for complex actions and different scenes. We hope this work encourages\nfuture development of 4D radar-based HPE methods."}]}