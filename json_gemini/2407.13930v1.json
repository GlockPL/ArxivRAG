{"title": "RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark", "authors": ["Yuan-Hao Hol", "Jen-Hao Cheng", "Sheng Yao Kuan", "Zhongyu Jiang", "Wenhao Chai", "Hsiang-Wei Huang", "Chih-Lung Lin", "Jenq-Neng Hwang"], "abstract": "Traditional methods for human localization and pose estimation (HPE), which mainly rely on RGB images as an input modality, confront substantial limitations in real-world applications due to privacy concerns. In contrast, radar-based HPE methods emerge as a promising alternative, characterized by distinctive attributes such as through-wall recognition and privacy-preserving, rendering the method more conducive to practical deployments. This paper presents a Radar Tensor-based human pose (RT-Pose) dataset and an open-source benchmarking framework. RT-Pose dataset comprises 4D radar tensors, LiDAR point clouds, and RGB images, and is collected for a total of 72k frames across 240 sequences with six different complexity level actions. The 4D radar tensor provides raw spatio-temporal information, differentiating it from other radar point cloud-based datasets. We develop an annotation process, which uses RGB images and LiDAR point clouds to accurately label 3D human skeletons. In addition, we propose HRRadar-Pose, the first single-stage architecture that extracts the high-resolution representation of 4D radar tensors in 3D space to aid human keypoint estimation. HRRadarPose outperforms previous radar-based HPE work on the RT-Pose benchmark. The overall HRRadar Pose performance on the RT-Pose dataset, as reflected in a mean per joint position error (MPJPE) of 9.91cm, indicates the persistent challenges in achieving accurate HPE in complex real-world scenarios. RT-Pose is available at https://huggingface.co/datasets/uwipl/RT-Pose.", "sections": [{"title": "1 Introduction", "content": "Human localization and 3D pose estimation [6, 20-22, 31, 49, 53, 59] are indispensable in Augmented/Virtual Reality [5,9,15,30], human-computer interaction [18,51], healthcare [3,8,29]. However, capturing human poses across diverse scenarios and generating comprehensive 3D representations for individuals in varying poses present significant challenges. Camera based motion capture systems are often used to collect human body posture [19, 28, 42]. However, these systems are sensitive to light and require careful multi-camera calibration, making them unreliable for outdoor scenarios and unable to provide overall pose estimation [7]. Furthermore, due to privacy concerns, deploying its application in home or long-term care center scenarios poses challenges.\nRadar-based method emerges as a promising alternative, characterized by distinctive attributes such as through-wall recognition [54,58]. In addition, radar is robust to lighting conditions and resilient to various weather conditions and occlusion [1,47]. These characteristics make it especially suitable for safety- and privacy-critical applications [2,39]. In smart automotive applications, radar, as a complementary sensing modality, can complement adverse scenarios such as low-lighting environments and adverse weather [25], challenging RGB-based sensing. In healthcare applications, RGB-based Human Pose Estimation (HPE)poses privacy risks and is hindered by occlusion, prompting a demand for radar in indoor care environments. Considering the advantages and prospects of radar technology, it is indispensable to provide a dataset that contains divwerse scenarios for fostering radar-based HPE research.\nThe previous radar-based HPE methods [14, 37, 48] often utilize Constant False Alarm Rate (CFAR) techniques [13,45,46] to extract radar point clouds. However, the effectiveness of CFAR-based point cloud extraction can be easily influenced by variations in radar module types and hardware parameters. Furthermore, different human body reflections in radar signals across diverse environments may require specific adjustments to CFAR parameters, making it challenging to generalize their applications. To address this issue, some works decompose the 4D radar tensor into vertical and horizontal directions as the input of the model to estimate human pose [26, 47, 50]. This approach computes the magnitude of radar signals for both directions, which effectively reduces data loss during the conversion to point cloud. However, a 4D radar tensor with Doppler values contains the velocity information, which is more informative than a decomposed radar tensor [10,34]. Therefore, this work focuses on exploring the use of raw 4D radar tensors for HPE and establishing a benchmark based on this data format.\nThe presented radar tensor-based human pose (RT-Pose) dataset is the first benchmark to integrate calibrated 4D radar tensors, RGB images, and LiDAR point clouds data in the field of HPE. Releasing a multi-modality dataset marks a significant advancement, particularly in the domain of human behavior analyses. There are a total of 72k frames in 240 sequences in the dataset. The actions are organized into sequences of increasing complexity, providing a realistic motion distribution. All experiments are conducted in 5 environmental conditions in 8 scenarios, which ensures the variety of scenarios for better comprehension of the model. Along with the release of this comprehensive RT-Pose dataset, we also build upon the High-Resolution Network (HRNet) [44] to propose HRRadarPose model, a robust baseline for 3D HPE utilizing 4D radar tensor data. Further-"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 3D Human Pose Estimation Datasets", "content": "As one of the fundamental tasks in the computer vision field, there are lots of works introducing datasets and benchmarks for 3D HPE, as shown in Table 1. Human3.6M [19] is the first large-scale dataset for 3D human sensing in lab environments, which contains 3.6-million frames of corresponding 2D and 3D human poses from mocap captured videos of 5 female and 6 male subjects. Compared to Human3.6M, MPI-INF-3DHP [32] is a more challenging 3D human pose dataset captured in the wild. There are 8 subjects with 8 actions captured by 14 cameras covering a greater diversity of poses. 3DPW [41], the first one to include video footage taken from a moving phone camera, includes 60 video sequences. Recently, several works have explored the possibility of HPE from"}, {"title": "2.2 Radar-based Human Pose Estimation", "content": "In spite of the popularity of RGB-based HPE applications, privacy concerns have driven the exploration of radio or radar modalities for HPE. RF-Pose [54] is a pioneering application in 2D HPE utilizing radio frequency signals. RFPose demonstrates the feasibility of pose detection in through-wall scenarios and provide the possibility of predicting the 3D pose using radio signals.\nOn the other hand, frequency-modulated continuous wave (FMCW) radar, which operates in the millimeter-wave band (30-300GHz), has been shown to be suitable for detection and HPE tasks. The radar point cloud is widely used to represent the radar signal in HPE [3,4,7,36,37, 48, 52]. mmMesh [48] introduces a temporal model structure that concurrently captures global and local 3D point cloud structures in spatial dimensions, ensuring precise localization performance for pose estimation. Nonetheless, in cluttered environments, the probability and intensity of radar signal reflections from the human body decrease. Consequently, implementing point cloud-based methods in real-world applications becomes challenging.\nCompared to the radar point cloud methods, utilizing 4D tensor radar signal proves to be more informative and reliable [26, 47, 55, 56]. RF-pose 3D [55] is the first work using the 4D RF tensor to predict 3D skeletons in 22 different locations. However, the used RF system is not commercially available, which is hard to reproduce. HuPR [26] utilizes two pieces of well-calibrated TI mmWave radar modules to capture vertical and horizontal radar signals, enhancing the resolution of radar elevation signals to improve pose estimation performance. RPM2.0 [47] adopts a similar hardware setup to HuPR but utilizes the HRNet [44] as the backbone and includes an attention model to reconstruct missing keypoints. However, their two-stage model is trained by only walking pose, restricting the actions' complexity for achieving better pose estimation performance. The methods proposed in HuPR and RPM2.0 fuse information from both"}, {"title": "3 RT-Pose Dataset", "content": "To construct the dataset, we recruit 10 participants to perform 6 types of actions. The experiments are divided into two categories. In the first category, participants are required to perform actions while standing, including waving, lifting legs, and random poses to increase the complexity of the actions. A video sequence of random poses involves a subject randomly performing one possible movement at a time, including stretching, bending, twisting, etc., each lasting 3 to 5 seconds. The second category focuses on walking, with additional actions added during the process, such as walking and waving or sitting down after walking. Each type of action is inherited from previous radar-based datasets [4,35,48]. Each sequence contains one action for 30 seconds, providing a variety of different difficulty levels. As a result, people can thoroughly evaluate their 3D HPE methods on a broad distribution of actions with diverse difficulties. Furthermore, we provide the dataset development kit to facilitate the usage and evaluation of future methods."}, {"title": "3.1 Sensors", "content": "The data collection hardware system comprises two RGB cameras, a non-repetitive horizontal scanning LiDAR, and a cascade imaging radar module, as shown in Figure 1. To extract human pose information from radar signals, the radar module's parameters have been specifically configured. The radar operates at ten frames per second in this work. The radar module is equipped with 12 transmit (TX) and 16 receive (RX) antenna elements and operates in a multiple-input multiple-output (MIMO) mode [23]. This radar module provides the azimuth and elevation angle resolutions of 1.4 degrees and 18 degrees, respectively. Furthermore, this work implements a high-frequency slope with a long ramp time to efficiently utilize the hardware's sweep bandwidth, ranging from 77 GHz to almost 81 GHz. The longer sweep bandwidth of a chirp increases the radar range resolution [24,37], which is 4.5 cm in this work. Each frame consists of 64 chirps used for calculating radar Doppler signals, providing a velocity resolution of 3.9 cm/s. These carefully chosen configurations ensure the system's accuracy and reliability in capturing human poses."}, {"title": "3.2 Data Collection", "content": "We collect the dataset in 40 scenes with indoor and outdoor environments. Figure 3 shows some of the collected data. Indoor environments include clean and cluttered conditions, while outdoor ones include normal, rainy weather, and low-lighting conditions. Each experimental condition includes 8 scenarios, which are nonocclusive, and three occlusive conditions (cardboard, cloth, and plastic pad). Each nonocclusive and occlusive condition is recorded in single- and multi-person settings. To consider the different fields of view (FoV) of all sensors, the sensor suite is situated 109 cm above ground, and an appropriate detection area is planned based on different scenarios."}, {"title": "3.3 Data Processing", "content": "The captured radar signal, referred to as 4D tensor data, is processed by a series of steps [16,33,45], as shown in Figure 4. On the transmitter side, the frequency of the transmitted signals, or chirps, periodically increases based on the set frequency slope. Consequently, the received signal exhibits a different frequency than the transmitted signal. The frequency difference between transmitted and received signals can be used to estimate the range of the object reflecting the signal to the radar sensor by the Fast Fourier Transform (FFT). At the beginning of the data processing, each radar frame consists of 64 chirps, resulting in 64 range estimation results. Utilizing this range information, the velocity of the responding object can be measured through FFT, a phenomenon known as the Doppler effect. Second, the radar signal data is re-modulated according to the antenna position. This step is essential for the calculation of the angle of arrival (AoA), enabling the generation of highangular resolution results. Third, the"}, {"title": "3.4 Annotation Workflow", "content": "To achieve accurate human subject detection in a larger area with high-quality ground truth, our system integrates LiDAR and RGB camera data, as illustrated in Figure 5. The images provided by the camera are processed using the HR-Net [38] to extract 2D human poses. Then, a 3D HPE model, called ZeDO [22], utilizes the 2D poses to estimate the 3D poses as the initial 3D pseudo ground truth. However, estimating 3D poses using a monocular camera often lacks precision and stability in depth. Therefore, this work incorporates LiDAR for depth estimation of the human subject center. The LiDAR sensor captures environmental information and the subject's point cloud data, ensuring errors are under 2 cm inside the detection range of 20 m. The human subject is manually labeled with a 3D bounding box, which can provide an accurate 3D position.\nZeDO, a state-of-the-art (SOTA) method for cross-dataset HPE, implements an optimization-based pipeline for 3D pose estimation using 2D poses. This method requires 2D poses as input, and ZeDO is able to estimate multiplehypothesis 3D poses. In this research, the center of the 3D bounding box provided by LiDAR is utilized as the pelvis of the initial pose, which enhances the accuracy of depth estimation compared to relying solely on the monocular camera data, and improves the performance of ZeDO. With the imported 2D pose from HRNet [43], ZeDO iteratively refines the 3D poses. To further improve annotation accuracy, an annotation tool is developed for manual selection and optimization of each frame to retain correct 3D human poses. The 3D HPE results from ZeDO will be put in the LiDAR point cloud for visualization, which is beneficial for humans to filter and modify the error pose annotation results. About 30% of the data is removed or optimized after human correction, and this process ensures the quality of keypoints annotation. More details can be found in the supplementary material."}, {"title": "4 HRRadarPose", "content": "Inspired by HRNet [43], renowned for its adeptness in extracting high-resolution representations, we build an architecture, HRRadarPose, with fully 3D convolutional layers. By treating the Doppler axis, D, as the input channels, our model extracts volumetric spatial features along the Z, Y, and X axes, representing vertical, horizontal, and depth directions in the Cartesian coordinate system. The HRRadarPose architecture is designed to preserve spatial resolution and assimilate semantic information from the 4D radar tensors, maintaining high-resolution representations and enabling the exchange of information between features of different resolutions during stage transitions. Figure 6 illustrates an instance of the HRRadarPose structure with three stages. The structure starts with using 3D convolutional layers to transform the 4D radar tensors, $T \\in \\mathbb{R}^{D \\times Z \\times Y \\times X}$, into features, $f_{in} \\in \\mathbb{R}^{C_{1} \\times Z_{1} \\times Y_{1} \\times X_{1}}$. As the network proceeds from one stage to the next, it develops additional branches through strided convolution to widen the receptive field. Each branch i employs strided convolution to elevate its input feature $f_{i}$ to a feature $f_{i+1}$ in the succeeding branch i + 1, simultaneously doubling the channels and halving the spatial dimensions, $f_{i+1} \\in \\mathbb{R}^{2C_{i} \\times \\frac{Z_{i}}{2} \\times \\frac{Y_{i}}{2} \\times \\frac{X_{i}}{2}}$.\nWithin each stage, a series of P modules execute M parallel convolutions. To align feature dimensions from different branches for information exchange, we use strided convolution followed by upsampling. At the end of the backbone, we take the feature with the highest resolution to serve as a unified representation for the pose estimation head. In the pose estimation head, two 3D convolutional branches are employed to process these features further. This bifurcation generates two outputs: a distribution map signifying the confidence of locating the human center and the location offset of joint keypoints. The right part of Figure 6 shows the decoding of the HRRadarPose network's output to multi-person poses. The process starts with picking top-k human centers, $C_{s}$, with confidence scores over a threshold value. We denote these picked centers' indices as S. Then, we query the centers corresponding joint keypoints' offsets, $K_{s}$, by looking up the output map of keypoint offsets head at S. Lastly, we decode their joint keypoint locations, $J_{s}$, by summing $K_{s}$ and $C_{s}$. In scenarios with multiple people, we apply range non-maximum suppression (NMS) to reduce overlapping detections. Our method is favorable in two folds: The HRRadarPose's architecture gains efficiency through a single-stage workflow, avoiding complexity brought by region-proposal-based methods [55]. In addition, our pose estimation head inherently learns per person's joint keypoints refraining from ambiguity of association between people's identity and joints.\nTo train the center probability head, we use the pixel-wise focal loss as the classification loss, $L_{class}$, defined as:\n$L_{class} =\n\\begin{cases}\n(1 - P_{xyz})^{\\alpha} log(P_{xyz}), & \\text{if } Y_{xyz} = 1, \\\\\n(1 - Y_{xyz})^{\\beta} (P_{xyz})^{\\alpha} log(1 - P_{xyz}), & \\text{otherwise,}\n\\end{cases}$\n\nwhere $Y_{xyz}$ is the three-dimensional human center Gaussian distribution map generated by pelvis locations, $P_{xyz}$ is the probability of a person's presence at location, $(x, y, z)$, N is the number of foreground samples, $\\alpha$ and $\\beta$ are hyperparameters addressing class imbalance and focusing training on foreground samples. To train the keypoint offset head, we construct the regression loss $L_{reg}$ by taking the average of 15 keypoints' $L_{1}$, comparing the distance between predicted offsets and ground-truth offsets to the center location of a voxel in Y when $Y_{xyz}$ equals to 1. The final loss is a weighted sum of $L_{class}$ and $L_{reg}$."}, {"title": "5 Experiments", "content": "15 obvious keypoints of a human body are selected, aligning with the body skeleton model of the Human3.6M dataset [19]. Each frame's 3D pose annotation in the sequence is manually validated to ensure the quality of the dataset. Figure 7 demonstrates the testing results across various levels of action complexity, presenting the feasibility of 3D pose estimation using the proposed baseline.\nEvaluation metrics We employ joint position error (JPE), which measures the mean of the Euclidean distance for every joint's predicted keypoints and the ground truth keypoints. Localization results are reported by the mean of the root position error (MRPE) [42], which is the JPE of the pelvis keypoint. For the evaluation of 3D HPE, we use Mean per joint position error (MPJPE) [57] and absolute-MPJPE (Abs-MPJPE) [40].\nBaselines Comparison We evaluate the performance of the HRRadarPose in global localization and 3D HPE on the RT-Pose dataset. We compare our method with three baseline methods, as shown in Table 2. mm-Pose [37] and mmMesh [48], utilizing radar point clouds for 3D HPE, are re-implemented to test on the RT-Pose dataset. Notably, mmMesh introduces a global localization model, combined with an RNN HPE module, showing more accurate estimation results in both MRPE and MPJPE compared to mm-Pose, which employs a simpler CNN model. RF-Pose 3D [55] uses the 4D radar tensor with a region proposal network to locate the subjects and estimate the 3D pose. The experimental results show that our HRRadarPose outperforms other baseline methods. On the RT-Pose dataset, we obtain 9.93 cm in MPJPE and 9.91 cm in MRPE. We find that radar-tensor-based methods reach less error than the methods using radar point cloud. We attribute this performance gap to complicated scenarios since it is difficult for point cloud-based methods to discriminate different key"}, {"title": "Action complexity Analysis", "content": "RT-pose dataset provides six difficulty levels of actions to simulate real-world scenarios, enabling comprehensive evaluation of localization and 3D HPE results. We report the performance of the proposed HRRadarPose method in Table 3. Generally, the joints along the human longitudinal axis, such as the thorax and the head, are more stable than those in the limbs. This phenomenon may be attributed to the larger reflective surface of the body's trunk compared to the limbs, providing a more reliable HPE based on radar signals.\nThe MRPE of standing posture is below 7.67 cm, indicating that the pose complexity slightly influences the localization accuracy. Due to the chair's strong radar reflection, distinguishing between human subjects and the chair poses a challenge for localization. The MPJPE of walking and sitting is 9.44 cm, showing our HRRadarPose model can reliably estimate the sitting pose. However, the MRPE of this action sequence achieves 14.11 cm, demonstrating that sitting is a key factor for evaluating the performance of localization. The MPJPE of simple actions, such as standing and waving, standing and lifting legs, are around 7 cm, and the walking is 8.93 cm, demonstrating that our HRRadarPose model is comparable to other recently published radar pose estimate methods [26, 47]. However, the current model still encounters difficulty in estimating HPE in complex actions such as walking and waving. Therefore, the RT-pose dataset is a challenging benchmark, which can enable HPE models to realistically learn diverse pose scenarios."}, {"title": "5.1 Study", "content": "To illustrate the advantage of modeling motion features using Doppler information by our HRRadar Pose, we remove Doppler information by averaging 4D radar tensors along the Doppler axis and train the models with and without Doppler information using the same setup. Table 5 shows that with the inclusion of Doppler information, our HRRadarPose enjoys a performance gain due to its capability to extract motion features, enhancing keypoint localization.\nCompared with RPM 2.0, our input data and backbone design are different. The proposed HRRadarPose network extracts better feature representation from 4D radar tensors through modified convolutional blocks. As shown in Figure 9, we replace the batch norm with the group norm of convolutional blocks in HRNet. Since we treat the channel-wise features as a Doppler axis, the group norm inherently handles the feature in different speed groups, which normalizes the data effectively. Table 6 shows the results, proving that the modified CNN block is more suitable for Doppler radar than the original HRNet. Compared with our method, most previous work did not leverage Doppler information [47,50]. We validate that motion features [17,27] are crucial for aiding deep learning models in capturing more human body characteristics over the environment.\nTo ablate our design choice of HPE heads, we use the same backbone, replace our pose estimation head with RF-Pose3D's [55] head, and train both models for"}, {"title": "6 Limitation and Future Work", "content": "First, to leverage the input data types for training the 3D HPE model, the computational resource is a crucial part. 4D radar tensor in Cartesian coordinate consumes up to 100 MB per frame, which decreases the training speed. Second, we limit the experimental data collection scope from 2m to 8 m because the camera is hard to clearly capture 2D pose skeletons at a large distance and LiDAR can not easily collect completed point clouds in near vision due to the constraints of FoV. Third, the proposed baseline HRRadarPose lacks robustness in accurately estimating poses during complex activities. Capturing human pose accurately on 4D radar tensor remains an unresolved challenge."}, {"title": "7 Conclusion", "content": "In this paper, we propose RT-Pose, the first dataset for human pose estimation (HPE) with synchronized and calibrated 4D radar tensors, LiDAR point clouds, and RGB images. RT-Pose provides cluttered scenarios and human activities of different complexity levels to enhance the difficulty of pose estimation for practical applications. In addition, having various modalities is beneficial for our semi-automatic 3D pose annotation optimization process. The proposed HRRadarPose is the first single-stage architecture designed for estimating human pose using 4D radar tensors as the input. The results indicate that the proposed HRradarPose outperforms previous works, in which datasets are collected with simpler actions and cleaner scenarios. Our work contributes to offering a three-modality dataset with 4D radar tensors for HPE, which is promising to be applied for complex actions and different scenes. We hope this work encourages future development of 4D radar-based HPE methods."}]}