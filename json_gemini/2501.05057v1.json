{"title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models", "authors": ["Zengqi Peng", "Yubin Wang", "Xu Han", "Lei Zheng", "Jun Ma"], "abstract": "Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning work-flow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advancement of artificial intelligence (AI) tech- nologies, significant breakthroughs have been made in gener- ative models, with large language models (LLMs) being one of the most prominent applications [1], [2]. LLMs excel in under- standing and generating text, while their integration with ad- ditional specialized modules enables multimodal capabilities, such as processing and generating images and videos. This inherent versatility demonstrates the immense potential for application across various fields [3]. Meanwhile, autonomous driving technology has made remarkable progress, becoming a focal point of research in AI and transportation [3], [4], [5]. In general, urban driving scenarios are characterized by diverse road structures and task types, such as multi-lane overtaking, on-ramp merging, and intersection crossing. These driving scenarios demand frequent interactions with surrounding ve- hicles (SVs) exhibiting varying driving styles. The diversity of driving environments and the uncertainty of SV behaviors collectively present significant challenges to achieving a safe and efficient closed-loop urban driving system [6], [7]. Conse- quently, autonomous driving systems are required to enhance the ability to prevent potential collision risks and optimize task efficiency across different scenarios. From this perspective, urban autonomous driving requires robust interaction-aware decision-making and planning capabilities to safely interact with SVs while efficiently accomplishing various driving tasks. Reinforcement learning (RL) has demonstrated significant potential for autonomous driving solutions. It enables policies to optimize decision-making through interactions with the environment and the feedback received from these interactions [4], [8], [9], [10]. Despite the significant progress of RL in the autonomous driving community, it still encounters two major challenges. The first challenge lies in the design of reward functions, which RL relies on to guide agents in exploring the environment and improving policies. However, in real-world tasks like autonomous driving, reward signals are often sparse, and this significantly hinders efficient policy learning [11]. Reward shaping is a common approach to provide incremental learning signals to mitigate this issue [12]. Nevertheless, designing an appropriate reward function for complex autonomous driving tasks remains highly challenging. Traditional manual design methods are not only constrained by the subjective experience of the designer but also time-consuming and tedious [13], [14], [15]. Due to the distinct characteristics of different tasks, the reward functions that are capable of effectively guiding RL policies to learn satisfactory behaviors could vary significantly. Furthermore, the reward function is typically fixed at the beginning of the policy training and cannot be adapted in real-time during the training process. Therefore, the manual design of reward functions often fails to encapsulate the nuanced behaviors needed and provide effective guidance for RL agents in dynamic urban driving scenarios, leading to poor policy convergence. The second challenge is the sample efficiency during online explo- ration. The complexities of urban driving tasks involve diverse"}, {"title": "II. RELATED WORK", "content": "The reward function plays a critical role in guiding RL agents as they interact with the environment. A high-quality reward function can significantly enhance the performance of RL policies. Therefore, the design of reward functions, often referred to as reward engineering, plays a vital role in RL. Designing suitable reward functions for real-world tasks, such as autonomous driving and robotics, poses challenges due to the sparsity of rewards over long time horizons and the balance between exploration and exploitation. In existing studies, manual design is the most commonly used routine for constructing reward functions, followed by minor adjustments through trial and error [10], [20], [21]. This approach not only makes the effectiveness of the reward function dependent on expert experience but also renders the entire training process time-consuming and labor-intensive. Furthermore, when deal- ing with complex multi-task problems, relying solely on expert experience often fails to yield a satisfactory reward function. Multi-task RL techniques are introduced by sharing policies or designing task-specific policies [22], [23], [24], which still do not consider the reward design problems.\nTo address the challenges arising from reward design, inverse RL (IRL) is utilized to extract reward functions from collected data by observing the agent behaviors of interaction with the environment [25], [26]. Specifically, the deep IRL methods are adopted to infer the driving behaviors by distilling the learned reward model from collected expert demonstrations to score and evaluate trajectories in autonomous driving tasks [27], [28]. However, due to the non-uniqueness of reward functions, the inferred reward function could not fully capture the true motivations of the agent. On the other hand, IRL relies on a large amount of expert data, and collecting high-quality data in complex and dynamic environments is challenging, if not impractical. This reward inference method typically involves complex optimization processes, resulting in heavy computational burdens, long training time, and limitations due to environmental modeling. Additionally, evolutionary algorithms have been introduced for reward shaping to evolve reward functions [12], [29]. With the advancement of foun- dation model technologies, the design of reward functions for RL tasks can be accomplished by providing relevant prompts to LLMs [30], [31], which offers a promising solution to solve the challenges of reward design in RL."}, {"title": "B. Training Efficiency for Deep Reinforcement Learning", "content": "RL policies improve performance by interacting with the environment to collect experience. However, low sample ef- ficiency remains a significant challenge in applying RL to complex tasks. Directly employing random policies to gather sample information in intricate environments is highly in-efficient, which can significantly prolong the time required for policy convergence or even lead to failure in achieving convergence. To tackle this issue, an environment model is introduced to generate virtual samples to improve the sample efficiency, thereby accelerating the training process [32]. How- ever, the effectiveness of the trained policy highly depends on the accuracy of the model.\nCL offers a promising solution to mitigate the above chal- lenges [17], [33]. A stage-decaying CL approach is utilized to guide the policy learning of the RL agents [34]. Never- theless, the predetermined manual scheduling of curriculum transitions heavily relies on expert knowledge, which limits the robustness and effectiveness of the training outcomes. To tackle the aforementioned challenges, various automated CL methods have been proposed [35], [36]. However, these approaches are designed under the assumption that SVs do not respond to the behaviors of the ego vehicle (EV). Be- sides, the future trajectories of SVs are accessible to the EV."}, {"title": "C. Large Language Model Applications", "content": "With the rapid advancement of LLMs, their potential in various tasks has attracted significant attention [3]. Currently, LLMs are primarily utilized in two ways. The first one is the LLMs-in-the-task-loop solution, which utilizes LLMs for embodied inference. The second one is the LLMs-in-the- training-loop solution, which incorporates LLMs for policy learning.\n1) LLMs for Embodied Inference: One of the most direct ways to apply LLMs technology to autonomous driving tasks is by embedding the LLMs as a decision-making or planning module within the autonomous driving system. Depending on the embedding method, there are primarily two approaches, serial LLMs solutions and parallel LLMs solutions [37], [38], [39]. For example, DiLu achieves decision-making based on common-sense knowledge by integrating an LLMs-based inference module into the autonomous driving system [37]. However, this sequential structure limits the response speed of the autonomous driving system due to the inference latency of LLMs, posing challenges to meeting real-time requirements. To address this issue, DriveVLM-Dual connects a traditional end-to-end pipeline with an LLMs-based inference system in a parallel manner, forming a fast-slow system that alleviates the real-time performance challenges of LLMs-based autonomous driving solutions [40].\n2) LLMs for Policy Learning: In addition to directly de- ploying LLMs within autonomous driving systems, LLMs are also applied during the training phase [31], [41], [42], [43]. Recent research has explored the use of LLMs for designing reward functions in reinforcement learning and assisting in CL for task decomposition. A general RL training framework has been proposed to generate proxy reward functions by lever- aging the in-context learning capabilities and prior knowledge of LLMs [30]. Auto MC-Reward enhances learning efficiency by automatically designing dense reward functions through the introduction of three automated reward modules [31]. Eureka is proposed as an LLM-driven human-level reward generation method for sequential decision-making tasks [14], [44]. CurricuLLM is proposed to decompose complex robotic skills into a sequence of subtasks, facilitating the learning of intricate robotic control tasks [45]. However, the entire training curriculum sequence is predetermined before training and the timing of curriculum transitions is neglected, which potentially hinders the sample efficiency. AutoReward leverages LLMs and CoT-based modules to achieve closed-loop reward gen- eration for autonomous driving [46]. However, this approach requires access to the internal code of the simulation envi- ronment, which could result in the leakage of environmental"}, {"title": "III. PROBLEM FORMULATION", "content": "This study aims to tackle the challenge of developing safe, effective, and interaction-aware driving policies for various urban driving scenarios, such as multi-lane overtaking and on- ramp merging. The initial and target positions of both the EV and SVs are randomly generated, while ensuring adherence to traffic regulations. The number of SVs is also random, with interactive behaviors driven by different driving styles. In this context, the EV is required to complete various driving tasks within an environment that contains interactive SVs. Since different driving tasks have distinct characteristics and requirements, this presents a significant challenge in designing appropriate reward functions and suitable training curricula. The goal is to automatically generate reward functions and training curricula to train RL policies that infer decision sequences, guiding the EV to safely and efficiently complete driving tasks across different scenarios. Here, we assume that the EV can access the exact position and velocity information of SVs accurately, yet their goal tasks and driving intentions are unknown. These configurations inject a significant level of randomness into the driving scenarios, rendering the tasks challenging but close to real-world situations."}, {"title": "B. Learning Environment", "content": "In this work, the target tasks are formulated as a Markov Decision Process (MDP). Here, we represent the MDP as a tuple E = (S, A, P, R, \u03b3), with each element defined as follows:\nState space S: In this work, S includes kinematic features of driving vehicles within the observation range of the EV. The state matrix at time step k is defined as shown below:\n$S_k = [s_k^0 \\ S_k^1 \\ ... \\ S_k^{N_{max}^{SV}}]^T$,\nwhere $N_{max}^{SV}$ indicates the maximum number of SVs observed by the EV; $s_k^0$ and $s_k^i$ (i = 1, 2, ..., $N_{max}^{SV}$) denote the state of the EV and the state of the i-th SV, respectively. In particular, $s_k^i$ is defined as follows:\n$s_k^i = [x_k^i, y_k^i, v_k^i, \\psi_k^i]^T$,\nwhere $x^i, y^i, v^i, \u03c8^i$ are the X-axis and Y-axis coordinates, the speed, and the heading angle of the i-th vehicle, respectively.\nAction space A: In this work, a multi-discrete action space consisting of three discrete sub-action spaces is utilized for the RL agent:\nA = {A1, A2, A3},\nwhere A1, A2, and A3 denote the waypoint, reference velocity, and lane change sub-action spaces, respectively. Further details will be provided in Section IV-E.\nState transition dynamics P(Sk+1|Sk, ak): P describes the transitions of the environmental state while satisfying the Markov property. It is implicitly determined by the external environment and remains inaccessible to the RL agent.\nReward function R: The reward function plays a crucial role in RL. It reinforces the correct actions of the agent by providing rewards and penalizes incorrect actions, guid- ing the exploration of the agent within the environment. A well-designed reward function can significantly enhance the efficiency and performance of the training process. However, designing rewards manually for complex tasks remains chal- lenging. In this work, we leverage the extensive knowledge base of LLMs to design and iteratively refine efficient reward functions for RL agents.\nDiscount factor \u03b3: \u03b3\u2208 (0,1) is utilized to discount future accumulated rewards."}, {"title": "C. Curriculum Sequence Generation Problem", "content": "Under the problem defined in Section III.A, we establish the following two-layer curriculum set. The first layer con- siders the traffic densities, while the second layer takes into account the motion modes of SVs. Specifically, the designed curriculum set consists of Nta subsets, each comprising Nmm elements. This two-layer curriculum set can be expressed as:\n$\\Omega = {\\Omega_{ij} | i = 0, 1, ..., N_{max}^{mm}, j = 0,1,..., N_{max}^{mm} },$\nwhere $N_{max}^{mm}$ and $N_{max}^{mm}$ denote the number of traffic density types and motion modes of SVs, respectively. In CRL, a sequence of training curricula is required to set up the envi- ronment for optimizing the RL policy, which can be expressed as follows:\n$\\{m\\} = \\underset{\\{m\\}}{\\arg \\max} \\mathbb{E}R,$\nwhere {m} is the optimal curriculum sequence, and R is the reward of the RL agent. In this study, the curriculum sequence is formulated through the use of LLMs, which are employed for their robust generative abilities to guide the learning process."}, {"title": "IV. METHODOLOGY", "content": "The detailed architecture of the proposed LearningFlow is illustrated in Fig. 2. First, general knowledge prompts related to autonomous driving tasks are generated separately to the curriculum analysis agent and reward analysis agent to analyze training tasks. Then the generated key points on curriculum generation and reward generation, along with general knowl- edge prompts for CL, reward generation, and code generation, are input into the curriculum generation agent and reward gen- eration agent to facilitate training course selection and reward generation. Subsequently, the relevant training curricula and reward function codes are extracted from responses of the LLM to initialize the RL agent and interactive environment. Finally, the downstream RL executor explores and learns within the environment and reward functions designed by the LLM agents. The responses from LLM agents, along with training history, are recorded in a memory module; and after a specified number of episodes, they are fed back to the LLM agents to update the training curricula and reward functions. In this work, the proposed LearningFlow framework con- sists of a reasoning module, a reflection module, and a memory module for curriculum generation and reward generation. By providing appropriate prompts to the proposed framework, the system is able to perform automated policy learning for various autonomous driving tasks. Details will be discussed in the following sections."}, {"title": "B. Memory Module for Closed-Loop Policy Training Workflow", "content": "LLMs inherently lack persistent memory capabilities, mean- ing that they cannot retain interaction information from pre- vious sessions when processing new queries. Consequently, LLMs are unable to recall past exchanges with the user during subsequent reasoning tasks. Therefore, a memory module is introduced to store historical information from the training process to enable closed-loop online tuning workflow through- out the training process. This includes inference results from various LLM agents, generated training curricula, designed reward functions, recorded total reward and the individual reward components during training, and training metrics. The extracted information is stored in textual or vectorized form via an extraction module and integrated into prompts for the next session through a reflection module. By incorporating the memory module, LLMs can retain and leverage cross-session historical training data, enabling closed-loop reasoning and au- tomated training adjustments, thereby significantly enhancing the efficiency and adaptability of policy learning."}, {"title": "C. Iterative Curriculum Sequence Generation", "content": "1) Context Descriptor: Given the generality of LLMs, it is necessary to provide relevant contextual information to help them understand task objectives. In this study, we use contextual descriptors to describe the current training task in natural language comprehensively. Considering the complexity of the task, the description includes characteristics of the driving scenario and objectives of policy learning. For the curriculum generation LLM agent, a carefully designed two- layer curriculum set and curriculum descriptions are included as part of the context. The generated description is then integrated into the system prompts of different agents to facilitate task-specific reasoning of curriculum analysis and generation.\n2) Curriculum Analysis Agent: Our target scenarios are complex and challenging, involving not only SVs with diverse driving styles but also diverse traffic densities. To enhance sample efficiency, the designed two-layer curriculum set incor- porates not only these factors but also subsets of curricula with different motion modalities of SVs. The unquantifiable nature of driving difficulty across different scenarios poses challenges to traditional curriculum selection algorithms and heightens the requirements for the curriculum generation agent.\nDirectly generating training curricula could result in LLMs producing low-quality outputs from the LLMs, such as inap- propriate course transitions or the omission of critical curric- ula. To address this issue, we introduce a curriculum analysis agent. In the prompt for the curriculum analysis agent, we provide the foundational knowledge for CL, along with the structure of the two-layer curriculum set (4) and the design objectives for each layer. Specifically, the curriculum set is designed to guide the policy in progressively exploring the en- vironment. It starts with simpler tasks and gradually transitions to more complex ones, thereby enhancing sample efficiency. Before generating training curricula, this agent incrementally analyzes the two-layer curriculum set by considering the current training context. It leverages contextual information from context descriptors, curriculum set information, historical curriculum sequence, current training progress, and historical training data of the RL policy. Based on this analysis, the agent identifies critical aspects to consider in the setup of the next round of CRL training and provides a rationale for the curriculum generation agent to make informed inferences.\n3) Curriculum Generation Agent: After the curriculum analysis agent completes its reasoning, the analyzed results are passed to the curriculum generation agent as a reference. Specifically, the prompt generator utilizes the definition and analysis of the curriculum set, principles of CL, and output format prompts as textual context for training curriculum generation. It then directs the LLMs to perform reasoning and return the inferred results, which include the selected training curriculum. Then the selected curriculum is decoded from the response of the LLMs. Through the designed prompts and the collaboration with the curriculum analysis agent, the curriculum generation agent can effectively select appropriate curriculum tasks during the initialization phase and throughout the training process.\nGiven the complexity of environmental rules and state tran- sitions, as well as the potential suboptimality in the selections generated by LLMs, we incorporate an e-greedy strategy, com- monly used to address the exploration-exploitation dilemma in RL [13], to configure the ultimate training environment. This strategy helps the RL agent better understand environmental features and adapt to environmental changes. The specific e- curriculum selection strategy is defined as follows:\n$C_k = \\begin{cases}C_{LLM}, \\text{with probability } 1 - \\epsilon, \\\\C_{random}, \\text{with probability } \\epsilon,\\end{cases}$\nwhere Ct is the employed curriculum setting at k-th episode, CLLM, Crandom represent the curriculum selected by the agent and randomly, respectively; and e is a parameter that decays over the training process.\n4) Curriculum Evaluation and Reflector: In the initial phase of training, open-loop automatic curriculum generation is achieved through the collaboration of the analysis agent and the selection agent, which can provide RL agents in different training stages with suitable training environments. Meanwhile, interaction data generated during policy training is recorded in the memory module. To facilitate timely curricu- lum switching based on historical information, it is necessary to express in language whether the current curriculum is appro- priate for subsequent training. For this purpose, a curriculum reflection module is introduced to generate feedback for the curriculum analysis and selection modules, enabling a closed- loop automatic curriculum agent workflow. Specifically, the curriculum reflection agent summarizes the characteristics of the historical curriculum sequence, the trajectory of policy rewards, and task performance metrics. It then generates reflection prompts to guide the analysis and selection of the next curriculum phase.\nThe entire workflow of curriculum sequence generation can be expressed as follows:\n$A_0^c = L(P_A^c), \\\\C_0 = D_C(L(P_A^c, A_0^c)), \\\\A_{n+1}^c = L(C_{hist}, A_C^c, P_C^c, P_A^c), \\\\C_{n+1} = D_C(L(C_{hist}, A_C^c, P_C^c, P_{A_{n+1}}^c)), \\\\n = 0, 1, ..., N_{max}.$\nwhere L refers to the LLM; $A^c, C^c$ represent the analysis and selected curriculum for the n-th training interval; $C_{hist}$ is the historical curriculum sequence; $P_A^c, P_C^c$, and $P_F^c$ denote the analysis prompt, selection prompt, and feedback prompt for the curriculum sequence generation, respectively; Dc(.) is the decode function to extract the curriculum."}, {"title": "D. Iterative Reward Generation", "content": "1) Context Descriptor: Similar to curriculum agents, it is essential to provide appropriate contextual information to guide LLMs in understanding the objectives of reward generation. Here, we also employ contextual descriptors to express the requirements of reward generation tasks in natural language comprehensively. Considering the intricacy of the reward design process, the descriptors include both general principles of reward functions and task-specific goals, such as the characteristics of driving interactions and the expected outcomes of RL agent training. These descriptions are incor- porated into the system prompts of different agents to enable task-specific reasoning for reward analysis and generation.\n2) Reward Analysis Agent: In our study, the complexity of autonomous driving tasks arises not only from varying driving scenarios but also from differing traffic densities. This increases the demand for the design of reward functions. Gen- erating the reward function directly could lead to ineffective outputs from the LLMs.\nTo address this issue, we introduce a reward function analysis agent that analyzes the task before the reward function is designed. For standard RL tasks, manually crafted reward functions typically incorporate accessible environment state variables, action variables, and constants. Therefore, it is reasonable to provide code segments with annotated notes containing accessible environmental variables as context for the reward analysis agent. This approach prevents exposing the internal mechanisms of the environment and eliminates the need for explicit state transition dynamics. The LLM agent leverages contextual information from context descriptors, the code segment of accessible variables, the next training environment determined by the curriculum agents, the current training progress, and the feedback data. This agent offers the key considerations during the reward generation process and provides a reference value range and analytical basis for the reward components, thus supporting the inference process of the reward generation agent.\n3) Reward Generation Agent: After the reward analy- sis agent completes its reasoning, the analyzed results and reference value range are passed to the reward generation agent. Besides, accessible variables, reward function signature, principles of reward function construction, and output format specifications are also provided as textual context for designing reward functions. Specifically, the generation of the reward function is required to output both the total reward and the individual components of the designed sub-rewards. Given the above comprehensive information as context, a general reward generation prompt is crafted for input into the LLMs. Then the reward generation agent calls LLMs to perform reasoning to generate a response containing the designed reward function by utilizing its vast knowledge base and emergence ability. Subsequently, the executable code of the reward function is extracted from the response content and added to the environment program. Through carefully designed prompts and collaboration with the reward analysis agent, the reward generation agent can effectively construct appropriate reward functions throughout the training process.\n4) Reward Evaluation and Reflector: Through the collab- oration of multiple agents, an open-loop reward generation process is implemented to provide RL agents with an initial reward function. Then the RL agent can explore the environ- ments with the guidance of the generated reward function and related data is collected for policy update and memory storage. To facilitate the timely refinement of reward functions based on historical data, it is essential to evaluate and articulate whether the current reward function aligns with the train- ing objectives. For this purpose, a reward reflection module is introduced to generate feedback prompts for the reward analysis and generation agents, forming a closed-loop reward generation process and online tuning workflow together. This reflection workflow generates reflection prompts to reshape and mutate the reward function based on feedback information and the previous reward function for subsequent training phases.\nTo enable effective closed-loop iteration, a comprehensive evaluation of the designed reward functions is required as a ba- sis for subsequent improvements. It includes information from the interactions of the RL agent, such as historical data about the reward functions and their components, success rates, collision rates, and timeout rates. According to these metrics, the reward agents can integrate information at different levels of granularity. This includes coarse-grained data on the entire reward function and success rates, as well as fine-grained details on individual reward components. These capabilities allow for more effective optimization of the reward function design and the proposal of more targeted reward components. A prompt regarding context-based reward improvements is then provided to analysis and generation agents to propose a new, enhanced generation of the reward function based on previously designed ones. This prompt includes descriptions of the closed-loop feedback and suggestions for improvements, such as adding or removing reward components or modifying the reward coefficients.\nThe entire workflow of reward generation and online tuning can be expressed as follows:\n$A_R = L(P_R), \\\\R_0 = D_R(L(P_R, A_R)), \\\\A_{n+1} = L(R_{hist}, A_R, P_R, P_R), \\\\R_{n+1} = D_R(L(R_{hist}, A_R, P_R, P_{A_{n+1}})), \\\\n = 0, 1, ..., N_{max}.$\nwhere An, Rn represent the analysis and design reward for the then-th training interval; Rhist is the historical reward function; PR, PR, and PR denote the analysis prompt, reward gen- eration prompt, and feedback prompt for the reward function, respectively; DR(\u00b7) is the decode function to extract the reward function code."}, {"title": "E. Downstream RL Executor", "content": "This work adopts an integrated decision-planning-control strategy using an RL policy as the downstream executor. Specifically, the RL policy generates decision variables based on observations, which are then used as the reference of the model predictive controller to calculate the control signal. The specific details are presented as follows. We use the notations of subscript k to represent k-th time step within an episode. The observations of the RL agent are expressed as follows:\n$O_k = [\\delta x_k^1 \\ ... \\delta x_k^{N_{max}^{obs}} \\delta y_k^1 \\ ... \\delta y_k^{N_{max}^{obs}}  \\delta v_k^0 \\delta v_k^1 \\ ... \\delta v_k^{N_{max}^{obs}} \\delta \\psi_k^0 \\delta \\psi_k^1 \\ ... \\delta \\psi_k^{N_{max}^{obs}}]^T,$\nwhere $N_{max}^{obs}$ is the maximum number of the observed SVs for the EV; $v^0$ denotes the current speed of the EV; \u03b4\u03b1\u03ba, \u03b4\u03c5\u03ba, \u03b4\u03c5\u03ba, and 8 represent the differences in the X-axis coordinate, Y-axis coordinate, speed, and heading angle, respectively, between the EV and the destination position (i = 0) as well as the i-th SV (i = 1, 2, ..., Nmax).\nHere, the RL policy is represented by a neural network \u03c0 parameterized by 0. Given the RL observation Ok at time step k, the action of RL agent is generated by:\n$a_k^{RL} = \\pi_{\\theta}(O_k)$\nThe specific definition of the sub-action spaces in (3) are introduced as follows. The waypoint sub-action space is defined as:\nA1 = {WP0, WP1, ..., WP4},\nwhere WP = [xWPyWP WP]T is the i-th waypoint, which includes the reference information about the X-axis, Y-axis coordinates, and heading angle of the waypoint, respectively. Waypoints are provided by a predefined road map and several path-searching methods, such as A* search algorithm. A reference waypoint set is generated at the beginning of the task. The 5 waypoints closest to the EV (WP, i = 0, 1, ..., 4) are added to the A1. The reference velocity sub-action space is defined as:\nA2 = {0, $\\frac{U_{limit}}{4}, \\frac{U_{limit}}{2}, \\frac{3U_{limit}}{4}, U_{limit}$},\nwhere Ulimit is speed limitation of the road. The lane change sub-action space is defined as:\nA3 = {-1,0,1},\nwhere -1,0, and 1 represent left lane change, lane keeping, and right lane change maneuvers, respectively.\nThe coordination of the above three sub-action spaces can enable flexible motion patterns for the EV to interact with SVs exhibiting diverse behaviors. When rapid movement toward the target is needed, the RL agent can select a distant waypoint and high speed; while it can choose the nearest waypoint and low speed for emergency braking. The lane- changing sub-action adds further flexibility. Selecting a closer waypoint on an adjacent lane indicates an urgent lane change, while a distant waypoint enables a smoother lane change for collision avoidance or overtaking. The reward function and the training environment are provided by the LLM agents. After the RL policy generates action outputs based on observations, these actions are decoded and passed to the model predictive controller, which converts them into execution trajectories and control commands to be applied to the EV.\nOnce the training curriculum and reward function are decoded from the answers of LLM agents, the RL agent explores the environment set by the configuration of the selected curriculum. Relevant historical training information is recorded in the replay buffer. Once a certain number of episodes have been gathered, the RL policy undergoes training to optimize the cumulative objective function that is associated with the sequence of generated training curricula (Co, C1, ..., CNmax+1) as follows:\n$\\theta^* = \\underset{\\theta}{\\arg \\max} J(\\theta),$\nwhere J(0) denotes the objective function for the RL policy with parameter 0."}, {"title": "V. EXPERIMENTS", "content": "In this section, we implement the automated policy learning workflow in two urban driving scenarios. The experiments are carried out on the Ubuntu 18.04 system with Intel(R) Core(TM) i9-14900K CPU and NVIDIA GeForce RTX 3090 GPU. All self-driving scenarios involved in experiments are constructed on the CARLA simulator [47]. Here, we select the Tesla Model 3 as the EV and SVs. To validate the capability"}, {"title": "C. Comparative Results and Analysis", "content": "To quantitatively compare the performance of LearningFlow with the baseline methods, we conduct statistical testing on analysis and content generation from the cooperative interac- tions of four LLM agents, as shown in Figs. 3-4. As a com- parison, a reward generation demonstration of LearningFlow without the analysis process is shown in Fig. 5.\nAs shown in Fig. 3, at the beginning of training, the curricu- lum analysis agent evaluates the upcoming training curriculum based on the training progress and the core principles of CL, considering factors such as initial curriculum settings, switching strategies, and switching criteria. Subsequently, the curriculum generation agent selects a suitable training curricu- lum from the curriculum set by leveraging the analyzed results and adhering to CL principles, generating the selection in a standard format. During the subsequent training process, the curriculum analysis and generation agents iteratively analyze and evaluate training curricula by incorporating the historical curriculum sequence and RL policy training data, enabling automatic curriculum switching and thereby improving the sample efficiency of policy learning.\nIn Fig. 4, during the early stages of training, the reward analysis agent begins by examining the current performance of the policy, considering various factors relevant to the au- tonomous driving task, and analyzing each reward component, its significance, and the reference value range that should be considered in the reward generation process. Next, the reward generation agent, based on this analysis, generates a reward function in a standardized format, adhering to the function's signature and coding conventions. The generated reward function is then extracted and embedded into the environment code for training. After a certain number of episodes, the reward analysis and generation agents adjust the reward components and coefficients based on the policy's performance, providing reasons for the adjustments along with the reference value range. Through the collaboration of the reward function agents, we achieve the automatic design and online adjustment of the reward function, thereby enhancing the performance of the policy.\nIn the failure case shown in Fig. 5, the reward genera- tion agent fails to generate an appropriate reward function. Specifically, the reward generation agent misinterprets the expected impact of traffic density N_sv on the reward func- tion, reversing the operator, which hinders the RL policy when exploring tasks with higher traffic density. Furthermore, without the support of analysis agents, the reward generation agent fails to accurately understand the contribution of the lane_change_times variable in the lane change penalty, which should be calculated in an event-triggered manner. Additionally, it also leads to difficulties in designing rea- sonable coefficient ranges. For instance, the coefficient and calculation method of the speed reward term could cause its accumulated value to exceed the reward for task completion, leading the agent to prioritize speed rewards at the expense of driving safety and encouraging reckless behavior. This demonstration highlights the crucial role of the analysis agent in the LearningFlow framework."}, {"title": "E. Adaptation to Different RL Algorithms", "content": "To evaluate the generalization of the proposed LearningFlow framework across different RL algorithms, we replace the downstream PPO algorithm with DQN and SAC algorithms and conducted training and testing on the multi-lane overtaking task. The test results are shown in Table II. As seen from the statistics in the table, LearningFlow effectively facilitates policy learning across different RL algorithms, improving the success rate of the trained policies in task scenarios. Compared to the RL policies trained with the PPO algorithm in Table I, the success rates for DQN and SAC are slightly lower. This can be attributed to the fact that the performance of the RL policies also depends on the configuration of the action space and state space. Specifically, DQN could not perform as well as PPO in handling continuous state spaces and multi-dimensional discrete action spaces. Meanwhile, SAC, which is designed for continuous action spaces, is inherently limited in tasks involving multi-dimensional discrete action spaces. Nevertheless, in general, the RL policies trained with the LearningFlow framework show enhanced performance compared to Vanilla RL algorithms. This demonstrates that the proposed approach for reward and curriculum generation is adaptable to various RL algorithms, and this showcases the generalization capability of the proposed framework."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose LearningFlow, a closed-loop automated policy learning workflow for autonomous driving, which utilizes the collaboration of LLM agents to generate and dynamically adjust the training curriculum and reward functions of the RL policy. By incorporating analysis and generation agents, our approach enhances the understanding of complex driving tasks, thereby improving the efficiency of automatic curriculum and reward generation, as well as the performance of RL policies. Experimental results demonstrate the effectiveness of the proposed framework. Compared to baseline and SOTA methods, the RL policy trained by Learn- ingFlow achieves the highest success rate. Furthermore, ab- lation studies validate the effectiveness of our policy"}]}