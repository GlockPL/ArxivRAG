{"title": "EEG-DCNET: A FAST AND ACCURATE MI-EEG DILATED CNN\nCLASSIFICATION METHOD", "authors": ["Wei Peng", "Kang Liu", "Jiaxi Shi", "Jianchen Hu"], "abstract": "The electroencephalography (EEG)-based motor imagery (MI) classification is a critical and chal-\nlenging task in brain-computer interface (BCI) technology, which plays a significant role in assisting\npatients with functional impairments to regain mobility. We present a novel multi-scale atrous\nconvolutional neural network (CNN) model called EEG-dilated convolution network (DCNet) to\nenhance the accuracy and efficiency of the EEG-based MI classification tasks. We incorporate the\n1 \u00d7 1 convolutional layer and utilize the multi-branch parallel atrous convolutional architecture in\nEEG-DCNet to capture the highly nonlinear characteristics and multi-scale features of the EEG\nsignals. Moreover, we utilize the sliding window to enhance the temporal consistency and utilize the\nattension mechanism to improve the accuracy of recognizing user intentions. The experimental results\n(via the BCI-IV-2a,BCI-IV-2b and the High-Gamma datasets) show that EEG-DCNet outperforms\nexisting state-of-the-art (SOTA) approaches in terms of classification accuracy and Kappa scores.\nFurthermore, since EEG-DCNet requires less number of parameters, the training efficiency and\nmemory consumption are also improved. The experiment code is open-sourced at here.", "sections": [{"title": "Introduction", "content": "Brain-computer interface (BCI), as one of the most well known technologies to enable individuals to control external\ndevices with their own limbs, has attracted lots of attentions. It can help patients with functional impairments regain\nmobility [1]. It is recognised that the most chanllenging task is to accurately identify the control signal from the brain\nof the patient via limbs. As an efficient signal obtained from non-invasive technique, EEG signal has been utilized in\nwidespread due to its high temporal resolution [2]. Obviously, precisely deciphering the relationship between EEG\nsignals and motor imagery (MI) is a crucial pathway for BCI [3]. Unfortunately, EEG signals usually suffer from\nlow signal-to-noise ratio, poor spatial resolution, and significant individual variability. Additionally, the underlying\nmechanisms are not fully understood, and there is a lack of effective feature extraction methods, posing significant\nchallenges for accurate classification [4].\nSince the EEG signals are collected from multiple in position electrodes on the scalp which can reflect the activity of\ndifferent brain regions, these signals usually present particular spatial characteristics. Thus, the deep learning method\nhas been applied in EEG signal classification problems due to its remarkble ability to learn complex spatial features\nfrom raw data [5]. In recent years, a great number of Convolutional Neural Network (CNN)-based methods have\nappeared to significantly improve the prediction accuracy and efficiency of the EEG signal classification [6, 7, 8, 9, 10].\nThe foundamental reason is that CNN is good at processing high-dimensional data with spatial local correlations,\nextracting features along the spatial dimensions through convolutional kernels to capture spatial relationships in EEG\nsignals [11]. Moreover, by using one-dimensional convolutions or adding temporal convolutional layers, CNNs can also\ncapture the temporal dependencies, e.g., [6] introduced a new three-dimensional EEG representation method, converting\nEEG signals into 3D arrays to retain spatial distribution information. On the other hand, the demanding real-time\nprocessing requirement in BCI systems is suitable for CNN model since most of the calculations can be performed\nin off-line parallel training stage [12]. Another particular feature in EEG signals is that they are often affected by the\nsubject's emotions and other physiological states, potentially introducing noise. Thus, the noise reduction procedure\nsuch as the pooling [13, 11] is necessary to avoid its influence on the feature extraction.\nAfter the preprocessing, the EEG signals can be converted into two-dimensional representations such as time-frequency\nmaps and power spectral density plots. Therefore, the image recognition techniques can be utilized in EEG signals\nclassification. The work of [6] designed a multi-branch 3D CNN architecture, including networks with different receptive\nfield sizes to extract features at different scales. They used a cropped training strategy to increase training samples and\nimprove the model's generalization ability. This method demonstrated excellent performance and robustness across\ndifferent subjects and significantly improved practicality even with only 9 sampling electrodes. The work of [14]\ndesigned a class-aware loss function to effectively handle data imbalance issues, introduced a multi-resolution CNN and\nan attention feature reweighting module to extract features from different frequency bands and enhance feature learning.\nDue to the temporal sequence characteristics of EEG signals, another alternative approach is to utilize the Recurrent\nNeural Networks (RNNs) and their variants Long Short-Term Memory (LSTM) networks [15, 16, 17, 18]. However,\nin current MI classification research, temporal sequence models are usually combined with other methods to jointly\nextract spatial and temporal features to improve the performance. In EEG signal classification, temporal networks\nare often combined with CNNs [18]. In [18], LSTM was used to construct a recurrent attention network to capture\ntemporal dependencies between different EEG time segments, allowing the model to focus on periods when subjects\nwere paying attention during the experiment while ignoring other periods. The work of [19] used the LSTM-RNN and\ncontinuous conditional random fields for automatic and continuous emotion detection. The study also explored the\ninterference effect of facial muscle activity on EEG signals and analyzed the supplementary information provided by\nEEG signals in the presence of facial expressions. In [20], LSTM and gate recurrent unit (GRU) were used to build\ndeep RNN architectures to extract spatial-frequency-sequential relationships in EEG signals, improving performance in\nMI classification.\nRecently, various attention mechanisms have been applied in EEG classification networks [7, 8, 14, 18, 21]. The\nattention mechanism was initially proposed to address the information bottleneck in sequence-to-sequence (Seq2Seq)\nmodels when processing long sequences [22]. Its core idea is to allow the model to selectively focus on different parts of\nthe input sequence when processing the current step, rather than relying on a fixed hidden state. Applying the attention\nmechanisms to EEG signal processing can enable the model to focus on important temporal segments and spatial\nlocations, improving the effectiveness of feature extraction by automatically ignoring irrelevant or noisy signal parts.\nIn [18], the attention mechanism was used to focus on features most relevant to MI tasks in EEG signals, combining\nattention-guided inception CNN and LSTM to adaptively focus on different spatial contexts and reduce overfitting issues\nthrough a two-layer attention mechanism. The work in [21] proposed a multi-scale attention-based fusion CNN, which\nextracts spatiotemporal multi-scale features of EEG signals through spatial multi-scale modules, attention modules,\ntemporal multi-scale modules, and dense fusion modules. The attention mechanism enhances the network's sensitivity\nto EEG signal features, allowing the network to selectively amplify valuable feature channels and suppress useless\nones. The work of [23] proposed an EEG-ConvTransformer network based on multi-head self-attention and temporal\nconvolution. This network combines self-attention modules and convolutional filters to capture inter-regional interaction\npatterns and temporal patterns within a single module. The attention mechanism is used to capture inter-regional\ninteraction patterns in EEG signals and learn the temporal patterns of the signals.\nThe work of [24] developed a new deep learning model that enhances adaptability and robustness across different\nindividuals by introducing the local reparameterization trick into CNNs. This method performs well in handling\ninter-individual variability but still faces certain challenges. The work of [25] proposed a novel dual attention relation\nnetwork, including temporal attention and aggregation attention modules, along with a fine-tuning strategy to adapt to the\ndata distribution of unseen subjects. The attention mechanism is used to improve classification accuracy of EEG signals\nfrom unseen subjects, especially in cases of scarce labeled data. Compared with existing few-shot learning methods, this\nmethod exhibits superior performance. The work of [8] proposed an Attention-based Temporal Convolutional Network\n(ATCNet), combining scientific machine learning (SciML) with attention mechanisms and temporal convolutional\nnetwork (TCN) architectures to improve the decoding performance of MI-EEG signals, representing one of the advanced\napproaches currently available.\nIn summary, current research on MI classification of EEG signals mainly builds upon CNN architectures, integrating\ntemporal neural network architectures and attention mechanisms to extract and utilize key features. We propose a new\nCNN-based model which can achieve the highest classification accuracy while mantaining a high training efficiency"}, {"title": "EEG-DCNet Architecture", "content": "In this section, we propose the EEG-DCNet, designed to enhance feature representation capabilities in EEG-based MI\ntasks. EEG-DCNet improves upon traditional linear two-dimensional (2D) convolutions and depthwise convolutions by\nincorporating 1 \u00d7 1 convolutional layers, which enhance the network's ability to capture nonlinear characteristics of the\ndata. Additionally, it introduces a multi-branch parallel atrous convolutional architecture to improve the perception of\nmulti-scale features. Specifically, the 1 \u00d7 1 convolutions are utilized to fuse outputs from atrous convolutions with\ndifferent dilation rates, facilitating effective integration of multi-scale information. Then, a sliding window mechanism\nis employed to process the time-series EEG data, and when combined with an attention mechanism, it enables the\nnetwork to capture continuous changes in EEG signals during the experiment, thereby enhancing temporal consistency\nand improving the accuracy in recognizing user intentions. The complete architecture of EEG-DCNet is depicted in\nFig. 1.\nThe input to EEG-DCNet consists of raw EEG signals with dimensions $C \u00d7 T$, where $C$ denotes the number of channels\nand $T$ denotes the number of time samples. The signals are expanded by one dimension to serve as the input channel\nfor convolutional operations. The output of the model is the predicted probability distribution over the four MI task\ncategories. As illustrated in Fig. 1, the EEG-DCNet architecture comprises three core modules: the convolutional (CV)\nmodule, the multi-branch parallel atrous convolution (SP) module, and the attention (AT) module.\n\u2022 CV module: By incorporating 1 \u00d7 1 convolutions and applying nonlinear activation functions like ELU after\neach convolution operation, the network overcomes the limitations of linear operations in traditional 2D and\ndepthwise convolutions, thereby enhancing its ability to capture the nonlinear characteristics inherent in EEG\ndata.\n\u2022 SP module: By incorporating multi-branch parallel atrous convolutions with varying dilation rates and fusing\ntheir outputs using a 1 \u00d7 1 convolution, the network effectively captures multi-scale information, enhancing its\nability to perceive complex features and ensuring that features extracted at different scales complement each\nother.\n\u2022 Sliding Window and AT modules: By implementing a sliding window strategy to segment the time series\ninto multiple local sub-sequences and integrating a channel attention mechanism (specifically, a Squeeze-\nand-Excitation block) within each window, the network increases training data, enhances temporal modeling\nprecision, and selectively emphasizes important channels, thereby improving feature representation and\nultimately boosting the accuracy of temporal predictions in recognizing user intentions.\nNext, the design ideas of each module will be introduced one by one according to the forward order of DCNet."}, {"title": "2.1 Data Preprocessing", "content": "We train and fine-tune the architecture and parameters of EEG-DCNet on the BCI-2a dataset and test the performance on\nBCI-2a, BCI-2b and HGD datasets. To preserve the integrity of the MI EEG signals, no preprocessing is applied to the"}, {"title": "2.2 Convolutional Block", "content": "The convolutional block consists of three convolutional layers, with a architecture similar to EEGNet. However,\nin contrast to EEGNet, EEG-DCNet introduces a 1 \u00d7 1 convolution layer in the depthwise convolution stage for\ndimensionality reduction. The detailed architecture is shown in Fig. 2 and the shape parameters of each layer are given\nin Table. 2.2.\nThe first layer applies $F_1$ filters of size $(1, K_C)$ to the input EEG signals, where $K_C$ is the temporal kernel length set\nto one-quarter of the sampling rate (for the BCI-2a dataset, $K_C = 64$). This layer performs temporal convolution along\nthe time axis to extract multi-band temporal features, capturing frequency-specific patterns essential for distinguishing\ndifferent MI tasks, and generates $F_1$ temporal feature maps representing the temporal dynamics of the EEG signals.\nThen, a 1 \u00d7 1 convolution with $F_2$ filters is applied to the output of the first layer. This operation increases the number of\nfeature maps, effectively expanding the feature dimensions and enhancing the network's capacity to represent complex\npatterns by allowing more combinations of features. As a result, it produces $F_2$ enhanced temporal feature maps with\nincreased depth.\nSubsequently, the network employs depthwise convolution using $F_2$ filters of size $(C, 1)$, where $C$ represents the\nnumber of EEG channels. This layer captures spatial features by convolving across the electrode channels, modeling\nspatial dependencies and interactions between different scalp regions. It results in $F_1 \u00d7 D$ feature maps, where $D$ is the\nnumber of depthwise filters per input channel (empirically set to 2), effectively learning spatial filters specific to each\ntemporal feature map.\nTo reduce the dimensionality of the feature maps and decrease computational complexity while retaining essential\ninformation, a 1 \u00d7 1 convolution layer is introduced after the depthwise convolution layer, producing a compact set\nof feature maps suitable for subsequent processing. A nonlinear activation function, such as the Exponential Linear\nUnit (ELU, the formulation is shown as (1)), is applied after each convolutional operation to introduce nonlinearity.\nThis allows the network to model complex, nonlinear relationships inherent in EEG data that linear operations cannot\ncapture, thereby enhancing the network's representational capacity and improving its ability to distinguish between\ndifferent EEG signal patterns associated with various MI tasks.\n$ELU(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ a(e^x - 1) & \\text{if } x \\leq 0 \\end{cases}$        (1)"}, {"title": "2.3 Multi-Branch Parallel Atrous Convolution Block", "content": "After the convolutional block, we incorporate a multi-branch parallel atrous convolution architecture to process the\nEEG feature maps at different scales. This architecture combines 1 \u00d7 1 convolutions and parallel atrous convolutions,\nwhere the dilation rate of each atrous convolution is customized to flexibly enhance the model's ability to capture\nmulti-scale features. Drawing inspiration from the atrous spatial pyramid pooling (ASPP) method, we design a parallel\natrous convolution layer that uses multi-scale receptive fields, processing and integrating features from different scales\nindependently across multiple branches. The architecture of SP block is shown as Fig. 3 and the shape parameters are\ngiven in Table 2.3.\nFor an atrous convolution with dilation rate r and kernel size k, the receptive field is calculated as\n$Receptive Field = 2 \u00d7 (r - 1) \u00d7 (k \u2212 1) + k$             (2)\nIn this work, the multi-branch parallel atrous convolution architecture consists of three branches, each with a different\ndilation rate $R = (2, 4, 6)$ and a kernel size of 8 \u00d7 1. The receptive fields of these branches are 17 \u00d7 1, 50 \u00d7 1, and\n78 \u00d7 1, respectively. By using this multi-branch design, the model can efficiently capture features at different scales,\nthereby improving its ability to represent complex EEG signals and providing a richer set of features for subsequent\nlayers. This approach significantly enhances the network's capability to process multi-scale information, which is\ncrucial for interpreting the diverse temporal patterns in EEG data.\nAs shown in the Fig 3, the input consists of raw feature maps, which are processed by atrous convolutions with different\ndilation rates in parallel branches. These features are then concatenated along the channel dimension. To reduce\nthe computational cost, a 1 \u00d7 1 convolution is applied before concatenation to reduce the number of channels, thus\noptimizing resource utilization."}, {"title": "2.4 Sliding Window and SE Attention Block", "content": "After processing the EEG signals through the CV and SP modules to capture frequency domain features, the sliding\nwindow and attention mechanism will be applied to extract the temporal domain information. The sliding window\nmethod helps expand the data volume and improve classification accuracy. We divide the time series into multiple\noverlapping windows of length Tw, and each window z is fed into the attention block (AT) for classification prediction.\nSubsequently, the SoftMax classifier calculates the probability for each class, and the results are passed to the fully\nconnected layer (FC), where the average prediction of all windows is computed to obtain the final classification output.\nThe architecture of SP block is shown as Fig. 4 and the shape parameters are given in Table 3.\nThe sliding window method enables effective capture of features over different time segments during a trial, rather than\nfocusing on a single time point, which is important in MI tasks where the specific time points may vary across trials\nand sessions. In BCI tasks, the user may not immediately respond after receiving the cue, often exhibiting a response\ndelay, and the response time can vary significantly between individuals. Analyzing the EEG signal over different time\nsegments within a sliding window can help to obtain more consistent and reliable classification results.\nIn this work, we adopt a convolutional-based sliding window approach, which integrates convolutional layers within each\nwindow processing step to efficiently capture localized temporal features. This approach enables parallel processing of\nall windows, thereby reducing training and inference time. The length of the sliding window $T_w$ is calculated by the\nfollowing formulas, depending on the pooling operations applied to the sequence\n$\u03a4_\u03c9 = T_c - n + 1, T_c > n \u2265 1$              (3)\n$\u03a4_\u03c9 = \\frac{T}{8P_2} - n + 1$          (4)"}, {"title": "3 Experiment and Result Analysis", "content": "The EEG-DCNet model, along with comparison models, was trained and tested using the TensorFlow framework on a\nsingle GPU (Nvidia GTX 3070Ti with 8 GB of memory). For all experiments, the following training configuration was\napplied:\nThe model was optimized using the Adam optimizer with a learning rate of 0.001, a batch size of 64, and trained\nfor 500 epochs, with early stopping set to a patience of 300 epochs. The proposed EEG-DCNet model achieved an\noverall accuracy of 87.94% and a Kappa score of 0.8392, outperforming existing SOTA approaches on all test datasets.\nAdditionally, the EEG-DCNet model requires fewer parameters and demonstrates faster training times compared to\nexisting models, further highlighting its efficiency and effectiveness in EEG classification tasks."}, {"title": "3.1 Dataset", "content": "Our approach is evaluated on three widely used EEG datasets: the BCI Competition IV dataset 2a, the BCI Competition\nIV dataset 2b, and the High Gamma Dataset. These datasets were collected using different acquisition devices,\nexperimental paradigms, numbers of subjects, and sample sizes, enabling us to assess the generalizability of our\napproach.\nDataset 1: The BCI-IV-2a dataset consists of EEG data from 9 subjects, each of whom performed four different MI\ntasks during the experiment: left hand (class 1), right hand (class 2), both feet (class 3), and tongue (class 4) MI. For\neach subject, two sessions were recorded on different days, one used for training (T) and the other for testing (E) the\nclassifier. Each session was further divided into 6 runs, with each run containing 48 trials. Each trial was recorded using\n25 channels (including 22 EEG channels and 3 EOG channels), with a sampling rate of 250 Hz and a bandpass filter\nfrom 0.5 Hz to 100 Hz.\nDataset 2: The BCI-IV-2b dataset consists of EEG data from 9 right-handed subjects with normal or corrected-to-normal\nvision. During the experiment, the subjects performed two different MI tasks: left hand (class 1) and right hand (class\n2) MI. For each subject, EEG data were recorded over 5 sessions. The first two sessions involved MI without visual\nfeedback, while the last three sessions included visual feedback. The non-feedback sessions were divided into 4 runs,\nand the feedback sessions into 2 runs, with each run consisting of 20 trials. Each trial was recorded from 3 channels\n(C3, Cz, C4), with a sampling rate of 250 Hz and a bandpass filter from 0.5 Hz to 100 Hz.\nDataset 3: The High Gamma Dataset consists of EEG data from 14 subjects, each performing four different MI tasks\nduring the experiment: left hand (class 1), right hand (class 2), both feet (class 3), and rest (class 4) MI. For each subject,\napproximately 1000 four-second MI trials were conducted, divided into 13 runs. Each trial was recorded using 128\nchannels."}, {"title": "3.2 Evaluation Metrics", "content": "The proposed model in this paper is evaluated using accuracy and the Kappa score. The accuracy measures the\nproportion of correct predictions made by the model over all classes and the Kappa score evaluates the agreement\nbetween predicted and true labels, adjusting for the chance of agreement. Equations (9) and (10) give their calculation\nmethods respectively.\n$Accuracy = \\frac{1}{n} \\sum_{i=1}^{n} T P_i/I_i$              (9)\nwhere $TP_i$ denotes the number of correctly predicted samples for class i, $I_i$ is the total number of samples in class i,\nand n is the number of classes.\n$Kappa = \\frac{\\sum_{a=1}^{n} P_a - P_e}{1 - P_e}$                 (10)\nwhere $P_a$ is the average accuracy across all classes, and $P_e$ is the expected accuracy by random guessing."}, {"title": "3.3 Comparison of number of sliding windows", "content": "Table 3.3 illustrates the relationship between the number of sliding windows and the accuracy of the EEG-DCNet\nmodel. Currently, there is no analytical method to determine the optimal number of sliding windows directly. Instead, it\nmust be tested sequentially to identify the setting that yields the highest accuracy. The results show that increasing\nthe number of sliding windows generally improves performance up to a certain point (specifically, 6 windows in this\ncase). However, further increases beyond this optimal number can lead to fluctuations in accuracy and Kappa scores, as\nobserved in the table. This behavior suggests that while more windows may capture finer temporal features, excessive\nwindows may introduce redundancy or noise, reducing model efficiency and stability."}, {"title": "3.4 Comparison of Different Attention Mechanisms", "content": "Table 3.4 presents a comparison of four attention mechanisms: CBAM, CA, ECA, and SE-demonstrating their\nimpact on the performance of the EEG-DCNet model. The results indicate that all attention mechanisms contribute to\nenhancing DCNet's performance, with CBAM and SE showing superior results in terms of accuracy and Kappa score.\nHowever, CBAM operates significantly slower than SE. This suggests that SE may be more suitable for applications\ninvolving 2-D EEG features, because it can balance the accuracy and computational performance."}, {"title": "3.5 Ablation Analysis", "content": "In this part, we conduct an ablation analysis to evaluate the contribution of each block within the EEG-DCNet model to\nthe overall performance on MI classification using the BCI-2a dataset. Table 3.5 shows the performance impact of each\nblock in terms of accuracy and Kappa score. Each block was systematically removed from the model before training\nand validation to isolate its effect on classification performance.\nThe results indicate that incorporating the SP block increases the model's accuracy by 4.84%, the SW block by 2.83%,\nand the AT block by 1.58%. These findings highlight that each block contributes a unique and valuable improvement to\nthe model's performance, confirming that the combination of all blocks yields the highest accuracy and Kappa score.\nThis analysis underscores the importance of each component within the EEG-DCNet architecture and demonstrates that\ntheir integration enhances classification performance on EEG data."}, {"title": "3.6 Baseline Comparison", "content": "This section presents a summary of the accuracy and Kappa scores of the proposed EEG-DCNet model based on the\nBCI-2a dataset, along with a comparison against several reproduced baseline models: EEGNet, ShallowConvNet,\nMBEEG_SENet, EEGTCNet, EEGNeX, and ATCNet. For each of these models, the results are based on the hyperpa-\nrameters specified in this study, with consistent preprocessing, training, and evaluation procedures applied across all\nmodels. Below is a brief overview of the baseline models included in this comparison:\n1. EEGNet[26]: Utilizes 2D temporal convolutions, depthwise convolutions, and separable convolutions to\nprovide consistent handling across various BCI tasks.\n2. ShallowConvNet[27]: Uses two convolutional layers and one mean pooling layer to perform MI EEG\nclassification.\n3. MBEEG_SENet[28]: Employs a multi-branch CNN model with SE attention blocks to capture EEG features\nat varying time scales.\n4. EEG-TCNet[29]: Integrates a CNN model with a TCN (Temporal Convolutional Network) module to capture\ntemporal information in EEG signals.\n5. EEGNEX[30]: Applies two layers of 2D temporal convolutions and depthwise convolutions to extract both\ntemporal and frequency information from EEG signals.\n6. ATCNet[8]: Implements MHA (Multi-Head Attention) with a sliding window approach, using a TCN module\nto enhance temporal feature extraction for EEG classification.\nAs shown in Table 3.6, EEG-DCNet achieves the highest performance among the models tested, with an average\naccuracy of 87.94% and a Kappa score of 0.8392. This represents a notable 10.26% improvement in accuracy compared\nto the baseline EEGNet model."}, {"title": "4 Conclusion", "content": "The EEG-DCNet model proposed in this paper has demonstrated significant performance improvements in the EEG-\nMI classification task, validating its effectiveness in processing EEG signals and enhancing classification accuracy.\nBy introducing 1\u00d71 convolution layers and a multi-branch parallel dilated convolution structure, EEG-DCNet can\ncapture the nonlinear features and multi-scale information of EEG signals, greatly improving the model's feature\nextraction capability. Additionally, by combining sliding windows and attention mechanisms, the model is able to\nbetter capture the temporal dynamics of EEG signals, thus enhancing the recognition of user intentions. Experimental\nresults on three different EEG datasets show that EEG-DCNet outperforms existing SOTA approaches in terms of\nclassification accuracy and Kappa score, while also demonstrating superior performance in terms of parameter count\nand computational efficiency. These results not only confirm the potential of EEG-DCNet as an efficient and accurate\nEEG-MI classification model, but also provide new directions for the future development of BCI technology.\nHowever, EEG-DCNet still has many areas that need improvement, and there is still much to explore in the MI EEG\nclassification task. Beyond the proposed blocks, we also tried adding temporal convolution modules after the sliding\ntime window[8], but the classification performance did not show evident improvement. The effectiveness of TCNs\nfor EEG signals still requires further validation, especially in specific datasets and task scenarios, where TCNs may\nnot necessarily outperform traditional methods. Furthermore, while EEG-DCNet's model architecture achieves the\nbest performance on multiple datasets, it requires training from scratch each time, and its generalization ability across\ndatasets is limited. This makes it challenging to perform real-time updates in practical applications, particularly when\ndeployed on edge devices. Incremental learning is an effective approach to address this issue[31], and although there is\nlimited research in this area, we have conducted several experiments and obtained promising results. The attention\nmechanism in the model helps in understanding key features of EEG signals. By analyzing the weights in the attention\nmodule, we can identify the EEG signal waveforms that play a crucial role in classification decisions, providing\na better understanding of how the model makes decisions[32]. This is vital for enhancing clinical acceptance and user\ntrust. Additionally, multimodal fusion offers a new opportunity for improving model performance. In recent years,\nclassification using only EEG signals has gradually reached a performance ceiling, and combining other modalities,\nsuch as functional near infrared spectroscopy imaging data, can effectively reduce the impact of individual differences\nand help the model learn more common features, thereby improving classification accuracy."}]}