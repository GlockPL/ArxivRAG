{"title": "STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking", "authors": ["Yidi Li", "Hong Liu", "Bing Yang"], "abstract": "Audio-visual speaker tracking aims to determine the location of human targets in a scene using signals captured by a multi-sensor platform, whose accuracy and robustness can be improved by multi-modal fusion methods. Recently, several fusion methods have been proposed to model the correlation in multiple modalities. However, for the speaker tracking problem, the cross-modal interaction between audio and visual signals hasn't been well exploited. To this end, we present a novel Speaker Tracking Network (STNet) with a deep audio-visual fusion model in this work. We design a visual-guided acoustic measurement method to fuse heterogeneous cues in a unified localization space, which employs visual observations via a camera model to construct the enhanced acoustic map. For feature fusion, a cross-modal attention module is adopted to jointly model multi-modal contexts and interactions. The correlated information between audio and visual features is further interacted in the fusion model. Moreover, the STNet-based tracker is applied to multi-speaker cases by a quality-aware module, which evaluates the reliability of multi-modal observations to achieve robust tracking in complex scenarios. Experiments on the AV16.3 and CAV3D datasets show that the proposed STNet-based tracker outperforms uni-modal methods and state-of-the-art audio-visual speaker trackers.", "sections": [{"title": "I. INTRODUCTION", "content": "SPEAKER tracking is a fundamental task in human-computer interaction that determines the position of the speaker in each time step by analyzing data from sensors such as microphones and cameras [1]. It has wide applications in intelligent surveillance [2], multimedia systems [3], and robot navigation [4]. In general, the basic approaches for solving the tracking problem include computer vision-based face or body tracking methods [5-7] and auditory-based Sound Source Localization (SSL) methods [8, 9]. However, it is difficult for uni-modal methods to adapt to complex dynamic environments. For example, visual trackers are susceptible to object occlusion and changes in illumination and appearance. Besides, acoustic tracking is not subject to visual interference, but the intermittent nature of speech signals, background noise, and room reverberation constrain the performance of SSL-based trackers. To this end, audio-visual fusion methods have become an essential approach to improve the accuracy and robustness of speaker tracking systems, taking advantage of the rich information provided by multi-modal sensors.\nMost current audio-visual speaker tracking algorithms are based on the Bayesian inference framework for multi-sensor data fusion, which approximates the state distribution of the target from noisy multi-modal measurements [10-17]. Among them, Particle Filter (PF) is a representative method due to its robustness in nonlinear non-Gaussian systems and flexibility in supporting multiple types of features. The PF framework is applicable to fuse independently generated visual and acoustic observations with a likelihood model, usually in the form of multiplicative or additive fusion [13-17]. Visual features are usually modeled based on manual features such as color histograms and appearance representations of the detected objects [17-22]. Meanwhile, acoustic signals from microphone arrays are used to extract multi-channel spectral cues to generate SSL estimates such as Time Difference of Arriva (TDOA) [23-25] and Direction of Arrival (DOA) [26-28]. The DOA angle estimates are projected onto the image plane to guide the particle propagation step and reweight the visual posterior [12, 29]. 3D visual observations obtained from face detectors and geometric transformations are used to assist acoustic maps in computing multi-modal likelihood components [30, 31]. The visual and audio particles distributed in two spaces are fused in an improved likelihood by a two-layer PF [15]. In GAVT, a multi-pose face detector is used to improve the visual likelihood, and a probabilistic steered response power is designed to enhance the audio observation model [32]. However, these methods prefer to use the detection of a single modality to refine the observation model of another modality, which leads to a dependence on the results of the dominant modality while ignoring the effect of unreliable noisy observations. In addition, likelihood models using standard multiplicative or additive fusion do not provide much flexibility in capturing complex relationships between different modalities and lack low-level cross-modal information interaction.\nWith the development of deep learning, multiple neural networks have been widely proposed for multi-modal tasks, bringing great inspiration to audio-visual fusion study [33]. Deep learning-based audio-visual fusion methods have been proven to enhance many machine perception tasks. They are in principle able to address the challenges of data uncleanness, noise, missing, and conflicts [34]. The Convolutional Neural Network (CNN) is introduced to build an audio-visual object tracking framework, where an additive merging layer is designed to fuse audio and visual features before the standard classification network [35]. A multi-channel Deep Neural Network (DNN) is proposed for sound source localization"}, {"title": "II. BACKGROUND", "content": "In this section, we briefly review the related effective methods in two areas: audio-visual speaker tracking and audio-visual fusion. Then we introduce an acoustic measurement method that is the basis of the proposed audio-visual measurement method."}, {"title": "A. Audio-Visual Speaker Tracking", "content": "Using multi-modal information is an essential approach to enhance tracking performance because the complementary nature of audio and visual modalities can be used to improve the respective strengths and compensate for the individual weaknesses in tracking. Audio measurements for audio-visual speaker tracking systems are typically derived using SSL algorithms, such as delay estimation and DOA estimation [23-28]. Meanwhile, visual cues are typically derived using either hand-crafted generative appearance features [12, 16, 18-22] or deep neural network-based discriminative features [35, 37].\nFor audio-visual speaker tracking, the state space approach based on the Bayesian framework is a representative model-driven approach due to its strength in fusing multi-modal information [13]. PF is the most commonly used tracking framework, which makes approximate inferences of the filtering distribution of the target and exhibits robustness in nonlinear non-Gaussian systems [13-15, 29, 38, 39]. The Probability Hypothesis Density (PHD) filter is proposed to overcome the problem of the unknown and varying number of speakers by propagating multiple target posteriors [10, 12]. The Poisson Multi-Bernoulli Mixture (PMBM) filter is introduced with a phase-aware filter and a separation-before-localization method for multi-speaker tracking [25, 40]. The graphical model solved by variational approximations is proposed to deal with the observation missing problem due to intermittent streams [20, 37]. The deep learning method is widely used in multi-modal fusion studies [41-43] but rarely introduced to audio-visual tracking due to insufficient labeled data [35, 44]. For audio-visual robotic speaker DOA estimation, a cross-modal attentive fusion mechanism is proposed to explore intra-modal temporal dependencies and inter-modal alignment [45]. A multi-modal perceptual attention module is trained by a cross-modal self-supervised learning method [44]. This module explicitly models the multi-modal perceptual process and is further combined into a PF-based tracking framework. Instead of the combination of feature extraction and Bayesian filter, in this paper, we design an end-to-end speaker tracking paradigm based on deep audio-visual learning."}, {"title": "B. Audio-Visual Fusion", "content": "The trend from uni-modal learning to multi-modal learning is crucial for the development of machine perception. Based on the two most significant perceptual modalities, visual and auditory, audio-visual learning aims to exploit the relationship between audio and visual modalities. It is introduced to many challenging tasks, such as audio-visual separation [46-50], object localization [43, 51-53], audio-visual speech recognition [54, 55], and audio and visual generation [56-58]. Taking advantage of deep learning, modality-specific features and joint representations are learned implicitly in increasingly advanced fusion schemes. The binaural audio-visual network extracts and integrates features from binaural recordings and images concurrently to produce a pixel-level SSL map [53]. The convolution-augmented transformer proposed in [55] extracts features directly from the raw pixels and audio waveforms and fuses them via a multi-layer perceptron. The dual multi-modal residual network is adopted to update the separately processed audio and visual features and extract supplementary useful information [43]. Popular learning paradigms typically"}, {"title": "III. STNET NEURAL ARCHITECTURE", "content": "In this work, we combine the video frames captured by a monocular camera and the audio signals collected by microphone arrays in an end-to-end framework to localize the position of the specific speaker. The framework of the proposed STNet is illustrated in Figure 1, including the visual-guided acoustic measurement, feature embedding, cross-modal attention, quality-aware module, and prediction head. First, audio cues are derived by the visual-guided acoustic measurement. The image and speech signals are mapped from mutually independent spaces to a unified feature space through the audio-visual processing and feature embedding. Then, the global interaction of audio and visual features is learned in a cross-modal attention module for feature fusion. Finally, a prediction head and a quality-aware module are used to estimate the speaker position and evaluate the task quality information. Let $I_{1:t}$ denote the image frame set, and $S_{1:t}$ denote the set of windowed audio signals, a video sequence is composed of time-synchronized audio-visual sample pairs $(s_t, I_t)$, t = 1, ...,T, where T is the total number of frames. We aim to estimate the face position of the speaker at time t: $p_t = (\\hat{u}_t, \\hat{v}_t)$, where $\\hat{u}_t$ is the coordinate of the center of the target face in the image plane. In this section, we describe the components of STNet and the tracking algorithm in detail."}, {"title": "A. Visual-Guided Acoustic Measurement", "content": "In the audio-visual processing, we extract the stGCF-based acoustic maps [44], whose peak distribution indicates the location of the sound source. However, the distribution is subject to interference from ambient noise and reverberation. Therefore, we use visual guidance to obtain an improved acoustic map in cases where reliable observations are obtained in the visual branch. Face detection results are used to guide spatial sampling instead of the entire image frame. Visual guidance further narrows the search area and improves SSL accuracy by excluding noise interference from other areas.\nFirstly, to construct the spatial grid, the 2D sample point set $P_{2d}(I_t)$ is obtained by sampling on the image frame $I_t$. Then, the 2D points in the image plane are projected into a set of 3D points with different depths in 3D world coordinates through a pinhole camera model. Assume that the set of d depths is denoted as $D = {D_k, k = 1, ..., d}$, and the image-to-3D projection process on depth $D_k$ is defined as:\n$P_k^{3d}(I_t) = \\Pi(P_{2d}(I_t); D_k)$, (1)\nwhere $\\Pi$ is the projection operator. Given the sampling point set $P_k^{3d}(I_t)$ of image frame $I_t$ at the given depth $D_k$, the formula of GCF map is defined as:\n$R_0(t, P_k^{3d}(I_t)) =\\begin{bmatrix} r(t, p_{11k}) & : & r(t, p_{1wk}) \\\\ : & : & :\\\\ r(t, p_{h1k}) & : & r(t, p_{hwk}) \\end{bmatrix}_{h \\times w}$, (2)\nwhere $r(t,p)$ denotes the GCF value calculated at position p (see [44] for more details). k is the index of the depth, h and w are the number of sampling points in the vertical and horizontal directions. Considering GCF maps at d depths in time interval $[t - m_1,t]$, we select $m_2$ maps with top peak values to form stGCF maps:\n$R(t, I_t) = \\{R_0(t', P_k^{3d}(I_t))| t' \\in T', k' \\in K'\\}$, (3)\nwhere $T'$ is the time index set of the $m_2$ frames, and $K'$ is the corresponding depth index set. the peaks (yellow) near the speaker's location (green cross) in acoustic maps indicate a higher probability of sound source presence. However, some pseudo points appear at other positions due to noise or geometric configuration of the sensors. Therefore, we perform spatial sampling only in the face region. When the face bounding box $O_t$ exists, the region is used to calculate the stGCF by Equation 3, which is denoted as $R(t, O_t)$. After deriving the corresponding time index set and depth index set, the complete GCF maps are calculated in the region of the image frame. The face-guided GCF (fgGCF) maps are calculated as:\n$R'(t, O_t, I_t) = \\{R_0(t^\\circ, P_k^{3d}(I_t))| t^\\circ \\in T^\\circ, k^\\circ \\in K^\\circ\\}$, (4)\nwhere $T^\\circ$ and $K^\\circ$ represent the time and depth index set to construct $R_0(t, O_t)$. Finally, the visual-guided GCF (vgGCF) is defined as:\n$R_{vg}(t, I_t) = \\begin{cases} R'(t, O_t, I_t) & \\text{if } O_t \\neq \\emptyset \\\\ R(t, I_t) & \\text{else}. \\end{cases}$ (5)\nThe overall visual-guided acoustic measurement process is depicted in Algorithm 1."}, {"title": "B. Feature Embedding", "content": "In this module, we use a two-stream network architecture including a visual branch and an audio branch to process image frames and audio cues, respectively."}, {"title": "1) Visual network:", "content": "The visual tracking problem is typically solved by similarity matching between the target template and the search region. Therefore, a Siamese-based [64] network is employed to capture visual observations. A pair of image patches, the template image patch $I_{tpl}$ and the search region image patch $I_s$, is fed into a weight-sharing fully convolutional network (Conv) to extract features. Then, the cross-correlation operation is performed to obtain a response map, representing the probability of the template at each location in the search region. Further, we utilize a multi-scale fusion layer to handle the scale variation of targets. As the multi-scale features extracted from the reshaped search images are added after up-sampling and down-sampling. In terms of three scales, the process of the visual network is summarized as follows:\n$f_l^s = \\text{Conv}^s(I_{tpl}) * \\text{Conv}^s(I_f), l = 1, 2, 3$, (6)\n$f^v = \\varphi_{\\text{up}}(f_1^s) \\oplus f_2^s \\oplus \\varphi_{\\text{down}}(f_3^s)$, (7)\nwhere I is the index of the scale, * and $\\oplus$ denote the operations of cross-correlation and feature addition. Conv refers to the visual CNN similar to the five Conv layers of Alexnet [65]. $f^v \\in R^{C \\times H' \\times W'}$ represents the multi-scale visual features where C = 256. $\\varphi_{\\text{up}}$ and $\\varphi_{\\text{down}}$ denote up-sampling and down-sampling operations, respectively. We sum the feature vector $f^v$ over the channel dimension to get the score map $F^v \\in R^{H' \\times W'}$. The visual observation $O_t$ is derived as follow:\n$O_t = \\begin{cases} (o_t, b_{l_{\\text{max}}}), & \\text{if } \\max(F^v) > \\theta^v\\\\ \\emptyset, & \\text{else}, \\end{cases}$ (8)\nwhere $o_t$ is the center of the face bounding box, which is derived from the position with the maximum score in the score map $F^v$. $b_{l_{\\text{max}}}$ is the scale of the bounding box, where $l_{\\text{max}} = \\arg \\max_{l=1,2,3} (F_l^s)$. $F_l^s$ is the score map of each feature $f_l^v$. When the maximum value of the map $F^v$ is higher than the set threshold, the face bounding box is reliable enough to be used in visual-guided acoustic measurement."}, {"title": "2) Audio network:", "content": "The audio cues derived from acoustic measurements are projected onto the image plane via the camera model. Therefore, a network similar to the visual network can be used to embed the audio cues into a consistent localization space containing the position context. Further, the audio network contains an additional Conv layer to make the feature dimension consistent with the visual feature.\nWe design a pre-trained network called GCFNet to learn position representations implicitly. the following Conv layers, a Multilayer Perceptron (MLP)-based predictor is used to integrate the features into an activation map, which can be formulated as follow:\n$f^a = MLP(\\text{Conv}_a(R_{\\Omega})),$ (9)\nwhere $R_{\\Omega} \\in R^{m_2 \\times H \\times W}$ is short for GCF maps and $f^a \\in R^{H' \\times W'}$ is the output activation map. $Conv_a$ refers to the six-layer audio CNN, whose output vector is the audio feature embedding $f_a \\in R^{C \\times H' \\times W'}$. MLP refers to the MLP-based predictor with three hidden layers and an average pooling layer. According to the ground-truth (GT) box of the target face $(u, v, h_f, w_f)$, the Gaussian distribution is used to generate a binary map as the pseudo label for GCFNet. The pseudo label is calculated by the following formula:\n$L_{\\text{GCF}}(x, y) = e^{\\Lambda(\\mathcal{N}(x, y | \\mu, \\Sigma))}$, (10)\nwhere $(x, y)$ represents the coordinates of the pixel, $\\mathcal{N}(\\cdot)$ is the two-dimensional Gaussian distribution with mean $\\mu = (u, v)^T$ and covariance $\\Sigma = \\text{diag}(h_f / \\iota, w_f / \\iota)$. $\\iota$ is a scale factor to adjust the span of the Gaussian surface. $\\varepsilon^{\\Lambda}(\\cdot)$ is a step function that binarizes the label by a fixed threshold $\\Lambda$. The MSE loss is adopted to train GCFNet."}, {"title": "C. Cross-Modal Attention", "content": "Inspired by the strong capacity of the transformer model in exploring the interaction among contextual information, features from the visual network and the audio network are fused in a Cross-Modal Attention (CMA) module. The core component of the transformer model is the Multi-Head Attention (MHA) [66], which performs the attention function in parallel to consider various attention distributions from multiple representation subspaces. In each head, the input vector is linearly projected to generate a set of query ($Q_i$), key ($K_i$), and value ($V_i$) with dimension $d_k$, $d_k$ and $d_v$. Attention vectors from multiple heads are concatenated and mapped into a single vector. The mechanism of MHA is summarized as:\n$MHA(Q, K, V) = \\text{Concat}(head_1, ..., head_{n_h})W^O,$ (11)\n$\\text{head}_i = \\text{Softmax}(\\frac{Q_iK_i^T}{\\sqrt{d_k}})V_i,$ (12)\nwhere $W^O \\in R^{n_h d_v \\times C}$ is the parameter matric. $n_h$ is the number of heads and $d_k = d_v = C / n_h$ in our implementation. The definition of CMA is similar, where K and V are from the same modality and Q is from another modality, allowing each modality to update its features with external information from the other modality. CMA integrates multi-modal information by using MHA in the form of residuals. The mechanism of CMA is formulated as follow:\n$f_{av} =LN(f^v + MHA(Q^v, K^a, V^a))LN(f^a + MHA(Q^a, K^v, V^v)),$ (13)\nwhere $Q^a$ is the linear transformation of $f^a + E^a$ (the same goes for $Q^v$). $E^a$ is the positional encoding. $f_{av}$ is the audio-visual fusion feature. LN denotes the layer normalization. CMA enables one modality to receive information from another modality, capturing the dependencies of multiple modalities in the global spatial space. Finally, in the predictor module, the audio-visual fusion feature is fed into an MLP-based regression head to estimate the face coordinates of the target in the current frame."}, {"title": "D. Quality-Aware Module", "content": "In the presence of multiple speakers in a scene, target occlusions and overlapping speech often lead to tracking drift and ID switching. To this end, we design a template and search region update/reset strategy for multi-speaker tracking, which is based on a Quality-Aware Module (QAM). We construct a light MLP to regress the quality score. Based on the difference between the ground truth position and the output of the tracking network, we define the degree of proximity between the two as a training objective for the quality assessment network. Let $\\hat{p}_t$ be the output of the prediction head, the label of the QAM sub-network is defined by a sigmoid function:\n$L_{\\text{QAM}} = \\Xi(e^{ \\kappa||p_t - p_t^{gt}||_2} + 1)^{-1},$ (14)\nwhere $\\Xi$ and $\\kappa$ are hyperparameters to control the intercept and slope. $||\\cdot||_2$ is the Euclidean norm. In the inference phase, the regression results of QAM can indicate the quality of the tracking task on the current frame. The input of QAM is the sum of audio features, visual features, and audio-visual fusion features. We apply two pooling layers and an MLP with a sigmoid activation function in the last layer. The structure of QAM is formulated as follow:\n$f^q = LN(f^a \\oplus f^v \\oplus f_{av}),$ (15)\n$f^{q'} = \\text{AvgPool}(f^q) + \\text{MaxPool}(f^q),$ (16)\n$q^s = \\text{Sigmoid}(MLP(f^{q'})),$ (17)\nwhere $q^s$ is the quality score. We employ the standard MSE loss for the quality-aware module and the prediction head."}, {"title": "E. Tracking with STNet", "content": "To demonstrate the effectiveness and generalization capability of the proposed STNet on the audio-visual speaker tracking task, a simple algorithm is utilized to perform the tracking. Given a tracking target, an STNet is applied to track a specified single speaker. In the multi-speaker case, multiple STNets perform the tracking task in parallel. In detail, in each frame, the search area is cropped to a fixed size that is consistent with the size of the input image of the network. The crop center is set to the predicted target position of the previous frame (the center of the original template for the first frame).\nIn addition, the quality-aware module is added for template and search region update/reset in multi-speaker tracking. QAM measures the quality of the outputs of convolutional neural networks and cross-modal attention modules. Higher quality scores signify more reliable predictions, so updating the template using the current prediction result ensures tracking continuity even if the target appearance changes. A lower score indicates that the visual observation may be corrupted by target occlusion, or that the audio cues are disturbed by mixed multi-sound source signals, or result in confusing fusion features. In this case, we reset the template to the original template that provides highly reliable information and set the crop center of the search region for the next frame according to the visual measurements. The template update and reset process is summarized as follow:\n$I_{t+1}^{tpl} = \\begin{cases} crop(I_t,\\hat{p}_t, b) & \\text{if } q^s > \\theta_1\\\\ I_{tpl} & \\text{if } q^s < \\theta_2\\\\ I_{tpl} & \\text{else}, \\end{cases}$ (18)\nwhere $\\theta_1$ and $\\theta_2$ are threshold parameters used for template update and reset, determined by empirical evaluation. The superscript tpl denotes the template image patch, and $I_{tpl}$ is a pre-defined template in the initial phase. The operation crop(\u00b7) means to crop the image $I_t$ using $\\hat{p}_t$ as the center and b as the range. When $O_t \\neq \\emptyset$, b is taken as $b_{l_{\\text{max}}}$, otherwise b is set to the scale of $I_{tpl}$. When the window exceeds the image range, the missing part is filled with the average RGB value.\nThe proposed tracker further executes 3D tracking through the image-3D projection operator provided by a camera model. After obtaining $\\hat{p}_t$ in the image plane estimated by the network, with the depth derived from the acoustic measurement, the 3D coordinate is calculated as:\n$p_t^{3d} = \\Phi(\\hat{p}_t; D_{k_{\\text{max}}}),$ (19)\nwhere $D_{k_{\\text{max}}}$ is the depth where the peak of vgGCF maps is located and $k_{\\text{max}}$ is the corresponding index. $\\Phi$ is the image-3D projection operator, which requires camera calibration parameters. The complete tracking process of the STNet-based Tracker (STNT) is summarized in Algorithm 2."}, {"title": "IV. EXPERIMENTS AND DISCUSSIONS", "content": "In this section, the proposed tracker is compared with state-of-the-art methods and uni-modal methods on the AV16.3 [67] dataset. First, the dataset, evaluation metrics, and experimental setups are described in detail. Then, the comparative results and the effectiveness of each component are discussed. Finally, the visualization results are presented to further demonstrate the effectiveness of the proposed method."}, {"title": "A. Dataset", "content": "AV16.3 [67] is an audio-visual corpus containing indoor multispeaker recordings designed to test algorithms for audio-only, video-only, and audio-visual speaker localization and tracking. In the provided sequences, 1-3 participants speak in the conference room while moving and displaying various actions, which are simultaneously recorded by three calibrated cameras and two circular eight-element microphone arrays. Audio data is captured at a sample rate of 16kHz by microphone arrays mounted on the table. The images are collected at 25Hz by monocular color cameras installed in three corners of the room, with resolution as 288 \u00d7 360 pixels.\nIn each experiment, data captured by two microphone arrays and one of the three cameras are used sequentially to validate the effectiveness of the algorithm at different viewpoints. For training, we use the single-speaker sequences seq01, 02, 03. For evaluation, we test the same sequences specified in [12, 17, 29-31, 44], using seq08, 11, 12 for Single Object Tracking (SOT) and seq24, 25, 30 for Multiple Object Tracking (MOT). CAV3D [30] is an audio-visual corpus of speaker tracking collected by a co-located sensor platform. The dataset contains multi-speaker data collected in a room of size (4.77 \u00d7 5.95 \u00d7 4.5, m). The co-located sensor platform placed on the conference table consists of a camera and an 8-element circular microphone array. The 8-channel audio signal is recorded at 96 kHz and the 768 \u00d7 1024 pixel video signal is recorded at 15 frames per second with a field of view of 90\u00b0. In addition, four corner cameras are configured for 3D trajectory calibration. The dataset consists of 20 sequences divided into three types of subsets, CAV3D-SOT (9 single-speaker sequences), CAV3D-SOT2 (6 sequences with one speaker and one non-vocalized participant), and CAV3D-MOT (5 multi-speaker sequences). Compared to AV16.3, the scenes in the CAV3D dataset are more complex, including scenes with occlusion, non-frontal views, entering/exiting the camera field of view, and long periods of silence. We collect data on 9 sequences in the CAV3D-SOT subset for model training."}, {"title": "B. Metrics", "content": "Mean Absolute Error (MAE) is a tracking performance metric that is estimated by calculating the Euclidean distance between the predicted positions and ground-truth positions. This metric is widely used because it explicitly measures the accuracy of target localization for performance comparison. MAE is defined as follow:\n$MAE = \\frac{1}{TN} \\sum_{t=1}^T \\sum_{n=1}^N ||p_{t,n}^{gt} - \\hat{p}_{t,n}||_2,$ (20)\nwhere T is the total number of frames and N is the total number of objects in each frame. MAE can be evaluated in the image plane (in pixels) or 3D world coordinate (in m). MOT Metric includes MOT Precision (MOTP) and MOT Accuracy (MOTA) [68]. Unlike MAE, MOTP only calculates the errors in successfully matched frames, avoiding the considerable impact of huge errors caused by target losses. MOTA provides an intuitive measure of the tracker's performance in detecting objects and maintaining trajectories, which is independent of the accuracy of object position estimation. MOTA considers the number of false decisions made by the tracker, including FP (errors greater than a threshold), FN (missing targets), and IDS (speaker identity being switched):\n$MOTA 100 \\times (1- \\frac{\\sum_t(FP_t + FN_t + IDS_t)}{\\sum_t n_{it}}),$ (21)"}, {"title": "C. Implementation Details", "content": "In the audio branch, the speech signal from circular microphone arrays is enframed to 40ms by a Hamming window with 1/2 frame shift. The parameters for calculating vgGCF are set as d = 9, m\u2081 = 15, m\u2082 = 3. The 2D sample points are separated by 3 pixels. According to the actual situation of the room configuration of the two datasets, the 3D sampling points located outside the room range and below the tabletop are removed. A pinhole camera model is built using camera calibration parameters provided by the datasets to implement the image-3D projection. In the visual branch, the search images are resized to 3 sizes, 2.5x, 3.3x, and 4.2x, to handle scale variations. The confidence threshold $\\theta^v$ for visual observation is set to 0.3 on AV16.3 and 0.35 on CAV3D. In the training phase, we collect 44,958 sets of audio-visual sample pairs from the training sequences in AV16.3, and 73,256 pairs on CAV3D-SOT. We capture 120 \u00d7 120 image samples from AV16.3 and 320 \u00d7 320 image samples from CAV3D in a data enhancement manner of surrounding cropping, as shown in Figure 6. The image patches are randomly cropped around the ground-truth bounding box and completely contain the ground-truth box. The audio samples are vgGCF maps extracted in the corresponding sampling area. The parameters to build $L_{GCF}$ are set to $l = 2.5$, $ \\Lambda= 0.5$. The parameters to construct $L_{QAM}$ are set to $ \\Xi= 2$, $ \\kappa= 0.05$. In the CAM module, $n_h$ is set to 8. In the tracking test, the thresholds that control update and reset are set to $\\theta_1= 0.98$, $ \\theta_2 = 0.94$ on AV16.3 and $\\theta_1= 0.985$, $ \\theta_2 = 0.945$ on CAV3D.\nFor model training, the STNet is trained using pre-training with fine-tuning. In the visual branch, the Siamese-based feature extraction network is pre-trained on 214 sequences on the GOT-10k [69] dataset. We use the Adam optimizer with a learning rate of 10-\u00b3 and a batch size of 32. In the audio branch, the GCFNet model is pre-trained using data from random surrounding cropping on the training sequences in AV16.3 and CAV3D. We use the Adam optimizer with the learning rate of 10-\u2074 and a batch size of 16. The early stopping strategy is employed to prevent overfitting. Subsequently, the fusion module and QAM block are added after the uni-modal branch to train the complete network with an optimizer of SGD, a learning rate of 10-\u2074, and a batch size of 16.\nThe experiments are conducted on the PyTorch framework with one NVIDIA RTX 2080 Ti GPU. The code is publicly available at https://github.com/liyidi/STNet."}, {"title": "D. Comparisons with State-of-the-art Results", "content": "Our method is compared with uni-modal methods and previous state-of-the-art Audio-Visual (AV) methods. Where the Audio-Only (AO) and Visual-Only (VO) methods are implemented based on stGCF and the visual network. The results of MOT and SOT on two datasets are shown in Table I, II and III, respectively. used in Sec. III-A and Sec. III-B1. We reproduce the methods of [12, 29, 30, 44", "17": [31], "25": "are reported in their papers. Note that [29", "30": "considerably in tracking accuracy and achieves a lower MAE (3.53 pixels) than MPT presented in [44", "12": "uses PHD filtering and [30, 31"}]}