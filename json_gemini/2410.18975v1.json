{"title": "UNBOUNDED: A GENERATIVE INFINITE GAME OF CHARACTER LIFE SIMULATION", "authors": ["Jialu Li", "Yuanzhen Li", "Neal Wadhwa", "Yael Pritch", "David E. Jacobs", "Michael Rubinstein", "Mohit Bansal", "Nataniel Ruiz"], "abstract": "We introduce the concept of a generative infinite game, a video game that tran-\nscends the traditional boundaries of finite, hard-coded systems by using genera-\ntive models. Inspired by James P. Carse's distinction between finite and infinite\ngames (Carse, 1986), we leverage recent advances in generative AI to create UN-\nBOUNDED: a game of character life simulation that is fully encapsulated in gen-\nerative models. Specifically, UNBOUNDED draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character in\na virtual world by feeding, playing with and guiding it - with open-ended mechan-\nics generated by an LLM, some of which can be emergent. In order to develop\nUNBOUNDED, we propose technical innovations in both the LLM and visual gen-\neration domains. Specifically, we present: (1) a specialized, distilled large lan-\nguage model (LLM) that dynamically generates game mechanics, narratives, and\ncharacter interactions in real-time, and (2) a new dynamic regional image prompt\nAdapter (IP-Adapter) for vision models that ensures consistent yet flexible visual\ngeneration of a character across multiple environments. We evaluate our system\nthrough both qualitative and quantitative analysis, showing significant improve-\nments in character life simulation, user instruction following, narrative coherence,\nand visual consistency for both characters and the environments compared to tra-\nditional related approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "In his work \"Finite and Infinite Games: A Vision of Life as Play and Possibility\" Carse (1986),\nJames P. Carse introduces a distinction between two types of games. Carse defines finite games as\nthose \"played for the purpose of winning,\" with boundaries, fixed rules, and a definitive endpoint.\nIn contrast, infinite games are \u201cplayed for the purpose of continuing the play,\u201d with no fixed bound-\naries and evolving rules. Traditional video games are, inherently, finite games due to the limitations\nof computer programming and computer graphics. For example, all game mechanics have to be fully\npre-defined in the programming language and all graphics assets have to be pre-designed (modulo\nprocedural generation which still grapples with structural limitations). This allows for only a finite,\nand sometimes predefined, set of actions and paths that can be taken. They also feature predefined\nrules, boundaries, and win conditions.\nRecent advances in generative models have been impressive. We hypothesize that these develop-\nments have finally opened up the possibility of creating the first generative infinite video game.\nTwo advancements have made this achievable: (1) large language models (LLMs) that can encode\npersistent video game mechanics (e.g. interactions with the game environment or characters, char-\nacter state tracking, object permanence), generate interactive stories, and produce spontaneous, and\nsometimes emergent, behaviors; and (2) visual generative models capable of producing high-quality\nimages that follow prompts. In this work, we present UNBOUNDED, what we believe to be the first\ninteractive generative infinite game, where game behaviors and outputs are generated by AI models,\ntranscending the constraints of hard-coded systems. UNBOUNDED draws inspiration from sandbox\nlife simulations and digital pet games such as Little Computer People, The Sims and Tamagotchi.\nIt incorporates elements from tabletop roleplaying games like Dungeons & Dragons, which offer\nunconstrained storytelling experiences that have been unattainable in video games.\nUNBOUNDED offers a gameplay loop centered around character simulation and open-ended inter-\naction (Figure 2). Players can insert their characters into the game, defining their appearance and\npersonality. The game generates a world where these characters can explore environments, inter-\nact with objects, and engage in conversations. The game generates new scenarios, stories, and\nchallenges based on the player's actions and choices, creating a personalized and infinite gaming\nexperience. Specifically, UNBOUNDED has the following capabilities: (1) Character Personalization: players\ncan insert their characters into the game, defining their appearance and personality. (2) Game En-\nvirnoment Generation: UNBOUNDED generates a persistent world that the characters can explore\nand interact. (3) Open-Ended Interaction: Players can interact with the character using natural lan-\nguage instructions, and there are no pre-defined rules to constraint the interaction. (4) Real-Time\nGeneration: we pay special attention to the speed of the game and achieve 5-10x speedups over a\nnaive implementation, serving each new scene with a latency of about one second.\nOur approach introduces technical innovations in both LLM and vision generation domains. On\nthe language side, we developed a LLM based game engine capable of maintaining consistent\ngame mechanics, generating coherent narratives, and producing contextual character responses in\nreal-time. Our distilled specialized model is fine-tuned on data automatically generated with two\ncollaborative strong LLM agents, without the need for human annotation in the loop. Our distilled\nLLM model handles the dynamic generation of game rules and scenarios, adapting to player input\nand game state. In visual generation, we introduce a new regional IP-Adapter, which allows for\nthe consistent generation of characters and environments while maintaining visual coherence across\nmultiple images. Specifically, our regional IP-Adapter conditions the image generation on the game\nenvironment and character appearance encoded modulated by a dynamic mask obtained from at-\ntention outputs in cross-attention layers. This is in order to mitigate the interference between the\nenvironment and character, in order to have both reliably appear in the scene. This approach enables\nreal-time image generation that reflects the game state and player actions.\nThe contributions of this work are conceptual and technical. We introduce the notion of a gener-\native infinite game, demonstrating its feasibility and potential impact on the future of interactive\nentertainment. We present a new paradigm for game design where the game logic and content are\nencapsulated within generative models. Our main technical contributions include the specialized\ndistilled LLM for game logic and narrative generation and the regional IP-Adapter for consistent\nvisual generation. We demonstrate the effectiveness of our regional IP-Adapter through both quan-"}, {"title": "2 RELATED WORK", "content": "Controllable Text-to-Image Generation. Controllable text-to-image generation becomes a key\nresearch direction in diffusion model applications, enabling diverse ways to guide the generation\nprocess. For instance, ControlNet (Zhang et al., 2023) introduces conditioning mechanisms that uti-\nlize control signals such as depth maps, poses, edges, and segmentation maps to guide image gener-\nation. Other works focus on layout control using bounding boxes to control object placement within\nthe generated images (Li et al., 2023; Shin et al., 2022). Beyond these control signals, another major\narea of research involves personalization, where the goal is to generate consistent characters (Ruiz\net al., 2023; Gal et al., 2022; Kumari et al., 2023) or consistent face identities (Li et al., 2024; Wang\net al., 2024b; Yan et al., 2023; Ruiz et al., 2024) across multiple generations. However, most existing\napproaches lack support for conditioning both characters and environments separately, often requir-\ning predefined masks for generating characters (Chen et al., 2024; Lugmayr et al., 2022; Yang et al.,\n2023), with the environment remaining identical to the input image. This limitation makes it difficult\nto seamlessly integrate characters into different environments while ensuring both consistency and\nalignment with the input prompts. IP-Adapter (Ye et al., 2023) tackles this task by conditioning the\ngeneration on the environment and character images. However, IP-Adapter tends to over reconstruct\nthe conditions, which causes interference between them. In this paper, we build our approach on\nIP-Adapter and propose an improved regional IP-Adapter with block drop, separating character and\nenvironment generation to enhance consistency.\nLarge Language Models in Image Generation. Large language models have demonstrated\nstrong in-context learning capability (Brown, 2020), which enables them to solve diverse customized\ntasks based on human instructions and in-context examples. In the field of image generation, large"}, {"title": "3 METHOD", "content": "We introduce UNBOUNDED, an interactive generative infinite game powered by text-to-image gen-\neration models and large language models. UNBOUNDED offers: (1) Custom Character Person-\nalization: users create unique characters with customizable appearances and personalities; (2) Dy-\nnamic World Creation: the system generates a persistent, interactive game world for exploration;\n(3) Open-Ended Interaction and Gameplay: players interact with their characters via natural lan-\nguage, with the game dynamically generating new scenarios and storylines based on player actions;\nand (4) Generation in Interactive Speed: the game runs with near real-time interactivity, achieving\na refresh rate close to one second. We detail the methods enabling these capabilities in this section."}, {"title": "3.1 PERSONALIZATION OF LATENT CONSISTENCY MODELS FOR CHARACTER\nCONSISTENCY", "content": "A key feature of UNBOUNDED is its ability to serve a fully generative model-based game with real-\ntime interaction. This is made possible through the use of latent consistency models (LCM) (Luo\net al., 2023) which allow for high-resolution image generation with as few as two diffusion steps.\nBy utilizing LCMs, UNBOUNDED achieves real-time text-to-image (T2I) generation, critical for\ndelivering an interactive gaming experience with a refresh rate close to one second."}, {"title": "3.2 REGIONAL IP-ADAPTER WITH BLOCK DROP FOR ENVIRONMENT CONSISTENCY", "content": "Another key feature of UNBOUNDED is generating the character in pre-defined environments per-\nforming different actions based on user instructions. Thus, maintaining both character and environ-\nment consistency is essential. While character consistency is handled as discussed in Sec. 3.1, two\nadditional challenges arise: ensuring the environment consistency across different generations and\naccurately placing the character within the environment, without losing alignment with text prompts.\nWe find that existing method fails to consistently perform well for all requirements in interactive\nspeed. As one of our main technical contributions we propose a novel regional IP-Adapter in order\nto consistently implant a character in pre-defined environments following text prompt."}, {"title": "3.2.1 REGIONAL IP-ADAPTER", "content": "We propose an improved version of IP-Adapters (Ye et al., 2023) that enables dual-conditioning on\nboth subjects and environments, allowing for the generation of a pre-defined character in a user-\nspecified environment. Unlike the original IP-Adapters, which focus on single-image conditioning,\nour approach introduces dual-conditioning and dynamic regional injection mechanism to represent\nboth concepts simultaneously in the generated images.\nLet us start with an example. As shown in Figure 4, given the text prompt \"Beneath the desert sky,\n[V] witch coaxes the cacti to blossom with vibrant, glowing flowers\" and the desert environment"}, {"title": "3.2.2 BLOCK-WISE DROP ON ENVIRONMENT CONDITIONING", "content": "For our regional IP-Adapter, we use\na dynamic mask from the cross-\nattention between character text and\nhidden states. This mask's quality\nis key for separating character and\nenvironment generation."}, {"title": "3.3 LANGUAGE MODEL GAME ENGINE WITH OPEN ENDED INTERACTIONS AND\nINTEGRATED GAME MECHANICS", "content": "UNBOUNDED simulates character actions in pre-defined environments with images generated based\non scene text descriptions while monitoring the character's state and providing the user with natural\nlanguage interaction with the character and game environment. For example, the user can take the\ncharacter to different environments, interact with the character (e.g. \"I pet the character on its\nhead\"), or essentially take any open ended action e.g. \"I pet the character and then take them for\na rocket ride at the space station.\". Since the game is ultimately built on a language model, these\nexpansive capabilities present several challenges: (1) Environment Binding: the model needs to place\nthe character in the correct environment based on the natural language instructions. (2) Coherent\nStory Generation: the model generates coherent narrative descriptions and character responses that\nalign with user-specified character traits. (3) Game Mechanics Generation: the model needs to\nmonitor the state of the character (i.e., hunger, energy, fun, hygiene), and update them based on user\ninteractions and story events. (4) Prompt Rewriting: the model needs to rewrite the narratives for the\ndiffusion model (i.e., append special token \u201c[V]\u201d for the character, align environment descriptions\nto the pre-generated environments for better environment consistency).\nSurprisingly, we find that a very large language model (e.g., GPT-4, GPT-40) with detailed instruc-\ntions and using in-context learning (Brown, 2020) can exhibit these capabilities. Nevertheless, using\nsuch large models as game engines is not directly feasible due to the large latency (e.g., 5 seconds\nfor a 7B model to give one response). Given this, we propose to distill these capabilities from a\nvery large model into a smaller model based on Gemma-2B (Team et al., 2024) for game logic and\nnarrative generation that supports real-time interaction. In this section we propose two key technical\ncontributions (1) a design for a character life simulation game using two very large language models\nthat control for world modeling and user interaction respectively (2) a framework for distilling this\nknowledge into one smaller language model that is fast enough to achieve interactive speeds."}, {"title": "3.3.1 CHARACTER LIFE SIMULATION WITH MULTI-LLM COLLABORATION", "content": "We build a character life simulation game with two LLMs agents. One agent serves as the world\nsimulation model, responsible for setting up game environments, generating narratives and image\ndescriptions, tracking character states and simulating the character's behavior. The second agent\nfunctions as a user model, simulating the player's interactions with the world simulation model. It\nhas three types of interactions: continuing the story within the current environment, moving the\ncharacter into different environments, or interacting with the character to maintain the healthy state\nof the character. In each interaction category, the user has the option to provide personality details\nof the character or guide the character actions that, in turn, guide the simulator's narrative genera-\ntion. This interaction between the world simulation LLM and the user LLM allows for a dynamic\ncharacter life simulation game with virtually unlimited interaction possibilities and narrative paths."}, {"title": "3.3.2 FRAMEWORK FOR SMALL LLM DISTILLATION", "content": "We propose a framework for distilling the capabilities of larger LLMs into the smaller, more efficient\nmodel using synthetic data generated by the multiple stronger LLMs. Our framework contains two\nsteps: (1) automated data collection and (2) small model distillation.\nAutomated Data Collection Our goal is to build a general-purpose character life simulator ca-\npable of understanding a wide range of character traits and generating games across diverse topics.\nTo achieve this, the first step is to gather a diverse dataset of topics and characters. We prompt a\nlarge LLM to generate pairs of topics and corresponding main characters. To ensure data diversity,\nwe retain only the generated pairs whose ROUGE-L similarity to existing data is below 0.7, follow-\ning (Wang et al., 2022), which demonstrated the importance of diverse data for enhancing an LLM's\nability to follow instructions. This process results in 5,000 unique topic-character pairs, which serve\nas the basis for user-simulator interaction data.\nIn the second step, we collect multi-round interaction data between the world simulation LLM and\nthe user LLM. The process begins with the world simulation LLM setting up the game environment\nand initiating a character action based on a randomly sampled topic-character pair from the dataset.\nThe user LLM is then prompted to provide interaction inputs, while the world simulation LLM\ngenerates updated character actions, states, and responses. This iterative process continues for five\ninteraction rounds per session, resulting in a total of 5,000 user-simulator interaction examples. All\nthe prompt templates are in Appendix.\nDistillation Once the interaction data has been collected, we fine-tune the smaller Gemma-2B\nmodel using the 5,000 synthetic user-simulator interaction samples. During supervised fine-tuning,\nwe mask out the loss on user input data, focusing the optimization on learning the world simulation\nmodel's behavior based on multi-round interaction history and current user input. This approach\nenables Gemma-2B to replicate the capabilities of larger LLMs as a game engine while supporting\nreal-time interaction. Our distilled Gemma-2B demonstrates performance comparable to GPT-40,\neffectively following user input and supporting unbounded interactions."}, {"title": "4 EXPERIMENTAL SETUP", "content": ""}, {"title": "4.1 EVALUATION BENCHMARKS", "content": "Evaluation of Image Generations To evaluate our image generation approach, we collect an eval-\nuation dataset consisting of 5,000 (character image, environment description, text prompt) triplets\nwith GPT40 (OpenAI, 2023). It includes 5 characters (dog, cat, panda, witch, and wizard), 100\ndiverse environments, and 1,000 text prompts (10 per environment). We evaluate the image genera-\ntion performance under three criteria: environment consistency, character consistency and semantic\nalignment with the text prompt. To measure similarity between images, we employ CLIP-I (Rad-\nford et al., 2021), DINO and DreamSim. We denote the\nsimilarity between environment reference image and the generated images as CLIP-IE, DINOE,\nDreamSimE, and the similarity between the character reference image and the generated images as\nCLIP-IC, DINOC, DreamSimC. Additionally, we use CLIP-T (Radford et al., 2021) to evaluate the\nsemantic alignment with the text prompt. Given that UNBOUNDED is a character life simulation\ngame, ensuring the presence of the character in the image is important. Therefore, we further utilize\nGrounding-DINO to detect the presence of the character in the generated images.\nWe set similarity scores to 0 and distance scores to 1 if there is no character in the generated image.\nEvaluation of LLM Generations We collect an additional evaluation dataset with 100 user-\nsimulator interaction samples using the pipeline in Sec. 3.3. Each user-simulator interaction sample\ncontains five rounds of interaction between the user and the world model. We use GPT-4 (OpenAI,\n2023) as a judge, scoring the response between two models (baseline model vs. our model) in over-\nall score, and then in four aspects: accuracy of character state update, environment relevance, story\ncoherence, and user input instruction following. Scores range from 0 to 10."}, {"title": "5 RESULTS AND ANALYSIS", "content": ""}, {"title": "5.1 COMPARISON WITH DIFFERENT APPROACHES FOR MAINTAINING ENVIRONMENT\nCONSISTENCY AND CHARACTER CONSISTENCY", "content": "Quantitative Results We compare our regional IP-Adapter with block drop with previous ap-\nproaches in maintaining environment consistency and character consistency. For all the approaches,\nwe merge the character LoRA and LCM LORA with the model to support fast inference and im-\nprove character consistency and provide an apples-to-apples comparison. As shown in Table 1,\nour approach consistently outperforms previous approach in maintaining environment consistency\nand character consistency, while achieving comparable performance in maintaining semantic align-\nment. Specifically, our approach significantly overtakes StoryDiffusion by\n0.047 in CLIP-IC, and 0.057 in DreamSim for character consistency, and 0.035 in CLIP-IE, 0.065\nin DINOE, and 0.058 in DreamSimE for environment consistency, demonstrating the effectiveness\nof our approach. Besides, our approach also achieves comparable performance in maintaining se-\nmantic alignment, suggesting strong text following capabilities.\nQualitative Results We present a qualitative comparison with other approaches in Figure 7. Our\nregional IP-Adapter with block drop consistently generates images with high character consistency,\nwhereas other methods may fail to include the character or generate characters with inconsistent\nappearances Furthermore, we show that our approach balances environment\nconsistency and character consistency well, while other approaches might generate environments\nthat differ from the condition environment."}, {"title": "5.2 EFFECTIVENESS OF DYNAMIC REGIONAL IP-ADAPTER WITH BLOCK DROP", "content": "Quantitative Results We demonstrate that our regional IP-Adapter with block drop is essential\nfor placing the character in the environments following the text prompt, while maintaining both\nenvironment and character consistency with ablation studies. As shown in Table 2, adding block\ndrop improves both environment and character consistency compared with multi-IP-Adapter with an increase of 0.291 in CLIP-IE and 0.264 in CLIP-IC, alongside better alignment\nbetween the text prompt and the generated image. Furthermore, our regional IP-Adapter enhances"}, {"title": "Ac = \\frac{WqOt * WkKT}{\\sqrt{d}}", "content": ""}, {"title": "Mc =\n{\n1 Ac\u2264threshold\n0 Ac > threshold", "content": ""}, {"title": "O = Ot + aeMc * Oe + ac(1 \u2013 Mc) * \u041e\u0441", "content": ""}, {"title": "5.3 EFFECTIVENESS OF DISTILLING SPECIALIZED LARGE LANGUAGE MODEL", "content": "We show that our diverse user-simulator interaction data effectively distills Gemma-2B into a ca-\npable game engine. As shown in Table 3, zero-shot inference with small LLMs (i.e., Gemma-2B, Llama3.2-3B), or a slightly larger LLM (i.e., Gemma-7B) results in lower performance compared to\nours, highlighting the importance of distillation from a stronger LLM for game world and character\naction simulation. Furthermore, we show that our model achieves performance comparable to GPT-\n40, validating the effectiveness of our approach. We also investigate the impact of distillation data\nsize on performance by comparing a Gemma-2B model distilled with 1K data and 5K data. Results\nshow that using a larger dataset consistently improves performance across all aspects, highlighting\nthe potential for further enhancements with more data to fully match GPT-40's performance."}, {"title": "6 CONCLUSION", "content": "We introduce UNBOUNDED, an interactive generative infinite game based on generative models.\nUNBOUNDED is built on two main components, a specialized, distilled LLM for real-time inter-\naction, and a fast diffusion model with our proposed regional IP-Adapter for consistent generation\nacross multiple scenes. We show that our proposed approach allows for an interactive game sub-\nsumed in generative models, with consistent characters, environments and story and an expansive\ngameplay characteristic of an infinite game."}, {"title": "APPENDIX", "content": "In this appendix, we present the following:\n\u2022 Prompts we use for user-simulation data collection in Sec. A.\n\u2022 Evaluation prompts we use for querying GPT-4 as a judge for LLM evaluation in Sec. \u0412.\n\u2022 Reproducibility statement in Sec. C."}, {"title": "A PROMPT FOR SYNTHETIC USER-SIMULATOR INTERACTION DATA\nCOLLECTION", "content": "In this section, we provide the prompt templates we use to collect the user-simulation data. Specif-\nically, we query GPT-3.5 to generate diverse topics and character descriptions using the template\nshown in Figure 9. The prompt template for guiding the potential user interactions for user LLM\nis shown in Figure 10. To ensure user interactions align with the dialogue history, we include the\ninteraction history as input to the user LLM. The world simulation LLM prompt template, shown\nin Figure 11, also takes in the dialogue history and generates the next character actions, states and\nnarratives. We constraint the world simulation LLM to generate one storyline at a time, allowing\nusers to choose how to continue the story."}, {"title": "B EVALUATION PROMPT", "content": "We include the prompt template we use to compare the outputs from two LLMs in Figure 12. The\nprompt is adapted from Vicuna and has been validated as an effective tool for\ncomparing the performance of two LLMs on a given task."}, {"title": "CREPRODUCIBILITY STATEMENT", "content": "First, we include the implementation details of UNBOUNDED in Sec. 4.2, covering the training\nhyperparameters for Dreambooth fine-tuning, and LLM distillation, and the hyperparameters we use\nduring inference for both LLM and image generation. Second, we provide detailed description of\nthe user-simulation data we collect for training in Sec. 3.3, and further include the prompt template\nused to query GPT models in Appendix Sec. A. Lastly, for the LLM-based evaluation, the prompt\ntemplate for querying GPT-4 to compare two LLM outputs is provided in Appendix Sec. B."}]}