{"title": "GEIC: Universal and Multilingual Named Entity Recognition with Large Language Models", "authors": ["Hanjun Luo", "Yibing Jin", "Xuecheng Liu", "Tong Shang", "Ruizhe Chen", "Zuozhu Liu"], "abstract": "Large Language Models (LLMs) have supplanted traditional methods in numerous natural language processing tasks. Nonetheless, in Named Entity Recognition (NER), existing LLM-based methods underperform compared to baselines and require significantly more computational resources, limiting their application. In this paper, we introduce the task of generation-based extraction and in-context classification (GEIC), designed to leverage LLMs' prior knowledge and self-attention mechanisms for NER tasks. We then propose CascadeNER, a universal and multilingual GEIC framework for few-shot and zero-shot NER. CascadeNER employs model cascading to utilize two small-parameter LLMs to extract and classify independently, reducing resource consumption while enhancing accuracy. We also introduce AnythingNER, the first NER dataset specifically designed for LLMs, including 8 languages, 155 entity types and a novel dynamic categorization system. Experiments show that CascadeNER achieves state-of-the-art performance on low-resource and fine-grained scenarios, including CrossNER and FewNERD. Our work is openly accessible.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) has revolutionized natural language processing (NLP) (Zhao et al., 2023; Naveed et al., 2023). Recent LLMs, such as GPT-4 (Achiam et al., 2023) and Claude-3 (Anthropic, 2023), with their powerful generalization and attention mechanisms (Vaswani et al., 2017), excel not only in text generation tasks like Text Summarization (Wilson et al., 2024; Jin et al., 2024), Question Answering (Tan et al., 2023), and Machine Translation (Vilar et al., 2022; Moslem et al., 2023), but also in discriminative tasks, including Named Entity Extraction (Sancheti et al., 2024) and Text Classification (Gasparetto et al., 2022).\nNevertheless, unlike other discriminative tasks, existing LLM-based approaches (Shao et al., 2023; Keloth et al., 2024; Li and Zhang, 2023) struggle to replace supervised methods in Named Entity Recognition (NER). NER is typically framed as a sequence labeling task, where the model assigns entity labels to each token in a sentence. This structure emphasizes sequential dependencies, which contrasts with LLMs' focus on global attention mechanisms. Although existing methods leverage generalization and prior knowledge of LLMs to outperform supervised baselines in low-resource scenarios, they often fall short significantly in standard scenarios or complex entity classifications. Moreover, their reliance on commercial models, typically GPT, introduces significant challenges related to cost, data security, and transparency, limiting their broader applicability.\nTo address these issues, we introduce Generation-based Extraction and In-context Classification (GEIC), a novel task designed to harness LLMs' attention mechanisms and prior knowledge for NER. As illustrated in Figure 1, unlike traditional sequence labeling methods, GEIC redefines NER as two sequential in-context text generation sub-tasks, extraction and classification, which decreases the task complexity and enhances the capability to capture in-context dependencies. To achieve GEIC, we propose CascadeNER, a universal and multilingual GEIC framework. CascadeNER employs model cascading (Varshney and Baral, 2022), assigning the each sub-tasks to corresponding fine-tuned small-parameter LLMs, referred to as Small Language Models (SLMs). This method reduces computational resource consumption while maintaining high performance, enabling efficient NER with SLMs. Leveraging the prior knowledge, CascadeNER can complete NER tasks in multiple languages in both few-shot and zero-shot scenarios. Extensive experiments demonstrate that CascadeNER achieves state-of-the-art (SOTA) performance on low-resource and fine-grained datasets. Considering the lack of datasets designed for LLM-based NER, we collect AnythingNER for CascadeNER, a multilingual and fine-grained NER dataset, featuring 8 major languages, 8 coarse-grained types, 31 medium-grained types, 155 fine-grained types, and a novel dynamic categorization system. As the first dataset optimized for LLMs' generalization and few-shot/zero-shot learning in NER, AnythingNER provides an unprecedented level of detail and coverage. Moreover, this work offers the first comprehensive comparison and analysis of existing LLM-based NER methods, with a emphasis on multilingual and fine-grained scenarios."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Named Entity Recognition", "content": "Named Entity Recognition (NER) is the task of identifying named entities in text and classifying it into predefined categories. Supervised methods, such as BiLSTM (Yu et al., 2020) and BERT-MRC (Li et al., 2019a), currently dominate this task. They generally rely on large amounts of train data to achieve strong performance, which limits their application in low-resource scenarios. With the advancement of LLMs, leveraging their few-shot and zero-shot learning capabilities to address this. Xie et al. (2023) explore the application of GPT-3 (Brown, 2020) for NER, but its performance is significantly lower than supervised methods'. GPT-NER (Wang et al., 2023) improve GPT-3's NER capabilities by re-framing the task as single-entity labeling through prompt engineering. While GPT-NER enhanced performance, it still lags behind supervised methods. PromptNER (Ashok and Lipton, 2023) achieves better results with GPT-4, yet failed to deliver a robust zero-shot solution. Furthermore, several studies investigate LLM-based NER in domain-specific tasks (Li and Zhang, 2023; Shao et al., 2023; Keloth et al., 2024). These studies, however, all rely on commercial models, increasing costs and limiting flexibility. Existing results highlight both the potential of LLMs in NER and the pressing need for more adaptable and cost-effective solutions."}, {"title": "2.2 Small Language Models", "content": "While commercial LLMs undergoes rapid development, small-parameter and open-source LLMs, known as small language models (SLMs), also attracts significant attention (Touvron et al., 2023; Hu et al., 2024a; Zhang et al., 2024; GLM et al., 2024; Bai et al., 2023). These models employ techniques such as quantization (Han et al., 2015) and distillation (Hinton, 2015) to reduce their parameters to be-tween 1.5B and 7B,offering advantages like easier fine-tuning and lower computational resource consumption, albeit at the cost of some performance degradation in complex tasks. This trade-off have led to the widely use of SLMs like LLaMA-2-7B in both academia and industry (Clemmer et al., 2024; Gade et al., 2023; Luo et al., 2024). However, NER, as a relatively complex task, places high demands on model performance. Existing LLM-based methods fail to effectively decomposed the task, which exacerbates the performance limitations of SLMs in handling complex tasks. Consequently, SLMs have not been effectively applied to NER."}, {"title": "2.3 NER Dataset", "content": "A considerable number of NER datasets in various domains have been developed due to the significance of NER (Tjong Kim Sang, 2002; Kim et al., 2003; Doddington et al., 2004; Walker et al., 2006; Weischedel et al., 2011; Pradhan et al., 2013; Derczynski et al., 2017). However, these existing datasets exhibit several limitations, making them unsuitable for the GEIC task. Most previous multilingual NER datasets adopt coarse-grained classification. For example, PAN-X (Pan et al., 2017) includes only three types. Such datasets no longer meet the fine-grained requirements of contemporary flat NER applications. Even existing fine-grained datasets demonstrate clear limitations in category coverage and granularity, falling short of being truly \"universal.\" For instance, FewNERD, despite having 66 entity types, suffers from highly imbalanced data distribution, which affects its reliability for evaluating few-shot learning capabilities. Furthermore, current datasets fail to adequately address the generalization and few-shot/zero-shot learning capabilities of LLMs, hindering the comprehensive training and evaluation of LLM-based NER methods."}, {"title": "3 GEIC", "content": ""}, {"title": "3.1 Definition", "content": "Typical NER methods regards NER as a sequence labeling task, where each word is assigned a label, usually BIO representation, and use models to predict entity labels by maximizing the conditional probability of the label sequence:\n\n$P(Y|X) = \\prod_{t=1}^{T} P(y_t|X, Y_{1:t-1})$"}, {"title": "3.2 CascadeNER", "content": "To implement GEIC tasks, we propose CascadeNER, a novel framework based on LLMs. A simplified pipeline of CascadeNER with an ex-ample is illustrated in Figure 2. To optimize per-"}, {"title": "3.2.1 Extraction", "content": "In the extraction sub-task, we utilize a generation-based extraction method, where special tokens \"##\" are used to surround any entities identified in the sentence, regardless of the number of entities or their types For example:\nQuery: Kobe was in NBA\nResponse: ##Kobe## was in ##NBA##\nThis method, compared to conventional sequential extraction, avoids requiring LLMs to perform text alignment, thus reducing task complexity. In contrast to similar methods (Wang et al., 2023; Hu et al., 2024b), CascadeNER's query contains only the sentence, without any task descriptions, demonstrations, or category information. The response exclusively uses \"##\" as the identifiers, and all entities are extracted without specifying categories. CascadeNER achieves low-cost NER by using simple prompts and better generalization by treating all entities uniformly. Moreover, extracting in the sentence-level leverages LLMs' self-attention mechanisms for global understanding, improving accuracy in low-context scenarios. A detailed com-parison with existing methods and further advantages of our method are shown in Appendix D.\nAfter conducting extensive experiments, we find that the extractor's precision consistently exceeds recal, regardless of the model or dataset, indicating that while correct entities are effectively identified, there is a tendency for under-detection. To miti-gate this issue, we introduce a union strategy in result fusion (Ganaie et al., 2022), allowing mul-tiple extraction for one sentence and taking the union of the results to maximize recall. For cases of entity nesting, where different extraction rounds produce overlapping or nested entities, we apply a length-first strategy, retaining the longer entity, as longer entities generally carry more semantic meaning (Nguyen and Cao, 2010). For example,"}, {"title": "3.2.2 In-context Classification", "content": "In the classification sub-task, we employ a generation-based in-context classification method, where we input the categories and the sentence with one entity surrounded by \"##\", and require the classifier to generate the label for that entity. This method re-embeds the entity into the sentence for classification, which utilizes the self-attention architecture of LLMs for contextual information and improves accuracy compared to entity-level classification.\nFor multi-granularity data, we apply hierarchical classification, ensuring the accuracy of classifica-tion for complex categories. After obtaining the coarse-grained result, CascadeNER use the result to index the corresponding sub-categories and clas-sify again, continuing this process until no further classification is possible:\n$L_{fine} = f_{fine-classify}(L_{coarse}, subcategories)$"}, {"title": "3.2.3 Few-shot Data Sampling", "content": "In existing datasets, only CrossNER (Liu et al., 2021), designed for low-resource scenarios, and FewNERD (Ding et al., 2021), designed for few-shot scenarios, meet our requirements for evaluat-ing CascadeNER in few-shot scenarios. However, relying solely on them is insufficient for compre-hensively evaluating CascadeNER, particularly its multilingual NER performance.\nTo address this, we develop a sampling algorithm to construct datasets for few-shot evaluation. Con-sidering that basic random sampling cannot ensure a balanced category distribution, we employ a stratified sampling algorithm, which divides the dataset into strata based on the labels. Each stratum cor-responds to a distinct entity type, and we ensure an relatively equal number of samples per category by drawing from these strata, thereby maintaining balance across categories in the results. The size for each stratum is calculated with the formula:\n$s_i = min(\\frac{S}{m}, \\frac{n_i}{N})$"}, {"title": "4 AnythingNER Dataset", "content": "Due to the tremendous parameters and the self-attention mechanism, LLMs demonstrate uniquely high performance in low-resource scenarios for NER tasks. However, there are no existing NER datasets optimized for the characteristics of LLMs, limiting the comprehensive training and evaluation of LLMs in these tasks and making it difficult to effectively evaluate GEIC. Moreover, there is also a lack of multilingual and fine-grained datasets.\nWe thus constructed AnythingNER, the first NER dataset specifically designed for LLM-based NER, particularly GEIC tasks. AnythingNER spans 8 major languages: English, Chinese, Spanish, French, German, Japanese, Korean, and Russian. In terms of classification, it is the first NER dataset to feature three-level granularity categorization, encompassing 8 coarse-grained types, 31 medium-grained types, and 155 fine-grained types, as illustrated in Figure 4.\nCompared to existing datasets, AnythingNER offers several key advantages. It adopts a three-level granularity categorization with 155 categories, ensures comprehensive coverage across diverse do-mains, including rare entities and emerging fields like technology products, which are not adequately considered in earlier datasets. It is specifically formatted to be compatible with GEIC tasks and supports 8 major languages to leverage LLMs' multi-lingual capabilities. Additionally, the dataset en-sures balanced category distribution for few-shot scenarios and applies a novel dynamic categoriza-tion for zero-shot scenarios, making it highly suit-able for current research focuses."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experiment Setting", "content": "Implementation In our experiments, we use Qwen2-1.5B and Qwen2-7B (Yang et al., 2024) as the base models for CascadeNER, as they represent the current best-performing SLMs. We also evaluate Gemma, another prominent SLM, as detailed in Appendix C.2. Based on the dynamic categorized version of AnythingNER, we utilize the SWIFT framework (Team, 2024) to perform supervised fine-tuning (SFT) on the instruct ver-sions of these models. Each model is fine-tuned separately on the corresponding dataset to obtain an extractor and a classifier. In addition to model sets where the extractor and classifier are of the same size, we also experiment with a configuration where the extractor uses the 1.5B model and the classifier uses the 7B model, aiming to balance ac-curacy and efficiency since the classification task is more complex. Due to page constraints, in exper-iments outside of AnythingNER, we consistently use the Qwen2-1.5B model in CascadeNER, em-"}, {"title": "5.2 Results", "content": "AnythingNER The results are shown in Table 1. Notably, since the base models of CascadeNER are fine-tuned with AnythingNER, we only pro-vide zero-shot results. The results indicate that, due to its unprecedentedly detailed categoriza-tion and multilingual coverage, AnythingNER is a extremely challenging flat NER dataset, placing higher demands on methods' generalization. Ad-"}, {"title": "Discussion and Limitation", "content": "The results indicate that when handling NER tasks with ample training resources and simple classifications, LLM-based methods still lag behind existing methods, whether on the English-only CoNLL2003 or the multilin-gual PAN-X. However, in low-resource scenarios, LLM-based methods, whether supervised or few-shot, achieve significantly better results. CascadeNER surpasses existing methods on CrossNER except Music, and FewNERD, highlighting its ex-ceptional generalization and capability to handle complex entity categorization.\nHowever, some challenges remains. Although the GEIC task is designed to be able to accommodate nested and discontinuous NER, CascadeNER is currently limited to flat NER tasks. This limita-tion arises from the fact that the models in Cas-cadeNER are pre-trained on the GEIC version of AnythingNER, and there is a significant lack of suitable multilingual corpora for nested and dis-continuous NER. Our resources are insufficient to collect enough open-source data for this purpose, which lead to AnythingNER containing only flat NER labels, and thus constraining CascadeNER to flat NER. Furthermore, due to resource limi-tations, AnythingNER currently supports only 8"}, {"title": "6 Conclusion", "content": "This paper introduces Generation-based Extraction and In-context Classification (GEIC), a novel task which is designed for applying LLMs in NER tasks, especially few-shot and zero-shot scenerios. The idea of GEIC can benefit a wide range of sce-narios, not only the LLM-based NER tasks, but also inspiring researchers to apply relatively small LLMs in other complex NLP tasks, such as Rela-tion Extraction. To demonstrate GEIC, we propose CascadeNER, the first GEIC framework, and Any-thingNER, the first dataset for LLM-based NER tasks like GEIC. Extensive experiments show that CascadeNER excels in low-resource and complex classification scenarios, demonstrating strong prac-ticality. We hope that our work will inspire further research on the application of LLMs in NLP tasks."}, {"title": "A Dynamic Categorization", "content": "In addition to the BIO and GEIC format with the traditional fixed categorization for AnythingNER,\nwe introduce a GEIC version using dynamic cat-\negorization. Dynamic categorization is a novel\ncategorization method designed for LLM-based\nNER training and evaluation, particularly suited\nfor GEIC. In the dynamic categorized version of\nAnythingNER, we dynamically adjust the labels\nand corresponding entity type list for entities dur-\ning annotation, overcoming the constraints of static\nclassification. By flexibly altering the granular-\nity and numbers of categories during data annota-\ntion, this method effectively trains and evaluates the\nmodel\u2019s generalization. Adjustments include mix-\ning types of different granularities, using type lists\nwithout irrelevant types, replaced types with syn-\nonyms, and and merging certain types into miscella-\nneous/others, as shown in Figure 6. This approach\nrevolutionarily addresses the issue of traditional\ndatasets being unsuitable for few-shot and zero-\nshot training and evaluation, simulating real-world\nscenarios. CascadeNER, trained on this version of\nthe dataset, exhibits significantly improved gener-\nalization ability with extraordinary few-shot and\nzero-shot performance."}, {"title": "B Detail of AnythingNER", "content": "In this section, we briefly present some key met-rics of AnythingNER and compare AnythingNERwith existing datasets. Table 4 presents a simplecomparison between AnythingNER and existingmultilingual or fine-grained datasets."}, {"title": "C Ablation Study", "content": ""}, {"title": "C.1 Result Fusion", "content": "In Section 3.2.1, we introduce our union strategyin result fusion to address the issue of extractor"}, {"title": "C.2 Varying the Base Models", "content": "In this section, we use four different SLMs as thebase models for CascadeNER, i.e., two versions"}, {"title": "C.3 Context in Classification", "content": "In the early stages of our research, the prompt usedfor classification contained only the entity itselfwithout any context.\nFigure 8 provides an examplecomparing the two types of prompts. Although thismethod makes the prompt more concise, it lacksany contextual information.\nOur final in-contextclassification queries significantly improve classifi-cation accuracy, as shown in Table 8."}, {"title": "C.4 Number of Few-shot Demonstrations", "content": "In this section, we conduct experiments to es-timate the effect of the number of demonstra-tions. We compare CascadeNER's performancein the few-shot scenario on the CONLL2003dataset as the number of demonstrations increaseswith the current SOTA method for this dataset,ACE+document-context. The results are shownin Figure 9. While CascadeNER already demon-strate excellent performance in the zero-shot sce-nario, its performance improves further when pro-vided with a certain number of few-shot demonstra-tions. CascadeNER shows a significant advantagein low-resource scenarios, but as the number ofdemonstrations increases, this advantage graduallydiminishes until it is eventually surpassed by a tinydifference."}, {"title": "D Comparsion of Prompt", "content": "In this section, we compare our prompt with twoexisting LLM-based baselines, GPT-NER (Wanget al., 2023) and PromptNER (Ashok and Lipton,2023). These methods are the currently main meth-ods to achieve general NER with LLMs.\nA breifcomparsion is shown in Figure 10."}, {"title": "E Data Contamination Statement", "content": "Given that LLMs are trained on data from diverseand complex sources, there is a possibility thatportions of the evaluation sets may have been en-countered during pre-training.\nHowever, as priorresearch (Chowdhery et al., 2023) indicates, con-taminated data that has been seen during trainingdoes not significantly influence performance.\nThus,we consider this issue negligible."}, {"title": "F Ethical Statement of AnythingNER", "content": "When constructing AnythingNER, we strictly ad-here to existing ethical guidelines (Bender andFriedman, 2018; Gebru et al., 2021; Hovy andSpruit, 2016), ensuring that our data sources andprocessing methods comply with legal and ethi-cal standards while maintaining high-quality anno-tations.\nAll the text in AnythingNER is sourcedfrom Wikipedia, ensuring no violations of privacyor copyright, as Wikipedia is an open-source plat-form with user-contributed content from around theworld.\nDuring data collection and annotation, webalance category distribution to minimize the riskof bias in the model.\nFurthermore, we maintaintransparency by detailing the dataset developmentprocess and data partitioning in this paper, ensuringclarity and reproducibility for future research.\nFor the annotators, each language in AnythingNERis annotated by two junior or higher-level studentsfrom the corresponding language departments atour university.\nDue to the double-blind reviewprocess, the annotators\u2019 identities cannot be dis-closed in this version.\nEach annotator receives ex-tensive training and follows AnythingNER\u2019s multi-granularity classification system to ensure consis-tent and accurate entity annotations across variouslanguages and domains.\nThe annotation process foreach language are divided into two parts equally,with each annotator independently handling onepart.\nAfter the initial annotation, GPT-40 (Ope-nAI, 2023) is utilized for machine review, and theannotators revise their work based on the review re-sults.\nFor ambiguous terms or specialized domainterms, the annotators either collaborate with eachother or consult experts to ensure the accuracy andreliability of the annotations."}, {"title": "G Detailed Categories of AnythingNER", "content": ""}, {"title": "G.1 Person", "content": "Real Person Politician, Artist, Author, Athlete,Director, Actor, Scholar, Military, Musician, Busi-ness Executive, Other Person.\nFictional Figure Mythological Figure, OtherFigure."}, {"title": "G.2 Location", "content": "Geographical Entity Water Body, Mountain, Is-land, Desert, Other Geographical Entity.\nGeo-Political Entity Continent, Country, Stateor Province, City, District, Region, Other GPE."}]}