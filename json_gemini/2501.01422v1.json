{"title": "Multi-Modal Video Feature Extraction for Popularity Prediction", "authors": ["Haixu Liu", "Wenning Wang", "Haoxiang Zheng", "Penghao Jiang", "Qirui Wang", "Ruiqing Yan", "Qiuzhuang Sun"], "abstract": "This work aims to predict the popularity of short videos using the videos themselves and their related features. Popularity is measured by four key engagement metrics: view count, like count, comment count, and share count. This study employs video classification models with different architectures and training methods as backbone networks to extract video modality features. Meanwhile, the cleaned video captions are incorporated into a carefully designed prompt framework, along with the video, as input for video-to-text generation models, which generate detailed text-based video content understanding. These texts are then encoded into vectors using a pre-trained BERT model. Based on the six sets of vectors mentioned above, a neural network is trained for each of the four prediction metrics. Moreover, the study conducts data mining and feature engineering based on the video and tabular data, constructing practical features such as the total frequency of hashtag appearances, the total frequency of mention appearances, video duration, frame count, frame rate, and total time online. Multiple machine learning models are trained, and the most stable model, XGBoost, is selected. Finally, the predictions from the neural network and XGBoost models are averaged to obtain the final result. Our team ultimately achieves first place on the leaderboard. The code, video features extracted by our model and final result are Google Colab, Google Drive and Final Result.", "sections": [{"title": "1. Introduction", "content": "In recent years, short videos have become a dominant content format across various fields, especially in social media. Accurate popularity prediction models help content creators customize their videos, optimize content creation, and enhance viewer engagement. Therefore, understanding and predicting the popularity of short video content is increasingly important. This work aims to predict the popularity of short videos using the videos themselves and their related features (such as release time, number of followers of the author, video description, etc.). Popularity is measured by four key engagement metrics: view count, like count, comment count, and share count. The prediction accuracy is evaluated using mean absolute percentage error (MAPE)."}, {"title": "2. Methodology", "content": ""}, {"title": "2.1. Tabular Feature Engineering", "content": "Table-based data mining and feature engineering are key highlights of this work. The features constructed in this study significantly improve model accuracy compared to the original features. First, the moviepy.editor library is used to extract information such as the duration, total frame count, frame rate, width, and height of each video. We assume that variations in video duration may lead to different viewing and interaction patterns, while quality metrics like frame rate and resolution might affect the viewing experience, thereby influencing the interaction rate. However, damaged videos may fail to provide these features, so the missing values in these features are imputed. Considering that videos posted by the same author may have consistent characteristics, median imputation is applied to maintain the central tendency of the data and reduce the impact of outliers. Specifically, the missing values in both the training and test datasets are filled using the median of these features for each author_id in the training data.\nNext, feature engineering is performed on the video_create_date attribute to capture time- related patterns that may influence interactions. The timestamps are first converted into year, month, day, and hour, and it is determined whether the date falls on a U.S. holiday. Based on the hour, the time is categorized into \"working hours,\" \"leisure hours,\" or \"sleeping hours.\" The hypothesis is that the time of day when a video is posted, as well as whether it is a holiday, could influence user activity and behavior online, thereby affecting the interaction rate. Specifically, the video_create_date is a timestamp, and it is normalized so that the earliest posted video in the dataset has a value of 0, and the most recent has a value of 1. The intuition to construct this feature is that the longer a video has been posted, the more interactions it accumulates.\nConsidering that short video promotion often involves recommendation algorithms that are related to the hashtags and mentions in the video captions, the ideal approach would be to use the TikTok API to obtain hashtag usage counts and mention user counts. However, since our team is based in Australia and is restricted from using the TikTok API due to policy constraints, this study proposes an alternative feature engineering scheme. Specifically, regular expressions are used to extract all hashtags and mentions from the video description text, and the frequency of each"}, {"title": "2.2. Video Feature Extraction", "content": "This work first checks the availability of training set videos, as some of the videos are corrupted to varying degrees. Thus, using the ffmpeg command, errors during video reading are ignored, and the healthy frames are uniformly encoded using more robust H.264 encoding. After preprocessing, only 1588 videos are retained.\nNext, we select four state-of-the-art (SOTA) neural network architectures and training strategies for video feature extraction. The videos are downsampled according to the input requirements of each network, and features are extracted using the pre-trained weights of these models. The four selected models are summarized below:\n\u2022 TimeSformer: A video classification model developed by Meta. Its token acquisition process is similar to ViT, where a fixed number of frames are sampled from the video, and each frame is further divided into small patches. These patches are linearly embedded into high-dimensional feature vectors. The Divided Space-Time Attention mechanism is used to compute temporal and spatial attention separately, replacing the self-attention in the classic Transformer.\n\u2022 ViViT: Developed by Google, this video classification model introduces a method called Tubelet embedding based on the concept of 3D convolution. It fuses temporal and spatial information at the token stage, rather than during the encoder stage like TimeSformer. The Factorised encoder is introduced to replace the Transformer architecture, which uses separate spatial and temporal encoders.\n\u2022 VideoMAE: Developed by Nanjing University and Tencent AI Lab, this video understanding model is based on the Masked Autoencoders architecture. It is a self-supervised learning framework"}, {"title": "2.3. Model Training", "content": "We notice a few videos with very high interaction metrics. While this is reasonable in reality, such videos could lead to large errors during model training. Even applying a logarithmic transformation to the prediction metrics cannot completely eliminate its interference with the training process. Therefore, this study removes those outliers based on the interquartile range (IQR).\nNext, we use the preprocessed data to train ensemble tree models for prediction. The hyperparameters are tuned using Bayesian optimization and ten-fold cross-validation. The tuning results show that the XGBoost model achieves the best MAPE in the tabular modality; see Table 2. We visualize the feature importance of the XGBoost model after training; see Figures 3-6.\nWe also train neural network models to fuse the extracted video features in Section 2.2. Specifically, all input features are first mapped to a unified length through fully connected layers. For features extracted from natural language using BERT, we apply a combination of the GELU activation function and LayerNorm. For features extracted from the video feature extraction models, we apply a combination of ReLU and BatchNorm. After concatenating all vectors mapped to the unified length, the features pass through several sets of linear layers with decreasing neuron numbers and Dropout layers to make predictions. Then, the six sets of feature vectors extracted from the six models mentioned in Section 2.2 are input into the neural network model. We train four neural network models, corresponding to four interaction metrics, using early stopping."}, {"title": "2.4. Results", "content": "By observing the outputs of the neural network and XGBoost, the study finds that the neural network tends to give conservative predictions, likely because its training process is based on the global optimization of MSE Loss. When the training set is right-skewed, making smaller overall predictions is more beneficial for the convergence of the Loss than making larger predictions. On the other hand, XGBoost is based on an ensemble of multiple decision trees, where each leaf node takes the average output. If there are samples with larger labels in a leaf node, it tends to give an exaggerated prediction. Therefore, we average the outputs of both models and find that the MAPE of the new averaged output is lower than that of the previous models; see Table 2. Our team ultimately achieves first place on the leaderboard."}, {"title": "3. Conclusion", "content": "We summarize this study as follows.\n1. In cases where there is a significant magnitude difference in the labels or a highly-skewed distribution, if the sample size cannot support highly accurate regression predictions, neural networks tend to provide conservative predictions, while ensemble tree models tend to give more expansive predictions. Averaging the results can help offset some errors.\n2. This work proposes an effective method for constructing features based on the statistics of Hashtags and Mentions to enhance model performance. According to the feature importance chart, two extracted features\u2014video duration and the time since the video was published are particularly significant. Additionally, removing outliers using the IQR method can significantly improve the regression performance of tree models.\n3. Among all open-source video feature extraction models, X-CLIP performs the best in predicting the popularity of short videos, while LLaVA-NeXT is clearly more suitable for generating text for short video understanding tasks."}, {"title": "4. Team Members", "content": "\u2022 Haixu Liu, the University of Sydney, hliu2490@uni.sydney.edu.au\n\u2022 Qiuzhuang Sun, the University of Sydney, qiuzhuang.sun@sydney.edu.au\n\u2022 Wenning Wang, the University of Sydney, wwan2926@uni.sydney.edu.au\n\u2022 Penghao Jiang, University of New South Wales, pjia0498@uni.sydney.edu.au\n\u2022 Haoxiang Zheng, University of New South Wales, z5574306@ad.unsw.edu.au\n\u2022 Qirui Wang, University of New South Wales, qirui.wang@unilabs.org.cn\n\u2022 Ruiqing Yan, University of New South Wales, ruiqing.yan@unilabs.org.cn"}]}