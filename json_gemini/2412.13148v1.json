{"title": "SWAN: PREPROCESSING SGD ENABLES ADAM-LEVEL PERFORMANCE ON LLM TRAINING WITH SIGNIFICANT MEMORY REDUCTION", "authors": ["Chao Ma", "Wenbo Gong", "Meyer Scetbon", "Edward Meeds"], "abstract": "Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the success of large language models. However, they maintain additional moving average states throughout training, which results in memory requirements several times greater than the model. This overhead imposes constraints on scalability and computational efficiency. On the other hand, while stochastic gradient descent (SGD) is optimal in terms of memory efficiency, their capability in LLM training is limited (Zhao et al., 2024b).\nTo address this dilemma, we show that pre-processing SGD is sufficient to reach Adam-level performance on LLMs. Specifically, we propose to preprocess the instantaneous stochastic gradients with two simple operators: GradNorm and GradWhitening. GradNorm stabilizes gradient distributions, and GradWhitening counteracts the local curvature of the loss landscape, respectively. This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any accumulative state variables. Empirically, SWAN has the same memory footprint as SGD, achieving \u2248 50% reduction on total end-to-end memory compared to Adam. In language modeling tasks, SWAN demonstrates the same or even a substantial improvement over Adam. Specifically, when pre-training the LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation perplexity in less than half tokens seen.", "sections": [{"title": "1 INTRODUCTION", "content": "Adaptive optimizers, such as Adam and its variants (Kingma & Ba, 2015; Loshchilov & Hutter,\n2019; Shazeer & Stern, 2018; Pagliardini et al., 2024; Liu et al., 2023; Zhao et al., 2024a), have\nbeen central to the success of training large language models (LLMs) (Brown et al., 2020; Touvron\net al., 2023; Dubey et al., 2024; Bi et al., 2024; Bai et al., 2023; Zhang et al., 2022). However,\nmost adaptive optimizers for LLMs are stateful, meaning they require tracking and maintaining\ninternal states. While achieving remarkable empirical success, these states introduce significant\nmemory overhead. For instance, Adam (Kingma & Ba, 2015)\u2013the de facto optimizer for LLM\ntraining-involves the tracking of exponential moving averages (EMAs), effectively doubling memory\nrequirements. AdEMAMix (Pagliardini et al., 2024) \u2013 an extension of Adam that achieves significant\nconvergence speed boost \u2013 requires storing even more states, tripling the memory requirements.\nThis overhead can be significant especially in distributed settings, where the optimizer states could\nconsume a significant amount of the GPU memory (Dubey et al., 2024; Korthikanti et al., 2023). On\nthe other hand, while stochastic gradient descent (SGD) is optimal in terms of memory efficiency (i.e.,\nthey are stateless), their capability to train LLMs is limited (Zhao et al., 2024b), and the performance\ngap between SGD and Adam is significant (Zhang et al., 2020; Kunstner et al., 2023; 2024).\nTo address the memory-performance dilemma for LLMs, this work questions the requirement of\naccumulative state variables during training to achieve good performance in LLMs. We propose"}, {"title": "2 PRELIMINARIES", "content": "One of the most used adaptive optimizers is the Adam optimizer (Kingma & Ba, 2015). Adam is an\nexample of stateful optimizers, which involves tracking and maintaining internal states. It combines\nthe advantages of two earlier methods: AdaGrad (Duchi et al., 2011), which adapts learning rates\nbased on the historical gradient magnitude, and RMSProp (Tieleman, 2012), which mitigates the\naggressive decrease in learning rates by using a decaying average of squared gradients.\nConsider a loss function $\\mathcal{L}_w : \\mathcal{X} \\rightarrow \\mathbb{R}$, parameterized by weight matrices $W \\in \\mathbb{R}^{m\\times n}$, and denote\n$\\mathbf{x}^{(t)}$ a mini-batch of inputs provided at the t-th training step that is sampled from data distribution\n$\\mathbb{P}_{data}(\\mathbf{x})$. Let $\\mathcal{G}^{(t)}$ be the stochastic gradient of $\\mathcal{L}_w$ (i.e., a random variable induced by sampling"}, {"title": "3 THE SWAN OPTIMIZER: PREPROCESSING SGD WITH NORMALIZATION\nAND WHITENING", "content": "As discussed in Section 2, we believe the key to designing stateless, adaptive, and effective optimizers\nlies in designing new matrix-level operations that exploit rich information in the gradient matrix.\nTo this end, we propose a new stateless optimizer (Algorithm 1), SWAN (SGD with Whitening\nAnd Normalization). SWAN preprocesses SGD based on two simple operators, GradNorm and\nGradWhitening, both are stateless, matrix-level operators. When applied in tandem, they achieve\nsimilar desirable properties of adaptive optimizers, without the need to store historical gradient\nmoments. In this section we describe the update rules of SWAN."}, {"title": "3.1 SWAN UPDATE RULES", "content": "In SWAN (Algorithm 1), the raw SGD gradient $\\mathcal{G}_t$ is processed by the following operations 1:\nHere G is assumed to be the gradient matrix for some parameter block in the model (e.g. a linear layer);\nenabling us to take advantage of the matrix structure to achieve our smoothing and whitening goals."}, {"title": "4 ANALYSIS: A LLM LEARNING DYNAMICS PERSPECTIVE", "content": "As a new stateless adaptive optimizer, the complete theoretical properties of SWAN is an open\nquestion which we leave for future work. However, as a first analysis, we consider SWAN from\na learning dynamics perspective, specifically the dynamics of an LLM based upon a simplified\ntransformer block. It is this analysis that led to the design of SWAN."}, {"title": "4.1 SETUP", "content": "We assume the following simplified transformer block (STB) architecture recently proposed in Tian\net al. (2023):\nDefinition 1 (Simplified Transformer Block (STB)). Given the input activation $\\mathbf{x} \\in \\mathbb{R}^{M_c\\times 1}$, query\ntoken index $q$, context embedding matrix $U_c \\in \\mathbb{R}^{d \\times M_c}$, and the query embedding $u_q \\in \\mathbb{R}^{d\\times 1}$,\nthe STB computes the output $\\mathbf{h} \\in \\mathbb{R}^{n\\times 1}$ as $\\mathbf{h} = \\varphi\\left(\\mathbf{W}^T \\left(U_c \\left(exp\\left(z_q\\right) \\mathbf{x}\\right) + u_q\\right)\\right)$, where $M_c$\nis the context length, the attention logits $z_q \\in \\mathbb{R}^{M_c\\times 1}$ are given by $z_{ql} = \\mathbf{u}_{ql}^\\top \\mathbf{W}_Q \\mathbf{W}_K \\mathbf{u}_c$, with\n$\\mathbf{W}_Q, \\mathbf{W}_K \\in \\mathbb{R}^{d\\times d}$ being weight matrices for the queries and keys, respectively, $\\mathbf{W} \\in \\mathbb{R}^{d\\times n}$ is the\nweight matrix for the feedforward network, and $\\varphi$ is a nonlinearity function such as the ReLU.\nGiven a STB, we consider a loss function $\\mathcal{L}_{w,z}(\\mathbf{x}^{(t)})$, where $\\mathbf{x}^{(t)}$ is a mini-batch of inputs provided at\nthe t-th training step sampled from data distribution $\\mathbb{P}_{data}(\\mathbf{x})$. Standard mini-batch learning dynamics\nis then given by\n$\\hat{\\mathbf{W}}^{(t)} = \\mathbb{E}_{q=m}\\left[\\frac{\\partial \\mathcal{L}_{w,z_q}(\\mathbf{x}^{(t)})}{\\partial \\mathbf{W}} z_{i q}^{(t)} - z_{i q}^{(t)} =  \\mathbb{E}_{q=m}\\left[\\frac{\\partial \\mathcal{L}_{w,z_q}(\\mathbf{x}^{(t)})}{\\partial z_q}\\right]$.\nIn this case, both $\\hat{\\mathbf{W}}^{(t)}$ and $z_{i q}$ are viewed as random variables induced by random mini-batch $\\mathbf{x}^{(t)}$.\nFor example, for each row $i$, $\\hat{\\mathbf{W}}^{(t)}[i, :] $ can be re-written as $\\hat{\\mathbf{W}}^{(t)}[i, :] = \\mathbb{E}\\left[\\hat{\\mathbf{W}}^{(t)}[i, :]\\right] + \\epsilon_W[i, :]$,\nwhere $\\epsilon^{(t)}[i, :] $ is zero mean random variable with covariance $Cov[\\hat{\\mathbf{W}}^{(t)}[i, :]]$."}, {"title": "4.2 GradNorm STABILIZES GRADIENT DISTRIBUTIONS OF SIMPLIFIED TRANSFORMERS", "content": "As discussed in Section 3, GradNorm contains rich information of the gradient scaling while offering\ninvariance properties over certain transformations. Here we show that, based on the dynamics of the\nSTB, GradNorm also stabilizes $\\epsilon_W^{(t)}$:\nTheorem 1 (GradNorm stabilizes gradient distributions across time for the STB). Consider the STB\n(Definition 1). Assuming we inherit the assumptions in Theorem 1 of Tian et al. (2023), as described in\nAppendix B. Then consider $U_\\mathbf{W}$, the composition of the MLP project-up matrix and the embedding\nmatrix as a whole. Then, its standardized stochastic gradients $\\tilde{\\mathcal{G}}_{U\\mathbf{W}} := GradNorm\\left(\\frac{\\partial \\mathcal{L}_{w,z}(\\mathbf{x}^{(t)})}{\\partial U\\mathbf{W}}\\right)$ satisfy:\n$Cov[\\tilde{\\mathcal{G}}_{U\\mathbf{W}}[i, :](t_1)] = Cov[\\tilde{\\mathcal{G}}_{U\\mathbf{W}}[i, :](t_2)]$ for all $t_1, t_2$, and $i$.\nIn other words, the covariance structure of $\\tilde{\\mathcal{G}}_{U\\mathbf{W}}$ is identical across all time steps $t$, achieving distri-\nbutional stability across time. The same relationship also holds for the gradient of attention score\n$\\tilde{G}:=GradNorm\\left(\\frac{\\partial \\mathcal{L}_{w, z_q}(\\mathbf{x}^{(t)})}{\\partial z_q}\\right)$.\nTheorem 1 suggests that GradNorm implicitly aligns with the dynamics of transformer architectures\nand removes the time-heterogeneity in gradient covariance structures."}, {"title": "4.3 GradWhitening IS AN EFFICIENT NON-DIAGONAL SECOND-ORDER UPDATE", "content": "In this section, we show that GradWhitening is equivalent to non-diagonal approximate second-\norder method, under a specific Kronecker factorization assumption of the Hessian. The assumption is\nas below:\nAssumption 1 (Assumption of GradWhitening). At time t, the local Hessian H of the loss has\nshared block-diagonal structure, such that $\\mathbf{H} = I_{n \\times n} \\otimes \\hat{H}$, where $\\hat{H} \\in \\mathbb{R}^{m \\times m}$."}, {"title": "5 EXPERIMENTS", "content": "In this section, we perform empirical studies on SWAN. The main goals of those studies are as\nfollows:\nVerifying whether GradNorm and GradWhitening indeed effectively fulfilled their design goals\n(stabilizing gradient noise, and modeling/counteracting local curvatures). This is performed in\nSection 5 and Section 5.\nVerifying whether SWAN can consistently offer Adam-level or even better performance, while\nmaintaining low memory cost and high-throughput. This is performed in Section 5, Section 5 and\nSection 5.5, respectively.\nAblation study (Section 5.6).\nAll experiments run on NVIDIA A100 GPUs."}, {"title": "5.1 DOES GradNorm STABILIZE GRADIENT DISTRIBUTIONS OF SGD?", "content": "To investigate whether GradNorm indeed stabilizes stochastic gradient distributions as claimed in\nTheorem 1, we conduct a series of controlled experiments using a scaled-down version of the LLaMA\narchitecture. We aim to assess the impact of GradNorm on the gradient distribution during training\nby analyzing the statistical properties of gradients across multiple training steps. Specifically, we\nemploy a small-scale LLaMA-based model with approximately 10 million parameters (Lialin et al.,\n2023). Training is conducted on the C4 dataset (Raffel et al., 2020).\nBaselines We compare the following methods:"}, {"title": "5.2 DOES GradWhitening COUNTERACTS LOCAL CURVATURE AND PROVIDE FAST\nCONVERGENCE ON ILL-CONDITIONED PROBLEMS?", "content": "We aim to evaluate the pure optimization capability of GradWhitening processed gradient descent\non classic optimization problems. We consider the following three settings:\nHigh-dimensional quadratic optimization. We construct quadratic optimization problem\nof the form in Equation (23), with $W \\in \\mathbb{R}^{50 \\times 50}$.\nIll-conditioned quadratic optimization. We construct a quadratic optimization problem\nwith the same dimensionality and formulation as above, but this time we deliberately choose\nan ill-conditioned H.\nNon-convex optimization with multiple local optimas. We consider the following multi-\nvariate Rastrigin function:\n$f(W) \\triangleq \\frac{1}{2} m^2 A + \\frac{1}{2} Tr[W^T W] - A \\sum_i \\sum_j cos(2 \\pi W_{ij})$\nwhere $W$ is $m \\times m$ matrix. This function has $10^{m^2}$ possible local optima. This is a\ncomplicated stress test for optimization algorithms, where many methods are expected to\nfail at different regimes. We set $m = 50$, and compare the same methods as in the previous\nexperiment.\nBaselines On all three settings (quadratic, quadratic ill-conditioned, and Rastrigin) we compare\n5 different methods, including 3 baselines and 2 GradWhitening based variants. For baselines,\nwe have: gradient descent (with theoretical optimal learning rate in Theorem 4); Adam ($\\beta_1 = 0.9$,\n$\\beta_2 = 0.999$ with hand-tuned learning rate); and Newton's method with tuned learning rate. For 2"}, {"title": "5.3 SWAN PERFORMANCE ON LLM PRE-TRAINING TASKS", "content": "Setup Finally, we present results on the task of pre-training large language models. Below we\nconsider LLaMa architecture used in Zhao et al. (2024a) with RMSNorm and SwiGLU activations.\nWe consider sizes of 60M, 130M, 350M and 1.3B parameters on C4 dataset Raffel et al. (2020),\ntrained with an effective batch size of 130K tokens (512 batchsize and 256 context length). SWAN is\napplied to all linear modules (in both attention and MLP layers). We run all experiments with BF16\nformat by default."}, {"title": "5.4 IS THE IMPROVEMENT MULTIPLICATIVE OR ADDITIVE?", "content": "One natural question when evaluating speedup factors is whether our method provides a multiplica-\ntive or additive speedup over Adam. Specifically, multiplicative speedup indicates an optimizer's\nperformance advantage over a baseline increases proportionally with time, maintaining a consistent\nspeed-up ratio. This is the ideal scenario we would like to achieve. In contrast, additive speed-up\nimplies only a fixed, constant advantage in steps over baseline, which is less desired. To investigate\nthis, we utilize two plots: the speed-up ratio comparison and the perplexity comparison (Figure 7),\nacross different model sizes. Before understanding those plots, we first introduce the following\nconcepts:\nSpeedup ratio definition The speedup ratio $\\mathcal{R}(P)$ is defined as the ratio of the number of training\nsteps Adam requires to reach a specific evaluation perplexity (PPL) to the number of steps SWAN\nrequires to achieve the same PPL. Mathematically, for a given PPL threshold $P$, the speedup ratio is\nexpressed as:\n$\\mathcal{R}(P) = \\frac{\\mathcal{S}_{Adam}(P)}{\\mathcal{S}_{SWAN}(P)}$"}, {"title": "5.5 \u039c\u0395\u039cORY EFFICIENCY AND THROUGHPUT ANALYSIS", "content": "This section, we benchmark the memory footprint of SWAN as well as its throughput.\nMemory footprint We benchmark the memory footprint of SWAN, Adam, and Galore in a practical\nscenario.. Unlike Zhao et al. (2024a) where memory is benchmarked under a layer-wise training\nstrategy, we directly measure end-to-end memory under full-model training, with batch size = 1\nsequence, across different model sizes (1.3B, 7B, and 13B). Results are shown in Figure 1 (c), SWAN\nachieves near-theoretical limit of optimizer memory efficiency of full-parameter training (assuming\nno quantization). This means that SWAN can reproduce the memory footprint of vanilla SGD,\nreaching \u2248 50% reduction on total memory, and \u2248 100% reduction on optimizer states. This result\nagain highlights the advantage of the stateless design.\nEffective throughput To measure the throughput of SWAN, we consider training a 1.3 B model\non 4 x A100, under constant batch size = 130K tokens. Here We present two metrics: absolute\nthroughput, measured by number of training tokens consumed per second; and effective throughput,\nwhich is absolute throughput adjusted by the efficiency of optimizer relative to Adam. The purpose\nof the first metric (absolute throughput) is to measure whether SWAN (especially the use of N-S\niterations in GradWhitening) create a significant drop in training throughput; and the second metric\n(effective throughput) takes into account the fact that different optimizers utilizes training tokens at\ndifferent efficiency level. Results are shown in Figure 1 (d), SWAN closely reproduces the absolute\nthroughput of Adam, implying that unlike full SVD used in (Zhao et al., 2024a), the computational"}, {"title": "5.6 ABLATION STUDIES", "content": "How does GradNorm and GradWhitening contribute to the performance boost? We consider\nthe following 6 ablation settings: SWAN(full), SWAN (GradNorm only), SWAN (GradWhitening\nonly), Adam (full), Adam (momentum only), and Adam (second moment only). Results are shown\nin Figure 8 (a). It suggests that both GradNorm and GradWhitening makes significant contribution\ntowards the final performance of SWAN; and removing any of them results in suboptimal performance.\nThis is similar to how the first and second moment EMAs of Adam jointly contributes to the final\nperformance.\nIs it true that SWAN works only because its GradNorm component might have increased the\neffective learning rates? To answer this question, we remove the GradNorm component of SWAN,\nand perform learning rate sweep for it. We start from the default base learning rate used by full\nSWAN, and then scale it with multipliers ranging from 1 to $10^3$. Results are shown in Figure 8 (b),\nwhere the final validation ppl of SWAN without GradNorm under different learning rate multipliers\nare shown in blue line; and the performance of the full SWAN under default learning rate is shown\nas a dashed red reference line. The result suggests that, although increasing the effective learning\nrates might improve the performance of SWAN without GradNorm, there is still a significant cap\nbetween between SWAN without GradNorm and the full SWAN. Together with results in Section 5,\nwe can conclude that the gradient noise stabilization of GradNorm is indeed essential, and it cannot\nbe trivialized as larger effective learning rates.\nHow does warm-up affect the performance? In Section 5, we showed that SWAN does not need\nany learning rate warm-ups and can train with large learning rates (the default 0.001 learning rate\nused by all methods is considerably large in LLMs settings). Here we perform ablation and compare\nAdam and SWAN both under warm-ups and not warm-ups. As suggested by the results in Figure 8 (c),\nwe found that for SWAN, training without learning rate warm-ups indeed gives better performance.\nMeanwhile, SWAN still outperforms Adam (with warm-up) under the same warm-up schedule of\nAdam. On the contrary, Adam only works with proper learning rate warmup. If we remove warmups,\nthe performance of Adam becomes drastically worse. This confirms our claim that SWAN is more\nrobust to the local geometries of the optimization problems and can work without (and with) learning\nrate warm ups."}, {"title": "6 RELATED WORKS", "content": "Low-rank memory efficient methods Low-rank memory-efficient optimizers have become pivotal\nin training large language models (LLMs) by reducing memory consumption while maintaining\nmodel performance. A foundational approach in this domain is LORA introduced by Hu et al. (2021),\nwhich fine-tunes pre-trained models using low-rank adaptors for each layer, thereby minimizing the\nmemory footprint. This leads a full line of low-rank adaptation research, but they are mostly focused"}, {"title": "7 CONCLUSION", "content": "The most important insight that we conclude for the paper is, preprocessing SGD might already be\nenough for LLM training tasks. This presents an elegant solution to address the memory-performance\ndilemma of training LLMs using adaptive optimizers like Adam. The other insight comes from the\nfact that the key components of the SWAN optimizer were designed using key insights from LLM\ndynamics under a simplified setting. In that sense, this paper serves as a proof-of-concept on the\nvalue of studying the dynamics of LLM architectures when designing optimizers. We hope both\nresearch insights can inspire more research alone this line to further advance this field."}, {"title": "A DESIRED PROPERTIES OF ADAPTIVE OPTIMIZERS", "content": "There is a rich literature on understanding adaptive methods' inner workings and unreasonable\neffectiveness. Using Adam as an example, we first summarize from the literature below the key\ndesired properties of stateful adaptive optimizers that contribute to their empirical success: gradient\nsmoothing, gradient invariance, and gradient whitening. Then we discuss how these understandings\nwill leads to the design of stateless adaptive optimizers."}, {"title": "B THEORETICAL ANALYSIS", "content": "B.1 ANALYZING THE GRADNORM: A TRANSFORMER LEARNING DYNAMICS PERSPECTIVE\nWe take the transformer LLM architecture as an example, and demonstrate that GradNorm effectively\nremoves the time-variant components of gradient noises. For our analysis, we assume the following"}, {"title": "C PROOF OF THEOREM 1", "content": "Proof. We first consider the noiseless, full batch dynamics. Define V \u2208 RMc\u00d7n as V := UW.\nThen following Theorem 2 in Tian et al. (2023), each column of V satisfies the following differential\nequation:\n$V^{[j]}[:] = \\mathcal{G}^{(t)}_\\infty([\\sqrt{2}]+\\mathcal{C})\\odot \\mathbb{E}_q[gh^{(t)};x]$\nThe corresponding dynamics of attention score is given by:\n$z_q = \\frac{1}{2} \\Sigma \\mathcal{V}V_{[i,:]}$"}, {"title": "D PROOF OF THEOREM 3", "content": "Proof. We first show that \u2207L(W(0)) = HW(0) (and hence \u2207L(Whitened) with t \u2260\u221e) are non-\nzero with probability 1 under Assumption of the theorem. Given \u2207L(W(0)) = HW(\u00b0), the set\nof matrices W(0) such that Tr(HW(0)) = 0 forms a hyperplane in the space of d \u00d7 d matrices.\nSpecifically, it is defined by the linear equation: Tr(HW(0)) = 0. Since H is positive definite, at\nleast one entry of H is non-zero. Thus, the hyperplane Tr(HW(0)) = 0 has zero Lebesgue measure\nin the space of d \u00d7 d matrices. Given that W(0) is sampled from a continuous distribution, the\nprobability that Tr(HW(0)) = 0 is zero. Therefore, \u2207L(W(0)) \u2260 0 (and hence \u2207L(Whitened) with t\u2260\u221e) with probability 1.\nNext, we define the cost-to-go as:\n$\\mathcal{L}(\\mathcal{W}^{(t)}) - \\mathcal{L^*} = \\frac{1}{2} tr[(\\mathcal{W}^{(t)})^T H \\mathcal{W}^{(t)}]$\nand the per-step improvement is (since $\\mathcal{L^*} = 0$ under $\\mathcal{W} = 0, )$:\n$\\mathcal{L}(\\mathcal{W}^{(t)}) - \\mathcal{L^*} = \\frac{1}{2} tr[(\\mathcal{W}^{(t)})^T H \\mathcal{W}^{(t)}] - \\frac{1}{2} tr[(\\mathcal{W}^{(t)})^T H \\mathcal{W}^{(t)}]$\nSubstituting the update rule $\\mathcal{W}^{(t+1)} = \\mathcal{W}^{(t)} - \\eta Graudnorm( \\mathcal{G^{(t)}}, 1)=\\mathcal{W}^{(t)} - \\eta \\mathcal{U\\Sigma} \\mathcal{V}^T$\nwe get:\n$\\mathcal{L}(\\mathcal{W}^{(t)}) - \\mathcal{L^*} = \\frac{1}{2} tr[(\\mathcal{W}^{(t)})^T H \\mathcal{W}^{(t)}] - \\frac{1}{2} tr[(\\mathcal{W}^{(t)} - \\mathcal{U\\Sigma} \\mathcal{V}^T)^T H (\\mathcal{W}^{(t)} - \\mathcal{U\\Sigma} \\mathcal{V}^T)]$\nExpanding the right-hand side, we have:\n$\\mathcal{L}(\\mathcal{W}^{(t)}) - \\mathcal{L^*} = \\frac{1}{2} tr[(\\mathcal{W}^{(t)})^T H \\mathcal{W}^{(t)}] - \\frac{1}{2} tr[(\\mathcal{U\\Sigma} \\mathcal{V}^T)^T H (\\mathcal{U\\Sigma} \\mathcal{V}^T)]$\nNow, noticing that $\\mathcal{G} = H \\mathcal{W}^{(t)} = \\mathcal{U\\Sigma} \\mathcal{V}^T$\nthen we have:\n$\\mathcal{L}(\\mathcal{W}^{(t)}) - \\mathcal{L^*} = \\frac{1}{2} tr[(\\mathcal{W}^{(t)})^T H (\\mathcal{W}^{(t)}] - tr[ (\\mathcal{W}^{(t)})^T H \\mathcal{U\\Sigma} \\mathcal{V}^T] - \\frac{1}{2} tr[(\\mathcal{U\\Sigma} \\mathcal{V}^T)^T H (\\mathcal{U\\Sigma} \\mathcal{V}^T)] \\frac{1}{2} tr[\\mathcal{V} \\Sigma \\mathcal{U}^T \\mathcal{U\\Sigma} \\mathcal{V}^T]$\n\\newline $tr[V \\Sigma \\mathcal{U}^T \\mathcal{U\\Sigma} \\mathcal{V}^T]$\n\\newlinetr[[\\mathcal{W}^{(t)})^T H (\\mathcal{W}^{(t)}]"}, {"title": "E PROOF OF PROPOSITION 2", "content": "Proof. Since V is orthogonal, VTV = I, and \u2211 is diagonal, we obtain:\n$\\frac{1}{2} tr[(\\mathcal{U} \\Sigma V^T)^T H (\\mathcal{U \\Sigma} V^T)] =tr[\\Sigma] = ||H \\mathcal{W}||_1$\nSimilary:\n1-\\mathcal{L}(\\mathcal{W^{(t)}}- \\mathcal{L^*}-1\ntr[(((\\mathcal{W})()^ H(W)|||(W)H (W) [((()^(W)]]||1"}, {"title": "F PROOF OF PROPOSITION 3", "content": "To prove Proposition 3, we first generalize existing work on the convergence rate lower bound (via\ncontraction factor) of gradient descent and Adam (we only consider $\\beta_2 = 1$) under the same setting:\nTheorem 4 (Contraction factor lower bound for gradient descent, generalized based on Zhang et al.\n(2024)). Consider the optimization problem in Equation (12). Let W GD_k be the output of GD after k\nsteps. Then, for any step size n, there exists an initial condition such that the following lower bound\non the contraction rate holds:\n$\\mathcal{L}(\\mathcal{W}_{i+1}^{GD})- \\mathcal{L^*} = \\frac{k - 1}{2(\\kappa+1}$"}, {"title": "G PROOF OF PROPOSITION 1", "content": "Proof. First, define V \u2208 RMc\u00d7n as V := UW, and consider the Hessian with respect to V\ninstead of W. Notice that although the loss function L is unknown, its first-order derivatives are\nknown. Specifically, they are given by:"}, {"title": "H PROOF OF THEOREM 2", "content": "Proof. Assume that we have a function f (W) defined on W \u2208 [Rm\u00d7n that we want to minimize", "as": "nmin  \\mathcal{F"}, "w)(22)\nAssume f is a twice differentiable function and has a Lipschitz continuous Hessian around W*, then\nboth GradWhitening and Newton's method approximate the loss equation 22 up to second order\n(ignoring first-order terms without loss of generality) as:\nmim /wew -HW (23)\nwhere w = Rmxntl is the flattened version of W, and H Rmxm is the Hessian matrix of &at W*.\nWe may apply second-order Newton's method to optimize equation 23 which gives the followingupdate:\nwe+iwe -MH-F (w24)\nHowever, this update is very expensive and requires O(men) computation. SWANcan then beviewed as a fast approximation to estimate the update of equation 24. Under our assumption on\nHessian structure, we have:H=diag(a.... A 25\nUnder this assumption the optimization problem equation (2) can be rewritten asimin A HW (26"]}