{"title": "ALIGNED VECTOR QUANTIZATION FOR EDGE-CLOUD COLLABRATIVE VISION-LANGUAGE MODELS", "authors": ["Xiao Liu", "Lijun Zhang", "Deepak Ganesan", "Hui Guan"], "abstract": "Vision Language Models (VLMs) are central to Visual Question Answering (VQA) systems and are typically deployed in the cloud due to their high computational demands. However, this cloud-only approach under-utilizes edge computational resources and requires significant bandwidth for transmitting raw images. In this paper, we introduce an edge-cloud collaborative VQA system, called LLaVA-AlignedVQ, which features a novel Aligned Vector Quantization algorithm (AlignedVQ) that efficiently compress intermediate features without compromising accuracy to support partitioned execution. Our experiments demonstrate that LLaVA-AlignedVQ achieves approximately 1365\u00d7 compression rate of intermediate features, reducing data transmission overhead by 96.8% compared to transmitting JPEG90-compressed images to the cloud. LLaVA-AlignedVQ achieves an inference speedup of 2-15\u00d7 while maintaining high accuracy, remaining within -2.23% to +1.6% of the original model's accuracy performance across eight VQA datasets, compared to the cloud-only solution.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual Question Answering (VQA) systems allow machines to respond to natural language questions based on visual inputs. They have numerous practical applications, including assistive technologies for visually impaired individuals, real-time video analytics, and human-computer interaction (Singh et al., 2023). Recent advancements in Vision Language Models (VLMs) (Alayrac et al., 2022; Li et al., 2023a; Liu et al., 2023) have greatly enhanced VQA capabilities, making VLMs the core component of these systems. A typical VLM architecture includes a transformer-based vision encoder for processing visual inputs, a visual-text alignment adaptor that maps visual features to the textual domain for improved joint understanding, and a Large Language Model (LLM) that generates the answer.\nVLMs are typically deployed in the cloud due to their substantial computational requirements. For instance, LLaVA, a leading VLM, uses CLIPViT-large-336 as visual encoder, which demands approximately 167.45 GMACs per forward pass, making it challenging to run efficiently on resource-constrained edge devices. While using a smaller vision encoder could reduce computational needs, this often results in significant performance degradation, especially for tasks requiring precise visual-text alignment (Lin et al., 2024; Liu et al., 2024b). This problem is compounded by the much more intensive resource requirements of LLaVA's LLM component.\nThis cloud-only approach, however, has two main limitations. First, it under-utilizes the computing power of edge devices, especially those already equipped with AI capabilities. Second, it requires transmitting raw images to the cloud, assuming high bandwidth availability to achieve low inference latency. Compression methods like JPEG can alleviate this issue by reducing transmission overhead but compromise task accuracy in the process. As compression rates increase to fit lower transmission bandwidths, accuracy declines significantly. \nIn this paper, we address these limitations with a partitioned execution approach, where the initial layers of the VLM's visual encoder run locally when edge resources permit, while the remaining layers of the visual encoder and the other VLM components execute in the cloud. This partitioned execution allows the VQA system to shift some processing from the cloud to edge devices, reducing strain on cloud infrastructure and associated costs. It can also benefit from task-aware compression on intermediate features from the visual encoder, minimizing transmission overhead and po-"}, {"title": "2 BACKGROUND AND MOTIVATIONS", "content": "In this section, we provide the necessary background on Vector Quantization (VQ), which AlignedVQ builds on, and motivate our approach."}, {"title": "2.1 Vector Quantization (VQ)", "content": "VQ compresses high-dimensional data by mapping continuous feature vectors to a discrete set of values (Gersho & Gray, 2012). We first introduce Vanilla VQ and then its variants Residual VQ and Grouped VQ.\nVanilla VQ partitions the feature space into clusters, where each data point is assigned to its nearest cluster centroid. Let the codebooke \u2208 RK\u00d7D represents the set of centroids, where K is the number of centroids and D is the feature dimension. We define VQ with the codebook e as Qe. For an input vector z \u2208 RD, VQ maps z to a discrete index i by selecting the nearest centroid in the codebook,\ni = arg \\min ||e_i - z||_2.\nThus, the quantization operation is represented as Qe(z) = e_i, where e_i is the nearest centroid. This process reduces data size by transmitting only the index i rather than the full feature vector z. To obtain the trained codebook e in VQ, we can perform K-means clustering on a set of embeddings Z\u2208 RNXD in the feature space, using the centroids of these clusters as the codebook entries. These centroids can also be updated online with an exponential moving average strategy (Van Den Oord et al., 2017)."}, {"title": "2.2 Motivations and Challenges", "content": "In this work, we explore the application of VQ in compressing intermediate features of the vision encoder in pretrained Vision-Language Models (VLMs). It is motivated by the observation that VQ can dramatically lower the data size to transmit with a low computation overhead.\nAssume that the vision encoder uses a transformer architecture, the size of the intermediate feature from a transformer block is [B, N, C'], where B is the batch size, N is the num-"}, {"title": "3 DESIGN OF ALIGNEDVQ", "content": "We propose AlignedVQ, a lightweight and effective compression algorithm designed to mitigate accuracy loss while retaining the compression benefits of Vanilla VQ. By applying AlignedVQ to compress intermediate features from the vision encoder in the pretrained VLM, the partitioned model achieves lower inference latency thanks to reduced data transmission overhead, while preserving high accuracy compared with cloud-only solutions."}, {"title": "3.1 Overview", "content": "Figure 3 illustrates the architecture and workflow of an VLM with the proposed AlignedVQ for efficient edge-cloud collaborative execution. The VLM consists of a vision encoder equipped with an AlignedVQ module, a projector for modality alignment, and a language model that interprets question embeddings and generates language responses. AlignedVQ allows the visual encoder to be partitioned at any transformer blocks, enabling flexibly shifting variable amount of VQA processing on edge devices. The overall workflow is divided into two main stages, training and deployment. The key design features of AlignedVQ are detailed in Section 3.2.\nTraining Stage. The offline training stage prepares the VLM model for partitioned execution by fine-tuning the AlignedVQ module to adapt to the specific VQA dataset. AlignedVQ employs a codebook that maps continuous fea-"}, {"title": "3.2 The AlignedVQ Algorithm", "content": "We now detail the three key design features of AlignedVQ that enable it to achieve a high compression rate while preserving task accuracy."}, {"title": "3.2.1 Normalized Feature Compression", "content": "AlignedVQ partitions the visual encoder of a VLM by inserting Vector Quantization (VQ) immediately after the normalization layers within a transformer block. This design choice is motivated by a fundamental connection between"}, {"title": "3.2.2 Dual Linear Projection", "content": "AlignedVQ includes Dual Linear Projection (DLP) to ad-"}, {"title": "3.2.3 Training Strategy", "content": "To further enhance task accuracy, AlignedVQ fine-tunes the LORA parameters in the LLM of a VLM alongside the VQ codebook and the DLP layers. Mathematically, in a neural network represented by a mapping function F(x) where VQ is applied to quantize the intermediate features Z, a commitment loss (Van Den Oord et al., 2017) is introduced to encourage Z to move closer to its corresponding centroid. The overall loss function for finetuning the trainable parameters is:\nL = \\hat{L}(F(x)) + \\beta||Z - sg(Qe(Z))||_2,\nwhere \\hat{L} is the task loss, sg represents the stop gradient operation, and \u1e9e is a scaling factor for the commitment loss. In Section 4.5, we demonstrate that fine-tuning the LoRA parameters further enhances model's performance, with a notable accuracy boost of 0.50-0.61%."}, {"title": "3.3 The LLaVA-AlignedVQ Implementation", "content": "We implement AlignedVQ on top of the LLAVA-1.5 model (Liu et al., 2024a), a state-of-the-art VLM designed for VQA tasks with outstanding performance. It employs a pretrained CLIPVIT (Radford et al., 2021) as the vision encoder, an MLP projection for aligning visual and textual information, and a well-trained Vicuna-7B (Chiang et al., 2023) with a LoRA adaptor for generating answers.\nBy default, LLaVA-AlignedVQ utilizes a single code-book with 4096 entries of 1024-dimensional features as CLIPVIT's intermediate features has 1024 channels. We evaluate the impacts of different number of codebooks and groups on the performance of LLaVA-AlignedVQ in Section 4.5. LLaVA-AlignedVQ is fine-tuned for one epoch with the Adam optimizer (Kingma, 2014). The training dataset used for training LLaVA-1.5, which is a mixture of multiple datasets, is used to update the codebook and trainable parameters, including Dual Linear Projection and LoRA. The 1-epoch finetuning takes 20h on 4 A100 machine.\nAlignedVQ can be applied to any block within the visual encoder of LLaVA, with optimal partitioning points depending on the specific optimization objectives. Here, we focus on optimizing inference latency. For this purpose, LLaVA-AlignedVQ applies VQ immediately after the first normalization layer of the first block of the visual encoder. We evaluate the impacts of different partition points on the performance of LLaVA-AlignedVQ at Section 4.5. In CNNs, it is often possible to find a partitioning point with relatively small intermediate feature sizes. However, transformer models generally have consistent computational cost"}, {"title": "4 EXPERIMENTS", "content": "Our experiments aim to answer the following questions: Q1: How is the task accuracy of LLaVA-AlignedVQ compared to the original LLaVA model and alternative approaches? (\u00a7 4.2) Q2: How is the data compression rates of LLaVA-AlignedVQ compared to alternative approaches? (\u00a7 4.3) Q3: How is the execution latency of LLaVA-AlignedVQ compared to the cloud-only alternative at various network bandwith settings? (\u00a7 4.4) Q4: How does model partitioning points, each design features of AlignedVQ, and quantization hyperparameters affect the performance of LLaVA-AlignedVQ? (\u00a7 4.5)"}, {"title": "4.1 Experimental Settings", "content": "Benchmarks. We evaluate LLaVA-AlignedVQ on eight diverse Vision Question Answering (VQA) benchmarks: VQA-v2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for open-ended questions, VizWiz (Gurari et al., 2018) for questions from visually impaired users, TextVQA (Singh et al., 2019) for text-rich queries, POPE (Li et al., 2023b) for model's degree of hallucination on three sampled subsets of COCO (Lin et al., 2014), random, common, and adversarial, MMBench (Liu et al., 2025) for all-round shuffling on multiple choice answers, LLaVA-Wild (Liu et al., 2023) and MM-Vet (Yu et al., 2023) for visual conversations on a diverse range of tasks. POPE reports F1 Score, LLaVA-Wild and MM-Vet utilize the answer score evaluated by GPT-40 (Achiam et al., 2023), and all the other datasets use accuracy. LLaVA-AlignedVQ uses the same codebook to compress intermediate feature for all these benchmarks.\nAlternatives for Comparison. We compare the accuracy and latency of LLaVA-AlignedVQ against the following alternative approaches:\n\u2022 LLaVA-Ori: Transmits raw images directly to the cloud for processing, with all computations handled on the cloud side.\n\u2022 LLaVA1+: An advanced version of LLaVA, in which the LORA parameters of the pretrained model are further fine-tuned on clean images for an additional epoch, matching the training duration of LLaVA-AlignedVQ.\n\u2022 LLaVA-JPEG: Reduces transmission overhead by sending JPEG-compressed images to the cloud at vary-"}, {"title": "4.2 Performance on VQA task Accuracy", "content": "Figure 5 illustrates the trade-off between VQA task accuracy and compressed data size on the VQA-v2 dataset, presenting an expanded set of baselines compared to Figure 1. Overall, LLaVA-AlignedVQ achieves the lowest transmission overhead while effectively preserving accuracy performance.\nWe make two main observations. First, LLaVA-AlignedVQ consistently demonstrates high accuracy, staying within -2.23% to +1.6% of the original LLaVA model and -0.86% to +1.82% of LLaVA1+, achieving seven top two accuracy among the eleven results Although the variants LLaVA-JPEG-90 and LLaVA-JPEG-901+ achieve competitive high accuracy, lower than LLaVA-AlignedVQ by approximately -0.16% and -0.66% on average, they incur a larger transmission overhead as indicated in Table 3, leading to longer inference latency as shown in Section 4.4. Although using JPEG-10 greatly reduces the transmission overhead compared to JPEG-90, from 26.47KB to 4.14KB per sample as reported in Table 3, it suffers from a significant accuracy drop of 5.09% on average.\nSecond, models fine-tuned with compressed images or compressed intermediate features, such as LLaVA-AlignedVQ and LLaVA-JPEG-901+, achieve even higher accuracy than"}, {"title": "4.3 Performance on Data Compression Rates", "content": "Table 3 summarizes compressed data size and the compression rates of AlignedVQ and alternative approaches. The raw image resolution is 336 \u00d7 336, which is the required dimension for the visual encoder CLIPVIT-large-336 in LLaVA. For intermediate features, CLIPVIT-large-336 produces 577 visual tokens with a dimension of 1024, stored in 16-bit floating point format. For JPEG compression, we evaluated both a commonly-used quality level of 90 and a more aggressive level of 10, offering higher compression with greater quality loss (JPEG-10 and JEPG-90 in Table 3). For compressing intermediate features, we compared AlignedVQ with two extreme cases, 1-bit quantization and an autoencoder that reduce the feature dimension to 1 (1-Bit Quantization and 1-Dim Autoencoder in Table 3)."}, {"title": "4.4 Performance on Inference Latency", "content": "We evaluate LLaVA-AlignedVQ using an NVIDIA Jetson AGX Xavier as the edge device and a workstation equipped with an A100 GPU as the cloud server. To simulate various network conditions, we measure the inference latency of"}, {"title": "4.5 Ablation Study", "content": "Impacts of Partitioning at Different Blocks. The red curve in Figure 7 shows the impact of different partitioning points (blocks 1, 6, 11, 16, and 21) on the task accuracy of LLaVA-AlignedVQ. As discussed in Section 3.3, transformer models maintain identical computational overhead and intermediate feature sizes across blocks, making early partitioning the optimal strategy for minimizing latency. In this context, AlignedVQ provides a two-in-one solution by preserving accuracy performance while achieving the lowest inference latency when applied to the first transformer block.\nImportance of AlignedVQ's Design Features. AlignedVQ has three design features including compressing normalized features after layer normalization layers, aligning features using Dual Linear Project (DLP) module, and including LORA parameters in the finetuning. Figure 7 illustrates each design feature's importance by comparing LLaVA-AlignedVQ with the following variants. (1) LLaVA-VQ-LN1-DLP: does not finetune LoRA parameters during the 1-epoch fine-tuning; (2) LLaVA-VQ-LN1 further excludes the DLP module; (3) LLaVA-VanillaVQ applies VQ after a transformer block (i.e., after the FFN layer instead of after the first normalization layer). Compared to the baseline LLaVA-VanillaVQ, applying VQ on normalized intermediate features improves the vanilla VQ accuracy by 10.4 \u2013 37.91%, adding Dual Linear Projection provides an additional 0.03 \u2013 1.17% accuracy boost, and post-"}, {"title": "Impacts of the Number of Codebooks and Groups.", "content": "LLaVA-AlignedVQ uses the Vanilla VQ with a single code-book and one group. Here, we examine the effect of adding more codebooks and groups in AlignedVQ by incorporating Residual VQ (Zeghidour et al., 2021) and Grouped VQ (Yang et al., 2023) on the task performance of LLaVA-AlignedVQ. Table 4 shows that increasing the number of codebooks and groups yields no or minimal accuracy improvement but results in increased transmission overhead, which scales proportionally with the number of codebooks and groups. Therefore, adding more codebooks and groups is unnecessary in LLaVA-AlignedVQ."}, {"title": "5 RELATED WORK", "content": "Vision Language Models (VLMs). VLMs (Alayrac et al., 2022; Li et al., 2023a; Chung et al., 2024) typically consist of a vision encoder to process images, a pretrained Large Language Model (LLM) to handle text, and a multimodal alignment module that bridges the two modalities, allowing the model to understand and generate responses based on both visual and textual inputs. LLaVA-1.5 (Liu et al., 2024a) is one of the state-of-the-art VLMs designed with simple structure and outstanding performance, which is our focus in this paper. It employs a pretrained CLIPViT (Radford et al., 2021) as the vision encoder, an MLP projection for aligning visual and textual information, and a well-trained Vicuna (Chiang et al., 2023) with a LoRA adaptor for generating answers. Although large models achieve impressive results in understanding and generating multimodal content, their computational and memory demands make them impractical for resource-constrained edge devices. To address these limitations, model partitioning offers a potential solution, where part of the model's computation is offloaded to the cloud. In this paper, we analyze the challenges of partitioning VLMs and propose AlignedVQ, a module that can be integrated into the vision encoder of a pretrained VLM.\nModel Partitioning. Model partitioning is a strategy that distributes the computational workload of deep neural networks between edge devices and cloud servers, optimizing metrics such as latency, energy consumption, and resource utilization. One common problem in partitioned execution is to find out the best partition point given a DNN model."}, {"title": "6 CONCLUSION", "content": "We presented AlignedVQ (Aligned Vector Quantization) as a lightweight and effective compression algorithm designed to reduce the transmission overhead of intermediate features while maintaining accuracy in edge-cloud collaborative Vision-Language Models (VLMs). Our experiments showed that AlignedVQ reduces the intermediate feature size of the vision encoder of the pre-trained LLaVA-1.5 model by 1356\u00d7 while preserving VQA task accuracy. This significant data size reduction can help shorten inference latency in the VLMs system. Future work will focus on extending AlignedVQ's effectiveness to other large-scale models and further optimizing performance across diverse deployment environments."}]}