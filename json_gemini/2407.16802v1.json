{"title": "Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels", "authors": ["Jae Soon Baik", "In Young Yoon", "Kun Hoon Kim", "Jun Won Choi"], "abstract": "Deep neural networks have demonstrated remarkable advancements in various fields using large, well-annotated datasets. However, real-world data often exhibit long-tailed distributions and label noise, significantly degrading generalization performance. Recent studies addressing these issues have focused on noisy sample selection methods that estimate the centroid of each class based on high-confidence samples within each target class. The performance of these methods is limited because they use only the training samples within each class for class centroid estimation, making the quality of centroids susceptible to long-tailed distributions and noisy labels. In this study, we present a robust training framework called Distribution-aware Sample Selection and Contrastive Learning (DaSC). Specifically, DaSC introduces a Distribution-aware Class Centroid Estimation (DaCC) to generate enhanced class centroids. DaCC performs weighted averaging of the features from all samples, with weights determined based on model predictions. Additionally, we propose a confidence-aware contrastive learning strategy to obtain balanced and robust representations. The training samples are categorized into high-confidence and low-confidence samples. Our method then applies Semi-supervised Balanced Contrastive Loss (SBCL) using high-confidence samples, leveraging reliable label information to mitigate class bias. For the low-confidence samples, our method computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve their representations in a self-supervised manner. Our experimental results on CIFAR and real-world noisy-label datasets demonstrate the superior performance of the proposed DaSC compared to previous approaches. Our code is available at https://github.com/JaesoonBaik1213/DaSC.", "sections": [{"title": "1 Introduction", "content": "In recent years, notable achievements have been made in various machine-learning applications owing to large-scale datasets with high-quality annotated labels [11,16]. However, in practice, obtaining a large-scale dataset with accurately annotated labels is challenging and expensive. A realistic alternative is to collect data from the Internet through web crawling, but this is typically affected by long-tail distributions as well as mislabeled data [55]. Training a model on such datasets results in biases toward head classes and suffers from memorization of label noise [12], consequently leading to poor generalization performance.\nNumerous methods have been explored to address the challenges of long-tailed distributions and noisy labels. In long-tailed (LT) learning, re-balancing techniques are the most straightforward method to mitigate bias toward head classes. These techniques include oversampling, undersampling [5,60], and adjusting instance-level or class-level weights in loss functions [4, 10, 41, 44]. Recently, inspired by the success of supervised contrastive learning (SCL) [25], several studies [9, 22, 30, 46, 61] have aimed to generate balanced representations by incorporating class distribution information into SCL. For learning with noisy labels (NL), various strategies have been proposed. These include robust loss functions [31,41], sample selection methods for distinguishing clean from noisy samples [1,12,18,45], and correcting noisy labels using model predictions [28,39,45]. Semi-supervised learning (SSL) has also been applied to enhance learning performance by utilizing noisy data in training [24,28]. These methods often employ suitable distance metrics to identify noisy samples, which are then utilized as unlabeled data for SSL. Recent studies have leveraged SCL with reliable label information [18,29,36,59] and unsupervised contrastive learning [6,53] to produce representations robust to noisy labels.\nDespite the success of individual approaches, simultaneously addressing long-tailed distributions and label noise remains a significant challenge. The combined issue of noisy labels and long-tailed distributions (NL-LT) complicates the accurate identification of true data distributions, significantly degrading the effectiveness of these methods. Recent studies [27,33,51,52] have employed feature-based noisy sample selection methods to identify noisy labels. As illustrated in Fig. 1b, these methods rely on the distance of input features from the class centroids, which are obtained by averaging the features of high-confidence samples within each class.\nHowever, this approach has several limitations. First, it uses only a portion of the training data within each class for estimating class centroids. As shown in Fig. 1a, samples mislabeled as other classes are excluded from the centroid estimation process. This exclusion results in insufficient data samples to reliably determine class centroids, which is particularly problematic for tail classes. Second, this method assigns equal weight to all samples when calculating class centroids. In the presence of labeling errors, this uniform weighting overlooks the potential for certain samples to be erroneous. Last, this approach does not actively seek to enhance representation quality. This limitation diminishes its effectiveness for tail classes, where obtaining high-quality representations is more critical than for head classes.\nWe aim to address these limitations by introducing a novel feature-based noisy sample selection method called Distribution-aware Class Centroid Estimation (DaCC). As shown in Fig. 1c, DaCC estimates class centroids by weighted averaging the features of samples from various classes. DaCC determines these weights based on model predictions, without relying on noisy labels, allowing for the adaptive inclusion of data samples from different classes in the centroid estimation process. Consequently, our method effectively utilizes informative samples with high-confidence scores when estimating class centroids.\nFurthermore, we propose a Confidence-aware Contrastive Learning strategy to achieve balanced and robust representations in noisy, long-tailed classification. Specifically, we categorize samples into high-confidence and low-confidence groups based on the confidence scores derived from their pseudo-labels. For the high-confidence group, we employ Semi-supervised Balanced Contrastive Loss (SBCL), which mitigates class bias using confident label information. Simultaneously, for the low-confidence group, we introduce Mixup-enhanced Instance Discrimination Loss (MIDL), which enhances data representation in a self-supervised manner. MIDL utilizes mixup augmentation [57] to construct diverse negative keys, which leads to more robust representations for the low-confidence group.\nWe integrate all the aforementioned components, presenting a novel training framework, referred to as Distribution-aware Sample Selection and Contrastive Learning (DaSC). We conduct experiments on both modified CIFAR datasets and real-world datasets that exhibit NL-LT. The experiments demonstrate that DaSC achieves significant performance improvements over existing methods.\nThe key contributions of this study are summarized as follows:\nWe present a novel class centroid estimation method called DaCC. This method utilizes the entire set of training samples as compared to the existing methods. Class centroids are generated by averaging samples with weights"}, {"title": "2 Related Work", "content": "Numerous approaches have been proposed to address the challenges of learning from long-tailed data. Re-balancing is a classical strategy for addressing class bias, including data re-sampling [5,43] and loss re-weighting [4, 10, 34, 44]. Recently, long-tailed data issues have been studied from the perspectives of both representation learning and classifier learning [23,34]. Ensemble-based methods have also been proposed to achieve robust performance under long-tailed data distributions [49,60]."}, {"title": "2.1 Long-Tailed Learning", "content": "Numerous approaches have been proposed to address the challenges of learning from long-tailed data. Re-balancing is a classical strategy for addressing class bias, including data re-sampling [5,43] and loss re-weighting [4, 10, 34, 44]. Recently, long-tailed data issues have been studied from the perspectives of both representation learning and classifier learning [23,34]. Ensemble-based methods have also been proposed to achieve robust performance under long-tailed data distributions [49,60]."}, {"title": "2.2 Learning with Noisy Labels", "content": "Various methods have been explored to achieve robust learning against noisy labels. These methods can be broadly classified into three approaches: 1) estimation of the noise transition matrix [7,38,54,56]; 2) robust loss functions [31,41,48]; and 3) noisy sample selection [1, 12, 18, 20, 28, 45]. Recently, some studies have explored applying SSL to address a noisy label issue [24,28,58]. These methods treated clean samples as labeled data and samples with noisy labels as unlabeled data after detecting correctly labeled samples."}, {"title": "2.3 Long-Tailed Learning with Noisy Labels", "content": "A series of studies have been conducted to resolve both noisy labels and long-tailed distributions. Meta-learning methods [41,44] have been proposed to prioritize clean data by adaptively learning explicit instance weights from metadata. HAR [3] introduced an adaptive Lipschitz regularization that takes into account the uncertainty inherent in a particular data sample. To better differentiate mislabeled examples, RoLT [52] and PCL [51] calculated a centroid of each class by taking the average of the features with high confidence in the corresponding target class. The obtained class centroids were subsequently used for noisy sample selection in RoLT and as part of the classifier in PCL. Recently, SFA [27] adopted"}, {"title": "2.4 Contrastive Learning for Addressing Long-Tailed Distribution and Noisy Labels", "content": "Contrastive learning aims to enhance the representation by comparing sample pairs, pulling semantically similar samples closer together while pushing semantically dissimilar samples further apart. Contrastive learning has been successfully applied to various tasks [6, 13, 15,47]. Recently, SCL [25] introduced fully supervised contrastive learning to encourage the samples of the same class to lie closer in feature space compared to those of different classes. Several LT methods [9,22,30,46,61] have enhanced SCL by utlizing supervision provided by long-tailed training data. PaCo [9] introduced parametric learnable class centers to learn a balanced representation. BCL [61] proposed a balanced contrastive loss that uses logit compensation to induce all samples to form a regular simplex. In NL learning, several methods [18,29,36,59] have employed contrastive learning [6, 15, 25] to better distinguish between noisy and clean data. MOIT [36] proposed interpolated contrastive learning that combines mixup with SCL to learn robust representations. Sel-CL [29] identified confidence pairs and modeled their pair-wise relations to obtain a robust representation."}, {"title": "3 Proposed Method", "content": "Consider a K-class classification task. We train the model using the training dataset $D = \\{(x^{(i)}, \\tilde{y}^{(i)})\\}_{i=1}^N$, where N denotes the total number of training samples, $x^{(i)}$ denotes the i-th training sample, and $\\tilde{y}^{(i)} \\in \\{1, ..., K\\} = \\mathcal{Y}$ is its associated label. As a result of labeling errors, a fraction of the training samples are incorrectly labeled, i.e., $\\exists i \\in [1,N], \\tilde{y}^{(i)} \\neq y^{*(i)}$, where $y^{*(i)}$ indicates the true label for the sample $x^{(i)}$. We denote the training sample set of class k as $D_k = \\{(x^{(i)}, \\tilde{y}^{(i)})|\\tilde{y}^{(i)} = k\\}$. The training dataset D follows a long-tailed distribution with an imbalance ratio of $\\rho$, where $\\rho = \\min_k |D_k|/ \\max_k |D_k|$. Our goal is to achieve robust classification performance on unseen data, given a long-tailed noisy training dataset D.\nOverview. Fig. 2 illustrates the overview of the proposed DaSC framework. Our architecture comprises DaCC, a sample selection module, a shared feature extractor f, a conventional classifier $g^c$, a balanced classifier $g^b$, a pseudo-label generator, and a Multi-Layer Perceptron (MLP) projector q. Initially, DaCC computes the class centroids. Then, correctly labeled samples are identified based"}, {"title": "3.1 Overview of the Proposed Method", "content": "Consider a K-class classification task. We train the model using the training dataset $D = \\{(x^{(i)}, \\tilde{y}^{(i)})\\}_{i=1}^N$, where N denotes the total number of training samples, $x^{(i)}$ denotes the i-th training sample, and $\\tilde{y}^{(i)} \\in \\{1, ..., K\\} = \\mathcal{Y}$ is its associated label. As a result of labeling errors, a fraction of the training samples are incorrectly labeled, i.e., $\\exists i \\in [1,N], \\tilde{y}^{(i)} \\neq y^{*(i)}$, where $y^{*(i)}$ indicates the true label for the sample $x^{(i)}$. We denote the training sample set of class k as $D_k = \\{(x^{(i)}, \\tilde{y}^{(i)})|\\tilde{y}^{(i)} = k\\}$. The training dataset D follows a long-tailed distribution with an imbalance ratio of $\\rho$, where $\\rho = \\min_k |D_k|/ \\max_k |D_k|$. Our goal is to achieve robust classification performance on unseen data, given a long-tailed noisy training dataset D.\nOverview. Fig. 2 illustrates the overview of the proposed DaSC framework. Our architecture comprises DaCC, a sample selection module, a shared feature extractor f, a conventional classifier $g^c$, a balanced classifier $g^b$, a pseudo-label generator, and a Multi-Layer Perceptron (MLP) projector q. Initially, DaCC computes the class centroids. Then, correctly labeled samples are identified based"}, {"title": "3.2 Distribution-Aware Class Centroid Estimation", "content": "Conventional sample selection methods proposed in [27,33,52] used only target class samples to estimate their class centroids. In contrast, the proposed DaCC method estimates class centroids using samples from all classes, weighting each sample based on its confidence score. However, noisy labels can adversely affect"}, {"title": "3.3 Confidence-Aware Contrastive Learning", "content": "Confidence-aware contrastive learning applies different strategies to high-confidence and low-confidence samples. We classify a sample x(i) by comparing the maximum confidence score in the pseudo-label \u0177(i) with a threshold T. If the score"}, {"title": "3.4 Training with Semi-supervised Learning", "content": "Various methods [27, 49, 60] have implemented balanced classifiers to mitigate class bias towards head classes. Our method also employs a balanced classifier trained with Balanced Softmax [40]. We input the mixup image $x^{mix} (i)$ into the feature extractor f, followed by the balanced classifier, to produce the logit $\\eta^{(mix,b)}(i) = [\\eta_1^{(mix,b)}(i),...,\\eta_K^{(mix,b)}(i)]$, where k is the class index. The loss function for Balanced Softmax is expressed as\n$\\mathcal{L}_{BS}(i) = - \\log \\frac{n_k \\exp {(\\eta_k^{(mix,b)}(i))}}{\\sum_{k'=1}^K n_{k'} \\exp {(\\eta_{k'}^{(mix,b)}(i))}}$\nwhere $n_k$ denotes the number of samples of class k among the data samples selected by the proposed DaCC. During the warmup phase, we determine $n_k$ based on the distribution of the training dataset D.\nTo effectively utilize noisy samples, we consider them as unlabeled data and train the model under a semi-supervised learning (SSL) framework [2]. The total training loss is\n$\\mathcal{L} = \\mathcal{L}_{MixMatch} + \\mathcal{L}_{BMixMatch} + \\lambda_{SBCL}\\mathcal{L}_{SBCL} + \\lambda_{MIDL}\\mathcal{L}_{MIDL}$"}, {"title": "Balanced Classifier"}, {"title": "4 Experiments", "content": "The proposed method was evaluated on long-tailed version of the CIFAR-10 and CIFAR-100 datasets [26], with synthetic noise introduced to the labels. Additionally, we used real-world noise datasets including CIFAR-10N, CIFAR-100N [50], and Red mini-ImageNet [19] for evaluation.\nFor the synthetic CIFAR-10 and CIFAR-100 datasets, we followed the experimental setup in [33]. We initially created long-tailed version of the CIFAR datasets by setting the number of samples for the k-th class as $N_k = \\max_k |D_k|\\rho^{k-1}$, where $\\rho$ is the imbalance ratio and K indicates the number of classes. Subsequently, we introduced two types of synthetic noise into the datasets: symmetric (Sym.) and asymmetric (Asym.) label noise. Symmetric noise involves randomly assigning labels to any other class based on a specified noise ratio. Asymmetric noise, on the other hand, changes the labels to semantically related classes (e.g., truck \u2192 automobile), taking class information into account. In our experiments, we used noise ratios of 0.4 and 0.6 for symmetric noise and 0.2 and 0.4 for asymmetric noise.\nFor CIFAR-10N (10N) and CIFAR-100N (100N) [50], we modified the CIFAR datasets to the aforementioned long-tailed version and then introduced noisy labels created by human annotations. Red mini-ImageNet (Red) [19] is a smaller version of ImageNet [42] that includes real-world noisy web labels with specific ratios. In this study, we used web labels with noise ratios of 0.2 and 0.4 for Red mini-ImageNet.\nThe following baseline methods were compared with our method. Methods addressing long-tailed distribution include LA [34], LDAM [4], IB Loss [37], and BBN [60]. Methods for dealing with noisy labels include DivideMix [28], UNICON [24], and TCL [18]. The latest methods addressing both noisy labels and long-tailed distribution include Meta-weight Net [44], ROLT [52], HAR [3], ULC [17], SFA [27], and TABASCO [33]."}, {"title": "4.1 Setup"}, {"title": "4.2 Implementation Details", "content": "We used PreAct ResNet18 and ResNet18 [14] as backbone networks for the experiments on the CIFAR and Red mini-ImageNet datasets, respectively. The model was trained for 100 epochs using a stochastic gradient descent optimizer"}, {"title": "4.3 Main Results", "content": "Tables 2a and 2b display the performance of the proposed DaSC on long-tailed noisy versions of the CIFAR datasets. We examine both symmetric and asymmetric noise in Tables 2a and 2b, respectively. The proposed DaSC consistently outperforms other baseline methods in all experimental setups. For CIFAR-10 with a symmetric noise ratio of 0.4, DaSC achieves performance gains of 2.23% and 3.51% over the current best methods, SFA and TABASCO, respectively. These performance gains tend to increase as the noise ratio and the number of classes rise. On CIFAR-100 with a symmetric noise ratio of 0.6, DaSC achieves substantial performance gains of 6.69% and 8.42% over SFA and TABASCO, respectively."}, {"title": "Results on CIFAR Datasets"}, {"title": "4.4 Performance Analysis", "content": "We conducted an ablation study to analyze the impact of the main components in the proposed DaSC. Table 4 presents the experimental results on CIFAR datasets with various noise types and ratios. DaCC offers significant performance gains over the baseline, confirming the effectiveness of the proposed centroid estimation method. Specifically, it achieves a 1.85% performance gain on CIFAR-10 with a symmetric noise ratio of 0.4. Incorporating TS into DaCC leads to further performance improvement of 0.4% gain under the same setup. Adding SBCL further enhances performance by 0.76%. The combination of SBCL and MIDL provides a 1.51% performance gain, resulting in a total performance gain of 3.76% over the baseline. These performance gains are consistent across various datasets and noise ratios, demonstrating the robustness and versatility of the proposed DaSC."}, {"title": "Ablation Study"}, {"title": "5 Conclusion", "content": "In this study, we proposed a novel robust learning framework, DaSC, to address the challenges of long-tailed class distribution and noisy label simultaneously. Recent approaches have employed feature-based noisy sample selection methods"}, {"title": "3.3 Confidence-Aware Contrastive Learning", "content": "Confidence-aware contrastive learning applies different strategies to high-confidence and low-confidence samples. We classify a sample x(i) by comparing the maximum confidence score in the pseudo-label \u0177(i) with a threshold TC. If the score exceeds the threshold, the sample is classified as a high-confidence one; otherwise, it is classified as a low-confidence one.\nGiven a set of high-confidence samples $B^{high}$ within a mini-batch B, the SBCL is expressed as\n$\\mathcal{L}_{SBCL} = \\frac{1}{|B^{high}|}\\sum_{i \\in B^{high}} \\mathcal{L}_{SBCL(i)}$,\nwhere\n$\\mathcal{L}_{SBCL(i)} = -\\log \\frac{\\frac{1}{|B^{high}_y |} \\sum_{p=1}^{|B^{high}_y |} \\exp {(z(i) \\cdot z(p) / T_s)}}{\\sum_{k=1}^K  \\frac{1}{|B^{high}_k|} \\sum_{e \\in B^{high}_k} \\exp {(z(i) \\cdot z(k) / T_s)}}$,\nwhere A \\x denotes the set subtraction operation, yi is the index of the largest entry of the pseudo-label \u0177(i), $B^{high}_j$ represents the set of sample indices for high-confidence samples whose predictions correspond to the class j within a mini-batch B, and Ts is the temperature parameter. This method effectively leverages reliable label information from high-confidence samples to penalize head classes, aiding in the learning of balanced representations.\nBuilding on a simple instance discrimination task [53], we propose mixup-enhanced instance discrimination loss for low-confidence samples. For each sample x(i) within the mini-batch $B^{low}$ of low-confidence samples, we generate two augmented images, $x^1(i)$ and $x^2(i)$, serving as the query and positive key, respectively. We obtain the feature representations $z^1(i)$ and $z^2(i)$ from $x^1(i)$ and $x^2(i)$ by applying the backbone network, followed by the MLP projector. Additionally, we devise a variant that generates a negative key from a mixup image $x^{mix}$ [57]. Note that mixup images are used to enhance the diversity of negative samples. The mixup image is obtained by taking a convex combination between $x^1(i)$ and $x^2(i)$ as\n$x^{mix} (i) = \\lambda x^1(i) + (1 - \\lambda) x^2(i)$,\nwhere \u03bb denotes the mixing ratio obtained from Beta distribution, e.g., \u03bb ~ Beta(\u03b1, \u03b1). We extract the key representations $z^{mix}(i)$ from $x^{mix}(i)$ and feed this negative key into the queue (i.e., the memory bank) with the fixed size |M|. Mixup samples serve as hard negative samples [21, 35], effectively improving the performance of instance discrimination loss. The proposed mixup-enhanced instance discrimination loss (MIDL) can be expressed by\n$\\mathcal{L}_{MIDL} = \\frac{1}{|B^{low}|}\\sum_{i \\in B^{low}}  \\mathcal{L}_{MIDL(i)}$,\nwhere\n$\\mathcal{L}_{MIDL(i)} = - \\log \\frac{\\exp {(z^1 (i) \\cdot z^2 (i)/ T_m)}}{\\exp {(z^1(i) \\cdot z^2 (i)/ T_m)} + \\sum_{z^{mix} \\in M} \\exp {(z^1(i) \\cdot z^{mix}/ T_m)}}$"}]}