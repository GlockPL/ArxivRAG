{"title": "Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels", "authors": ["Jae Soon Baik", "In Young Yoon", "Kun Hoon Kim", "Jun Won Choi"], "abstract": "Deep neural networks have demonstrated remarkable advancements in various fields using large, well-annotated datasets. However, real-world data often exhibit long-tailed distributions and label noise, significantly degrading generalization performance. Recent studies addressing these issues have focused on noisy sample selection methods that estimate the centroid of each class based on high-confidence samples within each target class. The performance of these methods is limited because they use only the training samples within each class for class centroid estimation, making the quality of centroids susceptible to long-tailed distributions and noisy labels. In this study, we present a robust training framework called Distribution-aware Sample Selection and Contrastive Learning (DaSC). Specifically, DaSC introduces a Distribution-aware Class Centroid Estimation (DaCC) to generate enhanced class centroids. DaCC performs weighted averaging of the features from all samples, with weights determined based on model predictions. Additionally, we propose a confidence-aware contrastive learning strategy to obtain balanced and robust representations. The training samples are categorized into high-confidence and low-confidence samples. Our method then applies Semi-supervised Balanced Contrastive Loss (SBCL) using high-confidence samples, leveraging reliable label information to mitigate class bias. For the low-confidence samples, our method computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve their representations in a self-supervised manner. Our experimental results on CIFAR and real-world noisy-label datasets demonstrate the superior performance of the proposed DaSC compared to previous approaches. Our code is available at https://github.com/JaesoonBaik1213/DaSC.", "sections": [{"title": "1 Introduction", "content": "In recent years, notable achievements have been made in various machine-learning applications owing to large-scale datasets with high-quality annotated labels [11,16]. However, in practice, obtaining a large-scale dataset with accurately annotated labels is challenging and expensive. A realistic alternative is to collect data from the Internet through web crawling, but this is typically affected by long-tail distributions as well as mislabeled data [55]. Training a model on such datasets results in biases toward head classes and suffers from memorization of label noise [12], consequently leading to poor generalization performance.\nNumerous methods have been explored to address the challenges of long-tailed distributions and noisy labels. In long-tailed (LT) learning, re-balancing techniques are the most straightforward method to mitigate bias toward head classes. These techniques include oversampling, undersampling [5,60], and adjusting instance-level or class-level weights in loss functions [4, 10, 41, 44]. Recently, inspired by the success of supervised contrastive learning (SCL) [25], several studies [9, 22, 30, 46, 61] have aimed to generate balanced representations by incorporating class distribution information into SCL. For learning with noisy labels (NL), various strategies have been proposed. These include robust loss functions [31,41], sample selection methods for distinguishing clean from noisy samples [1,12,18,45], and correcting noisy labels using model predictions [28,39,45]. Semi-supervised learning (SSL) has also been applied to enhance learning performance by utilizing noisy data in training [24,28]. These methods often employ suitable distance metrics to identify noisy samples, which are then utilized as unlabeled data for SSL. Recent studies have leveraged SCL with reliable label information [18,29,36,59] and unsupervised contrastive learning [6,53] to produce representations robust to noisy labels.\nDespite the success of individual approaches, simultaneously addressing long-tailed distributions and label noise remains a significant challenge. The combined issue of noisy labels and long-tailed distributions (NL-LT) complicates the accurate identification of true data distributions, significantly degrading the effective-ness of these methods. Recent studies [27,33,51,52] have employed feature-based noisy sample selection methods to identify noisy labels. As illustrated in Fig. 1b, these methods rely on the distance of input features from the class centroids, which are obtained by averaging the features of high-confidence samples within each class.\nHowever, this approach has several limitations. First, it uses only a portion of the training data within each class for estimating class centroids. As shown in Fig. 1a, samples mislabeled as other classes are excluded from the centroid estimation process. This exclusion results in insufficient data samples to reliably determine class centroids, which is particularly problematic for tail classes. Second, this method assigns equal weight to all samples when calculating class centroids. In the presence of labeling errors, this uniform weighting overlooks the potential for certain samples to be erroneous. Last, this approach does not actively seek to enhance representation quality. This limitation diminishes its effectiveness for tail classes, where obtaining high-quality representations is more critical than for head classes.\nWe aim to address these limitations by introducing a novel feature-based noisy sample selection method called Distribution-aware Class Centroid Estimation (DaCC). As shown in Fig. 1c, DaCC estimates class centroids by weighted averaging the features of samples from various classes. DaCC determines these weights based on model predictions, without relying on noisy labels, allowing for the adaptive inclusion of data samples from different classes in the centroid estimation process. Consequently, our method effectively utilizes informative samples with high-confidence scores when estimating class centroids.\nFurthermore, we propose a Confidence-aware Contrastive Learning strategy to achieve balanced and robust representations in noisy, long-tailed classification. Specifically, we categorize samples into high-confidence and low-confidence groups based on the confidence scores derived from their pseudo-labels. For the high-confidence group, we employ Semi-supervised Balanced Contrastive Loss (SBCL), which mitigates class bias using confident label information. Simultaneously, for the low-confidence group, we introduce Mixup-enhanced Instance Discrimination Loss (MIDL), which enhances data representation in a self-supervised manner. MIDL utilizes mixup augmentation [57] to construct diverse negative keys, which leads to more robust representations for the low-confidence group.\nWe integrate all the aforementioned components, presenting a novel training framework, referred to as Distribution-aware Sample Selection and Contrastive Learning (DaSC). We conduct experiments on both modified CIFAR datasets and real-world datasets that exhibit NL-LT. The experiments demonstrate that DaSC achieves significant performance improvements over existing methods.\nThe key contributions of this study are summarized as follows:\nWe present a novel class centroid estimation method called DaCC. This method utilizes the entire set of training samples as compared to the existing methods. Class centroids are generated by averaging samples with weights obtained by temperature-scaled model predictions. This strategy leads to more reliable class centroid estimation and efficient use of training data.\nWe also introduce a confidence-aware contrastive learning strategy to improve the quality of feature representations. This method applies two different contrastive losses, SBCL and MIDL, to low-confidence and high-confidence sample groups, respectively. SBCL uses label information to achieve a balanced representation of high-confidence samples, while MIDL employs a mixup augmentation to learn a robust representation for low-confidence samples.\nOur experimental results confirm that DaSC achieves state-of-the-art performance across various configurations."}, {"title": "2 Related Work", "content": "2.1 Long-Tailed Learning\nNumerous approaches have been proposed to address the challenges of learning from long-tailed data. Re-balancing is a classical strategy for addressing class bias, including data re-sampling [5,43] and loss re-weighting [4, 10, 34, 44]. Recently, long-tailed data issues have been studied from the perspectives of both representation learning and classifier learning [23,34]. Ensemble-based methods have also been proposed to achieve robust performance under long-tailed data distributions [49,60].\n2.2 Learning with Noisy Labels\nVarious methods have been explored to achieve robust learning against noisy labels. These methods can be broadly classified into three approaches: 1) estimation of the noise transition matrix [7,38,54,56]; 2) robust loss functions [31,41,48]; and 3) noisy sample selection [1, 12, 18, 20, 28, 45]. Recently, some studies have explored applying SSL to address a noisy label issue [24,28,58]. These methods treated clean samples as labeled data and samples with noisy labels as unlabeled data after detecting correctly labeled samples.\n2.3 Long-Tailed Learning with Noisy Labels\nA series of studies have been conducted to resolve both noisy labels and long-tailed distributions. Meta-learning methods [41,44] have been proposed to prioritize clean data by adaptively learning explicit instance weights from metadata. HAR [3] introduced an adaptive Lipschitz regularization that takes into account the uncertainty inherent in a particular data sample. To better differentiate mislabeled examples, RoLT [52] and PCL [51] calculated a centroid of each class by taking the average of the features with high confidence in the corresponding target class. The obtained class centroids were subsequently used for noisy sample selection in RoLT and as part of the classifier in PCL. Recently, SFA [27] adopted"}, {"title": "3 Proposed Method", "content": "3.1 Overview of the Proposed Method\nProblem Setup. Consider a K-class classification task. We train the model using the training dataset $\\mathcal{D} = \\{(x^{(i)}, \\tilde{y}^{(i)})\\}_{i=1}^N$, where N denotes the total number of training samples, $x^{(i)}$ denotes the i-th training sample, and $\\tilde{y}^{(i)} \\in \\{1, ..., K\\} = \\mathcal{Y}$ is its associated label. As a result of labeling errors, a fraction of the training samples are incorrectly labeled, i.e., $\\exists i \\in [1,N], \\tilde{y}^{(i)} \\neq y^{*(i)}$, where $y^{*(i)}$ indicates the true label for the sample $x^{(i)}$. We denote the training sample set of class k as $\\mathcal{D}_k = \\{(x^{(i)}, \\tilde{y}^{(i)})|\\tilde{y}^{(i)} = k\\}$. The training dataset $\\mathcal{D}$ follows a long-tailed distribution with an imbalance ratio of $\\rho$, where $\\rho = \\min_k |\\mathcal{D}_k|/ \\max_k |\\mathcal{D}_k|$. Our goal is to achieve robust classification performance on unseen data, given a long-tailed noisy training dataset $\\mathcal{D}$.\nOverview. Fig. 2 illustrates the overview of the proposed DaSC framework. Our architecture comprises DaCC, a sample selection module, a shared feature extractor f, a conventional classifier $g^c$, a balanced classifier $g^b$, a pseudo-label generator, and a Multi-Layer Perceptron (MLP) projector q. Initially, DaCC computes the class centroids. Then, correctly labeled samples are identified based on their distance from the class centroids. The next step is to train the model using noisy samples as unlabeled samples within the SSL framework [2]. For each training sample $(x^{(i)}, \\tilde{y}^{(i)})$, two strongly augmented samples $x_1^{(i)}$ and $x_2^{(i)}$ and a weakly augmented sample $x_w^{(i)}$ are generated [28]. Additionally, a mixup sample $(x_{mix}^{(i)}, y_{mix}^{(i)})$ is created from $x_1^{(i)}$ and $x_2^{(i)}$ using mixup augmentation [57]. The conventional classifier $g^c$ is applied to $x_{mix}^{(i)}$ to produce a model prediction $p^{(mix,c)} (i) = [p_1^{(mix,c)}(i),..., p_K^{(mix,c)}(i)]$ where $p_k^{(mix,c)}(i)$ denotes the predicted probability of the k-th class. The balanced classifier $g^b$ is then applied to $x_{mix}^{(i)}$ to produce a model prediction $p^{(mix,b)}(i)$. This balanced classifier uses Balanced Softmax [40] to mitigate class bias. Following the DivideMix [28], we employ a pseudo-label generator to produce a pseudo-label $\\hat{y}^{(i)}$, based on the provided labels and model predictions for labeled samples or solely on the model predictions for unlabeled samples. The projector q is applied to both $x_{mix}(i)$ and the concatenation of $x_1(i)$ and $x_2(i)$, producing lower-dimensional representations $z_{mix}(i)$ and $z(i)$, respectively. These representations are used in confidence-aware contrastive learning to enhance representation quality. Specifically, we classify an input sample $x(i)$ into a high-confidence sample or a low-confidence sample by comparing the maximum confidence score in $\\hat{y}^{(i)}$ with a threshold $T_c$. Subsequently, we employ SBCL for the high-confidence sample group and MIDL for the low-confidence sample group.\n3.2 Distribution-Aware Class Centroid Estimation\nConventional sample selection methods proposed in [27,33,52] used only target class samples to estimate their class centroids. In contrast, the proposed DaCC method estimates class centroids using samples from all classes, weighting each sample based on its confidence score. However, noisy labels can adversely affect"}, {"title": "3.3 Confidence-Aware Contrastive Learning", "content": "Confidence-aware contrastive learning applies different strategies to high-confidence and low-confidence samples. We classify a sample x(i) by comparing the maximum confidence score in the pseudo-label $\\hat{y}(i)$ with a threshold $T_c$. If the score exceeds the threshold, the sample is classified as a high-confidence one; otherwise, it is classified as a low-confidence one.\nSemi-Supervised Balanced Contrastive Loss. Given a set of high-confidence samples $\\mathcal{B}^{high}$ within a mini-batch $\\mathcal{B}$, the SBCL is expressed as\n$\\mathcal{L}_{SBCL} = \\frac{1}{|\\mathcal{B}^{high}|}\\sum_{i \\in \\mathcal{B}^{high}} \\mathcal{L}_{SBCL}(i)$,\nwhere\n$\\mathcal{L}_{SBCL}(i) = - log \\frac{1}{|\\mathcal{B}_{y_i}^{high}\\{i\\}|}\\sum_{p \\in \\mathcal{B}_{y_i}^{high}\\{i\\}} \\frac{exp(z(i) \\cdot z(p)/T_s)}{\\sum_{k=1}^K \\sum_{k \\in \\mathcal{B}^{high}} exp(z(i) \\cdot z(k)/T_s)}$,\nwhere A\\x denotes the set subtraction operation, $y_i$ is the index of the largest entry of the pseudo-label $\\hat{y}(i)$, $\\mathcal{B}^{high}_{y_i}$ represents the set of sample indices for high-confidence samples whose predictions correspond to the class j within a mini-batch $\\mathcal{B}$, and Ts is the temperature parameter. This method effectively leverages reliable label information from high-confidence samples to penalize head classes, aiding in the learning of balanced representations.\nMixup Enhanced Instance Discrimination Loss. Building on a simple in-stance discrimination task [53], we propose mixup-enhanced instance discrimination loss for low-confidence samples. For each sample $x(i)$ within the mini-batch $\\mathcal{B}^{low}$ of low-confidence samples, we generate two augmented images, $x_1(i)$ and $x_2(i)$, serving as the query and positive key, respectively. We obtain the feature representations $z_1(i)$ and $z_2(i)$ from $x_1(i)$ and $x_2(i)$ by applying the backbone network, followed by the MLP projector. Additionally, we devise a variant that generates a negative key from a mixup image $x_{mix}$ [57]. Note that mixup images are used to enhance the diversity of negative samples. The mixup image is obtained by taking a convex combination between $x_1(i)$ and $x_2(i)$ as\n$x_{mix} (i) = \\lambda x_1(i) + (1 - \\lambda) x_2(i)$,\nwhere $\\lambda$ denotes the mixing ratio obtained from Beta distribution, e.g., $\\lambda \\sim Beta(\\alpha,\\alpha)$. We extract the key representations $z_{mix} (i)$ from $x_{mix}(i)$ and feed this negative key into the queue (i.e., the memory bank) with the fixed size |M|. Mixup samples serve as hard negative samples [21, 35], effectively improving the performance of instance discrimination loss. The proposed mixup-enhanced instance discrimination loss (MIDL) can be expressed by\n$\\mathcal{L}_{MIDL} = \\frac{1}{|\\mathcal{B}^{low}|}\\sum_{i\\in \\mathcal{B}^{low}} \\mathcal{L}_{MIDL}(i)$,\nwhere\n$\\mathcal{L}_{MIDL}(i) = - log \\frac{exp(z_1 (i) \\cdot z_2(i)/T_m)}{exp(z_1(i) \\cdot z_2(i)/T_m) + \\sum_{z_{mix}\\in M} exp(z_1(i) \\cdot z_{mix}/T_m)}$,"}, {"title": "3.4 Training with Semi-supervised Learning", "content": "Balanced Classifier. Various methods [27, 49, 60] have implemented balanced classifiers to mitigate class bias towards head classes. Our method also employs a balanced classifier trained with Balanced Softmax [40]. We input the mixup image $x_{mix} (i)$ into the feature extractor f, followed by the balanced classifier, to produce the logit $\\eta^{(mix,b)}(i) = [\\eta^{(mix,b)}_1(i),..., \\eta^{(mix,b)}_K(i)]$, where k is the class index. The loss function for Balanced Softmax is expressed as\n$\\mathcal{L}_{BS}(i) = - log \\frac{n_k exp (\\eta_k^{(mix,b)}(i))}{\\sum_{k'=1}^K n_{k'} exp (\\eta_{k'}^{(mix,b)}(i))}$,\nwhere nk denotes the number of samples of class k among the data samples selected by the proposed DaCC. During the warmup phase, we determine nk based on the distribution of the training dataset $\\mathcal{D}$.\nTotal Loss. To effectively utilize noisy samples, we consider them as unlabeled data and train the model under a semi-supervised learning (SSL) framework [2]. The total training loss is\n$\\mathcal{L} = \\mathcal{L}_{MixMatch} + \\mathcal{L}_{BMixMatch} + \\lambda_{SBCL}\\mathcal{L}_{SBCL} + \\lambda_{MIDL}\\mathcal{L}_{MIDL}$,\nwhere $\\mathcal{L}_{MixMatch}$ denotes the MixMatch loss for the conventional classifier, $\\mathcal{L}_{BMixMatch}$ denotes the MixMatch loss with $\\mathcal{L}_{BS}$ for the balanced classifier, and $\\lambda_{SBCL}$ and $\\lambda_{MIDL}$ are the coefficients for SBCL and MIDL, respectively. Algorithm 1 presents the summary of our DaSC training framework. During the test time, we use the average of outputs from the classifiers $g^c$ and $g^b$ as the final model output. Furthermore, following previous studies [27,28,52], we employ a co-training framework against confirmation bias."}, {"title": "4 Experiments", "content": "4.1 Setup\nDatasets. The proposed method was evaluated on long-tailed version of the CIFAR-10 and CIFAR-100 datasets [26], with synthetic noise introduced to the labels. Additionally, we used real-world noise datasets including CIFAR-10N, CIFAR-100N [50], and Red mini-ImageNet [19] for evaluation.\nFor the synthetic CIFAR-10 and CIFAR-100 datasets, we followed the experimental setup in [33]. We initially created long-tailed version of the CI-FAR datasets by setting the number of samples for the k-th class as $N_k ="}, {"title": "4.2 Implementation Details", "content": "We used PreAct ResNet18 and ResNet18 [14] as backbone networks for the experiments on the CIFAR and Red mini-ImageNet datasets, respectively. The model was trained for 100 epochs using a stochastic gradient descent optimizer"}, {"title": "4.3 Main Results", "content": "Results on CIFAR Datasets. Tables 2a and 2b display the performance of the proposed DaSC on long-tailed noisy versions of the CIFAR datasets. We examine both symmetric and asymmetric noise in Tables 2a and 2b, respectively. The proposed DaSC consistently outperforms other baseline methods in all experimental setups. For CIFAR-10 with a symmetric noise ratio of 0.4, DaSC achieves performance gains of 2.23% and 3.51% over the current best methods, SFA and TABASCO, respectively. These performance gains tend to increase as the noise ratio and the number of classes rise. On CIFAR-100 with a symmetric noise ratio of 0.6, DaSC achieves substantial performance gains of 6.69% and 8.42% over SFA and TABASCO, respectively.\nResults on Real-World Datasets. Table 3 shows the performance of DaSC on real-world datasets with long-tailed distributions. These datasets are more challenging than those with synthetic labeling noise because samples are more prone to being mislabeled into semantically similar classes, and noisy labels are un-evenly distributed. Notably, our DaSC method achieves significant performance gains on real-world datasets created by human annotations, aligning with the experimental results observed in the previous experiments. Our DaSC achieves a performance gain of 2.04% over the existing best method, TABASCO [33] with a noise ratio of 0.2 on the Red mini-ImageNet dataset. Additionally, DaSC out-performs TABASCO by 5.40% on CIFAR-10N and by 4.27% on CIFAR-100N."}, {"title": "4.4 Performance Analysis", "content": "Ablation Study. We conducted an ablation study to analyze the impact of the main components in the proposed DaSC. Table 4 presents the experimental results on CIFAR datasets with various noise types and ratios. DaCC offers sig-nificant performance gains over the baseline, confirming the effectiveness of the proposed centroid estimation method. Specifically, it achieves a 1.85% perfor-mance gain on CIFAR-10 with a symmetric noise ratio of 0.4. Incorporating TS into DaCC leads to further performance improvement of 0.4% gain under the same setup. Adding SBCL further enhances performance by 0.76%. The com-bination of SBCL and MIDL provides a 1.51% performance gain, resulting in a total performance gain of 3.76% over the baseline. These performance gains are consistent across various datasets and noise ratios, demonstrating the robustness and versatility of the proposed DaSC.\nHyperparameters. The proposed DaSC performs consistently well across var-ious hyperparameter configurations. The experimental results with different hy-perparameter configurations are provided in the supplementary material."}, {"title": "5 Conclusion", "content": "In this study, we proposed a novel robust learning framework, DaSC, to address the challenges of long-tailed class distribution and noisy label simultaneously. Recent approaches have employed feature-based noisy sample selection methods"}, {"title": "A Experimental Results with Higher Imbalance Ratio", "content": "In this section, we provide the performance of the proposed method compared to other baselines on long-tailed CIFAR and mini-ImageNet datasets with an imbalance ratio of 0.01 in Tables 7 and 8. The performances of other meth-ods were taken from [33]. For methods not reported in [33], we reproduced their performances. For all experimental setups, the proposed DaSC consistently outperforms other baseline methods. Specifically, on CIFAR-10 with symmetric noise of 0.4, DaSC achieves 2.37% and 8.01% better performance over SFA and TABASCO, respectively. Similarly, for CIFAR-10 with asymmetric noise of 0.2, DaSC outperforms SFA and TABASCO by 4.93% and 10.58%, respectively. For CIFAR-10N with human annotations, DaSC achieves significant performance im-provements of 6.97% and 6.07% over SFA and TABASCO, respectively. These results demonstrate the effectiveness and robustness of DaSC against extremely noisy labels and long-tailed distributions."}, {"title": "B Ablation Study on Hyperparameters", "content": "In Figs. 6 and 7, we present experimental results for the confidence threshold $T_c$, temperature parameter for temperature scaling $\\tau_t$, and memory bank size |M| on the long-tailed CIFAR-10 dataset with asymmetric noise of 0.2 and symmetric noise of 0.4, respectively. Additionally, Figs. 8 and 9 provide experimental results"}, {"title": "C Performance of Different Representations and Model Prediction for Noisy Sample Selection.", "content": "Table 9 presents the performance of DaSC using different representations and model predictions for detecting correctly labeled samples. For representations, we use either $f(x(i))$ from the backbone network or $z'(i)$ from the MLP projector. For model predictions in DaCC, we employ $\\hat{p}^c(i)$ from the conventional classifier or $\\hat{p}^b(i)$ from the balanced classifier.\nThe results show that the DaSC using $z'(i)$ and $\\hat{p}^c(i)$ outperforms all other setups. Using the low-dimensional representation from the MLP projector yields better performance than using the representation from the backbone network. Additionally, predictions from the conventional classifier are more effective than those from the balanced classifier. This is likely due to the early training insta-bility of the balanced classifier, which is trained using an estimate of the data distribution at each epoch, impacting the accurate identification of correctly labeled samples."}, {"title": "D Ablation Study on Subset DI", "content": "DaSC leverages samples from a specific subset D\u00b9 rather than directly from the training dataset D. This strategy enhances performance by leveraging a variety of classes to estimate class centroid while filtering out unreliable samples due to noisy labels and long-tailed distributions. As shown in Table 10, using samples from the subset D\u00b9 achieves better performance than using them directly from the training dataset D."}, {"title": "E Effect of Temperature Scaling", "content": "The proposed DaSC employs temperature scaling to mitigate the inherent bias in model predictions. To explore its impact, Fig. 10 presents the prediction score of each sample relative to the distance from the closest class centroid. These results indicate that temperature scaling assigns higher weights to reliable samples closer to the centroid (i.e., higher prediction scores), highlighting their importance, while giving lower weights to unreliable samples farther from the centroids."}]}