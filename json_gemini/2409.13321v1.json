{"title": "SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report Automation", "authors": ["Jinge Wu", "Yunsoo Kim", "Daqian Shi", "David Cliffton", "Fenglin Liu", "Honghan Wu"], "abstract": "Inspired by the success of large language models (LLMs), there is growing research interest in developing LLMs in the medical domain to assist clinicians. However, for hospitals, using closed-source commercial LLMs involves privacy issues, and developing open-source public LLMs requires large-scale computational resources, which are usually limited, especially in resource-efficient regions and low-income countries. We propose an open-source Small Language and Vision Assistant (SLaVA-CXR) that can be used for Chest X-Ray report automation. To efficiently train a small assistant, we first propose the Re\u00b3Training method, which simulates the cognitive development of radiologists and optimizes the model in the 'Recognition', 'Reasoning', and 'Reporting' training manner. Then, we introduce a data synthesis method, RADEX, which can generate a high-quality and diverse training corpus with privacy regulation compliance. The extensive experiments show that our SLAVA-CXR built on a 2.7B backbone not only outperforms but also achieves 6 times faster inference efficiency than previous state-of-the-art larger models 1.", "sections": [{"title": "1 Introduction", "content": "In recent years, the integration of artificial intelligence into medical imaging has advanced diagnostics and patient care, particularly as assistance tools. Notable large language models (LLMs) include GPT-4, GPT-4-Vision (OpenAI, 2023), and LLaVA (Liu et al., 2023c), which demonstrate impressive performance in general domain tasks and show promising performance in medical (vision and) question answering (Zhou et al., 2023), as in MedPaLM (Singhal et al., 2022), LLaVA-Med (Li et al., 2023) and Med-PaLM M (Tu et al., 2023).\nHowever, these LLMs encounter limitations that hinder their practical application in real-world medical data involving patient information. For instance, assistants like ChatGPT utilizing the GPT-4-Vision model or similar proprietary API services raise concerns regarding the privacy of patient information. To avoid any release of patient information, some hospitals adopt a cautious approach by storing the data in an environment with restricted intranet access and no internet connection (Basil et al., 2022; Basu and Guinchard, 2020). Even for hospitals storing data with internet access, the API services must adhere to strict regulations such as the Health Insurance Portability and Accountability Act (U.S. Department of Health and Human Services). In other words, hospitals must de-identify clinical notes and establish secure connections to mitigate the risk of privacy breaches, which limits the usage of such services.\nWhile proprietary models may face privacy concerns, open-source LLMs such as LLaVA (Liu et al., 2023c) provide the advantage of local usage even within an offline environment. However, it is essential to note that even open-source models, despite their greater accessibility, their relatively high computational resource demands, and lower performance in comprehending medical knowledge still remain unresolved (Jin et al., 2024). Meeting these demands proves to be a significant hurdle in many hospitals, highlighting the need for work to make an efficient model with improved performance for a medical imaging assistant. Moreover, when adapting LLMs to the medical domain, most existing works adopt the public MIMIC-III and MIMIC-IV (Johnson et al., 2016, 2023) datasets for training. However, access to these data is restricted to credentialed individuals with CITI training (CITI). This restriction extends to any models/products derived from these datasets, including synthetic data or generative models trained using them.\nTo this end, we first propose an efficient training method Re\u00b3Training, which employs a strategically layered approach, systematically deepening the model's expertise from fundamental visual comprehension to advanced clinical articulation. Mirroring the cognitive development of radiologists, the training pipeline comprises three sequential stages: Radiological Pattern Recognition Study (Recognition), Diagnostic Reasoning with Instruction Tuning (Reasoning), and Clinical Reporting Learning (Reporting). (1) The recognition stage lays the foundational groundwork, focusing on basic visual feature extraction and medical concept alignment. (2) Building upon this, the Reasoning stage elevates the model's capabilities to interpret these patterns within a diagnostic framework. (3) Finally, the Reporting stage hones the model's ability to synthesize its learned knowledge into coherent, professional-grade radiology reports. This progressive intensification of skills ensures that each subsequent stage leverages and extends the competencies acquired in the previous one, ultimately yielding a model proficient in generating comprehensive and accurate radiology reports.\nThen, we further introduce an efficient data synthesis method RADEX, which can synthesize a high-quality and diverse training corpus (RADiology Expertise corpus) from publicly available clinical-standard case reports and X-ray image pairs. These are sourced from radiopaedia.org, which is publicly accessible and free of privacy concerns. We use the synthesized RADEX to train SLAVA-CXR, a small but efficient vision and language assistant specifically tailored for CXR report automation. Our extensive experiments show that our SLAVA-CXR achieves the best performance in CXR report generation and summarization compared to existing state-of-the-art LLMs.\nOur paper makes the following contributions:\n\u2022 We propose a small vision and language assistant SLAVA-CXR, which outperforms larger models with a maximum of 6 times faster inference time.\n\u2022 We propose an efficient training method Re\u00b3Training and an efficient data synthesis method RADEX to enable our SLAVA-CXR to accurately comprehend complex medical data, achieving superior performance.\n\u2022 We perform extensive experiments on two standard benchmarks and further invite medical experts to conduct human evaluation to prove the effectiveness of our model."}, {"title": "2 Related Works", "content": "In the critical domain of clinical applications, where precise interpretation of medical images is paramount for accurate diagnosis and patient care, the demand for sophisticated multimodal large language models has become increasingly apparent\nNotable contributions in this field include PMC-VQA (Zhang et al., 2023) and LLaVA-Med (Li et al., 2023), which have demonstrated promising capabilities in bridging the gap between visual input and textual understanding in medical contexts. However, these models often encounter significant limitations when tasked with generative assignments, such as comprehensive report generation. This shortcoming can be largely attributed to their training paradigm, which predominantly relies on visual question answering datasets extracted from PubMed literature articles, specifically focusing on figure captions and associated legends.\nRecent advancements in the field have seen a shift towards addressing the complex task of medical report generation (Liu et al., 2021). Notable examples include CXR-LLaVA (Lee et al., 2023) and MAIRA-2 (Bannur et al., 2024), which have made substantial strides in generating detailed and clinically relevant reports from medical images. However, these models are not without their drawbacks. They often rely on intensive computational resources, which can pose significant challenges for real-time clinical applications due to their resource requirements and inference latency. Furthermore, many of these models are trained on proprietary or credentialed datasets, raising concerns about their accessibility and the potential for widespread adoption and validation by the broader community."}, {"title": "3 Re\u00b3Training", "content": "Our innovative Re\u00b3Training pipeline is designed to systematically develop a model's capabilities in chest X-ray (CXR) report automation. This approach mirrors the cognitive development of radiologists, progressing through three critical stages: Recognition, Reasoning, and Reporting. Each stage builds upon the previous, gradually deepening the model's expertise from fundamental visual comprehension to advanced clinical articulation."}, {"title": "3.1 Recognition", "content": "In the recognition stage, we train the model to associate visual features in chest X-ray images with corresponding medical concepts. This process involves generating captions for image-text pairs, which helps the model learn to describe radiological patterns accurately. More specifically, our approach focuses on training the projector \\(P\\), which connects the vision encoder \\(E\\) and the Phi-2 language model \\(L\\). We keep \\(E\\) and \\(L\\) frozen during this phase to preserve their pre-trained knowledge while allowing \\(P\\) to learn the specific visual-textual mapping required for chest X-ray interpretation. This method enables the model to develop radiological pattern recognition skills without altering its fundamental visual and linguistic capabilities.\nWe formulate the training objective for generating accurate captions as follows:\n$\\min_{\\theta} L_{R(1)} = \\sum log p(y|E(x), P_{\\theta}(E(x)))$ (1)\nwhere \\(\\min_{\\theta} L_{R(1)}\\) denotes the optimization objective for the first stage (Recognition) of our Re\u00b3Training pipeline, aiming to minimize the loss function \\(L_{R(1)}\\) with respect to the parameters \\(\\theta\\) of the projector \\(P\\). \\(x\\) is the input image, \\(y\\) is the corresponding text, and \\(p\\) is the probability distribution over the vocabulary. To ensure comprehensive learning, we utilize a diverse dataset \\(D_{R(2)}\\) comprising 560 image-text pairs from Liu et al. (2023c), and extract 1,436 CXR-related pairs from Li et al. (2023). This curated dataset ensures broad coverage of medical imaging concepts while maintaining a focus on CXR-specific features."}, {"title": "3.2 Reasoning", "content": "The reasoning stage builds upon the pattern recognition skills developed in the previous phase, extending the model's capabilities to include diagnostic reasoning. In this stage, we train the model to interpret chest X-ray images beyond simple feature identification, enabling it to draw clinical implications and suggest potential diagnoses.\nIn this stage, we fine-tune all components - \\(E\\), \\(P\\), and \\(L\\) - to capture the nuances of CXR images and associated diagnoses. The loss function is:\n\\(L_{R(2)} = L_{ce}(y_{pred}, y_{true}) + \\lambda L_{reg}(\\theta_E, \\theta_P, \\theta_L)\\) (2)\nwhere \\(L_{ce}\\) is the cross-entropy loss between predicted and true outputs, \\(L_{reg}\\) is a regularization term, and \\(\\lambda\\) is a hyperparameter balancing the two terms. The input-output relationship is defined as \\(y_{pred} = L(P(E(x)), i)\\), where \\(x\\) is the input image and \\(i\\) is the instruction.\nFor this phase, we curate a collection of instruction tuning dataset \\(D_{R(2)}\\), consisting of 632k samples from Liu et al. (2023c) and CXR-specific instructions extracted from Li et al. (2023). This dataset is designed to expose the model to a wide range of diagnostic scenarios and instruction formats, enhancing its ability to reason about CXR findings in various clinical contexts."}, {"title": "3.3 Reporting", "content": "The reporting stage focuses on optimizing the model for CXR report generation and enhancing its ability to follow specific writing instructions. In this final phase, we fine-tune the model to produce radiology reports that are coherent, accurate, and clinically relevant. Our approach addresses two key aspects: the generation of comprehensive CXR reports and the adherence to varied writing instructions. This stage builds upon the diagnostic reasoning skills developed in earlier phases, extending them to include the structured articulation of findings in a format consistent with radiological reporting standards. By integrating these elements, we aim to create a system capable of generating professional-quality reports while adapting to different reporting styles and requirements. To achieve these objectives, we continue to fine-tune all components with a multi-task objective:\n\\(L_{R(3)} = \\alpha_1 L_{rep} + \\alpha_2 L_{instr} + \\alpha_3 L_{reg}\\) (3)\nwhere \\(L_{rep}\\) is the report generation loss, corresponding to our first key aspect of producing comprehensive CXR reports. \\(L_{instr}\\) is the instruction-following loss, addressing our second key aspect of adapting to various writing instructions. \\(L_{reg}\\) is a regularization term to prevent overfitting, and \\(\\alpha_1, \\alpha_2, \\alpha_3\\) are weighting coefficients that balance the importance of each component in the overall objective. For this phase, we employ the RADiology Expertise Corpus (RADEX), a custom dataset we developed to support our training objectives. RADEX comprises comprehensive radiological reports, expert discussions, and diverse instructions, providing a rich information source for enhancing the model's diagnostic reasoning capabilities. This dataset is designed to expose the model to real-world clinical scenarios, improving its ability to interpret chest X-ray images accurately. Further details about RADEX, including its composition and creation process, will be discussed in the following."}, {"title": "4 RADiology Expertise Corpus (RADEX)", "content": "To address limitations in traditional clinical report generation datasets, as shown in Figure 2, we introduce RADEX, a novel corpus derived from peer-reviewed, open-access radiology case studies. Unlike datasets such as MIMIC-CXR, which often lack comprehensive assessments and may be biased towards severe conditions, RADEX offers a more diverse and holistic representation of radiological cases. This approach aims to enhance model training by providing richer contextual information, including diagnostic rationales and alternative interpretations, thereby improving the quality and breadth of generated reports.\nMore specifically, our data construction process encompasses the following key components: (1) Synthetic Clinical Note Generation: Utilizing GPT-4, we generate structured clinical notes that integrate the following components: case description that provides basic background information for the case; case representation of the image that includes the specific imaging findings; case discussion that provides in-depth analysis and diagnostic reasoning for the case. This process ensures a consistent format while preserving the depth and nuance of the original case studies. (2) Conversational Context Integration: We enhance the model's instruction-following capabilities by incorporating diverse conversational data. This includes relevant dialogue samples from Wu et al. (2023) and case discussion information from our dataset. By exposing the model to various instruction formats and contextual scenarios, we aim to improve its adaptability in understanding and responding to a wide range of clinical communication styles and requirements."}, {"title": "5 Experiments", "content": "In this section, we first introduce the settings for our evaluation\u00b2. We then illustrate the detailed results of our proposed method.\n5.1 Experiment Setup\nEvaluation Data. The evaluation datasets contain MIMIC-CXR (Johnson et al., 2019) and IU-Xray (Demner-Fushman et al., 2016) for the task of radiology report generation and summarization.\nTask Description. We perform the evaluation on CXR report generation and summarization. Each"}, {"title": "5.2 Automatic Evaluation", "content": "Generation Results. Table 3 presents a comparison of various models' performance in radiology report generation across multiple datasets. The results reveal that our SLaVA-CXR consistently outperforms other models across a wide range of metrics. The results show that larger models such as LLaVA-Med, LLaVAv0, and LLaVAv1.5 did not demonstrate the expected performance gains in these domain-specific tasks, challenging the assumption that increased model size inherently leads to improved performance in specialized domains. Moreover, the medical domain-finetuned variant, LLaVA-Med, fails to distinguish itself significantly in our evaluation metrics. The discrepancy between LLaVA-Med's performance and that of our SLaVA-CXR model underscores the importance of not only our constructed high-quality data, but also the introduced Re\u00b3Training methodology employed in adapting LLMs to specialized medical applications.\nSummarization Results. We further report the summarization performance in Table 4. As we can see, with fewer parameters, our SLaVA-CXR outperforms previous strong baselines and archives the best results on most metrics. This observation highlights the need for more nuanced and targeted training strategies when developing models for complex medical imaging interpretation and reporting tasks. TinyGPT-V, while generally lagging behind SLaVA-CXR, shows promise in the CheXbert metric across both tasks, suggesting a potential strength in capturing clinically relevant information despite its compact architecture. These findings underscore the importance of tailored training approaches and architectural considerations in developing effective models for specialized medical tasks, particularly in the realm of radiology report automation."}, {"title": "Classification Results", "content": "To further validate our model's capacity to effectively utilize visual information, we perform the classification task and report the results in Table 5. For a fair comparison, we employ the CheXpert labeler to analyze the generated report and compared the classification performance against the original MIMIC-CXR labels. This approach allows us to assess the model's ability to accurately identify and describe clinically relevant findings from CXR images. We calculate the Area Under the Curve (AUC) scores for 14 distinct radiological findings, comparing our SLAVA-CXR model against several baseline models including LLaVA0, LLaVA1.5, LLaVA-Med, TinyGPT-V, and LLaVA_phi.\nThe results show that SLaVA-CXR consistently outperforms other models across the majority of findings, showcasing superior classification capabilities. Notably, in detecting critical conditions such as 'No Finding', 'Edema', and 'Lung Opacity', SLaVA-CXR exhibits markedly improved performance compared to its counterparts. This enhanced accuracy is also evident in complex cases that require nuanced interpretation of radiographic features, such as 'Enlarged Cardiomediastinum', and 'Lung Lesion'. Furthermore, the model's high performance in identifying 'Pneumonia' and 'Pleural Effusion' underscores its advanced capability in recognizing both parenchymal and pleural abnormalities. The model's success in these areas suggests a sophisticated understanding of the radiographic manifestations of diseases affecting different anatomical compartments of the thorax, from the lung parenchyma to the pleural space."}, {"title": "5.3 Human Evaluation", "content": "We further invite radiologists and doctors with expertise in CXR to enrich evaluations. This evaluation is designed to assess three critical aspects of report generation and summarization: correctness, completeness, and coherence. A total of 50 samples are evaluated, with experts rating each aspect on a scale of 0 to 5, where 0 represents the lowest quality and 5 is the highest."}, {"title": "6 Analysis", "content": "We further provide several analyses to better understand our approach.\n6.1 Quantitative Analysis\nTable 6 presents a ablation study of the SLaVA-CXR model, which incorporates a Re\u00b3Training pipeline consisting of three stages: Recognition, Reasoning, and Reporting. Notably, we observe a clear and consistent progression in performance from Setting (a), which only includes the Recognition stage, through Setting (b), which adds the Reasoning stage, to our full SLaVA-CXR model incorporating all three stages. For instance, in the MIMIC-CXR Generation task, our full model achieves substantial improvements in ROUGE-L and CIDEr. Similarly, for the IU-Xray Summarization task, we see significant gains in BERTScore and CIDEr. This progression is evident across most metrics for both generation and summarization tasks on both datasets, validating the cumulative benefits of each proposed training stage. These results underscore the effectiveness of our Re\u00b3Training approach and the value of the RADEX data synthesis method in enhancing the model's capabilities for medical image analysis and report generation tasks."}, {"title": "6.2 Inference Efficiency", "content": "Table 7 presents a comprehensive analysis of inference efficiency across different models and tasks. The results demonstrate that SLaVA-CXR outperforms both LLaVA-Med and LLaVA v1.5 in terms of computational efficiency across all tasks.\nFor the report generation task, SLaVA-CXR exhibits remarkable speed improvements. On the IU-Xray dataset, it achieves an inference time of 4.45 seconds per instance, which is approximately 4.4 times faster than LLaVAv1.5 and 15.1 times faster than LLaVA-Med. The efficiency gap is even more pronounced for the MIMIC dataset, where SLAVA-CXR processes each instance in just 3.32 seconds, outpacing LLaVA-Med by a factor of 5.3.\nThe summarization task showcases even more impressive efficiency gains. Our SLaVA-CXR processes IU-Xray summarization in 1.10 seconds"}, {"title": "6.3 Qualitative Analysis", "content": "Table 4 presents a qualitative comparison of CXR interpretations generated by LLaVA-Med and SLAVA-CXR across three exemplar cases, revealing significant performance disparities.\nIn Example 1, SLaVA-CXR accurately identifies a right-sided pneumothorax with complete lung collapse and a small right pleural effusion, closely aligning with the ground truth. LLaVA-Med, however, misinterprets these findings, incorrectly identifying a left-sided pleural effusion and missing the pneumothorax. Example 2 demonstrates SLaVA-CXR's ability to provide concise, accurate assessments for normal findings, mirroring the ground truth. In contrast, LLaVA-Med generates irrelevant and potentially misleading information, showing a tendency towards hallucination when faced with normal findings. Example 3 further highlights SLaVA-CXR's superior performance, correctly identifying subtle hilar adenopathy and accurately noting the absence of pleural effusion, while LLaVA-Med focuses on less relevant details and misses key findings.\nThroughout, SLaVA-CXR consistently demonstrates more accurate anatomical localization, correctly differentiating between right and left-sided findings. This spatial accuracy is crucial in radiological reporting. Moreover, SLaVA-CXR's outputs generally exhibit a more structured and professional reporting style, closely resembling the language and format used in clinical radiology reports. These qualitative results complement our quantitative findings, providing compelling evidence of SLaVA-CXR's advanced capabilities in CXR interpretation and report generation. The analysis underscores SLaVA-CXR's enhanced proficiency in producing precise, clinically pertinent CXR interpretations compared to LLaVA-Med."}, {"title": "7 Conclusion", "content": "In this work, we propose SLaVA-CXR, a small-scale language and vision assistant for radiology report automation. Our proposed approach addresses key challenges in developing medical domain LLMs through the innovative Re\u00b3Training method and RADEX data synthesis technique. Built on a 2.7B parameter backbone, SLAVA-CXR outperforms larger models while achieving six times faster inference efficiency. This research demonstrates significant improvements in chest X-ray report automation, offering an efficient, privacy-compliant solutiofor medical imaging AI."}, {"title": "Limitations", "content": "Despite the promising results, there remains significant room for improving SLaVA-CXR's performance. A critical challenge that persists, common among large language models (LLMs), is the issue of hallucination - the generation of plausible but factually incorrect or unsupported information. In the context of medical reporting, such hallucinations could lead to serious clinical misinterpretations. Our model, while advanced, is not immune to this phenomenon. Future research could dive intensively into developing robust strategies to mitigate hallucinations.\nAdditionally, SLaVA-CXR, along with other models in this study, is trained on single-image inputs. However, clinical practice often involves multiple views (e.g., frontal and lateral) for CXRs to provide a more complete assessment of a patient's condition. This limitation may restrict the model's ability to detect certain pathologies or anatomical variations that are more apparent in alternative views. Future developments should explore multi-view support for both training and inference to enhance the model's diagnostic capabilities and align more closely with clinical workflows.\nLastly, our current model focuses primarily on CXRs. However, radiology encompasses a wide range of imaging modalities and body systems. Expanding the model's capabilities to other areas of radiology and medical imaging would increase its utility in broader clinical contexts."}, {"title": "Ethical Considerations", "content": "This research adhered to strict ethical guidelines in handling medical data. Our research uses de-identified clinical notes from MIMIC-CXR, and IU-Xray ensuring patient privacy protection. To access MIMIC-CXR data, researchers have completed necessary training course and signed the data use agreement. In compliance with data protection regulations, we exclusively employed locally hosted Large Language Models (LLMs) for data processing, preventing any unauthorized access or transmission of sensitive information."}, {"title": "C Related works", "content": "C.1 Large Language Vision Model (LLVM)\nSimilar to how ChatGPT quickly evolved into a multimodal vision and language model, open-source large language models (LLMs) have been a driving force in the development of vision-language models. This progress is evident in models like LLaVA (Liu et al., 2023c) and MiniGPT4 (Zhu et al., 2023), as well as subsequent versions such as LLaVA-v1.5 (Liu et al., 2023b) and MiniGPT-v2 (Chen et al., 2023). These models have effectively showcased that visual instruction tuning significantly enhances multimodal comprehension abilities. Notably, following the success of phi-2, also referred to as a small language model (SLM), both TinyGPT-V(Yuan et al., 2023) and LLaVA-phi (Zhu et al., 2024) mark a paradigm shift towards cost-effective and powerful models, facilitating research in smaller vision-language models.\nC.2 Training Method for LLVM\nLLMs usually benefit from the significant advancements from pretraining on vast amounts of data with unsupervised learning. However, it may not be optimized for a specific domain on a specific task. Supervised finetuning in this case bridges this gap by taking advantage of the general language understanding captured during pre-training and adapting it to a target task by guiding with the labeled data. Recent works have shown that following natural language instructions and completing real-world tasks can effectively improve the zero-shot and few-shot generalization abilities of LLMs (Taori et al., 2023; Chiang et al., 2023). Furthermore, this has been expanded into LLVM by Liu et al. (2023c) with 2 staged training methods, concept alignment, and visual instruction tuning. Later, Li et al. (2023) tested this 2 staged training method in the biomedical domain, by training with biomedical multimodal instruction-following data from PMC-15M biomedical figure and caption pairs.\nTo address these issues, this study contributes open-source, small-scale multimodal large language model which is built on the Phi-2-2.7B model, specifically tailored for radiology report automation. The training method for the model is a three-stage training process in which we add the final training stage with high-quality CXR images and diverse textual data formats to optimize its performance and applicability in radiological contexts."}, {"title": "D Prompts Templates", "content": "Listing 1: Prompts template for synthetic clinical note generation.\n[Instructions]:\nYou are an expert medical assistant AI capable\nof modifying clinical documents to user\nspecifications. You make minimal changes to\nthe original document to satisfy user requests.\nYou never add information that is not already\ndirectly stated in the original document.\nRestructure the given text into a radiology\nreport finding. Remove any information not\ndirectly observable from the current imaging\nstudy. For instance, remove any patient\ndemographic data, past medical history, or\ncomparison to prior images or studies.\n[Input]:\n<Case Description>\nLinear radiolucencies are noted along left\nmediastinal borders.\nNo evidence of pneumothorax or pleural\neffusion.\nLung fields appear clear.\nNo concerning bony abnormality identified.\n<Case Presentation>\nRight sided pleuritic chest pain, shortness of\nbreath, diminished breath sounds at right\nlower zone.\n<Case Discussion>\nThis is a classical case of pneumomediastinum\nand its presentation.\nAsthma is the most common cause.\n[Output]:\nFindings: Chest X-ray demonstrates linear\nradiolucencies along the left mediastinal\nborders, suggestive of pneumomediastinum. The\nlung fields appear clear without evidence of\npneumothorax or pleural effusion. No\nconcerning bony abnormalities are identified.\nThe cardiac silhouette is within normal limits.\nImpression: Pneumomediastinum present. No\nother acute cardiopulmonary abnormalities are\nidentified."}]}