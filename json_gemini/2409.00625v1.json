{"title": "ENTITY-AWARE BIAFFINE ATTENTION MODEL FOR\nIMPROVED Constituent PARSING WITH REDUCED\nENTITY VIOLATIONS", "authors": ["Xinyi Bai"], "abstract": "Constituency parsing involves analyzing a sentence by breaking it into sub-\nphrases, or constituents. While many deep neural models have achieved state-\nof-the-art performance in this task, they often overlook the entity-violating issue,\nwhere an entity fails to form a complete sub-tree in the resultant parsing tree. To\naddress this, we propose an entity-aware biaffine attention model for constituent\nparsing. This model incorporates entity information into the biaffine attention\nmechanism by using additional entity role vectors for potential phrases, which en-\nhances the parsing accuracy. We introduce a new metric, the Entity Violating Rate\n(EVR), to quantify the extent of entity violations in parsing results. Experiments\non three popular datasets\u2014ONTONOTES, PTB, and CTB\u2014demonstrate that our\nmodel achieves the lowest EVR while maintaining high precision, recall, and F1-\nscores comparable to existing models. Further evaluation in downstream tasks,\nsuch as sentence sentiment analysis, highlights the effectiveness of our model and\nthe validity of the proposed EVR metric.", "sections": [{"title": "INTRODUCTION", "content": "Constituent parsing is to construct the syntactic tree for a given sentence whose words constitute\nleaf nodes. In the syntactic tree, non-terminal nodes are called constituents. For intuition, This tree serves as an important feature to represent a sentence, and has been applied\nin many high-level natural language tasks, such as sentiment analysis Kim et al. (2019), relation\nextraction Jiang & Diesner (2019), natural language inference Chen et al. (2017), and machine\ntranslation Ma et al. (2018).\nIn this subject recent chart-based neural models have achieved state-of-the-art results by using ad-\nvanced text encoders (i.e., BERT and XLNet) to represent all possible spans, where each span stands\nfor a single word or several consecutive words from a sentence. Such models score each span, and\nthen employs CKY algorithm to choose the resultant tree in terms of the highest score Kitaev et al.\n(2019); Kitaev & Klein (2018); Mrini et al. (2020); Tian et al. (2020). Despite their astonishing suc-\ncess in light of precision, recall and F1-score, few consider whether such arts also work well in the\nentity-violating issue: the entity span parsed by a neural model does not conform to the true phrasal\none of human natural language. As in Fig. 1, \u201cDepartment of Defense\u201d as a true organization (ORG)\nentity should be in a complete sub-tree but is parsed by previous neural models so that its words\nare distributed into two separate sub-trees .\nFinkel et al. Finkel & Manning (2009; 2010) firstly focus on this issue. They manually annotate\nthe original dataset ONTONOTES Hovy et al. (2006) by adding entity nodes to the constituent\nparsing trees. This new dataset can greatly promote the model to output the consistent entity spans.\nBesides, other works Li et al. (2013; 2018) endeavor to explore the entity-related label as heuristic\ninformation, for instance, Chinese word 'SHI' can be labeled with 'GPE-END'. Different from\naforesaid works that absorb entity-related information, Yang et al. Wang et al. (2019) considered\nutilizing the named entity recognition (NER) task to capture the entity information, and combining\nthe PCFG algorithm for parsing, of which the parameter sharing enjoys the benefits of two tasks.\nHowever, since these entity-related parsing models either demand manual annotations or implicitly\nmerge entity information into training process of constituent parsing, it may be underestimate the\npotential of the entity information.\nTo attack such problems above, we propose a biaffine attention based parsing model by integrat-\ning entity information into constituent parsing process without any manual annotations. Our model\noverally follows the basic biaffine attention model Zhang et al. (2020). Similarly, each word is rep-\nresented as two role vectors: start role vector and end role vector. Differently, we encode the entity\ninformation as the entity role vector and append it to the previous two role vectors. In this simple\nway, the proposed vector is more informative and reasonable than before. By using the proposed\nentity-aware vector for each word, the basic biaffine attention model can boost potential score for\neach entity span. In our empirical studies, we find that although the entity information reinforces the\nability of attention model to parse consistent phrasal span, it is lack of effective supervision and the\nentity information might be ignored in the learning process. To this end, we treat a simple NER task\nas the supervision similar to Yu et al. (2020). In our model, we add a binary NER model to share\nthe same word embeddings with the primary parsing model. It helps our parsing model to capture\ninformative entity structures. The overall model is light-weight, simple yet effective, free from large\namounts of manual annotations, and even achieves the state-of-the-art performance on the bench-\nmarks. To evaluate the entity-violating degree, we propose an very intuitive metric to calculate the\nratio of the entity-violating spans to the entire samples.\nIn summary, this paper makes the following contributions.\n1) We put forward an entity-aware biaffine attention model for constituent parsing, which\nencodes the entity information of a span as the attention input component.\n2) To further exert the entity information in the attention model, we introduce an auxiliary\nbi-nary NER model in the whole parsing model, in order to make the parsing model aware\nof entity information.\n3) Experiments on three datasets including ONTONOTES, PTB and CTB show that our strat-\negy greatly promote the parsing performance, especially based on entity-violating metric.\nMore importantly, the proposed model achieves the sound performance in terms of preci-\nsion, recall and F1-score.\n4) To make our model more convincing, we apply our parsing model for a typical downstream\ntask\u2014sentence classification Kim et al. (2019). Extensive results verify that our model\nperforms best when comparing with several well-behaved parsing siblings."}, {"title": "RELATED WORK", "content": ""}, {"title": "CONSTITUENT PARSING.", "content": "Most constituent parsing models can be roughly classified into three types: transition-based,\nsequence-based and chart-based ones, according to which decoding algorithm they choose. For\ntransition-based models, without no need of the decoder, they directly build the parsing trees through"}, {"title": "METHOD", "content": ""}, {"title": "PARSING MODEL", "content": "This section explores the entity-aware biaffine attention model for constituent parsing, which follows\nthe two-step parsing model Zhang et al. (2020). The first step (span-parsing) decides whether a span\nis a node in the resultant constituent tree, and the second label-parsing model labels each node with\nPOS tag. The two steps share the same contextual word embeddings.\nFor the given sentence, the first input\nembedding for each word (e.g. Xi) is the concatenation of word embedding, and its char-level\nfeature gained through CharLSTM on each word. To obtain contextual information, we feed the\nembedding of each word into a 3-layer BiLSTM layer. The BiLSTM module outputs two hidden\nembeddings f and b for each word from forward and backward direction. Then for each word, e.g.\nwordi we concatenate fi and bi+1 to form its contextual representation hi:\n$h_i = [f_i; b_{i+1}].$\nIn biaffine model, the role of each word could be either the start of a span or the end of a span.\nAfter yielding the contextual representation h of each word, we implement four MLP (multi-layer\nperceptron) modules for each word, resulting in four vectors, $U_{span_l,i}$, $U_{span_r,i}$, $U_{label_l,i}$ and $V_{label_r}$\n(collectively referred to as $v_{l,i}$, $U_{r,i}$ in Fig. 2). The first two participate the span-parsing model\ndeciding whether a span exists or not while $V_{label_i}$ and $V_{label_r}$ are used to label the span with POS\ntag. The dimension size of $U_{span_l,i}$ and $U_{span_r,i}$ is 450, while that of $U_{label_l}$ and $V_{label_r}$ is 100."}, {"title": "SPAN PARSING.", "content": "We construct an entity-aware biaffine attention model for constituent parsing. In our baseline biaffine\nmodel Zhang et al. (2020), word always uses the same start vector $U_{span_l,i}$; when pairs with different\nend word; or wordk. We design a unique start and end vector for each span(i,j) based on start\nvector of wordi i.e. $U_{span_l,i,i}$, end vector of wordj i.e. $U_{span_r,j}$ and entity information of span(i,j).\nMore specifically, for a given sentence with n words, we construct two n * n embedding matrixes,\nembed-matrix-l: L and embed-matrix-r: R, which store the start and end vectors for all possible\nspans. For example, L[i, j] and R[i, j] are the start and end vectors applied in biaffine calculation\nof span(i,j). As a consequence, the issue left now is how to build L and R? For a given sentence\nwith n words, the n * n matrix S represent all possible spans. S[i, j] means a span start from wordi\nand end at wordj. It is obvious that these nodes in the Lower triangle of matrix S bear no meaning\n(we cannot build a span of words starts from the back and ends at the front), which will not be\nconsidered in practice. We traverse every node located in the upper triangle of S. For a certain node\nS[i, j], we check whether span(i,j) is an entity, and embed entity information (i.e. 0 or 1) into a\nvector $V_{is_entity}$ with length 50. Then we concatenate $V_{is_entity}$ to $U_{span_l,i,i}$ and $U_{span_r,j}$, resulting\nin two vector $U_{l(i,j)}$ and $U_{r(i,j)}$ of length 500. Store $U_{l(i,j)}$ in L and $U_{r(i,j)}$ in R at position (i, j),\nrespectively. When doing biaffine operation, we traverse the upper triangle of matrix S again, for\nevery node(i, j), and fetch the start vector $v_{l(i,j)}$ from L and end vector $U_{r(i,j)}$ from R at position\n(i, j). Then both $V_{l(i,j)}$and $V_{r(i,j)}$ are fed to the following biaffine operation:\n$v^T_{l(i,j)} W U_{r(i,j)},$\nwhere $v_{l(i,j)}$ and $U_{r(i,j)}$ are vectors with length d=500, and W is the learning parameter with d * d.\nThe result is a scalar indicating the potential score for span(i,j) being a node in the sentence's\nconstituent parsing tree, and we mark it as s(i, j).\nFor a sentence x, the score of a tree y is the sum score of all spans containing in the tree:\n$s(x,y) = \\sum_{(i,j) \\in y} s(i, j).$\nUnder TreeCRF algorithm, the condition probability of the golden tree is:\n$p(\\hat{y} | x) = \\frac{e^{s(x,y)}}{Z(x)} = \\frac{e^{s(x,y)}}{\\sum_{y' \\in T(x)} e^{s(x,y')}},$\nwhere Z(x) is the sum score for all possible trees, which can be calculated by inside algorithm. T(x)\nis a set including all possible trees for x.\nBased on the scores of all spans calculated above, we use the CKY algorithm to decode the parse\ntree with the highest score:\n$\\bar{y} = arg \\max_y s(x, y) = arg \\max_y p(y | x).$"}, {"title": "LABEL PARSING.", "content": "After finishing the span parsing work, we obtain a constituent tree structure without labels. We\nfeed $V_{label_l,i}$ and $U_{label_r,j}$, into the following biaffine attention operation to finish the labeling job for\nspan(i,j):\n$V^T_{label_l,i} W U_{label_r,j}$\nIn this module, the corresponding parameter W is c * d * d, where c is the number of POS types and\nd is the length of $V_{label_l,i}$ and $U_{label_r,j}$. The result is a probability vector of length c."}, {"title": "ENTITY COMPATIBLE SPLIT METHOD", "content": "Our model belongs to chart-based parsing model. It relies on CKY algorithm to decode. The result\ntrees are binarized trees agreeing with CNF rule. However, little training data satisfies a binarized\ntree structure. It is common to use third-party tool (e.g. NLTK) to convert an original parsing\ntree into its binarized form with split choice right or left. As we observed, many entities satisfy a\nsub-tree structure in their original non-binarized parsing trees, violating after these trees binarized.\nOne group of samples suffers from left binarized operation, while other samples suffering from\nright choice. The baseline biaffine attention model Zhang et al. (2020) uniformly chooses left split\nmechanism, which is not friendly to first group. In our model, we compare entity violating number\nof the two split choices for every single sample, and choose those results suffering from lower entity\nviolating number."}, {"title": "NER SUB-TASK", "content": "Besides adding entity information in parsing process, to make our model more related with entity\nstructure, we add a bi-nary NER model that only judges whether a span is an entity or not. The NER\nmodel is also implemented with biaffine attention architecture, which shares the same contextual\nword embedding h with parsing model. Computing entity score of a certain span(i,j) is analogous\nto parsing model above. Two extra MLPs are applied to obtain entity span boundaries representation\n$V_{entity_l}$ and $V_{entity_r}$, then follows the same biaffine process as Label parsing, $V^T_{entity_l} W V_{entity_r}$,\nW is a 2 * d' * d' tensor, 2 represents two types (a span is an entity or not). d' is the dimensions of\n$V_{entity_l,i}$, $V_{entity_r,j}$ and is set to 150 in this paper."}, {"title": "TRAINING LOSS", "content": "The losses for the whole model originate from three parts: Span parsing loss, Label parsing loss and\nNER loss:\n$Loss = Loss_{span} + Loss_{label} + Loss_{entity}.$\nThe first item $Loss_{span}$ is to maximize the probability of the golden parsing tree and has the follow-\ning form:\n$L_{span}(x, y) = \u2212s(x, y) + log Z(x).$\nThe latter two terms correspond to label parsing loss and NER loss, which belong to classification\ntasks and have the common cross entropy loss, i.e.,\n$p(i, j)_c = \\frac{exp (score(i, j)_c)}{\\sum_{c'=1}^C exp (score(i, j)_{c'})},$\n$loss = - \\sum_{(i,j) \\in \\hat{y}} \\sum_{c'=1}^C y_{(i, j)c'} log p_{(i, j)c'}$"}, {"title": "EXPERIMENT", "content": ""}, {"title": "DATASET.", "content": "We conduct experiments on PTB, CTB, and ONTONOTES. The first two datasets are popular bench-\nmarks in constituent parsing tasks, while ONTONOTES contains both parsing and NER tags, which\nis rather suitable for discussing the entity-violating issue. Given that there is no NER tags in datasets\nPTB and CTB5.1, we use third party tool StanfordCoreNLP to obtain the entities on these two\ndatasets. We follow the conventional train/dev/test data split approaches on the three datasets."}, {"title": "METRICS.", "content": "The main idea of this paper is to alleviate the entity-violating issue in constituent parsing task.\nBesides the following frequently-used three metrics, i.e. precision, recall and F1-score, we introduce\na new metric named entity-violating rate named EVR, which indicates how many samples suffer\nfrom entity violating problem. We calculate EVR as follows:\n$EVR = \\frac{num_v}{nums},$\nwhere num, is the number of entities conflicting with constituent trees, and nums is the total\nnumber of samples."}, {"title": "PARAMETER SETTING.", "content": "To compare with baseline biaffine attention method and illustrate the effectiveness of our model in\nEVR aspect, we follow most of hyper-parameter values in Zhang et al. (2020). The main parameters\nare shown in Table 1. We set all the dropout rate to 0.33, and the batch size to 1000, respectively.\nOur model is optimized by Adam, and the learning rate is 0.001 with decay rate 0.999 after every\n100 steps."}, {"title": "COMPARED MODELS.", "content": "We compare our model with 5 methods that have ranked among the best in constituent parsing.\nThe abbreviation of model names are shown in Table 2. B-biaffineZhang et al. (2020) is the ab-\nbreviation of baseline biaffine model, it achieves constituent parsing through basic biaffine atten-\ntion. BeneparT5Kitaev & Klein (2018) is proposed by Kitaev and Klein, it encodes spans with\nself-attention, using MLP to obtain the confidence score of a span to be a node in constituent tree.\nLALXLNetMrini et al. (2020) is a model that represents spans with label attention and follows the\nsame decoding framework as BeneparT5Kitaev & Klein (2018). SAParBertTian et al. (2020) is also\na chart-based model, it encodes spans by n-gram attention. HPSGBertZhou & Zhao (2019) adjusts\nthe form of Head-Driven phrase structure grammar (HPSG) to satisfy both constituent parsing and\ndependency parsing, and fulfills a joint constituent and dependency parsing model sharing syntactic\ninformation of each task.\nGiven that BeneparT5Kitaev & Klein (2018), LALXLNetMrini et al.\n(2020), SAParBertTian et al. (2020) and HPSGBertZhou & Zhao (2019) have not been trained on\nONTONOTES dataset, we just run prediction on PTB and CTB for these four models. For BeneparT5Kitaev &\nKlein (2018), we run parsing operation based on the published benepar tool package. For LALXLNetMrini et al. (2020),"}, {"title": "RESULTS.", "content": "The results of three conventional metrics precision, recall, F1-score and our introduced EVR are\nshown in Table 2 and Table 3. In each experiment performance was averaged over seven runs.\nThe superscripts of these compared models indicate the pre-trained embeddings, while in the lower\npart of our models, these superscripts bear the following meaning: OursGC means that we use\nglove word embedding concatenating char-level features as initial input, which is also our kernel\nmethod mentioned above. OursGB replaces char-level feature with Bert-feature, and OursB ini-\ntializes word embedding randomly (without glove), using Bert embedding as feature. Oursright is\na variant based on OursGC, using right-choice binarized tree to compare with our proposed entity\ncompatible split method. OursnoNER cuts off the NER model of OursGC.\nComparison with other methods: When comparing our proposed model with the other five base-\nline models, we can see that our proposed models outperform all the other models on metric EVR.\nThe observations are detailed as follows: a) On ONTONOTES dataset, OursGC reduces more than\n75% of EVR when compared with B biaffine, while maintaining a higher level of the other\nthree metric(precision, recall ,F1-score). b) On PTB dataset, HPSG Bert obtains the highest perfor-\nmance on F1-score with the pre-trained Bert embeddings and shared syntactic information between\nconstituent and dependency parsing. However, these 5 models perform worse more than 5 points of\nEVR than our proposed models OursGC, OursGB. c) On CTB dataset, OursB achieves lower\nF1-score than SAPar Bert, may be due to the inappropriate way of Bert feature used in our model,\nhowever we still gain the best performance on EVR indicator. d) The EVR on ONTONOTES is\nmuch lower than that on PTB and CTB. Since ONTONOTES is a professional NER dataset with\nhigh quality NER labeled data while we apply the third-party tool on PTB and CTB to get NER\nlabels, which has an unexpected result with noise.\nAblation study: a) When comparing OursGC with OursGB across the three datasets, the latter\nimproves performance on PTB and CTB datasets after using Bert features while OursGC achieves\nthe lowest violating rate on ONTONOTES. It indicates that our defined EVR is not that sensitive\nto these pre-trained embeddings. b) When the original binarized tree method is applied, Oursright\nsuffers from higher EVR than OursGC across all the three datasets, which proves the effectiveness"}, {"title": "PERFORMANCE IN DOWNSTREAM TASKS.", "content": "To make our method and the introduced EVR metric more convincing, we extend our parsing model\nto a downstream task: Sentiment Analysis. Kim et al. Kim et al. (2019) introduced a Tree-LSTM\nframework for sentence sentiment classification based on constituent parsing tree. It implements\nbottom-up LSTM operation recursively and sends the root node embedding into a inference layer\nfor classification results. We deploy the tree structure used in Kim et al. (2019) with the counter-\nparts from BeneparT5Kitaev & Klein (2018), LALXLNetMrini et al. (2020), SAParBertTian et al.\n(2020), HPSGBertZhou & Zhao (2019) and B-biaffineZhang et al. (2020), and our proposed model,\nrespectively, in order to compare the effectiveness of parsing tree generated by our model.\nOur model and LALXLNetMrini et al. (2020) achieve the highest accu-\nracy 96.2% on sentence classification on TREC. It implies that our entity-aware biaffine attention\nmodel is more in line with the language model."}, {"title": "CASE STUDY.", "content": "Figure 3 illustrates a case study performed by our proposed model (Fig. 3 (a)) and the baseline\nbiaffine attention model (Fig. 3 (b)) for sentence \u201cWhere is John Wayne airport?"}, {"title": "CONCLUSION", "content": "In this paper, we investigate entity-violating problem in constituent parsing tasks. To alleviate the\nviolating issue, we construct an entity-aware parsing model based on biaffine attention method. We\nmodify the basic biaffine model, making every biaffine operation correlated with its span's entity in-\nformation, without extra manual annotations. Experimental results on ONTONOTES, PTB and CTB\nshow that our proposed model achieves lowest EVR on the three datasets. The best performance of\nour parsing model on downstream task also demonstrates the superiority of our method."}]}