{"title": "Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives", "authors": ["XINYAO MA", "RUI ZHU", "ZIHAO WANG", "JINGWEI XIONG", "QINGYU CHEN", "HAIXU TANG", "L. JEAN CAMP", "LUCILA OHNO-MACHADO"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing scenarios, particularly in simulating domain-specific experts using tailored prompts. This ability enables LLMs to adopt the persona of individuals with specific backgrounds, offering a cost-effective and efficient alternative to traditional, resource-intensive user studies. By mimicking human behavior, LLMs can anticipate responses and actions based on concrete demographic, professional, or experiential profiles. In this paper, we evaluate the effectiveness of LLMs in simulating individuals with diverse backgrounds and analyze the consistency of these simulated behaviors compared to real-world outcomes. In particular, we explore the potential of LLMs to interpret and respond to discharge summaries provided to patients leaving the Intensive Care Unit (ICU). We evaluate and compare with human responses the comprehensibility of discharge summaries among individuals with varying educational backgrounds, using this analysis to assess the strengths and limitations of LLM-driven simulations. Notably, when LLMs are primed with educational background information, they deliver accurate and actionable medical guidance 88% of the time. However, when other information is included in the query, performance significantly drops, falling below random chance levels. This preliminary study sheds light on the potential benefits and pitfalls of automatically generating patient-specific health information for individuals from diverse backgrounds and populations. While LLMs show promise in simulating health personas, our results highlight critical gaps that must be addressed before they can be reliably used in clinical settings. Specifically, our findings suggest that a straightforward query-response model could outperform a more tailored approach in delivering health information. Introducing even simple patient-specific information can either improve or degrade the model's effectiveness. We discuss these insights as an important first step in understanding how LLMs can be optimized to provide personalized health communication while maintaining accuracy and safety.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing (NLP) and artificial intelligence (AI) across a range of tasks, from text generation to complex reasoning. A key aspect of their versatility is their capability for in-context learning (ICL), which allows LLMs to dynamically adapt to different roles and instructions without requiring retraining [10]. This flexibility makes LLMs highly suitable for role-playing tasks, where they can be quickly configured to simulate individuals with specific demographic, professional, or experiential attributes. Through ICL, LLMs can be tailored to serve as agents for various applications, offering an efficient and adaptable alternative to traditional user studies.\nRecent studies have highlighted the potential of LLMs in role-playing scenarios. Zero-shot reasoning abilities have been leveraged with role-play prompting [24] to explore demographic biases by emulating age-specific behaviors [38]. In addition, LLMs have been applied as both subjective and objective evaluators in tasks like text summarization [53] and qualitative coding of health-related free-text survey data [29], illustrating the breadth of perspectives these models can provide with targeted queries. Beyond individual tasks, frameworks such as the Role-Playing Framework [27] allow LLMs to collaborate in multi-agent environments, opening new possibilities for cooperative problem-solving.\nIn this study, we focus on the ability of LLMs to role-play individuals from diverse backgrounds, with a particular emphasis on simulating the comprehension of discharge summaries in clinical settings. Discharge summaries often contain complex medical information that can be challenging for patients with limited medical knowledge to fully grasp. Therefore, tailoring these summaries to align with a patient's background-such as their level of education, frequency of doctor visits, and other demographic factors-is crucial for effective communication and informed decision-making in healthcare.\nTo implement this approach, we crafted role-specific prompts to guide the LLM's behavior. For each scenario, we began with the statement \u201cIf you were a {persona}\u201d, where \u201cpersona\u201d is replaced by a description of the target identity, such as educational attainment, socioeconomic status, or medical experience. This structured prompting allows the LLM to adapt its responses based on the specified profile, enabling it to mimic individuals with distinct levels of medical literacy, ranging from healthcare professionals to patients with no prior medical training. Our assessment focuses on both the consistency and accuracy of the LLM's ability to understand and convey complex medical content to these different audiences. By comparing the outcomes of these LLM-driven simulations with real-world behaviors and reactions, we seek to determine how accurately these models replicate the nuances of human understanding and identify any limitations in their ability to adapt to diverse populations.\nThis work also explores the limitations of LLMs in role-playing such nuanced interactions, revealing challenges such as over-simplification, potential biases, and the boundaries of LLM comprehension. Through this analysis, we identify key areas where current models may fall short in conveying patient-specific information while recognizing their potential as a cost-effective and scalable solution for personalized communication.\nUltimately, this research represents a first step toward leveraging LLMs to automatically produce tailored discharge summaries that are more accessible and understandable for individuals from varied backgrounds. Yet our results showed that the current LLMs are failing for traditionally"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Large Language Models", "content": "Large Language Models (LLMs) [3, 10, 14, 32, 43] have demonstrated exceptional capabilities in creatively addressing diverse NLP and AI tasks. Recent studies have utilized instruction tuning[15,\n39, 49] and reinforcement learning from human feedback (RLHF) [6, 34] that are better aligned with human understanding and develop advanced AI assistants. Efforts are also underway to construct open-source LLMs [9, 40, 44, 57] to accelerate both research and industrial advancements.\nA significant aspect of LLMs' success is their in-context learning (ICL) capabilities [10], which enables models to carry out tasks based on the context provided in the input without requiring explicit retraining. This dynamic adaptability allows LLMs to respond to a wide range of prompts and conditions with increased flexibility. By leveraging ICL, models can assume different roles and perspectives, enhancing their reasoning capability and responsiveness through role playing. The growing emphasis on ICL is crucial for broadening the scope of LLM applications, especially in tasks that demand nuanced adaptation, such as collaborative problem-solving and personalized AI assistance."}, {"title": "2.2 Discharge Summary and LLM-based Summary Generation", "content": "Medical discharge summaries serve as vital communication tools between patients and healthcare providers, ensuring continuity of care after patients leave the hospital. These summaries typically include essential information such as medications, diagnoses, prescriptions, and follow-up instruc-tions and often contain professional medical terms or abbreviations. It is crucial for patients to understand this information in order to adhere to their treatment plans and manage their recovery. Studies have shown that a high-quality discharge summary should be concise, delivered promptly, and contain relevant data. In a survey of 100 hospital-based physicians, respondents emphasized the importance of focusing on pertinent information that patients can easily comprehend and apply to their post-discharge care [45].\nAs a result, there has been growing interest in leveraging LLMs to automatically generate or assist medical discharge summaries [13, 46]. LLMs, with their advanced natural language processing capabilities, offer the potential to generate summaries that are not only accurate but also customized to the patient's comprehension level. LLM-based summary generation is showing great potential in healthcare. By analyzing large sets of medical data and fine-tuning these models with domain-specific knowledge [25], and a tool, such as \u201cDischarge Me!\u201d can automatically generate discharge summaries of clinical electronic health record [52]. Unlike our study, they did not evaluate medical accuracy or alignment with human subjects. The goal of these discharge summary generation tools is to capture the most important medical information while keeping things simple and clear."}, {"title": "2.3 LLMs Personas", "content": "As the ICL capabilities of large models continue to be uncovered, a growing body of work has focused on in-context role-playing and assisted evaluations. While human evaluation remains an option, manually assessing generated text is challenging, resource-intensive, and difficult to scale or repeat as requirements and quality criteria evolve. Consequently, many researchers have begun utilizing LLMs as natural language evaluators, demonstrating promising results [18, 23]. Kong et al. [24] proposed enhancing the zero-shot reasoning abilities of LLMs through role-play"}, {"title": "3 Impersonation Methodology", "content": "Our method consists of two main steps. First, we prompt the LLM using specific role-playing instructions. Second, we evaluate how closely the LLM's generated responses align with those of real human individuals in similar contexts."}, {"title": "3.1 Prompting Large Language Model with Personas", "content": "LLMs are trained to predict the most probable next token, $t_k$, based on the sequence of preceding tokens, $t_1, ..., t_{k-1}$, by maximizing the likelihood function $P_{LLM}(t_k|t_1, ..., t_{k-1})$. This approach enables the model to learn linguistic patterns and generate coherent responses. In this study, we employ pre-trained LLMs without any further fine-tuning, relying entirely on their existing knowledge and capacity to understand and generate natural language.\nFor each task, we generate one or more tokens by providing a task-specific context $c$, which is designed to instruct the LLM on how to approach the problem. The context typically includes a description of the task along with a specific prompt that guides the model towards a relevant answer. In particular, for impersonation tasks, we prefix the context with the instruction \u201cIf you were a {persona}\", where $p$ represents the persona to be simulated. This could be a social identity, such as \u201ca patient with no medical training\u201d, or an area of expertise, such as \u201ca healthcare professional\u201d. By doing so, we tailor the LLM's behavior to fit the expected perspective of the impersonated persona, encouraging the model to generate responses as if it were that individual. This process leverages the model's in-context learning capabilities, allowing it to assume different roles based solely on the information provided in the prompt, without requiring explicit retraining.\nOnce the context is defined, we sample the generated tokens $t$ from the following distribution:\n$P_{LLM}(t/c^{(P)}) = \\prod_{k=1}^{K} P_{LLM}(t_k/c^{(P)},..., c_p^{(P)}, t_1,..., t_{k-1})$\nwhere $c^{(P)}, ..., c_p^{(P)}$ represent the context tokens related to the specific persona $p$, and $t_1, ..., t_{k-1}$ represent the previously generated tokens. This probabilistic approach allows the model to iter-atively generate a sequence of tokens based on both the task context and the previous outputs, ensuring continuity and coherence in the response.\nWe term this approach of shaping the model's behavior based on the provided context as \u201cin-context impersonation.\u201d Using this method, the LLM dynamically adjusts its responses to reflect the persona's perspective, demonstrating its ability to role-play different individuals based on minimal input cues. This technique is particularly useful for generating tailored outputs in complex scenarios, such as simulating how individuals with varying levels of medical knowledge might interpret healthcare information [25, 45, 52].\nIn this paper, in-context impersonation forms the foundation of our evaluation framework, allow-ing us to assess how well LLMs can simulate and align with real human behaviors and perspectives across diverse social and professional contexts. This method highlights the flexibility of LLMs in\""}, {"title": "3.2 Human Subjects Discharge Summary Evaluation", "content": "Discharge summaries serve multiple purposes, with one of their key functions being to facilitate communication between clinicians and patients. A high-quality discharge summary should ac-curately convey medical advice while being easy for patients to understand. To assess patient comprehension and compare their responses to those generated by current LLMs, we conducted a human subject survey focusing on the evaluation of a given discharge summary. Participants read a section of a discharge summary and then answered ten follow-up questions aimed at assessing their understanding and recall of the information provided.\nThe questions were divided into two categories: information-based and perception-based. The eight information-based questions tested whether participants could accurately extract information from the discharge summary. These included direct questions such as \u201cDo you know the name of all your medications?\u201d and \u201cDo you know your diagnosis?\" as well as questions that need context understanding like \u201cDo you know what kind of treatment you need to follow based on the discharge instructions?\" These questions are intended to evaluate the human subjects' ability to acquire correct information from the discharge instructions.\nThe perception-based questions assessed participants' perceived difficulty in understanding the summary. The first perception question, \u201cQ1\u201d in Appendix C.2, was placed first, immediately after reading the discharge instructions, asking participants to rate their understanding level with options ranging from \u201cVery clear\u201d to \u201cSomewhat clear\u201d and \u201cNot clear at all.\u201d A second perception question was included at the end of the questionnaire for cross-validation, asking participants to rate the difficulty of understanding the discharge instructions, with responses ranging from \"Extremely easy\" to \"Extremely difficult.\" Details of left experiment questions and summary are provided in Appendix C."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Data and Setup", "content": "Human Response Collection. All recruitment and enrollment procedures were reviewed and approved by the institutional IRB. We recruited participants from two groups based on their educational achievements (measured by degree received), as recorded by the Prolific platform. In addition, we included our own demographic questions covering self-reported education level, race, doctor visits, and emergency room visit frequency. The detailed demographic questions can be found in Appendix C. Each participant was required to read a section of the provided discharge summary and answer ten follow-up questions. They were allowed to refer back to the discharge summary while answering. To account for potential order effects and reduce reading fatigue, we randomized the presentation of three different discharge summaries across participants.\nParticipants were compensated $3 for completing the survey, based on an estimated completion time of 10 to 15 minutes. A $1 bonus was offered for high-quality responses, defined by thoughtful engagement, such as taking more than the minimum possible time to complete the survey and answering all questions. In total, we received 96 valid responses from participants with diverse educational backgrounds. Among them, 49 identified as having a college or post-graduate degree, 47 had not received a college degree, and 35 reported having only a high school diploma. The demographic information for these participants is provided in Table 1.\nDischarge Summary Selection. The discharge summaries were selected from an anonymized medical database, specifically focusing on sections intended for patient communication. Our selec-tions balanced length and the difficulties in understanding, for example, if it contains professional medical words or abbreviations. We select one sample from each Discharge Summary (DS) category: DS1: short and hard to understand, DS2: long and easy to understand, DS3: long and hard to understand, and DS4: short and easy to understand. These sections included details on medications, treatments, and recommendations relevant to the patient's condition. Details of the discharge summary contents and categories explanations can be found in Figure 1.\nPersonas Considered. The first key question we explored was whether LLMs could effectively simulate the behaviors of individuals from different groups based on: education level, gender, frequency of doctor visits, and frequency of emergency room (ER) visits. To investigate this, we prompted the LLM to imagine itself as a patient given certain background information, like \u201cyou are someone who has never received a college degree.\u201d or \u201cyou are someone whose gender identified as female\". The following is a description: \u201cYou will be reading a Discharge Summary written by a clinician for a patient. After reading the Discharge Summary, I will ask you a question. Your job is to think of someone with your background and choose the correct answer by selecting the letter of the answer (e.g., A, B, C, etc.). You should only output the letter with your final answer without any explanation.\u201d A prompt example can be found in Appendix A.\""}, {"title": "4.2 Effectiveness", "content": "In this section, we evaluate the overall effectiveness of the LLM's ability to simulate personas. We conduct experiments involving four distinct human group attributes: education level (low/high), gender (male/female), frequency of doctor visits (low/high), and frequency of emergency room (ER) visits (low/high). The experimental setup includes two types of tasks: information-based and perception-based. We also provided the random guess similarity accuracy based on the overall accuracy of the questions if they were answered on a strictly random basis. Specific statistics are presented in Table 2.\nOur results show that the LLM achieves a significantly higher alignment rate compared to random guessing, suggesting that it has the ability to simulate human-like personas for some tasks and groups. Specifically, the average alignment rate across all tasks and human groups is 54.97%, whereas random guessing would only achieve an alignment rate of approximately 26.7%. This highlights the LLM's potential to model human-like responses and adapt to different personas effectively."}, {"title": "4.3 Impact of Categories in Discharge Summaries", "content": "Recall that we divided the discharge summaries (DS) into four categories: DS1 (short and hard to understand), DS2 (long and easy to understand), DS3 (long and hard to understand), and DS4 (short and easy to understand). In this section, we analyze how the DS category impacts the LLM's persona-simulation capability, using education level as a case study."}, {"title": "Finding 3: The alignment rate for simulating individuals with lower education levels is mainly driven by short discharge summaries.", "content": "As shown in Figure 2a, the LLM achieves its highest alignment rates when simulating individuals with low education levels for the DS1 and DS4 categories, both of which feature short summaries. This suggests that individuals with lower education levels respond more consistently to shorter discharge summaries, likely because these summaries reduce cognitive load and are less demanding to comprehend. Given that individuals with lower education levels may have limited reading proficiency, shorter texts could allow for more straightforward processing, resulting in a higher alignment between the LLM's simulation and actual human responses.\nAdditionally, we see that the LLM struggles to accurately simulate low-education personas with longer, more complex summaries (DS3). This may be due to the fact that longer or harder summaries introduce additional layers of complexity, such as advanced vocabulary or medical jargon, making it more difficult for both the LLM and individuals with lower education levels to process and respond in a predictable manner."}, {"title": "Finding 4: The alignment rate for simulating individuals with higher education levels is mainly driven by easier-to-understand discharge summaries.", "content": "From Figure 2b, we observe that the LLM achieves its highest alignment rates for individuals with higher education levels when handling the DS2 and DS4 categories, both of which are characterized as easy to understand. This suggests that individuals with higher education levels respond more consistently to discharge summaries that are clear and concise, regardless of length. Their stronger reading comprehension skills may enable them to process both short and long summaries effectively, as long as the content is presented in an accessible and straightforward manner.\nConversely, when the LLM simulates responses for more complex discharge summaries (DS1 and DS3), the alignment rate drops, even for individuals with higher education levels. This finding suggests that the difficulty of the content itself plays a significant role in reducing response predictability. Both high- and low-education groups tend to exhibit more diverse and unpredictable behaviors when faced with difficult-to-understand summaries, likely due to the increased cognitive effort required to interpret medical jargon or ambiguous phrasing.\nThus, the LLM's ability to simulate human responses is not solely influenced by education level but is also affected by the complexity of the information presented. While higher education groups tend to perform well with easy-to-understand texts, more difficult summaries can challenge even the most educated individuals, leading to more varied and less predictable responses."}, {"title": "4.4 Misalignment Analysis", "content": "While the LLM demonstrates strong persona-simulation capabilities, significant misalignments still occur in certain cases. In this section, we take {Q10: Please rate the difficulty in understanding this discharge instruction} as an example to analyze how these misalignments arise. For Q10, respondents could choose from five different answers: A) Extremely easy, B) Somewhat easy, C) Neither easy"}, {"title": "Finding 5: The LLM performs better at simulating human groups with more homogeneous response patterns.", "content": "As seen in Figure 3, the LLM's selection patterns are generally more concentrated, often cover-ing only one or two adjacent response categories. For example, in the case of shorter discharge summaries (DS1), the LLM's predictions are focused on responses C) Neither easy nor difficult, with minimal variation across the other options. In contrast, the distribution of human responses spans all five categories, reflecting the more diverse nature of human comprehension levels. This pattern indicates that the LLM is better at simulating personas within groups where responses are more homogeneous, as it tends to assign probabilities heavily to a smaller set of options.\nIn real-world scenarios, however, human responses to complex information, such as discharge instructions, tend to be more dispersed due to varying levels of familiarity, personal experience, and cognitive abilities. Therefore, the LLM's centralized predictions reveal a limitation in capturing the full range of human diversity, especially in scenarios where individuals interpret the same information very differently."}, {"title": "Finding 6: The LLM tends to overestimate human comprehension, particularly when processing long discharge summaries.", "content": "A deeper analysis reveals that the LLM consistently overestimates human comprehension when dealing with longer discharge summaries, such as DS2 and DS3. In these cases, the LLM only selects A) Extremely easy or B) Somewhat easy as the response, indicating that it perceives the information"}, {"title": "5 Broader Impact", "content": "In this paper, we focus on evaluating the role-playing capabilities of LLMs, examining how well they can mimic individuals from varied demographic and experiential profiles. Using discharge instructions as a test case, we explore whether the responses generated by LLMs align with the understanding and behavior of actual patients. Through this study, we highlight the potential of LLMs to replace human feedback in training phases, such as in RLHF, and demonstrate their capacity to simulate human-like responses, ultimately contributing to the development of more inclusive and effective patient communications. These insights serve as a first step towards leveraging LLMs to automate healthcare systems while ensuring that communications remain patient-centric and tailored to diverse needs.\nWe believe that a better understanding of the role-playing capabilities in LLMs can not only advance automated, patient-tailored healthcare communication but also reduce the dependency on traditional, resource-intensive user studies. A strong role-playing capability in LLMs could allow these models to be supplementary to real human feedback in key processes, such as Reinforcement Learning from Human Feedback (RLHF), where human users are typically required to guide models toward more accurate and human-like behavior. For example, if LLMs can effectively impersonate diverse patient backgrounds, they could provide simulated feedback that reflects real-world patient interactions, enabling the system to fine-tune responses without direct human intervention.\nHowever, we have also identified failure modes of LMM for exactly the categories of discharge information where automated assistance may be most valuable. Lower educated and female patients are less well represented by LMMs. This may reflect historical data from medical research that excludes women [7, 30]. Significant gender biases in LLM-generated content have been analyzed by Wang et al. in [47].\nWe are targeting specially adjusted and trained medical LLMs based on the limited data collected from minority populations. One of the limitations for current LLMs replacing human participants is they can misportray and flatten identity groups as noted by [8, 48]. \u201cMisportray\u201d represents the case where LLMs prompted with a demographic identity will more likely represent what out-group members think of that group than what in-group members think of themselves. A utilized LLM role-playing should learn and understand from these specific groups, not using a \u201ctraditional\u201d and stereotype impersonation. Direct adoption of LLMs without evaluating their efficacy in diverse heterogeneous populations risks creating a situation in which only highly educated male participants are correctly represented. This has historically been a challenge as medical studies have focused on well-educated male participants, resulting in high levels of bias in existing data [19].\nLLMs responses to the same prompt are inherently identical or leptokurtic. LLMs are well-suited to support populations that are self-similar, such as individuals with similar levels of formal education in a given field. In addition, it is easier to model the answers of individuals who understand factual instructions than the answers of those who are confused. Additional qualitative research is needed to better understand how low-education individuals approach and process discharge instructions.\nThis could greatly enhance the clarity of medical communications like discharge instructions, ensuring that the information is more comprehensible and accessible to all patients with different medical knowledge levels, languages, education levels, races, or cultural backgrounds."}, {"title": "6 Related Work", "content": "In-context learning allows large language models (LLMs) to adapt to tasks by leveraging context provided in input prompts [11]. This approach differs from conventional methods that require"}, {"title": "7 Conclusion and Future Works", "content": "This study demonstrates the potential of Large Language Models (LLMs) to simulate diverse individuals in healthcare settings, specifically for the comprehension of discharge summaries. By employing role-playing prompts, LLMs can adapt to personas with various backgrounds, providing an innovative method to customize complex medical information for different audiences. Our findings suggest that LLMs, when guided by structured prompts, can generate responses that align with the understanding of individuals from diverse backgrounds, demonstrating their promise in enhancing patient communication. However, the role-playing capabilities of LLMs are not without limitations. Over-simplification and the nuances of individual comprehension present challenges that must be addressed to fully realize their potential. These limitations indicate that while LLMs can be a powerful tool for generating personalized content, further refinement is needed to ensure their outputs meet real-world patient needs effectively.\nFuture work will focus on enhancing the accuracy and consistency of LLM role-playing, ensuring that these models can better capture subtle variations in individual comprehension. Additionally, research should explore more sophisticated techniques for integrating LLMs into healthcare systems,"}, {"title": "A Prompt Example with Lower Education Level Persona", "content": "You are someone who has never received a college degree. You will be reading a Discharge Summary written by a clinician for a patient. After reading the Discharge Summary, I will ask you a question. Your job is to think of someone with your background and choose the correct answer by selecting the letter of the answer (e.g., A, B, C, etc.). You should only output the letter with your final answer without any explanation.\nHere is the Discharge Summary: \u201cYou were admitted and found to have an ulcer in the duodenum. To help this heal, we are prescribing new medications (pantoprazole). Please be sure to take this until you are seen in follow-up.\"\nNow, answer the following question: Question: Please rate your understanding level of this discharge instruction. A. Very clear B. Somewhat clear C. Not clear at all Your task: Only respond with the letter of the answer that best reflects your understanding.\""}, {"title": "B Discharge Summary Samples", "content": "DS1 You were admitted and found to have an ulcer in the duodenum. To help this heal, we are prescribing new medications (pantoprazole). Please be sure to take this until you are seen in follow-up.\nDS2 Call Dr. xxx if experience:\n-Take stool softeners with narcotics\n-Fever > 101 or chills\n-Increased shortness of breath or cough\n-Chest pain\n-You may shower. No swimming for 4 weeks\n-No driving while taking narcotics.\nDS3 Keep splint/dressing on until follow-up\nKeep splint clean & dry at all times\nFollow up with Dr. xxx in xxx days\nWean off of narcotics\nTake aspirin for 2 weeks\nPhysical Therapy:\nNWB left lower extremity\nLeave splint on until follow-up\nDS4 Ok to shower today but wear tegaderm dressing over the drain site. No heavy lifting. Return to ED for anything that concerns you."}, {"title": "C Survey Questions for Each Discharge Summary", "content": ""}, {"title": "C.1 Information-based Questions", "content": "Q2 Do you know the name of all your medications? (If yes, please type.) A. Yes B. I don't know C. Not provided\nQ3 Do you know your diagnosis? (If yes, please type.) A. Yes B. I don't know C. Not provided\nQ4 Do you know the common side effects of all your medications? (If yes, please type.) A. Yes B. I don't know C. Not provided\nQ5 Are there other prescriptions given besides the medication? (If yes, please type.) A. Yes B. I don't know C. Not provided\nQ6 Do you know what kind of condition you have mentioned in the discharge instructions? A. Stomach disease B. Ulcer in the duodenum C. Wearing Tegaderm C. Keep splint D. I don't know\nQ7 Do you know what kind of treatment you need to follow based on the discharge instructions? A. Nothing B. Take a new medication C. See a doctor again D. I don't know\nQ8 Are there any activities or foods you need to avoid? A. Avoid fruit B. Avoid strenuous exercise C. Others D. I don't know\nQ9 Is there anything about your discharge instructions that is unclear or worrying you? A. Medi-cation schedule B. Follow-up appointments C. Symptoms to watch for D. Dietary restrictions E. Activity limitations F. Other, please specify. G. No, it's very clear"}, {"title": "C.2 Perception-based Questions", "content": "Q1 Please rate your understanding level of this discharge instruction. A. Very clear. B. Somewhat clear. C. Not clear at all.\nQ10 Please rate the difficulty in understanding this discharge instruction. A. Extremely easy B. Somewhat easy C. Neither easy nor difficult D. Somewhat difficult E. Extremely difficult"}]}