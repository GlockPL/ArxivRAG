{"title": "MRP-LLM: Multitask Reflective Large Language Models for Privacy-Preserving Next POI Recommendation", "authors": ["Ziqing Wu", "Zhu Sun", "Dongxia Wang", "Lu Zhang", "Jie Zhang", "Yew Soon Ong"], "abstract": "Large language models (LLMs) have shown promising potential for next Point-of-Interest (POI) recommendation. However, existing methods only perform direct zero-shot prompting, leading to ineffective extraction of user preferences, insufficient injection of collaborative signals, and a lack of user privacy protection. As such, we propose a novel Multitask Reflective Large Language Model for Privacy-preserving Next POI Recommendation (MRP-LLM), aiming to exploit LLMs for better next POI recommendation while preserving user privacy. Specifically, the Multitask Reflective Preference Extraction Module first utilizes LLMs to distill each user's fine-grained (i.e., categorical, temporal, and spatial) preferences into a knowledge base (KB). The Neighbor Preference Retrieval Module retrieves and summarizes the preferences of similar users from the KB to obtain collaborative signals. Subsequently, aggregating the user's preferences with those of similar users, the Multitask Next POI Recommendation Module generates the next POI recommendations via multitask prompting. Meanwhile, during data collection, a Privacy Transmission Module is specifically devised to preserve sensitive POI data. Extensive experiments on three real-world datasets demonstrate the efficacy of our proposed MRP-LLM in providing more accurate next POI recommendations with user privacy preserved.", "sections": [{"title": "I. INTRODUCTION", "content": "Next Point-of-Interest (POI) recommendation has been widely utilized on location-based social networks (LBSNS) such as Yelp\u00b9 and Foursquare\u00b2. Its goal is to learn users' check-in preferences and suggest relevant locations for their next visit, thereby reducing the information overload and enhancing user experiences [1].\nConventional next POI recommender systems (RSs) usually train a domain-specific model based on users' check-in sequences to identify users' preferences on locations. In pursuit of higher accuracy, next POI RSs have evolved from Markov-chain-based methods [2] and RNN-based methods [3] to transformer-based methods [4] and graph-based methods [5]. Despite the success, they still face challenges including data sparsity and cold-start users. Fortunately, recent advancements in large language models (LLMs) such as Chat-GPT3 have presented new opportunities to overcome the limitations and further enhance next POI RSs. First, LLMs have demonstrated the ability to transfer knowledge, such as spatial and temporal information, from the training corpus of other domains [6], which could facilitate users' POI preferences learning even with sparse user data. Second, in-context learning (ICL) has demonstrated promising potential in delivering effective next-item recommendations, enabling robust learning even for cold start users [7]. Third, due to their natural language reasoning capabilities, LLMs can provide explainable hints for their recommendations, thereby improving the interpretability of the recommendations as well as users' trust in the system.\nDespite their remarkable achievements across various recommendation domains [8], [9], LLMs for next POI recommendation remain a relatively unexplored area. Existing work attempts to perform zero-shot prompting based on users' own check-in history [10]. However, such an approach exhibits several obvious shortcomings.\n1) Ineffective extraction of user preferences. Successful POI RSs usually focus on accurately capturing users' POI-related preferences [11]. Instead, LLMs-based next POI RSs lack a clear reasoning of the impact of fine-grained user preferences on their decisions. Thus, they have shown weaknesses in understanding the spatial-temporal information of user check-ins and mining user transition preferences, which can undermine the accuracy [12].\n2) Insufficient injection of collaborative signals. Conventional next POI RSS use collaborative signals by analyzing behaviors of similar users to obtain global behavioral patterns and enhance both accuracy and generalizability. In contrast, existing LLM-based methods rely exclusively on the user's own check-in history, resulting in a limited view that does not incorporate insights from broader datasets.\n3) Lacking of privacy protection. The existing LLM-based methods directly expose user check-in history to LLMs, which may contain sensitive information such as users' whereabouts, home addresses, and behavioral habits [13]. The potential sensitive information leakage could erode user trust in next POI RSS.\nTherefore, we propose a novel Multitask Reflective Large Language Model for Privacy-preserving Next POI Recommendation (MRP-LLM) to address the identified issues. As illustrated in Figure 1, MRP-LLM incorporates the user's and her neighbors' fine-grained preferences extracted from multitask prompting and self-reflection mechanisms to improve recommendation accuracy. Moreover, sensitive check-in-related inputs are protected when uploading to the RS. Specifically, to effectively extract users' behavioral patterns, a Multitask Reflective Preference Extraction Module first distills each user's categorical, temporal, and spatial preferences on POIs into a fine-grained preference knowledge base (KB). The Neighbor Preference Retrieval Module then identifies users' neighbors based on their historical distribution and social relationship, and retrieves and summarizes their preferences from the KB to inject collaborative signals. Subsequently, the Multitask Next POI Recommendation Module leverages the user's preferences with those of her neighbors to facilitate the next POI recommendation based on her check-in history. Moreover, to mitigate sensitive information leakage, check-in history and social relationships are retained locally. A Privacy Transmission Module is specially devised to leverage differential privacy to safeguard each type of user data uploaded to the RS. As such, MRP-LLM not only exploits the LLMs to enhance recommendation accuracy but also provides robust protection for user data privacy.\nIn summary, the contributions of this study lie three-fold.\n1) We propose a novel multitask reflective LLM-based next POI RS (MRP-LLM), which could enhance recommendation accuracy by reflectively extracting fine-grained user preferences and integrating collaborative signals from neighbors into the LLM recommendation process.\n2) We design a novel differential privacy mechanism to comprehensively protect the sensitive user data including users' check-in history and social relationships for more secure LLM-based next POI recommendations.\n3) We conduct extensive experiments on three real-world datasets to verify the superiority of MRP-LLM over state-of-the-art methods (SOTAs). Specifically, when privacy protection is relaxed, it gains an average lift of 8.4% and 7.0% in ACC and MRR; with full-scale protection on users' sensitive data, it still achieves performance comparable to other LLM-based next POI RSs, with a slight drop of 1.3% in ACC, and a lift of 0.8% in MRR."}, {"title": "II. RELATED WORK", "content": "Although the application of LLMs in next POI recommendation is less explored, both fields have been extensively studied independently. Additionally, privacy preservation for both LLMs and RSs has received considerable attention in recent research. This section reviews related work in four key areas to our research, the next POI recommendation, the LLM-based recommendation, the privacy-preserving approaches for conventional RSs, and the privacy-preservation techniques specifically designed for LLMs."}, {"title": "A. Next POI Recommendation", "content": "Early studies on next POI recommendation such as FPMC-LR [14] utilize Markov chain models to learn users' consecutive check-in behaviors [2]. With the evolution of recurrent neural networks (RNNs) [15], researchers start to model the sequential check-in by incorporating POI-specific spatial and temporal signals into RNN structures. For example, ST-RNN [3] introduces time- and distance-specific transition matrices to enhance RNNs' capability of capturing spatiotemporal transition signals; STGN [16] introduces time and distance gates into the structure of long short-term memory network(LSTM) [17].\nLater methods utilize the advanced capabilities of attention mechanisms and transformer architectures [18]. Methods such as Deepmove [19] and ATST-LSTM [20] fuse the attention network into RNN and LSTM model respectively to capture periodicity and spatio-temporal context; STAN [4] and CFPRec [21] both achieve remarkable recommendation accuracy by leveraging the bi-layer attention and bidirectional transformer architecture.\nRecent works further explore the integration of graph-based models to further enhance performance. For example, HMT-GRN [5] models POI relations with a graph recurrent network on both temporal and spatial POI-POI graphs; STHGCN [22] learns the spatial semantic relationships between check-ins with graph convolutional networks (GCN); DCHL [23] adopt adjusted hypergraph convolutional networks on users' check-in graph to learn the evolution of users' multi-aspect preferences. Nevertheless, all conventional next POI recommenders face challenges of data sparsity and lack of interpretability."}, {"title": "B. LLM-based Recommendation", "content": "1) General Recommendation: Thanks to the extensive knowledge and powerful reasoning capability, LLMs have gained much attention in the field of recommendation. One popular branch is to leverage the zero-shot ability of LLMs by directly prompting LLMs with recommendation-specific instructions. For instance, Liu et al. [24] first investigate the capability of ChatGPT to transfer knowledge from other domains to perform five different recommendation tasks such as rating prediction, recommendation generation, and explanation.\nTo achieve better performance, later methods attempt to obtain more effective prompts via various means. For example, Wang et al. [25] apply Chain-of-Thought (CoT) technique [26] with a three-stage prompting to extract user preferences on movies step-by-step; Sun et al. [27] proposes a sliding window prompting strategy that repeatedly ranks candidates within a sliding window to obtain overall rankings. Other methods utilize LLMs as a supplement to conventional RSs. For example, KAR [28] encodes the reasoning and factual knowledge provided by LLMs as vectors to enrich the user and item representations in RSs; LLMRec [29] utilize LLMs to augment user and item side information in the GCN recommender; and GaCLLM [30] utilize LLMs as means for propagation and aggregation in graph-based RSs.\nLastly, some works attempt to fine-tune the LLMs on the recommendation tasks. For example, GPTRec [31] fine-tunes a GPT2 model for generative recommendation tasks; P5 [32] fine-tunes the T5 model concurrently on five recommendation tasks to achieve zero-shot generalization; Tallrec [33] uses LoRA to pre-train LLaMA based on Alpaca and a recommendation-specific dataset; Once [34] proposes to use prompting for close-source LLMs such as GPT3 together with the fine-tuned open-source LLMs such as LLaMA to boost the recommendation performance.\n2) Sequential Recommendation: LLMs demonstrated potential in processing sequential user-item interaction history and performing next-item recommendations. Using user purchase history sequences, Gao et al. [35] employ zero-shot prompting to directly instruct LLMs to recommend the next item. Liu et al. [24] and Hou et al. [36] verify that few-shot prompting may help LLMs generate more accurate next item recommendations. Furthermore, later works attempt to leverage more advanced techniques to boost ICL performance. For example, DRDT [7] and LLM4ISR [9] have demonstrated that incorporating a self-reflection mechanism could help LLMs capture users' preference evolution or optimize prompts for sequential recommendation tasks; Re2LLM [37] train a lightweight reinforcement learning model to retrieve hints generated by LLMs from external knowledge bases to enhance task instructions. Recent attempts have also begun to explore using LLMs for next POI RSs. LLMMob [10] employed zero-shot prompting for next POI recommendation based on individual users' check-in data and time slot; LLMMove [12] further incorporates the geographical information of the POIs. However, they are unable to effectively extract fine-grained user preferences. Another line is to fine-tune LLMs on the next POI recommendation task but works like LLM4POI [38] demand substantial training data and computational resources."}, {"title": "C. Privacy-Preservation for Recommendation", "content": "User privacy has been recognized as a prominent problem and widely discussed for conventional RSs. In order to protect users' interaction history and other side information, recent works mainly leverage cryptographic techniques and perturbations to prevent privacy leakage. Cryptographic techniques ensure sensitive user data remains secure and inaccessible to unauthorized parties, including the RS itself. For example, Zhou et al. [39] propose a lightweight fully homomorphic data encapsulation mechanism in a distributed RS and ensure that both user rating data and recommendation results are kept private against the RS; PCAPR [40] uses symmetric homomorphic encryption to detect users' physical distance and social distance privately for more accurate recommendation. The perturbation method achieves an individual's privacy by adding noise to the data or computations to mask individual contributions and achieving differential privacy [41]. Compared to cryptographic techniques, differential privacy perturbation typically requires simpler communication protocols and is better suited for deep learning-based RSs with complex architectures and computations.\nSpecifically for POI RSs, the protection usually focuses on users' raw check-in data with data isolation [42], encryption [43] or perturbation techniques [44]. Some work further protects users' preferences and friendship relations [45]. Another major privacy challenge is to protect the spatial information of check-ins. To retain the spatial closeness without revealing the actual position of the POI, methods have been proposed to guarantee differential privacy [46], [47]."}, {"title": "D. Privacy-Preservation for LLMs", "content": "Although LLMs have shown substantial potential across various research domains, their application has also introduced new privacy concerns. In particular, LLMs may first be susceptible to passive privacy leakage, i.e., LLMs may expose sensitive information in user prompts or training corpus [48]. What is more, LLMs may also be vulnerable to active privacy attacks such as model inversion attacks [49], and membership inference attacks [50], which can reveal private texts or confirm the existence of specific training samples. Thus, some works have been proposed to tackle the emerging privacy risks of LLMs at the fine-tuning and inference stage. For fine-tuning LLMs, works such as EW-Tune [51] and JFT [52] apply differential privacy to the tuning data. Other works such as FedBPT [53] fine-tune LLMs with federated learning to avoid raw data sharing. For LLM inference, primitive methods detect and eliminate sensitive inputs in user prompts [54]. Cryptographic methods including fully homomorphic encryption [55] and multi-party computation [56] allow private user input to Transformer-based models. Differential privacy could also be applied to protect privacy text generation. For example, Mai et al. [57] apply local differential privacy on LLM output embeddings to prevent inference attacks.\nSpecifically for privacy-preserving LLM-based RSs, some methods focus on privacy protection for centralized service providers, assuming that these providers are completely honest [58]. Other methods, such as RAPT [59], attempt to perturb sensitive keywords in user prompts before submitting them to LLMs, thereby obscuring sensitive inputs within the prompts. So far, limited studies have examined privacy solutions for LLM-based RSs. A recent attempt leverages the differential back-propagation techniques to fine-tune a privacy-preserving LLM, which requires excessive computation resources [60]. Although privacy protection has gained attention for general LLMs, privacy protection in LLM-based RSs, especially the next POI recommendation tasks, requires further exploration."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. Notations and Problem Statement", "content": "Notations. Let U = {U\u2081, U\u2082, ..., U|U|}, L = {l\u2081,l\u2082,...l|L|}, C = {C\u2081, C\u2082, ...C|c|} and R = {r\u2081,r\u2082,\u2026\u2026r|r|} denote the sets of users, POIs, POI categories, and POI regions respectively. Users' social relations are represented by matrix S. A check-in record (u, l, t, r, c, d) denotes user u visited POI l at time t, where l located in region r is characterized by category c and is d km away from the user's previous check-in. For each user, we chronologically order her check-in records to form check-in sequences. The most recently visited POI sequence is the current POI sequence, Lour, whereas the POIs visited previously form the history POI sequence, Chist. Both current and history POI sequences comprise a list of POIs and check-in time, i.e., Lu = {(lt\u2081, t\u2081), (lt\u2082, t\u2082), ..., (ltk, tk)}. The corresponding category, region, and distance sequences are Cu = {(Ct\u2081, t\u2081), (Ct\u2082, t\u2082), ..., (Ctk, tk)}, Ru = {(rt\u2081, t\u2081), (rt\u2082, t\u2082), ..., (rtk, tk)}, and Du = {(dt\u2081, t\u2081), (dt\u2082, t\u2082), ..., (dtk, tk)}, respectively. The important notations utilized in our paper are summarized in Table I.\nProblem Statement. The task of MRP-LLM is to recommend the next POI ltk+1 given users' current sequences Leur, Cour, Reur, Deur, together with their history sequences Chist Chist Rhist Dhist, and social relationship matrix S."}, {"title": "B. Framework Overview", "content": "Figure 2 presents an overview of MRP-LLM. Users' raw data are strictly kept on their local devices. Various types of user data are transmitted to different modules on the RS server with privacy protection in place. Specifically, (1) Before recommendation, the Multitask Reflective Preference Extraction Module distills each user's fine-grained preferences from their Cu, Ru, and Du via multitask preference probing and self-reflection. The preferences are stored in a preference knowledge base (KB) to enrich recommendation prompts. (2) During recommendation, the Neighbor Preference Retrieval Module first retrieves and summarizes relevant users' preferences to introduce the collaborative signal for the recommendation prompts. (3) The Multitask Next POI Recommendation Module then generates next POI recommendations by aggregating both the user's and her neighbors' fine-grained preferences. (4) The Privacy Transmission Module mitigates users' check-in"}, {"title": "C. Multitask Reflective Preference Extraction Module", "content": "This module aims to guide LLMs in understanding fine-grained user preferences to achieve more accurate recommendations. Research has identified key factors influencing users' POI choices: the function of the POI (category), spatial location (region and distance), and the temporal context of the check-in. the spatial location (region and distance), and the temporal context of the check-in (check-in order and time) [61]. We reckon that temporal context may impact user preferences for category, region, and distance, which further breaks down preferences into five categories.\nBased on the temporal context, a user's choice of POI categories may depend on her (1) categorical transition preference, indicating the order of category she prefers to visit e.g., restaurant \u2192 movie theatre, and her (2) categorical temporal preference, indicating the time slot she prefers to visit a category, e.g., a restaurant at 6 pm. A user's choice of regions may also differ based on temporal contexts, resulting in the (3) regional transition preference, indicating which regions the user tends to visit consecutively, e.g., r\u2081 \u2192 r\u2082, and the (4) regional temporal preference, indicating which region the user tend to visit at a certain time, e.g., r\u2081 at 6 pm. Lastly, the time slot may affect a user's preferred traveling distance, i.e., the (5) distance temporal preference. For instance, a user may commute a long distance at 9 am for work, and travel within 1km at 12 pm for lunch.\nAs direct prompting could not effectively identify such fine-grained user preferences [12], we customize the prompts for next POI recommendation to probe user preferences step by step and refine iteratively.\n1) Multitask Preference Probing: We first use CoT to explicitly guide LLMs in capturing five types of user preferences across three subtasks. Specifically, after providing the general instruction in Prompt 1, we present the LLM with Cour and ask it to extract the user's categorical transition and temporal preferences in Prompt 2. Using the same approach, we then probe the user's regional transition and temporal preferences with Rour and her distance temporal preference based on Deur, step by step."}, {"title": "Prompt 1: Task Instruction", "content": "Your task is to recommend a user's next point-of-interest (POI) from the candidate POIs {L} by analyzing the users' preferences on category, region, and distance."}, {"title": "Prompt 2: Category Preference Probing", "content": "Given the user's Category sequence: {Cu}, what is the user's categorical transition preference? Considering: what are the 'category pairs' the user usually visits consecutively? (format:{category-category,...})\nWhat is the user's categorical temporal preference? Considering: what are the 'categories' the user visits at a certain time (day/ hour)? (format:{time: [categories]})"}, {"title": "2) Dynamic Preference Self-Reflection", "content": "Multitask preference probing could extract more fine-grained user preferences. However, it still faces two limitations: (i) a single prompting may not capture the recent evolution of user preferences; and (ii) the context limit makes it infeasible to input long history sequences into LLMs for preference extraction. As such, we introduce a dynamic preference self-reflection mechanism to address these challenges.\nFirst, to capture the recent evolution of user preferences, we sample the most recent m segments, each of length n, from the current sequence as examples. Then the self-reflection mechanism [62] is adopted to learn the recent preference changes from these segments. For instance, for category preference reflection, we sample m most recent segments Cour' from Cour. For each Cour, we send the segment, excluding the last record, to the LLM and ask it to predict the final category, as demonstrated in Prompt 3. Prompt 4 then provides the LLM with the ground truth category and guides it to rectify its previous conclusions on the user's categorical transition and categorical temporal preferences. A similar process will be conducted to update the user's region and distance preference reflection after the category preference reflection.\nSecond, to integrate more information from the lengthy history sequence, we also sample m segments of length n each from the history sequence as examples to conduct the self-reflection. The segments are selected based on their contextual relevance to the recommendation task. Specifically, since the next POI recommendation task is to predict ltk+1 with current sequence {lt\u2081,...ltk_1,lth}, any segment in the history sequence whose second last check-in is ltr may have the highest context relevance to the recommendation task, followed by segments whose second last check-in is ltk\u22121, and so on. We feed these m segments to LLMs and perform preference probing and self-reflection with Prompts 2-4.\nUltimately, the five types of user preferences will be updated and stored in the fine-grained preference knowledge base (KB)"}, {"title": "Prompt 3: Category Preference Prediction", "content": "The user has visited categories {Cour' }. Now is {day} at {hour}, based on the user's categorical transition preference and categorical temporal preference, predict users' next most likely visiting 'category'. (format:category)"}, {"title": "Prompt 4: Category Preference Reflection", "content": "The user actually visited category {Ctk}.\nBased on the actual visited category, what is the new insight you can get for the user's categorical transition preference? Generate the updated categorical transition preference. (format:{category-category,...})\nWhat is the new insight for the user's categorical temporal preference? Generate the updated categorical transition preference. (format:{time: [categories]})"}, {"title": "D. Neighbor Preference Retrieval Module", "content": "Solely relying on the user's own data fails to leverage global patterns. Thus, the Neighbor Preference Retrieval Module aims to leverage neighbors' preferences to introduce collaborative signals and further improve recommendation accuracy. We identify three types of neighbors. (1) Geographical neighbors means users having similar preferences on geographical regions, symbolized by the Kullback-Leibler (KL) divergence [63] of the regional check-in distribution, RP(u) = {P(r\u2081), ..., P(r|r|)}, \n\\(D_{R}(u_i, u_j) = KL(RP(u_i) || RP(u_j));\\) \n(2) Semantic neighbors are users with similar category preferences, quantified by the KL divergence of check-in category distribution, CP(u) = {P(c\u2081), ..., P(c|c|)}, \n\\(D_{C}(u_i, u_j) = KL(CP(u_i) || CP(u_j));\\) \nand (3) Social neighbors are the social network neighbors, i.e., Ds(ui, uj) = 1 if ui and uj are friends and vice versa.\nBased on Dr, Dc, and Ds, it selects the closest neighbor of each type, retrieves their fine-grained preferences from the fine-grained preference KB, and leverages LLMs to summarize each of the five types of preferences with Prompt 5 (taking categorical transition preference as an example)."}, {"title": "Prompt 5: Preference Summarization", "content": "The users' geographical neighbors' categorical transition preferences are {preferences}; the users' semantic neighbors' categorical transition preferences are {preferences}; the users' social neighbors' categorical transition preferences are {preferences}. Summarize the neighbors' categorical transition preferences by considering their commonalities.\n(format: {category-category,...})"}, {"title": "E. Multitask Next POI Recommendation Module", "content": "This module aims to provide next POI recommendations by taking into account users' preferences on category, region, and distance. Specifically, we first predict the next preferable category, region, and distance in three separate tasks with the guide of the user's and the neighbors' preferences, exemplified by Prompt 6. The predictions on subtasks provide hints on different aspects of user preferences. Finally, the LLM is asked to consider the preference hints and select the most suitable POI candidates using Prompt 7. Additionally, the relative importance of each aspect and an explanation of the recommendation are required to enhance interpretability."}, {"title": "Prompt 6: Next Category Prediction", "content": "Now is {day} at {hour}, based on the users' current category sequence {Cour}, his own categorical transition preference and categorical temporal preference, and his neighbors' categorical transition preference and categorical temporal preference, predict the user's next most likely visiting 'category'. (format: category)"}, {"title": "Prompt 7: Next POI Recommendation", "content": "Given users' current check-in sequence {Laur}, recommend 10 POIs from {L} considering his next likely visiting category, region, and distance. State the reason for each recommendation and rank the importance of category, region, and distance preferences.\n(format: {POI: reason; [importance ranking])}"}, {"title": "F. Privacy Transmission Module", "content": "This module seeks to preserve the privacy in four types of user data uploaded to the three modules of MRP-LLM by employing differential privacy techniques [41]. The detailed protection measures are described below.\n(1) The Multitask Reflective Preference Extraction Module receives users' category, region, and distance sequences that could reveal users' preferences. Thus, we represent each record in a sequence by a one-hot vector and perturb it with optimized unary encoding (OUE) method [64]. OUE algorithm perturbs the i-th bit of the original vector x into x' with privacy budget \u03f5 based on probability,\n\\(P(x'[i] = 1) =\begin{cases} 0.5, & \text{if } x[i] = 1 \\ 1/(exp(\\epsilon) + 1), & \text{if } x[i] = 0. \\end{cases}\\) \n(2) The Neighbor Preference Retrieval Module requires category and region check-in distribution to identify semantic and geographical neighbors, RP(u) and CP(u), and users' social relationship for neighbor retrieval. To protect the distribution, we inject noise sampled from the Laplace distribution. Specifically, we set new distribution P'(u) based on original distribution P(u) and privacy budget \u03f5,\n\\(P'(u) = P(u) + Lap(\\Delta f/\\epsilon),\\)"}, {"title": "Algorithm 2: Recommendation Process of MRP-LLM", "content": "Input: U, Lu, Cu, Ru, Du\nOutput: Next POI recommendation list l\n// Before recommendation request\n1 Input general instruction (Prompt 1);\n2 for u in U do\n3 Collect C', R', D' perturbed as Equation 3;\n4 Collect RP'(u), CP'(u) perturbed as Equation 4;\n5 Collect S' perturbed as Equation 5;\n6 Pref(u) PreferenceExt(C'u, R'u, D'u);\n7 Store Pref (u) into fine-grained KB;\n// Upon each recommendation request\n8 for u in U do\n9 Find u's geographical, semantic, and social neighbors;\n10 Summarize neighbor preferences (Prompt 5);\n11 Collect POI sequences perturbed by Algorithm 1;\n12 l next POI recommendation (Prompts 6-7);\n13 return l;\n14 Function PreferenceExt(Cu, Ru, Du):\n15 Select m segments from Cu, Ru, Du;\n16 for each example do\n17 Pref'(u) preference extraction (Prompt 2);\n18 Pref(u) preference reflection (Prompts 3-4);\n19 return Pref(u);"}, {"title": "Algorithm 1: (\u03c1, h)-Privacy for POI check-ins", "content": "Input: l with coordinate (x\u03b9, y\u03b9) and category c,\nprivacy parameter \u03f5\nOutput: replacement POI l'\n1 h random(hmin, hmax);\n2 \u03c1 minimum radius containing at least h POIs;\n3 \u03b4 random(0, \u03c1);\n4 \u03b8 random(0, 2\u03c0);\n5 O circle at (x\u03b9 + \u03b4 cos \u03b8, y\u03b9 + \u03b4 sin \u03b8) with radius \u03c1;\n6 c' random_flip(\u03f5);\n7 if c' 0 and O contains POI of category c then\n8 l' a random POI of category c in O;\n9 else\n10 l' a random POI in O;\n11 return l';"}, {"title": "G. Summary", "content": "The entire process for MRP-LLM to perform next POI recommendation task is summarized in Algorithm 2. First, users upload their perturbed category, region, and distance sequences, together with their perturbed category, region distributions, and social relationship (lines 3-5). MRP-LLM then samples m segments from uploaded sequences based on time and contextual relevance (line 15) and uses the segments to perform multiple rounds of multitask preference probing and dynamic preference self-reflection (lines 16-18). The resulting users' preferences are stored in the fine-grained preference KB (line 7). Second, when a user initiates a recommendation quest, MRP-LLM identifies her neighbors based on geographical, semantic, and social closeness, retrieves their preferences, and utilizes LLM to summarize their fine-grained preferences (lines 9-10). Finally, the user's and her neighbors' preferences serve as important hints for LLM to perform the multitask next POI recommendation based on the user's perturbed current category, region, distance, and POI sequence (lines 11-13)."}, {"title": "IV. EXPERIMENTS AND ANALYSIS", "content": "We conducted experiments on three real-world datasets to answer the following four research questions.\n\u2022 (RQ1): How does the recommendation accuracy of MRP-LLM compare to existing conventional and LLM-based next POI RSs?\n\u2022 (RQ2): How does each component affect the performance of our proposed MRP-LLM?\n\u2022 (RQ3): How do hyper-parameters affect the performance of our proposed MRP-LLM?\n\u2022 (RQ4): How reasonable and interpretable is the output of our proposed MRP-LLM?"}, {"title": "A. Datasets", "content": "We utilized the Foursquare dataset [66] in three cities, namely Singapore (SIN), New York (NY), and Phoenix (PHO) constituting user check-in history, POI geographical location, and user friendships. Details are shown in Table II. Following existing works [67], we preprocess the dataset first by applying 5-core filtering. For each user, we split the check-ins by day and filter out users with less than 3 sequences. The sequences are then chronologically split into training, validation, and test sets with a ratio of 8:1:1. It is noteworthy that since in MRP-LLM, we are performing ICL without the need for model training, the training set is used as history sequences. For validation and test sets, we leave the last check-in as ground truth and use the previous check-ins as the current sequence."}, {"title": "B. Evaluation Metrics", "content": "In line with SOTA approaches [10], [12], we use Accuracy (ACC@K) and Mean Reciprocal Rank (MRR) to evaluate all methods. In general, higher metric values indicate better ranking performance. To ensure robust comparisons, we repeated each evaluation 10 times and reported the average result."}, {"title": "C. Compared Baselines", "content": "We compare MRP-LLM with eight baselines for conventional next POI RSs (1-6) and LLM-based next POI RSs (7-8),:\n1) MostPop: recommending the most popular POIs;\n2) Dist: selecting the nearest POIs;\n3) BPRMF: [68] performing matrix factorization on check-in history;\n4) STRNN: [3] an RNN-based model incorporating check-in time interval and distance;\n5) STAN: [4] a bi-attention model that exploits the check-in spatiotemporal correlation;\n6) STHGCN: [22] a hypergraph model leveraging POI, category, and spatiotemporal differences.\n7) LLMMob: [10] performing zero-shot recommendation via LLMs based on users' history and current sequences;\n8) LLMMove: [12] performing zero-shot recommendation using LLMs based on users' check-in sequences, distance, and transition patterns."}, {"title": "D. Implementation Details", "content": "We empirically find all the best hyper-parameters for all methods on the three datasets. For each evaluation, we randomly sample 100 POIs containing the ground truth as candidate sets. For LLM-based methods, we utilize gpt-3.5-turbo as the LLM engine. The detailed parameter searching space and best parameter settings for all methods are reported in Table III to support reproducibility."}, {"title": "E. Model Performance Comparison (RQ1)", "content": "Tables IV presents the performance of all baselines and our methods across the three datasets, where '\u2020' marks the best results achieved by the conventional baselines; \u2018\u2021' highlights the best results achieved for LLM-based baselines. Our method without privacy preservation (MR-LLM) is marked with \u2018*', and the version with privacy preservation (MRP-LLM) is marked with \u2018**'. \u2018Improvements' indicates the relative improvements between two methods, e.g., \u2018* vs. \u2021' in row 12 is the improvements comparing the results marked with \u2018*' (achieved by MR-LLM) and those marked with \u2018\u2021' (obtained by the best LLM-based baseline).\nSeveral observations are noted. (1) SOTA conventional POI recommendation methods, such as STAN and STHGCN trained with full-size data, achieve the highest accuracy among all approaches. However, our method without privacy-preserving (MR-LLM), despite not requiring the resource"}]}