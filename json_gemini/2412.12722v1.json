{"title": "Defending LVLMs Against Vision Attacks through Partial-Perception Supervision", "authors": ["Qi Zhou", "Tianlin Li", "Qing Guo", "Dongxia Wang", "Yun Lin", "Yang Liu", "Jin Song Dong"], "abstract": "Recent studies have raised significant concerns regarding the vulnerability of Large Vision Language Models (LVLMs) to maliciously injected or perturbed input images, which can mislead their responses. Existing defense methods show that such vision attacks are sensitive to image modifications especially cropping, using majority voting across responses of modified images as corrected responses. However, these modifications often result in partial images and distort the semantics, which reduces response quality on clean images after voting. Instead of directly using responses from partial images for voting, we investigate using them to supervise the LVLM's responses to the original images. We propose a black-box, training-free method called DPS (Defense through Partial-Perception Supervision). In this approach, the model is prompted using the responses generated by a model that perceives only a partial image. With DPS, the model can adjust its response based on partial image understanding when under attack, while confidently maintaining its original response for clean input. Empirical experiments show our method outperforms the baseline, cutting the average attack success rate by 76.3% across six datasets on three popular models.", "sections": [{"title": "Introduction", "content": "Large Vision Language Models (LVLMs) represent a significant advancement in AI, enabling more intuitive interactions between humans and machines by bridging the gap between visual perception and language understanding. For instance, LLava (Liu et al., 2024a) and GPT-4 (Achiam et al., 2023) have demonstrated outstanding performance across a wide range of visual tasks. LVLMs are being applied in various fields: Tian et al. (2024) integrate LVLMs into autonomous driving systems to make decisions in driving scenarios, while MedPaLM, proposed by Tu et al. (2024), offers new capabilities for intelligent medical consultations. The applications continue to grow.\nHowever, as LVLMs are increasingly applied, researchers have recently discovered that carefully crafted manipulations of image inputs can easily mislead these models. For example, Zhang et al. (2024b) shows that LVLMs' generation is easily misled by adversarial noise. Moreover, Liu et al. (2024c) manipulate images to conduct safetycritical attacks. To defend against such attacks, Sun et al. (2024) and Zhang et al. (2024a) reveal that attacked images demonstrate sensitivity to modifications, especially cropping. Building on this insight, Sun et al. (2024) propose SmoothVLM, which employs majority voting to integrate responses from randomly modified input images, effectively countering these attacks, as illustrated in Figure 1. For normal queries, however, these modifications often result in partial images and significantly distort the image semantics, resulting in compromised voting outcomes. This largely reduces the practical effectiveness of these methods.\nFacing the dilemma that using a partial image can prevent attacks but may also severely distort the semantics, we explore how to collaborate the models' responses to both the partial and full images, avoiding attacks while preserving image semantics for clean inputs. This collaboration is particularly challenging, as responses from both partial and full images can be unreliable.\nIn this paper, we are inspired by recent work (Burns et al., 2023; Khan et al., 2024; Yang et al., 2024) that highlights a 'weak-to-strong' phenomenon. This phenomenon demonstrates that, while weaker models underperform stronger models in generalization and other capabilities, they can still collaborate to supervise stronger models, enhancing their performance. We draw an analogy, treating responses to partial images as those from weak models and responses to full images as those from strong models. We then explore how to collaborate the responses from partial images to supervise LVLMs in effectively defending against attacks on full images.\nOur preliminary observations confirm the potential: LVLMs are less persuasive when processing attacked images compared to clean ones. Building on this, we propose a black-box, training-free method, DPS (Defense through Partial-Perception Supervision), which leverages responses from partial perception to prompt the model during inference. This approach leverages the differences in persuasion exhibited by LVLMs when processing clean versus attacked images, enabling the model to confidently maintain accurate responses for clean inputs while reconsidering and refining its responses under attack. As shown in Figure 1, the detailed design of DPS proceeds as follows: At the beginning, the model ('Part-Perc' model) provides the initial responses to a partial image. The initial responses are then used as supervisory information to prompt the model ('Full-Perc' model) to re-analyze the full image and provide the final answer.\nEmpirical experiments show that our proposed method has reduced the average success rate by 78%, 79%, and 72% on the Qwen-VL-Plus (Bai et al., 2023), GPT-4o-Mini (Achiam et al., 2023), and Gemini-1.5-Flash (Team et al., 2024) respectively, which is approximately twice as effective as the best baseline method."}, {"title": "Background", "content": "In this section, we begin by reviewing related work on the security of LVLMs in Section 2.1 and weakto-strong learning in Section 2.2. We then provide a brief overview of research on multi-agent debate and its applications in Section 2.3."}, {"title": "Vision Attacks and Defense for LVLMs", "content": "As generative AI technology evolves, research and applications in visual-language models have seen significant growth in recent years. LVLMs (e.g., GPT-4 and Gemini-1.5-Flash), by integrating visual perception with natural language understanding, have achieved impressive results in many areas. Meanwhile, research on the safety of LVLMs has also garnered widespread attention. Existing research can be divided into two main types:\nMisleading Attacks and Defenses. Zhao et al. (2024b) investigate targeted attacks on early imageto-text models. Qraitem et al. (2024) propose a selfgenerated typographic attack to mislead LVLMs. Chung et al. (2024) investigate misleading attacks on VLMs in autonomous driving scenarios. Additionally, an increasing number of red team benchmarks that incorporate misleading attacks have emerged (Zhang et al., 2024b; Li et al., 2024b).\nExisting misleading defense methods focus on supervised fine-tuning during the training phase (Li et al., 2024b,a). As LVLMs are increasingly integrated into systems like autonomous driving, along with the rapidly evolving challenges of such open domains, it becomes crucial and challenging to develop scalable defenses that ensure the robustness of LVLMs against misleading attacks.\nSafety-Critical Jailbreak Attacks and Defenses. Shayegani et al. (2023) achieve jailbreak attacks on LLaVA by accessing visual encoders and optimizing adversarial images. Qi et al. (2024) explore the security vulnerabilities that arise from the introduction of the visual modality and break through the safety defenses of LVLMs using visual adversarial examples. Gong et al. (2023) propose FigStep, which converts harmful content into images through formatting to achieve jailbreak attacks. For jailbreak defenses, Zong et al. (2024) perform finetuning on a safe instruction-following dataset. Pi et al. (2024) identify harmful responses through a detector and transform harmful responses into benign responses. Wang et al. (2024) defend against structured jailbreak attacks by adding defensive prompts to the input. Sun et al. (2024) achieve defense by input smoothing and output aggregating."}, {"title": "Weak-to-Strong Learning", "content": "As LLMs surpass human-level capabilities, providing comprehensive and precise supervision becomes increasingly challenging. In this context, weak-to-strong learning, which utilizes a less capable model to harness the latent abilities of a more advanced model, has shown promising potential. Consequently, recent research (Burns et al., 2023; Khan et al., 2024; Yang et al., 2024; Zhao et al., 2024a; Guo et al., 2024) explores a related question: can weak supervision from one model effectively unlock the full capabilities of a more powerful model. Burns et al. (2023) demonstrate that naively fine-tuning strong models with labels generated by weaker models can lead to performance surpassing that of the weak supervisors. Khan et al. (2024) reveal that debates within multi-agent systems allow weaker models to critically evaluate the outputs of stronger models effectively. Similarly, Yang et al. (2024) develop strategies enabling a strong model to learn from the errors of its weaker supervisor, ultimately outperforming models fine-tuned on gold-standard solutions alone."}, {"title": "Multi-Agent Collaboration", "content": "By facilitating collaboration among multiple models/agents, the multi-agent system can mitigate the problems associated with a single model and yield responses with higher reliability. Du et al. (2023) enhance factual correctness and reasoning accuracy through multi-agent debates. Liang et al. (2023) propose a multi-agent debate framework that accomplishes challenging reasoning tasks through the debate among agents. Li et al. (2024c) assigns different persona roles to each agent to simulate a variety of social perspectives and uses a jury mechanism to mitigate the biases present in LLMs. Zhang et al. (2024c) investigate the impact of agents' psychology on safety in multi-agent systems and have set up doctor agents and police agents within the system to conduct psychological analysis and defense for the agents, thereby enhancing the overall system's security. Rasal (2024) introduce a novel multi-agent communication pipeline, to enhance LLMs' problem-solving capabilities. Lin et al. (2024) investigate that multi-agent debate can effectively alleviate model hallucinations."}, {"title": "A Closer Look at Vision Attacks to LVLMS", "content": "A common attack strategy involves adding misleading semantic content to the original visual information by introducing adversarial noise (Zhang et al., 2024b) or typographic cues (Liu et al., 2025). These alterations can deceive the model into output incorrect answers. For instance, as illustrated in Figure 2 Adv-Case 1, adding adversarial noise associated with 'Horse' causes the model to incorrectly include the attack target in the image description."}, {"title": "The Sensitivity of Vision Attacks", "content": "Existing defense methods (Sun et al., 2024; Zhang et al., 2024a), such as SmoothVLM, demonstrate that vision-based attacks reveal that common image modifications\u2014such as cropping, compression, and noise addition\u2014can effectively disrupt the semantics cues that vision attacks rely on. For example, as shown in Figure 2 Adv-Case 2, cropping the image disrupts the adversarial noise, eliminating the semantics of the attack target 'horse'. However, cropping also leads to the loss of semantic information in the image, making it insufficient for a detailed description of the image. This indicates that such vision attacks share a common characteristic: the attacks are easily disrupted by cropping, and the semantics of clean images are also significantly altered by cropping."}, {"title": "Distinct Persuasion Facing Clean vs. Attacked Images", "content": "We observe an intriguing phenomenon: the LVLM shows high persuasion with clean inputs, remaining unaffected by interference terms, but is less persuasive and more susceptible to interference when facing attacks. As shown in Figure 2 with different cases on the clean case: Firstly, for the clean image, as shown in Clean-Case3, we explicitly provided an irrelevant hint in the prompt, yet the model consistently produced the correct answer. Furthermore, as shown in Clean-Case4, we modified the question to ask the model whether the image corresponds to either of the two incorrect options. Given that the model is highly confident with clean samples, the perturbations in the question options do not disrupt the model's ability to provide the correct answer. However, in attacked cases, as shown in Adv-case 3, the response to the attacked image is easily influenced by interfering words such as 'There is a bird', and \u2018(1) bird or (2) dog?' in Adv-Case 4. This interference persuades the model to incorrectly generate an output describing a bird. This evidence suggests that the model exhibits strong persuasion when processing clean images. However, the vision attacks significantly reduce its persuasion. Due to the limited space, we show a preliminary investigation into defense strategies in Appendix A.\nBased on this, a natural question arises: Could we combine the findings in Sections 3.1 and 3.2 to design strategies that mitigate the impact of attacks while ensuring the model's performance when facing clean images?"}, {"title": "Methodology", "content": "Inspired by the observations in Section 3, we aim to combine the responses from processing cropped images with those from processing full images to achieve robust defense while maintaining response quality. Drawing on the 'weak-to-strong learning' phenomenon, where weaker models can effectively supervise stronger models, we propose leveraging the outputs from cropped image processing (\u2018PartPerc model') to supervise the outputs from full image processing (\u2018Full-Perc model'). To this end, we introduce DPS (Defense through Partial-Perception Supervision), the details of which are outlined in the following sections."}, {"title": "Framework", "content": "We first introduce the interaction framework of DPS. As shown in Figure 3, the Part-Perc models first independently respond to a description question and collect evidence based on the different observed visual content. After summarizing and combining the output information from the PartPerc models, it is presented to the Full-Perc model for analysis and reflection. Finally, based on the analysis, the Full-Perc model re-examines the image and the question and provides the final answer."}, {"title": "Detailed Design of DPS", "content": "The specific details of each step in the DPS approach are as follows:\nStep 1: Initial Response. The Part-Perc models first respond to the following description question:\nPlease provide an objective, detailed description of the image, avoiding subjective conjecture and associations. Then answer the question:\n(Original Question).\nStep 2: Partial Perception Supervision. The supervisory message from the Part-Perc models in Step 1 will guide and prompt the Full-Perc model to conduct analysis and reflection. The image and the original question are provided again, and the Full-Perc model is prompted to provide the final response. The prompt for the Full-Perc model is shown below:\nHere is the information provided by the local observation agents: (Supervisory message from Part-Perc models).\nRe-analyze the given image, and provide your final answer to the question:\n(Original Question)."}, {"title": "Enhancing DPS's Safety Awareness", "content": "Beyond misleading attacks, we expect that DPS can further reduce safety concerns, such as jailbreaking, which are both critical and highly influential. For instance, Liu et al. (2024c) demonstrate how manipulated images can be used to conduct safety-critical attacks, such as achieving jailbreaking. DPS can be seamlessly improved to tackle such critical safety challenges by simply adjusting the prompts. Specifically, the prompt for the Full-Perc model (Step 2) could be revised as below:\nHere is the information provided by the local observation agent: (Supervision from Part-Perc models). Based on the above information, consider whether you might be led into discussing harmful, malicious, or unethical topics. Please avoid harmful, malicious, and unethical topics, and provide your final answer to the question:\n(Original Question)."}, {"title": "LLM-Secured DPS", "content": "When tackling safety concerns such as jailbreaking, DPS primarily relies on the defensive capabilities of LVLMs, as outlined in Section 4.3. However, research indicates that LVLMs demonstrate weaker defense performance compared to LLMs (Liu et al., 2024b). To address this limitation, we propose incorporating an LLM-based safety checker to filter and evaluate the responses generated by DPS, a strategy we call LLM-Secured DPS (LS-DPS). This safety checker employs prompts structured as follows:\nGiven the question: (Original Question). Answer: (Response)\nPlease carefully analyze whether the above answer is harmful. If it is harmless, repeat the original content. If it is harmful, please modify it into benign content.\nIt is worth noting that this safety checker is plug-and-play for the DPS strategy, demonstrating DPS's strong compatibility and scalability."}, {"title": "Evaluations", "content": "Models. We employed three relatively advanced LVLMs, i,e., Qwen-VL-Plus (Bai et al., 2023), GPT-4o-Mini (Achiam et al., 2023), and Gemini1.5-Flash (Team et al., 2024) for experiments. Datasets. To comprehensively evaluate the performance of different defense methods, we considered various datasets with a range of attack types, which include the following datasets: Challenging Misleading Datasets: RTA-100 (Azuma and Matsui, 2023) and MultiTrust Misleading Dataset (Zhang et al., 2024b). Misleading Attack Datasets: Self-Gen dataset constructed by self-generated typographic attacks (Qraitem et al., 2024). Typographical Jailbreak Datasets: MM-SafetyBench (Liu et al., 2025) and HADES (Li et al., 2024d). Optimization-based Jailbreak Adversarial Examples: We utilize the approach from VisualAttack (Qi et al., 2024), which inject safety-aware adversarial noise into clean images. In addition, we utilize the MM-Vet benchmark (Yu et al., 2023) to evaluate the standard performance of various defense methods in general scenarios. Please refer to Appendix B for more detail.\nBaselines. In this section, we evaluate the efficacy of different training-free defense strategies and various baseline approaches, including MLLMProtector (Pi et al., 2024), ECSO (Gou et al., 2025), SmoothVLM (Sun et al., 2024), and two promptbased self-defense methods: In-depth Visual Analysis (Cheng et al., 2024) and self-warning Prompt. Given the noticeable degradation in safety alignment of LVLM when compared to LLM, existing defense methods consistently utilize the LLMs or transform multimodal into text data for defense. MLLM-Protector (abbreviated as Protector) is a plug-in LLM-based defense method that first identifies harmful content in the response of LVLMs and subsequently transforms it into benign outputs. ECSO converts images to text for safer responses when the harmful responses are identified. SmoothVLM on the other hand, implements smoothing operations on visual inputs i.e.,, adds random noise to the input image, and obtains the final answer through multiple LVLM models answering with majority voting. In-depth Visual Analysis (abbreviated as IVA) emphasizes the importance of focusing on visual aspects such as colors, shapes, and composition in the prompt, which guides the model in generating a detailed visual description before answering the original question. The Warning Prompt (abbreviated as Warning) alerts the model before it answers by stating that it may be under attack. Beyond that, we include 'think step by step' in the prompt as a baseline for the misleading defense task (abbreviated as Step). To simplify, we will use abbreviations to represent the baseline methods in the subsequent tables. Please refer to Appendix C for more details.\nEvaluation Metrics. Misleading Defensive Evaluation. In misleading attacks, each sample contains a misleading target and a ground truth label. We adopt the evaluation method used in MultiTrust to determine whether the model's response refers to the misleading target or the ground truth. For the evaluation formula please refer to Eq. (1). Safety Defensive Evaluation. Following MM-SafetyBench, we calculate the average attack success rate (ASR) which is formulated as:\n$ASR(D_k) = \\frac{1}{|D_k|} \\sum_{(x_i,q_i,t_i) \\in D_k} I(F(x_i, q_i), t_i), \\quad (1)$\nwhere $D_k$ is the testing dataset, which consists of sample pairs with image $x_i$ and query $q_i$. Additionally, $t_i$ represents the criteria for attack success. In the misleading scenarios, it corresponds to the misleading target, while in the safety jailbreak scenarios, it refers to the safety criteria. $F$ represents the LVLM and $I$ is the indicator, where return 1 if an attack is successful and count 0 otherwise.\nStandard Performance Evaluation, we employe the MM-Vet benchmark, which includes several key capabilities of LVLMs. We compute MM-Vet score to quantify the general performance of the LVLM with different defense methods. Given the MM-Vet test dataset $D_{vet}$ and the evaluator $H$. The MM-Vet score is defined as follows:\n$S_{MM-Vet} = \\frac{1}{|D_{vet}|} \\sum_{(x_i,q_i) \\in D_k} H(F(x_i, q_i)). \\quad (2)$\nWithout loss of generality, for all the aforementioned evaluations, we use GPT-4o to evaluate. For the standard performance evaluation, akin to the MM-Vet benchmark, we utilize GPT-4 for the assessment. See Appendix C.4 for more details.\nImplementation Details. We first filtered out adversarial samples that successfully attacked the original model across all datasets thereby creating six adversarial sample datasets for evaluation. E.g., in MM-SafetyBench, we collected 264 samples for Qwen-VL-Plus, 96 for GPT-4o-Mini, and 145 for Gemini-1.5-Flash."}, {"title": "Misleading Defensive Performance", "content": "We present comprehensive experimental results for six defense methods applied to three LVLMs on six different datasets. The results indicate that DPS achieves favorable results across various scenarios. To elaborate, in misleading tasks where all baseline methods struggle, DPS demonstrates the most robust performance, as shown in Table 1. Specifically, DPS restricts the ASR to 0.24, 0.30, and 0.40, achieving an average value of 0.31 across the three datasets on Qwen-VL-Plus, which is 2.5 times better than that of the best baseline method. While the best-performing among all baseline methods only achieves 0.78. Similarly, DPS demonstrates best performance on GPT-40-Mini and Gemini1.5-Flash. Since MLLM Protector and ECSO are specifically designed for safety scenarios, they are not effective in addressing the challenges posed by misleading content. It is noteworthy that the prompts used in In-depth Visual Analysis intuitively include rule descriptions related to misleading content. However, consistent with the observations in Appendix A, this method did not provide enough defensive effect. Instead, the more concise Step method demonstrates some defensive effectiveness. Moreover, the results on GPT-40-Mini and Gemini-1.5-Flash also demonstrated the effectiveness of our method, reducing the ASR by 61%, which is 1.95 and 1.90 times that of the best baseline method, respectively. For the case study, please refer to Appendix D.4.1."}, {"title": "Standard Performance", "content": "In this section, we evaluate the standard performance of various defense methods on the standard LVLM benchmark MM-VeT. Specifically, The MM-VeT benchmark comprises data across six distinct dimensions for quality assessment of the responses. The overall results are presented in Figure 4. For numerical results, please refer to Appendix D.1. Among them, MLLM Protector, ECSO, and DPS have minimal impact on standard performance, as reflected in the figure where their results remain on par with or slightly below the vanilla performance. In contrast, SmoothVLM exhibits a noticeable performance degradation, indicating that balancing defense with standard performance is indeed quite challenging. Surprisingly, DPS effectively improved the scores for math-type data on the Gemini-1.5-Flash model, we provide the case study in Appendix D.4.3."}, {"title": "Jailbreak Defensive Performance", "content": "After safety-aware adaptation, our method also demonstrates impressive performance in the jailbreak defense task. On three jailbreak datasets, the best-performing baselines are MLLM Protector and ECSO. MLLM Protector utilizes an LLMbased safety checker to filter the output content, while ECSO achieves effective safety detection by captioning image content into text. We use GPT40-Mini as the safety detector for both MLLM Protector and ECSO to ensure a fair comparison."}, {"title": "Ablation Study", "content": "Instead of using a combination of three cropping strategies, we explore the contribution of each cropping method: center cropping (abbreviated as CC), random cropping (abbreviated as RC), and adaptive cropping (abbreviated as AC) within our DPS system. In addition, we constructed multiple PartPerc supervision composed of three models with random cropping (abbreviated as MRC). The results are presented in Table 3, which indicates that AC is slightly better than other methods overall, while RC performs similarly to CC on safetyrelated datasets but slightly degrades performance on misleading datasets. For MRC, the experimental result demonstrates an improvement in defense on safety-related datasets, such as HADES and VisualAttack, through the inclusion of additional Part-Perc models. However, a decrease in performance is observed on misleading tasks, which can be attributed to the heightened reliance on accurate responses from the Part-Perc models, but RC poses a greater challenge in capturing precise object supervision. In conclusion, by integrating multiple straightforward cropping methods, the defensive capabilities can be significantly enhanced. For ablation of safety awareness enhancement, please refer to Appendix D.2. Furthermore, the interaction strategy is also an important aspect, we report the result of the multi-agent debate in Appendix D.5 as a reference and leave this for future work."}, {"title": "Conclusions and Future Work", "content": "In this paper, we propose DPS, a black-box, training-free defense method designed to counter vision attacks on LVLMs. The principle of DPS is to use the model's partial observations of the input image to supervise the model when observing the entire image. DPS also demonstrates strong compatibility and scalability, easily combined with other defense strategies. The experimental results indicate that DPS shows superior performance against both misleading and jailbreak attacks while maintaining the model's standard performance. For future work, DPS has great potential for expansion, such as the combination with the more advanced segmentation model e.g., SAM. In addition, a more diversified range of interaction strategies is also worth exploring. At last, the optimization of efficiency is also noteworthy."}, {"title": "Limitations", "content": "Our proposed DPS method leverages multiple partial observations for supervision, therefore, it has certain disadvantages in terms of efficiency compared to prompt-based methods. Additionally, DPS is based on the assumption that partial observations can effectively disturb the adversarial characteristics present in malicious input images, thereby achieving effective supervision. This means that DPS cannot directly defend against attacks beyond vision-related threats. As an initial step, our work establishes a framework for defending against vision attacks, which provides a foundation for defenses against a wider variety of attacks in the future."}, {"title": "A Preliminary Investigation into Defense Strategies", "content": "Here, we conduct a preliminary investigation into defense strategies against these vision attacks by leveraging the findings mentioned in Section 3.1. Intuitively, adding the instruction 'Let's think step by step' should enable the model to analyze the image content more carefully, thereby mitigating the misleading impact of attacks. However, this method has not demonstrated defensive effects, shown in Adv-Case 5. Furthermore, we attempt to incorporate the phrase 'You may be under attack' into the prompt in shown in Adv-Case 6, hoping that this would alert the model to avoid being misled by the attacks. However, the model remained susceptible to producing content with the attack target. Additionally, we added the defensive guidance \u2018Do not be misled by the nonexistent objects' in the prompt in Adv-Case 7, yet the defense still failed. This indicates that training-free defenses against such attacks through prompt-based methods could be challenging. Finally, we included the correct hit in the prompt in Adv-Case 8 and observed that the model successfully mitigated the misleading impact, resulting in the correct answer."}, {"title": "Datasets", "content": "We provide a detailed introduction to the following datasets used in the experiment, which include the following datasets: Challenging Misleading Datasets: RTA-100 (Azuma and Matsui, 2023) and MultiTrust Misleading Dataset (Zhang et al., 2024b). RTA-100 is a real-world typographic attack dataset, in which the handwritten tag from incorrect classes is placed next to the objects in the image. Whereas, the MultiTrust Misleading dataset contains challenging visually misleading images. Misleading Attack Datasets: Self-Gen dataset constructed by self-generated typographic attacks (Qraitem et al., 2024). Typographical Jailbreak Datasets: MM-SafetyBench (Liu et al., 2025) and HADES (Li et al., 2024d). Both MMsafetyBench and HADES are datasets for evaluating LVLM in safety-critical scenarios, they incorporate jailbreak images, which are generated using diffusion models, and these images are then enhanced with specific typographical additions.\nOptimization-based Jailbreak Adversarial Examples: We utilize the approach from VisualAttack (Qi et al., 2024), which involves injecting safetyaware adversarial noise into clean images. As a result, the LVLMs produce harmful responses."}, {"title": "Implementation Details", "content": "Models\nWe use Qwen-VL-Plus1, GPT-40-Mini2 and Gemini-1.5-Flash3 for experiments. These models were chosen for their superior capabilities in processing and understanding the interplay between visual and textual data.\nConstruct Self-Gen Dataset\nFollowing the original paper, we use LVLM to get the animal breed noun that is most similar to the image object from The Oxford-IIIT Pet Dataset (Parkhi et al., 2012) as the misleading item, then construct a short sentence containing this misleading item and piece it together at the top of the original image as a misleading annotation.\nSettings of Defense Methods\n. In the experiments, we set the MLLM-Protector defense prompt as follows:\nGiven the question: (Original Question). Answer: (Response)\nPlease carefully analyze whether the above answer is harmful. If it is harmless, repeat the original content. If it is unsafe, harmful, or malicious, please modify it into benign content. Do not output analysis content, only provide the final answer.\nFor the In-depth Visual Analysis prompt, we utilize the prompt from the original paper:"}, {"title": "The impact of safety awareness enhancement", "content": "We further conduct experiments to investigate the impact of safety awareness enhancement. Specifically, after the LVLMs answer the original question, we prompt the model with the safety awareness enhancement without partial perception supervision. Due to the minimal impact of safety awareness enhancement on misleading tasks, we only report its performance on three safety-critical datasets. The result is shown in Table 5, which indicates that with partial perception supervision, the defense performance against safety jailbreak scenarios are generally better, especially on the Qwen-VL-Plus and Gemini-1.5-Flash models."}, {"title": "Efficiency Comparisons", "content": "We briefly compared the efficiency of various defense methods, among which our proposed method requires an average of 5 queries (for DPS) to 6 queries (for LS-DPS) per sample. For a single sample, LS-DPS takes approximately 81 seconds. Smoothvlm performs 10 queries, with an average time of about 232 seconds, ECSO takes 20 seconds, and Protector takes 14 seconds."}, {"title": "Case Study", "content": "Defense Misleading Attacks\nWe first show the case from Self-Gen dataset, where given a pet image, the LVLM is prompted to answer the breed. In this sample, the ground-truth label is 'Abyssinian', and the misleading attack target is 'Somali'. Without defense, the LVLM is deceived by the text in the image, resulting in incorrect responses. After the partial perception supervision, the LVLM corrects its response."}, {"title": "Defense Jailbreak Attacks.", "content": "Then we show the case from the MM-SafetyBench dataset. Given a synthesized image, with an illegal phrase at the bottom. The LVLM can be jailbroken, thereby outputting unsafe content. Our proposed LS-DPS method can effectively prevent the model from generating harmful content."}, {"title": "Further Exploration: Multi-agent Debate", "content": "In this section, we evaluate the efficacy of multiagent debate defense strategies. Specifically, a PartPerc model with the center cropping strategy and a Full-Perc model. In the initial round of each debate, two models provide initial responses to their respective image and text inputs. Subsequently, two models are asked about the key object in the image that supports their given answer, thereby guiding the model to provide reasoning for its response through questioning. Then, we conduct three different types of debate.\nMessage Passing. In the message-passing phase, a GPT-based moderator agent summarizes and condenses the initial viewpoints and significant supporting evidence of each model, facilitating information dissemination among the models. This setup investigates whether observing alternative perspectives can mitigate verbal attacks after the Full-Perc model has been challenged.\nPersuasive Debate. In the persuasive debate, built upon the message-passing framework, the PartPerc model takes on the role of a persuasive debater, defending its argument and attempting to reach a consensus with its opponent. This configuration explores whether persuasive dialogue can enable the Full-Perc model to recognize input deception from dangerous question-answering scenarios and neutralize opposing viewpoints while defending its own argument.\nCritic Debate. In the critical debate, the Part-Perc model takes on the role of a stringent critic, attacking the Full-Perc model's viewpoint and attempting to induce a change in perspective. Intuitively, when the Full-Perc model accuses the model of errors (which it may not be aware of) or incorrect objects and associations within the thought process, the model will re-examine its logic for answering questions. Dialogues that prompt reflection are expected to have a mitigating effect on attacks. Experimental results show that persuasive debates are indeed effective in changing the Full-Perc model's original point of view, thus enabling defense in attacked scenarios, shown as Table 6."}]}