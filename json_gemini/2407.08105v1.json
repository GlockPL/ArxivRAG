{"title": "Federated Learning and AI Regulation in the European Union: Who is Responsible? \u2013 An Interdisciplinary Analysis", "authors": ["Herbert Woisetschl\u00e4ger", "Simon Mertel", "Christoph Kr\u00f6nke", "Ruben Mayer", "Hans-Arno Jacobsen"], "abstract": "The European Union Artificial Intelligence Act mandates clear stakeholder responsibilities in developing and deploying machine learning applications to avoid substantial fines, prioritizing private and secure data processing with data remaining at its origin. Federated Learning (FL) enables the training of generative AI Models across data siloes, sharing only model parameters while improving data security. Since FL is a cooperative learning paradigm, clients and servers naturally share legal responsibility in the FL pipeline. Our work contributes to clarifying the roles of both parties, explains strategies for shifting responsibilities to the server operator, and points out open technical challenges that we must solve to improve FL's practical applicability under the EU AI Act.", "sections": [{"title": "1. Introduction", "content": "With the introduction of the European Union Artificial Intelligence Act (AI Act) (Council of the European Union, 2021) and other international regulations being on the horizon, e.g., in the United States (The White House, 2023) and Canada (House Of Commons of Canada, 2022), everyone concerned with the development and deployment of AI has to adapt to new game rules. This entails data governance, robustness against adversarial scenarios, and energy considerations (Woisetschl\u00e4ger et al., 2024a). The AI Act puts the service provider into the spotlight, who has to assume responsibility for model development and deployment within the meaning of Article 3. Especially regarding data governance, the AI Act instantiates extensive rules for high-risk and general-purpose AI applications (GPAI, Article 52) that cater to data privacy and system security. The majority of generative AI applications fall under the GPAI definition in Article 3.\nFederated Learning (FL) presents a privacy-enhancing and data-protecting machine learning technique (McMahan et al., 2017) that has recently received increased attention for enabling access to data silos for generative AI applications (Woisetschl\u00e4ger et al., 2024b). In FL, a server operator provides an ML model sent to several clients and then trained on the clients' local data, which collaboratively train a global model via a central server, aggregating their local model updates. Private and secure computing techniques like Differential Privacy or Trusted Execution Environments help improve data privacy and system security (Bonawitz et al., 2017; Andrew et al., 2021). FL's data locality removes the key challenge of monitoring data lineage and simplifies accounting for user consent. Specifically, we study the FL workflow in alignment with related work (Li et al., 2020; Hard et al., 2018; McMahan et al., 2017) to touch up on the following:\nData Acquisition. The server operator can only employ a variety of client sampling strategies (Malinovsky et al., 2023; Wang & Ji, 2022; McMahan et al., 2017) for an FL training round, without the ability to directly investigate client data or process integrity.\nData Storage. Similarly, the clients decide how, where, and when to store data. This has implications on data availability, which directly touches upon the AI Act data governance requirements (Article 10)\u00b9.\nData Preprocessing. While the server operator can provide instructions on how to preprocess data so that the data is compatible with the ML model, the clients have the freedom to run additional preprocessing steps."}, {"title": "2. Technical Solutions Need to Focus on Transferring Responsibility to the Server Operator", "content": "For practical FL applications, the server operator must assume the role of the service provider by employing appropriate technical solutions.\nWhen establishing an FL system that could potentially entail thousands of clients at a time, managing responsibilities is likely to become a key challenge. Thus, we require solutions that provide for auditability, verifiability, integrity, and privacy.\nAuditability & verifiability. There is a natural trade-off between privacy and data audits. The core paradigm of FL is to not share data beyond a client's area of control. Data is strictly inaccessible for everybody but the data owner, and even in-house restricted to authorized personnel. Thus, we face a challenge when aiming to audit all steps that happen on a client device or data server. For instance, a work by (Liu et al., 2023) uses a Bayesian Nash equilibrium and a market mechanism to incentivize truthful client behavior, i.e., submission of useful model updates. While this approach significantly reduces the risk of adversarial attacks, it does not meet the requirements for auditing in the context of the AI Act, which are well-defined. Quintessentially, any data that is being captured, processed, and used in a training process must be evaluated for potential bias or adversarial information. To achieve this, numerous works combining FL with blockchain technology explore auditing the data processing steps and the training itself (Nguyen et al., 2021; Ma et al., 2020). What remains open is to develop solutions against data tampering.\nIntegrity & privacy. Particularly, we have to rethink the obligations of the provider concerning data integrity and protection (Articles 8\u201310), such that responsibility is transferred to the FL server. To account for the asymmetry of access and control-by-design in FL systems, we must develop data integrity measures that capture the nature of client data at the time of collection, while preprocessing the data, and immediately before starting the training process. Peer-based verification schemes of model updates are a promising direction to identify adversarial clients (Roy Chowdhury et al., 2022). Extending such schemes from client models to client data without infringing privacy would be interesting. At the same time, technical solutions must be in line with the requirements set out in the GDPR, which are not (necessarily) aligned with the concepts and rules of the AI Act."}, {"title": "3. Regulatory Implementations Need to Foster Integrity and Verifiability", "content": "We need FL server operators to assume full responsibility; clients are technically and legally obligated to comply.\nService Provider. The GDPR (Council of the European Union, 2016) defines the term data controller. Complementary, the AI Act defines the service provider of AI systems. For data protection assessment when processing personal information at first, we need to clarify who is the data controller responsible and accountable for each distinct phase of the data processing and must demonstrate compliance with the requirements of the GDPR (Article 5). The AI Act does not have a differentiated allocation of roles for separate processing phases and focuses on one central \u201cprovider\" of a (compliant) AI system, defined in Article 3, with the obligations arising from Article 8.\nWhile both the FL server and clients could be considered providers under the AI Act, since the AI Act (unlike the GDPR) focuses less on responsibilities for individual, definable data processing phases and more on secure system design as a whole, the provider concept has to be teleologically limited to the FL server. Thus, the server acts as the fully responsible service provider under the AI Act (Article 8), especially concerning data governance (Article 10) and General-Purpose AI service (Article 52).\nGeneral Terms and Conditions for AI Systems. While in Article 4 of the GDPR, the controller is the person who, alone or jointly with others, decides on the purposes (\"why\") and means (\"how\") of the processing of personal data, the AI Act focuses on the (traditionally single and) central provider of an AI system. However, since clients are autonomously in control of their data while the server is in control of the model, we see an inconsistency between what is controllable by the service provider and what he is responsible for. To close this gap, we need a two-pronged approach - technical and legal (\"how\" & \"why\"). Responsibility in FL should depend on the server's physical, technical, and legal ability to influence decentralized model training and configuration. This poses challenges. Unlike Article 26 and 28 GDPR, Article 8 et seq. of the AI Act do not provide details on governance in networked processing environments like FL systems.\nThe data protection assessment of the FL lifecycle may be impacted if the FL server sets requirements for the clients, which may lead to the server being classified as the controller under the GDPR. At first glance and from a strictly technical perspective, both the FL server and the FL clients fall under the provider concept of Article 3 GDPR, yet a \"joint providership\" (based on \u201cjoint controllership\" under the GDPR, Article 26) does not exist under the AI Act. If the FL server, in fulfillment of its obligations under the AI Act, in particular Article 10, sets far-reaching requirements for the FL clients concerning the training of models and handling of training data. These requirements could lead to the FL server being classified as a controller under data protection law within the means of Article 4 GDPR, while the FL client is classified as a mere processor within the scope of Article 28 GDPR.\nThus, a key legal instrument for ensuring compliance with the AI Act and GDPR (Articles 26 & 28) is likely the development of specific General Terms and Conditions binding for both FL server and clients. The server operator, as the service provider, has to oblige clients to provide sufficient reporting compliant with the AI Act. This can be supported by cryptographic tools that minimize the need for trust among entities (Nguyen et al., 2021)."}, {"title": "4. Considerations on Major Federated Learning Architectures", "content": "Cross-silo FL may allow for more flexibility in system design and responsibility distribution between clients and server than cross-device FL.\nWhile Section 2 and Section 3 are generally applicable to any FL application, there are two major system architectures that create further opportunities to organize responsibilities: cross-silo and cross-device training. Article 10 does not call for shared responsibility and leaves it open to \"appropriate measures\".\n4.1. Cross-Device Federated Learning\nCross-device FL typically entails a large number of devices (> 1,000). In such a setup, FL clients are characterized by having a very small number of local data samples and little participation time in the federated training process (Hard et al., 2018). This is a major challenge regarding client accountability and, ultimately, becomes problematic when a client should assume responsibility as a service provider. Hence, for practical considerations, all responsibility has to be assumed by the server in the cross-device setting and there is practically no room for client-side responsibility and a strong need for tools and methods that allow the FL server to cover all compliance criteria. This implies that the runtime environment on clients must be as encapsulated as possible, coupled with strict terms of service agreements.\n4.2. Cross-Silo Federated Learning\nIn contrast, in cross-silo settings, individual clients hold a significant amount of data and participate in multiple training rounds, and usually come with higher computational capabilities than in cross-device FL. Typically, cross-silo FL can involve large institutions such as hospitals (Huang et al., 2022), which themselves have a high commitment to regulatory compliance and take strong precautions regarding security and privacy protection. As such, it is an open research direction to explore the synergies between established institutional processes (e.g., medical record keeping) and the AI Act requirements (e.g., on data transparency). The terms and conditions must be balanced between ensuring appropriate regulatory compliance and practical utility such that clients are incentivized to participate in training, and we can assume partial responsibility on the side of clients. Such synergies could help better balance the service provider responsibilities and reduce costs for clients and the server, not only improving the economic viability of FL but also its ecological footprint."}, {"title": "5. Conclusion", "content": "We study the FL life-cycle responsibilities under the AI Act. We find client-side responsibility for numerous steps, which practically limits the applicability of FL to open up additional data silos that would benefit the training of foundation models. Yet, there are promising directions that deserve increased attention such that a server operator can become the service provider without clients being required to assume extensive liability. With this, one can drive the adoption of FL and help decrease data bias by directly relying on user data. Further clarifying the outlined service provider question directly responds to the EU AI Office's call for contributions to help implement the AI Act (Nature, 2024)."}]}