{"title": "DisMix: Disentangling Mixtures of Musical Instruments for Source-level Pitch and Timbre Manipulation", "authors": ["Yin-Jyun Luo", "Kin Wai Cheuk", "Woosung Choi", "Toshimitsu Uesaka", "Keisuke Toyama", "Koichi Saito", "Chieh-Hsin Lai", "Yuhta Takida", "Wei-Hsiang Liao", "Simon Dixon", "Yuki Mitsufuji"], "abstract": "Existing work on pitch and timbre disentanglement has been mostly focused on single-instrument music audio, excluding the cases where multiple instruments are presented. To fill the gap, we propose DisMix, a generative framework in which the pitch and timbre representations act as modular building blocks for constructing the melody and instrument of a source, and the collection of which forms a set of per-instrument latent representations underlying the observed mixture. By manipulating the representations, our model samples mixtures with novel combinations of pitch and timbre of the constituent instruments. We can jointly learn the disentangled pitch-timbre representations and a latent diffusion transformer that reconstructs the mixture conditioned on the set of source-level representations. We evaluate the model using both a simple dataset of isolated chords and a realistic four-part chorales in the style of J.S. Bach, identify the key components for the success of disentanglement, and demonstrate the application of mixture transformation based on source-level attribute manipulation.", "sections": [{"title": "1 Introduction", "content": "Disentangled representation learning (DRL) captures semantically meaningful latent features of observed data in a low-dimensional latent space (Bengio, Courville, and Vincent 2013). By applying a generative framework such as variational autoencoders (VAEs) (Kingma and Welling 2014), we can train an encoder to encode the data and associate data-generating factors with separate subspaces in the latent space, and a decoder to reconstruct the data given the original encoding or to render novel data given manipulated latent features (Tschannen, Bachem, and Lucic 2018; Chen et al. 2018; Kim and Mnih 2018; Higgins et al. 2016).\nBecause each feature representation lives in a subspace corresponding to a unique concept or data attribute such as the size or shape of a physical object in an image, manipulating specific representations only renders variation of a few and particular factors in the decoder output, and thereby facilitates controllable transformation of existing data.\nDRL has been applied to music audio to extract representations of timbre (e.g., the musical instrument played in a recording) and pitch (e.g., the melody played by the instrument) (Luo, Agres, and Herremans 2019; C\u00edfka et al. 2021; Tanaka et al. 2021; Luo et al. 2020; Tanaka et al. 2022; Luo, Ewert, and Dixon 2022; Engel et al. 2017; Bitton, Esling, and Chemla-Romeu-Santos 2018; Wu et al. 2023; Liu et al. 2023; Wu et al. 2024a). Disentangling the two attributes enables applications such as transferring the instrument played in a reference audio to a target instrument provided by another audio example, while preserving the melody played by the reference instrument. This is similar to voice conversion, which aims at replacing a reference speaker's identity such that the converted speech sounds as if a different speaker spoke the content originally uttered by the reference speaker (Hsu, Zhang, and Glass 2017; Qian et al. 2019).\nDespite being widely adopted for instrument attribute transfer, pitch-timbre disentanglement has been mostly applied to single-instrument audio. Therefore, the analysis and the synthesis of the two attributes are only amenable to a single instrument at a time, which excludes cases where multiple instruments are presented in music audio.\nAlternatively, one can use off-the-shelf source separation models to first separate each instrument from a mixture and apply the aforementioned approaches for disentanglement. However, it introduces artefacts into the separated instrument and limits the instruments that can be handled to those supported by the state-of-the-art source separation models (e.g., drums, bass, vocals, and others) (Lu et al. 2024; Rouard, Massa, and D\u00e9fossez 2023; Hennequin et al. 2020).\nTo fill the gap, we propose DisMix, a framework which disentangles a mixture of instruments and renders a novel mixture conditioned on a set of pitch-timbre disentangled representations. DisMix represents each instrument, or source, in a mixture by a source-level representation that combines a pair of pitch and timbre latent variables. The two attributes are encoded separately so that they can be independently manipulated for each instrument. A decoder then takes as input the manipulated set of source-level representations and renders a new mixture that consists of sources whose pitch and timbre are dictated by the manipulated source-level representations.\nThe pitch and timbre representations act as modular building blocks to construct the melody and instrument of a source or an \"audio object\". The decoder assembles these objects in a way that preserves their pitch and timbre. This is reminiscent of \"object representation\u201d motivated by humans' capability to understand complex ideas in terms of reusable and primitive components (Greff, van Steenkiste, and Schmidhuber 2020; Bizley and Cohen 2013)."}, {"title": "2 Related Work", "content": "Pitch and Timbre Disentanglement Strategies for pitch-timbre disentanglement include supervised learning (Luo, Agres, and Herremans 2019; Engel et al. 2017; Bitton, Esling, and Chemla-Romeu-Santos 2018; Wu et al. 2023), metric learning based on domain knowledge (Esling, Chemla-Romeu-Santos, and Bitton 2018; Tanaka et al. 2021, 2022; C\u00edfka et al. 2021; Luo et al. 2020), and more general inductive biases (C\u00edfka et al. 2021; Luo, Ewert, and Dixon 2022; Liu et al. 2023; Wu et al. 2024a; Luo, Ewert, and Dixon 2024). Despite their success for tasks such as attribute swapping, these methods are focused on single-instrument input.\nInstead, we disentangle mixtures of instruments and represent each instrument by a source-level latent representation which captures pitch and timbre in separate dimensions.\nOnly a few studies explicitly extract pitch and timbre information from mixtures of instruments. Hung et al. (2019) and Cwitkowitz et al. (2024) encode the overall timbre of a mixture and consider applications of symbolic music rearrangement and transcription, respectively. Cheuk et al. (2023) propose a multi-task framework that conditions music pitch transcription on intermediate timbre information.\nLin et al. (2021) tackle transcription and separation of mixtures and single-instrument generation in a unified framework. Rather than focusing on single-instrument cases, we are interested in sampling novel mixtures by conditioning a DiT on a set of per-instrument representations and we evaluate our model by source-level attribute manipulation.\nObject-Centric Representation Learning Learning object representations entails encoding object entities in a visual scene by unique representations (Greff, van Steenkiste, and Schmidhuber 2020; Locatello et al. 2020). Singh, Kim, and Ahn (2022); Wu, Lee, and Ahn (2023); Wu et al. (2024b) take a step further to disentangle attributes of individual objects for achieving compositional scene generation. For audio, Gha et al. (2023) and Reddy et al. (2023) learn separate representations for different sources from mixtures. We further disentangle pitch and timbre of individual sources and tackle a more complex dataset with a conditional LDM."}, {"title": "3 DisMix: The Proposed Framework", "content": "Given a mixture $X_m$ of $N_s$ instruments and a query $x_q^{(i)}$ (detailed in Section 3.1), our goal is to extract latent representations of pitch $\\nu^{(i)}$ and timbre $\\tau^{(i)}$ for an instrument $i \\in [1, N_s]$, and to be able to sample a novel mixture conditioned on a set of manipulated pitch and timbre latents.\nWe propose DisMix to achieve the goal. Its generative process on the left side of Fig. 1 samples $x_m$, through a neural network $\\Theta_m$, conditioned on a set of source-level representations $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ where $s^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(i)})$ is a deterministic function of timbre and pitch. The pitch latent $\\nu^{(i)}$ is computed via a neural network $\\Theta_\\nu$, by its corresponding ground-truth pitch annotation $y^{(i)}$. The joint distribution over the variables can be written as follows:\n$\\begin{equation}  P_{\\Theta_m}(x_m|S) \\prod_{i=1}^{N_s} P_{\\Theta_\\nu,\\phi}(\\nu^{(i)}|y^{(i)}) \\rho(\\tau^{(i)}), \\tag{1} \\end{equation}$\nwhere $\\rho(\\tau^{(i)}) = \\mathcal{N}(0, I)$. Note that $\\nu^{(i)}$ and $\\tau^{(i)}$ are sampled independently and so are $\\{s^{(i)}\\}_{i=1}^{N_s}$, whereby the pitch and timbre latents can act as modular building blocks to compose a mixture.\nWe propose two instances of DisMix: 1) an auto-encoder for a simple case study using a simplistic dataset (Section 4) and 2) an LDM implemented using a DiT verified with a more complex dataset (Section 5). We specify $\\Theta_m$, $\\Theta_s$, and $\\Theta_\\nu$ for the two instances in Sections 4 and 5, respectively. Here, we describe the common components between them.\nFollowing VAEs (Kingma and Welling 2014), we learn DisMix using an inference network to approximate posteriors over the latent variables that are otherwise intractable, and learns the model by maximising a lower bound to the marginal likelihood $p(x_m|\\{y^{(i)}\\}_{i=1}^{N_s})$. For the two instances of DisMix, we employ a common parameterisation of the inference network illustrated on the right side of Fig. 1, which we explain in the following sections."}, {"title": "3.1 Mixture and Query Encoders", "content": "Given a mixture of instruments, additional information would be necessary to specify which instrument's latents are to be extracted. Motivated by Lin et al. (2021) and Lee, Choi, and Lee (2019), we use the query $x_q^{(i)}$ so that the extracted latents of the source $i$ and the query share the same timbre characteristics, while they can carry arbitrary pitch information. We describe the objective function Eq. (7) that could encourage this behaviour in Section 3.3.\nDuring training, we pair each mixture $x_m$ with $X_q = \\{x_q^{(i)}\\}_{i=1}^{N_s}$, a set of $N_s$ queries with their instruments corresponding to the constituent instruments of the mixture, and each $x_q^{(i)}$ is randomly sampled from a subset of the complete training dataset consisting of a constituent instrument.\nAs illustrated on the right side of Fig. 1, the inference of $\\tau^{(i)}$ and $\\nu^{(i)}$ is conditioned on both $x_m$ and $x_q^{(i)}$. In practice, we extract their compact counterparts $e_m = E_{\\Theta_m}(x_m)$ and $e_q^{(i)} = E_{\\Theta_q}(x_q^{(i)})$, respectively, where $E_{\\Theta_m}(\\cdot)$ and $E_{\\Theta_q}(\\cdot)$ are neural network encoders with parameters $\\Theta_m$ and $\\Theta_q."}, {"title": "3.2 Pitch and Timbre Encoders", "content": "As suggested by Fig. 1, we propose that the pitch and timbre encoders admit a common factorised form:\n$\\begin{equation}  q_{\\Phi_u}(U|x_m, X_q) = \\prod_{i=1}^{N_s} q_{\\phi_u}(u^{(i)}|e_m, e_q^{(i)}), \\tag{2} \\end{equation}$\nwhere $u \\in \\{\\nu, \\tau\\}$ and $U \\in \\{\\{\\nu^{(i)}\\}_{i=1}^{N_s}, \\{\\tau^{(i)}\\}_{i=1}^{N_s}\\}$. $\\Phi_u$ and $\\phi_u$ are parameters of the two encoders. Given $e_m$ and $e_q^{(i)}$, the pitch and timbre latents of the $i$-th source are encoded independently of each other and of other sources.\nWe apply a binarisation layer (Dong and Yang 2018) to constrain the capacity of the pitch latent, which is proven crucial for disentanglement in our empirical studies. We also show that imposing the standard Gaussian prior $\\rho(\\tau^{(i)})$ is a simple yet effective way to constrain the timbre latent.\nConstraining Pitch Latents The pitch encoder combines a transcriber $E_{\\phi_{\\nu}}(\\cdot)$ that extracts pitch logits of the $i$-th source $\\hat{y}^{(i)} = E_{\\phi_{\\nu}}(e_m, e_q^{(i)})$, a stochastic binarisation layer (SB) that constrains the information capacity, and a translator $f_{\\Theta_{\\nu}}(\\cdot)$ that computes the final outcome:\n$\\begin{equation}  \\phi_{\\nu}(\\nu^{(i)}|e_m, e_q^{(i)}) := \\delta(\\nu^{(i)} - f_{\\Theta_{\\nu}}(b_\\text{bin}^{(i)})), \\tag{3} \\end{equation}$\nwhere $\\delta(\\cdot)$ is the Dirac delta function, corresponding to the diamond node of $\\nu^{(i)}$ on the right of Fig. 1, and:\n$\\begin{equation}  y_\\text{bin}^{(i)} = SB(\\hat{y}^{(i)}) = \\mathbb{1}\\{\\text{Sigmoid}(\\hat{y}^{(i)}) > h\\}, \\tag{4} \\end{equation}$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function, and $h$ is the threshold sampled from the uniform distribution $\\mathcal{U}(0,1)$ at each training step and is fixed at 0.5 during evaluation. The straight-through estimator (Bengio, L\u00e9onard, and Courville 2013) is used to bypass the non-differentiable operator.\nConstraining Timbre Latents The timbre encoder parameterises a Gaussian with a neural network $\\phi_\\tau$:\n$\\begin{equation}  q_{\\phi_\\tau}(\\tau^{(i)}) = \\mathcal{N}(\\tau^{(i)}; \\mu_{\\phi_\\tau}(e_m, e_q^{(i)}), \\sigma^2_{\\phi_\\tau}(e_m, e_q^{(i)})I), \\tag{5} \\end{equation}$\nwhere $q_{\\phi_\\tau}(\\tau^{(i)}) := q_{\\phi_\\tau}(\\tau^{(i)}|e_m, e_q^{(i)})$ and sampling $\\tau^{(i)}$ is reparameterised as $\\mu_{\\phi_\\tau}(\\cdot) + \\epsilon \\sigma_{\\phi_\\tau}(\\cdot)$, where $\\epsilon \\sim \\mathcal{N}(0, I)$, to be differentiable (Kingma and Welling 2014).\nWe let $\\rho(\\tau^{(i)}) = \\mathcal{N}(0,I)$ to constrain the timbre latent through the Kullback\u2013Leibler divergence (KLD) in Eq. (6)."}, {"title": "3.3 Training Objectives", "content": "ELBO We start with an evidence lower bound (ELBO) to the marginal log-likelihood $\\log p(x_m|\\{y^{(i)}\\}_{i=1}^{N_s})$:\n$\\begin{equation}  L_\\text{ELBO} = \\mathbb{E}_{\\prod_{i=1}^{N_s} q_{\\phi_\\tau}(\\tau^{(i)})} \\Big[\\log P_{\\Theta_m}(x_m|\\{\\nu^{(i)}, \\tau^{(i)}\\}_{i=1}^{N_s})\\Big] + \\sum_{i=1}^{N_s} \\log p_{\\Theta_\\nu}(\\nu^{(i)}|y^{(i)}) - D_{KL} \\Big(q_{\\phi_\\tau}(\\tau^{(i)})||p(\\tau^{(i)})\\Big), \\tag{6} \\end{equation}$\nwhere $\\nu^{(i)} = f_{\\Theta_\\nu}(y_\\text{bin}^{(i)})$ by Eq. (3). Note that the conditionals of the first term can be expressed in terms of $S = \\{s^{(i)}\\}_{i=1}^{N_s}$, where $s^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(i)})$ as described previously for Eq. (1). $L_\\text{ELBO}$ is derived as a result of the factorised posteriors in Eq. (2) and the deterministic pitch encoder by Eq. (3), which we detail in Appendix A. We specify $\\Theta_m, \\Theta_s$, and $\\Theta_\\nu$ in Section 4 and 5.\nIntuitively speaking, we extract both a pitch and a timbre latent for each source given a mixture and a query, and the collection of which is used to reconstruct the mixture. The pitch and timbre latents adhere to certain constraints and priors to encourage the disentanglement of the two attributes.\nApart from mixtures, we also observe individual sources $x_s^{(i)}$, so we include a source-wise reconstruction loss. This is a special case of the first term in Eq. (6) when $N_s = 1$ and $x_m$ becomes $x_s$, and we can reuse $\\Theta_m$ to reconstruct individual sources.\nPitch Supervision To enhance the disentanglement, we also minimise a binary cross entropy loss $L_\\text{BCE}(\\hat{y}^{(i)}, y^{(i)})$ where $y^{(i)}$ is the pitch annotation of the $i$-th source. We explore relaxing the model by excluding this term, with the results reported in Table 3.\nBarlow Twins Finally, we minimise a simplified Barlow Twins loss (Zbontar et al. 2021) to enhance the correlation between the query and the timbre latent for them sharing the timbre characteristics as described in Section 3.1:\n$\\begin{equation}  L_\\text{BT} = \\sum_{i=1}^{N_s} \\sum_{d=1}^{D_T} (1 - C_{dd}(e_q^{(i)}, \\tau^{(i)}))^2, \\tag{7} \\end{equation}$\nwhere $C$ is a cross-correlation matrix, and both $e_q^{(i)}$ and $\\tau^{(i)}$ share the same dimensionality $D_T$. Empirically, $L_\\text{BT}$ counteracts the over-regularisation effect of the prior $\\rho(\\tau^{(i)})$ and promotes a discriminative timbre space as shown in Fig. 2.\nThe Final Objective In summary, we maximise:\n$\\begin{equation}  L_\\text{DisMix} = L_\\text{ELBO} - L_\\text{BCE} - L_\\text{BT}. \\tag{8} \\end{equation}$\nWe do not find explicitly weighting each loss term necessary.\nNext, we detail the implementations specific to the simple and more sophisticated variants of DisMix in Sections 4 and 5, respectively."}, {"title": "4 A Simple Case Study", "content": "Reconstructing Mixtures $P_{\\Theta_m}(x_m|S)$ in Eq. (6) is a Gaussian likelihood parameterised by a decoder $\\Theta_m$, which can be a permutation invariant function such as a transformer (Vaswani et al. 2017) without positional embeddings that outputs the Gaussian mean, reconstructing a mixture given a set of source-level representations.\nWe opt for a simple implementation of $\\Theta_m$ here and discuss a transformer in Section 5. In particular, we slightly deviate from Eq. (6) and reconstruct $x_m$ using $e_m$ in Section 3.1, instead of $S$, whereby the likelihood becomes $P_{\\Theta_m}(x_m|e_m)$, and we maximise an additional term:\n$\\begin{equation}  \\mathbb{E}_{\\prod_{i=1}^{N_s} q_{\\phi_\\tau}(\\tau^{(i)})} \\Big[\\log p(e_m|\\{\\nu^{(i)}, \\tau^{(i)}\\}_{i=1}^{N_s})\\Big], \\tag{9} \\end{equation}$\nwhere $p(e_m|S) = \\mathcal{N}(e_m; \\sum_{i=1}^{N_s} s^{(i)}, \\sigma_m^2 I)$ and $\\sigma_m = 0.25$ is a hyperparameter. Intuitively, we extract the source-level latents $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ whose summation $s_\\text{sum} = \\sum_{i=1}^{N_s} s^{(i)}$ and $e_m$ are pulled together as measured by a mean square error weighed by $\\sigma_m^2$.\nDuring evaluation, we instead use $s_\\text{sum}$ to reconstruct the mixture $x_m$ or render a novel one by manipulating $\\{s^{(i)}\\}_{i=1}^{N_s}$ before the summation. The assumption is that both $e_m$ and $s_\\text{sum}$ can reconstruct $x_m$ comparably well by maximising Eq. (9) as we reconstruct $x_m$ by $e_m$ during training.\nThis approach avoids implementing a (potentially expensive and complicated) permutation invariant decoder and imposes linearity between $e_m$ and $s_\\text{sum}$ in the latent space. The linearity could enable other applications, which we explain in Appendix B.1 and leave for future work.\nIntegrating Pitch and Timbre We employ FiLM (Perez et al. 2018; Kim et al. 2018a) and derive the source-level representation as $s^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(i)}) = \\alpha_{\\Theta_s}(\\tau^{(i)}) \\nu^{(i)} + \\beta_{\\Theta_s}(\\tau^{(i)})$, where $\\nu^{(i)}$ is scaled and shifted element-wise by the factors determined as a deterministic function $\\Theta_s$ of $\\tau^{(i)}$.\nConfiguring Pitch Priors We study pitch priors at different levels of capacity. The first follows Eq. (1) and defines $p_{\\Theta_\\nu}(\\nu^{(i)}|y^{(i)}) = \\mathcal{N}(\\nu^{(i)}; f_{\\Theta_{\\nu}}(y^{(i)}), \\sigma_\\nu^2 I)$, a Gaussian parameterised by $\\Theta_\\nu$, given the ground-truth pitch $y^{(i)}$. $\\sigma_\\nu$ is a hyperparameter.\nWe also consider a richer prior to capture the source interaction: $p_{\\Theta_\\nu}(\\nu^{(i)}|Y_\\backslash i) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\nu^{(i)}; \\mu_{\\Theta_{\\nu, k}}(Y_\\backslash i), \\sigma_\\nu^2 I)$, where $Y_\\backslash i$ denotes a set of pitch annotations excluding that of the $i$-th source, and $\\Theta_{\\nu,k}$ parameterises the mean of the $k$-th Gaussian in a Gaussian mixture. The rationale is that $\\hat{y}^{(i)}$ is conditionally dependent on pitch of other sources in a mixture to confer musical harmony."}, {"title": "4.1 Implementations", "content": "Dataset Gha et al. (2023) compile synthetic audio using 3,131 unique chords from JSB Chorales (Boulanger-Lewandowski, Bengio, and Vincent 2012), rendered by sound fonts of piano, violin, and flute via FluidSynth.\nGiven a chord, each composite note is synthesised to an audio waveform at 16kHz with a sound font randomly sampled with replacement, whereby a sound font $i$ can play multiple notes in a chord. These notes together define a source $x_s^{(i)}$ whose pitch annotation is a mutli-hot vector $y^{(i)} \\in \\{0,1\\}^{N_p}$. The note waveforms are summed to form the chord's waveform which defines a mixture $x_m$.\nThere are 28,179 samples of mixtures split into the train, validation, and test sets with a ratio of 70/20/10. The waveforms are converted into mel spectrograms using 128 mel-filter bands, a window size of 1,024, and a hop length of 512. We crop a 320ms segment, or 10 spectral frames, from the sustain phase of each sample.\nArchitecture Simple layers including the MLP and RNN are used for implementation to evaluate this simple case study, and we extend DisMix to a more complex setup with an LDM in Section 5. A major difference between the two setups is that both $\\nu^{(i)}$ and $\\tau^{(i)}$ are represented as a single vector, as pitch and timbre are time-invariant within the simplified scope of the MusicSlot dataset (Gha et al. 2023). On the other hand, the CocoChorale dataset (Wu et al. 2022) features time-varying melodies in each sample. We elaborate the implementation details in Appendix B.1.\nOptimisation We use Adam (Kingma and Ba 2015) and a batch size of 32, a learning rate of 0.0004, and a gradient clipping value of 0.5. Training is terminated if Eq. (8) stops improving on the validation set for 260k steps."}, {"title": "4.2 Results", "content": "Evaluation Given $x_m$ and a set of queries $\\{x_q^{(i)}\\}_{i=1}^{N_s}$ corresponding to the constituent instruments of the mixture, we first extract $\\{\\tau^{(i)}, \\nu^{(i)}\\}_{i=1}^{N_s}$. To evaluate disentanglement, we conduct a random permutation so that the pitch latent of the source $i$ can be swapped for that of the source $j$, while the timbre remains unchanged, which yields $\\hat{s}^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(j)})$. Then, we render a novel source $\\hat{x}_s^{(i)}$ by passing $\\hat{s}^{(i)}$ to the decoder $P_{\\Theta_m}$. Note that we render sources instead of mixtures by using $\\hat{s}^{(i)}$ as the input instead of the summation of multiple source-level representations.\nA successful disentanglement entails that judges of pitch and instrument should classify $\\hat{x}_s^{(i)}$ as the pitch of $x_s^{(j)}$ (as it was swapped) and the instrument of $x_s^{(i)}$ (as it was preserved), respectively. We pre-train a pitch and an instrument classifier using the training set and direct them as the judges. The classification accuracy is reported under \u201cDisentanglement\u201d in Table 1 and 2.\nTo see if the model can render novel mixtures, we first produce a new set $\\{\\hat{s}^{(i)}\\}_{i=1}^{N_s}$ after the permutation and render a novel mixture $\\hat{x}_m$ by passing $P_{\\Theta_m}$ the summation $\\hat{s}_\\text{sum} = \\sum_{i=1}^{N_s} \\hat{s}^{(i)}$. To check whether the constituent attributes of $\\hat{x}_m$ are indeed dictated by the manipulated $\\{\\hat{s}^{(i)}\\}_{i=1}^{N_s}$, we once again extract its source-level representations with the original queries and reconstruct the sources, which are then fed to the judges. \u201cMixture Rendering\u201d in Table 1 and 2 report the classification accuracy."}, {"title": "5 A Latent Diffusion Framework", "content": "We also implement DisMix by an LDM framework (Rombach et al. 2022), where the decoder $P_{\\Theta_m}(x_m|S)$ is a diffusion transformer (DiT) (Peebles and Xie 2023) that directly reconstructs a mixture given a set of source-level latents, and no additional objective such as Eq. (9) is introduced.\nData Representation The LDM framework (Rombach et al. 2022) improves the compute efficiency of diffusion models (DMs) (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020) by first projecting data to a low-dimensional latent space. We leverage the pre-trained VAE from AudioLDM2 (Liu et al. 2024) which is trained on multiple music and audio datasets to extract $z^{(i)} = E_\\text{VAE}(x_s^{(i)})$, where $E_\\text{VAE}(\\cdot)$ is the VAE encoder, and we use the pre-trained VAE decoder $D_\\text{VAE}(\\cdot)$ to recover $x_s^{(i)}$.\nLatent Diffusion Models LDMs operate DMs in a latent space and sample $z_0$, the latent representation of data, from a Markov chain: $p(z_t) = \\prod_{i=1}^{T} p_\\theta(z_{t-1}|z_t)$, with $p(z_T) = \\mathcal{N}(z_T; 0,I)$ and $p_\\theta(z_{t-1}|z_t) = \\mathcal{N}(z_{t-1}; \\mu_\\theta(z_t, t), \\Sigma_\\theta(z_t,t))$ parameterised by $\\theta$. The posterior is a linear Gaussian $q(z_t|z_{t-1}) = \\mathcal{N}(z_t; \\sqrt{\\alpha_t} z_{t-1}, (1 - \\alpha_t)I)$, where $\\alpha_t$ is a hyperparameter evolving over the diffusing step $t$.\nGiven that the forward process $q$ is known and fixed, Ho, Jain, and Abbeel (2020) employ specific forms of $\\mu_\\theta$ and $\\Sigma_\\theta$ to match $q$ and simplify the training to essentially minimising $||f_\\theta(z_t, t) - z_0||^2$, which boils down to training a decoder $f_\\theta$ to predict the clean $z_0$ given its corrupted counterpart $z_t$."}, {"title": "5.1 Adapting Diffusion Transformers", "content": "Different from the vanilla LDMs, we condition the decoder with $S$, replacing the first term of Eq. (6) with:\n$\\begin{equation}  P_{\\Theta_m}(z_{m,0:T}|S) = p(z_{m,T}) \\prod_{t=1}^{T} P_{\\Theta_m}(z_{m,t-1}|z_{m,t}, S). \\tag{10} \\end{equation}$\n$z_{m,t}$ denotes the noised latent feature of mixtures at diffusing step $t$ and is defined in terms of $\\{z_s^{(i)}\\}_{i=1}^{N_s}$, where $z_s^{(i)} = z_s^{(i)}$. $P_{\\Theta_m}$ models the interaction among the elements in the set $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ and iteratively performs the reverse process. Defining $z_{m,t}$ in terms of the constituent sources facilitates the conditioning mechanism that we explain next.\nPartition DiTs (Peebles and Xie 2023) operate on a sequence of image patches to work with a transformer. Similarly, we first apply a sinusoidal positional encoding to $z_s^{(i)}$ to preserve the temporal order and partition it by $Par(z_s^{(i)}): \\mathbb{R}^{T \\times (D_z \\times C)} \\rightarrow \\mathbb{R}^{L \\times D'}$, where $T_z = 100, D_z = 16, C = 8$, and $L = 25$ are the numbers of time frames, feature dimensions, channels, and patches, respectively. That is, we partition along the time axis and flatten each patch whose size becomes $D' = \\frac{T_z}{L} \\times D_z \\times C$. We repeat the process for all $N_s$ elements in $\\{z_s^{(i)}\\}_{i=1}^{N_s}$, the outcome of which finally defines $z_{m,t} \\in \\mathbb{R}^{(N_s \\times L) \\times D'}$ in Eq. (10). In other words, we can consider it as a sequence of $N_s \\times L$ patches, with the size of each patch being $D'$.\nSimilarly, we partition the source-level representations $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ and obtain $s_c \\in \\mathbb{R}^{(N_s \\times L) \\times D'}$, where $D' = \\frac{T_z}{L} \\times D_s \\times C$, which we detail in Section 5.2. We can then represent the set condition $S$ by $s_c$ in Eq. (10).\nAs mentioned earlier, we define $z_{m,t}$ in terms of its constituent sources and conveniently align the dimensions of $s_c$ and $z_{m,t}$ (except for their sizes $D_z$ and $D_s$) which facilitates the conditioning described next.\nWe do not add another positional encoding to $z_{m,t}$ and $s_c$, ensuring the permutation invariance w.r.t. the $N_s$ sources.\nConditioning A transformer block consists of the multi-head self-attention mechanism and a feedforward network. Each of these modules is followed by a skip connection and a layer normalisation (LN) (Vaswani et al. 2017). We replace the standard LN with its adaptive variant (adaLN) for the conditioning (Peebles and Xie 2023).\nIn particular, $s_c$ is added with a diffusing step embedding and is used to regress the scaling and shifting factors $\\rho$ and $\\gamma$ of adaLN layers which match the dimension of $z_{m,t}$. Then, by passing $z_{m,t}$ through the transformer blocks, it is modulated by $\\rho$ and $\\gamma$ which carry the source-level pitch and timbre information computed from $s_c$."}, {"title": "5.2 Implementations", "content": "Dataset The CocoChorale dataset ("}]}