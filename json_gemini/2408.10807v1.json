{"title": "DisMix: Disentangling Mixtures of Musical Instruments\nfor Source-level Pitch and Timbre Manipulation", "authors": ["Yin-Jyun Luo", "Kin Wai Cheuk", "Woosung Choi", "Toshimitsu Uesaka", "Keisuke Toyama", "Koichi Saito", "Chieh-Hsin Lai", "Yuhta Takida", "Wei-Hsiang Liao", "Simon Dixon", "Yuki Mitsufuji"], "abstract": "Existing work on pitch and timbre disentanglement has been\nmostly focused on single-instrument music audio, exclud-\ning the cases where multiple instruments are presented. To\nfill the gap, we propose DisMix, a generative framework\nin which the pitch and timbre representations act as mod-\nular building blocks for constructing the melody and in-\nstrument of a source, and the collection of which forms a\nset of per-instrument latent representations underlying the\nobserved mixture. By manipulating the representations, our\nmodel samples mixtures with novel combinations of pitch\nand timbre of the constituent instruments. We can jointly\nlearn the disentangled pitch-timbre representations and a la-\ntent diffusion transformer that reconstructs the mixture condi-\ntioned on the set of source-level representations. We evaluate\nthe model using both a simple dataset of isolated chords and\na realistic four-part chorales in the style of J.S. Bach, identify\nthe key components for the success of disentanglement, and\ndemonstrate the application of mixture transformation based\non source-level attribute manipulation.", "sections": [{"title": "1 Introduction", "content": "Disentangled representation learning (DRL) captures se-\nmantically meaningful latent features of observed data in\na low-dimensional latent space (Bengio, Courville, and\nVincent 2013). By applying a generative framework such\nas variational autoencoders (VAEs) (Kingma and Welling\n2014), we can train an encoder to encode the data and asso-\nciate data-generating factors with separate subspaces in the\nlatent space, and a decoder to reconstruct the data given the\noriginal encoding or to render novel data given manipulated\nlatent features (Tschannen, Bachem, and Lucic 2018; Chen\net al. 2018; Kim and Mnih 2018; Higgins et al. 2016).\nBecause each feature representation lives in a subspace\ncorresponding to a unique concept or data attribute such as\nthe size or shape of a physical object in an image, manip-\nulating specific representations only renders variation of a\nfew and particular factors in the decoder output, and thereby\nfacilitates controllable transformation of existing data.\nDRL has been applied to music audio to extract represen-\ntations of timbre (e.g., the musical instrument played in a\nrecording) and pitch (e.g., the melody played by the instru-\nment) (Luo, Agres, and Herremans 2019; C\u00edfka et al. 2021;\nTanaka et al. 2021; Luo et al. 2020; Tanaka et al. 2022; Luo,\nEwert, and Dixon 2022; Engel et al. 2017; Bitton, Esling,\nand Chemla-Romeu-Santos 2018; Wu et al. 2023; Liu et al.\n2023; Wu et al. 2024a). Disentangling the two attributes en-\nables applications such as transferring the instrument played\nin a reference audio to a target instrument provided by an-\nother audio example, while preserving the melody played\nby the reference instrument. This is similar to voice con-\nversion, which aims at replacing a reference speaker's iden-\ntity such that the converted speech sounds as if a different\nspeaker spoke the content originally uttered by the reference\nspeaker (Hsu, Zhang, and Glass 2017; Qian et al. 2019).\nDespite being widely adopted for instrument attribute\ntransfer, pitch-timbre disentanglement has been mostly ap-\nplied to single-instrument audio. Therefore, the analysis and\nthe synthesis of the two attributes are only amenable to a sin-\ngle instrument at a time, which excludes cases where multi-\nple instruments are presented in music audio.\nAlternatively, one can use off-the-shelf source separation\nmodels to first separate each instrument from a mixture and\napply the aforementioned approaches for disentanglement.\nHowever, it introduces artefacts into the separated instru-\nment and limits the instruments that can be handled to those\nsupported by the state-of-the-art source separation mod-\nels (e.g., drums, bass, vocals, and others) (Lu et al. 2024;\nRouard, Massa, and D\u00e9fossez 2023; Hennequin et al. 2020).\nTo fill the gap, we propose DisMix, a framework which\ndisentangles a mixture of instruments and renders a novel\nmixture conditioned on a set of pitch-timbre disentan-\ngled representations. DisMix represents each instrument, or\nsource, in a mixture by a source-level representation that\ncombines a pair of pitch and timbre latent variables. The two\nattributes are encoded separately so that they can be inde-\npendently manipulated for each instrument. A decoder then\ntakes as input the manipulated set of source-level represen-\ntations and renders a new mixture that consists of sources\nwhose pitch and timbre are dictated by the manipulated\nsource-level representations.\nThe pitch and timbre representations act as modular build-\ning blocks to construct the melody and instrument of a\nsource or an \"audio object\". The decoder assembles these\nobjects in a way that preserves their pitch and timbre. This\nis reminiscent of \"object representation\u201d motivated by hu-\nmans' capability to understand complex ideas in terms of\nreusable and primitive components (Greff, van Steenkiste,\nand Schmidhuber 2020; Bizley and Cohen 2013)."}, {"title": "2 Related Work", "content": "Pitch and Timbre Disentanglement Strategies for pitch-\ntimbre disentanglement include supervised learning (Luo,\nAgres, and Herremans 2019; Engel et al. 2017; Bitton, Es-\nling, and Chemla-Romeu-Santos 2018; Wu et al. 2023), met-\nric learning based on domain knowledge (Esling, Chemla-\nRomeu-Santos, and Bitton 2018; Tanaka et al. 2021, 2022;\nC\u00edfka et al. 2021; Luo et al. 2020), and more general induc-\ntive biases (C\u00edfka et al. 2021; Luo, Ewert, and Dixon 2022;\nLiu et al. 2023; Wu et al. 2024a; Luo, Ewert, and Dixon\n2024). Despite their success for tasks such as attribute swap-\nping, these methods are focused on single-instrument input.\nInstead, we disentangle mixtures of instruments and rep-\nresent each instrument by a source-level latent representa-\ntion which captures pitch and timbre in separate dimensions.\nOnly a few studies explicitly extract pitch and timbre in-\nformation from mixtures of instruments. Hung et al. (2019)\nand Cwitkowitz et al. (2024) encode the overall timbre of\na mixture and consider applications of symbolic music re-\narrangement and transcription, respectively. Cheuk et al.\n(2023) propose a multi-task framework that conditions mu-\nsic pitch transcription on intermediate timbre information.\nLin et al. (2021) tackle transcription and separation\nof mixtures and single-instrument generation in a unified\nframework. Rather than focusing on single-instrument cases,\nwe are interested in sampling novel mixtures by condition-\ning a DiT on a set of per-instrument representations and we\nevaluate our model by source-level attribute manipulation.\nObject-Centric Representation Learning Learning ob-\nject representations entails encoding object entities in a vi-\nsual scene by unique representations (Greff, van Steenkiste,"}, {"title": "3 DisMix: The Proposed Framework", "content": "Given a mixture $X_m$ of $N_s$ instruments and a query $x_q^{(i)}$\n(detailed in Section 3.1), our goal is to extract latent rep-\nresentations of pitch $\\nu^{(i)}$ and timbre $\\tau^{(i)}$ for an instrument\n$i \\in [1, N_s]$, and to be able to sample a novel mixture condi-\ntioned on a set of manipulated pitch and timbre latents.\nWe propose DisMix to achieve the goal. Its generative\nprocess on the left side of Fig. 1 samples $x_m$, through a neu-\nral network $\\Theta_m$, conditioned on a set of source-level repre-\nsentations $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ where $s^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(i)})$ is a\ndeterministic function of timbre and pitch. The pitch latent\n$\\nu^{(i)}$ is computed via a neural network $\\Theta_\\nu$, by its correspond-\ning ground-truth pitch annotation $y^{(i)}$. The joint distribution\nover the variables can be written as follows:\n$P_{\\Theta_m}(x_m|S) \\prod_{i=1}^{N_s} P_{\\Theta_\\nu}(\\nu^{(i)}|y^{(i)})\\rho(\\tau^{(i)}),$   (1)\nwhere $\\rho(\\tau^{(i)}) = \\mathcal{N}(0, I)$. Note that $\\{\\nu^{(i)}\\}_{i=1}^{N_s}$ and $\\{\\tau^{(i)}\\}_{i=1}^{N_s}$ are sam-\npled independently and so are $\\{s^{(i)}\\}_{i=1}^{N_s}$, whereby the pitch\nand timbre latents can act as modular building blocks to\ncompose a mixture.\nWe propose two instances of DisMix: 1) an auto-encoder\nfor a simple case study using a simplistic dataset (Section 4)\nand 2) an LDM implemented using a DiT verified with a\nmore complex dataset (Section 5). We specify $\\Theta_m$, $\\Theta_s$, and\n$\\Theta_\\nu$ for the two instances in Sections 4 and 5, respectively.\nHere, we describe the common components between them.\nFollowing VAEs (Kingma and Welling 2014), we learn\nDisMix using an inference network to approximate posteri-\nors over the latent variables that are otherwise intractable,\nand learns the model by maximising a lower bound to the\nmarginal likelihood $p(x_m|\\{y^{(i)}\\}_{i=1}^{N_s})$. For the two instances\nof DisMix, we employ a common parameterisation of the in-\nference network illustrated on the right side of Fig. 1, which\nwe explain in the following sections."}, {"title": "3.1 Mixture and Query Encoders", "content": "Given a mixture of instruments, additional information\nwould be necessary to specify which instrument's latents are\nto be extracted. Motivated by Lin et al. (2021) and Lee, Choi,\nand Lee (2019), we use the query $x_q^{(i)}$ so that the extracted\nlatents of the source $i$ and the query share the same timbre\ncharacteristics, while they can carry arbitrary pitch informa-\ntion. We describe the objective function Eq. (7) that could\nencourage this behaviour in Section 3.3.\nDuring training, we pair each mixture $x_m$ with $X_q =$\n$\\{x_q^{(i)}\\}_{i=1}^{N_s}$, a set of $N_s$ queries with their instruments corre-\nsponding to the constituent instruments of the mixture, and\neach $x_q^{(i)}$ is randomly sampled from a subset of the complete\ntraining dataset consisting of a constituent instrument.\nAs illustrated on the right side of Fig. 1, the inference of\n$\\tau^{(i)}$ and $\\nu^{(i)}$ is conditioned on both $x_m$ and $x_q^{(i)}$. In practice,\nwe extract their compact counterparts $e_m = E_{\\Theta_m}(x_m)$ and\n$e_q^{(i)} = E_{\\Theta_q}(x_q^{(i)})$, respectively, where $E_{\\Theta_m}(\\cdot)$ and $E_{\\Theta_q}(\\cdot)$ are\nneural network encoders with parameters $\\Theta_m$ and $\\Theta_q."}, {"title": "3.2 Pitch and Timbre Encoders", "content": "As suggested by Fig. 1, we propose that the pitch and timbre\nencoders admit a common factorised form:\n$q_{\\Theta_U}(U|x_m, X_q) = \\prod_{i=1}^{N_s} q_{\\Theta_u}(u^{(i)}|e_m, e_q^{(i)}),$  (2)\nwhere $u \\in \\{\\nu, \\tau\\}$ and $U \\in \\{\\{\\nu^{(i)}\\}_{i=1}^{N_s},\\{\\tau^{(i)}\\}_{i=1}^{N_s}\\}$. $\\Theta_\\nu$\nand $\\Theta_\\tau$ are parameters of the two encoders. Given $e_m$ and $e_q^{(i)}$,\nthe pitch and timbre latents of the $i$-th source are encoded\nindependently of each other and of other sources.\nWe apply a binarisation layer (Dong and Yang 2018) to\nconstrain the capacity of the pitch latent, which is proven\ncrucial for disentanglement in our empirical studies. We also\nshow that imposing the standard Gaussian prior $\\rho(\\tau^{(i)})$ is a\nsimple yet effective way to constrain the timbre latent.\nConstraining Pitch Latents The pitch encoder combines\na transcriber $E_{\\Theta_\\phi}(\\cdot)$ that extracts pitch logits of the $i$-th\nsource $\\hat{y}^{(i)} = E_{\\Theta_\\phi}(e_m, e_q^{(i)})$, a stochastic binarisation layer\n(SB) that constrains the information capacity, and a transla-\ntor $f_{\\Theta_{\\nu}}(\\cdot)$ that computes the final outcome:\n$\\varphi_{\\Theta_\\nu}(\\nu^{(i)}|e_m, e_q^{(i)}) := \\delta(\\nu^{(i)} - f_{\\Theta_{\\nu}}(b^{(i)}))$  (3)\nwhere $\\delta(\\cdot)$ is the Dirac delta function, corresponding to the\ndiamond node of $\\nu^{(i)}$ on the right of Fig. 1, and:\n$y_{bin}^{(i)} = SB(\\hat{y}^{(i)}) = 1\\{\\text{Sigmoid}(\\hat{y}^{(i)})>h\\},$  (4)\nwhere $1\\{\\cdot\\}$ is the indicator function, and $h$ is the threshold\nsampled from the uniform distribution $U(0, 1)$ at each train-\ning step and is fixed at 0.5 during evaluation. The straight-\nthrough estimator (Bengio, L\u00e9onard, and Courville 2013) is\nused to bypass the non-differentiable operator.\nTable 1 suggests that the bottleneck imposed by the bina-\nrisation layer is crucial for disentanglement even if a pitch\nclassification loss is included. We also show that using only\nSB without the pitch supervision still yields a decent perfor-\nmance in Table 3."}, {"title": "Constraining Timbre Latents", "content": "Constraining Timbre Latents The timbre encoder pa-\nrameterises a Gaussian with a neural network $\\Theta_\\tau$:\n$q_{\\Theta_\\tau}(\\tau^{(i)}) = \\mathcal{N}(\\tau^{(i)}; \\mu_{\\Theta_\\tau}(e_m, e_q^{(i)}), \\sigma^2_{\\Theta_\\tau}(e_m, e_q^{(i)})I),$ (5)\nwhere $q_{\\Theta_\\tau}(\\tau^{(i)}) := q_{\\Theta_\\tau}(\\tau^{(i)}|e_m, e_q^{(i)})$ and sampling $\\tau^{(i)}$ is\nreparameterised as $\\mu_{\\Theta_\\tau}(\\cdot) + \\epsilon\\sigma_{\\Theta_\\tau}(\\cdot)$, where $\\epsilon \\sim \\mathcal{N}(0, I)$, to\nbe differentiable (Kingma and Welling 2014).\nWe let $\\rho(\\tau^{(i)}) = \\mathcal{N}(0, I)$ to constrain the timbre latent\nthrough the Kullback\u2013Leibler divergence (KLD) in Eq. (6).\n3.3 Training Objectives\nELBO We start with an evidence lower bound (ELBO) to\nthe marginal log-likelihood $\\log p(x_m|\\{y^{(i)}\\}_{i=1}^{N_s})$:\n$L_{ELBO} = \\mathbb{E}_{\\prod_{i=1}^{N_s} q_{\\Theta_{\\tau}}(\\tau^{(i)})} [\\log P_{\\Theta_m}(x_m|\\{\\nu^{(i)}, \\tau^{(i)}\\}_{i=1}^{N_s})]$\n$+ \\sum_{i=1}^{N_s} \\log p_{\\Theta_\\nu}(\\nu^{(i)}|y^{(i)}) - D_{KL}(q_{\\Theta_{\\tau}}(\\tau^{(i)})||p(\\tau^{(i)})),$  (6)\nwhere $\\nu^{(i)} = f_{\\Theta_{\\nu}}(y_{bin}^{(i)})$ by Eq. (3). Note that the condi-\ntionals of the first term can be expressed in terms of $S =$\n$\\{s^{(i)}\\}_{i=1}^{N_s}$, where $s^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(i)})$ as described previ-\nously for Eq. (1). $L_{ELBO}$ is derived as a result of the fac-\ntorised posteriors in Eq. (2) and the deterministic pitch en-\ncoder by Eq. (3), which we detail in Appendix A. We specify\n$\\Theta_m$, $\\Theta_s$, and $\\Theta_\\nu$ in Section 4 and 5.\nIntuitively speaking, we extract both a pitch and a timbre\nlatent for each source given a mixture and a query, and the\ncollection of which is used to reconstruct the mixture. The\npitch and timbre latents adhere to certain constraints and pri-\nors to encourage the disentanglement of the two attributes.\nApart from mixtures, we also observe individual sources\n$x_s^{(i)}$, so we include a source-wise reconstruction loss. This\nis a special case of the first term in Eq. (6) when $N_s = 1$\nand $x_m$ becomes $x_s$, and we can reuse $\\Theta_m$ to reconstruct\nindividual sources.\nPitch Supervision To enhance the disentanglement, we\nalso minimise a binary cross entropy loss $L_{BCE}(\\hat{y}^{(i)}, y^{(i)})$\nwhere $y^{(i)}$ is the pitch annotation of the $i$-th source. We ex-\nplore relaxing the model by excluding this term, with the\nresults reported in Table 3.\nBarlow Twins Finally, we minimise a simplified Barlow\nTwins loss (Zbontar et al. 2021) to enhance the correlation\nbetween the query and the timbre latent for them sharing the\ntimbre characteristics as described in Section 3.1:\n$L_{BT} = \\sum_{i=1}^{N_s} \\sum_{d=1}^{D_T} (1 - C_{dd}(e_q^{(i)}, \\tau^{(i)}))^2,$  (7)\nwhere $C$ is a cross-correlation matrix, and both $e_q^{(i)^d}$ and $\\tau^{(i)^d}$\nshare the same dimensionality $D_T$. Empirically, $L_{BT}$ coun-\nteracts the over-regularisation effect of the prior $\\rho(\\tau^{(i)})$ and\npromotes a discriminative timbre space as shown in Fig. 2.\nThe Final Objective In summary, we maximise:\n$L_{DisMix} = L_{ELBO} - L_{BCE} - L_{BT}.$  (8)\nWe do not find explicitly weighting each loss term necessary.\nNext, we detail the implementations specific to the sim-\nple and more sophisticated variants of DisMix in Sections 4\nand 5, respectively."}, {"title": "4 A Simple Case Study", "content": "Reconstructing Mixtures $P_{\\Theta_m}(x_m|S)$ in Eq. (6) is a\nGaussian likelihood parameterised by a decoder $\\Theta_m$, which\ncan be a permutation invariant function such as a trans-\nformer (Vaswani et al. 2017) without positional embeddings\nthat outputs the Gaussian mean, reconstructing a mixture\ngiven a set of source-level representations.\nWe opt for a simple implementation of $\\Theta_m$ here and dis-\ncuss a transformer in Section 5. In particular, we slightly\ndeviate from Eq. (6) and reconstruct $x_m$ using $e_m$ in Sec-\ntion 3.1, instead of $S$, whereby the likelihood becomes\n$P_{\\Theta_m}(x_m|e_m)$, and we maximise an additional term:\n$\\mathbb{E}_{\\prod_{i=1}^{N_s} q_{\\Theta_{\\tau}}(\\tau^{(i)})} [\\log p(e_m|\\{\\nu^{(i)}, \\tau^{(i)}\\}_{i=1}^{N_s}],$  (9)\nwhere $p(e_m|S) = \\mathcal{N}(e_m; \\sum_{i=1}^{N_s} s^{(i)}, \\sigma_m^2 I)$ and $\\sigma_m =$\n0.25 is a hyperparameter. Intuitively, we extract the source-level\nlatents $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ whose summation $s_{sum} = \\sum_{i=1}^{N_s} s^{(i)}$\nand $e_m$ are pulled together as measured by a mean square\nerror weighed by $\\sigma_m^2$.\nDuring evaluation, we instead use $s_{sum}$ to reconstruct the\nmixture $x_m$ or render a novel one by manipulating $\\{s^{(i)}\\}_{i=1}^{N_s}$\nbefore the summation. The assumption is that both $e_m$ and\n$s_{sum}$ can reconstruct $x_m$ comparably well by maximis-\ning Eq. (9) as we reconstruct $x_m$ by $e_m$ during training.\nThis approach avoids implementing a (potentially expen-\nsive and complicated) permutation invariant decoder and im-\nposes linearity between $e_m$ and $s_{sum}$ in the latent space. The\nlinearity could enable other applications, which we explain\nin Appendix B.1 and leave for future work.\nIntegrating Pitch and Timbre We employ FiLM (Perez\net al. 2018; Kim et al. 2018a) and derive the source-level\nrepresentation as $s^{(i)} = f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(i)}) = \\alpha_{\\Theta_s}(\\tau^{(i)}) \\nu^{(i)} +$\n$\\beta_{\\Theta_s}(\\tau^{(i)})$, where $\\nu^{(i)}$ is scaled and shifted element-wise by\nthe factors determined as a deterministic function $\\Theta_s$ of $\\tau^{(i)}$.\nConfiguring Pitch Priors We study pitch priors at dif-\nferent levels of capacity. The first follows Eq. (1) and de-\nfines $p_{\\Theta_{\\nu}}(\\nu^{(i)}|y^{(i)}) = \\mathcal{N}(\\nu^{(i)}; f_{\\Theta_{\\nu}}(y^{(i)}), \\sigma_\\nu^2 I)$, a Gaussian\nparameterised by $\\Theta_\\nu$, given the ground-truth pitch $y^{(i)}$. $\\sigma_\\nu$ is\na hyperparameter.\nWe also consider a richer prior to capture the source inter-\naction: $p_{\\Theta_{\\nu}}(\\nu^{(i)}|Y_{\\setminus i}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\nu^{(i)}; \\mu_{\\Theta_{\\nu, k}}(V_{\\setminus i}), \\sigma_\\nu^2 I)$,\nwhere $Y_{\\setminus i}$ denotes a set of pitch annotations excluding that\nof the $i$-th source, and $\\Theta_{\\nu, k}$ parameterises the mean of the\n$k$-th Gaussian in a Gaussian mixture. The rationale is that\n$\\hat{y}^{(i)}$ is conditionally dependent on pitch of other sources in a\nmixture to confer musical harmony."}, {"title": "4.1 Implementations", "content": "Dataset Gha et al. (2023) compile synthetic audio us-\ning 3,131 unique chords from JSB Chorales (Boulanger-\nLewandowski, Bengio, and Vincent 2012), rendered by\nsound fonts of piano, violin, and flute via FluidSynth.\nGiven a chord, each composite note is synthesised to an\naudio waveform at 16kHz with a sound font randomly sam-\npled with replacement, whereby a sound font $i$ can play\nmultiple notes in a chord. These notes together define a"}, {"title": "4.2 Results", "content": "Evaluation Given $x_m$ and a set of queries $\\{x_q^{(i)}\\}_{i=1}^{N_s}$ cor-\nresponding to the constituent instruments of the mixture,\nwe first extract $\\{\\tau^{(i)}, \\nu^{(i)}\\}_{i=1}^{N_s}$. To evaluate disentanglement,\nwe conduct a random permutation so that the pitch latent\nof the source $i$ can be swapped for that of the source $j$,\nwhile the timbre remains unchanged, which yields $\\hat{s}^{(i)} =$\n$f_{\\Theta_s}(\\tau^{(i)}, \\nu^{(j)})$. Then, we render a novel source $\\hat{x}_s^{(i)}$ by pass-\ning $\\hat{s}^{(i)}$ to the decoder $P_{\\Theta_m}$. Note that we render sources in-\nstead of mixtures by using $\\hat{s}^{(i)}$ as the input instead of the\nsummation of multiple source-level representations.\nA successful disentanglement entails that judges of pitch\nand instrument should classify $\\hat{x}_s^{(i)}$ as the pitch of $x_s^{(j)}$ (as\nit was swapped) and the instrument of $x_s^{(i)}$ (as it was pre-\nserved), respectively. We pre-train a pitch and an instrument\nclassifier using the training set and direct them as the judges.\nThe classification accuracy is reported under \u201cDisentangle-\nment\u201d in Table 1 and 2.\nTo see if the model can render novel mixtures, we first\nproduce a new set $\\{\\hat{s}^{(i)}\\}_{i=1}^{N_s}$ after the permutation and ren-\nder a novel mixture $\\hat{x}_m$ by passing $P_{\\Theta_m}$ the summation\n$\\hat{s}_{sum} = \\sum_{i} \\hat{s}^{(i)}$. To check whether the constituent attributes\nof $\\hat{x}_m$ are indeed dictated by the manipulated $\\{\\hat{s}^{(i)}\\}_{i=1}^{N_s}$, we\nonce again extract its source-level representations with the\noriginal queries and reconstruct the sources, which are then\nfed to the judges. \u201cMixture Rendering\u201d in Table 1 and 2 re-\nport the classification accuracy."}, {"title": "5 A Latent Diffusion Framework", "content": "We also implement DisMix by an LDM framework (Rom-\nbach et al. 2022), where the decoder $P_{\\Theta_m} (x_m|S)$ is a diffu-\nsion transformer (DiT) (Peebles and Xie 2023) that directly\nreconstructs a mixture given a set of source-level latents, and\nno additional objective such as Eq. (9) is introduced.\nData Representation The LDM framework (Rombach\net al. 2022) improves the compute efficiency of diffusion\nmodels (DMs) (Sohl-Dickstein et al. 2015; Ho, Jain, and\nAbbeel 2020) by first projecting data to a low-dimensional\nlatent space. We leverage the pre-trained VAE from Audi-\nOLDM2 (Liu et al. 2024) which is trained on multiple mu-\nsic and audio datasets to extract $z_s^{(i)} = E_{vae}(x_s^{(i)})$, where\n$E_{vae}(\\cdot)$ is the VAE encoder, and we use the pre-trained VAE\ndecoder $D_{vae}(\\cdot)$ to recover $x_s^{(i)}$.\nLatent Diffusion Models LDMs operate DMs in a latent\nspace and sample $z_0$, the latent representation of data, from\na Markov chain: $p(z_t) = \\prod_{t=1}^{T} p_\\theta(z_{t-1}|z_t)$, with $p(z_T) =$\n$\\mathcal{N}(z_T; 0, I)$ and $p_\\theta(z_{t-1}|z_t) = \\mathcal{N}(z_{t-1}; \\mu_\\theta(z_t, t), \\Sigma_\\theta(z_t, t))$\nparameterised by $\\theta$. The posterior is a linear Gaussian\n$q(z_t|z_{t-1}) = \\mathcal{N}(z_t; \\sqrt{\\alpha_t} z_{t-1}, (1 - \\alpha_t)I)$, where $\\alpha_t$ is a hy-\nperparameter evolving over the diffusing step $t$.\nGiven that the forward process $q$ is known and fixed, Ho,\nJain, and Abbeel (2020) employ specific forms of $\\mu_\\theta$ and $\\Sigma_\\theta$\nto match $q$ and simplify the training to essentially minimis-\ning $||\nf_\\theta(z_t, t) - z_0||^2$, which boils down to training a decoder\n$f_\\theta$ to predict the clean $z_0$ given its corrupted counterpart $z_t$."}, {"title": "5.1 Adapting Diffusion Transformers", "content": "Different from the vanilla LDMs", "with": "n$P_{\\Theta_m"}, "z_{m, 0:T}|S) = p(z_{m, T}) \\prod_{t=1}^{T} p_{\\Theta_m}(z_{m, t-1}|z_{m, t}, S).$  (10)\n$z_{m,t}$ denotes the noised latent feature of mixtures at dif-\nfusing step $t$ and is defined in terms of $\\{z_s^{(i)}\\}_{i=1}^{N_s}$, where\n$z_{s,0}^{(i)} = z_s^{(i)}$. $P_{\\Theta_m}$ models the interaction among the elements\nin the set $S = \\{s^{(i)}\\}_{i=1}^{N_s}$ and iteratively performs the reverse\nprocess. Defining $z_{m,t}$ in terms of the constituent sources\nfacilitates the conditioning mechanism that we explain next.\nPartition DiTs (Peebles and Xie 2023) operate on a se-\nquence of image patches to work with a transformer. Simi-\nlarly, we first apply a sinusoidal positional encoding to $z_s^{(i)}$\nto preserve the temporal order and partition it by $\\text{Par}(z_s^{(i)})$ :\n$\\mathbb{R}^{T_z \\times (D_z \\times C)} \\rightarrow \\mathbb{R}^{L \\times D'}$, where $T_z = 100, D_z = 16$,\n$C = 8$, and $L = 25$ are the numbers of time frames, feature\ndimensions, channels, and patches, respectively. That is, we\npartition along the time axis and flatten each patch whose\nsize becomes $D' = \\frac{T_z}{L} \\times D_z \\times C$. We repeat the process for\nall $N_s$ elements in $\\{z_s^{(i)}\\}_{i=1}^{N_s}$, the outcome of which finally\ndefines $z_{m,t} \\in \\mathbb{R}^{(N_s \\times L) \\times D'}$ in Eq. (10). In other words, we\ncan consider it as a sequence of $N_s \\times L$ patches, with the\nsize of each patch being $D'$.\nSimilarly, we partition the source-level representations\n$S = \\{s^{(i)}\\}_{i=1}^{N_s} and obtain $s_c \\in \\mathbb{R}^{(N_s \\times L) \\times D'}$, where $D' =$\n$\\frac{T_z}{L} \\times D_s \\times C$, which we detail in Section 5.2. We can then\nrepresent the set condition $S$ by $s_c$ in Eq. (10).\nAs mentioned earlier, we define $z_{m,t}$ in terms of its con-\nstituent sources and conveniently align the dimensions of $s_c$\nand $z_{m,t}$ (except for their sizes $D$ and $D'$) which facilitates\nthe conditioning described next.\nWe do not add another positional encoding to $z_{m,t}$ and $s_c$,\nensuring the permutation invariance w.r.t. the $N_s$ sources.\nConditioning A transformer block consists of the multi-\nhead self-attention mechanism and a feedforward network.\nEach of these modules is followed by a skip connection and\na layer normalisation (LN) (Vaswani et al. 2017). We replace\nthe standard LN with its adaptive variant (adaLN) for the\nconditioning (Peebles and Xie 2023).\nIn particular, $s_c$ is added with a diffusing step embedding\nand is used to regress the scaling and shifting factors $\\rho$ and $\\gamma$\nof adaLN layers which match the dimension of $z_{m,t}$. Then,\nby passing $z_{m,t}$ through the transformer blocks, it is"]}