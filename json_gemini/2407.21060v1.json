{"title": "Using Large Language Models for the Interpretation of Building Regulations", "authors": ["Stefan Fuchs", "Michael Witbrock", "Johannes Dimyadi", "Robert Amor"], "abstract": "Compliance checking is an essential part of a construction project. The recent rapid uptake of building information models (BIM) in the construction industry has created more opportunities for automated compliance checking (ACC). BIM enables sharing of digital building design data that can be used for compliance checking with legal requirements, which are conventionally conveyed in natural language and not intended for machine processing. Creating a computable representation of legal requirements suitable for ACC is complex, costly, and time-consuming. Large language models (LLMs) such as the generative pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT, can generate logically coherent text and source code responding to user prompts. This capability could be used to automate the conversion of building regulations into a semantic and computable representation. This paper evaluates the performance of LLMs in translating building regulations into LegalRuleML in a few-shot learning setup. By providing GPT-3.5 with only a few example translations, it can learn the basic structure of the format. Using a system prompt, we further specify the LegalRuleML representation and explore the existence of expert domain knowledge in the model. Such domain knowledge might be ingrained in GPT-3.5 through the broad pre-training but needs to be brought forth by careful contextualisation. Finally, we investigate whether strategies such as chain-of-thought reasoning and self-consistency could apply to this use case. As LLMs become more sophisticated, the increased common sense, logical coherence and means to domain adaptation can significantly support ACC, leading to more efficient and effective checking processes.", "sections": [{"title": "1. Introduction", "content": "The increasing adoption of building information models (BIM) in construction projects creates more opportunities for automation to boost productivity, minimise duplications of effort, and ultimately improve the quality of buildings and provide a safer and more comfortable living environment. Automation in regulatory compliance checking has been an active research field in the industry for half a century (Dimyadi and Amor, 2013). BIM enables project stakeholders to share digital building data, which is the subject of the automated compliance checking (ACC) process. Legal requirements, the standard to which BIM data must be checked against, are conventionally conveyed in natural language, not intended for machine processing. Having a digital representation of legal requirements is essential for the ACC process. Manually converting regulatory clauses has been considered in many cases (Fenves, 1966; Beach and Rezgui, 2018; Dimyadi et al., 2020), but expertise in multiple domains, such as knowledge engineering, law and construction, is required to achieve high-quality translations. Considering the high number of relevant normative documents for compliance checking (e.g., there are over 600 building-related standards in New Zealand), this process is very time-consuming and costly.\nConsequently, natural language processing is a promising direction to automate this task. However, interpreting natural language regulations and converting them into a computer-processable representation poses significant challenges. Complex legalese, ambiguities, and other linguistic intricacies make this task inherently difficult. A common strategy to reduce the complexity is to extract semantic information elements first (Zhang and El-Gohary, 2016; Xu and Cai, 2019) and transform these elements into the final representation in a second step (Zhang and El-Gohary, 2015). Early approaches used hand-crafted rules based on linguistic features and ontologies (Zhou and El-Gohary, 2017). While such rule-based approaches can achieve high accuracy for narrow domains (Zhang and El-Gohary, 2019), they lack scalability and rely on the quality and availability of the ontologies. Machine learning (ML) can be used to circumvent the problems of scalability and manual efforts for crafting rules. Li et al. (2020) and Wang and El-Gohary (2022) extracted entity-relation triplets, and Y. Zhou et al. (2022) conducted semantic labelling using deep neural networks. Nevertheless, human effort is still required to prepare training data for each ML task.\nTo reduce this effort, Fuchs et al. (2022) used T5 (Raffel et al., 2020), a pre-trained transformer model (Vaswani et al., 2017), to translate building regulations into a formal representation. Transformer is a neural network architecture proposed by Vaswani et al. (2017), which relies heavily on self-attention to generate representations of text. The original transformer"}, {"title": "2. Methodology", "content": "Large language models (LLMs) are highly receptive to the input prompt. Based on the information provided in the context, the model can learn to perform a wide range of tasks. The model outputs can vary significantly depending on how well one formulates the task, what context is provided, and what and how many exemplars are given. In this light, LLMs are limited by their context length. For example, GPT-3.5 could handle 4,096 tokens at the time the experiments were conducted and GPT-4 (OpenAI, 2023) 8,192 or 32,768 tokens depending on the model version. A token refers to sub-words, and the 4,096 tokens correspond to approximately 3,000 words. The greater length of the LRML rules in our dataset compared to many other semantic parsing datasets limits the number of exemplars to around 30.\nWe evaluate a range of prompting strategies using GPT-3.5. The following strategies are investigated: 1) Sampling, 2) Contextualisation, 3) Chain-of-thought reasoning, 4) Self-consistency, and 5) GPT as a teacher model."}, {"title": "2.1. LegalRuleML Dataset", "content": "LegalRuleML (LRML) is an XML-based (i.e., Extensible Markup Language) formal representation of legal requirements, which was evaluated for representing building regulations by Dimyadi et al. (2017). The representation helps to differentiate between common phenomena in legal text, such as term definitions (i.e., \"ConstitutiveStatement\") and normative requirements (i.e., \u201cPrescriptiveStatement\"). Statements are modelled as conditions (i.e., \"if\") and conclusions (i.e., \"then\"). For prescriptive statements, deontic operators (e.g., \u201cobligation\u201d, \u201cpermission\u201d, \u201cprohibition\u201d) are used within the conclusion. The actual domain logic is modelled as conjunctions and disjunctions of boolean statements. Further aspects, such as defeasibility and linkages to the source text and ontologies, are well supported.\nThis study utilises the corpus of building regulations with their representation in LRML from Fuchs et al. (2022). This dataset was derived from clauses of the New Zealand Building Code translated into LRML by Dimyadi et al. (2020). To make the dataset usable for semantic parsing, the LRML representation was compacted by replacing the XML tags with brackets and removing recoverable information. It was then aligned with the relevant parts of the regulatory clauses. In this study, we utilise the newest version of this dataset by Fuchs et al. (2023b), including the reversible intermediate representation (IR) seen in Listing 1 instead of the original, more verbose representation as the prediction target. Using this IR simplifies the semantic parsing task while a loss-less transformation into the original representation is ensured."}, {"title": "2.2. In-context Learning", "content": "Large language models (LLMs) are highly receptive to the input prompt. Based on the information provided in the context, the model can learn to perform a wide range of tasks. The model outputs can vary significantly depending on how well one formulates the task, what context is provided, and what and how many exemplars are given. In this light, LLMs are limited by their context length. For example, GPT-3.5 could handle 4,096 tokens at the time the experiments were conducted and GPT-4 (OpenAI, 2023) 8,192 or 32,768 tokens depending on the model version. A token refers to sub-words, and the 4,096 tokens correspond to approximately 3,000 words. The greater length of the LRML rules in our dataset compared to many other semantic parsing datasets limits the number of exemplars to around 30.\nWe evaluate a range of prompting strategies using GPT-3.5. The following strategies are investigated: 1) Sampling, 2) Contextualisation, 3) Chain-of-thought reasoning, 4) Self-consistency, and 5) GPT as a teacher model."}, {"title": "2.2.1. Sampling", "content": "First, we test different sampling strategies - Random, hand-picked, diversity, and representative sampling. Samples are provided in a \u201cSource/Target\u201d format, as shown in Listing 2. For improved tokenisation, spaces are inserted, and the camel case notation is reverted, as in Fuchs et al. (2023a). The last \"Source\" represents the new clause to translate, and \"Target\" indicates to the model to produce the corresponding LRML rule."}, {"title": "2.2.2. Contextualisation", "content": "To support the model further with the translation, we provide a description of the translation task, with the relevant syntax, special cases, and the most common relations and entities. While such information could be inferred from the exemplars, we evaluate if a task description leads to superior understanding. In particular, it is of interest whether such a description can reduce the number of samples required for the task. This is examined by comparing the few-shot learning experiment with hand-picked exemplars in Section 2.2.1 with similar runs having additional context."}, {"title": "2.2.3. Chain-of-thought prompting", "content": "A generally successful method to increase performance and elicit reasoning is chain-of-thought prompting. This method provides a range of exemplars that show step-by-step descriptions of how one might get to the solution (Wei et al., 2022). Similar behaviour was observed by simply adding \"Let's think step by step:\" at the end of the prompt (Zhang et al., 2022). An example of chain-of-thought prompting for LRML is shown in Listing 3."}, {"title": "2.2.4. Self-consistency", "content": "Self-consistency (Huang et al., 2022) and self-reflection (Shinn et al., 2023) are common strategies for improving neural networks for natural language processing tasks. Self-consistency dates back to ensemble methods (Dietterich, 2000), where multiple outputs from differently trained models are sampled, and the best prediction is selected per majority vote. In contrast, self-reflection points to the model's ability to evaluate its outputs. For example, Chen et al. (2023) let the LLM describe generated source code line by line and judge the generated source code. Then, they feed this information back into the model for self-debugging purposes.\nSince majority votes are more suited for classification tasks with a limited output space, we propose a strategy similar to Drozdov et al. (2022). We generate multiple predictions with different samples and sampling strategies. In the second step, we feed both the source text and the predictions into the LLM and let it pick the best translation. To introduce the task to the model, we provide in-context exemplars where the source text is followed by different predictions and the prediction with the best F1-Score (i.e., Option 2 in Listing 5) is the target. Alternatively, we test the model's capability to interpolate between the predictions using the ground truth as the target of the exemplars. One example of the second strategy can be seen in Listing 5. For the first strategy, the target would be replaced by the best scoring option (i.e., Option 2). An important question for self-consistency is how the predictions should be generated and whether predictions with high scores or greater diversity are more beneficial. We will examine this aspect by using the best three predictions of the previous experiments on the one hand and a more diverse set of predictions with different sampling or prompting strategies on the other hand."}, {"title": "2.2.5. GPT as a teacher model", "content": "Being able to translate regulations into formal representations with only a few exemplars offers the opportunity to start a semi-automated translation process with little time, cost, and effort. Inspired by Wang et al. (2022), we test the use of LLM"}, {"title": "3. Results", "content": "Unless explicitly stated otherwise, all experiments are conducted using GPT-3.5-turbo, the model powering the standard version of ChatGPT. We send requests to this model via the OpenAI API4. We changed the decoding temperature to 0 to get the most likely output given the prompt, which leads to more factual but less creative outcomes. All prompts were passed in with the user role, which performed better than the system role. Additionally, we ensure that the LRML rule can be extracted from the response (i.e., the response contains \"if(\"). If not, we regenerate the translation with increasing temperatures until we receive a suitable response."}, {"title": "3.1. Sampling", "content": "A natural and straightforward way of picking exemplars is randomly selecting samples from the training set. We randomly sample 50 exemplars and reduce the number until they fit into the maximum token length. By repeating this process multiple times, we can gain insight into the variance of different sets of exemplars and the number of exemplars while keeping the number of input tokens approximately the same. While the results in Fig. 1 show that the F1-Scores range between 53.5% and 60.6%, the majority of runs were above 57.1%. A weak positive correlation (i.e., 0.275 Pearson's Correlation) between F1-Scores and the number of exemplars can be noticed, but this result is not significant (i.e., 0.441 P-Value), and more data would be required to draw any conclusions."}, {"title": "3.2. Contextualisation", "content": "Due to the limited context length, there is a trade-off between the additional context information and the resulting decrease in the number of exemplars. We evaluated this trade-off with the manually filtered samples in Section 3.1. Including a task introduction reduced the exemplar number from 30 to 28. The number of exemplars was further reduced to 27 by adding the format specification, to 26 by adding the explanation for references, and to 24 by listing the most common terms. This setup can be contrasted with having only ten in-context exemplars, and contextualisation is additional information. Lastly, we combine contextualisation with representative sampling per clause in reverse order. Since the most representative exemplars will be selected first, reducing the number of exemplars might have less effect. As shown in Fig. 3, adding a short task introduction improved all scores continuously over all setups. The specification and reference examples were counterproductive in all setups. This could be rooted in a poor wording, which confuses the model more than it helps. Providing the most common terms and emphasising the important aspects led to higher F1-Scores for predictions with fewer exemplars. Notably, the BLEU scores increased for all setups, which might be related to the low baseline compared to Table 1. Finally, contextualisation with representative sampling per clause had similar improvements of 0.8% F1-Scores with only the introduction and with the full context but higher BLEU scores with the full context. Despite overall mixed results, we believe LLM should be contextualised succinctly to maximise their potential."}, {"title": "3.3. Chain-of-thought Prompting", "content": "While chain-of-thought (CoT) prompting was a successful strategy to elicit reasoning and improve accuracy in recent works, including semantic parsing, Table 2 suggests this is not the case for generating the long and complex LRML rules. This was confirmed in multiple setups, where the ten manually filtered exemplars in Section 3.1 were reformulated as CoT exemplars in a colloquial language and an alignment-based style. Additionally, we experimented with providing the additional exemplars without CoT before the CoT exemplars to provide overall more exemplars but still enforce CoT reasoning. Finally, we provided additional context together with the CoT exemplars. While these strategies brought slight improvements, the results were consistently worse than when predicting LRML directly. Different methods to formulate the CoTs and recursive prompting strategies should be tested to confirm these results."}, {"title": "3.4. Self-consistency", "content": "In the proposed self-consistency strategy, the model is required to reflect on previous outputs and choose the best one. While self-reflection was argued to be an emergent ability related to the size of LLM, in our experimental setup, it could also be ascribed to the intuition that discrimination between different predictions is a simpler task than generating a new LRML rule. The results presented in Table 3 indicate such a behaviour only in one of the setups. Using the three best runs with representative sampling per clause with and without contextualisation (71.1%, 71.1%, and 70.3% F1-Scores) and teaching the model to pick one of the LRML rules yielded an improvement of 0.9% F1-Score outperforming previous supervised approaches using intermediate representations and self-reflection (Fuchs et al., 2023b). Nevertheless, comparing these results to the Oracle F1-Score of 75.4% indicates that the model makes more often than not non-optimal choices. All other setups have negative consequences indicating that high variability in the quality of the LRML options is not helpful. We picked the best runs with representative sampling per clause (71.1% F1-Score), random sampling (60.6% F1-Score), and clustering (59.7% F1-Score) for the mixed setup, but the results were closer to the average instead of bringing improvements. Also, letting the model interpolate between the LRML options yielded worse results than choosing one option."}, {"title": "3.5. GPT as a Teacher Model", "content": "By using GPT as a teacher model, we determine if LLMs can be used to commence training a semantic parser for building regulations with only little effort. In the first setup, we selected the best run from the previous experiments with a fixed set of exemplars, i.e., random sampling with 60.6% F1-Score. By using these exemplars, we generate LRML rules for all remaining clauses in the training set (i.e., 543 LRML rules). We trained the more lightweight and freely available T5 model"}, {"title": "4. Discussion", "content": "The results in this paper show the suitability of LLM to support the translation of building regulations into formal representations insofar as higher scores could be reached compared to traditional supervised models. Additionally, such supervised models could be augmented with additional training data. Nevertheless, since the BLEU and F1-Scores only act as an approximation to how close the translation is to the given ground truth translation, we examine some of the predictions and propose directions for future work.\nWe recognise that slight changes to the rules can cause significant penalties in F1-Scores. Furthermore, some variations in the predictions, such as different granularities, different terminologies, or alternative expressions, might not impact the actual use cases of the formal representation. For example, the prediction in Listing 6 uses \u201chas\u201d instead of \u201cinclude\", and misses the \"and\" keyword to indicate the conjunction. While the \u201cand\u201d would reduce the F1-Scores by about 1.2%, the \"has\" has a stronger impact of 9.7% since it also influences the scoring of the logical and deontic keywords, \u201cthen\" and \u201cpermission\".\""}, {"title": "5. Conclusions and Future Work", "content": "This work proposed the use of large language models (LLMs) to automate the translation of building regulations into a formal representation usable for automated compliance checking (ACC). The pre-training of LLMs familiarises them with a large variety of tasks and topics so that only a few exemplars are enough to generate translations with reasonable F1-Scores. The sample efficiency opens up new means to provide support to translators and shows potential for full automation even if no initial training data is available. Furthermore, the sample efficiency gives more freedom in selecting the representation format without dependence on data availability. LLMs can outperform supervised methods by using sophisticated methods to select the most appropriate exemplars for each clause to translate. In addition, small models with supervised learning could be improved further with the help of synthetic training data and limited human supervision, which might be the more feasible solution to apply these techniques in practice.\nIn recent years, the exploration of prompting techniques for LLMs has gained considerable momentum. Accordingly, a plethora of prompting strategies evolved, which could not all be exhaustively examined in the scope of this study. Notably, advanced techniques like recursive prompting and strategies that involve querying knowledge bases have the potential to be particularly beneficial within the context of the evaluated use case. Furthermore, the examination of various LRML rules reveals that numerous translations exhibit not only logical coherence but also surpass the quality indicated by the F1-Scores. Still, a disparity persists in terms of translation quality suitable for ACC application. Even when nearing flawless translation, the potential requirement for certification and review remains, emphasising the need for a human-assisted workflow.\nBuilding on these conclusions and limitations, our future work takes two paths. Firstly, we will explore the feasibility of involving regulators in translating regulations into formal representations during regulatory drafting and publishing. We will also investigate the types of tool support that could enhance the feasibility of this process. Secondly, we aim to advance the implementation of the resulting rules in real-world situations. This not only enhances the practical usability of these rules but also contributes to refining the translation process. By placing more emphasis on logical correctness in evaluations, we aim to achieve a more dependable assessment and thus enhance the overall quality of the formal representation of building regulations."}, {"title": "Author Contributions", "content": "Stefan Fuchs contributed to conceptualisation, methodology, implementation, validation, analysis, draft preparation, visualisation, and manuscript editing. Michael Witbrock contributed to conceptualisation, computational resources, manuscript editing, and supervision. Johannes Dimyadi contributed to manuscript editing and supervision. Robert Amor contributed to conceptualisation, methodology, manuscript editing, and supervision. All authors have read and agreed with the manuscript before its submission and publication."}, {"title": "Funding", "content": "This research was funded by the University of Canterbury's Quake Centre's Building Innovation Partnership (BIP) programme, which is jointly funded by industry and the Ministry of Business, Innovation and Employment (MBIE)."}, {"title": "Ethics Statement", "content": "Applying machine learning to solve legal tasks can have broad implications since the correctness of the algorithm cannot be guaranteed. In the building domain, this could cause major adverse events that put building occupants at risk if an unsafe building is issued a compliance certificate and opened to the public. Nevertheless, this work treats LLMs as a supporting tool, which helps to automate compliance checking rather than executing the compliance checks directly. The entire means of having a formal representation of building codes, which is verifiable and can be deterministically executed, is to guarantee interpretable and reliable automated compliance checking."}]}