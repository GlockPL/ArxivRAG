{"title": "Using Large Language Models for the Interpretation of Building Regulations", "authors": ["Stefan Fuchs", "Michael Witbrock", "Johannes Dimyadi", "Robert Amor"], "abstract": "Compliance checking is an essential part of a construction project. The recent rapid uptake of building information models (BIM) in the construction industry has created more opportunities for automated compliance checking (ACC). BIM enables sharing of digital building design data that can be used for compliance checking with legal requirements, which are conventionally conveyed in natural language and not intended for machine processing. Creating a computable representation of legal requirements suitable for ACC is complex, costly, and time-consuming. Large language models (LLMs) such as the generative pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT, can generate logically coherent text and source code responding to user prompts. This capability could be used to automate the conversion of building regulations into a semantic and computable representation. This paper evaluates the performance of LLMs in translating building regulations into LegalRuleML in a few-shot learning setup. By providing GPT-3.5 with only a few example translations, it can learn the basic structure of the format. Using a system prompt, we further specify the LegalRuleML representation and explore the existence of expert domain knowledge in the model. Such domain knowledge might be ingrained in GPT-3.5 through the broad pre-training but needs to be brought forth by careful contextualisation. Finally, we investigate whether strategies such as chain-of-thought reasoning and self-consistency could apply to this use case. As LLMs become more sophisticated, the increased common sense, logical coherence and means to domain adaptation can significantly support ACC, leading to more efficient and effective checking processes.", "sections": [{"title": "1. Introduction", "content": "The increasing adoption of building information models (BIM) in construction projects creates more opportunities for automation to boost productivity, minimise duplications of effort, and ultimately improve the quality of buildings and provide a safer and more comfortable living environment. Automation in regulatory compliance checking has been an active research field in the industry for half a century (Dimyadi and Amor, 2013). BIM enables project stakeholders to share digital building data, which is the subject of the automated compliance checking (ACC) process. Legal requirements, the standard to which BIM data must be checked against, are conventionally conveyed in natural language, not intended for machine processing. Having a digital representation of legal requirements is essential for the ACC process. Manually converting regulatory clauses has been considered in many cases (Fenves, 1966; Beach and Rezgui, 2018; Dimyadi et al., 2020), but expertise in multiple domains, such as knowledge engineering, law and construction, is required to achieve high-quality translations. Considering the high number of relevant normative documents for compliance checking (e.g., there are over 600 building-related standards in New Zealand), this process is very time-consuming and costly.\nConsequently, natural language processing is a promising direction to automate this task. However, interpreting natural language regulations and converting them into a computer-processable representation poses significant challenges. Complex legalese, ambiguities, and other linguistic intricacies make this task inherently difficult. A common strategy to reduce the complexity is to extract semantic information elements first (Zhang and El-Gohary, 2016; Xu and Cai, 2019) and transform these elements into the final representation in a second step (Zhang and El-Gohary, 2015). Early approaches used hand-crafted rules based on linguistic features and ontologies (Zhou and El-Gohary, 2017). While such rule-based approaches can achieve high accuracy for narrow domains (Zhang and El-Gohary, 2019), they lack scalability and rely on the quality and availability of the ontologies. Machine learning (ML) can be used to circumvent the problems of scalability and manual efforts for crafting rules. Li et al. (2020) and Wang and El-Gohary (2022) extracted entity-relation triplets, and Y. Zhou et al. (2022) conducted semantic labelling using deep neural networks. Nevertheless, human effort is still required to prepare training data for each ML task.\nTo reduce this effort, Fuchs et al. (2022) used T5 (Raffel et al., 2020), a pre-trained transformer model (Vaswani et al., 2017), to translate building regulations into a formal representation. Transformer is a neural network architecture proposed by Vaswani et al. (2017), which relies heavily on self-attention to generate representations of text. The original transformer architecture is especially suited for translation and semantic parsing tasks since it consists of an encoder to process the input text and a decoder to generate the output while paying attention to specific parts of the input. Through unsupervised pre-training, the model requires less training data and has a better understanding of natural language and common sense. T5 includes further tasks such as translation and summarisation into the pre-training by specifying task prefixes in natural language. By treating the conversion of the regulations into LRML as an end-to-end semantic parsing task, the existing formal representation of New Zealand's building regulations (Dimyadi et al., 2020) could be used for the training rather than having to annotate entities for information extraction. Furthermore, newly translated regulations can be utilised to refine the model and improve the translation quality over time. Nevertheless, this process is strongly dependent on the quality and quantity of the training data (Fuchs et al., 2023a), and it can be difficult to get started with the translation process, especially if there are no earlier efforts.\nLarge Language Models (LLMs), including generative pre-trained transformers (GPT) (Radford et al., 2018), such as GPT-3.5 by OpenAI, which powers the standard version of ChatGPT\u00b9, Bard\u00b2 by Google, and open-source initiatives like Large Language Model Meta AI (LLaMA) (Touvron et al., 2023) and Alpaca (Taori et al., 2023) have exhibited impressive capabilities in a wide range of tasks without explicit training. These capabilities can be related to the model size and the amount of data used for the pre-training. During inference, tasks are presented to the model using an input prompt, possibly accompanied by a limited set of exemplars. These processes are called in-context and few-shot learning. Using such approaches offers the advantage of initiating the translation of building regulations with as few as five exemplars.\nIn this study, we aim to evaluate the performance of LLMs, specifically GPT-3.5, in generating a formal representation of building regulations. Results by D. Zhou et al. (2022) and Drozdov et al. (2022) for related semantic parsing tasks indicate that even a small number of exemplars is sufficient to generate the desired syntactical structure. Nevertheless, challenges arise from the extensive vocabulary, complex expressions, and the need to adhere to specific encoding guidelines.\nTo address these challenges, we have investigated a robust sampling strategy inspired by the work of Drozdov et al. (2022). This strategy helps us identify samples that closely resemble the clause to be translated, resulting in enhanced performance compared to the supervised T5 model. Additionally, we have explored the potential benefits of leveraging the self-consistency technique proposed by Huang et al. (2022) to improve our approach further. Moreover, we have investigated the feasibility of utilising GPT-3.5 as a teacher model to enhance the performance of less resource-intensive models trained under supervision using GPT-generated data. This investigation aims to leverage the knowledge and capabilities of GPT-3.5 to improve the translation performance of these models, despite their limited resources.\nOverall, this research contributes to the advancement of automated compliance checking regarding the automated interpretation of building regulations. We harness the power of LLMs to translate natural language legal requirements into formal representations and provide valuable insights into the performance and challenges associated with this approach, paving the way for future improvements and enhanced compliance automation."}, {"title": "2. Methodology", "content": "LegalRuleML (LRML) is an XML-based (i.e., Extensible Markup Language) formal representation of legal requirements, which was evaluated for representing building regulations by Dimyadi et al. (2017). The representation helps to differentiate between common phenomena in legal text, such as term definitions (i.e., \"ConstitutiveStatement\") and normative requirements (i.e., \u201cPrescriptiveStatement", "if\") and conclusions (i.e., \"then": ".", "obligation": "permission", "prohibition": "are used within the conclusion. The actual domain logic is modelled as conjunctions and disjunctions of boolean statements. Further aspects, such as defeasibility and linkages to the source text and ontologies, are well supported.\nThis study utilises the corpus of building regulations with their representation in LRML from Fuchs et al. (2022). This dataset was derived from clauses of the New Zealand Building Code translated into LRML by Dimyadi et al. (2020). \u03a4\u03bf make the dataset usable for semantic parsing, the LRML representation was compacted by replacing the XML tags with brackets and removing recoverable information. It was then aligned with the relevant parts of the regulatory clauses. In this study, we utilise the newest version of this dataset by Fuchs et al. (2023b), including the reversible intermediate representation (IR) seen in Listing 1 instead of the original, more verbose representation as the prediction target. Using this IR simplifies the semantic parsing task while a loss-less transformation into the original representation is ensured."}, {"title": "2.2. In-context Learning", "content": "Large language models (LLMs) are highly receptive to the input prompt. Based on the information provided in the context, the model can learn to perform a wide range of tasks. The model outputs can vary significantly depending on how well one formulates the task, what context is provided, and what and how many exemplars are given. In this light, LLMs are limited by their context length. For example, GPT-3.5 could handle 4,096 tokens at the time the experiments were conducted and GPT-4 (OpenAI, 2023) 8,192 or 32,768 tokens depending on the model version. A token refers to sub-words, and the 4,096 tokens correspond to approximately 3,000 words. The greater length of the LRML rules in our dataset compared to many other semantic parsing datasets limits the number of exemplars to around 30.\nWe evaluate a range of prompting strategies using GPT-3.5. The following strategies are investigated: 1) Sampling, 2) Contextualisation, 3) Chain-of-thought reasoning, 4) Self-consistency, and 5) GPT as a teacher model."}, {"title": "2.2.1. Sampling", "content": "First, we test different sampling strategies - Random, hand-picked, diversity, and representative sampling. Samples are provided in a \u201cSource/Target\u201d format, as shown in Listing 2. For improved tokenisation, spaces are inserted, and the camel case notation is reverted, as in Fuchs et al. (2023a). The last \"Source\" represents the new clause to translate, and \"Target\" indicates to the model to produce the corresponding LRML rule.\nFor the random sampling strategy, we select different sets of random samples within the maximum allowable context length. This context length is identified by appending the longest validation clause with its LRML translation to the exemplars and determining the number of tokens required. Comparing different exemplars, which are summing up to the maximum context length, can not only give indications about the variations between different selections but also about the suitability of longer or shorter exemplars.\nWe manually selected a range of exemplars to test the human judgement of good and representative exemplars. We reviewed the samples of the training set in two iterations. First, we selected subjectively good and representative samples with little overlap. Second, we picked a selection out of those samples that cover a broad range of topics, different clause lengths and LRML constructs. We randomly select different numbers of samples to evaluate different scales of few-shot learning with values of: 1, 3, 5, 10, 20, and 30.\nCommon strategies for retrieving valuable samples can be found in active learning research (Monarch, 2021). We test two strategies: diversity sampling and representative sampling. For diversity sampling, the training set is clustered based on semantic similarity between either the clauses or the LRML rules using the Sentence Bidirectional Encoder Representations from Transformers (S-BERT) library\u00b3. The exemplars are then sampled from all the clusters. Alternatively, we use stratified sampling with the different Acceptable Solutions documents as populations. To retrieve representative samples, the most representative sample for each validation clause was calculated using semantic similarity, and the maximum number of those clauses was used as exemplars. Instead of using the same fixed set of exemplars for all predictions, we also retrieve a different set of exemplars for each clause in the validation set. These exemplars are retrieved either with semantic similarity or similar to Drozdov et al. (2022). For the second case, we sort all clauses based on n-gram similarity with the clause in the validation set. We pick the clause with the highest n-gram overlap and continue with the following clause with different n-grams overlapping. When no new n-grams overlap, we start the process from the beginning with a different $n \\in \\{1, 2, 3\\}$. This process ensures that we have exemplars covering all aspects of the validation set, as far as such clauses exist in the training set."}, {"title": "2.2.2. Contextualisation", "content": "To support the model further with the translation, we provide a description of the translation task, with the relevant syntax, special cases, and the most common relations and entities. While such information could be inferred from the exemplars, we evaluate if a task description leads to superior understanding. In particular, it is of interest whether such a description can reduce the number of samples required for the task. This is examined by comparing the few-shot learning experiment with hand-picked exemplars in Section 2.2.1 with similar runs having additional context."}, {"title": "2.2.3. Chain-of-thought prompting", "content": "A generally successful method to increase performance and elicit reasoning is chain-of-thought prompting. This method provides a range of exemplars that show step-by-step descriptions of how one might get to the solution (Wei et al., 2022). Similar behaviour was observed by simply adding \"Let's think step by step:\" at the end of the prompt (Zhang et al., 2022).\nSince the exemplars are lengthy, only a limited number can be provided and especially for long clauses, we need to ensure that the reasoning steps fit into the context length. Accordingly, a shorter decomposition, which consists only of the alignment between natural language phrases and LRML concepts or expressions, is tested. Additionally, we evaluate a mixture of chain-of-thought and normal exemplars for the superior strategy."}, {"title": "2.2.4. Self-consistency", "content": "Self-consistency (Huang et al., 2022) and self-reflection (Shinn et al., 2023) are common strategies for improving neural networks for natural language processing tasks. Self-consistency dates back to ensemble methods (Dietterich, 2000), where multiple outputs from differently trained models are sampled, and the best prediction is selected per majority vote. In contrast, self-reflection points to the model's ability to evaluate its outputs. For example, Chen et al. (2023) let the LLM describe generated source code line by line and judge the generated source code. Then, they feed this information back into the model for self-debugging purposes.\nSince majority votes are more suited for classification tasks with a limited output space, we propose a strategy similar to Drozdov et al. (2022). We generate multiple predictions with different samples and sampling strategies. In the second step, we feed both the source text and the predictions into the LLM and let it pick the best translation. To introduce the task to the model, we provide in-context exemplars where the source text is followed by different predictions and the prediction with the best F1-Score is the target. Alternatively, we test the model's capability to interpolate between the predictions using the ground truth as the target of the exemplars. One example of the second strategy can be seen in Listing 5. For the first strategy, the target would be replaced by the best scoring option. An important question for self-consistency is how the predictions should be generated and whether predictions with high scores or greater diversity are more beneficial. We will examine this aspect by using the best three predictions of the previous experiments on the one hand and a more diverse set of predictions with different sampling or prompting strategies on the other hand."}, {"title": "2.2.5. GPT as a teacher model", "content": "Being able to translate regulations into formal representations with only a few exemplars offers the opportunity to start a semi-automated translation process with little time, cost, and effort. Inspired by Wang et al. (2022), we test the use of LLM for generating training data in two setups. First, we use the assumption that we do not have any training data except the best-performing exemplars from Section 2.2.1. All remaining samples in the training data are newly translated using the LLM with those best-performing exemplars for in-context learning. We then train a T5 model with the best-performing exemplars plus the LLM-translated training data.\nIn comparison, we use this method for extending our existing training set with up to 855 new training samples with LLM-generated LRML rules. This setup has the benefit of having a preferable ratio between manually and automatically created data. Furthermore, it allows us to use representative sampling per clause, which is expected to yield superior-quality translations."}, {"title": "3. Results", "content": "Unless explicitly stated otherwise, all experiments are conducted using GPT-3.5-turbo, the model powering the standard version of ChatGPT. We send requests to this model via the OpenAI API. We changed the decoding temperature to 0 to get the most likely output given the prompt, which leads to more factual but less creative outcomes. All prompts were passed in with the user role, which performed better than the system role. Additionally, we ensure that the LRML rule can be extracted from the response (i.e., the response contains \"if(\"). If not, we regenerate the translation with increasing temperatures until we receive a suitable response."}, {"title": "3.1. Sampling", "content": "A natural and straightforward way of picking exemplars is randomly selecting samples from the training set. We randomly sample 50 exemplars and reduce the number until they fit into the maximum token length. By repeating this process multiple times, we can gain insight into the variance of different sets of exemplars and the number of exemplars while keeping the number of input tokens approximately the same. While the results in Fig. 1 show that the F1-Scores range between 53.5% and 60.6%, the majority of runs were above 57.1%. A weak positive correlation (i.e., 0.275 Pearson's Correlation) between F1-Scores and the number of exemplars can be noticed, but this result is not significant (i.e., 0.441 P-Value), and more data would be required to draw any conclusions.\nWe manually filtered the training samples to include a diverse and high-quality set of exemplars to evaluate if human judgement benefits the exemplar selection. Since the remaining 45 exemplars represent the training data well, we use this set to investigate how many exemplars are necessary for achieving satisfactory results. We randomly selected 30 exemplars fitting into the context window and reduced the exemplars gradually. Fig. 2 indicates that three exemplars are enough to make meaningful predictions. Still, the model did not stop consistently after predicting the LRML rules causing the low BLEU scores, which got resolved after having five or more exemplars. Having better scores with ten exemplars than with 15 exemplars shows that adding more exemplars does not necessarily improve the performance, especially if such exemplars are not representative of the ground truth data. Nevertheless, a consistent upward trend in F1-Scores can be observed, reaching a peak of 57.8%. This value falls within the range of random sampling results and suggests that human selection does not yield better results for this particular use case.\nWe investigate strategies to either diversify the exemplars or increase their similarity to the validation data. The results are presented in Table 1. Diversifying the exemplars via stratified sampling per document had adverse effects, likely because, for many documents, there are only a few samples, and those are not representative of other documents. In contrast, clustering the training data based on clauses or LRML rules yielded results that fell within the higher range of the random sampling outcomes depicted in Fig. 1. While representative sampling for the entire validation set was counterproductive, retrieving representative exemplars for each clause in the validation set individually brings significant improvements. The proposed n-gram-based retrieval of exemplars outperforms the semantic-based method and even the T5 model with supervised learning, which had an F1-Score of 68.4% on the validation set using the evaluation procedure from Fuchs et al. (2023b). Furthermore, it can be noticed that having the most similar exemplars at the end of the prompt, i.e., closest to the prediction, brings the best results."}, {"title": "3.2. Contextualisation", "content": "Due to the limited context length, there is a trade-off between the additional context information and the resulting decrease in the number of exemplars. We evaluated this trade-off with the manually filtered samples in Section 3.1. Including a task introduction reduced the exemplar number from 30 to 28. The number of exemplars was further reduced to 27 by adding the format specification, to 26 by adding the explanation for references, and to 24 by listing the most common terms. This setup can be contrasted with having only ten in-context exemplars, and contextualisation is additional information. Lastly, we combine contextualisation with representative sampling per clause in reverse order. Since the most representative exemplars will be selected first, reducing the number of exemplars might have less effect. As shown in Fig. 3, adding a short task introduction improved all scores continuously over all setups. The specification and reference examples were counterproductive in all setups. This could be rooted in a poor wording, which confuses the model more than it helps. Providing the most common terms and emphasising the important aspects led to higher F1-Scores for predictions with fewer exemplars. Notably, the BLEU scores increased for all setups, which might be related to the low baseline compared to Table 1. Finally, contextualisation with representative sampling per clause had similar improvements of 0.8% F1-Scores with only the introduction and with the full context but higher BLEU scores with the full context. Despite overall mixed results, we believe LLM should be contextualised succinctly to maximise their potential."}, {"title": "3.3. Chain-of-thought Prompting", "content": "While chain-of-thought (CoT) prompting was a successful strategy to elicit reasoning and improve accuracy in recent works, including semantic parsing, Table 2 suggests this is not the case for generating the long and complex LRML rules. This was confirmed in multiple setups, where the ten manually filtered exemplars in Section 3.1 were reformulated as CoT exemplars in a colloquial language and an alignment-based style. Additionally, we experimented with providing the additional exemplars without CoT before the CoT exemplars to provide overall more exemplars but still enforce CoT reasoning. Finally, we provided additional context together with the CoT exemplars. While these strategies brought slight improvements, the results were consistently worse than when predicting LRML directly. Different methods to formulate the CoTs and recursive prompting strategies should be tested to confirm these results."}, {"title": "3.4. Self-consistency", "content": "In the proposed self-consistency strategy, the model is required to reflect on previous outputs and choose the best one. While self-reflection was argued to be an emergent ability related to the size of LLM, in our experimental setup, it could also be ascribed to the intuition that discrimination between different predictions is a simpler task than generating a new LRML rule. The results presented in Table 3 indicate such a behaviour only in one of the setups. Using the three best runs with representative sampling per clause with and without contextualisation (71.1%, 71.1%, and 70.3% F1-Scores) and teaching the model to pick one of the LRML rules yielded an improvement of 0.9% F1-Score outperforming previous supervised approaches using intermediate representations and self-reflection (Fuchs et al., 2023b). Nevertheless, comparing these results to the Oracle F1-Score of 75.4% indicates that the model makes more often than not non-optimal choices. All other setups have negative consequences indicating that high variability in the quality of the LRML options is not helpful. We picked the best runs with representative sampling per clause (71.1% F1-Score), random sampling (60.6% F1-Score), and clustering (59.7% F1-Score) for the mixed setup, but the results were closer to the average instead of bringing improvements. Also, letting the model interpolate between the LRML options yielded worse results than choosing one option."}, {"title": "3.5. GPT as a Teacher Model", "content": "By using GPT as a teacher model, we determine if LLMs can be used to commence training a semantic parser for building regulations with only little effort. In the first setup, we selected the best run from the previous experiments with a fixed set of exemplars, i.e., random sampling with 60.6% F1-Score. By using these exemplars, we generate LRML rules for all remaining clauses in the training set (i.e., 543 LRML rules). We trained the more lightweight and freely available T5 model for semantic parsing using the newly generated training data. We followed the evaluation procedure described in Fuchs et al. (2023b). We report the results for the random validation set and test set in Table 4, which makes results comparable with the earlier research as well as with this work. BLEU scores were dropped since they were calculated based on the LRML representation reversed to the original (see Section 2.1) rather than the intermediate LRML representation, as done in this work. It can be noticed that this method achieved reasonable results with a 58.3% F1-Score for the test set. Nevertheless, this score is 8.1% lower compared to training with the original training data. In this case, the quality of the generated training data with a 59.5% F1-Score might be the limiting factor.\nIn the second setup, we used additional clauses translated with the representative sampling per clause strategy and contextualisation, intending to improve the T5 model's parsing quality. The first 150 clauses were extracted from unseen New Zealand's (NZ) Acceptable Solution documents. Since there was only a negative effect visible, we added 705 more clauses from NZ Standards, which brought an improvement of 1.2% in F1-Scores showing that generated additional training data could help the model performance. However, improvements to the quality of the generated data might make this strategy more helpful. Returning to the hypothesis that discrimination is easier than generation, asking humans to rank different translations for the same clause might be a valuable strategy to improve the generated data's quality."}, {"title": "4. Discussion", "content": "The results in this paper show the suitability of LLM to support the translation of building regulations into formal representations insofar as higher scores could be reached compared to traditional supervised models. Additionally, such supervised models could be augmented with additional training data. Nevertheless, since the BLEU and F1-Scores only act as an approximation to how close the translation is to the given ground truth translation, we examine some of the predictions and propose directions for future work.\nWe recognise that slight changes to the rules can cause significant penalties in F1-Scores. Furthermore, some variations in the predictions, such as different granularities, different terminologies, or alternative expressions, might not impact the actual use cases of the formal representation. For example, the prediction in Listing 6 uses \u201chas\u201d instead of \u201cinclude", "and\" keyword to indicate the conjunction. While the \u201cand\u201d would reduce the F1-Scores by about 1.2%, the \"has\" has a stronger impact of 9.7% since it also influences the scoring of the logical and deontic keywords, \u201cthen\" and \u201cpermission\u201d.\nListing 7 shows an example where \u201cventilation pipe": "a term from the LRML vocabulary, was used in the ground truth rather than \"vent pipe"}, {"title": "4.  Discussion", "content": "Still, a disparity persists in terms of translation quality suitable for ACC application. Even when nearing flawless translation,\nthe potential requirement for certification and review remains, emphasising the need for a human-assisted workflow.\nBuilding on these conclusions and limitations, our future work takes two paths. Firstly, we will explore the feasibility of\ninvolving regulators in translating regulations into formal representations during regulatory drafting and publishing. We will\nalso investigate the types of tool support that could enhance the feasibility of this process. Secondly, we aim to advance the\nimplementation of the resulting rules in real-world situations. This not only enhances the practical usability of these rules\nbut also contributes to refining the translation process. By placing more emphasis on logical correctness in evaluations, we\naim to achieve a more dependable assessment and thus enhance the overall quality of the formal representation of building\nregulations."}, {"title": "Author Contributions", "content": "Stefan Fuchs contributed to conceptualisation, methodology, implementation, validation, analysis, draft preparation,\nvisualisation, and manuscript editing. Michael Witbrock contributed to conceptualisation, computational resources,\nmanuscript editing, and supervision. Johannes Dimyadi contributed to manuscript editing and supervision. Robert Amor\ncontributed to conceptualisation, methodology, manuscript editing, and supervision. All authors have read and agreed with\nthe manuscript before its submission and publication."}, {"title": "Funding", "content": "This research was funded by the University of Canterbury's Quake Centre's Building Innovation Partnership (BIP)\nprogramme, which is jointly funded by industry and the Ministry of Business, Innovation and Employment (MBIE)."}, {"title": "Ethics Statement", "content": "Applying machine learning to solve legal tasks can have broad implications since the correctness of the algorithm cannot be\nguaranteed. In the building domain, this could cause major adverse events that put building occupants at risk if an unsafe\nbuilding is issued a compliance certificate and opened to the public. Nevertheless, this work treats LLMs as a supporting\ntool, which helps to automate compliance checking rather than executing the compliance checks directly. The entire means\nof having a formal representation of building codes, which is verifiable and can be deterministically executed, is to guarantee\ninterpretable and reliable automated compliance checking."}]}