{"title": "Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation", "authors": ["Federico Julian Camerota Verd\u00f9", "Lorenzo Castelli", "Luca Bortolussi"], "abstract": "We introduce Limited Rollout Beam Search (LRBS), a beam search strategy for deep reinforcement learning (DRL) based combinatorial optimization improvement heuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson Problem, LRBS significantly enhances both in-distribution performance and generalization to larger problem instances, achieving optimality gaps that outperform existing improvement heuristics and narrowing the gap with state-of-the-art constructive methods. We also extend our analysis to two pickup and delivery TSP variants to validate our results. Finally, we employ our search strategy for offline and online adaptation of the pre-trained improvement policy, leading to improved search performance and surpassing recent adaptive methods for constructive heuristics. Our source code is available at anonymous-url.", "sections": [{"title": "1 Introduction", "content": "Combinatorial Optimization (CO) problems can be found in several domains ranging from air traffic scheduling and supply chain optimization to circuit board design and phylogenetics . Although general-purpose solvers exist and most CO problems are easy to formulate, in many applications of interest getting to the exact optimal solution is NP-hard and said solvers are extremely inefficient or even impractical due to the computational time required to reach optimality. Specialized solvers and heuristics have been developed over the years for different applications. However, the latter are often greedy algorithms based on hand-crafted techniques that require vast domain knowledge, thus they cannot be used on different problems and may get stuck on poor local optima.\nCO problems have gained attention in the last few years within the deep learning community where neural networks are used to design heuristics that can overcome the limitations of traditional solvers. In particular, an extensive literature has been developed on methods to tackle the travelling salesperson problem (TSP) due to its relevance and particular structure that allows to easily handle constraints with neural heuristics. Deep learning approaches for CO problems can be divided into constructive and improvement methods. The former follows a step-by-step paradigm to generate a solution starting from an empty one and sequentially assigning decision variables. Instead, improvement approaches iteratively improve a given initial solution using an operator to turn a solution into a different one. Moreover, deep learning solvers can be classified based on their learning strategies: supervised learning and deep reinforcement learning (DRL).\nMany recent advancements in neural solvers for CO primarily lie within the constructive framework. This approach eliminates the necessity for manually crafted components, thereby providing an ideal means to address problems without requiring specific domain knowledge. However, improvement heuristics can be easier to apply when complex constraints need to be satisfied and may yield better performance than constructive alternatives when the problem structure is difficult to represent or when known improvement operators with good properties exist. Still, generalization, i.e., scaling from training sets with small problems to large instances while retaining good performance, is an open issue when using DRL neural heuristics in CO, particularly for the TSP.\nContributions. While generalization has been studied for constructive methods, to the best of our knowledge no prior work has been done on improvement heuristics. In this paper, we focus on improvement heuristics for the TSP based on DRL policies and propose an inference-time beam search approach, Limited Rollout Beam Search (LRBS),"}, {"title": "2 Preliminary and Related Work", "content": "This section begins by introducing the fundamental concepts of the DRL framework for learning improvement heuristics. Subsequently, we discuss relevant search and adaptive methods in the neural CO literature."}, {"title": "2.1 Improving TSP Solutions with DRL", "content": "A TSP instance is defined by a graph G = (V, E) and the objective is to find a tour \u03b4, i.e. a sequence of nodes xi \u2208 V, such that each node is visited only once, the tour starts and finishes in the same node and minimizes the tour length\n$L(\\delta) = \\sum_{i=1}^{N-1} w_{\\delta_i,\\delta_{i+1}} + \\sum_{\\delta_N,\\delta_1}$,\nwhere N = |V|, Wij \u2208 R+ and (i, j) \u2208 E are edges in the graph. In this work, we consider instances of the Euclidean TSP where Wij = ||xi - Xj ||.\nTo solve TSP instances in the improvement framework we start from a given randomly generated initial solution \u03b40 and use a policy \u03c0\u03b8, parametrized by learnable weights \u03b8, to sequentially improve \u03b40. The policy selects actions in the neighbourhood defined by an operator g that, given a solution and an action, returns another solution to the problem. We formulate the DRL framework as follows.\nState. The state is given by the current solution ST and the best solution found so far \u03b4*T = argmin\u2081\u2264t\u2264T L(\u03b4t).\nAction. Actions are elements within the neighbourhood defined by the operator g. For TSP, we consider 2-opt moves that consist of selecting a tuple of indices (i, j) and reversing the order of the nodes"}, {"title": "2.2 Search in Neural CO", "content": "Search algorithms such as Beam Search (BS) and Monte Carlo Tree Search (MCTS) have been widely used in the literature on neural CO. Typically, they are used online with autoregressive constructive methods to boost their performance at inference time. However, many of the search techniques that work well for constructive heuristics are difficult to extend efficiently to the improvement setting. This is due to the fact that constructive methods work on a short horizon, i.e. the number of steps required to obtain a solution, which is defined by the number of variables in the problem. On the contrary, improvement policies often require many more iterations to achieve good performance. Although MCTS has been widely applied in conjunction with DRL policies, yielding impressive results, a notable drawback lies in the computational cost associated with its backpropagation procedure. This limitation renders MCTS less suitable for the context of CO, particularly when dealing with large search spaces that are difficult to explore. In the literature on neural CO, BS has emerged as a practical alternative to MCTS. This approach strikes a favourable balance between search capability and runtime complexity, making it a promising choice for addressing the challenges inherent in CO scenarios."}, {"title": "Adaptive Methods for Neural CO", "content": "In recent developments within the field of neural CO, a novel trend has emerged in search methods that incorporate techniques for online adaptation of policy parameters during inference."}, {"title": "3 Searching with LRBS", "content": "In this section, we describe our beam search strategy for CO improvement heuristics. To overcome the limitations of previous methods in the improvement MDP, we propose an effective beam search approach that allows to trade-off between the additional computational cost of search and heuristic performance. Additionally, our approach mitigates the effect of the longer episodic horizon in the improvement MDP by reducing the effective horizon on which the DRL policy works."}, {"title": "3.1 The LRBS algorithm", "content": "Solving a CO problem with the DRL framework in Section 2.1 can be seen as traversing a search tree using policy \u03c0to decide the path to follow. Nodes in the tree represent solutions to the problem, with the initial solution \u03b40 being the root node, and edges possible improvement actions (e.g. 2-opt moves) that transform one solution into the other. In Algorithm 1 we present LRBS, the algorithm starts at the root node and carries out its search down the tree in a breath-first fashion by keeping a beam of \u03b2 active nodes for each depth level and exploring \u03b1 of their children, thus limiting the branching factor. Contrary to other search problems, there are no terminal nodes to reach in the improvement MDP of Section 2.1. Hence, exploration is carried out until the explored paths in the search tree reach a fixed depth (Tmax) and the best solution found is returned. In the LRBS algorithm, there are two main operations that can be described as follows."}, {"title": "4 Adapting Pre-Trained Policies with LRBS", "content": "While the search capabilities of LRBS mitigate the effect of distributional shifts when scaling to larger problem instances than those seen while training, its performance is limited by the pre-trained policy. In this section, we introduce an adaptive framework in which we combine LRBS with EAS to update the pre-trained DRL policy. We study the effectiveness of this approach in two different scenarios: offline fine-tuning (FT) and online adaptation (OA).\nIn EAS, a small set of new parameters \u03c6 is introduced by adding a few layers into the agent's neural network, that in encoder-decoder architectures are usually placed in the final layers of the decoder. To reduce the computational burden of previous adaptive methods, Hottung, Kwon, and Tierney proposed to only train the new weights \u03c6, making EAS extremely fast. To update \u03c6 in constructive heuristics, EAS utilizes a loss function consisting of an RL component, aiming to reduce the cost of generated solutions, and an imitation learning component, which increases the probability of generating the best solution seen so far. However, it is not straightforward to apply EAS in the improvement MDP since running multiple times the improvement heuristic for the total number of steps required to achieve a good solution and then adapting \u03c6 would incur extremely long computational times. Instead, in LRBS we can incorporate easily EAS by updating the new weights on the limited rollouts used in node expansion.\nTo fine-tune the pre-trained policy, we assume a limited set (SFT) of instances in the target problem distribution is available and train \u03c6 to maximize the reward achieved over the LRBS rollouts with the RL loss function of EAS, leading to the gradient:\n$\\nabla_\\phi L( \\phi) = \\mathbb{E}_\\pi[(R(LRBS) - b) \\log \\pi_\\phi(LRBS)]$\nwhere SLRBS is a rollout of ns steps and b is a baseline (as in other works, we use the one proposed in Kwon et al. (2020)). This scenario is representative of many domains where similar CO problems have to be solved several times and past instances can be used for fine-tuning. In our experiments, the instances in SFT are solved only once by the LRBS algorithm and after each policy rollout the new parameters \u03c6 are updated according to the gradient in Equation 1. Similarly, in the online adaptation scenario, we update the EAS weights at inference time with the approach described above. However, the EAS parameters are reset before solving each batch of test problems, hence, the extra policy weights adapt solely to the instance being solved."}, {"title": "5 Experimental Results", "content": "In this section, we report experimental results on the search capabilities of LRBS and its effect on the generalization of pre-trained DRL agents to large TSP instances and two pickup and delivery variants. In our study, we use checkpoints of models from de O. da Costa et al. (2020), pre-trained on Euclidean TSP instances with 100 nodes, and from (Ma et al. 2022), pre-trained on PDTSP and PDTSPL instances with 100 nodes. Ma et al. (2021) recently proposed the Dual-Aspect Collaborative Transformer (DACT) architecture for the improvement of TSP solutions with 2-opt moves. Even though DACT performs better than the model from de O. da Costa et al. (2020) in the authors' study, the latter architecture showed much better scalability in our preliminary investigations and even outperformed DACT when both were coupled with LRBS.\nIn all our experiments on TSP, for LRBS, we set a \"search budget\" such that \u03b1 \u00d7 \u03b2 = 60 and fix the other parameters to ns = 20 and Tmax = 5000. On PDTSP and PDTSPL we reduce the budget to 40 and when doing adaptation we use ns = 10 to lower memory consumption. The best values of \u03b1 and \u03b2 for each dataset were determined by testing the method on a set of 10 randomly generated instances of the same size as those in the test set.\nTests datasets. The TSP instances in our experiments are generated according to the method in Kool, van Hoof, and Welling (2019) where the coordinates of nodes are sampled uniformly at random in the unit square. We consider problems with N = {100, 150, 200, 500, 1000} nodes. To ensure a fair comparison with the pre-trained policies, for N = 100 we use the same 10,000 test instances of de O. da Costa et al. (2020). For the other problems, we generate datasets with 1,000 random instances for N = {125, 200} and with 128 instances for N = 500, 1000. For PDTSP and PDTSPL experiments we generate sets of 128 random instances with 200 and 500 nodes. In the following, we refer to the test dataset with problems with N nodes as TSPN, PDTSPN and PDTSPLN, respectively.\nBaselines. To assess the performance of our method, we compare it with the pre-trained policy of de O. da Costa"}, {"title": "5.1 Boosting In-Distribution Performance", "content": "In Table 1 we report the results of LRBS on test instances with 100 and 150 that are close to the training data distribution. Our method outperforms all the considered baselines on TSP100 and has the best results among improvement heuristics on TSP150. On TSP150 the constructive baselines show slightly better gaps than LRBS, but our approach has considerably lower runtime. From our analysis, on test instances close to the training data distribution the best LRBS configuration is (\u03b2 = 60, \u03b1 = 1). While such a configuration corresponds to \u03b2 parallel runs of the policy, introducing limited rollouts allows us to perform online adaption and achieve improved performance."}, {"title": "5.2 Out-of-Distribution Exploration with a Pre-Trained Policy", "content": "In the first part of Table 2 we show results on the generalization power of LRBS when solving larger TSP problems with 200, 500 and 1000 nodes. The configurations used for LRBS are (\u03b2 = 30, \u03b1 = 2), (\u03b2 = 15, \u03b1 = 4) and (\u03b2 = 5, \u03b1 = 12) for the TSP200, TSP500 and TSP1000, respectively. As the test set distribution shifts away from the training distribution we observe that increasing the number of children evaluated for each node in the beam front improves on generalization. While on smaller instances the policy can select good actions and more exploitation with lower \u03b1 leads to the best performance, on larger instances increasing \u03b1 allows to compensate for the imprecision of the agent and yields better results. On these test datasets, LRBS scales better than other improvement heuristics achieving optimality gaps close to those of constructive approaches. From our experiments, we can observe that the augmentations employed by Ma et al. (2021) improve considerably the policy performance on instances with the same size as the training set. However, when considering larger graphs the benefit of the augmentations becomes less pronounced and the algorithm fails to scale. On the contrary, online exploration with LRBS mitigates the performance degeneration due to distributional shift and our method even improves on the results that the policy of de O. da Costa et al. (2020) would achieve if exploring the solution space for the same time as LRBS and using on average 12x more 2-opt operations. On larger instances, LRBS is not competitive with Meta-SAGE but achieves optimality gaps comparable to those of EAS and SGBS+EAS even improving on their performance as the instances get larger.\nTurning our attention to the comparison of LRBS to BS, the results in Table 2 present an interesting phenomenon. On the smaller instances with up to 500 nodes, LRBS is faster and achieves much lower optimality gaps, even 6x smaller than BS. However, on the largest problems of the TSP1000 dataset BS has superior performance than LRBS. This result strongly suggests that as the distributional shift between the training and test instances gets very large the step-wise selection process of BS is better than the rollouts of LRBS in limiting the performance degradation of the policy. This further motivates the need for adaptive strategies to overcome the limitations posed by the pre-trained model."}, {"title": "5.3 Generalization via Adaptation", "content": "In the second part of Table 2, we show results on the generalization of LRBS after fine-tuning the DRL policy on a small set of randomly generated instances with the same number of nodes as the test set (FT) and when adapting the policy parameters online (OA). The LRBS configurations are the same used for the non-adaptive experiments, with the only exception of the LRBS + FT on the TSP1000 where we use (\u03b2 = 10, \u03b1 = 6). For all the considered problems, the FT dataset of randomly generated instances is of size equal to"}, {"title": "5.4 Pickup and Delivery Problems", "content": "The pickup and delivery variant of TSP (PDTSP) consists of n one-to-one pickup-delivery requests, meaning that there are n pickup nodes with goods that need to be transported to n corresponding delivery nodes. The objective is to find the shortest Hamiltonian cycle under the precedence constraint that every pickup node has to be visited before its corresponding delivery node. We also study PDTSP with the last-in-first-out constraint (PDTSPL) that enforces a stack ordering between collected goods and delivery is allowed only for the good at the top of the stack. For these problems, Ma et al. (2022) define a removal-reinsertion operator that selects a pickup-delivery request nodes (\u03b4i+, \u03b4\u03b9-), positions (j, k) and places node \u03b4i+ after node \u03b4j and node \u03b4\u2081- after \u03b4\u03ba.\nIn Table 4 we report the results of applying LRBS on model checkpoints from Ma et al. (2022) (N2S-A), pre-trained on pickup and delivery instances of size 100, when solving PDTSP and PDTSPL instances with N = 200 and 500 nodes. In these experiments, for N2S-A we use the same exploration strategy adopted by the authors where at inference time each instance solved is transformed into different ones, using the augmentations of (Kwon et al. 2020), and the policy is rolled out from each new instance. Our results show that the online exploration approach of LRBS is much more effective than N2S-A when generalizing to larger instances. Not only in terms of pure performance but also computational efficiency. On the smaller instances with 200 nodes, LRBS achieves a good reduction of optimality gaps requiring less time than N2S-A even when performing online adaptation. The PDTSP500 benchmark results are not satisfactory with optimality gaps well above 50% but still, LRBS shows improved generalization compared to N2S-A reducing its gap by almost 6x. On the much more constrained PDTSPL500 problems instead, online search through LRBS outperforms N2S-A with a gap reduction close to 10x when adaption is employed. Overall, the results of Table 4 are still far from being competitive with traditional solvers such as LKH but show the generalization potential of pre-trained policy with online search and adaptation."}, {"title": "6 Conclusion", "content": "In this study, we have introduced LRBS, a novel beam search method designed to complement DRL-based improvement heuristics for combinatorial optimization problems enhancing inference time performance and generalization. LRBS offers a tailored approach that enables pre-trained models to efficiently handle problem instances of significantly larger scales, up to ten times bigger than those encountered during the DRL policy initial training phase. To further enhance the generalization of pre-trained models, we integrate LRBS with EAS in offline and online adaptive scenarios. Our experimental evaluation shows LRBS's superiority over existing DRL improvement methods in the context of solving the Euclidean TSP and two pickup and delivery variants. LRBS consistently outperforms alternative approaches proposed both for constructive and improvement heuristics. Moreover, in our analysis, LRBS exhibits superior runtime efficiency when scaling to larger instances compared to established constructive baselines, showing how improvement heuristics coupled with adaptive and search approaches can be a viable alternative to constructive methods."}, {"title": "A Sensitivity Analysis: Performance of Different LRBS Configurations", "content": "In Section 5 we report the results of LRBS with the best-performing parameter configurations, here we show its performance (without online adaptation) for different values of the parameters \u03b1 and \u03b2 on TSP datasets. For TSP500 and TSP1000 the datasets considered have 1000 and 200 instances, respectively, instead of 128 as in the main results. Thus, computational times increase accordingly."}]}