{"title": "Sparsity May Be All You Need:\nSparse Random Parameter Adaptation", "authors": ["Jesus Rios", "Pierre Dognin", "Ronny Luss", "Karthikeyan Natesan Ramamurthy"], "abstract": "Full fine-tuning of large language models for\nalignment and task adaptation has become pro-\nhibitively expensive as models have grown in\nsize. Parameter-Efficient Fine-Tuning (PEFT)\nmethods aim at significantly reducing the com-\nputational and memory resources needed for\nfine-tuning these models by only training on\na small number of parameters instead of all\nmodel parameters. Currently, the most popu-\nlar PEFT method is the Low-Rank Adaptation\n(LoRA), which freezes the parameters of the\nmodel to be fine-tuned and introduces a small\nset of trainable parameters in the form of low-\nrank matrices. We propose simply reducing the\nnumber of trainable parameters by randomly\nselecting a small proportion of the model pa-\nrameters to train on. In this paper, we compare\nthe efficiency and performance of our proposed\napproach with PEFT methods, including LoRA,\nas well as full parameter fine-tuning.", "sections": [{"title": "1 Introduction", "content": "It has become common practice to train application-ready language models in two phases (Radford et al., 2018; Kenton and Toutanova, 2019): first, the model is pre-trained on a very large and general corpus of (unlabeled) text; then further trained (or fine-tuned) on a smaller specific set of examples demonstrating the intended behavior for a particular application, such as instruction following (Ouyang et al., 2022b).\nOverall, supervised fine-tuning (SFT) requires less computational resources than pre-training (PT) due to the significantly smaller size of the train-ing set as well as the typical use of early stop-ping to deal with overfitting. This means orders-of-magnitude less gradient computations and parame-ter updates are needed during SFT compared to PT. However, a major drawback is that memory require-ments remain the same, unless a parameter-efficient fine-tuning (PEFT) technique is used. The main memory bottleneck during training is the number of trainable parameters, since additional memory must be allocated for their gradients and other per-parameter statistics needed by the optimizer. The idea behind PEFT (Lialin et al., 2023) is to signifi-cantly reduce the number of trainable parameters during fine-tuning while maintaining performance.\nLow Rank Adaptation (LoRA), first introduced by Hu et al. (2022), currently remains the most popular PEFT technique. LORA freezes all the pre-trained model parameters \u04e9\u0440\u0442 and introduces trainable low-rank matrices (e.g., B, A) for a pre-selected subset of matrix tensors to represent the changes (${\\Delta} = BA$) needed for adapting the model to a new task. The adapted model parameters are given by ${\\theta_{PT} + \\Delta}$. Memory and computational ef-ficiency are achieved by optimizing only over the parameters of these newly added, but significantly smaller, matrices.\nThe success of LORA begs one to ask what proper-ties make this method perform well. Is the low-rank structure critical, i.e., does A need to be low-rank? Is it sufficient to constrain A to be low dimen-sional? A main goal of this paper is to investigate these questions. An abundance of research is going"}, {"title": "2 Motivation", "content": "into new methods for structured A (see Section 3\nbelow) and novel directions into unstructured meth-\nods for fine-tuning could open avenues in areas\nsuch as model merging (Wortsman et al., 2022;\nMatena and Raffel, 2022) or pluralistic alignment\n(Feng et al., 2024).\nIn this work, we propose a different approach\nwhere A is not factorized into low rank matrices\nbut rather chosen to be a random subset of the\nmodel parameters. This Sparse Random parameTer\nAdaptation (SpaRTA) method imposes a sparsity\nconstraint on the adapter that can be easily con-\ntrolled. By changing the desired sparsity, one can\nchange the number of adaptation parameters. Re-\ngardless of how the selected parameters are sam-\npled from the model parameters, subsequent up-\ndates only affect these parameters. This sparsity\nconstraint and randomness of selected parameters\nis in contrast to techniques such as LoRA that ef-\nfectively affect all parameters in \u04e9\u0440\u0442. See Figure\n1 for an illustration of the method. Generally, one\nsamples m parameters from the pre-trained model\nOPT, stores the indices of these parameters in 4, and\nuses adapter A to fine-tune \u03b8\u03c1\u0442.\nTo investigate the performance of SpaRTA, we\nbuild adapters with different sparsity levels and\ntest on the GLUE classification tasks (Wang et al.,\n2019) as well as the IMDB dataset (Maas et al.,\n2011). SpaRTA is compared to a fully fine-tuned\nmodel (Full FT), and PEFT approaches including\nLORA. SpaRTA is quite competitive compared to\nLORA given that it only modifies a small sparse\nnumber of model parameters.\nYu et al. (2024) look at the differences between a\nlanguage model's parameters before and after fine-\ntuning, and demonstrate empirically that it is possi-\nble to randomly drop (i.e., set to zero) up to 99%\nof these parameter changes without significantly\naffecting the model performance. The sparsity in\nthe parameter changes represented by the tensor A\nis known as A-sparsity.\nThis motivates our approach, SpaRTA, which pro-\nduces a performant fine-tuned model by directly\nadapting only a small percentage of the pre-trained\nmodel parameters: SpaRTA randomly selects the\n(scalar) parameters to train and freezes the remain-\ning parameters (i.e., setting the corresponding A\nvalues to zero). This parameter sparsity also helps\nin reducing overfitting, as pre-trained models typi-"}, {"title": "2.1 Is low-rank necessary for \u2206?", "content": "cally have enough capacity to learn the often lim-\nited amount of (labeled) data used for fine-tuning.\nThere is no guarantee of A-sparsity in LORA, but\nthis is a desired property since it reduces parame-\nter interference (Yadav et al., 2023) when merging\nfine-tuned, task-specific models into a single one\nthat can perform all tasks simultaneously.\nOne byproduct of A-sparsity is sparse \u2206 up-\ndates which reduce gradient computations during\ntraining, ensure fast merge back of the sparse A\ninto the model for inference, and ultimately same\ninference cost as the original model (like LoRA).\nThe properties of low memory, low computation\nneeds in training, identical inference time as the\noriginal model are all quite desirable for an adapta-\ntion technique. SpaRTA has all these properties plus\nthe unique added benefit of producing only sparse\nchanges in a small number of the parameters of the\noriginal model that can be beneficial for merging\nmultiple SpaRTA adapters.\nIn Appendix A, we show that the weight matrix\nchanges (i.e., \u2206) during full parameter fine-tuning\nare, in fact, not generally low-rank for capable mod-\nels such as gemma-2b and mistral-7b. This indi-\ncates that LoRA works, not because of the low-rank\nconstraint particularly, but rather due to the c\u0430\u0440\u0430\u0441-\nity reduction achieved since fine-tuning with LoRA\nis done with limited training data. Such insight\nalso hints that any constraint which reduces the\ncapacity of the original model could perform com-\npetitively, motivating our approach that selects a\nsmall number of parameters of the original model\nto be updated during training."}, {"title": "3 Related work", "content": "The last few years has seen many advances in\nfine-tuning Large Language Models (LLMs). Per-\nhaps the most well-known method (also most used\nin practice) is LORA (Hu et al., 2022), which has\nspurred many variants including DoRA \u2013 which\nadapts only the directions of pre-trained weights\n(Liu et al., 2024), VeRA \u2013 which shares low-rank\nmatrices across layers (Kopiczko et al., 2024),\nAdaLoRA - which adaptively allocates parame-\nter budgets among weight matrices according to\ntheir importance (Zhang et al., 2023), and SoRA \u2013\nwhich dynamically adjusts the intrinsic rank during\nthe adaptation (Ding et al., 2023). This set of meth-\nods each have a different structure to the A being"}, {"title": "4 SpaRTA: Adapting a Random Subset of\nModel Parameters", "content": "optimized with the commonality being the training\nof some function of low-rank matrices. For exam-\nple, further expanding on VeRA (Kopiczko et al.,\n2024), the authors propose to learn ${\\Delta} = DBEA$\nwhere A and B are random low-rank matrices and\nD and E are diagonal matrices to be trained.\nBeyond adding structured parameters is the con-\ncept of fine-tuning a small subset of the total param-\neters of the model, i.e., sparse fine-tuning, where\none must first decide which subset of parameters\nto fine-tune (similar to deciding which parameters\nin each layer to add adapters to in LoRA). Ansell\net al. (2024) start with an initial set of parameters\nand offer procedures to drop (according to param-\neter magnitude) and grow (according to gradient)\nthe set, i.e., they learn the set of parameters to\ntrain. Alternatively, Ma et al. (2024) focus on the\nsparsity of neuron activations during training by\npre-computing neuron importance scores and only\nincluding important neurons in computations dur-\ning training. Ansell et al. (2022) first fine-tune on\nall parameters, select the parameters that change\nthe most, and then fine-tune again from scratch on\nthe selected parameters. In contrast to these, our\nproposed SpaRTA method produces a performant\nfine-tuned model by directly adapting only a small\npercentage of the pre-trained model parameters\nchosen completely at random.\nYet another related direction is that of compres-\nsion which results in sparse models; algorithms\nin this genre take a dense fine-tuned model with\nthe goal of compressing it while maintaining per-\nformance. Compression (see Zhu et al. (2024) for\na survey) could be accomplished by quantization,\nlow-rank approximation, pruning (i.e., removing\nneurons, attention heads, or even layers), or distil-\nlation. The focus of this paper, however, is on fine-\ntuning dense models rather than learning sparse\nmodels as in Liu et al. (2023).\nSuppose the vectorized parameters of the pre-\ntrained language model are ${\\theta_{PT} \\in R^n}$ where n\nis the number of parameters, and full parameter\nfine-tuning (FT) of the model is performed with\na labeled dataset characterizing a task. FT typ-\nically updates Opr using a stochastic first-order\ngradient-based optimization algorithm (e.g., Adam\nfrom Kingma and Ba (2015)) to maximize the con-\nditional probability of the labels in the training"}, {"title": "5 Memory Usage", "content": "Recall that SpaRTA freezes n m (m < n)\nof the model parameters. We define sparsity as\ns = 1- m/n \u2208 (0,1), the percentage of frozen\nmodel parameters (e.g., if 1% of parameters are\ntrainable, then the sparsity is 99%). Subsequently,\ndensity is defined as k = m/n = 1-s, the percent-\nage of trainable model parameters. In practice, for\na chosen sparsity s, one can freeze a model param-\neter with probability s, expecting a total sparsity\npercentage of s over all model parameters. Thus, in\nexpectation, k = m/n percent of the model param-\neters are chosen as trainable, for a total of n k = m\ntrainable parameters.\nFor SpaRTA, only \u2206 (of size m) is trainable,\nwhich is significantly smaller than the total num-\nber of trainable parameters for FT since m \u00ab \u043f.\nHowever, the indices of these randomly chosen\nmodel parameters must be recorded into 4, adding\nto the memory requirements. Indices can be stored\nin 16-bit integers for all the Transformer models\n(Vaswani et al., 2017) considered in this paper.\nSpaRTA sparsifies neither the model head (which\nis kept fully trainable) nor the embeddings (which\nis kept frozen during training). The parameters in\nTransformer networks consist of bias vectors and\ntwo-dimensional weight matrices. Storing indices\nfor all these trainable parameters would require at\nmost m (2 \u00d7 16) bits of memory (m parameters, two integers to index a 2-dimensional matrix, 16\nbits per integer).\nThe values in Af are of the same type as the\nmodel parameters, e.g., using 16-bit brain floating-\npoint (bfloat16) tensors. Thus, SpaRTA requires\nup to m (2 \u00d7 16 + 16) = 3m \u00d7 16 bits of ex-\ntra memory to specify the index & and delta \u0410\u0444\ntensors. That is 3 k times more memory than the\noriginal model, which requires just n \u00d7 16 bits for\nstoring its parameters. For instance, using SpaRTA\non a model with sparsity 80%, 90%, 95% and 99%\nwould require up to 60%, 30%, 15% and 3% more\nmemory, respectively.\nWe next analyze\u00b9 SpaRTA's memory savings dur-\ning training. Optimizing the full set of model pa-\nrameters (FT) using Adam (Kingma and Ba, 2015)\nrequires memory for the parameters, their gradi-\nents with their (adaptive) first and second moments\nestimates. This requires 4 n \u00d7 16 bits of memory\nwhen using a bfloat16 representation.\nIn contrast, SpaRTA only optimizes A, requir-\ning a total of m (4 \u00d7 16 + 2 \u00d7 16) + n \u00d7 16 bits\nof memory: (i) m (4 \u00d7 16) bits for the statistics\nneeded by Adam to optimize \u2206 of size m, (ii)\nm (2 \u00d7 16) bits for 4 with the indices identifying\nthe model parameters associated with A, and (iii)\nthe PT model parameters (n \u00d7 16 bits).\nMemory savings appear if and only if\n$m (4\\times16+2\\times16) + n \\times 16 < 4n\\times16$,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (1)$\n$kn (6\\times16) < 3n \\times 16$,\n$k< \\frac{1}{2}$,\nthat is, SpaRTA is a real PEFT method iff k < 0.5,\ni.e., a sparsity higher than 50% is required (s >0.5),\nmaking less than 50% of the model parameters\ntrainable. For instance, using SpaRTA on a model\nwith sparsity s = 80%, 90%, 95%, and 99% re-\nquires 45%, 60%, 67.5% and 73.5% less memory\nthan full parameter FT, respectively, see Table 1."}, {"title": "6 Experimental Setup", "content": "We next detail the framework for conducting the\nexperiments in Section 7. Motivation is first given\nfor the tasks followed by a description of models\nto be used and how these models are adapted (with\nmanipulations) for the desired tasks."}, {"title": "6.1 Tasks", "content": "weights, downloadable from Hugging Face\u00b3(HF).\nNote that when using an instruction-following (it)\nmodel, inputs are formatted according to the con-\nventions established when training the model on\ninstructions.\nThe main experimental focus is on Natural Lan-\nguage Understanding (NLU) tasks, specifically se-\nquence classification, which involves classifying\nnatural language sequences into a given number\nof classes. NLU tasks are easier to evaluate than\nNatural Language Generation (NLG) tasks, as one\ncan use simple metrics (Accuracy, F1, Recall, Pre-\ncision, etc.) that can avoid the inherent ambiguity\nof evaluating more general generative tasks. While\nSpaRTA is also applicable to NLG tasks, they are\nnot used in the following demonstrations due to the\nchallenges associated with their evaluation, typi-\ncally requiring human judgments as the gold stan-\ndard measure of performance. Human evaluations\ncan be expensive and time consuming, do not scale\neasily, and are not as objective as NLU evaluations."}, {"title": "6.2 Language Models", "content": "Starting with available open-weight language mod-\nels, the goal is to adapt them to perform se-\nquence classification. Two types of trained mod-\nels are used: base and instruction-tuned models,\nwhere the latter have additionally been trained\nto follow users' intent when prompted, such as\nanswering questions or responding to instruc-\ntions. Specifically, we consider the following\nlanguage models: gemma-2b and gemma-2b-it\nfrom the Gemma (Team et al., 2024) family, and\nmistral-7b and mistral-7b-it from the Mis-\ntral\u00b2 (Jiang et al., 2023) family, developed by\nGoogle and Mistral AI respectively. The \"it\" suf-\nfix refers to instruction-tuned models. They are of\nparticular interest for our experiments as they will\nshow results for models with two different numbers\nof parameters. All models are readily available text-\nto-text, decoder-only transformer models with open"}, {"title": "6.3 Task Adaptation", "content": "When using a base model for sequence classifica-\ntion, the raw sequences to be classified are input\ndirectly into the model. However, when using an in-\nstruction model, these sequences are first wrapped\ninto a classification-specific instruction. For exam-\nple, a possible instruction could be: \u201cDetermine\nif the following sentence has a positive sentiment.\nRespond Yes or No.\", followed by the sequence\nitself to be classified.\nA (generative) pre-trained Transformer model\nwith a decoder-only architecture has a head that\ntransforms the final hidden state of each token in\nthe input sequence into vocabulary logits. Effi-ciently adapting this model for sequence classifica-\ntion requires the swap of this head for a sequence\nclassification head, which uses only the final hidden\nstate of the last token in the input sequence h \u2208 Rd\nto do the classification. This reduces the param-\neter size of the head, which is just a linear layer\nwith no bias applied to a token's final hidden state,\nfrom a weight matrix W \u2208 Rv\u00d7d to W \u2208 Rcxd,\nwhere d is the dimension of the model's hidden\nstates (e.g., 2,048 and 4,096 for the gemma-2b and\nmistral-7b models respectively), v is the num-\nber of tokens in the vocabulary (e.g., 256,000 for\ngemma-2b or 32,768 for mistral-7b), and c is the\nnumber of classes in the individual task (e.g. 2 to\n3 in the experiments that follow). With this small\nchange, the model outputs classification probabili-\nties for each whole input sequence through\n$p = softmax(h W^T) \\in R^C$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (2)\nThe classification head of our base pre-trained\nmodels (i.e., gemma-2b and mistral-7b) is initial-\nized with random weights, sampled from a nor-\nmal distribution $N(0,\\sigma^2)$, with \u03c3 chosen such\nthat the model at initialization predicts either class\nwith equal probability. This is achieved with\n${\\sigma} = \\frac{0.06}{\\sqrt{d}}$. With this, the loss starts at ap-\nproximately ${\\text{- log}(1/2) \u2248 0.693}$ and the accuracy\nat ${\\approx 50\\%}$ for a balanced evaluation dataset.\nRegarding instruction-tuned models (i.\u0435.,\ngemma-2b-it and mistral-7b-it), the original\""}, {"title": "7 Experimental Results", "content": "vocabulary weights are rather reused when initial-\nizing their classification heads. To do so requires\nto first identify the tokens in the vocabulary that\nthe model is expected to use for classification\nfollowing the instruction. For example, these\ncould be the tokens associated with a \"Yes\" or\n\"No\" response by the model. The embeddings in\nthe original model (head) associated with those\nclassification tokens are extracted and used to\ninitialize the classification head. While many\nmodels tie their vocabulary heads to their tokens\nembedding matrices, these new classification heads\nare never tied to the model's input embedding\nmatrix.\nEfficacy of SpaRTA is demonstrated empirically by\ncomparing it to the following baselines:\n\u2022 Full parameter Fine-Tuning (Full FT): all\nmodel parameters are optimized.\n\u2022 LORA: Only the additional matrices represent-\ning low-rank adapters are optimized.\n\u2022 Head: Only the classification head is opti-mized; all other parameters remain frozen.\nThis enables the exploration of the performance of\nSpaRTA on a range of sparsity levels, varying the\nnumber of trainable parameters. A complete set of\nresults can be found in Appendix D accompanied\nby a detailed description of the training setup in\nAppendices C and E.\nEach sequence classification task considered in our\nexperiments is given by a dataset of labeled ex-\namples. Table 2 summarizes these datasets and\ntheir splits for training, development, and testing\nfor both the GLUE classification tasks (Wang et al.,\n2019) and IMDB (Maas et al., 2011). See Ap-\npendix B for more detailed descriptions."}, {"title": "7.2 IMDB", "content": "Table 3 shows the results for the IMDB classifica-\ntion task where each model is asked to rate a review\nas positive or negative. Each model is fine-tuned\nusing adaptation techniques including: (i) Full FT\nwhere all model parameters are updated (100% den-\nsity); (ii) SpaRTA for different orders of magnitude\nof density levels 5%, 0.5%, \u22480.05% \u2013 the last one\nallowing for SpaRTA to have close to the same num-\nber of trainable parameters as for LoRA; (iii) LORA\nwith rank r = 8 equivalent to \u2248 0.05% of trainable parameters compared to the model full parameter\nsize; (iv) Head adaptation where only the classifi-\ncation head is updated (\u22482e-4% density).\nIn Table 3, the results for adaptation techniques\n(rows) are sorted in order of descending density as\nto show the impact of decreasing the number of\ntrainable parameters on the overall performance.\nFor gemma-2b and gemma-2b-it, Full FT adapta-\ntion gives strong test loss and accuracy numbers,\noften the best available \u2013 this is expected since all\nmodel parameters are fine-tuned. SpaRTA results\nat 5% density are close to Full FT, even matching\nthem for gemma-2b-it. As the density decreases\nby orders of magnitude, the results slowly de-\ngrade. For \u22480.05%, gemma models' performances\nare slightly behind or matching performance from\nLORA. Head adaptation provides the worse results\nas one would expect since it has so few parameters\nto work with.\nFor mistral models, the trend is similar with\nFull FT adaptation showing strong loss and accu-\nracy numbers. SpaRTA performs well, matching\nand even improving upon Full FT with a density\nof 0.05%. SpaRTA even improves over LORA for\nboth mistral-7b and mistral-7b-it at this low\ndensity. Once again, Head adaptation gives the\nworst results. Overall these results are encouraging\nand show that SpaRTA is competitive and provides\nsimilar performance to LORA for low densities."}, {"title": "7.3 GLUE", "content": "Table 4 compares the results of gemma-2b-it on 7\nof the GLUE tasks for the same adaptation meth-\nods as in Table 3. An equivalent set of results for\nthe gemma-2b base model is given in Table 10, see\nAppendix D. Here again, adaptation results (rows) are ordered in descending order of density with\nSpaRTA evaluated for different orders of magnitude\nof density levels 5%, 0.5%, 0.037% \u2013 the last one\nallowing SpaRTA to have approximately the same\nnumber of trainable parameters as LORA. LORA has\nrank r = 8 equivalent to 0.037% of trainable pa-\nrameters; Head adaptation uses a very small 1.63e-\n4% density. Overall, the same trend as for IMDB\nis observed. Full FT results are very often the best,\nsometimes bested by SpaRTA 5% (RTE, MNLI).\nFor a low density, where LoRA and SpaRTA use the\nsame number of trainable parameters, SpaRTA can\nbest LORA (QNLI, SST2, RTE) but is overall com-\npetitive with LoRA results.\nA similar set of results on the same GLUE tasks\nis provided for mistral-7b-it in Table 5. Again,\nresults for the mistral-7b PT base model can be\nfound in Table 11, see Appendix D. Full FT re-sults can often be matched or bested by SpaRTA 5%\n(RTE, SST2, QQP, MNLI, MRPC, COLA). SpaRTA\noften gives better results than LoRA (QNLI, RTE,\nSST2, QQP, MNLI) for a low density (0.0048%)\nwhere both techniques have comparable numbers\nof parameters. Once again, SpaRTA is a competi-\ntive adaptation technique, often providing similar\nresults to LORA with low density."}, {"title": "7.4 Remarks", "content": "Results on GLUE and IMDB establish that SpaRTA\nis a viable adaptation technique that can be competi-\ntive with LORA, especially for larger LMs. These re-\nsults demonstrate that a simple sparsifying scheme\ncan offer a valid adaptation technique. This opens\nup the possibility for more investigation in sparse\nadaptation as the low rank approximation of a\nmodel's parameter changes is not the only mech-\nanism to provide a performant adaptation method.\nThis indicates that what matters is not necessarily\nthe adapter structure used in PEFT techniques, but\nrather the number of independent parameters, that\nis relevant to the adaptation task at hand."}, {"title": "8 Exploring Model Merging", "content": "Yu et al. (2024) demonstrate that one can per-\nform Full FT, compute ${\\Delta = \\theta_{FT} - \\theta_{PT}}$, sparsify\nA to get \u2206, and obtain good performance using\n${\\theta_{FT} = \\theta_{PT} + \\Delta}$ as model parameters. Furthermore,\nthey show that merging models via their sparsified\nA-s from different tasks can also maintain the per-"}, {"title": "9 Conclusion", "content": "formance of the individual models. This motivates\nexploring model merging via SpaRTA, where no\nFull FT is required and sparse adapters are instead\nfine-tuned.\nIn our setup, because the heads of our SpaRTA-tuned models are fully trained, merging them into\na single head would create parameter interference.\nWe thus merge everything except the heads and add\neach unmerged head on top of the merged model\ncreating a multi-head model. We need the merged\nmodel to produce multiple outputs (i.e., one for\neach task) per input and this is exactly what a multi-head merged model does. In the forward pass, a\nsequence of input tokens goes through the merged\nmodel, producing a final state representation of\nthe tokens. This representation is shared by all\ntasks and is passed to each head to produce one\nprediction per task as output. This way all tasks are\nprocessed concurrently on the same input.\nWe experiment merging two models: one indi-vidually fine-tuned with SpaRTA for SST2 (classi-fying text sentiment) and another for COLA (clas-sifying grammatical correctness), using gemma-2b\nas PT model and 99% sparsity in both cases. We\nmerge these two models by adding the respective\n\u2206 to their PT model's original parameters \u04e9\u0440\u0442,\nand obtaining a two-headed model that can simulta-neously classify any input sentence on both criteria\nin a single forward pass. Table 6 compares per-formance of the merged model with that of the\nunmerged models. This result encourages future\nexploration of this practical usecase for SpaRTA.\nAs PT language models have grown in size, PEFT\nhas become crucial for enabling fine-tuning large\nPT language models on limited hardware and fi-\nnancial resources. We have introduced SpaRTA, an"}, {"title": "10 Limitations", "content": "approach that sharply decreases the set of trainable\nparameters, reducing the GPU memory used by\nthe optimizer and speeding up training. We have\ndemonstrated on a variety of task adaptation sce-\nnarios that our fine-tuning approach is parameter-\nefficient and competitive with LoRA, the current\nPEFT standard. Experiments with 2B and 7B pa-\nrameter pre-trained models demonstrate good per-\nformance, and as per Yu et al. (2024), we expect\nlarger models to allow for higher levels of spar-\nsity in training; meaning that efficiency of SpaRTA\nshould get better with larger model sizes as also\nsuggested in Table 1.\nRegarding future directions, while SpaRTA has\nbeen applied to supervised learning, it is also\namenable to reinforcement learning often used for\nmodel alignment (Ouyang et al., 2022a). Further-more, SpaRTA allows one to select which modules\nto sparsify and at what layers. Like LoRA, this deci-sion affects both the number of trainable parame-ters and the performance of the fine-tuned model,\nand we plan to investigate its impact in SpaRTA.\nLastly, we plan to explore model merging as dis-cussed in Section 8; SpaRTA opens the potential for\nmodel merging without Full FT but with possibly\nlittle interference.\nWe have demonstrated various benefits of SpaRTA, including low memory and high performance. Re-garding limitations, questions remain about how to best deal with overfitting, though we have some insights. We have observed in our experiments that as we increase the sparsity level, and reduce, in turn, the number of trainable parameters, there is less overfitting. Indeed, there is a point in which both the training and validation losses converge together without significantly diverging from each other, eliminating the need for explicit overfitting mitigation techniques. Moreover, further increas-ing of the sparsity level beyond this point results in underfitting. Thus, we can think of our approach as a technique to improve generalization by limiting a model's capacity to overfit to the training data. However, finding the breaking point at which this happens requires expensive experimentation. We leave as future work the investigation of how such an optimal sparsity level depends on the model, dataset sizes, and task complexity. Knowing this relation will allow us to determine in advance how much sparsity in the trainable parameters is needed for reducing the capacity of a large model to a point where it learns a new task on a relatively small amount of examples without overfitting."}]}