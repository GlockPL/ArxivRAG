{"title": "HILBERT CURVES FOR EFFICIENT EXPLORATORY LANDSCAPE ANALYSIS NEIGHBOURHOOD SAMPLING", "authors": ["Johannes J. Pienaar", "Anna S. Bosman", "Katherine M. Malan"], "abstract": "Landscape analysis aims to characterise optimisation problems based on their objective (or fitness) function landscape properties. The problem search space is typically sampled, and various landscape features are estimated based on the samples. One particularly salient set of features is information content, which requires the samples to be sequences of neighbouring solutions, such that the local relationships between consecutive sample points are preserved. Generating such spatially correlated samples that also provide good search space coverage is challenging. It is therefore common to first obtain an unordered sample with good search space coverage, and then apply an ordering algorithm such as the nearest neighbour to minimise the distance between consecutive points in the sample. However, the nearest neighbour algorithm becomes computationally prohibitive in higher dimensions, thus there is a need for more efficient alternatives. In this study, Hilbert space-filling curves are proposed as a method to efficiently obtain high-quality ordered samples. Hilbert curves are a special case of fractal curves, and guarantee uniform coverage of a bounded search space while providing a spatially correlated sample. We study the effectiveness of Hilbert curves as samplers, and discover that they are capable of extracting salient features at a fraction of the computational cost compared to Latin hypercube sampling with post-factum ordering. Further, we investigate the use of Hilbert curves as an ordering strategy, and find that they order the sample significantly faster than the nearest neighbour ordering, without sacrificing the saliency of the extracted features.\nKeywords Fitness landscape analysis sampling Hilbert curve", "sections": [{"title": "1 Introduction", "content": "Search landscape analysis has established itself as a useful approach for understanding complex optimisation problems and analysing evolutionary algorithm behaviour [18]. Many landscape analysis techniques have been developed over the years, with the most widely used techniques including fitness distance correlation [11], local optima networks [26], and exploratory landscape analysis (ELA) [24]. When landscape analysis produces numeric outputs, the resulting feature vectors can be used as abstract representations of problem instances, where instances with similar feature vectors are assumed to fall into similar problem classes. If these feature vectors effectively capture the important characteristics of problems, they can be used as the feature component for automated algorithm design (AAD) \u2013 specifically, automated algorithm configuration and selection. A number of recent studies have achieved different aspects of AAD using landscape analysis in specific contexts [2, 13, 14, 16, 17, 32].\nLandscape analysis approaches differ in terms of what they measure or predict (e.g. ruggedness, modality, presence of funnels, and so on), and also on what they produce (e.g. numerical results or a visualisation of a phenomenon) [21]. They can also be distinguished based on the scale of the analysis [27] \u2013 a global approach attempts to characterise the features of the search space as a whole, while a local approach will consider the features of the landscape in the neighbourhood of solutions. Fitness distance correlation [11] and local optima networks [26] are both examples of global approaches, whereas the average length of an adaptive walk [37] is an example of a local landscape feature.\nMany landscape analysis techniques that measure local features are based on samples that are spatially correlated, i.e. sequences of neighbouring solutions, as opposed to a sample of independent solutions from the whole search space. Techniques that require such sequences of sampled solutions include correlation length for measuring ruggedness [38], entropic profiles of ruggedness and smoothness with respect to neutrality [35, 36] \u2013 adapted as a single measure of ruggedness for continuous spaces [19], approximations of gradient [20], information content features [25], and measures of neutrality [34].\nIn the context of numerical optimisation, a wide range of landscape analysis approaches have been implemented in the R package flacco [12], which has also been re-implemented in Python (pflacco\u00b9). Over 300 landscape metrics are included in flacco, organised into 17 sets, which include the original ELA metrics [24] covering six of the feature sets, and the set of information content features [25] consisting of five metrics. In a study of the subset of the \"cheap\" feature sets in flacco, Renau et al. [29] found that the two most salient (in terms of distinguishing between problems) and robust feature metrics were from the information content and ELA meta-model feature sets. Unlike the other feature sets in flacco, the information content metrics require spatially correlated samples.The saliency of the information content feature metrics and the additional requirement of neighbourhood ordering is the motivation for this study.\nTo date, the two main strategies proposed for generating spatially correlated samples in continuous search spaces are random walks [22] and post-factum ordering of global samples [25]. Desirable properties of a sampling strategy for estimating local features are that the solutions provide good coverage of the search space [15], successive points are positioned close to each other (compared to other solutions in the sample) to capture landscape changes in the neighbourhood, and that the process has low computational expense. The most commonly used approaches to neighbourhood sampling for numerical optimisation are uniform random sampling or Latin hypercube sampling (LHS) [28], followed by either random or nearest-neighbour ordering. These are the approaches implemented in flacco and pflacco.\nIn this paper, we investigate the use of Hilbert curves [10] as a new sampling strategy. Hilbert curves are fractal space- filling curves with two desirable properties: (1) they guarantee uniform search space coverage, and (2) neighbouring points on the curve are located close to one another. We show that Hilbert curves are comparable to ordered LHS in terms of the saliency of the extracted features, but are significantly cheaper to compute. Additionally, we show that a Hilbert curve can be used to spatially order an LHS sample, resulting in significant computational gains compared to the commonly used nearest neighbour ordering method."}, {"title": "2 Hilbert curves", "content": "A space-filling curve is a surjective continuous function from the unit interval [0, 1] to a unit hypercube $[0, 1]^d$. Surjectivity implies that every point in the hypercube maps to at least one point in the interval, and continuity ensures that no areas in $[0, 1]^d$ are missed [5]. Space-filling curves are a special case of fractal curves, and are guaranteed to fill a continuous space in the limit.\nOur interest in space-filling curves derives from two useful properties that they offer, namely (1) uniform coverage of a bounded d-dimensional space, and (2) the ability to provide a unique mapping between points in the d-dimensional space and points on the 1-dimensional curve [31]. Specifically, this study considers the Hilbert space-filling curve, first proposed by D. Hilbert in 1891 [10]. The Hilbert curve is defined recursively, and can be constructed through a limit process of iteration. Each successive iteration creates an approximation of the true Hilbert curve that passes through more points in the d-dimensional unit hypercube. For practical purposes, the number of iterations is chosen to be finite, and is further referred to as the order of the Hilbert curve.\nConsider the construction of a Hilbert curve in 2D. For the first iteration, a line on the closed interval [0, 1] and a square $[0, 1]^2$ are taken. Four equidistant points are selected on the line, where the starting point is at 0 and the end point is at 1. The square is subdivided into four equal parts. Intervals of the line connecting each pair of points are then mapped onto the square such that the intervals adjacent on the line share a common edge on the square. This results in a simple U-shape, illustrated in Figure 1(a). At each subsequent iteration, the curve from the previous iteration is divided into four equal parts. Each part is then shrunk by a factor of 1/2, rotated, and repositioned such that the four curves connect at their endpoints in a U-shaped or reverse U-shaped pattern. Figure 1 shows 2D Hilbert curves of order 1 to 3.\nThe Hilbert curve is bijective, i.e. the mapping between points on the curve and a d-dimensional space is reversible. Bijectivity of space-filling curves has been useful in organising multi-dimensional data storage and retrieval systems,"}, {"title": "3 Hilbert curves as samplers", "content": "Given a Hilbert curve of a particular order, the vertices can be used as a basis for a sample in the associated multi- dimensional space. When sampling for ELA, it is common practice to use sample sizes of $10^2 \\times d$ to $10^3 \\times d$ [25]. \nSince the generation of a Hilbert curve is deterministic, the following two strategies were evaluated to introduce stochasticity:\n\u2022 Selecting random points along the edges of the Hilbert curve: Given any two sequential vertices, $P_i$ and $P_{i+1}$, a new point is selected $P_j = rP_i + (1 \u2212 r)P_{i+1}$, where $r \\sim U(0, 1)$.\n\u2022 Selecting points near vertices of the curve: For each vertex $P_i$, generate a new point drawn from a normal distribution centred on $P_i$ with a standard deviation $\\sigma$ (constrained by the bounds of the search space). The step size of the Hilbert curve before scaling is 1, so $\\sigma$ was empirically chosen to be 0.3 to prevent excessive potential overlap with points generated by neighbouring vertices.\nNeighbouring vertices on a Hilbert curve are equidistant, so the sequence of vertices has a constant step size. The two randomisation techniques introduce variation into the step size. For all further experiments we have used the randomisation around vertices strategy."}, {"title": "3.2 Search space coverage", "content": "We now investigate the extent to which Hilbert curve sampling covers the search space compared to competing strategies such as Latin hypercube sampling (LHS). To investigate the search space coverage, the following three strategies were used to draw samples from search space [-5, 5]d of sizes $n \\in \\{100d, 316d, 1000d\\}$ for dimensions $d \\in \\{5, 10, 20, 30\\}$:\n1. Hilbert curve: randomly select the required number of points from the curve (without replacement).\n2. LHS: generate the required sample using a Latin hypercube design [23].\n3. Random walk: use a simple random walk [22] with maximum step size of 1 to generate the sample.\nFor each sample size and dimension combination, a random uniform sample was drawn as a reference set. Thirty independent runs of each sampling strategy were implemented, and for each run, the Hausdorff distance [9] to the reference set was calculated. The Hausdorff distance as a measure of search space coverage was first proposed and investigated by Lang and Engelbrecht [15], and their methodology is followed in this study. Statistical significance tests were performed for each sampling strategy as proposed by Derrac et al. [3] and described by Lang and Engelbrecht [15].\nTable 2 shows that the Hilbert curve and LHS have similar Hausdorff distances, implying a similar coverage of the search space, whereas the random walk provides the worst coverage. Lower values were achieved in most cases by the Hilbert curve sampler, confirmed by the statistical significance tests presented in Table 3. The null hypothesis is that there is no significant difference between the Hausdorff distances for each of the sampling strategies. Results show that the null hypothesis can be rejected, given the p-values and a significance level of 0.05. We therefore conclude that the Hilbert curve sampler achieves a more uniform search space coverage than LHS."}, {"title": "3.3 Computational cost", "content": "We now compare the cost of the Hilbert curve as a sampler to alternative sampling strategies. Generating the Hilbert curve is not computationally cheap, but we expect that this upfront investment will be justified if the sample is subsequently used to calculate landscape metrics that require ordered samples."}, {"title": "3.4 Predictive performance of Hilbert curve samples", "content": "We have shown that Hilbert curve sampling provides an efficient method for generating spatially correlated sequences of solutions from continuous search spaces, and that these sequences provide good coverage of the search space. We now investigate whether the landscape features extracted from these samples provide good predictive performance for algorithm selection.\nFollowing the approach by Mu\u00f1oz et al. [25], we use the task of predicting the class of each BBOB function as a proxy for algorithm selection: if the landscape features of function instances can be used to discriminate between problem classes, then they should also be effective predictor variables for algorithm selection. Five target classes corresponding to the BBOB function groupings as outlined in [8] were used, namely (1) separable functions: $\\{ f_1 ... f_5\\}$, (2) functions with low or moderate conditioning: $\\{f_6 ... f_9\\}$, (3) unimodal functions and functions with high conditioning: $\\{f_{10}... f_{14}\\}$, (4) multimodal functions with adequate global structure: $\\{f_{15} ... f_{19}\\}$, (5) functions with low or moderate conditioning: $\\{f_{20}...f_{24}\\}$.\nThe experimental setup is as follows: 24 BBOB functions in dimensions $d \\in \\{2, 5, 10, 20\\}$ are used, with an evaluation budget of $1000 \\times d$. For each combination of instance and dimension, 1 000d samples are drawn from [-5, 5]d, using three sampling strategies: Hilbert curve (HC), Latin hypercube (LH) and random walk (RW). Four feature sets from the pflacco package were selected that did not require further function evaluations, namely: dispersion (disp), ELA y-distribution (ela_distr), ELA meta model (ela_meta), and information content (ic).\nTable 4 gives the testing accuracy of a decision tree, k-nearest neighbour, and random forest classifier for the task of predicting the function class from the landscape metrics. The scikit-learn version 1.1.3 default settings were used in all instances. Results show that a random forest model was the most effective at predicting the function class across all sampling strategies. Overall, the RW strategy emerges as the least competitive performer, while the Hilbert curve and Latin hypercube sampling strategies were comparable for all three classifiers."}, {"title": "4 Hilbert curves as an ordering tool", "content": "In this section, we investigate the use of Hilbert curves as an ordering aid for samples generated from other sampling methodologies. A Hilbert curve of order $p$ maps each point on a 1-dimensional curve $[0,..., 2^{dp}]$ to a point in $[0,..., 2^p - 1]^d$ and vice versa, as illustrated earlier in Figure 1. The mapping is bijective, and allows for an efficient mapping from a point in the d-dimensional space to a point on the 1-dimensional Hilbert curve.\nThis d-D to 1-D mapping can be exploited, given that a Latin hypercube sample of $n$ points divides each axis into $n$ intervals. By selecting the appropriate curve order $p$, i.e. $p = [log_2(n + 1)]$, we can ensure that the resulting Hilbert curve is of sufficient length to accurately map all LHS points to points on the Hilbert curve. The ordering of the points on the Hilbert curve can then be used to provide a spatially correlated ordering of the Latin hypercube sample.\nGiven that the current best practice of using a greedy nearest neighbour strategy to order LHS requires calculating at least half of the pairwise distance matrix between all points in the sample, the Hilbert curve ordering strategy has the potential to be significantly faster. Furthermore, the nearest neighbour strategy tends to start with relatively short step sizes, which increase as the number of unvisited points decrease. Hilbert curve ordering is likely to provide more consistent step sizes."}, {"title": "4.1 Step size consistency", "content": "Figure 5 shows a visual comparison of the Hilbert curve and nearest neighbour ordering strategies on a 2D Sphere function sample. The underlying sample X for both strategies is identical, and was obtained using LHS. In the figures the orderings produced by the Hilbert curve (HC) and the nearest neighbour (NN) approach are depicted as a red line. Colour scale is used to indicate the fitness values of the sampled points. It is evident from Figure 5 that the maximum step size is smaller for HC than for NN. The step size distributions shown in Figures 5(c) and 5(d) confirm that the maximum step size of HC is lower, and show a greater skew in the step size distribution for NN."}, {"title": "4.2 Computational cost", "content": "To evaluate the computational cost of the Hilbert curve ordering, we compare it to the nearest neighbour and random (RND) ordering strategies applied to a sample generated using LHS. The Latin hypercube samples of sizes $n \\in \\{100d, 316d, 1000d\\}$ were drawn from search space [-5, 5]d for dimensions $d \\in \\{5, 10, 20, 30\\}$.\nFigure 6 shows the performance comparison between the various ordering strategies. Hilbert curve sampling as discussed in Section 3 is included for completeness, as it does not require additional ordering. From Figure 6, NN ordering performs the worst, especially for large sample sizes. Furthermore, RND sorting and Hilbert curve sampling are equally fast (red and blue curves overlap). Hilbert curve ordering produces an intermediate result, slightly slower than random ordering, but significantly faster than the NN ordering. Since the underlying sample is generated by LHS, Hilbert curve ordering retains all the benefits of LHS for ELA features insensitive to order (such as ela_meta, or disp)."}, {"title": "4.3 Evaluation of features generated using Hilbert curve ordering", "content": "We now compare the information content features generated from a Latin hypercube sample using the HC, NN, and RND ordering. Since the feature values are evaluated rather than the computational cost, this experiment was restricted to dimensions $d \\in \\{2,5\\}$ and sample sizes $n \\in \\{100d, 1000d\\}$. A single sample set X was generated and evaluated on each of the BBOB functions to generate corresponding y = f(X) results. Information content metrics were then calculated on these (X, y) pairs using each of the ordering strategies. This procedure was repeated 30 times.\nFigures 7 and 8 present initial partial information ($M_0$) and maximum information content ($H_{max}$) landscape features based on the entropy of the sample. We consider BBOB problems in 5D and calculate feature values based on NN, RND, and HC ordering for n = 1000d.\nIt is evident from Figure 7 that the HC ordering produces values that lie between RND and NN, but follow a pattern similar to that of NN. It is clear that $M_{RND}$ converges to the same value regardless of the underlying function (confirming the results of [25]). $M_{NN}$ and $M_{HC}$, on the other hand, produce values that allow for discrimination between functions. Since larger values for $M_0$ indicate a more rugged landscape (more changes in concavity over the length of the walk), the HC ordering would on average indicate a more rugged landscape than the NN ordering.\nAccording to Figure 8, $H_{RND}^{max}$ varies over a wider range than $M_{RND}$. However, the $H_{RND}^{max}$ values still tend to cluster around the upper edge of the graph, closer to the maximum value of 1. Thus, HC ordering again indicates a more rugged landscape than the NN ordering.\nTo evaluate the relative saliency of the features produced by the various ordering strategies, permutation-based feature importance technique [4] is employed. We train a random forest (RF) classifier using information content features as input and the function class as output\n(as defined in Section 3). Feature saliency is gauged by randomly permuting each input feature in turn across the dataset, and observing the corresponding reduction in RF accuracy. For this purpose, the features are grouped by the ordering strategy, and a RF is trained on 3 of the data, with the remaining held out for testing.\nAfter training, the base accuracy of the RF classifier is evaluated on the held out data. The accuracy of the RF classifier is then re-evaluated as each feature in the held-out dataset is randomly permuted in turn, and the difference to the base accuracy is recorded. This procedure is repeated 10 times for each sampling strategy. Table 6 shows that $e_s$ was the most salient feature across all sample orderings, with the RF trained on the HC ordered features relying on $e_s$ more heavily than the RF trained on the NN or RND ordered features. RND sample ordering made the $M_0$ feature near useless for the classifier, in line with the observations made in Figure 7. Conversely, RF trained on the NN ordering relied on $M_0$ more than RF trained on the HC ordered features. Overall, HC ordering yielded marginally better RF performance compared to the NN and RND ordering approaches."}, {"title": "5 Conclusion", "content": "This paper proposed the use of Hilbert space-filling curves in the context of optimisation problem landscape analysis for the purpose of (1) sampling the search space in a spatially correlated manner that also guarantees uniform coverage, and (2) spatially ordering samples generated using other sampling algorithms such as the Latin hypercube. Experiments were conducted to evaluate the relative computational efficiency of the Hilbert curves, as well as the saliency of the landscape features extracted using Hilbert curve sampling and Hilbert curve ordering. In the context of sampling, Hilbert curves were significantly faster than Latin hypercube sampling for the purpose of generating order-sensitive features such as the information content metrics. Features extracted by the Hilbert curves were informative, and allowed for successful discrimination between problem classes. As an ordering tool, Hilbert curves performed significantly faster than the commonly used nearest neighbour ordering, and also yielded salient landscape features. Thus, Hilbert curves present a viable computationally efficient alternative to both Latin hypercube sampling, and nearest neighbour ordering of a sample for the purpose of landscape analysis."}]}