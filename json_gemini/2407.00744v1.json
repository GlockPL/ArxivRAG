{"title": "Disentangled Representations for Causal Cognition", "authors": ["Filippo Torresan", "Manuel Baltieri"], "abstract": "Complex adaptive agents consistently achieve their goals by solving problems that seem to require an understanding of causal information, information pertaining to the causal relationships that exist among elements of combined agent-environment systems. Causal cognition studies and describes the main characteristics of causal learning and reasoning in human and non-human animals, offering a conceptual framework to discuss cognitive performances based on the level of apparent causal understanding of a task. Despite the use of formal intervention-based models of causality, including causal Bayesian networks, psychological and behavioural research on causal cognition does not yet offer a computational account that operationalises how agents acquire a causal understanding of the world. Machine and reinforcement learning research on causality, especially involving disentanglement as a candidate process to build causal representations, represent on the one hand a concrete attempt at designing causal artificial agents that can shed light on the inner workings of natural causal cognition. In this work, we connect these two areas of research to build a unifying framework for causal cognition that will offer a computational perspective on studies of animal cognition, and provide insights in the development of new algorithms for causal reinforcement learning in AI.", "sections": [{"title": "1 Introduction", "content": "Causal cognition, the ability to acquire and exploit causal information about oneself and the world, is a core aspect of adaptive and intelligent behaviour, in non-human and human animals [62, 65, 149, 203]. At the same time, in recent years it has become apparent that artificial systems displaying various forms of seemingly intelligence behaviour still fall short of performing at the level of the majority of non-human animals that showcase various kinds of causal cognitive abilities [38, 124, 162, 195]. It has thus been suggested that an understanding of the mechanisms of causal cognition will play a crucial role in cognitive science and artificial intelligence for the next decades [67, 75, 124, 126, 129, 162, 180, 195].\nThe study of causal cognition both in human and non-human animals has a long history, with roots in behavioural studies trying to establish the extent to which an organism's behaviour reflects proper causal understanding of the world instead of simpler forms of associative learning [63, 65, 70, 163, 202, 203, 206, 220]. Some of the most influential studies in this area have combined theoretical and modelling work based on the formalism of causal Bayesian networks to account for the cognitive performance of various subjects in tasks designed for testing causal cognition abilities [63, 64, 66, 70, 71, 220]."}, {"title": "2 Causal cognition", "content": ""}, {"title": "2.1 Causal cues and the debate on associative vs. cognitive explanations", "content": "Early work on causal cognition in human and non-human animals focused on how subjects learn about the strength of cue-reward relationships, where some of the given cues could be attributed the \"causal power\" of eliciting rewarding outcomes [32, 113, 206]. A significant part of this early work can be contextualised within an old debate on whether causal learning is just a form of associative or contingency learning, the dominant theoretical framework to study animal learning [172], or a form of learning that requires more cognition-laden processes.\nThe crux of this debate was not whether there are causal relationships or structures in the world, which is more a philosophical type of issue. Granted that there are, associative accounts have usually tried to show that the successful performance of some agents in seemingly causal learning tasks can be modelled, and ultimately explained, by means of purely associative learning mechanisms. Roughly, these would track the relevant causal associations between certain variables without the need of invoking more sophisticated cognitive processes or structures involving a notion of causality"}, {"title": "2.2 Causal understanding as a building block for causal cognition", "content": "Moving past the associative vs. cognitive debate using a more comprehensive definition of cognition at different levels has however brought forward a perhaps more fundamental disputes about the presence, or not, of forms of causal understanding in agents, and what such an understanding amounts to. This is especially evident in the behavioural research on causal cognition in non-human animals, where the goal is to design behavioural tasks specifically intended to try and measure some manifestation of causal understanding [206]. In other words, tasks that would ascertain whether a subject is capable of the feats of causal cognition, where the assumption is that a solution of the task requires certain causal, cognitive strategies.\nAn example of this research is represented by studies on capuchin monkeys using the trap-tube task [239, 241-243], where causal cognition is characterised as the comprehension of key cause-effect relationships within the task. The trap-tube task consists in pushing a food reward out from a transparent tube (anchored to the floor) using some kind of tool (e.g. a stick), by inserting it into one of the tube's two openings, see fig. 2b. In general, capuchin monkeys struggle to learn how to solve the task, either because they would pick the wrong kind of tool (a stick that could not be inserted into the tube because of its shape) or because they would pick the wrong side to put the stick, making the reward fall into a trap positioned underneath the tube.\nThe persistent error patterns of the (few) subjects that could solve the task after extensive trial and error are thought to be evidence of a distinction between 1) successful performance based on a \u201cstroke of luck\u201d after extensive active experimentation, and 2) successful performance based on an understanding of relevant causal variables inherent within the task requirements [239, 243]. It is in fact well known that capuchin monkeys have a propensity to produce a wide variety of actions and complex combinations thereof, even involving tools, to the point that they could be described as expert tool-users. Because of this, it is unclear whether they have an appreciation of the causal relationships between their behaviour and the resulting outcome. In other words, they might learn that using certain tools is an effective way to achieve certain results, but they may not appreciate the reasons for why their actions are successful [239].\nIn contrast, experimental evidence in chimpanzees suggests that they may have an understanding of the causal relationships between certain actions and their associated outcomes [134, 155, 186]. The key finding here is that some subjects, tested with different configurations of the trap-tube task, were able to select the right side of insertion (almost) immediately, allegedly displaying an ability to plan their actions according to the different causal relationships present in the task configuration. Consequently, this evidence suggests that the successful subjects were not using heuristics such as a distance-based rule, which would for example determine the correct action based on the distance of the reward from the tube's openings without an understanding of the causal structure of the problem. Instead, subjects appeared to take into account the causal features of the task configuration and choose beforehand what action to perform. This would thus amount to a representational strategy that delineates the key requirements of the task in advance and results in the correct behaviour without the need of extensive trial-and-error learning. More specifically, one could argue that those successful chimpanzees exhibited some kind of causal understanding of the consequences of pushing the stick inside the tube (but see Martin-Ordas et al. [148] and Seed et al. [186] for opposing views). Unlike for instance the capuchin monkeys of other experiments [239, 243], where a constant monitoring of the effects of one's ongoing action (to check one is on the the right track) and attempting a variety of actions' combinations (in the hope to find the right one) was instead unnecessary."}, {"title": "2.3 Causal interventions and tool use", "content": "A strong candidate for the presence of plans based on action-driven outcomes is the ability to produce a causal intervention, an action that involves a causal control on a particular effect [240]. On the one hand, this seems to provide strong evidence for causal cognition since producing an intervention requires some form of causal understanding. In particular, it requires an agent to understand that its actions, in the form of movements of its own body, could be used as external probes for the causal texture of the world (cf. second rung of the causality ladder in Pearl et al. [162]).\nOn the other hand, the attribution of causal interventions to cognitive agents appears still controversial because there are only limited reports that hint at intervention-like abilities in, for instance, rats [20, 128] and primates [62, 244]. At the same time it is unclear whether tool-use, the ability to skilfully manipulate objects, common in species like corvids [102, 218], should count as a form of intervention or not. In general, it is not entirely obvious what the markers of causal interventions are and how to design experiments that could determine their presence or absence.\nWork on rats, for example, suggests that these animals can learn a common-cause model, where a light being turned on is perceived as the cause of two effects, a noisy sound and the release of some food. After exposure to patterns of causal relationships for a certain number of (training) trials, the rats enter a test condition characterised by a lever that produces a noise when pressed. Interestingly, it has been reported that after (accidental) lever presses, rats exhibit a less resolute search for food (measured by the number of nose-poking in the cage's hopper) than when the noise is presented alone. A possible explanation for this behavioural response would regard these rats as capable of recognising their action (the level press) as an intervention, an independent self-generated perturbation on one variable of the learnt causal model. In fact, an effect (noisy sound) cannot be an indication that a cause is present (light) when that effect is produced by an intervention (lever press). Therefore, by conceiving of their action as an intervention on one variable of the learnt causal model, the rats do not expect that the other effect (food release) will occur, which then induces a less vigorous search for food [20]. While these findings are consistent with the claim that rats can differentiate between predictions based on observations and predictions based on interventions, they do not exhaustively prove that rats can produce interventions to activate a certain causal path, in this case the one leading from the light to the food dispensation (as discussed by Blaisdell et al. [20] themselves).\nWork on corvids on the other hand, see for instance Taylor et al. [216], testing New Caledonian crows with a few variations of the trap-tube task (see fig. 2b), suggests that they possess critical causal understanding abilities, e.g. an appreciation of causal relationships involving object-hole interaction, on which their exceptional tool-using skills might be built. Similarly, Jelbert et al. [105] and Logan et al. [140] present experiments on a task (inspired by Aesop's fables) in which crows have to learn to drop some objects (e.g. stones) into the right water-filled tube so that the water displacement brings a floating reward (e.g. a piece of meat) closer to the tube opening (see fig. 2a). The results here point at the fact that the birds managed to solve the task, seemingly by attending to the relevant causal information, e.g. the fact that larger and not hollow objects will produce a bigger water displacement. For a variation of the task however, where the setup instead consisted of three water-filled tubes arranged in a row on a wooden board, with some space between each other, results were less clear. In this task, the baited tube is the one in the centre and, crucially, it is connected with one of the others by means of a U-shape tube hidden from view (located underneath the board). Dropping objects into one of the lateral tubes will have as an effect a water-level rise in the baited tube. Since the central tube is too narrow to drop anything in it, to bring the reward on the surface it is crucial to recognise this counter-intuitive effect and exploit it, i.e. to infer the presence of and reason about hidden causal mechanisms. Here, all tested birds performed at chance level, meaning that they dropped objects randomly on either of the two lateral tubes [105], see also Logan et al. [140] for similar conclusions on a slightly modified setup."}, {"title": "3 A Conceptual framework for causal cognition", "content": "After decades of research in the field, by now it is evident that different works on causal cognition have often appealed to different conceptualisations of the subject matter, to the point that a consensus has yet to be formed on what even constitutes causal understanding, see Penn et al. [163] and Sloman et al. [203] for some reviews, and the contributions to Gopnik et al. [65] and McCormack et al. [149] for other perspectives. As briefly illustrated by the previous section, different lines of work place causal understanding at different levels of a hypothetical cognitive spectrum, and have portrayed very diverse views on how to characterise it in terms of key cognitive functions and behavioural outputs. Most researchers might agree on the idea that a causal agent has the kind of behavioural flexibility that is unattainable by agents lacking causal understanding. Yet, the variety of positions trying to describe what underpins it appears to only contribute to the confusion.\nSome works point to a representational strategy for agents to picture in advance what the key causal features for solving a task are. This would then characterise a distinction between performing and understanding, i.e. whether the solution of a task is achieved via some sort of shortcut, or in a robust and reliable way [134, 239, 241, 243]. Others are more demanding, and see causal understanding as the result of some form of causal reasoning, yet another ambiguous expression that has been described in different ways. For instance, causal reasoning could involve structural or symbolic (causal) knowledge abstracted from perceptual cues [149, 168, 187], or in other words, the ability to search for cause-effect relations that could reveal how and why two events are connected, or why some actions lead to certain outcomes (i.e. diagnostic causal reasoning), requiring thus the presence of some causal beliefs [45, 240]. Others would further maintain that without an ability to perform causal interventions, perhaps even involving unobservable entities (see hidden causal mechanisms in section 2.3), it is unlikely that an agent is able to grasp causality as opposed to just behave as if it does. Going back to tool-use then, the question of whether adaptive tool use may reveal the presence of some of the abilities just described or whether it may be a confounder instead [102, 186, 218] remains unanswered.\nIn a recent attempt to put causal cognition research on a more precise and coherent footing, we find different proposals discussing experimental findings framed with respect to a few recurring themes, drawing attention to key aspects of causal cognition [208, 262-265] (see also Goddu et al. [62] for a recent review). Starzak et al. [208] in particular dissect the main disagreements over causal understanding in non-human animals, proposing a more precise way"}, {"title": "3.1 The explicitness of causal information", "content": "The explicitness dimension, refining some intuitions presented in Woodward [262], aims to capture a spectrum of causal information where on the one end, implicit models are essentially blind to causal relationships. These models represent cases where actions and outcomes/rewards are entangled or \"fused\" [262], i.e. models based on an associative correlations where the causal structure (see the web of causal possibilities in section 2.2) is essentially hidden and inaccessible to the agent. In this class of models, agents cannot necessarily come up with a complex plan on how to adjust their actions that is sensitive to the web of causal possibilities in order to achieve a certain goal, since they lack or have a limited understanding of their own actions and other variables in the environment as causally relevant for bringing about certain outcomes or rewards [262]. They can however take actions in a less structured way, for example using knowledge acquired from repeated trial-and-error in an associative manner, leading to a continuum of explicitness that is apparent in several experimental studies as seen in section 2. For instance, associatively pairing the action of pressing a button (cause) with the presence (very often, but not always) of some food (end goal, an effect) can be considered as an example of implicit model.\nOn the other side of the spectrum we find explicit models, models where the causal structure is completely unpacked and relations between actions and outcomes/rewards are disentangled and available for an agent to take advantage of. Looking at the previous example, we can imagine a different scenario where an agent realises that a button press will activate a food dispenser mechanism (some intermediate variable) and that the food will become available if and only if there is no obstructing object in the mechanism. In this case, the action of clearing the dispenser from the obstructing piece is an action that can be said to require a more explicit understanding of the causal structure of the world, at least compared to the first situation, an operation that is directed at altering one of the intermediate causal variables, the object obstructing the food dispenser, so as to obtain the food.\nA qualitative description of explicitness thus amounts to establishing what an agent can do with the acquired causal information, for example by investigating an agent's degree of flexibility in using what is has learned in a causal task (e.g. clearing the dispenser mechanism of the obstructing object). To a first approximation, the key idea is that the more explicit a model is, the more causal information is available to an agent, because the means to reach a certain goal have been recognised as distinct from each other and from the goal itself (the mediating variables of a certain causal influence have been identified, cf. Pearl [161]), thereby leading to a higher degree of flexibility in behavioural responses.\nTo see how different degrees of explicitness appear in the animal cognition literature, we can take a closer look at the research on the trap-tube task described in section 2.2. Facing the trap-tube, an agent that can only form implicit models where actions and outcomes are entangled, i.e. where actions a leading to states $s$ are a-causally associated to outcomes $x$, has a very limited ability to discern the possibly relevant intermediate variables that could be exploited to reach the goal. These include for instance the position of the trap, necessary for an understanding of whether its opening (the hole) is on the tube's lower surface or not, i.e. whether it affects the desired outcome (recall that in the latter case the tube has been rotated so the trap is ineffective).\nAs the literature on these experiments suggests that most capuchin monkeys that were tested do not seem to appreciate the relevance of the trap, and consequently perform poorly when the tube is inverted, we could say that these agents rely only on implicit models of the form \u201cinsert the stick, out comes the reward"}, {"title": "3.2 Sources for learning causal information", "content": "The sources dimension of Starzak et al. [208] is also inspired by Woodward [262], where causal cognition is proposed to be best explained in terms of some key, distinct abilities, namely, egocentric and non-egocentric sources of causal information.\nEgocentric causal information captures the idea of an agent that can acquire an understanding of the causal structure of the world from its own behaviour, focusing on the performance of their own actions and how they can can reliably and robustly result in certain outcomes. For example, a subject could learn that performing action $a_1$ makes a difference for obtaining outcome $x_1$ but not for $x_2$. This is the realm of instrumental conditioning (or learning) investigated extensively in animal research [262].\nNon-egocentric causal information can, on the other hand, be obtained from two main sources: the behaviour of other agents and the unfolding of natural events. Social causal information implies that an agent can learn about important action-outcome contingencies by paying attention to other agents' behaviour. For instance, observations of a conspecific performing action $a_1$ and reliably obtaining outcome $x_1$, but not $x_2$, provide important causal information for a subject that is aiming at outcome $x_1$ (or $x_2$).\nNatural causal information similarly suggests that events in the natural world can disclose ecologically important causal relationships. For instance, observing a piece of fruit falling from a tree-branch shaken by the wind could reveal to an attentive observer important causal information on how to get some food, as long as it is capable of performing a causal analysis of the situation [233]. Compared to the acquisition of the previous types of causal information, natural information imposes, arguably, a higher cognitive load on the subject because the event in question does not tell the subject what action may be (causally) relevant and for what reason(s). In other words, there is an additional cognitive effort that the subject needs to make to appreciate that, given certain observations, action $a_1$ can produce outcome $x_1$. [208] remark that empirical evidence so far suggests that egocentric causal learning is the most widespread in the animal kingdom whereas social causal learning and observational causal learning (as they call the two more sophisticated forms of causal information acquisition) are fully present in adult human beings only. Following Woodward [262], they agree with the idea that in principle these forms of learning could dissociate but they also add that it is not clear the extent to which these abilities are independent from one another, or whether they form a hierarchy of cognitive processes."}, {"title": "3.3 The integration of causal information", "content": "Integration appears more ambiguously in Starzak et al. [208] but can be understood, in general, as consisting of operations of update, combination, extension etc. of one source of information with another one to form a coherent structure. More concretely, in our framework integration will be later framed in terms of meaningful combinations of different sources of information (egocentric, causal and natural) that can describe if and when agents are capable of"}, {"title": "4 A mathematical framework for causal cognition", "content": ""}, {"title": "4.1 Disentanglement in machine learning", "content": "A key proposal in modern approaches to deep (reinforcement) learning is that of disentanglement [16], roughly stating that in order to acquire an understanding of the causes behind some given observations, it is necessary to interpret those causes as distinct (high-level) factors, and recognise the different causal power they exert when giving rise to observations [182]. For example, if we see a red ball made of rubber bouncing on the ground, what makes it bounce? While different factors including colour, shape, and material, are intertwined and together produce observations captured by our eyes, some of these factors have no causal influence on the bouncing behaviour, i.e. colour. According to the disentanglement hypothesis, the ability to discern different factors is thus a crucial step in a theory of causal understanding.\nSimilarly to other influential proposals, disentanglement has been used to characterise a general intuition based, however, on different implementations and interpretations. Following Zhang et al. [278]'s review, we thus look at some of the common structure behind different definitions of disentanglement. To do so, we focus in particular only on sets and functions between them. This is technically equivalent to stating we are working in the category of sets and functions, Set [147], and while in various ways limiting, this allows us to focus on the central parts of a our proposal to connect disentanglement to explicitness down the line using a relatively simple mathematical toolkit (sets and functions, without introducing more advanced tools from category theory).\nFollowing [278] we start by defining $S, X, Z$ as state-spaces, represented as sets of (generative) factors, observations and codes respectively. The sets of factors and codes are further assumed to be (Cartesian) products of $n \\in \\mathbb{N}$ factors and $l\\in \\mathbb{N}$ codes:\n$S = S_1 \\times S_2 \\times ... \\times S_n$\n$Z = Z_1 \\times Z_2 \\times ... \\times Z_l$ \t\t(1)\nAt a high level, the main idea driving this framework is that representing distinct factors, i.e. having disentangled codes that faithfully map to disentangled factors, is the starting point for acquiring a causal understanding of the world: an agent with knowledge of what factors generated its observations is an agent that understand what data-generating mechanisms brought observed data to its sensory peripheries, see fig. 3 for a way to frame the example of the red ball in this initial setup, to be unpacked next. The setup for disentangled representation can then be captured, in a compact form but with more specific constraints to be imposed below, by the following commutative diagram:\nWe introduce next the formal definition of disentanglement that we will be referring to throughout this work, following Zhang et al. [278].\nDefinition 4.1 (Disentangled representations). A disentangled representation is a product of codes $Z_i$ for $i \\in \\mathbb{N}$ defined through the following:"}, {"title": "4.2 Causal representation learning", "content": "As highlighted by Zhang et al. [278], disentanglement has been described in various different ways across the literature. In one of the most influential accounts, disentanglement can be viewed as a component of causal models recovering the causal factorisation of a process generating a collection of observations of interest [182, 252]. In this view, disentanglement is thus a crucial part of the answer to the question of how causal model are acquired, providing a way to operationalise a process deemed necessary for an agent to learn a causal characterisation of the world it acts in [182, 252]. As we will see, taking this perspective allows us to relate explicitness, one of the dimensions of causal cognition (see section 3.1), to disentanglement, in light of the nascent field of causal machine learning.\nCausal machine learning is a collection of methods and applications based on the notion that exploiting causal information in data can lead to a more robust, accurate, and efficient kind of (data or system) modelling, thereby viewing causality as a fundamental notion to move past some of the limitations of machine learning methods based on statistical learning (from now on we will refer to these methods as \"traditional machine learning\") [107, 181, 227].\nWithin this line of research, the subfield of causal representation learning can be regarded as a way to recover disentangled representations from data. Traditionally, representation learning has been conceived as the task of learning a generative model in the form of a low-dimensional feature vector (codes) of high-dimensional data (observations) produced by a generative process whose features (factors) remain hidden. The idea driving this approach is that if those codes capture key, informative, aspects of a dataset, they would aid in solving downstream tasks (i.e. predicting a label) [16]. However, these models have often sidestepped questions regarding the origins of particular datasets, overlooking structural knowledge of the data-generating process that could have produced them. This in turn affects what a generative model can account for, often limiting its scope to only statistical correlations with little to no causal power.\nCausal representation learning extends these ideas by bringing into representation learning (and, more generally, deep learning) some of the principles, methodologies, and objectives of classic causal inference research [87, 161, 166], with the goal to learn a low-dimensional vector of causal codes from high-dimensional observations generated by causal factors [17, 107, 182]. In this paradigm, the data-generating process can be formalised as a structural causal model capturing the causal relationships between factors underlying the data distribution. Importantly, recalling the distinction between generative process and generative model, learning a generative model means to represent something about the generative process described as a structural causal model. In the best case scenario, a generative model recovering the full gamut of causal information assumed to exist in the generative process can itself be described as a structural causal model of the same form as the generative process. This particular scenario assumes however causal sufficiency, i.e. that there are no hidden common causes (also referred to as hidden confounders) on factors in the generative process,"}, {"title": "4.3 (Causal) Reinforcement learning", "content": "Reinforcement learning (RL) provides a natural avenue for ways to combine work from machine learning and decision making in agents [212]. Similarities between classical RL and causality have been put forward in previous works [57-59], however a clear-cut notion of causality appears to be missing [107]. Here we provide some background for standard RL implementations, which will be then placed in context and used once we overview recent work in causal RL in section 6.\nA typical reinforcement learning setup involves the definition of a problem in terms of (a model of) an environment represented by a (discrete-time) Markov decision process (MDP).\nDefinition 4.3 (Markov decision process (MDP)). A Markov decision process is a tuple $(S, A, T, \\gamma, r)$, where:\n*   $S$ is the state space\n*   $A$ is the action space,\n*   $T: S \\times A \\rightarrow P(S)$ is the transitions dynamics, such that for a given state $s$ and $a$, $T(s, a)$ gives a probability distribution of states $P(S)$ an agent can transition to from state $s$ while taking action $a$, often written as $P(S_{t+1} | S_t, a_t)$,\n*   $\\gamma \\in [0, 1)$ is called the discount factor,\n*   $r: S \\times A \\rightarrow \\mathbb{R}$ is the reward function, giving a reward every time a transition is taken.\nAlternatively, it is also common to define a problem as a partially observable Markov decision process (POMDP), where information of the environment is only indirectly available through some observations.\nDefinition 4.4 (Partially observable Markov decision process (POMDP)). A partially observable Markov decision process is a tuple $(S, A, X, T, M, \\gamma, r)$, where $S, A, T, \\gamma, r$ follow the definition of an MDP and\n*   $X$ is the observation space,\n*   $M: S \\rightarrow P(X)$ is the observation or measurement map.\nThe goal of agents in an RL setup is to select sequences of actions that maximise expected cumulative discounted reward, also known as expected return, based on past and current experiences acquired through meaningful interactions with the environment. Action policies representing sequences of actions are defined by the following\nDefinition 4.5 (Action policy). Given an MDP $(S, A, T, \\gamma, r)$, a policy $\\pi$ is defined as either\n*   a deterministic function $\\pi: S \\rightarrow A$, or\n*   a stochastic map $\\pi: S \\rightarrow P(A)$ assigning a distribution of actions to each state in $S$.\nFor a POMDP $(S, A, X,T, M, \\gamma, r)$, a policy $\\pi$ is usually defined instead as either"}, {"title": "5 A computational framework for causal cognition", "content": ""}, {"title": "5.1 Explicitness as degrees of disentanglement", "content": "One of the main practical instantiations of disentanglement originates with models built on the architecture of variational autoencoders (VAEs) [117, 118], where a disentangled representation is defined as one where single latent units (codes) of a VAE are independently responsive to single factors generating observations [89].\nFollowing the notation in section 4.1, the goal of a VAE is to learn, given a dataset $D$ of observations $X$, a probabilistic generative model that can approximate factors $S$ using hidden variables (codes) $Z$. To do so, in a standard VAE architecture we find two neural networks, aptly named encoder and decoder, see fig. 4. An encoder with weights $\\phi$ parameterises a distribution $Q_{Z|X}^{\\phi}$, while a decoder network with weights $\\theta$ parameterises a distribution $P_{X|Z}^{\\theta}$. The encoder plays the role of the function $f$ in definition 4.1, mapping observations to codes, while the decoder is a clever construction that corresponds to a map\nwhere $X'$ can be seen as reconstructions of $X$, i.e. observations that should be \u201cas close as possible\u201d to the original ones according to some measure, in this case given by the VAE optimisation function provided below. Notice that, since we only required $f$ to be injective for $g(S)$, $k$ need not be a function and thus is not well-defined for the simple setup of sets and functions we adopted in section 4.1. It is however a map that can easily be defined for more general"}, {"title": "5.2 Trajectories as sources of causal information in RL", "content": "In online learning an agent uses the current policy to perform an action in the (training) environment, which responds with a reward signal, at each time step. Trajectories of state-action $\\tau := [s_0, a_0, ..., s_T, a_T]$ or observation-action sequences $\\tau := [x_0, a_0, ..., x_T, a_T]$, often called histories, see section 4.3, can be stored in a replay buffer as sequences of tuples together with their respective rewards at each time step, e.g, $(s_t, a_t, r_t, s_{t+1})$ or $(x_t, a_t, r_t, x_{t+1})$, where one sequence corresponds to a trajectory. This information forms an agent's experience: the state the agent was in, $s_t$, the action it performed, $a_t$, the reward it collected, $r_t$, and the next state $s_{t+1}$ reached from $s_t$ by performing $a_t$ [135, 153, 154].\nConcretely, this experience can be used to obtain an estimate of the expected cumulative discounted reward in eq. (15) based on trajectories sampled from the replay buffer, which is regularly updated and acts as a rudimentary database of memories for the agent. To see this idea in action, we look at a popular class of approaches represented by actor-critic methods for which the approximate gradient of the RL objective (see eq. (15)), used to update policy parameters $\\omega$ is computed as follows [212]:\nwhere $N$ is the number of trajectories sampled from the replay buffer; $\\pi_{\\omega}$ is the current policy whose parameters $\\omega$ will be updated with the computed gradient;  is the discounted sum of rewards ($G_t$, cf. eq. (14)) evaluated for the (partial) sampled trajectory $n$ acquired from time step $t$ until the terminal state $T$, and  is a (state) value function that acts as a baseline with respect to the discounted return, defined as the expected return $J(\\omega)$ from a chosen state sampled from trajectory $n$ at time $i$, $s_{n,i}$, if a policy $\\omega$ is followed from that point onwards, i.e.\nIn eq. (22), it is common [40, 72, 183, 212] to approximate the discounted return , the sum of rewards obtained from a particular, realised trajectory $n$, with\nThis is the Q-function (or action value function) for the policy under consideration, quantifying the value of performing an action in a certain state, after which the policy is followed until the end of the episode. With this substitution, one defines the advantage $Adv^{\\pi_{\\omega}}$, specifying how much better it is to take action $a_n$ action $a_{n,i}$ as opposed to an average action\nthat is approximated using a critic neural network trained to estimate only the state value function from the reward (because the Q-function can be rewritten as the sum of the reward at the current state and the expected state value function at the next, i.e. ). The actor part is represented instead by the policy $\\pi_{\\omega}$, parameterised by a policy neural network that outputs the most"}, {"title": "5.3 Combining different sources of causal information in RL", "content": "Following the idea of a buffer containing stored trajectories representing experience to update avalue function or policy, integration can be interpreted as the ability of an agent to combine different kinds of experience into its own decision-making process, appropriately weighted based on context, task demands, origin, resources, etc. In principle, these kinds of experience can include single-source causal information, e.g. when an agent uses experience acquired at different points in time or in different environments/tasks, such as in multi-task or meta-RL (see Beck et al. [15]). However, in this context we focus on integration of different sources (see section 3.2), with the goal of highlighting synergistic forms of causal understanding that truly take advantage of the amalgamation of different causal perspectives. This allows us thus to delineate an operational, computational account of the notional idea of integration presented in Starzak et al. [208].\nIn computational terms, the question of how best to integrate and use information coming from different sources is a foundational aspect of offline RL, where the replay buffer can store trajectory data collected by any policy in a variety of virtual environments, more or less realistic, or from real-world tasks. For example, recent datasets for offline RL tend to include trajectories from experts (e.g. hand-designed controllers or human demonstrators), from other RL agents trained online in a certain domain, from the same agent operating in the same environment but performing slightly different tasks (multi-task, past experience), from unsupervised (i.e. reward-free) exploratory policies [54, 73, 271, 281].\nThe challenge of designing an offline RL algorithm is precisely that of exploiting the collected data in such a way that the learned policy can be safely applied to a given environment. This means that the learning algorithm has to acquire and integrate causal information from various (PO)MDPs (those in the training set) in such a way that the most appropriate actions for new downstream tasks/environments can be extrapolated successfully from past experience and generalised into unfamiliar contexts. A vanilla approach consists of using importance sampling, originally tailored for dealing with off-policy data (see previous section) [106, 108, 120, 164, 169, 212].\nIn this context, importance sampling corresponds to the introduction of importance weights, ratios (computed over a trajectory) between the current policy to be optimised, $\\pi_{\\omega}$, and a behavioural policy, $\\pi_{\\rho}$, used to collect the transitions sampled from the replay buffer\nThese weights are then used in eq. (22), obtaining:"}, {"title": "6 Bringing together causality in natural and artificial agents", "content": "Recent work in (deep) reinforcement learning, in the area now called causal reinforcement learning, can help us shed light on ways to translate algorithms from machine learning into a more systematic study of causal learning agents. Using this line of work, we thus review a series of algorithmic implementations and models from causal RL and place them on a spectrum of increasingly high disentanglement, providing a comparative analysis with empirical and conceptual works in the animal cognition literature, see fig. 7. This will in turn suggest a more concrete connection with the explicitness dimension of Starzak et al. [208]'s framework, paving then the way for an understanding of causal information from different sources and possible strategies to integrate them sensibly."}, {"title": "6.1 Explicitness of causal representations", "content": ""}, {"title": "6.1.1 Weak disentanglement", "content": "At the lower end of the explicitness spectrum (see fig. 7), we find agents of traditional (non-causal) deep RL setups that are successful at solving a variety of narrow tasks by engaging in forms of dense learning, meaning that they often appear to learn at least some of the dependencies between their actions and desired outcomes/rewards [76, 154, 198, 212], and in some cases they are augmented with more sophisticated forms of planning, curiosity-based exploration and the ability to achieve a variety of goals in high-dimensional environments [51, 78, 79, 111, 151, 160, 254]. Nonetheless, the web of dependencies learned by these agents are usually dense because, as dependencies that are associative in nature, they include spurious features and/or relationships. In other words, dense learning in these agents goes hand in hand with a lack of causal information processing. These agents are in several ways akin to animal subjects engaging in instrumental learning [148, 170, 171, 214, 239, 241, 243] (see section 2), except for the amount of data samples used during training.\nTo have a better understanding of algorithms and empirical results higher in our explicitness scale, and their relation to weak and strong disentanglement, it is then useful to look at more specific features of causal representations. In classical RL, particularly when the problem is presented as a POMDP, a representation can be understood in two different ways:\n*   in model-free RL, these are representations of factors (i.e. codes) given as inputs to a policy, $\\pi(a|s)$, i.e. the state representations (implemented as vectors of state variables) fed to the policy network to produce an action, while\n*   in model-based RL, the term representation points at a model of the transition dynamics (involving, in turn, the state representations) $P(s_{t+1}|s_t, a_t)$.\nBased on this distinction, we suggest that there are two different ways to understand what a causal representation involves in causal reinforcement learning: 1) in causal model-free RL, a causal representation describes a particular encoding, often a compression, of the observations into latent states (with a causal flavour) while 2) in causal model-based RL, a causal representation models both the latent states and the causal dynamics of the environment. Here we suggest that the first kind of causal representation has a lower degree of explicitness than the second one, since it fails to capture the causal dynamics of the environment. We thus view it as a possible example of weak disentanglement. The latter instead is defined precisely in terms of its ability to map causal dynamics and is thus an example of strong disentanglement (section 5.1).\nIn causal model-free RL, in particular for a partially observable setting, an agent is said to learn an explicit causal representation if it can map high-dimensional observations to state representations that are disentangled, uncovering codes that represent some parts of the causal structure (the causal factors) of the data-generating process, but not including a complete disentangled representation (see section 5.1). Thus, an agent can be said to exploit this (partial) causal information if such information facilitates policy learning or has a positive impact on policy execution when the causal representation is fed to the policy network."}, {"title": "6.1.2 Strong disentanglement", "content": "In causal model-based reinforcement learning the agent is specifically trained to learn a world model, a model of the dynamics of the environment, then used for planning and decision-making (i.e. selecting the next action). In this context, causality-inspired approaches involve revealing and exploiting more causal aspects of the (modelled) environmental dynamics, regarded as crucial for having more capable learning agents that, for instance, do not fall prey to spurious correlations like agents with less explicit models might.\nTo achieve this, we have attempts to handle confounders, hidden common causes that can have an impact on factors and their state transitions (see section 4.2), by deconfounding the dynamics of a POMDP. Practically, this entails modelling state-transitions as affected by confounders, whose presence is either assumed from the start [24, 123, 132, 277], or can emerge from initially unaccounted parts of the dynamics/predictive model through a process of decomposition of observations into confounding and relevant state information [173, 251]. This leads to more explicit models because the effects of confounding factors are isolated to obtain a more robust understanding of how events in the environment unfold.\nMore in detail, we can look at Li et al. [132] as an example of the first kind (known confounders), based on object-centric learning (using graph-neural networks) combined with a model of the transition dynamics that is assumed to be confounded by time-invariant hidden variables, e.g. the object's masses, friction coefficients, etc. The goal of the agent here is to solve a POMDP, but this requires learning a generative model that is deconfounded, estimating the confounding variables for each object (using tools from do-calculus [161, 162]), which in turn can be used to generate accurate observation trajectories had the initial conditions been different (e.g. the objects' position). Effectively, this causal world model enables a kind of future counterfactual planning that starts with the question of what would have happened under alternative initial conditions, i.e. given an intervention that changes the starting states. On the other hand, for the second group (unknown confounders) we can consider Rezende et al. [173] where agents with partial models, i.e. models learnt using past actions and the initial agent's state as opposed to the full trajectory of past observations, are shown to be less robust to policy changes. These partial models are in fact confounded by past observations, which are not used to train the model but do anyway influence the policy picked by the agent, but can be adjusted for such confounders by using once again techniques from do-calculus.\nIn the animal cognition literature, understanding the influence of potential confounders can be linked to an appreciation of causal unobservables, such as in crows adjusting their actions depending on changes in experimental variables that are not visible to them [104, 217]. In one study, crows were tested on task consisting of extracting some food from a box, placed on a table and in front of a curtain. From behind the curtain, a human could operate a wooden stick that through a hole in the curtain could come close to the food box, therefore causing trouble for the crows trying to reach the food. The presence/absence of the human thus confounds whether it is \"safe\" to go and retrieve the food from the box (because in principle a stick's movement does not create danger, unless it is intentionally used to poke through a hole, for example by a human experiment), therefore it would be useful to be able to reason about what is behind the curtain. The evidence reported by Taylor et al. [217] suggests that crows can attribute the movement of the stick to a hidden agent behind the curtain and act accordingly, e.g. being more cautious when they do not observe anyone leaving the experiment's room (because the stick could move again). Similarly, in a context where a food dispenser is activated by means of placing objects on it and where an object's weight confounds the food release (only heavy enough objects activate the dispenser), crows can learn to infer the weights of the objects from their movements in a breeze and pick the appropriate ones to get the food from the dispenser [104]. In both studies, the animal subjects were able to adjust their behaviour by paying attention to the reward dynamics, i.e. to whether narrow or wide tube were to be preferred (according to the respective water level), or to whether light or heavy objects were activating the dispenser.\nBeyond confounding factors, model-based reinforcement learning can be improved by observing that key causal relationships in the environment, relevant for solving a particular problem, do not involve all state variables and transitions among them. That is, the causal dependencies among variables in the environment that an agent can have"}, {"title": "6.2 Sources of causal information", "content": ""}, {"title": "6.2.1 Egocentric causal information", "content": "Successfully learning from online interactions in an environment implies appreciating, to some extent, the relevance of certain action-outcome contingencies for reward maximisation or reaching a certain goal. Learning from online interaction amounts to instrumental learning, which chiefly involves egocentric causal information (see fig. 8) and has"}, {"title": "6.2.2 Social causal information", "content": "As already mentioned in section 5.2, artificial agents can be designed to learn to solve a task through imitation learning, e.g. by relying on demonstrations of the expected behaviour for the given task. The imitation learning pipeline can be implemented in various ways, tailored to the specific domain of application (for a recent review of the main techniques, see Hussein et al. [98]). In RL, the general idea is to allow a learning agent to have access to the experience of an expert, i.e. trajectories of optimal interactions for solving the task at hand, which are conveniently pre-processed in the same representational format of information in the replay buffer, so that they can guide the learning process towards a policy that achieves similar rewards [92, 133, 176]. While broadly successful, imitation learning approaches do not necessarily entail the processing of social causal information in a comparable way to natural agents. Imitation learning in and by itself does not in fact prevent a learning agent from simply exploiting correlations between state variables and actions present in the dataset of expert's demonstrations to learn an optimal policy for a certain task. In the presence of distributional shift, which arises every time trajectory information used for training comes from a policy different from the one currently used by the learning agent, agents that learn by imitation, but without causal knowledge, are usually prone to causal confusion or misidentification (e.g. of what prompted the expert to act in certain ways) [41]. If a correlation ceases to exist, performing the same action in response to a certain state could in fact turn out to be inappropriate in most cases. This knowledge deficit has been highlighted and studied in depth by a few recent causal RL works, making a first important step towards artificial agents trained via imitation that are better equipped to deal with confounders and spurious correlations [41, 123, 143, 277], making them more \u201caware\" of the causal structure of the problem under consideration.\nThe emergence of offline RL has marked another milestone in approaches to learning from imitation insofar as the emphasis is placed on the ability to learn from a dataset of previously recorded trajectories, potentially coming from other agents performing similar or different tasks [130, 251]. This represents a more challenging problem because during training the agent can no longer receive feedback from the environment, using its current policy to collect more trajectories through trial-and-error, as is typically done in imitation learning. Optimal behaviour must be learned from a dataset that is not updated during training, and that inevitably will not provide a complete picture of the environment/task in which the agent will be deployed. Techniques to ensure that a policy will perform well enough when deployed include conservative methods to bound the learned value functions (to avoid the risk of assigning high values to wrong states) [122], algorithms that take into consideration the agent's uncertainty about the identity of the test environment (enabling a kind of policy adaptation at test time) [61], and causal approaches to off-policy policy evaluation (see Levine et al. [130] and Bannon et al. [13] for comprehensive reviews).\nTo gain a better understanding of the extent to which current imitation learning approaches in RL are linked to causal cognition, it is instructive to consider a line of research in the animal cognition literature directed at investigating what kind of learning strategy is adopted in a social context by non-human primates, using imitation vs. emulation tasks. The distinction between imitation and emulation revolves around the particular \u201ccopying\u201d strategy used by the learning agent when observing the behaviour of a conspecific, i.e. either adhering to the demonstrator's actions (imitation) or focusing more on the action's results or outcomes (emulation) [231-233, 258, 273]. The imitating agent will reproduce virtually the very same actions of the demonstrator whereas the emulating agent will try to reproduce the results of those actions, e.g. a rewarding outcome, using the same or different behavioural strategies, depending on context [27, 96, 167, 170, 171, 222, 223]. For instance, to collect a floating peanut from a water-filled tube (an example of a floating-reward task), one has to increase the water level in the cylindrical container; a higher water level is the key instrumental result (or precondition) required to solve the task. In a social setting with expert demonstrators, a subject that overlooks that piece of information and learns to solve the task by copying all the particular actions of the expert conspecific (e.g. the ambulatory behaviour to collect the water) will fail at the task if those actions are no longer appropriate, or available, to produce the desired outcome (e.g. the water can be accessed only by climbing) [222]. Importantly, there is empirical evidence suggesting that adopting an emulative vs. imitative learning strategy can depend on the availability of causal information about the effects of certain actions, and their connection with the final, desired outcome. For instance, in Horner et al. [96] chimpanzees witnessed a human demonstrator securing a reward from a puzzle-box using a tool. When the box was opaque, hiding the relevant tool movements necessary to unlock the reward, at test time the subject reproduced all the actions seen in the demonstrations (learning by imitation). Conversely, with a clear box the subjects learned to ignore the irrelevant actions, thereby solving the task more efficiently. Thus, learning by emulation implies attending to goal/instrumental information (e.g. higher water levels in the tube) and being able to act upon it whether relevant behaviour has been demonstrated or not, i.e. attending to causal information pertaining to the causal states and/or variables that form a sort of precondition to reach a final outcome. As such, this learning strategy affords efficiency and flexibility because the learning subject is free to explore and select the best course of action to reach an end goal."}, {"title": "6.2.3 Natural causal information", "content": "Beyond the ability of learning from online interactions and social demonstrations, some (natural) agents also display a propensity for the acquisition of causal information from natural sources. Natural causal information is precisely information about the existence (or absence) of certain causal relationships or structures that is gleaned from observing the occurrence of natural events (see section 3.2).\nSince the tree-branch thought experiment of Tomasello et al. [233], it seems that the general consensus on observational (or impersonal) causal learning being an exclusively human ability has not shifted [62]. However, there is empirical evidence coming from some observational causal tasks, in which key causal relations can only be inferred from observations, suggesting that observational causal cognition in some non-human animals might be more developed than what it has been normally thought. For instance, corvids have demonstrated an ability to take into consideration the potential effects of hidden causes, e.g. other agents or properties like the weight of an object, from observations alone [104, 217] while chimpanzees have been shown to be capable of inferring the presence of causal relationships from patterns of covariation (with a blicket-like experiment) [244] and using temporal cues [221].\nSimilarly, \"ghost\u201d-condition tasks, showing an apparatus in a final desired state and/or how a mechanism works (by pulling invisible strings), used to study emulation learning, also suggest that non-human primates exploit observational causal information to guide subsequent successful behaviour [94, 95, 223].\nThus, arguably, despite not reaching the performance achieved by humans, some non-human animals appear to have the ability to learn about the causal structure of not only systems they interact with, but also of systems they can merely observe. This places them in a category beyond imitation (causal) learners, as they can make use of experience other than theirs, processing and capturing in causal terms events generated by external sources with a different body or physical configuration."}, {"title": "6.3 Integration of Causal Information in Natural and Artificial Agents", "content": "Following section 5.3, integration can be seen, from our perspective, as the process of incorporating and fusing different domains of causal information since, regardless of its source, any experience can be stored in a replay buffer (see also fig. 9). It is however important that different kinds of experience are integrated by concurrently taking into account their different roles, relevance for the given task, and/or potential weights based on the identification of key causal relationships. The studies in animal cognition we examined so far can give us some clues, in the form of particular behavioural profiles, about what type of causal information integration happens in non-human animals. However, it is important to keep in mind that behavioural traits are here used as a rudimentary proxy for cognitive operations of integration that remain still largely unknown.\nAs we saw in section 6.2, different animal cognition studies involve social causal information, e.g. demonstrations of a desired behaviour by an expert [27, 96, 167, 222, 223]. Despite the existence of negative results (e.g. Renner et al. [171]), these studies provide supporting evidence for the claim that non-human animals are capable of egocentric + social integration, see fig. 9. Within the same group, in RL, recent approaches have started to tease out the impact of certain causal relationships in imitation learning [41, 123, 143, 277]. These works represent a first step towards a better understanding of what integrating egocentric and social causal information might mean and especially entail, e.g. looking for invariances, confounders, direct causes of an expert actions. Yet, it remains to be seen whether these approaches can be successfully extended and/or combined with methods to deal with high-dimensional, partially-observable scenarios (POMDPs) as their counterpart, natural causal learners, can integrate causal information starting from observations alone [27, 96, 167, 222, 223].\nOn the other hand, the fact that an animal's behaviour is influenced by the observation of certain causal relationships can be explained by invoking cognitive operations of integration that combine natural and egocentric causal information. Observational learning experiments suggest that basic forms of egocentric + natural integration are present in non-human animals [95, 104, 221, 244]. For instance, inferring that the presence of a causal factor (e.g. the weight of an object) has an impact on what an action can accomplish (e.g. whether one can get food from a dispenser with a certain object or not), and behaving accordingly, can be considered as an example of this type of integration.\nConversely, there is almost no evidence for two other forms of integration in non-human animals, which are likely to require causal reasoning abilities about natural events and other agents that we know are present only in adult humans. Integrating social and natural causal information, social + natural integration, might entail scenarios where"}, {"title": "7 Discussion", "content": "Our comparative analysis so far has highlighted areas where animal cognition and causal reinforcement learning share some evident common ground in their otherwise different approaches to the study of causality in cognition and decision making. In this final section we look in more detail at some of the opportunities offered by our unifying formal account of causal cognition, showcasing ways to make a more synergistic use of its strengths, and speculating on areas we believe will be of particular interest for future explorations."}, {"title": "7.1 Computational interpretations of studies of natural agents", "content": "Modelling approaches in the literature of animal cognition are mainly concerned with capturing the cognitive and psychological processes of subjects exposed to tasks that are assumed to require an understanding of causal information [20, 45, 247]. However, simply providing evidence that subjects can learn complex causal structures from patterns of conditional (in)dependence shown to them and reason about interventions and counterfactuals in the world, leaves open the more fundamental question we asked in the introduction. Specifically, how does an adaptive agent interacting with and receiving feedback from an environment become sensitive to certain causal information and process it in ways that are conducive to reaching its goals? To address this, we propose to use current models and algorithms developed in the fields of causal RL, and provide next some more specific examples."}, {"title": "7.1.1 Measuring explicitness in natural agents", "content": "In this work, we considered the question raised by Starzak et al. [208] of how to define and measure explicitness in animal cognition studies and proposed to think of it as disentanglement (see section 5.1), roughly the degree of causal factorisation of a representation, to gain access to a relevant class of candidate metrics. While a widely-accepted measure of disentanglement is still missing, different proposals have been put forward, providing thus multiple options that could be considered for the modelling and testing of explicitness in animal cognition [91, 252, 278].\nAs a first step, we believe that a setup based on the AnimalAI Olympics framework [37, 38, 245] could be used to introduce an experimental pipeline involving training artificial agents on the same class of causal tasks used in the animal cognition literature, to compare their performance with that of animals. If the performance of two agents (artificial and natural) were to be comparable according to some appropriate success metric (e.g. solving a task, behavioural similarity), one could then measure the degree of disentanglement of the artificial agent's representation and use that to gain some understanding of a possible computational theory reflecting the natural agent's modelling capabilities. Furthermore, we believe that this approach has the potential to become a standard benchmark for causal AI research to test if, and what kind of, causal representations can unlock the necessary skills to tackle problems of different complexity, showcasing causal learning abilities akin to the ones that appear to be present in natural agents [36, 124]."}, {"title": "7.1.2 Zero-shot learning for high(er) explicitness", "content": "A second example of the type of formalisation work afforded by causal RL implementations revolves around the fact that high explicitness is assigned to those animal agents that are capable of solving a task without much visual or sensorimotor feedback from the task at hand. Despite contrasting empirical evidence [50], there are in fact suggestions that some non-human animals are capable of finding solutions to a certain problem \u201cin their head\" [185, 222], without the need for extended trial and error learning, which has been regarded as an indication of causal understanding [134].\nSimilarly, in causal RL, zero-shot, offline, and continual learning describe models of agents capable of solving certain tasks with extremely limited training data, in virtue of having a causal representation of the environment usually supported by a process of planning that reuses and transfers previously acquired causal knowledge. These agents are at the forefront of RL research, with problem definitions, formalisations, and benchmarks in constant evolution [2, 115, 119, 234] and we believe they are likely to provide another ideal baseline for the development of computational theories of causal reasoning in animals."}, {"title": "7.1.3 Emulation as inverse RL", "content": "A large number of approaches in RL that make use of social causal information, i.e. causal information derived from observing the behaviour of other agents (see section 3.2), can be said to implement a form of imitation learning. Computationally, this form of learning can be described as behavioural cloning: trying to copy the policy of an expert"}, {"title": "7.2 Causal cognition inspired RL", "content": "Looking then at figs. 7, 8 and 9 we can also identify potential areas where current causal RL frameworks can take inspiration from ideas developed in animal cognition. In particular, we refer to areas where works and standard theories of causal understanding in animals have currently no counterpart in RL: causal insight in fig. 7, learning from natural causal information in fig. 8 and complete integration in fig. 9."}, {"title": "7.2.1 Causal insight for causal RL", "content": "In section 6.1, causal insight was described as the (1) capacity to produce adaptive responses as a result of reorganising one's causal knowledge, encoded in (2) highly explicit causal representations that lead to (3) an innovative solution to a problem.\nThe first defining element (1), involving flexible reuse of information and past knowledge in new tasks and/or environments is a long-standing challenge in AI research [28, 56, 179, 226, 272]. The second aspect (2), based on the relevance of causal representations for strong generalisation, the ability to generalise out-of-distribution, encompasses transfer/meta/multi-task learning paradigms and has been noted in several causal machine learning works, with consensus that disentangled, structured, modular, causal representations can provide several benefits [4, 5, 9, 67, 178, 182, 257]. The third feature (3), suggesting that solutions that innovative, in the sense that they give a new take on an existing problems or can be used for a new and unseen problem, is not fully captured or does not clearly emerge from the generalisation approaches just mentioned. In fact, systematic studies of, e.g. out-of-distribution learning as currently found in the literature are mostly limited to synthetic datasets and/or toy problems characterised by narrow task distributions, neglecting more realistic and ecological settings [46, 68, 112, 236].\nMore specifically, these sorts of investigations have not been carried out by means of evaluation methods and bench-marking that can take advantage of work found in the animal cognition literature. As suggested in some recent works [36, 38, 192], using training and testing protocols from animal cognition experiments has the potential to improve current architectures towards the goal of reproducing common sense abilities of different non-human animals (e.g. understanding of everyday physical notions like objecthood, containers, obstructions, and the related sets of affordances). Since common sense abilities are deeply intertwined with an understanding of causality, studies of this kind could help to ascertain the extent to which causal RL can truly capture the manifestations of causal cognition in natural agents.\nMore generally, embracing a learning paradigm in which the central question is how an agent should gather and store key causal information for the purpose of subsequently extrapolating a strategy for tasks never seen before, seems essential for causal insight to develop in artificial agents. Further to the requirement of different training procedures and/or more computational power and data, tackling these kinds of questions will help to overcome persisting limitations, e.g. by introducing a more formal notion of causal insight connected with strong generalisation."}, {"title": "7.2.2 Natural causal information in causal RL", "content": "Offline reinforcement learning appears to be one of the closest available formalisations of a decision-making problem like the one posed by the tree-branch example [233] where a successful learning agent ought to be able to conduct a causal analysis of the natural scene it was part of, and then proceed to shake a fruit-bearing branch, just on the basis of having witnessed a fruit falling due to the wind shaking the tree. One of the crucial aspect of this decision problem is the requirement of acting in an optimal manner immediately, based on the (natural) experience collected, i.e. without the ability to take advantage of further trial and error learning, which is precisely the setting of offline reinforcement learning.\nHowever, while Tomasello et al. [233] highlights the importance of causal concepts to deal with this decision-making challenge, current approaches to deal with offline learning instead pursue strategies that try to mitigate the degree of distributional shift without any reference to causality. In a nutshell, some methods introduce constraints on the policy being learned so as to minimise its divergence from the policy that collected the transitions stored in the replay buffer, while others use uncertainty measures to learn more conservative value functions in order to avoid catastrophic mistakes due to distributional shifts, see Levine et al. [130] for a review.\nIt is also important to note that, while the state transitions stored in the replay buffer could come from any policy, e.g. even those employed by agents with different bodily configurations or \u201cnature\u201d itself, to the best of our knowledge there are still no techniques to infer trajectory information from visual data in offline RL. More specifically, we refer to the ability to process natural happenings, i.e. physical phenomena that don't involve any particular agent (the \"ghost\" conditions from the animal cognition literature, see section 6.2.3), through a causal lens. In practice, this would correspond to extracting state transitions tuples, , from high-dimensional visual input where a crucial step is to come up with a physicals interpretation of some external event as an impersonal action, producing an environmental state transition (the tree shaken by the wind). Extending the offline framework in causal RL to include mechanisms to learn from nature has the potential to uncover further aspects of causality not yet understood and might spur a series of novel algorithmic solutions, getting closer to the design of an observational causal agent, where the \u201cghost\u201d conditions from the animal cognition literature [94, 95, 223] could be used as a test bed for this new generation of agents."}, {"title": "7.2.3 Interventions in causal RL Agent", "content": "The ability to reason about and perform interventions, which in a technical sense can be described as local perturbations of a system that set one or more causal factors/mechanisms to certain fixed values [161, 181], has often been described as one of the hallmarks of causal reasoning agents [20, 63, 66, 80, 162] and has been used to draw a possible distinction between acting and intervening. In this view, the former only implies an appreciation of the consequences of one's bodily movements (e.g. locomotion, reaching, grasping) leading to changes in perception and conditions for achieving certain goals. The latter additionally involves an intentional modification of a certain aspect of the environment, exploiting an existing or induced causal relationship, to elicit a desired effect [62, 128]. For instance, using a stick to make a fruit fall from a tree branch (intervening) is in many ways different from climbing the same tree to grab the fruit (acting), even though the outcome is ultimately the same (eating the fruit). This suggests that not all actions of an agent qualify as interventions but all interventions are actions, whether realised or only imagined. Importantly however, while an agent might be regarded as performing an intervention from the perspective of an external observer, it does not follow that the agent itself conceives of its actions as interventions.\nBy examining the animal cognition literature, at least two markers for interventional-aware agents can be identified. One is the ability of certain agents to infer that an environmental causal path cannot be activated if an action is directed at producing an effect along that path. If the tone is produced by the rat through a lever press, the cause (the light) that usually elicits the tone and makes the food available is likely absent and so will be the food [20, 128]. The second one is the capacity to implement an innovative behavioural response simply following the exposure to certain observational patterns, i.e. what we called causal insight [102, 218]. For instance, the primates solving floating-reward tasks seem to showcase a capacity to intervene, i.e. manipulating the environment (water level in the tube) to obtain a desired effect (reward is closer to the surface level) from observations alone, sometimes even without the need for visual feedback [185].\nOn the other hand, artificial RL agents can be hardly regarded as intervention-aware insofar as they are mostly engaged in acting in the sense described above (when the agent is embodied, either in a simulation or in the real world), and certainly they do not regard some of their actions as interventions. It is yet unclear what the most promising approach to develop intervention-aware causal agents will require. On a basic level, it might be crucial to revisit"}, {"title": "8 Conclusion", "content": "In this work", "262-265": [208], "dimensions": "explicitness, sources and integration of causal information. In the present work, we introduced a formal framework that provides more rigorous and clear underpinnings for those dimensions, and offers some precise coordinates to study various aspects of causal cognition.\nMore specifically, levels of explicitness were defined"}]}