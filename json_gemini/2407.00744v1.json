{"title": "DISENTANGLED REPRESENTATIONS FOR CAUSAL COGNITION", "authors": ["Filippo Torresan", "Manuel Baltieri"], "abstract": "Complex adaptive agents consistently achieve their goals by solving problems that seem to require an understanding of causal information, information pertaining to the causal relationships that exist among elements of combined agent-environment systems. Causal cognition studies and describes the main characteristics of causal learning and reasoning in human and non-human animals, offering a conceptual framework to discuss cognitive performances based on the level of apparent causal understanding of a task. Despite the use of formal intervention-based models of causality, including causal Bayesian networks, psychological and behavioural research on causal cognition does not yet offer a computational account that operationalises how agents acquire a causal understanding of the world. Machine and reinforcement learning research on causality, especially involving disentanglement as a candidate process to build causal representations, represent on the one hand a concrete attempt at designing causal artificial agents that can shed light on the inner workings of natural causal cognition. In this work, we connect these two areas of research to build a unifying framework for causal cognition that will offer a computational perspective on studies of animal cognition, and provide insights in the development of new algorithms for causal reinforcement learning in AI.", "sections": [{"title": "1 Introduction", "content": "Causal cognition, the ability to acquire and exploit causal information about oneself and the world, is a core aspect of adaptive and intelligent behaviour, in non-human and human animals [62, 65, 149, 203]. At the same time, in recent years it has become apparent that artificial systems displaying various forms of seemingly intelligence behaviour still fall short of performing at the level of the majority of non-human animals that showcase various kinds of causal cognitive abilities [38, 124, 162, 195]. It has thus been suggested that an understanding of the mechanisms of causal cognition will play a crucial role in cognitive science and artificial intelligence for the next decades [67, 75, 124, 126, 129, 162, 180, 195].\nThe study of causal cognition both in human and non-human animals has a long history, with roots in behavioural studies trying to establish the extent to which an organism's behaviour reflects proper causal understanding of the world instead of simpler forms of associative learning [63, 65, 70, 163, 202, 203, 206, 220]. Some of the most influential studies in this area have combined theoretical and modelling work based on the formalism of causal Bayesian networks to account for the cognitive performance of various subjects in tasks designed for testing causal cognition abilities [63, 64, 66, 70, 71, 220]."}, {"title": "2 Causal cognition", "content": ""}, {"title": "2.1 Causal cues and the debate on associative vs. cognitive explanations", "content": "Early work on causal cognition in human and non-human animals focused on how subjects learn about the strength of cue-reward relationships, where some of the given cues could be attributed the \"causal power\" of eliciting rewarding outcomes [32, 113, 206]. A significant part of this early work can be contextualised within an old debate on whether causal learning is just a form of associative or contingency learning, the dominant theoretical framework to study animal learning [172], or a form of learning that requires more cognition-laden processes.\nThe crux of this debate was not whether there are causal relationships or structures in the world, which is more a philosophical type of issue. Granted that there are, associative accounts have usually tried to show that the successful performance of some agents in seemingly causal learning tasks can be modelled, and ultimately explained, by means of purely associative learning mechanisms. Roughly, these would track the relevant causal associations between certain variables without the need of invoking more sophisticated cognitive processes or structures involving a notion of causality [43, 44, 193, 194]. In contrast, other works have highlighted how certain behavioural responses, especially from humans, are indicative of causal models that the agent in question would exploit to reason about causal relationships of various sorts [20, 246, 248, 249].\nA paradigmatic example of how this debate has typically unfolded can be found within the analysis of backward blocking in conditioning experiments with rats. In these experiments, test subjects are exposed to cues, say $C_1$, $C_2$, and the compound cue given by their combination, i.e. $C_1C_2$, that may or may not lead to a rewarding outcome, indicated by + (see fig. 1). After some trials, the subjects, often rats, learn about the relationships between those cues and the reward, and react accordingly when similar cues are shown. In the backward blocking scenario, after rats have been trained with cue-outcome sequences like $C_1 C_2+, C_1+$ (in that specific order), one finds that they don't react to the presentation of $C_2$ alone in subsequent trials. The response to $C_2$ has been \"blocked\" by the animal upon witnessing $C_1+$, i.e. the causal role of $C_2$ is reconsidered after understanding that it is not involved in producing the reward.\nThis sort of retrospective evaluation (of what happened in a earlier trial) is a problem for associative accounts because they usually assume that a cue-outcome association can increase or decrease in strength only when the cue is present (together or without the reward). However, in backward blocking scenarios the change in behavioural response to $C_2$ occurs following the presentation of $C_1+$ and despite the fact that $C_2$ has always (or most of the time) appeared at the same time as the reward +. An advocate for causal models would see retrospective evaluation as an example of their influence on cue-outcome learning. Given the evidence represented by $C_1+$, the decreased response towards $C_2$ could be explained in terms of a re-evaluation of the role played by that cue when it appeared as part of the sequence $C_1C_2+$. Such evidence would in turn suggest that $C_2$ was not included as part of a causal relationship with the rewarding outcome.\nAt the same time, over the years several revisions of traditional associative accounts to model retrospective evaluation have been proposed to account for backward blocking and more complex scenarios involving higher-order relations between cues, i.e. relations between two cues that never occur together but that appear in combination with another cue, see for instance Dickinson [44]. However, these revisions usually depart from traditional associative principles in significant ways, e.g. requiring more sophisticated information-processing capabilities, see for instance the discussion in Penn et al. [163] and references therein. This thus leaves us with architectures based on, or inspired by associative principles [42], begging however the question of whether these models are still associative, or not."}, {"title": "2.2 Causal understanding as a building block for causal cognition", "content": "Moving past the associative vs. cognitive debate using a more comprehensive definition of cognition at different levels has however brought forward a perhaps more fundamental disputes about the presence, or not, of forms of causal understanding in agents, and what such an understanding amounts to. This is especially evident in the behavioural research on causal cognition in non-human animals, where the goal is to design behavioural tasks specifically intended to try and measure some manifestation of causal understanding [206]. In other words, tasks that would ascertain whether a subject is capable of the feats of causal cognition, where the assumption is that a solution of the task requires certain causal, cognitive strategies.\nAn example of this research is represented by studies on capuchin monkeys using the trap-tube task [239, 241-243], where causal cognition is characterised as the comprehension of key cause-effect relationships within the task. The trap-tube task consists in pushing a food reward out from a transparent tube (anchored to the floor) using some kind of tool (e.g. a stick), by inserting it into one of the tube's two openings, see fig. 2b. In general, capuchin monkeys struggle to learn how to solve the task, either because they would pick the wrong kind of tool (a stick that could not be inserted into the tube because of its shape) or because they would pick the wrong side to put the stick, making the reward fall into a trap positioned underneath the tube.\nThe persistent error patterns of the (few) subjects that could solve the task after extensive trial and error are thought to be evidence of a distinction between 1) successful performance based on a \u201cstroke of luck\u201d after extensive active experimentation, and 2) successful performance based on an understanding of relevant causal variables inherent within the task requirements [239, 243]. It is in fact well known that capuchin monkeys have a propensity to produce a wide variety of actions and complex combinations thereof, even involving tools, to the point that they could be described as expert tool-users. Because of this, it is unclear whether they have an appreciation of the causal relationships between their behaviour and the resulting outcome. In other words, they might learn that using certain tools is an effective way to achieve certain results, but they may not appreciate the reasons for why their actions are successful [239].\nIn contrast, experimental evidence in chimpanzees suggests that they may have an understanding of the causal relationships between certain actions and their associated outcomes [134, 155, 186]. The key finding here is that some subjects, tested with different configurations of the trap-tube task, were able to select the right side of insertion (almost) immediately, allegedly displaying an ability to plan their actions according to the different causal relationships present in the task configuration. Consequently, this evidence suggests that the successful subjects were not using heuristics such as a distance-based rule, which would for example determine the correct action based on the distance of the reward from the tube's openings without an understanding of the causal structure of the problem. Instead, subjects appeared to take into account the causal features of the task configuration and choose beforehand what action to perform. This would thus amount to a representational strategy that delineates the key requirements of the task in advance and results in the correct behaviour without the need of extensive trial-and-error learning. More specifically, one could argue that those successful chimpanzees exhibited some kind of causal understanding of the consequences of pushing the stick inside the tube (but see Martin-Ordas et al. [148] and Seed et al. [186] for opposing views). Unlike for instance the capuchin monkeys of other experiments [239, 243], where a constant monitoring of the effects of one's ongoing action (to check one is on the the right track) and attempting a variety of actions' combinations (in the hope to find the right one) was instead unnecessary."}, {"title": "2.3 Causal interventions and tool use", "content": "A strong candidate for the presence of plans based on action-driven outcomes is the ability to produce a causal intervention, an action that involves a causal control on a particular effect [240]. On the one hand, this seems to provide strong evidence for causal cognition since producing an intervention requires some form of causal understanding. In particular, it requires an agent to understand that its actions, in the form of movements of its own body, could be used as external probes for the causal texture of the world (cf. second rung of the causality ladder in Pearl et al. [162]).\nOn the other hand, the attribution of causal interventions to cognitive agents appears still controversial because there are only limited reports that hint at intervention-like abilities in, for instance, rats [20, 128] and primates [62, 244]. At the same time it is unclear whether tool-use, the ability to skilfully manipulate objects, common in species like corvids [102, 218], should count as a form of intervention or not. In general, it is not entirely obvious what the markers of causal interventions are and how to design experiments that could determine their presence or absence.\nWork on rats, for example, suggests that these animals can learn a common-cause model, where a light being turned on is perceived as the cause of two effects, a noisy sound and the release of some food. After exposure to patterns of causal relationships for a certain number of (training) trials, the rats enter a test condition characterised by a lever that produces a noise when pressed. Interestingly, it has been reported that after (accidental) lever presses, rats exhibit a less resolute search for food (measured by the number of nose-poking in the cage's hopper) than when the noise is presented alone. A possible explanation for this behavioural response would regard these rats as capable of recognising their action (the level press) as an intervention, an independent self-generated perturbation on one variable of the learnt causal model. In fact, an effect (noisy sound) cannot be an indication that a cause is present (light) when that effect is produced by an intervention (lever press). Therefore, by conceiving of their action as an intervention on one variable of the learnt causal model, the rats do not expect that the other effect (food release) will occur, which then induces a less vigorous search for food [20]. While these findings are consistent with the claim that rats can differentiate between predictions based on observations and predictions based on interventions, they do not exhaustively prove that rats can produce interventions to activate a certain causal path, in this case the one leading from the light to the food dispensation (as discussed by Blaisdell et al. [20] themselves).\nWork on corvids on the other hand, see for instance Taylor et al. [216], testing New Caledonian crows with a few variations of the trap-tube task (see fig. 2b), suggests that they possess critical causal understanding abilities, e.g. an appreciation of causal relationships involving object-hole interaction, on which their exceptional tool-using skills might be built. Similarly, Jelbert et al. [105] and Logan et al. [140] present experiments on a task (inspired by Aesop's fables) in which crows have to learn to drop some objects (e.g. stones) into the right water-filled tube so that the water displacement brings a floating reward (e.g. a piece of meat) closer to the tube opening (see fig. 2a). The results here point at the fact that the birds managed to solve the task, seemingly by attending to the relevant causal information, e.g. the fact that larger and not hollow objects will produce a bigger water displacement. For a variation of the task however, where the setup instead consisted of three water-filled tubes arranged in a row on a wooden board, with some space between each other, results were less clear. In this task, the baited tube is the one in the centre and, crucially, it is connected with one of the others by means of a U-shape tube hidden from view (located underneath the board). Dropping objects into one of the lateral tubes will have as an effect a water-level rise in the baited tube. Since the central tube is too narrow to drop anything in it, to bring the reward on the surface it is crucial to recognise this counter-intuitive effect and exploit it, i.e. to infer the presence of and reason about hidden causal mechanisms. Here, all tested birds performed at chance level, meaning that they dropped objects randomly on either of the two lateral tubes [105], see also Logan et al. [140] for similar conclusions on a slightly modified setup."}, {"title": "3 A Conceptual framework for causal cognition", "content": "After decades of research in the field, by now it is evident that different works on causal cognition have often appealed to different conceptualisations of the subject matter, to the point that a consensus has yet to be formed on what even constitutes causal understanding, see Penn et al. [163] and Sloman et al. [203] for some reviews, and the contributions to Gopnik et al. [65] and McCormack et al. [149] for other perspectives. As briefly illustrated by the previous section, different lines of work place causal understanding at different levels of a hypothetical cognitive spectrum, and have portrayed very diverse views on how to characterise it in terms of key cognitive functions and behavioural outputs. Most researchers might agree on the idea that a causal agent has the kind of behavioural flexibility that is unattainable by agents lacking causal understanding. Yet, the variety of positions trying to describe what underpins it appears to only contribute to the confusion.\nSome works point to a representational strategy for agents to picture in advance what the key causal features for solving a task are. This would then characterise a distinction between performing and understanding, i.e. whether the solution of a task is achieved via some sort of shortcut, or in a robust and reliable way [134, 239, 241, 243]. Others are more demanding, and see causal understanding as the result of some form of causal reasoning, yet another ambiguous expression that has been described in different ways. For instance, causal reasoning could involve structural or symbolic (causal) knowledge abstracted from perceptual cues [149, 168, 187], or in other words, the ability to search for cause-effect relations that could reveal how and why two events are connected, or why some actions lead to certain outcomes (i.e. diagnostic causal reasoning), requiring thus the presence of some causal beliefs [45, 240]. Others would further maintain that without an ability to perform causal interventions, perhaps even involving unobservable entities (see hidden causal mechanisms in section 2.3), it is unlikely that an agent is able to grasp causality as opposed to just behave as if it does. Going back to tool-use then, the question of whether adaptive tool use may reveal the presence of some of the abilities just described or whether it may be a confounder instead [102, 186, 218] remains unanswered.\nIn a recent attempt to put causal cognition research on a more precise and coherent footing, we find different proposals discussing experimental findings framed with respect to a few recurring themes, drawing attention to key aspects of causal cognition [208, 262-265] (see also Goddu et al. [62] for a recent review). Starzak et al. [208] in particular dissect the main disagreements over causal understanding in non-human animals, proposing a more precise way to study causal cognition using a three-dimensional conceptual framework inspired by and overall consistent with other conceptual treatments [262-265]. The starting point is to regard causal cognition as the processing of causal information, understood as information about the nature of certain causal relationships, instead of referring to the more ambiguous and ill-defined notion of causal understanding. One of the advantages of this move is to put on one side normative issues, e.g. what really counts as causal understanding, and instead focus on empirically tractable parts of the matter [208]. To a first approximation, the main idea is to score the performance of subjects on causal tasks (see section 2) along three dimensions that have the potential to cover the full spectrum of causal cognition. With these as a way to ground the discussion of different empirical results, [208] then suggest ways to draw a more fine-grained comparison of the extent to which non-human animals and humans process causal information. More specifically, following Starzak et al. [208] causal information processing can be characterised along three dimensions:\n1. the level of explicitness of causal information,\n2. the sources of causal information, and\n3. the level of integration of causal information."}, {"title": "3.1 The explicitness of causal information", "content": "The explicitness dimension, refining some intuitions presented in Woodward [262], aims to capture a spectrum of causal information where on the one end, implicit models are essentially blind to causal relationships. These models represent cases where actions and outcomes/rewards are entangled or \"fused\" [262], i.e. models based on an associative correlations where the causal structure (see the web of causal possibilities in section 2.2) is essentially hidden and inaccessible to the agent. In this class of models, agents cannot necessarily come up with a complex plan on how to adjust their actions that is sensitive to the web of causal possibilities in order to achieve a certain goal, since they lack or have a limited understanding of their own actions and other variables in the environment as causally relevant for bringing about certain outcomes or rewards [262]. They can however take actions in a less structured way, for example using knowledge acquired from repeated trial-and-error in an associative manner, leading to a continuum of explicitness that is apparent in several experimental studies as seen in section 2. For instance, associatively pairing the action of pressing a button (cause) with the presence (very often, but not always) of some food (end goal, an effect) can be considered as an example of implicit model.\nOn the other side of the spectrum we find explicit models, models where the causal structure is completely unpacked and relations between actions and outcomes/rewards are disentangled and available for an agent to take advantage of. Looking at the previous example, we can imagine a different scenario where an agent realises that a button press will activate a food dispenser mechanism (some intermediate variable) and that the food will become available if and only if there is no obstructing object in the mechanism. In this case, the action of clearing the dispenser from the obstructing piece is an action that can be said to require a more explicit understanding of the causal structure of the world, at least compared to the first situation, an operation that is directed at altering one of the intermediate causal variables, the object obstructing the food dispenser, so as to obtain the food.\nA qualitative description of explicitness thus amounts to establishing what an agent can do with the acquired causal information, for example by investigating an agent's degree of flexibility in using what is has learned in a causal task (e.g. clearing the dispenser mechanism of the obstructing object). To a first approximation, the key idea is that the more explicit a model is, the more causal information is available to an agent, because the means to reach a certain goal have been recognised as distinct from each other and from the goal itself (the mediating variables of a certain causal influence have been identified, cf. Pearl [161]), thereby leading to a higher degree of flexibility in behavioural responses.\nTo see how different degrees of explicitness appear in the animal cognition literature, we can take a closer look at the research on the trap-tube task described in section 2.2. Facing the trap-tube, an agent that can only form implicit models where actions and outcomes are entangled, i.e. where actions a leading to states s are a-causally associated to outcomes x, has a very limited ability to discern the possibly relevant intermediate variables that could be exploited to reach the goal. These include for instance the position of the trap, necessary for an understanding of whether its opening (the hole) is on the tube's lower surface or not, i.e. whether it affects the desired outcome (recall that in the latter case the tube has been rotated so the trap is ineffective).\nAs the literature on these experiments suggests that most capuchin monkeys that were tested do not seem to appreciate the relevance of the trap, and consequently perform poorly when the tube is inverted, we could say that these agents rely only on implicit models of the form \u201cinsert the stick, out comes the reward"}, {"title": "3.2 Sources for learning causal information", "content": "The sources dimension of Starzak et al. [208] is also inspired by Woodward [262], where causal cognition is proposed to be best explained in terms of some key, distinct abilities, namely, egocentric and non-egocentric sources of causal information.\nEgocentric causal information captures the idea of an agent that can acquire an understanding of the causal structure of the world from its own behaviour, focusing on the performance of their own actions and how they can can reliably and robustly result in certain outcomes. For example, a subject could learn that performing action $a_1$ makes a difference for obtaining outcome $x_1$ but not for $x_2$. This is the realm of instrumental conditioning (or learning) investigated extensively in animal research [262].\nNon-egocentric causal information can, on the other hand, be obtained from two main sources: the behaviour of other agents and the unfolding of natural events. Social causal information implies that an agent can learn about important action-outcome contingencies by paying attention to other agents' behaviour. For instance, observations of a conspecific performing action $a_1$ and reliably obtaining outcome $x_1$, but not $x_2$, provide important causal information for a subject that is aiming at outcome $x_1$ (or $x_2$).\nNatural causal information similarly suggests that events in the natural world can disclose ecologically important causal relationships. For instance, observing a piece of fruit falling from a tree-branch shaken by the wind could reveal to an attentive observer important causal information on how to get some food, as long as it is capable of performing a causal analysis of the situation [233]. Compared to the acquisition of the previous types of causal information, natural information imposes, arguably, a higher cognitive load on the subject because the event in question does not tell the subject what action may be (causally) relevant and for what reason(s). In other words, there is an additional cognitive effort that the subject needs to make to appreciate that, given certain observations, action $a_1$ can produce outcome $x_1$. [208] remark that empirical evidence so far suggests that egocentric causal learning is the most widespread in the animal kingdom whereas social causal learning and observational causal learning (as they call the two more sophisticated forms of causal information acquisition) are fully present in adult human beings only. Following Woodward [262], they agree with the idea that in principle these forms of learning could dissociate but they also add that it is not clear the extent to which these abilities are independent from one another, or whether they form a hierarchy of cognitive processes."}, {"title": "3.3 The integration of causal information", "content": "Integration appears more ambiguously in Starzak et al. [208] but can be understood, in general, as consisting of operations of update, combination, extension etc. of one source of information with another one to form a coherent structure. More concretely, in our framework integration will be later framed in terms of meaningful combinations of different sources of information (egocentric, causal and natural) that can describe if and when agents are capable of translating observations from different perspectives into their own (egocentric) perspective, forming new sources of egocentric + social causal information, egocentric + natural causal information or complete causal information (egocentric + social + natural), or whether social and natural information can be integrated without a direct effect on the agent own egocentric perspective to form social + natural causal information.\nWhile laying the foundations for formal characterisation of causal cognition across the animal kingdom, the inherent ambiguity of not only integration, but of explicitness and sources too, mixed with the more general focus on high-level discussion over a clear operationalisation, pushes the idea that the conceptual framework proposed by Starzak et al. [208] is in several ways still heavily relying on interpretative work to be done by the reader. In the next section we fix some core ideas in a mathematical language that will form the basis of our proposal to formalise Starzak et al. [208]'s work in a pragmatic way in section 5."}, {"title": "4 A mathematical framework for causal cognition", "content": ""}, {"title": "4.1 Disentanglement in machine learning", "content": "A key proposal in modern approaches to deep (reinforcement) learning is that of disentanglement [16], roughly stating that in order to acquire an understanding of the causes behind some given observations, it is necessary to interpret those causes as distinct (high-level) factors, and recognise the different causal power they exert when giving rise to observations [182]. For example, if we see a red ball made of rubber bouncing on the ground, what makes it bounce? While different factors including colour, shape, and material, are intertwined and together produce observations captured by our eyes, some of these factors have no causal influence on the bouncing behaviour, i.e. colour. According to the disentanglement hypothesis, the ability to discern different factors is thus a crucial step in a theory of causal understanding.\nSimilarly to other influential proposals, disentanglement has been used to characterise a general intuition based, however, on different implementations and interpretations. Following Zhang et al. [278]'s review, we thus look at some of the common structure behind different definitions of disentanglement. To do so, we focus in particular only on sets and functions between them. This is technically equivalent to stating we are working in the category of sets and functions, Set [147], and while in various ways limiting, this allows us to focus on the central parts of a our proposal to connect disentanglement to explicitness down the line using a relatively simple mathematical toolkit (sets and functions, without introducing more advanced tools from category theory).\nFollowing [278] we start by defining $S, X, Z$ as state-spaces, represented as sets of (generative) factors, observations and codes respectively. The sets of factors and codes are further assumed to be (Cartesian) products of $n \\in \\mathbb{N}$ factors and $l\\in \\mathbb{N}$ codes:\n$S = S_1 \\times S_2 \\times ... \\times S_n$\n$Z = Z_1 XZ_2 \\times ... \\times Z_l$    (1)\nAt a high level, the main idea driving this framework is that representing distinct factors, i.e. having disentangled codes that faithfully map to disentangled factors, is the starting point for acquiring a causal understanding of the world: an agent with knowledge of what factors generated its observations is an agent that understand what data-generating mechanisms brought observed data to its sensory peripheries, see fig. 3 for a way to frame the example of the red ball in this initial setup, to be unpacked next. The setup for disentangled representation can then be captured, in a compact form but with more specific constraints to be imposed below, by the following commutative diagram:\nWe introduce next the formal definition of disentanglement that we will be referring to throughout this work, following Zhang et al. [278].\nDefinition 4.1 (Disentangled representations). A disentangled representation is a product of codes $Z_i$ for $i \\in \\mathbb{N}$ defined through the following:\n*   a generative process or data generating process, as a function\n    $g: S \\rightarrow X$    (3)"}, {"title": "4.2 Causal representation learning", "content": "As highlighted by Zhang et al. [278", "252": "."}, {"252": ".", "traditional machine learning\") [107, 181, 227": ".", "16": ".", "166": "with the goal to learn a low-dimensional vector of causal codes from high-dimensional observations generated by causal factors [17", "182": ".", "10": "and Peters et al. [166", "9": ".", "following": "n*   a collection $S = (S_1, . . ., S_n)$ of $n \\in \\mathbb{N}$ causal variables, or causal factors,\n*   a collection $X = (X_1, ..., X_d)$ of $d (\\geq n) \\in \\mathbb{N}$ observables,\n*   a collection $C = (C_1, . . ., C_m)$ of $m \\in \\mathbb{N}$ confounders,\n*   a collection $N^S = (N^S_1, \u2026\u2026\u2026, N^S_n)$ of $n$ noise variables on causal variables,\n*   a collection $N^X = (N^X_1,\u2026\u2026\u2026, N^X_d)$ of"}]}