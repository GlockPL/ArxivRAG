{"title": "Dual Channel Multi-Attention in ViT for Biometric Authentication using Forehead Subcutaneous Vein Pattern and Periocular Pattern", "authors": ["Arun K. Sharma", "Shubhobrata Bhattacharya", "Motahar Reza"], "abstract": "Traditional biometric systems, like face and fingerprint recognition, have encountered significant setbacks due to wearing face masks and hygiene concerns. To meet the challenges of the partially covered face due to face masks and hygiene concerns of fingerprint recognition, this paper proposes a novel dual-channel multi-attention Vision Transformer (ViT) framework for biometric authentication using forehead subcutaneous vein patterns and periocular patterns, offering a promising alternative to traditional methods, capable of performing well even with face masks and without any physical touch. The proposed framework leverages a dual-channel ViT architecture, designed to handle two distinct biometric traits. It can capture long-range dependencies of independent features from the vein and periocular patterns. A custom classifier is then designed to integrate the independently extracted features, producing a final class prediction. The performance of the proposed algorithm was rigorously evaluated using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the superiority of the algorithm over state-of-the-art methods, achieving remarkable classification accuracy of 99.3 \u00b1 0.02% with the combined vein and periocular patterns.", "sections": [{"title": "I. INTRODUCTION", "content": "In the evolving landscape of biometric identification, the quest for more secure, reliable, and non-invasive technologies has become paramount, especially in the wake of global challenges such as the COVID-19 pandemic [1]. Traditional biometrics like face recognition [2], [3], fingerprint recognition [4], [5], gesture recognition [6], iris recognition [7], hand palm vein [8], hand dorsal vein [9], wrist vein [10], scleral [11], and finger vein [12] etc has inherent limitations and vulnerabilities that undermine their efficacy and user acceptance. Among these, facial and fingerprint recognition systems stand out as the most prevalent in commercial and semi-commercial applications due to their non-intrusiveness and ease of use. However, the advent of mask-wearing norms and hygiene concerns have significantly impacted their reliability and acceptance [1], propelling the need for alternative biometric solutions that can offer comparable or superior levels of identification accuracy with no drawbacks associated with full-face or touch-based systems.\nIn recent decades, the forehead vein pattern recognition [13] has emerged as a promising candidate in this context, offering a novel approach to biometric identification that leverages the unique physiological features of the human forehead. This method not only aligns with the requirements for non-intrusive and touchless hygienic biometric systems but also addresses the challenges posed by facial occlusions. The rationale behind focusing on the forehead as a biometric trait lies in its inherent advantages [13], [14], [15]. Firstly, the forehead remains largely unaffected by facial expressions and is less likely to be obscured by accessories or hair, providing a stable region for identification. Secondly, the area is easily accessible for imaging, even in the presence of face masks, making it an ideal candidate for recognition in today's real-world scenario. Additionally, the skin texture and other physiological features of the forehead can be captured with high-resolution imaging techniques, offering a rich dataset for developing robust recognition algorithms."}, {"title": "II. LITERATURE REVIEW", "content": "Bhattacharya et. al. [13] has attempted to explore the viability of a contactless and real-time system based on the forehead subcutaneous vein pattern and periocular biometric pattern as an alternative biometric identification method. It uses convolutional neural network-based feature extraction for the vein pattern and the periocular pattern independently, followed by feature concatenation for final classification. Convolutional-based feature extraction method relies on spatial features in the input images, which requires preprocessing of the forehead and particular images to produce good-quality patterns. Therefore, to alleviate the need for preprocessing and improve the robustness against uncertainty against the quality of the captured images, this paper presents dual channel multi-attention in ViT for capturing the sequential dependencies of features in the raw images of the forehead and the particular portions. The major contributions of this research work are listed below:\ni) An innovative architectural framework of dual-encoder ViT is formulated to handle raw images of the forehead subcutaneous portion and the periocular portion of a subject, simultaneously and independently. Using a dual-encoder approach with independent multi-head attention, the system effectively captures long-range dependencies among features, uniquely representing various pattern classes.\nii) In conjunction with the dual-encoder vision transformer, a custom classifier has been meticulously designed to process further the feature outputs derived from the two distinct channels. It integrates the independently processed features to produce a final class prediction. This tailored approach enhances the system's ability to accurately classify the input data, leveraging the detailed analysis provided by the dual-encoder framework to achieve high levels of accuracy in class prediction.\niii) The performance of the algorithm, underpinned by the dual-encoder vision transformer and the custom classifier, was rigorously evaluated using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) database [13]. This evaluation process involved both training and testing phases, allowing for a comprehensive assessment of the algorithm's effectiveness for biometric-based person identification. The results of this evaluation were then compared with those obtained by state-of-the-art algorithms to demonstrate the superiority of the proposed algorithm in terms of identification accuracy.\nThe remaining parts of the paper are organized as follows. Section III provides the brief theoretical background of the transformer model for natural language processing and ViT for image classification. Section IV describes the proposed framework of dual-encoder vision transformer. Section V presents the effectiveness of the proposed framework and its comparison with state-of-the-art methods on the FSVP database. Finally, Section VI concludes the work."}, {"title": "A. Review of Vein Pattern Recognition based on Approach", "content": "1) Graph-based Vein Pattern Recognition: As can be seen from the distribution of vein patterns attention will undoubtedly be attracted by the geometrical property, and image coding techniques designed for describing such spatial structures are believed to obtain satisfactory results. Following this pipeline, the line-like and/or curve-like models including the maximum curvature point [16], mean curvature [17], Gabor filter [18], and repeated line tracking methods [19], and statistical analysis tools including principal curvature [20], vectorgrams of maximal intra-neighbor difference [21], and the multi-scale Gaussian matched filtering and scale production methods [22] have been successfully adopted for robust and discriminative feature extraction. The main problem, however, is that the low contrast distribution usually results in inaccurate features. Also the sensitivity of these methods to uneven illumination [23] makes them unfavorable when compared with both hand-crafted features and CNN features introduced in the following sections.\n2) Shallow Descriptors based Vein Pattern Recognition: Selecting a suitable feature representation for vein pattern authentication requires robustness to image transformations and discrimination for sparse vein patterns. SIFT and its variants have been popular for their state-of-the-art performances. However, contrast enhancement (CE), a common pre-requisite for SIFT [24] can cause mismatches,"}, {"title": "III. THEORETICAL BACKGROUND", "content": "Vaswani et al. [67] introduced a transformer architecture in the paper \u201cAttention is All You Need\", which has a significant impact on the field of Natural Language Processing (NLP) for capturing long-range sequential dependencies. The framework demonstrated outstanding performance for many applications like machine translation, text summarization, etc. The main constituent of the transformer architecture is the attention mechanism in encoder and decoder blocks which enables the model to efficiently capture the intricate relationships within long sequential inputs. This mechanism empowers the model to consider the context of each word within the entire input sequence, which results in an enhanced learning representation. The encoder block handles the processing of the input text sequences and the decoder is responsible for generating an output sequence. Each layer of both the encoder and decoder consists of multi-head self-attention modules. The attention mechanism in transformer architecture helps to overcome the limitations of traditional recurrent models by leveraging the capability of capturing long-range dependencies of a token in the sequence.\""}, {"title": "B. Transformer in Computer Vision", "content": "With the outstanding performance of transformer architecture for various applications in NLP, Dosovitskiy et al. [68] demonstrated the encoder block of transformer to be very efficient for image classification in computer vision applications. The transformer encoder architecture introduced for computer vision applications is called the Vision Transformer (ViT). The attention mechanism in the ViT provides the capability of capturing the long dependencies in the flattened sequence of image tokens created by splitting an image into patches of size 16 \u00d7 16. The input image is converted into non-overlapping sequential tokens by splitting the image into patches, followed by flattening into a linear sequence of the patches. This mechanism allows the attention mechanism in the transformer encoder to produce class-specific image features by capturing the contextual information in the sequential dependencies of image patches. The final feature output can be further used with a suitable classifier head to predict the class of the input image. The schematic diagram of ViT for image classification with reference to the original paper by Dosovitskiy et al. [68] is shown in Fig. 1."}, {"title": "IV. PROPOSED VISION TRANSFORMER", "content": "The proposed framework consists of two-channel multi-head attention-based encoder blocks, called dual-channel multi-attention in the vision transformer architecture. The schematic diagram of the proposed framework has been shown in Fig. 2. The infrared camera-based images of the forehead and the periocular regions are given to the dual channel encoder through image patching and position embeddings as shown in Fig. 2. The processing of the bifurcated input images in various stages of each channel is described in the following subsections."}, {"title": "A. Patch Embedding", "content": "This step converts the input image to the sequence of token embeddings [68]. First, the input images from each channel are independently split into non-overlapping patches of size 16 \u00d7 16 using the 2-dimensional convolutional process. The patching step converts each batch of the input images from each channel in the form of $b \\times N_{patches} \\times d_{patch} \\times d_{patch}$, where $b$ denotes the batch size, $N_{patches}$ the number of patches, and $d_{patch} \\times d_{patch}$ the size of each patch. Here, $d_{patch}$ is usually selected as 16. Now, patches are arranged as a flattened sequence, where each patch (usually a square patch) is called as a token. Each 2D patch in the flattened sequence is flattened to 1D and added with position embeddings to represent the spatial information of the patch in the input image grid. Therefore, each patch now gets a shape of $(d_{embd} \\times 1)$, where $d_{embd} = d_{patch} \\times d_{patch} + 1$. Here, the position embeddings, also called class tokens are learnable parameters. Now the position embedded patch tokens are reshaped into a 2D tensor having dimensions: $b \\times N_{patch} \\times d_{embd}$, where, $b$, $N_{patch}$, and $d_{embd}$ represent the batch size, number of patches, and the dimension of embeddings, respectively. The embedding dimension represents the dimensionality of the embeddings."}, {"title": "B. Dual-Encoder block", "content": "The embedded tokens from the patch embedding step are fed to dual-encoder blocks. The two encoders accept token embeddings from the forehead and periocular channels independently. The components of each encoder are further described below."}, {"title": "1) Multi-Head Attention:", "content": "The multi-head attention block consists of several self-attention heads. Every self-attention head takes three inputs: Query (Q), Key (K), and Value (V). These three elements Q, K, and V are the projected output of the embedded tokens via a separate linear neural network (NN) layer with trainable parameters. The self-attention module comprises a scaled dot attention mechanism. The three inputs Q, K, and V are processed through self-dot attention, producing an attention score and output. If the number of patches is $N_{patches}$, the hidden dimension of the attention head in each encoder channel should be kept the same $(d_h = N_{patches})$. Therefore, the output of each attention head in each encoder channel has a size of $b \\times d_h \\times d_{embd}$. The attention score and the output are as given in Eq.'s (1) and (2), respectively.\n$M_{score} = SoftMax \\left\\{\\frac{QK^T}{\\sqrt{d}}\\right\\}$\n$M = VM_{score}$ where $M_{score}$ represents the probabilistic attention score, $d$ represents the hidden size of the attention head (embedding dimensionality), and $M$ presents the attention output of the attention head. The attention outputs (M's) from all such attention heads are concatenated and then linear projected the original size of M."}, {"title": "2) Feed-Forward Layer:", "content": "The linear projection of the concatenated attention outputs is now passed through a two-layer feed-forward neural network (multi-layered perceptron (MLP)). The two layers of the MLP block sandwich a non-linear activation layer that adds non-linearity to the output of the first layer before it is fed to the second layer. Thus the whole process consists of linearly transforming the input features into a higher-dimensional space by the first layer, applying a non-linear activation, and then transforming back to its original dimension by the second layer. Overall, the feed-forward MLP block introduces non-linearity while in higher dimensionality space to capture intricate non-linear relationships between the sequential features and the class tokens.\nThe attention block, the concatenation, and the MLP block all together constitute the single encoder block. The output features from the transformer encoder are now treated as h-level features. They can be fed to a suitable classifier to produce the predicted class level of the input image."}, {"title": "C. Classification Head", "content": "Finally, feature outputs from the two encoders are concatenated along the feature dimension. The concatenated features are processed by the custom classifier head designed to accept the concatenated features. The classification block with a softmax classifier at the last layer is known as the classification head. The feature output of each encoder channel has a shape of $b \\times d_h$. The concatenation features get a shape of $b \\times 2 * d_h$. Therefore, the classifier head has the first linear layer with the number of nodes equal to $2 * d_h$, followed by activation later, and the last softmax layer with nodes equal to the number of classes or subjects in the dataset."}, {"title": "V. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "In this study, we utilized the Forehead Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) Database [13], which was created with images from a large number of individuals, providing several samples per person. The images were captured in a controlled environment with ambient lighting maintained between 20 and 40 Lux to reduce the impact of external lighting, enabling the Near-Infrared (NIR) sensors to effectively capture the LED light reflected off the participants. Subjects were asked to stay within a 4 to 6 centimeter range from the NIR camera during the image collection phase. Prior to the session, individuals were recommended to clean their faces to remove any sweat or oil, ensuring the quality of the images. Each subject was positioned in front of the camera for about 10 seconds, allowing for the capture of multiple frames. The FSVP-PBP Database contains 2000 images, sourced from 200 individuals aged between 21 and 42 years. From the multitude of frames taken, the top 10 frames per subject were chosen. A subset of these selected images was then randomly picked to form the training dataset. This careful compilation process of the database guarantees a varied and comprehensive collection of data, bolstering the FSVP-PBP Database's utility for studies in biometric pattern recognition and analysis. A few"}, {"title": "B. Evaluation metrics", "content": "The proposed framework is evaluated in terms of the following evaluation metrics:\n1) Classification accuracy (CA): Classification accuracy (CA) as widely accepted in the literature [69]. CA is defined as\n$CA=\\frac{Number \\:of \\:correct \\:classifications}{Total \\:number \\:of \\:test \\:samples} \\times 100\\%$\n2) The Equal Error Rate (EER) is a metric for evaluating biometric system performance, where the False Acceptance Rate (FAR) and False Rejection Rate (FRR) meet. It balances security (lowering false acceptances) and usability (reducing false rejections), with a lower EER indicating higher accuracy.\n3) Training statistics of the models are evaluated in terms of %CA on the training and the validation datasets and are presented as training and validation curves with respect to epochs."}, {"title": "C. Implementation Details", "content": "The proposed dual encoder ViT as shown in Fig. 2 is implemented using the PyTorch library of Python on the Google Colaboratory platform. All the blocks for individual channels in the dual encoder ViT are defined similarly to the standard ViT model. The model's hyper-parameters were selected as follows: number of encoder blocks = 12, number of heads in multi-head attention blocks = 4, patch size = 16 \u00d7 16, input image size = 256, and number of hidden sizes for multi-head attention as well as MLP block = 768. Output features of each encoder channel assume the shape of 4\u00d7768, where 4 represents the batch size. Both feature outputs are concatenated along the feature dimension to obtain a feature size of 4 \u00d7 1536.\nA custom classifier is designed to accept input features of size 1536 with two intermediate layers to transform the feature size successively as 1536 \u2192 768 512 \u2192 200, where 200 is the number of classes in the dataset.\nThe weights of the model are initialized using Xavier initialization. The model is trained using raw images of the Forehead and the Periocular as described in V-A for 40 epochs on the NVIDIA A100-SXM GPU from the Google Co-Laboratory. The training statistics curve in terms of training accuracy and validation accuracy is shown in Fig. 4"}, {"title": "D. Comparison with State-of-the-art Methods on FSVP-PBP Database", "content": "1) Single channel framework: The provided data showcases the performance of various algorithms in recognizing vein and periocular patterns, indicating significant advancements in biometric identification techniques. For vein pattern recognition, the algorithms compared include Local Binary Patterns (LBP), VGG-16, ResNet-50, Lightweight CNN (L.CNN), and Vision Transformer (ViT), with their respective accuracies being 73.8\u00b12.1%, 82.6\u00b12.2%, 89.3 \u00b13.0%, 90.6 \u00b1 2.4%, 92.7 \u00b1 1.8%, and a proposed high of 97.5 \u00b1 1.4% as per the references [70], [34], [71], [72], [13] and the proposed method. Similarly, in periocular pattern recognition, the same set of algorithms demonstrates accuracies of 80.5 \u00b1 1.2%, 88.4 \u00b1 1.2%, 91.3 \u00b1 2.3%, 92.1\u00b11.9%, 96.5\u00b11.3% and 97.3\u00b11.6%, respectively. The Vision Transformer (ViT) algorithm, as the proposed method, shows a remarkable performance leap, achieving near-perfect accuracy rates of 97.5\u00b11.4% and 97.3\u00b11.6% for vein and periocular pattern recognition, respectively. This indicates a robust potential for ViT to enhance the security and reliability of biometric identification systems, outperforming traditional and deep learning-based methods by a significant margin.\n2) Double channel framework: In the exploration of biometric authentication methods, a comparative analysis was conducted on the performance of various algorithms in recognizing combined vein and periocular patterns. The study encompassed a range of techniques, including Local Binary Patterns (LBP), VGG-16, ResNet-50, Lightweight Convolutional Neural Networks (L.CNN), and the proposed Vision Transformer (ViT) model. According to the findings, the proposed ViT model outperformed the other evaluated algorithms, achieving an impressive accuracy of 99.3 \u00b1 0.02%. This was significantly higher compared to the accuracies obtained by LBP (84.6 \u00b1 2.1%), VGG-16 (90.3 \u00b1 1.6%), ResNet-50 (93.7 \u00b1 1.9%), L.CNN (93.2 \u00b1 1.0%), and VP-CNN (98.6 \u00b1 1.3%), as cited in references [70], [34], [71], [72], [13] and the proposed work, respectively. These results underscore the potential of Vision Transformers in enhancing the reliability"}, {"title": "E. Comparison with State-of-the-art Methods with other Biometric modalities", "content": "The proposed Vision Transformer (ViT) framework for biometric authentication, utilizing forehead subcutaneous vein patterns (FSVP) and periocular biometric patterns (PBP), demonstrates superior performance in comparison to state-of-the-art (SOA) biometric systems. The dual-channel ViT model achieves a classification accuracy of 99.32% and an Equal Error Rate (EER) of 0.0093 on the FSVP-PBP database, significantly outperforming other methods. For instance, the dual-channel convolutional neural network (DCNN) approach on the same database achieves a classification accuracy of 98.60% and an EER of 0.08, indicating the enhanced precision of the ViT framework. Other biometric systems, such as those utilizing finger vein patterns (FVP), dorsal hand vein patterns (DHVP), and dynamic palm vein matching (DPVM), exhibit lower classification accuracies and higher EERs. For example, the DPVM method on the PolyU database achieves a classification accuracy of 95.59% with an EER of 0.08, while another DPVM method on the CASIA database achieves an EER of 0.12. Furthermore, multimodal systems combining various patterns, such as iris and periocular patterns (IP+PBP), sclera and periocular patterns (SRP+PBP), and palm print with dorsal hand vein patterns (PPP+DHVP), also fall short in comparison. The IP+PBP system achieves a classification accuracy of 93.50% with an"}, {"title": "F. Complexity Analysis", "content": "The proposed dual-encoder ViT model consists of two channels of encoder blocks. Each channel had N hidden units of encoders. The time complexity of each encoder block is contributed by the multi-head attention, projection layer, and feedforward MLP layers. The time complexity of the multi-head attention layer is given by $O(n^2 \u00b7 d)$, where $n$ and $d$ are the sequence length and dimension of the heads, respectively [68]. The complexity of the MLP block and the projection layer is given by $O(n^3)$, where $n$ represents the number of learnable parameters. Therefore, the total time complexity of the one encoder block is given by $O(n^2 \u00b7 d+n^3)$. If the total number of hidden blocks in each channel encoder is N, then the total time complexity of the dual-encoder ViT is given by $O(2N \u00b7 (n^2 \u00b7 d+n^3))$, which can be approximated as $O(n^4)$."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we presented a novel dual-channel multi-attention Vision Transformer (ViT) framework for biometric authentication using forehead subcutaneous vein pattern and periocular pattern. Our research introduced an innovative architectural design, incorporating a dual-channel mechanism to process two distinct biometric traits independently, coupled with a custom classifier for integrating and classifying the extracted features. This approach leveraged the strengths of vision transformers in handling complex image data, enabling precise feature extraction and enhanced pattern recognition capabilities. The evaluation of our algorithm on the Forehead Subcutaneous Vein Pattern (FSVP) database demonstrated its superiority over state-of-the-art algorithms, achieving remarkable accuracy and setting a new benchmark in the domain of biometric authentication. The proposed framework's ability to accurately identify and classify patterns from the FSVP database underscores its potential as a robust and reliable solution for secure identity verification in various applications. Our findings not only contribute to the advancement of biometric authentication technologies but also open avenues for further research in exploring the capabilities of vision transformers in other domains of pattern recognition and image analysis."}]}