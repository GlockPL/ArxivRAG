{"title": "Twisting Lids Off with Two Hands", "authors": ["Toru Lin", "Zhao-Heng Yin", "Haozhi Qi", "Pieter Abbeel", "Jitendra Malik"], "abstract": "Abstract-Manipulating objects with two multi-fingered hands\nhas been a long-standing challenge in robotics, attributed to\nthe contact-rich nature of many manipulation tasks and the\ncomplexity inherent in coordinating a high-dimensional bimanual\nsystem. In this work, we consider the problem of twisting lids of\nvarious bottle-like objects with two hands, and demonstrate that\npolicies trained in simulation using deep reinforcement learning\ncan be effectively transferred to the real world. With novel\nengineering insights into physical modeling, real-time perception,\nand reward design, the policy demonstrates generalization capa-\nbilities across a diverse set of unseen objects, showcasing dynamic\nand dexterous behaviors. Our findings serve as compelling\nevidence that deep reinforcement learning combined with sim-\nto-real transfer remains a promising approach for addressing\nmanipulation problems of unprecedented complexity.", "sections": [{"title": "I. INTRODUCTION", "content": "We study the task of twisting or removing lids with\ntwo multi-fingered robot hands. This task is both practically\nimportant and profoundly interesting. For one, the ability\nto twist or remove lids from containers is a crucial motor\nskill that toddlers acquire during their early developmental\nstages [36, 54]. For another, the manipulation skills required\nfor this task, such as the coordination of fingers to manipulate\na multi-part object, can be generally useful across a large\ncollection of practical tasks.\nOur work demonstrates the feasibility of learning a dex-\nterous and dynamic bimanual manipulation policy purely in\nsimulation and zero-shot transferring it to the real world. By\nabstracting a class of articulated objects into a simplistic two-\npart object model with randomized physical properties, we\nsuccessfully train a policy that can generalize to a diverse set of\nnovel objects with different shapes, sizes, visual appearances,\nand other physical properties (Figure 1). Our method does\nnot require precise modeling of any individual object, or\nhardcoding prior knowledge on object properties. Instead, it\nallows for stable and natural twisting behaviors to emerge\nthrough large-scale reinforcement learning (RL) training.\nIn developing a system with these capabilities, we uncover a\nnumber of novel engineering insights, which we outline below.\nPhysical Modeling. Our work features a novel class of objects\nfor in-hand manipulation: articulated objects defined as two\nrigid bodies connected via a revolute joint with threaded\nstructure. Accurately modeling friction and contact with rev-\nolute joints and threaded structure has traditionally been a"}, {"title": "II. BACKGROUND", "content": "For decades, the problem of bimanual manipulation has\nremained a fascinating unsolved challenge in robotics [6, 23,\n35, 48, 49, 53]. While multi-fingered robot hands seem to be a\nnatural choice for bimanual robot systems in theory, designing\ncontrollers for high-dimensional action spaces remains an open\nproblem due to difficulties in mechanical design and reliable\nactuation. Most previous works in bimanual manipulation thus\nuse simple and durable parallel jaw-grippers as end-effectors.\nFor example, Caccavale et al. [5] demonstrate carrying and\nmoving objects using two arms with impedance control. Sarkar\net al. [45] show controlling object states using rolling contact.\nUnlike most previous works, our work focuses on bimanual\nmanipulation with two multi-fingered hands.\nWhile there exist prior works that utilize two multi-fingered\nhand for manipulation, they differ substantially from our work\nin terms of approach and results. Vahrenkamp et al. [53]\npresents a bimanual grasp planner that could be used to build\ngrasps for large objects using multi-fingered hands, but without\ndemonstrating any real-world results. Platt et al. [39] achieve\nobject reconfiguration using two three-fingered robot hands,\nbut do so by planning a sequence of low-level controllers.\nSteffen et al. [50] study a screwing task similar to our task\nof interest but synthesize the control motion sequence with a\nkernel approach applied to human motion data captured with\na data glove. Our work fills a missing space in the bimanual\nmulti-fingered manipulation literature, as we do not rely on\nany planner, accurate object model, or human data. Instead,\nwe directly work on a high-dimensional action space with RL\non randomized environments."}, {"title": "B. Learning Approaches for Bimanual Manipulation", "content": "In recent years, bimanual manipulation has been more\nactively studied with learning-based methods, as a result of\nprogress in learning algorithms and compute infrastructure.\nThese learning-based approaches can generally be categorized\ninto two types: 1) learning from real-world data; 2) learning\nin simulation, then transferring to the real world (sim-to-real).\nLearning from Real-World Data. Rapid progress has been\nmade in reinforcement learning (RL) in the real world. Zhang\net al. [58] learn to chain motor primitives for vegetable cutting,\nwith relatively motion primitives; much of the task difficulty\nis bypassed via the use of specialized end-effectors [1, 14].\nChiu et al. [12] learn precise needle manipulation with two\ngrippers by integrating RL with a sampling-based planner.\nWhile impressive, these works cannot easily scale to higher\ndimensional action space due to their sample inefficiency or\nthe need to define heuristic-based action primitives.\nMost recent successes in bimanual manipulation are\nachieved by learning from demonstrations [47, 51, 59]. How-\never, successes so far are largely limited to simple end-\neffectors like parallel jaw grippers. One reason is the lack\nof high-quality demonstration data from multi-fingered robot\nhands [24]; the availability of such hand hardware is itself lim-\nited, not to mention the sophisticated data collection infrastruc-\nture. Although several works aiming to improve demonstration\ndata collection with two arms [15, 26, 28] or multi-fingered\nhands exist [2, 3, 16, 42, 43], their latency and retargeting\nerrors limit their practical applicability and scalability. Our\nmethod uses RL in simulation and is thus not limited by the\nhardware and data collection infrastructure problems faced by\nlearning from demonstration approaches.\nSim-to-Real. There has been growing interest in sim-to-real\napproaches for robotics \u2013 i.e. learning policies in simulation\nand transferring them to the real world stimulated by\nseveral notable successes in recent years ranging from loco-\nmotion [19, 25, 32] to manipulation [8, 17, 34, 40]. Existing\nworks in manipulation, however, are mostly done with either a\nsingle multi-fingered hand [7, 10, 21, 38, 41, 44, 52, 55, 56], or\ntwo arms with simpler end-effectors [20, 27, 30]. While Chen\net al. [9] and Zakka et al. [57] feature bimanual tasks with\ndexterous hands, only simulation results are shown. Perhaps\nthe work most related to ours is Huang et al. [18], where\nthe authors demonstrate throwing and catching objects using"}, {"title": "III. TASK FORMULATION", "content": "Twisting lids of container objects is a complex in-hand ma-\nnipulation process that requires dynamic dexterity of multiple\nfingers and precise coordination between two hands. Below,\nwe define the specific task being considered in this work.\nTask Initialization. Each object of interest consists of two\nrigid, near-cylindrical parts (a \"body\" and a \"lid\"); the two\nparts are connected via a continuous revolute joint, allowing\nthem to rotate about each other. To better benchmark the\nbimanual twisting capability, we consider a class of articulated\nbottles with lids that can be twisted infinitely (see Section IV-B\nfor more details). The two robotic hands are initialized in a\nstatic pose with upward-facing palms. At the beginning of\neach episode, a bottle-like object is gently dropped or placed\nonto the fingers. The initial pose of the object is randomized\nboth in translation and rotation to a fixed default pose; the\ninitial joint positions of the hands are randomized about a\npredefined canonical pose by adding Gaussian noise. Note\nthat since we do not assume a stable grasp configuration at\ntask initialization, the control policy needs to learn in-grasp\nreorientation to place the object in a stable location to perform\nsuccessive manipulation.\nTask Objective. The goal of this task is to twist the lid\nabout the object's axis of rotation in one direction as much\nas possible; during this process, the object should always\nstay in hand. Achieving this involves a sequence of delicate\nmovements: 1) after initialization, the robot hand should firmly\ngrasp and slightly rotate the bottle to a suitable pose; 2) the\nhand that is closer to the object lid should place its finger"}, {"title": "IV. SYSTEM SETUP", "content": "To address the aforementioned challenges in bimanual ma-\nnipulation and our specific task choice, we develop a sim-to-\nreal pipeline based on a deep RL approach. In this section, we\ndescribe our overall system setup."}, {"title": "A. Real-world System", "content": "Hardware Setup. As shown in Figure 1, we use two 16-DoF\nAllegro Hands from Wonik Robotics for our experiments. Each\nAllegro Hand is mounted on a fixed UR5e arm. We employ\na single RealSense D435 depth camera to provide visual\ninformation, from which we extract object state information.\nWe send control commands to the robot at a frequency of\n10 Hz via a Linux workstation.\nPerception. Figure 3 shows an overview of our perception\npipeline. Instead of directly using pixels as the RL agent's"}, {"title": "V. LEARNING TO TWIST LIDS", "content": "Bimanual in-hand dexterous manipulation involves highly\ncomplex hand-object contacts, and remains challenging to\nsolve with traditional methods. In this work, we address\nthe control challenge through RL. We formulate our control\nproblem as a partially observable Markov Decision Process\n$M = (S, O, A, R, P)$, where S is the state space, A is the\naction space, O is the observation space, R is the reward\nfunction, and P is the environment dynamics. The control\npolicy generates an action $a_t \\in A$ to maximize the cumulative\nreward given an observation $o_t \\in O$."}, {"title": "A. Observation and Action", "content": "Observation Space. At each time step t, the control policy\nobserves the following information from the environment: the\nproprioceptive hand joint positions $q_t$, the estimated center-of-\nmass 3D positions of the bottle base and lid, and previously\ncommanded target joint positions $\\tilde{q}_t$."}, {"title": "Action Space.", "content": "We use an impedance PD controller to drive\nthe robot hand. The control policy produces a relative target\njoint position as the action $a_t$, which is added to the current\ntarget joint position $\\tilde{q}_t$ to produce the next target: $q_{t+1} =$\n$\\tilde{q}_t + \\eta EMA(a_t)$. $\\eta$ is a scaling factor. Note that we smooth the\naction with its exponential moving average (EMA) to produce\nsmooth motion. The next target position is sent to the PD\ncontroller to generate torque on each joint."}, {"title": "B. Reward Design", "content": "The reward function plays a critical role in defining the\nrobot's behavior. The bimanual in-hand manipulation system\nhas a higher action space dimension and much more complex\ncontact patterns compared to single-hand scenarios. This mag-\nnifies the challenges inherent in the RL exploration process,\nunderlining the need for more sophisticated reward designs.\nWhile one way to approach hard exploration problems is\nto add intrinsic rewards [4, 29], we propose a fine-grained,\nbehavior-aware reward design for this task. Our reward func-\ntion contains the following terms:"}, {"title": "Twisting Reward.", "content": "We define the twisting reward as\n$r_{\\text{twisting}} = \\Delta \\theta^{t+1}_{\\text{bottle}} \\cdot  \\Delta \\theta^{t}_{\\text{bottle}}$\nwhich is the rotation angle of the lid during one-step execution."}, {"title": "Finger Contact Reward.", "content": "We encourage natural object ma-\nnipulation behavior by defining a set of keypoints positioned\non the object to guide the fingers' interactions (Figure 4). We\ndefine two set of keypoints $X^L \\in \\mathbb{R}^{n \\times 3}$ and $X^R \\in \\mathbb{R}^{m \\times 3}$\nattached on the bottle base and lid respectively. Then, we\ndefine the finger contact reward as\n$r_{\\text{contact}} = \\sum_{i} \\left[ \\frac{1}{1 + \\alpha d(X^L_i, F^L)} + \\frac{1}{1 + \\alpha d(X^R_i, F^R)}\\right]$\nwhere $F^L \\in \\mathbb{R}^{4 \\times 3}$ and $F^R \\in \\mathbb{R}^{4 \\times 3}$ are the position of left\nand right fingertips, $\\alpha$ is a scaling hyperparameter, and d is a"}, {"title": "distance function", "content": "defined as\n$d(A, x) = \\min_{A_i} \\| A_i - x \\|_2$\nTherefore, we encourage each fingertip to stay as close to\none of the keypoints as possible. By choosing the keypoint\nproperly around the bottle base and lid, we can elicit natural\ngrasping and lid-twisting behavior."}, {"title": "Pose Reward.", "content": "We also introduce a pose matching reward\nterm to encourage the bottle main axis $x_{\\text{axis}}$ aligned with\na predefined direction v. This term is defined as\n$r_{\\text{pose}} = \\arccos( \\langle x_{\\text{axis}}, v \\rangle)$."}, {"title": "Other Regularizations.", "content": "The above three reward terms specify\nthe objective of our task. Besides these three rewards, we\nalso introduce another few regularization terms as in previous\nworks [55], including work penalty and action penalty to\npenalize large, jerky motions:\n$r_{\\text{action}} = -\\|a_t\\|^2$\n$r_{\\text{work}} = - \\langle \\tau, \\dot{q} \\rangle$."}, {"title": "", "content": "The reward function is a weighted sum of the above terms:\n$r = \\alpha_1 r_{\\text{contact}} + \\alpha_2 r_{\\text{twist}} + \\alpha_3 r_{\\text{pose}} + \\alpha_4 r_{\\text{action}} + \\alpha_5 r_{\\text{work}}$"}, {"title": "Reset Strategy.", "content": "Due to the high dimensionality of our learning\nproblem, there exist many possible ways how the robot interact\nwith the object; among these, most modes lead to failures such\nas dropping the object beyond recovery. Exploring these modes\nrarely leads to meaningful learning progress. To circumvent\nthis issue, we introduce several early termination criteria. Most\nimportantly, we reset an episode if the robot hands fail to rotate\nthe bottle into a desired pose for bimanual twisting within a\nshort time limit. Additionally, we also reset when the bottle's\nz-position is below a certain threshold, as the fingertips of the\ntwo hands can pinch the bottle at a low position without being\nable to reposition it into the palm."}, {"title": "C. Domain Randomization", "content": "We apply a wide range of domain randomizations to ensure\nzero-shot sim-to-real transfer, including both physical and non-\nphysical randomizations. Physical randomizations include the\nrandomization of object friction, mass, and scale. We also\napply random forces to the object to simulate the physical\neffects that are not implemented by the simulator. Non-\nphysical randomizations model the noise in observation (e.g\njoint position measurement and detected object positions)\nand action. A summary of our randomization attributes and\nparameters is shown in Table I."}, {"title": "D. Training", "content": "We use the proximal policy optimization (PPO) algorithm\nto learn RL policies. We use an advantage clipping coefficient\n$\\epsilon$ = 0.2; a horizon length of 16, with $\\gamma$ = 0.99, and general-\nized advantage estimator (GAE) [46] coefficient $\\tau$ = 0.95.\nThe policy network is a three-layer MLP with ELU [13]"}, {"title": "VI. SIMULATED EXPERIMENTS", "content": "We study the following questions in simulation:\nReward Design. How important is the keypoint-based reward\nfor eliciting desired twisting behavior in bimanual manip-\nulation? How effective is it compared to the other reward\nfunctions used in recent works?\nPerception. How important is the visual information in solving\nthis task? Is a sparse, keypoint representation enough for\nlearning a policy that can handle multiple objects?"}, {"title": "A. Setup", "content": "Object Set. In the simulated experiments, we utilize a collec-\ntion of simulated cylindrical bottles with varying aspect ratios\nfor both training and evaluation. Some samples are visualized\nin Figure 5. We consider two setups in simulation: 1) Multi-\nobject setup, in which all the objects are used, and 2) Single-\nobject setup, in which we only use a medium-sized bottle that\nrepresents the mean of the dataset.\nEvaluation Metric. We introduce the following metrics for\nevaluating the performance:\nAngular Displacement (AD) is the total number of degrees\nthrough which the lid has been twisted."}, {"title": "B. Main Results", "content": "Reward Design. We first compare our approach with the\nreduced finger reward baseline (Figure 6). After decreasing\nthe scale of finger contact reward, learned policies fail to\nmaster the desired lid-twisting skill and have low performance\nin general. We hypothesize that this is because the motion of\nlid-twisting requires a very specific pose pattern for holding\nthe object; without explicitly encouraging such a pose pattern\n(e.g., via its contact modes), RL exploration becomes so hard\nthat it is unsolvable within the available training time. We also\nobserve a positive correlation between the intensity of finger\ncontact reward and both 1) sample efficiency during learning,\nand 2) performance of learned policies (as reflected by the AD\nscore).\nVision vs No Vision. We also study the importance of vision\nmodality in solving our considered task. Existing works show\nthat certain rotation behavior can be achieved through implicit\ntactile sensing (joint proprioception data) [40], but it is unclear\nwhether the same conclusion can be drawn for the bimanual\nlid-twisting task. Our empirical results show that, in both\nsingle and multi-object setups, the no-vision baseline performs\nsubstantially worse than our full method. This suggests that\nknowledge of the position of bottle keypoints is essential for\nsuccessful lid-twisting.\nSingle Object vs Multi Object. We run RL training with\ntwo object settings: (1) using a single bottle-like object and\n(2) using multiple bottle-like objects with more variation in\nthe ratio between the bottle base and lid. The two settings\npose a trade-off between specialization and generalization: in\nthe single-object scenario, the policy might learn successful\nbehaviors more easily but find it harder to generalize to unseen\nobjects, and vice versa. To our surprise, we observe that multi-\nobject training yields slightly better performance compared\nto single-object training. We hypothesize that multi-object\nmakes exploring lid-twisting behavior an easier process by\nintroducing an object curriculum that covers both easy and\nhard object instances during training."}, {"title": "C. Qualitative Results", "content": "In this section, we study the reward design by examining\ntheir induced behavior. Specifically, we compare our method\nto the reduced contact reward baseline and the gaining reward\nbaseline. Visualization of trajectories from policies trained\nwith each method is shown in Figure 7. Policies trained\nwithout the finger contact reward exhibit extremely unstable\ngrasping or screwing motions; the behaviors not only appear\nunnatural in simulation, but also completely fail to transfer to\na real-world setup. We hypothesize that the use of two hands,\neach with a high degree of freedom, leads to a massive search\nspace for in-hand object manipulation; introducing task priors\nlike behavior-aware contact reward is therefore essential for\nreducing the search space and eliciting natural behavior."}, {"title": "VII. REAL-WORLD EXPERIMENTS", "content": "In this section, we first demonstrate that our final learned\npolicy can be directly transferred to the real world in a zero-\nshot manner. Then, we investigate the effect of several key\ndesign choices on the success of sim-to-real transfer. Finally,\nwe examine our policy's ability to generalize, as well as its\nrobustness against various kinds of perturbations."}, {"title": "A. Setup", "content": "For quantitative evaluation, we evaluate the sim-to-real\ntransfer capabilities of our policies on five different articulated\nbottle objects (Figure 5). Among them, four are in-distribution"}, {"title": "C. Generalization to Novel Objects", "content": "We further test our policy's ability to generalize by testing\nit on an additional 10 novel objects commonly found in\nhouseholds (Figure 1). These objects substantially differ from\nour training objects in terms of shape, size, mass, material,\nand color. Besides, they are also fundamentally different in\nterms of mechanical design. While the lids of the synthetic\nbottles that we use for both simulation training and real-world\ntesting can be twisted infinitely, the lids of these household\nobjects cannot. Therefore, to evaluate our policy's ability to\ngeneralize the lid-twisting skill to these novel objects, we use\nlid removal as a success criterion. We define lid removal as\nthe object lid being completely detached from the object body,\nas can be seen in the case when the lids fall from the robot\nhands in Figure 1. We find that our policy can achieve about\na 30% success rate. The results can be found in the video\nsupplementary materials."}, {"title": "D. Robustness against Perturbation", "content": "Finally, we also evaluate our policy's robustness against\nforce perturbation. Specifically, we perturb the object during"}, {"title": "VIII. CONCLUSION", "content": "In this paper, we consider the task of twisting or removing\nlids of bottle-like objects with two hands. We present a RL-\nbased sim-to-real system to solve this problem, and propose\nvarious techniques to handle the challenges that arise \u2013 includ-\ning a novel reward design, a sparse object representation for\nreal-time perception, and an efficient yet high-fidelity method\nto simulate twisting bottle caps. We conduct experiments in\nboth simulation and real world to demonstrate the effectiveness\nof our approach. Our real-world results show generalization\nacross a wide range of seen and unseen objects. We hope\nour system can inspire future researchers to tackle bimanual\ndexterous hands challenges with RL and sim-to-real."}, {"title": "APPENDIX", "content": ""}, {"title": "A. Object Details", "content": "Simulated Bottles. We use Isaac Gym [31] to model the\nsimulated learning environments.\nFor the multi-object environment, we use bottles whose\nbodies range from 82cm to 86cm in diameter and 55cm to\n67cm in height, and whose caps range from 62cm to 70cm in\ndiameter and 20cm to 33cm in height.\nFor the single-object environment, we use a bottle whose\nbody is 84cm in diameter and 60cm in height, and whose cap\nis 67cm in diameter and 26cm in height."}, {"title": "B. Implementation Details", "content": "Action Hyperparameters. To generate action commands, we\nclip neural network policy output to [-1,1] range. We then\napply an action scale of 0.1 and a moving average parameter\nof 0.75 to the actions.\nReward Hyperparameters. We use $\\alpha_1$ = 2.5, $\\alpha_2$ = 500.0,\n$\\alpha_3$ = 20, $\\alpha_4$ -0.001, and $\\alpha_5$ = -1.0 as reward weights.\nAsymmetric States. In addition to the policy inputs, we\nprovide the following privilege state inputs to the value net-\nwork of asymmetric PPO: hand joint velocities, all fingertip\npositions, all contact keypoint positions, object orientation,\nobject velocity, object angular velocity, random forces applied\nto object, object brake torque, object mass randomization\nscale, object friction randomization scale, and object shape\nrandomization scale."}, {"title": "C. Real-World Experiment Details", "content": "Camera Calibration. We use a novel marker-based approach\nto calibrate the extrinsics matrix of our camera. Specifically,\nwe add the marker tag used in our real-world setup into\nthe simulation environment, such that pair coordinates of the\nmarker corners can be obtained easily in both the camera\nframe and the world frame (Figure 10). We then use the\npaired coordinates to solve for camera extrinsics. Doing so\ngreatly reduces the manual labor required by other camera\ncalibration approaches, such as capturing checkerboard images\nand solving for multiple extrinsic matrices."}]}