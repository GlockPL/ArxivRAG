{"title": "UNSUPERVISED ANOMALY DETECTION THROUGH MASS REPULSING OPTIMAL TRANSPORT", "authors": ["Eduardo Fernandes Montesuma", "Adel El Habazi", "Fred Ngol\u00e8 Mboula"], "abstract": "Detecting anomalies in datasets is a longstanding problem in machine learning. In this context, anomalies are defined as a sample that significantly deviates from the remaining data. Meanwhile, optimal transport (OT) is a field of mathematics concerned with the transportation, between two probability measures, at least effort. In classical OT, the optimal transportation strategy of a measure to itself is the identity. In this paper, we tackle anomaly detection by forcing samples to displace its mass, while keeping the least effort objective. We call this new transportation problem Mass Repulsing Optimal Transport (MROT). Naturally, samples lying in low density regions of space will be forced to displace mass very far, incurring a higher transportation cost. We use these concepts to design a new anomaly score. Through a series of experiments in existing benchmarks, and fault detection problems, we show that our algorithm improves over existing methods.", "sections": [{"title": "1 Introduction", "content": "An anomaly, or an outlier, is a data point that is significantly different from the remaining data [Aggarwal, 2017], to such an extent that it was likely generated by a different mechanism [Hawkins, 1980]. From the perspective of machine learning, Anomaly Detection (AD) wants to determine, from a set of examples, which ones are likely anomalies, typically through a score. This problem finds applications in many different fields, such as medicine Salem et al. [2013], cyber-security Siddiqui et al. [2019], and system monitoring Isermann [2006], to name a few. As reviewed in Han et al. [2022], existing techniques for AD are usually divided into unsupervised, semi-supervised and supervised approaches, with an increasing need for labeled data. In this paper, we focus on unsupervised AD, which does not need further labeling effort in constituting datasets. As discussed in Livernoche et al. [2024], the growing number of applications involving high-dimensional and complex data begs the need for non-parametric algorithms.\nMeanwhile, Optimal Transport (OT) is a field of mathematics concerned with the transportation of masses at least effort Villani et al. [2009]. In its modern treatment, one can conceive transportation problems between probability distributions, which has made an important impact in machine learning research [Montesuma et al., 2024a]. Hence, OT is an appealing tool, as it can be estimated non-parametrically from samples from probability distributions. Likewise, the plethora of computational tools for computing OT [Peyr\u00e9 and Cuturi, 2020, Flamary et al., 2021] further stresses its usability.\nIn this context, the application of OT for AD is not straightforward, as we are interested in analyzing a single probability distribution, rather than 2. This paper puts forth a new OT problem between a distribution and itself, by restricting where a sample can send its mass to. More specifically, we design an exclusion zone, prohibiting samples from keeping its mass, or sending its mass to a small vicinity. Especially, we assume that anomalies lie in low density regions of space, with only a few samples in their vicinity. By restricting the transport of mass in the vicinity of samples, anomalies are naturally forced to send its mass to the high density region, which is assumed to be far away from the anomaly samples."}, {"title": "2 Related Work", "content": "Anomaly Detection. Following Han et al. [2022], AD methods can be mainly divided into 3 categories. First, supervised methods consider AD through the lens of binary classification under class imbalance. Second, semi-supervised methods either consider partially labeled data, or labeled normal samples, so that an algorithm can be characterize what a normal sample is. The third, more challenging category is unsupervised AD, where the training data contains both anomalies and normal samples and labels are not available. This paper considers precisely the last setting. Next, we review ideas in unsupervised AD.\nThe first kind of methods rely on encoder-decode architectures to detect anomalies. The insight is that, by embedding data in a lower dimensional space, anomalies can be detected via the reconstruction error of the auto-encoding function. This is the principle of Principal Component Analysis (PCA) Shyu et al. [2003], which employs linear encoding and decoding functions, but also of kernelized versions Sch\u00f6lkopf et al. [1997], Hoffmann [2007], as well as neural nets Vincent et al. [2008], Bengio et al. [2013], which rely on non-linear embedding techniques.\nThe second type of strategies are based on the paradigm of 1-class classification. As Sch\u00f6lkopf et al. [1999] puts, the idea is to define a function that outputs 0 on a small, dense region of the space where normal samples lie, and 1 elsewhere. In this context, Sch\u00f6lkopf et al. [1997] extends the celebrated Support Vector Machine (SVM) to AD, and Liu et al. [2008] extends Random Forests (RFs) of Breiman [2001].\nA third kind of approaches focuses on neighborhoods and clustering, to model the data underlying probability distribution, especially through the density of samples over the space. This is the case of k-Nearest Neighbors (k-NN) [Ramaswamy et al., 2000], who use distances and nearest neighbors to determine anomalies, Local Outlier Factor (LOF) Breunig et al. [2000], who devised a score that measures the local deviation of a sample with respect its neighbors. Finally, Clustering-based LOF (CBLOF) He et al. [2003] proposed an extension of LOF based on the relative sizes of clusters within the data.\nA fourth kind of emerging ideas in the field considers generative modeling for detecting anomalies. In this context, one devises a model for the data density P, for instance, through neural nets Goodfellow et al. [2014], Kingma and Welling [2013]. The idea from these models, is that, either the data density can be accurately estimated [Dias et al., 2020], or some score can be derived from the generative model [Livernoche et al., 2023].\nAs we cover in the next section, our method uses nearest neighbors and OT to model, non-parametrically, the density of samples over the space. More specifically, we prohibit samples in OT to keep their mass, or sending it over a region of space defined through their k-NN. Differently from Ramaswamy et al. [2000] and Breunig et al. [2000], we do not rely on distances, which might not have a meaning in high-dimensions. Rather, we rely on the effort of transportation, measured through the samples' mass times the ground-cost."}, {"title": "Optimal Transport with Repulsive Costs", "content": "In general OT theory (see, e.g., Section 3.1 below), samples are transported based on a ground-cost that measures how expensive it is to move masses between measures. In its original conception by Monge [1781] and Kantorovich [1942], this ground cost is the Euclidean distance $c(x_1, x_2) = ||x_1 - x_2||^2$. As reviewed in Di Marino et al. [2017], it may be interesting to consider repulsive costs, i.e. functions c that are big when x1 and x2 are close to each other and small otherwise. An example of such costs, arising from physics, is the Coulomb interaction $c(x_1, x_2) = (||x_1 - x_2||^2)^{-1}$. Still following Di Marino et al. [2017], these kinds of transportation problems proved useful in physics, e.g., for quantum mechanics and N-body systems.\nIn this paper, we consider a different kind of repulsive cost, which we call the mass repulsive cost (see, e.g., Section 3.2 below). Our notion of cost defines an exclusion zone where sending mass is too costly. This exclusion zone is defined on the nearest neighbor analysis of points in a probability measure. As a result, our approach captures the local"}, {"title": "3 Proposed Method", "content": "OT is a field of mathematics, concerned with the displacement of mass between a source measure, and a target measure, at least effort. In the following, we cover the principles of OT in continuous and discrete settings. We refer readers to Peyr\u00e9 and Cuturi [2020] for a computational exposition of the main concepts, and Montesuma et al. [2024a] for applications in machine learning. In the following, we are particularly interested in the formulation by Kantorovich [1942], which is defined as,\nDefinition 3.1. (Kantorovich Formulation) Let P and Q be 2 probability measures over a set X. Let $c : X \\times X \\rightarrow R$ be a ground-cost, measuring the effort of transporting units of mass from x to y. Let $\\Gamma(P,Q) = {\\gamma \\in P(X \\times X) : \\int_{\\mathcal{X}} \\gamma(x, B)dx = Q(B) \\text{ and } \\int_{\\mathcal{X}} \\gamma(A,y)dy = P(A)}$ be the set of transportation plans, whose marginals are P and Q. The optimal transportation problem is written as,\n$\\gamma^* = OT(P,Q) = \\underset{\\Upsilon\\in\\Gamma(P,Q)}{\\operatorname{arg\\ inf}} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x,y)d\\gamma(x,y)$.\nEquation 1 defines the transportation problem as an infinite dimensional linear program on the variable $\\gamma$, called transport plan. In our case, instead of having access to a closed-form P, one has samples $\\{x_i^{(P)}\\}_{i=1}^n$, each $x_i^{(P)} \\in \\mathcal{X}$, with probability $p_i$. In such cases, P may be approximated with an empirical measure,\n$P(x) = \\sum_{i=1}^{n} p_i \\delta(x - x_i^{(P)})$.\nSince the $p_i$s represent the probability of sampling $x_i^{(P)}$, $\\sum_i p_i = 1$ and $p_i \\geq 0$. Plugging back equation 2 into equation 1 leads to a finite linear program,\n$\\gamma^* = \\underset{\\gamma \\in \\Gamma(p,q)}{\\operatorname{arg\\ min}} (\\gamma, C)_F = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\gamma_{ij} c(x_i^{(P)}, x_j^{(Q)})$,"}, {"title": "3.1 Optimal Transport", "content": "characteristics of the probability measure being analyzed, especially its density. A special characteristic of our approach is to give a sense of the transportation from a measure to itself."}, {"title": "3.2 Mass Repulsing Optimal Transport", "content": "In this section, we propose a new OT problem called MROT. This problem is inspired by the Kantorovich formulation, described in section 3.1. However, instead of considering two different probability measures P, Q, it considers the transportation of P to itself. Due the properties of OT, if we consider the OT plan $\\gamma^* = OT(P, P)$, it is supported on the set $\\{(x, x), x \\in \\mathcal{X}\\}$, i.e., each point keeps its own mass [Santambrogio, 2015]. This motivates our new problem, in which we force points to repell its mass. We do so, by engineering the ground-cost $c : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}_{+}$. For pairs $x \\in \\mathcal{X}, y \\in \\mathcal{X}$,\n$\\check{c}_k(x, y) = \\begin{cases} c(x, y) & \\text{if } y \\notin \\mathcal{N}_k(x), \\\\ \\underset{z \\in \\mathcal{N}_k(x)}{\\operatorname{sup}} c(x, z) & \\text{otherwise}. \\end{cases}$\nthis principle is different from repulsive costs [Di Marino et al., 2017], which are designed to model the interaction between particles in multi-marginal OT. Indeed, in our work we are designing a transportation problem from a probability measure to itself. While repulsive costs incentive transportation towards distant points in space, our mass repulsing cost induces transportation just outside an exclusion zone.\nWhile the mass repulsing principle can be applied to any ground-cost c, let us focus on the metric case, i.e., when $c(x,y) = d(x,y)^2$, where $(\\mathcal{X}, d)$ is a metric space. The ground-cost we are proposing is essentially defining an exclusion zone, i.e., the k-vicinity $\\mathcal{N}_k (x)$ centered at x, where x cannot send its mass. Of course, since $x \\in \\mathcal{N}_k(x)$, it is forced to send its mass elsewhere \u2013 thus, we call this idea mass repulsive OT.\nOur main hypothesis for anomaly detection is that anomalous points lie in low density regions of P. On the one hand, If these points are forced to send its mass outside its vicinity, it will be forced to send it to high density regions of P otherwise, mass conservation in OT would not hold. On the other hand, if the exclusion zone is smaller than high density regions of P, points on these regions will be sent close-by. As a result, anomalous points will have a higher transportation cost, given by $\\gamma(x, y)\\check{c}_k (x, y)$, than normal points. We thus consider the following OT problem,\n$\\Upsilon^* = MROT_{k,\\epsilon} (P) = \\underset{\\Upsilon\\in\\Gamma(P,P)}{\\operatorname{inf}} \\int_{\\mathcal{X} \\times \\mathcal{X}} \\check{c}_k(x, y)d\\gamma(x, y) + \\epsilon KL(\\gamma|P \\otimes P)$,"}, {"title": "3.3 Building and Generalizing an Anomaly Score", "content": "Through MROT, we want to build a score for samples, based on how anomalous they are. Assuming some samples lie in a low density region of space, our MROT problem forces those samples to send their mass to distant parts of the feature space. In contrast, normal samples can send their mass to the immediate neighborhood outside the exclusion zone. As a result, we can sort out normal from anomalous samples using the transportation effort,\n$T(x) = \\mathbb{E}_{y \\sim \\gamma(x)} [\\check{c}_k (x, y)] = \\int_{\\mathcal{X}} \\check{c}_k(x, y)d\\gamma(y|x)$,"}, {"content": "where $\\gamma(y|x)$ corresponds to the conditional probability, calculated through the joint $\\gamma(x, y)$, given x. For empirical measures, this quantity can be calculated as follows,\n$T_i = \\sum_{j=1}^{n} \\frac{\\gamma_{ij}}{p_i} \\check{c}_k (x_i^{(P)}, x_j^{(P)})$,"}, {"content": "where $p_i$ is the importance of the i-th sample (e.g, $p_i = n^{-1}$ for uniform importance). Interestingly, equation 9 is similar to the barycentric map, widely used in domain adaptation Courty et al. [2016], Montesuma et al. [2023]. T has 2 shortcomings as an anomaly score. First, it is hardly interpretable, as its range depends on the choice of ground-cost c. Second, it is only defined in the support of P. We offer a solution to both of these problems.\nConcerning interpretability, we propose to transform it using the Cumulative Distribution Function (CDF) of its values. Let $\\mathcal{P}_T$ be the probability measure associated with $T(x)$. The CDF is simply $F_T(t) = \\Pr((-\\infty, t])$. Naturally, since $\\mathcal{P}_T$ is not available, it may be approximated from samples $\\{T_i\\}_{i=1}^n$, obtained through equation 9. We do so through, Kernel Density Estimation (KDE),\n$\\hat{k}(t) = \\frac{1}{n\\sigma} \\sum_{i=1}^n \\phi(\\frac{t - T_i}{\\sigma})$,"}, {"content": "where K is a kernel function (e.g., the Gaussian kernel $\\phi(x) = \\exp(-x^2/2)$), and $\\sigma$ is the bandwidth parameter, controlling the smoothness of k and determined through Scott's rule [Scott, 1979]. Equation 10 provide an approximation for the density of transportation efforts $\\{T_i\\}_{i=1}^n$. The CDF can be easily calculated from this estimate, via,\n$\\hat{K}(t) = \\int_{-\\infty}^t \\hat{k}(t)dt$.\nThe CDF is appealing, as it is a monotonic function over transportation efforts $t \\in \\mathbb{R}$, and it takes values on [0, 1], both of which are desirable properties for an anomaly score.\nThe issue on how to extrapolate the anomaly score for new samples remain. For instance, even if we use the CDF values as anomaly scores, one needs to recalculate $t = \\hat{T}(x)$ for a new sample, before evaluating $\\hat{K}(t)$ or $\\hat{k}(t)$. As we previously discussed, this is challenging, as $\\hat{T}$ is only defined on the support of P. A naive approach would be"}, {"title": "4 Experiments", "content": "We divide our experiments in 4 parts. Section 4.1 shows a toy example, going through all the steps in our algorithm. Section 4.2 shows our results on AdBench [Han et al., 2022]. Section 4.3 shows our experiments in fault detection on the Tennessee Eastman Process [Montesuma et al., 2024b, Reinartz et al., 2021]. Finally, Section 4.4 explores the robustness of our methods to various hyper-parameters and design choices."}, {"title": "4.1 An introductory toy example", "content": "Before diving into comparing our method with prior art, we give an introductory example that illustrates how we create anomaly scores out of samples. In this example, we sample normal examples $x_i \\sim \\mathcal{N}(0,0.25 I_2)$, and anomalous samples $y_j \\sim \\mathcal{N}([-3, -3], 0.01I_d)$, where $I_2$ is a $2 \\times 2$ identity matrix. The dataset for this toy example consists of the concatenation $\\{x_1, \\ldots,x_n, y_1, \\ldots, y_m\\}$, where n = 500 and m = 25, which means that roughly 5% of the total number of samples are anomalies. In the following, we compare our engineered cost with a regularized Coulomb interaction, $C_{ij} = (1 + ||x_i^{(P)} - x_j^{(P)}||^2)^{-1}$."}, {"title": "4.4 Robustness", "content": "Our method has two hyper-parameters, the entropic regularization penalty e, and the number of nearest neighbors k. In our experiments, we evaluated our methods over the values $\\epsilon \\in \\{0,10^{-2},10^{-1}, 10^{0}\\}$, where e = 0 implies using exact OT, i.e., linear programming. For MROT, we use k $\\in \\{5, 10, 20, ,50\\}$. Note that, while e is linked to the transportation plan, k is linked to the ground-cost."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel, general purpose anomaly detection algorithm based on optimal transport theory. Our method works under the assumption that the local neighborhood of anomalous samples is likely more irregular than that of normal samples. Based on this idea, we engineer the ground-cost in optimal transport, to encourage samples to send their mass just outside an exclusion zone, defined through its k-nearest neighbors. While this idea bears some similarity to optimal transport with repulsive costs [Di Marino et al., 2017], the defined ground-cost is not repulsive, and, as we show in our experiments, it leads to better anomaly detectors. We thoroughly experiment on the AdBench [Han et al., 2022] benchmark, and the Tennessee Eastman process [Downs and Vogel, 1993, Reinartz et al., 2021, Montesuma et al., 2024b], showing that our method outperforms previously proposed methods. However, as we highlight in section 4.2, our method inherits the limitations of classic optimal transport [Montesuma et al., 2024a]. Especially, it needs O($n^2$) computational storage, has O($n^3 \\log n$) computational complexity and is difficult to estimate in high dimensions. Nonetheless, as our experiments demonstrate, our method is practical and performative in real-world anomaly detection scenarios."}]}