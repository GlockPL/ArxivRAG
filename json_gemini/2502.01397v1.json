{"title": "Can message-passing GNN approximate triangular factorizations of sparse matrices?", "authors": ["Vladislav Trifonov", "Ekaterina Muravleva", "Ivan Oseledets"], "abstract": "We study fundamental limitations of Graph Neural Networks (GNNs) for learning sparse matrix preconditioners. While recent works have shown promising results using GNNs to predict incomplete factorizations, we demonstrate that the local nature of message passing creates inherent barriers for capturing non-local dependencies required for optimal preconditioning. We introduce a new benchmark dataset of matrices where good sparse preconditioners exist but require non-local computations, constructed using both synthetic examples and real-world matrices. Our experimental results show that current GNN architectures struggle to approximate these preconditioners, suggesting the need for new architectural approaches beyond traditional message passing networks. We provide theoretical analysis and empirical evidence to explain these limitations, with implications for the broader use of GNNs in numerical linear algebra.", "sections": [{"title": "1. Introduction", "content": "Preconditioning sparse symmetric positive definite matrices is a fundamental problem in numerical linear algebra (Benzi, 2002). The goal is to find a sparse lower triangular matrix L such that $L^{-T}AL^{-1}$ is well-conditioned, which allows faster convergence of iterative methods for solving linear systems. Recently, there has been significant interest in using Graph Neural Networks (GNNs) to predict such sparse preconditioners (Chen, 2024; Trifonov et al., 2024; H\u00e4usner et al., 2023). The key idea is to represent the sparse matrix A as a graph, where nodes correspond to variables and edges correspond to non-zero entries, and use GNN architectures to predict the entries of the preconditioner L, minimizing the certain functional.\nWhile this GNN-based approach has shown promise in some cases, we demonstrate fundamental limitations that arising from the inherently local nature of message-passing neural networks. Specifically, we show that there exist classes of matrices, starting from simple ones such as tridiagonal matrices arising from discretization of PDEs, where optimal sparse preconditioners exist but exhibit non-local dependencies - changing a single entry in A can significantly affect all entries in L. This means, that message passing GNNs, having limited receptive field, can not represent such non-local mappings. To address these limitations, we introduce a new benchmark dataset of matrices where optimal sparse preconditioners are known to exist but require non-local computations. We construct this dataset using both synthetic examples and real-world matrices from the SuiteSparse collection. For synthetic benchmarks, we carefully design tridiagonal matrices where the Cholesky factors depend non-locally on the matrix elements by leveraging properties of rank-1 semiseparable matrices. For real-world problems, we explicitly compute so-called K-optimal preconditioners based on the inverse matrix with sparsity patterns matching the lower-triangular part of the original matrices.\nOur experimental results demonstrate that current GNN architectures, including variants like Graph Attention Networks and Graph Transformers, struggle to approximate these preconditioners. This suggests fundamental limitations in the ability of message-passing neural networks to capture the non-local dependencies required for optimal pre-conditioning. We provide both theoretical analysis and empirical evidence showing why new architectural approaches beyond traditional GNNs are needed for this important problem in scientific computing."}, {"title": "2. Problem formulation", "content": "Let A be a sparse symmetric positive definite matrix. The goal is to find a sparse lower triangular matrix L such that $LL^T$ approximates A well, i.e. the condition number of $L^{-T}AL^{-1}$ is small. This is known as incomplete Cholesky factorization.\nRecent works propose using Graph Neural Networks to learn such factorizations. The key idea is to represent the sparse matrix A as a graph, where nodes correspond to variables and edges correspond to non-zero entries. A GNN then processes this graph to predict the non-zero entries of L.\nSpecifically, each node i has features derived from the corresponding diagonal entry Aii, while each edge (i, j) has features based on the off-diagonal entry Aij. Multiple rounds of message passing aggregate information from neighboring nodes and edges. The final node/edge embeddings are used to predict the entries of L that preserve the sparsity pattern of A. This architecture is local, which means if we modify a single entry of A, the change will propagate only to the neighboring nodes and edges. The size of this neighborhood is limited by the receptive field of the GNN, which is proportional to the depth of the network. to the number of message passing layers. Each layer, however, adds additional parameters to the model, makeing it more difficult to train."}, {"title": "2.1. Limitations of GNN-based Preconditioners", "content": "Conside the mapping $f : A\\rightarrow L$, where A is a given symmetric positive definite matrix, and L is a sparse lower triangular matrix with a given sparsity pattern. In this section we will provide a an example of sparse matrices A, when:\n\u2022 A is a sparse matrix and there exists an ideal factorization $A = LL^T$, where L is a sparse matrix.\n\u2022 The mapping of A to L is not local: a change in one entry of A can significantly affect all entries of L. The message-passing GNN are inherently local, and therefore cannot learn such mappings directly.\nThe simplest class of such matrices are positive definite tridiagonal matrices. Such matrices appear from the standard discretization of one-dimensional PDEs. It is well-known that for such matrices the Cholesky factorization is given as\n$A = LL^T$,\nwhere L is bidiagonal matrix,a and that is what we are looking for: the ideal sparse factorization of a sparse matrix. Our goal is to show tha the mapping (1) is not local: i.e. changing one entry of A will change other entries of L. Lets consider first the case of discretization of the Poisson equation on a unit interval with Dirichlet boundary conditions. The matrix A is given by the second order finite difference approximation,\n$A =\\begin{pmatrix}\n2 & -1 & 0 & \\cdots & 0\\\\\n-1 & 2 & -1 & \\cdots & 0\\\\\n0 & -1 & 2 & \\vdots & \\vdots\\\\\n\\vdots & \\vdots & & & -1\\\\\n0 & 0 & \\cdots & -1 & 2\n\\end{pmatrix}$"}, {"title": "3. Constructive approach", "content": "The class of tridiagonal matrices will serve as the basis for our synthetic benchmarks for learning triangular preconditioners. What approaches can we take for other, more general sparse positive definite matrices? In this subsection, we present a constructive approach for building high-quality preconditioners that cannot be represented by GNNs (as demonstrated in our numerical experiments).\nFor this task, we draw attention to the concept of K-condition number, introduced by Kaporin (Kaporin, 1994). By minimizing this condition number, we can constructively build sparse preconditioners of the form $LL^T$ for many matrices, where the sparsity pattern of L matches the sparsity pattern of the lower triangular part of A. The K-condition number of a matrix $A = A^* > 0$ is defined as:\n$K(A) = \\frac{Tr(A)}{(det(A))^{1/n}}$\nThe interpretation of (3) is that it represents the arithmetic mean of the eigenvalues divided by their geometric mean. For matrices with positive eigenvalues, it is always greater than 1, equaling 1 only when the matrix is a multiple of the identity matrix. Given a preconditioner X, we can assess its quality using K(XA). This metric can be used to construct incomplete factorized inverse preconditioners $A^{-1} \\approx LL^T$ where L is sparse. However, our focus is on constructing incomplete factorized preconditioners $A \\approx LL^T$ with sparse L. Therefore, we propose minimizing the functional:\n$K(L^TA^{-1}L) \\rightarrow min$,\nwhere L is a sparse lower triangular matrix with predetermined sparsity pattern. The strategy of utilizing the inverse matrix in preconditioner optimization is very promising and as been explored in other works (Li et al., 2023; Trifonov et al., 2024) through the functional:\n$||LL^TA^{-1} - I|| \\rightarrow min$.\nMore naive functionals like $||A \u2013 LL^T ||$ tend to prioritize approximating components corresponding to larger eigenvalues. For matrices arising from partial differential equations (PDEs), high frequencies are often \"non-physical\", making the approximation of lower frequencies more crucial for preconditioner quality. The distinctive advantage of functional (4) is that the minimization problem can be solved explicitly using linear algebra techniques. This enables us to construct pairs (Li, Ai) for small and medium-sized problems where $L_iL_i^T$ serves as an effective preconditioner. These pairs provide valuable benchmarks for evaluating preconditioner learning algorithms and comparing their properties against matrices that minimize (4)."}, {"title": "4. K-optimal preconditioner based on inverse matrix for sparse matrices", "content": "In this section, we analyze the preconditioner quality functional:\n$K(L^TA^{-1}L) \\rightarrow min$,\nwhere L is a sparse lower triangular matrix with predetermined sparsity pattern. We will derive an explicit solution to this optimization problem."}, {"title": "4.1. Solution of the optimization problem", "content": "Let us demonstrate how to minimize the K-condition number in the general case, then apply the results to obtain explicit formulas for K-optimal preconditioners. Consider the optimization problem:\n$K(XBX) \\rightarrow min$,\nwhere X belongs to some linear subspace of triangular matrices:\n$x = vec(X) = \\Psi z$,\nwhere \u03a8 is an $n^2 \\times m$ matrix, with m being the subspace dimension. For sparse matrices, m equals the number of non-zero elements in X.\nInstead of directly minimizing functional (6), we minimize its logarithm:\n$\\Phi(X) = log K(X^TBX) = \\frac{1}{n} log Tr(X^TBX) - \\frac{1}{n} log det(X)^2 - \\frac{1}{n} log det (B)$,\nThe third term is independent of X and can be omitted. For the first term:\n$Tr(X^TBX) = (BX, X)$,"}, {"title": "5. Benchmark construction", "content": "5.1. Synthetic benchmarks\nOur first benchmark focuses on tridiagonal matrices, which present an interesting challenge for testing GNN's capabilities. Constructing tridiagonal matrices where the Cholesky factors exhibit strong non-local dependencies requires careful consideration. Through empirical investigation, we found that simply fixing the diagonal elements of L to 1 and sampling the off-diagonal elements from a normal distribution does not produce the desired non-local behavior - the resulting matrices $A = LL^T$ tend to show primarily local dependencies. The key insight is that non-locality emerges when the inverse matrix $L^{\u22121}$ is dense.\nWe leverage the well-known fact that the inverse of a bidiagonal matrix L has a special structure called rank-1 semiseparable, where elements are given by the formula $L_{i,j}^{\u22121} = u_iv_j$ for $i < j$, representing part of a rank-1 matrix. This relationship is bidirectional - given vectors u and v, we can construct $L^{\u22121}$ with this structure and then compute L as its inverse. Our benchmark generation process exploits this property by randomly sampling appropriate vectors u and v to create matrices with guaranteed non-local dependencies.\nThe primary goal of this synthetic benchmark is to evaluate whether GNNs can accurately recover the matrix L in these cases. While our theoretical results suggest this should be impossible due to the inherent locality of message passing, it remains an open question whether GNNs with sufficiently large receptive fields could achieve reasonable approximations. Poor performance on this benchmark would raise serious concerns about the fundamental suitability of current GNN architectures for matrix factorization tasks."}, {"title": "5.2. Matrices from the SuiteSparse collection", "content": "To complement our synthetic examples with real-world test cases, we curated a comprehensive benchmark from the SuiteSparse matrix collection. We selected symmetric positive definite matrices for which dense inverse computation was feasible, resulting in a diverse set of 150 matrices varying in both size and sparsity patterns. For each matrix, we explicitly solved the optimization problem (6) to obtain sparse lower-triangular preconditioners.\nFollowing common practice in incomplete factorization methods, we restricted the sparsity pattern of our preconditioners to match the lower-triangular part of the original matrix A, similar to IC(0) preconditioners. Our experimental results showed that the inverse K-optimal preconditioners generally outperformed traditional IC(0) preconditioners \u2013 in many cases, IC(0) either failed to exist or required excessive iterations (>10000) for convergence. However, we observed that for a small subset of matrices, IC(0) achieved better convergence rates.\nThe final benchmark consists of (Ai, Li) pairs, where each Ai comes from SuiteSparse and Li represents either the IC(0) or K-optimal preconditioner, whichever demonstrated superior performance. Matrices for which neither preconditioner achieved satisfactory convergence were excluded. This benchmark serves two key purposes: it provides a robust baseline for sparse preconditioners with fixed sparsity patterns, and it creates a challenging yet practically relevant test set for evaluating GNN-based approaches. The relative performance distribution between Inv-K and IC(0) preconditioners is visualized in Figure 2, highlighting the general superiority of Inv-K preconditioners, while also showing cases where IC(0) remains competitive or where one or both methods fail to converge."}, {"title": "6. Experiments", "content": "6.1. Message Passing Layers\nThe problem considered in this paper can be reformulated as a regression with loss that penalizes edges discrepancy with the target. Most of the classical GNNs either do not take into account edges (e.g., GraphSAGE (Hamilton et al., 2017)) or takes them into account as scalar weighted adjacency matrix (e.g., Graph attention network (Veli\u010dkovi\u0107 et al., 2017)). To allow edge updates during message-passing we use a Graph Network (Battaglia et al., 2018) block as a message-passing layer.\nTo validate GNNs on the proposed benchmarks we utilize very popular Encoder-Processor-Decoder configuration. Encoder consists of two separate MLPs for nodes and edges. Processor consists of multiple blocks of Graph Networks. Graph Network first updates edge representations with Edge Model, after which nodes are update by Node Model with message-passing mechanism. In our work we do not consider models that achieve larger receptive field by graph coarsening or updates of the graph-level information, hence the Global Model in Graph Network is omitted. Decoder is a single MLP that decode edges hidden representations into single value per edge.\nAs a neural network baseline that does not perform information propagation between nodes we use a simple two-layer MLP as Node Model in Graph Network (MLPNodeModel). Following message-passing GNNs are used as Node Model in Graph Network: (i) graph attention network v2 (GAT) (Brody et al., 2021), (ii) generalized aggregation network (GEN) (Li et al., 2020) and (iii) message-passing (MessagePassingMLP) (Gilmer et al., 2017) with two MLPs for \u03b8o and fo:\n$h_i = f_{\\Theta, o} \\Big( h_i, \\frac{1}{|N(i)|} \\sum_{j \\in N(i)} f_{\\Theta, e}(h_i, e_{ij}) \\Big)$\nFinally, we tested two graph transformers as Node Models: (i) graph transformer operator (GraphTransformer) from (Shi et al., 2020) and (ii) fast graph transformer operator (FastGraphTransformer) from (Wu et al., 2024)."}, {"title": "6.2. Graph Neural Network Architecture", "content": "In our experiments we set encoders for nodes and edges to two layer MLPs with 16 hidden and output features. Node Model is single layer from a following list: MLPNodeModel, GAT, GEN, MessagePassingMLP, GraphTransformer, FastGraphTransformer. Edge Model is a two layer MLP with 16 hidden features. Node Model and Edge Model form Graph Network which is used to combine multiple message-passing layers in Processor. Edge decoder is a two layer MLP with 16 hidden features and single output feature.\nThe maximum depth of message-passing layers within the Processor block varies across different Node Models and is determined by GPU memory allocation for each Node Model but not greater than 7 blocks.\nFor training we use negative cosine similarity between target and predicted edges as a loss function, since for preconditioner the matrix L is defined up to a scaling factor. Note that in terms of sparse matrices vectors of edges correspond to vectorized elements of sparse matrix.\nWe use PyTorch Geometric (Fey & Lenssen, 2019) framework for GNNs training and main layers implementation. For FastGraphTransformer we use official implementation from (Wu et al., 2024). We used a single GPU Nvidia A40 48Gb for training."}, {"title": "6.3. Learning Triangular Factorization", "content": "We start our experiments with synthetic benchmark generated as described in Section 5.1. Modified training pairs $(A_m, L_m)$ are obtained as follows:\n$A_m = A + e_1e_1^T ,   L_m = chol(A).$\nwhere chol is a Cholesky factorization.\nA trivial empirical justification of the non-local behaviour of the considered problem is performed with a deep feed-forward network, MLPNodeModel, which has no information about the context (Figure 3). Surprisingly, the classical graph network layers GAT and GEN have a slightly higher final accuracy than MLPNodeModel. We assume that this behaviour is explained by the fact that these architectures are not designed to properly pass edge-level information, which is a primary goal of our work. MessagePassingMLP GNN, on the other hand, makes direct use of edge features,"}, {"title": "9. Conclusions and Future Work", "content": "Our work provides a new perspective on the limitations of message-passing GNNs for preconditioning and also shows that in order to learn factorizations, we need to have non-local information about the matrix, not just local transformations. The inspiration for new architectures can be actually taken from the linear algebra as well, and we plan to explore this direction in future work. Finally, there are many other preconditioning approaches besides factorization methods, that may be much better suited for GNNs (Benzi, 2002). Still, improvements of numerical linear algebra algorithms by machine learning methods is a very challenging task."}]}