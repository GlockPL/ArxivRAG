{"title": "LL\u00e4Mmlein: Compact and Competitive German-Only Language Models from Scratch", "authors": ["Jan Pfister", "Julia Wunderle", "Andreas Hotho"], "abstract": "We create two German-only decoder models, LL\u00e4Mmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use\u00b9. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics.\nCompared to state-of-the-art models on the SuperGLEBer benchmark, both LL\u00e4Mmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.", "sections": [{"title": "1 Introduction", "content": "The recent success of Large Language Models (LLMs) has brought progress across many areas and languages. However, much of this progress has been concentrated on English, resulting in a notable gap for other languages, including German. In comparison to English, where large research labs and companies actively compete and heavily invest in training and tuning new models on new datasets commonly from scratch nearly every week, the German LLM landscape seems left behind. Previously, a few BERT or smaller GPT models have been pre-trained in German corpora (Chan et al., 2020; Scheible et al., 2020). However, as of recent developments, LLMs have become larger and have either been multilingually trained on other languages as well as German (e.g. mT5 (2021)), trained on English data and then finetuned to German (e.g. LeoLM (2023)) or there is no usable information about the (German) pretraining data at all (e.g. Mistral (2023)). This always makes non-English (i.e. German) language modeling more of an \"afterthought\" in these models, which can lead to various performance deteriorations on non-English languages (Ansch\u00fctz et al., 2023; Virtanen et al., 2019). These issues can even be noticed in the most recent Llama 3 (Dubey et al., 2024) instruct models, which often fall back to answering in English, despite the previous conversation being held in German. These issues highlight the pressing need for dedicated, German-specific research and model development to identify and address the limitations of the current German LLM landscape. Specifically, there is a significant lack of transparency regarding the German-language data used to train existing models and how this data contributes to their capabilities. To address this gap, we (plan to) open-source our entire training pipeline, including data, code, and artifacts, to provide comprehensive insights into the training process, foster reproducibility, and encourage collaboration in this analysis within the research community.\nTo train the model, we have carried out the following steps: First, we filter and preprocess a large dataset, sourced from RedPajama V2 (Computer, 2023), to only contain high-quality German data. Second, we build a new German tokenizer with a vocabulary size of 32,000 tokens and train it on different amounts of data to compare its performance to existing German tokenizers. Next, we pretrain two exclusively German autoregressive language models from scratch on this dataset: LL\u00e4Mmlein 120M and 1B. Training two models not only provides a basis for comparison, but also enables analysis of scaling effects on model performance and efficiency. In line with the Pythia experiments (Bi-"}, {"title": "2 Methodology", "content": "Pretraining and evaluating a German LLM from scratch, end-to-end involves several steps, including dataset preprocessing (section 2.1.2), tokenizer training (section 2.2), model pretraining (section 2.3), model evaluation using German downstream task and translated prompt-based few-shot QA tasks (section 2.4), and downstream adaptations (section 2.5)."}, {"title": "2.1 Dataset", "content": "RedPajama V2 (Computer, 2023) is an open, multilingual dataset designed for training large language models. It consists of over 100 billion text documents collected from 84 CommonCrawl snapshots between 2014 and 2023 and encompasses multiple languages, including English, German, French, Italian, and Spanish. The dataset was originally preprocessed using the CCNet pipeline (Wenzek et al., 2020) leading to about 30 billion overall documents further enriched with quality signals and duplicate indicators. Using a perplexity score, the RedPajama dataset was divided into three quality categories: head, middle, and tail."}, {"title": "2.1.1 Dataset Analysis", "content": "The aim of the following preliminary dataset analysis is to gain a deeper understanding of the German portion of the dataset used. The \u201cofficial\u201d estimate of the size of the German segment within the RedPajamaV2 dataset, derived through extrapolation from a smaller sample analyzed with Mistral-7B, is approximately 3 trillion German tokens (Computer, 2023). We will perform an exploratory analysis of the dataset to gain a clearer understanding of the actual amount of German data it contains, alongside its domain distribution and the most prevalent data sources.\nStatistics Our own analysis on the entire head and middle part, using the gbert-large tokenizer, led to a token count of 2.7 trillion German tokens for head and middle categories combined.\nFigure la shows the data distribution of the full German dataset in pink, the full unique data including, head and middle, in blue and the partition of duplicates in gray (according to the \u201cis_duplicate\" key). Most samples are unique (1.9 billion samples) and only significantly less are marked as duplicates (777 million samples) across the entire dataset. All three lines exhibit similar patterns. The most common token frequency per document can be found at nine, with approximately 3.6 million occurrences across the entire dataset. A second peak occurs at around 100 tokens per document. In total, the 2.7 trillion German tokens are distributed across samples with lengths ranging from 1 to 1,034,799 tokens, averaging approximately 1,000 tokens per sample.\nFigure 1b breaks down the distribution of each partition, i.e. head unique, middle unique, head duplicate and middle duplicate separately. The middle unique partition contains the largest amount of data, with approximately 1.2 billion samples, which corresponds to 45% of the full dataset. The head unique partition, by comparison, includes around 400 million fewer samples.\nDomain Analysis The dataset contains content crawled from various domain names. Figure 2 displays the top 20 sources from which the data was collected, with the overall count illustrated as gray bars and separate plots for the head (pink) and middle (blue) unique splits.\nWikipedia clearly stands out as the largest contributor, with a combined total of over 11.5 million samples. Among these, about 10 million entries belong to the head category, while about 1.45 million stem from the middle partition. This distribution aligns with the fact that the split into head, middle and tail was created using a perplexity score criterion based on a language model trained on Wikipedia (Computer, 2023) - consequently, texts closer in style to Wikipedia tend to be ranked higher. Besides Wikipedia, it is evident that news websites also constitute a significant portion of the dataset. For the middle split, \"welt.de\" emerges as the most frequent domain, contributing around 2.47 million samples. With the exception of domains like eBay, Handelsblatt, hotels and Perlentaucher, the list is largely dominated by general news outlets."}, {"title": "2.1.2 Further Dataset Preprocessing", "content": "To remove common web boilerplate, such as GDPR notices or similar repetitive content, we utilize a paragraph-level deduplication scheme powered by Dolma a framework that enables efficient deduplication through a Rust-based Bloom filter (Soldaini et al., 2024). This ensures that highly redundant text is filtered out, improving the overall quality and diversity of the dataset. This approach may inadvertently over-remove valid and relevant content, such as short texts mistakenly treated as entire paragraphs and removed across the dataset. To mitigate this, and to preserve meaningful short text sections - such as lists or frequently occurring itemized phrases that are contextually significant - we excluded texts containing fewer than two words from the deduplication process. Additionally, we applied further cleaning to remove superfluous line breaks (\\n) and whitespaces (\\s).\nDespite these efforts, some unusual artifacts, such as e.g. long sequences of guitar chords, remained. To address this, we built a token-to-word ratio filter. Here, we compared the word count (using whitespaces \\s as separation) with the token count using the German GPT-2 tokenizer (Staatsbibliothek). According to our intuition, a large discrepancy between the two counts indicates abnormal or low-quality text, whereas a close match suggests valid content. A simple example illustrates this clearly: The phrase \u201cDer Himmel ist blau\" consists of 4 words and 4 tokens, so it is not removed by our filter. In contrast, \"/de/c/trebic-unesco\" counts as 1 word but 11 tokens, and should therefore be excluded by this token-to-word ratio filter.\nPreliminary examinations and manual review suggested a ratio of tokens to words of eight as a valid threshold. Thus, paragraphs with ratios exceeding this threshold were excluded from the"}, {"title": "2.2 Tokenizer Training", "content": "We chose to train our own tokenizer due to the lack of an existing German tokenizer with a vocabulary size of 32,000. This decision ensures better alignment with the German language, the TinyLlama setup and dataset, while also enhancing transparency and traceability throughout the model's from-scratch training process. Using the setup outlined in TinyLlama (Zhang et al., 2024), we trained a Byte-Pair Encoding (BPE) tokenizer with a 32,000-token vocabulary tailored specifically for German."}, {"title": "2.3 Model Pretraining Framework", "content": "While there are several existing resources and repositories for training a Large Language Model (LLM) from scratch2, we chose the TinyL-lama GitHub repository as the backbone of our project (Zhang et al., 2024). It was used to pretrain the 1B English TinyLlama model from scratch before and builds upon the lit-gpt repository (AI, 2023), which provides robust tooling for data preparation, fine-tuning, pretraining, and deploying LLMs using PyTorch Lightning.\nIt includes all relevant features such as multi-GPU and multi-node distributed training with FSDP as well as flash attention 2. Additionally, it provides scripts to convert the models into HuggingFace format for easier use and distribution."}, {"title": "2.4 Model Evaluation Setup", "content": "To get a better understanding of the training, we monitor the progress and regularly evaluate intermediate checkpoints on six representative Super-GLEBer tasks (Pfister and Hotho, 2024). These tasks were selected to include different task types to assess the performance of our model and will be discussed in the following:\n\u2022 Classification:\nNatural Language Inference (NLI) (Conneau et al., 2018): Predict if a hypothesis is entailed, neutral, or contradictory to a premise.\nFactClaiming Comments (Risch et al., 2021): Binary classification of whether a text contains a fact-checkable claim.\nDB Aspect (Wojatzki et al., 2017): Multilabel classification of categories and polarities in input sentences.\nWebCAGe (Henrich et al., 2012): Predict if a word's sense matches across two contexts (true/false).\n\u2022 Sequence Tagging\nEuroParl (Faruqui and Pad\u00f3, 2010): Token classification on European Parliament data.\n\u2022 Sentence Similarity\nPawsx (Liang et al., 2020): Identify if sentence pairs are paraphrases by generating meaningful vector representations."}, {"title": "2.4.1 Intermediate Checkpoint Evaluation", "content": null}, {"title": "2.4.2 Final Model Evaluation", "content": "To assess general knowledge and abilities, we evaluated our final models on the full SuperGLEBer benchmark (29 tasks across classification, sequence tagging, question answering, and sentence similarity) (Pfister and Hotho, 2024), as well as machine-translated, prompt-based, few-shot QA tasks using the lm-evaluation-harness-de (if not stated otherwise, they are evaluated measuring accuracy and output length normalized accuracy (Pl\u00fcster; Gao et al., 2021):\n\u2022 ARC-Challenge-DE (Clark et al., 2018): Translated grade-school science questions (1,471 samples) from ARC-Challenge. Few-shot evaluation with 25 samples.\n\u2022 MMLU-DE (Hendrycks et al., 2021): Translated version of MMLU with 6,829 multiple-choice questions across 57 topics (e.g., math, medicine, law). Few-shot evaluation with five samples.\n\u2022 HellaSwag-DE (Zellers et al., 2019): Commonsense reasoning dataset with 11,000 translated samples, featuring incomplete sentences with multiple-choice completions. Few-shot evaluation with ten samples.\n\u2022 TruthfulQA-DE (Lin et al., 2022): Dataset of 817 questions across 38 categories (e.g., health, law) designed to evaluate model truthfulness, particularly in handling misconceptions. Few-shot evaluation with zero samples. Performance is measured in single true (mc1) and multi true (mc2) (Pl\u00fcster; Gao et al., 2021)."}, {"title": "2.5 Downstream Adaptations", "content": "We fine-tune our model exemplarily to two downstream use cases: instruct-tuning and Bavarian. It should be noted that the results of these adaptations are not included in the evaluation of the model's performance on the SuperGLEBer benchmark, and we did not clean or preprocess the data for these tasks extensively before training. While this is not the main focus of this paper, we want to demonstrate the versatility of our model and its potential for further research and development."}, {"title": "2.5.1 Instruct-tuning", "content": "A common use case for modern LLMs is to generate text based on a given prompt. For this purpose, the model is tuned on input/output pairs, where the input is a user-given prompt and the output is the desired continuation. To this end, we employ LORA (Hu et al., 2021) to fine-tune our model on various existing German instruct-tuning datasets."}, {"title": "2.5.2 Bavarian", "content": "To exemplarily adapt our model to a specific dialect, we fine-tune the model on a few thousand Bavarian samples using LoRA (Hu et al., 2021)."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Tokenizer", "content": "We follow the TinyLlama setup and use a Llama 2 Byte-Pair Encoding (BPE) tokenizer. We trained three tokenizers on different amounts of data and compared the results to two established German tokenizers, german-gpt2 and gbert-large in section 4.1.\n\u2022 1TB: Spans backward from the most recent data until 1TB of data is processed\n\u2022 2023-2021: Includes all splits of the high quality data split from years 2023 to 2021 (847GB)\n\u2022 2023_14: Consists of the most recent 2023_14 split (67GB)"}, {"title": "3.2 Model Pretraining", "content": "For the training of our larger 1B model, we use the head and middle part of our preprocessed dataset, while for the smaller 120M model, we will use the head part only."}, {"title": "3.2.1 LL\u00e4Mmlein 120M", "content": "Finally, we trained the LL\u00e4Mmlein 120M model on the filtered head data (section 2.1), a maximum learning rate of 6e-4, grouped query attention of 4, a sequence length of 2048, a batch size of 1024, and the full-shard FSDP strategy. This model was trained on 32 L40 GPUs distributed across 16 nodes. This resulted in a total pretraining token count of: 954,485,440,512\nFigure 3 displays the model's loss curve, where each training resume is distinguished by a unique color. Overall, 10 restarts were necessary: Due to cluster settings, training was resumed at least every two days, and additionally, training had to be restarted a few times to address GPU and NCCL errors."}, {"title": "3.3 LL\u00c4Mmlein 1B", "content": "After the preliminary tests with the smaller model, we trained the 1B model. For training data, we utilized the full processed and filtered head and middle partitions, which resulted in a total token count of 2.7T (2,649,407,619,072) unique German tokens and 3T (3,000,001,101,824) German training tokens in total. As we closely follow the TinyL-lama setup, we want to maintain the same step count, and we traverse our dataset once, and then again with a small overlap, like done in the original TinyLlama and Pythia setup. We trained our model on 64 A100 GPUs distributed across eight nodes. To keep the global batch size at 1024, we set the micro batch size to 16, and the maximum learning rate was 6e-4.\nInitially, we attempted to estimate the runtime for replicating the original TinyLlama training run on our hardware. Based on our extrapolations, the process would have taken over 200 days using 16 A100 GPUs, compared to the 90 days reported for TinyLlama. By adapting the Fully Sharded Data Parallel (FSDP) strategy to a hybrid sharding approach, we reduced the extrapolated runtime to approximately 100 days, which we deemed satisfactory.\nNext, we scaled our training to 64 GPUs, bringing the extrapolated runtime down to 36 days. Early in the training run, we identified inefficiencies related to improper use of the available InfiniBand. We halted the training, corrected the configuration, switched back to full sharding, and implemented dataset pre-caching in RAM on each node. After these optimizations, we restarted the training, achieving a final overall runtime of 32 days.\nFigure 4 shows the loss curve of our 1B model. During pretraining, we had to resume training twice: once at 150,000 steps (blue) and at 590,000 steps (pink). The visible jumps in the loss curve correspond to different chunks of training data, each sampled from a distinct part of the dataset."}, {"title": "3.4 Downstream Adaptations", "content": null}, {"title": "3.4.1 Instruct-tuning", "content": "We use LoRa (Hu et al., 2021) to fine-tune our model on various German instruct-tuning datasets, using supervised finetuning for three epochs with PEFT (Mangrulkar et al., 2022) and TRL (von Werra et al., 2020). We employ default hyperparameters and train for three epochs.\nFor training, we use the following datasets from huggingface: \u201cLSX-UniWue/Guanako\u201d, \u201cFreedomIntelligence/alpaca-gpt4-deutsch\", \"FreedomIntelligence/evol-instruct-deutsch\", and \"FreedomIntelligence/sharegpt-deutsch\". We train and publish a separate adapter for each dataset, and also a combined model across all datasets."}, {"title": "3.4.2 Bavarian", "content": "To exemplarily adapt our model to a specific dialect, we fine-tune the model on about 25,700 Bavarian Wikipedia pages, using LoRA (Hu et al., 2021) with supervised finetuning, PEFT (Mangrulkar et al., 2022) and default hyperparameters.\nFor this, we use the Bavarian column from the \"cis-lmu/bavarian_to_english\u201d dataset from HuggingFace."}, {"title": "4 Evaluation", "content": null}, {"title": "4.1 Tokenizer", "content": "We tested the performance of our custom tokenizers on random samples and training splits, comparing the results to those of two established German tokenizers german-gpt2 (vocabulary size: 50,266) and gbert-large (vocabulary size: 31,102). While the different vocabulary sizes skew direct comparisons, the results still provide valuable insight into the relative performance of each tokenizer. The tables presented in Table 1 compare the token counts generated by the tokenizers when applied to two unseen data snapshots from the training dataset.\nNotably, the german-gpt2 and gbert-large tokenizers obtain the lowest token counts. Interestingly, among our own tokenizers, the one trained on the smallest dataset (2023_14) performed better (i.e. produced fewer tokens) than those fitted on larger datasets. Our results suggest that the smaller dataset might allow the tokenizer to be more efficient by focusing on the most frequent tokens, while larger datasets might introduce more variation, leading to less efficient tokenization. Consequently, we chose to use the tokenizer trained on the 2023_14 snapshot."}, {"title": "4.2 Pretraining Process", "content": "During training, we regularly saved and evaluated checkpoints to monitor the training process (section 2.4.1)."}, {"title": "4.2.1 LL\u00e4Mmlein 120M", "content": "The results, assessed on six SuperGLEBER tasks - FactClaiming, EuroParl, Pawsx, NLI, DB Aspect and WebCAGe (see Section 2.4.1) are summarized in Table 2. We compared LL\u00e4Mmlein 120M's performance with models of similar parameter sizes, including german-gpt2, gbert-base and bert-base-german-cased. While our model consistently outperformed the only other decoder-only model gpt2, the BERT-based models showed superiority in the first three tasks (FactClaiming, EuroParl and Pawsx). This performance gap is consistent with the known limitations of autoregressive, decoder-only architectures on tasks such as sequence tagging and sentence similarity (Pfister and Hotho, 2024). LL\u00e4Mmlein 120M particularly excelled in the more complex classification tasks. For the NLI task, it outperformed all other models starting from the first evaluated checkpoint \u201c010000\u201d, with its best checkpoint exceeding the highest-performing bert-base-german-cased by 15%. Similarly, our model obtained the highest scores for DB Aspect and WebCAGE classification tasks.\nInterestingly, performance improvements varied across tasks. We calculate the Spearman correlation coefficient r to measure the strength and direction of the monotonic relationship between training steps and performance, and the corresponding p-value to assess the statistical significance of the correlation. While for FactClaiming and EuroParl only minimal variation across checkpoints is observable, Pawsx (r = 0.607, p = 0.04), NLI (r = 0.947, p < 0.0001) and DB Aspect (r = 0.909, p < 0.0001) exhibited clear, linear improvement trends.\nThis suggests that while LL\u00e4Mmlein quickly reached a plateau for certain tasks, i.e. those that might require more basic structure recognition (FactClaiming/EuroParl), it continued to learn and improve on some more complex tasks."}, {"title": "4.2.2 LL\u00c4Mmlein 1B", "content": "We compared our results with those of all other models evaluated on the SuperGLEBer benchmark, focusing particularly on the best-performing model for each task (Table 3). Results of models that experienced out-of-memory (OOM) errors on an A100 80 GPU are indicated with a \u201c-\u201d.\nWhile LL\u00e4Mmlein 1B may not secure the top results overall, it consistently ranks among the top three for each task listed in the table, demonstrating its reliability and stability compared to other models, which may exhibit significant performance variations. Interestingly, there is one exception in NLI at checkpoint 1,420,000, where performance drops by almost 40% compared to the previous checkpoint. However, performance recovers instantly at the following checkpoint. We are currently trying to investigate the cause of this anomaly. Similar to the 120M model, LL\u00e4Mmlein 1B is clearly outperformed for the sentence similarity task. However, it excels in the EuroParl task, setting a new maximum score with 0.732.\nExamining task progress over time reveals noticeable improvements across all tasks, except for FactClaiming. Compared to the LL\u00e4Mmlein 120M model, Spearman correlation analysis indicated a significant positive relationship between training time and performance of EuroParl (r = 0.431, p = 0.009) and WebCAGe (r = 0.92, p < 0.0001). All remaining tasks also demonstrated clear positive correlations with training time, suggesting that LL\u00e4Mmlein 1B continues to benefit from extended training."}, {"title": "4.3 Final Model Evaluation", "content": "Detailed results for our models on SuperGLEBer can be found on the official SuperGLE-Ber website https://1sx-uniwue.github.io/SuperGLEBer-site/leaderboard_v1."}, {"title": "4.3.1 LL\u00e4Mmlein 120M", "content": "After the evaluation of LL\u00e4Mmlein's performance over time across different tasks, we now compare its effectiveness against other models across the whole SuperGLEBer benchmark. Figure 5a indicate that LL\u00e4Mmlein significantly outperforms the german-gpt2 model, confirming its superiority among German decoder models in this size range. When comparing LL\u00e4Mmlein to the two BERT models gbert-base and bert-german-cased, no statistically significant difference was found (Figures 5b and 5c). This is noteworthy, especially considering that BERT models generally excel in sequence tagging and similarity tasks. The comparable performance of LL\u00e4Mmlein indicates that it competes well with established BERT-based models, despite BERT's inherent architectural advantages for some tasks.\nWe further evaluated our results on the lm-evaluation-harness-de evaluation benchmark for autoregressive models. Table 4 provides the normalized and unnormalized accuracy scores for ARC-Challenge, HellaSwag and MMLU and mc1 and mc2 for TruthfulQA (see Section 2.4.2). We compared LL\u00e4Mmlein 120M with german-gpt2, the only other decoder model available at this parameter size. Additionally, we fine-tuned LL\u00e4Mmlein on a German translated Alpaka dataset for instruction tuning. Notably, the instruction-tuned model achieved the best performance on TruthfulQA. For the other tasks, the base version of LL\u00e4Mmlein demonstrated superior performance."}, {"title": "4.3.2 LL\u00e4Mmlein 1B", "content": "To further assess LL\u00e4Mmlein 1B's performance, we conducted pairwise t-tests to compare results with other models on the SuperGLEBer benchmark. For consistency, we excluded tasks from the comparison, where a larger model lacked a score due to a cuda out of memory error. Among models with similar parameter size, we compared LL\u00e4Mmlein 1B to Llama 3.2 with 1B parameters and EuroLLM with 1.7B parameters. Figures 6a and 6b show that our model is able to significantly outperform both similar-sized models. Leo-hessian-7B, with its larger architecture, showed superior performance, indicating that the size advantage of 7B models still holds in many cases (Figure 6c). Interestingly, no significant difference was found between LL\u00e4Mmlein 1B and other much larger models, such as the German-finetuned Llama 8B (Figure 6d), Llama 3.1 8B (Figure 6e) as well as gbert-large (Figure 6f). This similarity with larger models highlights LL\u00e4Mmlein 1B's efficiency and competitiveness.\nTable 5 provides a comparative evaluation of the LL\u00e4Mmlein 1B and its instruction-tuned variants, alongside similar-sized Llama 3.2 1B and larger models. Notably, the German finetuned Llama 3 8B instruct model\u00b3 achieved the highest scores across all tasks, demonstrating the advantage of increased model size and instruction tuning for this benchmark. Interestingly, the model has no significant performance gain compared to LL\u00e4Mmlein 1B on the SuperGLEBer benchmark. This indicates that for actual generation tasks, size seems to matter more.\nRegarding the smaller models, Llama 3.2 1B achieved the best unnormalized score for TruthfulQA. However, on the normalized score LL\u00e4Mmlein 1B instruct-tuned on the alpaka dataset outperforms all 1B models by at least 6% as well as most of the larger models. In ARC-Challenge and HellaSwag, which are completion tasks requiring commonsense and contextual reasoning, the LL\u00e4Mmlein 1B Guanako Instruct model consistently outperformed both the base LL\u00e4Mmlein model and Llama 3.2 1B models. This result implies that the chat-tuned version of LL\u00e4Mmlein 1B may have an advantage in generating coherent responses that align well with the completion task structure. Llama 3.2 1B Instruct achieved the best results for the general knowledge MMLU benchmark. Interestingly, for the LL\u00e4Mmlein series, chat-tuning led to consistently higher scores across different tasks, whereas this trend did not appear for the Llama 3.2 1B models.\nThe results further highlight differences in task requirements: ARC-Challenge and HellaSwag, both completion tasks, differ structurally from TruthfulQA and MMLU, which are question-answering benchmarks. Smaller models, even when fine-tuned, may be less adept at true question-answering tasks that require a deep factual understanding. In addition, it is noticeable, that for our LL\u00e4Mmlein models the normalized score is better than the unnormalized score most of the time, indicating that or model tends to generate rather long answers. When comparing the 120M version with the 1B model, the 1B consistently outperforms the smaller version by nearly 10%, except on the MMLU task-again highlighting the - expected - advantages of a larger model size."}, {"title": "5 Conclusion", "content": "We created two German-only decoder models, LL\u00e4Mmlein 120M and 1B, from scratch. Achieving this involved preprocessing our training dataset in various steps, fitting a tokenizer to meet the specific requirements of our models and training, as well as evaluating the models.\nWe closely monitor the entire training process, evaluating several intermediate checkpoints to analyze the training dynamics. Our experiments revealed that the models learned different tasks at various speeds. Some tasks showed clear linearly correlated improvements over time, while others plateaued early on.\nWhen compared to the state of the art on the SuperGLEBer benchmark, our models consistently matched or surpassed models of comparable sizes. In particular, LL\u00e4Mmlein 1B outperformed the multilingual Llama 3.2 1B, underscoring the advantages of a monolingual model trained from scratch for language-specific tasks. Interestingly, mostly no significant difference was observed between our 1B model and larger models we evaluated, showcasing its effectiveness. However, for generative question answering, while our models performed similarly to other parameter-equivalent models, we observed notable differences to larger models, showcasing the clear advantage of the larger parameter counts for generation tasks and the limitations of smaller models in this regard.\nLooking ahead, there are several avenues for future work: First, further analysis of the training process can be carried out, using the various intermediate checkpoints we saved and published. This opens the possibility for more granular evaluations of training dynamics. Furthermore, after our preliminary experiments with instruct-tuning, we want to point out the need for a high-quality and natively German instruct dataset, and plan to explore the potential of domain-specific fine-tuning. Domain adaptation and evaluation thereof could provide valuable insights into the model's capabilities and limitations."}]}