{"title": "Learning in Multi-Objective Public Goods Games with Non-Linear Utilities", "authors": ["Nicole Orzan", "Erman Acar", "Davide Grossi", "Patrick Mannion", "Roxana R\u0103dulescu"], "abstract": "Addressing the question of how to achieve optimal decision-making under risk and uncertainty is crucial for enhancing the capabilities of artificial agents that collaborate with or support humans. In this work, we address this question in the context of Public Goods Games. We study learning in a novel multi-objective version of the Public Goods Game where agents have different risk preferences, by means of multi-objective reinforcement learning. We introduce a parametric non-linear utility function to model risk preferences at the level of individual agents, over the collective and individual reward components of the game. We study the interplay between such preference modelling and environmental uncertainty on the incentive alignment level in the game. We demonstrate how different combinations of individual preferences and environmental uncertainties sustain the emergence of cooperative patterns in non-cooperative environments (i.e., where competitive strategies are dominant), while others sustain competitive patterns in cooperative environments (i.e., where cooperative strategies are dominant).", "sections": [{"title": "Introduction", "content": "How can cooperation emerge and sustain itself in situations where agents do not necessarily have a direct motive for cooperation? This is one of the fundamental questions in various research areas, such as evolutionary biology [21, 35], political sciences [8, 9], cognitive sciences [42] and physics [11]. To answer this question, researchers developed and studied models of real-world scenarios involving tension between the collective and personal motives, called social dilemmas [13, 26]. The main characteristic of these social dilemmas is that players are better off defecting at the individual level, while, at the group level, the best outcome is mutual cooperation.\nIn this work, we focus on a specific class of social dilemmas known as Public Goods Games (PGG), extensively studied in literature [46, 3]. A PGG describes situations where cooperation by all agents is Pareto optimal, but because of the profitability of free-riding [5], rational agents fail to cooperate: defection by all agents is a Nash equilibrium [34]. We refer to this kind of game as mixed-motives, since the incentives of the agents are partially misaligned.\nIn addition to incentive misalignment, other factors influencing the emergence of cooperation in many real-world scenarios include uncertainty and different individual attitudes towards risk [20, 27].\nUncertainty can have different sources: we refer to environmental uncertainty when actors are unsure about the amount of goods they can receive from the environment [51, 4], and to social uncertainty when it comes to ambiguity about the opponents' possible actions [16, 10]. Individual preferences express a personal inclination towards one choice over another. In the specific context of PGGs, we are interested in modelling two main types of individuals in the presence of uncertainty: those that are biased towards taking risks in the presence of uncertainty, also called risk-seeking agents, and others which are inclined not to take risks, also called risk-averse agents. We model these attitudes using a parametric nonlinear utility function of the reward received by individuals as the result of their investment in the collective good.\nSince we are working with non-linear utility functions in the PGG, we need to decouple our perspective from the literature on the PGGs addressing non-linear public good productions. This branch focuses on settings where the public good results from a non-linear production process [40]. These are called non-linear public good games and allow one to model certain real-world situations (populations of bacteria, viruses, or cooperative hunting [38, 12]). In contrast, we shift our focus to an individual level and capture settings where potentially different attitudes towards risk can occur within a population.\nTo model risk attitudes, in our work, we explicitly decouple the collective versus the individual incentives experienced by the agents, and parameterize the collective incentive at the individual level. This choice allows us to model settings where individuals in a population can have different perceptions regarding these incentives. Furthermore, we take a multi-objective approach to the optimization of these two levels of rewards, drawing on multi-objective reinforcement learning (MORL) methods. This way we can investigate learned behaviours that emerge from individually preferred trade-offs between the cooperative and competitive objectives."}, {"title": "Related Work", "content": "The related literature can be grouped under two main categories: non-linear utilities in public good games and multi-objective reinforcement learning. In this section, we describe them respectively."}, {"title": "Non-Linear Utilities in Public Goods Games", "content": "Although the PGG with linear utility functions is the most known and commonly used, various models of non-linear PGGs have been proposed in the literature as well. In the threshold public goods game, the resulting public good is given by a step function of the number of cooperators: the resource is created only if a minimum fraction of actors participate in the production of the public good [14]. When the minimum number of participants is 1, this is called the Volunteer's Dilemma [6, 17]. A sigmoid public goods function closely models many biological systems where the output production is small for low input levels and bigger for intermediate inputs, decreasing again for even bigger ones [7, 12]. In other paradigms, public good production is modelled by applying a concave (convex) function over agents' contributions, where the produced good is lesser (greater) than the good provided by a linear function of the contributions.\nSeveral papers focused on analyzing non-linear public good games, by different means. In [33] authors employ non-linear PGG with different incentive structures to analyze behavioural subtyping, i.e., if cooperative behaviour in one task can predict cooperative behaviour in another. In [53], evolutionary dynamics techniques are employed to study the role of different non-linear production functions on the evolution of cooperation in finite populations, while in [40], the evolutionary dynamics of two different populations collaborating for the production of a non-linear public good is investigated. In [15] authors explore the effects of different non-linear PGGs on the evolution of cooperation using Darwinian dynamics.\nIn the aforementioned literature, non-linearities in PGGs are typically functions that influence the production of the public good. In our work, however, we take a different perspective by introducing non-linearities at the level of the individual utilities extracted from rewards. More specifically, our goal is to model individuals' attitudes towards risk. In doing so, we follow decision theory which seeks to understand human decision processes and derive optimal decision-making strategies [50, 29, 24], therefore the study of risk and uncertainty has been a central focus. Some studies have shown that people make decisions based on some subjective function of the investment they made [47, 18]. For instance, an individual's risk attitude is often described as a function of the investment made (x) by means of a utility function shaped as $u(x) = x^\\beta$. Here, the parameter $\\beta$ governs the risk preference of the individual: if 0 < $\\beta$ < 1, the function is concave, signifying risk-aversion; if $\\beta$ > 1, the function is convex, indicating a risk-seeking attitude [24]. In our work, we draw on this idea to formulate a utility function that allows us to model individual preferences for actors participating in the PGG."}, {"title": "Multi-Objective Reinforcement Learning", "content": "In the field of reinforcement learning, the main focus is often to solve single-objective problems, by determining the agent's best policy to reach a specific goal. However, real-world challenges are of a multi-objective nature most of the time [44]. Autonomous agents, whether human or artificial, need to optimize for multiple goals simultaneously, or find a trade-off between them. This is the central concept of multi-objective reinforcement learning (MORL) [23, 44], a field that has developed rapidly in recent years [2, 52]. In MORL, the core idea is to receive vector rewards from the environment instead of scalar rewards. Under the utility-based perspective [23], rewards can be combined by means of a scalarization function to determine the final optimisation goal. Often, a linear scalarization function is employed, which allows the employment of single-objective RL methods. Alternatively, other choices include monotonically increasing non-linear scalarization functions [1, 43]. These are of particular interest for our work since non-linear functions are often used to model utilities under uncertainty and risk, especially in the economics literature, which aims at modelling human behaviour [33, 49].\nAnother part of this field of research focuses on fairness, i.e., how to optimize the trade-off among the objectives of different individuals under particular fairness constraints [48, 22, 19]. For example, in [48], authors employ deep RL techniques to learn a policy that treats users equitably. We build on the framework developed in [48], but rather than focusing on the fair treatment of a set of users, we investigate the effect of uncertainty and individuals' attitudes towards risk. To this end, we extend their approach to work with a different scalarisation function customized for our scenario, which allows us to model individual preferences, and train independent reinforcement learning agents in a multi-objective setting. We thus adopt a multi-objective multi-agent reinforcement learning (MOMARL) [41] perspective, which extends MORL to multi-agent scenarios."}, {"title": "Preliminaries", "content": "In this section, we present the formal definitions and the background knowledge. These include the Extended Public Goods Game, multi-"}, {"title": "The Extended Public Goods Game", "content": "The Extended Public Goods Games [36] is a tuple $(N, c, A, f, u)$, where $N$ is the set of players whose size is denoted as $|N| = n \\in\\mathbb{N}$. Every player $i$ is endowed with some amount of wealth (or coins) $C_i \\in \\mathbb{R}_{\\geq 0}$, and $c = (C_1,..., C_n)$ denotes the tuple containing all agents' coins. Each agent can decide whether to invest in the public good (cooperate) or keep the endowment for themselves (defect); therefore, the set $A$ of allowed actions consists of cooperate $(C)$ and defect $(D)$ i.e., $A = \\{C, D\\}$. The vector $a = (a_1,...,a_n) \\in A^n$ represents the action profile of the agents. The quantity $f$ is called multiplication factor, and specifies the scalar by which the total investment is multiplied in order to produce the public good. The resulting quantity is then evenly distributed among all agents. The difference with respect to the original PGG lies in the interval of allowed values for $f$. While in the PGG $f \\in (1,n)$, in the EPGG we take $f \\in \\mathbb{R}_{\\geq 0}$. The reward function for each agent $i$ is defined as $r_i: A^n \\times \\mathbb{R}_{\\geq 0} \\times \\mathbb{R}^n_0 \\rightarrow \\mathbb{R}$, with:\n$r_i (a, f, c) = \\sum_{j=1}^n c I (a_j) \\cdot f + c_i(1 - I(a_i))$,\nwhere $a_j$ is the $j$-th entry of the action profile $a$ and $I (a_j)$ is the indicator function, equal to 1 if the action of the agent $j$ is cooperative, and 0 otherwise, and $c_j$ denotes the $j$-the entry of $c$. For the sake of simplicity, in the following, we assume all endowments to be equal, namely $c_i = c, \\forall i \\in N$.\nDepending on the value of $f$, the EPGG can model three types of scenarios. When $1 < f <n$, like in the classic PGG, we model mixed-motives scenarios, in which all agents playing defect is a dominant strategy equilibrium. Yet, this profile is Pareto dominated by the profile in which all agents cooperate. When $0 \\leq f \\leq 1$, playing defect is a Pareto optimal dominant strategy (and therefore Nash) equilibrium. In addition, the EPGG can also model fully cooperative scenarios (i.e., when $f \\geq n$) in which the cooperation profile is a Pareto optimal dominant strategy (and therefore Nash) equilibrium."}, {"title": "Multi-Objective Stochastic Games", "content": "We model the multi-objective multi-agent interactions using the multi-objective stochastic game (MOSG) framework, defined as the tuple $\\mathcal{M} = (S, A, T, \\gamma, R)$, with $n \\geq 2$ agents and $d \\geq 2$ objectives, where:\n$\\bullet$ $S$ is the state space,\n$\\bullet$ $A = A_1 \\times \\dots \\times A_n$ is the set of joint actions, with $A_i$ being the action set of agent $i$,\n$\\bullet$ $T: S \\times A \\times S \\rightarrow [0, 1]$ is the probabilistic transition function,\n$\\bullet$ $\\gamma$ is the discount factor,\n$\\bullet$ $R = R_1 \\times \\dots \\times R_n$ are the reward functions, where $R_i: S \\times A \\times S \\rightarrow \\mathbb{R}^d$ is the vectorial reward function of agent $i$ for each of the $d$ objectives.$^1$\nWe take a utility-based perspective [44] for multi-objective decision making, assuming that each agent $i$ has a utility function $u_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ that maps the received reward vector to a scalar value, determining the desired trade-off between the objectives.\n$1$ We note that in this article the terms reward and payoff are synonyms. For the sake of clarity and consistency, we stick to the former term which aligns with the reinforcement learning terminology."}, {"title": "Optimization Criteria", "content": "In reinforcement learning, the goal of an agent is to find a policy $\\pi$ that maximizes the expected scalar return $V^\\pi = \\mathbb{E}_\\pi[\\sum_{t=0}^\\infty \\gamma^t r_t]$. In MORL, depending on how agents derive their utility, there are two optimisation criteria one can employ in the scalarisation process when maximising the expected discounted long-term reward vector:\n$\\bullet$ The Scalarised Expected Return (SER) criterion:\n$V^\\pi = \\mathbb{E}_\\pi\\Big[u\\Big(\\sum_{t=0}^\\infty \\gamma^t r_t\\Big)\\Big]$\nwhere $\\pi : S \\times A \\rightarrow [0,1]$ is the agent's policy, and $r_t = R(s_t, a_t, s_{t+1})$ is the vectorial reward at timestep $t$.\n$\\bullet$ The Expected Scalarised Return (ESR) criterion:\n$V^\\pi = \\mathbb{E}_\\pi\\Big[\\sum_{t=0}^\\infty \\gamma^t u(r_t)\\Big]$\nWhich one of these criteria to choose depends on the problem at hand [41]. If we care about the goodness of a single policy execution, ESR is the correct criterion. If instead, we are interested in the quality of average policy executions, we should use SER. In this work, we opt for the SER criterion in the learning, modelling agents that are interested in optimising their behaviour in repeated interaction settings."}, {"title": "Multi-Objective EPGG", "content": "We formulate a multi-objective version of the EPGG, called Multi-Objective Extended Public Goods Game (MO-EPGG), by employing the framework of multi-objective stochastic games, outlined in Section 3.2. In our framework, the state space consists of the value of the multiplication factor $f$ of the game currently being played, and the action space coincides with that of the single-objective EPGG (Section 3.1). We notice that the transition function for this framework is simply a random sampling from the set of possible multiplication factors at the beginning of each episode and deterministically returns that same $f$ value at all the subsequent steps of the episode.\nTo complete our multi-objective formulation of the EPGG, we need to vectorize the scalar reward signal obtained by agents in the EPGG. This process is called multi-objectivization of single-objective problems [25, 30]. By observing the form of the reward function in Equation 1, we can easily distinguish between the part that defines the collective ($r^c$) and the individual payoff ($r^i$):\n$r^c_i (a, f, c) = \\sum_{j=1}^n c I (a_j) \\cdot f$\n$r^i (a, c) = c_i(1 - I(a_i))$.\nThen, in the proposed MO-EPGG, the vectorial reward received by agent $i$, given action profile $a$, current multiplication factor $f$, and a tuple of endowments $c$, is as follows:\n$r_i(a, f, c) = (r^c_i (a, f, c), r^i (a, c))$.\nThis completes our description of the MO-EPGG as a MOSG with $d = 2$ objectives. In Figure 1 we display an example of the vectorial rewards received by $N = 2$ agents playing the MO-EPGG for three different values of the multiplication factor $f$.\nTo define the agents' utility functions, we follow a similar approach to the incentive structure proposed by Mullett et al. [33], but"}, {"title": "Game Analysis", "content": "We analyse the MO-EPGG with the proposed utility function. We first analyse the MO-EPGG under ESR and SER, to examine the dynamics of best responses for different values of the game and utility function parameters. Second, we investigate the impact of different values of $\\beta$ and $f$ on the set of Nash equilibria under SER.\nFrom Equation 7, we can observe that the preference between the cooperative or defective behaviour in the MO-EPGG depends on the relationship between three values, namely, $f$, $c$ and $\\beta$. In particular, assuming a uniform value of $\\beta$ among the whole population of agents ($\\beta_i = \\beta$ for all $i \\in N$), the collective cooperative action $(a_C = (C,..., C))$ is preferred over the collective defective action $(a_D = (D,..., D))$ by all the agents when\n$r^c_i (a_C, f, c) > r^i(a_D, f, c)$, which is the case when $(cf)^\\beta > c$.\nThis relationship between the variables induces a shared preference over collective cooperative behaviour in otherwise defective scenarios (the cases when $f < 1$). In general, collective cooperation is preferred over collective defection whenever either of the following conditions holds:\n$\\beta< \\frac{log(c)}{log(cf)}$   if  $0 < cf < 1$\n$\\beta> \\frac{log(c)}{log(cf)}$    if  $cf > 1$.\nIn the same way, collective defection is preferred over collective cooperation whenever $(cf)^\\beta < c$.\nFrom this point onwards, we focus on analysing a 2-player MO-EPGG, under the SER criterion. We determine the minimum cooperation level of the opponent for which the player's best response is to cooperate. We compute this value for different values of $f$ and $\\beta$. We note that the minimum cooperation level of the opponent should be strictly greater than the value presented in the table, for the best response to be cooperation.\nWe can observe that, for the game with competitive incentive alignment ($f = 0.5$), the best response for each player is to defect every time the value of $\\beta < 2$ (i.e., the opponent's probability to cooperate cannot be > 1, hence the condition is unattainable). If $\\beta > 2$ the best response is cooperation whenever the strategy of the opponent is to cooperate with a probability bigger than the value presented in the table. For example, for $\\beta = 3$, the best response is cooperation whenever the strategy of the opponent is to cooperate with a probability bigger than 0.6. For $\\beta = 4$, the threshold moves to 0.4, and to 0.3 for for $\\beta = 5$. For both the games with $f = 1.0$ and the game with a mixed-motive incentive alignment, $f = 1.5$, the best response is to defect every time $\\beta < 2$, and to cooperate otherwise. For a game with cooperative incentive alignment, the best response is to cooperate every time $\\beta \\geq 1$."}, {"title": "Price of Anarchy", "content": "To evaluate the goodness of the possible outcomes of the system against the Nash equilibria of each game in the MO-EPGG (defined by a specific $f$ value), we adapt the metric known as Price of Anarchy (PoA) [28, 39]. The PoA is, the ratio between the welfare of the system in its \"best solution\", i.e. social optimum, and the welfare of the system at its worst NE. Thus, it expresses the potential degradation factor of the social optimum. We highlight that higher values of the PoA indicate the existence of possible worst outcomes for the system due to agents' selfishness, in comparison to the social optimum.\nOn the other hand, a PoA with a value of 1 indicates an overlap between the social optimum and the worst-case selfish action. To define the social optimum in our setting, we employ the utilitarian welfare function, summing the outcomes of the utility functions of the players:  $W(\\pi) = \\sum_{i=0}^{N-1} u_i(V_i^\\pi)$, where $V^\\pi$ is the expected vectorial return of player $i$, under the joint strategy $\\pi$. Then, the PoA is defined as follows:\n$PoA = \\frac{\\max_\\pi W(\\pi)}{\\min_{\\pi \\in Nash} W (\\pi)} $"}, {"title": "Experimental setup", "content": "We train independent RL agents by adapting the multi-objective version of the Deep Q-network (DQN) algorithm [32] described in [48], which allows us to optimize policies under the SER criterion. We adjust their approach to work with our scalarization function (Equation 7). The loss function for MO-DQN can be expressed as follows:\n$L(\\theta) = \\mathbb{E}_{s,a,s',r \\sim D} [(r - (r + \\gamma \\mathbb{E}_{a'} Q_{\\theta'}(s', a') - Q_{\\theta}(s,a)))]$,\nwhere $\\theta$ and $\\theta'$ represent the DQN weights at two different timesteps of the training. $D$ represents the buffer of stored transitions, and $r$ is the vector reward. We find the best action $a^*$ by applying the SER optimization criterion, namely, by applying our custom scalarization function $u$ to update the MO-DQN function:\n$a^* = \\arg \\max_{a'\\in A} \\mathbb{E}_{s,a,s'} (u(r + \\gamma Q_\\theta(s',a')))$"}, {"title": "Experiments", "content": "The experiments are run over a pool of $N = 20$ agents. At each iteration $t$ of the learning process, a multiplication factor $f_t$ is sampled uniformly from the interval $[f_{min}, f_{max}]$, where $f_{min}$ and $f_{max}$ are chosen such as to include cooperative, competitive, and mixed-motive games. Afterwards, a subset with $M = 4$ active agents is randomly sampled from the pool of $N$ agents, to participate in the game for 10 consecutive rounds. After these interactions, the MO-DQN networks are updated. Given $M = 4$, we picked $f_{min} = 0.5$ and $f_{max} = 6.5$, to enable agents to engage in competitive, mixed-motive and cooperative games. This enables sampling from a set that contains competitive ($f < 1$), mixed-motive ($1 < f < M$), and cooperative games ($f > M$). Each agent receives as observation the current value of the multiplication factor-which can be observed with uncertainty-together with the previous actions taken by each opponent at the previous time step: $O_i = (f_{obs}, a^{-i}_t)$, where $a^{-i} = (a^i_j)_{j\\in M, j \\neq i}$. Therefore, each agent learns a policy $\\pi_i: O_i \\times A \\rightarrow [0, 1]$, where $O_i$ is the set of all possible observations of agent $i$.\nWe model uncertainty over the observation of the multiplication factor as Gaussian noise over the value of $f$, received from the environment: $f_{obs} = f + N(0,\\sigma_i)$, where $\\sigma_i$ is the uncertainty experienced by agent $i$. To ensure that the value of the observed multiplication factor coheres with the set of allowed values of $f$ in the MO-EPGG, we round up every negative sampled value to 0.\nAll the experiments are run for 20000 epochs, and results are averaged over 20 runs for every condition. The RMSprop learning rate is set to $\\alpha = 0.001$, and $\\gamma = 0.99$. The action selection mechanism is $\\epsilon$-greedy, with $\\epsilon = 0.01$. The values of the weights are $w = w^1 = 1$ for all the agents. All the plots show the values of the average cooperation of the active agents at every evaluation step of the learning process. The DQN networks are composed of 2 hidden layers, and ReLU nonlinearities are employed between layers."}, {"title": "Results", "content": "We group our empirical results in two categories, namely homogeneous preferences, when the value of $\\beta$ is identical for every agent, and heterogeneous preferences, where each agent $i$ is characterised by a different $\\beta_i$ value. Both categories include experiments with and without uncertainty on the observation of the multiplication factor $f$."}, {"title": "Learning with homogeneous preferences", "content": "We first explore the impact of different values of $\\beta$ on the scenarios with and without uncertainty on the observations. We performed experiments for different values of $\\beta$, that define a linear ($\\beta = 1$), a convex ($\\beta > 1$) and a concave ($\\beta <1$) utility function. In each of these experiments, $\\beta$ values are identical for every agent. The results for these experiments are depicted in Figure 4.\nThe experiments with $\\beta = 1$ represent the baseline where agents are playing the linear version of the MO-EPGG. Therefore, in the games without uncertainty, we observe as expected convergence to cooperation whenever $f > M$, convergence to defection whenever $f < 1$, and a certain percentage of cooperation when $1 < f < M$, depending on whether the value of $f$ is closer to a cooperative or a competitive one. When uncertainty is introduced, cooperation is increased in the competitive and mixed-motive scenarios. This results"}, {"title": "Learning with heterogeneous preferences", "content": "Secondly, we investigate the impact of learning in the MO-EPGG when the agents' preferences $\\beta_i$ are heterogeneous, i.e., the value of $\\beta_i$ for every agent $i$ is sampled from a normal distribution centred in 1, i.e. $\\beta_i \\sim N(\\mu_\\beta, \\sigma_\\beta) \\forall i \\in N$, with $\\mu_\\beta = 1$. This allows us to get more values centred around risk neutrality, and few extreme risk-averse or risk-seeking tendencies. We performed experiments with different values of $\\sigma_\\beta$, i.e., 0.5, 2 and 3. The resulting system represents a population where every individual has a different risk preference and is centred on risk-neutrality ($\\beta = 1$).\nFigure 5 reports the results for the scenarios without (top row) and with (bottom row) uncertainty on the observations. We can observe that when the of of the distribution is small (i.e. $\\sigma_\\beta = 0.5$) and no uncertainty is introduced, the competitive equilibria for $f\\in \\{0.5, 1.5\\}$ is maintained, while the cooperative equilibria for $f\\in \\{3.5, 6.5\\}$ is lost. We also notice that the higher the value of $\\sigma_\\beta$, the higher the average cooperation of the system in all the games. This result signals the importance of the magnitude of beta on the risk attitude: the higher $\\beta$, the higher the risk appetite, which in our case translates to more cooperative behaviour.\nWhen uncertainty is introduced, we observe that cooperation is increased in the competitive and mixed games with respect to the cases without uncertainty. This result is consistent with previous findings on the presence of uncertainty in non-cooperative environments [36, 37]. Interestingly, only the non-cooperative games are affected by the presence of uncertainty: in the cooperative games, the average cooperation of the system is equal to the one observed in the scenario without uncertainty."}, {"title": "Equilibria and Learning", "content": "We compare now our experimental results with the analysis from Section 4.1. We underline that, while the computation of equilibria is game-specific, the outcomes of the experiments result from concurrent learning on the set of environments modeled by the MO-EPGG.\nComparing the plots in Figure 4 and the analytical results of the Nash equilibria from Figure 2, we can observe that, in the case without uncertainty, in all the games for which $f \\neq 0.5$, namely $f\\in \\{1.5,3.5, 6.5\\}$, the pool of agents learns the best response, which means that the system converges to the Nash equilibrium. Only for B = 1 the convergence is not perfect. In the case $f = 0.5$, the agents converge to defection when $\\beta < 2$, while for $\\beta > 2$ they cooperate with a certain probability, which is higher for higher B"}, {"title": "Conclusions and Future Work", "content": "In this work, we introduced and analyzed a novel multi-objective variant of the Extended Public Goods Game, the MO-EPGG. This allowed us to model individual risk attitudes, by decoupling the collective and the individual payoffs. We investigated the role of different factors such as misalignment of incentives, uncertainty and individual risk preferences on cooperation, utilizing multi-objective reinforcement learning. In particular, we observed how risk-averse attitudes can increase defection in cooperative environments, and, inversely, risk-seeking ones, can increase cooperation in competitive and mixed-motive games, especially when uncertainty is introduced. Moreover, we observed how a population with heterogeneous risk attitudes, centred on risk neutrality, can fail to reach cooperation in cooperative settings.\nAs future work, we plan to explore additional MORL approaches, such as policy-gradient based methods, in the MO-EPGG. Additionally, we will investigate the interplay between risk preferences, uncertainty and additional mechanisms, such as reputation mechanisms and social norms. Last but not least, we plan to apply other forms of non-linear utility functions, including exploring different dynamics between the collective and individual payoffs."}]}