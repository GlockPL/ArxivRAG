{"title": "Transfer Reinforcement Learning in Heterogeneous Action Spaces using Subgoal Mapping", "authors": ["Kavinayan Sivakumar", "Yan Zhang", "Zachary Bell", "Scott Nivison", "Michael Zavlanos"], "abstract": "In this paper, we consider a transfer reinforcement learning problem involving agents with different action spaces. Specifically, for any new unseen task, the goal is to use a successful demonstration of this task by an expert agent in its action space to enable a learner agent learn an optimal policy in its own different action space with fewer samples than those required if the learner was learning on its own. Existing transfer learning methods across different action spaces either require handcrafted mappings between those action spaces provided by human experts, which can induce bias in the learning procedure, or require the expert agent to share its policy parameters with the learner agent, which does not generalize well to unseen tasks. In this work, we propose a method that learns a subgoal mapping between the expert agent policy and the learner agent policy. Since the expert agent and the learner agent have different action spaces, their optimal policies can have different subgoal trajectories. We learn this subgoal mapping by training a Long Short Term Memory (LSTM) network for a distribution of tasks and then use this mapping to predict the learner subgoal sequence for unseen tasks, thereby improving the speed of learning by biasing the agent's policy towards the predicted learner subgoal sequence. Through numerical experiments, we demonstrate that the proposed learning scheme can effectively find the subgoal mapping underlying the given distribution of tasks. Moreover, letting the learner agent imitate the expert agent's policy with the learnt subgoal mapping can significantly improve the sample efficiency and training time of the learner agent in unseen new tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Transfer reinforcement learning is an area of active research interest in a variety of applications, spanning speech recognition [1], imaging [2], surgery [3], robot manipulation [4], and even gaming [5], [6]. Specifically, as a result of recent advancements in deep learning that have significantly improved computation and performance [7], transfer methods are now able to extract important abstract information from the experiences of expert agents performing different tasks and pass them on to other learner agents that have little to no prior knowledge about these tasks, thereby significantly improving the rates at which learner agents learn new tasks. A common approach to learn an optimal learner policy using the optimal policy of the expert relies on the notion of the Kullback-Leibler (KL) divergence. KL divergence measures the difference between two distributions and, therefore, can be used to capture whether information has been passed on from one agent to another. This approach is popular in teacher-student frameworks, where a teacher expert advises a student target on improving its policy [8], [9]. Other popular methods used to transfer knowledge across agents include behavioral cloning [10], [11] and demonstration-based methods [12], [13]. These methods focus on generating the teacher or expert trajectories which are then used to train the source's policy. However, teacher-student and behavioral cloning techniques cannot be directly applied to transfer information across heterogeneous policy functions shaped by different action spaces, as they can lead to unsafe trajectories for the student agent [14]. To address this limitation, methods to decompose the policy functions have been proposed so that transfer across different policy structures can be achieved. For example, [15] uses a modular approach that decomposes neural network policies into \"task-specific\" and \"robot-specific\" modules, the former being shared across robots and the latter shared across all tasks for each agent. Then, the learnt modules are combined to recompose a full policy which can transfer knowledge for a given task. However, meshing of modules together to find an optimal policy can be difficult in complex environments. An alternative transfer learning approach for heterogeneous agents under sparse rewards is proposed in [16] that focuses on the times when the learning agents should imitate the expert versus when they should choose to explore. In practice this method was shown to depend heavily on self-exploration of the learner agent and the benefit of learning from the expert was not clear.\nCompared to the transfer learning methods discussed before, mapping approaches are perhaps more intuitive as they capture latent connections between optimal agent trajectories in different action spaces, providing translations between the heterogeneous agents. One such approach is developed in [17] that uses a handcoded mapping between different tasks which have parallels to different action spaces; in both cases having a mapping can help with transferring information. The method in [18] also uses a handcoded mapping, but does not provide competitive experimental results. Although handcoded mappings have been proven to work in some capacity, they are also generally suboptimal and cannot be automatically generated; as a result, additional setup work is required before the transfer learning process can begin [18]. Transfer maps for alignment-based transfer learning have been developed in [19], [20] to transfer knowledge in robotic manipulation tasks. Specifically, this method uses low dimensional manifolds to represent the source and target datasets of two heterogeneous agents for a particular task and computes a linear mapping between the manifolds. The limitation of a linear mapping is that it may break down in environments with more complex agents. A different approach is proposed in [21] that relies on a Markov Decision Process (MDP) homomorphism to transfer knowledge both across different action and state spaces. This approach relies on partial mappings between some features of the source and target tasks and depends on context transferable tasks having the same set of optimal action values across spaces, which is not always guaranteed. Finally, when homogeneous action spaces are considered, mapping methods have been successfully applied for transfer learning, as shown, e.g., in [22], that proposes a mapping based on state features to transfer state and action representations between agents in a multi-agent environment, but this approach was not tested for heterogeneous agents.\nTo address the above limitations of existing solutions, in this paper we learn a mapping between subgoal sequences in the trajectories of an expert and a learner agent. Specifically, we use expert and learner trajectories in a training dataset to learn a Long Short Term Memory (LSTM) network using supervised learning that captures the mapping between expert and learner subgoals, and then use this mapping to predict a learner subgoal sequence for a new task in the testing set. The learner agent's policy is hierarchical, with a high level policy that decides which subgoal state the agent should achieve next and a low level policy that learns the sequence of primitive actions to reach that subgoal. The predicted subgoal sequence for this new task is then used to warm initialize the learner agent's high level policy, after which finding an optimal overall policy is made faster, as the low level policy only needs to optimize agent trajectories in shorter sgments in between subgoals. Our method removes the need for handcoded mappings and does not have any limitations based on linearity on the final mapping.\nTo our knowledge, most closely related to the method pro- posed here is the work in [23], which uses mutual informa- tion loss to train embeddings which extract knowledge from the teacher policy to blend with the student policy. However, blending between networks assumes that any representations of knowledge of a state's value from the teacher policy will be useful for the student policy, which is not guaranteed.\nThe rest of the paper is organized as follows. In Section II, we formulate the problem of transfer learning across different action spaces and introduce some preliminaries. In Section III, we develop our proposed algorithm. Finally, in Section IV, we present experimental results that illustrate the proposed method."}, {"title": "II. PROBLEM FORMULATION", "content": "Let $S_{expert} \\in S_{expert}$ and $S_{learner} \\in S_{learner}$ denote the states of an expert and a learner agent, respectively, where $S_{expert}$ is the state space of the expert agent and $S_{learner}$ is the state space of the learner agent. Moreover, let $A_{expert}$ and $A_{learner}$, denote the action spaces of the expert and learner agent that can contain primitive actions, macroactions, or a combination of both. A primitive action is a base action $a \\in A$ an agent can perform, while a macroaction is a sequence of primitive actions, ${a_1, a_2,..., a_T}$, where T is the total number of primitive actions in the macroaction. In this paper we assume that the action spaces $A_{expert}$ and $A_{learner}$ can be different.\nConsider also a distribution of tasks, $d \\sim D(p)$, where d is a sampled task parameterized by the task parameter p. For example, if D is a distribution of motion planning tasks that require an agent to move to a goal position, d is a task pertaining to a specific end goal position. Here, p may include specifications about the space in which the goal po- sition can be selected, such as length and width of the space.\nA trajectory associated with task d is defined as a sequence of state action pairs, i.e., $t = [(s_0, a_0), (s_1, a_1), ...(s_T, a_T)]$, that accomplishes the desired task, where T is the maximum time horizon.\nLet $r_d(s_t, a_t)$ denote the reward received by the learner agent at state $s_t$ when it takes action $a_t$ in a task d. We define a state transition function $s_{t+1} = p(s_t, a_t)$. The state value function is defined as $V^{\\pi}(s) = E_{\\psi^{\\pi}} [\\sum_{t=0}^{\\infty} \\gamma^t r_d(s_t, a_t)|s_0 = s, \\pi]$ where $\\gamma$ is the discount factor and $\\psi^{\\pi}$ is the distribution of states. We parameterize our policy $\\pi_\\theta$ with policy parameter $\\theta$. The objective is to find the optimal policy $\\pi_\\theta^*$ for the learner agent that solves the problem\n$\\max_{\\theta} J(\\theta)$ (1)\n=within a finite time horizon T where $J = E[V^{\\pi}(s(0))|\\psi(s(0)), \\pi_{\\theta}]$ is the objective function for the learner agent. In this paper, our goal is to solve the following problem:\nProblem 1: (Transfer Reinforcement Learning across het- erogeneous action spaces) Given state spaces $S_{expert}$ and $S_{learner}$, action spaces $A_{expert}$ and $A_{learner}$ and a data set $W \\sim D$ containing expert and learner demonstration trajectories $\\tau$ for some tasks sampled from the task distribution D, use the data in $W_s$ to learn an optimal learner policy $\\pi^*$ for a new unseen task sampled from D, given a new, single expert demonstration of this unseen task, much faster than without transfer of the data in $W_s$.\nWe note that transferring knowledge directly across action spaces is difficult. To see this, consider two agents 1 and 2 that both share state s and whose goal state is $s_f$. Given an action $^1a$ for agent 1, finding a function $M(s, s_f, ^1a) \\rightarrow ^2a$ that outputs the corresponding action for agent 2 to reach the same goal state $s_f$ from state s is difficult because there is no guarantee for a one-to-one mapping between actions in both action spaces. This is illustrated in Figure 1, where two agents, one diagonal and one vertical/horizontal are moving on a grid. This example demonstrates that mapping between different action spaces cannot be done at the primitive action level and it may be required that mappings are designed over sequences of actions. In the example in Figure 1, such a mapping over sequences of actions may take the form $M(s, s_f, \\{^1a_t\\}_{t=0}^T) \\rightarrow \\{^2a_t\\}_{t=0}^T$. Still, if this mapping M only maps to the terminal state $s_f$, it may fail to learn latent mappings between agent trajectories on shorter timescales. In this paper, we learn such latent mappings which we define to be mappings between sequences of expert and learner subgoals extracted from their corresponding trajectories. These sequences of subgoals effectively compress sequences of primitive actions and, as a result, avoid the need to map between primitive actions themselves. By extracting sequences of subgoals from agent trajectories, we can learn the high-level behavior of agents whose goal is to achieve a terminal goal state. Moreover, since subsequences of subgoals may appear repeatedly for different terminal goal states, these latent agent behaviors can allow for more robust learning as the mapping is reinforced through behaviors that appear often."}, {"title": "III. SUBGOAL TRAJECTORY MAPPING METHOD", "content": "In this section, we first parameterize the mapping function M using a recurrent neural network (RNN) and describe its training procedure. Then, we explain how to use the learned mapping to transfer knowledge from the expert agent to the learner agent so it may learn with fewer samples on unseen tasks within the task distribution.\nWe assume the expert and learner agents share the same initial state $s_0$ and terminal state $s_f$ and let their corre- sponding optimal trajectories be given by $\\tau_{expert}$ and $\\tau_{learner}$. Then, we can define the sets of subgoals of the expert and learner agentsf as $G_{expert}$ and $G_{learner}$ respectively, where $g_{expert} \\in G_{expert}$ and $g_{learner} \\in G_{learner}$ are states in $\\tau_{expert}$ and $\\tau_{learner}$ respectively [24]. Moreover, we define by $g_{t,expert}$ or $g_{t,learner}$ a state in $\\tau_{expert}$ or $\\tau_{learner}$ that the expert or learner agent seeks to achieve at time step t, respectively. In this paper, we assume that the subgoal sets $G_{expert}$ and $G_{learner}$ are given. In practice, these subgoal sets can be extracted from the trajectories in the source dataset $W_s$ using unsupervised learning methods, e.g., the algorithm in [25]. Finally, we also define by $\\{g_{t,expert}\\}_{t=0}^T$ and $\\{g_{t,learner}\\}_{t=0}^T$ sequences of subgoals of length T drawn from an optimal trajectory $\\tau$.\nSpecifically, in what follows, to solve Problem 1, we first learn a mapping function M that maps the expert agent's subgoal sequences to those of the learner agent through supervised learning; see Section III.A. Using this mapping, we then design a transfer learning algorithm in Section III.B that can reduce the training time for the learner agent in unseen tasks drawn from the distribution D(p), when the learning agent is also provided with a demonstration of this unseen task by the expert agent.", "sections": [{"title": "A. Subgoal Sequence Mapping", "content": "We define the subgoal trajectory mapping function as\n$M: (\\{g_{t,expert}\\}_{t=0}^T,p) \\rightarrow \\{g_{t, learner}\\}_{t=0}^T,$ (2)\nwhere $\\{g_{t,expert}\\}_{t=0}^T$ and $\\{g_{t, learner}\\}_{t=0}^T$ are defined above and p is the mapping function's context, which can be defined as environment parameters (length and width of a grid environment). Note that the sequence of subgoals returned by the mapping in equation (2) depend on the environmental parameters as the latter dictate the state space and the subgoals predicted by the mapping in (2) are defined on this state space (e.g. space restrictions). Given a set of subgoals used by an expert agent to solve a task with context p in the expert action space, equation (2) returns another sequence of subgoals that the learner agent can use to solve the same task in its own action space, which is different from the expert's.\nTo model the mapping function M, we adopt a Recurrent Neural Network (RNN), specifically a Long Short-Term Memory (LSTM) network. The network is aware of the total number of subgoals to choose from. In the worst case, these can be all states in the sets $S_{expert}$ and $S_{learner}$ and we discuss briefly how to define the subgoal set for our specific applica- tion in Section IV. Our Keras model [26] encodes the subgoal sequence of the expert and outputs a decoded, predicted subgoal sequence for the learner for a task d. For the encoder, we use a bidirectional LSTM network that outputs a fixed- size vector with dimensionality 150 for the input sequence of subgoals [27]. The decoder is a bidirectional LSTM network of dimensionality 100 connected to a densely connected softmax layer which outputs a sequence of subgoals for the learner. Bidirectional LSTMs can represent sequences both forward and reverse, allowing for more accurate predictions in sequence to sequence modeling [28]. Given the softmax probabilities, the model outputs the predicted sequence for the learner. A fully connected layer is used to connect both bidirectional LSTMs together.\nSince the sequences of subgoals are temporally related to each other, the positional order of subgoals within the sequence as well as connections need to be captured within the neural network. Then, the mapping function M in equation (2) can be defined by the below general update for LSTMs\n$h_t = \\sigma_{\\eta}(W_h g_{t,expert} + U_h h_{t-1} + b_h)$\n$g_{t, learner} = \\sigma_y (W_y h_t + b_y)$,\nwhere $g_{t,expert}$ and $g_{t,learner}$ are the t-th subgoal of the expert and the learner agent, respectively. The variables W, U, and b are parameter matrices and vector, ht is the hidden layer vector, and $\\sigma_{\\eta}, \\sigma_y$ are activation functions. The parameters of the mapping function M are trained by minimizing the Cross Entropy Loss between the predicted learner agent's subgoal trajectory and the known learner agent's trajectories for the tasks in the training set. This process is further explained in section IV.2."}, {"title": "B. Transfer Learning using Subgoal Sequence Mapping", "content": "Given the predicted sequence of subgoals $\\{g_{t,learner}\\}_{t=0}^T$ for the learner, we develop a hierarchical reinforcement learning framework where a high level policy selects the subgoal that the learner agent should pursue next and a low level policy selects primitive actions necessary to meet that selected subgoal. We define the high level policy as\n$\\pi_h : s_{t, learner} \\rightarrow g_{t, learner}$ (3)\nand the low level policy as\n$\\pi_l : (s_{t,learner}, g_{t,learner}) \\rightarrow \\{a_{t,learner}\\}$. (4)\nSpecifically, given a new unseen task, the expert agent pro- vides the learner with an expert trajectory of subgoals, which the learner uses to predict its own subgoal trajectory using the trained mapping M. Then, using a high level policy (3) that is warm initialized with the predicted subgoal trajectory to output the subgoal sequence needed and applying a low level policy (4) to achieve these subgoals, the learner agent can solve the unseen task using the expert agent's experience, even across different action spaces.\nDepending on the training set used to train the mapping M, the sequence of predicted learner subgoals may be suboptimal for an unseen task d. Therefore, it is desirable that the learner agent can correct for possible errors in the predicted subgoal sequences as it optimizes its policy during learning. Merely sampling from the predicted subgoal trajectory of the mapping M will not necessarily exploit the correctly predicted subgoals more often than the incorrectly predicted subgoals.\nTo take advantage of the correctly predicted subgoals in the mapping's predicted subgoal sequence for the learner and help the learner agent's policy converge to the optimal policy faster, we propose to warm initialize the high level policy of the learner with the mapping's predicted subgoal trajectory using supervised learning. This way we can bias the learner's exploration before learning, as seen in Algorithm 1. The input data for supervised learning is the high level policy's output weights associated with the state of the learner agent and the associated label is the predicted subgoal for that state. Along with these training labels, we also incorporate noise into the supervised learning process, which has been shown to improve learning [29], [30] and also help the learner's high level policy explore other subgoal options in case the mapping has not predicted the subgoal trajectory for the learner agent accurately. As seen in Algorithm 1, we add noise to the labels in the dataset by setting a probability parameter $U_{noise} \\in [0, 1]$, where for any probability $q > U_{noise}$, the label is $g_{t, learner}$, i.e., the subgoal predicted by the subgoal trajectory returned by the mapping function. For $q < U_{noise}$, the label for a state is a random subgoal from the subgoal set $G_{learner}$. Upon completion of this supervised learning process, the policy weights reflect a bias towards the next subgoal in sequence. We generate data points for the states by incorporating the previously achieved subgoal into the next state. We use cross entropy loss [31] as part of the supervised learning.\nAfter the above warm initialization process for the high- level policy, we also train the low level policy $\\pi_l$ in a supervised manner using the trajectories $\\tau$ from the learner demonstrations in the seen tasks from the distribution D. As opposed to the supervised learning approach to warm initialize the policy $\\pi_h$, here we do not incorporate noise as the expert trajectories contain no errors with regards to which action should be taken given a state and desired subgoal.\nFinally, we train both high-level and low-level policies using"}]}, {"title": "IV. EXPERIMENTS", "content": "In this section, we illustrate the proposed transfer learning method in a chess environment, where the two agents we consider are a dark squared bishop and a knight. Chess has long been a highly regarded game to test reinforcement learning algorithms as it requires calculating strings of moves which can lead to multiple branches [34]. It is also an ideal experimental environment for reinforcement learning prob- lems with heterogeneous agents, as it has pieces which move and interact very differently from each other. All experiments are implemented using PyTorch [35] on a Ubuntu system with Nvidia RTX 2080 Ti, while the mapping model M is implemented using Keras [26].\nFigure 2 describes the different action spaces and transi- tions of the bishop and knight agents respectively. Specifi- cally, each blue square represents the next square these agents can move to. Note that the bishop cannot surpass other pieces if they are on its way. According to Figure 2, these two agents have heterogeneous action spaces and transition functions.\nIn the experiments, the bishop or knight agent is located on an 8 \u00d7 8 board with a number of opposing pawns put in legal positions. Here, we define legal positions as any dark squares not on the first or last row of the chess board, so as to accommodate the dark squared bishop's limitations. The goal of the agents is to take as many pawns as possible with the least number of moves. The reward functions for the high level policy $\\pi_h$ and the low level policy $\\pi_l$ are defined as follows\n$\\Upsilon_{t,h} = \\{\n    \\begin{cases}\n        -1: \\text{ agent lands on a square that has no pawn,}\\\\\n        10: \\text{ agent captures a pawn,}\\\\\n        \\end{cases}$\n$\\Upsilon_{t,l} = \\{\n    \\begin{cases}\n        -1: \\text{ agent does not arrive at desired subgoal,}\\\\\n        10: \\text{ agent arrives at desired subgoal.}\\\\\n        \\end{cases}$\nWhen all pawns are captured or a finite number of timesteps has passed, the episode ends. Specifically, in our experiments, we have 2 pawns to capture. The set of tasks contains all possible positions of the pairs of pawns in legal positions. The overall task distribution contains 253 tasks, with the training set of size 228 and the testing set of size 25. The tasks in the training and testing sets are randomly allocated. In addition, we assume that we have the optimal sequences of moves of the bishop and the knight agents for the tasks in the training dataset, which are learnt through Djikstra's algorithm. The goal is to transfer the optimal moves of the bishop for the tasks in the testing dataset to help the knight agent learn its optimal moves for the unseen tasks in the testing dataset.", "sections": [{"title": "B. LSTM Network Prediction Accuracy", "content": "We train a LSTM network to represent the subgoal se- quence mapping function M in (2). Specifically, the LSTM network receives the embedding of the bishop agent's sub- goal trajectory as the input, and outputs the probability of selecting each square on the board as a subgoal for the knight agent. The initial hidden layer of the LSTM network, $h_0$, is defined as the starting position of the agent and the positions of the pawns. We use categorical cross entropy as the loss function [31], which is defined as\n$L = - \\sum_{i=1}^{N} P(g_{t,learner}) log(P(\\hat g_{t,learner})), (5)$\nwhere N is the total number of subgoals in the set $G_{learner}$, $P(g_{t, learner})$ is the probability of the target subgoal from a trained task at time step t, and $P(\\hat g_{t,learner})$ is the probability of the model's output subgoal at time step t. The neural network is trained so that the categorical cross entropy loss function is minimized given the bishop and knight subgoal trajectories in the training dataset.\nIn order to assess how accurate the mapping M is at predicting subgoal trajectories, we use a metric for machine translation output used in natural language processing (NLP), METEOR [36]. The METEOR scores across a K-fold cross-validation on the overall task set with K = 10.\nNote that the function M is trained with both the bishop and knight trajectories in the training set. Then, the scores reflect the quality of predicting the subgoal trajectories for the knight agent on tasks in the testing set. Specifically, if there is one error in the mapping's prediction, the METEOR score is approximately 0.6389, while if there are two errors, the METEOR score is approximately 0.4688. As a result, the average accuracy across the K-folds is between 1 and 2 errors in the mapping's prediction. As each subgoal trajectory in the dataset consists of 4 to 5 subgoals on average, we can see that the mapping predicts approximately half of the subgoals correctly."}, {"title": "C. Transfer Learning with Warm Initialization", "content": "In this section, we warm initialize the learner high level policy with the mapping's predicted subgoals as discussed in Section III-B. Then, we use deterministic policy gradient (DPG) [32] to train high level and low level policy functions $\\pi_h$ and $\\pi_l$. For the knight agent, we define its subgoal set at a specific position in Figure 3. Specifically, the knight can select any square at most 2 squares away from its current position. This is to ensure that successive subgoals are not too far away from the current position of the knight agent.\nWe can split the mapping's performance on a test set into three distinct cases: 1) the mapping predicts the subgoal trajectory without any errors; 2) the mapping predicts the subgoal trajectory with 1 or 2 errors; and 3) the mapping predicts the subgoal trajectory with 3 or more errors. Since most subgoal trajectories of the knight within this task set of 253 tasks have a length of 4, having 1 or 2 errors may still have significant impact. For each one of the three cases above, we compare the performance of three different approaches: transfer learning using the mapping's suggested subgoals as discussed in Section III; learning the optimal policy using DPG without any transfer; and warm initializing the high level policy $\\pi_h$ with the bishop's subgoal trajectory directly and then polishing the policy functions using DPG. We train each method for 20k episodes. Figures 4-6 show episode rewards averaged every 100 episodes. Each curve is averaged over 3 trials.\nIn Figure 4 we present learning curves for a task in the testing set for which the mapping has correctly predicted the sequence of subgoals. The policy that is warm initialized with the subgoal predictions given by the mapping function M reaches its optimal value while the other two methods fail to do so. In Figure 5, the mapping only partially predicts the correct subgoal trajectory for the knight in an unseen task. Even so, the policy warm initialized by the partially correct subgoal trajectory is able to achieve its optimal value within 20k episodes while the other two methods do not. This suggests that the policy function that is warm initialized with partially correct subgoal trajectories can still help high-level exploration and use fewer samples to find correct subgoals to overwrite the incorrect subgoals predicted by the mapping function M. Finally, in Figure 6, we present the learning curves when the mapping does not predict the subgoals correctly. The policy warm initialized by the mapping is still able to reach a local optimal value much faster than the policy that is trained with DPG without transfer or the policy warm initialized by the bishop subgoal trajectory directly.\nWe conclude that transferring the expert agent's subgoal trajectory to the learner agent using the method discussed in Section III can help exploration and accelerate the learning process for the learner agent even when the trained map- ping function M makes mistakes in predicting subgoals for unseen tasks. Moreover, the proposed method demonstrates significant improvements in learning performance compared to learning from scratch or transferring the subgoal trajectory directly from the other heterogeneous agent."}]}, {"title": "V. CONCLUSION", "content": "In this paper, we studied a transfer reinforcement learning problem across heterogeneous agents, where a recurrent neural network is used to map between subgoal sequences of both agents. The goal of the mapping is to predict the optimal subgoal sequence of the learner agent for an unknown task given the subgoal sequence of the expert. Given this prediction, we warm initialize the high level policy to bias it to pursue the predicted subgoal sequence from the mapping initially as it learns an optimal policy using a standard reinforcement learning algorithm. We demonstrated how to design this mapping using a LSTM recurrent neural network and provided numerical examples showing the effectiveness of our proposed method."}]}