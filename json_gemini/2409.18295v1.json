{"title": "Enhancing Lossy Compression Through Cross-Field Information for Scientific Applications", "authors": ["Youyuan Liu", "Wenqi Jia", "Taolue Yang", "Miao Yin", "Sian Jin"], "abstract": "Lossy compression is one of the most effective methods for reducing the size of scientific data containing multiple data fields. It reduces information density through prediction or transformation techniques to compress the data. Previous approaches use local information from a single target field when predicting target data points, limiting their potential to achieve higher compression ratios. In this paper, we identified significant cross-field correlations within scientific datasets. We propose a novel hybrid prediction model that utilizes CNN to extract cross-field information and combine it with existing local field information. Our solution enhances the prediction accuracy of lossy compressors, leading to improved compression ratios without compromising data quality. We evaluate our solution on three scientific datasets, demonstrating its ability to improve compression ratios by up to 25% under specific error bounds. Additionally, our solution preserves more data details and reduces artifacts compared to baseline approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Large-scale scientific simulation now play an important role in recent science research. These simulations can easily generate large amount of data. Take Nyx [1] as an example, one Nyx cosmological simulation with a resolution of 4096 \u00d7 4096 x 4096 cells can generate up to 2.8 TB of data for a single snapshot; a total of 2.8 PB of storage is needed, assuming the simulation runs 5 times with 200 snapshots dumped per simulation. The large volume of data presents two challenges: First, storing this scale of data entirely on disk is difficult, even for supercomputers. Second, the large data volume can consume significant time during file I/O or data transfer between devices, due to limitations in I/O bandwidth. Lossy compression is one of the best solutions to address this issue, as it can efficiently reduce the data size while only introducing data distortion within a controllable range. Compared to lossless compression, this approach is particularly effective for scientific data with significantly high compression ratio. The new generation of lossy compressors for scientific data, including SZ [2], [3], [4], [5] and ZFP [6], have been widely employed in various systems and frameworks to reduce data sizes and enhance performance [7], [8], [9], [10]. Researchers have also conducted extensive studies around various lossy compressors to improve their performance. For example, GPU variants of SZ and ZFP have been proposed to achieve exceptional throughput [11], [12], while novel predictors have been introduced to enhance compression ratios [5], [13]. Recently, researchers have tried to use machine learning to improve the performance of lossy compression. For instance, Liu et al. introduced AE-SZ [13], a framwork which use a convolutional autoencoder to improve error-bounded lossy compression for scientific data, and this framework exhibits better rate-distortion performance than SZ. However, several challenges remain: (1) The model size of the autoencoder is often quite large; for example, AE-SZ uses a model with approximately 380,000 parameters to predict the CLDTOT field in the CESM-ATM dataset, which contains only 1800 \u00d7 3600 data points. (2) Training the model can be very time-consuming, taking over 20 hours in some extreme cases. Jia et al. proposed GWLZ [14], a learning-based lossy compression framework, which uses multiple DNN models to reduce the reconstruction error introduced by SZ.\nHowever, existing studies only use information from the target field itself for prediction [5], transformation [6], or enhancement [14]. We have observed that there are strong correlations between different fields within the same dataset. In Figure 1, we present the visualization results of the 49th slice along the first dimension of the u, v, and w fields (i.e., zonal, meridional, and vertical wind speeds respectively) from the SCALE dataset. It is clear that they share similar structures, though their distributions in finer details are not the same, and their value ranges differ as well.\nExisting compressors for scientific data do not take advantage of this cross-field information. Therefore, in this paper, we propose a cross-field predictor to capture this information. We first use a CNN to predict the first-order backward difference of the target field by inputting the first-order backward difference of other fields (which we refer to as anchor fields). Then, we combine this difference predictor with the origi-"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "In recent years, a new generation of high-accuracy lossy compressors for scientific data has been proposed and developed for scientific floating-point data, such as SZ [2], [3], [4] and ZFP [6]. They have been widely employed in various systems and frameworks to reduce data sizes and enhance performance [7], [8], [9], [10]. Compared to lossless compression, lossy compression can provide significantly higher compression ratio by losing non-critical information. Unlike traditional lossy compressors such as JPEG [15] which are designed for images (in integers), SZ and ZFP are designed to compress floating-point data and can provide a strict error-controlling scheme based on user's requirements. Generally, lossy compressors provide multiple compression modes, such as the error-bounding mode. The error-bounding mode requires users to set an error type (e.g., relative error bound) and a bound value (e.g., 10-3). The compressor ensures that differences between original and reconstructed data do not exceed the error bound.\nSpecifically, prediction-based lossy compression, such as SZ, involves three main steps: (1) Each data point's value is predicted based on its neighboring points, using an adaptive, best-fit prediction method. (2) The difference between the actual value and the predicted value is quantized, based on the user-set compression mode and error bound. (3) Customized Huffman coding and additional lossless compression are applied to achieve a high compression ratio. The two most important metric types to evaluate the performance of lossy compression are: (1) compression ratio, i.e., the ratio between original data size and compressed data size, or bit-rate, i.e., the number of bits on average for each data point (e.g., 32/64 for single/double-precision floating-point data) before compression; and (2) data distortion metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) to measure the reconstructed data quality compared to the original data. Previous studies have focused on using surrounding data values to predict the current value, including using predictors such as Lorenzo [16], Regression [17], Interpolation [5], and autoencoder [13]. However, cross-field information has largely been overlooked."}, {"title": "B. Convolutional Neural Network", "content": "Convolutional Neural Networks (CNNs) are a class of feedforward neural networks that use filters to learn data features. In a CNN, there are input layers, hidden layers, and output layers, with the hidden layers containing one or more convolutional layers. These convolutional layers learn hidden features in the input data by applying filters that slide across the data, eventually producing a feature map for the next layer to learn from. In addition to convolutional layers, CNNs often include fully connected layers, various pooling layers, activation layers such as ReLU, and other specialized layers. Due to their exceptional feature extraction capabilities, CNNs have seen successful applications in many fields, including image and video recognition [18], image classification [19], and image segmentation [20].\nAdditionally, CNNs have shown strong performance in prediction tasks [21]. The goal of this paper is to learn cross-field information, and the local distribution similarity is an important prior knowledge. CNNs are particularly effective at extracting local features [22], making them well-suited for this task. Furthermore, the Lorenzo predictor used in SZ can actually be viewed as a masked CNN with fixed parameters. Therefore, we aim to leverage CNNs for cross-field prediction."}, {"title": "III. METHODOLOGY", "content": "In this section, we propose a machine learning-based method for predicting the current field using information from other fields. This additional information is utilized to enhance the existing Lorenzo predictor, with the objective of improving compression ratio. Our solution demonstrates significant improvements in compression ratio across various datasets and error bounds. By leveraging the mutual information between fields, our solution addresses several limitations of the traditional Lorenzo predictor and provides a more robust approach to data compression. The overview of our solution is shown in Figure 2. When compressing the target field, we use anchor fields to perform cross-field prediction, which enhances the accuracy of the prediction process. We start by calculating the first-order backward difference of the anchor field. This difference is then input into a CNN model to predict the first-order backward differences of the target field across different dimensions. The dashed line in the figure indicates that the target field is only involved during the training phase of the CNN model. The results of the cross-field prediction are then combined with the traditional prediction using local field information through a hybrid prediction model, leading to a more accurate overall prediction. Finally, the prediction is processed through encoding and lossless compression, resulting in the compressed data."}, {"title": "A. Cross-field Information", "content": "In scientific datasets, there are often inherent physical or mathematical relationships between different fields. Traditional lossy compressors typically rely solely on information from the target field for compression (i.e., local field information), overlooking the valuable cross-field information that could be utilized to improve the compression ratio.\nFor example, in the CESM-ATM dataset, the FLUT field closely mirrors the FLNT field, and the difference between the FLUTC and LWCF fields is also similar to the FLNT field in both value range and trend. Meanwhile, not all cross-field knowledge is as straightforward as this example. In many cases, the relationships between related fields are far more complex. Nonetheless, when visualized, these fields often exhibit significant correlations, particularly in the area of extreme values. Effectively capturing this cross-field information could greatly enhance the accuracy of predictions. However, the complexity of these relationships presents challenges for traditional algorithms to extract them. Furthermore, the large number of fields introduces additional difficulties in applying conventional methods. Consequently, we have chosen to apply machine learning to address this task, as it is more adept at extracting complex, non-linear relationships."}, {"title": "B. Cross-field Prediction", "content": "Physical and mathematical relationships exist between different data fields. A straightforward approach to capture this information is to use a neural network to directly predict the values of the target field based on the data from anchor fields. However, our evaluation shows that this approach rarely performs well, often resulting in poor prediction accuracy. This is because: (1) The value ranges of the data fields can be large, leading to a loss of precision when machine learning models attempt to predict these values; (2) The value itself can be highly irregular and difficult to learn; and (3) Machine learning models require a large input area to capture the basic value range of the target values, which results in slow inference throughput and large model sizes.\nTo solve this problem, we use a method that learns to predict the first-order backward difference of the target field in all directions. We propose a network named Cross-Field Neural Network (CFNN) to learn this backward difference. Compared to the original value, the first-order differences reflect local changes and are usually smoother, making them easier to learn with smaller input data area. The value range of these differences is usually smaller, which helps with normalization.\nWe choose to use the first-order differences of the original data from the anchor fields as the training data and use the decompressed data to predict the first-order differences of the target field. Although using decompressed data as training data can sometimes yield better prediction results, using the original data allows the CFNN to learn the true relationships between these fields. Additionally, it significantly reduces training time because, if we use decompressed data, the CFNN model would need to be retrained for each error bound. In fact, we found that learning the first-order central difference works better than backward difference. However, considering our overall design, we chose to learn the first-order backward difference in this paper. This choice will be explained in Section III-C."}, {"title": "C. Combine with SZ", "content": "In this paper, we aim to integrate the cross-field prediction to existing prediction-based lossy compressor. Specifically, we use SZ [2], [3], [4], [5] error-bounded lossy compression framework as baseline. SZ supports several types of predictors: Lorenzo, regression, and interpolation\u2014as well as their combinations. In this paper, we focus on enhancing with the Lorenzo predictor because (1) it is widely used and implemented in various SZ variants (e.g., cuSZ [11], SZx [23], cuSZp [24]); and (2) it performs well across most datasets, particularly in scenarios involving typical compression ratios (i.\u0435., 5\u201332).\nIn 2D datasets, predicting the data at position (i, j) using the 1-layer Lorenzo predictor is given by $f_{Lorenzo}(i,j) = V(i\u22121, j)+V(i, j\u22121)-V(i-1,j\u22121)$, where V represents the predicted value. Use this as a basis for reference., we formulate"}, {"title": "D. Detailed Design", "content": "1) Use Dual-quant to solve RAW dependency: In the SZ compressor, one critical challenge during the prediction and quantization stages is handling read-after-write (RAW) dependencies. In the original SZ algorithm, after predicting a data point, the error between the predicted and actual values is immediately quantized and written back to the dataset. This ensures consistency between compression and decompression, but it also introduces a RAW dependency, requiring each point to be processed sequentially. This sequential processing creates a significant performance bottleneck.\nTo address this issue, we implement a dual quantization approach, which is initially proposed in cuSZ [11]. Dual quantization involves two main steps: prequantization and postquantization. Here's how it works:\n\u2022 Prequantization: The original dataset is first quantized based on a user-defined error bound (eb). This step converts the original values into prequantized values that are easier to manage and ensures they are multiples of the error bound. This step is computationally straightforward and avoids the RAW dependency because it does not rely on the predicted values from previous steps.\n\u2022 Postquantization: After prequantization, prediction is performed using these prequantized values. The difference between the predicted value and the prequantized value is then calculated and stored. This difference, known as the postquantized value, does not introduce new errors and allows the prediction step to proceed without the sequential dependency that plagues the original SZ3 method.\nBy using dual quantization, we can eliminate the RAW dependency in compression process, enabling parallel processing of data points and significantly improving the compression throughput. During decompression, the prequantized values are used to reconstruct the original data by adding the postquantized differences, ensuring accuracy and consistency with the compressed data.\n2) Network For Cross-field Prediction: The CFNN is a specialized network designed for scientific data prediction, combining Depthwise Separable Convolutions [25] and a Channel Attention mechanism [26] to enhance computational efficiency and prediction accuracy. It takes the first-order backward differences of the anchor fields as input and outputs the predicted first-order backward differences of the target field. The network architecture of CFNN is illustrated in Figure 4. The network begins with an initial convolutional layer that performs preliminary feature extraction from the input data, capturing local spatial information. Following this, the Depthwise Separable Convolution module operates, consisting of a Depthwise Convolution that processes each channel independently to reduce computational complexity, and a Pointwise Convolution that recombines the channel information to extract more complex features.\nTo further refine the extracted features, the network incorporates a Channel Attention mechanism, which adaptively adjusts the importance of each channel. This mechanism utilizes global average pooling and max pooling to generate compact feature representations for each channel, which are then passed through two fully connected (FC) layers. These"}, {"title": "IV. EVALUATION", "content": "In this section, we present the evaluation results of our proposed framework for enhancing the performance of prediction-based lossy compression. We first provide details about the experimental setup, datasets, and baseline configurations. Next, we assess the effectiveness of cross-field information extraction to demonstrate its potential benefits. We then evaluate the performance of each individual component within our compression design. Finally, we conduct a comprehensive and detailed performance evaluation using datasets from real-world scientific applications."}, {"title": "A. Experimental Setup", "content": "1) System Configuration and datasets: The experiments are conducted on the HPC cluster at Temple University. Each node equipped with two Intel Xeon E5-2620v4 processors featuring 16 physical cores, and 4x NVIDIA Tesla V100 GPUs. Datasets used in our experiments are listed in Table I.\n2) Baseline: Our proposed model is designed to extract cross-field information and enhance the performance of prediction-based lossy compressors. While the underlying approach is general and can be extended to other contexts, this paper focuses specifically on improving the existing SZ3 framework. The baseline we use in our experiments is SZ3 with the Lorenzo predictor. Additionally, our proposed solution uses dual-quantization to eliminate the prediction dependency, the corresponding SZ3 is also modified to use dual-quantization, while all other steps remain unchanged. This also ensures that our solution is compatible with most CUDA versions of SZ [11], where dual-quantization is natively integrated. Note that our solution hybridizes information extracted from both cross-field and local-field predictions. Consequently, using the Lorenzo predictor (one of the local-field predictions) and dual-quantization as baselines ensures a fair comparison. We expect similar performance improvements with other compression configurations, which we plan to explore in future work."}, {"title": "B. Cross-field Prediction", "content": "We propose the CFNN in our solution to efficiently and effectively extract cross-field information. The CFNN prediction the first-order backward difference of the target field based on data from other fields without relying on any local information from the target field. As discussed in Section I, this design choice is based on the fact that local information is typically more dense and efficient to extract using existing predictors, such as Lorenzo. Therefore, it would be suboptimal to apply the same method for extracting information from both cross-field and local-field contexts.\nFigure 5 shows the training loss during the model training for a le-3 relative error bound. The left plot illustrates the steady decrease in loss for the CFNN model across epochs, where the data has been normalized to the range of 0-300. The right plot displays the loss reduction for the hybrid pre-"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce a novel cross-field prediction solution to enhance the performance of prediction-based lossy compressors. Our solution leverages CNNs to extract cross-field information, which is then combined with traditional predictors, effectively improving the overall compression ratio. Evaluations show that our solution can achieve compression ratio improvements of up to 27% across multiple scientific datasets. Additionally, our solution preserves more data details and reduces artifacts compared to baseline methods.\nIn the future, we plan to optimize the structure of the CFNN to better extract cross-field information. Additionally, we will refine the hybrid prediction model to more effectively combine cross-field information with existing information. We will also explore the use of transfer learning to identify more suitable anchor fields for cross-field predictions."}]}