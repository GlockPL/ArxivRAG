{"title": "Visual Fourier Prompt Tuning", "authors": ["Runjia Zeng", "Cheng Han", "Qifan Wang", "Chunshu Wu", "Tong Geng", "Lifu Huang", "Ying Nian Wu", "Dongfang Liu"], "abstract": "With the scale of Transformer-based vision models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets used in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as an effective and efficient solution for adapting large-scale Transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings, seamlessly integrating both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across various tasks, offering a general solution to address the data disparity challenge. Empirical results demonstrate that our approach outperforms several state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.", "sections": [{"title": "Introduction", "content": "\"Fourier's theorem is not only one of the most beautiful results of modern analysis,\nbut it may be said to furnish an indispensable instrument in the treatment of\nnearly every recondite question in modern physics.\u201d\nLord William Thomson Kelvin [1]\nPrompt tuning [2, 3] is initially introduced for parameter-efficient adaptation of large foundation models in natural language processing (NLP). As vision models continue to scale for enhanced performance, visual prompt tuning [4] has been applied to various vision domains (e.g., image classification [5], segmentation [6, 7], detection [8]), demonstrating superior performance and lower parameter usage compared to other parameter-efficient fine-tuning (PEFT) methods. However, a common challenge within the research community remains unaddressed: significant performance degradation occurs when there is a substantial disparity between the data used in pretraining and finetuning [9, 10]. This issue hinders the broader application of visual prompt tuning. Consequently, a natural question arises: \u2460 Can prompt tuning generalize across datasets with varying disparities?\nAs researchers commonly draw insights from human to replicate the principles in intelligent machines [11, 12, 13, 14], we consider to answer this question from the human visual cognition's perspective. While humans comprehend the world through past experiences/knowledge, it is essential to generalize and adapt this understanding to new tasks efficiently and effectively. The robust and rapid adaptability of human visual cognition thus arises from various domain analysis, capturing the new patterns from different channels and perspectives [15, 16, 17]."}, {"title": "Related Work", "content": "2.1 Visual Parameter-efficient Finetuning\nWith the significant growth in the scale of vision models, especially following the emergence of Vision Transformers [21, 22, 23, 24, 25], the development of PEFT methods under \"pretrain-then-finetune\" paradigm becomes increasingly critical. Current methods under this paradigm can be generally categorized into partial tuning [26, 27, 28], extra module (i.e., including reparameterization approaches such as Low-Rank Adaptation (LoRA) [29]) [30, 31, 32, 33, 34, 10, 35, 36], and prompt tuning [4, 37, 38, 39, 40, 41]. Partial tuning and extra module face several limitations that hinder their application. Unsatisfactory performance: they generally cannot reach competitive performance with regard to full finetuning [4, 26, 27, 28, 33, 10]; \u2461 Model-oriented design: most research requires to insert specific architecture/block design [31, 30, 32] during tuning, rendering them non-universal solutions when considering different backbones. In contrast, prompt tuning [2], originally proposed for language-domain [42, 43, 44, 45], provides a general and straightforward solution in vision with powerful performance gains. It signals a new paradigm in PEFT in the field of computer vision.\nGenerally, prompt tuning introduces a sets of learnable parameters to the input sequence of backbone models, updating only these parameters during the finetuning. Despite its apparent simplicity, the paradigm of visual prompt tuning has demonstrated notable performance enhancements. Current"}, {"title": "Fast Fourier Transform in Vision", "content": "FFT is a powerful mathematical algorithm used to compute the Discrete Fourier Transform (DFT) and its inverse [50, 51]. It is pivotal in information processing, allowing the detailed analysis of various signals (e.g., image [52, 53, 54], radar [55, 56, 57]) for frequency determinations. In vision, FFT's ability to transform complex data in spatial domain into frequency domain makes it an invaluable tool for abstracting critical features from noisy or high-dimensional datasets [58, 59]. This abstraction is particularly beneficial as the identification of salient features are shown to have better generalization ability across domains [60, 61, 62, 63], directly influences the performance [64, 65, 66, 67] of image analysis and processing tasks. Current research on FFT in vision predominantly explores areas such as conventional image processing [52, 68, 69, 70], image pre-processing for deep neural networks (DNNs) [71, 72] and DNN architectural design [20, 66, 65, 73, 74, 75, 76].\nDespite its profound utility and effectiveness, the integration of FFT within the paradigm of visual prompt tuning remains largely underexplored. Recent work [77] adapts the pretrained multi-modal network to the tasks under modality-incomplete segmentation scenarios via FFT prompt tuning. This approach demonstrates the potential of FFT operations to handle missing modalities (i.e., substantial disparity) effectively. However, it primarily focuses on task-specific optimization and design. The extensive applicability and generality of FFT, especially in cross-dataset analysis, have yet to be recognized or exploited. Another work [36] incorporates Fourier transform into the LoRA-based approach. While the expressive Fourier basis facilitates the recovery of weight changes, it does not fully integrate frequency domain information during finetuning, which remains orthogonal to our approach. In this paper, we aim to broaden the scope of exploration and contribute to advancing the field of Fourier-based research in vision. By studying the integration of FFT with visual prompt tuning, we fully explore how to improve both the efficacy (see \u00a73) and the adaptability of learning models to diverse and challenging datasets (see \u00a74). Furthermore, we present novel evidence indicating that VFPT establishes strong correlations within the Transformer's input space, aligning with the performance enhancements (see \u00a74.4). Overall, the generality of VFPT suggests a novel understanding of the Fourier-based method in current machine learning applications."}, {"title": "Methodology", "content": "In this section, we introduce VFPT, a novel visual prompt tuning approach for effective and general large-scale transformer-based model finetuning. We first define the problem and notations of visual prompt tuning and FFT in \u00a73.1. The integration of Fourier-based visual prompt tuning is presented in \u00a73.2. The overall framework is shown in Fig. 1(c), where we compare our model with original VPT."}, {"title": "Preliminary", "content": "Visual Prompt Tuning. Given a pretrained Transformer model T with N layers, the objective of prompt tuning in vision is to finetune a model T into a new task with only a few set of d-dimensional embedding vectors, i.e., prompts, in the input space after patch Emb layer. These learnable prompts are defined as P = {P1, P2, . . ., PN}, where Pi represents the learnable visual prompts in the ith encoder layer. Formally, the encoder layers with prompts are defined as:\n$Z^{1} = L_{1}(P^{1}, E)$\n$Z^{i} = L_{i}(P^{i}, Z^{i-1}) i=2,3,...,N$\nwhere the embeddings of the input image patches E are initialized with frozen Emb projection, and $Z^{i}$ is the contextual embeddings computed by the $i^{th}$ encoder layer. The colors trainable and frozen parameters, respectively. Here, trainable prompts only accounts for a small proportion of the total parameters (e.g., 1.14% on VTAB-1k [78] in VPT [4])."}, {"title": "Visual Fourier Prompt Tuning", "content": "Visual prompt tuning is particularly useful under the pretrain-then-finetune paradigm. However, it suffers a significant performance reduction when substantial disparities exist between pretrain and finetune datasets. The reason is that during finetuning on new data, the image distribution may deviate markedly from the examples used in pretraining the backbone model [9]. Existing prompt tuning [4, 5], focusing predominantly on spatial information, can only harness the shared information embedded within the pretrained backbone, limiting their capacity to adapt effectively to novel tasks. Thus, it is crucial to strengthen the ability to capture distinguishing feature from finetuning data.\nTo this end, we introduce VFPT, an intuitive yet powerful method with advanced performance and generality. Compared to VPT (see Fig. 1(a)), our model (see Fig. 1(c)) transforms partial prompts from spatial domain to frequency domain via 2D FFT (see \u00a73.1) to consider both the spatial and frequency domain information. Formally, for each learnable visual prompts in the $i^{th}$ encoder layer $P^{i} \\in P = {P^{1}, P^{2}, ..., P^{N} }$, we have $P^{i} = {p_{1}, p_{2}, ..., p_{M} }$. We select m partial prompts as visual Fourier prompts at each layer, where 0 < m < M. Further, a = m/M represents the fraction of Fourier participation, where zero indicates all prompts are original visual prompts, and one implies all prompts are given after FFT. We apply a 2D FFT on a visual prompt embedding input with respect to both sequence (i.e., $F_{seq}$) and hidden dimensions (i.e., $F_{h}$). Note that the operations $F_{seq}(F_{h}(x))$ and $F_{h}(F_{seq}(x))$ are mathematically equivalent due to the commutative property of the two one-dimensional FFTs [80]. Here, $\\oslash$ indicates Fourier operations.\n$P^{i} = R(F_{seq}(F_{h}([p_{1}, p_{2}, ..., p_{m}])))$.\nTo maintain the pretrained structure's consistency, we only alter the prompt embeddings, and thus retain only the real component (i.e., R) from the output. This design does not require any adjustments"}, {"title": "Methodology", "content": "of the model remain unchanged. Consequently, the overall integrated prompts $P^{i}$ in the $i^{th}$ encoder layer are formed by the concatenation between the visual Fourier prompts and visual prompts as:\n$P^{i} = [P^{i}, p_{m+1}^{i},...,p_{M}^{i}]$.\nOur elegant design of VFPT enjoys a few appealing characteristics:\n\u2022 Simplicity: VFPT only requires several lines of code based on the implementation of the visual prompt tuning. Its intuitive integration of information between spatial and frequency domains brings nearly free performance efficacy. The low complexity of FFT (i.e., O(n log n)) leads to an overall marginal reduction during the training schedule.(i.e., 2.8% on VTAB-1k [78]). In sharp contrast, current endeavors in visual prompt tuning mainly emphasize augmenting architectural complexity for superior performance [5, 81, 42], undermining the inherent simplicity of prompt tuning and introducing significant training overhead (e.g., [81] learns 2D prompt token map for densely image relationship construction, [5] incorporates additional self-attention K-V prompts).\n\u2022 Generality: The frequency and spatial analysis of imagery inputs can be mutually complemen- tary, leading to a more comprehensive feature understanding from distinct perspectives (e.g., the frequency domain allows for the distraction and decomposition of luminance and noise to a con- siderable degree [82], while the spatial domain excels in capturing intricate object details). By incorporating learnable prompts from both domains, VFPT demonstrates enhanced prompt learning capabilities, which makes it superior to finetune across diverse tasks (see \u00a74.2). The empirical findings of flatness and convexity of VFPT further strength our claim.\n\u2022 Interpretability: In visual prompt tuning, a notable challenge arises concerning the interpretability of learnable prompts. Unlike in NLP, where tokens explicitly represent these prompts, visual prompts have historically lacked a clear and explainable representation. In order to intuitively perceive the function of visual prompts, we offer a possible way to understand why prompts play an important role in fine-tuning a new task through the visualization of attention maps. Moreover, we can also observe a better and stronger global feature learning pattern through introducing visual Fourier prompts, showing how Fourier prompts work. More discussion will be elaborated in \u00a74.4."}, {"title": "Experiment", "content": "4.1 Experiment Setup\nDatasets. Following common practice [5, 4, 81, 83], our experiments are carried out on two image classification benchmarks. VTAB-1k [78] collects 19 benchmarked Visual Task Adaptation, separated into three groups: (1) Natural includes natural images captured by standard cameras, (2) Specialized consists of images taken by specialized equipment, and (3) Structured considers tasks considering geometric comprehension (i.e., counting, distance), which has substantial dataset disparities (i.e., tasks in Natural and Specialized are closely related to image classification and thus have low disparities, while tasks in Structured are regarded as distinct from image classification) when comparing to the pretrained dataset [9] (i.e., ImageNet21K [84]). Each task of VTAB-1k contains 1000 training examples with the 800/200 split for train/val set. FGVC contains 5 benchmarked Fine-Grained Visual Classification, including CUB-200-2011 [85], NABirds [86], Oxford Flowers [87], Stanford Dogs [88] and Stanford Cars [89]. The training set is split into 90% train and 10% val.\nBaselines. For consistency, we follow [4, 5] and compare VFPT with other widely applied parameter-efficient fine-tuning methods. Results of two vision transformer architectures, Vision transformer [23] (ViT) and Swin transformer [24] (Swin), on image classification are discussed in \u00a74.2. We also apply VFPT on two self-supervised objectives: MAE [90] and MoCo v3 [26].\nTraining. Following [4, 5], we conduct grid search to find the best tuning hyperparameters, learning rate (i.e., [50, 25, 10, 5, 2.5, 1, 0.5, 0.25, 0.1, 0.05]), and weight decay (i.e., [0.01, 0.001, 0.0001, 0.0]) on val set. Notably, VFPT does not require specific-designed large learning rate in [4]. The learning rate is scheduled by a cosine decay policy and trained for 100 epochs.\nReproducibility. VFPT is implemented in Pytorch [91]. Experiments are conducted on NVIDIA A100-40GB GPUs. To guarantee reproducibility, our full implementation will be publicly released."}, {"title": "Main Results", "content": "In this section, we demonstrate the effectiveness of VFPT from two key perspectives: Supe- rior Performance: Our model demonstrates significant performance improvements across diverse datasets, including challenging tasks with large disparities in data, thus showcasing its generalizability."}]}