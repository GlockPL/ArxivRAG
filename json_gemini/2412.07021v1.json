{"title": "Sequential Compression Layers for Efficient Federated Learning in Foundational Models", "authors": ["Navyansh Mahla", "Sunny Gupta", "Amit Sethi"], "abstract": "Federated Learning (FL) has gained popularity for fine-tuning large language models (LLMs) across multiple nodes, each with its own private data. While LoRA has been widely adopted for parameter-efficient federated fine-tuning, recent theoretical and empirical studies highlight its suboptimal performance in the federated learning context. In response, we propose a novel, simple, and more effective parameter-efficient fine-tuning method that does not rely on LoRA. Our approach introduces a small multi-layer perceptron (MLP) layer between two existing MLP layers-the up_proj (the FFN projection layer following the self-attention module) and down_proj-within the feed-forward network of the transformer block. This solution addresses the bottlenecks associated with LoRA in federated fine-tuning and outperforms recent LoRA-based approaches, demonstrating superior performance for both language models and vision encoders.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are central to Natural Language Processing (NLP), and fine-tuning them on task-specific datasets enhances their performance on downstream tasks. However, real-world data is often distributed and cannot be shared due to privacy concerns. Federated Learning Li et al. [2020], McMahan et al. [2017], Zhang et al. [2021] addresses this by enabling decentralized model training across institutions without exposing sensitive data. Fine-tuning foundational models like LLMs and ViTs Dosovitskiy et al. [2021] is computationally intensive, often requiring substantial resources. Parameter-efficient fine-tuning (PEFT) methods Houlsby et al. [2019] like LoRA Hu et al. [2021] help mitigate this challenge. Recently, new methods like FedFTG Mahla and Ramakrishnan [2024] have shed light on the sub-optimal nature of LoRA adapters in federated fine-tuning owing to constrained sub-spaces. Following up from this, we present a simple yet novel method of federated fine-tuning of foundational models as our main contribution of this paper. We add a small (or compressed) MLP layer between the first and second MLP layer just after the self-attention module. This new framework outperforms recent SoTA LORA based methods like FFA-LORA [Sun et al., 2024] and FedSA-LORA Guo et al. [2024] on both image and text modalities."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Federated Learning", "content": "Federated Learning (FL) is a widely used distributed learning approach for privacy-sensitive tasks, but it faces significant challenges when working with non-IID datasets, often resulting in accuracy gaps compared to centralized training. Recent research has shown that fine-tuning Pre-trained Language Models (PLMs) in FL can help mitigate these challenges [Nguyen et al., 2022], with vanilla FedAvg achieving performance on par with centralized methods. Foundational models like Large Language Models (LLMs) and Vision Transformers (ViTs), which need to be trained in privacy-preserving environments, are particularly well-suited for FL. However, fine-tuning these models remains difficult due to their high computational demands, driven by their large number of parameters. To address these resource limitations, Parameter-Efficient Fine-Tuning (PEFT) methods such as adapter-tuning, prompt-tuning, and LoRA have been introduced. Among these, LoRA-based techniques have gained widespread attention for their efficiency, effectiveness, and flexibility, which is the central focus of our work."}, {"title": "2.2 LoRA in Federated Learning", "content": "Low-Rank Adaptation (LoRA) [Hu et al., 2021] utilizes low-rank matrices to approximate gradient updates, enabling the adaptation of models without altering pre-trained weights. This method has gained popularity due to its efficiency, effectiveness, and adaptability. By significantly reducing the size of model updates, LoRA helps address communication bottlenecks in Federated Learning (FL). For instance, Yi et al. [2023] introduced FedLoRA, which integrates LoRA into FL to improve fine-tuning efficiency. Similarly, Yang et al. [2024] proposed FedDPA, a dual-adapter approach, and Qi et al. [2024] presented FDLORA, both leveraging personalized and global LoRA modules to capture local and shared knowledge. Federated Freeze-A LORA (FFA-LORA) Sun et al. [2024] freezes the randomly initialized A matrices and only fine-tuning the zero-initialized B matrices, further halving communication costs and showing promising performance improvements compared to the other LoRA baselines. On the other hand, FedSA-LORA Guo et al. [2024] which fine-tunes both matrices of low-rank adapter, but only aggregates the randomly initialized matrix A."}, {"title": "3 Methodology", "content": "layer is inspired by observations from Tian et al. [2024], which suggest that explicit fine-tuning of the self-attention module is unnecessary, as its information is already captured by the MLP layer directly following it. Consequently, we apply the compression layer right after the first MLP layer in the transformer block. By doing so, we manipulate the information passing through the attention module, projecting it into a smaller subspace and learning representations within this reduced dimension similar to what has been previously discussed in Mahla and Ramakrishnan [2024]. During training, only the parameters of the compression layer and the new projection between compression layer and MLP layer 2 are fine-tuned.\nAssuming linear activation between the projection layers, the We is randomly Gaussian initialized. For any output value x of the MLP-Layer 1 Projection, the following relation can be written:\n$W_2W_c = W$ (1)\n$W_2 = WW_c^+$ (1)\nWhere $W_c^+$ is the pseudoinverse (Moore-Penrose inverse) of $W_c$. Equation (1) represents how the weights of the modified MLP-2 Layer are initialized.\nLemma 1:For a bounded gradient (L2 norm of the gradients upper bounded by D) L2 norm of the weight matrix in the sequential compression layer based FedAvg aggregation framework is upper bounded linearly by the number of global aggregation steps S and the number of local training steps between two consecutive aggregation steps $t_{agg}$:\n$||W_{agg}|| \\leq B + \\eta St_{agg}D = O(St_{agg})$ (2)\nwhere $\\eta$ is the learning rate and B is a constant.\nProof: Weight update for a client i at some time $t_{agg}$ with FedAvg aggregation and S communication rounds being occurred, can be written as:\n$W_{agg} = W_0 - \\frac{\\eta}{N} \\sum_{i=1}^N \\sum_{j=0}^{S} \\sum_{t=0}^{t_{agg}} G_{t,j}^{(i)}$ (3)\nHere, G is the gradient. Taking L2 norm on both the sides:\n$||W_{agg}|| < ||W_0|| + \\frac{\\eta}{N} \\sum_{i=1}^N \\sum_{j=0}^{S} \\sum_{t=0}^{t_{agg}} D$ (4)\n$||W_{agg} || \\leq B + \\eta St_{agg} D$ (4)\nTheorem 1: For a convex loss L, let $\\Delta W^* \\in \\mathbb{R}^{d \\times k}$ (r < min(d,k)) be the optimal LoRA parameter matrix, $\\alpha$ be the learning rate, and let the L2 norm of the gradient to be bounded (i.e. $|| \\nabla_{W_C^{(i)}}(\\Delta W) ||_2 \\leq D$). Then excess risk (L($\\Delta W_{agg}$) - L($\\Delta W^*$)\\) bounds for our method, involving N clients and S global aggregation steps having occured every $t_{agg}$ local training iterations, can be expressed as follows:\n$|L(\\Delta W_{agg}) \u2013 L(\\Delta W^*)| \\leq \\alpha D^2St_{agg} + c = O(St_{agg})$ (5)\nProof:\n$L(\\Delta W) \u2013 L(\\Delta W^*) \\leq \\nabla_w L(\\Delta W)^T(\\Delta W \u2013 \\Delta W^*)$ (6)\nFor the excess risk just after the aggregation, we can replace the weight parameters with the average of it.\n$L(\\Delta W_{agg}) \u2013 L(\\Delta W^*) \\leq \\nabla_w L(\\Delta W_{agg})^T (\\Delta W_{agg} - \\Delta W^*)$ (6)\nTaking L2 norm on both the sides and using the bounds of $W_{agg}$ from Lemma 1:\n$|L(\\Delta W_{agg}) \u2013 L(\\Delta W^*)| \\leq D(\\alpha t_{agg}SD + k) = O(St_{agg})$ (7)\nTheorem 1 shows that the excess risk of our method has linear upper bounds, while LoRA methods such as FFA-LORA have quadratic upper bound as shown in Mahla and Ramakrishnan [2024]. In addition to this, excess risk upper bound does not depend on the number of clients N."}, {"title": "4 Experiments", "content": "In this section, we evaluate the performance of our method across text and image modalities, focusing on its performance with LLMs and ViTs. For text, experiments involve 3 and 4 clients, while for vision, 3, 4, and 5 clients are tested. Each client holds a unique non-IID shard of a dataset."}, {"title": "4.1 Datasets", "content": "We conduct experiments on both text and image datasets. For text, we use the MedQuAD dataset Ben Abacha and Demner-Fushman [2019], which includes 47,457 medical question-answer pairs from 12 NIH websites, covering 39 question types. Due to MedlinePlus copyright restrictions, answers from 3 subsets were removed. We also use the Databricks Dolly-15k dataset Conover et al. [2023], which contains 15,000 high-quality human-generated prompt-response pairs for instruction tuning LLMs, with categories like brainstorming, classification, summarization, and question answering. For the vision modality, we use the Brain Tumor classification dataset Cheng [2017], comprising 3,064 T1-weighted contrast-enhanced MRI images from 233 patients, categorized into meningioma, glioma, and pituitary tumor types."}, {"title": "4.2 Non-IID Data Preparation", "content": "To simulate non-IID conditions, we used Dirichlet Allocation to partition each dataset into non-IID splits, following Zhang et al. [2024a], Sun et al. [2024]. For text datasets (MedQuAD and Dolly-15k), we created 4 splits, and for the Brain Tumor dataset, 5 splits. The concentration parameter \u03b1 was set to 0.1, resulting in highly skewed distributions across clients. Splits were based on labels: question_type for MedQuAD and category for Dolly-15k. Class label distributions for MedQuAD and Dolly-15k are shown in Figure 2 and 3 respectively."}, {"title": "4.3 Models and Hyperparameters", "content": "For experiments on text datasets (MedQuAD and Dolly-15k), we use Gemma-2B Gemma Team et al. [2024] and Tinyllama-1.1B Zhang et al. [2024b]. For our approach, the compression-layer projection weights and the new MLP-layer 2 weights were fine-tuned. In contrast, FedSA-LORA and FFA-LORA fine-tuned the attention modules (query, key, value) with a LoRA rank of 8 and a scaling factor of 16. For experiments on vision modality, we fine-tune SigLIP Zhai et al. [2023]. For our approach, we fine-tuned the similar parameters as in language models along with the classifier layer. For FedSA-LORA and FFA-LORA, the attention parameters (query, key, value) were fine-tuned with a LoRA rank of 8 and a scaling factor of 32."}, {"title": "4.4 Experiment Results", "content": "show results on text and image modality respectively. We train the models for 10 epochs with early stopping owing to overfitting. We report the ROUGE_L (ROUGE longest common subsequence) score and BLUE-4 (4-gram) score for text modalities while F1 score for the experiments on image modality."}, {"title": "5 Conclusion", "content": "This paper introduces a compact sequential compression layer for federated fine-tuning of foundational models, addressing limitations of LoRA-based methods like constrained subspace learning and rising excess risk. The approach projects representations into an expressive subspace, achieving superior performance with minimal parameters. Experiments on text and vision datasets demonstrate consistent improvements over state-of-the-art LoRA methods across non-IID client setups. Theoretical analysis confirms linear bounds on weight updates and excess risk, ensuring scalability and"}]}