{"title": "STEMS: SPATIAL-TEMPORAL MAPPING TOOL FOR SPIKING NEURAL NETWORKS", "authors": ["Sherif Eissa", "Sander Stuijk", "Floran de Putter", "Andrea Nardi-Dei", "Federico Corradi", "Henk Corporaal"], "abstract": "Spiking Neural Networks (SNNs) are promising bio-inspired third-generation neural networks. Recent research has trained deep SNN models with accuracy on par with Artificial Neural Networks (ANNs). Although the event-driven and sparse nature of SNNs show potential for more energy efficient computation than ANNs, SNN neurons have internal states which evolve over time. Keeping track of SNN states can significantly increase data movement and storage requirements, potentially losing its advantages with respect to ANNs. This paper investigates the energy effects of having neuron states, and how it is influenced by the chosen mapping to realistic hardware architectures with advanced memory hierarchies.\nTherefore, we develop STEMS, a mapping design space exploration tool for SNNs. STEMS models SNN's stateful behavior and explores intralayer and interlayer mapping optimizations to minimize data movement, considering both spatial and temporal SNN dimensions. Using STEMS, we show up to 12x reduction in off-chip data movement and 5x reduction in energy (on top of intra-layer optimizations), on two event-based vision SNN benchmarks. Finally, neuron states may not be needed for all SNN layers. By optimizing neuron states for one of our benchmarks, we show 20x reduction in neuron states and 1.4x better performance without accuracy loss.", "sections": [{"title": "1 Introduction", "content": "Spiking Neural Networks (SNNs) are biologically inspired neural networks that have gained increasing popularity as a solution for efficient edge AI, due to their sparse spike-based computation. SNNs use state-full neuron models that evolve over time akin to recurrent neural networks, producing sparse spikes over time [1]. Direct SNN training has seen recent success, with accuracy on par with other deep learning solutions [2, 3]. Due to their state-full event-driven nature, SNNs are promising candidates for (spatial-)temporal problems such as event-based vision using event-based cameras (i.e Dynamic Vision Sensors (DVS)), which has recently gained popularity as an efficient solution for embedded computer vision [4].\nWhile SNNs might have better compute efficiency compared to Artificial Neural Networks (ANNs), the performance of deep models is usually dominated by off-chip data movement due to the large amount of memory (weights and neuron states) and features (spikes) that exceed on-chip capacity. Hence, limiting off-chip data movement can be crucial for SNN energy efficiency on memory-constrained devices. Additionally, recent digital neuromorphic architectures consist"}, {"title": "2 Related work", "content": "Earlier work on SNN mapping assumed fully distributed multicore architectures with no shared memory, where the focus was on partitioning and mapping neurons on a homogeneous multicore architecture, which is a well-known NP-hard problem [17]. Except for mapping tools that target specific hardware platforms [18], the optimization goal is to maximize resource utilization while minimizing inter-core communication. This problem has been solved with many different heuristics, such as particle swarm optimization [19, 20], graph partitioning [21, 22], and integer linear programming [23]. Some also consider hardware characteristics such as reliability and endurance [24]. All these methods do not consider off-chip memories, so they can not handle situations when an SNN does not fit on-chip. They also do not consider shared on-chip memory resources or loop nest optimizations.\nNeuProMa [25] is an SNN mapping framework that considers off-chip data movement, where an SNN can not fit onto on-chip and resources have to be time-multiplexed. Their framework maps SNNs in three steps: split, partition, and map. First, the SNN is divided into several subnetworks that fit on-chip resources. SNNs are split according to one of three splitting strategies: channel split, pixel split, or link split. The link split, which creates depth-first subnets similar to layer fusion, performed overall better than the other two on different benchmarks. Then, each subnetwork is mapped using heuristics to reduce inter-core spike communication. This work considers limited splitting strategies and does not consider shared on-chip memories and loop nest optimizations.\nSMART is a design flow for mapping SNNs in resource-constrained heterogeneous neuromorphic systems coupled with a CPU [23]. This work breaks down the mapping problem into four smaller sub-problems; throughput lower bound, operation mapping, activation/weight mapping, and parallel scheduling. The lower throughput bound is estimated using a self-timed execution schedule. Operation and activation/weight mapping are done using integer linear programming. For operation mapping, latency and memory requirements are combined in a cost function to determine whether operations (i.e. layers) are to be mapped on neuromorphic cores or a CPU. For activation/weight mapping, scratchpad memories are split into a pinned space and an immediate space. Activation/weights mapped to pinned space are never ejected to main memory, allowing better data reuse, while those mapped to immediate space can be ejected to free up space whenever necessary, with the objective of minimizing data movement between main memory and distributed memories. Finally, a parallel schedule is formed, with task- and batch-level parallelism possible. While this work optimizes data movement, it does not consider intra-layer nor inter-layer optimizations."}, {"title": "3 Background", "content": "This section introduces the SNN algorithm commonly used in vision applications and the general behavior of SNNs, as well as inter-layer mapping (i.e., layer fusion)."}, {"title": "3.1 Spiking Neural Network Algorithm", "content": "Recent advances have enabled direct training of large SNNs for complex event-based vision applications such as object detection and recognition [2, 3]. SNNs have inter-layer connections (i.e. projections) similar to ANNs such as (strided) spatial convolution layers, batch normalization, residual connections, and spatial pooling. However, there are two key differences between SNNs and ANNs. SNNs operate using sparse spike-based features and have self-recurrent activation functions inspired by biological neurons, whereas ANNs operate on real-valued features and have nonrecurrent (i.e., memoryless) activation functions. SNN neuron activation function is self-recurrent because it depends on the neuron's internal state [1]. Additionally, SNN inference happens over time, while ANN inference is not necessarily over time.\nSNNs commonly implement the leaky integrate-and-fire (LIF) neuron model, especially for vision applications. This is due to its simplicity in implementation in analog and digital systems, as well as its ability to replicate basic features of biological neurons such as leakage, integration of spikes, and all-or-none action potential generation [28]. The LIF neuron model has one recurrent state, its membrane potential (Vmem). Figure 1 illustrates the dynamics of an LIF neuron. The input current is integrated into the neuron's membrane potential, and the neuron fires and resets when it exceeds its firing threshold, whereas the membrane potential leaks over time.\nSNN neurons are simulated digitally using a discrete timestep model, with inferences taking a specific number of timesteps according to the SNN model and initializing neuron states at the beginning of inference. The LIF discrete timestep model can be simplified to\n$\\begin{aligned}\nI[t] &= \\sum WSin[t]  \\label{eq:1}\\\\\nV_{mem}[t] &= aV_{mem}[t \u2013 1] + I[t] \u2013 S_{out}[t - 1]I_{reset} \\label{eq:2}\\\\\nS_{out}[t] &= V_{mem}[t] \\geq V_{thr} \\label{eq:3}\n\\end{aligned}$\nwhere t is the discrete timestep, Vmem is the membrane potential, a is the membrane leakage, Sin and Sout are the input and output spikes, respectively, I is the input current, and Vthr and Ireset are the firing threshold and reset current, respectively. An SNN is computed by traversing these equations across space (i.e., neurons, layers) and time."}, {"title": "3.2 Layer Fusion in ANNS", "content": "ANN workloads consist of cascaded blocks, where each block is a directed acyclic graph of layer operators such as spatial convolution, addition (i.e. residual connection), and spatial reductions. Intra-layer loop nest optimizations can improve data reuse and reduce data movement within an operator [8]. However, off-loading and reloading features between operators may be detrimental to performance, which is dominated by off-chip data movement. Hence, applying inter-layer optimizations by considering all operators simultaneously can provide better mapping solutions.\nThe typical ANN inter-layer mapping is layer-by-layer (LBL), where one layer is completely scheduled before successive layers can start. In LBL schedules, complete inter-layer features are cached on-chip, if possible, or off-chip. On the other hand, layer-fused (LF) schedules, where layers are partitioned into nodes and interleaved across spatial dimensions (typically line-by-line) can significantly improve performance by reducing the size of cached intermediate features [10, 11, 12, 13]. This comes at the cost of reducing intra-layer weight reuse across space (spatial memory reuse) and increasing the variable lifetime of the weights, which increases the storage of cached weights. Figure 3 illustrates LBL and LF schedules on a simple 3-layer ANN block with 1D convolution kernels.\nLayer fusion is complementary to input batching. In input batching, multiple input frames are consumed simultaneously, resulting in larger intermediate features, but better reuse of weights (memory). In vision classification models, early blocks have shallow channel dimensions and large spatial dimensions. The channel dimensions grow deeper in deeper layers and the spatial dimensions shrink due to spatial pooling. Hence, earlier blocks tend to have more features, while later blocks tend to have more memory (weights). Hence, earlier blocks typically prefer layer fusion to reduce intermediate feature sizes, while later blocks typically prefer LBL schedules and input-batching to maximize memory (weights) reuse."}, {"title": "4 Methods", "content": "We present an overview on STEMS and its stages, as well as the inter-layer mapping space explored with STEMS. We also motivate and explain our neuron state reduction exploration for spatial (temporal) tasks such as event-based vision."}, {"title": "4.1 STEMS overview", "content": "STEMS is a mapping design space exploration tool for SNNs based on Stream, a state-of-the-art tool for multi-core ANN mapping [10]. STEMS explores both intra-layer mapping with loop nest optimizations, and inter-layer mapping with workload partitioning and scheduling along spatial (i.e., layer fusion) and temporal dimensions (i.e. time batching). Compared to Stream, STEMS supports temporal and stateful workloads such as SNNs and enables their inter-layer mapping exploration along spatial and temporal dimensions.\nFigure 4 shows an overview of STEMS. STEMS breaks down the mapping problem into pipeline stages similar to Stream. STEMS has three user-defined inputs: the workload and accelerator descriptions, and user-defined workload cuts along spatial and temporal dimensions. STEMS pipeline can be described in the following four stages: 1. user input is parsed, and the workload and accelerator models are generated; 2. fine-grain workload graph is generated according to user-defined workload cuts; 3. intra-layer mapping is generated by optimizing each unique tile-core mapping; 4. inter-layer schedule is generated by scheduling the fine-grained workload graph in a specific order while managing hardware resources. For multi-core accelerators, a genetic algorithm can be used in this stage to explore layer-to-core assignments."}, {"title": "4.2 STEMS components", "content": ""}, {"title": "4.2.1 Input parsing", "content": "In this stage, the workload and accelerator models are generated from user input.\nThe accelerator model consists of an external memory and one or more computation cores connected in a NoC. The NoC has a mesh structure and is defined by the data bandwidth and data movement costs between cores and between any core and external memory. Only one core can request data from the external memory at a time. Each core is described by a memory hierarchy that can store operand tiles at different memory levels, and an array of parallel processing elements. The memory blocks in the hierarchy are defined by their size, bandwidth (bits per cycle), and energy (per bit). While the processing elements are defined by their energy per operation, size, and unrolled dimensions."}, {"title": "4.2.2 Fine-grain workload graph generation", "content": "In this stage, a fine-grained workload graph is generated out of the workload graph and user-defined workload cuts. Such cuts define how the operators in each block are partitioned into smaller computation tiles (CT) that are used in intra- and inter- layer mapping stages. Such partitioning also defines how these computation tiles will be scheduled in the inter-layer mapping stage.\nTo support SNNs and temporal workloads, we introduced the temporal dimension to user-defined cuts and the graph generation process. Each operator can be partitioned along the spatial dimension, the temporal dimension, or both. The fine-grain graph is generated with proper inter-layer edges, as well as intra-layer edges. Intra-layer edges ensure a proper temporal schedule and an efficient spatial schedule. The IDs of the computation tiles include spatial and temporal dimensions."}, {"title": "4.2.3 Intra-layer mapping", "content": "In this stage, energy and latency are estimated for each unique CT-core combination using Zigzag [8]. If a CT does not fit on a core, the external memory is added to the core during cost estimation. Zigzag explores intra-layer mapping by breaking down the mapping space to loop prime factors. These factors are permuted and partitioned into different memory levels for each operand. For each valid mapping (i.e. tiles fit into their allocated memories), latency and energy are estimated. The latency and energy models translate the temporal mapping into data movement rates and volumes and allocated memory sizes. The latency model can identify memory-bound and compute-bound schedules and estimate latency accordingly. Memory-bound schedules occur because of data movement rates that are larger than the available memory bandwidth. The energy model is based on the volume of data movement between different levels of memory and the number of operations. We configure Zigzag to choose the best cost estimates based solely on energy.\nTo support SNNs, neuron state behavior and time are incorporated into Zigzag. Neuron states are added as explicit operands and their behavior is modeled into the memory allocation and data movement models. As a result, their costs are reflected in the latency and energy models. On the other hand, time is added as an explicit dimension of workloads (e.g., SNN layers), which increases the size of the temporal mapping space. The neuron states are reused over different timesteps; however, there is a chronological dependency between neuron updates over time, due to the non-linear activation at the end of each timestep. In other words, evaluating a neuron (and updating its state) in one timestep requires evaluating it first for previous timesteps. This scheduling constraint limits and reduces the temporal mapping space, where input-relevant loops (e.g. input channels, kernel projection) need to be iterated before temporal loops can be iterated (and possibly batched together). Between successive temporal iterations, all iterated neurons (i.e., iterated output-relevant loops such as output channels and spatial dimensions) are activated, and their output spikes are generated."}, {"title": "4.2.4 Inter-layer mapping", "content": "In this stage, computation tiles are scheduled according to layer-to-code allocations. For multi-core accelerators, different layer-to-core allocations are explored using a genetic algorithm which optimizes latency and energy. Latency and energy costs of one allocation are generated using an inter-core graph scheduler.\nThe inter-layer scheduling order is defined according to the workload partitioning. If a layer is spatially partitioned, it is said to be in a layer-fused schedule. Additionally, a new scheduling dimension is introduced for SNNs and other recurrent networks; time (sequence). In contrast to layer fusion (Section 3.2), batching multiple timesteps enlarges intermediate features. However, it increases memory reuse (neuron state and weights) across time (temporal memory reuse). Such effects are similar to input batching. In addition, neuron states are typically reset to zero at the beginning of an inference. Hence, batching all timesteps maximizes reuse of memory over time and may completely eliminate the off-chip data movement of neuron states under memory constraints. We call this a time-batched schedule (TB), in contrast to single-timestep schedule (ST) where only one timestep is scheduled at a time. Between TB and ST, we can batch just a few timesteps (e.g., 2T, 4T, 8T).\nTemporal techniques can be applied orthogonal to spatial techniques (LBL and LF), resulting in different spatio-temporal schedules. In Figure 5, we illustrate four combinations of spatio-temporal schedules which we use in our inter-layer schedule exploration: single-timestep layer-by-layer (ST-LBL), time-batched layer-by-layer (TB-LBL), single-timestep layer-fused (ST-LF), and time-batched layer-fused (TB-LF). Each schedule provides a different trade-off between spatial memory reuse, temporal memory reuse, and intermediate feature sizes. Batching T timesteps increases size of intermediate features and temporal memory reuse by T, while layer fusion decreases size of intermediate features and spatial memory reuse, resulting in the different schedule properties highlighted in Figure 5. Workload blocks with large feature sizes tend to profit from layer fusion, as it reduces the size of intermediate features, while workload blocks with large memory sizes (weights and/or neuron states) tend to profit from time batching, as it increases memory reuse across time. These four types of schedules can be applied separately to different blocks in an SNN, catering to different block characteristics (memory vs. features). In addition to these schedule, we can have different degrees of time batching resulting in different degrees of temporal reuse and intermediate feature sizes. The inter-layer schedule is generated according to user-defined cuts, as in Figure 5."}, {"title": "4.3 Hybrid Schedule Exploration", "content": "Given a workload with N blocks, we would like to explore different possible schedules per block. In this exploration, we consider the spatio-temporal schedules in Figure 5. An exhaustive search of this space results in 4N possible schedules, which is not feasible for larger network sizes. Instead, we apply our knowledge regarding the trade-offs of these schedules. We rank blocks according to their feature sizes and order, and according to their memory size and order. We apply layer fusion and/or time batching to blocks in this ranking order. This results in N + 1 possible ways (from 0 blocks to N blocks) to apply fusion or time batching, resulting in a total of (N + 1)\u00b2 possible schedules."}, {"title": "4.4 Neuron State Optimization", "content": "Neuron states enable learning patterns over time. They provide the network with the sequential memory necessary to learn (spatio-)temporal tasks. Deep learning models learn spatial features hierarchically, creating high-level semantic features in deeper layers [29]. Recent research in training networks on spatiotemporal tasks has shown that temporal learning in earlier blocks provides little to no gain and comes at a significant memory cost [14, 15, 30, 16]. The best strategy is often to apply spatial feature extraction in earlier blocks and only learn sequences from higher-level features in later blocks. This reduces model complexity and memory footprint and avoids learning dynamics of low-level features which are usually unnecessary and less stable\nHence, we propose a neuron state optimization targeting spatio-temporal problems such as event-based computer vision. We propose to remove neuron state memory (i.e. forget them between timesteps) from early SNN blocks. This optimization significantly reduces the number of neuron states. For example, removing neuron states from the first 11 blocks out of the 50 blocks of a SEW-ResNet-152 [2] network reduces its neuron states by approximately 40%. We remove neuron states while preserving the spike-based activation and surrogate gradient to have minimal effect on the computational model and make a fair comparison. In STEMS, we model such layers as normal ANN layers with spike-based features."}, {"title": "5 Experimental Setup", "content": "We define our hardware model and the two event-based vision benchmarks that we use in our experiments. In these experiments, we use STEMS and our hybrid schedule exploration as defined in Section 4."}, {"title": "5.1 Hardware Architecture", "content": "For our hardware model, we opt for an output stationary architecture, as it seems more natural to maximize the stationarity of neuron states inside the stationary accumulators, similar to other neuromorphic architectures [5, 6]. We adapt and modify the Meta VR prototype architecture [31] as our hardware model, shown in Figure 6. The PE array is a 16x32 output-stationary systolic array. The accelerator contains two local buffers for inputs and weights and the PE array parallelizes 32 output channels and 16 output spatial positions. Additionally, a global SRAM is integrated on-chip, which we vary in size during our experiments. We ignore the non-linear activation function (e.g. LIF, ReLU) as it is performed much less often than input integration[8].\nEach PE has an accumulator, which holds either the partial output sum of an ANN layer or the membrane potential of an SNN layer. It accumulates two inputs in parallel, which can be spiking synaptic operation (SOP) or valued multiply-and-accumulate (MAC). We assume 4-bit weight and 4-b inputs (for non-spiking features) and adjust the local buffer sizes accordingly, as 4-bit bit-width is sufficient for accurate inference [32, 33]. While the PEs use 16-bit accumulators to store the membrane potential or partial output sum, quantization research shows that 12 bits are sufficient for storing neurons in the global SRAM [33].\nWe estimate our hardware model in 22 nm FDX technology. By synthesizing and measuring the PE unit and the SRAM buffers, we estimate the model's energy parameters. DRAM energy is based on reported values in literature"}, {"title": "5.2 Benchmarks", "content": "For our experiments, we use two SOTA SNN models for two event-based vision benchmarks. We use the Spike-Element-Wise (SEW-) ResNet-18[2] model for the CIFAR10-DVS[38] dataset, and the hybrid RED-LIF[30] model for the Gen4 dataset[14]. The SEW-ResNet CIFAR10-DVS model is fully LIF-based and is easy to train. Hence, it can be easily used for neuron state optimization, which requires multiple training sessions. Both models are trained using surrogate gradients and back-propagation through time[1]."}, {"title": "5.2.1 CIFAR10-DVS SEW-ResNet-18", "content": "CIFAR10-DVS dataset [38] is a recording of CIFAR10 images using a DVS camera. The 128x128 DVS input is pre-processed into 16 timesteps with 2 input channels as in [2]. SEW-ResNet is a recent architecture for deep residual learning in SNN [2], which applies a spike-element-wise (SEW) residual connection. The SEW-ResNet-18 model consists of 7 SEW blocks. Each SEW block consists of convolutional LIF layers, a SEW residual connection, and a 2x2 spike max-pooling. We use a novel SEW OR function (g) instead of the SEW ADD function used in [2], to preserve the low precision of the spikes without losing accuracy compared to SEW ADD[2].\nMost of the SEW model's memory, features, and computations are in the first two blocks, with 80% of neuron states and features in the first block, and 15% in the second block."}, {"title": "5.2.2 Gen4 Recurrent Event-camera Detector (RED-LIF)", "content": "The Gen4 dataset is by far the largest event-based vision dataset currently available. It features hours of street recordings with millions of bounding boxes for cars, pedestrians, and bikers, recorded with a 1 Megapixel DVS camera [14].\nThe original RED architecture was trained on the earlier Gen1 dataset. It is a hybrid model which consists of two parts. First, it consists of feed-forward convolutional and squeeze-and-excitation layers to extract spatial features. Then, the high-level spatial features are passed through convolutional LSTM layers to learn spatial-temporal features. LSTM layers are necessary for learning features over time. The RED model achieves a mean average precision (mAP) of 0.44 [14]. Inspired by RED, RED-LIF was developed with 3 feed-forward residual blocks followed by 5 convolution LIF layers. It achieves a mean average precision (mAP) of 0.29 on the Gen4 dataset[30]. Similarly to RED, the 640x360 DVS input stream is pre-processed into 6 channels and each training sample consists of 12 timesteps[14].\nThe three feed-forward blocks contain most of the features and most of the computation, with 8.3 GMACs per timestep compared to 870 MSOPs per timestep in the LIF layers (ignoring sparsity). Most of the model's memory is in the LIF layers, with 2.6M weights and 1.3M neuron states compared to 0.4M weights in the feed-forward blocks. While the LIF layers have high sparsity (average 93%), the feed-forward analogue blocks have an average sparsity rate of only 50%."}, {"title": "5.3 Validation and scalability", "content": "To verify STEMS, we created a cycle-accurate simulation of some of STEMS's generated schedules on our target hardware. The resulting data movement from our simulation matched the cost estimates generated from STEMS. To demonstrate STEM's scalability, we also performed our experiments on the SEW-ResNet-152 model. Results are reported in the appendix."}, {"title": "6 Experiments", "content": "We perform three experiments. First, we highlight the effects of time batching on the reuse of neuron states. For that, we perform a time batching analysis on the SEW-ResNet-18 model, as it is a fully LIF-based model. Then, we explore our memory optimization from Subsection 4.4 on the SEW-ResNet-18 model. Finally, we perform our hybrid schedule exploration on the RED-LIF model, the SEW-ResNet-18 model, and the optimized SEW-ResNet-18 model."}, {"title": "6.1 Time batching analysis", "content": "In this experiment, we gradually apply time batching to the SEW-ResNet-18 model. We use a 1 MB global buffer, which is insufficient for storing all neuron states. Out of 16 timesteps (per sample), we batch 1T (ST), 2T, 4T, 8T, and 16T (TB).\nAs Figure 7 shows, the more timesteps we batch, the less neuron states traffic to DRAM. When time is batched completely, neuron states never leak to DRAM, but are rather generated and discarded on-chip during an inference (16 timesteps). However, intermediate feature sizes grow larger the more we batch time. The spikes start to leak to DRAM after batching 4 timesteps, and it gets worse the more timesteps you batch. Applying a layer fusion schedule in"}, {"title": "6.2 SEW-ResNet-18 neuron state optimization", "content": "We perform an ablation study on the neuron states of the SEW-ResNet-18 model. Following the approach of Section 4.4, we remove the hidden states from earlier blocks while preserving the same spike-based functionality. The results of our study are shown in Table 2. We define each model by the number of blocks with neuron states (SEW-X).\nBy removing the neuron state from the first two blocks (SEW-5), we reduce the memory of the neuron state of the model by 95% and slightly improve its test accuracy. Note that SEW-5 has similar structural properties as ANNs and RED-LIF where most features and computation are in earlier blocks (blocks 0 and 1), and most memory is in later blocks. While in the original model (SEW-7), most features, memory, and computation are in block 0 and 1. Assuming 12-bit states and 4-bit weights, the neuron states in SEW-7 require 5.5 MB of memory, while in SEW-5 require only 0.3 MB of memory, and weights require 0.5 MB. SEW-7 and SEW-5 models have the same average sparsity rates of 93%. Except for the input layer, both models perform SOP operations."}, {"title": "6.3 Hybrid schedule exploration", "content": "We perform hybrid schedule exploration on the RED-LIF model, and the original (SEW-7) and optimized (SEW-5) SEW-ResNet-18 models, under different on-chip memory constraints. We choose stringent memory sizes (512 KB for RED-LIF and 128 KB for SEW-ResNet-18) to highlight the benefits of inter-layer optimizations in reducing memory requirements for efficient inference. We report here a few of our results and report the rest of the results in the appendix."}, {"title": "6.3.1 RED-LIF", "content": "We present the results of hybrid schedule exploration for RED-LIF with a 512 KB on-chip global buffer. We explore time batching starting from the output block and layer fusion starting from the input block, since the feed-forward part contains most of the model's features, while the LIF layers contain most of the model's memory.\nFigure 8 shows the DRAM energy consumed per inference (12 timesteps) for all 81 possible schedules, where the horizontal axis represents the number of blocks that are time-batched (in this case, from the output side) and the vertical axis represents the number of blocks that are layer-fused (always from the input side). The best schedule deploys a single-timestep layer-fused (ST-LF) schedule for the feed-forward blocks and a time-batched layer-by-layer (TB-LBL) schedule for the LIF layers. Such results agree with the claim that blocks with more features favor schedules that minimize intermediate features, while blocks with more memory favor schedules that maximize memory re-use. The optimal hybrid schedule is illustrated in Figure 10.\nFigure 9 shows a complete breakdown of the energy consumed by the baseline schedule (ST-LBL) and the optimal hybrid schedule (ST-LF|TB-LBL). Inter-layer optimizations reduced off-chip data movement by 7.3x and energy consumption by 1.9x, compared to the baseline schedule."}, {"title": "6.3.2 SEW-ResNet-18", "content": "We present the results of the hybrid schedule exploration for the SEW-ResNet-18 with a 128 KB on-chip global buffer. For the original model (SEW-7), we explore both time batching and layer fusion starting from the input block, as earlier blocks have large amounts of both features and memory due to their large LIF layers. For the optimized model (SEW-5), we explore time batching starting from the output block, and layer fusion starting from the input block, as it does not"}, {"title": "7 Discussion", "content": "This study has led to some interesting insights on SNNs."}, {"title": "Proper scheduling can mitigate the effects of neuron state", "content": "In our experiments, we have shown how spatial-temporal inter-layer mapping optimizations can avoid a lot of DRAM traffic. Under stringent memory capacity, a single-timestep layer-by-layer schedule causes an off-chip traffic of neuron state that is detrimental to energy efficiency (Figure 7). Inter-layer scheduling optimizations drastically reduce on-chip memory requirements for efficient scheduling."}, {"title": "Event-based vision SNN models do not need most neurons", "content": "We have shown how most features and neuron states of a computer vision SNN model are in its earlier blocks. The neuron states in such blocks are expensive and have low accuracy returns. Hybrid models consisting of an ANN followed by an SNN are more suitable for event-based vision than fully recurrent models. This is due to the following reasons:"}, {"title": "Memory benefits", "content": "Not having hidden states in earlier blocks leads to significantly less neuron state overhead, as earlier blocks have large spatial dimensions, as shown in Table 2. Significantly reducing the number of neuron states may also allow them to easily remain in on-chip memory."}, {"title": "Structural benefits", "content": "Time-batching (TB) and layer fusion (i.e. LF) act on different dimensions (temporal vs. spatial) for different reasons (memory reuse vs. reducing intermediate features). Having a hybrid structure leads to a more balanced workload structure where the two strategies (TB and LF) do not compete within the same critical blocks. In a hybrid model,"}, {"title": "Algorithmic benefits", "content": "Learning spatial features first and transforming raw data into higher-level spatial features make temporal learning arguably simpler[14]. Our experiments on SEW-ResNet-18 agree with the literature that removing hidden states from earlier blocks has little effect on accuracy[15]."}, {"title": "Neuron state optimization creates hybrid SNN models", "content": "Reducing neuron states from earlier blocks, as done in Section 6.2, creates a structure similar to hybrid models. Even if the whole workload is spiking, the optimized blocks act as feed-forward layers as they do not have any hidden states. The optimized models have a distribution of features, hidden states, and weights similar to hybrid models. Additionally, our experiments show, under tight memory constraints, that SEW-5's best schedule is similar to RED-LIF's best schedule. Hence, optimizing neuron state memory, as in Section 4.4, brings the benefits of hybrid models to SNNs by improving their schedule, energy efficiency, and even accuracy."}, {"title": "8 Future Work", "content": "We have the following suggestions for future work:"}, {"title": "Improving on-chip energy", "content": "This work focused on optimizing off-chip traffic with limited on-chip memory. Future work can look at improving the on-chip energy. For that, we have the following suggestions:"}, {"title": "Code generation", "content": "In this work, we used a simulation model for performance estimation only. Future work can be towards completing the loop with code generation for hardware."}, {"title": "Exploring other recurrent models", "content": "SNNs are self-recurrent models, where the hidden states of different neurons do not directly affect each other. Other recurrent models can have relationships between its hidden states. For example, in a convolution LSTM layer, the hidden states interact together through a convolution kernel. Hence, updating one state requires knowing other neighboring states. This prohibits time-batched layer-fused (TB-LF) schedules, due to the spatial dependency between states.\nThere are also other emerging transformer-like SNN networks [39]. Such large networks have a completely different structure compared to image-based convolutional SNNs. Such networks can be explored in future work using STEMS."}, {"title": "9 Conclusion", "content": "In"}]}