{"title": "BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation", "authors": ["Alan Zhu", "Parth Asawa", "Jared Quincy Davis", "Lingjiao Chen", "Boris Hanin", "Ion Stoica", "Joseph E. Gonzalez", "Matei Zaharia"], "abstract": "As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT.", "sections": [{"title": "1. Introduction", "content": "As Large Language Models (LLMs) grow in size and capability, the demand for high-quality, diverse data in model training is outpacing human-generated data, necessitating the use of synthetically generated data (Villalobos et al., 2024). Consequently, we expect the compute spent on curating and generating data for model training to increase significantly in the coming years, especially in low-data domains where high-quality data is scarce. For example, it is common to use LLMs to generate synthetic data for a variety of tasks such as math, code, function calling, general reasoning, etc. (Yu et al., 2024; Guo et al., 2024; Patil et al., 2023; Liu et al., 2024).\nThe ease of data generation has led many dataset creators"}, {"title": "Challenges with Instruct-Tuned Models.", "content": "Though instruct-tuned models have become more and more capable at complex tasks resulting in higher quality generations, there have been discussions of how accepted methods for post-training can lead to mode collapse (Shumailov et al., 2024; Wong et al., 2024; Lambert et al., 2024). Mode collapse refers to an inability for models to generate diverse outputs to queries that don't have a single response, yet synthetic data is primarily useful when it is both high quality and diverse (Chen et al., 2024; Ravent\u00f3s et al., 2023).\nIntuitively, one might expect a variety of prompting techniques that explicitly encourage diversity to address this gap. For example, prompting methods we baseline against include conditioning a generation on all previous ones and prompting for it to be distinct, assuming different personas for different generations, or asking for multiple distinct entries in a single call (Zhang et al., 2024b; Naik et al., 2023; Fr\u00f6hling et al., 2024). However, we find that the amount of meaningful diversity from such methods is limited and cannot fully address the gap introduced by post-training. On an open-ended generation task like email generation, such prompting methods do not meaningfully improve dataset diversity (Table 2) or trained model accuracy (Figure 9)."}, {"title": "Base vs. Instruct-Tuned Models.", "content": "An alternative approach is to leverage base models, which are not constrained by post-training biases and thus produce outputs that better represent diversity present in real-world data (OpenAI,\n2024b). More quantitatively, as shown in Figure 1, base models generate outputs with noticeably lower pairwise cosine similarity (mean similarity: 0.313) compared to instruct-tuned models (mean similarity: 0.421). Here we use cosine similarity as a quantitative surrogate for diversity a dataset with greater cosine similarity has more similar items and therefore less diversity. The improved diversity in base model generations can improve downstream task performance, with Figure 2 showing that the accuracy of Llama-3.2-1B-Instruct on GSM8K increases to 22.5% from 17.8% when fine-tuning on the Llama-3.1-70B-Base generated dataset of word problems rather than the Llama-3.1-70B-Instruct generated dataset."}, {"title": "BARE.", "content": "In this work, we observe that diversity is critical for synthetic data generation when used in downstream training and fine-tuning tasks. We find that while the data generated from a base model may be more diverse, it also tends to be of lower quality (Figure 5). This can counteract the gains from diversity and hinder downstream training. Our key insight is that by combining base and instruct-tuned models, one can improve the diversity of a dataset while controlling the quality of individual data entries.\nTo this end, we introduce Base-Refine (BARE)  a novel approach that leverages the diverse data generated by the base models and refines it with instruct-tuned models. In a variety of settings, we find this two-stage process enhances diversity without compromising quality, enabling the generation of datasets that improve downstream performance.\nWe evaluate fine-tuning with BARE-generated data on contemporary tasks, such as the recently introduced Retrieval-Augmented Fine-Tuning (RAFT) method (Zhang et al., 2024c). RAFT generates synthetic question/answer pairs starting from just a corpus to fine-tune a RAG model, but we find that the diversity of data it generates is low. By replacing the data generator in RAFT with BARE, we show we can achieve up to a 18.4% fine-tuned accuracy lift over the existing work that solely uses instruct-tuned models.\nAs another example, BARE improves the generation of synthetic math training data. We generated grade school math problems similar to GSM8K problems with several methods, shown in Figure 2. We find that refining Llama-3.1-70B-Base data generations with Llama-3.1-70B-Instruct improves a fine-tuned model's accuracy on GSM8K from 22.4% (base) to 29.8% (refined). Likewise, using GPT-40 as a generator out of the box (22.4%) does not perform as well as using GPT-40 as a refiner of generations from Llama-3.1-70B-Base (35.8%)."}, {"title": "To summarize, our contributions are:", "content": "1. We quantitatively investigate the quality and diversity of base and instruct-tuned models for synthetic data generation across various sampling methods to motivate better system design. In Section 3, we find that base models tend to produce more diverse responses whereas instruct-tuned models offer higher quality.\n2. Using these insights, in Section 4, we propose Base-Refine (BARE), a practical new method for generating synthetic data. We show in Section 5 that BARE consistently improves fine-tuned model performance over various baselines, including SOTA generation methods, and with only 1000 samples can lead to fine-tuned models comparable to SOTA models of similar sizes."}, {"title": "2. Related Work", "content": "Synthetic Data. Synthetic data is commonly used to train state of the art models like Llama 3.3 (Dubey et al., 2024),\nQwen 2.5 (Qwen et al., 2025), Nemotron 4 (Nvidia et al.,\n2024), and o3-mini (Guan et al., 2025). However, prior work has shown its usage poses risks such as model collapse, where iterative training on low-diversity data shifts the generation distribution toward a high-probability mean, degrading both performance and diversity (Shumailov et al., 2024; Shimabucoro et al., 2024; Guo et al., 2023). Recent work has pointed towards diversity in training data improving downstream performance, though doesn't consider the quality of the data in tandem (Chen et al., 2024). In contrast, the BARE pipeline is designed with both of these objectives in mind, producing diverse high-quality data to support model training."}, {"title": "Generation Methods.", "content": "Current methods to improve LLM generation diversity include prompting, sampling, and multistage generation.\nPrompting methods leverage instruction-following by requesting distinct responses in single calls or varying prompts (e.g., assigning distinct \u2018personalities' or randomizing few-shot examples) (Zhang et al., 2024b; Naik et al., 2023; Fr\u00f6hling et al., 2024; Chen et al., 2024; Li et al., 2022). However, compared to BARE, these methods depend on pre-existing data or curated prompts, limiting scalability.\nSampling methods like temperature scaling and nucleus sampling (Holtzman et al., 2020) are widely used, but often prioritize token-level randomness over semantic diversity. Indeed, methods such as logit suppression (Chung et al., 2023) can enhance diversity but may require significant manual refinement to maintain quality.\nMulti-Stage generation methods vary in approach. SimpleStrat (Wong et al., 2024) explicitly generates strata of the solution space and then conditions responses on each, effective for short-answer responses (often a single word or phrase), though limiting applicability to broader synthetic data generation. More complex frameworks require extensive human setup or curation to seed the diversity with topics, making them less scalable (Lambert et al., 2024; Li et al., 2024a; 2023). In contrast, BARE, is a practical two-step pipeline that we show works generally with minimal human involvement.\nPrior work has studied differences in base and instruct models in calibration (OpenAI, 2024b) and agentic environments (Li et al., 2024b), though no work to our knowledge has leveraged base models for synthetic data generation."}, {"title": "Evaluating Synthetic Data.", "content": "Synthetic data utility is typically assessed by downstream performance, though to motivate better systems, we additionally study diversity and entry-wise quality.\nToken-level metrics like self-BLEU (Zhu et al., 2018) are common, though embedding-based approaches (e.g., BERTScore (Zhang et al., 2019), Sentence-BERT (Reimers"}, {"title": "3. Motivation", "content": "Synthetic data generation should result in a diverse dataset with high-quality entries. Thus we motivate our design of BARE with investigations on the diversity and entry-wise quality of synthetically generated data from base and instruct-tuned models, independent of downstream utility."}, {"title": "3.1. Diversity & Quality", "content": "Diversity. Following Tevet and Berant (2021), we use the average neural similarity score to measure the diversity of a generated dataset. Specifically, we use OpenAI's\n`text-embedding-3-small` (OpenAI, 2024a) to generate embeddings and use cosine similarity to calculate similarity scores, as recommended by OpenAI. We calculate pairwise cosine similarity scores for items in a generated dataset and analyze the resulting distribution of similarities. A lower average similarity indicates a more diverse dataset.\nEntry-wise quality. We seek a metric measuring the quality of individual entries in the generated datasets. To this end, we propose the indistinguishability rate (IR) of the entries as a quality metric. We believe that we are the first to propose this metric for the quality of an individual data entry. Specifically, we measure how often a strong LLM (e.g. GPT-40) fails to identify the synthetically generated entry as being of lower quality when combined with n = 3 real dataset entries. A high IR indicates that generated data closely matches properties of real-world data and the discriminator is randomly guessing, while a low IR indicates the generated data are out-of-distribution from real world data. An example IR prompt is available in Appendix B."}, {"title": "3.2. Experimental Setup", "content": "Models. To investigate diversity and quality differences between instruct-tuned and base models, we evaluate a synthetic dataset generated using Llama-3.1-70B-Instruct and Llama-3.1-70B-Base (Dubey et al., 2024). We additionally use GPT-40 (OpenAI, 2024c) as an instruct-tuned model with strong instruction-following capabilities.\nTo allow fair comparisons and simulate a low-data setting we always provide three few-shot examples."}, {"title": "Domains.", "content": "We examine a variety of domains covering many tasks. Our data generation domains are:\n\u2022 Enron Emails (Klimt and Yang, 2004) generating training data for classifying emails as spam or legitimate. We ensure class-balanced synthetic data by explicitly conditioning each generation on a uniform class distribution.\n\u2022 20 Newsgroups (Pedregosa et al., 2011) generating training data for classifying Usenet messages into one of 20 newsgroup sources. The model generates classes along with content, allowing the generation method to determine the class distribution of the synthetic dataset.\n\u2022 Retrieval-Augmented Fine-Tuning (RAFT) (Zhang et al., 2024c), a domain and generator-agnostic synthetic data generation framework for fine-tuning data in RAG tasks. Here, Q/A pair data generation needs to be conditioned on contexts mimicking retrieval results. We use:\n HotpotQA (Yang et al., 2018), a general\n Wikipedia-based short-answer task.\n PubMedQA (Jin et al., 2019), a medical abstractbased yes/no/maybe question-answering task.\n\u2022 GSM8K (Cobbe et al., 2021), generating grade-school math word problems and solutions for fine-tuning.\n\u2022 LiveCodeBench's Test Output Prediction (LCB TOP) (Jain et al., 2024), generating coding questions and answers on predicting test case outputs given a natural language description of an algorithm's expected behavior and test input for fine-tuning.\nFor the classification tasks of Enron and Newsgroups, we generate a dataset of size n = 500. For the generative model fine-tuning tasks of HotpotQA, PubMedQA, GSM8K, and LCB TOP, we generate a larger dataset of size n = 1000.\nWe sample at a default temperature of 0.7 for base models and the highest temperature at which we experimentally found data generation is still coherent for instruct models, which is 1.0 for Llama models and 1.2 for GPT-40. However, for Enron, we sample from GPT-40 at a temperature of 1.0 to maintain generation coherence.\nNote we do not measure diversity in the RAFT domains as we expect noise due to conditioning on different documents/-contexts. For Enron, due to the relatively large differences between spam and legitimate emails, only similarities between emails of the same type are calculated."}, {"title": "3.3. Investigation Results", "content": "Diversity. Looking at the pairwise cosine similarity distributions of the embeddings of the generated dataset in"}, {"title": "Entry-wise quality.", "content": "Figure 5 presents our results. In general, the instruct-tuned model has a higher IR, indicating that it is better at producing generations that resemble high-quality data. Note that some IRs are well above 75%. This outcome is not unexpected: if a model consistently generates data that aligns with the most common patterns in the real-world distribution, it becomes difficult to distinguish from actual data. Since the real-world entries often also include non-modal (less frequent) samples, a discriminator tasked with identifying lower quality data may instead misclassify these less common real-world samples as synthetic.\nAs mentioned above, on LCB TOP the base model repeats phrases from examples in the prompt. This leads to a higher IR as the generations copy real-world examples. Thus, while individual base generations are technically more realistic, their shortcomings are captured in the diversity metric.\nWe therefore find that the superior instruction follow-up capabilities of instruct-tuned models can help generate more realistic data, motivating our usage of instruct-tuned models in the second stage of BARE."}, {"title": "4. Base-Refine (BARE)", "content": "We leverage our insights to propose BARE, a practical synthetic data generation method combining the diversity of base models with the quality of instruct models. BARE uses a base model to generate an initial set of diverse but potentially lower quality data, after which an instruct-tuned model individually refines each example from the initial set as shown in the example in Figure 6."}, {"title": "5. Evaluation", "content": "We evaluate BARE for diversity, data quality, and downstream utility on the same domains and against the same baseline methods presented in Section 3. We implement BARE with Llama-3.1-70B-Base in the base generation stage and Llama-3.1-70B-Instruct in the refinement stage, and perform additional experiments with the Llama 3.1 8B family and with Llama 3.1 8/70B as the base model and GPT-40 as the refiner (Dubey et al., 2024; OpenAI, 2024c).\nWe follow generally the same sampling temperatures as in"}, {"title": "BARE Quality & Diversity.", "content": "We begin by looking at the quality/diversity trends of BARE when compared with previous methods. From the histograms plotted in Figure 7, we can see that BARE effectively does not change the similarity distribution of generated data when compared to the base model at all  it is able to successfully retain the diversity of base generations. Detailed results with average embedding similarity scores can be found in Appendix A.2.\nSimultaneously though, looking at Figure 8, we can see that while retaining diversity, BARE leads to a monotonic increase in the IR for every domain - suggesting that it is able to lift the quality of the generations to be on par or in some cases even surpass directly sampling from an instruct model. Combined, this indicates that BARE is capable of leveraging the diversity of base models and quality of instruct-tuned models in its end generations."}, {"title": "Fine-tuned Model Accuracy.", "content": "We now show the utility of BARE datasets as a whole. In Figure 10, we demonstrate the accuracy of a model fine-tuned on the datasets generated using different methods. Almost uniformly, BARE leads to greater improvements in downstream model quality than generating from either Base or Instruct models, and across all domains BARE-based data leads to the highest fine-tuned model accuracy.\nImpressively, in Figure 9, beyond just base and instruct sampling, we show that enhancing GPT-40 with BARE using only a small base model like Llama-3.1-8B-Base produces monotonically better fine-tuning data than all prompting methods discussed in Section 3.3 that use only GPT-40. These clear results showcase BARE's ability to out-perform the existing methods for high-quality, diverse data generation.\nFocusing on the RAFT domains, BARE improves upon the standard SOTA pipeline for fine-tuning LLMs for RAG by up to 18.4%, as seen with the 8B family on HotpotQA (standard RAFT is implemented in our Instruct generation results). BARE with both model families also outperforms existing RAFT pipelines on PubMedQA. While BARE with the Llama 3.1 70B family does not improve upon RAFT for HotpotQA, switching out the Llama-3.1-70B-Instruct refiner for GPT-40 does lead to an improvement (Appendix A.2).\nOn GSM8K, BARE is the only method that provides useful training data. The un-trained model performance was 21.8%, and fine-tuning on BARE generated data achieves accuracies of 24.9% and 32.8% with the Llama 8B and 70B families, respectively. Accuracy when training with data generated by single model methods either decreased accuracy or had little difference. In fact, training on data generated by Llama-3.1-70B-Instruct led to an accuracy of just 17.8%, which BARE with Llama-3.1-70B-Base refined by GPT-40 outperforms by 101% (35.8% accuracy).\nOn LCB TOP, fine-tuning a Llama-3.1-8B-Instruct model on 1000 examples generated by BARE using the Llama 3.1 8B model family for just 4 epochs resulted in performance of 28.1% accuracy, comparable to the current top models of similar size on the LCB leaderboard: DeepSeekCoder 6.7B Instruct (Guo et al., 2024) at 32.7% and MagicoderS DS 6.7B (Wei et al., 2024) at 32.4%. While both these models perform slightly better, they used orders of magnitude more data and trained for longer than us.\nSimilar to HotpotQA, on LCB TOP, BARE with the Llama 3.1 70B family shows less of an improvement than the 70B-instruct-only data generation method. Again, using GPT-40 as a refiner instead of Llama-3.1-70B-Instruct gives stronger results. This suggests that with the right refiner choice,"}, {"title": "Instruct-Instruct Ablation.", "content": "A possible challenge is that the gains in performance are due to a multi-step pipeline rather than combining a base and instruct-tuned model. To demonstrate the importance of using a base model in BARE, we performed an ablation where an instruct-tuned model's generations are refined by an instruct-tuned model on the GSM8K task. The accuracy on the test set after switching Llama-3.1-70B-Base to Instruct in the first step drops from 29.8% to 25.4%. This trend held when GPT-40 was the refiner as well, with a drop from 35.8% to 30.8%. At the same time, the pattern of little change in the average pairwise similarity before and after refinement remains, indicating that diversity must be introduced in the first stage. Having earlier established that base models are most effective at diversity, we conclude that the use of base models is a necessary component of BARE. Detailed results from this ablation are in Appendix A.4."}, {"title": "6. Conclusion", "content": "In summary, in this work we quantitatively investigated the quality and diversity of various synthetic data generation methods. Through this investigation, we find that base models are generally more diverse than instruct-tuned models while instruct-tuned models produce higher quality output than base models, validating hypotheses in the research community. These insights motivate the design of a better system to generate synthetic data, BARE.\nThrough extensive experiments, we validate the importance of each step in BARE and demonstrate its ability to preserve base model diversity while enhancing output quality. Moreover, by fine-tuning on BARE-generated data for various domains, we underscore BARE's practical utility, consistently outperforming existing synthetic data generation methods on downstream tasks such as GSM8K and LiveCodeBench, in addition to RAFT, for which we set a new SOTA."}, {"title": "6.2. Future Work", "content": "There are a lot of exciting directions for future work. For one, BARE is not necessarily the only way to elicit diversity in the synthetic data generation process. We were able to achieve all of our results with essentially basic prompting, leading us to believe there is room for even more improvement if one was to, for example, fine-tune the refiner specifically for this use case or use stronger refiner models. Introducing additional stages to BARE could also lead to improvements (e.g., additional refinement steps). Beyond multi-stage systems, the design space for diversity is vast - in one direction, training instruct-tuned models with different objective functions that encourage entropy is another area of exploration.\nFurthermore, the implications of a lack of high-quality, diverse generations from LLMs go beyond just synthetic data. For example, much of the work in inference time compute relies on models being able to generate sufficiently different possible trajectories and using these diverse trajectories (exploration) with a feedback signal to improve reasoning capabilities (Zhang et al., 2024a; Zelikman et al., 2022), yet only a few methods look explicitly at generator diversity (Wang et al., 2024).\nLastly, the same BARE method demonstrated in this paper for synthetic training data can also be used with no modifications to generate synthetic evaluation sets, which are especially valuable in so many real-world domains with low-data availability."}, {"title": "A. Additional Results", "content": "A.1. Downstream Evaluation - Additional Details\nA.1.1. FINE-TUNING TASK HYPERPARAMETERS\nWe list below the fine-tuning hyperparameters that were used in common for HotpotQA, PubMedQA, GSM8K, and LCB TOP. Learning rate was determined independently for each domain via learning rate sweeps (across orders of magnitude); each sweep gave the same optimal learning rate.\n\u2022 Learning Rate: 0.001\n\u2022 LORA a: 16\n\u2022 LORA Rank: 8\n\u2022 LORA Dropout: 0.0\nA.1.2. CLASSIFICATION TASK SETUP\nThe generated data is used to train a BERT-based classifier (Devlin et al., 2018) for 2 epochs on Enron and 9 epochs on Newsgroups. The trained models are evaluated on a static test set with n = 500 examples for each domain."}, {"title": "A.2. Core Experiment Results - All Domains", "content": "This appendix contains diversity, IR, and downstream performance results for all core experiments: generation with Llama 3.1 8B and 70B Base and Instruct models, BARE with Llama 3.1 models of both families, and BARE with the use of GPT-40.\nNote that HotpotQA RAFT and PubMedQA RAFT diversity results present here were not presented in Table 1 as we believe the numbers are noisy and not fit for drawing conclusions, due to the use of 100 different simulated retrieval contexts that generation was conditioned on (as required by RAFT). Not only does this introduce noise to the similarity calculation, but the strong instruction following capability of instruct models allow them to better leverage the inherent diversity in different prompts. However, for completeness, we report the values in the tables in this appendix."}, {"title": "A.3. Independent Sampling Temperature Ablations - HotpotQA, PubMedQA, and LCB TOP", "content": "This appendix contains diversity, IR, and downstream performance results for our temperature ablation experiments. We perform a temperature sweep for Llama-3.1-8B-Instruct generation with t = 0.5, 0.7, 1.0. We find that while adjusting the temperature can improve downstream performance, in general the gains are small relative to gains by using BARE."}, {"title": "A.4. BARE First Stage Ablations - GSM8K", "content": "This appendix contains diversity, IR, and downstream performance results for our ablation replacing the first stage of BARE with an instruct-tuned model, specifically Llama-3.1-70B-Instruct. We refine using Llama-3.1-70B-Instruct and GPT-40, and investigate the change in downstream performance compared to standard BARE (using Llama-3.1-70B-Base in the first stage). Note that dataset diversity is unchanged compared to direct generation from Llama-3.1-70B-Instruct, that IR improves after refinement, and that downstream performance is consistently worse than standard BARE."}, {"title": "B. GSM8K Prompt Examples", "content": "In this appendix, we provide exact prompts used for the GSM8K domains, representative of those used throughout this work. Examples are formatted for inclusion in the prompts in the \u201c{examples}\u201d fields, with \u201cEXAMPLE START\u201d and \u201cEXAMPLE END\" delimiters for the base prompt. BARE uses the standard Base Prompt in the base generation step."}, {"title": "Static Few-shot Examples", "content": "Example 1\nQuestion: Alice has 20 quarters. She wants to exchange them for nickels and so she goes to the bank. After getting back from the bank, she discovers that 20% of the nickels are iron nickels worth $3 each. What is the total value of her money now?\nAnswer: A quarter is worth five nickels because .25 / .05 = $\\ll.25/.05 = 5 \\gg$ 5. She gets 100 nickels from the bank because 20 x 5 = $\\ll 20 * 5 = 100 \\gg$ 100. 20 of the nickels are iron nickels because 100 x .20 = $\\ll 100 * .20 = 20 \\gg$ 20. 80 of the nickels are regular because 100 - 20 = $\\ll 100 \u2013 20 = 80 \\gg$ 80. The iron nickels are worth $60 because 20 x 3 = $\\ll 20 * 3 = 60 \\gg$ 60. The regular nickels are worth $4 because 80 x .05 = $\\ll 80 * .05 = 4 \\gg$ 4. Her money is now worth $64 because 60 + 4 = $\\ll 60 + 4 = 64 \\gg$ 64. #### 64\nExample 2\nQuestion: A church has 120 members. 40% are adults. The rest are children. How many children more children are there than adults?\nAnswer: There are 48 adults because 120 x .4 = $\\ll 120 * .4 = 48 \\gg$ 48. 60% of members are children because 100 - 40 =$\\ll 100-40 = 60 \\gg$ 60. There are 72 children because 120 x .6 = $\\ll 120 * .6 = 72 \\gg$ 72. There are 24 more children than adults because 72 - 48 = $\\ll 72 - 48 = 24 \\gg$ 24. #### 24\nExample 3\nQuestion: Lisa is looking to attempt a World Record. She has decided to try and match Joey Chestnut's record of eating 75 full hotdogs, buns included, in 10 minutes. Halfway through the time Lisa has eaten 20 hotdogs. How many hotdogs will she have to eat per minute to at least tie Joey Chestnut's record?\nAnswer: Joey Chestnut ate 75 hotdogs to claim the record and Lisa has eaten 20 hot dogs so far, so she still needs to eat 75 - 20 =$\\ll 75 \u2013 20 = 55 \\gg$ 55 hotdogs to tie Joey Chestnut. Lisa has a 10-minute time period to eat the hotdogs and half the time has already passed, which means Lisa has 10/2 = $\\ll 10/2 = 5 \\gg$ 5 minutes left until the competition is over. If she needs to eat 55 hotdogs to tie Joey Chestnut and there are 5 minutes left in the competition period, then she needs to eat 55/5 =$\\ll 55/5 = 11 \\gg$ 11 hot dogs per minute to have a chance of tying for a win. #### 11\nBase Prompt\nHere are a few examples of grade school math word problems that require performing a sequence of elementary calculations using basic arithmetic operations. A bright middle school student should be able to solve each problem. The numerical answer is provided at the end of each example after ####.\n{examples}\nEXAMPLE START\nInstruct Few-shot Prompt\nProvide an example of a grade school math word problem that requires performing a sequence of elementary calculations using basic arithmetic operations. A bright middle school student should be able to solve each problem. Problems require no concepts beyond the level of early Algebra. You must first specify the question, then provide the very concise reasoning and answer. Provide your example in the following format:\nQuestion:\n[question]\nAnswer:\n[answer]\nProvide only the question and answer in the given format. Note how the numerical answer is provided after #### after each brief reasoning for a question. Here are some examples:\n{examples}\nNow it's your turn. Start your response with the question.\nRefine Prompt\nImprove the given grade school math word problem. Edit the problem or answer to be more similar in style to the examples, and disambiguate as necessary, in addition to correcting any errors. Do not change the theme of the problem. A bright middle school student should be able to solve each problem. Problems require no concepts beyond the level of early Algebra. Note how the numerical answer is provided after #### after each brief reasoning for a question. Provide your edited problem in the following format:\nQuestion:\n[question]\nAnswer:\n[answer]\nProvide only the question and answer in the given format. Here are some examples of categories and problems on those categories:\n{examples}\nNow it's your turn. Here is the question and answer for you to edit:\nQuestion:\n{question}\nAnswer:\n{answer}\nProvide only the improved question and answer in the given format. Do not include any commentary or notes. Start your response with the question.\nSequential Prompt\nGenerate a new grade school math word problem that requires performing a sequence of elementary calculations using basic arithmetic operations. A bright middle school student should be able to solve each problem. Problems require no concepts beyond the level of early Algebra. Here are the previously generated examples:\n{examples}\nYour new problem should:\n1. Be different from the previous examples\n2. Follow the same format and style as prior problems\nNote how the numerical answer is provided after #### after each brief reasoning for a question. Provide only the question and answer in the given format here:\nQuestion: [question]\nAnswer:\n[answer]\nStart your response with the question.\nIn One Prompt\nProvide {num} examples of problems that might be grade school math word problems that require performing a sequence of elementary calculations using basic arithmetic operations. A bright middle school student should be able to solve each problem. Problems require no concepts beyond the level of early Algebra. You must first specify the question then provide the brief reasoning and answer. Note how the numerical answer is provided after #### after each brief reasoning for a question. Provide your examples in the following format:\nQuestion:\n[question]\nAnswer:\n[answer]\nHere are some examples:\n{examples}\nNow it's your turn. Generate {num} different problems following this format. Your question should be different in content from the examples. Make sure to only provide only the question and answer. Start each example with the question. Delimit the end of an example with the phrase"}]}