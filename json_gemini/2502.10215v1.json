{"title": "Do Large Language Models Reason Causally Like Us? Even Better?", "authors": ["Hanna M. Dettki", "Brenden M. Lake", "Charley M. Wu", "Bob Rehder"], "abstract": "Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. We find that LLMs reason causally along a spectrum from human-like to normative inference, with alignment shifting based on model, context, and task. Overall, GPT-40 and Claude showed the most normative behavior, including \"explaining away,\" whereas Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected independence of causes Claude the least - they exhibited strong associative reasoning and predictive inference when assessing the likelihood of the effect given its causes. These findings underscore the need to assess AI biases as they increasingly assist human decision-making.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have proven to be highly capable across a range of domains, including natural language understanding, answering questions, and engaging in creative tasks (Bubeck et al., 2023; Abdin et al., 2024; Gunter et al., 2024). In light of these recent advancements in LLMs, many believe that we are now truly entering an era of Artificial Intelligence (AI; Bottou & Sch\u00f6lkopf, 2023). The degree to which machines genuinely comprehend our environment carries significant implications for their reliability in various domains (Mitchell & Krakauer, 2023), including the automatic generation of news content, policy recommendations (Keki\u0107 et al., 2023), knowledge discovery, disease diagnosis (Nori, King, McKinney, Carignan, & Horvitz, 2023), and autonomous driving. The impressive capability of LLMs to produce text resembling human language raises the question of whether these models possess some form of world understanding, and if they reason similarly to humans.\nCausal reasoning is widely regarded as a core aspect of intelligence (Lake, Ullman, Tenenbaum, & Gershman, 2017). It involves recognizing and inferring the causal relationships between variables, moving beyond mere correlations to uncover underlying mechanisms. Such capabilities are essential in practical applications, including the development of pharmaceutical drugs or the planning of public health strategies. Therefore, causal reasoning is considered an important milestone in the pursuit of Artificial General Intelligence (AGI; Obaid, 2023). Causal reasoning can be formalized using causal Bayes nets (CBNs) providing a probabilistic calculus for reasoning about the probability of some variables given others that are causally related (Pearl, 1995). By comparing human reasoners to CBNs, CBNs can serve as a normative benchmark (Glymour, 2003; Waldmann, Hagmayer, & Blaisdell, 2006) and help reveal human biases that deviate from ideal causal reasoning (Rehder & Waldmann, 2017; Bramley, Lagnado, & Speekenbrink, 2015). For instance, when reasoning about a simple collider graph $C_1 \\rightarrow E \\leftarrow C_2$, people exhibit biases such as weak explaining away and Markov violations (explained later; Rehder & Waldmann, 2017). These systematic deviations highlight the interplay between normative principles and cognitive heuristics in human causal reasoning.\nA plethora of recent studies have assessed the capabilities of LLMs (e.g., K\u0131c\u0131man, Ness, Sharma, & Tan, 2023), and concerns have been raised regarding their reliance on learned patterns rather than genuine causal relationships (Willig, Zecevic, Dhami, & Kersting, 2023; Jiang et al., 2024).\nFor example, Shi et al. (2023) and Mirzadeh et al. (2024) demonstrated that introducing irrelevant context can drastically alter the outputs of LLMs. That even minor distractions influence their responses raises questions about the robustness of LLMs in high-stakes scenarios.\nIndeed, a growing number of researchers have proposed that current LLMs are unable to generalize causal ideas beyond their training distribution and/or without strong user-induced guidance (e.g., chain-of-thought prompting; Jin et al., 2023; K\u0131c\u0131man et al., 2023). Thus, understanding the extent to which LLMs reason causally, and whether they show similar biases to people when they deviate from normative principles has practical importance in deploying AI systems. To this end, Jin et al. (2023) introduced the CLADDER dataset, comprising 10,000 causal reasoning questions designed to evaluate the formal causal reasoning abilities of LLMs. While they tested colliders in their dataset, they didn't contrast LLMs with humans. In addition, while the dataset serves as a valuable benchmark to test whether LLMs honor the rules of probability, solving the tasks requires a substantial background in probability and statistics (college-level statistics class and pen and paper), making them less suitable"}, {"title": "Methods", "content": "Participants. We compare the human behavioral data collected in Rehder and Waldmann (2017) (Experiment 1, Model-Only condition, N = 48) with judgments gathered from four LLMs GPT-3.5 (), GPT-40 (\u25a0), Claude-3-Opus (), and Gemini-Pro () \u2014 which were prompted with the same inference tasks as humans over their respective APIs. The LLMs were tested with five temperature settings \u2208 {0.0, .3, .5, .7, 1.0} but we only report results for temperature 0.0 as this ensures consistent and reproducible outputs.\nMaterials. The collider causal structure $C_1 \\rightarrow E \\leftarrow C_2$ was embedded in one of three cover stories from three different knowledge domains (meteorology, economics, and sociology), allowing for a natural language description of the causal structure. The three domains were chosen because the undergraduate subjects were expected to be relatively unfamiliar, such that their causal inferences would reflect the causal structure given to them and not idiosyncratic prior knowledge. Nevertheless, as an additional safeguard, the direction of each variable was counterbalanced (e.g., in the domain of sociology, some subjects were told that high urbanization causes high socio-economic mobility, others that it causes low socio-economic mobility, etc). In fact, Rehder and Waldmann (2017) did not find significant effects of domain or the counterbalancing factor, suggesting that subjects' inferences were not strongly influenced by domain knowledge. An important question we ask here is whether this also holds for the LLMs.\nGiven a set of observations (a subset of the states of $C_1$, $C_2$, and E), both humans and LLMs were asked to provide a likelihood judgment on a continuous scale (0-100) for a specific query variable.\nBelow is an example prompt from the sociology domain, matching the visualization in Figure 1 and diagnostic task X in Figure 2e, where the query node () is $C_1$ = 1 and $C_2$ and the effect E are known to be absent. Note that only the italicized text following \":\" was presented to LLMs in one piece.\nDomain introduction:\nSociologists seek to describe and predict the regular patterns of societal interactions. To do this, they study some important variables or attributes of societies. They also study how these attributes are responsible for producing or causing one another.\nCausal mechanism:\nAssume you live in a world that works like this:\n* $C_1 \\rightarrow E$: High urbanization causes high socio-economic mobility.\nExplanation: Big cities provide many opportunities for financial and social improvement.\n* $C_2 \\rightarrow E$: Also, low interest in religion causes high socio-economic mobility.\nExplanation: Without the restraint of religion-based morality, the impulse toward greed dominates and people tend to accumulate material wealth.\nObservation:\nNow suppose you observe the following: low socio-economic mobility and low urbanization.\nInference task, here X:\nGiven the observations and the causal mechanism, how likely on a scale from 0 to 100 is low interest in religion? 0 means definitely not likely and 100 means definitely likely. Please provide only a numeric response and no additional information.\nTo summarize how humans reason with colliders, the empirical findings reported by Rehder and Waldmann (2017) are presented in Figure 2 () alongside the inferences drawn by the LLMs, which are discussed later. The eleven inference tasks (I-XI) are grouped into four types:\nPredictive inferences in a collider network involve inferring the state of the effect given information about one or more of the causes. Reasoners should judge, for example, that $p(E = 1 | C_1 = 0, C_2 = 0) < p(E = 1 | C_1 = 0, C_2 = 1) < p(E = 1 | C_1 = 1,C_2 = 1)$. Figure 2b reveals that human reasoners in fact exhibit this pattern, confirming that they made use of the causal knowledge on which they were instructed.\nIndependence of causes is another property of colliders. Because in CBNs exogenous causes are stipulated to be uncorrelated, reasoners should judge that the presence of one cause should not affect the likelihood of the other: $p(C_1 = 1 | C_2 = 1) = p(C_1 = 1 | C_2 = 0)$. Figure 2c reveals that humans"}, {"title": "Results", "content": "Comparison of LLMs and Humans. As an initial assessment of the LLMs we computed the Spearman correlation between their inferences and those of humans in each domain. Table 1 reveals correlations that are positive and substantial in magnitude, indicating the LLMs are exhibiting a degree of human-like performance on the causal reasoning task. The highest average correlations were displayed by Claude \u25a0(rs = .631) and GPT-40 \u25a0 (.626), followed by GPT-3.5 (.462) and Gemini (.373). This pattern was observed in all three domains.\nFigure 2 presents the LLMs' responses to the individual inference tasks averaged over conditions. The four inference types reveal distinct reasoning patterns across agents.\nPredictive inferences (see Figure 2b, I-III) for the LLMs were a monotonic increasing function of the number of causes present, similar to the human judgments. This indicates that the LLMs were sensitive to the most rudimentary aspect of the task, namely, that causes make their effects more likely.\nIndependence of causes (IV-V) means that the state of one cause should not affect the likelihood of the other. In fact, Figure 2c shows that GPT-3.5 and Gemini judged that $p(C1 = 1 | C2 = 1) > p(C1 = 1|C2 = 0)$ even more egregiously than humans. Claude violated independence the least whereas GPT-40 \u25a0 exhibited a small independence violation in the opposite direction.\nEffect-Present Diagnostic Inference (Figure 2d, VI-VIII) reveals whether agents explain away, indicated by a positive slope. GPT-40 \u25a0 exhibits the strongest explaining away, followed by Claude and then humans. Gemini and GPT-3.5 failed to explain away, judging instead that the presence of one cause would increase the probabililty of the other.\nEffect-Absent Diagnostic Inference (Figure 2e, IX-XI) has all agents produce lower ratings for the cause, with GPT-40 \u25a0 and Claude producing the lowest ratings across all conditions and Gemini seeming to be closest aligned with humans. While humans and Gemini are more likely to assign ratings in the middle of the scale, GPT-40 \u25a0 is most inclined to assign a rating of 0 and treated the causal relations as closer to necessary and sufficient than any other agent. This conclusion ends up being supported by the model fitting that follows, which yielded especially large estimates of the strengths of the causal relations for GPT-40 (see Figure 3).\nNote that the LLM responses in Figure 2 were distributed more broadly than human's: Whereas the difference between the highest and lowest judgment was at least 78 for the LLMs (and was 100 for GPT-40), it was only 66 for the humans. This tendency might stem from the experimental setup. Whereas LLMs were prompted to generate a single numeric value, humans responded using an interactive slider that defaulted to 50. This default could have introduced a motor bias that encouraged responses near the middle of the scale."}, {"title": "CBN Model Fitting", "content": "We evaluate LLMs and humans against normative inferences from a causal Bayes net (CBN). Since agents received only verbal descriptions, the CBN's parameters Om were treated as free parameters and fit to the data. These parameters include the prior probabilities over causes p(C1), p(C2), the causal strength parameters $w_{C_1,E}$, $w_{C_2,E}$ representing the strength of each causal relationship $C_{1,2} \\rightarrow E$, and the baseline parameter $w_E$ capturing the influence of any exogenous causal influence on E.\nThe CBN is used to derive a joint probability distribution which was then used to derive the conditional probability appropriate for that task. For a collider causal graph $C_1 \\rightarrow E \\leftarrow C_2$, the joint distribution was derived assuming that $p(C_1,C_2,E) = p(E|C_1,C_2)p(C_1)p(C_2)$ and that $p(E = 1|C_1,C_2) = 1/(1+exp(-(C_1w_{C_1,E}+C_2w_{C_2,E} +w_E)))$, where $w_{C_1,E}$ and $w_{C_2,E}$ represent the strength of $C_1 \\rightarrow E$ and $C_2 \\rightarrow E$, respectively, and C\u2081 and C2 are each coded as 1 when present and -1 when absent. The CBNs were fit to each agent's set of causal judgments by identifying parameters that mini-\nwc was constrained to the range [0, 1] whereas the causal strength parameters were constrained to the range [-3, 3]. For the human data these CBNs were fit to each subject. For the LLMs, they were separately fit to the judgments generated in each of the 3 domains \u00d7 4 counterbalancing = 12 conditions.\nTable 2 presents the CBNs' best fitting parameters averaged over conditions/subjects. Several interesting trends emerge. First, the judgments of all LLMs exhibited substantial correlations with their fitted CBNs, spanning the range .559-.882. These results compare with the corresponding correlations for the human data (about .77). The best fitting LLMs were GPT-40 and Claude-3-Opus; for both the correlations associated with their 4-parameter fits exceeded .88 and their model loss (the average absolute difference between predicted and observed values) was less than 13 points on the 0-100 scale. Indeed, the responses of these LLMs were more correlated with their fitted CBNs than those of their human counterparts. In contrast, the fits of GPT-3.5 and Gemini-Pro were worse than the other LLMs and the human reasoners.\nThe fits of Gemini-Pro were the poorest, exhibiting a model loss of 23.4 for even the 4-parameter CBN.\nRegarding the contrast between the 3- and 4-parameter CBNs, all LLMs were better fit by the model that allowed the two causal relations to have different strengths (accord-"}, {"title": "Fitting a Psychological Model", "content": "We also fit the LLM inferences with a model proposed as an account human causal reasoning, the mutation sampler (Davis & Rehder, 2020). The mutation sampler is an example of a rational process model, an algorithm that yields normative responses when cognitive resources are unlimited but that produces errors when they are not (Johnson & Busemeyer, 2016; Lieder, Griffiths, & Goodman, 2012; Vul, Goodman, Griffiths, & Tenenbaum, 2014).\nThe mutation sampler carries out MCMC sampling over a causal graph's state space and draws inferences on the basis of samples. But because sampling begins at one of the graph's prototypes states (when causal relations are all generative, the states where variables are all present or all absent), errors are introduced when the number of samples drawn is limited. Davis and Rehder (2020) showed that the mutation sampler accounts for the independence violations that arise in"}, {"title": "Discussion", "content": "We compared the causal reasoning abilities of large language models to those of people. In Rehder and Waldmann (2017) undergraduates were taught hypothetical causal knowledge consisting of three variables that formed a collider causal graph and then were asked to draw simple causal inferences.\nOur first main finding is that given exactly the same information, LLMs can do the task. That is, after being told that the presence of one variable C causes the presence of another E, LLMs will judge the effect E is more likely when a cause C is present versus absent (and vice versa). Indeed, across all domains and tasks, the Spearman correlation rs between LLM and human inferences ranged from .372 to .631.\nAs discussed an important property of collider structures is that they entail explaining away. On one hand, GPT-40 and Claude-3 exhibited the basic explaining away pattern at least partially, just like people did. GPT-4o exhibited the strongest explaining away (indicated by most positive slope in Figure 2d), while Gemini-Pro and GPT-3.5 did not exhibit explaining away. One interpretation of this result is that the two latter models were reasoning more \u201cassociatively,\" that is, without regard to causal semantics of a collider network. At least on this one test of \"causal understanding,\" Gemini-Pro and GPT-3.5 failed.\nA collider structure also entails that the two causes should be independent. As discussed, human reasoners often violate independence, which in a collider with generative causes means that the two causes are treated as positively correlated (Davis & Rehder, 2020). In other words, human causal inferences also exhibit a degree of \u201cassociative thinking.\u201d Gemini-Pro and GPT-3.5's also treated the causes as positively correlated, consistent with the interpretation of their explaining away inferences offered above. Interestingly, whereas Claude treated the causes as uncorrelated, GPT-40 treated them as somewhat negatively correlated.\nIn addition to humans, we compared LLMs to the normative inferences of fitted causal Bayes nets. The correlations between the LLM and normative inferences were between .559 and .882, as compared to about.77 for the humans. GPT-3.5 and especially Gemini-Pro exhibited the poorest correlations with the normative inferences and ones that were worse than for humans; GPT-40 and Claude exhibited the highest correlations and ones that were higher than for humans.\nThe parameters derived from the CBN fits also provided insight into how the agents were reasoning in the different domains. As mentioned, the materials were drawn from domains about which undergraduates were expected to know little. Consistent with this expectation, the standard deviations associated with the human's fitted CBN parameters were relatively low (e.g., for the average human reasoner the difference between estimated strength of the two causal relations differed by only .12). In contrast, the standard deviations for with the LLM's fitted model parameters were much higher (e.g., the difference between the two causal strengths was at least .45). These results provide evidence that the LLM inferences were affected by the their domain knowledge more than those of the human reasoners.\nFuture Work. There are numerous avenues for future research. Here we compared human and LLM inferences on only one simple causal structure, whereas humans have been tested on causal networks with different topologies (e.g., forks, chain, etc.), causal relations (inhibitory vs. generative), integration functions (e.g., causes that combine conjunctively rather than independently), with more than three variables, and with continuous variables rather than binary ones. Besides the simple causal inferences examined here, there is a wealth of data on how humans intervene on causal systems, make causal attributions in cases of actual causation, and learn causal systems from observed data. Regarding LLMs, a deeper analysis of the effects of domain knowledge on their inferences is warranted as such knowledge can affect both independence (via inferred causal connections between the collider's causes) and explaining away (via treating the two causal relations as interactive rather than independent; Cruz, Hahn, Fenton, & Lagnado, 2020; Morris & Larrick, 1995). It is also important to better understand how their inferences are affected by factors such as the temperature parameter.\nOverall, LLMs respond meaningfully to the same complex prompts used in research on human causal reasoning. GPT-40 and Claude-3 aligned more with normative inferences than humans, while Gemini-Pro and GPT-3.5 were often less normative than humans. Rather than treating the task abstractly, LLMs drew inferences based on their domain knowledge."}]}