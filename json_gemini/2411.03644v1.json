{"title": "Deploying Multi-task Online Server with Large Language Model", "authors": ["Yincen Qu", "Chao Ma", "Yiting Wu", "Xiangying Dai", "Hui Zhou", "Hengyue Liu"], "abstract": "In the industry, numerous tasks are deployed online. Traditional approaches often tackle each task separately by its own network, which leads to excessive costs for developing and scaling models, especially in the context of large language models. Although multi-task methods can save costs through parameter sharing, they often struggle to outperform single-task methods in real-world applications. To tackle these challenges, we present a three-stage multi-task learning framework for large language models. It involves task filtering, followed by fine-tuning on high-resource tasks, and finally fine-tuning on all tasks. We conducted comprehensive experiments in single-task and multi-task settings. Our approach, exemplified on different benchmarks, demonstrates that it is able to achieve performance comparable to the single-task method while reducing up to 90.9% of its overhead.", "sections": [{"title": "Introduction", "content": "In the industry, numerous natural language processing (NLP) tasks are deployed online, and all tasks are required to serve with punctuality and high accuracy. As the number of tasks increases, the demand for resources also grows. Preventing resource requirements from growing linearly with the number of tasks becomes one of the most critical challenge in cost-saving.\nTraditional approaches tackle each task separately by its own network and pipeline. This leads to excessive workloads for development and maintenance, as well as increased latency and resource usage. Moreover, in the context of large language models (LLMs), it may also lead to excessive costs for scaling up models for each task. We propose utilizing multi-task serving to deploy LLMs instead of single-task serving. single-task serving and multi-task serving are two types of online serving strategies, and their paradigms are shown in"}, {"title": "Related Works", "content": "Multi-task Learning. Multi-task learning (MTL) involves training a single model on multiple tasks simultaneously. Several studies have explored the effectiveness of MTL in various domains, such as natural language processing, computer vision. Recently, T5, ExT5 and Muppet have been proposed to explore the application of Multi-Task Learning (MTL) techniques in Large Language Models (LLMs). However, they selected different checkpoints for each task without aiming to train the model to handle tasks simultaneously. Moreover, most of recent works such as FLAN, TO, and GPT-3, etc., focused on zero-shot or few-shot performance and neglected to compare with the full fine-tuning method for single tasks. However, we found that it is not trivial to surpass single-task full-parameter fine-tuning method.\nData Imbalance. Due to the prevalence of imbalanced data distribution, data balancing has attracted increasing attention. Researchers have proposed static sampling to achieve a more balanced data distribution, which includes class-balanced sampling, temperature-scaled sampling. Previous works show evidence that static sampling approach yield optimal results in data rich regime (high-resources). Recently proposed to prevent model to overfit on the low-resource language in static sampling during multilingual pre-training. They focus on the performance of similar tasks under data imbalance, such as translation between different languages and multilingual pre-training. In our work, we integrated dissimilar tasks and explored whether data imbalance and heterogeneity could hinder multi-task performance."}, {"title": "Preliminaries", "content": "In this section, we present three common sampling strategies that aim to re-balance the task distribution. We will utilize these three sampling methods as baselines for subsequent experiments.\nInstance-balanced sampling. Instance-balanced sampling refers to sampling examples from each task based on the total size of each task's dataset. Specifically, the empirical distributions for different tasks are as follows."}, {"title": "Sampling Strategies", "content": "Pl = \\frac{\\pi_l}{\\sum_{l' \\in L} \\pi_{l'}}\nwhere $n_l$ is the data size of task $l$. Here data points from task $l$ will be sampled with the probability $p_l$, which is proportional to the cardinality $n_l$ of the task in the training set.\nClass-balanced sampling. Class-balanced sampling refers to sampling examples from each task with equal probability. In each batch, each example is sampled uniformly from one of the tasks used for training.\nTemperature-scaled sampling. Temperature-scaled sampling refers to re-scaling the sampling"}, {"title": "Methodology", "content": "Our proposed framework in Figure 2 features a pipeline that consists of three steps: 1) Task filtering; 2) High-resource task fine-tuning; 3) Tasks mixture fine-tuning. We provide a detailed breakdown of these steps below."}, {"title": "Task Filter", "content": "To prevent negative transfer between different tasks, it's important to filter out inappropriate tasks. We found that generation tasks and classification tasks would hinder each others' performance in multi-task training, as evidenced in the experiment sections. The output of classification tasks is fixed, whereas the output of generation tasks is flexible. For instance, the CLUE tasks encompass single sentence classification, sentence pair classification, and machine reading comprehension. We categorize the single sentence classification and sentence pair classification as classification tasks, and machine reading comprehension as generation tasks.\nMoreover, we also investigated that whether differences in input (such as single sentences or sentence pairs) or output (such as binary classification or multi-class classification) would further impede performance. We found that the more similar the tasks are, the higher the multi-task performance can be achieved, and the greater the number of qualified tasks becomes."}, {"title": "Filtered Task", "content": "In order to train a unified model for various tasks, we cast all of the collected tasks into a format called \"text-to-text.\" This format requires the model to be fed with some text for context and then generate output text for individual tasks. To indicate the specific task, we add a task-specific text prefix to"}, {"title": "Multi-task Fine-tuning", "content": "For tasks with imbalanced data, we utilize the multi-task learning approach to balance the performance of all tasks. However, the aforementioned sampling strategies are not ideal, as they sample all tasks with a constant probability throughout the entire training process, leading to over-fitting of low-resource tasks, while high-resource tasks still require learning.\nWe divide the tasks into two groups: high-resource and low-resource tasks. Since we have a variety of tasks with different training saturation steps, it is unfeasible to categorize them based on the amount of training data as in Choi et al. (2023). Instead, we categorize tasks based on the training saturation steps in the single-task setting. If a task achieves overfitting in fewer than 5 epochs, we refer to it as a \"low-resource task.\" If a task achieves overfitting after more than 5 epochs, we refer to it as a \"high-resource task.\"\nFor these task groups, we perform two-stage training, including high-resource task fine-tuning and tasks mixture fine-tuning."}, {"title": "High-resource task fine-tuning", "content": "For high-resource tasks, we utilize the method of instance-balanced sampling to train them, given that they each have a similar amount of training data."}, {"title": "Tasks mixture fine-tuning", "content": "After fine-tuning the model on high-resource tasks, we proceed to fine-tune it on the full mixture of tasks. We utilize temperature-scaled sampling and impose an artificial limit on dataset size to train all downstream tasks simultaneously. We set an artificial limit (K) on the dataset size to prevent over-fitting. The adjusted distribution of different tasks is as follows."}, {"title": "Experiments", "content": "In the following sections, we apply our proposed training method to CLUE tasks and our domain application tasks. In the CLUE experiments, we show that inappropriate sampling"}, {"title": "Sampling Strategies", "content": "\\rho_l = \\frac{min(n_l, K)}{\\sum_{l'\\in L} min(n_{l'}, K)} \\\\\nq_l = \\frac{{\\rho_l}^{\\frac{1}{\\tau}}}{\\sum_{l' \\in L} {\\rho_{l'}}^{\\frac{1}{\\tau}}}"}, {"title": "CLUE Tasks", "content": "The CLUE benchmark is synthetic, consisting of six classification datasets: CWSC, TNEWS, CSL, AFQMC, IFLYTEK, and OCNLI. We provide details and references in Appendix B. For each task, we used accuracy rate as the primary evaluation metric. We reported the macro-average accuracy across all tasks within the benchmark. In the multi-task setting, we also provided the count of qualified tasks, which are defined as those achieving 99% of the performance of their single-task counterparts. To measure the parameter and computational efficiency, we introduced a ratio: the number of qualified tasks divided by the number of models deployed. This ratio is 1 for the single-task baseline, as it deploys one model per task. For multi-task models, the ratio is calculated as 1 divided by the number of qualified tasks. This metric is labeled as \"overhead\" in the header of Table 1.\nIn the experiment, we take the 7B Qwen2 and 8B LLaMA3 as the base model. We present a comparative analysis of our two-stage sampling method against five benchmark approaches: few-shot prompting, single-task fine-tuning, instance-balanced sampling, class-balanced sampling, and UniMax. In the case of few-shot prompting, we prepend five random training instances (qi, ai); as the example to guide the model's input."}, {"title": "Experiment Setup", "content": "Table 1 shows the experimental results on the CLUE benchmark. We observed that an inappropriate sampling strategy would hinder the multi-task performance. The few-shot method performed the worst, suggesting that it is not yet capable of directly replacing current fine-tuning methods, particularly for multi-class classification tasks. Our 2-stage sampling strategy achieved the best performance among all sampling approaches, delivering the highest number of qualified tasks. Compared to our method without the two-stage training process,"}, {"title": "Main Results", "content": "Moreover, we noted that LLaMA's macro-average performance on Chinese tasks is inferior to that of Qwen, likely due to insufficient training on Chinese corpora. Given that Qwen has been pre-trained on Chinese corpora, it demonstrates superior multi-task performance in Chinese. Consequently, in Section 5.2, we carry out additional experiments to assess the performance of the generic model in comparison to the model that has undergone domain-specific pre-training."}, {"title": "Taxonomy Impact", "content": "In this section, we investigate the impact of taxonomy granularity on multi-task performance. We introduced the machine reading comprehension task CMRC into our task mixture, and trained a multi-task model with this expanded dataset. Unlike the original set of six classification tasks, CMRC, as a generation task, has a flexible output format. From the Table 2, we found that training generation and classification tasks concurrently significantly impacts the overall performance. It is particularly notable that the performance of the classification tasks not only lags behind their single-task counterparts but also fails to match the performance of the multi-task model that was trained only on classification tasks.\nTo delve deeper into whether task similarity can enhance performance, we categorized the tasks into groups based on differences in input and output types: single-sentence, sentence-pair, binary classification, and multi-class classification. A more detailed presentation of the tasks and their results is provided in Appendix D. From Table 9, we noticed that increased task similarity correlates with improved performance. However, the \"overhead\" metric does not decrease, as the number of models also rises. To meet our objective of cost saving, a lower overhead metric is desirable. Consequently, we decided against further subdividing these tasks into more similar categories."}, {"title": "Application Tasks", "content": "In this section, we expand from a six-task setting to the setting with dozens of tasks, to verify whether task filtering and sampling methods would affect the multi-task performance."}, {"title": "Experiment Setup", "content": "We tested with 17 classification tasks, which are all related to the domain of customer service. The details of these tasks are demonstrated in Appendix C. We also reported macro average performance, the"}, {"title": "Application Results", "content": "Table 3 shows the experimental results on the industry benchmark. We found that when task number increases, inappropriate sampling strategy has more obvious effect on the multi-task performance. Our method outperforms other sampling baselines by consistently enhancing both the macro-average performance and the number of qualified tasks. With an overhead of only 9.1% compared to the single-task approach, our method can potentially reduce the serving cost by up to 90.9% relative to the single-task method.\nWe observed that Qwend exhibits relatively high performance compared to Qwen. Specifically, Qwend demonstrates a higher average performance than Qwen. Furthermore, any sampling method with Qwend results in a greater number of qualified tasks than with Qwen. We attribute these improvements to domain adaptation. Given the substantial disparity between customer service conversations and the general domain text corpora utilized by original LLMs, incorporating domain-specific knowledge through continuous pre-training significantly aids in downstream task performance. Moreover, the amount of required updates for each task is reduced, leading to less conflict in gradient directions when training tasks concurrently."}, {"title": "Taxonomy Impact", "content": "Consistent with our previous experiment, we incorporated a generation task into our task mixture and trained them jointly with Qwend. From Table 4, we found that regardless of the sampling strategy employed, both classification and generation tasks experienced a significant decline in performance compared to their single-task counterparts. This suggests that the negative impact is indeed present, likely due to the substantial differences between the tasks."}, {"title": "Conclusion", "content": "In this work, we demonstrated the benefits of task filtering and two-stage multi-task training for multi-task optimization in the presence of task imbalance and heterogeneity. Through a variety of experimental setups, we show that inappropriate sampling and task selection strategies may hinder the overall multi-task performance. Our method, though straightforward, is a viable alternative to models trained with the single-task approach, potentially resulting in substantial cost savings."}, {"title": "Experiment Setting", "content": "For a fair comparison, we have capped the training steps for different sampling methods at 15,000. The hyper-parameters (e.g. learning rate, mini-batch size, etc) used in our experiments are summarized in Table 6."}, {"title": "CLUE Benchmark", "content": "Chinese Winograd Schema Challenge (CWSC). The CWSC dataset is designed for anaphora and coreference resolution. The model is asked to determine if a pronoun and a noun phrase whithin a sentence refer to the same entity. It's a binary classification task. It mirrors similar English datasets and consists of sentences carefully selected from 36 modern Chinese literary works. Their anaphora relations are meticulously annotated by linguists, resulting in a collection of 1,838 questions.\nTouTiao Text Classification (TNEWS). TNEWS consists of Chinese news from TouTiao, comprising 73,360 titles in total. Each title is assigned a label among 15 different news categories, such as finance, technology and sports. The goal of this task is to predict which category the title belongs to.\nIFLYTEK. The IFLYTEK is a Chinese multi-class classification dataset, comprising 17,332 descriptions of mobile applications. The objective is to categorize each description into one of the 119 available categories, including but not limited to food, car rental, and education. A data filtering method akin to that employed for the TNEWS dataset has been utilized in this process."}, {"title": "Chinese Scientific Literature (CSL)", "content": "CSL dataset comprises abstracts from Chinese scientific papers and their associated keywords, sourced from various core journals across natural and social sciences. This dataset includes artificially generated keywords using the tf-idf method, which are combined with genuine keywords. The task involves identifying whether the provided keywords for a given abstract are authentic to the paper. This primarily assesses the models' capacity to determine if the keywords accurately encapsulate the content of the document."}, {"title": "Ant Financial Question Matching Corpus (AFQMC)", "content": "AFQMC originates from Ant Technology Exploration Conference (ATEC) Developer competition. It presents a binary classification challenge designed to determine if two given sentences share a similar meaning."}, {"title": "Original Chinese Natural Language Inference (OCNLI)", "content": "OCNLI is a natural language inference dataset using a similar methodology to the MNLI dataset. It consists of 56,000 inference pairs across five different categories: news, government documents, fiction, TV transcripts, and telephone transcripts. The source material for the premises is Chinese, and hypotheses were authored by university students specializing in linguistics. The level of agreement among the annotators is comparable to that of MNLI."}, {"title": "Chinese Machine Reading Comprehension (CMRC)", "content": "CMRC is a machine reading comprehension dataset that is based on span extraction. It comprises approximately 19,071 questions, all of which are human-annotated and sourced from Wikipedia passages. Each entry in the CMRC dataset includes a context, a question, and the corresponding answer. The answers are segments of text extracted directly from the context."}, {"title": "Application Tasks", "content": "Reservation Cancellation (RC). Reservation cancellation refers to the hotel canceling a confirmed booking and not allowing guests to check-in. This is a binary classification problem where the input is a conversation, and we need to determine whether there is a booking cancellation mentioned in the conversation. Depending on the source of the input, which can be either from a phone call or an online chat, the task of reservation cancellation is considered as two separate tasks. The source of phone call is referred to as RC-A (Automatic speech recognition), while the source of online chat is referred to as RC-I (Instant messaging).\nUnforseen Circumstances (UC). Unforseen circumstances refers to unforeseeable and uncontrollable circumstances that prevent guests from checking in after the hotel has confirmed a reservation. This is a binary classification problem where the input is a conversation, and we need to determine whether there is a mention of unforeseeable circumstances in the conversation. Depending on the source of the input, which can be either from a phone call or an online chat, unforseen circumstances is considered as two separate tasks. The source of phone call is referred to as UC-A (Automatic speech recognition), while the source of online chat is referred to as UC-I (Instant messaging).\nPoaching Guests (PG). Poaching guests refers to persuading or forcing guests to book hotels and pay bills through alternative channels. This is a binary classification problem where the input is a conversation, and we need to determine whether there is a mention of poaching guests in the conversation. Depending on the source of the input, which can be either from a phone call or an online chat, poaching guests is considered as two separate tasks. The source of phone call is referred to as PG-A (Automatic speech recognition), while the source of online chat is referred to as PG-I (Instant messaging).\nInsult Detection (ID). Insult detection is a binary classification task that determines whether a customer service representative is insulting the customer. The input for this task is the historical conversation between the customer and the customer service representative."}, {"title": "Complaint Sentiment Analysis (CSA)", "content": "Complaint sentiment analysis refers to analyzing whether a customer is likely to post negative feedback on public platforms. The input is the customer's historical conversations, and the output is a binary classification indicating whether the conversation is likely to result in negative publicity."}, {"title": "No Room upon check-in (NR)", "content": "No room upon check-in refers to determining whether a customer has encountered a situation where there is no available room upon their arrival at the hotel. The input is the customer's historical conversations, and the output is a binary classification. Depending on the source of the input, which can be either from a phone call or an online chat, no room upon check-in is considered as two separate tasks. The source of phone call is referred to as NR-A (Automatic speech recognition), while the source of online chat is referred to as NR-I (Instant messaging)."}, {"title": "Hotel Shuttle (HS)", "content": "Hotel shuttle is a binary classification task that determines whether a hotel provides shuttle service, where the input is the conversation between the guest and the hotel."}, {"title": "Invoice and Deposit Matters (IDM)", "content": "Invoice and deposit issues matters is a binary classification task. The input for this task is the conversation between the guest and the output is a binary classification indicating whether the guest requires an invoice or not."}, {"title": "Customer Service Quality Rating (CSQR)", "content": "Customer service quality rating task involves evaluating the caliber of service provided during customer interactions. For this purpose, the input data comprises historical conversations between customer service agents and their clients. The task's output is categorized into four distinct levels, numbered from 1 to 4."}, {"title": "Scoring Extreme Emotion (SEE)", "content": "Scoring extreme emotion involves rating the level of customer agitation based on the dialogues. The resulting score ranges from 1 to 5, reflecting the intensity of their emotional state."}, {"title": "Review Text Classification (RTC)", "content": "Review text classification is a multi-label multi-class classification problem for categorizing reviews, where the input is the multi-lingual review texts and the output includes categories related to the review, such as hotel facilities, service attitude, etc."}, {"title": "Car Services Classification (CSC)", "content": "Car services classification is a multi-label multi-class classification task, where the input is the historical conversation of a customer when taking a taxi, and the output is the categories of taxi-related issues mentioned by the customer."}, {"title": "Email Categorization (EC)", "content": "Email categorization refers to classifying incoming emails based on their content. By categorizing the emails, they can be assigned to different business lines for processing. This is a multi-classification task where the input is the email content, and the output is the category of the email."}, {"title": "Conversation Summarization (CS)", "content": "In the task of conversation summarization, the input consists of the historical dialogues between customer and service agents, and the goal is to produce a concise summary."}, {"title": "CLUE Taxonomy Impact", "content": "For our CLUE dataset, we divided them into two combinations: single-sentence and sentence-pair classification, binary and multi-class classification. The single-sentence classification includes the CWSC, TNEWS, and IFLYTEK tasks, while"}, {"title": "Continual Pre-training", "content": "We continually pre-train the open-source foundation model on pre-processed domain-specific corpus. The following paragraphs illustrate the pre-training process, covering data sourcing, data processing, tokenization, and pre-training strategy."}, {"title": "Data sourcing", "content": "We have collected domain-specific and general data, and mixed them together to enhance the model's general and domain-specific knowledge. Specifically, in our domain, we collect proprietary data such as customer service training materials, introductions to tourist attractions and businesses, and domain-related dialogues. Additionally, we also sample partial data from WuDaoCorpora as general data to supplement general knowledge. This produces an approximately 150 GB collection of the pre-training corpus."}, {"title": "Data processing", "content": "We establish a comprehensive data processing pipeline to enhance pre-training data quality. This pipeline comprises four modules: document-wise filtering, line-wise corrections, exact deduplication, ML-based filtering, and fuzzy deduplication. Figure 3 outlines the full data processing pipeline. After cleaning the original data, we obtain approximately 20 billion tokens of the domain-specific corpus."}, {"title": "Tokenization", "content": "We add more domain-specific phrases as new tokens for faster training and inference. We utilize the Byte-Pair Encoding (BPE) algorithm implemented in Sentencepiece to train a domain-specific tokenizer with a vocabulary size of 13,000. We subsequently merge the domain-specific tokenizer into the original tokenizer by taking the union of their vocabularies. Specifically, the vocabulary size of the tokenizer has increased from 125,696 to 127,008. The compression rate in our domain-specific corpus has decreased from 0.6458 to 0.6104."}, {"title": "Pre-training strategy", "content": "We utilize the self-supervised learning approach, i.e. causal language modeling, to pre-train our model on the processed corpus. Causal language models refer to models that are trained to predict the next word in a sentence based on the preceding context, capable of capturing the causal relationships between words and generating coherent text. For efficiency, we utilize Megatron and DeepSpeed as foundational frameworks, and have integrated flash attention."}, {"title": "Sampling Strategies", "content": "qu = \\frac{{\\pi_l}^{\\frac{1}{\\tau}}}{\\sum_{l'\\in L} {\\pi_{l'}}^{\\frac{1}{\\tau}}}"}]}