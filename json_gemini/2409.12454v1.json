{"title": "FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling", "authors": ["Enze Shi", "Kui Zhao", "Qilong Yuan", "Jiaqi Wang", "Huawen Hu", "Sigang Yu", "Shu Zhang"], "abstract": "Electroencephalography (EEG) is a vital tool to measure and record brain activity in neuroscience and clinical applications, yet its potential is constrained by signal heterogeneity, low signal-to-noise ratios, and limited labeled datasets. In this paper, we propose FoME (Foundation Model for EEG), a novel approach using adaptive temporal-lateral attention scaling to address above-mentioned challenges. FoME is pre-trained on a diverse 1.7TB dataset of scalp and intracranial EEG recordings, comprising 745M parameters trained for 1,096k steps. Our model introduces two key innovations: a time-frequency fusion embedding technique and an adaptive time-lateral attention scaling (ATLAS) mechanism. These components synergistically capture complex temporal and spectral EEG dynamics, enabling FoME to adapt to varying patterns across diverse data streams and facilitate robust multi-channel modeling. Evaluations across four downstream tasks demonstrate FoME's superior performance in classification and forecasting applications, consistently achieving state-of-the-art results. To conclude, FoME establishes a new paradigm for EEG analysis, offering a versatile foundation that advances brain-computer interfaces, clinical diagnostics, and cognitive research across neuroscience and related fields.", "sections": [{"title": "I. INTRODUCTION", "content": "EG (electroencephalography), as a cost-effective neuro- physiological monitoring technique, holds significant re- search and application potential across cognitive neuroscience, psychiatry, and clinical diagnostics [1]. Its high temporal resolution enables near real-time reflection of dynamic brain patterns, offering valuable insights into cerebral mechanisms [2]. Both invasive and non-invasive EEG acquisition methods have found widespread application in brain-computer inter- faces [3], sleep monitoring [4], epilepsy detection [5], and emotion recognition [6], among other domains.\nExtant research has primarily focused on addressing specific tasks, such as the MEET model for emotion decoding [7], EEG Conformer for motor imagery classification [8], and SEEG-Net for epileptic seizure classification [9]. While these studies have yielded substantial progress, they typically require de novo model training and often rely on large volumes of labeled data, rendering large-scale signal annotation financially impractical [10]. Moreover, the inherent heterogeneity and low signal- to-noise ratio of EEG pose challenges to model generaliza- tion and transfer capabilities, impeding further exploration of EEG's potential.\nThe success of foundation models in natural language processing and computer vision domains has presented a compelling paradigm: self-supervised training on vast datasets to learn abstract representations, followed by fine-tuning with limited labeled data [11]\u2013[13]. This approach offers a powerful solution to the aforementioned limitations in EEG research, underscoring the urgent need for an EEG foundation model. However, developing an EEG foundation model from scratch presents several challenges:\nFirst of all, different studies often employ diverse acquisi- tion systems, resulting in significant differences in sampling rates, electrode positions, and quantities. Despite the stan- dardization efforts of the International 10-20 system, strict standardization of EEG acquisition protocols is still difficult to achieve due to variations in experimental personnel, environ- ments, and equipment. For instance, the DEAP dataset uses a 512 Hz sampling rate with 32 active AgCl electrodes arranged according to the 10-20 system [14], while a visual object recognition dataset employs a 64-channel EASYCAP system with a 1000 Hz sampling rate [15]. These differences pose challenges in the data processing phase of EEG foundation model construction.\nSecondly, EEG signals exhibit distinct physiological char- acteristics across various scenarios. In motor imagery tasks, for example, the temporal features of EEG signals may be highly task-correlated due to short duration [16]. In contrast, epilepsy detection requires modeling of long-term temporal signals, potentially confounded by limb movement artifacts [17]. In emotion analysis or sleep monitoring tasks, specific frequency band information may be more conducive to model discrimination [18]. Optimally leveraging both temporal and spectral information is thus crucial.\nFinally, the brain's role as the neural hub controlling and reflecting various behavioral and physiological indices necessitates a model with multifaceted capabilities. These include: (1) comprehensive EEG modeling to capture signal periodicity and trends, enabling both short-term and long-term sequence forecasting; (2) serve as a component in various classification or detection tasks (e.g., epilepsy detection, sleep quality monitoring, emotion classification); and (3) out-of-the-box functionality, unrestricted by specific datasets or tasks, achieving high performance through simple fine-tuning and ideally demonstrating zero-shot capabilities.\nTo address these challenges, we propose FoME: a foun- dation model for EEG using adaptive temporal-lateral atten- tion scaling. This model aims to bridge the existing gap by"}, {"title": "II. RELATED WORK", "content": "The development of large-scale, generalizable models for brain signals, particularly EEG data, has been a burgeoning area of research in recent years. Our work on the FoME builds upon advancements in self-supervised learning, transfer learning across datasets, and the emerging field of \"brain foundation models\".\nA. Self-supervised Pre-training for EEG Signals\nThe remarkable success of self-supervised pre-training tech- niques in natural language processing and computer vision has motivated their exploration in the domain of brain signal anal- ysis [19]. These methods leverage the abundance of unlabeled data to learn robust and transferable representations, which can then be fine-tuned for specific downstream tasks.\nIn the EEG domain, several studies have investigated self- supervised learning approaches. BENDR [20] adapted the Wav2Vec 2.0 framework [21] to learn compressed representa- tions of raw EEG signals using contrastive learning. Banville et al. explored temporal context prediction and contrastive predictive coding as self-supervised pretext tasks for EEG representation learning, demonstrating their effectiveness for sleep staging and pathology detection [22]. ContraWR [23] used global statistics to distinguish EEG signals associated with different sleep stages through a contrastive learning approach. While these studies have shown the promise of self-supervised learning for EEG data, they have primarily focused on specific BCI tasks or have been limited to small-scale datasets and models. The potential of self-supervised pre- training on large-scale, diverse EEG datasets remains largely unexplored, which is a key focus of our work on the FOME model.\nB. Transfer Learning for Diverse EEG Datasets\nA key challenge in EEG research is the diversity of datasets, which can vary significantly in terms of electrode configurations, sampling rates, and experimental protocols. Addressing this heterogeneity is crucial for developing robust and generalizable models.\nRecent studies have explored transfer learning techniques to bridge the gap between disparate EEG datasets. Han et al. combined graph neural networks and transfer learning to perform motor imagery EEG decoding across datasets with different electrode montages [24]. Gu et al. proposed a two- network architecture that could learn from both shared and complete channel information, enabling coherent performance improvements when transferring between datasets [25]. MMM introduced a pre-training framework that learns topology- agnostic representations, enabling the model to handle EEG data from different electrode configurations [6].\nC. Emergence of \u201cBrain Foundation Models\u201d\nThe concept of \"foundation models\", which has gained significant traction in natural language processing and com- puter vision, is now being explored in the domain of brain signal analysis. These large-scale, pre-trained models can be efficiently fine-tuned for a variety of downstream tasks, promising increased efficiency and generalization compared to task-specific models.\nIn the context of intracranial recordings, Wang et al. pro- posed BrainBERT, an embeddings-based model for stereo- electroencephalographic (SEEG) data [26]. BrainBERT uti- lizes self-supervised pre-training on SEEG spectrograms to generate versatile representations. Zhang et al. introduced Brant, a large-scale foundation model specifically designed"}, {"title": "III. METHOD", "content": "In this section, we provide a comprehensive overview of the proposed FoME architecture and pipeline in Section III-A, followed by a detailed description of the model's key designs in Section III-B. Subsequently, Sections III-C and III-D elabo- rate on FoME's pre-training strategy, leveraging a vast corpus of heterogeneous datasets, and its fine-tuning approaches for diverse downstream tasks.\nA. Overview\nAs illustrated in Fig. 2-A, the model processes raw EEG signals from diverse datasets, which are preprocessed and segmented into non-overlapping patches. These patches are sequentially fed into FoME, which comprises three primary components: a time-frequency fusion module, a temporal encoder, and an adaptive multi-channel encoder. The time- frequency fusion module integrates temporal, frequency, and positional information of each patch into a unified embedding. The temporal encoder captures temporal dependencies along the time dimension, while the adaptive multi-channel encoder learns latent associations between different channels, model- ing a global channel attention network. These components collaboratively process raw EEG data and learn meaningful representations that capture both the temporal dynamics and spatial relationships inherent in brain activity.\nThe model adopts a self-supervised learning paradigm, employing a masked signal reconstruction task to train on extensive unlabeled EEG data. As depicted in Fig. 2-B, the multi-channel signal sequence is partitioned into fixed-length subsequences, termed patches. We replace the embeddings of randomly selected channel and positional patches with learnable mask embeddings [MASK]. The FoME backbone then reconstructs the original input signal sequence through a lightweight linear reconstruction head. Fig. 2-C illustrates the fine-tuning methodology and broad applications of the model for various downstream tasks. By replacing the corresponding output projection head according to the specific task and signal input, FoME can be adapted for EEG signal classification,"}, {"title": "B. Foundation Model Architecture", "content": "1) Patching: Prior to feeding data into the model, we segment the continuous EEG signals into non-overlapping patches. This approach effectively addresses the challenges associated with processing long, continuous EEG recordings while preserving the ability to capture both local and global temporal patterns. Consequently, the model can process EEG data at multiple scales. Specifically, the preprocessed EEG and iEEG data are represented as $X_i \\in \\mathbb{R}^{(N\\times C \\times T)}, i \\in \\{1,2, ..., M\\}$, where M denotes the total number of datasets, N is the number of samples, C is the number of channels (electrodes), and T is the total number of time points. We divide the continuous EEG signals into non-overlapping seg- ments of length L, resulting in patches: $P_i \\in [\\mathbb{R}^{(C\\times(N\\times F)\\times L)}$, where $F = [T/L]$ represents the number of patches obtained from each sample.\n2) Time-Frequency Fusion Encoding: It is designed to comprehensively capture both temporal and spectral charac- teristics inherent in EEG signals. To achieve this, we integrate time-domain and frequency-domain features into a unified representation. As illustrated in Fig. 3, in addition to segment- ing the raw signal, we also extract the power density value corresponding to each frequency and calculated the power sum within the frequency band according to the preset n frequency band intervals $[f_i, f_{i+1}] (i \\in \\{1, 2, ..., M\\}$, where 0Hz < $f_i$ < Nyquist frequency) [29]. Inspired by [30] and [27], we divide the frequency spectrum into eight bands: $\\delta$ (1-4Hz), $\\theta$ (4-8Hz), $\\alpha$ (8-13Hz), $\\beta$ (13-30Hz), $\\gamma_1$ (30-50Hz), $\\gamma_2$ (50-70Hz), $\\gamma_3$ (70-90Hz), and $\\gamma_4$ (90-100Hz). The detailed calculation process is as follows:\nFor a given patch $P(t)$, we first apply the Fourier transform [31] to convert the time-domain signal into the frequency domain. The power spectral density, denoted as $P(f)$, is then calculated by squaring the magnitude of the frequency components:\n$\\begin{equation} P(f) = \\left|\\mathcal{F}\\{P(t)\\}\\right|^2 = \\frac{1}{T} \\left| \\int_{-T}^{T} P(t)e^{-j2\\pi ft} dt\\right|^2 \\end{equation}$\nwhere f is the frequency, T is the total duration of the signal segment, and F is the Fourier transform. Subsequently, we compute the sum of power within each sub-band $[f_i, f_{i+1}]$ and apply a logarithm to obtain Powerband. This normalization compresses the numerical range and promotes a more normal distribution of power values, facilitating model learning:\n$\\begin{equation} Power_{band} = log_{10}(\\sum_{f=f_i}^{f_{i+1}} P(f) + 1) \\end{equation}$\nThe $Power_{band}$ is then passed through a Softmax function and a linear embedding to obtain the frequency embedding $E_{freq}$:\n$\\begin{equation} E_{freq} = Linear(Softmax(Power_{band})). \\end{equation}$\nConcurrently, we employ a linear encoder to map the patches to a latent space, resulting in the embedding $E_{patch}$. And a learnable positional embedding vector $E_{pos}$ is intro- duced. The final input encoding $E_{input}$ is obtained as:\n$\\begin{equation} E_{input} = E_{patch} + E_{freq} + E_{pos} \\quad \\quad E_{patch}, E_{freq}, E_{pos} \\in \\mathbb{R}^{C\\times P\\times D} \\end{equation}$\nwhere P is the number of patches and D is the model dimension.\n3) Temporal Encoder: The Temporal Encoder is designed to capture long-range dependencies and intricate temporal pat- terns within EEG data. Leveraging the success of transformer architectures in sequential modeling, this component is tailored to accommodate the unique characteristics of EEG signals. As depicted in Fig. 3, the temporal encoder consists of a"}, {"title": "4) Adaptive Multi-channel Encoder", "content": "The adaptive multi- channel encoder is specifically designed to capture spatial relationships and inter-channel dynamics within EEG data. Recognizing the intricate interactions between different brain regions represented by distinct EEG channels, this encoder adapts to learn channel-specific characteristics and their inter- dependencies. The architecture of the adaptive multi-channel encoder mirrors that of the Temporal Encoder, as illustrated in Fig. 3. Given the embedding $E_{time} \\in \\mathbb{R}^{P\\times D}$ from the previous step, we decouple the channel dimension from the sample dimension, resulting in $E_{time\\_dec} \\in \\mathbb{R}^{C\\times P\\times D}$. The data is then rearranged, and a trainable linear transformation is applied along the channel dimension to obtain the query, key, and value matrices for each patch, denoted as $Q_{channel}, K_{channel}$, and $V_{channel} \\in \\mathbb{R}^{C\\times D}$. Finally, we compute the attention score $Attn_{channel}$, which involves the interactions between channels and represents the spatial dynamics of the brain:\n$\\begin{equation} Attn_{channel} = Softmax(Q_{channel} (K_{channel})^T/\\sqrt{D}) V_{channel} \\end{equation}$\nIn conclusion, heterogeneous data provide spatio-temporal characteristics of different structures contained in the signals of different tasks. By integrating time-frequency fusion, tem- poral encoding, and adaptive multi-channel processing, FoME effectively captures the complexity and multiscale properties of brain activity as reflected in EEG signals."}, {"title": "C. Pre-training Using Heterogeneous EEG Datasets", "content": "1) Preprocessing: Given the heterogeneity of EEG datasets, characterized by varying data structures, sampling rates, and electrode configurations, even within the same dataset, data from different subjects may be recorded using different acquisition devices. Thanks to FoME's robust support for heterogeneous data, we can simply partition data based on their structural differences, ensuring that signals within each training group exhibit consistent characteristics. The specific preprocessing pipeline is as follows:\nFirstly, a notch filter is applied at 50 Hz or 60 Hz to eliminate power line interference. Subsequently, a band-pass filter is applied between 0.5 Hz and 100.5 Hz to remove high- frequency noise. The data is then resampled to 250 Hz to enhance computational efficiency. To eliminate any linear or constant offsets that might bias the analysis, a detrending pro- cess is applied. Finally, the continuous EEG data is segmented into non-overlapping windows, each containing 1500 time points (6s). Exponential moving standardization is applied to each window, normalizing the signal by removing the moving average and scaling by the moving standard deviation. For the given EEG data $X \\in \\mathbb{R}^{C\\times T}$, the calculation formulas for the Exponential Moving Average (EMA) and the Exponential Moving Standard Deviation (ESD) are as follows:\n$\\begin{equation} EMA_t = \\alpha \\cdot X_t + (1 - \\alpha) \\cdot EMA_{t-1} \\end{equation}$\n$\\begin{equation} ESD_t = \\sqrt{\\alpha \\cdot (X_t - EMA_t)^2 + (1 - \\alpha) \\cdot ESD_{t-1}^2} \\end{equation}$\n$\\begin{equation} X' = \\frac{X - EMA}{ESD + \\epsilon} \\end{equation}$\nwhere $\\alpha$ is the smoothing factor, $X_t$ is the signal value at the time point t, and $X'$ is the normalized data.\n2) Pre-training Process: Inspired by [19], [32], [33], we employ a masked signal reconstruction task for FoME pre- training, as illustrated in Fig. 2-B. After preprocessing all datasets involved in pre-training, we further organize the data into multiple signal blocks with approximately equal data vol- umes. The model treats each signal block as the minimal input unit. This enables cyclical pre-training on multiple datasets. We begin by segmenting the input signal into patches and randomly replacing 40% of the patches with learnable mask embeddings [MASK]. The entire dataset is then fed into FoME to learn latent signal representations, and a linear mapping is used to reconstruct the original input sequence. The mean squared error between the original and reconstructed signals serves as the final pre-training loss."}, {"title": "D. Fine-tuning on Downstream Tasks", "content": "FOME can be seamlessly applied to various EEG analysis tasks, as depicted in Fig. 2-C. In this paper, we demonstrate its utility on four practical tasks: classification, short-term forecasting, long-term forecasting, and imputation. For clas- sification, we replace the reconstruction head with a classifi- cation head, using a Multi-Layer Perceptron (MLP) to reduce the dimensionality of the FoME output embeddings three times before applying softmax to obtain multi-class labels. For prediction tasks with a horizon of H, we replace the reconstruction head with a forecasting head, flattening the N-dimensional embeddings of size D output by FoME into an NxD dimensional vector, and then projecting it onto an H-dimensional signal sequence using a linear projection. For imputation tasks, we retain the linear reconstruction head."}, {"title": "IV. EXPERIMENTS", "content": "A. Pre-training Setup\n1) Model Variants: We developed two configurations of FOME: FOME-Base and FoME-Large. The Base variant con- tains 476.3 million parameters, while the Large variant com- prises 744.8 million parameters. Both configurations incorpo- rate 12 temporal encoder layers and 4 adaptive multi-channel encoder layers. The primary distinction lies in the hidden layer dimensions: FoME-Base employs a feedforward network (FFN) with a dimension of 3072, whereas FoME-Large uses an FFN dimension of 7168. In accordance with established scal- ing laws for large models [34], we initially evaluated FoME- Base on smaller-scale datasets to extrapolate the performance of FoME-Large on larger datasets. This approach facilitated the preliminary estimation of training outcomes and informed the selection of hyperparameters and training strategies. Unless otherwise specified, all evaluation results presented in this paper are from FoME-Large.\n2) Datasets: The pre-training corpus for FoME was derived from eight diverse datasets: TUEG [35], SEED [36], SEED- IV [18], CHB-MIT [37], [38], Sleep-EDFx [39], MI-Dataset [40], MAYO [41], and FNUSA [41]. Among these, MAYO and FNUSA contain invasive EEG signals, while the remainder comprise scalp EEG recordings. The aggregate dataset encom- passes over 30,000 recordings from more than 15,000 subjects, totaling approximately 26,000 hours of data and exceeding 1.7TB in volume. The six scalp EEG datasets exhibit sampling rates ranging from 100 Hz to 1024 Hz and incorporate over 40 distinct electrode configurations, with channel counts varying from 3 to 64. The two invasive EEG datasets feature sampling rates of 32 kHz (MAYO: 16-76 channels) and 25 kHz (FNUSA: 192 channels), respectively. The age distribution of subjects spans from 1 to 90 years. To construct the pre-training corpus, we sampled varying proportions of data from each dataset based on task requirements and data volume: 100% of TUEG, 80% of SEED, CHB-MIT, Sleep-EDFx, and MI- Dataset, and 50% of MAYO and FNUSA. The remaining data were reserved for downstream signal forecasting tasks.\n3) Pre-training Methodology: We concatenated contiguous signal segments from the pre-training data to form signal blocks ranging from 5 to 10 GB in size. Following prepro- cessing and signal segmentation, each signal block served as an input unit for the model, with one complete cycle through all blocks constituting an epoch. During pre-training, each input sample comprised 15 patches, each containing 1500 time points (6s). 40% of these patches were randomly replaced with learnable mask embeddings [MASK]. We utilized a minibatch size of 12 input samples. FoME was optimized using the AdamW optimizer with $\u03b2\u2081 = 0.9, \u03b22 = 0.99$, and eps = 10-6. A weight decay of 1\u00d710-2 was applied to stabilize the training process and mitigate overfitting. The learning rate schedule incorporated an initial rate of 2 \u00d7 10-6, which was linearly increased to a peak of 5\u00d710\u22125 over 10,960 steps, followed by a cosine decay to 5 \u00d7 10-9 over 1,096k steps. We employed gradient accumulation to simulate larger batch sizes, updating parameters every four steps. FoME was pre-trained for 350 hours on 6 NVIDIA RTX 4090 GPUs (24GB each)."}, {"title": "B. Downstream Tasks Setup", "content": "To assess the generalizability of FoME, we conducted extensive experiments across four critical downstream tasks: seizure classification, seizure detection, emotion recognition, sleep stage classification, short-term forecasting, long-term forecasting, and signal imputation.\n1) Classification: Electroencephalogram (EEG) signal clas- sification plays a crucial role in identification and medical diagnostics. We focused on four primary classification tasks: a) Seizure detection is the most commonly used diagnostic and treatment method, which involves identifying seizure events from continuous patient recordings. We utilized physiologi- cal and pathological signals extracted from the MAYO and FNUSA datasets for this binary classification task. b) Seizure classification: After removing power line interference, we con- ducted a three-category classification (physiological, patho- logical, and artifact) using the complete MAYO and FNUSA datasets. c) Emotion recognition, which aids researchers in understanding brain dynamics, was performed using the SEED dataset for a three-category classification (positive, negative, and neutral). While the SEED dataset provides preprocessed high-quality signal features (e.g., differential entropy, DCAU). Most of the existing work tends to use these provided well-featured data for classification tasks, which limits further ex- ploration of emotional data. While extracting good frequency- domain features can lead to improved classification perfor- mance, the utilization of temporal signals is equally important, and, signal data has significant advantages in real-time tasks; therefore, instead of using the preprocessed features provided by the authors of the dataset, we conducted comparative experiments using raw EEG signals to follow the necessary preprocessing procedure adopted in this paper, so as to provide a fair picture of the performance of the various types of models on the true performance on emotional data. d) Sleep stage identification, crucial for preventing and diagnosing related disorders, was conducted using the SleepEDFx dataset for binary classification. All classification tasks maintained con- sistent preprocessing methods with the pre-training phase and employed a 6:2:2 ratio for train-validation-test set partitioning. Evaluation metrics included accuracy, precision, recall, F1 score, and F2 score."}, {"title": "2) Forecasting", "content": "EEG signals present unique challenges for forecasting due to their low amplitude (microvolts), complex- ity, randomness, and transient nature. Despite these difficulties, accurate EEG prediction is crucial for various clinical appli- cations, including disease diagnosis. We conducted compre- hensive comparative tests for both short-term and long-term forecasting tasks using the MAYO and FNUSA datasets. For a signal X = $[x_1,..., x_L]$ where $x_i \u2208 R$, the objective was to predict the subsequent T time steps $[x_{L+1},..., x_{L+T}]$. The past signal length was set to 15 patches (90s) for both tasks, with short-term prediction requiring forecasting of two future patches (12 seconds) and long-term prediction requiring five future patches (30 seconds). Datasets were split into 6:2:2 ratios for training, validation, and testing. Evaluation metrics included Mean Absolute Error (MAE) and Mean Squared Error (MSE)."}, {"title": "3) Imputation", "content": "EEG signal imputation is essential in real- world applications where data loss can occur due to various factors such as subject movement or equipment malfunction. For a signal X = $[x_1,...,x_L]$ and mask M = $[m_1,...,m_L]$, where $m_i$ = 0 if $x_i$ is missing and $m_i$ = 1 if $x_i$ exists, the imputation task involves predicting missing values using existing ones. We considered a patch as valid data only when all values within it existed. For other patches, we employed a learnable mask embedding [MASK] and utilized FoME's default reconstruction head for prediction. The MAYO and FNUSA datasets were split into 6:2:2 ratios for training, validation, and testing to evaluate the model's imputation capabilities. Evaluation metrics included Mean Absolute Error (MAE) and Mean Squared Error (MSE)."}, {"title": "4) Baselines", "content": "We compared FoME with state-of-the-art deep learning models across various tasks. These included four Transformer-based pre-trained models: BrainBERT (2023) [26], Brant (2023) [27], Neuro-GPT (2024) [42], and LaBraM (2024) [28]; the classic RNN-based model LSTM (1997) [43]; the CNN-based SOTA model ConvNeXt (2022) [44]; and two advanced temporal models: PatchTST (2023) [32] and TimesNet (2023) [45]. For Neuro-GPT and LaBraM, which provided pre-trained parameters, we conducted both probe and fine-tuning experiments to thoroughly validate their performance. In total, we reproduced and tested 8 baseline models for comprehensive comparison."}, {"title": "C. Evaluation Results", "content": "Fig. 4 presents the average performance of FoME and the baseline methods across various downstream tasks. It is evident that FoME outperforms existing pre-trained EEG models and time series models on all tasks, with particularly pronounced advantages in signal forecasting and imputation. These results underscore the effectiveness of our pre-training strategy and the model's robust generalization capabilities. In the following sections, we delve deeper into the experimental results, providing detailed analyses for each of the three primary task categories.\n1) Classification Task :\na) Seizure Classification & Seizure Detection: Table II illustrates the classification performance of FoME and baseline models on epilepsy-related tasks. Given the importance of recall in tasks where false negatives carry high costs, such"}, {"title": "b) Emotion Classification & Sleep Stage Classification", "content": "Table III presents the three-category classification performance based on the SEED dataset. The results indicate that emotion recognition using minimally processed temporal data remains challenging. In this context, our proposed FoME outperformed other models across multiple metrics, including accuracy and precision. This superiority suggests that the adaptive temporal- lateral attention scaling mechanism effectively captures subtle emotional states in EEG signals.\nTable IV displays the results of sleep stage classification tests. In contrast to the SEED dataset, the SleepEDFx dataset presented a lower level of difficulty, with most models achiev- ing high classification efficiency. Notably, TimesNet exhibited exceptional performance on this dataset, potentially due to its compatibility with the two-electrode signal structure, which aligns well with the single-channel data typically emphasized in temporal models."}, {"title": "2) Signal Forecasting Task", "content": "Table V presents the results of short-term and long-term EEG signal forecasting tasks. FoME consistently outperformed other models across all tasks, further validating its efficient modeling of signal sequences and demonstrating precise prediction of future signal trends. This capability has multifaceted significance: In the medical domain, accurate prediction of EEG trends can facilitate timely warnings in the critical pre-ictal phase of epileptic seizures. In neuroscience research, precise EEG signal prediction can deepen scientists' understanding of brain mechanisms and neuronal activity patterns, enabling further exploration of information processing modalities and neuronal interactions across various brain states."}, {"title": "3) Imputation Task", "content": "The results of EEG signal imputa- tion tests are presented in Table V. FoME exhibited excep- tional performance in this task, significantly outperforming all other models. Compared to the current state-of-the-art model, PatchTST, FOME achieved an average loss reduction of 0.1488, underscoring its capacity for accurate reconstruction of missing data points."}, {"title": "D. Ablation Study", "content": "We strongly believe that FoME's superior performance stems from our proposed time-frequency fusion encoding and adaptive temporal-lateral attention scaling. To validate the ef- ficacy of these modules, we implemented the following FoME variants: 1) FoME without time-frequency fusion: This variant omits the encoding and integration of frequency information, relying solely on temporal embedding and positional encoding for token generation. 2) FoME without temporal encoder: Re- moval of the temporal encoder diminishes the model's compat- ibility with data featuring fewer electrodes. 3) FoME without multi-channel encoder: Removal of the adaptive multi-channel encoder results in a loss of spatial modeling capabilities. 4) FoME with CNN module replacing linear embedding: This variant substitutes the linear embedding with a CNN module. We conducted experiments on the TUEV dataset to evaluate these four model variants. Their performance is illustrated in Fig. 6, demonstrating that all four components contribute to FoME-Base's performance enhancement. Notably, the removal of the temporal encoder had the most substantial impact on model performance, affirming its crucial role in signal modeling."}, {"title": "The configuration and modulation of learning rates are pivotal to model pre-training", "content": "Consequently, prior to large- scale training, we conducted a learning rate range test using FoME-Base to identify the optimal learning rate range for our data and model architecture. It is worth noting that larger datasets typically accommodate higher learning rates due to the availability of more samples for gradient stabilization. To minimize discrepancies between the test environment and the final training environment, we selected a substantial dataset of approximately 500 GB for this validation, completing 10 epochs of training. The experimental results, as depicted in Fig. 7-left, indicate that learning rates of 1.0 \u00d7 10-4 and 1.0 \u00d7 10-5 yielded the lowest loss values and fastest con- vergence rates. A clear downward trend persisted even after 10 epochs. Based on these findings, we selected 5.0 \u00d7 10-5 as the peak learning rate for large-scale training. The scheduling process is illustrated in Fig. 7-right."}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "A. Conclusion\nWe propose FoME, a novel EEG foundation model that addresses the longstanding challenges in EEG signal analy- sis through the implementation of adaptive temporal-lateral attention scaling. FoME leverages self-supervised learning on a massive and diverse dataset exceeding 1.7TB, encompass- ing both scalp and intracranial recordings. This pre-training empowers FoME to capture intricate dynamics inherent in EEG signals, facilitating robust and generalizable performance across various downstream tasks. The key innovations of FoME are the time-frequency fusion embedding and the adap- tive time-lateral attention scaling (ATLAS) mechanism, which have proven instrumental in capturing the complex temporal and spectral dynamics inherent in EEG signals. These compo- nents synergistically enable FoME to adapt to varying patterns across diverse data streams and facilitate robust multi-channel modeling, addressing the heterogeneity challenge that has long plagued EEG analysis. Our comprehensive evaluations across four downstream tasks, including classification, short-term forecasting, long-term forecasting, and signal imputation, con- sistently demonstrate FoME's superior performance compared to state-of-the-art models. Notably, FoME's exceptional capa- bilities in signal forecasting underscore its potential for critical applications such as early seizure detection and advanced brain-computer interfaces. FoME represents a significant ad- vancement in EEG analysis, offering a versatile foundation that transcends the limitations of task-specific models. By enabling effective transfer learning and potentially zero-shot capabilities, FoME paves the way for more accessible and powerful EEG-based applications across neuroscience, clinical diagnostics, and brain-computer interfaces.\nB. Limitation\nDespite FoME's significant advancements in EEG analysis, several limitations warrant consideration. The pre-training dataset, while substantial, underrepresents certain EEG modal- ities such as emotion-related, sleep, and motor imagery data, and lacks representation from cognitive domains like working memory tasks. Additionally, the underutilization of invasive EEG data, which typically offers higher signal-to-noise ratios, presents an opportunity for improvement. The current scale of FoME (745M parameters) is constrained by the available pre-training data, suggesting potential for further scaling as the dataset expands. Moreover, while FoME excels in short- term signal prediction, its capabilities for long-term forecasting remain unexplored.\nFuture research directions should focus on addressing these limitations. Expanding and diversifying the pre-training dataset to include a wider range of EEG modalities and cognitive tasks is crucial. Furthermore, refining preprocessing strategies specific to EEG signals and exploring more sophisticated pre-training techniques, including the integration of decoder archi- tectures, could potentially improve the model's generalization capabilities and downstream task performance. Finally, future iterations should explore methods to increase the input se- quence length beyond the current 90-second limit, potentially improving performance on tasks requiring extended temporal context."}]}