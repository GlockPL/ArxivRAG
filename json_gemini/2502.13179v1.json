{"title": "PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models", "authors": ["Jiaqi Zhao", "Miao Zhang", "Ming Wang", "Yuzhang Shang", "Kaihao Zhang", "Weili Guan", "Yaowei Wang", "Min Zhang"], "abstract": "Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as LLaMA (Touvron et al., 2023a,b) and GPT (Brown et al., 2020; Ouyang et al., 2022) have demonstrated remarkable success in various natural language processing tasks. However, their colossal numbers of parameters bring tremendous storage and inference overheads. To alleviate the challenge, numerous model compression methods have been proposed such as quantization (Liu et al., 2022; Huang et al., 2019), pruning (Frantar and Alistarh, 2023; Ma et al., 2023) and knowledge-distillation (Gou et al., 2021; Tunstall et al., 2023). Among these works, post-training quantization (PTQ) methods (Yuan et al., 2023; Wei et al., 2022) have garnered particular attention for LLMs due to their computational efficiency compared to quantization-aware training (QAT) (Liu et al., 2023b; Esser et al., 2019) and other compression methods. Although maintaining nearly lossless performance at 4-bit or 8-bit, most existing state-of-the-art PTQ approaches fail when attempting to quantize weights to extremely low bit-width, i.e., 1-bit or sub 2-bit.\nPB-LLM (Shang et al., 2023) and BiLLM (Huang et al., 2024) are two most recent sub 2-bit PTQ methods for LLMs. They selectively preserve a portion of salient weights at 8-bit or with fine processing while quantizing the remaining weights to 1-bit. Although they demonstrate promising results, they are plagued by two critical issues. Firstly and the most importantly, both methods introduce additional unstructured fine-grained masks to dis-"}, {"title": "2 Related Works", "content": "tinguish salient weights which requires additional 1-bit per weight to store the mask and leads the memory of the quantized model to exceeding 2-bit per weight, where PB-LLM with 2.7-bit and BiLLM with 2.1-bit respectively (see Appendix A). Secondly, they independently and analytically derive the row-wise scaling factors used for mitigating binarization magnitude errors (Rastegari et al., 2016), violating the fact that weights exhibit implicit row-wise dependencies (Clark et al., 2019; Vig and Belinkov, 2019) and angular biases (Lin et al., 2020).\nMotivated by issues above and to push the real limit of PTQ methods on extremely low-bit quantization, we propose an extremely low-bit (1.61-bit) PTQ method for LLMs called PTQ1.61. Specifically, to eliminate the significant additional memory consumption caused by unstructured fine-grained masks, we dissect the quantization error through mathematical derivation to identify the structural influencing factors within it, and find that the upper bound of quantization error is significantly affected by input activation channels. Based on this discovery we propose a one-dimensional structured mask to preserve corresponding salient channels in the weight matrix at 4-bit, and successfully reduce the extra bit-width for each weight from over 1-bit to a negligible extent (0.0002-bit). Additionally, in order to capture the implicit row-wise correlations and directional biases jointly, we introduce a novel efficient block-wise scaling factors optimization framework.\nIn addition, unlike previous studies which always take the pretrained model with the best performance as the starting point for quantization, we find that the weights distribution also immensely affects the quantization performance. Specifically, existing per-channel PTQ methods usually consider a row-wise quantization pattern that assigns the same quantization parameter to all weights in a channel, while the distribution of salient weights in the pretrained model is scattered, which leads to significant quantization errors. Motivated by this row-wise nature, we propose a novel preprocessing strategy for LLMs quantization, which first transforms the weight distribution into a row-wise pattern through a lightweight restorative LoRA alignment, so that the preprocessed model is more suitable for per-channel PTQ than pretrained model. The proposed preprocessing strategy can be also applied to other extremely low-bit PTQ methods with notable performance enhancement, as shown in Figure 5. We further discuss the differences and advantages of our preprocessing strategy from existing post-quantization parameter-efficient fine-tuning (PEFT) approaches (Dettmers et al., 2023) in Section 3.4 and Appendix D.\nWith these enhancements, PTQ1.61 effectively quantizes the weights to extremely low-bit with outstanding performance, as illustrated in Figure 1. Our key contributions can be summarized as:\n\u2022 To explore the real limitation of post-training quantization, we present an efficient extremely low-bit PTQ method for LLMs named PTQ1.61 which is the first PTQ study to truly reduce the effective bit-width of LLMs weights to sub 2-bit (1.61-bit) with acceptable performance degradation.\n\u2022 Different from leveraging memory-intolerable unstructured masks to preserve salient information, we propose a one-dimensional structured mask based on input activations to reduce the upper bound of quantization errors, which only introduces negligible 0.0002-bit for each weight.\n\u2022 We further present a novel efficient block-wise optimization strategy to learn scaling factors to further consider the implicit row-wise dependencies and angular biases.\n\u2022 We demonstrate that pretrained model is not amenable to per-channel PTQ and accordingly propose a quantization preprocessing paradigm based on restorative LoRA to transform salient weights as a row-wise pattern to further enhance the quantization performance."}, {"title": "2.1 Post-Training Quantization", "content": "Post-training quantization is an efficient and expeditious quantization approach which merely necessitates a limited amount of calibration data to statistically determine the quantization parameters that help to scale float values to low-bit. AdaRound (Nagel et al., 2020) analyzes the quantization errors and employs a layer-wise optimization approach to learn the optimal rounding mechanism. BrecQ (Li et al., 2021) divides the model weights into multiple blocks and independently quantizes each block, allowing for finer control over quantization errors."}, {"title": "2.2 Extremely Low-Bit Quantization", "content": "Extremely low-bit quantization refers to approaches where the effective bit-width for weights is sub 2-bit. It has been widely welcomed due to significant compression ratio but suffers from severe performance degradation. BNN (Courbariaux et al., 2016) is the first model binarization method and XNOR-Net (Rastegari et al., 2016) presents scaling factors which reduce binarization errors with acceptable additional memory cost. RBNN (Lin et al., 2020) indicates that except for magnitude gaps, angular biases ought to be considered so that extra rotation matrices are introduced to overcome the drawback.\nSeveral extremely low-bit QAT methods for LLMs (Xu et al., 2024; Wang et al., 2023; Ma et al., 2024) have been proposed recently. Regrettably, the immense computational resource consumption and the lack of open-source availability have hindered their widespread application so that there is a growing demand for more economical PTQ methods. PB-LLM (Shang et al., 2023) investigate the importance of salient weights and design extra 1-bit unstructured masks to retain them into 8-bit while binarizing the others. BiLLM (Huang et al., 2024) further presents finer-grained masks to divide into multi-groups for binarization using different scaling factors. However, the fine-grained masks that cannot be compressed in both methods results in the equivalent bitwidths exceeding 2 bits. To make contributions for truly extremely low-bit PTQ research, we propose PTQ1.61 which addresses the issues above and obtains promising performance."}, {"title": "3 PTQ1.61", "content": "In this section, we provide a detailed introduction to our PTQ1.61, an extremely low-bit PTQ method for LLMs as demonstrated in Figure 2. We begin with briefly reviewing the basic concepts of model quantization and binarization in Section 3.1. Subsequently, to preserve salient information while avoiding insufferable memory overheads brought by unstructured masks in previous methods, we analyze the impact factors on quantization errors and then devise a one-dimensional mask based on input activations with negligible bit-width in Section 3.2. In Section 3.3, a novel block-wise optimization strategy is introduced for binarization to obtain optimal scaling factors considering implicit dependencies and angular biases. In Section 3.4, we explain why the pretrained model is not suitable for per-channel PTQ and how our proposed quantization preprocessing strategy works."}, {"title": "3.1 Preliminaries", "content": "Model Quantization Model quantization aims to convert float weights into corresponding low-bit integer forms thereby reducing computational and memory overheads. The quantization function can be elaborated as:\n$W_q = clamp( [\\frac{W}{S_q}] + Z_q, 0, 2^{b}-1), (1)$\nwhere $W \\in \\mathbb{R}^{n \\times m}$ and $W_q \\in \\mathbb{R}^{n \\times m}$ indicate full-precision and quantized weights respectively. [.] denotes round-to-nearest operator. $S_q$ is the quantization scalar and $Z_q$ represents zero-point.\nModel Binarization Binarization represents the most extreme form of quantization (1-bit) where weights are assigned as $\\pm1$ determined by the sign function. In more details:\n$sign(W) = \\begin{cases}\n+1, W \\geq 0,\\\\\n-1, W < 0.\\end{cases}$ $W_b = \\alpha sign(W), (2)$\nwhere $\\alpha$ denotes scaling factors commonly used in previous methods (Bulat and Tzimiropoulos, 2019; Xu et al., 2021; Liu et al., 2018; Shang et al., 2023; Huang et al., 2024) to reduce binarization errors and $W_b \\in \\mathbb{R}^{nxm}$ is binarized weights with scaling factor $\\alpha$. Assuming that weights in each row of W are independent, we define w as a row of weights with nw elements. The corresponding scaling factor $a_w$ can be derived analytically by $a_w = \\frac{||w||_1}{N_w}$"}, {"title": "3.2 Structured Mask", "content": "Following (Shang et al., 2023) and (Huang et al., 2024), we recognize that partially preserving higher-bit salient information is crucial for reducing quantization errors. However, their unstructured masks cannot be compressed, resulting in additional memory overheads due to the scattered nature of salient weights within the weight matrix. Hence, our objective is to identify factors that significantly impact quantization errors while maintaining some degree of regularity."}, {"title": "3.3 Block-wise Scaling Factors Optimization", "content": "For non-salient weights, we perform binarization following Equation (2). However, previous analytically derived scaling factors ignore implicit correlations among rows and directional shifts which cannot be accurately captured through mathematical derivation. To address this issue, we set scaling factors as learnable and propose a novel efficient block-wise optimization pipeline to learn them while considering implicit row-wise dependencies and angular biases, as demonstrated in the orange and green area of Figure 2.\nIn order to conduct an effective distance metric for optimization, we first consider MSE loss to reduce magnitude gaps. Then for angular biases described above, we take cosine similarity, a metric considering directional gaps, into account. We formulate the joint metric as follows:\n$E(f_1, f_2) = ||f_1 - f_2||^2 + D_{NLC}(f_1, f_2), (5)$\nwhere $E(\\cdot)$ is the distance metric for optimization. $f_1$ and $f_2$ are different features. $D_{NLC}(\\cdot)$ represents the negative logarithm of cosine similarity loss (Zhao et al., 2024), as given by:\n$D_{NLC}(f_1, f_2) = -log(C(f_1, f_2)), (6)$\nwhere $C(\\cdot)$ denotes cosine similarity. Followed by CBQ (Ding et al., 2023), our block-wise pipeline consists of two branches: the first branch aims at mitigating quantization error propagation and the second branch is tailored for quantifying the outputs distinction for the same inputs. Our final optimization objective is formulated as:\n$\\underset{\\alpha_s, \\alpha_r}{argmin}(E(F(X, W), F(X_q, W_q))+\nE(F(X_q, W), F(X_q, W_q))), (7)$\nwhere $\\alpha_s$ and $\\alpha_r$ are scaling factors for magnitude and angular biases, respectively. $W_q$ denotes the quantized weights (see Appendix C.2) and $F(\\cdot)$ represents the embedding function of a block. X indicates the input activation of the full-precision block while $X_q$ is that of the quantized block.\nWith novel optimization strategy, PTQ1.61 outperforms previous low-bit PTQ methods significantly. The contribution is assessed in Table 3 and Appendix C."}, {"title": "3.4 Quantization Preprocessing", "content": "It is well-known that the pretrained models exhibit the best performance so prior PTQ studies intuitively believe that quantizing the pretrained models should also yield optimal results. However, we discover that this notion is somewhat biased. As illustrated in Figure 4, we highlight the salient weights of a linear layer in LLMs based on magnitude-metric (Shang et al., 2023) where a scattered distribution pattern can be observed. The scattered pattern results in significant quantization errors when calculating row-wise scaling factors in per-channel quantization scheme. Therefore, we infer that under the premise of minimizing the bad impact on the pretrained model, transforming"}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to validate our novel extremely low-bit PTQ method PTQ1.61 on various benchmarks and LLMs with existing methods to demonstrate that our approach achieves outstanding performance under extremely challenging quantization."}, {"title": "4.1 Experimental Setup", "content": "Baseline Since our PTQ1.61 is an extremely low-bit PTQ method, we primarily choose PB-LLM (10% 8-bit) (Shang et al., 2023) and BiLLM (Huang et al., 2024), which claim to be extremely low-bit methods but actually have an equivalent bit-width larger than 2-bit, as baselines. Additionally, several state-of-the-art PTQ methods (2-bit) such as OmniQuant (Shao et al., 2023), AWQ (Lin et al., 2023), QuIP (Chee et al., 2024), and GPTQ (Frantar et al., 2022) are also be evaluated.\nModels We evaluate our method mainly on LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b) and LLaMA-3 (Dubey et al., 2024), for LLaMA-families are currently the most popular and widely applied among LLMs. Considering the comprehensiveness, experiments on OPT families (Zhang et al., 2022) are in Appendix D.\nTraining Details We initialize learnable scaling factors with $a_w = \\frac{||w||_1}{N_w}$ and AdamW optimizer (Loshchilov and Hutter, 2017) with zero weight decay is utilized to update them with learning rate 5e-4 and 1e-3. For PTQ, our calibration set sampled from WikiText2 (Merity et al., 2016) consists of 128 random 2048 token-segments and the block-wise training process includes 20 epochs with a batch size of 1. For quantization preprocessing, the number of steps and ranks in lightweight restorative LORA is 10K and 32 respectively. The entire process is deployed on 2 Nvidia A800 GPUs."}, {"title": "4.2 Experiments on Language Generation Tasks", "content": "The fundamental prowess of LLMs lies in their language generation capabilities. Consequently, evaluating such capabilities of a quantized model via perplexity serves as the core metric of a quantization method. As presented in Table 1, we compare the perplexities between our PTQ1.61 and other baselines to valid the effectiveness on extremely"}, {"title": "4.3 Experiments on Reasoning Benchmarks", "content": "Reasoning capability is becoming a crucial metric for evaluating PTQ approaches. The comparison results are indicated in Table 2, where our method exhibits superiority in most benchmarks. For instance, compared with the second best baseline, BiLLM, our method showcases an average performance increase of 1.58% ~ 5.92%. Particularly,"}, {"title": "4.4 Ablation Study", "content": "After demonstrating the advancement of our PTQ1.61, we conduct ablation study on LLaMA-13B to further validate the effectiveness of our each innovation as indicated in Table 3. As the first row, without any additional improvements, directly using the derived analytically scaling factors for binarization would almost entirely compromise the text generation capability of LLMs. When utilizing our structured masks to retain salient weights as the second row, there is a significant improvement, indicating the importance of salient weights and the effectiveness of our masks, but there remains considerable room for enhancement. Furthermore, our novel block-wise strategy for non-salient weights binarization lifts the performance to a excellent level as demonstrated by the forth row. Ultimately, the last row illustrates that the row-wise pattern obtained by our quantization preprocessing brings a remarkable enhancement. More detailed ablation results are available in Appendix B."}, {"title": "4.5 Quantization Preprocessing on Baselines", "content": "In addition to our PTQ1.61, we also employ the proposed quantization preprocessing scheme on other baselines to validate its scalability and the results can be found in Figure 5, which demonstrate that significant improvements appear in all baselines, especially for 2-bit GPTQ which completely collapses without preprocessing. With the effectiveness of our preprocessing scheme, future research can take a fresh perspective to focus on finding a more appropriately pretrained model."}, {"title": "5 Conclusion", "content": "In this paper, we explore the real limit of post-training quantization and propose an extremely low-bit PTQ approach namely PTQ1.61, which is truly the first PTQ method enables sub 2-bit quantization for LLMs. Firstly, one-dimensional structured mask with negligibly additional 0.0002-bit per weight is introduced to preserve salient weights. For non-salient weights binarization, we devise an efficient block-wise optimization strategy to learn scaling factors considering row correlations and angular biases. In addition to above contributions, we further propose a quantization preprocessing paradigm to transform the salient weights into a row-wise pattern which is able to alleviate the difficulty in per-channel quantization. Extensive ex-"}, {"title": "Limitation", "content": "Although showcasing superior performance, the preprocessing scheme still has limitations to be reckoned with, which requires more runtime to get a start point before quantization. For example, our runtime reaches 2h on LLaMA-7B, and fortunately, this falls within an acceptable range (OmniQuant reports 1.1h but exhibits worse performance and higher bit-width per weight). Considering that extremely low-bit quantization is the most challenging quantization scenario especially for PTQ, we believe it is worthwhile to sacrifice some computational resources within an acceptable range to pursue higher performance.\nIn addition, due to the limitation that commercial NVIDIA GPUs do not support such low-bit inference, and designing specific hardware requires larger research teams and financial support, we cannot provide real-world inference evaluation results yet. Our goal is to explore the performance limits of PTQ by fake-quantization before commercial hardware support is available. We believe this will eventually be realized as evidenced by the quick development of GPUs."}, {"title": "Ethics Statement", "content": "This paper introduces solutions to the challenges associated with Large Language Models (LLMs) quantization, with the overarching goal of facilitating the widespread adoption and application of LLMs. In the current landscape, ethical concerns tied to LLMs, including the presence of hidden biases encoded in the models, are garnering heightened attention. Following our investigation, we assert that our proposed method does not further amplify the biases and contravene any ethical standards."}, {"title": "Appendix", "content": ""}, {"title": "A Average Bit-width Per Weight of Linear Layer", "content": "For a weight in a mix-precision quantized linear layer, its average bit-width b is calculated by following formulation:\n$b = 1*r_b+b_{salient}*(1-r_b)+b_{index}+b_{additional}, (8)$\nwhere $r_b$ is the ratio of binarized weights in the layer and $b_{salient}$ denotes the bit-width of salient weights. The first two item is also called weight bit-width. $b_{index}$ represents the bit-width for index storing using the bitmap mechanism (Chan and Ioannidis, 1998) and $b_{additional}$ is used for saving quantization parameters, i.e., scaling factors.\nAssume the weight matrix is 4096 \u00d7 4096 in such layer. For our PTQ1.61 which saves 20% salient weights to 4-bit and binaries the others, the weight bit-width can be effortlessly determined as 1.6-bit and the overall bits number is 4096 \u00d7 4096 \u00d7 0.8 + 4096 \u00d7 4096 \u00d7 0.8 \u00d7 4 = 26,843,545. In addition, the shape of our 1-bit one-dimensional structured mask is 4096 \u00d7 1, so its $r_b$ is 1 \u00d7 4096\u00f7 26,843, 545 \u2248 0.0002. Moreover, quantization parameters in our method contains 3 low-dimensional scaling factors and a part of zero-points, so $b_{additional}$ will be (3 \u00d7 4096 \u00d7 1 \u00d7 16 +0.2 \u00d7 4096\u00d716) \u00f7 26, 843, 545 \u2248 0.008. Overall, the average bit-width per weight in a layer quantized by our PTQ1.61 is $b$ = 1.6 + 0.0002 + 0.008 \u2248 1.61.\nFor PB-LLM which selects 10% salient weights at 8-bit using a 1-bit fine-grained unstructured mask with the same shape as the weight matrix, the obtained average bit-width per weight is $b$ = 0.1 x 8 +0.9 \u00d7 1 + 1 = 2.7.\nBiLLM devises a finer-grained binarization scheme which divides all weights into 3 groups and calculates group-wise scaling factors. Specifically, they propose a structured mask based on Hessian for salient weights and an unstructured mask based magnitude for unsalient weights. From their paper we get that their weight bit-width is 1-bit and $b_{additonal}$ is 0.1-bit then we have $b$ = 1.0 +0.1+ 1.0 = 2.1."}, {"title": "B Structured Mask", "content": ""}, {"title": "B.1 The Impact of Salient Channels Ratio in Proposed Structured Mask", "content": "Inspired by PB-LLM which declares the importance of salient weights, we devise a one-dimensional structured mask to preserve top 20% salient channels of weight matrices at 4-bit based on input activations in our PTQ1.61. More comprehensively, we delve into the effects of the salient channels ratio on the quantized model. As illustrated in Figure 6, we compare the performance of PTQ1.61 on pretrained LLaMA-7B with different ratios and the results indicate that higher salient ratios lead to better performance. The reason why we give up the optimal 30% is that the average bit-width per weight in the quantized model nearly approaches 2-bit (1.91-bit), which violates the conditions for extremely low-bit quantization. Therefore, we opt for the second-best performance 20% as our preserved ratio. However, it is crucial to note that the aforementioned experiments do not diminish the significance of our other innovations on non-salient weights binarization. As evident from the Table 3, simply maintaining the salient weights without incorporating block-wise optimization will results in inadequate performance."}, {"title": "B.2 Distinctions with AWQ and OWQ", "content": "For AWQ, both their method and our PTQ1.61 take into account the relationship between input activation and weight, and utilize this relationship for subsequent processing. But notably, there is no structured mask in AWQ. They leverage this discovery to perform a grid search based on MSE loss to select appropriate quantization scalars. These scalars are used to scale the corresponding weights"}, {"title": "C Block-wise Optimization", "content": ""}, {"title": "C.1 Hyperparameters", "content": "In addition to the preservation ratio of the structured mask, the remaining hyperparameters are only the learning rates. We conduct extensive experiments with various learning rates from le-4 to le-2 and finally select the optimal. The limited number of hyperparameters also demonstrate that our PTQ1.61 is not complex and easy to deploy."}, {"title": "C.2 Angular Biases", "content": "Except for implict row-wise dependencies, our block-wise strategy also takes angular biases into account. Previous PTQ methods (OmniQuant, GPTQ or other quantization methods for CNN models such as BNN) only focuses on the mag-"}, {"title": "D More Details on Quantization Preprocessing", "content": ""}, {"title": "D.1 Enhancement on Our PTQ1.61", "content": "In addition to LLaMA-13B in Table 3, we evaluate the enhancements on our PTQ1.61 brought by novel quantization preprocessing paradigm on more LLMs and the results are listed in Table 6, from which we confirm that the preprocessing consistently enhances the performance of our PTQ1.61 on each model. Crucially, our results reveal that, apart from preprocessing, our other innovations alone offer comparable performance advantages over existing methods, while attaining a lower weight compression ratio.\nFurthermore, the quantization preprocessing also augments the common sense reasoning capabilites of our method as listed in Figure 7."}, {"title": "D.2 Resources Requirement", "content": "Due to quantization preprocessing, our PTQ1.61 has higher resource cost compared to other PTQ methods. For example, compared to OmniQuant, as shown in Table 8, our method has slightly higher memory cost. Although this is a limitation of"}, {"title": "D.3 OPT Results on Other Approaches", "content": "The effectiveness of proposed quantization preprocessing scheme on LLaMA families quantized by other existing low-bit PTQ methods has been illustrated in Figure 5. Besides, results on OPT families is available in Figure 8 and the similar phenomenon can be observed."}, {"title": "D.4 Comparison with Post-Quantization PEFT Methods", "content": "It is worth mentioning that the restorative LoRA in our quantization preprocessing scheme stands out significant advantages and differences from existing post-quantization PEFT methods (Dettmers et al., 2023; Xu et al., 2023), as shown in Figure 9. For advantages: (a) Compared with LoRA and"}, {"title": "D.5 Comparison with QAT", "content": "As is known to all that QAT frameworks improve quantization performance by directly training a quantized LLM to get optimal weights as well as quantization parameters to accommodate quantization errors at target bit-width, where its strategy for adjusting weights may share some similarities with us. To eliminate this confusion, several important distinctions and limitations need to be highlighted: (a) QAT trains all the weights in the LLM, which incurs extremely high training costs, i.e., LLM-QAT (Liu et al., 2023b) requires 3 days to retrain a quantized OPT-1.3B on 8 Nvdia-A100 GPUs and OneBit (Xu et al., 2024) spends over 24 days"}, {"title": "E More Evaluations", "content": ""}, {"title": "E.1 MMLU and GSM8K", "content": "In addition to the benchmarks in the content, we also valid the quantization performance on GSM8K and MMLU on several LLMs. However, as the near-random levels illustrated in Table 10, we observe that under extremely low-bit quantization, all existing PTQ methods nearly make LLMs loss the ability, which is consistent with previous research (Liu et al., 2023a). Considering the disappointing outcomes, we choose not to list the comparison into the content."}, {"title": "E.2 Long Context Understanding", "content": "LongBench is a novel benchmark for evaluating long context understanding capability which is a critical measurement for LLMs application. Due to LongBench only supports Chat-LLMs, we select LLaMA2-7b-Chat for evaluation in Table 11. A consistent superior performance proves the effectiveness of our method."}, {"title": "E.3 Throughput and Inference Memory", "content": "For real-world system evaluation, current NVIDIA GPUs do not yet support such low-bit inference. Designing specific hardware and operation kernals that meets the inference conditions requires larger research teams and financial support, so our goal is to explore the performance limits of PTQ by fake-quantization before commercial hardware support is available. We believe this will eventually be realized, as evidenced by the latest NVIDIA GPUs now supporting 4-bit inference, whereas only a year ago they were limited to 8-bit.\nCompared with PB-LLM and BiLLM which requires to load extra unstructured mask during inference, our method is much more efficient. Followed by previous research (Ma et al., 2024), we can obtain information about a 1.58-bit (ours is 1.61-bit) LLaMA-7B achieves a 2.9X speedup in latency and LLaMA2-70B gains an 8.9X increase in throughput (2977 tokens/s).\nIn addition, we provide the memory usage of LLMs quantized by PB-LLM, BiLLM and PTQ1.61 via calculation considering weight bits, scaling factors and masks. As indicated by Table 12, our PTQ1.61 has an advantage in memory efficiency, which is of practical benefits."}, {"title": "F Discussion on Practically Applicability of Extremely Low-bit Weight Quantization", "content": ""}, {"title": "F.1 Accuracy-Latency Tradeoff Analysis", "content": "One important aspect of LLM quantization is the accuracy-latency tradeoff. If speed is the sole priority, an aggressive compression ratio can significantly improve latency, but this often results in unacceptable accuracy degradation. Conversely, adding additional components can help recover accuracy but may hinder system acceleration, leading to slower performance. Therefore, it is crucial to analyze the accuracy-latency tradeoff.\nDelve deeper into the calculation process of a quantized model on a GPU. For weight-only quantization, the primary acceleration comes from the transfer of low-bit integer weights from memory to the MAC (Multiply Accumulate) processing unit, which reduces the amount of data transfer compared with FP model. To restore performance, existing quantization methods introduce additional components, such as FP16 channel-wise scaling factors, to reduce quantization errors. For the attention layer of an LLM, the size of the weight matrix is usually 4096 \u00d7 4096, and the scaling factors are a 1 \u00d7 4096 vector. Therefore, in the transfer process mentioned in the previous paragraph, the bad impact on latency from transferring such a small amount of FP16 scaling factors is almost negligible compared to the significant inference acceleration benefits brought by transferring the low-bit weight matrices. Considering all above, a small amount of additional components such as scaling factors will not have a significant bad impact on the inference latency in model quantization."}, {"title": "F.2 Compared with Weight-activation Quantization", "content": "In order to prove the necessity of extremely low-bit weight PTQ research, we provide the performance gap among FP16 results, W4A4 SmoothQuant and extreme low-bit PTQ methods. The results are shown as Table 13. The results demonstrate the necessity of research into extremely low-bit PTQ from two aspects. Firstly, compared to the results of FP16, previous method (PBLLM) indeed showed a significant gap, but our PTQ1.61 has narrowed the performance gap to an acceptable level. Secondly, compared to SmoothQuant, the most popular and wide applied weight-activation PTQ method, its performance of W4A4, which is currently supported by the latest commercial GPUs, is still inferior to our PTQ1.61, proving the research prospect of extremely low-bit weight quantization is as bright as weight-activation quantization. We are confident that with further advancements, the disparity between extremely low-bit weight quantization and full precision will progressively diminish."}]}