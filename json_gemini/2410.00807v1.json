{"title": "WiGNet: Windowed Vision Graph Neural Network", "authors": ["Gabriele Spadaro", "Marco Grangetto", "Attilio Fiandrotti", "Enzo Tartaglione", "Jhony H. Giraldo"], "abstract": "In recent years, Graph Neural Networks (GNNs) have demonstrated strong adaptability to various real-world challenges, with architectures such as Vision GNN (ViG) achieving state-of-the-art performance in several computer vision tasks. However, their practical applicability is hindered by the computational complexity of constructing the graph, which scales quadratically with the image size. In this paper, we introduce a novel Windowed vision Graph neural Network (WiGNet) model for efficient image processing. WiGNet explores a different strategy from previous works by partitioning the image into windows and constructing a graph within each window. Therefore, our model uses graph convolutions instead of the typical 2D convolution or self-attention mechanism. WiGNet effectively manages computational and memory complexity for large image sizes. We evaluate our method in the ImageNet-1k benchmark dataset and test the adaptability of WiGNet using the CelebA-HQ dataset as a downstream task with higher-resolution images. In both of these scenarios, our method achieves competitive results compared to previous vision GNNs while keeping memory and computational complexity at bay. WiGNet offers a promising solution toward the deployment of vision GNNs in real-world applications. We publicly released the code at https://github.com/EIDOSLAB/WiGNet.", "sections": [{"title": "1. Introduction", "content": "In the last decade, the field of computer vision has progressed significantly, largely due to the success of deep neural networks [20]. These models are now established as state-of-the-art in several tasks such as image classification, object detection, semantic segmentation, etc [1, 3, 14, 30\u201332]. In particular, Convolutional Neural Networks (CNNs) [21] exploit the locality of natural images to extract features while Vision Transformers (ViTs) [6, 24, 36] implement the attention operator to exploit long-range dependencies of the input image. Recently, vision-based Graph Neural Networks (GNNs) [11, 27, 28], have been proposed with promising results for vision tasks. Vision GNNs first build a graph over the features extracted from the image, and then apply graph convolutions instead of regular 2D convolutions (CNNs) or the self-attention mechanism (ViTs). As a result, vision GNNs have benefited from the rich literature of GNNs [43], adding a new dimension to the landscape of deep learning for image analysis.\nDespite the promising results achieved by vision GNNs, there still remain some open challenges. More precisely, the Vision GNN (ViG) model [11] relies on the k-Nearest Neighbors (k-NN) method to construct the graph."}, {"title": "2. Related Work", "content": "In this section, we first give an overview of deep learning networks typically adopted in computer vision like CNNs and ViTs. Then we review GNNs, analyzing their applications to visual tasks and their limitations."}, {"title": "2.1. CNNs and ViTs.", "content": "Convolutional Neural Networks (CNNs) started dominating the computer vision field since the seminal AlexNet [19] paper. CNNs exploit the locality of pixels to extract features from the input image useful to the task on which they are trained. CNNs represented the de-facto standard to solve different tasks from image classification to object detection [30, 31], semantic segmentation [13, 32], image compression [1, 26], and many others. The rapid development that these architectures have experienced over the past decade has led to the development of models such as ResNet [14] and MobileNet [16], among others [17, 34, 35].\nMore recently, researchers in computer vision have focused on Visual Transformers (ViTs), with the self-attention mechanism at its core. ViTs build upon the attention mechanism proposed by the Transformer architecture [37] for Natural Language Processing (NLP) tasks in origin, and later on applied with success to different computer vision tasks [3, 6, 24, 40]. Attention enables capturing long-range dependencies between pixels, achieving state-of-the-art results in several computer vision tasks. The Swin Transformer [24], in particular, proposed a hierarchical Transformer-based architecture to extract tokens at different scales and work with high-resolution images. The multi-head self-attention operator in Swin Transformer is computed in non-overlapped windows. To introduce cross-window connections, the authors proposed a shifted window partitioning approach that alternates with the regular window partitioning in consecutive blocks of Swin Transformer. This method allows for connections between neighboring windows in the previous layer, leading to improvements in image classification, object detection, and semantic segmentation [24].\nThe core function in Swin Transformers can be thought of as an attention operator applied in fully connected graphs from windows in images. WiGNet is, in essence, different from Swin Transformers since (i) we operate in k-NN graphs instead of fully connected graphs, and (ii) we use a GNN function instead of the self-attention mechanism. These two changes achieve competitive results regarding the Swin Transformer, other ViTs, and CNN models."}, {"title": "2.2. Graphs in Computer Vision.", "content": "GNNs emerged as an extension of the convolution operation of CNNs for regular-structured data such as images to the graph domain. GNNs are typically used for learning graph-structured data representations. Bruna et al. [2] proposed the first modern GNN by extending the convolutional operator of CNNs to graphs. Incorporating concepts of signal processing on graphs, Defferrard et al. [5] introduced localized spectral filtering for graphs. Later, Kipf and Welling [18] approximated the spectral filtering operation to obtain efficient Graph Convolutional Networks (GCNs). Inspired by these works, Veli\u010dkovi\u0107 et al. [38] presented the attention mechanism on GNNs, resulting in a graph where the connection weights are unique and learned for each edge. This allows GATs to effectively model complex relationships between nodes, although with increased computational complexity.\nEven though GNNs have generally been adopted for graph-based data [18], they have recently demonstrated remarkable success when applied to tasks such as image classification [11, 27, 28] and segmentation [8]. The Vision GNN (ViG) model [11], in particular, drew inspiration from the partition concept introduced in ViTs [6], dividing the input image into smaller patches, and considering each of these patches as a node in the graph of the image. To establish connections between these nodes, the k-NN algorithm is adopted by considering the similarity of nodes in the feature space. These features are updated using graph convolution operators, considering the features of the node itself and those of its neighbors [7]. Following a similar paradigm as Transformers, these features contribute to the classification of the entire graph, thereby classifying the entire image.\nUsing a graph can be beneficial in image processing tasks as it allows for the exploitation of non-local dependencies without the need for multiple convolutional layers and to model complex objects having irregular shapes. Furthermore, graphs are a more general data structure with respect to a grid of pixels (as modeled by CNNs) or a fully connected graph of patches (as modeled by ViTs). These advantages have led graph-based models to reach state-of-the-art not only in image classification but also in object detection and instance segmentation [11, 28]. However, vision GNNs still have a very high computational complexity, especially when working with high-resolution images. For this reason, in the first layers of ViG, the graph is constructed in a bipartite way, connecting patches of the original feature maps with a subsample version of it, obtained using a non-learnable subsampling filter. Although this approach decreases the complexity of ViG, it still has a quadratic time complexity, making it slow for processing large images.\nMore recently, new techniques have emerged to reduce the complexity of ViG by focusing on the graph construction phase. MobileViG [27], for instance, proposed a Sparse Vision Graph Attention (SVGA) module, in which the graph is statically constructed, thus without adopting k-NN. Here a patch of the image is connected to patches at a certain hop distance on the same row and column. In this way the number of connections depends on the size of the image, rapidly increasing the memory required to perform graph convolutions. Thus, to obtain a mobile-friendly model, MobileViG only adopts the SVGA module in the last stage of the architecture (where the input tensor is smaller) while the previous stages are implemented using classical depth-wise 2D convolutions. This results in a hybrid model in which the CNNs and GNNs are both adopted. Greedy ViG [28] proposed a dynamic version of SVDA named Dynamic Axial Graph Construction (DAGC).\nThis module starts from the same fixed graph of SVDA and dynamically masks some connections. To create this mask, Greedy ViG estimates the mean and standard deviation of the Euclidean distances between the patches in the original image and a diagonally flipped version. Moreover, this DAGC module is implemented in each stage of the architecture, making this CNN-GNN model highly memory-intensive.\nUnlike these previous methods, WiGNet always works on the original feature maps and not on an undersampled version (like ViG). Our method partitions these feature maps into windows of fixed size in which the graph can be constructed. Moreover, the k-NN operator is not replaced with a fixed graph structure, but we exploit the locality of pixels to reduce the complexity of this operation, making it linear with respect to the size of the input image."}, {"title": "3. Windowed Vision Graph Neural Network", "content": "This section describes in detail the proposed WiGNet architecture. Firstly, we provide some background on graphs and GNNs. Secondly, we describe the architectural design motivating our choices. Then we discuss the implication of computational complexity, comparing WiGNet with the ViG model. Finally, we propose three different WiGNet versions that we use for our experiments in Section 4."}, {"title": "3.1. Preliminaries", "content": "Graph. A graph is a mathematical entity that can be represented as $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$, where $\\mathcal{V}={1, \\ldots, N}$ is the set of nodes, and $\\mathcal{E} \\subseteq\\{(i, j) \\mid i, j \\in \\mathcal{V} \\text { and } i \\neq j\\}$ is the set of edges between nodes $i$ and $j$. We can associate $F$-dimensional feature vectors to every $i$-th node in $\\mathcal{G}$ such that $\\mathbf{x}_{i} \\in \\mathbb{R}^{F}$. Therefore, we represent the whole set of features in $\\mathcal{G}$ with the matrix $\\mathbf{X}=\\left[\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{N}\\right]^{\\top} \\in \\mathbb{R}^{N \\times F}$.\nMessage passing function. In GNNs, the message-passing function is the standard paradigm for computing graph convolutions [7]. Let $\\mathbf{x}^{\\prime}$ be the output of a generic graph convolution, we can thus define the message-passing function as follows:\n$\\mathbf{x}_{i}^{\\prime}=\\operatorname{UPDATE}\\left(\\mathbf{x}_{i}, \\operatorname{AGG}\\left(\\left\\{\\mathbf{x}_{j} \\forall j \\in \\mathcal{N}_{i}\\right\\}\\right)\\right),$   (1)\nwhere $\\mathcal{N}_{i}$ is the set of neighbors of $i$, $\\operatorname{AGG}(\\cdot)$ is a generic function used to aggregate neighbor information, and $\\operatorname{UPDATE}(\\cdot)$ updates the representation of the node itself. A graph convolutional layer can be thought of as an implementation of this message-passing operator by concretely defining the update and aggregation functions."}, {"title": "3.2. WiGNet Architecture", "content": "Architecture overview. Fig. 2a shows a bird's eye view of the WiGNet architecture, implementing a four-stage pyramidal feature extractor, where at each stage features of increasingly smaller sizes are extracted. A WiGNet is composed of three basic building blocks: (i) the Stem, (ii) the WiGNet block, and (iii) the downsampling module.\nThe Stem block is a simple feature extractor composed of three convolutional layers that receive as input an image of size $H \\times W \\times 3$, divides the image into $N$ patches and transforms them into a feature vector $\\mathbf{x} \\in \\mathbb{R}^{F}$ for each patch, obtaining $\\mathbf{X}=\\left[\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{N}\\right]^{\\top}$.\nThe WiGNet block is composed of a Window-based Grapher module and a Feed Forward Network (FFN). The Grapher module partitions the image into non-overlapping windows, builds a graph for each window, and then local GNN updates are applied to each window. This is a fundamentally different approach than ViG [11] where a large graph is built on top of the entire image, with the complexity implications discussed above. The FFN module further encourages feature diversity. The downsampling block reduces the feature dimension by merging node representations. Each of the downsampling modules reduces the number of nodes by a factor of 2 while increasing the size of the feature vectors associated with the remaining nodes.\nThe complete network architecture is composed of a stack of the Stem function and four WiGNet-plus-Downsampling blocks. Fig. 2a shows a final fully connected layer for producing class scores. We propose three versions of WiGNet in Table 1: (i) tiny (WiGNet-Ti), (ii) small (WiGNet-S), and (iii) medium (WiGNet-M). In the following, we describe in detail the Grapher module, at the core of WiGNet.\nThe Grapher module. The Window-based Grapher module illustrated in Fig. 2b is at the core of WiGNet. Preliminary, the feature vector generated from the Stem module (or the previous Grapher module) is processed by a fully connected layer with batch normalization. Firstly, the Windows Partitioning component splits the input tensor into non-overlapping windows having a fixed size of $M \\times M$. Secondly, the Dynamic Graph Convolution component builds a graph and performs graph convolution independently for each window. This component is in turn at the core of the Grapher and is described in detail in the following section. Next, the Windows Reverse component reshapes the output of the Dynamic Graph Convolution component into the original feature vector as generated by the Windows Partitioning component. The feature vector is then passed as input to a fully connected layer with batch normalization. Finally, the Window-based Grapher block is completed with a skip connection.\nDynamic Graph Convolution component. For each $w$-th window, the dynamic graph convolution component implements the k-NN algorithm to produce a graph $\\mathcal{G}_{w}=\\left(\\mathcal{V}_{w}, \\mathcal{E}_{w}\\right)$, where $\\mathcal{E}_{w} \\subseteq\\left\\{(i, j) \\mid i, j \\in \\mathcal{V}_{w}\\right\\}$ is the set of edges and $\\mathcal{V}_{w}$ is the set of nodes in $\\mathcal{G}_{w}$. In particular, two nodes $(i, j)$ are connected if $j \\in \\mathcal{N}_{i}^{w}$, where $\\mathcal{N}_{i}^{w}$ is the set of $k$ nearest neighbors for the node $i$ belonging to the same window $w$. Therefore, similarly to ViG [11], we apply the Max-Relative graph convolution proposed by Li et al. [23] to update the representation of the $i$-th node in the $w$-th window as follows:\n$\\mathbf{x}_{i}^{\\prime w}=\\mathcal{W}_{\\text {update }}\\left(\\mathbf{x}_{i}^{w} \\|\\| \\max \\left(\\left\\{\\mathbf{x}_{i}^{w}-\\mathbf{x}_{j}^{w} \\forall j \\in \\mathcal{N}_{i}^{w}\\right\\}\\right)\\right),$  (2)\nwhere $\\|\\|$ is the concatenation function, and $\\mathcal{W}_{\\text {update }}$ is a matrix of learnable parameters."}, {"title": "3.3. Shifted Windows.", "content": "To introduce cross-window connections while maintaining the efficient computation described above, we include in WiGNet a shifting operator similar to the one adopted in Swin Transfomer [24]. More precisely, we implement a Shifted Window-based Grapher module, where the graph construction and convolution are performed on shifted windows as illustrated in Fig. 3. To do this, we adopt a cycling operation to partition the feature map, and we use a masking mechanism to allow connection only between nodes adjacent to the feature map. In other words, multiple subgraphs may arise in the same window as shown in Fig. 3, where different colors and texture backgrounds are used to identify the masking mechanism (i.e., connections are allowed only between nodes that fall in the same color area). This phenomenon results in a heterogeneous construction of the graphs, implying a considerable drop in the number of neighboring nodes in certain regions. For instance, a node belonging to the top-left window of Fig. 3 will be connected to $k$ other nodes in that window out of $M \\times M$ possible nodes, where $M$ is the window size. Instead, a node in the section B of the bottom-right window will still be connected to $k$ other nodes but out of $S \\times S$ possible nodes, where $S$ is the shift-size typically set as $S=\\lfloor\\frac{M}{2}\\rfloor$. To attempt to solve this issue, we linearly adjust the number of neighbors of each node by considering the maximum number of possible neighbors that the masking mechanism allows it to have. In particular, given $k$, the window-size $M \\times M$, and the number of possible neighbors for the node $i$ ($\\mathcal{P}_{i}$), we can use $k_{i}=k \\times \\frac{\\mathcal{P}_{i}}{M^{2}}$ as the number of neighbors for that node."}, {"title": "3.4. Complexity Considerations.", "content": "Although both our method and ViG use k-NN to create the graph, one of the major advantages of WiGNet is the reduction in computational complexity as the image size increases. ViG's k-NN complexity, indeed, grows with the square of the number $h w$ of nodes (patches) of the whole feature map and is given by:\n$\\Omega(\\text {ViG}, k \\text {-NN })=(h w)^{2}$. (5)\nIn contrast, the windowed approach of WiGNet results in a complexity that grows linearly with the number of patches as follows:\n$\\Omega(\\text {WiGNet}, k \\text {-NN })=\\left(\\frac{h w}{\\nu_{w}}\\right)\\left|\\mathcal{V}_{w}\\right|^{2}=\\frac{h w}{\\nu_{w}}\\left|\\mathcal{V}_{w}\\right|$.  (6)\nwhere $\\left|\\mathcal{V}_{w}\\right|$ is the number of nodes on each window $w$. We compare Multiply-Accumulate (MACs) operations and memory footprint of ViG, WiGNet and two other graph-based models in Fig. 5."}, {"title": "4. Experiments", "content": "In this section, we first experiment with WiGNet over the ImageNet-1K [33] dataset comparing against ResNet [14], Pyramid Vision Tranformer [39], Swin Transformers [24], Poolformer [44], ViG [11], MobileViG [11], and GreedyViG [12]. For the sake of comparability, we consider $k=9$ neighbors for graph construction as in ViG [11]. Then, once we train our model on ImageNet, we evaluate its adaptability to a new classification task with higher-resolution images. To do this, we use our tiny model as a pre-trained frozen backbone for facial identification on the CelebA-HQ [22] dataset. Finally, we perform two ablation studies on key design choices: whether or not to use shifting windows and which graph convolutional layer to adopt."}, {"title": "4.1. Experimental Setup", "content": "Datasets. In image classification, the benchmark dataset ImageNet ILSVRC 2012 [33] is commonly used as a standard evaluation metric. ImageNet contains approximately 1.2 million in training images and 50,000 in validation images, spanning across 1,000 categories.\nThe CelebA-HQ [22] dataset is instead used to test the adaptability of our model in a downstream task with high-resolution images. Indeed, this dataset is a high-quality version of CelebA [25] that consists of 30, 000 images. We use this dataset to perform a facial identification of 307 classes by rescaling the images to a resolution of $512 \\times 512$. This rescaling is performed to be able to train the most memory-intensive models like ViG and GreedyViG.\nImplementation details. For training all WiGNet models on ImageNet we keep similar hyperparameters as ViG [11]. We adopt the commonly-used training strategy proposed in DeiT [36] for fair comparison. The data augmentation includes RandAugment [4], Mixup [46], Cutmix [45], random erasing [47]. Additionally, for WiGNet-M we adopt the repeated augmentation [15] and an Exponential Moving Average (EMA) scheme. We implement our models using PyTroch [29] and train all of them on 8 GPUs NVIDIA"}, {"title": "4.2. Main Results", "content": "First, we provide the classification results on ImageNet. Then, we show that our pre-trained backbone achieves a better trade-off between accuracy and complexity than other models using CelebA-HQ as a downstream task.\nImageNet. Table 2 shows the comparison between WiGNet and previous state-of-the-art methods. WiGNet outperforms or achieves competitive results against previous state-of-the-art models for similar complexity. For instance, comparing WiGNet with non-graph-based models, our tiny model with 78.8 of accuracy outperforms all previous methods for low MACs (around 2G), and a small number of parameters (around 10M). Similarly, the WiGNet-S achieves better results than the Swin-T model with comparable MACs and than ResNet-152 with almost half the parameters.\nIn addition, WiGNet shows competitive results against the previous graph-based method under similar conditions. Moreover WiGNet-Ti trained with slightly larger images ($256 \\times 256$) and using a window size of 8 \u00d7 8, works better than the same model trained on (224 \u00d7 224) images since in this case the window-size is smaller (7 \u00d7 7) and thus the number of possible neighbors.\nCelebA-HQ. To show the adaptability of WiGNet to new classification tasks having higher resolution images, we conduct experiments using our pre-trained model on ImageNet as a frozen backbone on the CelebA-HQ dataset [22] as a downstream facial identity classification task. Particularly, a new classification layer was trained on these features keeping the rest of the architecture frozen. Fig. 6 shows the results obtained by our backbone compared to other graph-based models in terms of accuracy, memory usage, and MACs using 512 \u00d7 512 resolution images. In this context we notice that ViG struggles to converge, while our backbone achieves the second-best result, only outperformed by Greedy ViG. However, by comparing the memory footprint required for each model, we observe that WiGNet needs only 0.5 GB, while for GreedyViG the occupancy is ~ 3\u00d7 more. MobileViG, instead, is the model with the lowest MACs. Nevertheless, it occupies more memory than WiGNet achieving worse results.\nIt is clear from Fig. 6 that WiGNet is the closest model to the optimal point (i.e. top-left corner of the plot), achieving similar Top-1 accuracy results to GreedyViG but using significantly less memory, even compared to MobileViG. In Fig. 5 we also analyzed the complexity of these models in terms of MACs and memory as the resolution of the input image increases. From these results we observe that WiGNet computation and memory requirements scale only linearly with the image size. By comparison, the memory requirements for Greedy ViG (and for ViG also the complexity in terms of MACs) scales quadratically with the image"}, {"title": "4.3. Ablation Studies", "content": "Shifted windows. Table 3 shows the results when the WiGNet uses the Shifted Window-based Grapher module explained in Sec. 3.3. We observe that contrary to the Swin Transformer, the shifting strategy does not bring any advantage to WiGNet in this context, despite the results seeming to improve slightly by increasing the model size. We hypothesize that, because of the low resolution of the images in ImageNet, is possible to independently analyze the windows and still obtain good results. Therefore, we conduct the same transfer learning experiment described in Sec. 4.2 to monitor the behavior of our backbone without shifting and with higher-resolution images, ablating also on the Adaptive k-NN strategy. In Tab. 4 we observe that for larger images the shifting operator is crucial, allowing for a gain of almost 2% points on average, and a significantly lower standard deviation. Moreover, this gain increases to 6% when the Adaptive k-NN strategy is implemented.\nGraph Convolutional Operator. Finally, we conduct an ablation study with some well-known graph convolutional functions in the Grapher module, including Max-Relative GraphConv [23], GraphSAGE [10] and EdgeConv [41]. Table 5 shows the results of this experiment when the WiGNet-Ti model is trained on ImageNet without the shifting operator, as it seems to work slightly better for the tiny size model. We observe that the Max-Relative graph convolution achieves competitive results with less complexity than the other operators."}, {"title": "4.4. Limitations", "content": "The main limitation of our method compared to other Graph-based approaches is the lack of global information during the feature update process. This problem is partially mitigated by the hierarchical structure of the architecture and the shifting operation, which allows to capture less local (but not global) information through cross-windows connections. However, this operation in WiGNet is not as straightforward as in Swin Transformers: we should dynamically adapt the $k$ value for the k-NN in the borders of the image to be consistent with the rest of the regions as shown in Fig. 3. By adopting this strategy, we successfully classify high-resolution images. Nevertheless, we believe that more global information might be useful in tasks where we need to capture long-range dependencies like, for example, image segmentation. One possible solution to this limitation is to promote connections among graphs in the same layer. However, this solution poses practical and theoretical challenges that deserve exploration in future works."}, {"title": "5. Conclusions", "content": "This work introduced a new Windowed vision GNN (WiGNet) for image analysis tasks. Our model partitions the input images into windows, and therefore graphs are constructed in the local windows. Thus, we use the Max-Relative graph convolution operation on each local window for feature updating. WiGNet's architecture is completed with FFN and downsampling operations. We show in theory and practice that the computational and memory complexity of WiGNet scales linearly with the image size. At the same time, for previous vision GNNs such as ViG [11], the complexity grows quadratically. This has profound implications for GNN-based vision models' applicability in tasks requiring high-resolution images. We conducted experiments in the ImageNet-1k benchmark dataset and then we show that WiGNet can be successfully adopted as a pre-trained backbone for high-resolution image classification on the CelebA-HQ dataset, achieving a better trade-off between accuracy and complexity with respect to other graph-based models. Thus, WiGNet offers a strong and scalable alternative to previous deep learning models for computer vision tasks, proving suitable for working with high-resolution images."}]}