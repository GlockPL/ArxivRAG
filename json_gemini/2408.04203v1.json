{"title": "MMRole: A COMPREHENSIVE FRAMEWORK FOR DEVELOPING AND EVALUATING MULTIMODAL ROLE-PLAYING AGENTS", "authors": ["Yanqi Dai", "Huanran Hu", "Lei Wang", "Shengjie Jin", "Xu Chen", "Zhiwu Lu"], "abstract": "Recently, Role-Playing Agents (RPAs) have garnered increasing attention for their potential to deliver emotional value and facilitate sociological research. However, existing studies are primarily confined to the textual modality, unable to simulate humans' multimodal perceptual capabilities. To bridge this gap, we introduce the concept of Multimodal Role-Playing Agents (MRPAs), and propose a comprehensive framework, MMRole, for their development and evaluation, which comprises a personalized multimodal dataset and a robust evaluation method. Specifically, we construct a large-scale, high-quality dataset, MMRole-Data, consisting of 85 characters, 11K images, and 14K single or multi-turn dialogues. Additionally, we present a robust evaluation method, MMRole-Eval, encompassing eight metrics across three dimensions, where a reward model is trained to score MRPAs with the constructed ground-truth data for comparison. Moreover, we develop the first specialized MRPA, MMRole-Agent. Extensive evaluation results demonstrate the improved performance of MMRole-Agent and highlight the primary challenges in developing MRPAs, emphasizing the need for enhanced multimodal understanding and role-playing consistency. The data, code, and models will be available at https://github.com/YanqiDai/MMRole.", "sections": [{"title": "INTRODUCTION", "content": "The advancement of large language models (LLMs) (Zhao et al., 2023) has significantly catalyzed the rise of Role-Playing Agents (RPAs) (Chen et al., 2024b), which are engineered to emulate specific characters and engage in dialogues with human users or other characters. Unlike AI productivity assistants, RPAs primarily focus on delivering emotional value (Li et al., 2023a; Wang et al., 2023; Shao et al., 2023) and facilitating sociological research (Zhou et al., 2023b; Wang et al., 2024a; Chen et al., 2024a; Gu et al., 2024), where typical applications include emotional companions, NPCs in video games, digital clones, and social simulations.\nThe primary characteristic of RPAs is their capability to engage in human-like and immersive interactions. However, existing studies in role-playing are primarily confined to the textual modality, which has considerable limitations. In the real-world context, human perception integrates multiple modalities, especially visual and textual, allowing for a more direct and comprehensive understanding of the environment than text alone can provide. Therefore, enhancing RPAs with multimodal capabilities is a crucial next step for conducting more realistic and engaging interactions.\nIn this paper, we introduce the concept of Multimodal Role-Playing Agents (MRPAs). MRPAS are designed to emulate specific characters and engage in dialogues centered around images, with either human users or other characters. Moreover, we propose MMRole, a comprehensive framework for developing and evaluating MRPAS. As presented in Figure 1, this framework includes two principal components: a large-scale, high-quality dataset and a robust evaluation method for MRPAs.\nDataset Construction: The dataset for MRPAs comprises character profiles, images, and dialogues centered around images. Specifically, we consider three categories of characters: fictional characters, historical and public figures, and hypothetical real-life characters. The profiles of the first two"}, {"title": "RELATED WORK", "content": "Role-Playing Agents. Recent advancements in large language models (LLMs) (Zhao et al., 2023), such as supervised fine-tuning (Wei et al., 2021) and in-context learning (Brown et al., 2020), have significantly catalyzed the rise of Role-Playing Agents (RPAs) (Chen et al., 2024b), which are interactive AI systems that can emulate designated personas. Specifically, the personas can be categorized into individual characters (Wang et al., 2023; Shao et al., 2023; Wang et al., 2024a; Gu et al., 2024) and groups of people with particular attributes (Li et al., 2023b; Hong et al., 2023; Xu et al., 2023). In this study, we primarily focus on the former.\nExisting RPAs that emulate individual characters are developed through either training or prompting LLMs with high-quality character-specific dialogues. In a pioneering study, Chen et al. (2023) extracted all dialogue sessions from original scripts to develop a Harry Potter-specific RPA. Furthermore, Wang et al. (2023), Zhou et al. (2023a), Shao et al. (2023) and Li et al. (2023a) constructed hundreds of characters and more comprehensive datasets of character dialogues. These efforts aimed to develop RPAs that can deliver emotional value to humans. In contrast, Gu et al. (2024) focused on facilitating sociological research. They proposed a character simulation framework that explores the role of language in shaping collective behavior through interactive debate scenarios. However, these studies are primarily confined to the textual modality. Conversely, our MMRole framework is the first to enhance RPAs with multimodal capabilities.\nThe evaluation of RPAs is also a crucial and challenging research direction. Various methods have been proposed. Specifically, Shen et al. (2023) and Chen et al. (2024a) assessed RPAs with multiple-choice questions. Tu et al. (2024) trained a reward model for scoring without a ground truth. Wang et al. (2024b) evaluated the personality fidelity of RPAs through interviews, scoring without a ground truth by GPT-4. Ng et al. (2024) engaged the acquaintances of the target individuals to distinguish between humans and RPAs. Nevertheless, the high expense of human annotation and the potential instability of scoring without a ground truth pose significant challenges. To address this, we develop a reward model to score RPAs with a ground-truth baseline for comparison.\nLarge Multimodal Models. Large Multimodal Models (LMMs) are advanced AI systems typically built upon LLMs, designed to integrate and comprehend multiple data modalities, particularly text and images (Yin et al., 2023). A variety of impressive LMMs have been released, including closed-source models with hundreds of billions of parameters like GPT-4V (Achiam et al., 2023), Gemini (Team et al., 2023), and Claude 3 (Anthropic, 2024), and open-source models with tens of billions or billions of parameters like MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023), LLaVA (Liu et al., 2023), QWen-VL (Bai et al., 2023), InternVL (Chen et al., 2024c), and Yi-VL (Young et al., 2024). Additionally, various innovative techniques have been explored to enhance the performance of LMMs, such as visual instruction tuning (Liu et al., 2023), mixture of experts (Lin et al., 2024), and multi-task balancing (Dai et al., 2024). LMMs are utilized in many vertical fields, such as healthcare (Li et al., 2024), document understanding (Ye et al., 2023), and GUI navigation (Hong et al., 2024). To further explore their potential, we apply LMMs to role-playing for the first time."}, {"title": "MULTIMODAL ROLE-PLAYING AGENTS", "content": "Role-Playing Agents (RPAs) are engineered to emulate specific characters and engage in dialogues with either human users or other characters. Expanding on this concept, Multimodal Role-Playing Agents (MRPAs) incorporate the capacity to comprehend vision-language multimodal information. This capacity enables dialogues that are centered around and informed by images. From another perspective, compared to traditional multimodal question answering, multimodal role-playing includes character profile input, adding greater complexity and depth to the interaction.\nIn scenarios where the dialogue partner is a human user without a specific identity, given an image I, the profile P of the designated character C, and the dialogue context D, the MRPA steps into the shoes of the character C, responding to the human user pertaining to the image I:\n$$D = [h_1, m_1, h_2, M_2, ..., M_n],$$\n$$m_n = MRPA(I, P, D),$$\nwhere D is a sequence of conversation pairs, with hi and mi representing the i-th utterances from the human user and the MRPA, respectively.\nConversely, in scenarios where the dialogue partner is another character Cother, given an image I, the profile P of the designated character C, the profile Pother of the character Cother, and the dialogue context D, the MRPA steps into the shoes of the character C and interacts with the character Cother,"}, {"title": "MMRole-Data: DATASET CONSTRUCTION", "content": "As shown in 1(a), we construct MMRole-Data, a large-scale, high-quality multimodal role-playing dataset. In this section, we first provide a detailed classification of characters and dialogue scenarios considered in MMRole-Data, then describe the pipelines for character profile generation and image collection and annotation, as well as the methodology for dialogue generation and filtering."}, {"title": "CHARACTERS AND DIALOGUE SCENARIOS", "content": "We consider three categories of characters: (1) Fictional Characters, characters created in fictional media such as literature, films, and games; (2) Historical and Public Figures, individuals who are specifically documented in historical records or well-known in real life; (3) Hypothetical Real-Life Characters, hypothetical individuals who are not explicitly known but could exist in real life.\nThe first two categories have been explored in previous role-playing research. Moreover, we propose the third category to enhance and evaluate MRPAs in characters that are not widely recognized. To effectively emulate hypothetical real-life characters, MRPAs must deeply understand and align with the provided character profiles, rather than relying on their inherent world knowledge.\nAs depicted in Figure 2, we introduce three types of dialogue scenarios consistently centered around images: (1) Commentary Interactions, single-turn dialogues where a character offers comments or reflections centered around an image, without any further interaction; (2) Human-Role Dialogues, multi-turn dialogues centered around an image between a human user without a specific identity and a character; (3) Inter-Role Dialogues, multi-turn dialogues centered around an image between two characters from the same series."}, {"title": "CHARACTER PROFILE GENERATION", "content": "Character profiles are crucial for the role-playing effectiveness of MRPAs, especially for those characters with which MRPAs are not familiar. To facilitate a thorough understanding of the designated characters, our character profiles encompass five core parts: brief introduction, personality, life story, main interpersonal relationships, and catchphrases, which are exampled in Appendix B.\nAs discussed in Section 4.1, three categories of characters are considered in MMRole. The majority of these characters are English, with a smaller proportion being Chinese. For fictional characters,"}, {"title": "IMAGE COLLECTION AND ANNOTATION", "content": "For each character, we utilize distinct generic images from MS-COCO (Lin et al., 2014) to ensure comprehensive coverage of a wide range of visual concepts. Additionally, we manually collect and annotate various character-related images, which can evoke the personal experiences and emotions of the characters more effectively. Specifically, we collect production stills for fictional characters, web illustrations for historical and public figures, and news photos for hypothetical real-life characters. Moreover, as presented in Figure 2, the information of characters, place, and scene is manually annotated for each character-related image."}, {"title": "DIALOGUE GENERATION AND FILTERING", "content": "As discussed in Section 4.1, three types of dialogue scenarios are introduced in MMRole. Based on the character profiles and images, GPT-4 generates dialogues corresponding to each scenario type. Interestingly, we observe that using the prompt, \u201cYou are a dedicated role-playing assistant...Please step into the shoes of {character} from {series}\u201d yields better results than the simpler prompt, \u201cYou are {character} from {series}\u201d. We suggest that the training data supplied by OpenAI optimizes GPT-4 to function more effectively as a helpful assistant, rather than as an immersive, human-like character. The prompts for dialogue generation are detailed in Appendix D.\nTo ensure accuracy and reliability, we manually filter all dialogues using several strategies. Specifically, we remove failed response data, as well as non-Chinese and non-English data. Additionally, we eliminate content that replies in the tone of an AI assistant, meaningless modal words frequently output by GPT-4, action and scene descriptions, and unnecessary explanatory prefixes and suffixes."}, {"title": "MMRole-Eval: PERFORMANCE EVALUATION", "content": "As illustrated in Figure 1(b), we propose MMRole-Eval, a robust evaluation method to stably and comprehensively assess MRPAs. In this section, we introduce eight evaluation metrics across three dimensions and the approach for score quantification."}, {"title": "EVALUATION METRICS", "content": "In contrast to textual RPAs, MRPAs must not only accurately emulate specific characters but also deeply comprehend both visual and textual information. Therefore, we propose a three-dimensional evaluation system, encompassing fundamental conversational skills, multimodal understanding abilities, and role-playing qualities.\nThe fundamental conversational skills of MRPAs present their capacity to sustain fluent and coherent interactions within role-playing scenarios, which are assessed by three metrics:\n\u2022 Instruction Adherence (IA): Do the responses accurately adhere to the task instruction, directly role-playing as the character and including only words that the character would say, without any unnecessary explanatory prefixes or suffixes?\n\u2022 Fluency (Flu): Are the responses grammatically correct and articulated smoothly?\n\u2022 Coherency (Coh): Do the responses maintain a coherent thread of dialogue without contradicting previous turns or containing internal inconsistencies within the current responses?\nThe multimodal understanding abilities of MRPAs indicate their capacity to effectively integrate and interpret both visual and textual information, which are assessed by two metrics:"}, {"title": "SCORE QUANTIFICATION", "content": "To quantitatively evaluate the performance of RPAs across various metrics, existing methods employ reward models or human annotators to directly score outputs without a ground truth (Zhou et al., 2023a; Tu et al., 2024). However, it may be unstable due to the variability of scoring criteria without a baseline for comparison. Therefore, we propose to develop a more stable reward model. Inspired by the evaluation methods of Vicuna (Chiang et al., 2023) and LLaVA (Liu et al., 2024b), our reward model first conduct a brief qualitative assessment of the relative performance between the evaluated MRPA and the constructed ground-truth data for each metric, followed by assigning a quantitative score pair. The final score of the MRPA is the ratio of the two scores within the score pair.\nTo develop the reward model, we initially employ GPT-4 to assess various MRPAs across all test samples. For each evaluated MRPA and corresponding test sample, GPT-4 outputs brief assessments and score pairs for all metrics through a single API call. The prompt for GPT-4 scoring is provided in Appendix F. Subsequently, these evaluation trajectories are converted into training and validation data for our reward model. Each evaluation trajectory is segmented into eight samples, with each sample evaluating a distinct metric. Compared to directly applying GPT-4 as the reward model, this approach renders MMRole-Eval both open-source and cost-effective."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "STATISTICS OF MMRole-Data", "content": "Table 1 presents the statistics of the MMRole-Data dataset. Totally, the dataset comprises 85 characters, 11,032 images, and 14,346 dialogues, yielding 85,456 training samples and 294 test samples."}, {"title": "DEVELOPMENT OF MMRole-Agent", "content": "We fine-tune the QWen-VL-Chat model (Bai et al., 2023) using 8\u00d7A100 GPUs on the training set of MMRole-Data to develop our specialized MRPA, MMRole-Agent. The learning rate is set to 1e-5, and the training is conducted over 3 epochs. Besides, to understand the detailed character profiles and dialogue history, the model maximum length is set to 3072. Other experimental setup and code remain the same as Bai et al. (2023)'s defaults."}, {"title": "EVALUATED MRPAS", "content": "To the best of our knowledge, no specialized MRPA has been developed prior to this work. Therefore, our experiments evaluate MMRole-Agent and various existing general-dialogue LMMs across different parameter scales. As presented in Table 3, we select four well-known closed-source LMMs with over 100 billion parameters (Achiam et al., 2023; Team et al., 2023; Anthropic, 2024; Bai et al., 2023), and six widely-used open-source LMMs with tens of billions or billions of parameters (Liu et al., 2024a; Bai et al., 2023; Young et al., 2024; Chen et al., 2024c). For the closed-source models, we utilize their official APIs to conduct performance evaluations. To ensure fairness, each MRPA is queried with the same prompt, as detailed in Appendix E."}, {"title": "REWARD MODEL IN MMRole-Eval", "content": "To develop the reward model, we initially utilize GPT-4 to evaluate various general-dialogue LMMs discussed in Section 6.3 across 294 test samples. Statistically, these evaluation trajectories are converted into a total of 23,520 samples, where 320 samples are reserved for validation, with the rest utilized for training. The validation set includes 20 questions, where the responses of two models are randomly selected for each question, and each response is evaluated on all 8 metrics. Subsequently, we fine-tune another QWen-VL-Chat model. The experimental setup and code are the same as those used for developing MMRole-Agent, except that the model maximum length is set to 4096, and the training is conducted over 10 epochs."}, {"title": "EVALUATION RESULTS AND ANALYSES", "content": "As shown in Table 5, we report the average results across all test samples for each evaluated MRPA, along with the detailed results on both the in-distribution test set (In-Test) and the out-of-distribution test set (Out-Test) for our MMRole-Agent. Notably, although some of the scores of MRPAs exceed 1, this does not necessarily mean that their performance is superior to that of GPT-4, which we use to construct MMRole-Data. To generate multi-turn dialogue data, we use a single GPT-4 API call to produce the entire dialogue directly. In contrast, when testing MRPAs, we supply dialogue histories and require MRPAs to generate responses. This method is relatively easier, but it is challenging to ensure the consistency of multi-turn dialogues if using this approach for data construction.\nIn the MRPA group with over 100 billion parameters, Claude 3 Opus exhibits superior performance. Meanwhile, in the MRPA group with tens of billions of parameters, LLaVA-NeXT-34B achieves the highest performance. Finally, in the MRPA group with billions of parameters, MMRole-Agent is the best. Notably, LLaVA-NeXT-34B outperforms Gemini Pro Vision, while LLaVA-NeXT-Mistral-7B surpasses Yi-VL-34B. This demonstrates that both the training methods and training data are equally important for enhancing LMMs, rather than merely expanding the model size.\nMoreover, the overall score of MMRole-Agent reaches 0.994, marking a significant improvement of 0.151 compared to its base model, QWen-VL-Chat. MMRole-Agent successfully acquires various"}, {"title": "CONCLUSION", "content": "In this paper, we propose the concept of Multimodal Role-Playing Agents (MRPAs) for the first time by extending RPAs with multimodal understanding abilities. Moreover, we construct MMRole-Data, a large-scale, high-quality dataset for developing and evaluating MRPAs. To stably and comprehensively assess MRPAs, we introduce MMRole-Eval, a robust evaluation method using a specialized reward model, comprising eight metrics across three dimensions. Comprehensive evaluation results indicate that MMRole-Agent, the first specialized MRPA, exhibits improved performance and strong generalization capabilities. Additionally, multimodal understanding abilities and role-playing qualities are more challenging aspects that require attention in the development of MRPAs."}]}