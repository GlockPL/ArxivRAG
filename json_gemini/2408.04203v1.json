{"title": "MMRole: A COMPREHENSIVE FRAMEWORK FOR\nDEVELOPING AND EVALUATING MULTIMODAL\nROLE-PLAYING AGENTS", "authors": ["Yanqi Dai", "Huanran Hu", "Lei Wang", "Shengjie Jin", "Xu Chen", "Zhiwu Lu"], "abstract": "Recently, Role-Playing Agents (RPAs) have garnered increasing attention for their\npotential to deliver emotional value and facilitate sociological research. However,\nexisting studies are primarily confined to the textual modality, unable to simulate\nhumans' multimodal perceptual capabilities. To bridge this gap, we introduce the\nconcept of Multimodal Role-Playing Agents (MRPAs), and propose a comprehen-\nsive framework, MMRole, for their development and evaluation, which comprises\na personalized multimodal dataset and a robust evaluation method. Specifically,\nwe construct a large-scale, high-quality dataset, MMRole-Data, consisting of 85\ncharacters, 11K images, and 14K single or multi-turn dialogues. Additionally, we\npresent a robust evaluation method, MMRole-Eval, encompassing eight metrics\nacross three dimensions, where a reward model is trained to score MRPAs with\nthe constructed ground-truth data for comparison. Moreover, we develop the first\nspecialized MRPA, MMRole-Agent. Extensive evaluation results demonstrate the\nimproved performance of MMRole-Agent and highlight the primary challenges in\ndeveloping MRPAs, emphasizing the need for enhanced multimodal understand-\ning and role-playing consistency. The data, code, and models will be available at\nhttps://github.com/YanqiDai/MMRole.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancement of large language models (LLMs) (Zhao et al., 2023) has significantly catalyzed\nthe rise of Role-Playing Agents (RPAs) (Chen et al., 2024b), which are engineered to emulate spe-\ncific characters and engage in dialogues with human users or other characters. Unlike AI produc-\ntivity assistants, RPAs primarily focus on delivering emotional value (Li et al., 2023a; Wang et al.,\n2023; Shao et al., 2023) and facilitating sociological research (Zhou et al., 2023b; Wang et al., 2024a;\nChen et al., 2024a; Gu et al., 2024), where typical applications include emotional companions, NPCs\nin video games, digital clones, and social simulations.\nThe primary characteristic of RPAs is their capability to engage in human-like and immersive in-\nteractions. However, existing studies in role-playing are primarily confined to the textual modality,\nwhich has considerable limitations. In the real-world context, human perception integrates multiple\nmodalities, especially visual and textual, allowing for a more direct and comprehensive understand-\ning of the environment than text alone can provide. Therefore, enhancing RPAs with multimodal\ncapabilities is a crucial next step for conducting more realistic and engaging interactions.\nIn this paper, we introduce the concept of Multimodal Role-Playing Agents (MRPAs). MRPAS are\ndesigned to emulate specific characters and engage in dialogues centered around images, with either\nhuman users or other characters. Moreover, we propose MMRole, a comprehensive framework for\ndeveloping and evaluating MRPAS. As presented in Figure 1, this framework includes two principal\ncomponents: a large-scale, high-quality dataset and a robust evaluation method for MRPAs.\nDataset Construction: The dataset for MRPAs comprises character profiles, images, and dialogues\ncentered around images. Specifically, we consider three categories of characters: fictional charac-\nters, historical and public figures, and hypothetical real-life characters. The profiles of the first two"}, {"title": "2 RELATED WORK", "content": "Role-Playing Agents. Recent advancements in large language models (LLMs) (Zhao et al., 2023),\nsuch as supervised fine-tuning (Wei et al., 2021) and in-context learning (Brown et al., 2020), have"}, {"title": "3 MULTIMODAL ROLE-PLAYING AGENTS", "content": "Role-Playing Agents (RPAs) are engineered to emulate specific characters and engage in dialogues\nwith either human users or other characters. Expanding on this concept, Multimodal Role-Playing\nAgents (MRPAs) incorporate the capacity to comprehend vision-language multimodal information.\nThis capacity enables dialogues that are centered around and informed by images. From another per-\nspective, compared to traditional multimodal question answering, multimodal role-playing includes\ncharacter profile input, adding greater complexity and depth to the interaction.\nIn scenarios where the dialogue partner is a human user without a specific identity, given an image\n$I$, the profile $P$ of the designated character $C$, and the dialogue context $D$, the MRPA steps into the\nshoes of the character $C$, responding to the human user pertaining to the image $I$:\n$D = [h_1, m_1, h_2, M_2, ..., m_n],$\n(1)\n$m_n = MRPA(I, P, D),$\n(2)\nwhere $D$ is a sequence of conversation pairs, with $h_i$ and $m_i$ representing the $i$-th utterances from\nthe human user and the MRPA, respectively.\nConversely, in scenarios where the dialogue partner is another character $C_{other}$, given an image $I$,\nthe profile $P$ of the designated character $C$, the profile $P_{other}$ of the character $C_{other}$, and the dialogue\ncontext $D$, the MRPA steps into the shoes of the character $C$ and interacts with the character $C_{other}$,"}, {"title": "4 MMRole-Data: DATASET CONSTRUCTION", "content": "As shown in 1(a), we construct MMRole-Data, a large-scale, high-quality multimodal role-playing\ndataset. In this section, we first provide a detailed classification of characters and dialogue scenarios\nconsidered in MMRole-Data, then describe the pipelines for character profile generation and image\ncollection and annotation, as well as the methodology for dialogue generation and filtering."}, {"title": "4.1 CHARACTERS AND DIALOGUE SCENARIOS", "content": "We consider three categories of characters: (1) Fictional Characters, characters created in fictional\nmedia such as literature, films, and games; (2) Historical and Public Figures, individuals who are\nspecifically documented in historical records or well-known in real life; (3) Hypothetical Real-Life\nCharacters, hypothetical individuals who are not explicitly known but could exist in real life.\nThe first two categories have been explored in previous role-playing research. Moreover, we propose\nthe third category to enhance and evaluate MRPAs in characters that are not widely recognized. To\neffectively emulate hypothetical real-life characters, MRPAs must deeply understand and align with\nthe provided character profiles, rather than relying on their inherent world knowledge.\nAs depicted in Figure 2, we introduce three types of dialogue scenarios consistently centered around\nimages: (1) Commentary Interactions, single-turn dialogues where a character offers comments or\nreflections centered around an image, without any further interaction; (2) Human-Role Dialogues,\nmulti-turn dialogues centered around an image between a human user without a specific identity and\na character; (3) Inter-Role Dialogues, multi-turn dialogues centered around an image between two\ncharacters from the same series."}, {"title": "4.2 CHARACTER PROFILE GENERATION", "content": "Character profiles are crucial for the role-playing effectiveness of MRPAs, especially for those char-\nacters with which MRPAs are not familiar. To facilitate a thorough understanding of the designated\ncharacters, our character profiles encompass five core parts: brief introduction, personality, life story,\nmain interpersonal relationships, and catchphrases, which are exampled in Appendix B.\nAs discussed in Section 4.1, three categories of characters are considered in MMRole. The majority\nof these characters are English, with a smaller proportion being Chinese. For fictional characters,"}, {"title": "4.3 IMAGE COLLECTION AND ANNOTATION", "content": "For each character, we utilize distinct generic images from MS-COCO (Lin et al., 2014) to ensure\ncomprehensive coverage of a wide range of visual concepts. Additionally, we manually collect and\nannotate various character-related images, which can evoke the personal experiences and emotions\nof the characters more effectively. Specifically, we collect production stills for fictional characters,\nweb illustrations for historical and public figures, and news photos for hypothetical real-life charac-\nters. Moreover, as presented in Figure 2, the information of characters, place, and scene is manually\nannotated for each character-related image."}, {"title": "4.4 DIALOGUE GENERATION AND FILTERING", "content": "As discussed in Section 4.1, three types of dialogue scenarios are introduced in MMRole. Based on\nthe character profiles and images, GPT-4 generates dialogues corresponding to each scenario type.\nInterestingly, we observe that using the prompt, \u201cYou are a dedicated role-playing assistant...Please\nstep into the shoes of {character} from {series}\u201d yields better results than the simpler prompt, \u201cYou\nare {character} from {series}\u201d. We suggest that the training data supplied by OpenAI optimizes\nGPT-4 to function more effectively as a helpful assistant, rather than as an immersive, human-like\ncharacter. The prompts for dialogue generation are detailed in Appendix D.\nTo ensure accuracy and reliability, we manually filter all dialogues using several strategies. Specif-\nically, we remove failed response data, as well as non-Chinese and non-English data. Additionally,\nwe eliminate content that replies in the tone of an AI assistant, meaningless modal words frequently\noutput by GPT-4, action and scene descriptions, and unnecessary explanatory prefixes and suffixes."}, {"title": "5 MMRole-Eval: PERFORMANCE EVALUATION", "content": "As illustrated in Figure 1(b), we propose MMRole-Eval, a robust evaluation method to stably and\ncomprehensively assess MRPAs. In this section, we introduce eight evaluation metrics across three\ndimensions and the approach for score quantification."}, {"title": "5.1 EVALUATION METRICS", "content": "In contrast to textual RPAs, MRPAs must not only accurately emulate specific characters but also\ndeeply comprehend both visual and textual information. Therefore, we propose a three-dimensional\nevaluation system, encompassing fundamental conversational skills, multimodal understanding abil-\nities, and role-playing qualities.\nThe fundamental conversational skills of MRPAs present their capacity to sustain fluent and coherent\ninteractions within role-playing scenarios, which are assessed by three metrics:\n\u2022 Instruction Adherence (IA): Do the responses accurately adhere to the task instruction, directly\nrole-playing as the character and including only words that the character would say, without any\nunnecessary explanatory prefixes or suffixes?\n\u2022 Fluency (Flu): Are the responses grammatically correct and articulated smoothly?\n\u2022 Coherency (Coh): Do the responses maintain a coherent thread of dialogue without contradicting\nprevious turns or containing internal inconsistencies within the current responses?\nThe multimodal understanding abilities of MRPAs indicate their capacity to effectively integrate and\ninterpret both visual and textual information, which are assessed by two metrics:"}, {"title": "5.2 SCORE QUANTIFICATION", "content": "To quantitatively evaluate the performance of RPAs across various metrics, existing methods employ\nreward models or human annotators to directly score outputs without a ground truth (Zhou et al.,\n2023a; Tu et al., 2024). However, it may be unstable due to the variability of scoring criteria without\na baseline for comparison. Therefore, we propose to develop a more stable reward model. Inspired\nby the evaluation methods of Vicuna (Chiang et al., 2023) and LLaVA (Liu et al., 2024b), our reward\nmodel first conduct a brief qualitative assessment of the relative performance between the evaluated\nMRPA and the constructed ground-truth data for each metric, followed by assigning a quantitative\nscore pair. The final score of the MRPA is the ratio of the two scores within the score pair.\nTo develop the reward model, we initially employ GPT-4 to assess various MRPAs across all test\nsamples. For each evaluated MRPA and corresponding test sample, GPT-4 outputs brief assessments\nand score pairs for all metrics through a single API call. The prompt for GPT-4 scoring is provided\nin Appendix F. Subsequently, these evaluation trajectories are converted into training and validation\ndata for our reward model. Each evaluation trajectory is segmented into eight samples, with each\nsample evaluating a distinct metric. Compared to directly applying GPT-4 as the reward model, this\napproach renders MMRole-Eval both open-source and cost-effective."}, {"title": "6 EXPERIMENTS", "content": null}, {"title": "6.1 STATISTICS OF MMRole-Data", "content": "Table 1 presents the statistics of the MMRole-Data dataset. Totally, the dataset comprises 85 charac-\nters, 11,032 images, and 14,346 dialogues, yielding 85,456 training samples and 294 test samples."}, {"title": "6.2 DEVELOPMENT OF MMRole-Agent", "content": "We fine-tune the QWen-VL-Chat model (Bai et al., 2023) using 8\u00d7A100 GPUs on the training set of\nMMRole-Data to develop our specialized MRPA, MMRole-Agent. The learning rate is set to 1e-5,\nand the training is conducted over 3 epochs. Besides, to understand the detailed character profiles\nand dialogue history, the model maximum length is set to 3072. Other experimental setup and code\nremain the same as Bai et al. (2023)'s defaults."}, {"title": "6.3 EVALUATED MRPAS", "content": "To the best of our knowledge, no specialized MRPA has been developed prior to this work. There-\nfore, our experiments evaluate MMRole-Agent and various existing general-dialogue LMMs across\ndifferent parameter scales. As presented in Table 3, we select four well-known closed-source LMMs\nwith over 100 billion parameters (Achiam et al., 2023; Team et al., 2023; Anthropic, 2024; Bai et al.,\n2023), and six widely-used open-source LMMs with tens of billions or billions of parameters (Liu\net al., 2024a; Bai et al., 2023; Young et al., 2024; Chen et al., 2024c). For the closed-source models,\nwe utilize their official APIs to conduct performance evaluations. To ensure fairness, each MRPA is\nqueried with the same prompt, as detailed in Appendix E."}, {"title": "6.4 REWARD MODEL IN MMRole-Eval", "content": "To develop the reward model, we initially utilize GPT-4 to evaluate various general-dialogue LMMs\ndiscussed in Section 6.3 across 294 test samples. Statistically, these evaluation trajectories are con-\nverted into a total of 23,520 samples, where 320 samples are reserved for validation, with the rest\nutilized for training. The validation set includes 20 questions, where the responses of two models are\nrandomly selected for each question, and each response is evaluated on all 8 metrics. Subsequently,\nwe fine-tune another QWen-VL-Chat model. The experimental setup and code are the same as those\nused for developing MMRole-Agent, except that the model maximum length is set to 4096, and the\ntraining is conducted over 10 epochs."}, {"title": "6.5 EVALUATION RESULTS AND ANALYSES", "content": "As shown in Table 5, we report the average results across all test samples for each evaluated MRPA,\nalong with the detailed results on both the in-distribution test set (In-Test) and the out-of-distribution\ntest set (Out-Test) for our MMRole-Agent. Notably, although some of the scores of MRPAs exceed\n1, this does not necessarily mean that their performance is superior to that of GPT-4, which we use\nto construct MMRole-Data. To generate multi-turn dialogue data, we use a single GPT-4 API call to\nproduce the entire dialogue directly. In contrast, when testing MRPAs, we supply dialogue histories\nand require MRPAs to generate responses. This method is relatively easier, but it is challenging to\nensure the consistency of multi-turn dialogues if using this approach for data construction.\nIn the MRPA group with over 100 billion parameters, Claude 3 Opus exhibits superior performance.\nMeanwhile, in the MRPA group with tens of billions of parameters, LLaVA-NeXT-34B achieves the\nhighest performance. Finally, in the MRPA group with billions of parameters, MMRole-Agent is the\nbest. Notably, LLaVA-NeXT-34B outperforms Gemini Pro Vision, while LLaVA-NeXT-Mistral-7B\nsurpasses Yi-VL-34B. This demonstrates that both the training methods and training data are equally\nimportant for enhancing LMMs, rather than merely expanding the model size.\nMoreover, the overall score of MMRole-Agent reaches 0.994, marking a significant improvement of\n0.151 compared to its base model, QWen-VL-Chat. MMRole-Agent successfully acquires various"}, {"title": "7 CONCLUSION", "content": "In this paper, we propose the concept of Multimodal Role-Playing Agents (MRPAs) for the first time\nby extending RPAs with multimodal understanding abilities. Moreover, we construct MMRole-Data,\na large-scale, high-quality dataset for developing and evaluating MRPAs. To stably and comprehen-\nsively assess MRPAs, we introduce MMRole-Eval, a robust evaluation method using a specialized\nreward model, comprising eight metrics across three dimensions. Comprehensive evaluation results\nindicate that MMRole-Agent, the first specialized MRPA, exhibits improved performance and strong\ngeneralization capabilities. Additionally, multimodal understanding abilities and role-playing qual-\nities are more challenging aspects that require attention in the development of MRPAS."}, {"title": "A LIST OF CHARACTERS", "content": "Figure 4 lists all character constructed in MMRole-Data, with the series to which the characters in\nthe out-of-distribution test set belong being underlined."}, {"title": "B EXAMPLES OF CHARACTER PROFILES", "content": "Figure 5 presents the profile of Iron Man from The Avengers, whereas Figure 6 illustrates the profile\nof Li Bai from Tang Dynasty of China. The character profiles include five core parts: brief introduc-\ntion, personality, life story, main interpersonal relationships, and catchphrases, undergoing rigorous\nmanual quality control to ensure accuracy and reliability."}, {"title": "C EXAMPLES OF THE TWO-STAGE GENERATION PROCESS FOR\nHYPOTHETICAL REAL-LIFE CHARACTERS", "content": "Figure 7 and Figure 8 presents the prompts used to generate meta information and expand detailed\nprofiles for hypothetical real-life characters, whereas Figure 9 exemplifies meta information for five\nrandomly selected hypothetical real-life characters."}, {"title": "D PROMPTS FOR DIALOGUE GENERATION", "content": "Figure 10 presents the prompts used to generate dialogues for the three types of scenarios involv-\ning English fictional characters, as well as historical and public figures. The prompts for Chinese\ncharacters and hypothetical real-life characters are similar to the ones provided here."}, {"title": "E PROMPTS FOR QUERYING MRPAS", "content": "Figure 11 details the prompt used to query MRPAs in human-role dialogues involving English fic-\ntional characters, as well as historical and public figures. The prompts for Chinese characters and\nhypothetical real-life characters are similar to the ones provided here."}, {"title": "F PROMPTS FOR GPT-4 SCORING", "content": "Figure 12 illustrates the prompt used to score MRPAs by GPT-4 in human-role dialogues involving\nfictional characters, as well as historical and public figures. The prompts for hypothetical real-life\ncharacters are similar to the ones provided here."}, {"title": "G PROMPTS FOR REWARD MODEL SCORING", "content": "Figure 13 details the prompt used to score MRPAs for Personality Consistency by the reward model\nin human-role dialogues involving fictional characters, as well as historical and public figures. The\nprompts for other metrics and hypothetical real-life characters are similar to the ones provided here."}]}