{"title": "KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data", "authors": ["Grace Tang", "Swetha Rajkumar", "Yifei Zhou", "Homer Rich Walke", "Sergey Levine", "Kuan Fang"], "abstract": "Building generalist robotic systems involves effectively endowing robots with the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIE controls the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.", "sections": [{"title": "I. INTRODUCTION", "content": "The capability to handle an open set of objects, behaviors, and task specifications is essential to the development of generalist robotic systems. Existing learning methods for robotic control can require extensive amounts of data collected on embodied systems [21, 29, 2]. The diversity and quality of the collected data determines the generalization capability that these methods can achieve, which is subject to robotics expertise and manual labor that humans can provide. How can we endow robots with generalizable skills for solving an open set of tasks in a scalable manner?\nLarge pre-trained models offer promising tools with their generalist visual understanding and commonsense reasoning abilities [28, 35, 39, 49]. Prior works have shown that pre-trained Large Language Models (LLMs) and Vision Language Models (VLMs) can be directly applied to robotics control through prompt engineering in a zero-shot manner [23, 13, 10, 32]. However, despite the capability of generalizing to unseen tasks, such systems often suffer from instability and require significant hard-coded domain knowledge to compensate for the pre-trained models' limited knowledge about robotic control. While fine-tuning on robotic data can mitigate this issue and proves more sample-efficient than training policies from scratch [3, 27], the largest available datasets [7, 15, 37] for robotic control are still far from comparable to the Internet-scale data used for pre-training the large models with billions of parameters. It remains a grand challenge to effectively employ and adapt pre-trained large models for robotic control.\nIn this paper, we aim to study an alternative solution to this challenge by training large models for manipulation without data collected on robots. Our key insight is to use visual affordances to guide robotic control and leverage the broad knowledge incorporated in large pre-trained models to efficiently learn to predict the affordances. Built upon point-based affordance representations defined on 2D images from Manuelli et al. [25] and Fang et al. [10], we fine-tune a VLM on labeled affordance data. Humans can easily collect such affordance data by randomizing scenes for a target task (e.g., cleaning a table), taking images through the camera, and annotating the affordances on the image, which bypasses the need for collecting demonstration trajectories through teleoperation of the robots or hard-coded policies. The main challenges are 1) how to repurpose the VLM pre-trained for visual-question answering for efficient affordance learning,"}, {"title": "II. RELATED WORK", "content": "An increasing number of works have employed foundation models in robotics through pre-training or fine-tuning on robot data manually collected through teleoperation or scripted policies [3, 9, 27, 33]. Due to the lack of datasets that can cover the vast complexity and diversity of robotics applications, most of the approaches focus on specific task categories such as grasping and object rearrangements. The generalization capability of the trained model is also constrained to the distribution of objects and environments covered by the manually collected datasets. Alternatively, other works have attempted to combine and prompt pre-trained models to solve unseen tasks in zero-shot manners [23, 13, 10, 32, 12]. However, the performance of these works is usually subject to the capability of the pre-trained models, as well as non-trivial expert knowledge and manual labor to design prompts and in-context examples. In contrast to these approaches, the proposed method fine-tunes a VLM to robustly solve the target tasks. To avoid the need of collecting extensive amount of robot data, our model trains the VLM to predict the point-based affordance representation from [10] and employs a diffusion model to automatically synthesize massive, high-quality data.\nDue to their versatile nature, Vision Language Models (VLMs) can be effectively fine-tuned to accommodate a variety of downstream tasks [11, 46, 4, 45]. Specifically, to facilitate the prediction of spatially grounded outputs, previous research has investigated methods such as sets of marks [42], scaffolding [28], and coordinate-based bounding boxes [38, 5, 39]. Of these methods, coordinate-based references are particularly adaptable, capable of pinpointing any location within an image. Thus, we employ this approach for predicting keypoints. Our contribution does not lie in proposing a new method for fine-tuning VLMs, but rather in integrating and adapting VLM fine-tuning for robotic control.\nTo alleviate the data bottleneck in robot learning, prior works have investigated various approaches to augment and synthesize data. Data augmentation, especially random image transformations, have been widely used to improve generalization to unseen visual inputs [19, 20]. These random operations can effectively improve the models generalization capabilities to unseen visual inputs during test time, but cannot extend the model's capabilities beyond the coverage of the training distribution. Domain randomization has also been broadly used to train robust models for robotic control [36, 26, 1, 30] when collecting simulated robot experiences. In contrast, our method directly diversifies the training data without the need for a physical simulator. Recent works in computer vision and robotics have leveraged deep generative models, such as diffusion models, to synthesize unseen environments by leveraging broad knowledge learned from Internet-scale images [14, 48, 40, 43, 44, 8, 41, 16, 22]. While the prior work can easily generate defected samples, we propose an affordance-aware data synthesis approach that enables the diffusion model to generate diverse data with much higher quality and consistent affordance annotations."}, {"title": "\u0399\u0399\u0399. \u039a\u0395\u03a5\u03a1OINT AFFORDANCE LEARNING FROM IMAGINED ENVIRONMENTS", "content": "We propose Keypoint Affordance Learning from Imagined Environments (KALIE) to adapt pre-trained Vision Language Models (VLMs) to acquire generalizable skills without robot experiences. In this section, we will first define the affordance prediction problem in the few-shot setting using point-based affordances labeled by human experts. Next, we will introduce a novel affordance-aware data synthesis recipe to diversify the training data, which automatically generates massive high-quality data based on the example data collected only for limited scenarios. Then, we will describe our VLM fine-tuning approach and discuss the key design options. Lastly, we summarize the overall system at the end of this section.\nWe consider the problem of open-world robotic manipulation involving unseen objects and initial arrangements of the scenes. As shown in Fig. 1, each task is single-stage and is specified by a free-form language description l, such as \"Use the brush to sweep the snack package.\" The robot observes an RGBD image from a third-person camera and performs a 6-DoF motion trajectory in open loop to complete the task.\nTo tackle this problem, we employ a VLM f pre-trained on Internet-scale data [39]. Following the practice of [10], we query the VLM to produce point-based affordance representations to guide a motion planner to generate motions. The VLM takes as inputs the prompt p, the task instruction l, and the input image s and predict the affordance representation \u0177 as:\n$\\hat{y} = f(p, l, s),$\nwhere y contains a set of keypoints, including the grasp keypoint, the function keypoint, the target keypoint, the pre-contact waypoint, and the post-contact waypoint, specified by 2D coordinates on the image (see Fig. 1). Additional properties such as the height and the orientation of the gripper will also be decided for each task. Based on the affordance representation, a low-level motion generator computes a motion trajectory to complete the task. We assume the desired motion to solve each task can be specified by the same subset of these points (e.g., the sweeping with a brush requires all five points, and drawer closing requires everything but the grasp point, as shown in Section IV-E), but the specific coordinates of these points depend on the objects and their poses with respect to the robot.\nIn this work, we consider tasks and objects which are challenging for VLMs to handle in zero-shot. In contrast to [10], we consider a few-shot learning setting, in which we fine-tune the pre-trained VLM to acquire and improve skills for unseen scenarios on limited and non-robot data. Notably, the point-based representations enable us to outsource motion generation to the low-level motion planner and focus on only the affordance prediction problem. Therefore, training only requires data of pairs of an observed image s and the ground truth keypoints y collected by human experts or generated automatically as described below. We assume access to an example dataset D containing a limited number of (s, y) pairs for each target task. In our experiments, we assume the most extreme case, in which the data is collected on a single set of objects for each task and the fine-tuned model is evaluated on unseen object sets.\nTo scale up the training of the VLM to enhance its generalization capabilities to unseen scenarios, we automatically synthesize a training dataset D' to cover a wide range of environments. Each of the new data points (s', y') \u2208 D' is synthesized by modifying an existing data point (s, y) \u2208 D collected by human experts.\nFollowing the practice of [44], we compute the segmentation mask of the object in the scene using open-vocabulary segmentation [18] and then inpaint the masked region with a diffusion model [31]. However, naively inpainting the masked region can lead to undesired results. Without a direct mechanism to specify the geometric properties of the object, it would be hard to diversify the inpainted images in a way that can cover the desired distribution of testing scenarios. Moreover, there is usually a discrepancy between the appearance of the inpainted object and the original key-point annotation, introducing the need to manually re-label keypoints. To generate massive, high-quality data without additional manual labor, we need to ensure that the generated images stay faithful to the context of the target task and the annotated keypoints, while aggressively diversifying the environments as much as possible.\nTo tackle this challenge, we design an affordance-aware data synthesis pipeline by leveraging additional context images to guide the generation process, as shown in Fig. 2.\nSpecifically, we employ the ControlNet [47] diffusion model g, which takes inputs as the input image s, the segmentation mask m, a context image c, and a language description of the object o and generates the new image as s' ~ g(\u00b7|s, m, c, o). We would like e to be a compact representation of the object's geometric properties that provide clues about the affordances, while minimizing other detailed visual information to leave enough free reign for the diffusion model. By inpainting an image s' in accordance with c, we hope to obtain new objects of the same point-based affordances. In this work, we choose to use a soft edge map as e computed by an external image processing algorithm [34], which outlines the contours and parts of the object.\nTo cover testing objects of unseen shapes, we introduce additional randomization operations to the geometry of the synthesized object. Directly modifying either the object's appearance in the pixel space can easily affect other parts of the image and create artifacts. Instead, we propose to apply transformation h(\u00b7) on the compact context c before calling the diffusion model G to inpaint the image. The transformation function h(\u00b7) can include basic data augmentation operations such as random scaling, translation, and rotation, as well as additional operations such as elastic distortion. To modify the context c, we transform the region under the masked as h(m * c). The region to be inpainted on the original image now becomes m + h(m), which includes the transformed silhouette of the object h(m) in addition to the white space left by removing the original object m. Accordingly, we also apply the same transformation to the annotated keypoints y with the context image to keep them consistent. Therefore, the sampling process with the transformation can be written as:\ns' ~ g(\u00b7|s, h(m) + m, h(m * c), o),\ny = h(y),\nwhere we slightly overload the notation by using h(.) to denote the transformation applied to both the images and the keypoint coordinates.\nWe fine-tune the VLM f to predict point-based affordance representations. To adapt f, which is pre-trained for visual-question answering by predicting tokens, we need to make our design choices around two considerations. First, how to represent the point-based affordances so that we can"}, {"title": "IV. EXPERIMENTS", "content": "We design our experiments to investigate the following questions: 1) Can KALIE synthesize diverse and high-quality data for affordance prediction? 2) Can KALIE fine-tune pre-trained VLMs to improve their performances on challenging manipulation tasks with unseen objects? 3) What design options are critical to the performance of KALIE?\nEnvironments and tasks. Our experiments are conducted in a real-world table-top manipulation environment with a 7-DoF robot arm and a Robotiq 2F-85 gripper. A top-down"}, {"title": "V. CONCLUSION", "content": "In this work, we propose KALIE to fine-tune pre-trained VLMs to predict affordances for robotic manipulation with open sets of objects and initial arrangements of scenes. Using the proposed affordance-aware data synthesis pipeline, we generate massive high-quality data to scale up training without extensive manual labor or domain expertise with robots. We fine-tune the VLM on the generated data to predict the point-based affordance representations. Across various manipulation tasks involving tool use, deformable objects, and articulated objects, we demonstrate KALIE can robustly solve the task and consistently outperform baselines. We hope KALIE will inspire future research towards adapting vision-language models for open-world robotic control.\nThe current affordance representation in KALIE is still limited to single-arm table-top manipulation. To apply KALIE to more complicated scenarios, such as dynamic manipulation and whole-body control, we would need to extend the design of the affordance representation. In addition, there is still a large discrepancy between the open-source VLM fine-tuned by KALIE and the state-of-the-art VLMs without accessible fine-tuning APIs. Lastly, we have only discussed few-shot generalization in this paper, and would need to train KALIE on more diverse set of tasks to generalize to unseen target tasks in a zero-shot manner. In the future, we hope to apply KALIE to more powerful VLMs to further improve its performance."}]}