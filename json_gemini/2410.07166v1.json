{"title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making", "authors": ["Manling Li", "Shiyu Zhao", "Qineng Wang", "Kangrui Wang", "Yu Zhou", "Sanjana Srivastava", "Cem Gokmen", "Tony Lee", "Li Erran Li", "Ruohan Zhang", "Weiyu Liu", "Percy Liang", "Li Fei-Fei", "Jiyuan Mao", "Jiajun Wu"], "abstract": "We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights for effective and selective use of LLMs in embodied decision making.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have emerged as powerful tools for building embodied decision-making agents capable of following human instructions (such as \u201ccleaning the refrigerator\u201d, \u201cpolishing furniture\u201d) and achieving the specified goals through a sequence of actions in various digital and physical environments [1-3]. Despite many reports of their success, our understanding of LLMs' full capabilities and limitations in embodied decision-making remains limited. Existing evaluation methods fall short of providing a comprehensive insight due to three key limitations: the lack of standardization of 1) embodied decision-making tasks, 2) modules that an LLM can interface with or be implemented for, and 3) fine-grained evaluation metrics beyond a single success rate. In this paper, we propose EMBODIED AGENT INTERFACE, to address these challenges.\n(1) Standardization of goal specifications: We want embodied agents to achieve goals. However, the specification of goals and the criteria for agents' success evaluation vary significantly across different domains, even for similar tasks. For example, BEHAVIOR [4] focuses on achieving a state that satisfies certain state goals (e.g., \u201cnot stained(fridge)\" in Figure 1), while VirtualHome [5] uses temporally extended goals by imposing temporal order constraints on actions. We include an extended\""}, {"title": "2 Embodied Agent Interface Based on LTL", "content": "Table 1 summarizes our EMBODIED AGENT INTERFACE. First, we define an embodied decision-making problem representation $(U, S, A, \\partial, \\phi, \\bar{a})$, which is a language-based, object-centric abstraction for embodied agent environments with objects $(o \\in U)$, states $(s \\in S)$, actions $(a \\in A)$, goal $g$, subgoal $\\phi$, and trajectories $\\bar{a}$. Second, we formally define four ability modules $(\\mathcal{G}, \\Phi, \\mathcal{Q}, \\mathcal{T})$, including their standardized input-output specifications. They are fundamental and commonly-used modules that LLMs can be implemented for and interface with: the goal interpretation module $\\mathcal{G}$, the action sequencing module $\\mathcal{Q}$, the subgoal decomposition module $\\Phi$, and the transition modeling module $\\mathcal{T}$. In this paper, we focus on object-centric modeling: states are described as relational features among entities in the environment, actions are defined functions that take entity names as inputs and can be executed in the environment, goals and subgoals are defined as linear-temporal logic (LTL) [10] formulas on states and actions. We define each component in detail as follows.\n2.1 Representation for Objects, States and Actions\nIn EMBODIED AGENT INTERFACE, a state is represented as a tuple $s = (U, F)$, where U is the universe of objects, assumed to be a fixed finite set. $F$ is a set of relational Boolean features. Each $f \\in F$ is a table where each entry is associated with a tuple of objects $(o_1,\u2026,o_k)$. Each entry has the value of the feature in the state, and k is the arity of the feature. Actions can be viewed as primitive functions that take objects as inputs, denoted as $(name, args)$. Throughout the paper, we focus on tasks where states and actions are described in abstract language forms, including object states (e.g., is-open(cabinet1)), relations (e.g., is-on(rag0, window3)), and actions (e.g., soak(rag0)).\n2.2 Representation for Goals, Subgoals, Action Sequences, and State-Action Trajectories\nIn EMBODIED AGENT INTERFACE, goals g, subgoals $\\phi$, and action sequences $\\bar{a}$ are modeled as linear temporal logic (LTL) formulas. This is motivated by two critical desiderata. First, we need an expressive and compact language to describe both state-based and temporally extended goals. Second,\nwe need a unified interface between different LLM-based modules. LTL addresses both challenges.\nAt a high level, an LTL formula can describe state constraints (e.g., a subgoal should be achieved),"}, {"title": "Ability Module 1: Goal Interpretation $\\mathcal{G} : (s_0,l_g) \\rightarrow g$", "content": "Input-Output Specification. The goal interpretation module takes the state $s_0$ and a natural language instruction $l_g$ as input, and generates an LTL goal $\\hat{g}$, as a formal goal specification which a symbolic planner can conceivably take as input. In this paper, we only generate simple LTL goals formed by an ordered action sequence and a conjunction of propositions to be satisfied in the final state.\nEvaluation Metric. An LTL goal can be evaluated by directly comparing it with the ground truth goal $g$. While we have restricted generated $\\hat{g}$ to be simple LTL goals, we do not require the ground truth goal $g$ to be simple. Therefore, we additionally define $\\mathcal{G}$ that takes the object universe $U$ as input to translate $g$ to a set of simple LTL goals $g_0, g_1,..., g_k$ where all $g_i$'s entail $g$. We describe our implementation in the Appendix. Given two simple LTL goals $g_i$ and $\\hat{g}$, the accuracy of $\\hat{g}$ can be computed as an F\u2081 set-matching score between them. Let $g = a_1 then ... a_k then (p_1 \\land...p_e)$. We define $set(g) = \\{\\{a_i\\}_{i=1}^{k}\\} \\cup \\{p_i\\}_{i=1}^{e}$ (i.e., the action sequence $\\{a_i\\}$ is treated as a single element). The F\u2081 score between $g$ and $\\hat{g}$ is defined as: $F_1(g, \\hat{g}) = max_{g_i\\in \\mathcal{G}(g,U)} F_1(set(g_i), set(\\hat{g})) $."}, {"title": "Ability Module 2: Subgoal Decomposition $\\Phi : (s_0, g) \\rightarrow \\Phi$", "content": "Input-Output Specification. The subgoal decomposition module takes the task $(s_0, g)$ as input and generates a sequence of subgoals $\\phi = \\{\\phi_i\\}_{i=1}^{k}$, where each $\\phi_i$ is an LTL formula. The entire sequence $\\phi$ can also be represented as a single LTL formula. One may refer to Appendix G.3 for decomposition choice-making.\nEvaluation Metric. To evaluate the subgoal decomposition module, we use a customized planner to refine it into an action sequence $\\bar{a}$. This subgoal-action mapping function AM$(\\Phi, s_0)$ takes the LTL representation of $\\phi$ and $s_0$ and generates a state-action sequence $t$. We implement this with a breadth-first search. Then, we use the same metrics in action sequencing for evaluation: trajectory feasibility and goal satisfaction. Since each $\\phi_i$ can be grounded into different action sequences, we restrict the number of actions per subgoal to generate a finite set of possible action sequences $\\bar{a}_i$ satisfying $\\phi_i$. Then, we compute the metrics for each $\\bar{a}_i$ and report the maximum score across all $\\bar{a}_i$'s as the trajectory feasibility and the goal satisfaction scores for $\\phi$."}, {"title": "Ability Module 3: Action Sequencing $\\mathcal{Q} : (s_0, g), \\mathcal{M} \\rightarrow \\bar{a}$", "content": "Input-Output Specification. The action sequencing module takes the task $(s_0, g)$ as input, and the transition model $\\mathcal{M}$, and generates an action sequence $\\bar{a} = \\{a_i\\}_{i=1}^{k}$.\nEvaluation Metric. We use two evaluation metrics for the action sequencing module. First, the trajectory feasibility evaluation focuses on evaluating whether the trajectory is executable (i.e., all actions are feasible). We will execute the trajectory $\\bar{a}$ from $s_0$ in the simulator. When infeasible action presents, the execution may stop at an early step and we categorize the execution failure into missing steps, additional steps, wrong temporal order, and affordance errors."}, {"title": "Ability Module 4: Transition Modeling $\\mathcal{T} : (s_0, g), o \\rightarrow \\langle pre, eff \\rangle$", "content": "Input-Output Specification. The transition modeling module takes the task $(s_0, g)$ and a set of operator definitions $\\{o_i\\}$ as input, and generates a PDDL operator definition [11] for each $o_i$. In this module, we aim to create a formal definition of actions in order to generate plans to solve the task. During evaluation, we first extract relevant operator definitions, $\\{o'_i\\}$, based on the ground truth action trajectory $\\bar{a}$ associated with each task, with details provided in Appendix N.3. Then, the LLM generates the preconditions and effects $\\{(pre_i, eff_i)\\}$ for all operators $\\{o'_i\\}$.\nEvaluation Metric. The transition modeling module can be evaluated in two ways. First, the logic matching score for an operator $o_i$ compares the generated $pre_i$ and $eff_i$ against the ground truth operator definition annotated by human experts. This comparison uses a surface form matching score to produce an F\u2081-based score between two logic formulas. Intuitively, when both the LLM-generated $pre_i$ and ground truth $pre^\\ast_i$ are conjunctions of propositions, the F\u2081 score is computed as the set matching score between the sets of propositions. More complex logic formulas (e.g., $\\forall x.\\phi(x)$) are evaluated recursively, as detailed in Appendix N.3. The evaluation of effects is performed similarly.\nFurthermore, the planning success rate assesses whether the preconditions and effects of different operators enable a viable plan. This is computed by running an external PDDL planner [12] based on generated operator definitions to achieve $g$ from the initial state $s_0$. For simplicity, we only state goals in $g$ (and ignore action subgoals). The planning success rate is 1 if the planner finds a plan."}, {"title": "3 Dataset Annotations and Benchmark Implementations", "content": "Annotations. Focusing on complex long-horizon tasks, we select BEHAVIOR (B) and VirtualHome (V) as our evaluation simulators based on their task length and scene complexity. We include a comparison of different simulators and detailed selection considerations in Appendix M.1. Table 2 shows our annotations. Apart from the goal and trajectory annotations, we introduce the Goal Action annotation to reflect necessary actions that do not have post effects, such as the goal action touch in the task \"pet the cat\", as detailed in Appendix R.2. In the subset of VirtualHome tasks we work on, 80.7% task categories include instructions with action steps longer than 10, and 33% of the instructions have step lengths of more than 10.\nWe select BEHAVIOR as another simulator for our evaluation due to its task complexity. BEHAVIOR BDDL goals may contain quantifiers, such as (forpairs (?jar ?apple)\n(inside ?apple ?jar)), which need to be translated into grounded goals with only atomic propositions, e.g., and ((inside\napple_1 jar_1) (inside apple_2 jar_2)). There can be different grounded\ngoals that satisfy the same BDDL goal, such as ((inside apple_2 jar_1) (inside\napple_1 jar_2)). We call them goal options. In general, one BDDL goal corresponds to a\nnumber of goal options. The average number of grounded goals for each task is 6.7, and\nthere are 4,164.4 goal options for each task on\naverage. We show data distributions of goal\noptions and other statistics in Appendix R.1.\nImplementation on simulators. As BEHAVIOR does not have an action transition model layer,\nwe implemented a symbolic simulator with an action transition model layer. Our implementation,"}, {"title": "4 Results", "content": "We evaluate 15 open-weight and proprietary LLMs on four embodied agent ability modules across two benchmark simulators: BEHAVIOR and VirtualHome. Table 3 gives an overview. Table 4, Table 5, Table 6, and Table 7 break down the analysis of four representative LLMs on four ability modules. Figure 4 shows examples of different types of error. We start with the overall analysis.\nModel Comparison. Shown in Figure 3, the top performing models overall are o1-preview, Claude-3.5 Sonnet and GPT-40, with o1-preview leading in all aspects except object states and Gemini 1.5 Pro leading in its object state reasoning ability. Among all open-weight models, the best performing models are Llama-3-70B and Mistral-Large-2402, while there is still a performance gap with commercial models.\nAbility Comparison. 01-preview shows a clear advantage over other models, particularly on the BEHAVIOR simulator, where it achieves 74.9% compared to 64.2%. It leads in several areas, including goal interpretation on VirtualHome and both action sequencing and transition modeling on BEHAVIOR. Moreover, it outperforms in subgoal decomposition across both BEHAVIOR and VirtualHome simulators. In contrast, Claude-3.5 Sonnet shines in goal interpretation on BEHAVIOR and transition modeling on VirtualHome, while Mistral Large stands out in action sequencing on VirtualHome. Mixtral-8x22B shines in transition modeling among open-weight LLMs, and Llama-3- 70B Instruct in goal interpretation.\nWe also observe a performance gap between different simulators. Models achieve significantly lower trajectory feasibility scores on BEHAVIOR compared to VirtualHome, but achieve higher scores on goal interpretation. This is because BEHAVIOR tasks have a much longer horizon (avg 14.6 steps) while VirtualHome goals have a larger state space to search (such as \u201cwork\u201d), as detailed in Appendix M.4. It shows the inverse correlation between trajectory evaluation performance and sequence length, as well as between goal evaluation performance and environment complexity. We further perform a systematic analysis to discover the cofactors for the goal success rate, including the number of task goals, particularly node goals, the ground truth action length, and the task object length, with details in Appendix H.5.\nObject States vs Relationship. Relational goals are generally harder to reason about compared to object-state goals. Spatial relations have a significantly lower recall in the goal interpretation task (Table 4) and a lower goal satisfaction rate (Table 5). Some non-spatial relations (e.g., hold) are even more difficult for LLM to predict than spatial relations, as shown in the transition modeling accuracy (Table 6): for example holding(toothbrush) should be a precondition for brushing teeth."}, {"title": "5 Related Work", "content": "Recent work in embodied decision making has been using LLMs to perform various tasks, and we include a comprehensive summary in Appendix O, see also Table 8 for a quick summary. LLMs can also be used to combine multiple of the above modules at once via chain-of-thought prompting or pipelined queries, such as goal interpretation with action sequencing [13\u201332], goal interpretation with subgoal decomposition [2, 27, 33], action sequencing with subgoal decomposition [27, 34, 18, 35], action sequencing with transition modeling [8, 28, 32, 36, 37, 13, 38]. Our work aims to standardize the interface between LLMs and various decision-making modules to support the seamless integration, modular evaluation, and fine-grained metrics, aiming to provide implications on using LLMs in embodied decision making more effectively and selectively. We provide additional related work on agent interfaces [39-43, 18, 44, 42, 45] and simulation benchmarks in Appendix O."}, {"title": "6 Conclusions and Future Work", "content": "We propose a systematic evaluation framework EMBODIED AGENT INTERFACE to benchmark LLMs for embodied decision-making. It focuses on 1) standardizing goal specifications using LTL formulas, 2) unifying decision-making tasks through a standard interface and four fundamental ability modules, and 3) providing comprehensive fine-grained evaluation metrics and automatic error identification. We highlight the limitations of current LLMs in interpreting complex goals and different errors in reasoning, further attributing errors to various cofactors, including trajectory length, goal complexity, spatial relation goals, etc.\nLimitations and future work: Our current evaluation is limited to states, actions, and goals that can be described in abstract language terms, with the input environment abstracted by relational graphs of objects. Future work should extend this to include sensory inputs and actuation outputs, possibly by extending the studied model class to include Vision-Language Models (VLMs), which we discuss further in Appendix P. Other aspects of extension include the integration of memory systems (episodic memory and state memory), geometric reasoning, and navigation."}]}