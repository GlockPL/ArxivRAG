{"title": "A SIMPLE AERIAL DETECTION BASELINE OF MULTIMODAL LANGUAGE MODELS", "authors": ["Qingyun Li", "Yushi Chen", "Xinya Shu", "Dong Chen", "Xin He", "Yi Yu", "Xue Yang"], "abstract": "Abstract-The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images.", "sections": [{"title": "I. INTRODUCTION", "content": "Earth observation systems have acquired vast remote sensing (RS) data, driving demand for automated RS image interpretation. The advancement of artificial general intelligence (AGI) has motivated researchers in this field to develop general agents that are outstanding on multiple tasks, such as scene classification, visual question answering, and object detection.\nMultimodal language models (MLMs) are built upon vision and language foundation models, enabling them to process data from multiple modalities and interpret textual instructions effectively. The task results are outputted in textual form. By leveraging powerful pre-trained foundation models and a flexible text interface, MLMs are considered a key component in the advancement of AGI. Recently, MLMs have been introduced into the RS field. While existing RS MLMs have emphasized visual localization capabilities, their detection performance still requires significant improvement. Geochat presents the first open-source RS MLLM learned on tasks including scene classification, image captioning, region description, visual question answering, and visual grounding. The proposed model demonstrates the ability to detect partial objects corresponding to textual instructions but is unable to perform more intensive detection. EarthGPT extends visual inputs with multisensor RS modalities covering optical, synthetic aperture radar, infrared data. While they achieve detection results comparable to traditional detectors, their model is limited to single-class detection performance. SkySenseGPT introduces scene graph generation to RS MLMs to enhance ability of understanding relations between objects. They also provide tolerance detection accuracies under a lower threshold of intersection over union (IoU). We consider that RS MLMs still require further investigation in aerial detection to fully explore their potential.\nThere is often skepticism about whether MLMs can effectively learn to perform aerial detection. First, the detection outputs consist of numerical coordinates for bounding boxes and object categories, which significantly differ from the textual outputs produced by language models. Second, language generation models are typically autoregressive, generating causal sequences, whereas detection models usually output all results in parallel. Additionally, aerial detection presents considerable challenges due to the presence of many small and densely packed objects, which impose high demands on both the visual input resolution and the output sequence length of MLMs.\nIn this paper, we present a simple baseline for aerial detection, with a focus on enhancing the detection capabilities of MLMs in the RS domain for the first time, named LMMRotate. Specifically, we propose a straightforward method to supervised fine-tune MLMs, achieving detection performance comparable to that of conventional detectors. The MLLM detectors produce parsable text outputs, offering both flexibility and expandability as multi-dataset joint-trained detectors. We then observe that the advantage of conventional detectors in detection metrics largely comes from object confidence scores. To address this, we propose an appropriate evaluation scheme that enables fair comparisons between MLMs and conventional detectors. Additionally, we conduct exploratory experiments using open-source MLMs, demonstrating the potential of MLMs as aerial detectors. We sincerely hope that our work contributes to advancing more comprehensive abilities of MLMs in the RS field."}, {"title": "II. METHOD", "content": "As depicted in Figure 2, the proposed method adopts the most popular MLM paradigm for image understanding, which connects a vision foundation model and a language foundation model using a bimodal projection operation. We fine-tune off-the-shelf pre-trained MLMs to inherit the localization abilities learned from grounding task.\nFor a language model, the input sentence is pieced and tokenized into a unique sequence of vocabulary indices. Each indice $i$ corresponds to a discrete token $t_i \\in \\mathbb{R}^D$, which is a learnable vector of dimensionality $D$. The model's output is also a sequence of indices, which is subsequently de-tokenized to generate the final response.\nAn input RS image is firstly transformed into flattened features $F \\in \\mathbb{R}^{N_1 \\times D_1}$ by image preprocessing operations (resizing or dynamic resolution strategies) and a well-designed vision Transformer, where $N_1$ and $D_1$ represent the number and dimensionality of the features, respectively."}, {"title": "A. Preliminary of Multimodal Language Models", "content": "Meanwhile, the prompt text of detection instruction is tokenized into $N_t$ text tokens $T_t \\in \\mathbb{R}^{N_t \\times D}$. To align the visual features to input space of the language model, a bimodal projection operation maps the image features into $N_v$ visual tokens $T_v \\in \\mathbb{R}^{N_v \\times D}$, $N_v \\times N_1$. The input of language model is:\n$\\begin{equation}\nT = \\text{concat}(T_v, T_t) \\in \\mathbb{R}^{(N_v+N_t)\\times D},\n\\end{equation}$\nwhere the $\\text{concat}(\\cdot, \\cdot)$ operation concatenates two matrices along the token number dimension.\nIn the training period, we optimize the model parameters $\\Theta$ using the standard language modeling strategy, i.e., next token prediction with cross-entropy loss:\n$\\begin{equation}\n\\mathcal{L} = - \\sum_{j=1}^{|r|} P_{\\Theta}(r_j, T), \\quad P_j(r, T) = \\log P_{\\Theta}(r_j|r_{<j}, T),\n\\end{equation}$\nwhere $r = (r_1, r_2, ..., r_{|r|})$ denotes the indices sequence of model response and $P_j(r, T)$ is the conditional probability distribution of the j-th token. During the inference phase, the model generates outputs in an auto-regressive manner, predicting tokens iteratively,. The j-th token is obtained:\n$\\begin{equation}\nr_j = \\arg \\max P_j(r,T) \\quad \\text{or} \\quad r_j \\sim P_j(r, T),\n\\end{equation}$\nwhere the former corresponds to deterministic methods, such as greedy search or beam search, while the latter corresponds to stochastic sampling strategies. The process continues until a stopping criterion is met, such as generating the end-of-sequence token."}, {"title": "B. Normalization of Detection Outputs", "content": "We investigate multi-class orientated aerial detection in this paper. Each object is represented with the class name and a 8-parameters quadrilateral bounding box $o = (n_o, x_{1o}, y_{1o}, x_{2o}, y_{2o}, x_{3o}, y_{3o}, x_{4o},y_{4o})$, where the $x_{io}$ and $y_{io}$ are the coordinates of the polygon vertices in clockwise order. The vertex with the smallest vertical coordinate is considered the starting vertex. Besides, $n_o$ is one phrase in the c proposal category names $\\{C_1, C_2, ..., C_c\\}$.\nWe follow the approaches in Florence-2 and In-ternVL to qualitize vertex coordinates by normalizing each axis from 0 to 1000 and then round the normalized coordinate into integer. This designing prevents the coordinates from being excessively continuous and ensures consistent predictions. The precision loss introduced by the quantized integer coordinates relative to the floating-point coordinates has a negligible impact on the location accuracy.\nThe template used to standardize detection annotations should ensure uniqueness and orderliness. For a given image, the model should output the detected objects in a logical and sequential manner. Specifically, the overall response is composed of detection results for each category, which are sorted alphabetically by category names. Within each category, the boxes are further sorted based on the position of the starting vertexes.\nAs shown, we enable the MLM to recognize objects of multiple categories in an aerial image, including categories and bounding box in the response. During the inference stage, the detection results can be extracted through simple regex processing from the response. Moreover, compared to most conventional detectors that rely on post-processing techniques like non-maximum suppression (NMS), MLM does not need to handle issues related to overlapping redundant objects."}, {"title": "C. Evaluation of MLM detectors", "content": "The common practices for aerial detection use mean average precision (mAP) as evaluation metric, which requires orientated bounding boxes, categories, and confidence scores of the detected objects. As mentioned, the designed response only includes categories and location. We found that the quality of the confidence scores has a significant impact on the mAP score, which is a disadvantage for MLM.\nRather than introducing a complex mechanism to obtain object confidence for MLM, we claim that the consideration of confidence is not necessary when comparing MLM and conventional detectors. Since the annotations and results for detection tasks inherently compose of object categories and bounding boxes, but not contains confidence. The confidence is merely an additional byproduct of the detector's inference process. It can assist in processing detection results but is not essential for evaluating model quality. We advocate for the use of metrics that do not rely on confidence, such as mF\u2081 and mAPnc.\nWe also evaluate the robustness of mAPnc as a metric. We calculate each mAPnc eleven times by replacing the confidence with ten random values and a consistent value. As shown in Figure 4, the standard deviations are generally lower than 0.5%, especially when the threshold is between 0.2 and 0.4.\nEspecially for benchmarks that lack publicly available test sets and require online server evaluation based on mAP, such as DOTA and FAIR1M , hence, we recommend adopting mAPnc as the evaluation metric in these benchmarks."}, {"title": "III. EXPERIMENT", "content": "We conduct experiments on four benchmarks for multi-class orientated aerial detection, comprising three optical RS image datasets DOTA, DIOR-R , FAIR1M-v1.0 , and two synthetic aperture radar (SAR) image datasets SRSDD and RSAR ."}, {"title": "A. Benchmark Datasets", "content": "As claimed in Section II-C, we use mAPnc and mF\u2081 as evaluation metrics.\nThe mAPnc and mF\u2081 of each conventional detector is calculated by the process exhibited in Figure 4. We calculate the two types of scores under a range of confidence thresholds and then select the best scores. We obtain the results with the default inference settings of each model and do not post-process any model with an additional NMS operation.\nFor the MLM detectors, we first extract the detection outputs with regular expressions. For cases in which the predicted category text does not exactly match the meta category names of the dataset (e.g., \"pool\" and \"swimming pool\"), we employ fuzzy matching with the Levenshtein distance. We assign a 100% confidence score to all the predictions. Then, we directly send the predictions to subsequent evaluation.\nWe emphasize a critical distinction between mAPnc and mF\u2081 is that the mF\u2081 cannot be utilized for online evaluation. Consequently, for DOTA-v1.0 and FAIR1M-v1.0, mF\u2081 can not be calculate in the commonly adopted benchmark settings. Therefore, we adjust the benchmark settings for mF1. For DOTA-v1.0, we re-train the model on the training set and compute mF\u2081 on the validation set. For FAIR1M-v1.0, we directly evaluate the model on the validation set of FAIR1M-v2.0, because the newer version adds a validation set and expands the test set without modifying the training set."}, {"title": "B. Evaluation Settings", "content": "We fine-tune Florence-2 , an advanced MLM for general vision tasks. We conduct experiment on the two available pre-trained model: Florence-2-base with 271 million parameters and Florence-2-large with 829 million parameters. The input size is 1024 \u00d7 1024 for all models.\nSince our data annotations are stored in pure text format, we can flexibly perform joint training across multiple datasets. We explore two methods for merging multiple dataset: The \"concat\" method simply merges the four datasets. The \u201cbalanced\" method oversamples the smaller datasets, thereby achieving a more balanced distribution among the four datasets.\nThe models are trained for 100 epochs, with the learning rate of $2 \\times 10^{-5}$ and cosine schedule. We open-source the code based on MMRotate and Huggingface Transformers, which employs resource-friendly MLM training techniques, such as mixed precision computing, gradient checkpointing, flash attention, and deepspeed."}, {"title": "C. Implementation Details", "content": "Overall, the fine-tuned MLM detectors achieve detection performance on par with conventional detectors. As shown in Figure 1, even in complex scenarios with numerous densely packed small objects, the MLM detector can still perform well. In the quantitative results presented in Table I, the MLM detectors even surpasses conventional detectors in terms of mF\u2081, as well as mAPnc on the DIOR-R and FAIR1M-v1.0 datasets.\nIt can also be observed that the joint training have great positive effect on MLM detectors. The overall performance is increased because of the larger amount of data, especally for the small dataset SRSDD, which seems to gain more benifits from the other datasets."}, {"title": "D. Comparison Results", "content": "In this paper, we demonstrate that a multimodal language model can also handle aerial detection tasks, even achieving performance comparable to conventional detectors. Our approach is straightforward: by normalizing detection outputs into text form, we can fine-tune general-purpose MLMs to meet our goal. We also present an appropriate evaluation scheme for MLM detectors. We hope our work will inspire future research toward enhancing aerial detection capabilities in next-generation RS MLMs, ultimately contributing to broader AGI developments."}, {"title": "IV. CONCLUSION", "content": ""}]}