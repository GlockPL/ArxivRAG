{"title": "CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR", "authors": ["Nian Shao", "Rui Zhou", "Pengyu Wang", "Xian Li", "Ying Fang", "Yujie Yang", "Xiaofei Li"], "abstract": "In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance. The proposed network takes as input the noisy and reverberant microphone recording and predicts the corresponding clean Mel-spectrogram. The enhanced Mel-spectrogram can be either transformed to speech waveform with a neural vocoder or directly used for ASR. The proposed network is composed of interleaved cross-band and narrow-band processing in the Mel-frequency domain, for learning the full-band spectral pattern and the narrow-band properties of signals, respectively. Compared to linear-frequency domain or time-domain speech enhancement, the key advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR. Experimental results on four English and one Chinese datasets demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model. Code and audio examples of our model are available online", "sections": [{"title": "I. INTRODUCTION", "content": "This work studies single-channel speech enhancement using deep neural networks (DNNs), to improve both speech quality and automatic speech recognition (ASR) performance. A large class of speech enhancement methods employ DNNs to map from noisy and reverberant speech to corresponding clean speech, conducted either in time domain [1], [2] or in time-frequency domain [3]\u2013[5]. These methods can efficiently suppress noise, but not necessarily improve ASR performance due to the speech artifacts/distortions caused by speech enhancement networks [6]. In [7], it was found that time-domain enhancement is more ASR-friendly than frequency-domain enhancement. A time domain progressive learning method is proposed in [8], which also shows the superiority of time domain speech enhancement, and the progressive learning mechanism is very effective for robust ASR by mitigating the over suppression of speech. In [9], [10], ASR performance is largely improved by decoupling frontend enhancement and backend recognition. In [10], it is shown that an advanced time-frequency domain network, i.e. CrossNet, can even outperform the time domain ARN network.\nSpeech enhancement in Mel-frequency domain, or similarly in rectangular bandwidth (ERB) domain, has been developed under various contexts in the literature. Mel-frequency and ERB bands model human speech perception of spectral envelope and signal periodicity, within which speech enhancement is more perceptually and computationally efficient than within linear-frequency domain or time domain. In [11]\u2013[13], spectral envelope enhancement is performed within the ERB bands, and then applying pitch filtering [11], [12] or deep filtering [13] to recover the enhanced speech. Sub-band networks [3], [14] and full-band/sub-band fusion networks, i.e. FullSubNet [15], [16], have been recently proposed and achieved outstanding performance. However, separately processing sub-bands in the linear-frequency domain leads to a large computational complexity. To reduce the number of sub-bands and thus the computational complexity, Fast FullSubNet [17] and the work of [18] proposed to perform sub-band processing in the Mel-frequency domain, and then transform back to linear frequency with a joint post-processing network.\nIn [19], [20], speech enhancement is directly conducted in the Mel-frequency domain and then a separate neural vocoder is used to recover speech waveform. These methods improve speech enhancement capability by alleviating the burden of enhancing full-band speech details and also by leveraging the powerful full-band speech recovery capacity of advanced neural vocoder, as a result, achieve higher speech quality compared to their linear-frequency counterparts.\nIn this work, we propose CleanMel, a single-channel Mel-spectrogram enhancement network for improving both speech quality and ASR performance. Different from the previous works [11]\u2013[13], [17] that performing speech enhancement in the ERB or Mel domain, and then applying a joint pitch filtering or deep filtering to obtain the enhanced speech, this work decouples the Mel-spectrogram enhancement and post-processing parts by targeting the enhancement network with clean Mel-spectrogram. The enhanced Mel-spectrogram can be directly used for ASR, or transformed back to waveform with a separate neural vocoder as is done in [19]. Compared to linear-frequency spectrogram or time-domain waveform, Mel-frequency presents speech in a more compact and less-detailed way (but still perceptually efficient) and has a lower feature dimension (number of frequencies) from the perspective of machine learning, which would result in lower the prediction error. This is beneficial for both speech quality improvement and ASR: (i) Neural vocoders have been extensively studied in the field of Text-to-Speech, and are capable of efficiently transforming Mel-spectrogram back to time-domain waveform. Therefore, the low-error property of Mel-spectrogram enhancement can be hopefully maintained by the neural vocoders and higher speech quality can be achieved. (ii) From the perspective of ASR, there seems no need to first recover the less-accurate full-band speech details and then compress to Mel-frequency. A direct and more-accurate Mel-spectrogram estimation would be preferred.\nWe adapt the network architecture of our previous proposed (online) SpatialNet [21], [22] with some modifications to better accommodate Mel-spectrogram enhancement. SpatialNet is composed of interleaved cross-band and narrow-band blocks originally proposed for processing multichannel STFT frequencies. The narrow-band block processes STFT frequencies independently to learn the spatial information presented in narrow-band (one frequency), such as the convolutive signal propagation and the spatial correlation of noise. And the cross-band block was designed for learning the across-frequency dependencies of narrow-band information. As for single-channel Mel-spectrogram enhancement in this work, the narrow-band block processes Mel frequencies independently to also learn the (single-channel) convolutive signal propagation of target speech, which is crucial not only for conducting dereverberation of target speech but also for discriminating between target speech and interfering signals. The cross-band block is now reinforced and utilized to learn the full-band spectral pattern in the Mel-frequency domain.\nIn addition, we have studied several critical issues when decoupling the Mel-spectrogram enhancement front-end and ASR/Vocoder back-ends. i) We have systematically studied and compared different learning targets that can be used for Mel-spectrogram enhancement, including logMel mapping, Mel ratio masking and the clipping issue of logMel; ii) We have developed a data normalization scheme to align the signal levels of cascaded front-end and back-end models; iii) We have developed an online neural vocoder to enable online speech enhancement.\nExperiments are conducted on five public datasets (four English and one Chinese) for speech denoising and dereverberation individually or jointly. Importantly, we adopt a more realistic evaluation setup: from multiple data sources of clean speech, real-measured room impulse responses (RIRs) and noise signals, we collected and organized a relatively large-scale training set, based on which we train the network for once and directly test it on all the five test sets. Experiments show that the proposed model achieves the state-of-the-art (SOTA) speech enhancement performance in term of speech perceptual quality. Moreover, on top of various pre-trained and advanced ASR models, the proposed model prominently improves the ASR performance on all datasets. These results demonstrate that our trained models have the potential to be directly employed to real applications."}, {"title": "II. PROBLEM FORMULATION", "content": "The noisy and reverberant single-channel speech signals can be represented in the time domain as\n$y(n) = s(n) * a(n) + e(n)$ (1)\nwhere n stands for the discrete time index. $s(n)$ and $e(n)$ represents the clean source speech and ambient noise, respectively. $a(n)$ denotes RIR and * the convolution operation. In this work, only static speaker is considered, thence the RIR is time-invariant. RIR is composed of the direct-path propagation, early reflections and late reverberation.\nWe conduct joint speech denoising and dereverberation in this work, which amounts to estimate the (Mel-spectrogram of) desired direct-path speech $x(n) = s(n) * a_{dp}(n)$ from microphone recording $y(n)$, where $a_{dp}(n)$ denotes the direct-path part in RIR. The training target of the proposed network and the training signals of neural vocoders will all be derived with $x(n)$.\nThe proposed method is performed in the time-frequency domain. By applying STFT to Eq. (1), based on the convolutive transfer function approximation [23], we can obtain:\n$Y(f,t) \\approx S(f,t) * A(f,t) + E(f,t)$ (2)\nwhere $f \\in \\{0,..., F - 1\\}$ and $t \\in \\{1,...,T\\}$ denote the indices of frequency and time frame, respectively. $Y(f, t)$, $S(f,t)$ and $E(f, t)$ are the STFT of respective signals, and $X (f,t)$ is the STFT of direct-path speech. $A(f,t)$ is the convolutive transfer function associated to $a(n)$. Convolution * is conducted along time. In the STFT domain, the time domain convolution $s(n) * a(n)$ is decomposed as (frequency-independently) narrow-band convolutions $S(f,t) * A(f,t)$. Speech dereverberation in this work highly relies on learning this narrow-band convolution. For nosie reduction, one important way for discriminating between speech and stationary noise is to test the signal stationarity, which can be modeled in narrow-band as well."}, {"title": "III. MEL-SPECTROGRAM ENHANCEMENT", "content": "In this work, we propose to enhance the Mel-spectrograms, which then can be directly fed into an ASR model, or transformed to waveforms with a neural vocoder.\n\n*A. Learning Target: Clean Mel-spectrogram*\nThe power-based or magnitude-based Mel-spectrogram of the target speech $X (f, t)$, denoted as $X_{mel} (f_{mel}, t)$, can be obtained by weighted summing the squared magnitude $|X(f,t)|^2$ or magnitude $|X(f,t)|$ over frequencies with the triangle weight functions of Mel filterbanks, where $f_{mel} \\in \\{1, ..., F_{mel}\\}$ denotes the index of Mel-frequency. Our preliminary experiments showed that using power-based or magnitude-based Mel-spectrograms achieve similar enhancement performance, thence we can choose either of them according to the Mel setup of ASR or neural vocoder backend models. In this work, we use the power-based Mel-spectrogram according to the setup of commonly-used ASR models. Then, the logMel-spectrogram, namely the logarithm of $X_{mel}(f_{mel}, t)$:\n$X_{logmel} (f_{mel}, t) = log(max\\{X_{mel}(f_{mel}, t), \\epsilon\\})$ (3)\ncan be taken as the input feature of ASR or neural Vocoder backends. The base of logarithm (e or 10) should be consistent to the one of back-ends as well, while e is used in this work. The Mel-spectrogram is clipped with a small value of $\\epsilon$ to avoid applying logarithm to close-zero values.\nNormally, $\\epsilon$ is set to be very small, e.g. le-10, to maintain complete speech information. However, in our preliminary experiments, we found that very small speech values are not very informative for both ASR and neural vocoder, and thus can be clipped without harming performance. Moreover, since those small values are highly contaminated by noise or reverberation, the prediction error of them could be very large. For these reasons, we set $\\epsilon$ to a relatively large value, e.g. le-5 (when the maximum value of time domain signal is normalized. The signal normalization methods will be presented in Section III-D). Fig. 1 gives an example of our target logMel-spectrogram, in which about 40% TF bins are clipped.\nIn this work, we evaluate two different learning targets as for Mel-spectrogram enhancement.\nLogMel mapping: The clean logMel-spectrogram can be directly predicted with the network. The training loss is set to the mean absolute error (MAE) loss between the predicted and the clean logMel-spectrogram, namely\n$\\mathcal{L}_{MAE} = \\frac{1}{F_{mel}T} \\sum_{f_{mel}=1}^{F_{mel}} \\sum_{t=1}^{T} |X_{logmel}(f_{mel}, t) - \\widehat{X}_{logmel}(f_{mel}, t)|,$ (4)\nwhere $\\widehat{X}_{logmel} (f_{mel}, t)$ is the network output.\nMel ratio mask: Ratio mask is one type of popular learning targets for speech magnitude enhancement [24]. For each time-mel-frequency bin, the Mel ratio mask is defined as\n$M(f_{mel}, t) = min\\{(\\frac{\\sqrt{X_{mel} (f_{mel}, t)}}{\\sqrt{Y_{mel} (f_{mel}, t)}}, 1\\}.$ (5)\nwhere $Y_{mel} (f_{mel}, t)$ denotes the power level of noisy Mel-spectrogram. The square root function transforms the power domain to the magnitude domain. The $min(\\cdot)$ function rectifies the mask into the range of [0,1]. The mean squared error (MSE) of the ratio mask is taken as the training loss, namely\n$\\mathcal{L}_{MRM} = \\frac{1}{F_{mel} T} \\sum_{f_{mel}=1}^{F_{mel}} \\sum_{t=1}^{T}(\\widehat{M}(f_{mel}, t) - M (f_{mel}, t))^2,$ (6)\nwhere $\\widehat{M}(f_{mel}, t)$ denotes the model prediction of $M(f_{mel}, t)$. Then, the enhanced logMel-spectrogram can be obtained as\n$\\widehat{X}_{logmel}(f_{mel}, t) = log(max\\{\\widehat{M}(f_{mel}, t)^2Y_{mel}(f_{mel}, t), \\epsilon\\}).$ (7)\n\n*B. The CleanMel Network*\nFig. 2 shows the network architecture. The proposed network takes as input (the real ($R(\\cdot)$) and imaginary ($I(\\cdot)$) parts of) the STFT of microphone recording, i.e. $Y(f,t)$, denoted as y:\n$y[f,t,:] = [R(Y(f,t)), I(Y(f,t))] \\in \\mathbb{R}^2,$ (8)\nwhere $[:]$ represents to take values of one dimension of a tensor. The network is composed of an input layer, interleaved cross-band and narrow-band blocks first in the linear-frequency domain and then in the Mel-frequency domain, a Mel-filterbank, and finally a Linear output layer. The input layer conducts temporal convolution on y with a kernel size of 5, obtaining the hidden representation with the dimensions of $F \\times T \\times H$. Then one cross-band block and one narrow-band block process the hidden tensor in linear-frequency. Mel-filterbank (with triangle weight functions) transforms the frequency dimension from F linear frequencies to $F_{mel}$ Mel frequencies, which is realized with a non-trainable matrix multiplication. Then L interleaved cross-band and narrow-band blocks process the tensor in Mel-frequency. After the final narrow-band block, the output Linear layer transforms H-dim to 1-dim as either the enhanced logMel-spectrogram or the Mel ratio mask. Note that Sigmoid activation is applied for predicting Mel ratio mask.\n*1) Narrow-band block:*\nAs shown in Eq. (2), the time domain convolution can be decomposed as frequency-independently narrow-band convolutions, while the latter has much smaller complexity compared to the former in terms of the order of room filters. Therefore, modeling the narrow-band convolution would be much more efficient than modeling the time domain convolution. The convolution model of target speech provides not only necessary information for dereverberation of the target speech but also discriminative information between the target speech and other interfering sources. In addition, in narrow-band, non-stationary speech and stationary noise can be well discriminated by testing the signal stationarity. For these reasons, we propose the narrow-band network, which processes frequencies independently along the time dimension, and all frequencies share the same network.\nThe narrow-band convolution in Eq. (2) is defined between the complex-valued STFT coefficients of source speech and room filter. Thence we process the complex-valued STFT coefficients of noisy signal (in a hidden space), instead of other features such as magnitude, to retain the convolution correlation. We first process in (the finer) linear-frequency with one narrow-band block to fully exploit the convolution information, then after Mel-filterbanks, we process in (the coarser) Mel-frequency with more narrow-band blocks.\nThe narrow-band network is composed of Mamba blocks [25]. Mamba is a recently proposed network architecture based on structured state space sequence models, which was shown very efficient for learning both short-term and long-term dependencies of sequential data. Besides short-term correlations of signals, there exist some long-term dependencies should be exploited for narrow-band speech enhancement. For example, the convolution model is time invariant in a very long period of time for static speakers. Moreover, Mamba has a linear computational complexity w.r.t time, which is suitable for streaming processing of long audio signals. Specifically, one narrow-band block consists of a forward Mamba for online processing and an optional backward Mamba (averaging with forward output) for offline processing.\n*2) Cross-band block:*\nThe cross-band block is used to learn full-band/cross-band dependencies of signals. In the original SpatialNet [21], the cross-band block is designed for learning the linear relationship of inter-channel features (e.g. inter-channel phase different) across frequencies. In this work, single channel does not have such inter-channel information. Instead, the cross-band block can learn the full-band spectral pattern of signals in the linear/Mel-frequency domain, which is also critical information for (especially single-channel) speech enhancement. The cross-band block processes frames independently along the frequency dimension, and all frames share the same network.\nSpecifically, we adopt the original cross-band block as in SpatialNet, which is composed of cascaded frequency convolutional layer (F-GConv1d), across-frequency linear layers (F-Linear) and a second frequency convolution layer. The frequency convolutional layers perform 1-D convolution along frequency to learn correlations between adjacent frequencies. The across-frequency linear layer processes all frequencies together to learn full-band dependencies. One cross-band block is first applied in the linear-frequency domain to learn detailed full-band correlations. To reduce the model complexity, the hidden dimension H is compressed to a much smaller dimension, such as H/12, and then each hidden dimension is independently processed by the across-frequency linear layer with a complexity of $F^2$. After Mel-filterbank, the cross-band blocks learn full-band correlations across Mel frequencies, where the model complexity is largely reduced from $F^2$ to $F^2_{mel}$. Correspondingly, the hidden dimension is remained as H (no longer compressed) to reinforce the capability of full-band learning. We set all the Mel-frequency cross-band blocks to share the same across-frequency linear layers.\n\n*C. Back-ends*\nAt the inference stage, CleanMel is followed by either an ASR model or a neural vocoder. The ASR model and neural vocoder are separately trained with the CleanMel network.\n*1) ASR:*\nAt inference, the enhanced logMel-spectrogram is directly fed to an already-trained ASR system, without performing any fine-tuning or joint-training. Different ASR systems may have different configurations in STFT settings, number of mel frequencies and base of logarithm. To seamlessly integrate the enhanced logMel-spectrogram into one ASR model, our CleanMel would adopt the same configurations as the ASR model. The training cost of CleanMel is not very high, so it can be easily re-trained for a new ASR system, especially for those large-scale ASR systems and already-deployed ASR systems. In this work, we only conduct offline ASR combined with offline CleanMel.\n*2) Neural vocoder:*\nThe vocoder we adopt in this work is Vocos [26], a recently proposed Generative Adversarial Network (GAN)-based neural vocoder. The generator of Vocos predicts the STFT coefficients of speech at frame level and then generates waveform through inverse STFT. Vocos uses the multiple-discriminators and multiple-losses proposed in HiFi-GAN [27], but it significantly improves the computational efficiency compared to HiFi-GAN that directly generates waveform at sample level. In this work, to unify the front-end and back-end processing, we have made several necessary modifications to Vocos as follows:\n\nThe magnitude-based Mel-spectrogram of original Vocos is modified as power-based to be consistent with the front-end and ASR models, where the two cases were shown to achieve similar performance in our preliminary experiments.\nThe sampling rate of signals, the STFT configurations and the number of Mel-frequencies of the Vocos are adjusted according to the setup of front-end and ASR models.\nThe original Vocos is designed for offline processing, as it employs non-causal convolution layers. To enable online processing, we modified Vocos to be causal by substituting each non-causal convolution layer with its causal version. The online vocoder still performs quite well. In addition, to reduce the computational complexity of online processing, the 75% STFT overlap of original Vocos is reduced to be 50% overlap, which still achieves comparable performance.\nThe offline and online Vocos are used to work with the offline and online CleanMel, respectively. Vocos models are trained with our direct-path target speech x(n).\n\n*D. Signal Normalization*\nWhen separate front-end and back-end models are cascaded, signal normalization should be performed not only to facilitate the training of respective models but also to align the signal level of cascaded models. For offline processing, the normalization method used in Vocos is also applied for CleanMel. Specifically, a random gain is applied to time domain signal to ensure that the maximum level of the resulting signal lies within the range of -1 to -6 dBFS. This normalization manner ensures the maximum level of sample values is close to and smaller than 1, thence the generated waveform can be directly played with full volume and without clipping effect. For CleanMel, we apply this normalization to noisy signal and utilize the same gain of noisy signal to the corresponding clean target signal. In this way, the enhanced Mel-spectrogram can be directly fed to Vocos. When applying this time-domain normalization, the clip value $\\epsilon$ for computing logMel-spectrogram is set to 1e-5 for both CleanMel and Vocos. As for ASR, ASR models normally have a separate Mel-spectrogram normalization operation, which will be applied to re-normalize the enhanced Mel-spectrogram.\nFor online processing, the time domain normalization method is no longer applicable. Instead, an online STFT-domain normalization is used for both CleanMel and Vocos. Specifically, for CleanMel, the noisy and target speech are normalized in the STFT domain as $\\widehat{Y}(f,t) = Y(f,t)/\\mu(t)$ and $\\widehat{X} (f,t) = X(f,t)/\\mu(t)$, where $\\mu(t)$ is a recursively calculated meanvalue of STFT magnitude of $Y(f,t)$, i.e. $\\mu(t) = \\alpha\\mu(t \u2212 1) + (1-\\alpha) \\sum_{f=0}^{F-1} |Y(f, t)|$. The smoothing weight is set to $\\alpha = \\frac{K-1}{K+1}$, by which the recursive smoothing is equivalent to using a K-long rectangle smoothing window. When training Vocos using target speech signal x(n), we still first apply the time domain normalization mentioned above, then apply an extra online normalization. Specifically, $\\mu(t)$ is computed with and also applied to X(f,t), and then the corresponding logMel-spectrogram is computed as the input of Vocos generator. Accordingly, the output of Vocos generator (before applying inverse STFT) would be an estimation of normalized X(f,t). To go back to the signal level of time domain normalization, the recursive normalization factor $\\mu(t)$ is multiplied back to the estimation of normalized X(f,t) and then applying inverse STFT, after which Vocos losses (including Mel loss and discriminator losses) are computed. At inference, online normalization is applied to the noisy input, and the enhanced logMel-spectrogram is directly fed into Vocos. The recursive normalization factor computed with the noisy input is multiplied to the estimated STFT coefficients by Vocos and then applying inverse STFT to obtain the final waveform which is time-domain normalized and can be directly played. When applying this online normalization, the clip value $\\epsilon$ for computing logMel-spectrogram is set to le-4 for CleanMel and the input of Vocos generator."}, {"title": "IV. EXPERIMENTAL SETUPS", "content": "In the section, we present the experiment datasets, experimental configurations, evalution metrics and comparison methods.\n\n*A. Dataset*\n*1) Speech enhancement training dataset:*\nThe proposed model is trained with synthetic noisy/clean speech pairs. Reverberant speech signals are generated by convolving source speech signals with RIRs, then added with noise signals. Clean speech signals are generated by convolving source speech signals with the direct-path part of RIRs.\nIn this work we conduct speech enhancement for both Mandarin Chinese and English, and we will evaluate our model on five different datasets (as will be shown later). We attempt to train the model once and test it on all the datasets, which will reflect the general capability of the model under various situations. To do this, we collect data (in terms of source speech, RIRs and noise) from multiple public datasets, and form a training set with sufficient speech quality and environment/device diversity.\nSource speech: Source speech signals are collected from 6 datasets, including AISHELL I [28], AISHELL II [29] and THCHS30 [30] for Chinese, and EARS [31], VCTK [32] and DNS I challenge [33] for English. For each language, about 200 hours of high-quality speech data are selected from the original datasets, based on the raw score of DNSMOS P.835 [34]. The selection thresholds are set to 3.6 and 3.5 for Chinese and English data, respectively. Except that the entire EARS training set is included, since EARS involves various emotional speech that cannot be well evaluated by the DNSMOS. The amount of data selected from each dataset is summarized in Table I.\nRIR: We use real-measured RIRs from multiple public datasets [35]\u2013[42]. Table II shows the statistics of these RIR datasets. For all (even multi-channel) datasets, all RIRs are used, except that we uniformly sampled 1,000 RIRs from the very large original ARNI dataset.\nThe reverberation time, i.e. $T \u2013 60$, of RIRs mostly lie in the range of (0, 1.5) seconds, except for a few rooms in AIR [36] and NaturalReverb [40]. Besides the wide distribution range of $T_{60}$s, these RIRs also have large diversity in terms of environments and measuring devices. For example, NaturalReverb [40] is recorded in 271 spaces encountered by humans during daily life, AIR [36] is measured with a dummy head for binaural applications, RWCP [42] use a Head-Torso as source speaker, etc. When synthesizing reverberant speech, 80% source speech samples are convolved with a randomly selected RIR, while there is no RIR convolution for the rest 20% samples to account for the near-field applications where reverberation is negligible.\nNoise: Speech and noise are mixed with a random signal-to-noise ratio (SNR) between -5 dB and 20 dB. We use the noise signals from the DNS challenge [43] and the RealMAN dataset [44]. The DNS challenge dataset has about 181 hours of noise sampled from AudioSet [45] and Freesound. The RealMAN dataset has 106 hours of ambient noise recorded in 31 daily life scenes, including various indoor, semi-outdoor, outdoor and transportation scenes.\n\n*B. Configurations*\n*1) Data configurations:*\nThe sampling rate of all data is set to 16 kHz. STFT is applied using Hanning window with a length of 512 samples (32 ms) and a hop size of 128 and 256 samples (8 and 16 ms) for the offline and online models, respectively. The offline model has a finer temporal resolution than the online model since it is used for ASR in this work and its temporal resolution is aligned with the ASR models. However, we empirically found that, compared to the 16-ms hope size, the 8-ms hope size does not benefit much for the speech enhancement performance. The number of Mel frequencies is set to $F_{Mel} = 80$ for the frequency range of 0-8 kHz. The same STFT implementation (ESPNet implementation) is used for the CleanMel networks, neural vocoders and ASR models, to avoid configuration mismatch. The natural logarithm (base of e) is used.\n*2) Network configurations:*\nFollow [21], the kernel size of the T-Conv1d in the input module and the F-GConv1d layers in the cross-band block are both set to 5. As shown in Fig. 2, in narrow-band block, forward-only and forward/backward Mamba layers are set for online and offline processing, respectively. We set up two model scales for the offline models, referred to as CleanMel-S and CleanMel-L. And the online model scale is set approximately to CleanMel-S. The configurations are shown in Table IV-B. The depth L of online models are set to twice the one of corresponding offline models to have the similar model size. Due to the different setups of STFT hop size, the computational complexity, i.e. FLOPs, of offline models are roughly twice the one of corresponding online models.\n*3) Training and inference setups:*\nFor CleanMel, AdamW optimizer [47] with an initial learning rate of $10^{-3}$ is used for training. The learning rate exponentially decays with $lr \\leftarrow 0.001 \\times 0.99^{epoch}$. Gradient clipping is applied with a gradient norm threshold 10. The batch size are set to 32. Training samples are synthesized in an on-the-fly manner, and 100,000 samples are considered as one training epoch. The CleanMel-S and CleanMel-L models are trained by 100 and 150 epochs, respectively. Afterward, we average the model weights of the last 10 epochs as the final model for inference. For training the Vocos neural vocoder [26], we synthesized 400,000 (direct-path) clean speech samples with both English and Chinese data used for CleanMel training. The training configurations keep unchanged as in its original work. For ASR evaluation, as already mentioned, the pre-trained ASR models obtained using in ESPNet are used.\n\n*C. Evaluation metrics*\nSpeech enhancement performance is evaluated with Perceptual Evaluation of Speech Quality (PESQ) [48], DNSMOS P.808 [49] and P.835 [34], where the background, signal and overall scores for P.835 are all reported. Word Error Rate (WER) and Character Error Rate (CER) are used to evaluate English and Chinese ASR performances, respectively.\n\n*D. Comparison models*\nWe compare with five advanced speech enhancement models, which were all claimed in their original papers to be able to conduct joint speech denoising and dereverberation, including (i) FullSubNet [15] is a LSTM-based full-band and sub-band fusion network originally proposed for online speech denoising, and extended to speech dereverberation in [16]. For offline processing, we change the uni-directional LSTMs to be bi-directional. (ii) Demucs [50] is an online speech enhancement model that operates directly on waveforms with a U-net. (iii) VoiceFixer [19] also performs Mel-spectrogram enhancement (using a ResUNet) and generates the waveform using a neural vocoder, but is developed only for offline speech enhancement. (iv) StoRM [51] is a powerful diffusion-based offline speech enhancement system. (v) SpatialNet [21] and oSpatialNet [22] perform speech enhancement in the STFT linear-frequency domain, and offer the backbone network for the proposed CleanMel model. In SpatialNet and oSpatialNet, self-attention (and temporal convolution) and Mamba are adopted for learning the narrow-band spatial information in an offline and an online manner, respectively. For offline processing, besides comparing with the original SpatialNet, we also implemented a variant of it with bi-directional Mamba for narrow-band blocks (same architecture with the proposed method), which is referred to as SpatialNet-Mamba, and serves as the linear-frequency baseline for the proposed Mel-frequency model. Note that, SpatialNet and oSpatialNet were originally proposed for multi-channel speech enhancement, and this work is the first one to fully evaluate and analyze the capability of them for single-channel speech enhancement.\nTo conduct fair comparisons, we re-train the FullSubNet, StoRM and oSpatialNet/SpatialNet models using the same training data utilized for the proposed model. However, we found that it is not easy to re-train the Demucs and Voicefixer models. In [50], different data augmentation strategies are applied when training Demucs on different datasets. Voicefixer [19] was designed for 44.1 kHz signals. Re-training Demucs and VoiceFixer request careful data engineering or hyperparameter search. Therefore, we use the pre-trained checkpoints of Demucs and VoiceFixer to perform speech enhancement on only English data. For Demucs, we use the \u2018dns64' model provided by the authors. For VoiceFixer, we use the default pre-trained model provided in the open-source package. Following the VoiceFixer default inference setup, test waveforms are first up-sampled to 44.1 kHz to perform speech enhancement and then down-sampled back to 16 kHz for evaluation."}, {"title": "V. RESULTS AND ANALYSES", "content": "In this section", "aspects": "n*1) Comparing the training targets of logMel mapping and Mel ratio mask:*\nFor online processing, mapping consistently outperforms mask in DNSMOS, mainly due to the higher residual noise of mask, which can be reflected by the much lower BAK scores of mask on the very noisy CHiME4 and RealMAN test sets. When applying the predicted ratio mask on the noisy spectrogram, if speech is highly contaminated by noise, there will exist certain residual noise even if the predicted error of mask is small. By contrast, mapping directly predicts the logMel-spectrogram of speech, which can avoid such residual noise. However, as shown in Table V, mask achieves higher PESQ scores than mapping on the highly noisy CHiME4 and DNS (w.o. reverb) sets. PESQ measures the perceptual similarity of enhanced speech and reference clean speech. The higher PESQ scores indicates that mask performs better on retrieving the target speech through extracting the target speech from the noisy speech with a mask. Directly mapping may erroneously remove or boost those speech components highly contaminated by noise. The enhanced logMel-spectrograms are transformed to waveforms with a neural vocoder, and the speech quality measured with DNSMOS is"}]}