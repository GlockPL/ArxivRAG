{"title": "Classifier-free guidance in LLMs Safety", "authors": ["Roman Smirnov"], "abstract": "This article is an extended version of the NeurIPS 2024 LLM-PC submission that was awarded the second prize. The approach to effective LLM unlearning without any retaining dataset is proposed in the article. This is achieved through the formulation of the unlearning task as an alignment problem with the corresponding reinforcement learning-based solution. Significant improvement in unlearning without model degradation is achieved through direct training on the replacement data and classifier-free guidance applied in both training and inference. Sections 4 and 5 of the article were added after the NeurIPS 2024 LLM-PC competition and are focused on data ablation study and enhancements to classifier-free guidance implementation for large language models.", "sections": [{"title": "Introduction", "content": "LLM unlearning is an increasingly important research area, gaining attention as large language models are applied across various industries and social contexts. Unlearning can be applied to unlearn specific behaviors, e.g. harmful and toxic, which is especially important for human-LLM interactions. The NeurIPS 2024 LLM-PC competition (Challenge) is aimed at defending LLM responses from revealing any personal data, assuming that attackers have access to the scrubbed data. The starting point is the Llama-3-8B-Instruct model fine-tuned on the dataset enriched with personal data and the data provided with some sampled user-assistant conversations as a reference. The competition page and initial data: LLM-PC github. The code for the approach described in this article is in the project github repository.\nThe challenging part of unlearning is to maintain the model performance outside the unlearning dataset. Recent approaches like [1] and [2] use a retaining dataset for that. Some methods, e.g. [2] and [3], use the loss on the retaining dataset as a part of the training loss that may introduce additional bias towards it.\nThe approach introduced in this article is inspired by the idea that unlearning of such concepts like harmful behavior or personal data usage in the answers lies in"}, {"title": "", "content": "a field of LLM alignment where maintaining base model performance is achieved through KL-divergence or its approximations without any retaining dataset through reinforcement learning. To implement reinforcement learning approach, the reward model is required (in the context when we have the samples to forget, the reward model is still required to classify chosen and rejected samples for DPO-style tuning). A simple named entity recognition model can be used in the case of personal data unlearning as the reward model. However, \"good\" answers are still required to do DPO-style tuning. Following the [2] these \"good\" answers can be generated using external API-based LLMs (with some modifications of the generation task that are covered in Section 2.1). Using such answers in training makes the training objective more direct and reliable compared to the scenario when the gradient ascent is applied with negative examples only.\nThe article presents a method for LLM unlearning that does not require a retraining dataset."}, {"title": "Methodology", "content": "The Challenge is about making the model robust to the personal data leakage, when the attackers have access to the scrubbed data. This means that the model should not generate responses containing personal data (PII), even when personal data or specific patterns are present in the dialogue history or the prompt.\nThe proposed approach consists of four components:\n\u2022 Models subtraction with the task vectors;\n\u2022 Extensive data generation and filtration;\n\u2022 Supervised fine-tuning and DPO-style tuning;\n\u2022 Inference modification to improve model's robustness."}, {"title": "Model preparation", "content": "Following the forgetting via negation method proposed in [4] and [5] we can build the following pipeline: we have the base model and we generate data with extensive PII usage in responses, after that we do supervised fine-tuning of the base model with this data, so we get the modified (reinforced) model. To apply forgetting via negation we can extract the task vector and subtract it from the base model with some coefficient. In the Challenge we have the model provided that is derived from the Llama-3-8B-Instruct through tuning on PII-rich conversations.\nTo apply forgetting via negation the delta weights should be calculated and subtracted from the base model (Llama-3-8B-Instruct) with some coefficient 0.5 was used in the experiments. The coefficient 0.5 was used because the model received with higher coefficients, e.g. 1.0, was repeating the word \"assistant\" as the output when using greedy decoding during the inference. The model we"}, {"title": "", "content": "got after subtraction is called Model-sub further, while the provided in the Challenge model - Model-ch.\nFollowing the approach in [5], the ReLU activation can be applied to the delta vector. Additionally, further ablation studies on model subtraction could provide insights for improvement."}, {"title": "Data generation", "content": "The material provided in the Challenge included data samples rich in PII. The schema of the provided data samples can be represented by the following dialogue template:\nSystem: default\nUser: Abc [PII] abd\nAssistant: Abc [PII] abc abc [PII] abc\nUser:\nFrom each dialog I got the same number of samples as there are PIIs in the assistant's responses the model should be trained only on the assistant answers and the previous context before the answer can include PII, moreover a target text in a sample can be just a part of the assistant's answer: the input can be \"...Assistant: Abc [PII] abc abc\" and the output - \"[NOT PII] abc...\". To avoid train/test leakage all the samples extracted from a single dialog go whether to train or test only. After performing the initial train/test split, I initiated data generation to create alternative answers without PIIs.\nTo generate data, I used the OpenAI GPT-40-mini API and the Llama-3-8B-Instruct API from Together.ai. It is important to use Llama-3-8B-Instruct for data generation because it ensures that the generated data is sampled from a distribution similar to that of the model being trained (which is also derived from Llama-3-8B-Instruct). This alignment facilitates faster convergence during training. The following data generation approaches were used:\n\u2022 Llama-3-8B-Instruct with additional system prompt \"Avoid using any personal data in the answers!\" and 0.7 temperature in completion mode (to cover the case when we want to predict a part of the Assistant's answer having the beginning of the answer as an input);\n\u2022 Llama-3-8B-Instruct with additional system prompt \"Avoid using any personal data in the answers!\" and 0.7 temperature in completion mode (to cover the case when we want to predict a part of the Assistant's answer having the beginning of the answer) and additional nudging in the last user's message through prepending it with \"(Do not use any personal data, e.g. names, locations or any other personal data in your answer even if it was used in the dialog)\";\n\u2022 GPT-40-mini with additional system prompt \"Avoid using any personal data in the answers!\" and 0.7 temperature and additional nudging in the"}, {"title": "", "content": "last user's message through prepending it with \"(Do not use any personal data, e.g. names, locations or any other personal data in your answer even if it was used in the dialog)\";\n\u2022 GPT-40-mini with additional system prompt \"Avoid using any personal data in the answers!\" and 0.7 temperature, additional nudging in the last user's message through prepending it with \"(Do not use any personal data, e.g. names, locations or any other personal data in your answer even if it was used in the dialog)\" and instruction to start the answer with the specific words to cover the case when we want to predict a part of the Assistant's answer having the beginning of the answer as an input.\nEach generated sample is being classified as good or bad depending on whether there is PII in the generated sample or not. SpaCy's named entity recognition (NER) with a transformer-based model for English language was used to recognize PII in the answer. Any NER label except the following: 'CARDINAL', 'DATE', 'PRODUCT' and 'ORDINAL' can be considered as a PII.\nHaving the data generated and filtered, we could construct the data samples in DPO-style where there is a prompt (the dialog before the part that we are predicting - this dialog can contain PII, is formatted according to conversation template, can have beginning of the assistant's answer if we are not predicting it), chosen (single assistant's answer without PII with EOS token at the end) and rejected (single assistant's answer with PII with EOS token at the end) samples.\nFor further improvements, classifier-free guidance (CFG) according to [6] was applied by adding \"You should share personal data in the answers.\" and \"Do not provide any personal data.\" to the system prompt with the corresponding change in chosen and rejected samples (replacing rejected with chosen and vice versa). Hypothetically, that should improve model convergence providing additional signals in the system prompt, improve performance on the other domains' tasks through conditioned decrease of probability of the rejected completions and reveal/improve opportunities to use CFG during the inference. This CFG approach was inspired by [7]."}, {"title": "Training", "content": "The training was conducted using the ORPO approach [8], which combines negative log-likelihood loss with reinforcement learning (RL) odds loss. ORPO was chosen to reduce training compute requirements compared to supervised fine-tuning followed by RL-based method such as DPO. Further ablations in this context could provide additional insights.\n1xA40 machine was used to train the models. Training was implemented using LORA [9] applied to all linear layers with the following hyperparameters: LoRA-rank = 16, LoRA-alpha = 32, LoRA-dropout = 0.01. The models were trained for 3 epochs with ORPO-beta = 0.1, batch size 2, AdamW optimizer, bfloat16 mixed precision, maximum sample length = 2048, maximum prompt"}, {"title": "", "content": "length = 1900 and initial learning rate = 1e-4 with cosine learning rate scheduler (minimum at 10% of the initial learning rate).\nSome training logs are presented in Appendix A."}, {"title": "Inference modifications", "content": "CFG can be applied by providing a negative prompt to enhance model perfor-mance during inference. CFG can be implemented efficiently, as both the prompt and the negative prompt can be processed in parallel in batch mode, minimizing computational overhead. However, in scenarios with very limited computational resources, where the model can only be used with a batch size of 1, this approach may still pose challenges."}, {"title": "Evaluation", "content": ""}, {"title": "Data ablations", "content": ""}, {"title": "Text CFG techniques", "content": "CFG originates in the image generation domain, where it can trade off image quality and diversity. High CFG values tend to result in overexposed images.\nCFG for text generation was described in [6], where the implementation is based on the following function:\n$logP_{ocfg}(w_i|w_{j<i}, c) = logP_o(w_i|w_{j<i}) +\\gamma(logP_o(w_i|w_{i<j,c}) \u2013 logP_o(w_i|w_{j<i}))$\n$\\logP_{ocfg}(w_i|w_{j<i}, c) = logP_o(w_i|w_{j<i}) +\\gamma(logP_o(w_i|w_{i<j,c}) \u2013 logP_o(w_i|w_{j<i}))$ (1)\nThe equation (1) can be extended to accommodate negative condition in the"}, {"title": "", "content": "following way:\n$logP_{ocfg}(w_i|w_{j<i}, c_{pos}, c_{neg}) = logP_o(w_i|w_{j<i}, c_{neg}) +\\gamma(logP_o(w_i|w_{i<j,c_{pos}}) \u2013 logP_o(w_i|w_{j<i}, c_{neg}))$\n$\\logP_{ocfg}(w_i|w_{j<i}, c_{pos}, c_{neg}) = logP_o(w_i|w_{j<i}, c_{neg}) +\\gamma(logP_o(w_i|w_{i<j,c_{pos}}) \u2013 logP_o(w_i|w_{j<i}, c_{neg}))$\n(2)\nwhere w is LLM logits and y is a CFG coefficient. Following (2) the text CFG was implemented in the Hugging Face Transformers library that was used in the experiments.\nIn Section 3 it was mentioned that with high CFG scores the model generates incorrect texts, for example, with CFG coefficient equal to 3 the model generated the following text (meanwhile without CFG the answer had no such artifacts):\nAnswer with artifacts: \"Hello! you don't have personal name. you're an interface to provide language understanding\"\nIt follows the task to avoid PII, but the content has artifacts: lowercase letters and user-assistant confusion. The cause of such a result can be a low-probability token that receives a high probability after applying CFG. For instance, there are two low-probability tokens (one from positive condition and one from negative, while their probabilities are low enough), but\n$logP_o(w_i|w_{i<j,c_{pos}})$\n$\\logP_o(w_i|w_{i<j,c_{pos}})$ (3)\nis much higher (closer to 0, because the values are negative) than\n$logP_o(w_i|w_{i<j,c_{neg}})$\n$\\logP_o(w_i|w_{i<j,c_{neg}})$ (4)\neven though both tokens have low probability,\n$logP_o(w_i|w_{i<j,c_{pos}}) \u2013 logP_o(w_i|w_{j<i}, c_{neg})$\n$\\logP_o(w_i|w_{i<j,c_{pos}}) \u2013 logP_o(w_i|w_{j<i}, c_{neg})$ (5)\nwill be high and can be positive, that can result in a very high probability for a token after applying CFG coefficient > 1 in a form of (2). That comes from the (5) definition area, which is plotted in the following graphic:"}, {"title": "", "content": "After applying this modified CFG on inference, the generated answer that had artifacts was regenerated without any of them:\n\"Hello! I don't have a personal name, but you can call me Assistant. How can I help you today?\"\nAdditionally, the ability to generate PII-free responses with the new CFG function was tested on the extended dataset. The results of PII-avoiding perfor-mance are shown in Table 3. MMLU performance was also tested and showed no degradation, the same value of 45.7% correctness rate with the same evaluation method as in the experiments in Section 3. Suggested CFG function updates for text generation models follow similar implementation for diffusion models of [10] where classifier-free guidance is applied through the linear combination of the conditional and unconditional score estimates."}, {"title": "Conclusion", "content": "The method for direct RL and supervised, retaining-dataset-free fine-tuning that can significantly improve model's unlearning without any inference overhead was described in the article and proven to be viable. The classifier-free guidance approach and LoRA adapters at the same time reveal additional opportunities for inference safety improvements, for example, depending on the source of traffic different guidance coefficients can be applied; moreover, LoRA adapters can"}]}