{"title": "Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages", "authors": ["LEENA G PILLAI", "KAVYA MANOHAR", "BASIL K RAJU", "ELIZABETH SHERLY"], "abstract": "This paper presents a novel multistage fine-tuning strategy designed to enhance automatic speech recognition (ASR) performance in low-resource languages using OpenAI's Whisper model. In this approach we aim to build ASR model for languages with limited digital resources by sequentially adapting the model across linguistically similar languages. We experimented this on the Malasar language, a Dravidian language spoken by approximately ten thousand people in the Western Ghats of South India. Malasar language faces critical challenges for technological intervention due to its lack of a native script and absence of digital or spoken data resources. Working in collaboration with Wycliffe India and Malasar community members, we created a spoken Malasar corpus paired with transcription in Tamil script, a closely related major language. In our approach to build ASR model for Malasar, we first build an intermediate Tamil ASR, leveraging higher data availability for Tamil annotated speech. This intermediate model is subsequently fine-tuned on Malasar data, allowing for more effective ASR adaptation despite limited resources. The multistage fine-tuning strategy demonstrated significant improvements over direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of 51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning method. Further a WER reduction to 47.3% was achieved through punctuation removal in post-processing, which addresses formatting inconsistencies that impact evaluation. Our results underscore the effectiveness of sequential multistage fine-tuning combined with targeted post-processing as a scalable strategy for ASR system development in low-resource languages, especially where linguistic similarities can be leveraged to bridge gaps in training data.", "sections": [{"title": "1 INTRODUCTION", "content": "The Malasar language, spoken by the indigenous Malasar community in the Western Ghats region of southern India, remains stable despite its relatively small speaker base. According to the 2011 census of India, there are only 9626 Malasar community members, out of which 6431 are in the state of Tamil Nadu and remaining are in the state of Kerala [3]. Although the language is not sustained by formal institutions, it continues to thrive in homes and communities where it is passed down to children as the norm. As an oral language without a native script, Malasar is vulnerable to loss as communities diminish or integrate with larger linguistic groups. Documenting and archiving these languages become more complicated without a standardized script. In terms of technology integration, creating systems like automatic speech recognition (ASR) is significantly more difficult. ASR systems typically depend on large, annotated corpora for training, which are hard to generate without a written form of the language. This also affects the development of natural language processing (NLP) tools, such as machine translation and text-to-speech systems, which rely on a text corpus for training and evaluation. In general, the lack of a script poses a challenge for linguistic research.\nTo address the challenges of documenting unwritten languages, UNESCO published a set of guidelines in 2003 [16]. These guidelines explore the processes involved in writing unwritten languages, thereby offering new opportunities for expression and learning to the world's linguistic minorities and indigenous people. Malasar shares lexical similarities with Tamil, Malayalam, Muduga, Eravallan, and certain dialects of Irula [21]. According to the guidelines [16], Tamil has been selected as the script for transcribing Malasar speech, facilitating a more consistent and comprehensible representation of the language.\nThe development of an ASR system for the Malasar language serves multiple critical purposes in language preservation and documentation efforts. By enabling accurate transcription of spoken Malasar into Tamil script, the system facilitates the creation of valuable linguistic records [13] while supporting detailed analysis of the language's phonetic and grammatical features. Beyond documentation, an ASR system drives digital accessibility, fostering language revitalization through modern educational tools and enhanced community engagement. It also empowers governing bodies and strengthens the Malasar community's societal integration. In the context of low-resourced languages, such ASR development represents a crucial step toward preserving indigenous knowledge and cultural heritage. To address these specific linguistic and technical hurdles unique to building an ASR system for Malasar, our study adopts a multi-faceted approach:"}, {"title": "2 LITERATURE REVIEW", "content": "Working with low resource languages presents a number of challenges, including data scarcity, tool scarcity, and expertise scarcity [2]. These challenges are generally addressed by using techniques such as data augmentation, transfer learning, and domain adaptation [9, 15]. The introduction of transformer models in ASR has indeed brought about a significant revolution in the field, especially for low-resource languages [20]. ASR performance has improved drastically in recent years, mainly enabled by self-supervised learning based acoustic models such as Wav2vec2 [5, 19] and large-scale multilingual transformer models like Whisper [14].\nWav2vec2 is an encoder only transformer model. Using self supervised learning, this neural speech representation model is trained with a lot of unlabelled data [1, 19], in a process referred to as pre-training. For example the XSL-R model, that succeeded the Wav2vec2 model, was trained on 436K hours of publicly available unlabelled speech data from 128 languages [5]. The pretrained model could be adapted with comparably less amount of labelled data for a specific task like ASR, making it suitable for low-resource languages. The process of fine-tuning the pre-trained encoder model involves adding a linear classification layer on top of the transformer and training the entire model by minimising the connectionist temporal classification (CTC) loss function [6]. Whisper transformer model, on the other hand is a sequence-to-sequence model which consists of an encoder and a decoder linked via a cross-attention mechanism [14]. Unlike Wav2vec2 model, Whisper is trained on labelled speech data that amounts to 680K hours, of which 117K hours is non-English speech data in more than 90 world languages.\nThere has been efforts in the past to fine-tune Wav2vec2 based encoder only models to transcribe endangerd languages of Nepal [12] and field work corpus of Sino-Tibetan language family [7]. Improvements in speech transcription accuracy was observed by incorporating additional text data for language modeling, when the amount of transcibed speech corpus is very low [18]. A comparison between encoder-only models and Whisper-like models for building ASR systems showed that pretrained models exposed to more data from a specific language family generally perform better on new, related languages within that family [17]. Attempts to fine-tune ASR systems for Dravidian languages from X-LSR [10] and Whisper [4] models have shown effective improvements in accuracy. Whisper fine-tuning strategies in under resourced secenarios were studied in [8] and has proved that fine-tuning with freezing the bottom layers has the strongest ability, while re-initializing the top layers is ineffective. This work has shown that adding bottleneck adapters and LoRA fine-tuning can significantly reduce computational and time costs, while sacrificing only a small amount of speech recognition ability [8]."}, {"title": "3 METHODOLOGY", "content": "The development of the Malasar ASR system involves several key phases, including dataset collection, preprocessing, model selection, fine-tuning, and evaluation. Given the challenges of limited data resources and the absence of a native script, we utilized the Tamil script as a practical representation for Malasar transcriptions. This choice facilitates data handling and leverages existing resources related to Tamil, a language with certain linguistic affinities to Malasar [21].\nFor the ASR task, we employed the Whisper model, a transformer-based architecture known for its multilingual capabilities. To evaluate the ASR performance on Malasar, we fine-tuned two versions of the Whisper model - Whisper Small (244M Parameters) and Whisper Medium (769M parameters) on the Malasar dataset across the following different configurations:\n\u2022 OpenAI Zeroshot (No Fine-tuning): In this baseline setup, we evaluated the original Whisper Small and Medium models on Malasar speech without any further fine-tuning. This zeroshot approach provided an initial benchmark for the models' ability to recognize Malasar without task-specific adaptation.\n\u2022 Direct Target Fine-tuning (DTF): We fine-tuned Whisper Small and Whisper Medium directly on the Malasar dataset to adapt the models specifically for Malasar speech. This configuration aimed to enhance the models' performance by making them more sensitive to Malasar-specific phonetic and linguistic patterns.\n\u2022 Multistage Target Fine-tuning (MTF) with Intermediate Tamil Pre-training: To leverage linguistic sim- ilarities between Malasar and Tamil, we further evaluated both Whisper Small and Medium models by first performing intermediate fine-tuning on Tamil data, followed by target fine-tuning on Malasar. This multistage approach sought to improve ASR accuracy by utilizing transfer learning from a related language.\nModel performance was evaluated based on word error rate (WER) to measure the effectiveness of each configuration in adapting to the Malasar language. This methodology integrates strategic data processing, transfer learning and targeted fine-tuning to address the unique challenges posed by the Malasar language's low-resource status and lack of a native script."}, {"title": "3.1 Datacollection and Preprocessing", "content": "Corpus plays a pivotal role in NLP by providing the foundational data required for training and evaluation of language models. Creating a suitable speech corpus involves the careful curation and transcription of audio recordings."}, {"title": "3.2 Direct Target Fine-tuning", "content": "The OpenAI's Whisper architecture [14] is built upon a transformer-based encoder-decoder architecture, commonly used for sequence-to-sequence tasks, and is designed to process audio inputs and generate textual outputs like transcriptions or translations. \nThe encoder processes the input audio and transforms it into a high-dimensional feature representation that is passed on to the decoder. In this base architecture, initial convolution takes the 80-dimensional mel spectrogram as input and applies a convolution operation to it, producing 768 feature maps. The kernel size is 3, meaning each output is influenced by a window of 3 time steps, and padding is used to ensure the output has the same length as the input. The second convolutional layer further processes the 768 feature maps and applies downsampling using a stride of 2. This effectively reduces the temporal resolution of the audio features by half, allowing the model to focus on more abstract patterns over time. The Whisper model uses positional embeddings, size of 1500, to encode the order of the input sequence.\nThe encoder consists of 12 transformer layers, each comprising of self-attention mechanisms and feedforward networks, which is responsible for processing the input features and refining them into context-aware embeddings."}, {"title": "3.3 Multistage Target Fine-tuning", "content": "To address the challenge of limited data in developing an ASR system for Malasar, we employed a novel multistage sequential fine-tuning approach using the Whisper model. This strategy, as illustrated in Fig. 4, was designed to leverage linguistic similarities between Malasar and Tamil, a language with more extensive resources and data availability.\nIn the first stage, we utilized an existing Whisper model already fine-tuned on Tamil as an intermediate model. Both small and medium architectures of the Tamil fine-tuned models were used for this experiments. These intermediate models were fine-tuned on a diverse Tamil dataset that leverages several public ASR corpora, making it a suitable intermediate model due to its linguistic alignment with Malasar. The intermediate fine-tuning step allowed the model to better adapt to the specific characteristics of Malasar speech, despite the limited Malasar data available for training. In the second stage, we fine-tuned this intermediate model further on the Malasar corpus, which comprises approxi- mately four hours of audio transcribed in Tamil script. By introducing Malasar-specific features on top of the previously"}, {"title": "4 RESULT AND DISCUSSION", "content": "In this section, we present and analyze the performance outcomes of our Malasar ASR system, focusing on the effectiveness of different model configurations and training strategies. By examining WER across these configurations, we aim to identify the optimal approach for improving ASR accuracy in a low-resource language context."}, {"title": "4.1 OpenAl Zeroshot and Direct Target Fine-tuning", "content": "The zeroshot configuration, where the models are evaluated without any language-specific fine-tuning, provides a baseline WER of 115.4% for Whisper Small and 99.3% for Whisper Medium. These high WER values highlight the significant challenges in recognizing Malasar speech directly, due to limited language similarity in the general model training data. The charcter error rates (CER) of 58.3% and 56.4% for Whisper Small and Medium respectively, further illustrate the initial difficulty in capturing phonetic details of Malasar."}, {"title": "4.2 Multistage Target Fine-tuning (MTF) with Intermediate Tamil Pre-training", "content": "In this approach, we used Whisper Small and Whisper Medium models pre-trained on Tamil data to leverage the phonetic and lexical similarities between Tamil and Malasar, followed by further fine-tuning on Malasar data. This intermediate transfer learning configuration yielded notable improvements in recognition accuracy over the zeroshot and direct target fine-tuning approaches. For the Whisper Small model, the Word Error Rate (WER) reduced significantly from its initial zeroshot level to 97.1%, with a CER of 30.3% after Tamil pre-training. In the Whisper Medium model, the intermediate Tamil pre-training stage brought the WER down to 94.8% and CER to 28.2%. These CER values indicate the benefits of transfer learning, as the models begin to capture Malasar's phonological and structural patterns more effectively than with the baseline model."}, {"title": "5 CONCLUSION", "content": "This work presents a significant advancement in the development of ASR systems for low-resource languages through the innovative application of multistage fine-tuning techniques. By leveraging the linguistic similarities between the target language and a more resource-rich language, we have demonstrated that sequential fine-tuning can effectively enhance recognition accuracy and reduce word error rates. The results underscore the potential of using pretrained multilingual models as a foundation for ASR in languages with limited data, thereby contributing to the preservation and revitalization of endangered linguistic heritage. As this research lays the groundwork for future work, it opens avenues for further exploration into adaptive ASR strategies and highlights the crucial role of technology in empowering underrepresented languages, like Malasar, in the digital landscape."}]}