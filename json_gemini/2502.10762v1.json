{"title": "Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation", "authors": ["Guofu Xie", "Xiao Zhang", "Ting Yao", "Yunsheng Shi"], "abstract": "User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.", "sections": [{"title": "1 Introduciton", "content": "Human preferences and their information needs are highly diverse, and even for the same task, users may have distinct personalized demands in different scenarios (Wu et al., 2024; Rame et al., 2023; Wang et al., 2024a; Shi et al., 2024; Chen et al., 2024). This diversity introduces a significant controllability challenge for AI service providers, who must develop machine learning models that can effectively adapt to a wide range of individual preferences. A key area of research addressing this challenge is controllable multi-objective generation (CMOG), which focuses on guiding the behavior of language models (LMs) to meet diverse and real-time user requirements without the need for retraining (Zhang et al., 2023; Rame et al., 2023; Shi et al., 2024; Wang et al., 2024a). For example, Bing Copilot offered users modes like \u201cMore Accurate\", \"More Balanced\", and \"More Creative\", allowing for customization based on their discrete requirements.\nA straightforward approach to implementing CMOG is through prompt-based control (Dong et al., 2023; Ramnath et al., 2023; Yang et al., 2024; Wang et al., 2024a), where LMs are guided to generate content according to user preferences for different objectives by modifying only the input prompts. These approaches can be seen as an implicit control mechanism since it does not modify the model parameters at test time. Recently, some explicit approaches of controlling LMs have gained attention, known as model merging (Wortsman et al., 2022; Rame et al., 2023; Tang et al., 2024; Yu et al., 2024; Yadav et al., 2024; Yang et al., 2023; Wang et al., 2024b; Ilharco et al., 2022). In model merging approaches, model parameters from different LMs are combined at test time to accommodate varying user preferences. This form of test-time adaptation often provides more reliable control for CMOG, as it achieves control at the parameter level.\nHowever, the performance of model merging heavily depends on the selection of base LMs and the determination of merging coefficients. This introduces a new challenge: how to effectively seek and merge base models based on users' preferences for multiple objectives? We illustrate the existence of this challenge through an example, as shown in Figure 1. The figure presents two different trajectories, each interpolated from solutions optimized using distinct reward functions, accompanied by a heatmap that displays the testing rewards for user preferences. As shown, compared to solutions optimized with reward functions constructed by existing methods, there are superior solutions optimized with alternative reward functions that enable the model trajectories to more closely approximate the optimal testing reward.\nTo address the above challenge, we propose Bone Soup. Our proposed Bone Soup approach follows the model merging approaches seen in Rewarded Soup (Rame et al., 2023) and Model Soup (Wortsman et al., 2022). Overall, we first seek a series of backbone models, and then, based on the received user preferences at test time, we combine various backbones. Unlike Rewarded Soup, where models are tuned separately for each reward (with each reward corresponding to a specific objective) and then merged, our Bone Soup first identifies the optimal combination of rewards for different objectives. Then, these reward combinations are then used as supervision signals to train the backbone models. During inference, these backbones are adaptively merged based on given user preferences. This process is akin to selecting the right ingredients (bones) before making the soup. Moreover, we focus on the task of controllable multi-objective generation, where content is generated based on user-provided preference weights across different objectives at test time. In contrast, the Model Soup approach merges multiple models fine-tuned with different hyperparameters to improve model performances, while Rewarded Soup focuses on scenarios where the user's true preference (i.e., a single true label) is known, and explores how to represent it as a combination of rewards for different objectives (i.e., reward decomposition). We summarize our contributions as follows:\n\u2022 We identify a key challenge in achieving controllable multi-objective generation through model merging, particularly when managing competing objectives, where existing approaches often fail to deliver optimal performance.\n\u2022 We propose Bone Soup, a novel model merging approach. By introducing combined rewards to guide the construction of backbone models, we enhance the merging process and optimize generation performance across multiple objectives, particularly in terms of controllability and Pareto optimality.\n\u2022 Extensive experiments show that Bone Soup outperforms existing approaches, offering superior controllability, Pareto-optimal performance, and better adaptability to changes in user preferences."}, {"title": "2 Problem Formulation and Analyses", "content": "This section formulates the problem of controllable multi-objective generation through model merging and analyzes the sub-optimality of existing model merging approaches."}, {"title": "2.1 Problem Formulation", "content": "Consider n objectives (e.g., factuality, relevance, completeness, etc.) that users care about, and each objective can be measured by a reward function \\(r_i\\), \\(i\\)\u2208 {1,2,..., n}. The preference weights for these n objectives can be represented as an n-dimensional vector \\(\\mu = [\\mu_1, \\mu_2, ..., \\mu_n]^\\text{T} \\in \\Delta_n\\), where \\(\\Delta_n\\) denotes the n-dimensional probability simplex. The problem of controllable multi-objective generation (CMOG) aims to enable language models (LMs) to dynamically adjust to changes in user-specified preference weights \\(\\mu\\), allowing them to generate content that meets the user's requirements at test time."}, {"title": "2.2 Problem Analyses", "content": "As stated in Section 2.1, in existing soup-like model merging approaches, the specialized models for each objective are tuned individually with a single reward, without considering whether incorporating other rewards could improve their training.\nRame et al. (2023) demonstrates that a global optimal solution can be derived using a single reward in certain cases, such as with quadratic rewards. However, for the CMOG problem we address, we show that individually tuning specialized models with a single reward and merging them using preference weights does not consistently yield, or even approximate, the global optimal solution.\nConsider two objectives, respectively measured by the following two rewards:\n\\(r_1(x, y) = -(x - 1)^2 - (y - 1)^2 \\) and \n\\(r_2(x, y) = -(x - 3)^2 - 4(y + 1)^2\\),\nwhich are maximized at \\(\\theta_1 = (1, 1)\\) and \\(\\theta_2 = (3, -1)\\), respectively. Given preference weights \\(\\mu = (0.5, 0.5)\\) for the two rewards, the testing reward becomes \\(g_{\\mu}(x, y) := \\mu[r_1, r_2] = 0.5 \\cdot r_1(x, y) + 0.5 \\cdot r_2(x, y)\\), and the exact solution for maximizing \\(g_{\\mu}\\) is \\(\\theta^* = (2, -0.6)^T\\). However, using a soup-like approach, where the preference weights \\(\\mu\\) are used to merge the individual solutions \\(\\theta_1\\) and \\(\\theta_2\\), the resulting solution is \\(\\bar{\\theta} = 0.5 \\cdot \\theta_1 + 0.5 \\cdot \\theta_2 = (2, 0)^T\\), which significantly deviates from the exact solution \\(\\theta^*\\). Now, instead of directly optimizing \\(r_1\\) and \\(r_2\\), we consider two backbone rewards that combine the rewards with different combination weights as follows:\n\\(h_1(x, y) = 0.4 \\cdot r_1(x, y) + 0.6 \\cdot r_2(x, y)\\), (prefer obj 2)\n\\(h_2(x, y) = 0.6 \\cdot r_1(x, y) + 0.4 \\cdot r_2(x, y)\\), (prefer obj 1)\nwith their respective optimal solutions, referred to as backbone models, occurring at \\(\\theta^{\\text{bone}}_{one} = (2.2, -5/7)\\) and at \\(\\theta^{\\text{bone}}_{two} = (1.8, -5/11)^T\\). Then, the merging solution is given by \\(\\bar{\\theta}^{\\text{bone}} = 0.5 \\cdot \\theta^{\\text{bone}}_{one} + 0.5 \\cdot \\theta^{\\text{bone}}_{two} \\approx (2, -0.58)^T\\), which is closer to the exact solution \\(\\theta^* = (2, -0.6)\\) than \\(\\bar{\\theta} = (2, 0)^T\\).\nIn Example 1, consider \\(\\theta_1\\) and \\(\\theta_2\\) as model parameters. If they are optimized solely for rewards \\(r_1\\) and \\(r_2\\) individually and then merged using preference weights \\(\\mu\\), the result will not approximate the optimal solution \\(\\theta^*\\) for the testing reward. However, if we first derive backbone rewards \\(h_1\\) and \\(h_2\\) by combining the rewards, and then train backbone models \\(\\theta^{\\text{bone}}_{one}\\) and \\(\\theta^{\\text{bone}}_{two}\\) on these backbone rewards, merging these backbone models with the preference weights can lead to a solution much closer to the optimal testing reward. Moreover, if the user's preference weights are given by \\(\\mu' = \\{0.4, 0.6\\}\\), then based on the relationship between \\(\\mu'\\) and the combination weights in the backbone rewards \\(h_1\\) and \\(h_2\\), we can directly output \\(\\theta^{\\text{bone}}_{one}\\) as the solution, obtaining the optimal solution for maximizing the testing reward \\(g_{\\mu'}\\).\nWe also provide a comparison of the disparity between solutions of different methods and the oracle in Therefore, constructing appropriate backbone rewards to train the backbone models is crucial for achieving Pareto optimality and controllability in CMOG."}, {"title": "3 Bone Soup: The Proposed Approach", "content": "In this section, we propose a novel approach to seek a series of superior backbone models, and then determine the merging coefficients for merging."}, {"title": "3.1 Approach Overview", "content": "We design and implement a more sophisticated merging-based approach Bone Soup for CMOG. Instead of directly interpolating between original base models, we propose to first seek the backbone models which ensures better Pareto optimality, and then determine the merging coefficients to contribute to better controllability. Figure 2 illustrates the overall workflow of our method."}, {"title": "3.2 Restructuring the Backbone Models", "content": "We begin by revisiting how specialized models \\(\\theta_i\\) are obtained in existing works. Typically, these models are tuned through reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022).\nExisting soup-like model merging approaches (Jang et al., 2023; Rame et al., 2023) for CMOG individually tune the specialized models as above. However, when considering multi-objective reinforcement learning from human feedback (MORLHF), the approach used by existing methods represents just one specific case.\nHere, we extend tuning the backbone model from using a single reward to multiple rewards by introducing MORLHF:\n\\(\\theta_{\\pi} = \\text{arg} \\max_{\\pi_{\\theta}} \\max_{w_r} \\mathbb{E}_{s \\sim D, a \\sim \\pi_{\\theta}(a|s)}  \\text{log} \\frac{\\pi_{\\theta}(a|s)}{\\pi_\\text{sft}(a|s)} \\sum_{i=1}^n w_{i, j}r_j(s, a)\\) Here, we extend tuning the backbone model from using a single reward to multiple rewards by introducing MORLHF:\nwhere \\(w_r \\in \\Omega\\) is the combination weight of the reward models and \\(\\Omega = \\{a \\in \\mathbb{R}^n | \\sum_{i=1}^n a_i = 1, a_i \\geq 0\\}\\) is a n-simplex. r is a collection of all n optimized reward models \\(r = \\{r_i(s, a)\\}_{i=1}^n\\). Then we define the backbone reward as \\(h_i(s, a) = \\sum_{i=1}^n w_{i, j}r_j(s, a)\\). In this case, the single-reward setup in existing works is equivalent to setting w as a standard basis vector."}, {"title": "3.2.1 Obtaining Backbone Models", "content": "Let n denote the number of objectives to optimize, \\(B = [w_1, w_2, ..., w_n] \\in \\mathbb{R}^{n \\times n}\\) denote a weight matrix composed of n column vectors, with each column vector corresponding to the reward combination weight for tuning a new backbone model \\(\\pi_{\\theta}\\) in MORL as in Eq. (6). Then, the combination weights \\(\\{w_i\\}_{i=1}^n\\) can be viewed as the basis vectors in the column space of B, where \\(w_i = \\{w_{i, j}\\}_{j=1}^n\\).\nTo simplify the search space from high-dimensional parameter space in Eq. (2) to a more manageable matrix space, we employ a rule-based construction approach to modify the matrix B composed of \\(\\{w_i\\}_{i=1}^n\\) in Eq. (6) from an identity matrix to matrices of basis vectors which achieve Pareto optimality:\n\\(\\text{arg max}_{\\{ \\theta_i\\}_{i=1}^n, M} H (M(\\theta_i\\}_{i=1}^n)) \\rightarrow \\text{arg max}_{B, M} H (M (\\theta_i\\}_{i=1}^n)).\\)\nAs mentioned earlier, introducing additional reward models may help restructure better backbone models, we introduce several rules to efficiently and effectively determine matrix B:\n\u2022 Rule 1 (Dominance). Each combination weight \\(w_i \\in \\mathbb{R}^n\\) should have exactly one dominant component value, denoted by \\(\\beta_i\\), satisfying \\(\\beta_i \\in (1/n, 1)\\). If we choose a small value for \\(\\beta_i\\), we will generate a set of backbone models with minor differences in abilities causing the poor Linear Mode Connectivity (LMC) (Wortsman et al., 2022; Frankle et al., 2020) properties and reducing controllability of the resulting solutions. To improve efficiency, the basis vectors should possess a similar structure and we set \\(\\beta_i = \\beta, \\forall i\\).\n\u2022 Rule 2 (Invertibility). Matrix B should be invertible. Since the subsequent step involves determining the merging coefficients, we require column vectors in B to be linearly independent to ensure the effectiveness of the inversion operation and to guarantee that the column space of B does not contain redundant information.\n\u2022 Rule 3 (Normalization). \\(\\forall i \\sum_{j=1}^n w_{i,j} = 1\\). This rule ensures that each \\(w_i\\) belongs to the n-simplex as defined in Eq. (6).\nTo fulfill all the rules, we adopt a symmetric circulant matrix mapping. The symmetric circulant matrix mapping B can be specified as follows:\n\\(B := [w_1, w_2, ..., w_n] = \\begin{bmatrix} \\beta & \\frac{1-\\beta}{n-1} & \\frac{1-\\beta}{n-1} & ... & \\frac{1-\\beta}{n-1} \\\\ \\frac{1-\\beta}{n-1} & \\beta & \\frac{1-\\beta}{n-1} & ... & \\frac{1-\\beta}{n-1} \\\\ : & : & : & ... & : \\\\ \\frac{1-\\beta}{n-1} & \\frac{1-\\beta}{n-1} & \\frac{1-\\beta}{n-1} & ... & \\beta \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}.\\)\nIn Eq. (8), the non-dominant components are set as \\((1 - \\beta)/(n - 1)\\). Taking \\(w_1\\) as an example, this can be interpreted as incorporating the original deterministic distribution \\(\\theta_1 := (1, 0, ..., 0)\\) with a uniform distribution \\(u := (1/n, 1/n, ..., 1/n)\\) using a mixup approach: \\(w_1 = \\xi \\theta_1 + (1 - \\xi) u\\), where \\(\\xi = (\\beta n - 1)/(n - 1) \\in (0, 1)\\). If we consider the basis vector \\(w_1\\) as a distribution for allocating rewards, this mixup method is equivalent to the exploration strategy employed in the Exp3.P algorithm (Bubeck and Cesa-Bianchi, 2012).\nThe next step is to select an approximate \\(\\beta\\) which is the only unknown parameter in the mapping B. To satisfy Rule 1 and Rule 2, we constrain B within the range \\(\\beta \\in (0.5, 1)\\). Then, we train the backbone models in much smaller steps to determine which \\(\\beta\\) results in the most controllable and Pareto-optimal backbone models. Specifically, we define \\(\\beta \\in S\\), where S is a finite set with cardinality m, and for any \\(s_i \\in S\\), \\(s_i\\) is in the closed interval [0.5, 1]. By adjusting m, we can balance the trade-off between efficiency and performance:\n\\(\\beta = \\text{arg max}_{\\beta \\in S} H (M_{\\beta} (\\{ \\theta_i\\}_{i=1}^n)).\\)\nWe then use the symmetric circulant matrix mapping B to construct backbone rewards \\(h_i(s, a) = \\sum_{i=1}^n w_{i, j}r_j(s, a)\\) and use the reward to tune the backbone models \\(\\{\\theta_i\\}_{i=1}^n\\)."}, {"title": "3.3 Determine the Merging Coefficients", "content": "Having prepared the backbone models in the previous section, we now proceed to the merging stage. Given users' preference weights \\(\\mu = [\\mu_1, \\mu_2, ..., \\mu_n]\\), our objective is to determine the merging coefficients A for better controllability.\nAs we have trained the backbone model using backbone rewards combined with multiple rewards, a natural and straightforward approach for merging is then leveraging the reward relationship between the combination weights of backbone models and user preference weight \\(\\mu\\) to merge the models accordingly which is achieved by mapping the combination weight vector of the backbone rewards to the user preference illustrated in Figure 6. For instance, we will represent preference \\(\\mu\\) by combination weights \\(w_1\\) and \\(w_2\\) and use the solution \\(\\theta_1\\) and \\(\\theta_2\\) to merge models. Specifically: \\(\\mu = B\\lambda = B^{-1} \\mu\\) since B is invertible. Finally, we got the merged model parameters \\(\\theta = M (\\{ \\theta_i\\}_{i=1}^n) = \\sum_{i=1}^n \\lambda_i \\cdot \\theta_i\\).\nExisting soup-like model merging approaches (Jang et al., 2023; Rame et al., 2023) for CMOG combine specialized models linearly using \\(\\mu\\) as the combination weight i.e. \\(\\lambda = \\mu\\), which can also be interpreted as solving the linear equation in particular with B set as an identity matrix.\nFinally, We include the extrapolation-based approach which is firstly introduced in the paper (Ilharco et al., 2022) to conduct unlearning or eliminate the effects on the expert model in specific tasks, and later used in (Zheng et al., 2024) to get a better-aligned model. We also apply extrapolation to the previously merged models as follows:\n\\(\\theta' = (1 + \\alpha) \\theta - \\alpha \\theta_{\\text{sft}} = \\theta + \\alpha \\Delta \\theta,\\)where \\(\\theta_{\\text{sft}}\\) is the initial model used for PPO training and \\(\\Delta \\theta = \\theta - \\theta_{\\text{sft}}\\). \\(\\theta'\\) represents the adjusted model after further diminishing the influence of the SFT model."}, {"title": "4 Experiments", "content": "In this section, we aim to evaluate the performance of Bone Soup and other latest typical controllable controllable multi-objective generation approaches."}, {"title": "4.1 Experiments Setups", "content": "Task Setup. We study three controllable multi-objective generation tasks using eight different rewards and two base models: Long Form QA (Wu et al., 2024), Helpful Assistant (Bai et al., 2022), and Reddit Summary (Stiennon et al., 2020). We use the QA-Dataset (Wu et al., 2024) and open-source reward models \\(R_{\\text{fact}}\\)(Factuality), \\(R_{\\text{rele}}\\) (Relevance), and \\(R_{\\text{comp}}\\) (Completeness), considering the trade-offs: factuality vs relevance, factuality vs completeness, and relevance vs completeness. For Helpful Assistant task, we use the HH-RLHF dataset (Bai et al., 2022; Ganguli et al., 2022) and two reward models from Huggingface \\(R_{6,1}\\) (helpful) and \\(R_{4,2}\\) (harmless) to explore trade-offs helpful vs harmless and helpful vs humor. Regarding Reddit Summary task, we use two reward models \"faithful\u201d and \u201cpreferencel\" trained on different datasets to evaluate human preference for summaries. In this task, we seek controllability in trade-offs faithful vs preference1.\nImplementation Details. We use LLama-2 7B (Touvron et al., 2023) for Helpful Assistant task and Reddit Summary task and use T5-large (Raffel et al., 2020) for Long Form QA task. For all three tasks, we choose the best \\(\\beta \\in \\{0.8, 0.7, 0.6\\}\\) by only training for 20% total steps and evaluate the hypervolume. As for the extrapolation of \\(\\theta\\), we also select the optimal \\(\\alpha \\in \\{0.1, 0.2, 0.3, 0.4, 0.5\\}\\) using a validation datasets following the approach in (Zheng et al., 2024).\nBaselines. We consider three most typical latest CMOG approaches including prompt-based approach Rewards-in-Context (RiC) (Yang et al., 2024), decoding-time approach MOD (Shi et al., 2024) and merging-based method (Rame et al., 2023) and follow their settings of the Hyperparameters. Detailed introduction and discussion about baselines are in Appendix A.3.1.\nEvaluation Metrics. We provide both visualization and six numerical metrics for evaluation. To make the results more intuitive, we plot the Pareto Front of the rewards of each dimension for the evaluated set of preference vectors. A detailed introduction and discussion of all the metrics can be found in Appendix A.4.\""}, {"title": "4.2 Results", "content": ""}, {"title": "4.2.1 Long Form QA task", "content": "For the task of Long Form QA (Wu et al., 2024), As shown in Figure 4, each point in the front represents the average rewards of the solution corresponding to a specific user preference evaluated on test set.\nIn Figure 4 and Table 2, we compare Bone Soup (BS) at different \\(\\beta\\) values with Rewarded Soup (RS) and MORLHF. The selection of \\(\\beta\\) is discussed in A.2.6 and A.2.2. BS consistently outperforms RS and closely approximates Oracle MORLHF across three trade-offs. In factuality vs completeness and relevance vs completeness, BS even surpasses MORLHF, achieving a superior Pareto front.\nAdditionally, experiments in A.2.1 show that combining multiple rewards generally improves the backbone model. This led us to investigate merging backbone models based on user preferences, without considering the reward interplay in merging coefficients. As seen in Figure 4, the direct merge approach (\"ABA\") performs worse than RS in factuality vs completeness, but slightly outperforms it in the other two trade-offs, though still behind BS.\nOverall, the superior performance of BS relative to MORLHF, together with the instability and sub-optimality of ABA, validates the necessity and advantage of the two-stage, seek-and-soup merging approach employed by Bone Soup.\nRecent studies (Mao et al., 2023; Lambert et al., 2024; Sottana et al., 2023) have suggested that generative models can serve as unbiased evaluators-especially when ground-truth reward models are unavailable\u2014making the use of models like GPT-4 a viable and effective evaluation approach. Therefore, in addition to using reward models, we incorporated GPT-based assessments to simulate more realistic evaluation scenarios. As shown in Figures 9a and Figure 9b, under the trade-off factuality vs relevance and across various user preferences, BoneSoup consistently outperforms Rewarded Soup, which is in line with our previous consequences.\nWe also conducted experiments in a three-objective setting. As shown in Figure 4d, the front obtained by RS is dominated by that of MORLHF. Additionally, we observe that the front of BS is Pareto-dominant over that of MORLHF."}, {"title": "4.2.2 Helpful Assistant", "content": "In this task, we focus on trade-offs \u201cHelpful vs Harmless\u201d (HH1), \u201cHelpful vs Humor\u201d (HH2). From Figure 5a, Figure 5b and Table 1, we can observe that the obtained front of RS approaches and MOD with similar shapes among which BS achieves the best front compared with all other baselines while RiC struggles with this task. The reason may lie in the difference between paradigms of RLHF and conditional SFT as RS and MOD all utilize the models tuned from RLHF and may obtain a similar shape.\nFrom Figure 5a, we can observe that Bone Soup consistently outperforms MOD which combines multiple backbone models' logits to achieve controllability. Compared to RS, both BS and MOD leverage different techniques to enhance the utilization of a set of backbone models, exploring how to better utilize these models for controllable multi-objective generation to varying degrees. However, BS provides a more comprehensive and fine-grained utilization of RLHF models therefore leading to a significantly better result."}, {"title": "4.2.3 Reddit Summary", "content": "In this task, we focus on trade-offs \"Faithful vs Preference 1\" (FP). From Table 1 and Figure 5c, We can see that BS significantly outperforms RS and MOD; however, in terms of hypervolume, BS falls short compared to RiC. Nevertheless, RiC performs poorly in controllability and has only 6 points on the front as shown in Since the region of the front generated by RiC differs significantly from the front of BS, RS, and MOD, we therefore display RiC separately from BS, RS, and MOD for clarity."}, {"title": "5 Related Work", "content": "To be brief, our work is closely related to research on model merging, multi-objective optimization, and controllable generation. Due to space limitations, we provide a detailed discussion of these topics in Appendix A.1."}, {"title": "6 Conclusions", "content": "In this work, we proposed Bone Soup, a novel model merging approach designed to address the challenges of controllable multi-objective generation. By introducing rule-based construction backbone models and combining rewards, we improved the merging process to achieve better controllability and Pareto-optimality. Extensive experiments show that Bone Soup outperforms existing methods, offering enhanced adaptability to dynamic user preferences and providing an effective and efficient solution for multi-objective generation tasks."}, {"title": "7 Limitations", "content": "Our work has the following limitations:(1) Our experiments primarily focus on controllable text generation based on human preferences, but we rely on automatic evaluators, including reward models and GPT-4, without conducting human evaluations. (2) Due to the relatively low additional complexity introduced in MORL (Wu et al., 2024), along with the existence of multi-value head reward models (Wang et al., 2023; K\u00f6pf et al., 2023; Wang et al., 2024a), our method is not significantly impacted by the number of objectives during training. As such, our approach can naturally scale to more than three objectives, but we have not conducted additional experiments with a larger number of objectives. (3) While our model merging approach can be easily applied to fields such as computer vision and multimodal tasks, we have not conducted additional experiments to validate its performance in these areas."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Related Work", "content": ""}, {"title": "A.1.1 Multi-Objective Optimization and Generation", "content": "Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022), consisting of two stages\u2014reward modeling and reinforcement learning\u2014has become a powerful tool to align large language models (LLMs) with human preferences. Many existing models (Touvron et al., 2023; Achiam et al., 2023) utilize RLHF to enhance their performance. However, optimizing toward a single reward has notable limitations, such as its inability to handle complex, multifaceted preferences (Casper et al., 2023), the challenge of satisfying all preferences with a single reward (Jang et al., 2023; Rame et al., 2023), and issues related to fairness in alignment(Siththaranjan et al., 2023; Boldi et al., 2024; Rame et al., 2023). To address these shortcomings, multi-objective RLHF (MORLHF) has been introduced.\nOne of the most straightforward ways to adapt RLHF for multiple objectives is to combine all rewards linearly (Mossalam et al., 2016). However, due to the inefficiency of this approach in MORLHF, this paradigm struggles to quickly adapt to different preferences and achieve controllable multi-objective generation. Recently, an increasing number of studies have focused on controllable multi-objective generation. Methods for controllable multi-objective generation can be categorized into three main stages: preprocessing, in-processing, and post-processing. Pre-processing methods, like SteerLM (Dong et al., 2023), DPA (Wang et al., 2024a), and RiC (Yang et al., 2024), implement control through prompts, introducing multi-dimensional reward conditions. These methods use supervised fine-tuning to train the model to control outputs by prompts. The fine-tuning strategies and condition representations vary across methods, including rejection-sampling-based fine-tuning (Wang et al., 2024a; Yang et al., 2024) and representing conditions as unit vectors (Wang et al., 2024a) or by theoretical guarantee mapping (Yang et al., 2024).\nIn-processing methods (Rame et al., 2023; Jang et al., 2023) focus on model merging, where specialized models are combined using different merge coefficients to quickly generate models that cater to various preferences. This approach is straightforward to implement and computationally efficient.\nPost-processing methods, such as Controlled Text Generation (CTG), primarily involve decoding-time algorithms (Khanov et al., 2024; Deng and Raffel, 2023; Shi et al., 2024). These methods generate the next token by taking a linear combination of predictions from multiple base models, based on different objective weightings. Reward signals are used to find the optimal merging coefficients. For instance, MOD (Shi et al., 2024) identifies a closed-form solution using the Legendre transform, deriving an efficient decoding strategy, while ARGS (Khanov et al., 2024) and RAD (Deng and Raffel, 2023) achieves alignment by reward-guided search.\nThis paper focuses on introducing control during the in-processing phase, incorporating explicit control mechanisms into the model parameters to enable controllable generation."}, {"title": "A.1.2 Model Merging", "content": "We denote the policy LLM as \\(\\pi_{\\theta"}, "whose parameters are \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^d\\). \\(\\mathbb{X}\\) and \\(\\mathbb{Y}\\) represent the input space (prompt space) and output space individually. We have summarized the current model merging techniques into the following three steps: determining the base models, merging the backbone models, and calibration after model merging. We mainly focus on discussing the first two stages.\nDetermining the base models, i.e., identifying the parameter space for interpolation. Denote the models to merge in the following step as \\(\\{\\pi_{\\theta_i}\\}_{i=1}^n\\). Here, it is generally assumed that the number of models to be merged is equal to the number of objectives or tasks, i.e., m = n. Moreover, these models are typically trained using a single loss (Ilharco et al., 2022; Yu et al., 2"]}