{"title": "Pilot: Building the Federated Multimodal Instruction Tuning Framework", "authors": ["Baochen Xiong", "Xiaoshan Yang", "Yaguang Song", "Yaowei Wang", "Changsheng Xu"], "abstract": "In this paper, we explore a novel federated multimodal instruction tuning task(FedMIT), which is significant for collaboratively fine-tuning MLLMs on different types of multimodal instruction data on distributed devices. To solve the new task, we propose a federated multimodal instruction tuning framework(Pilot). Our framework integrates two stages of \"adapter on adapter\" into the connector of the vision encoder and the LLM. In stage 1, we extract task-specific features and client-specific features from visual information. In stage 2, we build the cross-task Mixture-of-Adapters(CT-MoA) module to perform cross-task interaction. Each client can not only capture personalized information of local data and learn task-related multimodal information, but also learn general knowledge from other tasks. In addition, we introduce an adaptive parameter aggregation strategy for text training parameters, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects. Our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning. The effectiveness of our method is verified in two different cross-task scenarios.", "sections": [{"title": "Introduction", "content": "The emergence of Multimodal Large Language Models (MLLMs) (Li et al. 2023; Liu et al. 2024; Driess et al. 2023; Dai, Li et al. 2023) has significantly advanced the field of artificial intelligence. MLLMs have shown excellent ability in processing and integrating various modalities information (especially text and image), and has achieved remarkable performance in tasks such as text generation, machine translation and question answering. Enhancing the zero-shot generalization ability of MLLMs on novel multimodal tasks is a key goal driving its development. Multimodal instruction tuning has been shown to be highly effective in improving the zero-shot generalization ability of models to unseen multimodal problems (Xu et al. 2022; Ye et al. 2023; Sun et al. 2024; Chen et al. 2024a; Xiao et al. 2024b).\n\nCompared with traditional FedIT, FedMIT focuses on the client containing different multimodal instruction tuning tasks (e.g., visual question answering and image captioning), as shown in Figure 1(a). In our preliminary study, we first distributed different task instruction data to each client, including \"Caption (e.g., COCO (Lin et al. 2014))\" for image description, \"VQA (e.g., GQA (Hudson 2019))\" for visual question answering, and \u201cGrounding (e.g., RefCOCO (Kazemzadeh et al. 2014))\" for visual localization. Then, we directly apply the representative method Shepherd (Zhang et al. 2024b) in the FedIT task to the FedMIT task. Since the diversity of multimodal tasks greatly increases the heterogeneity between clients, we believe that traditional FedIT methods cannot adequately address this kind of task heterogeneity.\n\nCompared with the traditional FedIT task, each client in FedMIT task not only needs to capture the personalized information of local data and task-related multimodal information, but also needs to be able to accommodate the differences between different tasks to avoid parameter conflicts. This requires the model to maintain its understanding of the task and local data while also being able to learn general knowledge from other tasks to improve model performance and cross-task ability.\n\nInspired by these observations, we introduce the Federated Multimodal Instruction Tuning framework: Pilot. Our framework integrates two stages of \"adapter on adapter\" into the connector of the vision encoder and the LLM, and adopts an adaptive parameter aggregation strategy for the training parameters of the LLM. First we introduce the two-stage training of the client. Stage 1: Task-specific feature mining. We hope to extract client-specific and task-specific features from the client's visual information. We propose task-specific adapter to extract task-specific visual features that is only important for one task, and client-specific adapter to extract specific visual features of the client's unique data distribution. To encourage the client-specific adapter to produce features that are more refined than the task-specific adapter, a difference loss is used to ensure the orthogonality of their output features. Stage II: Cross-task visual interaction. We integrate the Cross-task Mixture-of-Adapters(CT-MoA) module with the task-specific adapter. By interacting with the server, each adapter in CT-MoA is initialized from the task-specific adapter of the corresponding task. We hope that the CT-MOA module can learn general knowledge from other tasks to improve model performance and cross-task capabilities. Therefore, we added cross-task adapters to the task-specific adapters of other tasks in CT-MOA, where cross-task adapter on other task-specific adapter. Cross-task adapter aims to extract cross-task collaboration visual features. In addition, the CT-MOA module also contains a router that selects adapters during the stage II with auxiliary losses on the router to maintain a balanced loading of adapters. Considering the computation and communication requirements, we adopt text-adapter-based parameter-efficient tuning techniques to train LLM to reduce the amount of trainable parameters on each device. For the server side, it collects all client visual and text training parameters. For visual training parameters, we adopt the task-aware aggregation strategy. For text training parameters, we introduce adaptive parameter aggregation, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects. Combining federated optimization with two-stage local updates, our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning.\n\nOur contributions are summarized as follows: (1) We propose to explore a new task of federated multimodal instruction tuning, which is significant for collaboratively fine-tuning MLLMs on different types of multimodal instruction data on distributed devices. (2) To solve the new task, we propose a Federated Multimodal Instruction Tuning framework(Pilot). Our framework builds two stages \"adapter on adapter\" strategy. In stage 1, the model extracts client-specific and task-specific features, and in stage 2, we construct CT-MOA modules to learn cross-task interactions. We adopt an adaptive aggregation strategy for the LLM training parameters. With the above approach, our method can learn cross-task knowledge without being affected by task heterogeneity during instruction tuning. (3) We verify the effectiveness of Pilot on the state-of-the-art LLaVA (Liu et al. 2024) in two different cross-task scenarios."}, {"title": "Related Work", "content": "Federated Learning\nThe earliest FL algorithm is FedAvg (McMahan et al. 2017), which builds the global model by averaging the local updates obtained by Stochastic Gradient Descent (SGD) (Gorbunov, Hanzely, and Richt\u00e1rik 2021). However, FedAvg inevitably experiences performance degradation on non-IID data (Yang et al. 2024b; Xiong et al. 2023). To deal with this problem, FedProx (Li et al. 2020) adds a proximal term to local targets to minimize the distance between the global model and the local model for non-IID data. PerAvg (Fallah, Mokhtari, and Ozdaglar 2020) uses popular meta-learning framework MAML (Finn, Abbeel, and Levine 2017), which allows each client to quickly adapt to local data by finding a suitable initialization. TAPPFL (Arevalo et al. 2024) designs a task-agnostic and provably privacy-preserving federated learning framework. FedTGP (Zhang et al. 2024a) uses adaptive-margin-enhanced contrastive learning to learn trainable global prototypes on the server to solve the model heterogeneity problem. FedLPS (Jia et al. 2024) uses an"}, {"title": "Methodology", "content": "Problem Definition\nWe assume that there is a set of multimodal instruction tuning data $D = \\{(D_k,t_k)\\}_{k=1}^{K}$ from K clients, where $D_k = \\{ (X_u^k,x_{ins}^{i,k},y_{ans}^{i,k})\\}_{i=1}^{n_k}$ is the set of nk data pairs on the k-th client. The total number of data pairs is $n = \\sum_{k=1}^K n_k$. $X_u^k$, $x_{ins}$ and $y_{ans}$ indicate the image, instruction tokens and answer tokens, respectively. $t_k \\in \\{1, ..., T\\}$ denotes the task type of the k-th client. T is the total number of tasks and T < K.\n\nIn the FedMIT task, all clients obtain the pre-trained MLLM from the server. Generally, MLLMs contain a visual encoder f, a connector $\u03c8$, and LLM L. Specifically, for a given input image $X_u^k$, the visual encoder extracts visual features $H_u^k = f(X_u^k)$. The connector is used to align the visual encoder with the LLM. Connector transforms $H_u^k$ into a language embedding tokens $x_{img}^k \\in R^{N\u00d7C}$, effectively facilitating the integration of multimodal information within the LLM framework, where N is the number of tokens and C is the hidden size.\n\n$x_{img} = \u03c8(H_u^k)$, with $H_u^k = f(X_u^k)$.   (1)\n\nFinally, we input $x_{img}$ and $x_{ins}$ into the LLM to generate response."}, {"title": "Federated Multimodal Instruction Tuning Framework", "content": "FedMIT task has greater heterogeneity between tasks, and traditional FedIT methods cannot be used directly to solve the problem. As shown in Figure 2, we propose the Federated Multimodal Instruction Tuning framework (Pilot) to address the task heterogeneity between clients. In our framework, we integrate a two-stage \"adapter on adapter\" method into the connector of the visual encoder and LLM. In stage 1, we extract task-specific features and client-specific features from visual information. Through federated aggregation, in stage 2, we build a CT-MOA module to perform cross-task interaction. We hope that each client not only needs to capture personalized information from local data and learn task-related multimodal information, but also needs to be able to learn general knowledge from other tasks to improve model performance and cross-task capabilities.\n\nStage 1: Task-specific Feature Mining. At stage 1, we hope to extract client-specific and task-specific features from the client's visual information. We propose task-specific adapter $\u03c8_t$ to extract task-specific visual features that is only important for one task, and client-specific adapter $\u03c8_s$ to extract specific visual features of the client's unique data distribution. We define these two adapters as two-layer of perceptrons. Finally, the image tokens represent: $x_{img} = x_t + x_s$, where $x^t = \u03c8_t(H_u^k) \u2208 R^{N\u00d7C}$, $x^s = \u03c8_s(H_u^k) \u2208 R^{N\u00d7C}$.\n\nTo encourage the client-specific adapter to produce features that are more refined than the task-specific adapter, a difference loss is used to ensure the orthogonality of their output features. Inspired by the domain separation network (Bousmalis et al. 2016), we adopt a soft subspace orthogonality constraint:\n\n$L_d = ||x^t^T x^s ||_F, $  (3)\n\nwhere $|| ||_F$ is Frobenius norm. Hence, the Stage 1 total loss is\n\n$L = L_{ce} + \u03bb_0 L_d, $ (4)\n\nwhere $L_{ce}$ represents the language modeling loss, which computes the cross-entropy of next-token predictions. $\u03bb_0$ denote coefficients for difference loss. After stage 1, each client sends the LLM training parameters text-adapter $\u0398_t$ and task-specific adapter $\u03c8_a$ parameters to the server.\n\nStage 2: Cross-task Visual Interaction. At stage 2, the client obtains the updated task-specific adapters for T tasks from the server. In other words, each client obtains (T-1) task-specific adapters of other tasks in addition to its local task-specific adapter. To learn general knowledge from other tasks to improve model performance and cross-task capabilities, we integrate the local task-specific adapter with other task-specific adapters to construct a Cross-task Mixture-of-Adapters (CT-MoA) module, as shown in Figure 3. In the CT-MoA module, we assume that the local task-specific adapter index is $\u03c8_1$ ($\u03c8_a$), and there are T adapters in total. However, directly loading task-specific adapters of distinct tasks may cause the model of the current task to be unable to fully utilize these adapters due to task heterogeneity. Therefore, we added cross-task adapters to the task-specific adapters of other tasks in CT-MOA. For task-specific adapters $\u03c8_i^k$, i = 2, ...,T, we add cross-task adapter $\u03c8_c^k$ on it to alleviate the discrepancy between other task-specific adapters and the local task-specific adapter due to task heterogeneity. Cross-task adapter aims to extract cross-task collaboration knowledge. It has the same structure as the task-specific adapter and is initialized by the local task-specific adapter $\u03c8_a^k$ parameters. In addition, the CT-MOA module includes a router for predicting the probability of selecting and activating each adapter from T total task-specific adapters. The router network($\u03c6$) has a linear layer for calculating the normalized weight matrix using $H_u^k$ for voting, and producting P:\n\n$P = Softmax(\u03c6(H_u^k)) \u2208 R^T.$(5)\n\nFinally, the output of CT-MOA module is expressed as:\n\n$x\u2217 = P[1](H_u^k) + \u2211_{i=2}^T P[i] (\u03c8_i^k (H_u^k) + \u03c8_c^k(H_u^k)).$   (6)\n\nFollowing (Zoph et al. 2022), we adopt an auxiliary losses based on the language modeling cross-entropy loss to maintain a load balance between task-specific adapters in the Cross-task MoA module. The auxiliary losses comprise load balancing loss and router z-loss. The load balancing loss is defined as:\n\n$L_b = \\frac{T}{N}\u2211_{j=1}^N D_i - \\frac{1}{T}$,\n(7)\n\nwhere $D_i = \\frac{1}{N}\u2211_{j=1}^N 1_{argmax \u03c6(H_u^k[j]) = i}$ represents the proportion of tokens distributed to adapter i, $1{\u00b7}$"}, {"title": "Federated Optimization", "content": "The proposed method is collaboratively optimized via local two stages instruction tuning and global aggregation. We train the Pilot with two alternate steps of local update and global aggregation for R rounds. In each round, the client receives the global model and updates the local model E epochs on the local data. In the first round, the server sends the base MLLM model to each client. Each client performs local instruction tuning of stage 1, then send the task-specific adapter and text-adapter parameters to the server. After global aggregation, the server sends T task-specific adapters and text-adapter parameters back to the client. Each client updates the model architecture and performs local instruction tuning of stage 2, and finally send the local task-specific adapter and text-adapter parameters to the server.\n\nTask-aware Visual-adapter Aggregation. After the server collects the task-specific adapter parameters of all clients, we adopt a task-aware aggregation method.\n\nTask-specific adapter mainly learns task-specific visual features and has the general understanding ability of this task. Therefore, the parameters learned in one client can also be shared by clients of the same task. We use weighted average to aggregate the task-specific adapter parameters $\u0398_a$ of the same task:\n\n$\u0398_a^t = \u2211_{k\u2208K_t} \\frac{n_k}{\u2211_{k'\u2208K_t} n_{k'}} \u0398_a^k,  t = 1, ..., T,  $(10)\n\nwhere $K_t$ is a set of clients with task type t. Finally, we obtain T number of task-specific adapters and send them to each client.\n\nAdaptive Text-adapter Aggregation. For all collected text-adapter parameters, compared with the fully weighted aggregation method used in the traditional FedIT method, we expect that the text-adapter parameters of each client can benefit from the positive impact to the greatest extent during the aggregation process, while effectively reducing the negative impact. Therefore, we propose an adaptive text-adapter aggregation method. We first calculate the Euclidean distance between each text-adapter parameter and the other (K-1) text-adapter parameters. Then select the nearest M points for weighted aggregation based on the distance. For example, text-adapter parameters $\u0398_{t,\u03ba}$ for client k:\n\n$d_{k,i} = E(\u0398_{t,\u03ba}, \u0398_{t,i}), for i \u2208 \\{1,2,...,K\\}\\{k\\}$,(11)\n\nwhere $E()$ is the Euclidean distance formula. We obtain the distance $D_k \u2208 R^{K-1}$ between the $\u0398_{t,\u03ba}$ and other text-adapter parameters, and then we select the Top-M closest parameters for weighted aggregation:\n\n$\u0398_{t,\u03ba} = \\frac{\u03b7_\u03ba}{\u03b7_\u03ba + \u2211_{k'\u2208K_m} \u03b7_{\u03ba'}} \u0398_{t,\u03ba} + \u2211_{k'\u2208K_m} \\frac{\u03b7_{\u03ba'}\u22c5w_{\u03ba'}}{\u03b7_\u03ba + \u2211_{k'\u2208K_m} \u03b7_{\u03ba'}} \u0398_{t,\u03ba'},$  (12)\n\nwhere $w_{\u03ba'} = \\frac{1/d_{\u03ba,\u03ba'}}{\u2211_{k'\u2208K_m} 1/d_{\u03ba,\u03ba'}}$, $K_m$ is the set of M clients closest to parameter $\u0398_{t,\u03ba}$."}, {"title": "Experiment", "content": "Experimental Setups\nNow, we introduce how to construct the federated multimodal instruction tuning task scenario. To ensure the diversity of instruction tuning datasets, we collect various publicly available and commonly used visual-language datasets. These instruction tuning datasets cover a wide range of tasks, including knowledge-based image question answering, image question answering reading comprehension, and optical character recognition VQA. The selected datasets include ScienceQA (Lu et al. 2022), GQA (Hudson 2019), and OCRVQA (Mishra et al. 2019). However, we observe that these datasets are limited to traditional QA tasks in visual-language tasks. Therefore, to enrich the diversity of tasks, we introduce the image caption dataset COCO (Lin et al. 2014) for image description task and the grounding dataset RefCOCO (Kazemzadeh et al. 2014) for visual localization task. For all the above datasets, we construct two different federated instruction tuning scenarios. FL-oriented visual understanding scenario: We use the GQA, COCO, and RefCOCO datasets. We randomly divide each dataset into 3 subsets, and each subset is regarded as a client. Then, we obtain a FedMIT task scenario with 9 clients and 3 different visual understanding tasks(9-client, 3-task). FL-oriented general VQA scenario: We use the ScienceQA, GQA, and OCRVQA datasets. Similar to the above operations, we obtain a FedMIT task scenario with 9 clients and 3 different VQA tasks(9-client, 3-task). For more data information, please refer to the supplementary materials.\n\nBaselines\nWe compare our framework Pilot with 5 state-of-the-art FL algorithms: FedAVG (2017), FedProx (2020), FedAdam (2020), Shepherd (2024b), and FedDPA (2024c). FedAvg takes the weighted average of all training parameters as a standard optimization method. FedProx focuses on local model correction, and FedAdam focuses on introducing momentum on the server side to stabilize global model updates. Shepherd and FedDPA are FedIT task method. We also show local training and centralized training as references, where local training is trained by using one client's"}, {"title": "Experimental Results", "content": "dataset without collaboration. Centralized training is training all datasets centrally.\n\nFor the VQA task (including ScienceQA, GQA, and OCRVQA), we calculate the accuracy of predicting answers against ground truth. For the caption task, we report the CIDEr score. For the grounding task, we employ Intersection-over-Union (IoU) as the evaluation metric. Specifically, a prediction is deemed accurate only when its IoU exceeds or equals 0.5. Table 1 and Table 2 show the comparison of Pilot with other five methods in the federated scenarios of visual understanding and General VQA. Our framework outperforms all baselines in two task scenarios. We found that the performance of FedAVG method is lower than local training. This shows that the heterogeneity of multimodal tasks leads to greater parameter conflicts, and simple aggregation has a negative impact on local models. At the same time, we observed that FedAvg method is better than Shepherd. The former aggregates all training parameters, while the latter only aggregates LLM training parameters. This result also indirectly reflects the importance of visual information to the FedMIT task. Compared with FedDPA, only improving the LLM training parameters does not achieve the desired effect, which also illustrates the need to consider the differences between different tasks and the necessity of learning general knowledge from other tasks. Finally, the results prove that our method can not only overcome the heterogeneity between tasks, but also collaborate with all clients to improve the performance of local models."}, {"title": "Ablation Studies", "content": "ing the above methods, the performance of our method is lower than that of local training, which does not have the ability to overcome task heterogeneity. Then we add adaptive text-adapter aggregation, and the performance of our method is improved and outperforms local training, which demonstrates that this module can effectively alleviate the impact of task heterogeneity. When we add the cross-task adapter, we observe that the model performance improves on all clients. Through auxiliary loss optimization, the performance of our framework can be further improved. The results show that the CT-MOA module is able to learn general knowledge from other tasks to improve model performance and cross-task capabilities. Removing the difference loss, our connector has only one and no longer distinguishes between client-specific adapter and task-specific adapter. The performance of the model has decreased, indicating that it is necessary to maintain the personalized information of the client. The above results demonstrate the importance of each component in our method."}, {"title": "Further Remarks", "content": "Building Cross-task CLIP. Our method modifies the MLLM connector to learn general knowledge from other tasks to improve model performance and cross-task capabilities. But the natural thought is: why not use CLIP? To answer this question, we unfreeze CLIP and train all MLP layers in CLIP in the same way as the connector, called CT-CLIP. We conduct experiments on the visual understanding scenario for federated learning. Experimental results show that for the FedMIT task, learning cross-task visual information from different tasks is an effective solution. We observe that although CT-CLIP outperforms the Pilot, it comes at the expense of additional training parameters and communication parameters. Compared with Pilot, the communication parameters sent to the server increase by 0.2B, and the local client activation parameters increase by 1.25B. With the increase of tasks, the computational cost is unacceptable. Therefore, we give priority to the more simple and efficient method."}, {"title": "Different Text-adapter Parameter Aggregation Strategies", "content": "In our framework, we adopt the adaptive text-adapter aggregation method. For all text adapter parameters, we adopt a weighted optimization aggregation strategy based on euclidean distance and select the Top-M parameters for weighted averaging. We compared different aggregation strategies, such as aggregating only parameters of clients with the same task (same task client), aggregating parameters of all clients (all clients), and different Top-M selections. As shown in Table 5, due to the heterogeneity between tasks, aggregating on all client will lead to parameters conflicts. In addition, aggregating only on the same task is a suboptimal solution because it does not utilize the semantic knowledge of other tasks. At the same time, we tested the impact of different top-M on model performance, and the results showed that the adopted adaptive aggregation method can benefit from positive influences while reducing negative interference."}, {"title": "Cross-task Adapter Initialization Strategy", "content": "In our framework, the role of the cross-task adapter is to extract cross-task knowledge. We compared initialization with task-specific adapter parameters with training from scratch (random). As shown in Table 6, training from scratch leads to performance degradation for all tasks. Using task-specific adapter parameters initialization provides a good starting point for the module and can help the local client better extract cross-task knowledge."}, {"title": "Conclusion", "content": "In this paper, we propose a federated multimodal instruction tuning framework to solve the new task of federated multimodal instruction tuning by collaboratively utilizing distributed data from different local clients to learn cross-task knowledge without being affected by task heterogeneity during instruction tuning. Through two stages \"adapter on adapter\" strategy, our model can capture the personalized information of local data and the task-related multimodal information, and can also adapt to the differences between different tasks. Our method achieves state-of-the-art results in two cross-task scenarios."}]}