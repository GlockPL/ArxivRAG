{"title": "A Brief Summary of Explanatory Virtues", "authors": ["Ingrid Zukerman"], "abstract": "In this report, I provide a brief summary of the literature in philosophy, psychology and cognitive science about Explanatory Virtues, and link these concepts to explainable AI.", "sections": [{"title": "1 Introduction", "content": "Explanatory Virtues (EVs) have been mainly discussed in the context of abductive reasoning (inference to the best explanation), where the explanation is a theory that accounts for observations. Over the years, practitioners in philosophy, psychology and cognitive science have studied various EVs, and coined many terms for them. In this report, I largely follow the taxonomy in (Keas, 2018), which comprises four main types of EVs: evidential, coherential, aesthetic and diachronic (Section 3). I have included additional EVs to those considered by Keas (2018), and added one category, viz coverage, that pertains to the wider applicability of a theory, and accounts for EVs that don't fit well into Keas's taxonomy.\nIt is worth noting that in the context of eXplainable AI (XAI), for global explanations, which explain an entire model, e.g., (Bastani et al., 2017; Lakkaraju et al., 2017), the \u201cobservations\" are the way a Machine Learning (ML) model works and its parameters, and the \u201ctheories\u201d are explanations that account for these observations. For local explanations, which pertain to the outcome predicted by an ML model for a specific instance, e.g., (Ribeiro et al., 2016; Biran and McKeown, 2017), the observations also include the feature values of the instance in question and the prediction made by the model for this instance.\nThe main EV dimensions are described in Section 2, followed by EVs categories according to our expanded taxonomy, in Section 3. In Section 4, I discuss how EVs apply to XAI."}, {"title": "2 Main EV dimensions", "content": "There is a large body of literature about EVs in philosophy, psychology and cognitive science. Here, I provide categorizations of these EVs along two dimensions: epistemic/pragmatic (van Fraassen, 1980) and ontological (Keas, 2018).\nEpistemic/pragmatic dimension. van Fraassen (1980) distinguishes between two types of EVs according to their roles: epistemic virtues, which mean that explanations (theories) are valuable in themselves as a means to understanding the world, and pragmatic virtues, which are concerned with the use and usefulness of a theory, and provide reasons to prefer a theory independently of questions of truth."}, {"title": "3 EVs in the ontological dimension", "content": "Here, I present definitions of EVs from the literature, and incorporate them in the categorization of Keas (2018). \nEvidential virtues. These virtues indicate different facets of how well a theory accounts for observations in the world.\n\u2022 Evidential accuracy (Kuhn, 1977; van Fraassen, 1980; Thagard, 1989; Lipton, 1991; Rosales and Morton, 2021) - A theory T fits the empirical evidence well (regardless of causal claims).\n\u2022 Causal adequacy (Thagard, 1989; Lipton, 1991; Psillos, 2002; Mackonis, 2013; Rosales and Morton, 2021) - A theory T's causal factors plausibly produce the effects (evidence) in need of explanation.\n\u2022 Refutability (Quine and Ullian, 1978; van Cleave, 2016) \u2013 Some imaginable event must suffice to refute the hypothesis/theory T.\n\u2022 Explanatory depth (Hitchcock and Woodward, 2003) \u2013 A theory T excels in causal history depth (how far back a causal chain goes) or in other depth measures, such as the range of counterfactual questions that theory T answers regarding the item being explained.\n\u2022 Optimality (McMullin, 2014) \u2013 Whether a theory T affords the best explanation available (inference to the best explanation).\nCoherential virtues. These virtues describe how well the components of a theory agree with each other and with other warranted beliefs.\n\u2022 Internal coherence (McMullin, 2014) \u2013 a theory T lacks ad hoc hypotheses \u2013 theoretical components merely tacked on to solve isolated problems.\n\u2022 Internal consistency (Kuhn, 1977; van Fraassen, 1980; Newton-Smith, 1981; Thagard, 1989; McMullin, 2014) \u2013 A theory T's components should not be contradictory and should be coordinated into an intuitively plausible whole.\n\u2022 Universal consistency (Hempel and Oppenheim, 1948; Ausubel, 1962; Kuhn, 1977; Quine and Ullian, 1978; van Fraassen, 1980; Newton-Smith, 1981; Psillos, 2002; Mackonis, 2013; McMullin, 2014) \u2013 A theory T sits well with (or is not obviously contrary to) other warranted beliefs."}, {"title": "4 EVs in XAI", "content": "Here, we focus on local explanations of ML models, and discuss the EVs that are applicable to explanations as theories, where the observations are ML models applied to a particular instance.\nEvidential virtues.\n\u2022 Evidential accuracy \u2013 The explanation must justify how the ML model reaches the predicted outcome with respect to the instance. For example, for regression and logistic regression, this means mentioning background information and important features; and for decision trees, this means mentioning background information, every feature in the path to a prediction and its effect on the probability of a class.\n\u2022 Causal adequacy \u2013 This EV is not relevant according to our conceptualization, as there is no causal effect between theory (explanation) and observation (our explanations don't cause the ML models or their outcomes).\n\u2022 Refutability \u2013 Since a local explanation pertains to the outcome of a particular model, the notion of an imaginable instance that refutes this explanation is not relevant. More generally, there could be ML models and outcomes for particular instances that refute the mechanism employed to generate explanations.\n\u2022 Explanatory depth \u2013 As for causal accuracy, the causal aspect of this EV is not relevant to XAI, but the ability of an explanation to handle counterfactuals and hypotheticals is relevant.\n\u2022 Optimality \u2013 The optimality of an explanation may be determined by a multi-attribute function that calculates the extent to which the explanation satisfies certain criteria, such as the Gricean maxims of cooperative conversation (Grice, 1975) the importance of each criterion may be empirically or subjectively determined.\nCoherential virtues.\n\u2022 Internal coherence \u2013 Explanations should not posit ad hoc facts that were added just to fit a particular outcome.\n\u2022 Internal consistency (Gricean maxim of Manner \u2013 clarity and order (Grice, 1975)) \u2013 Explanations must not have contradictory components, and must be organized into an intuitively plausible whole. It is worth noting that an explanation may have a contradictory component that points out how the outcome of an ML model seems incorrect or inconsistent with some features of the instance at hand (Consilience). Such an explanation is still internally consistent.\n\u2022 Universal consistency (Gricean maxim of Quality (Grice, 1975)) \u2013 Explanations must be Universally consistent in the sense that they must fit (or not be contrary to) warranted beliefs. For regression, logistic regression and decision trees, the warranted beliefs are shared knowledge in the form of background information (e.g., probabilities obtained from data) or generally accepted beliefs (e.g., being obese increases the chance of a heart attack, or high maintenance cost of a car is bad). As for internal consistency, an explanation that describes how the outcome of an ML model contradicts accepted knowledge is still universally coherent.\n\u2022 Analogy \u2013 Analogies and similes are good explanatory tools, if they are sound and intuitive, However, the automatic generation of analogies remains an open research problem (He et al., 2024).\n\u2022 Compatibility with metaphysical beliefs \u2013 Like refutability, this EV pertains to the approach for generating explanations, which must be in line with established practices, rather than to a particular explanation about the outcome of an ML model when applied to an instance.\n\u2022 Conservativeness \u2013 As mention above, a conservative explanation reduces the surprisingness (increases the plausibility) of an outcome. For regression, logistic regression and decision trees, this would involve explaining why a feature and/or an outcome that appear compelling are not predicted, e.g., the feature in question has no effect or other features carry more weight.\n\u2022 Depth (Gricean maxim of Quantity (Grice, 1975)) \u2013 Explanations should not include more information than necessary.\nCoverage virtues. The coverage of an explanation of an ML model measures whether the explanation describes the extent of the applicability of the model, e.g., to other instances and/or types of instances.\n\u2022 Completeness \u2013 An explanation E1 is more Complete than an explanation E2 if it explains more observations, i.e., the application of an ML model to more instances.\n\u2022 Scope - An explanation E1 has more Scope than an explanation E2 if it mentions additional types of situations or domains to which an ML model is applicable.\n\u2022 Consilience \u2013 An explanation E1 is more Consilient than an explanation E2 if it explains more observations, in more than one domain, and specifies what it cannot explain.\n\u2022 Unification \u2013 An explanation E1 is more Unifying than an explanation E2 if it relies on the same amount of theoretical content to explain more observations. For ML models, \u201cthe same amount on theoretical content\" can be taken to mean \u201cthe same feature values\". For instance, the following Unifying Explanation for a logistic regression model that predicts the acceptability of a car adds an applicability component (in italics) to an initial feature attribution component: \"this car is acceptable because it has four doors and a large boot. Indeed, 70 of 80 cars with four doors and a large boot are deemed acceptable by the model\u201d (Maruf et al., 2024).\n\u2022 Importance \u2013 This EV requires that the phenomena explained by an explanation E1 (applications of an ML model to an instance to yield an outcome) are more salient than those explained by an explanation E2.\nMaruf et al. (2024) showed that Unifying explanations for a logistic regression model are deemed largely equivalent to Conservative explanations in terms of how much an explanation is liked, and four explanatory attributes (Hoffman et al., 2018): completeness, helpfulness for understanding an AI, absence of extraneous information, and enticement to act on a prediction. However, Unifying explanations (and other forms of coverage) have been studied only by Bu\u00e7inca et al. (2021) and Maruf et al. (2024), while Conservative explanations have strong support in the XAI literature (Biran and McKeown, 2017; Guidotti et al., 2019; Maruf et al., 2023; Miller, 2019; Sokol and Flach, 2020; Stepin et al., 2020; van der Waa et al., 2018). The finding in (Maruf et al., 2024) indicates that explanations embodying coverage virtues warrant further investigation.\nAesthetic virtues.\n\u2022 Simplicity - An explanation E1 is Simpler than an explanation E2 if it has fewer requirements.\n\u2022 Modesty \u2013 An explanation E1 is more Modest than an explanation E2 if the requirements of El are a subset of the requirements of E2,\nDiachronic virtues. These virtues pertain to how theories fare over time, and are not applicable to explanations of ML models. As above, they may apply to the procedures employed to generate explanations."}]}