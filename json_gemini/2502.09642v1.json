{"title": "Krutrim LLM: Multilingual Foundational Model for over a Billion People", "authors": ["Aditya Kallappa", "Palash Kamble", "Abhinav Ravi", "Akshat Patidar", "Vinayak Dhruv", "Deepak Kumar", "Raghav Awasthi", "Arveti Manjunath", "Shubham Agarwal", "Kumar Ashish", "Gautam Bhargava", "Chandra Khatri"], "abstract": "India is one of the most vibrant and culturally diverse societies. Developing a general-purpose artificial intelligence system tailored for the Indian market presents unique challenges. These include accounting for the nation's cultural nuances, accommodating its linguistic diversity with numerous regional languages, adapting to the prominence of oral traditions, ensuring accessibility to relevant data sets, and achieving scalability to serve the vast population effectively. Careful consideration and innovative approaches are necessary to navigate these complexities successfully. Existing foundation models for natural language tasks are predominantly trained on English data, limiting their effectiveness for languages native to India's over 1 billion citizens. Thousands of regional languages, dialects, language or code mixing pose representation challenges, exacerbated by sparse training data; Indic languages comprise just 1% of Common Crawl corpora despite India representing 18% of the global population. Consequently, lack of Indic language relevance and context representation leads current models to exhibit cultural and linguistic biases oriented towards Western contexts. We present Krutrim Large Language Model (LLM), a 2 trillion token multilingual foundation model designed to serve Indian demographic needs through equitable representation of the country's array of native tongues. Training data incorporates the largest known Indic language dataset, mitigating associated data scarcity obstacles that encumber model parity across dialects. Evaluations demonstrate Krutrim's strong performance on Indic language benchmarks, surpassing or at par with state-of-the-art models despite being significantly smaller in training flops. Krutrim LLM also matches or exceeds standards set on English benchmarks by models trained on comparable flops (e.g. vs LLAMA-2 on 10 out of 16 tasks with average score of 0.57 vs 0.55 of LLAMA- 2), evidencing flexible multilingual fluency. We further integrated search to deliver real-time and more factually relevant information via Krutrim LLM conversational app, working to make next-generation AI widely accessible for a diverse set of over 1 billion worldwide users. Through intentional design choices that redress endemic data imbalances, Krutrim LLM signifies meaningful progress in the pursuit of ethical, globally representative AI foundation models.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Challenges in Developing an India-Centric Large Language Model (LLM)", "content": "Building Artificial General Intelligence (AGI) and AI models to cater to the diverse cultural context of India presents multifaceted challenges. The country's linguistic diversity[43], with hundreds of languages and dialects, poses a significant hurdle [30]. These languages span 4 major language families with Indo-Aryan and Dravidian being the most prominent ones [23]. Many of these languages thrive in oral traditions, leading to frequent mixing and evolving linguistic patterns that are not well-documented digitally. This oral culture complicates the collection and digitization of data necessary for training robust AI systems. Additionally, India's rich tapestry of social and economic backgrounds adds another layer of complexity. The vast differences in socio-economic statuses influence digital access and technology usage, making it challenging to develop AI solutions that are inclusive and equitable. Furthermore, cultural nuances, which vary widely across regions, need to be understood and respected in AI applications to ensure they are relevant and sensitive to users' needs. The combination of linguistic diversity, cultural richness, and socio-economic disparities requires innovative approaches to data collection, model training, and the development of algorithms that can adapt to and reflect India's multifaceted society.\nMost recent AI models, like, LLaMA [41], GPT-3.5, and others, face significant limitations when representing the Indian ethos and languages due to the unique linguistic and cultural challenges prevalent in the region [11]. In fact, the structure of some Indian languages, such as Sanskrit, which allows for the creation of virtually infinite compound words, presents a stark contrast to English's finite vocabulary. These languages are not only syntactically complex but are also semantically rich, making them difficult to model with the tokenizers designed primarily for Western languages. Such systems often result in inefficient processing of Indian languages, leading to excessively long sequences that can hamper the effectiveness of AI models.\nAdditionally, the reliance on digitized data for training these models poses a challenge, as there is a significant lack of local, undigitized data reflecting the vast cultural and linguistic diversity of India. This scarcity of data is compounded by the fact that Indian languages are under-represented in major digital corpora. For instance, Indian languages account for a mere 1% of the Common Crawl2 dataset [12, 31], despite India constituting 18% of the global population. This under-representation leads to biased models that do not adequately capture the nuances of Indian languages and culture. The poor representation of Indic data not only affects the accuracy and relevance of these AI models for Indian users but also highlights a broader issue of inclusivity and representation in the development of global AI models."}, {"title": "1.2 Contributions", "content": "The development of Krutrim LLM, India's first and premier Large Language Model (LLM), represents a monumental stride towards creating AI technologies that are deeply aligned with the country's linguistic and cultural diversity. The project embarked on an ambitious journey by curating an extensive range of Indic data from across the web, encompassing the myriad languages spoken throughout India. This foundational work enabled the training of the LLM on over 2 trillion tokens, a scale unprecedented in the context of Indian language processing. Recognizing the unique challenges of Indian languages, the team developed a specialized Indic tokenizer, tailored to efficiently process the complex morphologies and syntaxes inherent to these languages.\nTo enhance the model's performance further, Krutrim LLM incorporates state-of-the-art attention mechanisms such as Grouped Query Attention (GQA) [6] and AliBi [35], which significantly improve its ability to handle longer contexts and provide faster responses, thereby elevating the user experience. This advanced technical framework allows Krutrim LLM to outperform several open-source models of comparable size or computational requirements, particularly in its handling of Indic languages, without any compromise.\nFurthermore, the model underwent India-centric fine-tuning, covering a broad spectrum of topics and tasks relevant to the Indian context. This fine-tuning ensured that the model could effectively engage with and address the specific needs and nuances of Indian users. A linguistically and socially"}, {"title": "2 Related Work and Background", "content": "There have been rapid advancements in the field of AI since the Transformers [42] came out. It has led to notable increase in endeavors dedicated to constructing versatile conversational AI systems adept at comprehending and engaging with users across multiple topics and open ended conversations.\nGeneral purpose LLMs are fundamentally changing how software and applications are being built which is further dramatically changing user experiences. Some of the most popular LLMs which have lead to this change are GPT-3.5 [1, 11], LLaMA [41], Gemini [40], Anthropic Claude [7], Mistral [21], Inflection [3], and Grok-1 [2].\nBrown et al.[11] demonstrated the feasibility of generating coherent and contextually relevant responses in various languages, including Indic languages, using large language models. Building upon the success of GPTs and Claude [7, 8, 22], subsequent research endeavors have further propelled the development of conversational AI in the Indic language domain. One notable participant in this progression is Gemini/Bard project [40]. It has significantly advanced the state-of-the-art in conversational AI by leveraging large-scale pre-training techniques. Gemini/Bard's approach has led to substantial improvements in language understanding and generation, particularly in low-resource languages such as Hindi, Bengali and Tamil. Mistral [21] addressed low performance in European languages by collecting and prioritizing European languages along with English.\nAs most of these LLMs are trained on English or high resource languages, there are several limitations in adapting them for regional or local use cases, both at knowledge level (pre-training) and inference level (high token to word ratio). Performance of these state-of-the-art models on Indic languages is far from the performance on English and European languages and therefore remained an open problem. Furthermore, most of these LLMs are biased towards non-Indic regions due to significantly lower volume of Indic data and therefore do not work for countries like India that have a rich and diverse landscape, culture and languages.\nSeveral Indian-origin fine-tuned versions of LLMs have emerged, presenting a promising array of open-source models such as OpenHathi tuned Airavata [17], Gemma based Navarsa [27], Kannada LLaMA, Tamil LLaMA [9], Odia LLaMA [26], and other vernacular models. These models are constructed upon the open source Llama2 architecture, leveraging cutting-edge techniques like Parameter Efficient Fine-tuning (PEFT) [19, 20]. However, a predominant characteristic/limitations of these models is their focus on monolingual or bilingual generation capabilities and lack of Indic knowledge and sentence construction.\nFurthermore, initiatives such as Perplexity AI [5], BingChat [10] and You.com [45] have also made noteworthy strides in advancing conversational AI by providing concise answers with source links for verification, making it noteworthy for more factual research and getting factual information by having robust Web search with Retrieval Augmented Generation (WebRAG) pipelines. These efforts emphasises the importance of AI assistants being more personalized, truthful, accurate, and transparent.\nIn the landscape of natural language processing models, particularly in the realm of Indian languages, there exists a noticeable gap in understanding the nuances of the Indian context, including local vernacular dialects, socio-economic structures, and cultural intricacies. Many existing models lack the depth required to effectively cater to these specific requirements. Thus, the necessity arises to develop indigenous systems tailored to the needs of Indian users.\nIn light of these advancements, the Krutrim LLM endeavors to enhance both accuracy and speed, positioning itself as a versatile and efficient general-purpose chat assistant. By synthesizing insights from existing Indian-origin LLMs and incorporating innovative methodologies, Krutrim aims to"}, {"title": "3 Data Collection and Tokenization", "content": "Krutrim LLM is trained on a comprehensive dataset encompassing a diverse range of sources such as open web and proprietary sources. This dataset includes more than 2 trillion high-quality tokens meticulously crafted with a unique blend of data encompassing various languages, tasks, and sources. Notably, the dataset incorporates hundreds of billions of carefully curated Indic tokens, establishing Krutrim LLM as the largest known distribution of Indic data to date. We have trained new tokenizer based on sentencepiece\u00b3 byte pair encoding from scratch to represent EN and Indic languages."}, {"title": "3.1 Data collection and cleaning", "content": "For this study, a massive corpus comprising 2 trillion tokens was gathered through web scraping. The dataset, characterized by its unlabeled and unstructured nature, underwent essential cleaning processes, including de-duplication, removal of extra short passages and low quality text. In concurrent to our work, IndicLLMSuite [25] depicts the techniques applied to collect high-quality Indic LLM training data. We made particular focus on Indic data sources like NDL, requiring additional cleaning and therefore more work. To achieve this, we followed similar techniques across diverse sources such as OpenWeb (open source subset of data from web archive) and leveraged open-source cleaned datasets like the RedPajama dataset subset, Books data, PubMed, Wiki, and StackFast. We applied techniques similar to Dolma [38] and further enhanced the cleaning pipeline to address Indic languages."}, {"title": "3.2 Data quality", "content": "The data quality has momentous impact on the model's language learning and generation capability. Training with shorter prompts might result in generation of short responses and might also lead to non learning of a particular language in a multi lingual learning setting (e.g., observed in Hindi data). We also saw that providing large contexts only for a given language might not lead to learning of that languages altogether (e.g., observed in Gujarati data). The best setting for teaching a language is to show the model both smaller sentences and set of larger sentences.\nAchieving a balanced distribution of pre-training data among different languages proves crucial for optimal language learning outcomes. We maintain substantial representation of each of the Indic languages."}, {"title": "3.3 Tokenizer", "content": "Existing open source tokenizers do not perform well on Indic languages leading to high token to word ratio. A sub-optimal tokenizer leads to inferior training and inference performance in terms of speed and accuracy. In order to address that, we trained the tokenizer from scratch optimized for English and Indic languages."}, {"title": "4 Model and Architecture", "content": "The Krutrim model architecture draws from the standard decoder only transformer framework [42]. We train 7 billion parameter model on context length of 4096 tokens. We use ALiBi positional encoding method [34] which helps in expanding the context length. We also leverage GQA [6] for faster inference and lower KV cache memory footprint. We use clipping of QKV matrix values for stable training. The standard ReLU activation function is used."}, {"title": "5 Training", "content": ""}, {"title": "5.1 Pre-training", "content": "We pre-trained Krutrim LLM on a dataset of 2 Trillion tokens. The initial phase of unsupervised pre- training involves exposing the language model to vast datasets, allowing it to learn world knowledge and language capabilities through next word prediction. This critical stage sets the foundation for subsequent fine-tuning and specialized tasks. In this stage of training, the model learns about world knowledge and language capabilities by the means of next word prediction."}, {"title": "5.1.1 Pre-training", "content": "We trained Krutrim on H100 GPUs resulting in 1023 FLOPS. To select the optimal model checkpoint post pre-training (PT), a systematic analysis of checkpoints at intervals of 20k checkpoints is conducted."}, {"title": "5.2 Continual Pre-training (CPT)", "content": "Continual Pre-training (CPT) is imperative in the realm of natural language processing, serving as a foundational element for adapting to multiple domains and acquiring diverse knowledge incrementally. This section delves into the necessity of CPT across various domains, highlighting its role in adapting language models to evolving requirements viz."}, {"title": "5.3 Instruction Tuning", "content": "After the initial pre-training (PT) phase and in our case post CPT, the base language model acquired language generation capabilities and accumulates knowledge across Indic languages. However, it faces limitations in following instructions and engaging in conversations. The subsequent steps involve either supervised-Fine-Tuning (SFT) or Instruction-Fine-Tuning (IFT), each presenting a trade-off between knowledge retention and creativity learning. The challenge lies in finding the right balance, as infinite training on extensive datasets may result in forgetting of pre-training knowledge. Specifically, SFT introduces instruction-following capabilities but risks forgetting pre- training knowledge [44]."}, {"title": "5.3.1 Knowledge Forgetting vs. Creativity Learning Trade-off", "content": "The decision between SFT and IFT necessitates navigating the trade-off between knowledge retention and creativity learning. It is acknowledged that prolonged training on massive datasets may lead to a compromise in pre-training knowledge, necessitating strategic fine-tuning approaches."}, {"title": "5.3.2 Instruction Tuning for Diverse Tasks", "content": "We explored instruction tuning across various tasks to enhance the model's adaptability and perfor- mance. Tasks included:\n\u2022 Translation (In-En and En-In): Teaching bidirectional translation capabilities.\n\u2022 Summarization: Enhancing the model's ability to generate concise summaries.\n\u2022 Chain of Thoughts (COT) Reasoning: Enhancing the model's sequential reasoning capabili- ties.\n\u2022 Single and Multi-turn Dialogues and Conversations: Teaching conversational engagement.\n\u2022 Safety around Sensitive Topics: Ensuring appropriate responses on sensitive subjects.\n\u2022 General Knowledge: Verifying the model's grasp of broad factual information.\n\u2022 Coding and Programming: Teaching coding-related tasks and instructions.\n\u2022 Chat Bot Self-Identification: Teaching the model about self-identity in a chat bot context."}, {"title": "5.3.3 Task-Specific Personas", "content": "Tasks were designed with diverse user personas in mind, including students, technical and non- technical workers, and creative content creators. The aim was to tailor the language model's performance to suit the varied needs and expectations of different user groups pertaining to India.\nThese comprehensive fine-tuning tasks contribute to shaping a language model that not only possesses general language capabilities but also excels in task-specific instructions, accommodating a range of user personas and real-world scenarios. The ongoing challenge lies in optimizing the trade-off between knowledge retention and task-specific learning during fine-tuning phases."}, {"title": "5.4 SFT for Answering Factual Questions", "content": "For factual questions, we observed that the base Instruction-tuned model hallucinated around 33% of times giving false facts to the user. We also observed that for around 14% of times the model hallucinated and fell prey to confirmation bias for adversarial and factual incorrect questions. To overcome this, we conduct an SFT over the base SFT model for teaching the model the following: (1) Answering factual questions strictly from the provided knowledge sources (2) Identifying any kind of ambiguity or factual incorrectness in the query and highlighting it instead of generating answer for"}, {"title": "5.5 Alignment with Human Feedback", "content": "Direct Preference Optimization (DPO) has shown superiority over Proximal Policy Optimization[37] (PPO)-based Reinforcement Learning from Human Feedback (RLHF)[39] in certain aspects. DPO, as highlighted in the study by Rafailov et al., has demonstrated better control over sentiment in text generation and comparable or improved response quality in summarising and dialogue tasks compared to PPO-based RLHF [36]. Additionally, experiments in [36] have indicated that DPO can align language models with human preferences effectively and with simplicity, eliminating the need for extensive hyperparameter tuning or complex sampling from the model during fine-tuning. In our experimentation, we employed a reduced learning rate of 5e-7 in conjunction with a \u03b2 value set to 0.1, leveraging a dataset comprising approximately 20,000 instances focused on safety topics. We scale data points in later phases for myriad of the alignment topics. Notably, our observations underscore the necessity of maintaining a balanced language data mixture to prevent the model from exhibiting forgetting behaviors related to other linguistic capabilities."}, {"title": "6 Experimental Results and Analysis", "content": ""}, {"title": "6.1 Embeddings", "content": "We extract and analyse the embedding from the penultimate layer of Krutrim model using UMAP plots [29] on data points which are randomly drawn from a myriad of task categories like factual, content writing, reasoning, safety etc. This further helps in analysing the impact of every stage of training like PT, SFT etc. UMAP, short for Uniform Manifold Approximation and Projection, serves as a dimension reduction method applicable not only for visualization akin to t-SNE but also for general non-linear dimension reduction purposes. We also analyse LLaMA2 7B chat model against Krutrim SFT model and show superiority overall while being substantially superior for creative generation task."}, {"title": "6.2 Experiments on Probing Layer-Wise model learning", "content": "We analyze information present in layers at different depths by performing different probing tasks designed by chen et al.[13]. They perform analysis of LLaMA models of different sizes (7B, 13B, 70B parameters) tasked to answer complex multiple choice questions to capture models' performance on different aspects such as calculation, math problem solving (MPS), logical reasoning, truthfulness, and factual knowledge detection. We perform similar evaluation of our language model to gain insights into the layer-level performance. For a fair comparison, we perform the evaluation using the same test setting and the same datasets. We make the following observations from the experiments on English performed across different tasks on different layers of the model, as also depicted in Figure 6a."}, {"title": "6.3 Standard Indic Benchmarks", "content": "Traditional metrics like BLEU, ROUGE, and GLUE have long been the go-to measures for assessing the performance of language generation systems. However, these metrics often fall short when it comes to capturing the nuanced semantic similarity between sentences [32]."}, {"title": "6.4 Standard English Benchmarks", "content": "Recent advancements such as LLAMA 3 [4] and Mistral 7B [21] have demonstrated impressive capabilities. However, these models are trained with significantly higher computational resources and larger datasets compared to Krutrim model. Additionally, they do not perform well on Indic lan- guages. Consequently, we have not included comparisons with these models due to these substantial differences in FLOPS. We evaluate Krutrim fine tuned model on various English benchmark tasks and compare against Llama-2 7B chat SFT."}, {"title": "6.5 Human Evaluations", "content": ""}, {"title": "6.5.1 Continual Pre-training: Techniques and Result", "content": "The CPT data should include some of the data from the original pre-training mix. A decent starting point is 25% original data and 75% new data. The new data ought have better quality. These proportions can be adjusted based on the amount of forgetting we're seeing. The learning rate should be adjusted to pick from the last value where it was stopped in the pre-training step.\nTo emphasize the impact of CPT, we present the gain in the MOS (Mean Opinion Score) of qual- itative evaluation performed across 7 task categories averaged for major Indic languages."}, {"title": "6.6 SFT for Answering Factual Questions", "content": "To analyze the impact of the experiment mentioned in Subsection 5.4, we conducted a qualitative study. In this study, we analyzed a subset of real-world factual questions posed by users. The testing set was organized into three distinct categories.\n1. Factual Questions\n2. Ambiguous or Factually Incorrect Questions\n3. Generation-Based Questions\nWe employed a data tagging team to manually assess the responses generated by Krutrim model. These assessments were categorized into the following scenarios with corresponding labels:\n\u2022 Good: The response accurately addresses the query.\n\u2022 Bad: The response fails to address the query correctly.\n\u2022 Refrained: The model deliberately abstains from answering or does not provide relevant information.\nWe present a comparative analysis of accuracy and error rates across different versions of Krutrim and two leading conversational search engines on our test set. The results are detailed as follows:\nAccuracy Percentages on the Test Set:\n\u2022 Leading Conversational Search Engine #1: Achieved an accuracy of 80.87%.\n\u2022 Leading Conversational Search Engine #2: Recorded an accuracy of 77.43%.\n\u2022 Krutrim - Launch Version: Initially posted an accuracy of 68.67%.\n\u2022 Krutrim - Current Version: Demonstrated a significant improvement, reaching an accuracy of 79.13%.\nError Percentages on the Test Set:\n\u2022 Leading Conversational Search Engine #1: Registered an error rate of 19.17%.\n\u2022 Leading Conversational Search Engine #2: Showed a lower error rate of 6.13%.\n\u2022 Krutrim - Launch Version: Had an error rate of 18.93%.\n\u2022 Krutrim - Current Version: Marked improvement is seen with a reduced error rate of 7.47%.\nThese findings are visually represented in Figures 10.b and 10.b, providing a clear depiction of the performance metrics across the different systems."}, {"title": "7 Qualitative Analysis", "content": ""}, {"title": "8 Conclusion", "content": "Krutrim establishes itself as a pioneering effort in developing the first large language model (LLM) specifically catering to the Indian context. This model addresses the crucial aspects of encompassing cultural intricacies and rich linguistic landscape, incorporating a vast amount of Indic data, and ensuring accessibility for the extensive Indian population.\nKrutrim stands out by leveraging the largest known collection of Indian data for training, achieving performance that matches or sometimes surpasses several contemporary LLMs in India context, even those boasting significantly larger parameter sizes for general applications. Furthermore, Krutrim integrates real-time web search capabilities, empowering it to deliver factual and up-to- date information outperforming state of the art system such on adversarial and factually incorrect questions.\nTo facilitate user interaction, Krutrim LLM is readily available through a user-friendly conversa- tional interface. This accessibility aspect positions Krutrim to broadly contribute across various sectors in India and serve as a foundation for further advancements in Indian-centric AI applications."}, {"title": "9 Authorship, Recognition of Contributions, and Acknowledgements", "content": "Please cite this work as \u201cKrutrim (2024)\".\nPre-training: Aditya Kallappa, Palash Kamble, Abhinav Ravi, Akshat Patidar, Deepak Kumar, Vinayak Dhruv, Raghav Awasthi, Arveti Manjunath, Gautam Bhargava, Chandra Khatri\nFine-tuning and Alignment Aditya Kallappa, Palash Kamble, Abhinav Ravi, Kumar Ashish, Deepak Kumar, Sanket Shah, Sulabh Katiyar, Vinayak Dhruv, Sindhu Pawar, Soham Pendurkar\nEvaluations Arveti Manjunath, Pranav Raveendran, Bidyapathi Ray\nData and Tokenisation: Aditya Kallappa, Palash Kamble, Akshat Patidar, Vinayak Dhruv, Daud Ibrahim, Divyansh Rajput, Pidathala Sowjanya, Rahul Kumar, Rishabh Nahata, Pranav Raveendran, Bidyapathi Ray\"\n    }"}]}