{"title": "On the Utility of Domain Modeling Assistance with Large Language Models", "authors": ["MERIEM BEN CHAABEN", "LOLA BURGUE\u00d1O", "ISTVAN DAVID", "HOUARI SAHRAOUI"], "abstract": "Model-driven engineering (MDE) simplifies software development through abstraction, yet challenges such as time constraints, incomplete domain understanding, and adherence to syntactic constraints hinder the design process. This paper presents a study to evaluate the usefulness of a novel approach utilizing large language models (LLMs) and few-shot prompt learning to assist in domain modeling. The aim of this approach is to overcome the need for extensive training of AI-based completion models on scarce domain-specific datasets and to offer versatile support for various modeling activities, providing valuable recommendations to software modelers. To support this approach, we developed MAGDA, a user-friendly tool, through which we conduct a user study and assess the real-world applicability of our approach in the context of domain modeling, offering valuable insights into its usability and effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "Software modeling helps manage the complexity of engineering problems, enhancing the efficiency and effectiveness of software development through the power of abstraction [7, 50]. In particular, domain modeling is a crucial step in software design. It is defined as an explicit, structured, and visual representation of real-world objects within a domain and their interconnections [13]. These representations involve vocabulary, key concepts, behavior, and relationships.\nDomain modeling plays a pivotal role in prompting a comprehensive understanding of the problem at hand and establishing the groundwork for developing solutions. However, it is a complex and error-prone task. Firstly, it necessitates proficiency in two distinct areas of expertise: domain knowledge and modeling formalisms, posing challenges for both domain experts and software specialists. Furthermore, a domain is an open world, making it highly contextual to delineate the boundaries of what should be included in the model. Lastly, the subsequent design phases heavily rely on the resulting domain models, meaning that the omission of certain concepts can significantly impact the developed solutions.\nThese challenges have led to an increased demand for new tools and techniques that can assist modelers in their tasks. While Al has made significant progress in code-related tasks such as code completion, program repair, and testing [46], its application in the early software development phases, specifically modeling and design, remains limited. More specifically, domain modeling assistance has primarily been addressed by searching for model fragments in repositories [39] or by exploiting formalized knowledge [1]. More recently, deep-learning models have been trained or fine-tuned using modeling data (e.g., [57]). However, these research initiatives were limited by the scarcity of domain-specific training data required for high-quality recommendation systems.\nWith the increase in the performance of Large Language Models (LLMs), new approaches based on prompting have been recently proposed to assist in domain modeling by suggesting domain concepts, i.e., automated model"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this section, we provide an overview of the background concepts of our study and review the related work, focusing on assistance in software modeling (Sec. 2.1) and large language models (Sec. 2.2)."}, {"title": "2.1 Assistance in Software Modeling", "content": "The complexity of modern software systems often requires rigorous engineering methods. Permanent examples include cyber-physical systems, especially those of safety-critical nature, such as automotive systems. Model-driven engineering (MDE) [50] advocates modeling software systems before they get released and use these models for an array of crucial engineering activities, including requirements formalization [20], rapid prototyping and validation [40], formal verification of functional [22] or extra-functional properties, and code generation [29]. Within MDE, domain modeling [28, 44] focuses on externalizing core domain concepts and relationships among them, typically relying on a subset of the Unified Modeling Language (UML) Class Diagram notation [6]. Class diagrams have shown to offer a balanced combination of descriptive power and economical notation, allowing for an efficient modeling experience [35]. Unfortunately, domain modeling is still a time-consuming and error-prone task when carried out manually.\nTo alleviate the issues stemming from manual efforts, computer-aided automation has been of particular interest in software modeling [41]. Traditional software modeling assistance relies on pattern-based [30] or similarity-based [21] recommendations, and occasionally it relies on recommendations extracted from natural language specifications [27]. Savary-Leblanc et al. [49] show that effective modeling assistance requires incorporating high-context information in the assistance mechanism, such as domain knowledge and corporate methodology, as well as intuitive notations and tools. Additional criteria for effective and efficient modeling assistance include usability, competence, added value, adaptation to the user, and understanding of the work context. We focus on such human aspects when we gauge the perceived and objective utility of modeling assistance. While their advantages are clear, traditional model assistant techniques still suffer from limited availability of training data and the need for substantial computational resources.\nAI-based techniques opened new frontiers in modeling assistance. AI-based techniques promise better assistance quality due to the more powerful patterns extracted from the training data. By extension, however, the volume of available training data becomes a crucial consideration. Weyssow et al. [57] propose a similarity-based, data-centered approach in which datasets of models and metamodels relevant to the target objective are transformed into textual representations to train neural networks. A similarity-based approach is proposed in the work of Elkamel et al. [21]. It involves using clustering algorithms instead of neural networks to propose UML classes. Other techniques leverage the fact that software models are eventually used for generating source code and take a reverse engineering stance. Capuano et al. [10] train a neural network with data extracted from Java projects. Similarly, Di Rocco et al. [19], exploit large code bases to reverse-engineer to train a RoBERTa language model [38]. Their results show substantial bias in the obtained recommendations towards implementation-related concepts rather than high-level domain concepts, such as utilities (cf. Util classes in Java) and controllers (cf. Controller classes in Java). Natural language-based techniques alleviate such biases and constitute the state-of-the-art in software modeling assistance. Arora et al. [5] propose an approach to automatically extract domain models from requirement documents by rule-based techniques. Their results show promising results with about 74 - 100% recommendations deemed eventually correct in their experiments. Burgue\u00f1o et al. [8] exploit language models that are trained on general natural language documents to suggest domain concepts for model completion, and reach up to 62% reliability in identifying model elements. Unfortunately, natural language-based techniques provide solutions to a specific case and re-training is required for new cases, problems and projects. Such resource-intensive endeavors typically cannot be justified in practice, limiting the applicability of powerful and efficient natural language-based techniques. However, the recent emergence of large language models has opened up new opportunities for using natural language to help with modeling. In a study by Wang et al. [55], students interacted with LLMs and defined their own prompts to create use case models, class diagrams, and sequence diagrams. The"}, {"title": "2.2 Large Language Models and Few-shot Learning", "content": "Large language models (LLMs) are AI models pre-trained on vast amounts of textual data, enabling them to capture extensive human-generated knowledge and use this encoded knowledge for generative tasks such as creating human-like text [43]. The key mechanism of LLMs relies on predicting the conditional probability of a sequence of tokens based on previously observed tokens, e.g., generating the next word in a text. It is clear that the often human-like qualities of LLMs' output come from training it over a lot of data and the statistical rules that human language tends to follow [25]. The GPT family\u00b9 comprises the largest and most adopted LLMs at the time of writing this article. It has been used in numerous tasks in the overall software development lifecycle, from requirements engineering [45] through software architecture [2] to testing [23]; as well as organizing software engineering knowledge by accelerating literature surveys [52, 53].\nIn most occasions, before employing them, LLMs need to be fine-tuned. This is because of the generality of the corpus LLMs are pre-trained on. Fine-tuning adjusts model parameters slightly, rendering the LLM more adapted for a particular task. Thanks to the extensive nature of pre-training, fine-tuning is usually a minor effort that includes (i) hyperparameter tuning and (ii) instruction tuning [59]. Some of the main hyperparameters of a GPT model are temperature, controlling the randomness, creativity and diversity of the response and maxTokens, controlling the length of the response. We elaborate on our choices of hyperparameters in Sec. 4. Instruction tuning is achieved through systematically engineering prompts, which are brief textual inputs that LLMs are capable of responding to. By varying the wording of the prompt, different responses can be obtained [58].\nExpected input-output pairs, known as shots, may be included in certain prompt instances. Depending on the number of shots, we distinguish between zero-shot learning (no shots are provided), single-shot learning (one shot is provided), and few-shot learning (a small number of shots are provided) [36]. We employ few-shot learning, where shots are generated from already existing segments of the model under construction. (See Sec. 3 for details.)\nWithin software engineering, Model-Driven Software Engineering (MDSE) is in a prime position to leverage LLMs due to its emphasis on the formal semantics of languages [50]. While LLMs can produce output that often exhibits human-like qualities, it is still not guaranteed that these outputs make sense [4]. Therefore, a systematic approach to mapping LLMs' outputs to the target language's semantics is crucial for reliable LLMs-powered modeling methods in MDSE. As a consequence, there has been a surging interest in employing LLMs in MDSE lately, e.g., in automated software modeling [12, 26, 55]. In a related work, Tinnes et al. [54] achieves a semantic correctness of 62.30% via retrieval-augmented generation in a real-world industry setting.\nConversely, the benefits of MDSE for LLMs have been recognized, e.g., in model-driven prompt engineering. For example, Claris\u00f3 and Cabot [14] propose using domain-specific languages (DSLs) to define platform-independent prompts, which can then be adapted to generate high-quality outputs in target Al systems. Unfortunately, no systematic approach exists for prompting LLMs for domain model completion that involves semantic mappings, efficient tooling, and a clear evaluation of utility. In the following sections, we present such an approach and reflect on its challenges."}, {"title": "3 PROMPTING LLMS FOR DOMAIN MODEL COMPLETION", "content": "As highlighted by Camara et al. [9], while LLMs excel in many tasks, they are not yet fully capable of automating model creation, especially for domain-specific languages. Nevertheless, LLMs have accumulated a lot of valuable knowledge that can be used in modeling activities such as model completion. However, a crucial gap exists between inferring accurate knowledge for a task and effectively leveraging that knowledge to benefit users. This paper focuses on exploring the utility of automated domain model completion using LLMs. To evaluate this utility, we propose an approach and a tool specifically designed to assist modelers in their tasks. This section introduces our approach and provides a practical illustration using a running example. Our proposed approach builds upon our prior work on domain model completion utilizing LLMs [11]. Our approach is based on an iterative process with the domain modeler in the loop. Each iteration has a series of steps. First, we read the model under construction (either the textual representation of graphical models or its textual serialization \u2013 e.g. XMI, UML or Ecore models), and use parsers to automate the extraction of information. In particular, we extract information about each class name, its attributes and the associations between classes. Second, we establish what we refer to as semantic mappings. A semantic mapping is defined as the translation of elements within the domain model under construction into semantically equivalent text. For this, we use the obtained information and filter it depending on the purpose (i.e., create class suggestions, attribute suggestions and association suggestions). Third, each piece of generated text in the previous step is integrated into a prompt, ensuring its comprehensibility to the LLM at a later stage. We create different prompts for different kind of suggestions (e.g., new classes, new attributes, new associations). Fourth, we prompt the LLM, collect and parse the result and generate suggestions. Suggestions are presented to the modeler who can accept or ignore them. If accepted, the suggestion is integrated in the model under construction. These steps are presented with further details in Algorithm 1.\nAs mentioned before, depending on the nature of the suggestions that we would like to provide, we need to adapt our semantic mappings. In the following, we will provide all the details and we illustrate them using our running example.\nLet us assume that a modeler is building a domain model to capture information about hospitals. At a certain point in time, the model is in the state that Fig. 1 captures. It has only 3 basic entities: Hospital, Staff and Doctor. A hospital has an attribute of type String named name, an attribute numRooms of type int and one aggregation to the entity Staff. Staff has one attribute name, and Doctor has two attributes speciality and qualification.\nRegarding the creation of prompts, we incorporate the principles in [17, 37] to meticulously craft prompts tailored to our objectives. We use the strategy few-shot prompt learning. Therefore, each prompt is composed of three critical elements: the Instruction outlining the task at hand, Few-shot examples providing context and guidance, and the encoded Model under construction, all aimed at facilitating the completion of the model."}, {"title": "3.1 Suggesting model elements", "content": "3.1.1 Suggesting classes. For class suggestion, the goal is to obtain meaningful names for classes to recommend. The instruction within the prompt is the text 'Generate related concepts'. The list of shots (examples) is extracted from a catalog of diagrams from diverse and unrelated domains. For each shot i, first, we add to our prompt the package name"}, {"title": "3.1.2 Suggesting attributes", "content": "To generate attribute suggestions for a class, we follow a two-step process. First, we obtain attribute names, and then, for each name, we extract the attribute type.\nDuring the first step, the instruction within the prompt is \u201cGenerate missing attributes for each class in this class diagram\". The list of shots is again extracted from the previously mentioned catalog of diagrams.\nFor each shot, we gather a set of classes from a package i. We then combine the package name (Xi) with each class name Yij and its attributes Aij1..Aijn in square brackets. Following this, we append the symbol \"=>\" and merge another set of classes with their attributes.\nThat is, we employ the following syntax: X\u2081: Yi1: [Ai11, Ai12, ...Ai1n]; ..., Yik: [Aik1, Aik2, ...Aikn]]. Finally, to encode the model under construction, we use the elements extracted from the domain model for attribute suggestion (i.e., ga in Algorithm 1). We concatenate the package name, the existing class names with their attributes in square brackets, the class for which we are finding potential attributes and the symbol \u201c=>\u201d.\nIn our running example, a potential prompt (with only one shot) is shown in Listing 3."}, {"title": "3.1.3 Suggesting associations", "content": "To generate association suggestions for a domain model, we follow three steps. First, we try to identify whether there is a relationship between two classes by obtaining the association's name. Second, we try to obtain the kind of association. Finally, if inheritance is identified, we determine its direction.\nTo obtain association names, we use the following instruction that is included in the prompt \"Predict association name\". The list of shots is again compiled from the catalog of diagrams, from which we select random pairs of classes that have an association between them. For each pair, we concatenate the name of the classes C\u2081, Cj and the name of the association Aij using the following syntax: Ci, Cj => Aij. Each group of elements is separated with a semicolon. Then, we take the model under construction, and for each association, we generate the text using the same syntax and add it to the prompt. An example can be seen in Listing 7. Note that this prompt only contains Instruction and Shots. As there are no associations with name in the partial model, the partial model is not encoded nor added to the prompt.\nTo generate suggestions for the type of associations, the instruction included in the prompt is \"Specify the nature of the association between these concepts, inheritance or association or composition or no\u201d. Each pair of classes C\u012f, Cj is used to construct a shot by concatenating the class names and the association type Aij \u2208 {inheritance, association,"}, {"title": "3.2 Suggestion modes", "content": "To present the suggestions for the user, we implement three modes that address different stages of the modeling process and user preferences."}, {"title": "3.2.1 Suggestions On request", "content": "This mode allows users to actively seek out specific predictions at any point in the modeling process, aiming for precise searches of attributes, concepts, or associations. This flexibility is particularly useful for users who have a clear understanding of their needs and prefer to control when and what assistance they receive. The rationale behind this mode is based on the principles of direct manipulation interfaces, which support user control, as discussed in [51]."}, {"title": "3.2.2 Automatic suggestions", "content": "This mode provides real-time, automatic suggestions of relevant concepts and operations as users add elements to their model. It offers continuous support that adapts to the evolving context of the model. This mode helps maintain a steady flow of relevant ideas and ensures that the assistance is proactive and contextually aware [18]. Additionally, addressing interaction challenges with automated systems, as highlighted by Wessel et al. [56], ensures that suggestions are helpful without being disruptive."}, {"title": "3.2.3 Suggestions at the end", "content": "This mode presents suggestions for potential enhancements upon model completion, enabling users to conduct a final review and refinement. This mode is particularly useful for users who prefer to first develop their initial ideas independently and then seek out ways to refine and enhance their work based on comprehensive feedback. Such an approach aligns with the principles discussed in [16] which emphasizes the importance of post-design evaluation and refinement to enhance the quality and functionality of the final product."}, {"title": "4 TOOL SUPPORT", "content": "Conducting our experiments requires a suitable modeling tool. Unfortunately, state-of-the-art tools do not offer proper integration capabilities with LLM services or flexible customization of their internals. Therefore, we developed MAGDA (Modeling Assistance with Generative Ai for Domain representation), a research tool to support our work\u00b2. When creating the tool, we aimed to reduce potential validity threats from inadequate instrumentation or tools.\nThe high-level overview of the tool is shown in Fig. 3. We chose the Eclipse platform\u00b3 to implement our tool thanks to the rapid development curve it enables through its ready-to-use modeling framework and extensible plug-in-based architecture. The user interacts with the models through the visual Editor (Sec. 4.1) that allows the creation of domain models using UML Class Diagrams. The Modeling Framework (Sec. 4.2) serves for representing and persisting models. The Recommendation engine (Sec. 4.3) is responsible for querying the LLM (GPT Model) upon request or upon edit operations; collecting, organizing, and ranking recommendations; and passing this information to the editor by saving recommendations in the model. Finally, to investigate user behavior during modeling, we developed a Logging facility (Sec. 4.4).\nIn the following, we provide further details about these technical components."}, {"title": "4.1 Editor", "content": "Modeling is supported by a visual editor that we have developed using the Eclipse Sirius language workbench\u2074. Building a modeling environment with Sirius is achieved in three steps. First, we define a domain model for class diagrams and recommendations (as shown and discussed in detail in Fig. 5). Second, we define the concrete syntax, i.e., the notation the user interacts with when defining class diagrams. We follow the standard UML notation. Third, we develop productivity features to improve the user experience and mitigate any threats to the construct validity of our study that might stem from inappropriate tooling.\nThe eventual user interface is shown in Fig. 4. In the middle of the screen, a canvas shows the elements of the constructed class diagram. On the right side, a toolbox allows the user to edit the model on the canvas. On the left side,"}, {"title": "4.2 Modeling framework", "content": "The modeling framework serves to represent and persist models. We used the Eclipse Modeling Framework (EMF) for this purpose. EMF is a collection of Eclipse plug-ins allowing for modeling a domain and generating code (e.g., EDIT transaction handlers and user interface components)."}, {"title": "4.3 Recommendation engine", "content": "The recommendation engine is responsible for interacting with the LLM to generate, collect, collate, and persist recommendations by the metamodel in Fig. 5.\nUsing GPT as the LLM. The recommendation engine in our experiments uses GPT-3\u00ba. A GPT model is an appropriate choice for our purposes as the most powerful LLM at the time of the experiments and at the time of writing this article. We integrate MAGDA with GPT through the OpenAI's API\u201d. We parameterize GPT in accordance with community practices. Choosing tested and proven settings mitigates threats to validity. The key parameters are listed in Tab. 1."}, {"title": "4.4 Logging", "content": "To analyze participants' behavior, we collect data about their modeling exercise. To this end, we developed a logging system that records (i) every user operation that affects the model, such as creating and deleting model elements, and accepting recommendations; (ii) user requests for recommendations; and (iii) the recommendations generated by the LLM. Records are timestamped, allowing us to unambiguously reconstruct the modeling exercise from the logged data. Logs are persisted on the file system for further investigation in comma-separated files. The replication package contains the log files of the experiments reported in this paper. Listing 10 shows a sample of the log files."}, {"title": "5 STUDY DESIGN", "content": "In this section, we present the user study setup. The complete protocol, materials, task descriptions, questionnaires, as well as the anonymized data we collected, are available as part of a replication package."}, {"title": "5.1 Research questions", "content": "As outlined in the introductory section, we address the following five research questions. The first three pertain to the objective utility, while the remaining two questions focus on the utility as perceived by the modelers.\nRQ1 (Productivity). What is the impact of modeling assistance on the time required to complete the domain models?\nRQ2 (Contributivity). What is the level of contribution of the suggestions to the resulting models?\nRQ3 (Creativity). Does the assistance reduce the solution diversity among the modelers?\nRQ4 (Assistance Preference). What kind of assistance do participants prefer to choose when completing their tasks?\nRQ5 (Modeling experience). How does the assistance affect the modeling experience, as perceived by the modelers?"}, {"title": "5.2 Study locations and participant selection", "content": "As part of our study, we have conducted a series of experiments (Sec. 5.3) in two different regions to improve the diversity of the study: at the University of Montreal in Montreal, Canada; and at the University of Malaga in Malaga, Spain; with 15 participants at each location, i.e., 30 participants total.\nThe Montreal experiments are carried out between May 1 - May 10, 2023, conducted by the first author. The Malaga experiments are carried out between July 13 - September 28, 2023, conducted by the second author. The language of experiments is English. We ensure that participants possess an appropriate command in English. In exceptional cases, the participant's native language may be used for clarification purposes between the researcher and the participant.\nTo ensure consistent execution, we establish a protocol and adhere to it for every experiment. To validate the compliance with the protocol, the two researchers conducting the experiments randomly review each other's work and virtually attend and observe the procedures.\nWe did not face incidents during the experiments in Montreal. However, we experienced three incidents during the experiments in Malaga, in which unexpected technical problems surfaced leading to the experiments being terminated. We discarded those three incomplete experiments from our results and conducted three additional experiments (with new participants), resulting in 15 successful experiments at the Malaga location as well.\nWe use an extended convenience sampling, mostly recruiting participants from the universities we are affiliated with (University of Montreal, Canada; and University of Malaga, Spain), as well as from universities in close geographical proximity in Montreal, Canada. To battle selection bias, we also ensure that the experience and level of expertise of the population is diverse enough. To this end, we invited undergraduate and graduate (Master's and PhD) students, postdoctoral researchers, professors, and industry experts as shown in Fig. 6. To mitigate threats to construct validity,"}, {"title": "5.3 Experimental setup and tasks", "content": "Experiments are conducted in person, in an appropriately set up environment.\nPilot of the instrumentation. Before the experiments, we run a pilot with one participant testing the setup, including the tool, the room, and the understandability of the problem descriptions. The pilot was concluded successfully, and its results were discarded.\nPreparation of participants. Before an experiment takes place, each participant is provided with essential information about the basic workings of the tool used in the experiment. This information is shared the day before the experiment, in the form of a five-minute video, available online\u00b9\u00b3 and in the replication package\u00b9\u00b9. At the beginning of the experiment, the conductor requests confirmation from the participant regarding the video watching and offers to address any questions that may have arisen.\nEnvironment. Experiments are conducted in a room with only a researcher and a participant present. The participant is given access to a computer with the tool already installed and running. We use a standard computer system running a Windows operating system and equipped with a wide-screen monitor with a resolution of 1920\u00d71080.\nModeling. Each experiment consists of completing four tasks within a one-hour time frame as follows.\nIntroduction. At the beginning of the experiment, the lead researcher explains the details of the experiment to the participant and answers any questions the participant may have.\nTasks 1-3: Modeling based on a described problem. The first three tasks require modeling a domain based on a short description of three-to-four paragraphs.\nWe describe three domains: a hotel booking system (D1), an online shop system (D2), and a banking system (D3). Modeling these domains requires a diverse set of modeling elements, thereby providing a sufficiently complete coverage of the syntax of the modeling formalism (UML Class Diagrams)."}, {"title": "5.4 Data analysis", "content": "We define in this section the analysis techniques by which we answer the research questions."}, {"title": "5.4.1 RQ1: What is the impact of modeling assistance on the time required to complete the domain models?", "content": "Our objective is to precisely measure the amount of time required for each participant to complete the modeling task. Time efficiency is a direct indicator of how well the assistance is integrated with the modelers' workflow and whether it genuinely aids in accelerating the modeling process without sacrificing the depth or quality of the models. To achieve this, we employ a logging system that tracks all user operations and decisions, along with the corresponding timestamps of when they occurred. We provide participants with 10 minutes plus any additional time required to complete their tasks, ensuring that they were not rushed. This approach reflects real-world working conditions and helps maintain the quality of the final outputs."}, {"title": "Overlap Coefficient =", "content": "\\frac{|M_{i} \\cap M_{j}|}{\\min(|M_{i}|, |M_{j}|)}"}, {"title": "5.4.2 RQ2: What is the level of contribution of the suggestions to the resulting models?", "content": "We analyze the ability of the employed approach to support modelers by providing them with pertinent elements that are approved and integrated to finalize a model under construction. To achieve this, we scan the obtained models of each task, and apply model transformations to derive conclusive facts, making the data more manageable. The following listing shows an example of a fact that represents an entity named Staff and that has an attribute Name: String and an inheritance association with the entity named Doctor:\nDuring the experiments, the logging system, integrated within MAGDA as detailed in Fig. 4.4, tracks both the elements suggested by the recommendation system that were accepted by the user and those that were not. We were able then to accurately calculate two key metrics: Acceptance Rate and Contribution Rate. Both metrics provide valuable insights into the utility and success of implementing suggestions, though from distinct perspectives.\nThe Acceptance Rate measures the extent to which the suggested concepts are accepted by the users. The formula used for calculating it is as follows:"}, {"title": "Acceptance Rate =", "content": "\\frac{\\text { Accepted concepts from suggestions }}{\\text { Total number of suggested concepts }}"}, {"title": "Contribution Rate =", "content": "\\frac{\\text { Accepted concepts from suggestions }}{\\text { Total number of concepts in model }}"}, {"title": "5.4.3 RQ3: Does the assistance reduce the solution diversity among the modelers?", "content": "Our objective is to assess how the recommendation tool affects the creativity of participants. We do this by comparing the sequences of elements added to the model under construction by participants (concepts and attributes). We assess the similarity of these sequences to determine the participants' level of creativity and whether the tool is restricting or enhancing their diverse and creative output.\nTo gain a better understanding of the diversity of solutions, we separately compare the solutions obtained for each domain-suggestion mode configuration (see Tab. 3). For each configuration (see Table 2), 10 models are produced (5 in Montreal and 5 in Malaga). Thus, we have 45 pairs to compare (C20). The overlap coefficient for a configuration is the average of the coefficients of the model pairs in that configuration.\nAs shown in formula 3, for two models containing respectively M\u00a1 and M\u00a6 elements, the overlap coefficient is measured as the ratio between the number of common elements and the size of the largest model."}, {"title": "5.4.4 RQ4: What kind of assistance do participants prefer to choose when completing their tasks?", "content": "The final task in our study is an open-ended activity where participants can choose and switch between their preferred suggestion modes while selecting their own domain to model. This setup allows us to observe participant autonomy and gather insights into their preferences and diverse usage patterns. We specifically analyze how frequently each suggestion mode is used and how participants transition between these modes during their modeling sessions."}, {"title": "5.4.5 RQ5: How does the assistance affect the modeling experience, as perceived by the modelers?", "content": "To address this research question, we evaluate participants' overall modeling experience using questionnaires completed after each task. These questionnaires assess various dimensions of the experience, such as ease of use, tool effectiveness, satisfaction with the process, and comfort with different recommendation modes. We analyze the quantitative responses using bar plots, which visualize the frequency and distribution of responses for each question, facilitating easy comparison across different modes. Additionally, we incorporate qualitative insights from participants' open-ended responses to provide a deeper understanding of their experiences."}, {"title": "5.5 Threats to validity", "content": "Despite our best efforts in designing this study, several factors may limit the validity of our results. A first threat to validity is related to the configuration of the LLM. The parameters used influence the suggestions generated. To mitigate this threat, we employed well-established settings, e.g., the text-davinci-002 engine recommended by Zhao et al. [60], a balanced temperature hyperparameter [3], and a reasonable setting for response length [42].\nThe selection of metrics is another crucial factor. To avoid mono-method bias [47], we used two distinct metrics for questions 2 and 3. For question 3, to address the limitations of exact match methods, we opted for a manual matching approach. This method was chosen over automated tools like WordNet [24] to prioritize context-specific categorization.\nAnother concern relates to the definition and execution of the modeling tasks. To ensure fairness among the suggestion modes and to counteract maturation bias, we assigned the first three tasks to participants in varying orders, as shown in Tab. 3. For the fourth task, participants were allowed to freely choose the domain and suggestion modes without specific instructions. This approach aimed to simulate a realistic scenario in which participants had the autonomy to decide what and how to model. By this stage, participants were already familiar with all three suggestion modes from the previous tasks. Additionally, this allows us to interpret the choice of suggestion mode as a genuine preference rather than mere curiosity to explore the options.\nA notable threat to the validity of our study is the limited number of participants, which could affect the generalizability of our findings. However, our choice of 30 participants aligns with established guidelines in usability testing. These guidelines recommend a minimum sample size of 30 for cumulative assessments to ensure statistical significance [33]. This number provides sufficient data for robust comparative analysis across different suggestion modes and strikes a practical balance regarding statistical power, resource allocation, time, and participant availability. To further enhance the representativeness of our results, we conducted the study with diverse groups in Montreal and Malaga.\nLastly, in the questionnaire, we did not use Likert scales with explicit point labeling. This might lead participants to interpret each level differently. We made this choice to reduce the cognitive load associated with reading and interpreting labels and to allow more flexible and nuanced interpretation between the extremes. We specifically discussed this potential threat during our pilot study and refined the questionnaire based on that discussion. Additionally, we included open-ended questions alongside the Likert scales, allowing participants to explain their choices in more detail."}, {"title": "6 RESULTS", "content": "In this section, we present the results of the user study."}, {"title": "6.1 RQ1: Effects of content assistance on time to complete task", "content": "Table 4a shows the average times required to complete the tasks using different modes for producing domain models. The standard deviations for these times are relatively low, indicating minimal variability in the data. When comparing the On-Request and Automatic modes to the no-assistance, both modes demonstrate shorter average times, with reductions of 18% and 22%, respectively. This suggests that participants completed tasks more quickly when they could request assistance or receive automatic suggestions. Of all the modes, the Automatic mode has the fastest average completion time, slightly surpassing the On-Request mode. This indicates that participants were most efficient when the system provided suggestions automatically as they added elements to the canvas. The slight difference between the Automatic and On-Request modes might be due to the time taken to request suggestions."}, {"title": "Answer to RQ1", "content": "Within our sample, providing users with real-time suggestions and support during their modeling activities results in faster task completion, with a time reduction of approximately 20% and a higher rate of tasks finished ahead of the allocated time. However, the lack of statistical significance in these results prevents us from generalizing beyond our experiment."}, {"title": "6.2 RQ2: Suggestions Impact on Solution Elaboration", "content": "To address this research question", "perspectives": "acceptance, which measures how many suggestions were incorporated into the modeling solution, and contribution, which evaluates the portion of the resulting models attributable to these suggestions. We analyze the outcomes across different modes of assistance to understand how various types of guidance impact these metrics. The results, including their respective standard deviations, are detailed in Table 5a and discussed in Section 5.4.\nOur findings indicate that both the On-Request and Automatic modes of assistance yield higher rates of acceptance and contribution compared to providing suggestions only at the conclusion of the modeling tasks. These differences are statistically significant (see Table 5). This suggests that delivering suggestions during the modeling process"}]}