{"title": "Language-Guided Image Tokenization for Generation", "authors": ["Kaiwen Zha", "Lijun Yu", "Alireza Fathi", "David A. Ross", "Cordelia Schmid", "Dina Katabi", "Xiuye Gu"], "abstract": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focus on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization.", "sections": [{"title": "1. Introduction", "content": "Image generation has made remarkable progress in recent years, enabling high-quality synthesis across diverse applications [9, 11, 32, 37]. Central to this success is the evolution of image tokenization, a process that compresses raw image data into a compact yet expressive latent representation through training an autoencoder. Tokenization allows generative models, such as diffusion models [9, 32, 37] and autoregressive models [11, 28, 43] to operate directly in this compressed latent space instead of the high-dimensional pixel space, significantly improving computational efficiency while enhancing generation quality and fidelity.\nDespite various image tokenization efforts aimed at improving training objectives [11, 36, 45] and refining the autoencoder architecture [47, 50], current methods remain fundamentally limited by a trade-off between compression rate and reconstruction quality, especially in high-resolution generation. High compression reduces computational costs but often sacrifices reconstruction quality, while prioritizing quality leads to prohibitively high computational expenses.\nAddressing this limitation requires a fundamental shift in the tokenization process. At its core, tokenization involves finding a compact and effective representation of an image. The most concise and meaningful representation of an image often comes from its language description\u2014i.e., captioning. When describing an image, humans naturally start with high-level semantics before elaborating on finer details. Inspired by this insight, we introduce Text-Conditioned Image Tokenization (TexTok), a novel framework that leverages descriptive text captions to provide high-level semantic content, and thus the tokenization process focuses more on fine-grained visual details, thereby enhancing reconstruction quality without compromising compression rate.\nTo the best of our knowledge, we are the first to condition on detailed captions in the tokenization stage, an approach typically reserved for the generation phase. Text captions are easy to obtain from online image-text pairs or using a vision-language model to caption the images. Since text conditioning is widely used in image generation, e.g., text-to-image generation, our method can seamlessly incorporate these captions into the tokenization process without incurring additional annotation overhead.\nWe demonstrate the effectiveness of TexTok across a diverse set of tasks and settings. Compared to conventional tokenizers without text conditioning, TexTok achieves substantial gains in reconstruction quality, with average reconstruction FID improvements of 29.2% and 48.1% on ImageNet 256x256 and 512\u00d7512 resolutions, respectively. These enhancements in tokenization lead to consistent boosts in generation performance, with average improvements of 16.3% and 34.3% in generation FID for the two resolutions. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system achieves a 93.5\u00d7 inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet 512\u00d7512. Our best TexTok variant with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet 256x256 and 512\u00d7512 respectively.\nWe further demonstrate that incorporating text during the tokenization stage significantly enhances text-to-image generation, achieving 2.82 FID and 29.23 CLIP score on ImageNet 256\u00d7256. Since text captions are inherently available for this task, TexTok boosts performance without adding any extra annotation overhead."}, {"title": "2. Related Work", "content": "Image tokenization. Image tokenizers build a bidirectional mapping between high-resolution pixels and a low-dimensional latent space, significantly improving the learning efficiency of downstream tasks, such as image generation [4, 11, 26, 49], and understanding [47, 49]. Image tokenizers are usually formulated as an AutoEncoder (AE) [1] framework with an optional quantizer [45] and potentially in a variational [23] setup. These AutoEncoders are trained to minimize the discrepancy between the output and input images, measured by pixel-space distances, latent-space distances [52], or jointly trained discriminators [11]. Architectural variants for the encoder and decoder include ResNet [15] and vision transformers [10]. Spatial correspondence has been a common property of modern tokenizer designs, where one token largely refers to a square neighborhood of pixels. Recently, there has also been development of transformer-based models producing global tokens as a more compact representation [50]. In this work, we follow this paradigm to tokenize an image into a set of global tokens to flexibly control token budgets. However, unlike prior work, we are the first to propose to condition the tokenization process on image captions, which greatly improves the reconstruction quality and compression rate.\nImage generation. Generative learning of pixels has been explored under adversarial [3, 40], autoregressive [6], and diffusion [9, 18, 22] setups. For higher resolutions, generative learning in compressed latent spaces has become popular given its efficiency advantages. Among them, autoregressive [11, 26] and masked prediction [4, 49] models often operate in discrete token spaces following the practice of GPT [34] and BERT [8] in language modeling. Recent variants [28] could also use continuous latent spaces, akin to those used in latent diffusion models (LDMs) [37]. For LDMs, the architecture has evolved from convolution-based U-Net [38] to transformer-based DiT [32]. In this paper, we focus on diffusion-based image generation with DiT architecture, leveraging the flexible token lengths of TexTok.\nLeveraging external semantic information in image generation and tokenization. Many recent studies start to leverage external semantic information, such as image representations and semantic maps, to improve image generation [27, 33, 51]. Unlike these methods, which use external semantics to aid the generation process, our approach focuses on enhancing the tokenization process through conditioning on text semantics."}, {"title": "3. Method", "content": "3.1. Preliminary\nBased on the format of latent representation, image tokenizers can be broadly classified into: 1) Vector-Quantized (VQ) Tokenizers, such as VQ-VAE [45] and VQGAN [11], which represent images using a set of discrete tokens, and 2) Continuous Latent Tokenizers [37] which use a variational autoencoder (VAE) [23] to embed images into a continuous latent space. In this work, we focus primarily on continuous latent tokenizers. As shown in Appendix A, TexTok also works well on VQ tokenizers.\nThe standard continuous latent tokenizer typically consists of an encoder (tokenizer) $E$ and a decoder (detokenizer) $D$. Given an image $I \\in \\mathbb{R}^{H\\times W\\times 3}$, the encoder $E$ compresses it into a 2D latent space $Z = E(I) \\in \\mathbb{R}^{h\\times w\\times d}$, where $h = \\frac{H}{f}, w = \\frac{W}{f}$, and $f$ is the spatial downsampling factor. Each latent embedding $z \\in \\mathbb{R}^d$ is treated as a continuous token, with the image represented by a total of $hw$ tokens. For decoding, these embeddings $Z$ are fed into the decoder $D$ to reconstruct the image $\\hat{I} = D(Z)$. Recently, 1D tokenizers [50] were introduced to allow flexible token budgets for image representation, directly compressing $I$ into 1D latent embeddings $Z_{1D} = E(I) \\in \\mathbb{R}^{N\\times d}$ with $N$ tokens. Reconstruction, perceptual [52], and GAN [11] losses are applied to train the tokenizer by minimizing the distance between $I$ and $\\hat{I}$.\nIn this work, we adopt the 1D tokenizer paradigm to allow more flexible compression rates, demonstrating TexTok's efficacy and efficiency across varying token budgets.\n3.2. TexTok: Text-Conditioned Image Tokenization\nWe introduce Text-Conditioned Image Tokenization (TexTok), a simple yet effective tokenization framework that leverages language to provide high-level semantics and focuses more on tokenizing fine-grained visual details. Unlike existing methods that compress all visual information into latent tokens, we use descriptive text captions to represent high-level semantics and guide the tokenization process.\nTokenization stage. Given an image caption, we use a frozen T5 [35] text encoder to extract text embeddings. These embeddings are injected into both the tokenizer and detokenizer, providing semantic guidance throughout the tokenization process and allowing the learned tokens to focus more on capturing fine-grained visual details.\nAs shown in Figure 2, TexTok adopts a Vision Transformer (ViT) backbone for both the encoder (tokenizer) $E$ and the decoder (detokenizer) $D$ to enable flexible control of token numbers. The input to the tokenizer is a concatenation of three components: 1) image patch tokens $P \\in \\mathbb{R}^{hw \\times D}$ from patchifying and flattening the input image with a projection layer, where $h = \\frac{H}{s}, w = \\frac{W}{s}$, and $s$ is the patch size, 2) $N$ randomly-initialized learnable image token $L \\in \\mathbb{R}^{N\\times D}$, where $N$ is the number of output image tokens, and 3) linearly projected text tokens, $T \\in \\mathbb{R}^{N_t\\times D}$, derived from the text embeddings, where $N_t$ is the number of text tokens. In the tokenizer's output, only the learned image tokens are retained and linearly projected to produce the output image tokens $Z \\in \\mathbb{R}^{N\\times d}$.\nThe detokenizer also takes three concatenated inputs: 1) $hw$ learnable patch tokens $P' \\in \\mathbb{R}^{hw\\times D}$, 2) linearly projected image tokens $Z' \\in \\mathbb{R}^{N\\times D}$ from the input image tokens, and 3) linearly projected text tokens $T' \\in \\mathbb{R}^{N_t\\times D}$ that come from the same text tokens fed to the tokenizer. In the detokenizer's output, only the learned image patch tokens are retained, unpatchified, and projected to reconstruct the image patches.\nWe train the tokenizer and detokenizer using the combination of l2 reconstruction, GAN, perceptual, and LeCAM regularization [44] losses, following [49].\nBy directly injecting text tokens containing high-level semantic information into both the tokenizer and detokenizer, TexTok alleviates the need for the tokenizer and detokenizer to learn the semantics, enabling them to focus more on encoding the remaining fine-grained visual details into the image tokens, which greatly improves the compression rate without sacrificing reconstruction performance.\nGeneration stage. Since this work focuses on continuous latent tokens, we use the Diffusion Transformer (DiT) [32] as the generation framework and train the DiT on top of the latent tokens produced by TexTok. Note that only latent image tokens need to be generated in the generation stage, while the text tokens will be provided in detokenization.\nDiT is trained to model the distribution of TexTok latent tokens, conditioned either on a class category (for class-conditional generation) or on the text embeddings (for text-to-image generation).\nDuring inference, the process differs by the generation task. For text-to-image generation, we use the provided captions for both tokenization and generation, feeding the text embeddings and generated latent image tokens into the detokenizer to produce the output image. For class-conditional generation, DiT generates latent tokens based on the specified class; we then sample an unseen caption for that class from a pre-generated list, and inject it into the detokenizer along with the generated latent tokens to produce the final image. Notably, only the class category is used during generation, in line with standard practice."}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nThe implementation details of TexTok are described below. Please refer to Appendix C for further details.\nText caption acquision. Text captions are readily available for text-to-image generation tasks, where they can be directly used in the tokenization process. For other generation tasks without captions, such as our use of ImageNet [7], we employ a vision language model (VLM), Gemini v1.5 Flash [42], to generate detailed captions offline. For the training set, we caption each given image. For the evaluation set, in class-conditional generation, we pre-generate unseen captions for each category using a sampled caption list of this category from the training set as reference. By default, each image is captioned with up to 75 words, which are encoded into a 128-token sequence using the T5 text encoder [35] (XL for ImageNet-256 and XXL for ImageNet-512 experiments). Please see Appendix D for more details.\nTokenization & generation. By default, all TexTok experiments employ ViT-Base for both tokenizer and detokenizer, each comprising 12 layers with a hidden size of 768 and 12 attention heads (~176M parameters). For the GAN loss, we follow [47] and use the StyleGAN discriminator [19](~24M parameters). Unless otherwise specified, the image token channel dimension in TexTok is set to d = 8.\nWe use Diffusion Transformer (DiT) [32] as our default generator due to its effectiveness and flexibility of handling 1D tokens. We use a DiT patch size of 1 for all TexTok generation experiments, and by default, we train DiT for 350 epochs. Specifically, for class-conditional generation, we use the original DiT architecture. For text-to-image generation, referring to [5], we modify DiT architecture by adding an additional multi-head cross-attention layer following the multi-head self-attention layer in the DiT block to accept text embeddings. We refer to this architecture as \u201cDiT-T2I\u201d.\n4.2. Experiment Setup\nModel variants. We compare two setups to demonstrate the effectiveness of using text conditioning: TexTok incorporates text tokens in both the tokenizer and detokenizer, corresponding to the architecture shown in Figure 2. In contrast, Baseline (w/o text) does not condition on text tokens in both the tokenizer and detokenizer. For each image, we tokenize it into \"#tokens\" number of latent tokens and train the generator to generate these tokens.\nEvaluation protocol. To evaluate reconstruction performance of the tokenizer, we report reconstruction Frechet inception distance (rFID) [17], reconstruction inception score (rIS) [39], peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and learned perceptual image patch similarity (LPIPS) [52] on 50K samples from ImageNet training set. To evaluate class-conditional generation performance, we report generation Frechet inception distance (gFID) [17], generation inception score (gIS) [39], precision and recall [24] following the evaluation protocol and suite provided by ADM [9]. To evaluate text-to-image generation performance, we report FID and CLIP Score [16] on 50K samples from ImageNet validation set.\n4.3. Effectiveness of Text Conditioning\nWe begin by evaluating the effectiveness of text conditioning in image tokenization and generation. We compare our method, TexTok, with a Baseline (w/o text) that uses the same settings but excludes text conditioning, on ImageNet at resolutions of 256\u00d7256 and 512\u00d7512. We experiment with varying numbers of tokens, presenting the quantitative results in Table 1 and visualizing the relative improvement in rFID in Figure 3.\nOn ImageNet 256\u00d7256, across all settings, TexTok significantly enhances both reconstruction and generation performance. Specifically, TexTok achieves 37.2%, 25.0%, 30.2%, 24.2% improvements in rFID using 32, 64, 128, and 256 tokens respectively, which consistently translates to 28.6%, 12.7%, 13.8%, and 7.9% improvements in gFID. Notably, the fewer tokens used, the higher the gains from text conditioning. As shown in Figure 2a, TexTok can achieve similar rFID using half the number of tokens compared to the baseline (2\u00d7 compression rate). We note that our Baseline (w/o text) is highly competitive. As shown in Table 1(a), with 8\u00d7 fewer number of tokens, Baseline (w/o text) outperforms the widely used SD-VAE tokenizer [37] in both reconstruction and generation.\nOn higher resolution images, i.e., ImageNet 512\u00d7512, TexTok exhibits stronger efficacy. As shown in Table 1 and Figure 2b, TexTok achieves more significant improvement in the reconstruction quality and enables higher compression rates under this high-resolution setting. Specficially, it achieves 69.7%, 68.4%, 30.2%, and 24.2% improvements in rFID and 60.8%, 54.5%, 13.2% and 8.6% improvements in gFID, across 32, 64, 128 and 256 tokens respectively. As shown in Figure 2b, TexTok achieves similar rFID to the baseline using only 1/4 of the token number (4\u00d7 compression rate).\nFinally, the qualitative results in Figure 1 across varying token counts show that TexTok significantly enhances reconstruction quality, particularly for text within images and specific visual details, such as car wheels and beaks. This indicates that TexTok encodes finer visual details using the same number of tokens.\n4.4. System-level Image Generation Comparison\nWe experiment with image generation using TexTok as the tokenizer and adopt a vanilla DiT image generator [32] (denoted by TexTok + DiT), to study how this system performs against other leading image generation systems. We evaluate on class-conditional ImageNet 256\u00d7256 and 512\u00d7512 settings with varying number of tokens (compression rates).\nOn ImageNet 256\u00d7256 class-conditional image generation, as shown in Table 2(a), our TexTok-256 + DiT-XL achieves an FID of 1.46, surpassing previous state-of-the-art systems, even though using a simpler, vanilla DiT as the image generator. As we reduce the number of tokens and increase image compression rate, TexTok + DiT maintains generation performance. Notably, TexTok-64 + DiT-XL, where the diffusion transformer generates only 64 image tokens, outperforms the original DiT-XL/2, which uses 256 tokens after patchification in the diffusion transformer.\nOn higher resolution images, i.e., ImageNet 512\u00d7512, as shown in Table 2(b), TexTok-256 + DiT-XL also achieves state-of-the-art 1.62 gFID compared with previous methods, using only 256 image tokens. On the most compressed side, TexTok-32 + DiT-XL only uses 32 tokens yet achieves better generation performance than the original DiT that uses 1024 tokens after patchification.\nOur system not only achieves superior generation performance, but is also very efficient given its great compression rates. We plot in Figure 4a the speed v.s. performance tradeoffs of TexTok + DiT-XL compared to the original DiT on ImageNet 256\u00d7256. Simply replacing the tokenizer in DiT with TexTok can achieve a 14.3\u00d7 speedup with better FID, or 34.3% FID improvement with similar inference time. This verifies the effectiveness and efficiency of TexTok. This improved speed/performance tradeoff is further reflected on ImageNet 512\u00d7512 (Figure 4b), where we demonstrate that simply replacing the tokenizer in DiT with TexTok variants, it achieves a 93.5\u00d7 speedup with better FID using 32 tokens, or 46.7% FID improvement with 3.7\u00d7 less inference time using 256 tokens. This shows that as image resolution increases, providing the tokenization process with explicit text semantics yields greater improvements in generation performance and inference speedup.\nQualitative samples in Figure 6 demonstrate that TexTok enables class-conditional generation of semantically rich images with fine-grained details. More qualitative samples can be found in Appendix E.\n4.5. Text-to-Image Generation\nWe now demonstrate TexTok's superiority on text-to-image generation. We use the same VLM-generated captions on ImageNet 256\u00d7256 with our modified DiT-T2I architecture (detailed in Section 4.1). During training, the tokenizer and generator share the same text embeddings extracted by the T5 text encoder. During inference, we generate images condition on captions from ImageNet validation set. We calculate FID between these generated images and the original ImageNet validation set. As shown in Table 3, compared with Baseline (w/o text), TexTok consistently and significantly improves text-to-image generation, across varying numbers of image tokens. Since text captions are already available for text-to-image tasks and the tokenizer can directly use the same text embeddings used in the generator, TexTok's performance boost comes at no additional cost for captioning and text embedding extraction.\nQualitative samples in Figure 5 show that TexTok's generation is more realistic and follows the prompts better. More qualitative samples can be found in Appendix E.\n4.6. Tokenization/Generation Inference Efficiency\nWe have demonstrated that TexTok significantly enhances reconstruction, class-conditional generation, and text-to-image generation quality. In text-to-image tasks, our text conditioning incurs no additional cost for text embedding extraction, as text embeddings are also used as conditioning in generation. For other tasks, it introduces minimal computational overhead to generate text embeddings and use them during tokenization. As shown in Table 5, this overhead is negligible (~0.01 s/img). More importantly, the resulting reduction in generation computational cost compensates for this small increase, as evidenced by the comparison of computational costs between SD-VAE, Baseline (w/o text) and TexTok in Table 5 and the speedup results in Figure 4.\n4.7. Ablation Studies\nWe ablate TexTok to analyze the contribution of our design choices. We use the following default settings: TexTok-128, Base model size, and T5-XL text encoder. Captions are 75 words long and applied to both the tokenizer and detokenizer using in-context conditioning.\nAmount of text conditioning. In Table 4a, we ablate various types of class/text conditioning: (1) a learnable class embedding based on the class category, (2) text embeddings from a short text template with class names, (3) text embeddings from 25-word captions, and (4) (ours) text embeddings from 75-word captions. Our results show that more descriptive text conditioning improves performance.\nT5 text encoder size. In Table 4b, we study the effect of the text encoder model size. We find that a larger encoder leads to better reconstruction quality. We use T5-XL as the default setting on ImageNet-256 for its efficiency.\nConditioning architecture. Another design choice is how we inject text into the tokenizer and detokenizer. In Table 4c, we find that in-context conditioning (concatenating text embeddings with other input tokens and feeding them into the self-attention layers) outperforms adding an additional multi-head cross-attention layer in each ViT block.\nConditioning location. In Table 4d, we ablate the locations for text conditioning injection and find that applying it to both the tokenizer and detokenizer yields the best results.\nTexTok model size. In Table 4e, we investigate the influence of TexTok model size. We find using TexTok-Base performs much better than TexTok-Small, but increasing the model size further provides marginal improvements. Hence, we choose TexTok-Base as our default model size."}, {"title": "5. Conclusion", "content": "We present Text-Conditioned Image Tokenization (TexTok), a framework that leverages descriptive text captions to provide high-level semantics, allowing tokenization to focus on encoding fine-grained visual details into latent tokens. TexTok significantly improves reconstruction and generation performance, achieving state-of-the-art results on ImageNet with high computational efficiency. By mitigating the trade-off between reconstruction quality and compression rates, TexTok enables more efficient image generation."}, {"title": "A. Effectiveness of TexTok on Discrete Tokens", "content": "In the main paper, we demonstrate that TexTok works well with continuous tokens. In this section, we further validate the effectiveness of TexTok with Vector-Quantized (VQ) discrete tokens. We use a codebook size of 4096. As shown in Table 6, TexTok consistently delivers significant improvements over Baseline (w/o text) in reconstruction performance. As the number of tokens decreases, the performance gains are more pronounced. These results verify the effectiveness of TexTok on discrete tokens, highlighting its versatility as a universal tokenization framework. This inherent compatibility with both continuous and discrete tokens allows TexTok to seamlessly integrate with a wide range of generative models, including diffusion models, autoregressive models, and others."}, {"title": "B. Additional Training Analysis", "content": "In Figure 7, we further provide the training reconstruction FID comparison (evaluated on 10K samples) of TexTok-32 v.s. Baseline-32 (w/o text) on ImageNet 512\u00d7512. From the figure, it is clear that TexTok training is more efficient and effective, achieving faster convergence and better reconstruction quality."}, {"title": "C. Additional Implementation Details", "content": "We provide detailed default training hyperparameters for TexTok-N as listed below:\n\u2022 ViT encoder/decoder hidden size: 768.\n\u2022 ViT encoder/decoder number of layers: 12.\n\u2022 ViT encoder/decoder number of heads: 12.\n\u2022 ViT encoder/decoder MLP dimensions: 3072.\n\u2022 ViT patch size: 8 for 256 \u00d7 256 image resolution and 16 for 512 x 512.\n\u2022 Discriminator base channels: 128.\n\u2022 Discriminator channel multipliers: 1, 2, 4, 4, 4, 4 for 256 \u00d7 256 image resolution, and 0.5, 1, 2, 4, 4, 4, 4 for 512 x 512.\n\u2022 Discriminator starting iterations: 80,000.\n\u2022 Latent shape: N \u00d7 8.\n\u2022 Reconstruction loss weight: 1.0.\n\u2022 Generator loss type: Non-saturating.\n\u2022 Generator adversarial loss weight: 0.1.\n\u2022 Discriminator gradient penalty: r1 with cost 10.\n\u2022 Perceptual loss weight: 0.1.\n\u2022 LeCAM weight: 0.0001.\n\u2022 Peak learning rate: 10-4.\n\u2022 Learning rate schedule: linear warm up and cosine decay.\n\u2022 Optimizer: Adam with \u03b2\u2081 = 0 and \u03b2\u2082 = 0.99.\n\u2022 EMA model decay rate: 0.999.\n\u2022 Training epochs: 270.\n\u2022 Batch size: 256.\nWe provide detailed default training and evaluation hyperparameters for DiT as listed below:\n\u2022 Patch size: 1.\n\u2022 Peak learning rate: 5 \u00d7 10-4.\n\u2022 Learning rate schedule: linear warm up and cosine decay.\n\u2022 Optimizer: AdamW with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.99, and 0.01 weight decay.\n\u2022 Diffusion steps: 1000.\n\u2022 Noise schedule: Linear.\n\u2022 Diffusion \u03b2\u2080 = 0.0001, \u03b2\u2081\u2080\u2080\u2080 = 0.02.\n\u2022 Training objective: v-prediction.\n\u2022 Sampler: DDIM.\n\u2022 Sampling steps: 250.\n\u2022 Training epochs: 350.\n\u2022 Batch size: 1024.\nFor classifier-free guidance, we adopt the guidance schedule from [12] following prior arts [13, 49]. For the model configurations and other implementation details, please refer to the original DiT paper [32]."}]}