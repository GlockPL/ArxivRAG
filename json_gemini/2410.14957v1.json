{"title": "OFFLINE-TO-ONLINE REINFORCEMENT LEARNING FOR IMAGE-BASED GRASPING WITH SCARCE DEMONSTRATIONS", "authors": ["Bryan Chan", "Anson Leung", "James Bergstra"], "abstract": "Offline-to-online reinforcement learning (O2O RL) aims to obtain a continually improving policy as it interacts with the environment, while ensuring the initial policy behaviour is satisficing. This satisficing behaviour is necessary for robotic manipulation where random exploration can be costly due to catastrophic failures and time. O2O RL is especially compelling when we can only obtain a scarce amount of (potentially suboptimal) demonstrations\u2014a scenario where be- havioural cloning (BC) is known to suffer from distribution shift. Previous works have outlined the challenges in applying O2O RL algorithms under the image- based environments. In this work, we propose a novel O2O RL algorithm that can learn in a real-life image-based robotic vacuum grasping task with a small number of demonstrations where BC fails majority of the time. The proposed algorithm re- places the target network in off-policy actor-critic algorithms with a regularization technique inspired by neural tangent kernel. We demonstrate that the proposed al- gorithm can reach above 90% success rate in under two hours of interaction time, with only 50 human demonstrations, while BC and existing commonly-used RL algorithms fail to achieve similar performance.", "sections": [{"title": "INTRODUCTION", "content": "Imitation learning (IL) has been gaining attention over recent years due to the development of vi- sion language models (Padalkar et al., 2023; Zhao et al.; Haldar et al., 2024). However, while these approaches are more robust to various manipulation tasks, the training requires abundant demon- stration data. For niche robotic applications where data is scarce, supervised IL methods such as behavioural cloning (BC) are known to suffer from distribution shift (Ross et al., 2011; Rajaraman et al., 2020) and more generally cannot perform better than the demonstrator (Xu et al., 2020; Ren et al., 2021). Alternatively, we focus on offline-to-online reinforcement learning (O2O RL), which is a two-step algorithm that first pretrains a policy followed by continual improvement with online interactions (Song et al., 2023; Nakamoto et al., 2023; Tan & Xu, 2024).\nThe first step, known as offline RL (Levine et al., 2020), has made tremendous progresses on state- based environments (Kumar et al., 2020; Yu et al., 2020; Fujimoto & Gu, 2021; Tangri et al., 2024), but it has recently been observed that the transfer of algorithms to image-based environments can be challenging (Lu et al., 2023; Rafailov et al., 2024) (we also provide an example in Appendix B). Naturally, some have investigated the potential benefits of pretrained vision backbones, pretraining self-supervised objectives, and data-augmentation techniques (Hansen et al., 2023; Li et al., 2022; Hu et al., 2023). These directions are also investigated in the online RL setting (Sutton, 2018) concurrently, which has seen more successes with image-based domains in both simulated and real- life environments (Singh et al., 2020; Yarats et al., 2022; Wang et al., 2022; Luo et al., 2024). Nevertheless, these algorithms still leverage large amount of data and may require a long-duration of"}, {"title": "PRELIMINARIES AND BACKGROUND", "content": ""}, {"title": "PROBLEM FORMULATION", "content": "A reinforcement learning (RL) problem can be formulated as an infinite-horizon Markov decision process (MDP) $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, r, P, \\rho_0, \\gamma)$, where $\\mathcal{S}$ and $\\mathcal{A}$ are re- spectively the state and action spaces, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$ is the reward function, $P \\in \\Delta^{\\mathcal{S}\\times\\mathcal{A}}$ is the transition distribution, $\\rho_0 \\in \\Delta^{\\mathcal{S}}$ is the initial state distribution, and $\\gamma \\in [0,1)$ is the discount factor. A policy $\\pi\\in \\Delta^{\\mathcal{A}^{\\mathcal{S}}}$ can interact with the MDP $\\mathcal{M}$ through taking actions, yielding an infinite- length random trajectory $\\mathcal{T} = (s_0,a_0,r_0, s_1, a_1,r_1,\\dots)$, where $s_0 \\sim \\rho_0$, $a_t \\sim \\pi(S_t)$, $S_{t+1} \\sim P(s_t, a_t)$, $r_t = r(s_0, a_0)$. The return for each trajectory is $G = \\sum_{t=0}^{\\infty}\\gamma^t r_t \\leq 1$. We further define the value function and Q-function respectively to be $V^{\\pi}(s) := \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t|s_0 = s]$ and $Q(s,a) := \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r_t|s_0 = s, a_t = a]$. The goal is to find a policy $\\pi^*$ that maximizes the expected cumulative sum of discounted rewards for all states $s \\in \\mathcal{S}$, i.e. $\\pi^*(s) = arg \\max_{\\pi} V^{\\pi}(s)."}, {"title": "Offline-to-online Reinforcement Learning", "content": "RL algorithms require exploration which is often prohibitively long and expensive for robotic manipulation. To this end, we consider offline-to- online (O2O) RL, a setting where we are given an offline dataset that is generated by a potentially suboptimal policy. Generally, we can decompose O2O RL into two phases: (1) pretraining an offline agent using offline data, and (2) continually training the resulting agent through online interactions. Our goal is to leverage this offline dataset and a limited number of online interactions to train an agent that can successfully complete the task. We consider the setting where we also include offline data during the online interaction (Tan & Xu, 2024; Huang et al., 2024).\nIn this work, we assume access to $N$ trajectories truncated at $T$ timesteps $\\mathcal{D}_{off} = \\{(s^{(m)}_0, a^{(m)}_0, r^{(m)}_0, ..., s^{(m)}_T, a^{(m)}_T, r^{(m)}_T,s^{(m)}_{T})\\}_{m=1}^{N} \\subset \\mathcal{S}^{\\mathcal{T}}$ as the offline dataset, and denote $\\mathcal{D}_{on}$ as the interaction buffer for data collected during the online phase. We refer $\\mathcal{D}$ as the total buffer that samples from both $\\mathcal{D}_{off}$ and $\\mathcal{D}_{on}$ with equal probability, a technique known as symmetric sampling (Ball et al., 2023). We highlight that offline datasets with truncated trajectories are natural in the"}, {"title": "CONSERVATIVE Q-LEARNING", "content": "We build our algorithm on conservative Q-learning (CQL) (Kumar et al., 2020). CQL imposes a pessimistic Q-value regularizer on out-of-distribution (OOD) actions to mitigate unrealistically high-values on unseen data. Suppose the Q-funciton $Q_{\\theta}$ is parameterized by $\\theta$, the CQL training objective is defined by:\n$\\mathcal{L}_{CQL}(\\theta) := \\alpha (\\mathbb{E}_{\\mathcal{D},\\mu} [Q_{\\theta}(s, a')] - \\mathbb{E}_{\\mathcal{D}} [Q_{\\theta}(s, a)]) + \\frac{1}{2} \\mathbb{E}_{\\mathcal{D}} [(Q_{\\theta}(s, a) - B^n Q(s,a))^2],$ where $\\alpha$ is a hyperparameter controlling strength of the pessimistic regularizer, $\\mu$ is an arbirary policy, $a' \\sim \\mu(s)$, $a \\sim \\mathcal{D}$, and $B^n Q(s, a) := r_n(s, a) + \\gamma \\mathbb{E}_{\\pi} [Q(s', a')]$ is the $N$-step empirical Bellman backup operator applied to a delayed target Q-function $Q$, writing $B_1 = B^n$ for conciseness. The first term is the pessimistic Q-value regularization and the second term is the standard N-step temporal-difference (TD) learning objective (Sutton, 2018). There can be multiple implementations of CQL. Common implementation chooses $\\mu = \\pi$ and builds on top of soft actor-critic (Singh et al., 2020; Haarnoja et al., 2018). Alternatively, crossQ (Bhatt et al., 2024) has demonstrated that we can replace the delayed target Q-function with the current Q-function by properly leveraging batch normalization (Ioffe, 2015). Calibrated Q-learning (Cal-QL) (Nakamoto et al., 2023) further augments the regularizer to only penalize OOD actions when their corresponding Q-values exceed the value induced by the dataset $\\mathcal{D}$."}, {"title": "STABILIZING Q-LEARNING VIA DECOUPLING LATENT REPRESENTATIONS", "content": "It is desirable for RL algorithms to be stable and sample-efficient in robotic manipulation tasks-we propose an algorithm that encourages both properties. To address the former, we leverage ideas from the neural tangent kernel (NTK) literature and propose a regularizer to decouple representation during Q-learning. For the latter we leverage symmetric sampling from reinforcement learning with pretrained data (RLPD) (Ball et al., 2023) to encourage the agent to learn from positive examples. We build our algorithm on conservative Q-learning (CQL) (Kumar et al., 2020) to enable offline training and further include our proposed regularizer described in this section.\nQ-learning algorithms are known to diverge (Baird, 1995) and suffer from the overestimation prob- lem (Hasselt, 2010) even with double-Q learning (Yue et al., 2023; Ablett et al., 2024). Recent work leverages NTK to analyze the learning dynamics of Q-learning (Yue et al., 2023; Kumar et al., 2022; Yang et al., 2022; He et al., 2024; Ma et al., 2023; Tang & Berseth, 2024)-the Q-function of a state-action pair $(s', a') \\in \\mathcal{S} \\times \\mathcal{A}$ can be influenced by the Q-learning update of another state-action pair $(s, a) \\in \\mathcal{S} \\times \\mathcal{A}$. Let $\\theta$ and $\\theta'$ be the parameters of the Q-function before and after a stochastic gradient descent (SGD) step respectively, and define $\\kappa_{\\theta}(s,a, s', a') = \\nabla_{\\theta} Q_{\\theta}(s', a')^T \\nabla_{\\theta}Q_{\\theta}(s,a)$. By performing SGD on the TD learning objective (second term in equation 1) with state-action pair (s, a), we can write the Q-value of another state-action pair (s', a') after the gradient update as\n$Q_{\\theta'}(s', a') = Q_{\\theta}(s', a') + \\kappa_{\\theta}(s,a, s', a') (Q_{\\theta}(s, a) - B^n Q(s,a)) + O(||\\theta' - \\theta||^2).$\nHere, $\\kappa_{\\theta}$ is known as the neural tangent kernel (Jacot et al., 2018) and the last term approaches to zero as the dimensionality approaches to infinity, a phenomenon known as lazy training (Chizat et al., 2019). Intuitively, a small magnitude in $\\kappa_{\\theta}(s,a, s', a')$ will result in $Q_{\\theta'}(s', a')$ being less influenced by the update induced by (s, a).\nSuppose now the Q-function is parameterized as a neural network $Q_{\\theta}(s, a) := w^T\\Phi(s, a)$ (i.e. last layer is a linear layer), where $\\theta = [w, \\Phi]$, $w$ is the parameters of the last layer, and $\\Phi(s, a)$ is the output of the second-last layer, we can view $\\Phi(s, a)$ as a representation layer. Thus, freezing the representation layer during Q-learning update, we can write equation 2 as\n$Q_{w'}(s', a') = Q_{w}(s', a') + \\Phi(s', a')^T\\Phi(s, a) (Q_{w}(s, a) - B^n Q(s, a)) + O(||w' - w||^2).$\nKumar et al. (2022) is among the first to propose regularizing the representation layers of the Q- function with $R(\\Phi) = \\mathbb{E}_{\\mathcal{D}} [\\Phi(s,a) \\Phi(s', a')]$, where $(s, a, s', a') \\sim \\mathcal{D}$ is the current and next"}, {"title": "", "content": "state-action pairs from the buffer. Follow-up works modify the network architecture to include various normalization layers (Yang et al., 2022; Yue et al., 2023) and different regularizers (He et al., 2024; Ma et al., 2023; Tang & Berseth, 2024).\nAlternative approaches to mitigate this Q-divergence include using target network (Mnih et al., 2013) and double Q-learning (Hasselt, 2010). The former is introduced to reduce the influence of rapid changes of nearby state-action pairs during bootstrapping, which can also be addressed by decor- relating their corresponding features. Consequently, we remove the target network and introduce a regularizer that aims to decouple the representations between different state-action pairs. Specifi- cally, our regularizer is defined to be\n$\\mathcal{L}_{reg}(\\Phi) := \\mathbb{E}_{s \\sim \\mathcal{D}, s' \\sim \\mathcal{D}} [(\\Phi(s, a_U)^T \\Phi(s', a_{\\pi}))^2],$ where s, s' are states sampled independently from the buffer $\\mathcal{D}$, $\\Phi(s, a)$ is the latent representation from the second-last layer of the model, $a_U \\sim \\mathcal{U}(\\mathcal{A})$ is a uniformly sampled action, and $a_{\\pi} \\sim \\pi(s')$ is the next action sampled following the current policy. In other words, we are approximately orthogonalizing the latent representations through minimizing the magnitude of their dot products. Intuitively, equation 4 decorrelates the representations of any two states regardless of the current action, thereby minimizing the influence on other Q-values due to the current update. We note that in previous work the suggested regularizer is simply taking the dot product between the latent representations (Kumar et al., 2022; Ma et al., 2023) which can cause the NTK to be negative\u2014a Q-function update of a particular state-action pair may unlearn the Q-values of another state-action pair. Secondly our regularizer applies on not only the consecutive state-action pairs which decouples more diverse state-action pairs. Finally, the complete objective for updating the Q-function is\n$\\mathcal{L}_{Q}(\\theta) := \\mathcal{L}_{CQL}(\\theta) + \\beta \\mathcal{L}_{reg}(\\Phi),$ where $\\beta > 0$ is a coefficient that controls the strength of the decorrelation."}, {"title": "EXPERIMENTS", "content": "We aim to answer the following questions with empirical experiments: (RQ1) Can we perform offline RL using our proposed method? How does it compare with behavioural cloning (BC)? (RQ2) Can our proposed method enable O2O RL on real-life robotic manipulation? What about existing commonly-used RL algorithms? (RQ3) Can O2O RL eventually exceed imitation learning approaches with similar amount of data? (RQ4) How important is it to pre- train a vision backbone? (RQ5) Can the proposed NTK regularizer alleviate Q-divergence? We provide extra ab- lation results and our method's zero-shot generalization capability in Appendix C.\nEnvironment Setup. We conduct our experiments on a real-life image-based grasping task (Figure 1). The task consists of controlling a UR10e arm to grasp an item in- side a bin. The agent observes a 64 \u00d7 64 RGB image with an egocentric view, the proprioceptive information including the pose and the speed, and the vacuum pres- sure reading. The agent controls the arm at 10Hz fre- quency through Cartesian velocity control with vacuum action-a 7-dimensional action space. The agent can at- tempt a grasp for six seconds. The agent receives a +1 reward upon grasping the item and moving it above a certain height threshold and a +0 reward otherwise. In the former, there is a randomized-drop mechanism to ran- domize the item location, otherwise a human intervenes and changes the item location. The attempts are fixed at six seconds (i.e. episode does not terminate upon suc- cess) unless the arm has experienced a protective stop (P-"}, {"title": "MAIN RESULTS", "content": "We first address RQ1 by training a BC policy and offline RL policies. All RL algorithms use a pretrained image encoder to accelerate training in wall time while BC is trained end-to-end. We then evaluate each policy on 50 grasp attempts. Figure 2, left, shows that BC agent can achieve 34% success rate. On the other hand, offline RL with Simplified Q and DR3 can achieve 24% and 16% success rates respectively. Although these three policies fail to pick most of the time, behaviourally the policies reach into the bin 100% of the time, demonstrating satisficing behaviours. On the other hand, offline RL policies that are trained using CrossQ, SAC, and LN achieve 0% success rate and totally fail to learn similar behaviours as the satisficing policies (we omit CrossQ, SAC, and LN results in Figure 2, left).\nWe then deploy these offline-trained RL policies to the environment for further training with addi- tional online interactions (RQ2). CrossQ, SAC, and LN are unable to grasp the item at all, while Simplified Q and DR3 can immediately grasp the item. Notably, the latter two can also continually learn during the online phase. From Figure 3 we observe that Simplified Q is generally less suscepti- ble to randomness and consistently achieve higher success rate over 200 online interaction episodes when compared to DR3. Simplified Q is also safer in the sense that it experiences less P-stops com- pared to DR3. Inspecting the learning curves in Figure 4, we can clearly see that Simplified Q has a"}, {"title": "THE IMPORTANCE OF PRETRAINED IMAGE ENCODER", "content": "One may argue that leveraging the pretrained image encoder might have enabled the sample effi- ciency of Simplfied Q (RQ4). To this end we also train an O2O RL agent end-to-end (E2E) using Simplfied Q. We also include a frozen randomly initialzed image encoder as a baseline. Figure 5, middle, demonstrates that the E2E RL agent can achieve similar performance as using a pretrained image encoder, furthermore both RL agents have learned to pick with limited success after the of- fline phase. On the other hand, while the agent using a frozen randomly-initialized encoder can pick the item up sometimes, it cannot further improve as it gathered more transitions. This suggests that leveraging a pretrained image encoder does improve upon using a frozen randomly-initialized network, but does not provide visible performance improvement over training E2E. In fact we have"}, {"title": "LATENT FEATURE REPRESENTATION SIMILARITY AND Q-VALUE ESTIMATES", "content": "We now analyze the impact of our proposed regularizer. Our goal is to decorrelate the latent feature representation of different state-action pairs, which is measure by the dot product of the features $\\Phi(s, a) \\Phi(s', a')$, where $(s, a)$ and $(s', a')$ are independently drawn from $\\mathcal{D}$. We sample 512 ran- dom state-action pairs from a buffer of random trajectories and visualize their similarity in the feature space induced by each algorithm during offline RL and online RL. Figure 8a illustrates that using"}, {"title": "RELATED WORK", "content": "Reinforcement learning (RL) algorithms require extensive exploration to learn a performant policy, which is an undesirable property for robotic manipulation. Offline-to-online (O2O) RL is an alterna-"}, {"title": "CONCLUSION", "content": "In this work we introduced a novel regularizer inspired by the neural tangent kernel (NTK) lit- erature that can alleviate Q-value divergence. We showed that this NTK regularizer can indeed decorrelate the latent representation of different state-action pairs, as well as maintaining reasonable Q-value estimates. Consequently we removed the target network altogether and replaced it with this NTK regularizer, resulting in our offline-to-online reinforcement learning algorithm, Simplified Q. We conducted experiments on a real-life image-based robotic manipulation task, showing that Simplified Q could achieve satisficing grasping behaviour immediately after the offline RL phase and further achieve above 90% grasp success rate under two hours of interaction time. We further demonstrated that Simplified Q could outperform existing reinforcement learning algorithms and behavioural cloning with similar amount of total data. These results suggest that we can simply use reinforcement learning algorithms for both offline and online training once we mitigate Q-value divergence, and we should further reconsider whether we truly need large amount of demonstra- tions for learning near-optimal policies. Additional discussions on limitations and future work are in Appendix A."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "In the future we aim to demonstrate the robustness of Simplified Q through conducting experiments in other manipulation tasks and other domains, particularly longer-horizon tasks. Secondly, running reinforcement learning algorithms in real life still requires human intervention to reset the environ- ment that may discourage practitioners from applying these algorithms. This limitation might be addressed through reset-free reinforcement learning (Gupta et al., 2021). Furthermore, the current learning updates are performed in a serial manner, a natural direction is to parallelize this such that the policy execution and policy update are done asynchronously (Mahmood et al., 2018; Wang et al., 2023).\nOur work also takes advantage of using offline data that includes successful attempts to workaround the exploration problem. One question is to investigate whether we can include play data (Ablett et al., 2023) or data collected from other tasks (Chan et al., 2023)-leveraging multitask data can potentially enable general-purpose manipulation. It is still of interest to perform efficient and safe exploration-for example using vision-language models and/or constrained reinforcement learning algorithms to impose safe policy behaviour and goal generation (Garcia & Fern\u00e1ndez, 2015; Yang et al., 2021).\nThere remains a big gap between the theoretical understanding and the empirical results of Simpli- fied Q. Particularly we hope to show that this regularization can bound the Q-estimates to be within realizable returns with high probability, and guarantee convergence of the true Q-function. This may involve analyzing the learning dynamics of temporal difference learning with our regularizer (Kumar et al., 2022; Yue et al., 2023). It will also be interesting to analyze the differences in the learning dynamics with image-based and state-based observations (Pope et al., 2021). An alterna- tive theoretical question is to investigate whether Q-overestimation is truly problematic for policy learning (Mei et al., 2023)."}, {"title": "THE CHALLENGES IN VISUAL-BASED OFFLINE RL", "content": "In this section we reaffirm the difficulty of offline RL in image-based tasks, which is first observed in Lu et al. (2023) and Rafailov et al. (2024). We conduct an experiment on a real-life 2D reach- ing task using the UR10e arm. We define a state-based observation variant and an image-based observation variant to compare. The former accepts the same proprioceptive information speci- fied in Section 4, but replacing the image with a delta target position. The latter leverages the same information, but the image is an arrow that indicates the location of the target relative to the"}, {"title": "EXTRA EXPERIMENTAL RESULTS", "content": ""}, {"title": "TRAJECTORIES OVER TRAINING", "content": "Here we provide few online interaction trajectories for each RL algorithm during training (Fig- ure 10). Our Simplified Q agent can consistently go inside the bin, towards the item, and attempt"}, {"title": "ABLATIONS", "content": "We investigate the importance of adding successful episodes to the demonstration buffer over time and symmetric sampling introduced by RLPD (Ball et al., 2023). We conduct an experiment where we run our approach with and without self-imitation learning (SIL), and without symmetric sampling (SS). Excluding any of SIL or SS resulted in an increase on the P-stop rate which suggests that our proposed technique can be safer as it enforces the agent to sample more positive samples as training progresses (Table 1a). We observe that the overall success rate is higher with our method, up to 8% improvement. We also evaluate the importance of N-step temporal-difference learning, with $N = \\{1,3,5\\}$ (Table 1b). Similar to Seo et al. (2024) we found that setting $N = 3$ performs the best-notably when $N = 1$ the agent performance is the worst. It is likely due to higher bias compared to larger N.\nFinally, we conduct a sensitivity analysis on the coefficient of our proposed regularizer $\\beta \\epsilon \\{0.0, 0.1, 0.2, 0.4, 1.0\\}$. Table 1c shows that our proposed regularizer is generally robust with coef- ficients $\\beta \\epsilon [0.1,0.4]$. Notably we found that when $\\beta = 0.1$ both its cumulative success rate and P-stop rate to be superior than our chosen default, $\\beta = 0.2$. Behaviourally, when $\\beta = 0.0$ or $\\beta = 1.0$ we observe that the policy can only pick from a very small region of item locations. The latter is worse as it can only pick towards the center of the bin."}, {"title": "ZERO-SHOT GENERALIZATION", "content": "We now investigate the robustness of the RL policy trained with our proposed method without further parameter updates. We evaluate the agent on two other item types (Figure 11, right), one with different colour and one with different shape and rigidity. We also evaluate the agent on other scenarios where there are three items in the bin simultaneously and where the lighting condition changes. We evaluate each scenario for 50 grasp attempts. We emphasize that the agent has only seen a single item over the training runs, making these scenarios totally out-of-distribution (OOD).\nOur result is shown in Figure 11, left, and we can see that the agent does degrade in performance under OOD scenarios, but we observe that the success rate is around 70% on the different coloured item, while achieving around 50% success rate on the item with different shape and rigidity. The latter fails more frequently as the item is significantly taller, resulting in the agent P-stopping more frequently due to unseen item height. Furthermore, the agent also degrades in performance when there are multiple items in the bin, and we observe that the main faliure mode is when the gripper is in between two items at equidistance, which causes the policy to undercommit on one of the two items. This degradation is more significant when all the items are OOD. Finally, the modified lighting condition does cause a visible performance degradation on the policy, we suspect this is related to the light reflection on the item. Particularly, the policy cannot differentiate between the bottom of the bin and the reflection of the item, thereby neglecting the item completely."}, {"title": "HYPERPARAMETERS AND ALGORITHMIC DETAILS", "content": "We choose our hyperparameters based on experiments conducted on the real-life state-based reacher task. Specifically, Figure 14 demonstrates that enabling 3-step returns, offline RL training, and a larger offline buffer can greatly improve sample efficiency. The offline buffer in this case contains more than 2000 trajectories, including both failed and successful reach attempts. While we do not have a large offline buffer in practice, we apply a self-imitation learning technique is inspired by this experiment result where the offline buffer should become cover more successful episodes overtime.\nThe self-imitation technique is inspired by soft-Q imita- tion learning (Reddy et al., 2020) and RLPD (Ball et al., 2023), where the algorithm samples transitions symmet- rically from both the interaction buffer $\\mathcal{D}_{on}$ and the of- fline buffer $\\mathcal{D}_{off}$. However, symmetric sampling samples half of the transitions from the offline data $\\mathcal{D}_{off}$, which is undesirable when the average return induced by $\\mathcal{D}_{off}$ is lower than the current policy $\\pi$. We therefore include successful online interaction episodes into $\\mathcal{D}_{off}$ in addi- tion to the interaction buffer $\\mathcal{D}_{on}$, a technique inspired by self-imitation learning (Oh et al., 2018; Seo et al., 2024).\nThe consequence is twofold: (1) it allows the agent to see more diverse positive examples as it succeeds more, and (2) this results in the buffers being closer to on-policy data as the current policy becomes near optimal.\nWe choose the CQL regularizer coefficient to be $\\alpha = 1.0$ and the NTK regularizer coefficient to be $\\beta = 0.2$. We use Adam optimizer (Kingma, 2014) with learning rate 0.0003 for offline training and end-to-end online RL; we use a smaller learning rate 0.00005 for online RL with frozen image encoder as we found that using same learn- ing rate is less stable. The batch size is set to be 512 (i.e. sampling 256 from $\\mathcal{D}_{off}$ and 256 from $\\mathcal{D}_{on}$ during online phase), and we perform 60 gradient updates between every attempt for both the policy and Q-functions to avoid jerky motions. The models update at $\\geq 1$ update-to-data ratio-when P-stop occurs the ratio is higher due to shorter trajectory length.\nThe image encoder is a ResNet (He et al., 2016), followed by a 2-layer MLP. For the policy, the MLP uses tanh activation with 64 hidden units, whereas for the Q-function, the MLP uses ReLU activation with 2048 hidden units. Training with a frozen image encoder spends approximately 14"}]}