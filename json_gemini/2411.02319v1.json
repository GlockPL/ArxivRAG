{"title": "GenXD: GENERATING ANY 3D AND 4D SCENES", "authors": ["Yuyang Zhao", "Chung-Ching Lin", "Kevin Lin", "Zhiwen Yan", "Linjie Li", "Zhengyuan Yang", "Jianfeng Wang", "Gim Hee Lee", "Lijuan Wang"], "abstract": "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation. The dataset and code will be made publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Generating 2D visual content has achieved remarkable success with diffusion and autoregressive modeling, which have already been used in real-world applications, benefiting society. In addition to 2D generation, 3D content generation is also of vital importance, with applications in video games, visual effects, and wearable mixed reality devices. However, due to the complexity of 3D modeling and the limitations of 3D data, 3D content generation is still far from satisfactory and is attracting more attention. In this paper, we focus on the unified generation of 3D and 4D content. Specifically, static 3D content involves only spatial view changes, referred to as 3D generation in this paper. In contrast, dynamic 3D content includes movable objects within the scene, requiring the modeling of both spatial view and dynamic (temporal) changes, which we term 4D generation.\nMost previous works focus on 3D and 4D generation using synthetic object data. Synthetic object data are typically meshes, allowing researchers to render images and other 3D information (e.g., normals and depth) from any viewpoint. However, object generation is more beneficial to specialists than to the general public. In contrast, scene-level generation can help everyone enhance their images and videos with richer content. As a result, recent works have explored general 3D generation (both scene-level and object-level) in a single model, achieving impressive performance. Nonetheless, these works focus solely on static 3D generation, without addressing dynamics. In this paper, we propose a unified framework for general 3D and 4D generation, enabling the generation of images from different viewpoints and timesteps with any number of conditioning images (Fig. 1).\nThe first and foremost challenge in 4D generation is the lack of general 4D data. In this work, we propose CamVid-30K, which contains approximately 30K 4D data samples. 4D data require both multi-view spatial information and temporal dynamics, so we turn to video data to obtain the necessary 4D data. Specifically, we need two key attributes from the video: the camera pose for each frame and the presence of movable objects. To achieve this, we first estimate the possible movable objects in the video using a segmentation model and then estimate the camera pose using keypoints in the static parts of the scene. While successful camera pose estimation ensures multiple views, we also need to ensure that moving objects are present in the video, rather than purely static scenes. To address this, we propose an object motion field that leverages aligned depth to estimate true object movement in the 2D view. Based on the object motion field, we filtered out static scenes, resulting in approximately 30K videos with camera poses.\nIn addition, we propose a unified framework, GenXD, to handle 3D and 4D generation within a single model. While there are similarities between 3D and 4D data in terms of their representation of spatial information, they differ in how they capture temporal information. Therefore, 3D and 4D generation can complement each other through the disentanglement of spatial and temporal information (see Appendix. C for details). To achieve this, we combine both 3D and 4D data during model training. To disentangle the spatial and temporal information, we introduce multiview-temporal modules in GenXD. In each module, we use a-fusing to merge spatial and temporal information for 4D data, while removing temporal information for 3D data. Previous works typically use a fixed number of conditioning images (e.g., the first image). However, single-image conditioning can be more creative, whereas multi-image conditioning offers greater consistency. As a result, we implement masked latent conditioning in our diffusion model. By masking out the noise in the conditioning images, GenXD can support any number of input views without modifying the network. With high-quality 4D data and a 4D spatio-temporal generative model, GenXD achieves significant performance in both 3D and 4D generation using single or multiple input views. Our contributions are summarized as follows:"}, {"title": "2 RELATED WORK", "content": "3D Generation. Before the emergence of large-scale 3D data, early works distill the knowledge from 2D diffusion models for text- and image-based 3D generation. Later, with the development of 3D data, 3D generation has been mainly explored in two directions: multi-view priors and feed-forward models. Multi-view priors generate multi-view images and other features (e.g., normal maps and depths) based on camera embeddings, and then train a 3D representation using the generated samples or distill from generative priors. Feed-forward models directly predict NeRF, 3D Gaussians, or meshes from single or multi-view images. Compared to multi-view priors, feed-forward models are more efficient but produce lower quality results. In this paper, we follow the paradigm of multi-view priors.\n4D Generation. Similar to 3D generation, early 4D generation works distill 2D video generation models into 4D representation. Due to the variability and complexity of multi-view videos, these methods typically require long optimization time. Later, by leveraging animated 3D mesh data, researchers render multi-view videos and use them to train 4D diffusion models and feed-forward models. Although these models achieve good multi-view and video quality, they only focus on object-centric synthetic scenarios rather than entire scenes. This limitation is due to the lack of scene-level 4D data and the requirement for both multi-view static and dynamic information in these models. In this paper, we introduce a large-scale dataset of scene-level 4D data and address the challenge of general 4D generation.\nCamera-controlled Video Generation. In the real world, videos contain not only object motion but also camera movement. Therefore, controlling camera movement in video generation has also garnered attention in the community. MotionCtrl and CameraCtrl introduce a branch to encode camera information from multi-view 3D data and integrate it into a frozen video generation model. How-ever, due to the limitations of this integration approach, these methods cannot generate videos that align well with the camera pose. CamCo annotates some 4D data similar to ours and fine-tunes the entire video generation model using this data. However, due to limitations in camera pose quality and diversity, CamCo struggles to handle large camera movements."}, {"title": "3 CAMVID-30K", "content": "The lack of large-scale 4D scene data limits the development of dynamic 3D tasks, including but not limited to 4D generation, dynamic camera pose estimation, and controllable video generation. To address this, we introduce a high-quality 4D dataset in this paper. First, we estimate the camera poses using a Structure-from-Motion (SfM) based method, then filter out data without object movement using the proposed motion strength. The pipeline is illustrated in Fig. 2"}, {"title": "3.1 CAMERA POSE ESTIMATION", "content": "The camera pose estimation is based on SfM, which reconstructs 3D structure from their projections in a series of images. SfM involves three main steps: (1) feature detection and extraction, (2) feature matching and geometric verification, and (3) 3D reconstruction and camera pose estimation. In the second step, the matched features must be on the static part of the scene. Otherwise, object movement will be interpreted as camera movement during feature matching, which can impair the accuracy of camera pose estimations.\nTo address this, Particle-SfM separates moving objects from the static background using a motion segmentation module, and then performs SfM on the static part to estimate camera poses. However, it is extremely difficult to accurately detect moving pixels when the camera itself is moving, and we empirically observe that the motion segmentation module in lacks sufficient generalization, leading to false negatives and incorrect camera poses. To obtain accurate camera poses, it is essential to segment all moving pixels. In this case, a false positive error is more acceptable than a false negative. To achieve this, we use an instance segmentation model to greedily segment all pixels that might be moving. The instance segmentation model is far more generalizable than the motion segmentation module in, particularly on training categories. After segmenting the potentially moving pixels, we estimate the camera pose with Particle-SfM to obtain camera information and sparse point clouds (Fig. 2(a))."}, {"title": "3.2 OBJECT MOTION ESTIMATION", "content": "Unravel Camera and Object Motion. While instance segmentation can accurately separate objects from backgrounds, it cannot determine whether the object itself is moving, and static objects negatively impact motion learning. Thus, we introduce motion strength to identify true object motion and filter out videos with only static objects."}, {"title": "Object Motion Field.", "content": "With the aligned depth $d_{align}$, we can project dynamic objects in a frame into 3D space, providing a straightforward way to measure object motion. As shown in Fig. 2(b), if the object (e.g., the man in the green shirt) is moving, there will be displacement in the projected 3D point clouds. However, since SfM operates up to a scale, measuring motion directly in 3D space can lead to magnitude issues. Therefore, we project the dynamic objects into adjacent views and estimate the object motion field.\nSpecifically, we first need to find matching points in the 2D video. Instead of using dense representations like optical flow, we sample keypoints for each object instance and use video object segmentation and keypoint tracking in 2D videos to establish matching relationships. Each keypoint is then projected into adjacent frames. The keypoint $(u_i, v_i)^T$ in the i-th frame is first back-projected into world space to obtain the 3D keypoint $k_{pi}$:\n$k_{pi} = Z_i \\cdot K^{-1} . (U_i, V_i, 1)^T,$\nwhere $Z_i = d_{aligned} (U_i, v_i)$ is the depth value in the aligned dense depth map. Then the 3D keypoint is projected to j-th frame with the projection equation (Eq. 1) to obtain the 2D projected keypoint $(U_{ij}, V_{ij})^T$. Similar to optical flow, we represent the displacement of each 2D keypoint on the second camera view as object motion field:\n$(\\Delta U_{ij}, V_{ij})^T = ((U_j - U_{ij})/W, (V_j - V_{ij})/H)^T,$\nwhere H and W denotes image height and width."}, {"title": "4 GENXD", "content": "Since most scene-level 3D and 4D data are captured via videos, these data lack explicit represen-tations (e.g., meshes). Therefore, we adopt an approach that generates images aligned with spatial camera poses and temporal timesteps. Specifically, we incorporate the latent diffusion model into our framework, introducing additional multiview-temporal layers, including multiview-temporal ResBlocks and multiview-temporal transformers, to disentangle and fuse 3D and temporal information."}, {"title": "4.1 GENERATIVE MODEL", "content": "Mask Latent Conditioned Diffusion Model. Latent diffusion model (LDM) is leveraged in GenXD to generate images of different camera viewpoint and time together. LDM first encode an image/video into a latent code z with VAE and diffuse the latent with gaussian noise \u03f5 to obtain $z_t$. Then a denoising model $\u03f5_\u03b8(\u00b7)$ is leveraged to estimate the noise and reverse the diffusion process with conditions:\n$L_{LDM} := E_{x(x),\u20ac~N(0,1),t} [||e - Co (z_t, t, c)||_2],$\nwhere c is the condition used for controllable generation, which is commonly text or image.\nGenXD generates multi-view images and videos with camera pose and reference image, and thus it requires both camera and image conditions. Camera conditions are independent for each image, either conditioned or targeted. Therefore, it is easy to append it to each latent. Here, we opt for Pl\u00fccker ray as camera condition:\nr = (d, o \u00d7 d) \u2208 R6\nwhere o \u2208 R3 and d \u2208 R3 denote the camera center and the ray direction from camera center to each image pixel, respectively. Therefore, Pl\u00fccker ray is a dense embedding encoding not only the pixel information, but also the camera pose and intrinsic information, which is better than global camera representation.\nThe reference image condition is more complex. GenXD aims to conduct 3D and 4D generation with both single and multiple input views. The single view generation has less requirement while the multi-view generation has more consistent results. Therefore, combining single and multi-view generation will lead to better real-world application. However, previous works condition images by concatenating condition latent to the target latents and by incorporating CLIP image embedding via cross attention. The concatenation way requires to change the channel of the model, which is unable to process arbitrary input views. The CLIP embedding can support multiple conditions. However, both ways cannot model the positional information of multiple conditions and model the information among the input views. In view of the limitations, we leverage the mask latent conditioning for image conditions. As shown in Fig. 4, after encoding with VAE encoder, the forward diffusion process is applied to the target frames (2nd and 3rd frame), leaving the condition latent (1st frame) as usual. Then the noise on the two frames are estimated by denoising model and removed by the backward process.\nThe mask latent conditioning has three main benefits. First, model can support any input views without modification on the parameters. Second, for sequence generation (multi-view images or video), we do not need to constraint the position of the condition frame since the condition frame keeps its position in the sequence. In contrast, many works requires the condition image at a fixed position in the sequence (commonly the first frame). Third, without the conditioning embedding from additional models, the cross attention layers used to integrate conditioning embedding can be removed, which will greatly reduce the number of model parameters. To this end, we leverage mask latent conditioning approach for GenXD.\nMultiView-Temporal Modules. As GenXD aims to generate both 3D and 4D samples within a single model, we need to disentangle the multi-view information from the temporal information. We model these two types of information in separate layers: the multi-view layer and the temporal layer. For 3D generation, no temporal information is considered, while both multi-view and temporal information are required for 4D generation. Therefore, as illustrated in Fig. 4, we propose an a-fusing strategy for 4D generation. Specifically, we introduce a learnable fusing weight, \u03b1, for 4D generation, with a set to 0 for 3D generation. Using the a-fusing strategy, GenXD can preserve the multi-view information in the multi-view layer for 3D data while learning the temporal information from 4D data.\na-fusing can effectively disentangle the multi-view and temporal information. However, the motion is less controllable without any cues. Video generation models use FPS or motion id to control the magnitude of motion without considering the camera movement. Thanks to the motion strength in CamVid-30K, we can effectively represent the object motion. Since the motion strength is a constant, we combine it with the diffusion timestep and add it to the temporal resblock layer, as illustrated in MultiView-Temporal ResBlock of Fig. 4. With the multiview-temporal modules, GenXD can effectively conduct both 3D and 4D generation."}, {"title": "4.2 GENERATION WITH 3D REPRESENTATION", "content": "GenXD can generate images with different viewpoints and timesteps using one or several condition images. However, to render arbitrary 3D-consistent views, we need to lift the generated samples into a 3D representation. Previous works commonly optimize 3D representations by distilling knowledge from generative models. Since GenXD can generate high-quality and consistent results, we directly use the generated images to optimize the 3D representation. Specifically, we utilize 3D Gaussian Splatting (3D-GS) and Zip-NeRF for 3D generation, and 4D Gaussian Splatting (4D-GS) for 4D generation. More details can be found in Appendix. D."}, {"title": "5 EXPERIMENT", "content": ""}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Datasets. GenXD is trained with the combination of 3D and 4D datasets. For 3D datasets, we leverage five datasets with camera pose annotation: Objaverse, MVImageNet , Co3D, Re10K and ACID. Objaverse is a synthetic dataset with meshes, and we render the 80K subset from 12 views following. MVImageNet and Co3D are video data recording objects in 239 and 50 categories, respectively. Re10K and Acid are video data that record real-world indoor and outdoor scenarios. For 4D datasets, we leverage the synthetic data Objaverse-XL-Animation and our CamVid-30K. For the Objaverse-XL-Animation, we use the subset filtered by, and re-render the depth and images by adding noise to the oribit camera trajectory. With the ground truth depth, we estimate the object motion strength as in Sec. 3.2, and then filter out data without obvious object motion. Finally, we get 44K synthetic data from Objaverse-XL-Animation and 30K real-world data from CamVid-30K.\nImplementation Details. GenXD is partially initialized from Stable Video Diffusion (SVD) pre-trained model for fast convergence. Specifically, both the multi-view layer (multi-view convolution and multi-view self-attention) and temporal layer (temporal convolution and temporal self-attention) are initialized from the temporal layer in SVD, and the cross-attention layers in SVD are removed. GenXD is trained in three stages. We first train the UNet only with 3D data for 500K iteration and then fine-tune it with both 3D and 4D data for 500K iterations in single view mode. Finally, GenXD is trained with both single view and multi-view mode with all the data for 500K iteration. The model is trained on 32 A100 GPUs with batch size 128 and resolution 256x256. AdamW optimizer with learning rate 5 \u00d7 10\u22124 is adopted. In the first stage, data are center cropped to square. In the final stage, we make the images square by either center crop or padding, leading to GenXD working well on different image ratio."}, {"title": "5.2 4D GENERATION", "content": "4D Scene Generation. In this setting, videos with both object and camera movement are required for evaluation. Therefore, we introduce the Cam-DAVIS benchmark for 4D evaluation. Specifically, we use our annotation pipeline proposed in Sec. 3 to get the camera poses for videos in DAVIS dataset."}, {"title": "5.3 3D GENERATION", "content": "Few View 3D Generation. For few-view 3D reconstruction setting, we evaluate GenXD on both in-distribution (Re10K) and out-of-distribution (LLFF) datasets. We select 10 scenes from Re10K and all the 8 scenes in LLFF, and 3 views in each scene are used for training. The performance is evaluated with PSNR, SSIM and LPIPS metrics on the rendered test views. As a generative model, GenXD can generate additional views from sparse input views and improve the performance of any reconstruction method. In this experiment, we leverage two baseline methods: Zip-NeRF and 3D-GS. The two baselines are methods for many-view reconstruction, and thus we adjust the hyperparameter for better few-view reconstruction (more details in Appendix. D). As shown in Tab. 4, both Zip-NeRF and 3D-GS can be improved with the generated images from GenXD, and the improvement is more"}, {"title": "5.4 ABLATION STUDY", "content": "In this section, we conduct the ablation study of multiview-temporal modules. The ablation study is evaluated on the quality of the generated diffusion samples on few view 3D and single view 4D generation settings (Tab. 5). More ablation studies are conducted in Appendix. C\nMotion Disentangle (\u03b1-fusing). The camera movement and object motion are entangled in 4D data. To enable high quality generation in both 3D and 4D, GenXD introduces multiview-temporal modules (Sec. 4.1) to learn the multi-view and temporal information separately, and then fuse them together with \u03b1-fusing. For 3D generation, the \u03b1 is set to 0 to bypass the temporal module while the \u03b1 is learned during training for 4D generation. Removing the \u03b1-fusing will result in all 3D and 4D data passing through temporal modules, which will result in the model being unable to disentangle object motion from camera movement. The failure of disentanglement will adversely affect both 3D and 4D generation (Tab. 5).\nEffectiveness of Motion Strength. The motion strength can be used to effectively control the magnitude of the object's movement. As shown in the second to last row of Fig. 7, increasing the motion strength can increase the speed of the car. As a result of these observations, we can conclude that it is important to learn object motion and that the object motion field and motion strength in our data curation pipeline can accurately represent true object motion."}, {"title": "6 CONCLUSION", "content": "In this paper, we investigate the general 3D and 4D generation with diffusion models. To enhance the learning of 4D generation, we first propose a data curation pipeline to annotate camera and object movement in the videos. Equipped with the pipeline, the largest real-world 4D scene dataset, CamVid-30K, is introduced in this paper. Furthermore, leveraging the large-scale datasets, we propose GenXD to handle general 3D and 4D generation. GenXD utilize multiview-temporal modules to disentangle camera and object movement and is able to support any number of input condition views by mask latent conditioning. GenXD can handle versatile applications and can achieve comparable or better performance in all settings with one model."}, {"title": "ETHICS STATEMENT", "content": "In this paper, we introduce a 4D dataset, CamVid-30K, and a generative model for general 3D and 4D generation. CamVid-30K is curated from existing public video datasets, and we additionally estimate the camera poses and object motion. CamVid-30K adheres to the licenses and agreements of the original video datasets, and it does not raise any new ethical concerns. While we ensure compliance with the licenses of the curated datasets and advocate for responsible use, the ability to generate realistic images and videos from various viewpoints raises risks related to misinformation and privacy violations. We encourage the development of tools for detecting misuse, along with responsible dissemination of the dataset and model, to balance innovation with ethical considerations."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "The experiments in our paper mainly include the training of GenXD and generation with 3D rep-resentation in different settings. In Sec. 5.1, we describe the 3D and 4D datasets used to train the diffusion model, together with the training configurations. In Appendix. D, we introduce the back-bone models and the implementation details for generation with 3D representation in each setting. Our curated 4D dataset, CamVid-30K, and GenXD model will be made publicly available."}, {"title": "E LIMITATIONS", "content": "GenXD demonstrates remarkable performance in both 3D and 4D generation. However, there are two key limitations when applied to real-world scene generation. First, despite the abundance of 3D data, the diversity of real-world datasets is limited. For example, complex scenes , typically feature simplistic camera trajectories, such as forward-facing views. In contrast, datasets focused on objects, often have more varied camera paths, providing richer 3D information, but the object categories are generally few in number and the scene structure is quite simple. As a result, GenXD struggles with generating 360-degree views of complex scenes from single-view conditions.\nSecond, in 4D generation, temporal consistency and object motion are difficult to maintain during large camera movements in real-world scenarios. This limitation arises from the nature of available video data: video data typically provides limited object motion when the camera is moving quickly, whereas large object motion is often associated with static or slightly mobile cameras. With such video data, our curated 4D data do not contain many samples with both large object and camera movement.\nBoth limitations are primarily due to the constraints of current datasets. However, with our proposed data curation pipeline and the increasing availability of public data, GenXD has the potential for significant improvement in future applications."}]}