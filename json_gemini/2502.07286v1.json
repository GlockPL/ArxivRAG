{"title": "Small Language Model Makes an Effective Long Text Extractor", "authors": ["Yelin Chen", "Fanjin Zhang", "Jie Tang"], "abstract": "Named Entity Recognition (NER) is a fundamental problem in natural language processing (NLP). However, the task of extracting longer entity spans (e.g., awards) from extended texts (e.g., homepages) is barely explored. Current NER methods predominantly fall into two categories: span-based methods and generation-based methods. Span-based methods require the enumeration of all possible token-pair spans, followed by classification on each span, resulting in substantial redundant computations and excessive GPU memory usage. In contrast, generation-based methods involve prompting or fine-tuning large language models (LLMs) to adapt to downstream NER tasks. However, these methods struggle with the accurate generation of longer spans and often incur significant time costs for effective fine-tuning. To address these challenges, this paper introduces a lightweight span-based NER method called SeNER, which incorporates a bidirectional arrow attention mechanism coupled with LogN-Scaling on the [CLS] token to embed long texts effectively, and comprises a novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to reduce redundant candidate token-pair spans significantly and model interactions between token-pair spans simultaneously. Extensive experiments demonstrate that our method achieves state-of-the-art extraction accuracy on three long NER datasets and is capable of extracting entities from long texts in a GPU-memory-friendly manner.", "sections": [{"title": "Introduction", "content": "Named entity recognition (NER), a fundamental task in information extraction (IE), aims to identify spans indicating specific types of entities. It serves as the foundation for numerous downstream tasks, including relation extraction (Miwa and Bansal 2016), knowledge graph construction (Xu et al. 2017), and question answering (Moll\u00e1, Van Zaanen, and Smith 2006)."}, {"title": "Related Work", "content": "NER methods are generally categorized into span-based methods, generation-based methods, and other methods."}, {"title": "Span-based Methods", "content": "Span-based methods (Li et al. 2022; Yuan et al. 2022; Su et al. 2022; Zhu and Li 2022) reframe the NER task as a token-pair span classification task. They identify spans based on start and end positions, enumerate all possible candidate spans in a sentence, and perform classification. Most existing methods focus on obtaining high-quality span representations and modeling interactions between spans. CNN-NER (Yan et al. 2023a) utilizes Convolutional Neural Networks (CNNs) to model spatial relations in the token-pair span tensor. UTC-IE (Yan et al. 2023b) further incorporates axis-aware interaction with plus-shaped self-attention for the token-pair span tensor on top of CNN-NER. These methods offer parallel extraction, simple decoding, and advantages in handling nested entity recognition, leading to widespread use and excellent performance. However, calculating all span representations and aggregating interactions between token-pair spans requires substantial computational resources, which limits their effectiveness for long texts."}, {"title": "Generation-based Methods", "content": "Generation-based methods extract entities from text in an end-to-end manner, where the generated sequence can be text (Lu et al. 2022; Jiang et al. 2024), entity pointers (Yan et al. 2021), or code (Sainz et al. 2023). With the rise of large language models (LLMs), such methods (Wang et al. 2023a; Xie et al. 2023; Ashok and Lipton 2023) achieve good performance with only a few examples due to their generalization abilities. Some methods (Wang et al. 2023b; Dagdelen et al. 2024) enhance general extraction capabilities by using powerful LLMs, high-quality data, diverse extraction tasks, and comprehensive prior knowledge. GoLLIE (Sainz et al. 2023) ensures adherence to annotation guidelines through strategies such as class order shuffling, class dropout, guideline paraphrasing, representative candidate sampling, and class name masking. ADELIE (Qi et al. 2024) performs instruction tuning on a high-quality alignment corpus and further optimizes it with a Direct Preference Optimization (DPO) objective. However, compared to span-based methods, these methods often require significant computational resources and may perform poorly in generating accurate longish entities from long texts. The construction of instructions and use of examples can compress input text length, leading to low text utilization. Additionally, autoregressive generation can result in long decoding times."}, {"title": "Other Methods", "content": "In addition to the two main paradigms, there are a few other types of methods. Some methods (Ma and Hovy 2016; Yan et al. 2019; Strakov\u00e1, Straka, and Haji\u010d 2019) model the NER task as a sequence labeling task. However, these methods struggle with nested entities. Some methods (Li et al. 2019; Tan et al. 2021; Shen et al. 2022) use two independent multi-layer perceptrons (MLPs) to predict the start and end positions of entities separately, which can lead to errors due to treating the entity as separate modules. Some approaches (Lou, Yang, and Tu 2022; Yang and Tu 2022) employ hypergraphs to represent spans, but their decoding processes is complex."}, {"title": "Problem Definition", "content": "In this section, we introduce the problem formulation of named entity recognition from long texts.\nProblem 1 Named Entity Recognition from Long Texts (Long NER). Given a long input text, the goal is to extract different types of named entities or entity blocks that mark their start and end positions in the text. Note that the input length can exceed 1,000 tokens and the entity length can exceed 100 tokens in our problem.\nTaking scholar profiling (Schiaffino and Amandi 2009; Gu et al. 2018) as an example, \"birth place\u201d is a kind of entity, while \"work experience\" often appears as an entity block that involves multiple segments."}, {"title": "Method", "content": "As previously discussed, conventional NER methods fall into two main categories: span-based and generation-based. For NER in long texts, span-based methods need to model interactions between token-pair spans, which incurs substantial GPU memory and computation. In contrast, generation-based methods, commonly based on LLMs, are arduous to generate longish entity spans accurately.\nIn response to these limitations, we propose a lightweight span-based NER model, SeNER, that efficiently encodes long input texts and models token-pair spans interactions. First, we employ a pre-trained language model (PLM) with a arrow attention mechanism to encode long inputs efficiently. To alleviate entropy instability resulting from varied input lengths, we apply LogN-Scaling (Su 2021) to the [CLS] token. Next, we leverage a Biaffine model (Dozat and Manning 2017) to obtain the hidden representation of each token-pair span. Then, we present the token-pair span interaction module, where we propose a novel BiSPA mechanism to significantly reduce redundant candidate token pairs and model interactions between token pairs simultaneously. Finally, we introduce the training strategy and prediction method. An overview of our model is shown in Figure 3."}, {"title": "Long Input Encoding", "content": "Given a piece of text, we pass it into a PLM to obtain its contextual vector representation.\nH [h\u2081, h\u2082, ..., h_L] = PLM ([x\u2081, x\u2082, ..., x_L]) (1)\nwhere H \u2208 \u211d^(L\u00d7d), L is the input length, and d is the output dimension of the PLM.\nTraditional NER methods utilize PLMs with full bidirectional attention, incuring a large amount of GPU memory footprint and computation for long texts. Moreover, full attention for long texts is often unnecessary since distant tokens are usually semantically unrelated. In light of this, a straightforward idea is to use sliding window attention (SWA) (Beltagy, Peters, and Cohan 2020; Zaheer et al. 2020), which adopts a fixed window, say w, so that each token attends to w tokens to its left and w tokens to its right. However, SWA ignores the global context, impairing the ability of the Transformer layers to acquire a comprehensive understanding of the entire input text.\nTo this end, we propose an Arrow Attention mechanism, where the [CLS] token uses global attention while other tokens use local sliding window attention, as illustrated in Figure 4. Arrow Attention strikes a balance between global and local attention. Compared to the computational complexity of O(L\u00b2) for the full attention, arrow attention only requires O(wL). Furthermore, the global information captured by the [CLS] token supplements the knowledge of SWA, enhancing the representation of each token and mitigating the information loss caused by the fixed receptive field. Thus, the [CLS] token acts as an attention sink (Xiao et al. 2023) that balances the weights of global and local contexts.\nHowever, varying text lengths can cause entropy instability for the [CLS] token, where the scale of attention scores can change significantly. In this regard, we employ a LogN-Scaling technique on the [CLS] token to stabilize the entropy of attention scores. Specifically, LogN-Scaling is defined as follows:\nH_[CLS] = Attns (H_[t-1][CLS] W^Q, H_[t-1]W^K, H_[t-1]W^V) (2)\nAttns (Q, K, V) = softmax (\\frac{QK^T}{\u221ad} log\\frac{512L}{512})V (3)\nwhere Attns is the scaled attention, H^t is the hidden representation of the t-th Transformer layer, and W^Q, W^K, W^V \u2208 \u211d^(d\u00d7d) are projection matrices.\nNote that LogN-Scaling is commonly used for length extrapolation in LLMs and imposed on all input tokens. Here we utilize LogN-Scaling solely on the [CLS] token to improve the stability and robustness of our model."}, {"title": "Biaffine Model", "content": "Subsequently, the hidden representation H is fed into a Biaffine model to extract features for each candidate span.\nH^s, H^e = MLP_{start} (H), MLP_{end} (H) (4)\nS_{i,j} = (H^s_i)^T W_1 H^e_j + W_2 (H^s_i \u2295 H^e_j) + b (5)\nwhere MLP_{start} and MLP_{end} are multi-layer perceptrons, H^s/H^e \u2208 \u211d^(L\u00d7d) are hidden start/end embeddings, W\u2081 \u2208 \u211d^(c\u00d7d\u00d7d), W2 \u2208 \u211d^(c\u00d72d), b \u2208 \u211d^C, and c is the output dimension of the Biaffine model. The symbol \u2295 represents the concatenation operation. S \u2208 \u211d^(L\u00d7L\u00d7c), called token-pair span tensor, denotes the hidden representation of each candidate span. For example, S_{i,j} represents the features of [x_i, ..., x_j]."}, {"title": "Token-Pair Span Interaction Module", "content": "Note that the token-pair span tensor S considers each possible candidate span. However, for long input texts, it is unnecessary to consider every candidate span, especially for extremely long spans. Additionally, the GPU memory occupied by tensor S increases quadratically with the input length L. In light of this, we propose preserving only the hidden features of spans whose lengths do not exceed w' as shown in Figure 5. Thus, S is compressed to S^h \u2208 \u211d^(L\u00d7w'\u00d7c).\nPrevious studies (Yan et al. 2023a,b) show that modeling the interactions between token pairs, such as plus-shaped and local interaction, should be helpful. Plus-shaped attention applies the self-attention mechanism horizontally and vertically. However, plus-shaped attention cannot be performed directly on the compressed hidden feature tensor S^h since either the original horizontal or vertical dimension is disrupted. Therefore, we propose a novel bidirectional sliding-window plus attention (BiSPA) mechanism to perform plus-shaped attention on the compressed S^h.\nSpecifically, we first compute the horizontal self-attention on S^h, as shown in the top middle of Figure 5. Next, we propose a transformation method, that transforms the top left matrix S to the bottom middle matrix S_v, and then compute the vertical self-attention based on S_v. Finally, we concatenate the horizontal and vertical attention matrices and feed them into an MLP to aggregate plus-shaped perceptual information. Notably, the computational complexity of the BiSPA mechanism is reduced from O(L\u00b3) to O(L \u00d7 (w')\u00b2), optimizing the training efficiency significantly.\nZ^{h/v}_{i,:} = Attn (S^{h/v}_{i,:}, S^{h/v}, S^{h/v}, W^{Q/K/V}_{h/v}) (6)\nAttn (Q, K, V) = softmax (\\frac{QK^T}{\u221ac}) V (7)\nS' = MLP (Z^h \u2295 Z^v) (8)\nwhere W^{Q/K/V}_{h/v} \u2208 \u211d^(c\u00d7c), Z^{h/v} is intermediate representation after horizontal/vertical self-attention, and S' \u2208 \u211d^(L\u00d7w'\u00d7c) is the token-pair span feature after BiSPA mechanism.\nThe BiSPA mechanism endows the model with the capacity to perceive horizontal and vertical directions. We further use two types of position embeddings to enhance the sense of distances between token pairs and the area the token pair locates (Yan et al. 2023b). (1) Rotary Position Embedding (RoPE) (Su et al. 2024) encodes the relative distance between token pairs, which is used for both horizontal and vertical self-attention. (2) Matrix Position Embedding indicates whether each entry in S' is the original upper or lower triangles, which adds to S^h and S^v.\nAfter the BiSPA mechanism, we employ CNN with kernel size 3 x 3 on S' to model the local interactions between token-pair spans.\nS'' = Recover(Conv (\u03c3 (Conv (S')))) (9)\nwhere S'' \u2208 \u211d^(L\u00d7L\u00d7c) is recovered to the square size, and \u03c3 is the activation function.\nWe name the module encompassing the BiSPA mechanism and the convolutional module as the BiSPA Transformer block. The BiSPA Transformer blocks will be repeatedly used to ensure full interaction between token pairs."}, {"title": "Training and Prediction", "content": "We utilize MLP layers to transform the output of the final BiSPA Transformer block into output scores. We use binary cross-entropy as the loss function.\n\u0176 = MLP (S\" + S) (10)\nL = -\\sum_{i,j=1}^L \\sum_{r=1}^R (Y_{i,j}^r log (\\hat{Y}_{i,j}^r) + (1-Y_{i,j}^r) log (1-\\hat{Y}_{i,j}^r)) (11)\n+ (1-Y)log(1-1) ) (12)\nwhere \u0176 \u2208 \u211d^(L\u00d7L\u00d7R) represents the scores of candidate entities, and R is the number of entity types.\nTo improve the robustness and generalization of our model, we employ the whole word masking strategy (Cui et al. 2021) during training and utilize LoRA (Hu et al. 2021) technique to train the PLM parameters.\nDuring prediction, our model uses the average of the upper triangular and lower triangular values as the final prediction score, as follows:\nPr = \\frac{Y_{r_{i,j}} + Y_{r_{j,i}}}{2}, i \u2264 j (13)\nAll text spans that satisfy Pr > 0 are outputted. If the boundaries of multiple candidate spans conflict, the span with the highest prediction score is selected."}, {"title": "Experiment", "content": "We conduct experiments on three NER datasets: Scholar-XL (Zhang et al. 2024), SciREX (Jain et al. 2020), and Profiling-07 (Tang, Zhang, and Yao 2007; Tang et al. 2008).\nBaselines\nWe compare our model with several recent NER methods:\nSpan-based Methods: CNN-NER (Yan et al. 2023a): is a span-based method that utilizes Convolutional Neural Networks (CNN) to model local spatial correlations between spans. UTC-IE (Yan et al. 2023b): models axis-aware interaction with plus-shaped self-attention and local interaction with CNN on top of the token-pair span tensor.\nOthers Methods: DiffusionNER (Shen et al. 2023): formulates the NER task as a boundary-denoising diffusion process and thus generates named entities from noisy spans.\nGeneration-based Methods: UIE (Lu et al. 2022): uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions, and captures the common IE abilities via a large-scale pre-trained text-to-structure model. InstructUIE (Wang et al. 2023b): leverages natural language instructions and instruction tuning to guide large language models for IE tasks. GOLLIE (Sainz et al. 2023): is based on Code-Llama (Roziere et al. 2023) and fine-tunes the foundation model to adhere to specific annotation guidelines. ADELIE (Qi et al. 2024): builds a high-quality instruction tuning dataset and utilizes supervised fine-tuning (SFT) followed by direct preference optimization (DPO). TONER (Jiang et al. 2024): firstly employs an entity type matching model to discover the entity types that are most likely to appear in the sentence, and then adds multiple binary classification tasks to fine-tune the encoder in the generative model. GPT-40 (Achiam et al. 2023): employs the gpt-4o-2024-08-06 API, utilizing a 5-shot in-context learning approach to enhance performance. Claude-3.5 (Anthropic 2024): uses the claude-3-5-sonnet-20241022 API, also adopting 5-shot in-context learning.\nExperimental Setup\nAll experiments are conducted on an 8-card 80G Nvidia A100 server. The entire text is used for the Scholar-XL dataset, while the other two datasets are truncated to 5120 using a sliding window approach, as a trade-off due to limited GPU memory. For prediction, the prediction of the text segment is mapped to the starting/ending position of the original text. Hyper-parameters are selected based on the F1 score on the validation set. For each experiment, we run 3 times with different random seeds and report the average results. We choose DeBERTa-V3-large (He, Gao, and Chen 2023) as the PLM for span-based methods and DiffusionNER. We use AdamW (Loshchilov, Hutter et al. 2017) optimizer with a weight decay of le-2. The unilateral window sizes of the arrow attention and BiSPA mechanism are both set to 128. We only use low-rank adaptation on the Q and V matrix of the self-attention mechanism with a rank of 8.\nEvaluation Metrics\nWe report the micro-F1 score for all attributes. An entity is considered correct only if both the entity type and the entity span are predicted correctly. Precision (P) is the portion of correctly predicted spans over predicted spans, while Recall (R) is the portion of correctly predicted spans over ground-truth entity spans."}, {"title": "Main Results", "content": "Table 2 provides a holistic comparison of different NER methods on three datasets. Generally speaking, span-based methods (CNN-NER, UTC-IE, and our model SeNER) outperform other types of NER methods.\nGeneration-based methods utilize generation loss to fine-tune the foundation model to adapt to the long NER task, achieving unfavorable performance. UIE outperforms InstructUIE, possibly because UIE defines a structured extraction language that suits the long NER problem better than naively performing instruction tuning. GOLLIE and ADELIE achieve similar performance, except for Profiling-07 dataset, which is due to the fact that this dataset is sliced and diced with a high number of empty data and thus makes GOLLIE overfit these empty examples. TONER obtains unsatisfactory performance, possibly since the two-stage framework leads to error propagation and the usage of small language models for generation limits its potential. GPT-40 and Claude-3.5-sonnet are less effective, suggesting that the proprietary model prompting does not perform the long text NER task well.\nThe span-based NER methods (CNN-NER, UTC-IE, and SeNER) outperform other types of NER methods, including Diffusion-NER. DiffusionNER is a diffusion-based method that recovers the boundaries of the entities from a fixed amount of Gaussian noise and it is hard to recover longish entities from long texts. CNN-NER models fine-grained span interactions via CNN, achieving decent extraction performance. UTC-IE further improves CNN-NER by introducing plus-shaped attention on the token-pair span tensor, achieving consistent outperformance over CNN-NER.\nOur model SeNER exhibits noticeable improvements or is on par with the best baseline, suggesting that with the design of arrow attention coupled with LogN-Scaling on the [CLS] in the PLM encoder, as well as the BiSPA mechanism on the token-pair span tensor, our model is capable of saving computation and memory resources without degrading the extraction accuracy. In addition, longer text with more focused attention can effectively help the model understand the semantic information of the text in more detail and extract the corresponding entities."}, {"title": "Ablation Study", "content": "Table 3 presents a justification for the effectiveness of each component in our model. Removing either the arrow attention or BiSPA mechanism results in a decrease in model performance on Scholar-XL and Out-of-Memory (OOM) errors on SciREX and Profiling-07. It indicates that both modules effectively reduce explicit memory usage, enabling the model to handle longer texts and thereby improving overall performance. Specifically, BiSPA significantly reduces compute and memory footprint by reducing negative samples. In contrast, the arrow attention has a limited ability to reduce memory usage for short text on the Scholar-XL dataset. Substituting the arrow attention with sliding window attention (SWA) leads to a significant performance drop, highlighting the necessity of imposing attention scores on the [CLS] token to absorb global contextual information. Adding LogN-Scaling consistently improves the performance, thereby enhancing model stability and robustness. Although removing LORA does not cause OOM errors, the F1 score decreases across all datasets to some extent, demonstrating that LoRA can effectively reduce training parameters and prevent overfitting. Whole Word Masking (WWM) increases the diversity of input texts, thus improving the generalization capacity of the model."}, {"title": "Detailed Analysis for Entity Types", "content": "In this subsection, we focus on comparing the performance of our method with span-based methods (CNN-NER and UTC-IE) and LLM-based methods (InstructUIE, GOLLIE, and ADELIE) across entities of varying lengths and types.\nThe results on the Scholar-XL dataset are depicted in Figure 6, with the average length of the entity types increasing clockwise from \u201cGender\u201d to \u201cWork Experience\u201d. Generative methods, leveraging the powerful capabilities of LLMs, achieve superior performance in extracting \u201cGender\u201d and \u201cBirth Place\u201d types. However, for other types of entities, span-based methods demonstrate consistent superiority. Our model SeNER outperforms CNN-NER and UTC-IE in most types of entities, with particularly notable improvements for longish entities. Specifically, for \u201cSocial Service\u201d, our method achieves an improvement of 6.38% over CNN-NER and 4.14% over UTC-IE, respectively. The performance of SeNER for entity types \u201cEducation Experience\u201d and \u201cWork Experience\u201d falls behind the leading ones a little, indicating that the approximation strategy in our model inevitably loses some information, especially on very long entities."}, {"title": "Window Size Sensitivity", "content": "We investigate how the unilateral window size of the arrow attention and BiSPA mechanism impacts the performance, as shown in Figure 8. For the arrow attention, a small window restricts the information aggregation capability of local attention, leading to the loss of critical information. Conversely, an excessively large window increases the difficulty in information focusing, resulting in degraded performance.\nFor the BiSPA mechanism, a small window reduces the number of candidate entities in the token-pair span tensor, making it difficult to extract long entities effectively. On the other hand, a large window retains a large number of redundant candidate entities, introduces more false positives, and consumes significant computational resources. Additionally, when the window size is set to 512, an Out-of-Memory (OOM) error occurs, further demonstrating the effectiveness of the BiSPA mechanism."}, {"title": "Hyper-parameter Search", "content": "We optimize the model's hyper-parameters based on its performance metrics on the validation set, and then evaluate its final performance on the test set. The parameter search range is as follows: the learning rate search range includes 2e-4, 3e-4, 4e-4; the unilateral window size for the arrow attention and BiSPA mechanisms includes 32, 64, 128, 256, 512; and the masking strategy options are token masking, whole word masking, and span masking."}, {"title": "Prompt Construction", "content": "On the three datasets, we prompt GPT-40 and Claude-3.5 to extract entities by providing five similar demonstrations. The format of the prompt is as follows:\nRecognize entities from the following sentence and classify the entity type into the options. Options: [type 1, type 2, ..., type r]. Please give the answer in json format. example 1, example 2, ..., example 5. Text: Output:"}, {"title": "Detailed Experimental Setup", "content": "For the baseline methods, to balance GPU memory usage and training time overhead, the maximum length of the input text is set to 512. A sliding window of 512 is used to segment the text, and the results of these segments are integrated during prediction. The remaining hyper-parameters are determined through searching on the validation set, and the optimal ones are selected. Since some methods require annotation guidelines on entity types (GOLLIE and ADELIE) as complementary knowledge, we use GPT-4o to generate five detailed descriptions for each entity type."}, {"title": "Entity Types", "content": "The three datasets contain the following entity types, arranged from smallest to largest based on average length. The numbers in parentheses represent the average length of entities of the corresponding type, counted in words.\nThe Scholarst-XL dataset contains 12 entity types: Gender (1), Highest Education (1.07), Birthday (2.43), Position (2.49), Birth Place (2.67), Institution (5.21), Award (7.58), Interest (8.53), Honorary Title (8.83), Social Service (11.12), Educational Experience (40.34), and Work Experience (56.33).\nThe SciREX dataset contains 4 entity types: Metric (1.95), Material (2.02), Method (2.35), and Task (2.44).\nThe Profiling-07 dataset contains 13 entity types: Date (1.38), Major (2.18), Position (2.19), Degree (3.2), Univ (3.41), Interest (4.85), Phone (6.28), Fax (6.34), Affiliation (7.07), Email (7.41), Address (9.61), Contact_info (44.78), and Education_info (78.59). Here, Date and Univ denote the date and university of graduation, respectively. Contact_info and Education_info represent contact information and educational experience, respectively."}, {"title": "Conclusion", "content": "In this paper, we tackle the problem of extracting entities from long texts, a less explored area in Named Entity Recognition (NER). Current span-based and generation-based NER methods face issues such as computational inefficiency and memory overhead in span enumeration, along with inaccuracy and time costs in text generation. To address these challenges, we introduce SeNER, a lightweight span-based approach that featuring a bidirectional arrow attention mechanism and LogN-Scaling for effective long-text embedding. Additionally, we propose a bidirectional sliding-window plus-shaped attention (BiSPA) mechanism that significantly reduces redundant candidate token-pair spans and models their interactions. Extensive experiments show that SeNER achieves state-of-the-art accuracy in extracting entities from long texts across three NER datasets, while maintaining GPU-memory efficiency. Our innovations in arrow attention and the BiSPA mechanism have the potential to advance future research in information extraction tasks."}]}