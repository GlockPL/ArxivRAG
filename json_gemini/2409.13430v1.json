{"title": "CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction", "authors": ["Zhangchen Ye", "Tao Jiang", "Chenfeng Xu", "Yiming Li", "Hang Zhao"], "abstract": "Vision-based 3D occupancy prediction is significantly challenged by the inherent limitations of monocular vision in depth estimation. This paper introduces CVT-Occ, a novel approach that leverages temporal fusion through the geometric correspondence of voxels over time to improve the accuracy of 3D occupancy predictions. By sampling points along the line of sight of each voxel and integrating the features of these points from historical frames, we construct a cost volume feature map that refines current volume features for improved prediction outcomes. Our method takes advantage of parallax cues from historical observations and employs a data-driven approach to learn the cost volume. We validate the effectiveness of CVT-Occ through rigorous experiments on the Occ3D-Waymo dataset, where it outperforms state-of-the-art methods in 3D occupancy prediction with minimal additional computational cost. The code is released at https://github.com/Tsinghua-MARS-Lab/CVT-Occ.", "sections": [{"title": "1 Introduction", "content": "Vision-based 3D semantic occupancy prediction is rapidly evolving in the domain of 3D perception, driven by its critical applications in autonomous driving, robotics, and augmented reality. The task aims to estimate the occupancy state and semantic label of every voxel within a 3D space from visual inputs [11,29, 30,40].\nDespite its critical importance, 3D occupancy prediction presents significant challenges. When relying solely on monocular vision, these challenges are particularly pronounced due to the inherent ambiguity in estimating depth from a single image. While stereo vision has been proposed as a solution to augment depth estimation accuracy [14], its application remains limited in practice. It is impractical to use stereo cameras for widespread applications in autonomous vehicles and robotic systems because of the requirement for extensive calibration"}, {"title": "2 Related Work", "content": "Recent advancements in visual 3D object detection have shown the potential of incorporating temporal observations to bolster detection performance [8, 20, 25, 35, 37]. In our research, we have identified and categorized the emerging temporal fusion methods into three paradigms, as depicted in Figure 1.\nThe first two paradigms are categorized as warp-based methods. These methodologies primarily involve the alignment of Bird's Eye View (BEV) feature maps across multiple temporal instances, utilizing relative camera poses obtained from Inertial Measurement Units (IMUs). Subsequently, the aligned features undergo integration using two primary approaches. The first approach employs self-attention mechanisms, as depicted in Figure 1(a). A representative method utilizing this approach is BEVFormer [20]. Alternatively, the second approach integrates aligned features through concatenation, as illustrated in Figure 1(b). Representative methods employing this approach include BEVDet4D [8], BEV-Formerv2 [37], and PanoOcc [35]. These methods emphasize feature fusion using concatenation followed by convolutional operations. Despite their potential, these methods primarily leverage temporal information in an implicit manner, lacking a robust understanding of temporal geometry. As a result, they fall short in fully exploiting the geometric constraints inherent in 3D space.\nIn contrast, cost volume-based methods, exemplified by (c) in Figure 1 and represented by SOLOFusion [25], draw inspiration from stereo matching techniques. Constructing a cost volume from images captured from different viewpoints in the temporal sequence enables the utilization of geometric constraints to obtain depth-aware features. However, for multi-view vision tasks, the sheer"}, {"title": "2.1 3D Occupancy Prediction", "content": "The goal of 3D occupancy prediction is to estimate the occupancy of each quantized voxel in 3D space. This task is originally rooted in Occupancy Grid Mapping (OGM) [24,28] for mobile robot navigation, where the robot is equipped with range sensors (e.g., LiDAR) and navigates in static environments. Recent works transit to a more generic scenario: they make the occupancy prediction with visual systems in dynamic environments. MonoScene [1] reconstructs the 3D scene from sparse prediction with LiDAR points. VoxFormer [16] makes dense voxel predictions leveraging semantic labels and depth estimation from monocular RGB images. TPVFormer [11] proposes a tri-perspective view decomposition method for efficient 3D occupancy prediction. OccFormer [40] presents a dual-path transformer network to effectively process the 3D volume for semantic occupancy prediction. Besides, these advancements have led to new benchmarks such as visual 3D occupancy prediction [29, 30, 32] and semantic scene completion [15]. We emphasize that our proposed method is a simple and plug-and-play module that seamlessly integrates with existing occupancy prediction pipelines, significantly improving their performance."}, {"title": "2.2 Temporal Fusion for 3D Perception", "content": "Leveraging temporal information for occupancy prediction is a natural strategy since it provides sufficient spatial information for building geometric representations"}, {"title": "2.3 Stereo Matching and Multi-View Stereo", "content": "Stereo matching is rooted in constructing 3D cost volumes from 2D image features [3, 33, 39] and predicting the depth map from the cost volume. Recent works further advance it by proposing correlation-based cost volumes like GC-Net [2]. Meanwhile, multi-view stereo methods( [5,6,10,12,31,38]) leverage plane-sweep volumes for depth map generation. A recent trend of using stereo matching [4, 17, 19] focused on 3D perception in autonomous driving. While existing work in stereo matching often utilizes stereo image pairs, this format is unsuitable for handling multi-view and multi-temporal inputs in autonomous driving scenarios. Additionally, these approaches require generating a Plane-Sweep Volume for each image pair, leading to inefficiencies. Furthermore, recent advancements like OccDepth [23] demonstrate the potential of implicitly learning correlations between stereo images to improve 3D depth-aware feature fusion. Our proposed Temporal Cost Volume method, designed specifically for multi-view and multi-temporal data, offers a more effective and efficient solution. The core innovation of the CVT module lies in its ability to construct a comprehensive 3D representation of the scene by aggregating information over time."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem Setup", "content": "Given only RGB images as inputs, the model is designed to predict a dense semantic scene within a specified volume. Specifically, we utilize as input the current timestamp t along with previous images, denoted by $I = {I_t, I_{t-1},...I_{t-L+1}}^L_{i=1}$, where L represents the number of cameras. Our output is defined as a voxel grid $Y_t \\in {C_o, C_1, ..., C_M}^{H \\times W \\times Z}$, situated in the coordinate frame of the ego-vehicle at timestamp t. Each voxel within this grid is either empty (indicated by $c_o$) or occupied by a specific semantic class among ${C_1,C_2,...,C_M}$. Here, M denotes the total number of classes of interest, while H, W, and Z represent the length,"}, {"title": "3.2 Overall Architecture", "content": "In this section, we depict the comprehensive architecture of CVT-Occ, depicted in Figure 2. Our framework processes multi-frame, multi-view images to first extract multi-scale features through an image backbone. Subsequently, these features from image space are transformed into BEV space features, which are refined by a BEV encoder to generate 3D volumetric representations. There is a rich corpus of research focused on the transition from image space to BEV features. One line of works follows the lifting paradigm proposed in LSS [26];"}, {"title": "3.3 Cost Volume Temporal Module", "content": "Since direct depth information is not available for each pixel, transforming image features into 3D space introduces ambiguity; for instance, a single pixel may correspond to multiple voxels along the line of sight. To address this challenge, we propose the Cost Volume Temporal module, which leverages temporal data to infer depth information and resolve this ambiguity. Specifically, we construct 3D cost volume features using both historical and current BEV features. These cost volume features are then used to derive learned weights, which are subsequently employed to refine the current BEV features.\n3D Volume Features. We pre-define the volume $V\\in \\mathbb{R}^{H \\times W \\times Z}$, where H, W, Z represent the spatial dimensions of the volume space. Each voxel in the volume space corresponds to a cubic space in the real world with a size of s meters. By default, the center of the volume corresponds to the position of the ego car. For each voxel v = (i, j, k), its corresponding 3D location p = (x, y, z) can be calculated by Eqn. 1:\n$x = (i - \\frac{W}{2}) \\times s, \\quad y = (j - \\frac{H}{2}) \\times s, \\quad z = (k - \\frac{Z}{2}) \\times s$\nThe BEV feature is $F_{bev} \\in \\mathbb{R}^{H \\times W \\times E}$, where t represents the timestamp and E is the embedding dimensions. We reshape the BEV feature to volume space as the input of the CVT module, producing $V_t \\in \\mathbb{R}^{H \\times W \\times Z \\times C}$\nConstruct Cost Volume Feature. Given the ambiguity along the line of sight, for each point $p_t$, we sample several additional points ${p_i}_{i=1}^N$ within the current volume. Specifically, we calculate the sight direction $d_t \\in \\mathbb{R}^3$, which is the vector from the center of the volume to the point $p_t$, and then sample points using specific strides ${n}_{i=1}^N$ as shown in Eqn. 2:\n$p = p_t + d_t \\times n$\nSince these points project to the same pixel in the image space, points along the same line tend to have similar features. To accurately distinguish the correct"}, {"title": "3.4 Volume Features Refinement", "content": "position corresponding to the pixel, we use historical BEV features to obtain complementary information. Here comes the core insight of our proposed Cost Volume Temporal Module. Projecting these points into the historical coordinate frame ensures they are no longer in the same line of sight. This parallax provides additional information from the historical BEV features, which helps to reduce depth ambiguity in the current frame. Projection matrix $P_t \\in \\mathbb{R}^{4 \\times 4}$ can transform points from the coordinate frame of ego-vehicle to the global coordinate frame. Therefore the points ${p_i}_{i=1}^N$ are projected onto K \u2212 1 historical frames through the projection matrix as described in Eqn. 3:\n$P_{t-k} = p P_t^{-1} P_k$\nFinally, each point $p_k'$ is converted to voxel coordinates, which is the inverse process of Eqn. 1, and bilinear interpolation is used to sample features from corresponding BEV feature maps. The final cost volume feature map is $F\\in \\mathbb{R}^{H \\times W \\times Z \\times (K \\times N) \\times C'}$, where C represents the channels of the BEV features.\n$V_{occ} = V_t \\oplus Sigmoid(Conv(F))$\nAs described in the Eqn. 4, the constructed cost volume features F are processed through a sequence of convolutional layers, yielding an output weight $W\\in \\mathbb{R}^{H \\times W \\times Z}$. A Sigmoid activation function then normalizes the output weights W to the range [0,1]. These weights are directly supervised according to the voxel occupancy state: weights corresponding to occupied voxels are encouraged to reach 1, while those for unoccupied voxels are guided towards 0. The symbol $\\oplus$ in the Eqn. 4 denotes element-wise product of the original volume features at timestamp t with the learned weights W. This supervised learning approach produces an occupancy-aware volume feature map $V_{occ} \\in \\mathbb{R}^{H \\times W \\times Z \\times C}$. The goal of the learned weights is to diminish the influence of voxel features in incorrectly activated regions due to depth ambiguity and augment the features of correctly identified voxels simultaneously."}, {"title": "3.5 Occupancy Decoder", "content": "Upon the refinement of voxel features $V_{occ}$, our model employs a series of deconvolution layers to transform it into occupancy features. The occupancy features are projected onto the output space, yielding $X \\in \\mathbb{R}^{H \\times W \\times Z \\times M}$. The projection is formulated to map the occupancy features into a discrete set of semantic class predictions for each voxel within the grid. Consequently, the final occupancy output $Y\\in {C_o, C_1,...,C_M}^{H \\times W \\times Z}$ is generated, where each voxel is assigned one of M semantic labels, including an unoccupied state represented by $c_o$. This step effectively translates the enriched voxel features into a semantically segmented 3D occupancy map."}, {"title": "3.6 Training Loss", "content": "Our architecture integrates the CVT module and 3D occupancy prediction within an end-to-end trainable framework. We train the overall network with a multi-task loss as\n$Loss = L_{occ} + \\lambda L_{cvt}$\nwhere $L_{occ}$ denotes the occupancy prediction loss, and $L_{cvt}$ represents the cost volume temporal loss. The coefficient \u03bb serves as a balancing factor between these two loss components.\nFor the loss of occupancy prediction $L_{occ}$, we adopt the cross-entropy loss, which is widely used in semantic occupancy prediction task. $L_{occ} = \\sum_c w_c L(g_c, p_c)$, where $g_c$ and $p_c$ represent the ground truth label and the prediction result for the c-th semantic class, respectively. Class-specific weighting we mitigates class imbalance by inversely correlating with class frequency, as adopted from Occ3D [29].\nTo enhance the network's performance, the CVT module receives direct supervision. The loss attributed to the CVT module, $L_{cvt}$, employs a binary cross-entropy framework as shown in Eqn. 6:\n$L_{cvt} = - \\sum_{j=1}^J \\hat{y}_{j,0} \\log(1 - w_j) + \\hat{y}_{j,1} \\log w_j$\nwhere j indexes voxels in the grid, with J = H \u00d7 W \u00d7 Z. $w_j$ is the j-th element in the output weight $W \\in \\mathbb{R}^{H \\times W \\times Z}$. Specifically, a binary label $\\hat{y}_{j,0} = 1$ signifies the voxel's unoccupied status (i.e., its semantic classification is $c_o$), while $\\hat{y}_{j,1} = 1$ indicates the voxel is occupied in ground truth. This direct supervision on the CVT module not only improves performance but also underpins the importance of accurately learning temporal and spatial features from the input data."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Dataset", "content": "Occ3D-Waymo. We conduct experiments on the Occ3D-Waymo [29] dataset, which provides dense semantic labels for 3D occupancy grids. Occ3D-Waymo consists of 798 training scenes and 202 validation scenes. On the Occ3D-Waymo dataset, the range for the x and y axis as [-40m, 40m], and for the z axis as [-1m, 5.4m]. The voxel grid size is (0.4m, 0.4m, 0.4m), resulting in a resolution of (200\u00d7200\u00d716) for (H, W, Z). while the semantic labels encompass 16 categories, including \"Free\" class. Besides, it also provides visibility masks for LiDAR and camera modality.\nEvaluation Metrics. In our assessment of 3D semantic occupancy prediction capabilities, we utilize the IoU of each class to evaluate 3D semantic occupancy prediction. As Occ3D-Waymo provides camera visibility masks, only voxels within the camera's visible region are evaluated. The mean Intersection over Union (mIoU) is computed over 14 non-Free classes, excluding the \"Motorcycle\" class due to its insufficient voxel numbers."}, {"title": "4.2 Experimental Settings", "content": "Architecture Detail. Our architecture leverages the same image backbone and image neck as Occ3D [29] for image feature extraction. The transformation of image features to BEV space involves a four-layered view transformation layers, each layer comprising a sequence of cross-attention layers, normalization layers, feed-forward layers, and normalization layers. This BEV encoder generates a BEV feature representation $F_{bev} \\in \\mathbb{R}^{200 \\times 200 \\times 256}$, which is further reshaped to volume features $V\\in \\mathbb{R}^{200 \\times 200 \\times 16 \\times 16}$. The selected frame configuration includes a time interval of 0.5s and a sequence of 7 frames, covering a total time span of 3s, which ensures a comprehensive temporal analysis. For the CVT module, by default, it samples 9 points within each volume feature across 7 frames to capture a broad spectrum of temporal and spatial variations. Consequently, this configuration enables the CVT module to generate a cost volume feature map $F\\in \\mathbb{R}^{200 \\times 200 \\times 16 \\times (7 \\times 9) \\times 16}$. The occupancy decoder utilizes a series of deconvolution layers to transform the volume features into occupancy features, which is then followed by two additional convolution blocks responsible for generating the final 3D semantic occupancy predictions.\nTraining. During training, we utilized the AdamW [22] optimizer for 8 epochs and full dataset each epoch. Training was conducted on 8 NVIDIA A100 GPUs with a batch size of 1 per GPU. Learning Rate Scheduler Policy is the cosine annealing schedule with an initial learning rate of 4 \u00d7 10-4. The dimensions for the input images were standardized to 960\u00d7640 pixels. It is noteworthy that we abstained from employing any data augmentation techniques in order to ensure a fair and unbiased comparison among the models."}, {"title": "4.3 Main Results", "content": "As shown in Figure 1, We categorize previous temporal fusion methods into three types: (a) Attention-based methods, which adopt temporal self-attention to recurrently fuse the history BEV information; (b) WrapConcat-based methods, involving the alignment of BEV feature maps across different time steps; and (c) Plane-Sweep-based methods, which construct a cost volume for image pairs to utilize geometric constraints. Additionally, we introduce a novel approach, CVT-Occ, that utilizes geometric correspondences of 3D voxels over time to improve occupancy prediction accuracy. For comparative analysis, we extended some mainstream BEV models for the 3D semantic occupancy prediction task. We choose BEVFormer [20], BEVFormer-WarpConcat [37], and SOLOFusion [25] as representative methods for categories (a), (b), and (c), respectively.\nFor both BEVFormer [20] and SOLOFusion [25], we have replaced their original transformer decoders for object detection with a 3D occupancy prediction decoder. Additionally, for fairness, we omit the depth supervision module when reproducing SOLOFusion [25]. For BEVFormer-WrapConcat, we adopt a commonly used fusion method known as the Wrap and Concatenate strategy, as seen in BEVFormerv2 [37] and PanoOcc [35]. This strategy involves 3D coordinate alignment in BEV space based on the transformation matrix between historical"}, {"title": "4.4 Ablation Study", "content": "Time Span. The time span is defined as the difference between the timestamp of the earliest historical frame and that of the current frame, which plays a"}, {"title": "4.5 Analysis of Factors", "content": "The core insight of our method lies in leveraging parallax between different frames to enhance the accuracy of 3D voxel depth queries. In this subsection, we conduct several evaluations under various conditions to validate this concept. These experiments include binary classification, different BEV ranges, and varying ego vehicle speeds.\nBinary Classification. Within the framework of 3D semantic occupancy prediction, we aggregate all 15 other object classes into a single \"Non-Free\" class. This adjustment removes the influence of semantic information, allowing us to focus purely on depth accuracy. We then compare our approach against leading temporal fusion methods. The results in Table 3 demonstrate that our method"}, {"title": "4.6 Visualization Results", "content": "Figure 4 offers a qualitative comparison of CVT-Occ against the baseline method BEVFormer [20] on the Occ3D-Waymo dataset. To ensure clarity in visualization, a dataset-specific mask is applied to exclude regions not within the camera's field of view. These visual representations clearly illustrate the superiority of CVT-Occ over competing approaches. BEVFormer encounters difficulties with depth perception, leading to issues such as atmospheric distortion and blurring of objects. CVT-Occ marks a remarkable improvements in the accurate depiction of buildings and vegetation, thus providing a more precise and reliable interpretation of the environment."}, {"title": "5 Discussion and Conclusion", "content": "We have presented CVT-Occ, a novel approach that significantly enhances the accuracy of 3D occupancy predictions by leveraging temporal fusion and geometric correspondence across time. Unlike traditional methods that rely on"}]}