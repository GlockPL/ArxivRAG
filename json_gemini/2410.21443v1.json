{"title": "TACO: Adversarial Camouflage Optimization on Trucks to Fool Object Detectors", "authors": ["Adonisz Dimitriu", "Tam\u00e1s Michaletzky", "Viktor Remeli"], "abstract": "Adversarial attacks threaten the reliability of machine\nlearning models in critical applications like autonomous\nvehicles and defense systems. As object detectors be-\ncome more robust with models like YOLOv8, develop-\ning effective adversarial methodologies is increasingly\nchallenging. We present Truck Adversarial Camouflage\nOptimization (TACO), a novel framework that generates\nadversarial camouflage patterns on 3D vehicle models to\ndeceive state-of-the-art object detectors. Adopting Unreal\nEngine 5, TACO integrates differentiable rendering with\na Photorealistic Rendering Network to optimize adversar-\nial textures targeted at YOLOv8. To ensure the generated\ntextures are both effective in deceiving detectors and vi-\nsually plausible, we introduce the Convolutional Smooth\nLoss function, a generalized smooth loss function. Exper-\nimental evaluations demonstrate that TACO significantly\ndegrades YOLOv8's detection performance, achieving\nan AP@0.5 of 0.0099 on unseen test data. Furthermore,\nthese adversarial patterns exhibit strong transferability to\nother object detection models such as Faster R-CNN and\nearlier YOLO versions.", "sections": [{"title": "1 Introduction", "content": "In recent years, object detection has made significant ad-\nvances, with the YOLO family of models leading the way in\nreal-time applications. These models have become essential\nin areas like autonomous driving, surveillance, and robotics,\noffering high accuracy and efficiency in many applications [1].\nHowever, as these technologies become more integrated into\ncritical systems, their vulnerabilities have also come to light.\nAdversarial attacks, which involve subtle, often imperceptible\nperturbations, have been shown to cause machine learning\nmodels to make incorrect predictions, exposing significant\nsecurity risks [2]. While these attacks initially focused on\nclassifiers, the scope has since expanded, with adversarial\nmethods now being applied to domains such as object detec-\ntion, image segmentation [3], reinforcement learning [4] and\neven large language models [5].\nAdversarial attacks can be broadly categorized into digital\nand physical attacks. Digital attacks involve manipulating\npixel values in an image to fool a model. These attacks are ef-\nfective when the input to the model is a digital image, but they\nfail when applied to real-world objects. For instance, a pixel\nchange that deceives an object detector digitally may become\nineffective when printed or viewed under different lighting\nand perspectives, as changes in illumination or camera angles\ncan alter the pixel values of a printed pattern.\nThis limitation has driven the development of physical ad-\nversarial attacks, where the patterns are physically applied to\nobjects. These attacks must remain effective across varying\nl lighting conditions, angles, and environmental factors. Re-\ncent research has demonstrated that such adversarial attacks\nare feasible. For example [6], has shown that even natural\nphenomena, like certain shadows, can serve as effective ad-\nversarial attacks, revealing how deceivable object detection\nmodels can be. Similarly, [7] introduced a novel approach\nthat focuses on background adversarial perturbations in both\ndigital and physical domains, showing that Deep Neural Net-\nworks (DNNs) can be deceived by perturbations applied to\nthe background rather than the objects themselves.\nTo effectively craft physical adversarial patterns, a differen-\ntiable image generation pipeline is essential. Such a pipeline\nenables the optimization of adversarial patterns by allowing\ngradients of a loss function-typically tied to the object de-\ntection model's confidence score-to propagate through the\nentire rendering process. Ultimately, a successful pattern ef-\nfectively camouflages the object against detectors.\nIn this study, we introduce Truck Adversarial Camouflage\nOptimization (TACO), a novel framework designed to render\na specific truck model undetectable to state-of-the-art object\ndetection models by generating adversarial camouflage pat-\nterns. Leveraging Unreal Engine 5 (UE5) for photorealistic\nand differentiable rendering, TACO optimizes textures ap-\nplied to a 3D truck model to deceive detectors, specifically\ntargeting YOLOv8 [8]. Our fully differentiable pipeline inte-\ngrates advanced rendering techniques with neural networks\nto optimize adversarial patterns that prevent the detection of"}, {"title": "2 Related Works", "content": "Adversarial attacks on object detection systems have garnered\nsignificant attention in recent years. While early research\nprimarily focused on digital adversarial examples, the shift to-\nwards physical-world attacks has introduced new challenges\nand methodologies. This section reviews the evolution of\nphysical adversarial attacks, particularly those targeting vehi-\ncles, and highlights how our work advances the state of the\nart.\nOne of the initial approaches to physical adversarial attacks\ninvolved the use of adversarial patches. For instance, it was\nshown that attaching patches to specific regions in an image\ncould deceive object detectors into misclassifying or failing\nto detect objects [10]. Building on this concept, it was also\ndemonstrated that holding a printed adversarial patch in front\nof a person could successfully evade person detection systems\n[11]. While these studies provided valuable insights, they\nprimarily focused on human subjects and simple scenarios.\nShifting the focus to vehicle-based applications, which are\nparticularly relevant for autonomous driving and the military\nsector, researchers explored new methods to deceive object de-\ntectors. An innovative approach was to attach a screen to a car\nthat displays adversarial patterns dynamically adjusted based\non the camera's viewpoint [12]. Although this method proved\neffective, it relies on electronic displays and knowledge of\nthe detector camera location, which may not be practical or\ncovert in real-world scenarios.\nTo overcome the limitations of screen-based methods, a\nblack-box method was proposed to approximate both render-\ning and gradient estimation for generating adversarial pat-\nterns [13]. Notably, they observed that increasing the res-\nolution of the camouflage does not necessarily enhance the\nfooling rate of object detectors, suggesting a trade-off between\npattern complexity and effectiveness.\nFurther advancements were made by exploring white-box\nattacks that leverage knowledge of the target model's parame-\nters. Two primary strategies emerged for constructing differ-\nentiable pipelines for pattern optimization. The first strategy\ninvolved projecting a pattern onto the surface of the object us-\ning camera parameters. For example, a differentiable transfor-\nmation network approach (DTA) projected repeated patterns\nonto vehicles [14]. However, this approach suffered from pro-\njection errors, especially on non-planar surfaces, leading to\ninaccuracies in the application of adversarial patterns.\nAddressing these limitations, triplanar mapping was in-\ntroduced (ACTIVE [15]), which projects the pattern from\nthree different planes to reduce distortion on complex geome-\ntries. Despite these improvements, precise texture mapping\nremained challenging due to incorrect pixel correspondences\non intricate 3D models.\nAn alternative and more accurate approach involves the\nuse of neural mesh renderers, also known as differentiable\nrenderers [16]. By accurately mapping textures onto the faces\nof 3D models, differentiable renderers facilitate end-to-end\noptimization of adversarial textures. The Dual Attention Sup-\npression (DAS) approach [17] utilized this technique, aiming\nto suppress the attention maps of the target detection model\nwhile generating visually natural adversarial textures relying\non partial coverage using patches.\nHowever, partial coverage is less effective compared to full-\nbody textures; the superior performance of the Full-coverage\nCamouflage Attack (FCA) was demonstrated by Wang et\nal. [18], achieving increased robustness and performance in\ndeceiving object detectors. Building upon this, Zhou et al.\nfurther enhanced the effectiveness of adversarial patterns by\nsimulating various weather conditions, such as different times\nof day, rain, and fog [19]. They incorporated environmen-\ntal information from background images, which they passed\nthrough their Environment Feature Extractor network. Their\nresults indicated that patterns optimized under diverse scenar-\nios exhibit greater resilience in real-world applications.\nIn parallel, Duan et al. proposed a method to generate adver-\nsarial patterns for a 3D truck object, specifically targeting the\nFaster R-CNN object detector [20]. Their approach combined\n3D rendering with dense proposal attacks to train adversarial\ncamouflage across varying viewpoints and lighting conditions,\nfurther advancing the effectiveness of 3D adversarial attacks\non vehicle-based models.\nRecent advancements have explored alternative method-"}, {"title": "3 Methodology", "content": "3.1 Problem statement\nLet X = {X1,X2,...,Xn} be the dataset generated in UE5,\nwhere each sample X; includes the following elements:\n\u2022 Reference Image (Xref): A photorealistic image of the\ntruck in the scene, rendered with UE5's advanced light-\ning and shading techniques. The texture for this image\nis randomly sampled from a High-Resolution Texture\nDataset defined in section 4.2.\n\u2022 Gray Textured Truck Image (Xgray): A version\nof the truck rendered in a neutral gray texture\n(RGB:127,127,127)."}, {"title": "3.2 Truck Adversarial Camouflage Optimization", "content": "3.2.1 Neural renderer\nAt the heart of our TACO framework is the need to generate\nphotorealistic images of the truck adorned with adversarial\ncamouflage patterns in a differentiable manner. To accomplish\nthis, we first train a neural network-the Photorealistic Ren-\ndering Network (PRN)\u2014to learn the rendering characteristics\nof UE5. The complete rendering process and the training of\nthe neural renderer are illustrated in Figure 1.\nOur framework incorporates a two-step rendering process:\nthe first stage applies adversarial textures to the truck's 3D\nmesh using a differentiable renderer, referred to as Rdiff. This\nrenderer, given the truck mesh, adversarial texture Tadv, and\ncamera parameters 0c, outputs a raw image Xraw:\nXraw = Rdiff(Mesh,Tadv, \u0398c) (4)\nIn the second step, the pre-trained PRN, denoted as N, along\nwith additional inputs such as the gray textured truck Xgray\nand the depth map Xa, enhances the image to produce a pho-\ntorealistic result Xenh:\nXenh = N(Xraw, Xgray,Xd) (5)\n\nThe PRN is a convolution neural network with a U-Net ar-\nchitecture [23], specifically designed to enhance the raw ren-\ndered image into a photorealistic, high-fidelity output. Next,\nthe enhanced truck image Xenh is blended with the background\nfrom the reference image Xref using the binary mask M to get\nthe final adversarial image:\nXadv = Xenh M+Xref \u00b7 (1-M) (6)\nPRN is trained using a dataset of Xref from UE5. These\nreference images serve as the ground truth for training the\nnetwork to approximate the visual characteristics of UE5\nrendering. Additionally, we used a variety of colorful tex-\ntures in the training process to ensure the network generalizes\nwell across different visual conditions. The final output of\nthis rendering pipeline is the blended image Xadv, which can\nseamlessly blend into the scene.\nAn important innovation in TACO is the use of a truck\nimage Xgray, rendered by applying a gray texture to the 3D\nmodel. Unlike previous methods, such as RAUCA, which\nrely on the background from reference images to capture\ncharacteristics, our approach explicitly provides these details\nthrough Xgray.\nThe rendering loss, Lrender, is computed as the L1 loss\nbetween the blended final image Xadv and the reference image\nXref:\nLrender = ||Xadv - Xref ||1 (7)\nThis loss function ensures that the output of PRN maintains\nfidelity to the raw render while incorporating the photorealis-\ntic characteristics learned from the UE5 reference dataset.\nBy splitting the texture mapping and enhancement pro-\ncesses into Rdiff and PRN stages, our framework can generate\nrealistic images of the truck using any input textures, making\nthe result visually convincing in the photorealistic scenes."}, {"title": "3.2.2 Attack loss", "content": "Following the pre-training of the neural renderer, we now\nfocus on the optimization of the adversarial texture (Figure 2).\nIn our setup, we target the Ultralytics YOLOv8 model, which\nfeatures an updated detection head. Unlike previous YOLO\nversions, this updated head is anchor-free and removes the\nobjectness score as it was found to be redundant. Instead, the\nmodel directly outputs class confidence scores bcls, for each\nobject in the scene. All YOLO versions in our experiments\n(e.g., YOLOv3u, YOLOv5Xu, see section 5.1) utilize this\nupdated detection head from Ultralytics, which significantly\nincreases their performance [24]. Our primary goal is to mini-\nmize bcls values specifically for bounding boxes that overlap\nwith the truck. Additionally, we also reduce the Intersection\nover Union (IoU) between any predicted bounding boxes and\nthe ground truth bounding box of the vehicle. We found that\nthis helps to further reduce bcls.\nLet Bgt denote the ground truth bounding box of the truck,\nand Bi denote the i-th predicted bounding box. To identify\npredicted bounding boxes that overlap significantly with the\ntruck, we define the Intersection over Prediction (IoP) as:\nIoP(B'pred, Bgt) = \\frac{Area(B_{pred} \\cap B_{gt})}{Area(B_{pred})} (8)\nWe select only those predicted bounding boxes where the IoP\nexceeds a predefined threshold tiop. This filtering ensures that\nour attack focuses on reducing the detection confidence of\nbounding boxes representing the truck while avoiding patterns\nthat might be misclassified as another class. The class loss is\nthen defined as:\nLcls = \\sum_{c=1}^{C} \\sum_{i \\in L_{iop}} log(1-b_{cls}^{i}) (9)\n\nwhere:\n\u2022 Liop = {i | IoP(B'pred, Bgt) > Tiop} denotes the set of in-\ndices for bounding boxes with an IoP greater than the\nthreshold Tiop.\n\u2022\nb is the confidence score for class c in bounding box i.\ncls\n\u2022 C is the number of classes (80 in the case of YOLOv8).\nThe IoU loss is defined as:\nLiou = \\sum_{\\Omega_{iou}}IOU(B'_{pred}, B_{gt}) (10)\nwhere:\n\u2022 Liou = {i | IoU(B'pred, Bgt) > Tiou} denotes the set of pre-\ndicted bounding boxes with relatively large IoU values\nthat we want to minimize."}, {"title": "3.2.3 Convolutional Smooth Loss", "content": "An additional constraint is to generate physically producible\nadversarial patterns by ensuring a smooth texture. The tradi-\ntional smoothness loss function calculates the Total Variation\n(TV) between adjacent pixels, specifically looking at the im-\nmediate right and bottom neighbors [25]:\nLv = \\sum_{i,j}\\sqrt{(\u03b4_{i,j} \u2013 \u03b4_{i+1,j})^2 + (\u03b4_{i,j} \u2013 \u03b4_{i,j+1})^2} (12)\nTo improve upon this, we introduce Convolutional Smooth\nLoss a generalized approach, which is the extension of the\ntraditional TV loss. Rather than only considering the imme-\ndiate right and bottom neighbors, this method evaluates the\ndifferences between the central pixel and all pixels within a\nk \u00d7 k kernel. This captures the local variation over a larger\nneighborhood. Mathematically, let Ti,j represent the pixel\nvalue at position (i, j). We calculate the local variation Di, j\nas the sum of squared differences between Ti, j and all pixels\nwithin the k \u00d7 k neighborhood:\nDi,j = \\sum_{n=-\\lfloor \\frac{k}{2} \\rfloor}^{\\lfloor \\frac{k}{2} \\rfloor} \\sum_{m=-\\lfloor \\frac{k}{2} \\rfloor}^{\\lfloor \\frac{k}{2} \\rfloor} (T_{i,j} - T_{i+n,j+m})^2 (13)\nThe overall smoothness loss is then computed as:\nLsmooth(T) = \\frac{1}{W.H}\\sum_{i,j}D_{i,j} (14)\nwhere W and H are the width and height of the texture image\nrespectively. Note, that the term Di, j can be easily calculated\nfor every pixel of the image with the use of two convolutions.\nLet * denote the convolution operation, and K ak \u00d7 k kernel\nwith weights Kn,m = (assuming a uniform weighting for\nsimplicity). Using this kernel, we can compute D for the entire\nimage in a compact and efficient manner as:\nD = k\u00b2T2 \u2013 2T \u00b7 (T * K) + T2 * K (15)\nwhere T2 refers to the element-wise square of the pixel values.\nTK is the convolution of the texture T with the kernel K,\nwhile T2 * K is T2 convoluted with K. This formulation not\nonly captures smoother transitions over a larger neighborhood\nbut also allows for fast computation with convolutions. The\nability to choose k arbitrarily gives us further control over the\ndegree of smoothness."}, {"title": "3.2.4 Projected Gradient Descent", "content": "When optimizing the adversarial texture Tadv, the pixel val-\nues must remain within the valid range [0,1]. To enforce\nthis constraint during optimization, we use a method inspired\nby Projected Gradient Descent (PGD), which handles con-\nstrained optimization problems by ensuring the solution stays\nwithin a feasible set. In our case, this feasible set is defined\nby the pixel value range [0, 1].\nThe optimization problem can be formulated as:\nmin_{Tadv\u2208[0,1]3\u00d7H\u00d7W} L(Tadv),\nwhere L() is our proposed loss function.\nIn standard PGD, after each gradient descent step, the up-\ndated values are projected back into the feasible set. For our\nproblem, this would involve clipping the pixel values of Tadv\nto remain within [0, 1]. However, this projection causes the\ngradient to become zero in regions where the pixel values ex-\nceed the boundaries, leading to discontinuities in the gradient\nflow and slowing down convergence.\nTo address this issue, we instead clamp the gradients di-\nrectly based on the current pixel values of Tadv, ensuring that\nthe pixel values stay within the valid range without disrupting\nthe gradient flow. The unconstrained gradient update step is:\nT = Tadv - NVL(Tadv),\nwhere n is the learning rate. We modify the gradients elemen-\ntwise to prevent the pixel values from exceeding the range\n[0,1]:\n\\nabla_{proj}L(Tadv) = min (max (\\nabla L(Tadv), -Tadv), 1 - Tadv)\nThis approach preserves the gradient flow while ensuring\nthe texture remains within the valid range, leading to more sta-\nble and efficient optimization. The overall adversarial texture\noptimization process is illustrated in Figure 2, showing how\nthe Neural Renderer, Object Detector, and Gradient Projection\nblocks work together to iteratively update the texture."}, {"title": "4 Experimental Setup", "content": "4.1 Truck Model\nIn our framework, we utilized a M923 truck model consisting\nof 24,784 faces Figure 3. To improve computational efficiency\nduring adversarial texture generation, we divided the truck\ninto two distinct parts based on which components are typi-\ncally painted in real-life scenarios:\n\u2022 Body Parts: This segment includes the main body of the\ntruck, such as the carrosserie and tarp, which are usually\npainted for camouflage purposes. This part comprises\n1,282 faces.\n\u2022 Auxiliary Parts: The remaining components of the\ntruck-such as the wheels, bumper, exhaust stacks, and\nother parts that are not typically painted or are impracti-\ncal to paint-fall under this category. This section con-\ntains the remaining 23,502 faces of the model.\nBy separating the truck into these two parts, we ensure\nthat only the body part (approximately 5% of the full truck\nmodel) is processed through the neural renderer. This not only\nreduces the computational load but also significantly lowers\nthe memory requirements for the differential renderer, making\nthe overall pipeline more efficient."}, {"title": "4.2 Dataset", "content": "To solve the optimization problem and generate adversarial\npatterns, we developed a custom dataset in UE5 using the\nM923 3D truck model. This dataset is critical for training the\nneural renderer as well as testing the adversarial performance\nof our method. The dataset comprises two key parts: (1) a\ncore set of truck images captured at various positions and\norientations within the virtual UE5 environment, and (2) a\nsupplementary high-resolution texture dataset, specifically\ndesigned to enhance the neural renderer's generalization.\nCore Truck Dataset Our primary dataset consists of truck\nimages rendered with precise control over environmental con-\nditions and camera parameters. We placed the vehicle in 25\ndistinct positions across the scene. For each position, 2,000\nimages were generated by positioning the camera randomly\nwithin a radius of 5m to 35m from the vehicle with a randomly\nselected texture from the High-Resolution Texture Dataset.\nThe camera was always directed toward the truck and ran-\ndomly oriented with azimuth angles ranging from 5\u00b0 to 90\u00b0\nand elevation angles from 0\u00b0 to 360\u00b0, ensuring diverse cov-\nerage of viewing angles. For each sample of the dataset, the\nfollowing elements are generated Xref, Xgray, Xd, M, 0c, and\ncorresponding texture T.\nHigh-Resolution Texture Dataset In addition to the core\ndataset, we incorporated a diverse set of high-resolution tex-\ntures to train the neural renderer. These images are used as\nthe texture of the truck. This supplementary dataset contains\n4,000 texture images, each of size 2048x2048, drawn from\nmultiple sources:\n\u2022 Describable Textures Dataset [26]: 1,500 images were\ncarefully selected from this dataset to provide a variety\nof texture patterns.\n\u2022 Van Gogh Paintings [27]: 300 texture images were\nsourced from Van Gogh's paintings, chosen for their\ndistinct color patterns.\n\u2022 Random Uniform Color Images: 200 images consist-\ning of uniform color values were generated.\n\u2022 Random Noise Images: 2,000 noise textures were ran-\ndomly generated to simulate non-structured patterns vi-\nsually similar to adversarial patterns.\nBy incorporating such a variety of texture data, we ensure\nthat the neural renderer learns to map a wide range of surface\nappearances, from natural textures to random noise."}, {"title": "4.3 Implementation details", "content": "For the adversarial texture generation, only half of the Core\nTruck Dataset was used. The dataset was split into training\nand testing sets based on the truck positions. We assigned 17\npositions to the training set with 1,000 images per position,\nresulting in 17,000 training images. The remaining 8 posi-\ntions were assigned to the test set, yielding 8,000 images for\nevaluation.\nThe neural renderer was trained using the full dataset. The\ntraining set was split based on the textures. The training set\nincluded 3,250 textures, with 40625 images for the training\nset and 750 textures (9375 images) for testing. We used the\nAdam optimizer with a learning rate of 0.0004, and the train-\ning process was conducted for 100 epochs to minimize the\nrendering loss. The renderer achieved a final Lrender loss of\n0.024, and a structural similarity index (SSIM) of 0.9989.\nFor the adversarial texture generation, we found that ini-\ntializing the texture map with uniform RGB values of [0, 0,\n0] yielded the best results. The optimization was performed\nusing the Adam optimizer over 6 epochs on the 8000 unseen\nsamples, with a batch size of 6 and an increased learning rate\nof 0.006. We set the IoU loss term weight to \u1e9e = 0.01 and\nthe smoothness regularization term weight to y = 0.1 with a\nconvolution kernel size of k = 3 for this configuration. The\nIoP threshold was set to tiop = 0.6 and the IoU threshold to\nTiou = 0.45. These settings were used as the default configu-\nration in all subsequent experiments.\nIn evaluations where we tested different setups or other\nhyperparameters, the remaining parameters (such as the learn-\ning rate, the initialization, and the loss weights) were kept\nconsistent with the default configuration unless otherwise\nnoted."}, {"title": "5 Evaluation", "content": "In this section, we evaluate the effectiveness of the proposed\nTACO framework. We conduct a series of experiments to\nassess the attack performance against various object detec-\ntion models, analyze the impact of different loss components,\nexamine the influence of texture initialization strategies, and\nvisualize the attention shifts in the target model using Class\nActivation Maps (CAMs)."}, {"title": "5.1 Evaluation Metrics and Models", "content": "All evaluations are conducted on the test set of 8 unseen truck\npositions. Since trucks can sometimes be mistaken for cars\nby object detection models due to their similar appearance,\nwe treat cars and trucks as a single combined class when\ncalculating our metrics. We adopt two primary metrics to\nassess the performance of our adversarial textures:\n\u2022 Average Precision at IoU threshold 0.5 (AP@0.5): This\nmetric evaluates the precision of the object detector when\nthe Intersection over Union (IoU) between the predicted\nbounding box and the ground truth exceeds 50%.\n\u2022 Attack Detection Rate (ADR): The proportion of images\nin which the object detector successfully identifies the\ntruck.\nWe evaluate our adversarial textures against six object de-\ntection models:\n\u2022 YOLOv8X [8]: Our target model for the adversarial\nattack.\n\u2022 YOLOv3u [9]: A previous generation of the YOLO\nfamily with an upgraded detection head.\n\u2022 YOLOV5Xu: An intermediate version of the YOLO\nmodels with upgraded detection heads.\n\u2022 Faster R-CNN v2 (FRCNN) [28]: An improved version\nof the two-stage object detection model.\n\u2022 Fully Convolutional One-Stage Object Detection\n(FCOS) [29]: An anchor-free object detection frame-\nwork.\n\u2022 Detection Transformer (DETR) [30]: A transformer-\nbased object detection model.\nAll models are pre-trained on the COCO dataset [31] and\nare treated as black-box models except for YOLOv8, our\nwhite-box target."}, {"title": "5.2 Performance Comparison of Different Tex- tures", "content": "We first compare the effectiveness of our TACO-generated\nadversarial texture against three baseline textures:\n\u2022 Base: The original single-color texture of the truck with-\nout any adversarial modifications.\n\u2022 Naive: A simple camouflage texture common on military\ntrucks.\n\u2022 Random: A texture initialized with random pixel values."}, {"title": "5.3 Impact of Different Loss Functions", "content": "We investigate the contribution of each component in our total\nloss function by evaluating the adversarial textures generated\nusing different combinations of the proposed loss terms. Fig-\nure 4 (second row) illustrates the textures generated with dif-\nferent combinations of loss terms, including Lcls, Lcls + Liou, Lcls + Lsm, and the full Lrotal. Tables 3 and 4 present the\nAP@0.5 and ADR results, respectively, for each loss configu-\nration across all models.\nOptimizing with only the classification loss (Lcls) reduces\nthe detection performance, but not as significantly as when\nadditional loss terms are included. Incorporating the IoU loss\n(Liou) further decreases both AP@0.5 and ADR, particularly\nfor models like FCOS and DETR, indicating that minimizing"}, {"title": "5.4 Texture Initialization Study", "content": "We examine the impact of different texture initialization strate-\ngies on the optimization outcome. The initialization methods\ntested are:\n\u2022 Zeros: Initializing the texture with all zeros (black tex-\nture).\n\u2022 Ones: Initializing the texture with all ones (white tex-\nture).\n\u2022 Random: Initializing with random values.\n\u2022 Base: Starting from the truck's original texture."}, {"title": "5.5 Smoothness Loss Coefficient Analysis", "content": "We explore how varying the smoothness loss coefficient (\u03b3)\naffects the adversarial texture's effectiveness and visual qual-\nity. Figure 7 illustrates the relationship between different \u03b3\nvalues and the corresponding AP@0.5 and ADR on the object\ndetectors.\nWhen y > 1, the smoothness loss dominates the optimiza-\ntion, resulting in overly smooth textures that lack the neces-\nsary perturbations to deceive the detector, thereby reducing\nthe attack's effectiveness. Conversely, when y is too small\n(e.g., \u03b3 < 0.01), the texture may become overly noisy, poten-\ntially making it visually unrealistic.\nIn addition to the performance metrics, Figure 5 provides\na visual representation of how the generated textures evolve\nwith different y values, and how these textures appear when\napplied to the truck. The second row of the figure shows the\ntruck with the textures applied, along with the YOLOv8 de-\ntection results. As y increases, the textures become smoother.\nWhen y reaches 100, the texture becomes overly smooth and\nresembles a plain black color, which diminishes its adversarial\neffect."}, {"title": "5.6 Class Activation Mapping Analysis", "content": "To understand how the adversarial texture affects the target\nmodel's attention mechanisms, we adapt Ablation-CAM [32]\nto visualize the regions of interest in YOLOv8. Figure 8\ndisplays the CAM overlays on the truck images with and\nwithout the adversarial texture.\nIn the original image without the adversarial texture, the\nCAM highlights the truck, indicating that YOLOv8 correctly\nfocuses on the vehicle for detection. After applying the TACO\nadversarial texture, the CAM shifts away from the truck, and\nthe model's attention is dispersed to surrounding areas. This\nsuggests that the adversarial texture successfully misleads the\nmodel's attention mechanisms, contributing to the failure in\ndetecting the truck."}, {"title": "6 Conclusion", "content": "In this paper, we introduced TACO (Truck Adversarial Cam-ouflage Optimization), a novel framework for generating ad-versarial camouflage patterns on 3D vehicle models to deceive\nstate-of-the-art object detection systems. By using UE5 for\nphotorealistic rendering and a novel neural renderer compo-nent, TACO optimizes textures that are both visually smooth\nand highly effective in deceiving detectors. We introduced the\nConvolutional Smoothness Loss, which ensures that the gen-erated patterns maintain a realistic appearance. Experimental\nresults showed that our adversarial textures significantly re-duced the detection performance of YOLOv8, achieving anear-zero AP@0.5 and ADR on unseen test data. The adver-sarial patterns also exhibited transferability to other object\ndetection models including other YOLO versions, Faster R-CNN, and DETR. We also showed that balancing texture\nsmoothness with adversarial perturbations is crucial for op-timal performance. By targeting YOLOv8, we advanced the\nfield beyond previous works that focused on older detection\nmodels, demonstrating the viability of adversarial attacks\nagainst more robust and modern architectures."}]}