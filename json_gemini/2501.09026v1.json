{"title": "Intelligent Anti-Money Laundering Solution Based upon Novel Community Detection in Massive Transaction Networks on Spark", "authors": ["Xurui Li", "Xiang Cao", "Xuetao Qiu", "Jintao Zhao", "Jianbin Zheng"], "abstract": "Criminals are using every means available to launder the profits from their illegal activities into ostensibly \"legitimate\" assets. Meanwhile, most commercial anti-money laundering systems are still rule-based, which cannot adapt to the ever-changing tricks. Although some machine learning methods have been proposed, they are mainly focused on the perspective of abnormal behavior for single accounts. Considering money laundering activities are often involved in gang criminals, these methods are still not intelligent enough to crack down on criminal gangs all-sidedly. In this paper, a systematic solution is presented to find suspicious money laundering gangs. A temporal-directed Louvain algorithm has been proposed to detect communities according to relevant anti-money laundering patterns. All processes are implemented and optimized on Spark platform. This solution can greatly improve the efficiency of anti-money laundering work for financial regulation agencies.", "sections": [{"title": "I. INTRODUCTION", "content": "Money laundering (ML) refers to the use of a series of financial proceeds to cover up the illegal source of funds from corruption, fraud, smuggling and other forms of crime, making the money appear legitimate [1]. With the increasing rampant of upstream crime, ML is posing a more serious threat to financial institutions as well as national security. How to effectively detect abnormal financial activities has become a huge challenge faced by governments and financial institutions. Anti-money laundering (AML) systems have been deployed by some governmental and financial institutions to combat with criminals. Nevertheless, most of the AML systems are still rule-based, suffering from numerous of drawbacks such as insufficient data processing capability, lack of pattern recognition function and easy to be avoided [2]. Some machine learning technologies such as classification and sequence analysis based on historical transaction information for special account have been carried out, improving the efficiency of AML work to some extent [3]-[5]. However, analysis on isolated accounts may still lose important related information, because ML activities are often involved in gang crimes. AML systems should be more intelligent to be able to detect the suspicious ML gangs quickly and accurately.\nGraph mining methods are often used to explore the associations between individuals. Furthermore, community detection algorithm can be an effective method to find ML gangs [6]. While considering the huge volume of transactions, elements with little suspicion should be filtered out before community detection. On the other hand, transfer time and direction are two crucial factors during anti-money laundering processes. Further complex ML crimes can be predicted and prevented if inherent evolution law of the transaction structure is grasped in early times. Nevertheless, current existing methods such as GN (Girvan-Newman) [7], CPM (Clique Percolation Method) [8] or Louvain [9] algorithms can't handle the temporal-directed network well.\nIn this paper, a comprehensive method for detecting suspicious ML gangs in massive transaction networks has been presented. Noise information is filtered out first to reduce the total computation cost. An algorithm incorporated with rich AML experience has been proposed to detect communities. The algorithm has also been parallelized and optimized in Spark GraphX platform, which was applied to deal with the massive real transaction data. At last, the most suspicious ML communities can be picked out by reordering the calculated risk score. This solution has been proved to be a powerful auxiliary tool for monitoring department carrying out anti-money laundering work."}, {"title": "II. DESIGN AND IMPLEMENTATION", "content": "For a large financial institution like UnionPay, there are tens of millions of transactions every day. It is hard and a waste of time to divide all transactions into communities. Meanwhile, criminals always want to launder their money in a relatively short period of time. Thus, we focus on the transaction data in a certain time period Pr. The transfer net can be established according to the real transaction data in Pr. Each account which has transaction record during P\u0442 corresponds to a node, and the transfers between these accounts are treated as edges. First we need to merge the edges between node pairs with the same source and destination node. This can be easily done using the groupEdges function in Spark GraphX. The sum expression was used to measure money and times properties, while the average expression was used to measure the transfer time point property. Note that the edges should be repartitioned before using groupEdges function in Spark.\nGraphX exposes a triplet view, which logically joins the vertex and edge properties yielding an EdgeTriplet RDD. Supposing there are n edges in the whole graph. To express the key idea of our algorithm to the point, we only use crucial factors such as transfer money and times to calculate the primitive edge weight here. For a certain EdgeTriplet i, the total transfer money and times is defined as M\u2081 and Ci, respectively. As we know, the standardization of a variable A\u2081 is expressed as: $A_i = (A_i \u2013 \\overline{A_i}) / \\sigma_A$ where $\\overline{A_i} = (\\sum_{i=1}^n A_i) / n$ and $\\sigma_A = \\sqrt{(\\sum_{i=1}^n(A_i \u2013 \\overline{A_i})^2)/n}$. The primitive edge weight WBi then can be described as: $W_{Bi} = e^{\\omega_M\\cdot M_i + \\omega_C\\cdot C_i}$, where the symbols contain wrepresent the weight distribution ratio for each variables respectively, with their sum at 1. These ratios can be adjusted according to business requirements. For demonstration, these ratios are averaged allocated. The ratios expressed by symbol w in the following context are disposed in the same way. Note that WBi increases with M\u2081 and C\u2081,\nAfter merging edges, it can be found that there are many isolated edges, whose source node and destination node only connect to one edge. These edges have a great disruption to the following ML works, and can be filtered out by using subgraph function. Then the ConnectedComponents function in Spark can be applied to divide the graph into different maximal connected subgraph (MCS) [10]. According to domain knowledge, criminals are always trying to make the transfer network complicated to conceal the true origin and ownership of the proceeds of their criminal activity. A MCS with very little scale is unlikely to be a ML gangs. However, a MCS may be associated with some large merchants and has little possibility to be a ML gangs if it is particularly large and complex. Thus, the MCSs can be filtered using the following formula: V01 < Vmcs <Ve2, where Vmcs is the threshold scale of MCSs. In addition, the node whose degree exceeds a threshold De is defined as \u201chub node\". A MCS with higher ML possibility should have more hub nodes. MCSs can be further filtered by Nhubs > Ne, where Nhubs is the number of hub nodes in a MCS and Ne is the threshold.\""}, {"title": "B. Community detection according to ML characteristics", "content": "Analyzing a MCS directly may be complex and less effective because core ML structures are always mixed with some normal transactions. Thus further divisions of the MCSs were made here by using community detection algorithm. Louvain algorithm with both relatively good speed and performance was selected to implement the community detection [11]. However, the original Louvain algorithm is a common method, which may not have much effect in the field of ML. So a temporal-directed Louvain (TD Louvain) algorithm has been proposed here to detect the communities for AML goals. The details of the algorithm are described as below.\n a) Edge weight optimization by node correction\n The original modularity-based Louvain algorithm mainly measures the impact of edge weight to community. However, it overlooks the weight of node, which is very important in ML networks. Learning from the idea of PageRank [12], assuming node A is already known as an important node, all edges directly connected with A should be relative suspicious, no matter how little the transfer money and times are. For example, a small transaction between A and B is perhaps to be a pre-tentative transaction, if not found, a sequence of large transactions may be followed. For an EdgeTriplet i with starting node src and terminating node dst, the node correction for sre is $\\sigma_s = e^{\\omega_V\\cdot M_s + \\omega_{CV}\\cdot \\widehat{C_s} + \\omega_{Dv}\\cdot D_s}$. Here Ds, Ms and \u0108s are standardized degree, money and times for node src. The correction for node dst oa is calculated in the same way. Thus, current edge weight can be expressed as: $W_{Ni} = \\sigma_s * \\sigma_d * W_{Bi}$\n b) Temporal correction for edge weight\n Criminals are always trying to centralize or decentralize their illicit money in a short period. To deal with this problem, a graph-based pattern matching method is introduced here [13]. The average time point of all inbound and outbound transfers has been calculated as th and tout respectively. For a directed EdgeTriplet i with a starting node src and a terminating node dst, the average transfer time point property for edge src \u2192 dst has already been calculated using groupEdges function as Tsd. If the in-degree of node src is larger than out-degree ($Deg_{in} > Deg_{out}$), the edge src \u2192 dst is likely to follow a pattern P\u2081 called \"centralized out after multi transfer in\", whose weight should be modified. Otherwise, no additional correction will be made. Focusing on the word \"after\", it means an edge completely follows pattern P\u2081 if $T_{s\u2192d} > t_{in}$, with its edge weight promoted. Yet, if $T_{s\u2192d} < t_{in}$, it can be inferred that the risk of this edge may be very low, whose weight should be reduced.\nA correction factor for node src is defined as: $\\theta_s = e^{\\beta_s \\cdot t_{out}}$. Here $\\beta_s = (Deg_{in} \u2013 Deg_{out})/Deg_s$ and $t_{out} = P_T/(T_{s-d}-t_{in})$. The denominator $Deg_s = (Deg_{in} + Deg_{out})$ appears in the expression of \u03b2s is for the purpose of numerical standardization, preventing the weight correction factor from growing too large. The tout being divided by the whole time interval Pr not only aims at standardization, but also ensures that the absolute value of correction coefficient is larger if Tsed is closer to th (because this condition is more suspicious). As can be seen, if the edge src \u2192 dst satisfies \u03b2s > 0 Degin > Degout and $T_{s\u2192d} > t_{in}$ simultaneously, correction factor 05 > 1. For other cases, 05 \u2264 1. In a similar way, if the destination node dst follows a pattern P2 called \u201ccentralized in edge before multi transfer out\" pattern, the weight of edge src \u2192 dst should also be enhanced. Another weight correction factor for terminating node can be defined as: $O_d = e^{\\beta_a \\cdot t_{in}}$ in the same way, where Ba = (Degat \u2013 Degout)/Dega and t = Pr/(Tsd - tout). Then the final edge weight of src \u2192 dst in EdgeTriplet i can be expressed as:\n$W_{EI} =\\begin{cases}O_sO_dW_{Ni}, & \\beta_s > 0 \\text{ and } \\beta_d < 0\\\\O_s W_{Ni}, & \\beta_s > 0 \\text{ and } \\beta_d > 0\\\\O_d\\cdot W_{Ni}, & \\beta_s < 0 \\text{ and } \\beta_d < 0\\\\W_{Ni}, & \\beta_s < 0 \\text{ and } \\beta_d > 0\\end{cases} \\qquad (1)$ \nAn instance of the above idea is shown in Fig. 1. The source node A of edge E\u2082 satisfies BA > 0 and the destination node B satisfies \u1e9eB < 0, while other edges do not meet the two conditions simultaneously. So edge E2 should have the most weight corrections among all edges. If node A satisfies pattern P\u2081 and B satisfies pattern P2 after taking timing factor into account, then the weight of edge E2 obtains the most reinforcement from corrections if other conditions are the same for all edges. Thus the edge weights in the following selections all use WE\u00a1 calculated by Eq (1).\""}, {"title": "c) Directed optimization for modularity", "content": "Louvain method implements community detection in a network by maximizing modularity [14]. However, the asymmetry of information caused by the direction of edges has not been taken into account in original algorithms. In directed graph theory, it is considered that if node i has \"low in-degree and high out-degree\" while node j is just opposite, then link j \u2192 i is more abnormal than i \u2192 j [15]. In another word, link j\u2192 i will play a more significant role for community detection. This point also has a practical meaning for AML. Focusing on the fund flow illustrated in Fig. 2, there is no reason to doubt that edge j \u2192 i is more suspicious than i \u2192 j, if all other conditions are the same. It is because a more structurized transactions can be formed by edge j\u2192 i along with other related edges. Here, the edge j \u2192 i is very likely to be an intermediary channel between a decentralized in and decentralized out transfers.\nFollowing the above conception, it has previously been thought that the expression of kikj during modularity calculation can be modified to $k_i k_{out}$ [15]. However, this modification is not so accurate. Consider a source node i with no edges connected in, then the $k_{in}$ = 0 suggests that no matter how large $k_{out}$ is, the effect of edge i \u2192 j does not change. The same situation can happen for a destination node j with no edges out. Accordingly, a proportional power function revised factor for node n has been defined as: $\\delta_n = (k_{in} \u2013 k_{out})/k_n$, where $k_{in}, k_{out}$ and kn is the weight sum of edges linked into, linked out and linked with node n, respectively. Then the kik; expression can be revised into $e^{-\\delta_i-\\delta_j}k_ik_j$, and the modularity can be expressed as:\n$Q_D=\\frac{1}{2m} \\sum_{ij} [A_{ij}-\\frac{e^{-\\delta_i} k_i e^{-\\delta_j}k_j}{2m}] \\delta(c_i, c_j)  = \\frac{1}{2m} \\sum_{c}[ \\sum A_{ij}  -\\frac{( \\sum_{i \\in c} e^{-\\delta_i}k_i) (\\sum_{j \\in c}e^{-\\delta_j} k_j)}{2m}] = \\sum_c [\\frac{\\sum A_{ij}}{2m}-\\frac{( \\sum_{i \\in c} e^{-\\delta_i}k_i) (\\sum_{j \\in c}e^{-\\delta_j} k_j)}{2m}] = \\sum_c [ \\frac{\\sum_{ij \\in c} W_{Ec}}{2m}-\\frac{(\\sum_{i \\in c} e^{-\\delta_i}k_i)^2}{(2m)^2}]$\\\n$\\delta(c_i, c_j) =\\begin{cases}1, & \\text{if } c_i = c_j\\\\0, & \\text{otherwise}\\end{cases}$ (2)\nHere Aij represents the weight of the edge between i and j. m = \u2211ij Aij is the sum of edge weights in the whole graph. c\u2081 is the community where node i belongs to, and k\u2081 is the sum of all edge weights attached to node i. If c\u2081 = Cj, \u03b4(c\u2081, Cj) is equal to 1, otherwise the function value is 0. The corresponding matrix for each community Mc is:\n$M_c = \\begin{bmatrix}e^{\\delta_1-\\delta_1}k_1k_1, & e^{\\delta_1-\\delta_2}k_1k_2, &...&e^{\\delta_1-\\delta_N}k_1k_N \\\\ e^{\\delta_2-\\delta_1}k_2k_1, & e^{\\delta_2-\\delta_2}k_2k_2, &... &e^{\\delta_2-\\delta_N}k_2k_N \\\\ : & : & \\vdots\\\\ e^{\\delta_N-\\delta_1}k_Nk_1, & e^{\\delta_N-\\delta_2}k_Nk_2, &...&e^{\\delta_N-\\delta_N}k_Nk_N \\end{bmatrix} \\qquad (3)$$\n$\\sum_c M_c$ represents the sum of all elements in matrix Mc.  $\\sum c$ means accumulating in the original community c. It can be found that if kin or kout is very small, then the contribution to modularity of edge j\u2794i can be greater than that of edge ij.\nWhen the revised modularity has been defined, the iterative algorithm can be carried out to maximize the modularity. Detail steps are as follows:\n1) Initialize the community tag for each node by using its own node tag.\n2) Traversing all nodes by attempting to allocate each node i to the community where its neighbor node resides. Calculate the difference AQD before and after each allocation and pick out the allocation with maximum AQD. If maximum AQD is positive, then actualize this allocation; Otherwise, no change will be done. The formula for AQD is revised as:\n$\\Delta Q_D = [\\frac{W_c+k}{2m} -(\\frac{\\sum_i e^{-\\delta_i}k_i+k}{2m})^2] - [\\frac{W_{Ec}}{2m} -(\\frac{\\sum_i e^{-\\delta_i}k_i}{2m})^2] = \\frac{1}{2m} [k_{Mc_{new}}-\\frac{k_{i \\overline{MC}_{new}}-k_ik}{(2m)^2}] =  \\frac{1}{2m} [W_{Ec}-\\frac{kiki}{(2m)^2}] = \\frac{k}{2m} [\\Theta_i- (\\frac{k}{2m}) ] \\qquad (4)$$\nWhere kf is the total weight of edges formed between node i and all nodes in community c; $ \\Theta_i = \\frac{k_je^{-\\delta_i}  \\sum_c k_je^{-\\delta_i} +  k_je^{-\\delta_j}  \\sum_c k_j e^{-\\delta_j}}{2m} $, which corresponding to the sum of all colored elements in $M_{C_{new}}$. The detail expression of $M_{C_{new}}$ is:\n$M_{C_{new}} = \\begin{bmatrix}e^{\\delta_1-\\delta_1}k_1k_1, & e^{\\delta_1-\\delta_2}k_1k_2, &e^{\\delta_1-\\delta_1k} \\\\ e^{\\delta_2-\\delta_1}k_2k_1, & e^{\\delta_2-\\delta_2}k_2k_2, &e^{\\delta_2-\\delta_1k} \\\\ : & : & :\\\\e^{-\\delta_{ikk_1}},&e^{\\delta_{ikk_2}}, & e^{-\\delta_{ikk}} \\end{bmatrix} (5)$ \n3) Repeat the step 2) until the community tag of all nodes does not change.\n4) Compress nodes with same community label into a new node. The total edge weight among inner nodes in original community is transformed into self-link weight of new node; The weight between communities is transformed into that between new nodes.\n5) Repeat the step 2) until the modularity of the whole graph does not change."}, {"title": "C. Algorithm parallelization based on Spark GraphX", "content": "The original Louvain algorithm is not suitable for implementation on a distributed platform like Spark directly. Therefore, the algorithm needs to be optimized in parallel. The main idea of parallelization is to update the information of multiple nodes synchronously according to that of neighbor nodes in last iteration [16]. Details are as follows:\nThe 5 steps of the Louvain algorithm described above can be divided into two stages here: the original steps 1 to 3 are assigned to the first stage, which is to set the community tag of each node until no change is made; Steps 4 and 5 are assigned to the second stage, which is to build a new graph and re-perform the first stage until the whole modularity no longer increases. Parallelization can be implemented on each stage, respectively.\n a) Parallelization for the 1st stage\n A data structure has been defined as Info(i,j) = {ki, kin, kout, ci, kj, kin, kout, c;} to record the relevant information for each node pair (i,j) on each iteration. Here k\u2081 is the total weight of edges connected with node i, kin is total weight of edges linked into node i, kout is total weight of edges linked out from node i, and c\u2081is the community tag for node i. The same is for that of node j.\nThe parallelization can be achieved by using the aggregateMessages function in newest Spark, instead of mapReduceTriplet in old versions. Info(i,j) instances for each node and all its neighbor nodes are generated during map phases, while the neighbor information is assembled into an array for each node in reduce phases. The new community tag for each node can be determined when all its neighbor information is obtained.\n b) Extra corrections for 1st stage parallelization\n In above process, some problem may be encountered during the parallelization of 1st stage. As shown in 1st part of Fig. 3, the original nodes a, b, c, d, i, j belong to their own communities. In a round of iterations, there is a certain risk that node i may be assigned to the community where node j originally located, meanwhile node j is assigned to the community where node i originally located. Thus a \"Community Swap\u201d problem arises, as shown in the 2nd of part of Fig. 3. On the other hand, the 3rd part of Fig. 3 shows another potential issue called \"Ascription Lag\". In this situation, nodes a and b are assigned to the community where node j originally located, while node j itself alters its community ascription to i. This is apparently unreasonable, and may even generate communities with isolated node.\nc) Parallelization for the 2nd stage\nThe compression process can be parallelized directly on the EdgeTriplet RDDs. As shown in Fig. 4, if nodes 1\u30012\u3001 3 are assigned to the community where node j originally located, the weights of orange edges are transformed into that of the self-pointed edge in the new graph, and all edges of nodes 1, 2, 3, j connected with another node i are merged into the new edge between i and j. This process can be achieved by picking out the triplets associated with community c; for node i and j respectively during map phase by using edges.filter function, with related edge weights being merged during the reduce phase by using aggregateMessages function.\""}, {"title": "D. Money laundering risk quantization for Communities", "content": "So far the Parallelized version of community division algorithm for ML has been realized. The next thing to do is sorting communities by their ML risk scores. Generally speaking, a community is of higher ML risk if it has an abnormal volume of transfer, more complex transfer structure or more concentrated trading time. Here, the calculation of the temporal risk will be emphatically discussed because the volume and complexity have already been involved during the community detection process.\nIn information theory, greater entropy indicates a higher uncertainty of information. The value of entropy is only affected by the distribution of variables, regardless of the specific value of the variable itself [17]. Thus a variable called \"temporal entropy\u201d is calculated here to measure the transfer time concentration. The average time point T of a community is computed at first, and the absolute interval between each transfer time point and T is defined as \u0394T. Then each transaction can be divided into corresponding segment according the value of AT. Finally, the ratio of total transactions in each segment is calculated.\nAs shown in Fig. 5, the percentage of transactions in the segment of 0 \u2264 AT < T\u2081 is P\u2081, while it is P\u2082 in that of T\u2081 < AT < T2. The rest can be done in the same manner for all segments, and they should obey:  $\\sum_{i=1}^n P_i = 1$. Then the temporal entropy can be defined as: $H_c = \u2013 \\sum_{i=1}^n P_i * log_2P_i$ Supposing there is a community k with a total node numbers Vk, edge numbers Ek, money amount Mk, average node degree D and temporal entropy Hk. Then ML risk score for this community can be measured by following formula: $\u03c8_k = e^{\\omega_V\\cdot \\nabla_k+\\omega_E\\cdot \\overline{E}_k+\\omega_M\\cdot M_k+\\omega_D\\cdot D_k+\\omega_H\\cdot H_k}$, where $\\overline{V_k},\\overline{E}_k, M_k, \\overline{D_H}$ are the standardization of corresponding variables for community k. Then the communities with relatively higher k will be paid more attention to, and MCSs with more suspicious communities will be analyzed or reported further."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "The entire process has been verified with the ability to recognize the outliers from majority normal information. The solution was implemented in Spark-1.6.1. All the data were stored on HDFS cluster based on Cloudera CDH-5.9.0. The experimental cluster consists of 100 nodes, where each node contains an Intel Xeon CPU E5-2620 at 2.00GHz CPU and 8 GB RAM. About 10 million of real transfer records were extracted from the transactions in the first week of November 2016 to form a graph. The size of graph was scaled down to about 45% of the original one after filtering isolated edges. The remaining graph was divided into different MCSs after merging edges in 5 min.\nAs can be seen from the left part of Fig. 6, the orange circle line labeled as Vremain represents the number of remaining vertices after being filtered by the threshold scale of MCSS Vmcs, and the green triangles labeled as Mremain represents the number of remaining MCSs after filtered by Vmcs. The parameter thresholds were determined at the point where each curvature variation speed trend curve (calculated by the second derivative) becomes close to 0. The blue diamond line in inset shows the curvature variation speed trend of Vremain, labeled as Vremain. It vibrates sharply until Vmcs exceeds 10. The average scale of the remaining MCSs can be calculated as: Vavg_r = Vremain/Mremain. The brown square line shows the curvature variation speed trend of Vavg_r, labeled as Vavg_r. It becomes stable when Vmcs passes 2000. Thus, the conditions for filtering the MCSs by scale can be determined as: 10 <Vmcs <2000. The curvature variation speed trend of remaining vertices vs. degree threshold Dnode has been shown in the upper set of the right part of Fig. 6. The corresponding turning point set threshold De to 20 in a similar way. The parameter Ne can be determined to 7 from the lower set of the right part of Fig. 6 after the definition of De, according to the variation trend of remaining MCSs number with increasing the hub node scale threshold for a MCS.\nAfter being filtered by all above conditions, a subgraph with 30718 vertices and 135248 edges separated by 74 MCSs was picked out as relative suspicious collection. Community detection was then carried out to further determine the anomaly degree for each MCS. A serial version of original Louvain algorithm based on the primitive edge weight WBi was first tested. This was done on a single node programed in python, with the subgraph been divided into 736 communities in 38.64s. Then the parallelized version of original Louvain algorithm was implemented. The subgraph was divided into 765 communities in 5.73s with a modularity of 0.385. It achieves a very similar distribution of the serial version in a much shorter time. At last, the parallelized version of TD Louvain algorithm was implemented. It divided the subgraph into 536 communities in 5.97s with a modularity of 0.572, indicating there is no obvious decline in speed despite of the increased complexity and performance.\nML risk score k for each community k was then calculated and sorted. As shown in Fig. 7, \u03c8k are plotted in a descending order, and the first derivative for the risk score \u03c8 is plotted in orange curve. Community percentile curve with risk score are plotted in red circles. Some special percentile points such as 95%, 90% and 80% can be used to determine the suspicious levels. In a general process, communities in the range of 95~100%, 90~95% and 80~90% can be labeled as level 1, 2 and 3. MCSs with multiple communities at corresponding level can be defined as relative suspicious ones according to business requirements. Coincidentally, the suspicious range boundary around 45 inferred from the percentile 90% is very close to the turning point of the derivative curve, after which point the risk score drops regularly at a relatively slow speed. All these phenomena indicate that crucial point 45 can distinguish well whether a community is associated with ML gangs. In addition, the 45 communities with highest risk scores are distributed in 13 MCSs"}, {"title": "IV. CONCLUSIONS", "content": "In this paper, we presented a sophisticated solution to find transfer communities with high ML risks in massive transaction networks. Firstly, a whole transaction graph is built by merging edges. The next, transfers with less ML possibility are filtered out by selecting suspicious MCSs. Then a TD Louvain algorithm combined with AML patterns is proposed and implemented on remaining MCSs. The subgraph is further divided into different communities with their ML risk scores calculated. Finally, MCSs containing multi communities at high risk levels are further investigated and reported. Note that all these procedures are implemented on the distributed platform of Spark, and TD Louvain algorithm has also been parallelized and optimized. The results demonstrate that our solution can help to find out criminal gangs with high ML risks in massive transaction networks efficiently and intelligently."}]}