{"title": "log-RRIM: Yield Prediction via Local-to-global Reaction Representation\nLearning and Interaction Modeling", "authors": ["Xiao Hu", "Ziqi Chen", "Daniel Adu-Ampratwum", "Bo Peng", "Xia Ning"], "abstract": "Accurate prediction of chemical reaction yields is crucial for optimizing organic synthesis, potentially reducing time and resources spent on experimentation. With the rise of artificial intelligence (Al), there is growing interest in leveraging Al-based methods to accelerate yield predictions without conducting in vitro experiments. We present log-RRIM, an innovative graph transformer-based framework designed for predicting chemical reaction yields. Our approach implements a unique local-to-global reaction representation learning strategy. This approach initially captures detailed molecule-level information and then models and aggregates intermolecular interactions, ensuring that the impact of varying-sizes molecular fragments on yield is accurately accounted for. Another key feature of log-RRIM is its integration of a cross-attention mechanism that focuses on the interplay between reagents and reaction centers. This design reflects a fundamental principle in chemical reactions: the crucial role of reagents in influencing bond-breaking and formation processes, which ultimately affect reaction yields. log-RRIM outperforms existing methods in our experiments, especially for medium to high-yielding reactions, proving its reliability as a predictor. Its advanced modeling of reactant-reagent interactions and sensitivity to small molecular fragments make it a valuable tool for reaction planning and optimization in chemical synthesis. The data and codes of log-RRIM are accessible through https://github.com/ninglab/Yield_log_RRIM.", "sections": [{"title": "Related Work", "content": "Reaction yield prediction has evolved primarily through three types of approaches, each addressing the challenges of representing complex molecular structures and modeling their interactions in different ways. The approaches started with traditional machine learning models based on chemical knowledge-based descriptors. Next, sequence-based models were developed, representing each molecule as a SMILES string. These models are typically pre-trained on large molecule datasets to learn general molecule representations and then fine-tuned specifically for yield prediction tasks. Most recently, graph-based models have emerged as a powerful tool for learning molecular structures, treating molecules as graphs, and aggregating molecular information for prediction."}, {"title": "Traditional Machine Learning Models", "content": "Early approaches to yield prediction utilized traditional machine learning models, such as random forest (RF)18 and support vector machine (SVM), 17 to predict yields. These models relied on chemical knowledge-based descriptors to depict the molecule properties, which include density functional theory calculations, 15,16 one-hot encoding, 28 and fingerprint features.29 These methods were primarily evaluated on reaction datasets containing a single reaction class.26,30 However, they often demonstrated unsatisfactory performance. 15, 16, 28, 29 This highlighted two main limitations. First, the modeling ability of traditional machine learning methods is insufficient for this complex problem. Second, relying solely on pre-defined chemical descriptors for constructing reaction representations is inadequate. The suboptimal results obtained from these methods suggest that more sophisticated and effective approaches are needed to capture the complex information between molecular structures and reaction yields."}, {"title": "Sequence-based Models", "content": "Transformer-based models have recently gained prominence in chemical tasks. 4-8 These models are typically pre-trained on large molecular datasets represented by SMILES strings, learning general molecular representations. They are then fine-tuned on specific datasets containing yield information for the prediction. During fine-tuning, the models learn to process the SMILES string of the entire reaction as input, enabling them to capture relationships between all reaction components. For example, Schwaller et al. introduced YieldBERT, which employs the SMILES string of a whole reaction as input to a BERT-based yield predictor. 20 This BERT-based yield predictor is obtained from fine-tuning a yield regression head layer on a reaction encoder. 31 Similarly, Lu and Zhang developed T5Chem, utilizing the Text-to-Text Transfer Transformer (T5) model.19 T5Chem, pre-trained on the PubChem dataset,21 is designed for multiple reaction prediction tasks (e.g., product prediction, retrosynthesis) and employs a fine-tuned regression head for yield prediction purposes. The sophisticated sequence modeling techniques enable these methods to learn more informative reaction representation than handcrafted chemical knowledge-based descriptors by capturing contextual information embedded in the SMILES string of the entire reactions. Consequently, they demonstrate commendable prediction performance on datasets containing a single reaction class.\nHowever, the efficacy diminishes when testing on datasets with a wide variety of reaction types and diverse substances, such as the US Patent database (USPTO).32 Additionally, treating the whole reaction as input makes it challenging for the sequence-based models to distinguish the effects of different components in a reaction, as reactants and reagents have distinct impacts on yield. Also, small modifications in the molecules, even those involving only a few fragments (atoms, functional groups, or small-size molecules), can significantly affect reaction outcomes. 23 When sequence-based models treat the entire reactions as inputs, they tend to overlook the contributions of those small yet influential fragments. This occurs because the attention mechanisms used in these sequence-based models may not be sufficiently sensitive to those critical fragments, potentially leading to inaccurate predictions.\nTo address these challenges, we propose to apply a local-to-global learning process to ensure equal attention is allocated to molecules of varying sizes. The local-to-global learning process treats each reactant, reagent, and product separately before interacting and aggregating their information, intuitively depicting the role of different components in the reaction. This prevents the model from ignoring the impact of small fragments. Our experiment and analysis demonstrate the effectiveness of our modeling design."}, {"title": "Graph-based Models", "content": "Recent advancements have established graph neural networks (GNNs) as powerful tools for analyzing molecules and pre-dicting reaction yields.9, 10, 33\u201336 These approaches represent chemical structures as graphs, using GNNs to learn structural information and typically employing multilayer perceptrons (MLPs) to predict yields after aggregating molecular infor-mation into vector representations. Saebi et al. developed YieldGNN, which uses Weisfeiler-Lehman networks (WLNs)37 to aggregate atom and bond features over their neighborhood and finally obtain the high-order structural information. These learned structural features and the selected chemical knowledge-based reaction descriptors are then combined to predict the reaction yield through a linear layer. Their results highlight the importance of learned molecular structural features over the chemical descriptors. Yarish et al. introduced RD-MPNN,36 which first uses directed message passing networks (D-MPNN)33 to generate atom and bond embeddings from reactant and product graphs. Then, it creates the chemical transformation encoding according to the atom and bond mapping between the reactants and the products, which is combined with pre-computed molecular descriptors to predict the yield. Li et al. proposed SEMG-MIGNN,10 which similarly employs a GNN to update atom features and obtain molecule representations. Then, it applies an attention mechanism based on all involved components to model the molecular interplays and derive the reaction representation for prediction.\nWhile these graph-based methods demonstrate satisfactory performance on datasets of a single reaction class, they have not been extensively tested on more challenging datasets like USPTO. Furthermore, these approaches exhibit certain limitations in molecular interaction design. RD-MPNN and YieldGNN lack explicit modeling of interactions among reac-tants and reagents, while SEMG-MIGNN's design may not effectively capture the full complexity of molecular interactions. To address these limitations and better enable the model to learn the interactions between reactants and reagents, we propose to explicitly characterize the function of reagents on the reaction center. This approach uses a cross-attention mechanism38 to capture the complex interplay between different reaction components (reactants and reagents) more effectively, potentially leading to improved yield predictions. Our experiments and analysis demonstrate that this design improves the effectiveness of molecular interaction modeling."}, {"title": "Materials", "content": ""}, {"title": "Datasets", "content": ""}, {"title": "USPTO500MT Dataset", "content": "USPTO500MT is derived from USTPO-TPL31 by the authors of T5Chem.6 USTPO-TPL comprises 445,000 reactions, with yield reported, partitioned into 1,000 strongly imbalanced reaction types. USPTO500MT is obtained by extracting the top 500 most frequently occurring reaction types from USPTO-TPL. It consists of 116,360 reactions for training, 12,937 reactions for validation, and 14,238 reactions for testing purposes. The reactants, reagents, and products are encoded as SMILES strings. The yield distribution is summarized in Figure 1b and the entire dataset is skewed towards high-yielding reactions. Within the USPTO500MT dataset, approximately 95.5% of the reactions (129,437) are unique. Additionally, about 3.7% of the products (4,949) are documented with two distinct synthesized processes. Only a small fraction (0.1%) of products are synthesized through over five different processes. Moreover, the number and the function of reagents are varying among each reaction. These showcase the diversity and complexity of the reactions within the dataset."}, {"title": "Buchwald-Hartwig Amination Reaction Dataset", "content": "The Buchwald-Hartwig dataset, constructed by Ahneman et al., 26 has become a benchmark for assessing the performance of yield prediction models. This dataset comprises 3,955 palladium-catalyzed C-N cross-coupling reactions, with yields obtained through high-throughput experimentation (HTE). The dataset encodes information on reactants, reagents, and products as SMILES strings. It includes 15 distinct aryl halides paired with a single amine as reactants. These reactant pairs undergo experimentation with 3 different bases, 4 Buchwald ligands, and 22 isoxazole additives, resulting in 5 different products. The yield distribution, illustrated in Figure 1a, reveals a notable skew due to a substantial proportion of non-yielding reactions.\nIn comparison to broader datasets such as USPTO500MT, the Buchwald-Hartwig dataset is limited to a single reaction type and features a constrained set of reaction components. Moreover, reagent information is consistently organized, with each reaction entry containing ligand, base, and solvent information in a consistent order. While this structured format may facilitate easier predictive model learning, it potentially misrepresents real-world scenarios where chemical data is often comprehensive and less organized. This underscores the limitation in this dataset's ability to reflect the complexity and variability of practical chemical information, despite its value as a benchmark for yield prediction models."}, {"title": "Training data generation", "content": ""}, {"title": "Basic atom features", "content": "We follow Maziarka et al.39 and employ the open-source RDKit toolkit to extract the basic chemical features for atoms in molecules represented by SMILES strings. The basic atom features utilized in log-RRIM are delineated in Table 1. These features describe the basic chemical properties and environment, serving as the input of log-RRIM\u266d."}, {"title": "Learned atom representations from pre-trained models", "content": "To investigate the impact of atom features chosen on log-RRIM, we employ two approaches: one using the basic atom features directly, and another using learned atom representations derived from a pre-trained model MAT by Maziarka et al..39 We name the log-RRIM trained on basic atom features as log-RRIM\u266d, and the version trained on learned atom representations as log-RRIM\u2081. The pre-trained model MAT takes the basic atom features as input and utilizes node-level self-supervised learning 40 on a subset of 2 million molecules from the Zinc15 dataset 41 for molecule representation learning. These learned atom representations are then input for log-RRIM\u2081, potentially capturing more complex atomic relations and information. The hyperparameters of the pre-trained model are delineated in Table A8 and remain consistent across all experiments."}, {"title": "Reaction center identification", "content": "Identifying reaction centers is crucial for log-RRIM as it allows us to pinpoint the specific atoms involved in the chemical transformation. We follow GraphRetro's 42 approach to identify these reaction center atoms by comparing the changed bonds between the mapped reactant and product molecules. In log-RRIM, we model the interactions between these reaction centers and reagents, which enables us to more effectively capture the key information (reagents have an impact on bond-breaking and formation) that influences the reaction yield, potentially improving the accuracy of predictions."}, {"title": "Experimental setting", "content": "For the USPTO500MT dataset, we adopt the training, validation, and testing split used by T5Chem. We adhere to the data-splitting protocol for the Buchwald-Hartwig dataset as YieldGNN, using 10-fold 70/30 random train/test splits. We further allocate 10% of the training data for validation. After determining the optimal hyperparameters using three data splits, we apply the model across all ten data splits and compare its performance against other baselines. In addition, we exclude reactions that cannot be processed by the reaction center identification method. The reaction center identification process ensures that all reactions in the dataset have well-defined reaction centers and identifiable mechanistic pathways, which is critical for accurate modeling of reaction mechanisms and yield predictions. While this process does not filter out any reactions in the Buchwald-Hartwig dataset, it results in 78,201 reactions filtered out for training, 8,716 for validation, and 9,497 for testing in USPTO500MT. This curation enhances the overall integrity of USPTO500MT, allowing for more precise reactions to be considered. All performance comparisons are conducted on these curated datasets to maintain consistency in our evaluations."}, {"title": "Model evaluation", "content": "We use mean absolute error (MAE) and root mean squared error (RMSE) for evaluation purposes. Their calculations are given by the following equations:\n$MAE = \\frac{\\sum_{i=1}^N |y_i-\\hat{y_i}|}{N}$ (1)\n$RMSE = \\sqrt{\\frac{\\sum_{i=1}^N (y_i - \\hat{y_i})^2}{N}}$ (2)\nwhere $\\hat{y_i}$ is the predicted yield, $y_i$ is the ground-truth yield, and $N$ is the number of samples. The smaller the MAE and RMSE are, the more accurate the yield predictor model is. Previous methods6, 26 use the coefficient of determination ($R^2$) to evaluate the goodness of fit of the regression model, which is defined as follows:\n$R^2 = 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y_i})^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2}$ (3)\nwhere $\\bar{y}$ is the mean of N ground-truth yields and a larger value of $R^2$ implies a better goodness of fit of the models. However, $R^2$ is not an ideal metric to evaluate the accuracy and relationship, as it has several limitations. 43 One significant issue is that $R^2$ can be heavily influenced by outliers, potentially giving a distorted view of the model's overall fit. This"}, {"title": "Experiment results", "content": ""}, {"title": "Performance on the USPTO500MT dataset", "content": ""}, {"title": "Overall performance", "content": "Table 2 presents the performance comparison of log-RRIM\u266d, log-RRIM\u2081, and baseline methods YieldBERT and T5Chem on the USPTO500MT dataset. log-RRIM, demonstrates the best performance in terms of MAE and RMSE, achieving the lowest MAE of 0.179 and RMSE of 0.226. These results represent statistically significant improvements of 5.8% on MAE over the previous best-performing method T5Chem. The statistical significance of this improvement is underscored by a p-value of 5e-12 at a significance level of 5%, obtained from a paired t-test comparing the Absolute Errors (AE) of log-RRIM and T5Chem (Unless otherwise specified, the p-values mentioned in the following paper are all derived from this paired t-test).\nlog-RRIMb, which utilizes the basic atom features in contrast to log-RRIM, utilizing the learned atom representations, achieved comparable results to log-RRIM\u2081 with an MAE of 0.181. log-RRIM\u266d is still significantly better than T5Chem (p-value = 1e-8). We attribute the superior performance of log-RRIM to its effective framework design, specifically engineered to model and learn fundamental factors influencing reaction yield. The local-to-global learning scheme employed by log-RRIM allows for equal attention to all molecules of varying sizes before modeling their interactions, preventing the oversight of the contributions from small yet influential fragments (e.g., atoms, functional groups, or small molecules). This approach contrasts with sequence-based models like T5Chem and YieldBERT, which treat the entire reaction as input, where the attention mechanisms may not be sufficiently sensitive to critical fragments. Furthermore, log-RRIM's molecular interaction design explicitly models the function of reagents on reaction centers, more closely mimicking the synthetic reaction principle: reagents like catalysts have a huge impact on bond-breaking and formation. This targeted design is more effective than T5Chem and YieldBERT's interaction modeling, which indiscriminately applies global attention to all atoms. It is also worth noting that log-RRIM is pre-training-free, whereas T5Chem and YieldBERT are based on foundation models pre-trained on extensive molecule datasets (e.g. 97 million molecules from PubChem21). log-RRIM's superior performance suggests that pre-training may not be necessary if the training dataset is sufficiently large (e.g., 78K for USPTO500MT) when the reactions are modeled in a targeted and explicit way. By incorporating more effective designs, log-RRIM achieves better performance while saving huge resources required for pre-training.\nlog-RRIM and log-RRIM exhibit nearly identical performance, with MAE values of 0.181 and 0.179, respectively. The former employs basic atom features, while the latter utilizes atom representations derived from the pre-trained MAT model.39 The incorporation of learned representations does not obtain a substantial improvement in yield prediction accuracy over basic features. This outcome suggests that the atom representations acquired through the MAT model, which was originally developed for general molecule representation learning39, lack the specificity required for reaction-oriented tasks. Although basic atom features only provide elementary information about molecular properties, our findings underscore that the key to enhancing yield prediction accuracy lies in more sophisticated and effective modeling of intermolecular interactions.\nWhile other graph-based yield prediction methods 9, 10,36 exist, they are primarily designed for datasets with fixed reagent structures, such as the Buchwald-Hartwig dataset, which includes very specific reagent information (additive, base, solvent, and ligand). 26 However, these methods do not apply to the USPTO500MT dataset used in this study due to its varying number of reagents across reactions and lack of standardized reagent information. However, the USPTO500MT dataset more closely resembles real-world scenarios where reaction compositions are not strictly structured. In this context, log-RRIM, T5Chem, and YieldBERT demonstrate greater potential for practical applications compared to the graph-based methods just mentioned. log-RRIM's superior performance among those methods, as demonstrated in the previous results, combined with its flexibility in handling diverse inputs, positions it as a promising approach for accurate yield prediction in practical usage."}, {"title": "Performance comparison over different yield ranges", "content": "To gain deeper insights into the performance differences between log-RRIM\u266d and T5Chem, we conducted a detailed analysis of predictions across various yield ranges. Figure 2 visualizes these comparisons, with stacked asterisks indicating the level of statistical significance of the performance difference across yield ranges (see Table Al for exact values). Figure 2a shows that log-RRIM\u266d outperforms T5Chem in predicting yields within the 40% to 100% with t-test p-values all less than 0.05, indicating statistical significance at the 5% level. This pattern suggests that log-RRIM\u266d is a more reliable predictor for medium to high-yielding reactions, a crucial advantage in practical synthesis scenarios.44,45 Also, Figure 2b suggests"}, {"title": "Effectiveness in reactant-reagent interactions modeling", "content": "To assess the model's capacity to capture the influence of molecular interactions on yield, specifically how reactants and reagents affect each other in the context of a reaction, we conducted two analyses on the testing set of USPTO500MT. First, we identified 76 reaction pairs (152 reactions) with identical reactants but different reagents and yields. This setup allowed us to evaluate how our method is sensitive to the effects of reagents on yields. In this context, \"interactions\" refer to how the introduction of different reagents influences the reaction outcome with the same reactants. log-RRIM\u266d achieved a prediction MAE of 0.145, outperforming T5Chem's 0.182. Furthermore, log-RRIM\u266d correctly predicted the yield difference (how much the yield increases or decreases) in 62% (47 out of 76) of reaction pairs, compared to T5Chem's 38%. This suggests that log-RRIM is more sensitive to reagent changes and their effects on yield. Case 1 in Figure 4 illustrates this: in two identical aryl nitration reactions, adding ether as a solvent increases the ground-truth yield from 42.0% to 57.7%. log-RRIM\u266d correctly predicts this upward trend, while T5Chem does not. This shows log-RRIM's ability to capture how the addition of a solvent (ether) interacts with the existing reactants to influence the yield.\nSecondly, we examined 3,698 reactions grouped into 619 sets, each containing two or more reactions with identical reagents but different reactants. This analysis aimed to evaluate the models' ability to predict yields when the same reagents interact with various reactants. Here, \"interactions\" refer to how the same set of reagents behaves differently with varying reactants. log-RRIM\u266d exhibited more accurate predictions in 58% of sets (357 out of 619), with a lower MAE of 0.147 compared to T5Chem's 0.222. Case 2 in Figure 4 demonstrates log-RRIM's consistently more accurate predictions when the same reagents (carbon disulfide and bromine) interact with two different reactants. This indicates log-RRIM'S enhanced capability to learn and model specific reagent functions across different reaction contexts, capturing how the same reagents behave differently with varying reactants.\nOverall, These analyses suggest that log-RRIM\u266d is more sensitive to changes in reactant-reagent combinations, indi-cating better modeling of their interactions. This enhanced capability makes log-RRIM, a potential aid for chemists in selecting and optimizing reactants or reagents during synthesis planning. We attribute this superiority to log-RRIM'S explicit modeling of reagent function to reaction centersd. This approach, implemented through a cross-attention mech-anism, aligns with fundamental reaction principles. It allows log-RRIM to directly model how reagents influence the reaction center, providing a more nuanced understanding of the reaction process. An ablation study on the removal of explicit reagent function modeling, provided in Table A5, further supports this design choice. As a result, log-RRIMb demonstrates an enhanced ability to capture and interpret complex reactant-reagent interactions, leading to more accurate yield predictions across diverse reaction component combinations."}, {"title": "Sensitivity to small fragments modifications", "content": "To evaluate the models' ability to capture the influence of involved small fragments on reaction yields, we conducted a comparative analysis of their performance on similar reactions with small differences only on a few small fragments in"}, {"title": "reactants or reagents.", "content": "Given the absence of a standardized method for quantifying reaction similarity, we propose a novel similarity metric $Sim(X_i, X_j)$ between reactions $X_i$ and $X_j$, defined as the average of reactant and reagent similarities:\n$Sim(X_i, X_j) = \\frac{1}{2} [s(R_i, R_j) + s(A_i, A_j)]$ (4)\nwhere $X: R \\rightarrow A \\rightarrow P$ refers to the reaction, $R$ and $A$ are the concatenation of all reactants and reagents in the reaction, respectively. $s(\\cdot, \\cdot)$ is the Tanimoto coefficient between the two chemical structures of Morgan fingerprint. 46"}, {"title": "Performance on the external dataset CJHIF", "content": "To assess our model's performance on external datasets, we conducted an evaluation using a subset of the CJHIF dataset.27 This approach involves using models trained on USPTO500MT and testing them on a subset of the CJHIF dataset, which comprises 3,219,165 reactions sourced from high-impact factor journals. Our assessment involved 1,000 zero-yielding chemical reactions randomly selected from the initial 50,000 reactions in the CJHIF dataset. We specifically chose reactions with reported non-zero yields because CJHIF treats unreported yields as zeros, and we aimed to evaluate our model on reactions with confirmed, measurable outcomes. Importantly, these 1,000 reactions are not included in the training or testing data of USPTO500MT, thus providing an independent testing set for assessing our model's performance on external reactions.\nOverall, log-RRIM\u266d achieved an MAE of 0.149, representing a 16.8% improvement over T5Chem's MAE of 0.179. The results of analyzing performance across yield ranges are illustrated in Figure 5. log-RRIM\u266d significantly outperformed T5Chem for reactions with yields between 60% to 100% (confidence level 95%, more details are provided in Table A2). This superior performance aligns closely with our observations from the USPTO500MT dataset, particularly in log-RRIM'S enhanced accuracy for medium to high-yielding reactions, which suggests that log-RRIM's improved predictive power for high-yielding reactions is a generalizable feature, not limited to a specific dataset. We attribute this generalizability to log-RRIM's molecular interaction design which uses the cross-attention mechanism to effectively model the function of reagents in relation to the reaction center. This allows log-RRIM, to learn fundamental principles about how reagents impact bond-breaking and formation, which are key factors affecting reaction yield. The extensive data in USPTO500MT training data enables log-RRIM\u266d to learn such principles to achieve better test performance on external datasets.\nTo further validate that log-RRIM has effectively learned key factors influencing reaction yield, we visualized the contribution (weight) of each atom when log-RRIM\u266d aggregates atom embeddings and constructs the molecule representa-tion. Three exemplar reactions are shown in Figure 6. In reaction A, a sulfonylation reaction, the sulfur-bearing sulfonyl chloride group on the p-toluenesulfonyl chloride and the free hydroxyl (OH) group on the alcohol are the two reacting centers. The oxygen (O) acts as the nucleophile that displaces the chlorine (Cl) atom, and these atoms influence the yield of the reaction. Reaction B is an imine reduction reaction of the compound N-(4-methoxyphenyl)-1-phenylethylamine. The polar C=N bond between the Nitrogen (N) and Carbon (C) is the reactive site, and these two atoms influence the yield of the reaction, which results in the single C-N bond in the corresponding amine. In reaction C, the two atoms that ultimately influence the yield are Sulfur (S) of benzene sulfonyl chloride and Nitrogen (N) of the indole, producing the final compound. Combined with the weights highlighted by the colormap in Figure 6, we found that the atoms mentioned above that have a greater impact on yield are given higher weights by log-RRIM\u266d, and these atoms are also the atoms in the reaction center. This finding aligns with the fundamental chemical principle that reaction center atoms play a crucial role in the bond-breaking and bond-forming steps in the transition state, thereby exerting substantial influence on the"}, {"title": "yield.", "content": "The ability of log-RRIM, to prioritize these critical atoms in learning the molecule representation is essential for building more accurate models for predicting reaction yield, and our method demonstrates particular effectiveness in this regard."}, {"title": "Performance on the Buchwald-Hartwig dataset", "content": "On the Buchwald-Hartwig dataset, we conducted a performance comparison among pre-training-free models (YieldGNN, SEMG-MIGNN, and RD-MPNN), using 10-fold cross-validation, 47 and reported the testing results averaged over the 10 folds. Table 3 reports the mean and standard deviation (in parentheses) of MAE, RMSE, and R2. Table A4 provides a detailed comparison of testing MAE values for different models on each fold. Our method, log-RRIM\u266d, outperforms other pre-training-free (also graph-based) methods across all evaluation metrics (MAE 0.0348, RMSE 0.0544, and R2 0.953). Notably, it achieves a 14.7% improvement in MAE over the best-performing baseline, YieldGNN. We attribute log-RRIM'S superior performance to its more effective molecular interaction design, explicitly modeling the reagents' function to the reaction center. By incorporating this design, log-RRIM, captures crucial chemical insights that other methods may overlook, leading to more accurate predictions. Compared to the second-best baseline method, SEMG-MIGNN, log-RRIMb improves the MAE by 17.9%. To put this improvement in context, it's worth recalling that SEMG-MIGNN focuses on building more informative atom features (digitalized steric and electronic information). In contrast, log-RRIM\u266d emphasizes learning the characteristics of the reaction itself and molecular interactions. The performance difference between these approaches suggests that for yield prediction tasks, the latter strategy may be more effective. In summary, log-RRIM\u266d"}, {"title": "Discussion", "content": "In conclusion, in this paper, we present log-RRIM, a novel graph-transformer-based reaction representation learning framework for yield prediction. log-RRIM leverages a local-to-global representation learning process and incorporates a cross-attention mechanism to model reagent-reaction center interactions, facilitating improved capture of small fragment contributions and interactions between reactant and reagent molecules. This approach allows log-RRIM to tap into crucial aspects of chemical knowledge, particularly the importance of reagent effects and reaction center dynamics in determining reaction outcomes. Without reliance on pre-training tasks, log-RRIM demonstrates superior accuracy and effectiveness compared to other graph-based methods and state-of-the-art sequence-based approaches, particularly for medium to high-yielding reactions. Our analyses further show log-RRIM's advanced modeling of reactant-reagent interactions and sensitivity to small molecular fragments, making it a valuable asset for reaction planning and optimization in chemical synthesis.\nThe log-RRIM framework requires that predicted reactions consist of three parts (reactant, reagent, and product) and that reaction center atoms be correctly identifiable. While this may limit its practical applications in some scenarios, it enables log-RRIM to more effectively model the crucial intermolecular dynamics that significantly influence reaction out-comes. This approach underscores the importance of incorporating chemical-specific information into model architecture design, rather than directly adapting general-purpose foundation models for chemical tasks like yield prediction.\nWhile log-RRIM makes significant strides in leveraging chemical knowledge, particularly in modeling reagent-reaction center interactions, there remains a vast body of chemical expertise that could potentially be incorporated to further enhance the performance. For instance, research has elucidated detailed mechanisms for different reaction types, like transition states,48,49 which are not yet explicitly incorporated into our model. Furthermore, chemists have developed a deep understanding of the relative reactivity of different functional groups under various conditions, 50,51 which represents another rich source of knowledge that could be integrated into the model. Incorporating such additional aspects of chemical knowledge presents both a challenge and an opportunity for future research. It could potentially enhance the"}, {"title": "model's predictive power, improve its generalization to diverse reaction types, and provide more interpretable insights into the factors driving yield predictions.", "content": "Another promising direction for future research is the exploration of multi-task learning approaches, where the model could be trained simultaneously on yield prediction, reaction condition optimization, retrosynthesis planning, etc. This could lead to a more comprehensive understanding of chemical reactivity and potentially improve performance across all tasks.\nlog-RRIM represents a significant step forward in reaction yield prediction by leveraging graph-based representations and modeling reagent-reaction center interactions, and there is still room for further integration of chemical knowledge and enhancement of the model's capabilities. By continuing to merge data-driven techniques with established chemical principles, it is crucial to develop more robust, versatile, and reliable models for computational chemistry."}, {"title": "Data Availability", "content": "The data used in this manuscript is made publicly available at https://github.com/ninglab/Yield_log_RRIM."}, {"title": "Code Availability", "content": "The code for log-RRIM is made publicly at https://github.com/ninglab/Yield_log_RRIM."}, {"title": "Method", "content": "Our method, log-RRIM, is a novel local-to-global graph-transformer-based reaction representation learning and molecular interaction modeling for yield prediction. It employs a local-to-global learning process for reaction representation learning, beginning with molecule (reactants, reagents, and product) representation learning. It subsequently models the molecule interactions (between reactants and reagents) and ultimately represents the entire reaction. log-RRIM further uses the reaction representation to predict yield.\nSpecifically, log-RRIM consists of the following three modules: (1) Molecule Representation Learning (MRL) mod-ule: which uses graph transformers39 with multi-head self-attention layers to encode molecular structural information into atom embeddings, and then aggregate atom embeddings into molecule embeddings through Atomic Integration (Al). (2) Molecule Interaction (MIT) module: which learns the interactions between reactants and reagents through the cross-attention mechanism, resulting in interaction-aware embeddings for reaction centers. (3) Reaction Information Aggregation (RIA) module: which employs Molecular Integration (MI) to derive a comprehensive reaction representation from all involved molecules and their interaction representations. Finally, this reaction representation is utilized to predict the yield. An overview of log-RRIM is depicted in Figure 8."}, {"title": "Notations", "content": "In a reaction X, each reactant, reagent, and product is a molecule. We view each molecule Mas a graph, with basic node (atom) features $I \\in \\mathbb{R}^{n \\times s}$, graph adjacent matrix $J\\in \\{0,1\\}^{n \\times n}$, and inter-atomic distance matrix $D \\in \\mathbb{R}^{n \\times n}$, where $n$ is the number of atoms in the molecule and can be different for each molecule, $s$ is the dimension of basic atom features. The reaction X is represented as (R, A, P, y), where $R = \\{M_1,..., M_{N_r}\\}, A = \\{M_1,..., M_{N_a}\\}$, and $P = \\{M_1,..., M_{N_p}\\}$ are the set of $n_r$ reactants, $n_a$ reagents and $n_p$ products in the reaction, and $y$ is the reaction yield. $n_r$, $n_a$, and $n_p$ can be different for each reaction. Notably, we denote the reaction center atom embeddings in reactants as $C \\in \\mathbb{R}^{|C| \\times d}$, where $|C|$ refers to the number of reaction center atoms.\nIn MRL module, the atom embeddings of each molecule after the $l \\in [1,n_i"}]}