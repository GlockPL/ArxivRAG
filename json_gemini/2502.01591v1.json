{"title": "Improving Transformer World Models for Data-Efficient RL", "authors": ["Antoine Dedieu", "Joseph Ortiz", "Xinghua Lou", "Carter Wendelken", "Wolfgang Lehrach", "J. Swaroop Guntupalli", "Miguel Lazaro-Gredilla", "Kevin Murphy"], "abstract": "We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities\u2014such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.42% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) \u201cDyna with warmup\", which trains the policy on real and imaginary data, (b) \u201cnearest neighbor tokenizer\" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) \u201cblock teacher forcing\", which allows the TWM to reason jointly about the future tokens of the next timestep.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) (Sutton and Barto, 2018) provides a framework for training agents to act in environments so as to maximize their rewards. Online RL algorithms interleave taking actions in the environment\u2014collecting observations and rewards\u2014and updating the policy using the collected experience. Online RL algorithms often employ a model-free approach (MFRL), where the agent learns a direct mapping from observations to actions, but this can require a lot of data to be collected from the environment. Model-based RL (MBRL) aims to reduce the amount of data needed to train the policy by also learning a world model (WM), and using this WM to plan \"in imagination\".\nTo evaluate sample-efficient RL algorithms, it is common to use the Atari-100k benchmark (Kaiser et al., 2019). However, the near-deterministic nature of Atari games allows agents to memorize action sequences without demonstrating true generalization (Machado et al., 2018). In addition, although the benchmark encompasses a variety of skills (memory, planning, etc), each individual game typically only emphasizes one or two such skills. To promote the development of agents with broader capabilities, we focus on the Crafter domain (Hafner, 2021), a 2D version of Minecraft that challenges a single agent to master a diverse skill set. Specifically, we use the Craftax-classic environment (Matthews et al., 2024), a fast, near-replica of Crafter, implemented in JAX (Bradbury et al., 2018). Key features of Craftax-classic include: (a) procedurally generated stochastic environments (at each episode the agent encounters a new environment sampled from a common distribution); (b) partial observability, as the agent only sees a 63 \u00d7 63 pixel image representing a local view of the agent's environment, plus a visualization of its inventory (see Figure 1[middle]); and (c) an achievement hierarchy that defines a sparse reward signal, requiring deep and broad exploration.\nIn this paper, we study improvements to MBRL methods, based on transformer world models (TWM), in the context of the Craftax-classic environment. We make contributions across the following three axes: (a) how the TWM is used (Section 3.4); (b) the tokenization scheme used to create TWM inputs (Section 3.5); (c) and how the TWM is trained (Section 3.6). Collectively, our improvements result in"}, {"title": "2. Related Work", "content": "In this section, we discuss related work in MBRL - see e.g. Moerland et al. (2023); Murphy (2024); OpenDILab for more comprehensive reviews. We can broadly divide MBRL along two axes. The first axis is whether the world model (WM) is used for background planning (where it helps train the policy by generating imagined trajectories), or decision-time planning (where it is used for lookahead search at inference time). The second axis is whether the WM is a generative model of the observation space (potentially via a latent bottleneck) or whether is a latent-only model trained using a self-prediction loss (which is not sufficient to generate full observations).\nRegarding the first axis, prominent examples of decision-time planning methods that leverage a WM include MuZero (Schrittwieser et al., 2020) and EfficientZero (Ye et al., 2021), which use Monte-Carlo tree search over a discrete action space, as well as TD-MPC2 (Hansen et al., 2024), which uses the cross-entropy method over a continuous action space. Although some studies have shown that decision-time planning can sometimes be better than background planning (Alver and Precup, 2024), it is much slower, especially with large WMs such as transformers, since it requires rolling out future hypothetical trajectories at each decision-making step. Therefore in this paper, we focus on background planning (BP). Background planning originates from Dyna (Sutton, 1990), which focused on tabular Q-learning. Since then, many papers have combined the idea with deep RL methods: World Models (Ha and Schmidhuber, 2018b), Dreamer agents (Hafner et al., 2020a,b, 2023), SimPLe (Kaiser et al., 2019), IRIS (Micheli et al., 2022), A-IRIS (Micheli et al., 2024), Diamond (Alonso et al., 2024), DART (Agarwal et al., 2024), etc.\nRegarding the second axis, many methods fit generative WMs of the observations (images) using a model with low-dimensional latent variables, either continuous (as in a VAE) or discrete (as in a VQ-VAE). This includes our method and most background planning methods above 2. In contrast, other methods fit non-generative WMs, which are trained using self-prediction loss\u2014see Ni et al. (2024) for a detailed discussion. Non-generative WMs are more lightweight and therefore well-suited to decision-time planning with its large number of WM calls at every decision-making step. However, generative WMs are generally preferred for background planning, since it is easy to combine real and imaginary data for policy learning, as we show below.\nIn terms of the WM architecture, many state-of-the-art models use transformers, e.g. IRIS (Micheli et al., 2022), A-IRIS (Micheli et al., 2024), DART (Agarwal et al., 2024). Notable exceptions are DreamerV2/3 (Hafner et al., 2020b, 2023), which use recurrent state space models, although improved transformer variants have been proposed (Chen et al., 2022; Robine et al., 2023; Zhang et al., 2024)."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. MFRL Baseline", "content": "Our starting point is the previous SOTA MFRL approach which was proposed as a baseline in Moon et al. (2024)3. This method achieves a reward of 46.91% and a score of 15.60% after 1M environment steps. This approach trains a stateless CNN policy without frame stacking using the PPO method (Schulman et al., 2017), and adds an entropy penalty to ensure sufficient exploration. The CNN used is a modification of the Impala ResNet (Espeholt et al., 2018a)."}, {"title": "3.2. MFRL Improvements", "content": "We improve on this MFRL baseline by both increasing the model size and adding a RNN (specifically a GRU) to give the policy memory. Interestingly, we find that naively increasing the model size harms performance, while combining a larger model with a carefully designed RNN helps (see Section 4.3). For the RNN, we find it crucial to ensure the hidden state is low-dimensional, so that the memory is forced to focus on the relevant bits of the past that cannot be extracted from the current image. We concatenate the GRU output to the image embedding, and then pass this to the actor and critic networks, rather than directly passing the GRU output. presents a pseudocode for our MFRL agent.\nWith these architectural changes, we increase the reward to 55.49% and the score to 16.77%. This result is notable since our MFRL agent beats the considerably more complex (and much slower) DreamerV3 agent, which obtains a reward of 53.20% and a score of 14.5. It also beats other MBRL methods, such as IRIS (Micheli et al., 2022) (reward of 25.0%) and A-IRIS (Micheli et al., 2024) (reward of 35.0%). In addition, our MFRL agent only takes 15 minutes to train for 1M environment steps on one A100 GPU."}, {"title": "3.3. MBRL baseline", "content": "We now describe our MBRL baseline, which combines our MFRL baseline above with a transformer world model (TWM)\u2014as in IRIS (Micheli et al., 2022). Following IRIS, our MBRL baseline uses a VQ-VAE, which quantizes the 8 \u00d7 8 feature map $Z_t$ of a CNN to create a set of latent codes, $(q_1, ..., q_L) = enc(O_t)$, where $L = 64$, $q_i \\in \\{1, ..., K\\}$ is a discrete code, and $K = 512$ is the size of the codebook. These codes are then passed to a TWM, which is trained using teacher forcing\u2014see Equation (2) below. Our MBRL baseline achieves a reward of 31.93%, and improves over the reported results of IRIS, which reaches 25.0%.\nAlthough these MBRL baselines leverage recent advances in generative world modeling, they are largely outperformed by our best MFRL agent. This motivates us to enhance our MBRL agent, which we explore in the following sections."}, {"title": "3.4. MBRL using Dyna with warmup", "content": "As discussed in Section 1, we propose to train our MBRL agent on a mix of real trajectories (from the environment) and imaginary trajectories (from the TWM), similar to Dyna (Sutton, 1990). Algorithm 1 presents the pseudocode for our MBRL approach. Specifically, unlike many other recent"}, {"title": "3.5. Patch nearest-neighbor tokenizer", "content": "Many MBRL methods based on TWMs use a VQ-VAE to map between images and tokens. In this section, we describe our alternative which leverages a property of Craftax-classic: each observation is composed of 9 \u00d7 9 patches of size 7 \u00d7 7 each (see Figure 1[middle]). Hence we propose to (a) factorize the tokenizer by patches and (b) use a simpler nearest-neighbor style approach to tokenize the patches.\nPatch factorization. Unlike prior methods which process the full image O into tokens $(q^1, ..., q^L) = enc(O)$, we first divide O into L non-overlapping patches $(p^1, ..., p^L)$ which are independently encoded into L tokens:\n$(q^1, ..., q^L) = (enc(p^1), ..., enc(p^L))$.\nTo convert the discrete tokens back to pixel space, we just decode each token independently into patches, and rearrange to form a full image:\n$(\\hat{p}^1, ..., \\hat{p}^L) = (dec(q^1), ..., dec(q^L))$.\nFactorizing the VQ-VAE on the L = 81 patches of each observation boosts performance from 43.36% to 58.92%.\nNearest-neighbor tokenizer. On top of patch factorization, we propose a simpler nearest-neighbor tokenizer (NNT) to replace the VQ-VAE. The encoding operation for each patch $p \\in [0,1]^{h\\times w \\times 3}$ is similar to a nearest neighbor classifier w.r.t the codebook. The difference is that, if the nearest neighbor is too far away, we add a new code equal to p to the codebook. More precisely, let us denote $\\mathcal{CNN} = \\{e_1, ..., e_K\\}$ the current codebook, consisting of K codes $e_i \\in [0,1]^{h \\times w \\times 3}$, and $\\tau$ a threshold on the Euclidean distance. The NNT encoder is defined as:\n$q = enc(p) = \\begin{cases}\\operatorname{argmin}_{1 \\leq i \\leq K} ||p - e_i||_2 & \\text{if } \\min_{1 \\leq i \\leq K} ||p - e_i||_2 \\leq \\tau\\\\K+1 & \\text{otherwise.}\\end{cases}$ (1)\nThe codebook can be thought of as a greedy approximation to the coreset of the patches seen so far (Mirzasoleiman et al., 2020). To decode patches, we simply return the code associated with the codebook index, i.e. $dec(q^i) = e_{q^i}$."}, {"title": "3.6. Block teacher forcing", "content": "Transformer WMs are typically trained by teacher forcing which maximizes the log likelihood of the token sequence generated autoregressively over time and within a timeslice:\n$\\mathcal{L"}, {"q_{1": "L}^{"}]}