{"title": "Skill-Enhanced Reinforcement Learning Acceleration from Demonstrations", "authors": ["Hanping Zhang", "Yuhong Guo"], "abstract": "Learning from Demonstration (LfD) aims to facilitate rapid Reinforcement Learning (RL) by leveraging expert demonstrations to pre-train the RL agent. However, the limited availability of expert demonstration data often hinders its ability to effectively aid downstream RL learning. To address this problem, we propose a novel two-stage method dubbed as Skill-enhanced Reinforcement Learning Acceleration (SeRLA). SeRLA introduces a skill-level adversarial Positive-Unlabeled (PU) learning model to extract useful skill prior knowledge by enabling learning from both limited expert data and general low-cost demonstration data in the offline prior learning stage. Subsequently, it deploys a skill-based soft actor-critic algorithm to leverage this acquired prior knowledge in the downstream online RL stage for efficient training of a skill policy network. Moreover, we develop a simple skill-level data enhancement technique to further alleviate data sparsity and improve both skill prior learning and downstream skill policy training. Our experimental results on multiple standard RL environments show the proposed SeRLA method achieves state-of-the-art performance on accelerating reinforcement learning on downstream tasks, especially in the early learning phase.", "sections": [{"title": "Introduction", "content": "Despite the wide applicability of reinforcement Learning (RL) across robotics [1], video games [2, 3], and large language models [4, 5], a conventional deep RL agent often requires numerous iterative interactions with the environment to learn a useful policy by maximizing the expected discounted cumulative reward [6], resulting in prolonged training periods and limited computational efficiency that can be particularly problematic for large input scales. To overcome this problem, Learning from Demonstration (LfD), also known as imitation learning (IL), has been investigated to accelerate RL. In LfD, the agent is pre-trained on a small offline demonstration dataset provided by human experts [7, 8] to acquire knowledge and learn behaviors that can be executed in the environment, which can then be leveraged to accelerate the online learning process of the downstream RL task with fewer interactions with the environment. Due to the limited availability of expert demonstration data, some recent studies seek to supplement the expert data with a large task-agnostic demonstration dataset collected inexpensively using methods such as autonomous exploration [9, 10] or human-tele operation [11, 12].\nAs an RL technique that learns reusable skills from a given expert behavior [13, 14] or interacting with online environments [15, 16, 14, 9] to guide the RL training, skill-based RL shows great potential for LfD. Recently, researchers have introduced skill-based RL to LfD by learning reusable skill behaviors from the demonstration data and deploying them for the downstream tasks [11, 17\u201320]. However, these previous studies either focus solely on learning from the expert dataset or treat the general demonstration data as negative examples, impeding the effective utilization of the low-cost demonstration data that are readily available and can still contain valuable fragmented skills.\nIn this paper, we propose a novel SeRLA method, which stands for Skill-enhanced Reinforcement Learning Acceleration from demonstrations, to address the problem of learning from heterogeneous demonstration data and accelerating the downstream RL with the learned knowledge. SeRLA accelerates RL by pursuing skill-level learning in two stages with three coherent components: a skill-level adversarial Positive-Unlabeled (PU) learning model, a skill-based policy learning algorithm, and a skill-level data enhancement technique. In the offline skill prior training stage, the skill-level adversarial PU learning model induces useful skill prior knowledge by exploiting the general and task-agnostic demonstration data as unlabeled examples in addition to the positive expert data, instead of simply differentiating them. This strategy facilitates improved utilization of the extensive low-cost demonstration data and help alleviate the scarcity of the expert data. In the online downstream RL policy training stage, a skill-based soft actor-critic algorithm is deployed to integrate skills learned in the offline stage and accelerate skill policy learning. Moreover, a simple but novel Skill-level Data Enhancement (SDE) technique is introduced to improve the robustness of skill learning and adaptation at both stages. We conduct experiments on four standard RL environments by comparing the proposed SeRLA with the state-of-the-art skill-based imitation learning methods: SPiRL [17] and model-based SkiMo [18].\nThe main contributions of this paper can be summarized as follows:\n\u2022 This is the first work that conducts skill-level Positive-Unlabeled Learning for LfD/IL. The proposed SERLA takes the low-cost general demonstration data as unlabeled examples to statistically support skill learning from the limited positive examples (i.e., the expert demonstration data) through skill-level adversarial PU learning.\n\u2022 We propose a simple but novel skill-level data enhancement (SDE) technique, which automatically augments the skill-level representations for both the skill prior learning and the downstream policy learning processes to improve the robustness of the learned skill prior and accelerate the skill-policy function training.\n\u2022 The proposed SeRLA produces effective empirical results for accelerating downstream RL tasks. It largely outperforms the standard skill prior learning method, SPiRL, while producing notable improvements over the state-of-the-art model-based skill-level method, SkiMo, in the early downstream training stage."}, {"title": "Related Works", "content": "RL from Demonstrations Learning from Demonstration (LfD) or imitation learning (IL), unlike offline RL which learns optimal policies solely from offline data, focuses on accelerating downstream RL tasks by pre-training the RL agent on a small set of expert demonstrations without reward signals [7, 8]. In addition to the limited expert demonstrations, large task-agnostic demonstration datasets can also be collected inexpensively [9-12] from the environment for extracting potential learnable behaviors through LfD. There are three major methods to solve LfD/IL problems: Behavior Cloning (BC) [7], Inverse Reinforcement Learning (IRL) [21], and Generative Adversarial Imitation Learning (GAIL) [22]. BC learns a direct mapping from observations to actions through supervised learning but struggles with generalization and distribution shifts [23, 24]. IRL infers reward functions from demonstrations and trains the agent using standard RL algorithms, but it is computationally expensive and relies on the reward model's effectiveness [25]. GAIL uses a generative adversarial network where a discriminator distinguishes between agent and expert behaviors, achieving strong performance despite needing many interactions with the environment [26].\nSkill-Based RL Skill-based RL methods extract reusable skills as abstracted long-horizon behavior sequences of actions [13, 15, 17, 27, 14, 9, 11, 16], where the skills are predefined by experts or extracted from datasets. In recent works, skill-Prior RL (SPiRL) accelerates downstream RL tasks using learned skill priors from offline demonstration data [17]. Skill-based Learning with Demonstration (SkiLD) regularizes policy learning in downstream tasks using a skill posterior [28]. Few-shot Imitation with Skill Transition Models (FIST) learns skills from few-shot demonstration data and generalizes to unseen tasks [19]. Adaptive Skill Prior for RL (ASPiRe) adaptively learns distinct skill priors from different datasets [20]. Skill-based Model-based RL (SkiMo) applies planning on downstream tasks using a skill dynamics model [18]."}, {"title": "Problem Setting", "content": "Reinforcement Learning from Demonstrations (LfD) aims to accelerate the online downstream RL procedure by leveraging offline demonstration datasets. Specifically, we assume LfD has access to two demonstration datasets: a limited expert dataset \\(D_{\\pi_e}\\) and a low-cost general demonstration dataset \\(D_{\\pi}\\). The expert dataset is a task-specific small offline dataset that contains expert demonstration trajectories (state-action sequences) \\(D_{\\pi_e} = \\{\\langle s_0, a_0, \\dots, s_t, a_t, \\dots \\rangle\\}\\), which are generated by human experts or fully trained RL agents. The general demonstration dataset is a much larger task-agnostic offline dataset that consists of randomly collected trajectories \\(D_{\\pi} = \\{\\langle s_0, a_0, \\dots, s_t, a_t, \\dots \\rangle\\}\\). The action sequences contained in \\(D_{\\pi_e}\\) and \\(D_{\\pi}\\) can be denoted as \\(A_{\\pi_e}\\) and \\(A_{\\pi}\\), respectively. While the general demonstration dataset does not provide as much useful information as the expert dataset, it may still contain short-horizon behaviors that, if properly extracted, can guide the RL agent to behave with feasible actions and propel the policy learning.\nThe downstream RL task is a standard reinforcement learning problem that can be represented as a Markov Decision Process (MDP) \\(M = (S, A, T, R, \\gamma)\\), as described in [6]. In this MDP, \\(S\\) is the state space, \\(A\\) is the action space, \\(T : S \\times A \\rightarrow S\\) is the transition dynamics \\(p(s_{t+1}|s_t, a_t)\\), \\(R : S \\times A \\rightarrow R\\) is the reward function, and \\(\\gamma \\in (0, 1)\\) is the discount factor. The goal is to learn an optimal policy \\(\\pi^* : S \\rightarrow A\\) that maximizes the expected discounted cumulative reward (return): \\(\\pi^* = \\arg \\max_{\\pi} J_{\\gamma}(\\pi) = E_{\\pi}[\\sum_{t=0}^{T} \\gamma^t r_t]\\). The goal of this work is to learn useful skill prior from the offline demonstration datasets and then deploys such skill-level knowledge to facilitate fast policy training for the downstream RL task."}, {"title": "Method", "content": "The proposed SeRLA method has two stages: the offline skill prior training stage with skill-level adversarial PU learning and the online skill-based downstream policy training. The skill prior training induces useful high-level skill knowledge in form of skill prior from the given demonstration datasets (\\(D_{\\pi_e}\\) and \\(D_{\\pi}\\)), which is then used to accelerate the downstream skill-based policy training through a Skill-based Soft Actor-Critic (SSAC) algorithm. Moreover, a simple skill-level data enhancement technique is further devised for the two training stages to improve the overall performance. Below, we elaborate on these approach components."}, {"title": "Skill Prior Training with PU Learning", "content": "In the skill prior training stage, we build a regularized deep autoencoder model with skill-level adversarial PU learning to learn a conditional skill prior distribution function \\(q_{\\psi}(z_t|s_t)\\) from the trajectories provided in the two demonstration datasets, where latent variables \\(\\{z_t\\}\\) are used to capture the high level representations of skills, each of which can be interpreted as an action sequence. The model includes a skill encoder network \\(q_{\\mu}(\\cdot)\\), a skill decoder network \\(p_{\\nu}(\\cdot)\\), a skill prior network \\(q_{\\psi}(\\cdot)\\), and a discriminator \\(D_s\\). The first three components can be learned from the expert data \\(D_{\\pi_e}\\) within a conventional autoencoder framework, while the discriminator is innovatively deployed to incorporate the general demonstration data \\(D_{\\pi}\\) into the skill prior training process and alleviate skill data sparsity via adversarial PU learning.\nConventional Skill Learning Framework Under a deep autoencoder framework [17], the skill encoder \\(q_{\\mu}(z_t|a_t)\\) takes an action sequence \\(a_t = \\{a_t, \\dots, a_{t+H-1}\\}\\) with length \\(H\\) as input and maps it to a latent skill embedding \\(z_t\\). Conversely, the skill decoder \\(p_{\\nu}(\\hat{a}_t|z_t)\\) reconstructs an action"}, {"title": "Skill-level Adversarial PU Learning", "content": "Different from the expert data, the randomly collected large demonstration data \\(D_{\\pi}\\) can present a great number of short-horizon behaviors, some of which can be meaningful while many others can be spontaneous or arbitrary. Hence it is not suitable to directly deploy \\(D_{\\pi}\\) in the autoencoder model above in the same way as the expert data. To effectively filter out the noisy behaviors and exploit the useful ones from \\(D_{\\pi}\\), we deploy a PU learning scheme to perform skill learning simultaneously from both the small expert data \\(D_{\\pi_e}\\) and the large general demonstration data \\(D_{\\pi}\\). Specifically, we treat the skills (capturing the behaviors) from the expert data, \\(Z_e = q_{\\mu}(A_{\\pi_e})\\), as positive examples (i.e., useful skills), and treat skills from the general demonstration data, \\(Z = q_{\\mu}(A_{\\pi})\\), as unlabeled examples that can be either positive or negative. Then we propose to learn a binary probabilistic discriminator \\(D_s\\) from the positive and unlabeled skill examples by adapting a standard non-negative PU risk function derived in the literature [35] into the skill-level learning:\n\\(\\mathcal{L}(q_{\\mu}(A_{\\pi_e}), q_{\\mu}(A_{\\pi})) = \\alpha \\mathcal{L}_{D_s}^p(q_{\\mu}(A_{\\pi_e})) + \\max(-\\xi, \\mathcal{L}_{D_s}^u(q_{\\mu}(A_{\\pi})) - \\alpha \\mathcal{L}_{D_s}^n(q_{\\mu}(A_{\\pi_e})))\\)   (4)\nwhere \\(\\alpha > 0\\) and \\(\\xi > 0\\) are hyperparameters. Here the true positive risk \\(\\mathcal{L}_{D_s}^p(q_{\\mu}(A_{\\pi_e}))\\) is calculated on positive skill examples \\(Z_e\\), while the true negative risk is calculated on both positive and unlabeled data (\\(Z_e\\) and \\(Z\\)) using two terms, \\(\\mathcal{L}_{D_s}^u(q_{\\mu}(A_{\\pi}))\\) and \\(\\mathcal{L}_{D_s}^n(q_{\\mu}(A_{\\pi_e}))\\). These risk terms are defined in terms of the discriminator \\(D_s\\) as follows:\n\\(\\mathcal{L}_{D_s}^p(q_{\\mu}(A_{\\pi_e})) = \\mathbb{E}_{a_t \\sim A_{\\pi_e}}[\\log(1 - D_s(z_t \\sim q_{\\mu}(\\cdot|a_t))))]\\)   (5)\n\\(\\mathcal{L}_{D_s}^u(q_{\\mu}(A_{\\pi})) = \\mathbb{E}_{a_t \\sim A_{\\pi}}[\\log(D_s(z_t \\sim q_{\\mu}(\\cdot|a_t))))]\\)   (6)\n\\(\\mathcal{L}_{D_s}^n(q_{\\mu}(A_{\\pi_e})) = \\mathbb{E}_{a_t \\sim A_{\\pi_e}}[\\log(D_s(z_t \\sim q_{\\mu}(\\cdot|a_t))))]\\)   (7)\nwhere \\(D_s(z_t)\\) predicts the probability of the given skill vector \\(z_t\\) being a positive example and \\((1 - D_s(z_t))\\) denotes the probability of the given skill vector \\(z_t\\) being a negative example.\nThis PU loss \\(\\mathcal{L}\\) can be integrated into the deep skill learning model in an adversarial manner to enable the exploitation of the large demonstration data \\(D_{\\pi}\\): the discriminator \\(D_s\\) will be learned to minimize the PU loss in Eq.(4) given the skill examples extracted, while the skill encoder \\(q_{\\mu}(\\cdot)\\) will be learned to maximize the PU loss, aiming to alleviate the scarcity of expert data and generalize the skill learning to the large general demonstration data \\(D_{\\pi}\\)."}, {"title": "Skill Prior Training Algorithm", "content": "The total loss function for adversarial PU-learning based skill prior training can be expressed as the sum of four terms:\n\\(\\mathcal{L}(\\mu,\\nu,\\psi) = \\mathcal{L}_{rec}(\\nu, \\mu) + \\mathcal{L}_{prior}(\\psi, \\mu) + \\beta \\mathcal{L}_{reg}(\\mu) - \\rho \\min_{D_s} \\mathcal{L}(q_{\\mu}(A_{\\pi_e}), q_{\\mu}(A_{\\pi}))\\)   (8)\nwhere the reconstruction loss \\(\\mathcal{L}_{rec}\\) enforces consistency between the skill embedding \\(z_t\\) and the action sequence \\(a_t\\); the prior training loss \\(\\mathcal{L}_{prior}\\) ensures that the generated skill is consistent with the"}, {"title": "Downstream Policy Training", "content": "In the downstream policy training stage, we aim to exploit the skill knowledge learned from the demonstration data, encoded by the skill prior network \\(q_{\\psi}(\\cdot)\\) and decoder network \\(p_{\\nu}(z_t)\\), to accelerate the online RL process. To this end, we train a skill-based policy network \\(\\pi_{\\theta}(z_t|s_t)\\) for the downstream online RL task with skill-level behavior cloning.\nSpecifically, following the work [17], when interacting with the environment, we sample a skill \\(z_t\\) from the skill policy network \\(\\pi_{\\theta}(\\cdot|s_t)\\) given the current state \\(s_t\\). The skill \\(z_t\\) is then decoded to an action sequence \\(a_{t:t+H-1}\\) using the skill decoder \\(p_{\\nu}(\\cdot|z_t)\\) to guide the RL agent to reach state \\(s_{t+H}\\) in \\(H\\) steps. The cumulative reward over the \\(H\\) steps, i.e., the \\(H\\)-step reward, can be collected from the environment as \\(r_t = \\sum_{t}^{t+H} r_t\\). With the online skill-based transition data \\(D = \\{\\langle s_t, z_t, r_t, s_{t+H} \\rangle\\}\\), we deploy a Skill-based soft actor-critic (SSAC) algorithm utilized in [17] to conduct skill-based policy learning with skill-level behavior cloning. SSAC extends soft actor-critic (SAC) [37] to learn the skill policy function network \\(\\pi_{\\theta}(z_t|s_t)\\) with the support of a skill-based soft Q-function network \\(Q_{\\phi}(s_t, z_t)\\). It utilizes a KL-divergence regularization term to enforce the skill policy function \\(\\pi_{\\theta}(z_t|s_t)\\) to clone the behavior of the pre-trained skill prior network \\(q_{\\psi}(z_t|s_t)\\). In particular, SSAC learns the skill policy function network by maximizing the following regularized expected skill-based Q-value:\n\\(J(\\theta) = \\mathbb{E}_{s_t \\sim D,\\atop z_t \\sim \\pi_{\\theta}}[Q_{\\phi}(s_t, z_t) - \\alpha KLD(\\pi_{\\theta}(z_t|s_t), q_{\\psi}(z_t|s_t))]\\).   (9)"}, {"title": "Skill-Level Data Enhancement", "content": "Collecting expert demonstrations can be challenging and expensive, due to the involvement of human experts [8]. The scarcity of the limited expert data however hinders the robust and effective learning of skills. To alleviate the problem, in addition to incorporating general random demonstration data during the prior training stage, we further propose a Skill-level Data Enhancement (SDE) technique to augment the skill-level data in both the skill prior training stage and the downstream policy training stage, improving the robustness of learning at the skill-level."}, {"title": "Experiment", "content": "We conduct experiments with four demonstration-guided tasks with long-horizon and sparse rewards in four different environments that are commonly used for skill learning: Maze, Kitchen, Mis-aligned Kitchen from the D4RL datasets [38] and CALVIN [18] from CALVIN challenge [39]. Maze is a navigation environment, in which a point mass agent is required to find a path between a randomly initialized starting point and a goal point [28]. Kitchen is a robotic manipulation environment in which a robotic arm completes a sequence of four sub-tasks (Microwave-Kettle-Bottom Burner-Light) [11]. Mis-aligned Kitchen is modified from the Kitchen environment\nwith a different task sequence (Microwave-Light-Slide Cabinet-Hinge Cabinet) [28, 18]. CALVIN is a Language-Conditioned Policy Learning challenge[39], which has been adapted for skill learning in SkiMo [18], requiring the RL agent to complete a sequence of four sub-tasks in order: Open Drawer, Turn on Lightbulb, Move Slider Left, and Turn on LED.\nComparison Methods We compared the proposed SERLA with two state-of-the-art skill-based methods, SPIRL [17] and SkiMo [18], which train skill priors for the downstream tasks."}, {"title": "Experimental Results", "content": "We conducted experiments on the four environments to compare the proposed full method, SeRLA, and its variant without SDE, SERLA-w/o-SDE, with the other two skill-based methods, SPiRL and SkiMo. The skills were learned on the demonstration data prior to the downstream task and the reward was evaluated in the downstream RL learning process over \\(10^7\\) environment steps. The maximum trajectory reward for Maze environment is 1, while for Kitchen, Mis-aligned Kitchen, and CALVIN environments, it is 4. The experimental results are presented in Figure 1. The four plots on the left side report results (return v.s. environment steps) on the four environments separately. We can see that the proposed SeRLA-w/o-SDE largely outperforms the baseline SPiRL across all the four environments, especially on Maze, Mis-aligned Kitchen and CALVIN, which validates the effectiveness and contribution of the proposed PU Learning component and SSAC algorithm in extracting and deploying skill knowledge. The proposed full approach SeRLA further boosts the performance over SeRLA-w/o-SDE with notable gains across the four plots, which verifies the effectiveness of the proposed skill-level data enhancement technique. SeRLA also outperforms the model-based state-of-the-art SkiMo and produces best results on Kitchen and Mis-aligned Kitchen. On the other two environments, Maze and CALVIN, SERLA yields comparable overall performance to SkiMo throughout the downstream RL training, while producing the best results in the early training stage."}, {"title": "Conclusion", "content": "In this paper, we proposed a novel two-stage skill-level learning method SeRLA to exploit offline demonstration data and accelerate downstream RL tasks. SeRLA deploys skill-level adversarial PU learning to learn reusable skills from both limited expert demonstration data and large low-cost demonstration data. Then a skill-based soft actor-critic algorithm is deployed to utilize the learned skill prior knowledge and accelerate the online downstream RL through skill-based behavior cloning. The proposed approach conveniently provides a new augmentation space at the skill level without interfering with the real action space, which enables novel skill-level data enhancement (SDE) in both training stages. Our experimental results on four benchmark environments demonstrate that SERLA outperforms two state-of-the-art skill learning methods, SPiRL and SkiMo, especially in the early downstream training stage. Given the promise and effectiveness of our simple but innovative skill-level data enhancement technique, we plan to further investigate various data augmentation strategies in the high level latent skill space in the future."}]}