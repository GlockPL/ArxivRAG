{"title": "NEURAL NETWORKS DECODED: TARGETED AND\nROBUST ANALYSIS OF NEURAL NETWORK DECISIONS\nVIA CAUSAL EXPLANATIONS AND REASONING", "authors": ["Alec F. Diallo", "Vaishak Belle", "Paul Patras"], "abstract": "Despite their success and widespread adoption, the opaque nature of deep neu-\nral networks (DNNs) continues to hinder trust, especially in critical applications.\nCurrent interpretability solutions often yield inconsistent or oversimplified expla-\nnations, or require model changes that compromise performance. In this work, we\nintroduce TRACER, a novel method grounded in causal inference theory designed\nto estimate the causal dynamics underpinning DNN decisions without altering\ntheir architecture or compromising their performance. Our approach systematically\nintervenes on input features to observe how specific changes propagate through\nthe network, affecting internal activations and final outputs. Based on this analysis,\nwe determine the importance of individual features, and construct a high-level\ncausal map by grouping functionally similar layers into cohesive causal nodes,\nproviding a structured and interpretable view of how different parts of the network\ninfluence the decisions. TRACER further enhances explainability by generating\ncounterfactuals that reveal possible model biases and offer contrastive explanations\nfor misclassifications. Through comprehensive evaluations across diverse datasets,\nwe demonstrate TRACER's effectiveness over existing methods and show its po-\ntential for creating highly compressed yet accurate models, illustrating its dual\nversatility in both understanding and optimizing DNNs.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural networks have demonstrated transformative potential across various applications, notably\nimage classification (Krizhevsky et al., 2012), medical diagnostics (Esteva et al., 2017), and complex\npattern recognition (LeCun et al., 2015), even surpassing humans in certain domains (Silver et al.,\n2016; Rajpurkar et al., 2017). Yet, their inherent complexity obscures their decision-making processes,\nturning them into \u201cblack boxes\u201d that raise transparency and trust concerns, thus impeding their\nadoption in sectors requiring explainability, such as healthcare and cybersecurity (Zeiler & Fergus,\n2014; Castelvecchi, 2016; Doshi-Velez & Kim, 2017; Lipton, 2018; Papernot & McDaniel, 2018;\nZhang et al., 2021). Neural Network Explainability, pivotal in Explainable AI (XAI), aims to clarify\nDNN decision-making to ensure trust, ethical application, and bias mitigation. Although various XAI\nstrategies have been proposed, including saliency maps (Zhou et al., 2015), Grad-CAM (Selvaraju\net al., 2017), LIME (Ribeiro et al., 2016), and SHAP (Lundberg & Lee, 2017), they often present\ninconsistencies, over-simplification, or architectural constraints, underscoring an ongoing challenge\nin DNN understanding (Baehrens et al., 2010; Ba & Caruana, 2014; Rudin, 2019).\nIn this paper, we introduce TRACER, a novel approach based on causal inference theory (Pearl, 2009),\nto infer the mechanisms through which AI systems process inputs to derive decisions. Recognizing\nthat conventional evaluation metrics based solely on validation datasets may not be indicative of a\nmodel's performance in real-world settings and drawing inspiration from Pearl's causal hierarchy,\nour approach reveals how targeted modifications to input features influence the internal states of\nneural networks, thereby modelling the underlying causal mechanisms. Specifically, TRACER frames\nthe explainability of neural networks as a causal discovery and counterfactual inference problem,\nwhere we observe and analyze all intermediate and final outputs of a model, given any sample, its\ngenerated set of interventions, and its counterfactuals. Through the aggregation of multiple such\ninstances, we provide interpretability to state-of-the-art models without requiring any re-training or"}, {"title": "2 RELATED WORK", "content": "Various techniques have been developed for DNN interpretability, typically categorized by explain-\nability scope, implementation stage, input/problem types, or output format (Adadi & Berrada, 2018;\nAngelov et al., 2021; Vilone & Longo, 2021). Early endeavours like saliency maps by Zhou et al.\n(2015), Grad-CAM (Selvaraju et al., 2017) and Layer-wise Relevance Propagation (LRP) (Bach\net al., 2015) visually highlighted key features in input data. Such visual explanation methods often\nproduce inconsistent or coarse explanations, or require structural model changes, sometimes com-\npromising performance or overlooking individual nuances crucial for true comprehension (Rudin,\n2019). Model-agnostic approaches, such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg &\nLee, 2017) offer explanations by approximating model decision boundaries. However, these methods\npotentially face challenges such as resource intensiveness or inconsistencies in local explanations.\nWhile some works attempt to simplify complex DNNs to improve their interpretability (Che et al.,\n2016; Frosst & Hinton, 2017), they often compromise on performance, as simpler models cannot\nalways capture the nuances of complex DNNs. In contrast to the aforementioned methods, rather\nthan merely highlighting influential features, TRACER estimates the causal dynamics that steer DNN\ndecisions, without the need for altering the model or compromising its performance.\nDifferent from associative methods, causal inference techniques probe deeper, seeking to both\nunderstand statistical correlations and uncover the true cause-effect relationships between variables.\nThe idea of merging causal inference with AI is an emerging perspective, advocating for a more robust\nform of explainability. rior works on causal inference for AI have primarily revolved around the use of\ncausal diagrams and structural equation models to gain such associative understanding (Pearl, 2009;\nYang et al., 2019; Xia et al., 2021; Kenny et al., 2021; Chou et al., 2022; Geiger et al., 2022; Kelly\net al., 2023). For instance, methods such as those proposed by Kommiya Mothilal et al. (2021) and\nChockler & Halpern (2024) perform causal reasoning to explain decisions made by image classifiers,\nfocusing on identifying causal elements in the input space, whereas TRACER extends causal reasoning\ndeeper into the structure of DNNs themselves. By combining causal analysis of both the model's\ninternal workings and the input-output relationships, our approach enables explainabibilty at both the\nfeature and network-structure level, providing more comprehensive explanations for DNN behavior.\nRecent advances in explainability have also emphasized the importance of counterfactual explana-\ntions (Feder et al., 2021), which generate hypothetical instances showing how small changes in inputs\nwould alter a model's prediction. Deep generative models proposed for this task, such as those based\non Variational Autoencoders (VAEs) (Pawelczyk et al., 2020; Antor\u00e1n et al., 2020) or Generative\nAdversarial Networks (GANs) (Mirza, 2014; Nemirovsky et al., 2022), typically focus on producing\ncounterfactuals that minimize the required changes to the input features. Our approach improves on\nthese by introducing a dual objective that not only ensures realism through adversarial training but\nalso aligns generated counterfactuals closely with their nearest neighbors in the target class. This\nguarantees that the counterfactuals generated by TRACER are both plausible and interpretable.\nOur proposed approach sets itself apart in two main aspects: (1) rather than only focusing on input\nfeatures, our approach performs an intervention-based analysis that additionally examines the causal"}, {"title": "3 THEORETICAL FOUNDATIONS AND METHODOLOGY", "content": "To understand the internal-workings of DNN architectures, we must consider not only the operations\nperformed by individual layers, but also how they influence one another across the network. TRACER\naims to estimate an accurate model of these mechanisms, focusing on the dynamics that govern the\nnetwork's decisions. Therefore, our methodology, depicted in Figure 1, is structured around:\nCausal discovery: We analyze the interactions and dependencies within DNNs by systematically\naltering input features to observe the resulting changes, enabling an effective mapping of the decision\npathways. Through this process, we estimate the causal structures that drive the network's decisions,\nproviding a clear understanding of how different features and layers contribute to the outcome.\nCounterfactual generation: We simulate alternative scenarios by introducing targeted changes to\ninput features, allowing us to explore 'what-if' scenarios and observe how specific changes in inputs\ncan lead to different outcomes, providing further insights into the model's sensitivity and robustness."}, {"title": "3.1 CAUSAL THEORY", "content": "Causal theory provides the means to model cause-effect relationships, offering a departure from mere\nobservational statistics to tackle questions about interventions and counterfactuals (Pearl, 2009). \u03a4\u03bf\nthis end, the language of Structural Causal Models has been proposed to formalize these relationships.\nDefinition 1 (Structural Causal Model). A Structural Causal Model (SCM) M is a 4-tuple\n(U, V, F, P(U)), where U is a set of exogenous variables determined by factors external to the\nmodel; V = {V1, V2, . . ., Vn } is a set of endogenous variables, each influenced by variables within\nthe model; F = {f1, f2,..., fn} is a set of functions, each fi mapping a subset of U \\cup V to Vi; and\nP(U) is a probability distribution over U. For every endogenous variable Vi, its value is determined\nby Vi = fi(pa(Vi), U\u2081), where pa(Vi) represents the parents or direct causes of Vi, and U\u00bf \u2286 U.\nPearl's Causal Hierarchy (PCH), grounded in SCMs, further refines our understanding by categorizing\ncausal knowledge into three distinct levels, which serve as TRACER's foundations:\n1. Association: We extract dependency structures from the DNN activations and outputs P(Y(i) | X),\nwhere X and Y(i) represent the input and the i-th layer's output variables, respectively;\n2. Intervention: By selectively manipulating feature values, we estimate the intervention distributions\nP(Y(i) | do(X = x;)) to understand the effect of particular features on the final decision;"}, {"title": "3. Counterfactual", "content": "We explore alternative (or hypothetical) input scenarios and compute the\ncounterfactual distributions, P(YX | X = x), which quantifies the model's output distribution if\na certain input were set to a particular value, given that we actually observed another input.\nBy systematically identifying how specific input features and intermediate layer activations influence\nthe model's final predictions, TRACER provides a unique approach, based on the principles of PCH,\nfor capturing an abstract overview of the distinct computational components driving DNN decisions.\nThis structured approach allows us to produce explanations that clarify both the direct influence\nof features and how the model's predictions would change under different input conditions, thus\nproviding a more comprehensive understanding of the decision-making process.\nDefinition 2 (Explanation). Given a d-dimensional input X = x \u2208 \\mathbb{R}^d, an explanation for the output\ny of a model F is a masked input XE = x \u2299 M \u2208 {0,1}^d for which the following conditions hold:\nC1 (Correctness): The model F, when evaluated on the input x, produces the output y.\n(F, x) = (X = x) and F(x) = y.\nC2 (Sufficiency): There exists a mask M \u2208 {0,1}^d such that the resulting explanation XE =\nMx produces the same output as the original input: (F, XE) |= F(M\u2299x) = F(x) = y.\nThis condition ensures that the features selected by M are sufficient to explain y.\nLet a mask M' be defined such that M' \u00a3 M, then (F, x'\u00a3) |= F(M' \u2299 x) \u2260 y.\nC3 (Minimality): The mask M is minimal, meaning that no strict subset M' C M satisfies\nC2. That is, for every mask M' CM, the masked input X' E = M' x is insufficient to\nproduce the same output: \u2200 M' CM, (F,X'E) |\u2260 F(M \u2299 x) = y. C3 guarantees that\nM includes the smallest set of features necessary to explain y.\nNote that (i) in this definition, y can be set to any specific label to produce explanations for mis-\nclassifications or rare events; and (ii) partial explanations can be simplified to binary decisions (i.e.,\nwhether a feature is relevant or not) when computing feature attributions (defined in Section 3.3)."}, {"title": "3.2 CAUSAL DISCOVERY", "content": "To discover a faithful representation of the causal mechanisms underpinning DNN models, we\nperform an intervention-based analysis where we systematically change the values of input features\nand study the effects on a given classifier. Such interventions allow us to prove the models and\nmeasure the effects of specific changes on the classifier's representations. By observing the internal\nstates and outputs of the classifier, we can deduce how specific components contribute to the final\ndecisions, offering an understanding of the model's causal structure and enabling the identification of\nkey nodes or connections that highly influence the model's predictions. Furthermore, by collecting\nthe observed effects of all interventions, we establish an abstract causal map to visualize the interplay\nbetween different network components, and asses their collective influence on the DNN outputs.\nUltimately, these detailed visualizations can potentially be instrumental for debugging or refining\nclassifiers, or for designing more interpretable neural network architectures."}, {"title": "3.2.1 INTERVENTIONS", "content": "In our analysis, interventions are crucial for isolating and understanding the causal significance of\nspecific input features. Given an input vector x \u2208 \\mathbb{R}^d, where d denotes the dimensionality of the input\nspace, an intervention is simulated by replacing a subset of x with a predetermined baseline value b.\nFor a specified subset of indices I \u2286 {1, ..., d} corresponding to the features under intervention, the\nintervened features are given by: x' = b \u2022 1{i\u2208I} + xi \u2022 (1 \u2212 1{i\u22081}), where 1{i=1} indicates 1 when\ni is in the set I and 0 otherwise. Assuming b to be causally independent (e.g., binary mask), all input\nfeatures, before and after interventions, can be considered exogenous variables in the causal map.\nProposition 1 (Causal Isolation of Intervened Samples). Let F : X \u2192 Y denote the mapping\nfunction of a DNN. For any x \u2208 X, I \u2286 {1, . . ., d}, and b \u2208 \\mathbb{R}, the intervened sample x' isolates the\ncausal effect of the features in I on F by setting the values of xi, \u2200i \u2208 I to b. (Proof in Appendix A.1)\nBy performing such interventions, we effectively isolate and examine the causal impact of specific\nfeatures on the output. Through these controlled perturbations, we can determine which features are\ncausally pivotal for the model's decisions, and measure the depth of their influence."}, {"title": "3.2.2 CAUSAL ABSTRACTION", "content": "By systematically collecting intermediate and final outputs of the classifier, given an input sample\nand its interventions, TRACER enables a focused comparison of representations across network layers\nand extrapolates an accurate estimation of the causal dynamics driving the network's decisions. For\nthis analysis, we use Centered Kernel Alignments (CKA), a prevalent approach for quantifying the\nsimilarity between high-dimensional embeddings. Let fi and f; denote the activations of two distinct\nlayers in a neural network for a set of input samples, and let their respective kernel matrices be\ndefined as Ki = fififf and Kj = fiff. Their CKA similarity can then be obtained as:\nCKA(Ki, Kj) = HSIC(Ki, K\u2081)/\u221a/HSIC(Ki, Ki) \u00d7 HSIC(Kj, Kj),\nwhere HSIC(Ki, Kj) is the Hilbert-Schmidt Independence Criterion (HSIC) for the kernel matrices,\nand given by HSIC(Ki, Kj) = (n \u2212 1)\u22122 Tr(HK\u00bfHKj). Here, H is a centering matrix given by\nH = I \u2013 11, with n being the number of samples, I the identity matrix, and 1 a vector of\nones. Tr() denotes the trace of a matrix. The use of CKA for evaluating representation similarity\noffers several advantages, including: (i) Normalization: CKA scores range from 0 (completely\ndissimilar) to 1 (identical), allowing straightforward comparison across layers; (ii) Flexibility: It\naccommodates various kernel functions, such as linear or Gaussian, enabling flexibility based on\nspecific requirements of the analysis; and (iii) Robustness: The use of kernels allows CKA to operate\nin a richer feature space, providing a more comprehensive similarity measure. Given these properties,\nCKA stands out as a suitable choice for our similarity analysis of feature representations.\nUpon obtaining the similarity measures, we establish causality by grouping layers based on their\nCKA values, where we create a binary matrix B(Ki, K\u2081), which is defined as B(Ki, Kj) =\n1 if CKA(Ki, Kj) \u2265 1 \u2212 \u03f5, and 0 otherwise, with \u03f5 representing a predetermined threshold that\ndefines the maximum acceptable dissimilarity for two layers to be considered alike. For our causal\nanalysis, such similarity suggests that these layers contribute to a shared causal node representing an\nendogenous variable and describing a distinct structural equation in our causal model."}, {"title": "Definition 3 (Layer Groups)", "content": "Let F(x) = fo \u00b0 ... \u25cb fk(x) denote the compositional form of the\nneural network classifier, with fi representing the i-th layer of the network. And let B denote the\nbinary CKA matrix. Two distinct layers fi and fj are said to belong to the same layer group if and\nonly if |i - j| = 1 and B(Ki, Kj) = 1."}, {"title": "Theorem 1 (Layer Grouping)", "content": "Let a sequence of layers {fi, fi+1,..., fj} within a neural net-\nwork F(x) be classified under the same Layer Group if B(Ki, Ki+1) = B(Ki+1, Ki+2) = ... =\nB(Kj\u22121,Kj) = 1, where B(Ki, Kj) = 1 if CKA(Ki, Kj) \u2265 1 \u2212 \u03f5. The collective causal\ninfluence of this sequence on F(x)'s output is encapsulated by a single composite layer gij:\nF'(x) = fo \u00b0 ... \u25ca gij \u25ca . . . \u25ca fk (x), where gij = fj \u00b0 fj\u22121 \u00b0 . . . \u00b0 fi = fi. (Proof in Appendix A.2)\nThis definition of \"Layer Groups\" aggregates layers into cohesive groups, where each group estimates\na distinct node in the decision mechanism of the network. Through this aggregation, we effectively\nabstract the composition of layers into single causal nodes when their computations are found to be\nredundant, allowing for a more streamlined and high-level understanding of the network's processes."}, {"title": "Theorem 2 (Necessary and Sufficient Conditions for Causal Nodes)", "content": "Let F : \\mathbb{R}^n \u2192 \\mathbb{R}^m be a\nDNN defined by composition as F = fk \u00b0 . \u25cb f1 where each fi : \\mathbb{R}^{di-1} \u2192 \\mathbb{R}^{di} represents the\ntransformation applied by the i-th layer, d\u2081 = n, and dk = m. Let g = g(r) = . = g(s) = {fi}i=r\nwith 1 < r < s < k be a subset of consecutive layers. g constitutes a causal node if and only if\n\u2200i \u2208 {r, . . ., s - 1}, CKA(Ki, Ki+1) \u2265 1 \u2212 \u025b, where Ki is the kernel matrix of layer i and \u025b \u2208 (0, 1)\nis a predefined similarity threshold. (Proof in Appendix A.3)"}, {"title": "3.3 ESTIMATION OF CAUSAL EFFECTS", "content": "To quantify the causal impact of interventions on the neural network's outputs, we define the Average\nCausal Effect (ACE) to quantitatively capture both the direction and magnitude of the effect caused\nby altering the input features.\nDefinition 5 (Average Causal Effect). Let g(x) = softmax(gi(x)) and g(x') = softmax(gi(x'))\ndenote the normalized outputs of a Layer Group gi for a given input x and its intervention x'. The\nnormalization of these outputs is performed to transform the activation scores into valid probability\ndistributions, with which the Average Causal Effect (ACE) can be defined as the expected value of\nthe product of the signed Kullback-Leibler (KL) divergence between their probability distributions:\nACE\u2081 = [Ep(x) [|| KL(P(g(x) | do(X = x')) || P(g(x) | do(X = x)))],\nwhere \u2206 = g(x) - g(x') represents the sign of the change induced by the intervention, and KL(\u00b7)\nrepresents the KL divergence quantifying the changes between the probability distributions.\nThis definition provides a robust estimation of the causal effects, offering a comprehensive view of\nhow specific interventions are reflected within the neural network.\nRemark. Any intervention that produces outputs sufficiently similar to those produced by the original\ninput has little to no impact on the Average Causal Effect.\nIf the intervention on input x to produce x' results in minimal change in the output of a Layer Group gi,\nsuch that g'(x) \u2248 g'(x'), then with all other features of x remaining untouched, the change induced\nby x' approaches 0, leading to minimal or negligible contribution. Formally, if g'(x) \u2248 g'(x'), then:\nKL(P(g(x) | do(X = x')) || P(g(x) | do(X = x))) \u2248 0 \u21d2 CE\u2081 = 0.\nThis suggests that interventions which do not substantially alter the output of a Layer Group have a\nnegligible causal impact on the model's output, as measured by the ACE. Our approach henceforth\nconsists of generating interventions, such that those with no effect according to our definition above,\nare considered not part of the explanation."}, {"title": "3.4 COUNTERFACTUAL GENERATION", "content": "To improve classification performance and mitigate model biases, we additionally explain misclassi-\nfied samples through a TRACER analysis of counterfactuals, highlighting specific feature changes\nthat should be applied to samples in order to obtain the desired outputs. Counterfactuals are hypo-\nthetical data instances that, if observed, would alter the model's decision. Crafting such instances is\nchallenging due to the constraint that all counterfactuals should be valid and plausible. Therefore,\nusing generative models such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2020),\nwe aim to achieve this task by including such constraints into our training process. Particularly, we\npropose a novel plausibility constraint, whereby the counterfactual generators are trained using both\nadversarial training to ensure realism, and a proximity-based regularization term to enforce similarity\nbetween the generated counterfactuals and real instances from a target class. This ensures that the\ncounterfactuals are realistic and also require minimal changes to the original data. While our proposed\nconstraint can be adapted to various types of generative models (e.g., VAE, GAN, normalizing flows),\nthe model we discuss hereinafter assumes an autoencoder-based GAN architecture."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate our proposed explainability method, TRACER, emphasizing both its\ncausal discovery and its counterfactual analysis facets. We perform our initial experiments using the\nwell-known MNIST (Deng, 2012) and ImageNet (Deng et al., 2009) datasets, which are standards\nin image classification tasks, and on the CIC-IDS 2017 (Sharafaldin et al., 2018) network traffic\ndataset to demonstrate TRACER's applicability to tabular datasets. The MNIST dataset provides a\ncollection of handwritten digits that is ideal for the scrutiny of our methodology, while the diversity\nand scale of ImageNet offer a broader context for evaluating our approach's effectiveness across a\nwide range of image recognition challenges. We use pre-trained AlexNet (Krizhevsky et al., 2012)\nand ResNet-50 (He et al., 2016) models as our MNIST and ImageNet classifier, respectively, and\ndesign a GAN architecture tailored to our counterfactual generation tasks."}, {"title": "4.1 CAUSAL DISCOVERY AND FEATURE ATTRIBUTIONS", "content": "To evaluate the effectiveness of TRACER in un-\ncovering the intricate causal pathways that govern\ndecision-making in DNNs, the relationships be-\ntween activations of different layers are analyzed\nusing their CKA similarities. Comparing activa-\ntions produced by the original input and its cor-\nresponding interventions illuminates the effect of\nthese interventions on the decisions. As depicted in\nFigure 3, TRACER discerns layer groups forming\ncausal nodes and identifies the causal links between\nthem. For instance, eight activation outputs from\nthe AlexNet classifier are observed and analyzed,\nrevealing inherent groupings based on similarity\npatterns across the network layers. This observa-\ntion has led to the identification of four distinct\ncausal nodes. Notably, the lack of causal connec-\ntions between non-adjacent layer groups indicated\na linear causal chain that informs the network's\ndecision for the analyzed sample.\nTo quantitatively assess the reliability of TRACER,\nwe measure how often a given model's predictions\nremain consistent when key features identified by\nour approach are randomly perturbed.\nFormally, let f : X \u2192 Y be the classification model, where X and Y represent the input and output\nspaces, respectively. For a dataset X \u2286 X, each sample x \u2208 X is coupled with an explanation mask\nM(x) generated by an explainability method. Let P denote a perturbation function which modifies x\nby targeting a proportion p of the significant regions of the explanation. With x\u2032 = x \u2299 P(M(x),p)\ndescribing the perturbed sample, the reliability score for the explanations can thus be obtained as:\nS = |X|^{-1} \u03a3_{x\u2208X}1{f(x)=f(x')}, where|X| is the number of samples in the dataset, and\n1{.} is an indicator function returning 1 if the predictions before and after applying the explainability\nmask differ, i.e., f(x) \u2260 f(x').\nThis score captures the sensitivity of the model's\npredictions to changes in areas deemed critical by\nthe explainability method, thereby providing insights\ninto the reliability of the explanations generated. To\nassess the robustness of TRACER and compare its per-\nformance against that of existing explainability meth-\nods, we use this reliability metric on explanations\nproduced by the different approaches when evaluated\non all test samples of the MNIST dataset.\nThe results, depicted in Figure 4, show the average\nand standard deviation of each method's scores over\n10 trials, demonstrating TRACER's superior perfor-\nmance and consistency in producing meaningful and\nreliable explanations."}, {"title": "4.2 COUNTERFACTUAL ANALYSIS", "content": "In this experiment, we explore the use of counterfactuals as a means to understand causes for\nmisclassifications, as well as identify the minimal feature changes required to obtain correct outcomes.\nThrough a comparison of the causal mechanisms uncovered for the misclassified sample with"}, {"title": "4.3 GENERALIZATION AND SCALABILITY", "content": "In this experiment, we highlight the broad adaptability of our approach across various neural network\narchitectures and datasets. To this end, we evaluate TRACER on the ImageNet dataset, as well as\non a Network Intrusion Detection dataset, explaining the decisions of both simple and complex NN\narchitectures such as MLP and ResNet-50.\nGiven the wide variety and realisic nature of the samples in the ImageNet dataset, its classification\nresults with the ResNet-50 architecture provide a solid benchmark for highlighting the limitations\nof existing explainability methods and comparing their performances to that of TRACER. For this\ncomparison, we selected LIME, SHAP, LRP, and Grad-CAM as benchmarks, since they are among\nthe most widely adopted and representative explainability methods in the literature. The results,\ndepicted in Figure 5 show that while existing methods struggle to produce consistent explanations,\nTRACER provides coherent and comprehensive explanations that highlight the most important features\nand patterns that drive the classifier's decisions. Further comparison of these methods, discussed in\nAppendix D.1, highlight more distinctions between TRACER and existing methods, particularly when\nusing DNN architectures that exhibit complex interactions.\nDiving deeper into the versatility spectrum, we challenge TRACER with the intricacies of structured\ndata using the CIC-IDS 2017 network traffic dataset. This dataset, reflecting authentic network\ndynamics, unfolds a distinct set of challenges useful for evaluating explainability methods (e.g.,\ndiverse data types and intertwined correlations). For example, in an instance where a DDoS-attack-\ninduced traffic is erroneously classified as benign (see Appendix D.2), TRACER identifies and\nelucidates features emblematic of the attack through its causal analysis. Specifically, TRACER reveals\nthat features such as port numbers and data transfer dynamics are essential for the detection of such\nthreats. Overall, the granularity and transparency of explanations provided by TRACER, especially in\ndomains such as cybersecurity, accentuate its potential to build trust in critical applications."}, {"title": "4.4 BEYOND LOCAL EXPLAINABILITY", "content": "To evaluate TRACER's capacity for global explainability, we integrated individual local explanations\nto form a comprehensive view of a model's decision logic. For this task, we focus on a random\nsubset of the MNIST dataset, processed through the AlexNet architecture, to derive causal insights\nunderpinning the classifier's decisions for all class samples. The results of this analysis, detailed in\nAppendix E, reveal significant redundancies within AlexNet's architecture for MNIST, allowing us to\ndesign compressed representations of the model to optimize the computational efficiency.\nThe characteristics and comparisons of these compressed models, reported in Table 1, show that the\nmost refined model obtained (C1) exhibits a staggering 99.42% reduction in model size with only a\n0.16% drop in accuracy. This highlights TRACER's potential for catalyzing practical innovations in\nDNN design and optimization, without undermining the predictive performance of these models."}, {"title": "5 DISCUSSIONS AND LIMITATIONS", "content": "In this study, we focused our evaluations of TRACER on white-box neural networks. However, its\nflexibility and design extend beyond, making it equally applicable to black-box models where the\ninternal dynamics remain obscured and only the inputs and outputs are accessible. Under such\nconstraints, TRACER remains valuable, offering two distinct avenues of exploration. First, it can\nanalyze and quantify the influence of input features on the model's prediction. Alternatively, by using\na surrogate white-box model, we can effectively approximate the underlying causal mechanisms\ndriving the predictions. This adaptability underscores TRACER's potential in diverse environments.\nWhile our TRACER approach is highly parallelizable by design, its depth of analysis can require\na trade-off between granularity (the precision of the causal analysis determined by the number of\ninterventions generated for each sample) and computational efficiency."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced TRACER, a novel approach for accurately estimating the causal dynamics\nembedded within deep neural networks. Through seamless integration of causal discovery and\ncounterfactual analysis, our methodology enables a deep understanding of the decision-making\nprocesses of DNNs. Our empirical results demonstrate TRACER's ability to both identify the causal\nnodes and links underpinning a model's decisions, and also leverage counterfactuals to highlight the\nnuances that drive misclassifications, offering clear and actionable insights for model refinement\nand robustness. Beyond local explanations, we showcased the potential of our approach to capture\nthe global dynamics of DNNs, leading to practical advantages such as novel and effective model\ncompression strategies. Through our foundational principles and findings, we have ascertained that\nby producing intuitive, human-interpretable explanations, TRACER offers outstanding transparency\nto neural networks, significantly enhancing their trustworthiness for critical applications."}, {"title": "A PROOFS", "content": null}, {"title": "A.1 PROPOSITION 1 [CAUSAL ISOLATION OF INTERVENED SAMPLES]", "content": "Proof. By our definition of interventions, x' is derived from x by replacing the features indexed by I\nwith a constant b. Assuming this operation to be independent of the original data generation process\nof x and controlled externally, for the features in x' indexed by I, any variation in F(x') with respect\nto changes in these features can be causally attributed to the intervention itself. Hence, within the\nscope of our analysis, the causal effect of the features in I is indeed isolated by the intervention."}, {"title": "A.2 THEOREM 1 [LAYER GROUPING]", "content": "Proof. Let F(x) = fo \u00b0 f1 \u00b0 . . . \u25cb fk(X) represent a DNN", "if": "nCKA(Km, Km+1) \u2265 1-6, \u2200m \u2208 {i, i + 1, ..., j \u2212 1},\nwhere Km = fm fm is the kernel matrix for the layer fm, and e is a small threshold allowing minor\ndissimilarities. Since CKA(Km, Km+1) \u2248 1 \u2212 \u0454, \u2200m \u2208 {i, i + 1, . . ., j \u2212 1}, the activations of\nthese layers are highly similar and functionally redundant"}]}