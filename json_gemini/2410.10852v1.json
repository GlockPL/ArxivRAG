{"title": "SAFELLM: DOMAIN-SPECIFIC SAFETY MONITORING FOR\nLARGE LANGUAGE MODELS: A CASE STUDY OF OFFSHORE\nWIND MAINTENANCE", "authors": ["Connor Walker", "Callum Rothon", "Koorosh Aslansefat", "Yiannis Papadopoulos", "Nina Dethlefs"], "abstract": "The Offshore Wind (OSW) industry is experiencing significant expansion, resulting in increased\nOperations & Maintenance (O&M) costs. Intelligent alarm systems offer the prospect of swift\ndetection of component failures and process anomalies, enabling timely and precise interventions that\ncould yield reductions in resource expenditure, as well as scheduled and unscheduled downtime. This\npaper introduces an innovative approach to tackle this challenge by capitalising on Large Language\nModels (LLMs). We present a specialised conversational agent that incorporates statistical techniques\nto calculate distances between sentences for the detection and filtering of hallucinations and unsafe\noutput. This potentially enables improved interpretation of alarm sequences and the generation\nof safer repair action recommendations by the agent. Preliminary findings are presented with the\napproach applied to ChatGPT-4 generated test sentences. The limitation of using ChatGPT-4 and\nthe potential for enhancement of this agent through re-training with specialised OSW datasets are\ndiscussed.", "sections": [{"title": "Introduction", "content": "As global demand for renewable energy increases in line with sustainability goals, the Offshore Wind (OSW) sector is\npoised to grow rapidly, expected to supply 25% of electricity globally by 2050, circa 1,150 GW [Smith(2023)]. Nearly\n1/3 of the Levelised Cost of Electricity (LCOE) is from Operations and Maintenance (O&M), including inspections,\nroutine servicing, and repairs [BVGAssociates(2019)]."}, {"title": "Literature Review", "content": null}, {"title": "Wind Turbine Alarms", "content": "Currently, O&M planning in the OSW industry is based around alarms from SCADA data, high-\nlighting faults as they occur, complemented by routine inspections, including rope-access and Un-\nmanned Aerial Vehicle (UAV) inspections [BVGAssociates(2019)]. Recent work in the OSW sector\nhas focused on the diagnosis of faults from alarm data, the prediction of subsequent alarms based\non sequences of previous alarms [Walker et al.(2022)Walker, Rothon, Aslansefat, Papadopoulos, and Dethlefs,\nWei et al.(2023b)Wei, Qu, Wang, Liu, Qian, and Zareipour, Wei et al.(2023a)Wei, Wang, Liu, and Qian], reduction of\nchattering alarms leading to alarm overload [Gonzalez et al.(2016)Gonzalez, Reder, and Melero], and decision sup-\nport for O&M staff [Chatterjee and Dethlefs(2020), Chatterjee and Dethlefs(2022)]. Research on alarm predic-\ntion has made use of word embeddings and word2vec [Mikolov et al.(2013)Mikolov, Chen, Corrado, and Dean]\nfrom Natural Language Processing (NLP), treating alarms in a sequence as analogous to words in a sen-\ntence, allowing subsequent alarms to be predicted [Wei et al.(2023b)Wei, Qu, Wang, Liu, Qian, and Zareipour,\nWei et al.(2023a)Wei, Wang, Liu, and Qian].\n[Walker et al.(2022)Walker, Rothon, Aslansefat, Papadopoulos, and Dethlefs] proposes a system based on Long Short-\nTerm Memory (LSTM) and Bidirectional LSTMs (BiLSTMs) for prediction of repair actions from sequences of alarms\nin the OSW domain. This departs from previous works focusing on predicting subsequent faults as opposed to the\nrequired action."}, {"title": "Large Language Models", "content": "In recent years, progress on LLMs has been rapid, leading to wide interest in their adoption for a range of tasks.\n[Zhao et al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu,\nreviews recent progress on LLMs, including pre-training, adaptation tuning, utilisation, and capacity evaluation.\nOpen AI's Chat GPT [OpenAI et al.(2023)OpenAI, :, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altm=\nis perhaps the most widely used LLM-based chatbot, based on GPT-3\n[Brown et al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et al.].\nGPT-4 [OpenAI et al.(2023)OpenAI, :, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, and others\nthe most recent development in the GPT family, has shown high performance in diverse applications. Some sources\nhave reported human-level capabilities, although the precise definition of \u201chuman-level\u201d is debatable and open to\ninterpretation.\n[Lukens and Ali(2023)] investigates how ChatGPT may be applied to the automation of maintenance planning, identi-\nfying a range of criteria for assessment. The performance of ChatGPT is examined, and the risks and business case for\nadoption are discussed. [Li et al.(2023)Li, Wang, and Sun] reviews how recent developments in LLMs are impacting"}, {"title": "Hallucination Detection", "content": "A\nsignificant issue encountered with LLMs is that they can make non-factual and\ninaccurate statements, known as hallucinations.\nIn the GPT-4\ntechnical report\n[OpenAI et al.(2023)OpenAI, :, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, and others.],\nhallucinations are identified as a key issue with current LLMs, becoming more pressing as users build\ntrust with LLMs. In a benchmarking study comparing LLMs to humans in question answering tasks,\n[Lin et al.(2022)Lin, Hilton, and Evans] reports that the best-performing models at the time were truthful 58\n% of the time, while humans were truthful 94 % of the time, with models mimicking common human misconceptions.\n[Huang et al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin, and Liu] presents a survey of halluci-\nnation in LLMs, including a taxonomy of hallucinations and identification of causes. Hallucinations can be broadly\ngrouped as intrinsic and extrinsic hallucinations [Maynez et al.(2020)Maynez, Narayan, Bohnet, and McDonald]. In-\ntrinsic (or closed-domain) hallucinations arise from the source data, while extrinsic (or open-domain) hallucinations\nignore the source data.\nDetection of hallucinations has been a field of rapid development, in tandem with the rise of LLMs. One approach\nto avoiding issues arising from hallucinations is to take steps to prevent them from occurring. In a benchmarking\nstudy using TruthfulQA, [Lin et al.(2022)Lin, Hilton, and Evans] determined that the largest models were the least\ntruthful overall, and that fine-tuning smaller models for specific tasks would be beneficial in terms of truthfulness.\n[Rateike et al.(2023)Rateike, Cintas, Wamburu, Akumu, and Speakman] proposes a method for the detection of hallu-\ncinations in LLM activations from pre-trained models, scanning the internal states using an anomaly-free reference\ndataset.\nAnother approach is to detect hallucinations in generated content from the LLM, and then to remove or correct the\noffending parts of the output. SelfCheckGPT [Manakul et al.(2023)Manakul, Liusie, and Gales] is a hallucination\ndetection algorithm which is capable of fact-checking outputs from LLMs without external resources by comparing\nstochastically sampled responses. The offline approach proposed is effective in many applications but can fail if\nhallucinations are present in all sampled sentences. Chatprotect [M\u00fcndler et al.(2023)M\u00fcndler, He, Jenko, and Vechev]\naims to detect self-contradictory hallucinations in generated content from black-box LLMs, achieving 80 % F1 score for\ndetection of self-contradictory hallucinations on ChatGPT. The detected hallucinations are then refined by a mitigation\nalgorithm to preserve fluency in the generated text. Chainpoll [Friel and Sanyal(2023)] is a hallucination detection\nmethodology that uses the novel metrics of correctness and adherence to detect hallucinations in LLM outputs. Chainpoll\nis tested against other methods using, RealHall, a collection of benchmark datasets also presented by this work."}, {"title": "LLMs and Safety", "content": "As Machine Learning (ML) based systems are being deployed in safety-critical applications with\nlimited human oversight, it is vital that their dependability and robustness can be guaranteed.\n[Bianchi et al.(2024)Bianchi, Suzgun, Attanasio, R\u00f6ttger, Jurafsky, Hashimoto, and Zou] finds that most\npopular instruction following LLMs are susceptible to making unsafe responses when prompted.\n[Dong et al.(2024)Dong, Zhou, Yang, Shao, and Qiao] presents a comprehensive survey on LLM conversation\nsafety, considering Attacks, Defences, and evaluation methods, and presents a taxonomy of studies to date on LLM\nconversation safety. The defences identified are categorised into three groups: input/output filters, inference guidance,"}, {"title": "Research Questions", "content": "In 2020, discussions between experts on LLMs, including representatives from OpenAI, and the Stanford Institute for\nHuman-Centered AI, identified several research questions regarding the future of LLMs. One such research question\nwas\nWhat can academia do to best position itself to develop guardrails for the industrial development of such\nmodels [Tamkin et al.(2021)Tamkin, Brundage, Clark, and Ganguli]? To address this, and the aforementioned\nissues with LLMs in the context of OSW Maintenance, in this paper, we propose a novel approach called SafeLLM\nwhich has the following features:\n\u2022 Statistical methodology for unsafe response prevention in an embedded safety layer.\n\u2022 Hallucination detection to identify incorrect or irrelevant responses.\nWe define essential concepts, describe our methodology, and present preliminary results from an OSW application. The\ndiscussion offers insights into our findings, and the concluding section outlines future research directions, including the\npotential application of Reinforcement Learning (RL)."}, {"title": "Methodology", "content": "Existing model frameworks predominantly focus on the application of Cosine similarity to generate similarity scores of\nsentence embeddings, obtained from both the user input and generated output. However, if the response generated is\npreviously unseen, there is a potential risk it has started to hallucinate from the outset.\nWe propose the use of additional Empirical Cumulative Distribution Function (ECDF) statistical distance measures,\nnamely Wasserstein distance, also known as Earth Mover's Distance (EMD), to calculate sentence similarity. To\nbenchmark our results against existing methodologies, each sentence is tested on both measures: Cosine similarity and\nEMD."}, {"title": "Cosine Similarity", "content": "Cosine Similarity is a distance metric used to measure the similarity of two vectors by calculating the cosine of the\nangle between them. This methodology disregards magnitudinal differences and scale. Simply defined, it is the dot\nproduct of each vector divided by their respective magnitudes: $cos(0) = \\frac{A\\cdot B}{||A|||| B||}$ where A and B are the input vectors,\nor sentence embeddings, of the two sentences being compared."}, {"title": "Wasserstein (EM) Distance", "content": "EMD is a measure for comparison of probability distributions, proven effective in a diverse range of applications\n[Panaretos and Zemel(2019)], including ML and fault detection. Based on the Optimal Transport Theory (OTT), the\nmeasure is considered the most efficient method of moving a mass distribution into another. OTT is defined in the\npractical formulation:\nT : X \u2192 Ytransports \u03bc\u2208 P(X) to v \u2208 P(Y)\nCost can be determined by transporting one unit from x \u2208 X to y \u2208 Y giving a further definition of OTT:\ninfr f ||x - T(x)|| pdP(x)\nBuilding on this theory, EMD considers the movement of distributions to multiple locations, defined as:\nWp(P,Q) = (inf J\u2208j(P,Q) \u222b ||x \u2212 y|| pdJ(x, y))\nWhere, j(P, Q) denotes all joint distributions J for (X, Y) with marginals P and Q.\nCalculating P and Q as the CDF of p and q therefore allows the common simplification of EMD:\nWp(p,q) = \u222b+ \\P - Q|\n8.\n8+"}, {"title": "Data Collection & Pre-Processing", "content": "Confidentiality of data within the OSW sector provides limited access to maintenance records with valuable insight for\ntraining. Notes such as \"WTG1A HV MAINTENANCE\" cause difficulties in the comprehension and implementation\nof the suggested actions. As such, for testing feasibility of EMD as a statistical safety measure, we employ ChatGPT4\nto suggest sentences both unsafe and safe, creating an \"Unsafe Concepts Dictionary\". It should be noted that safe\ncategorisation of a sentence is therefore not verified beyond the existing safe training of ChatGPT, giving scope for\nfuture verification of more comprehensive data sources.\nIn the sentences' raw form, it is not possible to measure similarity using the aforementioned methods. Therefore, it is\nrequired that each sentence is embedded; a method used to encode sentences into vector form. In the work presented\nwithin this paper, we use the Universal Sentence Encoder (USE) transformer-based model. This model takes a string\ninput and provides a fixed dimensional vector output. As with LLMs, the encoding model is pre-trained, meaning\ncomparative analysis of other encoder models will be essential in finding the best suited to OSW application."}, {"title": "Training", "content": "Given an input of an alarm, or sequences of alarms within a timeframe, the LLM should predict and recommend a\nmaintenance procedure. Therefore, the training data is a concatenation of this form; the known maintenance procedure\npaired with correlated alarm sequences. A knowledge graph is also proposed, intended to support the ongoing training\nof an LLM as the complexity of the data increases with time. Once trained, generated responses are analysed through\nthe feature layers of SafeLLM; Hallucination Detection, and Safety, which are discussed in upcoming sections. Once a\nprediction is presented, it is then intended that the maintenance personnel can interact with the LLM as a training aid to\nbreak down the maintenance action(s) where there is ambiguity or lack of knowledge. Figure 4.5 shows an overall view\nof the proposed framework, including the training process.\nWhilst Llama2 is shown as the LLM of choice (see stage 2), other models could be used. With access to real-world\nOSW data in the future, various models will be trained and fine-tuned to select the best performing for this application.\nCurrently ChatGPT is utilised to generate test data."}, {"title": "Safety Filter", "content": "The SafeLLM framework obtains the generated responses' embeddings to calculate EMD against a pre-defined\ndictionary of unsafe concepts. Using the labelled category, we can categorise the response before comparing it to\na fine-tuned threshold. For EMD, the lower the value, the higher the similarity. If the distance is lower than the\nthreshold, we determine it unsafe and alert the O&M manager. Where an unsafe response is detected, and verified as a\ncorrect identification, this is then added to the dictionary for future monitoring. Figure 4.5 suggests an overview of the\nframework."}, {"title": "Hallucination Detection", "content": "Hallucinations occur where responses are generated from unseen inputs, allowing the model to respond without\nconfidence of knowledge gained from training data. We look to address the issue from a view that given an input, the\nmodel should be capable of generating consistent outputs with high similarity scores; failing to achieve this suggests a\nhallucination has occurred.\nThe LLM is proposed to generate N responses for every input, where N can be fine-tuned to optimise computational\nefficiency of the conversational agent whilst maintaining high-level accuracy. Each responses' similarity is then\ncompared using EMD to identify hallucination occurrence."}, {"title": "Hallucination Detection in Large Language Models", "content": "Step 1 - Deviation Matrix Construction:\nLet H = {h1,h2,..., hn} be the set of hypotheses generated by the LLM for a series of inputs. Construct\nthe deviation matrix R where each element rij is the deviation of hypothesis hi from a factual baseline F:\nrij = Deviation(hi, fj)\nStep 2 - Fidelity Constant Matrix, Fmatrix:\nFor each hypothesis hi, let fi be the corresponding ground truth. Calculate the fidelity constant for each\nhypothesis: Fi = \\frac{1}{1+Deviation(hi,fi)}\nStep 3 - Consistency Calculation:\nDetermine a threshold @ for significant deviations. Calculate the consistency of R and Fmatrix: $CR =\\frac{Number of elements in R that are <\\theta}{n^2}$, CF =$\\frac{Number of elements in Fmatrix that are >1-\\theta}{n}$\nStep 4 - Combined Metric:\nCreate a combined metric M using weighted averages of the consistency values from R and Fmatrix, with\nweights WR and WF, respectively: M = WR.CR+WF.CF\nA higher value of M indicates better factual alignment and lower hallucination frequency, while a lower value\nsuggests either significant hallucination, poor model grounding, or both."}, {"title": "Results and Discussion", "content": "This work reports on results using input data in the form of sentences generated by ChatGPT-4. Data is available from\nEDF's Teesside Windfarm, but lacks comprehensible repair actions. Example maintenance actions include \"Work in\nA4\" and \"Back Up Battery Error\", with no clear indication of tangible repair actions. We are currently working with\nengineers to augment this data with maintenance actions associated with each alarm type, allowing the development of\nOSW-specific data in the future. These would in turn help to successfully train and test the LLM for prediction accuracy.\nIn the absence of such data, the accuracy of the proposed SafeLLM methodology is determined by comparing the 'safe'\nand 'unsafe' categorisation to that of ChatGPT's categorisation during generation. Examples of the generated sentences\ninclude, unsafe; \"No fall protection measures should be required as the gearbox is away from hatches.\", and safe; \"PPE\nis mandatory for all aspects of repair tasks.\".\nTable 1 presents a comparison of Cosine similarity against EMD for each category. The accuracy is determined by the\nnumber of sentences correctly identified as safe and unsafe. During testing, each category threshold was incremented in"}, {"title": "Future Work", "content": "Through further development, it is hoped that industry engagement can be established with a fully functional demo\nversion of SafeLLM, gaining valuable data to train and test the models. Further, a comprehensive dictionary of unsafe\nconcepts with less generalised categories can be defined in line with current industry standards.\nSafeLLM focuses on the functional safety of LLMs with less focus on the user Graphical User Interface (GUI). However,\nit is proposed in future work that the implementation of a GUI will benefit the project further and allow successful\nintegration into industry. Future development of the conversation agent is hoped to introduce the framework as a training"}]}