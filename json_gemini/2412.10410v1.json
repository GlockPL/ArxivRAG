{"title": "GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents", "authors": ["Shaofei Cai", "Bowei Zhang", "Zihao Wang", "Haowei Lin", "Xiaojian Ma", "Anji Liu", "Yitao Liang"], "abstract": "Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and introduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. GROOT-2's effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.", "sections": [{"title": "1. Introduction", "content": "Developing policies that can follow multimodal instructions to solve open-ended tasks in open-world environments is a long-standing challenge in robotics and AI research. With the advancement of large-scale pretraining (Baker et al., 2022; Brohan et al., 2022; Brown et al., 2020), the research paradigm for instruction-following policies has shifted from reinforcement learning to supervised learning. In a supervised learning approach, researchers collect large amounts of demonstration data and annotate each demonstration with multimodal instructions\u2014such as videos (Duan et al., 2017; Jang et al., 2022), texts (Lynch et al., 2023; Padalkar et al., 2023), and episode returns (Chen et al., 2021)-using hindsight relabeling. In theory, the instruction-following capability of such policies improves as the dataset grows. However, annotating demonstrations with high-quality multimodal labels is prohibitively expensive, making it challenging to scale these methods in practice.\nAnother line of work (Ajay et al., 2020; Cai et al., 2023b; Lynch et al., 2020c) avoids the need for additional human annotations by learning from demonstration-only data in a self-supervised manner. These approaches leverage latent variable generative models (Kingma & Welling, 2013) to jointly learn an encoder and a latent-conditioned policy. The resulting policy is capable of completing multiple tasks specified by a reference video (Cai et al., 2023b). While a reference video is generally expressive enough to represent various tasks, the inherent ambiguity in videos can lead to a learned latent space that is misaligned with human intention. For example, the encoder module may capture the dynamics between adjacent frames in a video, thereby learning a latent representation of the action sequence\u2014a process we refer to as \u201cmechanical imitation.\u201d While this latent space accurately reconstructs the target action sequence, the resulting latent representation is difficult for human users to leverage during policy deployment. Another potential issue is \u201cposterior collapse,\u201d where the latent space collapses to a single point and loses its influence over the policy during inference. We attribute this mismatch between training and inference to the absence of direct supervision for aligning the latent space with human intention. As illustrated in Figure 2, an ideal controllable latent-induced policy space must strike a balance between these two extremes.\nWe present GROOT-2 (refer to Figure 1), a multimodal instructable agent developed using a latent variable model under weak supervision. To unify the training pipeline, we encode instructions from all modalities as distributions over the latent space. The training objectives consist of two key components: (1) constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the latent-conditioned policy to learn diverse behaviors; and (2) human intention alignment, which uses relatively small sets of multimodal labels to align the latent space with human intentions. Specifically, we apply the maximum log-likelihood method in the latent space for alignment. The underlying principle is that the latent embedding encoded by multimodal labels should also be sampled from the distribution learned from the corresponding video. Our approach is both general and flexible, as demonstrated through evaluations across four diverse environments\u2014ranging from video games to robotic manipulation\u2014including Atari Games (Bellemare et al., 2013), Minecraft (Johnson et al., 2016), Language Table Lynch et al. (2023), and Simpler Env (Li et al., 2024). These experiments highlight GROOT-2 's robust ability to follow multimodal instructions, with extensive tests showing that scaling up either unlabeled or labeled demonstrations further enhances performance."}, {"title": "2. Background and Problems", "content": ""}, {"title": "2.1. Latent Variable Models Enable Controllable Behavior Generation", "content": "In recent years, the GPT series (Brown et al., 2020; Radford, 2018; Radford et al., 2019) has demonstrated impressive capabilities in controllable text generation. Its success can be attributed to self-supervised pretraining and the advantageous properties of natural language. A natural language paragraph contains rich dependencies between sentences. For instance, the title of an article sets the central theme for its body, and the response in a question-answer or dialogue is highly correlated with the preceding text. This characteristic enables large language models, trained via next-token prediction, to achieve controllable text generation through prompts during inference. Unfortunately, such strong correlations do not exist between low-level actions. A desired behavior may not have a necessary preceding trajectory segment. Thus, it isn't easy to prompt a pre-trained policy model to generate a desired behavior. Instead, the generation of actions depends on an underlying latent intention variable. A natural approach is to employ latent variable generative models to jointly model trajectory data and the latent variables that drive them, allowing for controllable behavior generation by manipulating the latent variables during inference. Next, we will elaborate on how latent variable models model trajectory data.\nAs a classic latent variable generative model, Variational Autoencoder (VAE, Kingma & Welling (2013)) has been widely used in fields such as image generation and text generation. With the development of the offline pretraining paradigm, recent years have seen an increasing number of works utilizing VAE to model trajectory data. Typically, its architectures consist of three components: a posterior encoder, a prior encoder, and a policy decoder. The posterior encoder, $q(z|\\tau)$, encodes a specific behavioral trajectory $ \\tau = (o_{1:N}, a_{1:N})$ and generates a posterior distribution over the latent space. When the action sequence can be accurately inferred from the observation sequence (Baker et al., 2022; Zhang et al., 2022)-i.e., when the inverse dynamics model of the environment $P_{IDM}(A_{1:N}|O_{1:N})$ is easily learned\u2014the action sequence can be excluded from the posterior's input (Cai et al., 2023b), thus reducing the distribution condition to $O_{1:N}$. The prior encoder, $p(z|O_{1:k})$, generates a distribution over the latent space based on the history of observations, where $k$ denotes the length of the observation window. When $k = 0$, the prior distribution is independent of historical observations and is typically assumed to follow a standard normal distribution $\\mathcal{N}(0; 1)$. The decoder, $\\pi(a_t|o_{1:t}, z)$, is generally a latent-conditioned policy that takes in the environment's observations along with a specific latent variable to predict the next action to be executed. According to variational inference theory, we can optimize the VAE's modeling capabilities by maximizing the Evidence Lower Bound (ELBO)"}, {"title": "2.2. Modeling Behaviors with VAE Leads to Ambiguous Latent Space", "content": "Several studies on VAE (Abeer et al., 2024; Alemi et al., 2018) have pointed out that the Pareto frontier of the ELBO contains an infinite number of solutions for the latent space, a phenomenon we refer to as latent space ambiguity. To facilitate understanding, we provide an informal illustration in Figure 2, which shows several possible latent spaces when a VAE is used to model behaviors, all having similar ELBO values. We differentiate these latent spaces using the ratio $R = \\frac{BC}{BC+KL}$, where R = 0 and R = 1 represent two extremes of the latent space. When R approaches 0, the latent condition contains much information, nearly dictating every action of the policy's behavior. We refer to this as mechanical imitation, where the VAE effectively degenerates into an Autoencoder (AE). Conversely, when R approaches 1, the latent loses its ability to control the policy's output, a phenomenon known as posterior collapse (Fang et al., 2019; Pagnoni et al., 2018), in which the VAE reduces to an Auto-regressive (AR) model. Intuitively, as R increases, the information encoded in the latent space becomes more high-level, and the policy relies more on environmental feedback (observations) to make decisions that align with the dataset's distribution. On the other hand, when R is smaller, the policy tends to down-weight the environment's observations.\nNot all latent spaces effectively support following a reference video. As shown in Figure 3, the gap between the environment state in the reference video and during policy deployment requires the posterior encoder to extract intentions independent of environmental dynamics. For instance, in the Minecraft task \u201cmining a diamond underground,\u201d a reference video may show a player walking forward and mining a diamond. If the latent encodes only the trajectory sketch, the policy might fail by colliding with obstacles in the deployment environment. This mismatch occurs because humans interpret the video as \u201cmining the diamond\u201d rather than copying specific actions. Aligning the latent space with human intentions is critical for improving policy steerability."}, {"title": "3. Aligning Policy Learners with Weak Supervision", "content": "We explore the development of instructable agents based on latent variable models. To avoid \u201clatent space ambiguity\u201d, we introduce human intention knowledge into the generative pretraining process of the policy model to assist in shaping the latent space. As multimodal labels associated with demonstrations carry rich human intention details, we propose a weakly supervised policy learning algorithm to leverage large amounts of unlabeled demonstration data to learn the latent space while using a small amount of multimodal labeled data to align the latent space with human intention. Ultimately, this enables instructions from all modalities to be unified within the same latent space. Next, we will elaborate on the dataset collection, training pipeline, and inference procedure.\nDataset Collection. We can collect two types of training data from the web: a large set of unlabeled demonstrations $\\mathcal{D}_{dem} = \\{(o_{1:N}, a_{1:N})\\}$ and a relatively small set of annotated demonstrations $\\mathcal{D}_{lab} = \\{(o_{1:N}, a_{1:N}, w_{1:M})\\}$, where o is the image observation provided by the environment, a is the action taken by the policy, w is the word token, N is the length of a demonstration, M is the length of an annotation sentence. The annotation sentence can be multimodal, such as a language sentence (with $M \\geq 1$) or a scaler of the episode return (with $M = 1$), which explains the behavior or outcome of the demonstration from a human's perspective. Since the annotation data is expensive to collect, we have $|\\mathcal{D}_{lab}| < |\\mathcal{D}_{dem}|$.\nTraining Pipeline. Our goal is to learn a shared latent space Z, per-modal instruction encoders $e(z|c)$, and a latent-conditioned policy $\\pi(a_t|o_{\\leq t}, z)$. Leveraging past observations is essential for a policy to make decisions in a partially observable environment such as Minecraft (Johnson et al., 2016). We call the learned policy model GROOT-2, whose training pipeline is shown in Figure 4. For an unlabeled demonstration $(o_{1:N}, a_{1:N})$, we use the encoder module to produce a prior distribution $e(z|o_1)$ and a posterior distribution $e(z|o_{1:N})$. Using the reparameterization trick (Kingma & Welling, 2013), we sample the latent z from the posterior distribution $e(z|o_{1:N})$ and train the policy model, conditioned on z and $o_{1:t}$, to reconstruct the entire action sequence causally. To limit the information presented in the latent space, we introduce an auxiliary KL divergence term in the objective:\nThis allows the model to leverage demonstration-only data to enhance the complexity of the latent space, a process we refer to as \u201cconstrained self-imitating.\u201d For a labeled demonstration", "equations": ["L_{ELBO} = \\mathbb{E}_{z \\sim q(z|o_{1:N})} \\sum_{t=k}^{N} \\sum_{}  \\log \\pi(a_t | o_{1:t}, z)) + D_{KL}(q(z|o_{1:N}) || p(z|o_{\\leq k})).                                                                 \\tag{1}", "\\begin{equation}\\label{eq:Ldem} L_{dem}(o, a) =  \\mathbb{E}_{z \\sim e(z | o_{1:N})} \\Big[\\sum_{t=1}^{N} -\\log \\pi(a_t | o_{1:t}, z) + \\beta_1 D_{KL}(e(z | o_{1:N}) || e(z | o_{1}))\\Big].\\tag{2}\\end{equation}"]}, {"title": "4. Capabilities and Analysis", "content": "We aim to address the following questions: (1) How does GROOT-2 perform in open-world video games and robotic manipulation? (2) Can GROOT-2 follow instructions beyond language and video? (3) What insights can be gained from visualizing the learned latent space? (4) How does GROOT-2 scale with labeled and unlabeled trajectories? (5) What is the impact of backbone initialization on performance? (6) How do language and video losses influence performance?\nEnvironment and Benchmarks. We conduct experiments across four types of representative environments: classical 2D game-playing benchmarks on Atari (Bellemare et al., 2013), 3D open-world gameplaying benchmarks on Minecraft (Johnson et al., 2016; Lin et al., 2023), and Robotics benchmarks on Language Table simulator (Lynch et al., 2023) and Simpler Env simulator (Li et al., 2024),"}, {"title": "5. Related Works", "content": "Learning Policies Across Diverse Domains. Developing policies for sequential control tasks in real and virtual environments poses significant challenges. Research spans domains such as robotic manipulation (Lynch et al., 2023; Yu et al., 2019), video games (Bellemare et al., 2013; Guss et al., 2019), and embodied navigation (Hong et al., 2020; Huang et al., 2023; Savva et al., 2019), with approaches categorized into reinforcement learning (RL) and imitation learning (IL) based on reward function reliance. For video games with dense rewards (e.g., ALE platform (Bellemare et al., 2013)), online RL algorithms can achieve superhuman performance (Badia et al., 2020; Mnih et al., 2015) but suffer from low efficiency, risky interactions, and limited generalization. These challenges restrict their applicability to physical (Padalkar et al., 2023) or embodied environments (Guss et al., 2019), where rewards and cheap interactions are unavailable. IL, as a supervised learning paradigm, addresses these issues through batch efficiency and scalability with large datasets, leveraging Transformer architectures (Jang et al., 2022; Pashevich et al., 2021; Zhang & Chai, 2021). The RT-X series (Brohan et al., 2022, 2023; Padalkar et al., 2023) advances robotic manipulation by training Transformers on large expert demonstration datasets, achieving strong zero-shot generalization. Similarly, Baker et al. (2022) developed a Transformer-based policy for Minecraft using internet-scale gameplay data, solving the diamond challenge. Building on this, Schmidhuber (2019) frames RL as supervised learning, while Chen et al. (2021); Lee et al. (2022) introduce \u201cdecision transformers\u201d to model joint distributions of rewards, states, and actions from offline data, highlighting the potential for unified policy learning within Transformers.\nLearning Policies to Follow Instructions. Enabling policies to follow instructions is key to building general-purpose agents. A common approach involves using language annotations from offline demonstrations to train language-conditioned policies (Abramson et al., 2020; Brohan et al., 2022; Cai et al., 2023a; Huang et al., 2023; Raad et al., 2024; Reed et al., 2022; Wang et al., 2023a,b), leveraging the compositionality of natural language for generalization. However, obtaining high-quality annotations is costly. An alternative uses anticipated outcomes as instructions. Majumdar et al. (2022) trained an image-goal conditioned navigation policy via hindsight relabeling (HER) (Andrychowicz et al., 2017) and aligned goal spaces with text. Similarly, Lifshitz et al. (2023) used this strategy for open-ended tasks in Minecraft. Generative latent variable models offer another solution, using label-free demonstrations to train plan-conditioned policies (Ajay et al., 2020; Lynch et al., 2020b). Extending this, Cai et al. (2023b) applied a posterior encoder to interpret reference videos in Minecraft. Policy learning with weak supervision remains less explored. Lynch & Sermanet (2020) proposed a shared latent space conditioned on language and HER-generated goal images, while Jang et al. (2022) replaced goal images with video labels under full supervision. Jain et al. (2024) trained robots using human videos as task representations but required extensive paired video-trajectory data. Myers et al. (2023) combined labeled and unlabeled trajectories, aligning start-goal pairs with language via contrastive learning, effective for Table Manipulation but limited in handling complex tasks or generalizing to partially observable environments like Minecraft."}, {"title": "6. Conclusions, Limitations and Future Works", "content": "This paper investigates the joint learning of a latent intention space and a multimodal instruction-following policy under weak supervision. We identify the \u201clatent space ambiguity\u201d issue in latent variable generative models when handling text-free trajectory data, arising from the absence of direct human guidance in shaping the latent space. To address this, we propose a weakly supervised algorithm for training GROOT-2. Evaluations across four diverse environments, from video games to robotic manipulation, demonstrate GROOT-2 's generality and flexibility in following multimodal instructions. However, GROOT-2 's reliance on trajectory data for training limits its applicability to video data, which lacks action labels. Considering the abundance and diversity of video data available online compared to trajectory data, extending the weak supervision framework to leverage both play and trajectory data would be a promising avenue for future work."}, {"title": "A. Implementation Details", "content": ""}, {"title": "A.1. Model Architecture", "content": "This section outlines the architectural design choices employed in our approach. GROOT-2 utilizes a Transformer encoder-decoder architecture, augmented with a probabilistic latent space. We detail the components of the model in a structured sequence: extract representations, encode instructions, and decode actions.\nExtract Representations. This paragraph elaborates on the backbone networks used to extract representations from various data modalities. We denote the modalities of image observation, language instruction, and expected returns as $o_{1:N}$, $w_{1:M}$, and r, respectively. For vision inputs, we utilize a pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2020) initialized with CLIP (Radford et al., 2021) weights. Specifically, the t-step image observation $o_t$ is resized to 224 \u00d7 224 and processed to extract 7\u00d77 patch embeddings $x_t = (x_t^{[1]}, ..., x_t^{[49]})$. The video representation $x^v$ is then composed of the averages of these embeddings across the video frames, denoted as $x^v = (avg(x_1), \u2026, avg(x_N))$, where avg() refers to spatial average pooling to minimize computational overhead and N represents the video length. Textual inputs are processed using the BERT encoder (Devlin et al., 2019) of the CLIP model. Rather than utilizing the [CLS] token as the final representation, we retain all word embeddings generated by BERT as $x^w = (x^{[1]}, ..., x^{[M]})$. The BERT model parameters are kept frozen during training. For the scalar-form modality of expected returns, we employ a simple Multi-Layer Perceptron (MLP) to process these values, represented as $x^r \\leftarrow MLP(r)$. These embeddings are then forwarded to subsequent modules.\nEncode Multimodal Instructions with Non-Causal Transformer. Recent works (Lu et al., 2023; Reed et al., 2022; Team et al., 2023) have demonstrated the Transformer's effectiveness in capturing both intra-modal and inter-modal relationships, which inspires us to adopt a unified Transformer encoder for encoding multimodal instructions. This approach offers two significant advantages: (1) It eliminates the need for designing separate architectures and tuning hyperparameters for each modality. (2) It promotes the sharing of underlying representations across different modalities. Instructions are represented as a sequence of embeddings. Before encoding, each embedding is augmented with a modality-specific marker. For instance, video instructions are represented as $(x_1^v + [VID], \u2026, x_N^v + [VID])$, where [VID] is a learnable embedding.\nDecode Action with Causal Transformer. Given a latent z and a temporal sequence of perceptual observations $o_{1:t}$, the policy aims to predict the next action $a_t$. Following prior works (Baker et al., 2022; Cai et al., 2023b; Raad et al., 2024), we employ the Transformer-XL model (Dai et al., 2019) in our policy network, which enables causal attention to past memory states and facilitates smooth predictions. Additionally, we utilize the shared vision backbone to extract vision representations, thereby representing perceptual inputs as $x^v_t$. A significant challenge with this approach is low efficiency: each new observation $x^v_t$ adds up to 49 tokens to the input sequence, substantially increasing memory and computational demands. To address this issue, we introduce a pre-fusion mechanism inspired by Abramson et al. (2020); Alayrac et al. (2022); Lynch et al. (2023). Specifically, we deploy a lightweight cross-attention module $\\texttt{XATTN}(q = \\cdot; kv = \\cdot)$ to perform spatial pooling on $x^v_t$, using z as the query and $(x_t^{[1]}, ..., x_t^{[49]})$ as the keys and values:"}, {"title": "A.2. Hyper-parameters", "content": "Hyper-parameters for training GROOT-2 are shown in Table 6."}, {"title": "B. Atari", "content": "Environment Description. Atari 2600 games contain a lot of diverse video games, which is a widespread benchmark to evaluate the decision-making capability of an agent. The Atari games do not inherently support multitasking concepts; agents are typically tasked with optimizing for the highest possible rewards. However, an advanced human player can deliberately control their gameplay level and achieve any potential score. The ability to \u201ccontrol scores\u201d is generally considered a higher intelligence level compared with merely \u201cwinning the game\u201d. Therefore, this paper does not emphasize the highest absolute score an agent can achieve in the Atari environment. Instead, it focuses on evaluating the agent's ability to follow instructions in the form of videos and \"desired cumulative rewards\" and to perform at the appropriate level. Especially when videos serve as conditions, the agent needs to infer the player's level demonstrated in the reference gameplay, which poses a significant challenge for the current agents. To our knowledge, this setting has not been explored by previous works."}], "equations": ["L_{lab} (o, a, w) = \\mathbb{E}_{z \\sim e(z | w_{1:M})} \\Big[ \\sum_{t=1}^{N} - \\log \\pi(a_t| o_{1:t}, z) - \\beta_2 \\mathbb{E}_{z \\sim sg[e(z | w_{1:M})]} [\\log e(z | o_{1:N})]\\Big],\\tag{3}", "L(D_{dem}, D_{lab}) = \\mathbb{E}_{(o,a) \\sim D_{dem}} [L_{dem}(o, a)] + [\\mathbb{E}_{(o,a,w) \\sim D_{lab}} [L_{lab}(o, a, w)] . \\tag{4}", "x^{\\prime}_{1:t} \\leftarrow \\texttt{XATTN}(q = z; kv = x_{1:t}^{\\[1\\]}, \\cdot \\cdot \\cdot, x_{1:t}^{\\[49]}).\\tag{5}", "a_t \\leftarrow \\texttt{TransformerXL}(x_{1:t}^{\\prime}). \\tag{6}"]}