{"title": "Learning to Build by Building Your Own Instructions", "authors": ["Aaron Walsman", "Muru Zhang", "Adam Fishman", "Ali Farhadi", "Dieter Fox"], "abstract": "Structural understanding of complex visual objects is an important unsolved component of artificial intelligence. To study this, we develop a new technique for the recently proposed Break-and-Make problem in LTRON where an agent must learn to build a previously unseen LEGO assembly using a single interactive session to gather information about its components and their structure. We attack this problem by building an agent that we call InstructioNet that is able to make its own visual instruction book. By disassembling an unseen assembly and periodically saving images of it, the agent is able to create a set of instructions so that it has the information necessary to rebuild it. These instructions form an explicit memory that allows the model to reason about the assembly process one step at a time, avoiding the need for long-term implicit memory. This in turn allows us to train on much larger LEGO assemblies than has been possible in the past. To demonstrate the power of this model, we release a new dataset of procedurally built LEGO vehicles that contain an average of 31 bricks each and require over one hundred steps to disassemble and reassemble. We train these models using online imitation learning which allows the model to learn from its own mistakes. Finally, we also provide some small improvements to LTRON and the Break-and-Make problem that simplify the learning environment and improve usability. This data and updated environments can be found at github.com/aaronwalsman/ltron/blob/v1.1.0. Additional training code can be found at github.com/aaronwalsman/ltron-torch/tree/eccv-24.", "sections": [{"title": "1 Introduction", "content": "The ability to understand and execute complex assembly problems is one of the hallmarks of human intelligence. Humans use this ability to construct tools and reverse engineer previously unseen part-based objects.\nThe recently proposed Break-and-Make problem [42] is designed to train agents to develop these abilities using complex LEGO structures. In this problem, an agent must learn to build a previously unseen assembly by actively inspecting it. To do this the agent is given access to an interactive simulator that allows it to disassemble the structure in order to reveal hidden components and see how everything fits together. Once it is confident that it knows the structure, the"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Interactive Understanding and Assembly", "content": "Despite a proliferation of interactive environments with increasing sophistication [8, 34, 35, 37, 44, 47], long-term structural understanding and assembly problems remain an open challenge in artificial intelligence. Researchers have explored this in the context of furniture assembly [27,50], Minecraft [11], CAD models [18] and robotic assembly problems [48]. In this work, we build on LTRON [42], a recent LEGO simulator designed to provide a building environment for learning agents. Constructing plans for assembly using a disassembly process has also been explored in the context of multi-part CAD based models [39], however this work is more concerned with finding collision-free paths through free-space, and does not directly reason about connection points.\nLEGO bricks have been a popular substrate for learning assembly prob-lems across a variety of subfields in artificial intelligence. These include design problems [30], robotic assembly [14], shape reconstruction [20, 21, 26], genera-tive modelling [38], and image guided building [6, 23]. Most similar to our work, Wang et al. [43] build LEGO structures from existing instructions. Our setting is more challenging because the agent must learn to make its own instructions rather than assuming they are already provided. Furthermore, the action space in LTRON is more difficult as it requires the agent to use a 2D cursor to inter-act with the scene and contains assemblies with bricks attached to the sides of objects, that cannot be described using simple stacking."}, {"title": "2.2 Memory", "content": "Memory structures for interactive problems have been studied for decades. Early attempts at implicit (problem-agnostic) memory structures include simple RNNS [19,33] and more complex variants such as LSTMs [17] and GRUs [4]. These methods propagate information forward in time using specialized network archi-tecture. Neural Turing Machines [13] use a learned external memory module to read and write to long term storage. Recently attention-based transformers [40] have become one of the most popular ways to build neural networks that rely on past information to make future decisions. While transformers are quite ef-fective, they are computationally expensive for large sequences of observations, which limits their use for very long-term memory."}, {"title": "2.3 Inverse-Graphics", "content": "Reconstructing geometry and reasoning about 3D structures from images has been an important issue in research fields such as computer-aided design [10,46] and robotics [3, 22, 36]. In particular, prior works such as [25,45] use 3D shapes while [28, 29, 49] use images to guide the inverse inference process. However, when building complex structures such as LEGO models, it is challenging to generate a set of sufficient visual images to predict the reconstruction without dynamically interacting with the object or the environment."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Environments and Data", "content": "The Break and Make environment in LTRON [42] is designed to test an agent's ability to perform complex building tasks on previously unseen target LEGO assemblies. In this, and many other complex construction problems, no single view of the target object is enough to fully describe it. Therefore, in order for the model to complete its objective, it must first interactively discover all the components of the LEGO assembly by disassembling it and remembering where all the individual bricks went during that process. In order to allow for this kind of interactive discovery, the Break and Make problem divides each interactive episode into two distinct phases. In the first Break phase, the agent is presented with a previously unseen LEGO model and is allowed to interactively inspect and disassemble it, while constructing some long-term memory of the object. Once it is done, it takes a dedicated Phase Switch action at which point the agent is presented with an empty scene and must use the memory it built in the Break phase to rebuild the model from scratch.\nIn this paper, we use a slightly modified version of the original Break and Make environment with a few changes for simplicity of training. The original Break and Make environment provided two images as its observation space, one \"table\" image representing the current scene, and another \"hand\" image representing a single brick that had just been removed from the scene, or was about to be added. In order to simplify the observation space of our model, we have removed the \"hand\" image. In this updated environment, when a new brick"}, {"title": "3.2 Instruction Stack", "content": "Our new model InstructioNet works by storing an explicit stack of instruction images in order to remember the structure of an assembly at various stages of deconstruction. To do this, we augment the action space discussed in the previous section with two additional Push Instruction and Pop Instruction actions. Push takes the current image from the simulator and adds it to the top of the instruction stack, while Pop removes the top image from the instruction stack. During training and inference, we do not restrict the size of the instruction stack."}, {"title": "3.3 Model", "content": "The learned policy is implemented using a modified vision transformer [9] with multiple heads that are responsible for different components of the action space. This model tokenizes the workspace and the instruction images into 16 \u00d7 16 pixel patches. Given that the environment produces images that are 128 \u00d7 128 pixels, this results in 64 tokens per image. The patches from both images are passed through a single linear layer and added to a learned positional encoding. The model then concatenates a single decoder token and a binary embedding of the current phase (Break or Make) for 130 total tokens. The transformer consists of 12 blocks with 512 channels and 8 heads each.\nTo compute an action, the output of the decoder token is then passed through a set of decoder heads to predict distributions for the action mode such as Dis-assemble, Assemble, or Rotate and mode-specific parameters such as Rotate Direction when performing a Rotate action or Brick Shape and Color when in-serting a new brick. In order to predict 2D cursor click and release locations, the 64 output tokens of transformer blocks 3,6,9 and 12 that correspond to the current image are combined using two separate DPT [31] decoders to produce dense feature maps at the resolution of the original input image. We found it beneficial to condition these click locations on the high level action and param-eters sampled from the initial decoder heads. This is accomplished by passing the sampled high level actions to an embedding layer and adding the resulting feature to the output of the decoder token. This value is then used to compute a distribution over click locations using dot product attention over the dense features computed by the DPT decoder. This conditional structure is similar to models used in game AI with complex action spaces [41]."}, {"title": "3.4 Online Training", "content": "We trained our model using online imitation learning similar to DAgger [32] and show ablations using behavior cloning as well. To do this we built a fast expert that can provide online supervision for trajectories generated by the learning model during training. This is in contrast to [42] which used an expert that was too slow for online labeling. Note that while our new expert is much faster, it is not able to construct plans in cases where the agent makes too many mistakes or deviates too far from the target assembly. In these cases, we simply terminate the training episode early. When there are multiple possible best actions that the expert could suggest, one of them is selected at random. To avoid ambiguity, our expert instructs the agent to push an image to the instruction stack each time a brick is removed during the break phase.\nThe online training algorithm alternates between generating new data by acting in the environment according to either the expert or the learning model, and then training on a randomized subset of the data generated over the past several iterations. When generating data, a fixed percentage of the environment steps are generated by sampling actions according to the expert, and the rest are sampled by acting according to the agent. We refer to this expert mixture constant as \u03b1, and for our main experiments, we found that \u03b1 = 0.75, representing a mix of 75% expert-generated and 25% model-generated data worked well. Note that regardless of which model is controlling the simulator, the expert's actions are always used for supervision. Incorporating trajectories generated by the agent in this way allows the model to learn to recover from its mistakes: when the model takes an inappropriate action it will reach part of the state space that would not have been encountered if acting according to the expert,"}, {"title": "3.5 Cursor Losses", "content": "We experimented with several approaches to computing losses for the cursor click and release locations. During online training, we would like the click-and-release decoders to each produce a distribution over locations that can be sampled in order to generate a variety of training data. To compute the probability of clicking on a particular pixel p(i, j), the dense raw value $x_{i,j}$ predicted at that pixel location is normalized using a standard softmax:\n$p(i, j) = \\frac{exp(x_{i,j})}{\\sum_{i',j'} exp(x_{i',j'})}$\nWhen the expert's action suggests clicking on a particular LEGO connection point in the scene, there are usually multiple \"acceptable\" pixels that correspond to the same connection point which complicates the choice of loss function.\nOne option is to use binary cross-entropy loss using the mask of acceptable pixel locations as a target. This assumes a different probabilistic interpretation of the output, one where multiple pixels can be chosen at the same time instead of just one, but still may be a useful way to encourage the model to put a high probability on the acceptable pixels. This loss function encourages the model to increase the probability of all acceptable pixels, without considering the cross-pixel relationships.\nAnother option commonly used in keypoint detection is to construct target heatmaps using a small Gaussian distribution around correct locations and su-pervise the output values using a mean-squared-error loss [1]. We opted against this as it does not lend itself well to softmax sampling, and may add probabil-ity mass outside the desired pixel boundaries. However, we can use the mean-squared-error loss to simply push all acceptable pixels toward a large positive constant, and all unacceptable pixels towards a large negative constant. Here we used a constant such that if only one pixel in the image assumed the positive"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Evaluation", "content": "We use the metrics from [42] to evaluate the quality of our learned agents. The first (F1b) is an F1 score over the brick shape and color, ignoring pose. The second (Fla) is over the brick shape, color, and pose after computing the best single rigid transformation to bring the estimated and ground-truth assemblies into alignment with each other. Assembly Edit Distance (AED) measures how many rigid transforms (edits) are required to bring all the bricks in the estimated assembly into alignment with the bricks in the ground-truth assembly, with additional penalties for extra and missing bricks. The final (Fle) is an F1 score over the edges (connections) between bricks after using the alignment computed in AED to construct a mapping between bricks in the estimated assembly and bricks in the ground-truth assembly."}, {"title": "4.2 Break and Make", "content": "To evaluate the effectiveness of the InstructioNet model, we trained it on our modified version of the Break and Make task using the randomly constructed assemblies and vehicles discussed in Section 3.1. Note that due to the updated observation space and the small changes to the random construction data due to the improved collision checker, these models should not be considered to have been trained in the same environment,"}, {"title": "4.3 Online Training", "content": "In order to show the effectiveness of online training using sequences of actions and observations generated by the learning model, we also train a model on sequences generated only by the expert on the RC-2 and RC-4 datasets. To do this, we set the expert mixture (\u03b1) to 1.0, which is equivalent to behavior cloning. The results are shown in the Online Training section of Table 2. While these models underperform relative to the default expert mixture (\u03b1 = 0.75) that includes training from an online expert, it still shows that significant progress can be made with offline training on this problem."}, {"title": "4.4 Loss Functions", "content": "We evaluate the effectiveness of our cursor loss function by comparing it against the binary cross entropy and constant regression methods discussed in Section"}, {"title": "4.5 Conditional Action Generation", "content": "We also test the importance of sequentially conditioning the action heads on one another as discussed in Section 3.3 by training a new model where these condi-tional connections are cut. This corresponds to cutting the magenta connections coming out of the Action Head, Parameter Head and Click Head in Figure 4."}, {"title": "4.6 Selective Modification", "content": "We also tested our model on a new task that requires the agent to rebuild the model with one of the brick colors altered. In this setting, the model receives two additional tokens, one which specifies the color to change, and the other that specifies a new color. The agent's objective is to rebuild the model with all bricks using the original color instead built with the new color. While, performance degrades slightly on this problem, the model still performs quite well. This demonstrates the model's ability to reason not only about reproducing the exact same model seen during the break phase, but also incorporating new instructions when rebuilding."}, {"title": "4.7 Hyperparameters", "content": "Unless otherwise mentioned, we used AdamW with a learning rate of 5 \u00d7 10\u22125, \u03b21 = 0.9, \u03b22 = 0.95 and weight decay of 0.1."}, {"title": "4.8 Qualitative Evaluation", "content": "Figure 5 shows ten representative failure and success cases of our model on the RC-Vehicles dataset sorted by their Fla score."}, {"title": "5 Conclusion", "content": "We have demonstrated substantially improved performance over previous base-lines on the Break and Make problem, using a model with explicit instruction memory. The failure modes of this approach suggest that performance could be improved by working on solutions that avoid getting stuck, and that find ways to push forward even if it means making a local mistake. While our approach is successful, it requires an online expert which can provide explicit instructions not only for the inspection and reconstruction process but also for the process of storing and retrieving memory. This limits the utility of this method in real-world settings, where an online expert may not be available. The InstructioNet approach may not be appropriate for problems that do not follow our assump-tion that assembly can be completed by approximately reversing the disassembly process. Nevertheless the advantage of this method over prior approaches which considered the entire observation history points to the effectiveness of consider-ing only a portion of memory at a time when making decisions."}, {"title": "A Additional Ablations", "content": ""}, {"title": "A.1 Camera Motion", "content": "We also test the extent to which direct image comparison enables our results.\nFor reference, we also report numbers for the original model that was trained without camera motion evaluated on data with camera motion."}, {"title": "A.2 Expert Instruction Images", "content": "Given that our model only requires the current simulator image, the top image of the instruction stack and the current phase, we also explore the success of the model in a setting where we use the expert to generate the instruction stack during the break phase, but then do assembly with a learned agent using the expert's instruction stack as input."}, {"title": "B Online Expert Details", "content": "Here we include details on the logical procedure used to generate expert actions.\nDuring the Break phase, this expert provides advice that removes one brick at a time, then pushes a new instruction image until no more bricks remain."}, {"title": "C Online Training Algorithm", "content": "Our online training algorithm mixes off-policy data generated by the expert with on-policy data generated by the learner."}]}