{"title": "On the Resilience of Multi-Agent Systems with Malicious Agents", "authors": ["Jen-tse Huang", "Jiaxu Zhou", "Tailin Jin", "Xuhui Zhou", "Zixi Chen", "Wenxuan Wang", "Youliang Yuan", "Maarten Sap", "Michael R. Lyu"], "abstract": "Multi-agent systems, powered by large language models, have shown great abilities\nacross various tasks due to the collaboration of expert agents, each focusing on\na specific domain. However, when agents are deployed separately, there is a risk\nthat malicious users may introduce malicious agents who generate incorrect or\nirrelevant results that are too stealthy to be identified by other non-specialized\nagents. Therefore, this paper investigates two essential questions: (1) What is the\nresilience of various multi-agent system structures (e.g., A\u2192B\u2192C, A+B+C)\nunder malicious agents, on different downstream tasks? (2) How can we increase\nsystem resilience to defend against malicious agents? To simulate malicious agents,\nwe devise two methods, AUTOTRANSFORM and AUTOINJECT, to transform any\nagent into a malicious one while preserving its functional integrity. We run com-\nprehensive experiments on four downstream multi-agent systems tasks, namely\ncode generation, math problems, translation, and text evaluation. Results suggest\nthat the \"hierarchical\u201d multi-agent structure, i.e., A\u2192(B\u2194C), exhibits superior\nresilience with the lowest performance drop of 23.6%, compared to 46.4% and\n49.8% of other two structures. Additionally, we show the promise of improving\nmulti-agent system resilience by demonstrating that two defense methods, intro-\nducing an additional agent to review and correct messages or mechanisms for each\nagent to challenge others' outputs, can enhance system resilience. Our code and\ndata are available at https://github.com/CUHK-ARISE/MAS-Resilience.", "sections": [{"title": "1 Introduction", "content": "Multi-agent collaboration has further boosted Large Language Models' (LLMs) already impressive\nperformance across various downstream tasks, including code generation (Liu et al., 2024a; Lee\net al., 2024), math problem solving (Lu et al., 2024; Liang et al., 2023b), and text translation (Jiao\net al., 2023; Wu et al., 2024). In such multi-agent systems, improvements are achieved by decom-\nposing complex tasks into smaller, specialized sub-tasks handled by expert agents in a role-specific\nmanner (Chen et al., 2023b; Li et al., 2024b).\nHowever, the decentralized nature of multi-agent systems leaves them vulnerable to faulty or malicious\nagents, which could undermine or destroy collaboration. Consider a scenario where companies\nspecializing in different areas produce expert agents, the lack of centralized control means that the\nmulti-agent system may contain agents from various sources, some of which could be faulty or\nmalicious. In a multi-agent coding system like Camel (Li et al., 2024a), a malicious coding agent\ncould produce buggy code, causing severe errors or harmful outputs when executed by another agent."}, {"title": "2 Preliminaries", "content": "Humans have developed various\nmodes of collaboration due to their social nature (Yang & Zhang, 2019; Alexy, 2022), which also\ninfluences how different studies design the structures of multi-agent systems. In this paper, we select\nthree categories originating from management science: (1) Linear (Yang & Zhang, 2019): Agents\nengage in one-way communication, e.g., A\u2192B\u2192C. (2) Flat (Alexy, 2022): Agents exclusively\nuse mutual communication, e.g., A\u2192B\u2192C. (3) Hierarchical (Mihm et al., 2010): This system\nincorporates both one-way and mutual communications, e.g., A\u2192(B\u2192C), distinguishing it from (1)\nwhich is a purely linear model. These structures align with Zhang et al. (2024)'s categorization of\nHierarchical, Joint, and Hierarchical + Joint, based on agent interactions. An introduction to various\nmulti-agent systems is provided in \u00a76.\nSystem Resilience In human collaboration, the capacity to mitigate mistakes or intentional disrup-\ntions within a team and maintain functionality despite individual failures is usually referred to as\n\u201cresilience\u201d (Alliger et al., 2015; Boin & Van Eeten, 2013; Hartwig et al., 2020). Resilience reflects\nthe ability to handle internal errors, maintaining overall operation without being affected by a single\nfailure. LLM-based multi-agent systems face safety issues where malicious agents produce errors too\nstealthy to be found by other agents but can cause undesired consequences. Therefore, holding this\nsame ability as human collaboration becomes critical."}, {"title": "3 Methodology: Introducing Erorrs", "content": "We offer two methods for introducing errors in multi-agent systems: AUTOTRANSFORM converts\nagents into malicious entities that generate errors autonomously, while AUTOINJECT directly in-\ntroduces errors into messages. In this section, we first discuss the rationale behind designing the\nautonomous transformation agent in \u00a73.1. Next, we introduce the method for directly injecting\nerrors into messages within multi-agent systems in \u00a73.2. These two methods are designed to be\ngeneral-purpose, applicable to any agent profiles and downstream tasks. For presentation clarity, we\nuse \"message\" to refer to intermediate outputs between agents, and \"result\" to denote the final output\nfrom the last agent."}, {"title": "3.1 AUTOTRANSFORM: Malicious Agent Transformation", "content": "AUTOTRANSFORM is an LLM-based approach that takes any agent's profile as input and outputs a\nprofile of a malicious agent performing the same functions but introducing stealthy errors. Drawing\ninspiration from how we manually convert an agent into malicious one, the design of AUTOTRANS-\nFORM follows three key steps: (1) To ensure applicability to any target agent and downstream tasks,\nAUTOTRANSFORM first analyzes the input agent profile and extract the assigned task. This step helps\nto extract the task and identify potential ways to produce erroneous outputs. (2) Based on the task\nanalysis, AUTOTRANSFORM lists all possible methods to inject errors, emphasizing the need for\nstealth to avoid detection by other agents. (3) AUTOTRANSFORM then rewrites the agent's profile\nwith these error-injection methods, ensuring that the original functionalities of the agent remain\nunchanged. An example of using AUTOTRANSFORM to alter an agent's profile is shown in Fig. 2c.\nThe complete prompt is provided in \u00a7A.2 in the appendix."}, {"title": "3.2 AUTOINJECT: Direct Error Injection", "content": "While AUTOTRANSFORM can conveniently generate malicious agents, it is hard to ensure these agents\nintroduce a specific number and type of errors due to the inherent randomness of the generation\nprocess. For example, \"injecting syntax errors in 20% lines of the generated code\" cannot be"}, {"title": "4 Experiments", "content": "This section focuses on answering the following Research Questions (RQs):\nRQ1. Which of the three multi-agent system architectures exhibits the highest resilience (\u00a74.2)?\nRQ2. Do different downstream tasks vary in their resilience to errors (\u00a74.3)?\nRQ3. How do varying error rates (both Pm and Pe) impact system resilience (\u00a74.4)?\nRQ4. How do the two types of errors influence system resilience (\u00a74.5)?"}, {"title": "4.1 Settings", "content": "We evaluate general-purpose task-solving abilities using common tasks:\n\u2022 Code Generation: HumanEval (Chen et al., 2021) contains 164 hand-written programming\nproblems to assess LLMs' ability to synthesize correct and functional Python code. Accuracy\n(Pass@1) is used for evaluation.\n\u2022 Math Problem Solving: CIAR (Liang et al., 2023b) presents 50 questions with hidden traps to\nevaluate LLMs' Counter-Intuitive Arithmetic Reasoning abilities, requiring multi-step reasoning.\nAccuracy is used for evaluation.\n\u2022 Translation: CommonMT (He et al., 2020) consists of paired sentences to test models' handling of\nthree types of commonsense reasoning, especially in ambiguous contexts. We randomly sampled\n100 sentences from the most challenging type, Lexical, for our evaluation, using BLEURT-20 (Sel-\nlum et al., 2020; Pu et al., 2021) for evaluation, following the practice in Liang et al. (2023b).\n\u2022 Text Evaluation: FairEval (Wang et al., 2023a) includes 80 human-annotated \"win/tie/lose\" out-\ncomes comparing responses from ChatGPT and Vicuna-13B, aiming to determine if the model's\npreferences align with human judgments. Accuracy is used for evaluation.\nMulti-Agent Systems We consider three types of system architectures mentioned in \u00a72:\n\u2022 Linear: MetaGPT (Hong et al., 2023) employs Standard Operating Procedures (SOPs) to create\nan efficient workflow in a software company setting, utilizing five agents for code generation.\nSelf-collaboration (Dong et al., 2023) designs three roles, namely analyzers, coders, and testers,\nimplemented using 2-5 agents on code generation task.\n\u2022 Flat: Camel (Li et al., 2024a) presents a framework where a \"User\u201d agent iteratively refines\noutputs from an \u201cAssistant\u201d agent, applicable across various tasks. SPP (Wang et al., 2023b) uses\nSolo-Performance-Prompting to engage a single model into three personas for coding tasks.\n\u2022 Hierarchical: MAD (Liang et al., 2023b) introduces a Multi-Agent Debate framework with two\ndebaters and one judge to promote divergent thinking in LLMs for various tasks. AgentVerse (Chen\net al., 2023b) employs a dynamic recruitment process, selecting agents for multi-round collaboration\nas needed, utilizing four agents for our selected tasks.\nNot all systems are designed to support the four tasks studied in this paper. Therefore, we modified\nthe prompts of some systems to adapt to our selected tasks. The modified prompts are detailed in \u00a7A.1\nof the Appendix. By default, we use gpt-3.5-turbo-0613 as the backbone with a temperature of\nzero for all the six systems, AUTOTRANSFORM, AUTOINJECT, and the defense methods in \u00a75. We\nintroduce one malicious agent at a time to avoid interference and facilitate essential analysis. Non-\nmalicious agents remain unaware of the malicious agent's presence, reflecting a realistic information-\nasymmetric scenario (Zhou et al., 2024)."}, {"title": "4.2 RQ1: Impact of System Architectures", "content": "The hierarchical structure has a higher resilience than other two, exhibiting the smallest\naccuracy drop. Fig. 3a illustrates the impact of AUTOTRANSFORM and AUTOINJECT on various\nmulti-agent system types across different downstream tasks. System resilience, ranked from strongest\nto weakest, is: hierarchical, flat, and linear. The hierarchical architecture experiences relative accuracy\ndrops of 23.6% and 22.6% for AUTOTRANSFORM and AUTOINJECT, respectively. We attribute\nthis resilience to the presence of a higher-level agent (e.g., the evaluator in MAD), which is always\npresented with various versions of the answer by multiple agents performing the same sub-task,\nincreasing the likelihood of error recovery from a single agent. The flat structure shows similar\nresilience for AUTOTRANSFORM but significantly lower resilience for AUTOINJECT. This is due to\nthe lack of a high-level leader in the <A\u2192B\u2192C> structure to supervise and select the agent with the\nbest result. The linear architecture demonstrates the lowest resilience. In addition to lacking a leader,\nit also lacks communication between agents, resulting in a one-way assembly line.\nAUTOINJECT causes a larger performance drop than AUTOTRANSFORM. While one might\nassume AUTOTRANSFORM would have a greater negative impact on multi-agent collaboration due\nto its permanent modification of agents' profiles into malicious ones, it is AUTOINJECT that results\nin a more significant performance drop, although AUTOINJECT introduces errors into a fixed and\nrelatively small portion of messages. The reasons for this are two-fold: (1) Current LLMs have a\nweakness where they become less effective as the context lengthens, especially where conflict exists\nin instructions. For our malicious agents, they gradually lose track of the task to produce errors,\nprioritizing new instructions from other agents to correct errors in the message. (2) AUTOINJECT\nconsistently introduces errors, whereas AUTOTRANSFORM does not always ensure error generation.\nDespite being transformed into malicious agents, they sometimes fail to generate errors due to\nconstraints requiring errors to be stealthy."}, {"title": "4.3 RQ2: Impact of Downstream Tasks", "content": "Tasks requiring rigor and formalization, such as code generation and math, are more sensitive\nto agent errors and exhibit lower resilience compared to translation and text evaluation. Code\ngeneration and math demand greater objectivity than the more subjective tasks of translation and text\nevaluation. Fig. 3b illustrates the impact of AUTOTRANSFORM and AUTOINJECT across different\ndownstream tasks. We also present the performance of single-agent, as reported by Chen et al. (2021),\nLiang et al. (2023b), and Chan et al. (2023)\u00b3, for a clearer comparison. The results indicate several\nconclusions: (1) Objective tasks benefit more from multi-agent collaboration, while subjective tasks\ngain less. Additionally, errors in subjective tasks are often overlooked by other agents due to the\nlack of rigorous correctness standards. (2) In terms of system resilience, tasks ranked from least\nto most vulnerable are: code generation, math, translation, and text evaluation. Even minor errors\nin the first two tasks, particularly in code generation, significantly affect rigor and formalization.\nConversely, the latter two tasks are less sensitive to minor variations in a single agent's output. (3)\nAUTOTRANSFORM and AUTOINJECT perform similarly across most tasks, except in code generation.\nInjecting errors can surprisingly improve performance on downstream tasks. We find that certain\nmulti-agent collaboration systems, such as MAD, Camel, and AgentVerse, benefit from deliberately\ninjected errors rather than being hindered by them. Fig. 4 shows the performance changes of MAD"}, {"title": "4.4 RQ3: Impact of Error Rates", "content": "Increasing the number of erroneous messages causes a larger performance drop than the\nnumber of errors within a message. Since AUTOTRANSFORM lacks precise control over error rates\nand types, we focus on AUTOINJECT for RQ3 and RQ4. Fig. 5a presents two experiments: one with\na fixed Pe = 0.2 and varying Pm at 0.2, 0.4, and 0.6, labeled \u201cErroneous Message;\u201d The other with a\nfixed Pm = 0.2 and varying Pe at 0.2, 0.4, and 0.6, labeled \u201cErrors per Message.\" The performance\ndrop for erroneous messages is nonlinear, with the most significant decrease occurring between 0 and\n0.2. As Pm increases, the rate of decline diminishes. Regarding the error ratio in a single message are\nas follows: (1) In contrast to Pm, the performance decline is nearly linear as Pe increases. (2) As Pe\nincreases, performance decreases, implying that while higher error rates make errors more noticeable,\nthe agent system struggles to correct the increasing number of errors. An exception is observed\nwhen increasing Pe from 0.4 to 0.6, resulting in a performance increase in three systems (MetaGPT,\nSelf-collab, MAD). This occurs because excessive errors in a single message become noticeable,\nprompting other agents to request corrections. This phenomenon highlights the importance of stealth\nin introducing errors."}, {"title": "4.5 RQ4: Impact of Error Types", "content": "Semantic errors cause a greater performance drop than syntactic errors. Fig. 5b presents the\nperformance decline caused by syntactic and semantic errors across six systems, including the average."}, {"title": "4.6 Case Study", "content": "Introduced errors can cause performance increase. Fig. 6a depicts a conversation of two Camel\nagents completing a code generation task from HumanEval. An additional error is introduced by\nAUTOINJECT below an incorrect line of code. Subsequently, another agent identifies the injected\nerror and instructs the first agent to correct it without noting the pre-existing error. Ultimately, the\nsystem corrects both the introduced error and the original error successfully.\nCurrent LLMs prioritize natural language over code. Fig. 6b illustrates that distraction comments\ncan mislead LLMs into accepting incorrect code as correct across all six systems studied. This\nindicates that the systems tend to prioritize comments over the actual code. In the example, the\nsystem detects an error in the code when no comments are present. However, when a comment stating\n\"the bug had been corrected\" is added, the system overlooks the error and proceeds with the next task.\nAUTOTRANSFORM exploits this characteristic of LLMs to execute successful attacks."}, {"title": "4.7 Other Factors", "content": "Impact of Malicious Roles Previous experiments in \u00a74 focus on polluting the agents directly\nresponsible for the work, rather than those who delegate tasks to other agents. To examine the impact\nof polluting different types of agents and the generalizability of our AUTOTRANSFORM on agents\nwith varying roles, this section investigates the effects of polluting high-level agents. Specifically,\nwe apply AUTOTRANSFORM to the User and Assistant agents in Camel, and the Product Manager\nand Engineer agents in MetaGPT. The results of these systems completing code generation tasks are\nshown in Fig. 7a. The conclusions are as follows: (1) AUTOTRANSFORM is applicable to agents with\ndifferent profiles or functionalities, effectively disrupting collaboration. (2) Polluting higher-level"}, {"title": "5 Improving System Resilience", "content": "Based on our experimental observations and findings, we propose two strategies for improving\nresilience in multi-agent collaboration systems, defending against malicious agents.\nDefense methods The core idea behind our defense methods involves adding a correction mech-\nanism within the system. We explore two variants, the \u201cInspector\u201d and the \u201cChallenger.\u201d The\n\"Inspector,\" similar to our AUTOINJECT, is an additional agent that intercepts all messages spread\namong agents, checks for errors, and corrects them. This method draws inspiration from the \"Police\"\nagent in Zhang et al. (2024).\nIn contrast, the \u201cChallenger,\u201d akin to our AUTOTRANSFORM, is an additional description of func-\ntionalities added in agent profiles. This method addresses the limitation that many agents can only\nexecute assigned tasks and may not address certain problems they encounter, although they usually\nhave the knowledge to. By empowering agents to challenge the results of others, we enhance their\nproblem-solving capabilities. This is because most current multi-agent systems use the same LLM as\nthe backbone for all agents, indicating their underlying ability to partially solve tasks outside their\nspecialization. Detailed prompts for the \"Police\" agent and \u201cChallenger\u201d method can be found in\n\u00a7A.4 and \u00a7A.5 in the appendix.\nResults We apply these defense strategies to the two weaker architectures: the linear (Self-collab)\nand the flat (Camel). Results for the vanilla model, error injection with AUTOINJECT, and the\ntwo defense methods are shown in Fig. 8. Both defense methods improve performance against\nAUTOINJECT, though they do not restore it to the original level. Additionally, there is no definitive\nconclusion as to which method is superior. We recommend trying both in practice."}, {"title": "6 Related Work", "content": "LLMs enhance multi-agent systems through their exceptional capability for role-play (Wang et al.,\n2024). Despite utilizing a same architecture, like GPT-3.5, distinct tasks benefit from tailored\nin-context role-playing prompts (Min et al., 2022). Besides the six frameworks selected in this\nstudy, researchers have been exploring multi-agent collaboration in downstream tasks or simulated\ncommunities. ChatEval (Chan et al., 2023) is a multi-agent debate system for evaluating LLM-\ngenerated text, providing a human-like evaluation process. ChatDev (Qian et al., 2023) uses a linear\nstructure of several roles to address code generation tasks. AutoGen (Wu et al., 2023) offers a generic\nframework for building diverse applications with multiple LLM agents. AutoAgents (Chen et al.,\n2023a) enables dynamic generation of agents' profiles and cooperation, evaluated on open-ended\nQA and creative writing tasks. Agents (Zhou et al., 2023a) support planning, memory, tool usage,\nmulti-agent communication, and fine-grained symbolic control for multi-agent or human-agent\ncollaboration. There is also work simulating daily life and conversations (Park et al., 2023; Zhou\net al., 2023b). Additionally, there are studies on multi-agent competition (Huang et al., 2024; Liu\net al., 2024b; Liang et al., 2023a). These frameworks are not selected either because they are not\ntask-oriented (e.g.., simulated society or competitions) or their system design overlaps with those\nchosen for this study."}, {"title": "6.2 Safety Issues in Multi-Agent Systems", "content": "PsySafe (Zhang et al., 2024) is a framework that integrates attack, evaluation, and defense mechanisms\nusing psychological manipulation involving negative personalities. EG (Evil Geniuses) (Tian et al.,\n2023) is an attack method that automatically generates prompts related to agents' original roles,\nsimilar to our AUTOTRANSFORM. While PsySafe and EG are applied to different multi-agent\nsystems such as Camel and MetaGPT, they do not examine the impact of adversaries on downstream\ntasks like code generation or translation. Amayuelas et al. (2024) investigates how an adversary in\nmulti-agent debate can disrupt collaboration in tasks including MMLU (Massive Multitask Language\nUnderstanding) (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2022), MedMCQA (Pal et al.,\n2022), and LegalBench (Guha et al., 2024), finding that the adversary's persuasion skill is crucial\nfor a successful attack. Ju et al. (2024) proposes a two-stage attack strategy to create an adversary\nthat spreads counterfactual and toxic knowledge in a simulated multi-agent chat environment. This\nmethod can effectively break collaboration in MMLU. Unlike our study, Amayuelas et al. (2024) and\nJu et al. (2024) do not explore how different system architectures are affected by these adversaries."}, {"title": "7 Conclusion", "content": "This paper investigates the resilience of three multi-agent collaboration systems\u2014linear, flat, and\nhierarchical-against malicious agents that produce erroneous or misleading outputs. Six systems are\nselected and evaluated on four downstream tasks, including code generation, math problem solving,\ntranslation, and text evaluation. We design AutoTRANSFORM and AutoINJECT to introduce\nerrors into the multi-agent collaboration. Results indicate that the hierarchical system demonstrates\nthe strongest resilience, with the lowest performance drops of 23.6% and 22.6% for the two error\nintroduction methods. However, some systems can benefit from the intentionally introduced errors,\nfurther improving performance. Objective tasks, such as code generation and math, are more\nsignificantly affected by errors. Additionally, the frequency of erroneous messages impacts resilience"}, {"title": "Limitations", "content": "There are several limitations in this study. First, due to budget constraints, we exclusively use\ngpt-3.5-turbo-0125 for all experiments. Our primary goal is to fairly evaluate different multi-\nagent systems' resilience against malicious agents. We believe the results would not significantly\ndiffer from other models. However, using stronger models for AUTOTRANSFORM and AUTOINJECT\nmight yield improvements, which we plan to explore in future work. The second limitation is\nthe selection of multi-agent systems and downstream tasks, which cannot be comprehensive. We\nmitigate this by selecting representative systems from three well-established human collaboration\nmodes (Yang & Zhang, 2019; Alexy, 2022; Mihm et al., 2010) and using four commonly-used\ndatasets for benchmarking the abilities of multi-agent systems (Liang et al., 2023b; Chen et al., 2021).\nThe final limitation concerns the analysis, where latent variables affecting system resilience might\nbe unidentified. We examine system architectures, downstream tasks, error rates, error types, agent\nroles, and the number of agents' communications. To the best of our knowledge, no additional factors\ninfluencing system resilience are found."}, {"title": "Broader Impacts", "content": "The two error introduction methods developed in this study, AUTOTRANSFORM and AUTOINJECT,\ncould potentially pollute benign agents and result in negative social impacts. To mitigate this risk, we\nhave proposed effective defense mechanisms against them. We would like to emphasize that the goal\nof proposing these methodologies is to study and improve the behavior of LLM-based multi-agent\nsystems. We strongly oppose any malicious use of these methods to achieve negative ends."}, {"title": "A Prompt Details", "content": "All six multi-agent collaboration systems selected in this study support only some of the downstream\ntasks in their original design. Therefore, we extend three scalable systems-Camel, MAD, and\nAgentVerse-to adapt to all four downstream tasks. The first three systems provide a high-level,\nnon-task-oriented design for task division, while the other three systems are deeply intertwined\nwith code generation tasks. Using Camel as an example of adapting systems to other tasks: For\ntranslation and math, we improve system performance by adding \"step by step\" instructions in\nprompts. For instance, in translation, it correctly interprets \u201c\u62c9\u4e0b\u6c34 (pull into water)\u201d to its correct\nmeaning of \u201cengaging in wrongdoing\u201d in Chinese. In math, a single agent calculates \u201cAverage\nSpeed= (1 + 3)/2 = 1m/s,\" whereas Camel's multi-agent system correctly computes \u201caverage\nspeed= (1+3)/2 = 2m/s.\" The detailed instructions likely reduce the occurrence of \u201cseemingly\u201d\ncorrect answers and increase accuracy in these specific cases."}, {"title": "A.1 Multi-Agent Systems on Different Tasks", "content": null}, {"title": "A.1.1 Camel", "content": null}, {"title": "A.1.2 MAD", "content": null}, {"title": "A.1.3 AgentVerse", "content": null}, {"title": "A.2 AUTOTRANSFORM", "content": null}, {"title": "A.3 AUTOINJECT", "content": null}, {"title": "A.4 Defense Method: Inspector", "content": null}, {"title": "A.5 Defense Method: Challenger", "content": null}]}