{"title": "On the Resilience of Multi-Agent Systems with Malicious Agents", "authors": ["Jen-tse Huang", "Jiaxu Zhou", "Tailin Jin", "Xuhui Zhou", "Zixi Chen", "Wenxuan Wang", "Youliang Yuan", "Maarten Sap", "Michael R. Lyu"], "abstract": "Multi-agent systems, powered by large language models, have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, when agents are deployed separately, there is a risk that malicious users may introduce malicious agents who generate incorrect or irrelevant results that are too stealthy to be identified by other non-specialized agents. Therefore, this paper investigates two essential questions: (1) What is the resilience of various multi-agent system structures (e.g., A\u2192B\u2192C, A+B+C) under malicious agents, on different downstream tasks? (2) How can we increase system resilience to defend against malicious agents? To simulate malicious agents, we devise two methods, AUTOTRANSFORM and AUTOINJECT, to transform any agent into a malicious one while preserving its functional integrity. We run comprehensive experiments on four downstream multi-agent systems tasks, namely code generation, math problems, translation, and text evaluation. Results suggest that the \"hierarchical\u201d multi-agent structure, i.e., A\u2192(B\u2194C), exhibits superior resilience with the lowest performance drop of 23.6%, compared to 46.4% and 49.8% of other two structures. Additionally, we show the promise of improving multi-agent system resilience by demonstrating that two defense methods, introducing an additional agent to review and correct messages or mechanisms for each agent to challenge others' outputs, can enhance system resilience. Our code and data are available at https://github.com/CUHK-ARISE/MAS-Resilience.", "sections": [{"title": "1 Introduction", "content": "Multi-agent collaboration has further boosted Large Language Models' (LLMs) already impressive performance across various downstream tasks, including code generation (Liu et al., 2024a; Lee et al., 2024), math problem solving (Lu et al., 2024; Liang et al., 2023b), and text translation (Jiao et al., 2023; Wu et al., 2024). In such multi-agent systems, improvements are achieved by decomposing complex tasks into smaller, specialized sub-tasks handled by expert agents in a role-specific manner (Chen et al., 2023b; Li et al., 2024b).\nHowever, the decentralized nature of multi-agent systems leaves them vulnerable to faulty or malicious agents, which could undermine or destroy collaboration. Consider a scenario where companies specializing in different areas produce expert agents, the lack of centralized control means that the multi-agent system may contain agents from various sources, some of which could be faulty or malicious. In a multi-agent coding system like Camel (Li et al., 2024a), a malicious coding agent could produce buggy code, causing severe errors or harmful outputs when executed by another agent."}, {"title": "2 Preliminaries", "content": "Humans have developed various modes of collaboration due to their social nature (Yang & Zhang, 2019; Alexy, 2022), which also influences how different studies design the structures of multi-agent systems. In this paper, we select three categories originating from management science: (1) Linear (Yang & Zhang, 2019): Agents engage in one-way communication, e.g., A\u2192B\u2192C. (2) Flat (Alexy, 2022): Agents exclusively use mutual communication, e.g., A\u2192B\u2192C. (3) Hierarchical (Mihm et al., 2010): This system incorporates both one-way and mutual communications, e.g., A\u2192(B\u2192C), distinguishing it from (1) which is a purely linear model. These structures align with Zhang et al. (2024)'s categorization of Hierarchical, Joint, and Hierarchical + Joint, based on agent interactions. An introduction to various multi-agent systems is provided in \u00a76.\nIn human collaboration, the capacity to mitigate mistakes or intentional disruptions within a team and maintain functionality despite individual failures is usually referred to as \u201cresilience\u201d (Alliger et al., 2015; Boin & Van Eeten, 2013; Hartwig et al., 2020). Resilience reflects the ability to handle internal errors, maintaining overall operation without being affected by a single failure. LLM-based multi-agent systems face safety issues where malicious agents produce errors too stealthy to be found by other agents but can cause undesired consequences. Therefore, holding this same ability as human collaboration becomes critical."}, {"title": "3 Methodology: Introducing Erorrs", "content": "We offer two methods for introducing errors in multi-agent systems: AUTOTRANSFORM converts agents into malicious entities that generate errors autonomously, while AUTOINJECT directly introduces errors into messages. In this section, we first discuss the rationale behind designing the autonomous transformation agent in \u00a73.1. Next, we introduce the method for directly injecting errors into messages within multi-agent systems in \u00a73.2. These two methods are designed to be general-purpose, applicable to any agent profiles and downstream tasks. For presentation clarity, we use \"message\" to refer to intermediate outputs between agents, and \"result\" to denote the final output from the last agent."}, {"title": "3.1 AUTOTRANSFORM: Malicious Agent Transformation", "content": "AUTOTRANSFORM is an LLM-based approach that takes any agent's profile as input and outputs a profile of a malicious agent performing the same functions but introducing stealthy errors. Drawing inspiration from how we manually convert an agent into malicious one, the design of AUTOTRANSFORM follows three key steps: (1) To ensure applicability to any target agent and downstream tasks, AUTOTRANSFORM first analyzes the input agent profile and extract the assigned task. This step helps to extract the task and identify potential ways to produce erroneous outputs. (2) Based on the task analysis, AUTOTRANSFORM lists all possible methods to inject errors, emphasizing the need for stealth to avoid detection by other agents. (3) AUTOTRANSFORM then rewrites the agent's profile with these error-injection methods, ensuring that the original functionalities of the agent remain unchanged. An example of using AUTOTRANSFORM to alter an agent's profile is shown in Fig. 2c. The complete prompt is provided in \u00a7A.2 in the appendix."}, {"title": "3.2 AUTOINJECT: Direct Error Injection", "content": "While AUTOTRANSFORM can conveniently generate malicious agents, it is hard to ensure these agents introduce a specific number and type of errors due to the inherent randomness of the generation process. For example, \"injecting syntax errors in 20% lines of the generated code\" cannot be guaranteed by the malicious agents. However, precise error generation is crucial for analyzing the impact of various factors on system resilience. To address this, we introduce AUTOINJECT, an approach that takes the outputs of other agents and intentionally injects specific errors. This approach allows for exact control over the proportion of erroneous messages, the specific errors within a message, and the types of errors introduced. We start by discussing two key factors in our study: error rate and error type."}, {"title": "Error Rate", "content": "In this paper, we examine two aspects of error injection in multi-agent collaboration systems: Macro Perspective: We control the ratio of erroneous messages produced by a malicious agent in all its messages, which is a practical way to obscure its identity while facilitating stealthy errors. We denote this probability that a message is intentionally flawed as $P_m$. Micro Perspective: We manage the degree of error within each faulty message. For instance, in code generation tasks, we can adjust the number of errors per line of code. The proportion of a message that is erroneous is denoted by $P_e$."}, {"title": "Error Type", "content": "In tasks that demand formality, rigor, and logic, such as code generation, two types of errors can be identified. Syntactic Errors include mistakes that violate logical or factual correctness within a given context. Semantic Errors pertain to issues that, while logically sound and syntactically correct, are either irrelevant or fail to accurately execute the intended instruction."}, {"title": "4 Experiments", "content": "This section focuses on answering the following Research Questions (RQs):\nRQ1. Which of the three multi-agent system architectures exhibits the highest resilience (\u00a74.2)?\nRQ2. Do different downstream tasks vary in their resilience to errors (\u00a74.3)?\nRQ3. How do varying error rates (both $P_m$ and $P_e$) impact system resilience (\u00a74.4)?\nRQ4. How do the two types of errors influence system resilience (\u00a74.5)?"}, {"title": "4.1 Settings", "content": "We evaluate general-purpose task-solving abilities using common tasks:\n*   Code Generation: HumanEval (Chen et al., 2021) contains 164 hand-written programming problems to assess LLMs' ability to synthesize correct and functional Python code. Accuracy (Pass@1) is used for evaluation.\n*   Math Problem Solving: CIAR (Liang et al., 2023b) presents 50 questions with hidden traps to evaluate LLMs' Counter-Intuitive Arithmetic Reasoning abilities, requiring multi-step reasoning. Accuracy is used for evaluation.\n*   Translation: CommonMT (He et al., 2020) consists of paired sentences to test models' handling of three types of commonsense reasoning, especially in ambiguous contexts. We randomly sampled 100 sentences from the most challenging type, Lexical, for our evaluation, using BLEURT-20 (Sellam et al., 2020; Pu et al., 2021) for evaluation, following the practice in Liang et al. (2023b).\n*   Text Evaluation: FairEval (Wang et al., 2023a) includes 80 human-annotated \"win/tie/lose\" outcomes comparing responses from ChatGPT and Vicuna-13B, aiming to determine if the model's preferences align with human judgments. Accuracy is used for evaluation.\nWe consider three types of system architectures mentioned in \u00a72:\n*   Linear: MetaGPT (Hong et al., 2023) employs Standard Operating Procedures (SOPs) to create an efficient workflow in a software company setting, utilizing five agents for code generation. Self-collaboration (Dong et al., 2023) designs three roles, namely analyzers, coders, and testers, implemented using 2-5 agents on code generation task.\n*   Flat: Camel (Li et al., 2024a) presents a framework where a \"User\u201d agent iteratively refines outputs from an \u201cAssistant\u201d agent, applicable across various tasks. SPP (Wang et al., 2023b) uses Solo-Performance-Prompting to engage a single model into three personas for coding tasks.\n*   Hierarchical: MAD (Liang et al., 2023b) introduces a Multi-Agent Debate framework with two debaters and one judge to promote divergent thinking in LLMs for various tasks. AgentVerse (Chen et al., 2023b) employs a dynamic recruitment process, selecting agents for multi-round collaboration as needed, utilizing four agents for our selected tasks.\nNot all systems are designed to support the four tasks studied in this paper. Therefore, we modified the prompts of some systems to adapt to our selected tasks. The modified prompts are detailed in \u00a7A.1 of the Appendix. By default, we use gpt-3.5-turbo-0613 as the backbone with a temperature of zero for all the six systems, AUTOTRANSFORM, AUTOINJECT, and the defense methods in \u00a75. We introduce one malicious agent at a time to avoid interference and facilitate essential analysis. Non-malicious agents remain unaware of the malicious agent's presence, reflecting a realistic information-asymmetric scenario (Zhou et al., 2024)."}, {"title": "4.2 RQ1: Impact of System Architectures", "content": "The hierarchical structure has a higher resilience than other two, exhibiting the smallest accuracy drop. Fig. 3a illustrates the impact of AUTOTRANSFORM and AUTOINJECT on various multi-agent system types across different downstream tasks. System resilience, ranked from strongest to weakest, is: hierarchical, flat, and linear. The hierarchical architecture experiences relative accuracy drops of 23.6% and 22.6% for AUTOTRANSFORM and AUTOINJECT, respectively. We attribute this resilience to the presence of a higher-level agent (e.g., the evaluator in MAD), which is always presented with various versions of the answer by multiple agents performing the same sub-task, increasing the likelihood of error recovery from a single agent. The flat structure shows similar resilience for AUTOTRANSFORM but significantly lower resilience for AUTOINJECT. This is due to the lack of a high-level leader in the <A\u2192B\u2192C> structure to supervise and select the agent with the best result. The linear architecture demonstrates the lowest resilience. In addition to lacking a leader, it also lacks communication between agents, resulting in a one-way assembly line.\nAUTOINJECT causes a larger performance drop than AUTOTRANSFORM. While one might assume AUTOTRANSFORM would have a greater negative impact on multi-agent collaboration due to its permanent modification of agents' profiles into malicious ones, it is AUTOINJECT that results in a more significant performance drop, although AUTOINJECT introduces errors into a fixed and relatively small portion of messages. The reasons for this are two-fold: (1) Current LLMs have a weakness where they become less effective as the context lengthens, especially where conflict exists in instructions. For our malicious agents, they gradually lose track of the task to produce errors, prioritizing new instructions from other agents to correct errors in the message. (2) AUTOINJECT consistently introduces errors, whereas AUTOTRANSFORM does not always ensure error generation. Despite being transformed into malicious agents, they sometimes fail to generate errors due to constraints requiring errors to be stealthy."}, {"title": "4.3 RQ2: Impact of Downstream Tasks", "content": "Tasks requiring rigor and formalization, such as code generation and math, are more sensitive to agent errors and exhibit lower resilience compared to translation and text evaluation. Code generation and math demand greater objectivity than the more subjective tasks of translation and text evaluation. Fig. 3b illustrates the impact of AUTOTRANSFORM and AUTOINJECT across different downstream tasks. We also present the performance of single-agent, as reported by Chen et al. (2021), Liang et al. (2023b), and Chan et al. (2023), for a clearer comparison. The results indicate several conclusions: (1) Objective tasks benefit more from multi-agent collaboration, while subjective tasks gain less. Additionally, errors in subjective tasks are often overlooked by other agents due to the lack of rigorous correctness standards. (2) In terms of system resilience, tasks ranked from least to most vulnerable are: code generation, math, translation, and text evaluation. Even minor errors in the first two tasks, particularly in code generation, significantly affect rigor and formalization. Conversely, the latter two tasks are less sensitive to minor variations in a single agent's output. (3) AUTOTRANSFORM and AUTOINJECT perform similarly across most tasks, except in code generation.\nInjecting errors can surprisingly improve performance on downstream tasks. We find that certain multi-agent collaboration systems, such as MAD, Camel, and AgentVerse, benefit from deliberately injected errors rather than being hindered by them."}, {"title": "4.4 RQ3: Impact of Error Rates", "content": "Increasing the number of erroneous messages causes a larger performance drop than the number of errors within a message. Since AUTOTRANSFORM lacks precise control over error rates and types, we focus on AUTOINJECT for RQ3 and RQ4. Fig. 5a presents two experiments: one with a fixed $P_e$ = 0.2 and varying $P_m$ at 0.2, 0.4, and 0.6, labeled \u201cErroneous Message;\u201d The other with a fixed $P_m$ = 0.2 and varying $P_e$ at 0.2, 0.4, and 0.6, labeled \u201cErrors per Message.\" The performance drop for erroneous messages is nonlinear, with the most significant decrease occurring between 0 and 0.2. As $P_m$ increases, the rate of decline diminishes. Regarding the error ratio in a single message are as follows: (1) In contrast to $P_m$, the performance decline is nearly linear as $P_e$ increases. (2) As $P_e$ increases, performance decreases, implying that while higher error rates make errors more noticeable, the agent system struggles to correct the increasing number of errors. An exception is observed when increasing $P_e$ from 0.4 to 0.6, resulting in a performance increase in three systems (MetaGPT, Self-collab, MAD). This occurs because excessive errors in a single message become noticeable, prompting other agents to request corrections. This phenomenon highlights the importance of stealth in introducing errors."}, {"title": "4.5 RQ4: Impact of Error Types", "content": "Semantic errors cause a greater performance drop than syntactic errors. Fig. 5b presents the performance decline caused by syntactic and semantic errors across six systems, including the average."}, {"title": "4.6 Case Study", "content": "Introduced errors can cause performance increase. Fig. 6a depicts a conversation of two Camel agents completing a code generation task from HumanEval. An additional error is introduced by AUTOINJECT below an incorrect line of code. Subsequently, another agent identifies the injected error and instructs the first agent to correct it without noting the pre-existing error. Ultimately, the system corrects both the introduced error and the original error successfully.\nCurrent LLMs prioritize natural language over code. Fig. 6b illustrates that distraction comments can mislead LLMs into accepting incorrect code as correct across all six systems studied. This indicates that the systems tend to prioritize comments over the actual code. In the example, the system detects an error in the code when no comments are present. However, when a comment stating \"the bug had been corrected\" is added, the system overlooks the error and proceeds with the next task. AUTOTRANSFORM exploits this characteristic of LLMs to execute successful attacks."}, {"title": "4.7 Other Factors", "content": "Previous experiments in \u00a74 focus on polluting the agents directly responsible for the work, rather than those who delegate tasks to other agents. To examine the impact of polluting different types of agents and the generalizability of our AUTOTRANSFORM on agents with varying roles, this section investigates the effects of polluting high-level agents. Specifically, we apply AUTOTRANSFORM to the User and Assistant agents in Camel, and the Product Manager and Engineer agents in MetaGPT. The results of these systems completing code generation tasks are shown in Fig. 7a. The conclusions are as follows: (1) AUTOTRANSFORM is applicable to agents with different profiles or functionalities, effectively disrupting collaboration. (2) Polluting higher-level task distributors results in a greater performance drop for both systems. The second finding aligns with our intuition that instructors controlling the broader aspects are more crucial. For example, in Camel, the Assistant agent struggles to recognize \"toxic\" instructions from the User agent due to its role of merely following instructions."}, {"title": "5 Improving System Resilience", "content": "Based on our experimental observations and findings, we propose two strategies for improving resilience in multi-agent collaboration systems, defending against malicious agents.\nThe core idea behind our defense methods involves adding a correction mechanism within the system. We explore two variants, the \u201cInspector\u201d and the \u201cChallenger.\u201d The \"Inspector,\" similar to our AUTOINJECT, is an additional agent that intercepts all messages spread among agents, checks for errors, and corrects them. This method draws inspiration from the \"Police\" agent in Zhang et al. (2024).\nIn contrast, the \u201cChallenger,\u201d akin to our AUTOTRANSFORM, is an additional description of functionalities added in agent profiles. This method addresses the limitation that many agents can only execute assigned tasks and may not address certain problems they encounter, although they usually have the knowledge to. By empowering agents to challenge the results of others, we enhance their problem-solving capabilities. This is because most current multi-agent systems use the same LLM as the backbone for all agents, indicating their underlying ability to partially solve tasks outside their specialization. Detailed prompts for the \"Police\" agent and \u201cChallenger\u201d method can be found in \u00a7A.4 and \u00a7A.5 in the appendix.\nWe apply these defense strategies to the two weaker architectures: the linear (Self-collab) and the flat (Camel). Results for the vanilla model, error injection with AUTOINJECT, and the two defense methods are shown in Fig. 8. Both defense methods improve performance against AUTOINJECT, though they do not restore it to the original level. Additionally, there is no definitive conclusion as to which method is superior. We recommend trying both in practice."}, {"title": "6 Related Work", "content": "LLMs enhance multi-agent systems through their exceptional capability for role-play (Wang et al., 2024). Despite utilizing a same architecture, like GPT-3.5, distinct tasks benefit from tailored in-context role-playing prompts (Min et al., 2022). Besides the six frameworks selected in this study, researchers have been exploring multi-agent collaboration in downstream tasks or simulated communities. ChatEval (Chan et al., 2023) is a multi-agent debate system for evaluating LLM-generated text, providing a human-like evaluation process. ChatDev (Qian et al., 2023) uses a linear structure of several roles to address code generation tasks. AutoGen (Wu et al., 2023) offers a generic framework for building diverse applications with multiple LLM agents. AutoAgents (Chen et al., 2023a) enables dynamic generation of agents' profiles and cooperation, evaluated on open-ended QA and creative writing tasks. Agents (Zhou et al., 2023a) support planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control for multi-agent or human-agent collaboration. There is also work simulating daily life and conversations (Park et al., 2023; Zhou et al., 2023b). Additionally, there are studies on multi-agent competition (Huang et al., 2024; Liu et al., 2024b; Liang et al., 2023a). These frameworks are not selected either because they are not task-oriented (e.g.., simulated society or competitions) or their system design overlaps with those chosen for this study."}, {"title": "6.2 Safety Issues in Multi-Agent Systems", "content": "PsySafe (Zhang et al., 2024) is a framework that integrates attack, evaluation, and defense mechanisms using psychological manipulation involving negative personalities. EG (Evil Geniuses) (Tian et al., 2023) is an attack method that automatically generates prompts related to agents' original roles, similar to our AUTOTRANSFORM. While PsySafe and EG are applied to different multi-agent systems such as Camel and MetaGPT, they do not examine the impact of adversaries on downstream tasks like code generation or translation. Amayuelas et al. (2024) investigates how an adversary in multi-agent debate can disrupt collaboration in tasks including MMLU (Massive Multitask Language Understanding) (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2022), MedMCQA (Pal et al., 2022), and LegalBench (Guha et al., 2024), finding that the adversary's persuasion skill is crucial for a successful attack. Ju et al. (2024) proposes a two-stage attack strategy to create an adversary that spreads counterfactual and toxic knowledge in a simulated multi-agent chat environment. This method can effectively break collaboration in MMLU. Unlike our study, Amayuelas et al. (2024) and Ju et al. (2024) do not explore how different system architectures are affected by these adversaries."}, {"title": "7 Conclusion", "content": "This paper investigates the resilience of three multi-agent collaboration systems\u2014linear, flat, and hierarchical\u2014against malicious agents that produce erroneous or misleading outputs. Six systems are selected and evaluated on four downstream tasks, including code generation, math problem solving, translation, and text evaluation. We design AutoTRANSFORM and AutoINJECT to introduce errors into the multi-agent collaboration. Results indicate that the hierarchical system demonstrates the strongest resilience, with the lowest performance drops of 23.6% and 22.6% for the two error introduction methods. However, some systems can benefit from the intentionally introduced errors, further improving performance. Objective tasks, such as code generation and math, are more significantly affected by errors. Additionally, the frequency of erroneous messages impacts resilience more than the number of errors within a single message. Moreover, systems show greater resilience to syntactic errors than to semantic errors. Finally, we recommend designing multi-agent systems with a hierarchical structure, which reflects a prevalent collaboration mode in real-world human society."}, {"title": "Limitations", "content": "There are several limitations in this study. First, due to budget constraints, we exclusively use gpt-3.5-turbo-0125 for all experiments. Our primary goal is to fairly evaluate different multi-agent systems' resilience against malicious agents. We believe the results would not significantly differ from other models. However, using stronger models for AUTOTRANSFORM and AUTOINJECT might yield improvements, which we plan to explore in future work. The second limitation is the selection of multi-agent systems and downstream tasks, which cannot be comprehensive. We mitigate this by selecting representative systems from three well-established human collaboration modes (Yang & Zhang, 2019; Alexy, 2022; Mihm et al., 2010) and using four commonly-used datasets for benchmarking the abilities of multi-agent systems (Liang et al., 2023b; Chen et al., 2021). The final limitation concerns the analysis, where latent variables affecting system resilience might be unidentified. We examine system architectures, downstream tasks, error rates, error types, agent roles, and the number of agents' communications. To the best of our knowledge, no additional factors influencing system resilience are found."}, {"title": "Broader Impacts", "content": "The two error introduction methods developed in this study, AUTOTRANSFORM and AUTOINJECT, could potentially pollute benign agents and result in negative social impacts. To mitigate this risk, we have proposed effective defense mechanisms against them. We would like to emphasize that the goal of proposing these methodologies is to study and improve the behavior of LLM-based multi-agent systems. We strongly oppose any malicious use of these methods to achieve negative ends."}, {"title": "A Prompt Details", "content": "All six multi-agent collaboration systems selected in this study support only some of the downstream tasks in their original design. Therefore, we extend three scalable systems-Camel, MAD, and AgentVerse-to adapt to all four downstream tasks. The first three systems provide a high-level, non-task-oriented design for task division, while the other three systems are deeply intertwined with code generation tasks. Using Camel as an example of adapting systems to other tasks: For translation and math, we improve system performance by adding \"step by step\" instructions in prompts. For instance, in translation, it correctly interprets \u201c\u62c9\u4e0b\u6c34 (pull into water)\u201d to its correct meaning of \u201cengaging in wrongdoing\u201d in Chinese. In math, a single agent calculates \u201cAverage Speed= (1 + 3)/2 = 1m/s,", "average speed= (1+3)/2 = 2m/s.\\\" The detailed instructions likely reduce the occurrence of \u201cseemingly": "orrect answers and increase accuracy in these specific cases."}, {"title": "A.1 Multi-Agent Systems on Different Tasks", "content": ""}, {"title": "A.1.1 Camel", "content": ""}, {"title": "A.1.2 MAD", "content": ""}, {"title": "A.1.3 AgentVerse", "content": ""}, {"title": "A.2 AUTOTRANSFORM", "content": ""}, {"title": "A.3 AUTOINJECT", "content": ""}, {"title": "A.4 Defense Method: Inspector", "content": ""}, {"title": "A.5 Defense Method: Challenger", "content": ""}]}