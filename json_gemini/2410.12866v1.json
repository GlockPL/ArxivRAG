{"title": "TOWARDS HOMOGENEOUS LEXICAL TONE DECODING\nFROM HETEROGENEOUS INTRACRANIAL RECORDINGS", "authors": ["Di Wu", "Siyuan Li", "Chen Feng", "Lu Cao", "Yue Zhang", "Jie Yang", "Mohamad Sawan"], "abstract": "Recent advancements in brain-computer interfaces (BCIs) have enabled the decod-\ning of lexical tones from intracranial recordings, offering the potential to restore\nthe communication abilities of speech-impaired tonal language speakers. How-\never, data heterogeneity induced by both physiological and instrumental factors\nposes a significant challenge for unified invasive brain tone decoding. Traditional\nsubject-specific models, which operate under a heterogeneous decoding paradigm,\nfail to capture generalized neural representations and cannot effectively leverage\ndata across subjects. To address these limitations, we introduce Homogeneity-\nHeterogeneity Disentangled Learning for neural Representations (H2DiLR), a\nnovel framework that disentangles and learns both the homogeneity and hetero-\ngeneity from intracranial recordings across multiple subjects. To evaluate H2DiLR,\nwe collected stereoelectroencephalography (sEEG) data from multiple participants\nreading Mandarin materials comprising 407 syllables, representing nearly all Man-\ndarin characters. Extensive experiments demonstrate that H2DiLR, as a unified\ndecoding paradigm, significantly outperforms the conventional heterogeneous de-\ncoding approach. Furthermore, we empirically confirm that H2DiLR effectively\ncaptures both homogeneity and heterogeneity during neural representation learning.", "sections": [{"title": "1 INTRODUCTION", "content": "The human language system, with its intricate and expansive syntactic structure, enables rich and\ncomplex communication. Decoding spoken language from within human brains has emerged as a\nsignificant topic of interest in neuroscience. The decoding of vocal tone from brain measurements is of particular research interest, due to the prominence of tonal languages,\nwhich make up over 60% of the world's languages and are spoken by approximately\none-third of the global population. In these languages, tone plays a\ncritical role in distinguishing lexical meaning at the syllable level.\nMandarin, for instance, is a widely spoken tonal language that has an extensive inventory of over\n50,000 characters, with each associated with a syllable composed of an initial, a final, and a tone. Mandarin features four tones, each characterized by starting pitch height and contour.\nThe same initial and final components can yield entirely different semantic meanings when uttered\nwith different tones. For instance, the syllable formed by the initial /b/ and\nthe final /a/ can represent vastly different concepts depending on the tone: a high-level tone (Tone 1)\nsignifies \u2018eight' (\u516b), a rising pitch contour (Tone 2) indicates \u2018pull' (\u62d4), a low falling-rising tone\n(Tone 3) means \u2018handle\u2019 (\u628a), and a high falling tone (Tone 4) translates to \u2018father' (\u7238). Consequently,\nprecise tone identification is crucial for brain sentence decoding of tonal languages.\nRecent studies have shown the feasibility of decoding tones using non-invasive neurophysiological\nsignals such as electroencephalogram (EEG) and more promisingly,\nintracranial recordings such as electrocorticography (ECoG). While EEG provides a\nnon-invasive method, ECoG offers superior spatiotemporal resolution and reduced signal attenuation,\nleading to better decoding performance and interpretability. Nonetheless, the heterogeneity evoked by\nboth physiological and instrumental factors is a major challenge for invasive brain neural decoding."}, {"title": "2 RELATED WORK", "content": "Brain Language Decoding. With the mature application of biomedical circuits and the advance-\nment of deep learning, the in-depth exploration of the perceptual and processing mechanisms of the\nhuman brain in response to language and speech has attracted increasing research attention in neuro-\nscience. Recent studies have demonstrated the feasibility of decoding language and speech intentions\nfrom both non-invasive and invasive neural recordings. Early studies primarily focused on\nthe binary classification of language components such as syllables, phonemes, and words from non-\ninvasive brain signals like functional magnetic resonance imaging (fMRI), functional near-infrared\nspectroscopy (fNIRS), and electroencephalography (EEG). In contrast to non-invasive approaches,\nintracranial electroencephalography, such as electrocorticography (ECoG), offers superior spatial\nresolution and signal-to-noise ratios, leading to more robust decoding performance. Anumanchipalli et al. (2019) pioneered sentence-level English decoding\nusing recurrent neural networks (RNNs) to predict Mel-frequency cepstral coefficients (MFCC)\nfrom ECoG signals, which were then converted into speech via a vocoder. Beyond non-tonal languages, accurate recognition of lexical tones is crucial for decoding\ntonal languages from brain signals due to their distinctive pitch articulation and their critical role\nin differentiating lexical meaning. For instance, Liu et al. (2023) employed long short-term memory (LSTM)\nnetworks to predict Mel spectrograms from ECoG signals, successfully generating sound waves of\nsyllables /mi/ and /ma/ along with their corresponding four tones using the Griffin-Lim algorithm.\nHowever, these studies are often limited by small datasets, as data is typically recorded from patients\nundergoing neurosurgery, resulting in restricted data availability. Guo & Chen (2022) performed\nmulti-class classification of four tones across vowels /a/, /i/, /o/, and /u/ using manually extracted\nfeatures from fNIRS. Despite these advancements, existing studies are mostly confined to tone\ndecoding on limited syllables, using small subject-specific datasets. In this work, we expand upon\nprior research by performing full-spectrum tone decoding across all possible syllables in Mandarin\nChinese using stereoelectroencephalography (sEEG). Additionally, we propose a unified brain tone\ndecoding framework that integrates neural recordings from multiple subjects.\nHeterogeneity in Neural Representation Learning. Benefiting from large-scale training corpora,\nrecent breakthroughs in natural language processing (NLP) have demonstrated the exceptional\ncapabilities of large language models as general-purpose task solvers. Similarly,\nin computer vision (CV), generative models have shown remarkable performance when trained on\nextensive datasets. However, these advances often rely on the assumption that data\nis independent and identically distributed (IID), which is rarely applicable to neural representation\nlearning due to the heterogeneity inherent in neurological data acquisition. Heterogeneity in neural\ndata manifests in various ways, including physiological and neural differences across individuals,\nvariations in electrode configurations during data collection, and fluctuations in neural activity\nacross recording sessions. To address these challenges, several researchers have\nattempted to train universal models\u2014primarily spatiotemporal encoders-by combining data from\nmultiple subjects or sessions to overcome the heterogeneity caused by physiological and neural\nvariability. However, most existing approaches\nassume homogeneous experimental setups, where the number and placement of electrodes are\nconsistent across subjects. In addressing heterogeneity due to different electrode configurations,\nLaBraM (Anonymous, 2024) and MMM (Yi et al., 2023) introduced pre-training strategies based\non the standardized 10-20 and 10-10 EEG acquisition systems to mitigate channel compatibility\nissues during model training. However, these methods are restricted to non-invasive EEG systems,\nlimiting their applicability to broader neural decoding tasks. BIOT (Yang et al., 2023) tackled data\nheterogeneity by introducing handcrafted embeddings to align neural representations across subjects."}, {"title": "3 HOMOGENEITY-HETEROGENEITY DISENTANGLED LEARNING FOR NEURAL\nREPRESENTATION", "content": "We first present the overall learning paradigm of H2DiLR in comparison to other existing learning\nparadigms in Sec. 3.1. We then propose unified pattern-aware neural tokenization (UPaNT) in\nSec. 3.2, the prerequisite for homogeneity-heterogeneity disentanglement (H2D). The details of H2D\nare elaborated in Sec. 3.3, along with the corresponding model architectures described in Sec. 3.4.\nManaging heterogeneity caused by physiological or instrumental factors remains a fundamental\nchallenge in neural representation learning. As illustrated in Fig. 2-(a), existing approaches typically\nadopt a purely heterogeneous training paradigm, wherein subject-specific models are trained indepen-\ndently for each individual. This learning paradigm effectively handles heterogeneity with apparent\ndrawbacks: the lack of unified neural representation learning capability across individuals and poor\ngeneralization of learned representations, particularly in invasive scenarios with limited data.\nTo overcome these limitations, we propose a novel learning paradigm named H2DiLR, which contains\nan H2D stage and a neural decoding (ND) stage, as shown in Fig. 2. In the H2D stage, we learn\nneural representations that capture both homogeneous and heterogeneous features by leveraging\ndata from all subjects in an unsupervised, task-agnostic manner through vector-quantized (VQ) style\nreconstruction. We will elaborate on VQ in Sec. 3.2. At a high level,\nH2D stores homogeneous information in a shared, trainable codebook accessible to all subjects,\nwhile subject-specific private codebooks capture heterogeneous information. In the ND stage, the\nparameters of encoders and cookbooks are frozen for H2D representation extraction. A lightweight\ntransformer is adopted for specific downstream neural decoding tasks in a supervised manner.\nPrerequisites for Homogeneity-heterogeneity Disentanglement. To successfully disentangle\nhomogeneity and heterogeneity in neural recordings across multiple subjects, neural representation\nlearning must meet two key requirements: (i) Unify the latent representation space of hetero-\ngeneous neural recordings for neural decoding. This property allows the neural representation\nlearning algorithm to handle data heterogeneity and build a single decoding model; (ii) Extract"}, {"title": "3.2 UNIFIED PATTERN-AWARE NEURAL TOKENIZATION", "content": "features with explicit semantic patterns for further H2D. Take speech decoding for an example.\nThe articulation of speech involves the intricate coordination of oral organs, including the tongue,\nlarynx, vocal tract, and others. During vocalization, these organs exhibit explicit muscle movements\nassociated with specific states under neural control. For instance, the muscles in\nthe larynx bring the vocal cords closer to realize pitch voicing. Consequently, extracting neural\nrepresentations associated with the opening and closing of the vocal cords is critical for lexical tone\ndecoding. Likewise, capturing additional critical articulation patterns, such as manner of articulation\n(MOA) and place of articulation (POA), will benefit neural representation learning.\nDriven by the above design principles, we propose unified pattern-aware neural tokenization (UPaNT)\nto characterize neural patterns (pitch articulation in our case) in a unified manner during neural\nrepresentation learning based on vector-quantized (VQ) autoencoding. It's important to note that UPaNT, without H2D, could be considered a homogeneous training\nparadigm since it manages to learn neural representations from multiple subjects in a unified manner.\nThe concept of vector quantization (VQ) was initially introduced for learning discrete representations\nin the context of natural images. Holistically, VQ discretizes the continuous latent representations\ngenerated by the encoder by substituting them with the nearest quantized embeddings from an online\noptimizable codebook and further reconstructs the original input with quantized representations. The\ndiscretized embeddings in the codebook demonstrate explicit semantic information. In vision representation learning, for example, these discretized embeddings often correspond\nto interpretable patterns like color and texture.\nConsider we collect intracranial recordings from m subjects, {Si}1, given the same neural decoding\ntask. We assume substantial differences exist among the m sets of recordings {Si}\u2081 due to electrode\nconfiguration variations among subjects. Xi \u2208 RNi\u00d7T\u00d7Ci denotes the set of recordings collected\nfrom subject i with different number of data samples Ni, different number of channels Ci, and the\nsame segment length T. The neural recordings X are first mapped into latent feature, z = Ei (x, \u03b8\u03b5) \u0395\nIRLXD, in the continuous latent space by a set of encoders {E\u2081(\u00b7; 0i)}m1 parameterized by network\nparameters {0}1. A finite VQ codebook of K key-value pairs, C = {(k,e(k))}k\u2208[K], where\neach code k owns its learnable code embedding e(k) \u2208 RD, can discretize each token in z by a\nquantization function Q(., .):\n$M_{j} = Q(z_{j};C) = \\underset{k \\in [K]}{argmin} ||z_{j} - e(k)||_{2},$ (1)\nwhere zj \u2208 R1\u00d7D denotes the jth token of z with 1 \u2264 j \u2264 L, Mj \u2208 [K]\u00b9 is code mapping indices.\nWith assigned M, the latent feature z can be indexed and quantized to the VQ embedding by the\nclosest 1-of-K embedding vectors in the codebook Cas 2j = e(Mj). Then, a set of decoders\n{Di(; Vi)}1 parameterized by {$i}m1 maps the VQ embedding back to the input space to\nreconstruct the original neural recordings 2:\nx = D\u00bf(2; Vi) = Di(e(M); Vi),"}, {"title": "3.3 \u041d\u043e\u043cOGENEITY-HETEROGENEITY DISENTANGLEMENT", "content": "Since differentiation through the quantization in Eq. (1) is ill-posed during gradient backward,\nthe straight-through-estimator (STE) is employed as the approximation, i.e.,\n(e(Mj) + zj) \u2013 zj. Overall, the learning objective of VQVAE on X includes Lrec for reconstruction\nof autoencoders, Lcode for the codebook learning, and Lcommit for quantization:\n$L_{VQ} = \\sum_{i=1}^{m}||x_{i} - \\hat{x}_{i}||^{2} + \\sum_{j=1}^{N_{i}}||sg[z_{i}] - \\hat{z_{i}}||^{2} + \\beta|| z_{i} - sg[\\hat{z_{i}}||^{2},$ (3)\nwhere sg[.] denotes the stop gradient operation and \u00df > 0 is a hyper-parameter set to 0.25 by default.\nWe formalize homogeneity-heterogeneity disentanglement (H2D) based on UPaNT. To capture\nthe homogeneous and heterogeneous neural representations, we first define a shared codebook\nCS = {(k,es(k))}k\u2208[KS] to maintain the common neural embeddings with high-level semantic\npatterns extracted from all subjects under the same task. We also keep m private codebooks {C}=1\nto encode the unique patterns of m different subjects, where C = {(k, e\u00a3(k))}k\u2208[KP]. Given\nthe nth neural recording sample Ini from subject i, we first rank all tokens in the continuous\nencoded feature zn,i with the similarity between the tokens and the corresponding selected nearest\ncode embeddings in the shared codebook CS, Rn,i = rank({||Zn,i,j \u2013 eS(Mn,i,j)||}}=1), where\nrank(\u00b7) denotes the ascending ranking. Based on the ranking result, we split the tokens into the\nhomogeneous group and the heterogeneous group, where the top-vL tokens (the most similar ks\ntokens for CS) are quantized with the shared codebook, while the rest vL tokens quantized using the\ncorresponding private codebook of subject i. The partition factor v \u2208 [0, 1] to balance homogeneous\nand heterogeneous representations. The H2D quantization can be formulated as:\n$\\hat{z}_{n,i,j} = {e^{s}(M_{n,i,j}), R_{n,i}(j) \\le vL\\e^{p}(M_{n,i,j}), R_{n,i}(j) > vL}$ (4)\nThe shared codebook CS and the private codebooks C\nare then updated with two different strategies.\nThe shared codebook is updated by:\n$\\hat{z}_{n,i,j} = (1-a)z_{n,i,j} + a\\hat{z}_{n,i,j},  R_{n,i}(j) \\le vL$ (5)\nwhere a is the momentum coefficient for the exponential moving average (EMA). Note that the EMA\nupdate of the codebook in Eq. (5) reduces the training instability caused by updating conflicts of the\ncertain code from latent tokens of different subjects. The private codebooks are\nupdated as follows:\n$L_{n,i}^{Comm} = \\sum_{j=1}^{L}||sg[z_{n,i,j} - \\hat{z}_{n,i,j}||^{2}, R_{n,i}(j) > \\nu L.$ (6)\nNote that Chris applied only to the rest vL tokens. We use the commitment loss to align latent\nembeddings to the relevant codes as the same design as in VQVAE:\nComm = $L_{Comm} = \\sum_{j=1}^{L}||z_{n,i,j} - sg[\\hat{z}_{n,i,j}]||^{2}$ (7)\nMeanwhile, our proposed H2D adopts unsupervised autoencoding as the pretext task during neural\nrepresentation learning using the reconstruction loss from Eq. (3). Overall, the learning objective of\nH2D is defined as:\n$\\mathscr{L}_{H2D} = \\sum_{i=1}^{m} (\\mathscr{L} + \\mathscr{L} + \\beta \\mathscr{L}).$ (8)\nFor simplicity, we set the homogeneous neural representation component to have the same number of\ntokens as the heterogeneous component, i.e., v = 0.5. The size for the shared codebook KS is set to\nbe m \u00d7 KP, resulting in a total code size of K = 2mKP from all codebooks."}, {"title": "3.4 MODEL ARCHITECTURE", "content": "The architectures of VQ encoders, VQ decoders, and neural decoders can take on any arbitrary design,\nprovided that they effectively accomplish the reconstruction and disentanglement tasks in the H2D\nstage, as well as the decoding task in the ND stage. In our work, we discover that very lightweight\narchitectures can achieve promising results in lexical tone decoding. The VQ encoders consist of five\n1-D convolution layers with a kernel size of four and a stride of two. Due to varying input channel\nnumbers across different subjects, the channel dimension is first mapped to a uniform count of 64\nby the stem layer and then progressively increased to 512 before being reduced back to 256. The\nVQ-decoder adopts a symmetrical design to the VQ-encoder, wherein 1-D convolution layers are\nsubstituted with transpose convolution layers. For the ND stage, we adopt a lightweight transformer\nas the neural decoder with the multi-head self-attention (MSA) structure with pre-normalization and\nresidual connection as in ViT. The patch embedding is performed by a\n1-D convolution stem layer with a kernel size of five and stride five to ensure non-overlapping patch\nembedding. The output dimension of the stem layer is 128, and the hidden dimension of the Feed\nForward Network (FFN) is set to 512. A fully connected layer is added to map the output to desired\ndecoding output formats."}, {"title": "4 EXPERIMENTS", "content": "Data acquisition for tone decoding. We recruited four participants undergoing epilepsy monitoring\nwith stereo electroencephalograph (sEEG) electrodes implanted in an anonymous hospital (for\nanonymous submission requirement and will be made public upon acceptance) to participate in this\nstudy. The distribution of electrodes for all four participants is shown in Fig. 4. The experimental\nprotocol was approved by the Anonymous Hospital Institutional Review Board of Anonymous\nUniversity. All participants gave their written, informed consent prior to testing. For each participant,\nwe selected contacts related to speech and excluded those located in the visual cortex and white matter.\n      All participants are asked to read 407 monosyllabic Mandarin characters, each with a unique tone,\nthree times, covering all common pronunciations of Mandarin characters. To make the pronunciation\nprocess of the participants as similar as possible to normal speech, carrier words are added before\nand after each character to form a sentence. Therefore, in each trial, participants are required to read a\ncomplete sentence containing the target syllable. The collected sEEG signals are downsampled to\n1000 Hz with power line interference removed.\nEvaluation Protocols. We assess decoding performance using the top-1 accuracy (Acc). Data for\neach participant is divided into an 80% training set and a 20% testing set, with 20% of the training\ndata further allocated for validation. We conduct the experiment five times using different random\nseeds and reported the mean and standard deviation.\nBaseline. We select representative approaches of both the heterogeneous learning and homoge-\nneous learning paradigms as baselines in comparison to our proposed homogeneity-heterogeneity"}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "DATA ACQUISITION"}, {"title": "4.2 DECODING PERFORMANCE COMPARISON", "content": "disentangled learning paradigm (H2DiLR). For the heterogeneous learning paradigm, we consider\nthree supervised approaches featuring diverse backbone designs. Additionally, we include three methods utilizing contrastive and masked modeling\npre-training for a fair comparison, as the H2D\nstage of our H2DiLR can be regarded as a pre-training stage. Furthermore, we consider H2DiLR with\nv = 0, which indicates no shared codebook, as another variant within the heterogeneous learning\nparadigm. For the homogeneous learning paradigm, we examine BIOT and the\nUPaNT component of H2DiLR.\nResults comparison. We compare the performance of H2DiLR with baselines. It is\nobserved that baselines of the heterogeneous training paradigm with pre-training outperform those\nwith no pre-training. Heterogeneous approaches generally suffer from the scarcity of data from indi-\nvidual subjects, where self-supervised pre-training methods prove more effective in capturing neural\nrepresentations and subsequently improving decoding performance. Although BIOT can integrate\ndata from multiple individuals for homogeneous decoding, its performance does not significantly\noutperform heterogeneous approaches and even yields worse results compared to heterogeneous\ndecoding methods with pre-training. This is due to the fact that BIOT eliminates data heterogeneity\nin terms of channel count, sampling rate, and data length but fails to explore and utilize the inherent\nheterogeneity and heterogeneity embedded in the brain recordings from multiple subjects. Compared\nto BIOT and heterogeneous baselines, our proposed UPaNT demonstrates a better decoding perfor-\nmance due to its pattern-aware feature extraction capabilities. With H2D, our proposed H2DiLR\nfurther improves the decoding performance on top of UPaNT by disentangling the homogeneous and\nheterogeneous neural representations, leading to a significant gain over existing approaches."}, {"title": "4.3 PRELIMINARY VERIFICATION OF H2D", "content": "Homogeneous representations correlate with tonal decoding. We first evaluate whether the\nhomogeneous representations learned by our proposed H2D capture unified neural features across\nsubjects in the context of tone decoding. Specifically, we visualize the neural codes learned in the\nshared codebook of H2D in comparison to the codes learned by UPaNT using UMAP. Each neural code is assigned the tone class to which it is most frequently mapped\nduring the quantization process. The neural codes learned by UPaNT w.r.t.\neach tone class are scattered with no clear pattern, while the distribution of neural codes from the"}, {"title": "4.4 ABLATION STUDY", "content": "This section ablates three key designs and the scaling effect of the network parameters and subject\ncount. The average top-1 accuracy for tone decoding on all subjects is reported, and subject classi-\nfication tasks are designed using the same experimental setup as Sec. 4.1. Furthermore, additional\nexperiments on diverse neural decoding tasks demonstrate the effectiveness of the proposed H2DiLR\nas a general neural representation learning framework, detailed in Sec. C.1.\nAblation on \u03bd. We first study how the heterogeneous and homogeneous representation partition\nratio v influences the learned representations in terms of tone decoding and subject classification tasks.\nBy default, H2D sets v = 0.5, and a higher value of v indicates more homogeneous information\ncaptured during representation learning. With a v value of 1, H2D degenerates to the UPaNT. It is\nobserved that a smaller value of v leads to better subject classification performance with\nheterogeneous representation, indicating more heterogeneity captured. Also, a smaller value of v\nleads to inferior tone decoding performances with homogeneous representation. Results show that\nv 0.5 strikes a good balance for H2D.\nAblation on codebook size and dimension. By default, H2DiLR utilizes a codebook size and\ndimension of 256 and a fixed v value of 0.5. We study how value changes in size and dimension affect\nthe reconstruction performance in the H2D stage and the tone decoding performance in the ND stage.\nWe use the mean squared error (MSE) as the reconstruction performance metric and report results in\nTab. 3 with the default settings greyed out. We observe that codebook sizes and dimensions larger\nthan 128 lead to quite similar reconstruction performances, while a size and dimension of 64 yield\nmuch worse reconstruction performance due to the limited expressive capacity. It is worth noticing"}, {"title": "5 CONCLUSION AND LIMITATION", "content": "This paper presents homogeneity-heterogeneity disentangled learning for neural representations\n(H2DiLR), which disentangles and models the homogeneity and heterogeneity from intracranial\nrecordings of multiple subjects for neural decoding. Extensive tone decoding experiments on collected\nSEEG of multiple participants reading Mandarin suggest that H2DiLR enables unified tone decoding\nacross subjects with superior performance compared to existing methods. We list three potential\nlimitations of this work: (1) The generalization of the trained H2DiLR on unseen new subjects, which\nis of great practical value, remains to be explored. (2) Additional interpretability of the learned\nneural codes is required. Establishing mapping between learned neural codes with functionalities of\ndifferent brain regions for better interpretability remains a promising future research direction. (3)\nDue to the complexity of intracranial recording data acquisition, the current constraints prevent us\nfrom expanding our subject pool further."}, {"title": "APPENDIX", "content": "This work proposes to use stereotactic electroencephalography (sEEG) as a means of collecting\nintracranial neurophysiological data for tone decoding. sEEG is a novel international technique that\nhas emerged in recent years as a localization method for epileptic foci. This technique simultaneously\nrecords the brain electrical activity of epilepsy patients in different cranial structures, involving many\nbrain networks associated with advanced cognitive functions, such as the hippocampus, frontal lobe,\namygdala, cingulate gyrus, parietal lobe, and precuneus. Patients can undergo assessments and tests\nof advanced cognitive functions during the interictal period (when there are no symptoms of epilepsy,\nand the patient's behavior is indistinguishable from that of a normal, healthy person). Therefore,\nSEEG is currently recognized as an invasive method for studying advanced cognitive functions in\nthe human brain and does not pose additional risks, such as cranial trauma for patients during the\nresearch process.\nIn this study, we recruited four participants undergoing epilepsy monitoring with stereo electroen-\ncephalograph (sEEG) electrodes implanted in an anonymous hospital (for anonymous submission\nrequirement and will be made public upon acceptance) to participate in this study. The distribution of\nelectrodes for all four participants is shown in Fig. 4. We also provide basic information on partici-\npants, including sex, age, education level, and handedness in Tab. A1. The experimental protocol\nwas approved by the Anonymous Hospital Institutional Review Board of Anonymous University. All\nparticipants gave their written, informed consent before testing. For each participant, we selected\nchannels related to speech and excluded those located in the visual cortex and white matter. All partic-\nipants are asked to read 407 monosyllabic Mandarin characters, each with a unique tone, three times,\ncovering all common pronunciations of Mandarin characters. To make the pronunciation process of\nthe participants as similar as possible to normal speech, carrier words are added before and after each\ncharacter to form a sentence. Therefore, in each trial, participants must read a complete sentence\ncontaining the target syllable. For example, if the target syllable is '\u00e0i', the sentence presented and to\nbe read by the participant is \u201c\u6211\u8bfb\u7231\u4e09\u904d\u201d (I read love three times). Our reading material is carefully\ndesigned by Mandarin linguists to cover as many pronunciation phenomenons as possible to enable\na brain decoding algorithm with generality. The reading material contains syllables of four tones\nsubject to uniform distribution. It is worth noticing that there is an additional neutral tone, which\nhas no specific pitch contour and is typically used on less emphasized syllables where its pitch is\nprimarily influenced by the preceding syllable. It is not considered a fifth tone in addition to the four\ntones but rather a special tonal variation of Tone 4, which physically manifests itself as a shortening\nof the length of the tone and a weakening of the strength of the tone. Consequently, we do not treat\nthe neutral tone as an additional fifth class.\nNeural signals were recorded using a multi-channel electrophysiological recording device, specifically\nthe Neurofax EEG-1200 produced by Nihon Kohden Corporation, Japan, and were recorded at a\nsampling rate of 2000Hz. Each channel was subjected to visual and quantitative inspection for\nartifacts or excessive noise. We also record the audio signals of participants to provide time stamps\nfor slicing the targeted syllable from the sentence being read. sEEG signals are downsampled to\n1000 HZ with power line interference removed, and the signal fragments are padded to the maximum\nlength of 1000.\nAll our experiments are implemented by PyTorch and conducted on workstations with NVIDIA\nA100 GPUs. For all baselines (Woo et al., 2022; Eldele et al., 2021; Wu et al., 2022; Yang et al.,"}, {"title": "A TONE DECODING DATA AQUISITION", "content": "This section ablates three key designs and the scaling effect of the network parameters and subject\ncount. The average top-1 accuracy for tone decoding on all subjects is reported, and subject classi-\nfication tasks are designed using the same experimental setup as Sec. 4.1. Furthermore, additional\nexperiments on diverse neural decoding tasks demonstrate the effectiveness of the proposed H2DiLR\nas a general neural representation learning framework, detailed in Sec. C.1.\nAblation on \u03bd. We first study how the heterogeneous and homogeneous representation partition\nratio v influences the learned representations in terms of tone decoding and subject classification tasks.\nBy default, H2D sets v = 0.5, and a higher value of v indicates more homogeneous information\ncaptured during representation learning. With a v value of 1, H2D degenerates to the UPaNT. It is\nobserved that a smaller value of v leads to better subject classification performance with\nheterogeneous representation, indicating more heterogeneity captured. Also, a smaller value of v\nleads to inferior tone decoding performances with homogeneous representation. Results show that\nv 0.5 strikes a good balance for H2D.\nAblation on codebook size and dimension. By default, H2DiLR utilizes a codebook size and\ndimension of 256 and a fixed v value of 0.5. We study how value changes in size and dimension affect\nthe reconstruction performance in the H2D stage and the tone decoding performance in the ND stage.\nWe use the mean squared error (MSE) as the reconstruction performance metric and report results in\nTab. 3 with the default settings greyed out. We observe that codebook sizes and dimensions larger\nthan 128 lead to quite similar reconstruction performances, while a size and dimension of 64 yield\nmuch worse reconstruction performance due to the limited expressive capacity. It is worth noticing"}, {"title": "B IMPLEMENTATION DETAILS", "content": "This paper presents homogeneity-heterogeneity disentangled learning for neural representations\n(H2DiLR), which disentangles and models the homogeneity and heterogeneity from intracranial\nrecordings of multiple subjects for neural decoding. Extensive tone decoding experiments on collected\nSEEG of multiple participants reading Mandarin suggest that H2DiLR enables unified tone decoding\nacross subjects with superior performance compared to existing methods. We list three potential\nlimitations of this work: (1) The generalization of the trained H2DiLR on unseen new subjects, which\nis of great practical value, remains to be explored. (2) Additional interpretability of the learned\nneural codes is required. Establishing mapping between learned neural codes with functionalities of\ndifferent brain regions for better interpretability remains a promising future research direction. (3)\nDue to the complexity of intracranial recording data acquisition, the current constraints prevent us\nfrom expanding our subject pool further."}, {"title": "CADDITIONAL ABLATION STUDY", "content": "This section ablates three key designs and the scaling effect of the network parameters and subject\ncount. The average top-1 accuracy for tone decoding on all subjects is reported, and subject classi-\nfication tasks are designed using the same experimental setup as Sec. 4.1. Furthermore, additional\nexperiments on diverse neural decoding tasks demonstrate the effectiveness of the proposed H2DiLR\nas a general neural representation learning framework, detailed in Sec. C.1.\nAblation on \u03bd. We first study how the heterogeneous and homogeneous representation partition\nratio v influences the learned representations in terms of tone decoding and subject classification tasks.\nBy default, H2D sets v = 0.5, and a higher value of v indicates more homogeneous information\ncaptured during representation learning. With a v value of 1, H2D degenerates to the UPaNT. It is\nobserved that a smaller value of v leads to better subject classification performance with\nheterogeneous representation, indicating more heterogeneity captured. Also, a smaller value of v\nleads to inferior tone decoding performances with homogeneous representation. Results show that\nv 0.5 strikes a good balance for H2D.\nAblation on codebook size and dimension. By default, H2DiLR utilizes a codebook size and\ndimension of 256 and a fixed v value of 0.5. We study how value changes in size and dimension affect\nthe reconstruction performance in the H2D stage and the tone decoding performance in the ND stage.\nWe use the mean squared error (MSE) as the reconstruction performance metric and report results in\nTab. 3 with the default settings greyed out. We observe that codebook sizes and dimensions larger\nthan 128 lead to quite similar reconstruction performances, while a size and dimension of 64 yield\nmuch worse reconstruction performance due to the limited expressive capacity. It is worth noticing"}, {"title": "C.1 VERIFICATION OF H2DILR ON SEIZURE PREDICTION", "content": "This paper presents homogeneity-heterogeneity disentangled learning for neural representations\n(H2DiLR), which disentangles and models the homogeneity and heterogeneity from intracranial\nrecordings of multiple subjects for neural decoding. Extensive tone decoding experiments on collected\nSEEG of multiple participants reading Mandarin suggest that H2DiLR enables unified tone decoding\nacross subjects with superior performance compared to existing methods. We list three potential\nlimitations of this work: (1) The generalization of the trained H2DiLR on unseen new subjects, which\nis of great practical value, remains to be explored. (2) Additional interpretability of the learned\nneural codes is required. Establishing mapping between learned neural codes with functionalities of\ndifferent brain regions for better interpretability remains a promising future research direction. (3)\nDue to the complexity of intracranial recording data acquisition, the current constraints prevent us\nfrom expanding our subject pool further."}, {"title": "D BROADER IMPACTS", "content": "This section ablates three key designs"}]}