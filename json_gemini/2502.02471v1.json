{"title": "Mind the Gap: Evaluating Patch Embeddings from General-Purpose and Histopathology Foundation Models for Cell Segmentation and Classification", "authors": ["Valentina Vadori", "Antonella Peruffo", "Jean-Marie Gra\u00efc", "Livio Finos", "Enrico Grisan"], "abstract": "Recent advancements in foundation models have transformed computer vision, driving significant performance improvements across diverse domains, including digital histopathology. However, the advantages of domain-specific histopathology foundation models over general-purpose models for specialized tasks such as cell analysis remain underexplored. This study investigates the representation learning gap between these two categories by analyzing multi-level patch embeddings applied to cell instance segmentation and classification. We implement an encoder-decoder architecture with a consistent decoder and various encoders. These include convolutional, vision transformer (ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M, representing general-purpose foundation models. These are compared against ViT encoders from the recently released UNI, Virchow2, and Prov-GigaPath foundation models, trained on patches extracted from hundreds of thousands of histopathology whole-slide images. The decoder integrates patch embeddings from different encoder depths via skip connections to generate semantic and distance maps. These maps are then post-processed to create instance segmentation masks-where each label corresponds to an individual cell-and to perform cell-type classification. All encoders remain frozen during training to assess their pre-trained feature extraction capabilities. Using the PanNuke and CONIC histopathology datasets, and the newly introduced Nissl-stained CytoDArk0 dataset for brain cytoarchitecture studies, we evaluate instance-level detection, segmentation, and cell-type classification accuracy. This study provides insights into the comparative performance of general-purpose vs. histopathology foundation models, offering guidance for model selection in cell-focused histopathology and brain cytoarchitecture analysis workflows.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, foundation models have emerged as powerful tools in computer vision, driving significant performance improvements across various domains, including computational histopathology. Large-scale deep neural networks, tipically vision transformers (ViTs), are pre-trained on extensive histopathology data encompassing diverse stains, tissue types, and diseases, typically without task-specific labels. By applying self-supervised learning approaches, these models learn image representations that generalize effectively across a range of downstream tasks, such as diagnosis, disease subtyping, biomarker quantification, treatment response estimation, and survival prediction [2], [17], [26], [27], [29], [30]. Recent studies demonstrate the performance"}, {"title": "II. METHODS", "content": "We utilize the CISCA framework [23] for CS and CC, which has demonstrated greater robustness and accuracy compared to state-of-the-art methods such as Hover-Net and StarDist across different staining techniques, magnifications, and tissue types. CISCA addresses CS and CC using a multi-task approach that integrates 3-class pixel classification, distance map regression, and cell-type classification. Specifically, the framework predicts a semantic map (SM1) that assigns each pixel to one of three categories: cell body, boundary between neighboring cells, or background. Additionally, it generates four directional distance maps (DMs), and a second semantic map (SM2) where each pixel is classified by cell type. Post-processing is then applied to produce a label map, where each unique cell is assigned an integer ID, and to define the type of each detected cell. To implement CISCA, we adopt an encoder-decoder architecture similar to the original design, as illustrated in Fig. 1. In line with [23], we employ one decoder with three convolutional heads to predict SM1, SM2 and DMs. However, as described in Section II-B, the decoder and skip connections design is inspired by the UNETR architecture [10] rather than the original U-Net. This design choice ensures compatibility with the encoder-decoder architecture implementation in the cellseg_models.pytorch [12] library, facilitating the adoption of a variety of encoders from the timm library, including ViTs. Following the CISCA approach, we apply the same oversampling technique for the PanNuke and CONIC datasets and use an identical data augmentation strategy. No pre-processing is performed on the input patches. For further details on CISCA, we refer to [23]."}, {"title": "A. Encoders", "content": "We utilize four state-of-the-art architectures as encoders, which are either pre-trained on general-purpose datasets, i.e., the publicly available ImageNet-22K dataset [3] (a superset of the ImageNet-1K dataset with ~ 14M images), ImageNet-21K (Google specific variant of ImageNet-22K), or the proprietary LVD-142M dataset [18], or on specialized datasets, i.e., extensive, private histopathology datasets.\nViT [1]: ViT adapts the transformer architecture, originally developed for natural language processing [25], to image processing by partitioning images into sequences of flattened 2D patches, which are linearly projected into embeddings. These embeddings, combined with positional encodings to retain spatial context, are processed through multiple layers of multi-head self-attention and feedforward networks. This design enables ViT to capture long-range dependencies in visual data, outperforming ResNet-based baselines in image classification tasks.\nSwin Transformer [14]: In contrast to the quadratic complexity of ViTs, the Swin Transformer achieves linear computational complexity by limiting self-attention computations to non-overlapping local windows, thereby reintroducing the locality bias inherent in convolutional networks. Additionally, it constructs a hierarchical representation by progressively merging adjacent patches in deeper layers, facilitating seamless integration with dense prediction frameworks such as U-Net. In this study, we focus on Swin Transformer V2 [13], which enhances the scalability of its predecessor with multiple adjustments.\nConvNext [15]: A modernized ResNet architecture that"}, {"title": "B. Decoder", "content": "The decoder and skip connections design is inspired by the UNETR architecture [10]. Feature maps from the encoder are extracted via skip connections at four distinct levels and fused with upsampled feature maps from the preceding, deeper layer. This fusion involves concatenation, 3 \u00d7 3 convolution, ReLU activation. At the top of the decoder, convolutional heads are employed to mitigate \"fighting for features\" issues during semantic maps and distance maps predictions. In cases where feature maps from the encoder are not hierarchical-such as in the ViT architecture, where the resolution remains consistent across all levels-feature maps are first processed through a 1 \u00d7 1 convolutional block to align with a predefined number of feature channels and then upsampled to a specified resolution. For the four levels, we define channel sizes as (32, 64, 128, 256) with resolution reductions of (2, 4, 8, 16). For example, given an input image of size 256 \u00d7 256, if the feature maps from the second and third level both have dimensions (8, 1280, 16, 16), they are processed to produce feature maps of size (8, 64, 64, 64) and (8, 128, 32, 32), respectively."}, {"title": "C. Training", "content": "The training process aims to minimize a composite loss function that includes categorical cross-entropy loss and Dice loss for 3-pixel class prediction, mean absolute error (MAE) for distance map regression, and a combination of categorical cross-entropy loss and Tversky loss for cell type classification. Encoders are kept frozen during training. The decoder and skip connections are optimized using the AdamW optimizer and the OneCycleLR learning rate scheduler, which anneals the learning rate to a peak value of $10^{-4}$ and then to $10^{-6}$. The model is trained for a total of 100 epochs, with performance evaluated after each epoch. The best-performing model on the validation set is retained for evaluation on the test set."}, {"title": "III. EXPERIMENTS", "content": "We evaluated eight different models, each named after the encoder it utilizes: ConvNeXt-B-22K, MaxViT-B-21K, Swin2-B-22K, ViT-L-22K, ViT-H-142M, UNI2, Prov-GigaPath, Virchow2\u00b9. To evaluate whether feature maps from ViT blocks are sufficient for tasks such as CS and CC-and whether they retain localization information-we experiment with different strategies for feature extraction. Specifically, we consider three configurations for feature maps extraction: shallow, extraction from the first eight ViT blocks; deep, extraction from the last eight blocks; and mixed, extraction across all blocks of the ViT architecture. For instance, in a ViT model with 32 blocks indexed from 0 to 31, we extract feature maps from blocks (2, 4, 6, 8) for the shallow configuration, (25, 27, 29, 31) for the deep configuration, and (2, 5, 14, 28) for the mixed configuration. This results in 18 distinct 'models', which we evaluated on the PanNuke dataset. The top five models, defined as those ranking first or second in performance for at least one metric, were then chosen for further training and testing on the CoNIC and"}, {"title": "B. Datasets", "content": "PanNuke [4] The largest pan-cancer nuclear instance segmentation and classification dataset, with nearly 200,000 labelled nuclei in 7,901 H&E-stained histology images of size 256 \u00d7 256 from 19 different tissue types at 40x. Each nucleus is assigned to one of 5 classes: Neoplastic, Epithelial, Connective/soft tissue cells, Inflammatory, Dead cells.\nCONIC Challenge Dataset [5], [6] The largest single tissue nuclear instance segmentation and classification dataset, with nearly 500,000 labelled nuclei in 4,981 H&E-stained colon tissue histology images of size 256 x 256 at 20x. Each nucleus is assigned to one of 6 classes: Neutrophil, Epithelial, Lymphocyte, Plasma, Eosinophil, Connective tissue cells.\nCytoDArk0 [24] The first publicly available annotated dataset of Nissl-stained histological images of the mammalian brain for CS, comprising a total of 38, 755 cells, including neurons and glial cells. We use CytoDArk0_20x_256, a patched version of CytoDArk0 that consists of 1, 104 patches at 20x, each with a size of 256x256 pixels. CytoDArk0 was recently released to facilitate research in digital neuropathology and studies on brain cytoarchitecture [8], [9].\nEach dataset used for training was split into three subsets, following [23]: approximately 70% for training, 10% for validation, and 20% for testing. Details for replicating these splits can be found at https://github.com/vadori/cytoark."}, {"title": "C. Performance Evaluation", "content": "CS performance is assessed by calculating the panoptic quality (PQ) [6]. PQ is the product of detection quality (DQ), which evaluates the model's ability to identify cell locations, and segmentation quality (SQ), which determines the accuracy of delineating cell boundaries. A predicted cell instance is a true positive (TP) if its intersection over union (IoU) with the corresponding ground truth instance is greater than 0.5. Predicted instances without a corresponding match are classified as false positives (FP), while unmatched ground truth instances are counted as false negatives (FN). We provide results for recall $R = \\frac{|TP|}{(|TP| + |FN|)}$ and precision $P = \\frac{|TP|}{(|TP| + |FP|)}$. The F1-score corresponds to $DQ = \\frac{|TP|}{(|TP| + 0.5 \u00b7 (|FP| + |FN|))}$, whereas SQ is computed as the average IoU between matched predicted instances and their respective ground truth instances. All the above metrics are computed separately for each image, and the results are averaged. CC performance is assessed using the multi-class panoptic quality mPQ+ [6]. This entails computing panoptic quality for each cell type by aggregating TP, FP, and FN across all images, ensuring robustness to missing cell classes. The final metric is then averaged across classes."}, {"title": "IV. RESULTS", "content": "CS and CC performance on the PanNuke dataset for each model evaluated in this study are shown in Table I. The horizontal lines categorize different models according to the encoder types: non-ViT models (top section); shallow ViT configurations (second section); deep ViT configurations (third section); mixed configurations (bottom section), depending on the ViT blocks used for feature extraction. The Swin2-B-22K model emerges as the top-performing method overall, achieving the highest scores across multiple key metrics. It excels in PQ, demonstrating its ability to accurately detect and segment cell contours, and in mPQ+, reflecting its effectiveness in classifying cells into distinct types. Additionally, Swin2-B-22K maintains computational efficiency, requiring the fewest parameters and the second-lowest FLOPs among all models. MaxViT-B-22K achieves the best result on R and the second-best result on mPQ+. ConvNeXt-B-22K achieves the best result on SQ and the second-best result on PQ. ViT models are outperformed by non-ViT models across all indicators with a few exceptions. Virchow2 (mixed) ranks second in precision, while UNI2 (mixed) achieves the second-highest recall and DQ."}, {"title": "V. DISCUSSION", "content": "Semantic segmentation, and instance segmentation as a particular case, inherently faces a trade-off between semantic understanding and spatial localization: global context determines what is present, while local information identifies where it is located [16]. Feature hierarchies capture both through a nonlinear local-to-global pyramid. In convolutional encoders, feature maps progressively evolve from capturing low-level features such as edges and textures to high-level semantic representations, often resulting in the loss of fine-grained spatial details. In encoder-decoder architectures, skip connections enable the decoder to leverage this multi-scale feature representation by combining deep, coarse semantic features from the deeper layers of the encoder with shallow, fine-grained features from the shallower layers. Unlike CNNs, ViTs maintain more uniform feature representations across blocks due to their self-attention mechanism, which facilitates the early aggregation of global information and integrates contextual cues from distant regions within the image [19]. Cells are typically small structures that do not occupy large regions of the input image. The global context, essential for diagnostic and prognostic assessments, should enhance the overall comprehension of image semantics and aid in identifying cell type compositions, as these types are often interconnected and influenced by the surrounding tissue microenvironment. However, local information is crucial for precise detection and segmentation. The inductive biases inherent in convolutional networks and Swin Transformers\u2014such as spatial locality and hierarchical feature learning-may be critical for accurate CS and CC. In contrast, specialized foundation models with a ViT backbone rely on multiple blocks of multi-head self-attention, which may overly diffuse fine-grained information, impacting the detection and segmentation quality of small structures like cells. This may explain why ViT encoders are outperformed by non-ViT encoders, particularly ConvNeXt and the Swin Transformer V2, which combines the benefits of locality with attention mechanisms while retaining the hierarchical representation characteristic of convolutional architectures. This is not the sole factor that may explain our results. While models pre-trained on ImageNet-22K used a supervised learning approach, others underwent unsupervised pre-training, which could influence performance differences. At the architectural level, we used 1\u00d71 convolutions to reduce the number of channels in the feature maps extracted from the encoder (cf. Section II-B). Increasing the number of channels may help mitigate dimensionality loss in ViTs' latent space. Additionally, ViTs process images by dividing them into patches before encoding them. The image size and patch size used during training directly affect the model's parameterization. At inference time, images must either be rescaled to match the training resolution or require interpolation to adjust the patch embeddings and the Conv2D projection layer. This preprocessing step may impact performance, contributing to the observed differences. Further experiments are needed to determine whether rescaling would be more effective. \nOur findings also highlight the crucial impact of feature depth, with ViT shallow or mixed configurations outperforming deep ones. In a Swin Transformer V2 or ConvNeXt, the choice of blocks from which to extract features is somehow obliged, since these models have four stages that group a series of blocks, and it is customary to take the feature maps outputted by the last block for each stage. This is the choice supported in the timm library. In ViTs, any block can be chosen, and this flexibility translates into an optimization problem: which combination is the best?"}, {"title": "VI. CONCLUSIONS", "content": "This study highlights the superior representation capabilities of the general-purpose Swin Transformer V2 and ConvNeXt encoders, pre-trained on ImageNet-22K, in the context of CS and CC, outperforming both general-purpose and histopathology-specific ViT-based foundation models. Applied without fine-tuning, these encoders emerge as the most robust out-of-the-box choice. The underperformance of ViT-based models may stem from their lack of inductive biases, such as locality and hierarchical representation learning, though additional factors warrant further investigation. These include differences in learning paradigms, dimensionality reduction in feature maps, and the effects of patchification and interpolation. Moreover, as only a limited number of ViT encoder configurations were explored, further optimization of block combinations for feature extraction could enhance performance. These findings emphasize the need for deeper exploration of ViT encoders while offering insights to advance CS and CC tasks."}]}