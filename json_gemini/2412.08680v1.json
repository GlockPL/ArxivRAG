{"title": "Distinguishing Scams and Fraud with Ensemble Learning", "authors": ["Isha Chadalavada", "Tianhui Huang", "Jessica Staddon"], "abstract": "Users increasingly query LLM-enabled web chatbots for help with scam defense. The Consumer Financial Protection Bureau's complaints database is a rich data source for evaluating LLM performance on user scam queries, but currently the corpus does not distinguish between scam and non-scam fraud. We developed an LLM ensemble approach to distinguishing scam and fraud CFPB complaints and describe initial findings regarding the strengths and weaknesses of LLMs in the scam defense context.", "sections": [{"title": "1 INTRODUCTION", "content": "A variety of large language model-based conversational assistants (LLMs) are available via the web and are regularly relied upon by users for distilling complex information and potentially reducing the work of information retrieval via search engines. An LLM use case of growing popularity is protection from scams and fraud; this use case is endorsed by security vendors [15, 17, 25] and is the natural evolution of the user practice of seeking online advice for security-related tasks [14]. The user-friendly nature and broad availability of web LLMs makes them a particularly compelling defense tool for socially isolated users, a group that is known to be vulnerable to scams (e.g., [7, 26]).\nWhile the scam defense opportunities of LLMs are significant, the ability of the broadly accessible pretrained LLMs (e.g., ChatGPT, Google Gemini) to recognize scam markers has not been comprehensively assessed. In addition, LLMs are known to hallucinate information (e.g., [12]) and come with privacy risks due to training data leaks [3] and a potential to encourage over-sharing of personal information by users [27]. To understand how to manage these risks in the scam context, a corpus of user scam narratives with which to evaluate LLM performance, is needed.\nThe Consumer Financial Protection Bureau (CFPB) complaints database [4], is a promising source of scam narratives, however currently scam complaints are grouped with fraud complaints (through consumer selection of the \"fraud or scam\" issue or sub-issue when making a complaint). We have developed a prompting technique [16] for distinguishing CFPB complaints regarding a scam (i.e., a user has been tricked into taking a financially self-harming action) from those concerning fraud (i.e., a financially harmful action is taken without the user's consent). As part of the iterative development of this scam-identifying prompt, we have manually labeled a set of 300 CFPB complaints that consumers reported as \"fraud or scam\" complaints. We provide an analysis of this manually labeled set that suggests directions for improving the safety of user interaction with LLMs in the scam context.\nIn summary, we make two contributions in this short paper: 1. an ensemble prompting technique for distinguishing scam from fraud complaints in the CFPB database and 2. An analysis of LLM"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Consumer scams are a growing problem in financial services (up 14% in 2023 [6]). Many victims of scams (and other financial harms [1, 2, 13]) file complaints with the CFPB, a US financial sector consumer protection agency that takes actions against financial services providers on behalf of consumers. With consumer consent, complaint narratives are made publicly accessible in redacted form [22]. As of December 6, 2024 there are over 2.3 million narratives for which complainants have opted into inclusion in the public CFPB Complaints Database, almost 18,000 of which the complainant selected \"fraud or scam\" (the most granular scam-related annotation) as an issue or sub-issue.\nWhile scam and non-scam fraud narratives share harms, the definitional distinction is that in the case of scams, the user (or, complainant) takes self-harming actions [11]. In particular, while both types of complaints may involve transactions considered fraudulent, in the case of a scam a bad actor gains the user's trust allowing them to trick the user into authorizing the transaction themselves, whereas in non-scam fraud a bad actor authorizes the transaction (e.g., using stolen credentials).\nHuman ability to recognize scams has long been studied (e.g., [5, 23]), however, study of LLM ability to recognize scams is just emerging and to the best of our knowledge, all of the studies in this area (e.g., [18, 20]) experiment with synthetic scam narratives. We are the first to provide a corpus of organic consumer scam narratives. The corpus is built by iteratively developing a high-precision ensemble LLM prompt for detecting scams, and so builds on previous research showing that with well-designed practices, LLMs can serve as good annotators (e.g., [8, 21])."}, {"title": "2.1 Data and Ethics", "content": "We plan to make our scam corpus publicly available, including the redacted narratives as they appear in the CFPB complaints database and the Gemini and GPT-4 responses to the prompts in Section 3. The CFPB redaction process [22] is thorough, and to the best of our knowledge, none of the narratives include personally identifiable information. In addition, we have used the paid API service to access GPT-4, which ensures our queries are not used for future model development, as per OpenAI privacy policies."}, {"title": "3 PROMPT DEVELOPMENT", "content": "We developed a prompt with high precision and recall through an iterative process of evaluating prompts against training data and improving the prompt according to observed error patterns. While we experimented with 3 LLMs, GPT-4\u00b9, Gemini\u00b2 and Llama\u00b3, our final prompt uses only Gemini and GPT-4 so we focus the discussion in this short paper on those 2 LLMs.\nTraining data. To build a collection of complaints for training purposes, the authors independently labeled 150 CFPB complaints with the \"fraud or scam\u201d issue or subissue, relying on the definition of Section 2 as the codebook (following standard qualitative coding practices, e.g., [9]). Each complaint was labeled either \"scam\u201d or \"non-scam\", and each author added notes explaining their reasoning to facilitate later conversation. The authors met to discuss their labels and were able to resolve all disagreements. The authors also agreed to take a \"customer is always right\" approach, meaning, that in the absence of evidence to the contrary, the complainaint's statements would be assumed correct and the complaint would be labeled accordingly. As an extreme example, if a complainant asserts they have been scammed and no further detail on what happened is provided, the complaint would be labeled \"scam\". Since we were able to establish high inter-related reliability through this process, each author then independently labeled 50 complaints, asking for second opinions as needed. As a result, our training data consisted of 300 CFPB complaints, 64% of which are labeled scams. We denote this set of 300 labeled narratives by L. The narratives in L range in character length from 38 to 10,975 with a mean and median of 1,416.85 and 1,157.5, respectively. 93% of the narratives in L are somewhat redacted, and on average 4.6% of the narrative characters are redacted.\nPrompt Design and Iteration. Each prompt we used relied heavily on the definition of a scam (Section 2) and required the LLMs to explain their labels since a large body of research shows performance can improve when LLM responses include descriptions of their \"reasoning\" (e.g. [24]). In addition, several prompts included example complaints and labels since examples have been associated with improved performance in other contexts (e.g., [21]). Finally, we experimented with breaking prompts into multiple, simpler prompts (e.g., asking the LLM to just evaluate a single aspect of the definition and combining the LLM responses to decide on scam labels), and found this improved the performance of GPT-4.\nWe evaluated each prompt with Gemini and GPT-4 and calculated precision and recall. We also manually reviewed the errors and used the identified patterns to generate new prompts for evaluation.\nFinal Prompt and Performance. We achieved the best performance with an ensemble model using different prompts for Gemini and GPT-4 and requiring that the LLMs predict scam for each prompt. The prompts are:\n(1) Prompt A [used with Gemini]4: \u201cA scam is an attempt to defraud a person or group after first gaining their trust. Confidence tricks exploit victims using a combination of the victim's"}, {"title": "4 ERROR PATTERNS", "content": "In this section we describe some of the patterns in the LLM errors that surfaced in responses to the labeled narratives, L.\nReliance on Secondary Information. When complaint narratives are unclear and common scam markers are absent, we observed that both GPT-4 and Gemini can rely on secondary characteristics such as customer service or claim denials, to make decisions, rather than declining to decide. For example, CFPB Complaint #4473515 devotes less than 14% of its 950 characters to describe the financial harm that LLMs are determining to be a scam or non-scam fraud, and 6% of the financial harm content is redacted: \"Someone took money from my Citibank account of My corporation and used it to pay XXXX XXXX i have no idea who these individuals are...\" The remaining 86% of the complaint describes the complainant's experience trying to resolve the harm with Citibank. While the narrative describes fraud (an unauthorized withdrawal) both GPT-4 and Gemini predict scam in response to prompt C and explain their decisions with secondary information. GPT says: \u201cThe poor customer service experience combined with the unauthorized monetary transaction strongly suggests a potential scam.\" and Gemini responds: \"...the lack of communication from citibank, coupled with the unclear nature of the transaction and the involvement of a potentially fictitious individual or entity, points towards a potential scam.\"\nImpact of Reputation. Scams are committed by entities of unknown or poor reputation and begin by building trust, thus persuading targets to act in the scammer's interest (e.g., [19]). Hence, a poor or unknown reputation is an important indicator of a potential scammer. We did not find evidence that LLMs consider reputation when identifying scams. Rather, there is some evidence of over-reliance on official business names as indicators of positive reputation. For example, the credit repair company, Lexington Law, has been fined by the CFPB for illegal fees and deceptive advertising and there is ample web evidence of its poor reputation (e.g., [10]."}, {"title": "5 DISCUSSION AND OPEN PROBLEMS", "content": "We have developed an ensemble approach that uses Gemini and GPT-4 to distinguish scam from non-scam fraud CFPB complaints. We emphasize that the ensemble model is designed for the specific task of distinguishing scam from fraud and is not a general purpose scam complaint identifier. Indeed, we evaluated the prompt on a random subset of CFPB complaints that are not labeled with the"}]}