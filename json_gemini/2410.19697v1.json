{"title": "IPPON: Common Sense Guided Informative Path Planning for Object Goal Navigation", "authors": ["Kaixian Qu", "Jie Tan", "Tingnan Zhang", "Fei Xia", "Cesar Cadena", "Marco Hutter"], "abstract": "Navigating efficiently to an object in an unexplored environment is a critical skill for general-purpose intelligent robots. Recent approaches to this object goal navigation problem have embraced a modular strategy, integrating classical exploration algorithms-notably frontier exploration-with a learned semantic mapping/exploration module. This paper introduces a novel informative path planning and 3D object probability mapping approach. The mapping module computes the probability of the object of interest through semantic segmentation and a Bayes filter. Additionally, it stores probabilities for common objects, which semantically guides the exploration based on common sense priors from a large language model. The planner terminates when the current viewpoint captures enough voxels identified with high confidence as the object of interest. Although our planner follows a zero-shot approach, it achieves state-of-the-art performance as measured by the Success weighted by Path Length (SPL) and Soft SPL in the Habitat ObjectNav Challenge 2023, outperforming other works by more than 20%. Furthermore, we validate its effectiveness on real robots. Project webpage: https://ippon-paper.github.io/", "sections": [{"title": "I. INTRODUCTION", "content": "Imagine asking an intelligent robot, \u201cCan you fetch me an apple?\u201d The robot's response involves initially navigating to find the apple, picking it up, returning to your location, and finally delivering it. In a more practical scenario, the robot lacks prior knowledge of the apple's location and must actively explore a cluttered, unknown environment to locate it. This problem is usually referred to as Object Goal Navigation [1], where, given a description of the object of interest (OOI), the agent must decide where to explore and when to terminate within an unknown map.\nThe success of a navigation task hinges on three pivotal components: semantic mapping that identifies the placement"}, {"title": "II. RELATED WORK", "content": "Before the formal introduction of object goal navigation [1], roboticists had long been working on finding objects in known or unknown environments. Early work on unknown environments [9] proposed greedily selecting viewpoints to maximize the likelihood of observing OOI. Another seminal work [10] developed a model to balance the exploration of unknown areas and the exploitation of known maps. At that time, object detection was limited, so researchers mostly relied on SIFT-based features [11], with minimal to no semantic guidance available.\nOver the past decade, advancements in computer vision have enabled the finding of diverse objects using RGB-D cameras, and the focus has shifted towards enhancing efficiency through semantic guidance. Researchers have developed models that utilize 2D semantic maps to predict exploration targets with goal-oriented semantic policy [12], predict the presence of objects in unseen areas [13], [14], learn graphs of object relationships [15], or form a correlational object search POMDP, with the correlations between objects provided by domain experts [16]. Nevertheless, such semantic guidance only operates within a closed set of objects and cannot interpret complex contextual instructions.\nThe recent approaches have acknowledged the importance of exploration planners, notably using frontier-based exploration (FBE) [2]. FBE suggests that robots should always navigate to frontiers, defined as the boundary between free and unexplored areas. The current state of the art semantically chooses the best frontier to explore using a learned policy [17], [18] or the commonsense reasoning from LLMs [19], [20] or vision-language models [21]. Nonetheless, FBE focuses solely on the gain at the endpoint of each path, overlooking the possibility of gains along the way. In addition, it does not account for the camera's view frustum, which is essential for accurate spatial reasoning.\nMore recent work in exploration has sought to address these limitations, such as the NBV planner [6], which combines sampling-based planning [4] with the next best view [3]. NBV assigns a gain to each node based on potential volumetric gain while also considers the total gains accumulated along the path, achieving notable improvements over FBE. Schmid et al. [7] extended this approach by framing it as an online informative path planning (IPP) method and demonstrating its effectiveness in 3D reconstruction. However, this online IPP has yet to be applied to tasks that require termination and rely heavily on semantic information, such as object goal navigation."}, {"title": "III. PROBLEM FORMULATION", "content": "We adhere to the Habitat ObjectNav Challenge 2023 problem setup [8], where the agent takes actions in a continuous velocity space, including moving forward, turning left or right, and looking up or down (i.e., tilting the camera). After each action, the agent receives an observation that includes an RGB-D image and its current state, and then decides on the next action. An episode is considered successful if the agent triggers a termination signal when an OOI is within 1 meter and can be seen by turning around and tilting the camera. Failing to trigger the termination signal correctly or within the step or time limit is deemed a failure."}, {"title": "IV. METHOD", "content": "We illustrate the IPPON pipeline in Fig. 2, where IPPON begins by performing semantic segmentation with the SAN model [22]. SAN is an open-vocabulary semantic segmentation model that takes a list of labels and an RGB image as inputs, and outputs the probability of each pixel belonging to these labels (represented as heatmaps). Using an open-vocabulary model requires an adequate number of labels, which we refer to as common objects, to ensure that most elements in the image are correctly identified. For the Habitat ObjectNav Challenge, we naturally use the provided 40 semantic labels; in real-world experiments, we manually select significant objects. These pixel probabilities are then merged into 3D voxels using the Bayes filter to estimate the posterior probabilities of each voxel associated with different objects. We query an LLM to evaluate the proximity between common objects and the OOI and create a proximity score map that semantically guides the informative path planning to decide exploration targets. Further details on these modules are provided below."}, {"title": "A. Object Probability Mapping", "content": "The mapping algorithm dynamically updates the probability that voxel v belongs to a common object category $C_j$ or the object of interest O, considering all image observations $I_{1:k}$ up to time k. We illustrate this computation with the Bayes filter for O, but the same principle applies to $C_j$.\n$p(\\upsilon \\in O | I_{1:k}) = \\frac{p(I_k | \\upsilon \\in O) p(\\upsilon \\in O | I_{1:k-1})}{p(I_k | I_{1:k-1})}$\nHowever, the observation model $p(I_k | \\upsilon \\in O)$ is unavailable, as the semantic segmentation network provides $p(\\upsilon \\in O | I_k)$. The relationship between these probabilities can be obtained using another application of Bayes' rule, which leads to the following expression:\n$p(v \\in O | I_{1:k}) = \\frac{1}{Z} \\frac{p(v \\in O | I_k) p(v \\in O | I_{1:k-1})}{p(\\upsilon \\in O)}$\nwhere $Z = p(I_k | I_{1:k-1}) / P(I_k)$ is the normalization constant. Following similar approaches as in [24], we assume the prior probabilities are equal across all categories, i.e., $p(\\upsilon \\in O) = p(v \\in C_0) = p(v \\in C_1) = ... = p(v \\in C_N)$. In subsequent sections, with a slight abuse of notation, we use $p(v \\in O)$ to denote the posterior probability (not the prior), omitting conditioning on observations for simplicity.\nBayes filters play a key role in building accurate semantic maps by correcting misclassifications over multiple iterations of Bayesian updates. This helps mitigate issues like flickering or inconsistency in semantic segmentation. Furthermore, Bayes filters iteratively refine object probabilities, generally increasing the likelihood of detected objects towards 1.0, while reducing that of undetected objects towards 0.0.\nOne key difference between mapping OOI and common objects lies in the distance range that we fuse the pixel probability into 3D. The maximum distance is generally smaller for OOI because they typically require closer inspection for accurate semantic segmentation, as incorrect predictions may cause the robot to stop at the wrong object or mistakenly assume its absence and thus avoid approaching it further. However, in the Habitat ObjectNav Challenge, where the depth camera features a narrow field of view and limited depth range, we directly set the maximum mapping distance for all objects to the maximum depth (5 m)."}, {"title": "B. Semantic Guidance", "content": "The goal of semantic guidance in our framework is to compute an imagined probability $P_{img}$ for each node, which represents the likelihood that an unknown voxel belongs to the OOI within its view frustum. Similar to [19], [20], we query an LLM (GPT-4 [25]) to evaluate the proximity of each common object to the OOI, categorizing it into four levels: certain, near, average, far. The rough outline for the prompt is listed in Listing 1, and the full template and the example responses can be found on the project webpage. Note that LLMs have the capability to understand additional contextual information, as illustrated in Fig. 1.\nWe then transform the proximity level into a probability $p(I_O | C_j)$ based on predefined probability levels where $P_{certain} > P_{near} > P_{average} > P_{far}$. These probabilities then form a weighted average at the voxel level, indicating the likelihood of locating the OOI O near that voxel:\n$1(\\upsilon) = \\sum_{j=1}^{N} 1(I_O | C_j) p(\\upsilon \\in C_j).$\nFinally, each viewpoint counts the nearby visible voxels linked to each proximity level and selects the highest-ranking"}, {"title": "C. Informative Path Planning", "content": "The informative path planning [7] grows a tree with sampling-based planning. Each node, representing a viewpoint in 3D space, is associated with a gain, while each edge, representing a traversable path between nodes, is assigned a cost of traveling time. The planning algorithm can select the next optimal target based on various criteria, such as the gain-to-cost ratio as proposed in [7]. Upon reaching the next node along the optimal path, the planner sets this node as the new root and rewires the tree. Then, it updates the gain and cost and re-evaluates the next optimal target.\n1) Traversability Estimation: A pose or a path is considered untraversable if the robot's body collides with the environment or the base fails to maintain contact with the ground. The robot is approximated by a list of collision spheres, and the distance is checked using the Euclidean signed distance field sdf(\u00b7) from Voxblox [23]. Since the robot does not have prior knowledge about the map, it initially looks down to estimate its height (ranging between 1.30 and 1.42 meters in the Habitat ObjectNav Challenge) and then looks around to understand its vicinity. The robot maximizes its camera pitch movement (looking up/down) when traveling to perceive as much as possible. However, the robot may still collide with the environment due to mesh artifacts; when this happens, the planner marks the triangular region ahead as untraversable and finds an alternative path.\n2) Node Evaluation: For each pose n, we conduct ray tracing on the 3D map to detect all visible voxels, denoted by V(n). The gain for each node is determined by the total probability of the voxels in V(n) are the OOI, considering its occupancy state,\n$G(n) = \\sum_{\\upsilon \\in V(n)} (1_{\\upsilon \\in V_O} \u00b7 p(\\upsilon \\in O) + 1_{\\upsilon \\notin V_O} \u00b7 P_{img}(n)) \u00b7 P_{occ}(\\upsilon)$,\nwhere 1 is the indicator function and $V_O$ contains voxels mapped with the OOI probability $p(\\upsilon \\in O)$. $P_{img}$ is the imagined probability from the semantic guidance (see Sec. IV-B). The probability of a voxel being occupied, $P_{occ}$, is mapped to 0 if it is known to be free, 1 if occupied, and to a user-defined occupied probability if its state is unknown.\nIn addition to exploration, the robot must terminate in front of the OOI. Hence, we implement an additional termination evaluation that counts the number of nearby visible voxels whose OOI probability is higher than a non-class-specific threshold $p_r$ (referred to as OOI voxels),\n$T(n) = \\sum_{\\upsilon \\in N_T(n)} 1_{\\upsilon \\in V_O} 1_{p(\\upsilon \\in O) > p_r} \u00b7 P_{occ}(\\upsilon).$\nThe terminating nodes T, where the robot can conclude navigation, is defined as the set of all nodes whose T value is higher than $T_{min}$. $T_{min}$ varies depending on the physical size of the OOI (smaller for plants and larger for beds). $N_T(n)$ represents all the voxels visible within a 1-meter radius from n, which the robot can observe by turning around or looking up/down. This aligns with the success criteria in the Habitat ObjectNav Challenge.\nFollowing the methodologies in [7], [26], we sample position coordinates (x, y) of nodes and optimize their yaw and pitch for better gain and termination. The position coordinates are first sampled densely within a local region before expanding to the remaining space. To enable object framing, we set the orientation for terminating nodes based on the highest number of OOI voxels in $N_T(n)$. For nodes that do not explore much, we set their orientation to the direction of travel to mitigate the turning costs.\n3) Node Connection: Once a new node has been evaluated, the planner adds it to the existing tree using the RRT* algorithm [5]. Specifically, the planner first connects it to a neighboring node and then rewires neighboring nodes to create a tree with asymptotically minimum cost to every node. The connection verification typically only validates a straight-line path; if this check fails, the path is deemed invalid. However, in the informative path planning, nodes or viewpoints are deliberately spaced apart because it assumes viewpoints have independent gains from each other [7]. Such spacing can render direct connections infeasible, especially in constrained environments like narrow \"L\"-shaped corridors. Therefore, we introduce another local sampling-based planner (SBP) from OMPL library [27] to explore local"}, {"title": "4) Node Selection", "content": "When selecting the target node, the planner aims to maximize efficiency, particularly the gain-to-cost ratio, and gives priority to terminating nodes T,\n$n^* = \\begin{cases}\\text{arg max}_{n \\in T} \\frac{T(n)}{C(P_n)} & \\text{if } T \\neq 0 \\\\ \\text{arg max}_n \\frac{G(P_n)}{C(P_n)} & \\text{otherwise}\\end{cases}$\nHere, $P_n$ denotes the path from the root to node n, with G($P_n$) and C($P_n$) representing the total gain and cost along path $P_n$, respectively."}, {"title": "V. RESULTS", "content": "We evaluated our method on the Habitat ObjectNav 2023 Challenge, standard phase (1000 episodes) [8]. Compared to the previous year's challenge, the action space of this year has transitioned from discrete to continuous, aligning well with the assumptions of sampling-based planning. The considered categories in this challenge are fixed: \u201cchair\", \"bed\", \"plant\", \"toilet\", \"tv\", and \"sofa\".\nUnlike other teams that are all trained on this dataset, our method follows a zero-shot approach. Still, it achieves the best performance in the Success weighted by Path Length (SPL) and Soft SPL, outperforming the previous state of the art by more than 20%. In addition, our planner supports open-vocabulary OOI, whereas SkillTron [30], SkillFusion [30], and Auxiliary RL [28] all use a dedicated fixed-class semantic segmentation network."}, {"title": "B. Hardware Experiments", "content": "For the real-world experiments, we use a legged robot ANYmal [31] with an arm mounted on top. A ZED X camera, featuring a 74\u00b0 horizontal FoV, is mounted on the elbow of the arm. The robot's pose is estimated by a SLAM system [32]. All computations are performed onboard the robot, except for querying the LLM for proximity levels. The Jetson Orin handles heatmap computation and ZED depth rendering, while mapping and planning modules run on an"}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "In this paper, we introduce IPPON, a method that extends the application of informative path planning from exploration and 3D reconstruction to open-vocabulary object goal navigation. By integrating common-sense reasoning from LLMs, the robot achieves a more effective understanding of exploration areas and knows when to abandon the current search zone. Additionally, through the use of 3D object probability mapping, the planner consistently positions the target object within the field of view upon termination. Our evaluation in the Habitat ObjectNav 2023 Challenge demonstrates that our planner significantly outperforms other state-of-the-art methods by a large margin, even as a zero-shot approach. We have extensively validated its performance with real robots across various objects in both indoor and outdoor environments.\nHowever, IPPON currently faces several limitations. First, even though the objects of interest can be open-vocabulary, the list of common objects still needs to be predefined. This may lead to misclassification and incorrect termination signals if an object does not match any provided labels. For common objects, our mapping is performed solely at the object level, without differentiating between a sink in the kitchen and one in the bathroom. Therefore, when the OOI is a toilet, the robot might still attempt to explore the kitchen due to the typical proximity of toilets to sinks. Additionally, the system struggles to detect very small objects, such as a student card.\nTo address these limitations, we plan to integrate more advanced visual language models, such as GPT-4V, to dynamically expand the list of common objects during exploration. Applying a Bayes filter to dynamically changing categories will also be a key area of investigation. By introducing room-level detection using scene graphs [33], [34], [35] or text-based maps [36], we expect to improve performance as we can integrate fine-grained relationships between rooms and objects. The robotic arm will be used more actively to examine objects at higher elevations (such as a book on a shelf) or to uncover occluded items (like shoes under a table), thus expanding our navigation capabilities into more challenging 3D scenarios."}]}