{"title": "CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs", "authors": ["Davide Buffelli", "Farzin Soleymani", "Bastian Rieck"], "abstract": "Graph neural networks have become the default choice by practitioners for graph learning tasks such as graph classification and node classification. Nevertheless, popular graph neural network models still struggle to capture higher-order information, i.e., information that goes beyond pairwise interactions. Recent work has shown that persistent homology, a tool from topological data analysis, can enrich graph neural networks with topological information that they otherwise could not capture. Calculating such features is efficient for dimension 0 (connected components) and dimension 1 (cycles). However, when it comes to higher-order structures, it does not scale well, with a complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order of the structures. In this work, we introduce a novel method that extracts information about higher-order structures in the graph while still using the efficient low-dimensional persistent homology algorithm. On standard benchmark datasets, we show that our method can lead to up to 31% improvements in test accuracy.", "sections": [{"title": "Introduction", "content": "The research area of Graph Neural Networks (GNNs) has received a huge amount of interest in the past few years [1, 2]. As a result, GNNs are now the first choice for many graph-related tasks, like graph classification, node classification, and link prediction. Nevertheless, GNNs present some shortcomings when it comes to modeling higher-order structures (i.e., structures in the graph that go beyond pairwise interactions, like cliques and cycles [3, 4]). In fact, GNNs follow a message-passing framework [5] in which each node communicates with its neighbors, but there is no mechanism to go beyond pairwise interactions, which limits the expressiveness of these models [6]. However, information regarding certain topological structures like cycles and cliques, can be of great importance to graph-related tasks, significantly improving performance [7].\nTopological data analysis (TDA) [8] is an emerging area of study that uses techniques from topology, and in particular the tool of persistent homology, to study the shape of the data at multiple scales. TDA is particularly well-suited for capturing higher-order information in graphs, as it can provide information related to structures of different orders, including cliques and cycles, which are known to be particularly informative for several graph-related tasks [7]. Recent works have incorporated topological information related to persistent homology into graph learning tasks with great success [9]. These methods are however limited to persistent homology up to dimension 1, due to the high computational cost that is required for higher dimensions. While this already empowers the model with important structural information that GNNs cannot capture, it is still missing a vast amount of higher-order information.\nIn this paper we introduce CliquePH, a new topological layer that can be added to any GNN, and that provides information about persistent homology on higher-order structures. Performing persistent homology up to dimension 1 is extremely efficient and scalable, while higher-dimensional persistent homology is prohibitively expensive [9]. We then devise a strategy that captures higher-order information while still using the efficient low-dimensional persistent homology. In more detail, we first \"lift\" the graph into multiple clique graphs describing the connections of higher-order structures, and then apply persistent homology up to dimension 1 to each \u201chigher-order\u201d graph. This strategy allows us to use the very efficient (with complexity linear in the number of nodes) persistent homology of dimension 1, while still extracting information from higher-order structures.\nThe contributions of this paper can be summarized as follows:\n\u2022 We introduce a topological layer named CliquePH that can be incorporated into any GNN and that provides information related to persistent homology on higher-order graph structures.\n\u2022 We highlight theoretical results that provide a strong motivation for the use of higher-order persistent homology in graph learning tasks.\n\u2022 We evaluate our method applied to three popular GNNs on standard graph classification bench-marks, and show performance improvements of up to 31%."}, {"title": "Preliminaries", "content": "We denote a graph with $G = (V, E)$, where $V$ is the set of nodes, with $|V|= n$, and $E$ is the set of edges. We consider attributed graphs, in which every node has a set of $d$ features. The features for each node in a graph are contained in a matrix $X \\in R^{n \\times d}$. We use $\\mathcal{N}_v$ to indicate the neighbors of node $v$ (i.e., the nodes connected to $v$ by an edge).\nClique Graphs. A $k$-vertex clique in a graph $G = (V, E)$ is a subset of vertices such that every two distinct vertices in the clique have an edge between them, i.e., $k = \\{v_1, v_2, ..., v_k\\}$, with $v_i \\in V$, is a $k$-vertex clique if $(v_i, v_j) \\in E \\forall i \\neq j; i, j = 1, . . ., k$. Let $k_1, k_2, . . . k_z$ be the $r$-vertex cliques in the graph $G$, then the $r$-vertex clique graph of $G$ is a graph $K^{(r)}(G) = (V_{K^{(r)}}, E_{K^{(r)}})$, with $V_{K^{(r)}} = \\{k_1, k_2, ... k_z\\}$, and $(k_i, k_j) \\in E_{K^{(r)}}$ if and only if $i \\neq j$ and $k_i \\cap k_j \\neq \\emptyset$. In other words, the $r$-vertex clique graph $K^{(r)}(G)$ summarizes the structure of the $r$-vertex cliques in the graph $G$.\nPersistent Homology. Persistent homology [10, 11], a technique for capturing topological features of data, constitutes the foundation of our proposed method. In contrast to other graph learning techniques, persistent homology permits us to quantify topological features at different scales. The crucial element enabling multi-scale calculations is a filtration, i.e., a consistent ordering of the vertices and edges of a graph, defined using a function $f: G\\rightarrow R$. Filtrations can be either obtained from static descriptor functions that measure certain aspects of a graph, such as its number of connected components or its curvature [12], but recent work also demonstrates that it is possible to learn filtrations in an end-to-end fashion [9, 13]. Regardless of the origin of a filtration, the crucial insight is that, since a graph is a discrete object, its topology can only change at a finite number of critical thresholds $t_1,..., t_k$. Thus, any function $f: G \\rightarrow R$ that gives rise to a filtration enables us to turn a graph into a sequence of nested subgraphs $G_1 \\subseteq G_2 \\subseteq ... \\subseteq G_k = G$, where each $G_i$ is defined based on the critical points of $f$ via $G_i := \\{x \\in V \\cup E | f(x) < t_i\\}$. Intuitively, each subgraph consists of all elements of the graph $G$ whose function value is less than or equal to the critical threshold. Alongside this sequence of subgraphs, we now calculate the homology groups. These groups capture the topological features-connected components, cycles, voids, and higher-order features of each subgraph. Persistent homology represents all topological features arising from the filtration in a set of persistence diagrams. Each persistence diagram consists of a multiset of intervals of the form $(c_i, d_i)$ with $c_i, d_i \\in R \\cup \\{ \\infty \\}$, with $c_i$ denoting the \u201ccreation time\" (in terms of the critical thresholds ) and $d_i$ denoting the \"destruction time.\" Features with"}, {"title": "Our Method", "content": "Our proposed method aims at introducing information from persistent homology on higher-order structures into GNNs. While our method is theoretically different from performing persistent homology up to higher dimensions, we show it brings substantial performance improvements. Our method can be decomposed into four components (as shown in Figure 1), which we present below.\nGraph Lifting\nStarting from the original graph $G$, we perform a \u201clifting\u201c operation to extract its clique graphs:\n$lift(G) = \\{G, K^{(3)} (G), K^{(4)} (G), ..., K^{(r)} (G)\\}$\nthe maximum size $r$ for the clique graphs can be defined arbitrarily according to the data at hand (e.g., by taking the value that leads to a clique graph with a number of nodes that is least 5% the number of nodes in the original graph). We start from the 3-vertex clique graph, as the information about nodes and edges is already present in the original graphs, while the higher dimensions are the ones in which GNNs struggle. This step can be done just once, as pre-processing on all graphs in the dataset.\nIn practice, we use $r = 5$ in our experiments as available datasets tend to have a very small number of cliques larger than that. When dealing with real-world graphs, several works have proposed methods for enumerating all cliques in a graph that in practice have a fast runtime [15\u201318]. In all datasets considered in our experiments, we have been able to perform this pre-processing step for each dataset, up to $r = 5$, within 1 hour. For instance, complete preprocessing time on MNIST dataset for up to 3-cliques, up to 4-cliques, and up to 5-cliques takes 36.4 min, 58.1 min, and 60.7 min consecutively.\nMessage-Passing\nIn this step, we obtain node embeddings for the original graph $G$. In more detail, we perform $L_m$ steps of message passing on $G$. Any GNN could be used for this step, and the output is a matrix of node embeddings $H^{(L_m)}$.\nLearnable Persistent Homology\nOnce we have obtained embeddings for the nodes in $G$, we incorporate information from persistent homology for all graphs in lift(G). This step is composed of three sub-components presented below.\n1 - Filtration values for nodes and edges. To perform persistent homology, we first need to compute filtration values for dimension 0 (nodes), which we indicate with $F^{(0)}$, and dimension 1 (edges), which we indicate with $F^{(1)}$, in all graphs in the lifted set. We first describe how we obtain filtration values for the original graph $G$, and then for the clique graphs $K^{(\\cdot)}(G)."}, {"title": "Original graph $G$.", "content": "For dimension 0, we use a two-layer MLP $f_0$ to map the embedding of each node into $d_f$ filtration values: $F_0^{(G)} (v) = f_0(H^{(L_m)})$. For dimension 1, we first obtain a representation $r_e$, for each edge $e_i = (u, v)$ by concatenating the representations of the nodes it connects: $r_{e_i} =\ncat(H^{(L_m)} (u), H^{(L_m)} (v))$. We then obtain the filtration value for the edge using a 2-layer MLP function $f_1$ as follows:\n$F_1^{(G)} (e_i) = max(\\{F_0^{(G)}(u), F_0^{(G)}(v)\\}) + f_1(r_{e_i})$"}, {"title": "Clique graphs $K^{(\\cdot)}(G)$.", "content": "For the lifted graphs, we obtain filtration values in a similar way. Let $k_i$ be a node in a clique graph, related to a clique between the nodes $\\{u_1, u_2, . . ., u_j \\}$, we first obtain the embedding for $k_i$ as $e_{k_i} =\ncat(H^{(L_m)} (u_1), ..., H^{(L_m)} (u_j))$. We then compute the filtration value as\n$F_0^{K^{(k_i)}} = max(\\{F_0^{(G)} (u_1), ..., F_0^{(G)} (u_j)\\}) + f_k (e_{k_i})$\nwhere $f_k$ is a two-layer MLP (a separate one for each clique graph). We then obtain the filtration values for dimension 1 following an analogous procedure to the one used for dimension 1 on the original graph $G$, using a separate learnable function for each clique graph."}, {"title": "2- Persistent homology.", "content": "We now have filtration values for all the nodes and edges in all the graphs in the lifted set. We can then perform the efficient persistent homology up to dimen-sion 1 to all the graphs, obtaining two persistence diagrams for each graph in the lifted set:\n$\\{ (D_0^{(G)}, D_1^{(G)}), (D_0^{K^{(3)}}, D_1^{K^{(3)}}), ..., (D_0^{K^{(r)}}, D_1^{K^{(r)}}) \\}$. The diagrams for dimension 0 have a number of entries equal to the number of nodes in the respective graph, while the diagrams for dimension 1 have a number of entries equal to the number of independent cycles in the respective graph. In other words, these diagrams are summarizing the evolution (according to the learned filtration values) of the connected components and cycles in each graph."}, {"title": "3 - Embed persistence diagrams.", "content": "The diagrams for dimension 0 of the original graph $G$ are passed to a permutation equivariant (set-to-set) DeepSet network (a separate network for each graph in the lifted set is used), which returns a vector for each node. These vectors are stored in a matrix $E^{(0)}$. The diagrams for dimension 0 for all clique graphs are passed to a permutation invariant DeepSet network (a separate network for each clique graph is used), which returns a single vector $E_0^{K^{(k_i)}}$ for each clique graph $K^{(\\cdot)}(G)$. The diagrams of dimension 1 for all graphs are passed through a permutation invariant (set-to-vector) DeepSet network (a separate network for each graph in the lifted set is used) to obtain a unique embedding, which returns a single vector for each graph: $E_1^{(G)}, E_1^{K^{(3)}},..., E_1^{K^{(r)}}$."}, {"title": "Information Combination", "content": "We summarize the information from the persistent homology of each clique graph into a unique vector for each dimension. In more detail, we concatenate the vectors $E_0^{K^{(\\cdot)}}, E_1^{K^{(\\cdot)}}$ and $E_1^{K^{(1)}}$, and pass them through a two-layer MLP to obtain a vector $E^{K^{(3)}}$ for each dimension $j \\geq 3$. Finally, we combine all the information from the persistent homology computations with the embeddings obtained in the message passing step. This is done by adding the embeddings together:\n$H^{(L_m)} + E^{(0)} + SCATTER(E_1^{K^{(G)}} + SCATTER(E^{K^{(3)}}) + SCATTER(E^{K^{(3)}}) + ... + SCATTER(E^{K^{(r)}})$\nwhere the function $SCATTER$ indicates that the embeddings of each clique are added to all the node embeddings of the nodes that form that clique. More formally, given a 3-vertex clique $k = (u, v, z)$ with embedding $e_k$, the operation $H + SCATTER(e_k)$ is doing the following\n$H_u = H_u + e_k; H_v = H_v + e_k; H_z = H_z + e_k$\nand leaving the rows of $H$ related to other nodes unaltered. Additional round(s) of message passing on $G$ can then be performed before passing the embeddings to the final classifier. In the case of a graph-level task, a standard graph pooling method (e.g., averaging) is used.\nFinally, we mention that CliquePH does not affect the permutation equivariance of GNNs, and is fully differentiable (hence allowing end-to-end training). More details can be found in Appendix A.\nWe futher provide a comparison between CliquePH and TOGL in Appendix B."}, {"title": "Limitations", "content": "There are three main limitations to our method. Firstly, for very large graphs, it may become difficult computationally to use values of r larger than 3. While our experiments show that $r = 3$ is already enough to provide benefits, this can prevent from exploiting the full potential of CliquePH. Secondly, our method avoids the computational complexity of the exact higher order persistent homology procedure, but it is not equivalent to it, and hence does not have the same theoretical guarantees in terms of expressiveness. Finally, in tasks in which higher-order structures are not informative, CliquePH may not be useful, or may even promote overfitting."}, {"title": "Theoretical Motivation", "content": "The expressivity of GNNs is measured by considering their ability of distinguishing non-isomorphic graphs. This is done by relating GNNs to the Weisfeiler-Lehman algorithm (or WL algorithm) [19]. We refer to Morris et al. [20] for a complete introduction to WL, its variants, and its use in machine learning. The WL algorithm cannot completely solve the graph isomorphism problem (in fact, there is no known polynomial time solution), but it can still distinguish a large number of graphs [21]. Modifications to the WL algorithm have been made to increase its distinguishing power. In particular, some \u201chigher-order\u201d variants have been proposed. These variants form a hierarchy of algorithms: 1-WL, 2-WL, k-WL, each more expressive than the previous, i.e., i-WL can distinguish more graphs than (i \u2013 1)-WL. It has been shown that standard message-passing GNNs are strictly less powerful than the 1-WL algorithm [6]. Two recent results, which we provide below, present a strong motivation for using persistent homology to enhance the expressivity of GNNs.\nTheorem 4.1 (Persistent Homology is at least as expressive as the 1-WL - Thm. 2 in Horn et al. [9]). Persistent homology is at least as expressive as the 1-WL algorithm, i.e. if the 1-WL label sequences for two graphs $G$ and $G'$ diverge, there exists an injective filtration $f$ such that the corresponding 0-dimensional persistence diagrams $D_0^G$ and $D_0^{G'}$ are not equal.\nTheorem 4.2 (k-dimensional Persistent Homology is as expressive as k-WL - Thm. 3 in Ballester and Rieck [22]). Given k-WL colorings of two graphs $G$ and $G'$ that are different, there exists a filtration of $G$ and $G'$ such that the corresponding persistence diagrams in dimension $k - 1$ or dimension $k$ are different.\nTheorem 4.1 entails that first-order persistent homology information can already make any GNN strictly more expressive than the 1-WL. Theorem 4.2 is a weaker version of this result; it shows that higher-order persistent homology can in fact match the expressiveness of higher-order variants of the WL algorithm\u2014without requiring an enumeration of all k-tuples.\nAs CliquePH makes use of first-order persistent homology information, it is straightforward to see from Theorem 4.1 that a GNN empowered with CliquePH is strictly more powerful than the 1-WL algorithm. While we cannot use Theorem 4.2 to theoretically prove that a version of CliquePH with cliques up to order k leads to the expressiveness of k-WL (as CliquePH does not perform exact higher-order persistent homology), we provide below empirical evidence of the practical increase in expressiveness provided by CliquePH.\nEmpirical Validation of Expressiveness. We consider 6 datasets of strongly regular graphs [22] (i.e., graphs in which all nodes have the same degree), which are known to be hard to distinguish with the 1-WL test. We then randomly initialize a GCN [23], a GCN with TOGL [9], and a GCN with CliquePH (with a lifting up to 3-vertex clique graphs), and use them to produce graph embeddings. All models are set to have the same number of layers (set to 4), and the internal same dimensionalities (set to 128). We then count the number of unique representations (a network that can distinguish all graphs will provide a different representation for each graph). We show results averaged of 10 random seeds in Table 3. CliquePH increases the capability of distinguishing regular graphs, improving also over TOGL, which has an expressivity strictly higher than the 1-WL, empirically showing that persistent homology information from clique graphs provides important performance benefits."}, {"title": "Experiments", "content": "To demonstrate the benefits provided by our method in practical scenarios, we follow prior liter-ature [9] and apply our method to three GNNs among the most used by practitioners: GCN [23],"}, {"title": "Full Ablation results", "content": "We start by considering a 4-layer GNN, with 3 message passing layers and 1 CliquePH layer, and we change the position of the latter. We consider the structure-based scenario to exclude the influence of node features. Results are shown in Figure 3 (Center) for the DD, ENZYMES, and PROTEINS datasets. We notice that the effects are highly dataset dependent: for DD using CliquePH as the first layer is more effective, while for Enzymes the best performance is when the layer is 3rd, similar to PROTEINS.\nWe then consider a GNN with a number of layers that vary from 1 to 5 and with CliquePH as the last layer. Results are shown in Figure 4. Again we notice that the performance is highly dependent on the dataset, as for DD there is little change in performance for different numbers of layers, while for ENZYMES it is important to have 2 to 4 layers prior to CliquePH. On PROTEINS we notice how the addition of CliquePH significantly increases performance even for a 1-layer architecture. Finally, we notice how in all cases the addition of CliquePH always improves the final performance.\nFinally, in Figure 5 we show the performance of a GCN, GIN, and GAT model with different maximum dimension of the lifted graph (varying between 3, 4, and 5) on the DD dataset in a structure-based setting. The results confirm the importance of tuning this parameter to avoid overfitting the"}, {"title": "Node Classification Experiments", "content": "While we believe the benefits of CliquePH are more visible for graph classification tasks (in which higher-order structural information usually plays a much more important role), we performed a small experiment on the Cora-ML [50], Coauthor CS [51], and Coauthor Physics [51] datasets. We followed the same practices and architectures we used for the graph classification datasets plus an"}, {"title": "A Permutation Equivariance & Differentiability", "content": "Permutation Equivariance. Most popular GNNs are permutation equivariant (as we do not want the ordering of the nodes to affect the final output), and it is hence important to address whether our\nproposed method impacts this property. Assuming the use of a permutation equivariant GNN for the message passing operations in CliquPH, and noticing that persistent homology is permutation equivariant by design, we can confirm that CliquePH does indeed preserve this property.\nDifferentiability. The persistent homology computations are differentiable (and hence allow end-to-end training) only with an appropriate choice of function to embed the persistence diagrams. In particular, we refer to the following theorem from Horn et al. [9, Theorem 1].\nTheorem A.1. Let $f_\\theta$ be a vertex filtration function $f_\\theta : V \\rightarrow R$ with continuous parameters $\\theta$, and let I be a differentiable embedding function (used to embed persistence diagrams) of unspecified dimensions. If the vertex function values of $f_\\theta$ are distinct for a specific set of parameters $\\theta'$, i.e. $f_\\theta(v) \\neq f_\\theta(w)$ for $v \\neq w$, then the map $\\theta \\rightarrow \\Psi(ph(G, f_\\theta))$ is differentiable at $\\theta$.\nDifferentiability thus hinges on unique function values at the vertices of the graph. We observe that this condition is always satisfied in practice (as the chances of having all floating point values be exactly the same are infinitesimally small); if need be, it can be enforced by a random perturbation of function values, similar to the well known strategy of adding random features to GNNs [48, 49]. Since we use DeepSet (which is a differentiable architecture) as our function to embed the persistence diagrams, we are guaranteed to be respecting Theorem A.1, thus ensuring the differentiability of our CliquePH method."}, {"title": "B Differences Between CliquePH and TOGL", "content": "Our method CliquePH is strictly related to TOGL [9] as both methods provide a \"plug-in\u201d topological layer for GNNs. Furthermore, both methods rely on a differentiable (and learnable) implementation of persistent homology as the main tool for capturing topological information.\nThere are however several differences which we highlight below\n\u2022 CliquePH introduces a lifting operation that allows the model to obtain persistent homology information on the clique graphs representing the connectivity of higher-order structure in the graph. TOGL on the other hand is strictly limited to information up to dimension 1. This point implies that CliquePH has several functions that are not present in TOGL (e.g., the functions for computing filtration values for the lifted graphs, the functions for embedding higher-order persistence diagrams, the functions for combining information from lifted graphs).\n\u2022 In eq. 3, the learnable function $f_1$ is not present in the original TOGL layer, and provides CliquePH with more flexibility. In fact, this provides the model with an additional learnable function that can modify the filtration value of the edge without modifying the values for the nodes connected by that edge.\n\u2022 CliquePH uses an MLP for combining information (section 3.4) from persistence diagrams into node embeddings. TOGL does not use a learnable function for this step."}, {"title": "C Full Ablation results", "content": "We start by considering a 4-layer GNN, with 3 message passing layers and 1 CliquePH layer, and we change the position of the latter. We consider the structure-based scenario to exclude the influence of node features. Results are shown in Figure 3 (Center) for the DD, ENZYMES, and PROTEINS datasets. We notice that the effects are highly dataset dependent: for DD using CliquePH as the first layer is more effective, while for Enzymes the best performance is when the layer is 3rd, similar to PROTEINS.\nWe then consider a GNN with a number of layers that vary from 1 to 5 and with CliquePH as the last layer. Results are shown in Figure 4. Again we notice that the performance is highly dependent on the dataset, as for DD there is little change in performance for different numbers of layers, while for ENZYMES it is important to have 2 to 4 layers prior to CliquePH. On PROTEINS we notice how the addition of CliquePH significantly increases performance even for a 1-layer architecture. Finally, we notice how in all cases the addition of CliquePH always improves the final performance.\nFinally, in Figure 5 we show the performance of a GCN, GIN, and GAT model with different maximum dimension of the lifted graph (varying between 3, 4, and 5) on the DD dataset in a structure-based setting. The results confirm the importance of tuning this parameter to avoid overfitting the"}]}