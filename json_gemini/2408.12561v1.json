{"title": "ssProp: Energy-Efficient Training for Convolutional Neural Networks with Scheduled Sparse Back Propagation", "authors": ["Lujia Zhong", "Shuo Huang", "Yonggang Shi"], "abstract": "Recently, deep learning has made remarkable strides, especially with generative modeling, such as large language models and probabilistic diffusion models. However, training these models often involves significant computational resources, requiring billions of petaFLOPs. This high resource consumption results in substantial energy usage and a large carbon footprint, raising critical environmental concerns. Back-propagation (BP) is a major source of computational expense during training deep learning models. To advance research on energy-efficient training and allow for sparse learning on any machine and device, we propose a general, energy-efficient convolution module that can be seamlessly integrated into any deep learning architecture. Specifically, we introduce channel-wise sparsity with additional gradient selection schedulers during backward based on the assumption that BP is often dense and inefficient, which can lead to over-fitting and high computational consumption. Our experiments demonstrate that our approach reduces 40% computations while potentially improving model performance, validated on image classification and generation tasks. This reduction can lead to significant energy savings and a lower carbon footprint during the research and development phases of large-scale AI systems. Additionally, our method mitigates over-fitting in a manner distinct from Dropout, allowing it to be combined with Dropout to further enhance model performance and reduce computational resource usage. Extensive experiments validate that our method generalizes to a variety of datasets and tasks and is compatible with a wide range of deep learning architectures and modules. Code is publicly available at https://github.com/lujiazho/ssProp.", "sections": [{"title": "Introduction", "content": "In recent years, deep learning and neural networks have advanced rapidly, bringing significant benefits but also raising concerns about energy consumption and environmental impact. While deep learning research communities primarily focus on novel technologies and enhancing performance, the significant concerns of energy consumption and carbon footprint are often overlooked. According to the Stanford AI Index Report 2024 (HAI, Stanford University 2024), training leading AI models across language and vision-language fields (Achiam et al. 2023; Anil et al. 2023; Liu et al. 2024; Alayrac et al. 2022; Esser et al. 2024) can cost millions of dollars and require over 10 billion petaFLOPs (1 petaFLOPs \u2248 10^{15} FLOPs, FLOPs: floating-point operations). This immense computational need results in high power and cooling requirements, contributing to a growing carbon footprint (Kuo and Madni 2023; de Vries 2023; Lannelongue, Grealey, and Inouye 2021; Schwartz et al. 2020; Wu et al. 2022; Xu et al. 2021).\nSparsification is a major technique to address this challenge, distinct from methods that reduce training costs by speeding up model convergence, such as initialization (He et al. 2015; Glorot and Bengio 2010) and normalization (Ioffe and Szegedy 2015; Wu and He 2018). Previous research has explored various strategies of sparsification. Me-Prop (Sun et al. 2017) significantly reduces computational costs by computing only a subset of gradients, yet its application is limited to multi-layer perceptrons (MLPs). MeProp-CNN (Wei et al. 2017; Sun et al. 2018) extends this approach to convolutional neural networks (CNNs) with a different design for gradient sparsification from ours. Another approach, presented by Wang wt al. (Wang, Nelaturu, and Amarasinghe 2019), approximates backward gradient maps with a top-k selection method. MSBP (Zhang et al. 2020) and Resprop (Goli and Aamodt 2020) apply gradient sparsification to accelerate training by reusing calculated gradients for future updates. SWAT (Raihan and Aamodt 2020) sparsifies both forward and backward passes in a simulated environment with minimal accuracy degradation, yet it is restricted to A100 or equivalent GPUs that support hardware sparsification acceleration. Additionally, (Ye et al. 2020) proposes a gradient pruning method, demonstrating training speed-up on CPUs with minor accuracy loss. (Zhou et al. 2021) achieves sparse forward and backward by formulating sparsification as an optimization problem and estimating gradient with a guaranteed variance bound, requiring two forward passes for each update iteration. These works have seen some success but suffer from one or more of the following limitations: insufficient evaluation (validated on small datasets, e.g., MNIST, CIFAR-10), compromised accuracy, complex implementation, limited generality, and reliance on specific hardware. Besides, most approaches focus on speeding up training by sparsification without validating computational cost reduction.\nTo handle these problems, we introduce scheduled channel-wise sparsity (ssProp) during back-propagation, designed for seamless integration with PyTorch, i.e., simply replacing the built-in CNN module with our custom efficient CNN module, and compatibility with any machine or device. This flexibility allows our method to scale across various datasets and model architectures, as demonstrated in the experiment section where we apply it to a range of datasets from MNIST to the large-scale dataset of Imagenet-1k (Deng et al. 2009), and from classification tasks to generation tasks using the most advanced generative model of denoising diffusion probabilistic models (DDPM) (Ho, Jain, and Abbeel 2020). Extensive experiments reveal that our method reduces computational resources required for the backward process by nearly 40%, the most computation-intensive part, while also enhancing model performance by alleviating over-fitting similar to Dropout. Additionally, experiments show that our method is trustworthy to reduce energy consumption during the research and development (R&D) of new AI models."}, {"title": "Method", "content": "We first introduce the forward and backward processes of CNN and then an efficient and effective computation acceleration method that inspires us to sparsify CNN back-propagation gradient maps."}, {"title": "Preliminaries", "content": "Forward Propagation We consider a single convolution layer with input X in the shape of (B_t, C_{in}, H_{in}, W_{in}) and output Y in the shape of (B_t, C_{out}, H_{out}, W_{out}), where B_t is the batch size, C_{in} and C_{out} are the number of input and output channels, respectively; H_{in}, W_{in}, H_{out}, and W_{out} are the height and width of the input feature maps, and the height and width of the output feature maps, respectively. Let W be the convolution weights (filters) of shape (C_{out}, C_{in}, K, K) and B be the bias vector of shape (C_{out}), where K represents the kernel size. Without losing generality, we assume the following for simplicity :\n1. The kernel height equals the kernel width, denoted as K.\n2. The stride of the convolution operation is denoted by s, which is a variable in this context.\n3. The padding parameter p, dilation parameter d, and groups parameter g are set to 0, 1, and 1 (default values), respectively.\nThese assumptions are made for better explanation in this paper, but formulas can be easily extended to handle different parameters. In our implementation, parameters are not fixed and can be any values as mainstream deep learning frameworks support.\nFollowing these assumptions, the forward pass is given by:\n\\begin{equation}\nY_{b,q,i,j} = \\sum_{p=0}^{C_{in}-1} \\sum_{m=0}^{K-1} \\sum_{n=0}^{K-1} W_{q,p,m,n} \\cdot X_{b,p,si+m,s.j+n} + B_q\n\\end{equation}\nwhere Y is the output feature map of shape (B_t, C_{out}, H_{out}, W_{out}), and B_q is the bias for the qth output channel."}, {"title": "Backward Propagation", "content": "The back-propagation formulas for this convolution operation are:\n1. Gradient w.r.t. single output element Y_{b,q,i,j} from the output feature maps Y:\n\\begin{equation}\n\\frac{\\partial L}{\\partial Y_{b,q,i,j}}\n\\end{equation}\nWhere L indicates the loss.\n2. Gradient w.r.t. single weight element W_{q,p,m,n} from the weights W (\\frac{\\partial L}{\\partial W}):\n\\begin{equation}\n\\frac{\\partial L}{\\partial W_{q,p,m,n}} = \\sum_{b=0}^{B_t-1} \\sum_{i=0}^{H_{out}-1} \\sum_{j=0}^{W_{out}-1} X_{bp,i+sm.j+sn} \\frac{\\partial L}{\\partial Y_{bq.i.j}}\n\\end{equation}\n3. Gradient w.r.t. single input element X_{b,p,i,j} from the input feature maps X (\\frac{\\partial L}{\\partial X}):\n\\begin{equation}\n\\frac{\\partial L}{\\partial X_{b,p,i,j}} = \\sum_{q=0}^{C_{out}-1} \\sum_{m=0}^{K-1} \\sum_{n=0}^{K-1} W_{q,p,m,n} \\frac{\\partial L}{\\partial Y_{b,q,i,j}}\n\\end{equation}\n4. Gradient w.r.t. single bias element B_q from biases B (\\frac{\\partial L}{\\partial B}):\n\\begin{equation}\n\\frac{\\partial L}{\\partial B_q} = \\sum_{b=0}^{B_t-1} \\sum_{i=0}^{H_{out}-1} \\sum_{j=0}^{W_{out}-1} \\frac{\\partial L}{\\partial Y_{b,q,i,j}}\n\\end{equation}\nImg2col and Col2img Implementing forward and backward propagation in a pure 7-layer-loop manner is impractical due to its inefficiency and slow performance. Nowadays, many deep learning frameworks adopt various techniques to accelerate such computation-intensive processes, including the img2col and col2img algorithms. These algorithms have been explicitly implemented in frameworks like Caffe (Jia et al. 2014) and are commonly used by other deep learning frameworks under the hood, especially in computation environments where, for example, GPU is unavailable.\nAn intuitive understanding of img2col is that it transforms the element-by-element multiplication of the convolution operation into matrix multiplication, enabling parallel computation and accelerating model training by introducing redundancy. As shown in Fig. 1 (b), img2col stretches each convolution unit in the input X (a 2D demonstration) into a single row, then concatenates all such rows together in the first dimension. Different data in a batch can also be stacked in a sequence for batch processing, finally resulting in a transformed col_X in the shape of (B_tH_{out}W_{out}, C_{in} \\cdot K\\cdot K). Likewise, the weights (filters) W can also be stretched in a similar way as Fig. 1 (b) demonstrates, resulting in a transformed col_W in the shape of (C_{in} \\cdot K \\cdot K, C_{out}). By multiplying col_X with col_W, we'll obtain the final convoluted results col_Y, which can be reshaped to (B_t, C_{out}, H_{out}, W_{out}) to match the size of Y. As for backward, the calculation of gradients of weights W and input X in Eq. 3 and Eq. 4 can be easily simplified as matrix multiplications between col_X and col[\\frac{\\partial L}{\\partial Y}], and between col_W and col[\\frac{\\partial L}{\\partial Y}], respectively, where the col[\\frac{\\partial L}{\\partial Y}] indicates the columnized form of $\\frac{\\partial L}{\\partial Y}$. The col2img algorithm is the reverse process of img2col, transforming the calculated gradients of col_X in the shape of $(B_t \\cdot H_{out} \\cdot W_{out}, C_{in} \\cdot K \\cdot K)$ back to the shape of input $X$, i.e. $(B_t, C_{out}, H_{out}, W_{out})$, by summing the gradients that correspond to the same position in X."}, {"title": "Backward Computation", "content": "To quantify energy efficiency, we approximately measure the FLOPs of convolution back-propagation, which consumes the majority of computational resources. Although almost negligible, we account for the FLOPs of BatchNorm and Dropout modules. In the columnized form of convolution, the backward FLOPs for a single convolution operation can be easily calculated as:\n\\begin{equation}\n(B_t\\cdot H_{out} \\cdot W_{out})(4C_{in} \\cdot K^2 + 1)C_{out}\n\\end{equation}\nIndependent of convolution operations, the backward FLOPs of BatchNorm and Dropout modules can be calculated as demonstrated in Eq. 7 and Eq. 8, respectively.\n\\begin{equation}\n12(B_t\\cdot H_{in} \\cdot W_{in} \\cdot C) + 10C\n\\end{equation}\n\\begin{equation}\n2(B_t\\cdot H_{in} \\cdot W_{in} \\cdot C)\n\\end{equation}"}, {"title": "Scheduled Sparse Back Propagation", "content": "To introduce sparsity to gradient maps during back-propagation, previous works (Wei et al. 2017; Sun et al. 2018; Raihan and Aamodt 2020) typically set a majority of matrix elements to zero, which enables skipping most matrix multiplication operations to reduce computational costs. However, such methods depend on specific sparsity patterns such as 2-out-of-4 non-zero pattern (NVIDIA 2020) and specialized instructions support from hardware like NVIDIA A100. While training on GPUs that do not support sparsity acceleration, which is much more common nowadays, these methods usually do not lead to desirable sparsity training acceleration and computational savings. To promote general sparse training on any machines and devices, we seek more structured matrix sparsity that doesn't rely on hardware support to reduce computational costs. Specifically, we introduce channel-wise sparsity similar to works using parametric channel-level sparsification (Zhou et al. 2021). Instead of introducing the regularization term into loss functions during training, we utilize gradients w.r.t top-K important output channels during backward and propose gradients selection schedulers to boost sparse training performance. As illustrated in Fig. 1 (a), with output gradients, we first compute the absolute values and average them spatially. This results in a vector where each element represents the magnitude, or 'importance,' of the corresponding output channel. Channels with larger importance are prioritized as they contribute more significantly to the gradients of inputs and weights/biases. We then sort this importance vector to create a mask that identifies the indices of the top K important channels. Gradients of these top-K channels are retained for calculating downstream gradients, while others are dropped. After gradients selection, we end up with a shrunk col[\\frac{\\partial L}{\\partial Y}]' in the shape of $(B_t \\cdot H_{out} \\cdot W_{out}, C_{out})$, where the $C_{out} \\leq C_{out}$ denotes the selected top-K output channels. The gradients of columnized inputs col_X and weights col_W can be calculated by multiplication between col_W and col[\\frac{\\partial L}{\\partial Y}]', and between col_X and col[\\frac{\\partial L}{\\partial Y}]', respectively.\nPrevious sparsification-based approaches often compromise model performance as the drop rate increases. To address this issue, we draw inspiration from learning rate schedulers and propose the use of drop schedulers during sparse training to improve model performance. Specifically, we implement an 80% sparsification strategy (discarding 80% channels of output gradients) on a periodic basis across CNN layers. This involves training the model normally in epochs 1, 3, 5, and so on while applying the 80% drop rate sparsification during epochs 2, 4, 6, etc., and similarly in subsequent epochs.\nBelow, we conduct a sensitivity analysis to evaluate different sparsification strategies and validate the specifics of our approach. We focus on the following key aspects: the sparsified dimensions (channel, width, height, or a combination), the gradients selection method (Top-K or random), the drop rate, and the drop schedulers.\nSensitivity Analysis We run experiments on CIFAR10 using ResNet18 with different drop rates ranging from 5 percent to 95 percent. All experiments are conducted 3 times and reported with the mean and standard deviation values unless explicitly specified otherwise. The sparsification is applied to all convolution layers within the model.\nFig. 2 plots the impact of various dropping behaviors on CIFAR10 test accuracy. We first explore three types of ways to sparsify the gradient maps: sparse-channel, sparse-height-and-weight, and sparse-all. These methods have exactly the same sparsification process as demonstrated in Fig. 1 (a), with a difference in averaging dimensions. The sparse-channel method averages gradients on dimensions of $B_t$, $H_{out}$, and $W_{out}$. The sparse-height-and-weight and sparse-all methods have similar processes with averaging dimensions of $B_t$ and $C_{out}$, and $B_t$, respectively. Fig. 2 (a) illustrates decreasing test accuracy as the drop rate increases, with minor differences among the three dropping behaviors across different drop rates. In practice, the sparse-channel mode is more practical due to its concise implementation and efficient integration with PyTorch, motivating us to continue with this mode in further experiments. To determine the necessity of sorting and retaining the largest, we compare the top-k method with random dropping in Fig. 2 (b), which shows that the model performance degrades much faster with random sparsity.\nIn Fig. 2 (c), we explore how drop schedulers affect the model convergence. We apply drop schedulers that gradually increase the drop rate from 0% to a specified rate (the X-axis) in linear, cosine, and bar patterns from the first epoch to the last epoch. For instance, with a target drop rate of 55%, we conduct four experiments independently using bar, linear, cosine schedulers, and a constant mode. In each experiment, the drop rate starts at 0% in the first epoch and gradually increases to 55% by the final epoch, according to the specific schedulers. The bar scheduler functions like a step function, maintaining a 0% drop rate for the first half of the training before abruptly increasing to the target rate in the second half. The constant mode serves as a baseline, keeping the drop rate fixed throughout the entire training process. The results indicate that schedulers can significantly improve model performance compared to the constant mode, even with a high drop rate of 95%. Among the three types of schedulers, the bar scheduler performs the best with drop rates ranging from 25% to 95% (Fig. 2 (c) blue line).\nNext, we compare various scheduler periods, ranging from 30 iterations to 300 iterations in increments of 30 (with 300 iterations nearly representing an epoch) and a 2-epoch-long period, which alternates between 0% drop rate for one epoch and a certain drop rate for the next epoch. In this analysis, we conduct each experiment only once. The mean and the standard deviation for bar and cosine schedulers in Fig. 2 (d) are calculated among different periods of iterations. The bar scheduler with a period of 2 epochs is plotted separately. The results indicate that the period length does not significantly impact performance when it ranges from 30 to 300 iterations, as evidenced by the narrow shadow area (low standard deviation) in Fig. 2 (d). Moreover, the bar scheduler with a period of 2 epochs outperforms the others by more than one standard deviation.\nTo optimize computational efficiency while preserving model performance, we implement sparsification along the channel dimension by discarding 80% of the smallest gradients, utilizing a bar scheduler with a period of 2 epochs.\nDrop Rate Lower Bound Because of the necessity of finding the smallest/largest gradients, we determine the minimum drop rate required to save computation without introducing additional overhead. In computing FLOPs for CNN, BatchNorm, and Dropout modules as shown in Eq. 6, Eq. 7, and Eq. 8, each operation of Addition, Subtraction, Multiplication, or Division is counted as one FLOP. The process of finding the smallest/largest gradients includes a summation operation across the batch, height, and width dimensions and a sorting operation along the channel dimension. The sorting operation only involves comparisons and does not generate floating-point computation, but the summation operation introduces additional $(B_t \\cdot H_{out} \\cdot W_{out} - 1) \\cdot C_{out}$ FLOPS."}, {"title": "Experiments", "content": "The img2col algorithm allows us to discard unwanted gradients and compute FLOPs savings conveniently, but it is quite impractical for parallel training on GPUs, limiting our method to small models and datasets. Fortunately, we can integrate our method into PyTorch by leveraging its built-in back-propagation implementation for fast training on any machine. Our implementation includes both the img2col version and PyTorch built-in backward version, which can be shifted as needed under various computing environments.\nTo validate our method, we run ResNet-18 and ResNet-50 (He et al. 2016) on the MNIST, FashionMNIST, CIFAR10, CIFAR100, CelebA, and Imagenet-1k datasets (LeCun et al. 1998; Xiao, Rasul, and Vollgraf 2017; Krizhevsky, Hinton et al. 2009; Liu et al. 2018; Deng et al. 2009), as detailed in Table 1. The CelebA dataset, which contains 40 binary labels per image, initially has image dimensions of (3, 218, 178). These images are resized to (3, 64, 64) in our experiments. For the Imagenet-1k dataset, we address varying image sizes by first resizing the images to (3, 256, 256) and then cropping the center to (3, 224, 224). To train the ResNet models on these datasets, we normalize each image using the mean and standard deviation specific to the respective dataset. To thoroughly evaluate our method, we assess its performance on both classification and generation tasks.\nThe training hyperparameters for the classification and generation tasks are shown in Table 2 and Table 3, respectively. All classification tasks are trained with the Adam optimizer with betas (0.9, 0.999), while all generation tasks are trained using the AdamW optimizer with default parameters. No learning rate schedulers and augmentation techniques are used in the training of any models, and all models are initialized with Kaiming Initialization (He et al. 2015) before training. For the classification tasks in Table 4, all experiments, except for ImageNet-1k, are conducted three times with different seeds and reported with average accuracy. The generation tasks in Table 5 are conducted once with the same seed. All experiments are conducted on Ubuntu 20.04.5 LTS (Focal Fossa) with the PyTorch 2.3.0 built-in backward version for efficiency, where the computing resources include a single 24G NVIDIA RTX A5000 GPU with AMD Ryzen Threadripper 3960X 24-Core Processor.\nTo compute the backward FLOPs consumption of ResNet and DDPM models during training, we calculate the FLOPs for the convolution layers, BatchNorm layers, and Dropout layers using Eq. 6, Eq. 7, and Eq. 8, respectively. These three types of modules account for almost all FLOPs consumption. For DDPM, we use GroupNorm instead of BatchNorm, but we exclude it from the calculation since convolution modules dominate the FLOPs consumption up to 99.7%."}, {"title": "Results", "content": "Table 4 presents the classification performance of ResNet-18 and ResNet-50 on six different datasets, with data sizes ranging from tens of thousands to over one million and image scales from grayscale 28\u00d728 to RGB 224x224. According to the FLOPs per iteration and total FLOPs consumption shown in the 3rd and 4th columns of Table 4, our method reduces FLOPs during back-propagation by nearly 40% compared to normal training. Despite these computational savings, our method achieves slightly better performance on almost all datasets compared to normally trained models. Additionally, using our method for back-propagation slightly decreases the training time. This may seem contradictive at first because the time consumption introduced by sorting is higher than the time saved from shrunk matrix multiplication, especially on GPUs with good computation capability. However, under the same GPU memory allocation, a large amount of shrunk matrices during back-propagation can significantly release memory occupation and speed up the data transfer between CPU and GPU, leading to faster training.\nThe superiority of our method is evident in generation tasks as well. Table 5 shows that our method not only reduces up to 40% computation during back-propagation but also slightly improves model performance, as measured by FID, and reduces training time. Additional visual results of our method are illustrated in Fig. 3."}, {"title": "Analysis of Key Properties", "content": "Although our method shows promising results in both classification and generation tasks, it is also important to investigate the following key properties:\n1. Overfitting Prevention (Q1): Can it prevent overfitting like Dropout?"}, {"title": "Overfitting Prevention (Q1)", "content": "Dropout is a regularization technique to prevent overfitting, which sets partial inputs to zeros during the forward process. During backward, those zeroed elements contribute zero gradients, helping prevent the network from becoming overly reliant on specific elements and enhancing model generalization. Our method aims to reduce computational expense during training without compromising performance, distinguishing itself from Dropout, which even increases computational demands in both forward and backward passes. As shown in Table 6, adding Dropout layers leads to higher training costs in terms of time and FLOPs due to slower convergence, resulting in higher test accuracy than the baseline. In contrast, our method also improves test accuracy over ResNet-50 while saving 40% FLOPs during backward, leading to comparable performance to Dropout with only around 4% and 8% backward FLOPs consumption on CIFAR10 and CIFAR100, respectively. This indicates that our method saves computation with the benefit of preventing overfitting.\nAdditionally, experiments show that our method prevents overfitting in a different way compared to the Dropout. The last two rows for each dataset in Table 6 illustrate that combining Dropout with our method results in even higher test accuracy than using Dropout alone. For CIFAR10 and CIFAR100, the 0.4+0.4 and 0.2+0.2 modes perform best, respectively. This is expected since the ResNet-50 model is less likely to overfit on the more complex dataset of CIFAR100, requiring less sparsification compared to CIFAR10. Even the 0.2+0.2 mode on CIFAR10 and 0.4+0.4 mode on CIFAR100 achieve comparable performance to using only Dropout while also saving 40% of computation. These improvements highlight the effectiveness of our approach to integrate with Dropout to enhance model performance and reduce computational costs."}, {"title": "Comparison to Models with Similar FLOPs (Q2)", "content": "To compare our method to a normally trained model with similar FLOPs consumption, we design a new ResNet model called ResNet-26, featuring BasicBlock layers in the configuration (2, 3, 5, 2). This model is compared to a sparsely trained ResNet-50, which uses Bottleneck layers in the configuration (3, 4, 6, 3). Both models exhibit similar backward FLOPs consumption during training. As shown in Table 7, we run both ResNet-26 and ResNet-50 in two different modes. The results demonstrate that the sparsely trained ResNet-50 does have equivalent test accuracy to the normally trained ResNet-26. This raises a key question: why not simply downscale the model by 40% instead of applying sparsification? The answer lies in the tendency of the larger model (ResNet-50) to overfit the dataset more than the smaller model (ResNet-26). A larger model is still preferable when dealing with diverse and complex datasets like Imagenet-1k. Either way, by introducing sparsification, both ssProp-50 and ssProp-26 (i.e., sparsely trained ResNet-50 and ResNet-26) outperform their counterparts (i.e., normally trained ResNet-50 and ResNet-26) by a considerable margin, demonstrating the superiority of our method."}, {"title": "Model Reliability (Q3)", "content": "A reasonable question arises when developing new AI models: can we really trust the sparsely trained model? In other words, will the model that performs best under normal training also perform best while sparsely trained? If so, this approach could make AI research and development more environmentally sustainable by substantially lowering energy consumption and reducing the carbon footprint.\nTo conduct a large-scale hyperparameters searching experiment, we adopt a simple CNN architecture with a few convolutional layers followed by a fully connected layer for classification. The number of CNN layers in the model ranges from 2 to 11, and the learning rate ranges from 0.0002 to 0.1024, increasing by a factor of 2. We run models on CIFAR100 and compare sparsely and normally trained models by observing the test accuracy patterns. As Fig. 4 presents, the overall performance patterns are very similar between the normal and sparse modes, except for some cases with extremely unsuitable learning rates. This is particularly evident in the blue-highlighted area in Fig. 4, where the best-performing models in both modes share the same hyperparameters of 6 CNN layers and a learning rate of 4e-4. This indicates that our method is reliable to use during the development and research phase to find the best hyperparameters while saving computational resources."}, {"title": "Conclusion", "content": "In this work, we propose a novel sparsification method that saves nearly 40% of computational resources during back-propagation while slightly improving model performance by preventing overfitting. Our method flexibly scales up to large-scale datasets like Imagenet-1k and performs well on both discriminative and generative tasks. It is efficient and compatible with many deep learning techniques, such as BatchNorm, GroupNorm, and Dropout, without requiring hardware sparsification acceleration support from GPUs like A100. To facilitate efficient AI development and reduce the carbon footprint, our method can be utilized during the R&D phases of AI technologies without compromising model performance and hyperparameters searching.\nAlthough this paper has made progress and improvements, there is still much to be investigated in the future: (1) Other techniques to enhance model performance, such as warm-up before sparsification, different drop rates and schedulers across layers within the model, etc; (2) Other properties of our sparsification, like sensitivity to data noise or feature distortion; (3) Improvement of sparsification by getting rid of sorting. Additionally, it is crucial to extend our method to MLPs and Transformers, which are the foundations of large language models, to facilitate efficient training in the natural language processing field."}]}