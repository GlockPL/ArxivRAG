{"title": "LLMS ARE IN-CONTEXT REINFORCEMENT LEARNERS", "authors": ["Giovanni Monea", "Antoine Bosselut", "Kiant\u00e9 Brantley", "Yoav Artzi"], "abstract": "Large Language Models (LLMs) can learn new tasks through in-context supervised learning (i.e., ICL). This work studies if this ability extends to in-context reinforcement learning (ICRL), where models are not given gold labels in context, but only their past predictions and rewards. We show that a naive application of ICRL fails miserably, and identify the root cause as a fundamental deficiency at exploration, which leads to quick model degeneration. We propose an algorithm to address this deficiency by increasing test-time compute, as well as a compute-bound approximation. We use several challenging classification tasks to empirically show that our ICRL algorithms lead to effective learning from rewards alone, and analyze the characteristics of this ability and our methods. Overall, our results reveal remarkable ICRL abilities in LLMs.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have been shown to exhibit in-context learning (ICL), a form of supervised learning that does not require parameter updates (Brown et al., 2020). ICL relies on including supervised input-output pairs in the LLM context (i.e., prompt),\u00b9 and has been shown as effective with few (Brown et al., 2020) or many (Bertsch et al., 2024; Agarwal et al., 2024) examples. In this paper, we ask whether the ability to learn in-context extends to reinforcement learning (RL), i.e., whether language models can effectively perform in-context reinforcement learning (ICRL)."}, {"title": "IN-CONTEXT REINFORCEMENT LEARNING", "content": "ICL operates by providing a model with correct demonstrations of a task. A demonstration includes an input (e.g., What is the best football club in Europe?) and its corresponding correct output (e.g., AC Milan). In its reliance on gold-standard labels, ICL follows the common supervised learning paradigm, although without any change in the model parameters.\nHowever, in-context learning could also be performed differently. Instead of providing models with correct demonstrations, the model could first try to guess answers, then observe the outcomes (i.e., rewards) of its predictions, and eventually learn from these signals, in an online learning setting, all within the context. This alternative way of learning in context follows the reinforcement learning paradigm (RL; Sutton & Barto, 2018), where models learn by reinforcing good behaviors and suppressing bad choices.\nFormally, we are concerned with an RL scenario where the model \\( \\pi \\) observes an input \\( x^{(t)} \\sim D \\) sampled from the data distribution D at time t, generates a prediction \\( \\hat{y}^{(t)} \\), and then observes a reward \\( r^{(t)} \\sim R(x^{(t)}, \\hat{y}^{(t)}) \\). We denote the tuple \\( (x^{(t)}, \\hat{y}^{(t)}, r^{(t)}) \\) as an episode.\nIn common RL terminology, the model \\( \\pi \\) is the policy, the input \\( x^{(t)} \\) is the state, and the prediction y(t) is the action. Throughout our formulation, the policy is also conditioned on previous episodes in the form of an LLM context, similar to how supervised examples are provided in ICL. These past episodes are not part of the state. Instead, the context is used to perform in-context policy"}, {"title": "NAIVE ICRL", "content": "Algorithm 1 outlines the most straightforward way to implement ICRL. The model repeatedly observes a new example, predicts its output, and observes its reward. Each such model interaction creates an episode, which is added to an episode buffer. For each interaction, we construct a context C from existing episodes (line 3). As long as the LLM context window allows it, at each time step, all past episodes & are included in the context in the order they were observed. This allows re-using past computations (i.e., through the KV cache), leading to relatively efficient computation.\u00b2 If the context window length is reached, we only consider recent episodes that fit into the context window, essentially running ICRL with a sliding window as big as the LLM context window allows.\nUnfortunately, Naive fails miserably in practice, as we empirically show in Section 4 and Figure 2. Its poor performance is due to its incapacity to explore the output space. Figure 3 visualizes how Naive degenerates to predicting just a few labels, far from the real distribution in the data."}, {"title": "EXPLORATIVE ICRL", "content": "Explorative ICRL addresses the exploration deficiency observed with Naive ICRL by leveraging the sensitivity of LLMs to their prompt. It has been widely observed that changes in prompt composition lead to variance in LLM behavior, including changes in the set of ICL examples (Zhang et al., 2022; Liu et al., 2022; Chen et al., 2023; Levy et al., 2023) or even seemingly meaningless stylistic changes (Sclar et al., 2024; Lu et al., 2022). Generally, this property of LLMs is not viewed positively. However, it provides an opportunity to introduce stochasticity into the ICRL process, and thereby introducing a level of exploration. We achieve this by randomly choosing the subset of past episodes to include in the prompt each time the model observes a new input.\nIn addition, we empirically observe that LLMs have a harder time benefiting from negative learning signals (i.e., episodes with negative reward). This has been observed in past feedback-driven continual learning work (Kojima et al., 2021; Suhr & Artzi, 2023). Negative episodes are also not very informative for learning \u2013 indicating that one output is bad, essentially encourages an almost uniform distribution over outputs. This leads to the second design decision in Explorative: only include examples with a positive reward in the constructed contexts.\nAlgorithm 2 describes Explorative ICRL. For each input, we construct a new context (lines 3\u20137). We decide what past episodes to include in this context by sampling from a Bernoulli variable parameterized by \\( p_{keep} \\) (lines 4\u20137. We sample independently for each past episode. This results in different reasoning for each input, because each is done with a different context. When storing past episodes, we only include episodes with positive reward (lines 13\u201314)."}, {"title": "APPROXIMATE ICRL", "content": "Explorative ICRL addresses the exploration deficiencies of Naive ICRL, but incurs high computational costs (Section 2.2). We propose an approximation of Explorative ICRL that strikes a balance between computational cost and learning effectiveness. Similar to Explorative, the approximate version also excludes episodes with negative reward and focuses on exploration by stochasticity in the context.\nAlgorithm 3 describes Approximate ICRL. The core idea behind the approximation is to limit the number of contexts, so we can simply gradually expand them with new episodes, rather than always create and compute new contexts. We maintain K contexts C, which all start empty (line 1). At each time step t, we sample a context C from the K contexts (line 3), and use it for episode t (lines 4\u20136. If the reward r(t) > 0, we use the episode to expand all contexts stochastically. For each context in C, we expand it with the t-th episode with a probability of \\( p_{keep} \\) (lines 8\u201311).\nApproximate introduces stochasticity in two places: sampling the context to use for each episode and the expansion of the stored contexts. In Algorithm 3, we use uniform sampling to choose the context (line 3). This is a uniform approximation of the probability of a context, which can also be easily computed exactly using the probabilities of the episodes it contains and \\( p_{keep} \\). In practice, we find the exact computation to work poorly, because contexts that are assigned more episodes or have"}, {"title": "EXPERIMENTAL SETUP", "content": "Models We use the instruction-tuned versions of Llama 3.1 8B (Llama Team, 2024) and Phi-3.5- mini 3.8B (Abdin et al., 2024). We chose these models because, at the time of this work, they are the only popular open-source large language models that support more than 100k tokens in the context, while still having less than 10B parameters. We use both in a multi-turn chat format. We compute the maximum number of episodes the context window can take for each model and task combination. Appendix A.2 reports the exact numbers. Both models are used for the main experiments, but only Llama for a few secondary experiments, due to the computational costs. We use constrained decoding when generating model predictions, similar to recent work on ICL (Bertsch et al., 2024).\nTasks We follow Bertsch et al.'s (2024) study of many-shot ICL in focusing on five classification problems: Banking-77 (77 labels; Casanueva et al., 2020), Clinic-150 (150 labels; Larson et al., 2019), NLU (68 labels; Liu et al., 2021), TREC (6 labels: Li & Roth, 2002; Hovy et al., 2001), and TREC-fine (50 labels; Li & Roth, 2002; Hovy et al., 2001). Because of the large output spaces (up to 150 labels in Clinic-150), these tasks are challenging for large language models, as empirically shown by Bertsch et al. (2024) and replicated in our ICL experiments.\nThe datasets are of different sizes. The size of the datasets dictates the number of time steps in our experiments. We randomly sub-sample Banking-77, Clinic-150, and NLU to 10k examples. TREC and TREC-fine are smaller, so we only use 5k training examples for each. This allows the experiments to be of relatively standard length. The training data corresponds to the data distribution D in our algorithms. We also sub-sample all test sets to 500 examples each, to reduce the computational cost of experiments. NLU does not provide a standard test set, so we create our own train and test splits. In all experiments, the datasets contain the same examples in the same order.\nRewards and Prompt Design We use a deterministic binary reward function. Rewards are computed from the gold-standard labels in the dataset. We automatically transform the numerical rewards into a natural language format indicating if the model prediction is correct or not, which is more suitable for LLM reasoning. Appendix A.1 provides more details about our prompting.\nEvaluation We report running test accuracy. For test accuracy, we use the held-out test set of each dataset. We compute it every 500 steps for each test example separately, using the context used to process that step's training example. In some cases, we also report train accuracy as the running mean accuracy over the most recent 256 episodes.\nWe also report regret, the forgone utility from an actual model prediction in comparison to the oracle choice. Intuitively, regret measures how many interactions the model handled poorly throughout the experiment. In our experiments, regret is the accumulated number of incorrect examples throughout learning. Regret gives a single number that considers both the final performance and how fast the model reached it. A good system would reach high performance as fast as possible, making fewer mistakes overall (i.e., would have a low regret).\nComparisons We compare our ICRL algorithms against the zero-shot setting, which corresponds to the performance on the test set at step zero (i.e., without any in-context examples). We also report supervised ICL performance for all tasks to contextualize the results. We generally expect supervised ICL to outperform ICRL, because it has access to gold-standard labels. In particular, at each time step where we report supervised ICL performance, we provide the model all examples observed so far by the ICRL methods, but with gold-standard labels. We stop the supervised ICL experiments when the number of examples becomes bigger than the maximum number supported by the context window."}, {"title": "RESULTS AND ANALYSIS", "content": "We show the test accuracies and training regrets Figure 2.3 We also show the performance of supervised ICL for comparison, although it relies on a much higher degree of supervision.\nLLMs Can Learn In-Context From Rewards Alone Explorative effectively learns in all tasks and for both models, showing significant improvements over zero-shot. Explorative improves over the performance of zero-shot Llama by +48.8% in Banking-77, +56.8% in Clinic-150, +36.8% in NLU, +36.0% in TREC, and +50.2% in TREC-fine; and the same with Phi by +46.2% in Banking-77, +55.2% in Clinic-150, +33.4% in NLU, +9% in TREC, and +22.4% in TREC-fine. In general, its accuracy approaches the supervised ICL upper bound in some settings, and it always outperforms zero-shot. For both models, Explorative also demonstrates a continual growth in performance over time, suggesting that with more data its performance would improve. This is especially evident for the most challenging datasets, i.e., the ones with the most labels (i.e., Banking-77, Clinic-150, NLU), as they require a much stronger exploration effort. Thus, our empirical findings show that LLMs can learn in-context from rewards alone.\nNaive Fails to Explore The Naive does not learn and in most cases even deteriorates below zero- shot (Figure 2). One key issue is exploration. Figure 3 shows prediction confusion matrices, output distributions, and data distributions for the Banking-77 task with Llama, comparing zero-short, Naive, and Explorative. A perfect classifier would have non-zero counts only on the diagonal, and the output distribution would be identical to the data distribution. Both Naive and Explorative start from zero-shot. After learning, Explorative shows a clear focus on the diagonal and higher similarity between the prediction and data distributions. Naive fails to learn to effectively classify. The output distribution explains why: its focus on just a few labels indicates it failed to explore.\nBoth Modifications of Explorative are Important Explorative modifies Naive in two ways: stochasticity for exploration and episodes with positive rewards to simplify the context. Explorative with both positive and negative rewards learns, but much less effectively than if we omit episodes with negative rewards (Figure 4a). On the other hand, Figure 4b shows that even though omitting negative examples from Naive helps, there remains a large gap to Explorative. Figure 4a also shows the impact of reward. We see some level of learning without rewards or with inverted rewards. This aligns with past observations of a domain effect in ICL (Min et al., 2022; Pan et al., 2023; Lyu et al., 2023; Kossen et al., 2024). However, this learning is relatively minimal, and including both positive and negative episodes improves performance significantly, albeit it remains much lower than with positive episodes only. Including only negative episodes, on the other hand, is catastrophic.\nWe also observe that when providing noisy rewards (i.e., with a probability of 10% the reward is inverted) performance does not degrade significantly, suggesting that ICRL has some robustness to environments with noisy learning signals. Of course, this is an initial experiment of robustness to noise, and we leave a more detailed analysis for future work.\nUniform Context Sampling in Approximate is Better We observe that exact context sampling in Approximate performs worse than uniform sampling. This happens because exact computation often leads to always using the same context at later steps, as small changes in the probability of sampling can compound once the model is biased towards one context. Table 5b reports regret and final accuracy for exact and uniform strategies, on Llama and both Banking-77 and Clinic-150 tasks. Appendix B provides more details on this comparison, including visualization of context selection.\nApproximate is an Effective Alternative to Explorative In the Figure 2, Approximate performs almost as well as Explorative ICRL when trained with Llama, across all tasks. The results are very different with Phi: despite early learning, Approximate deteriorates quickly. This stems from one of the contexts being biased towards one label and therefore predicting only this label. Eventually, the bias towards the label spreads to other contexts, leading to the collapse in performance we observe. It is empirically possible to recover, as we see in Banking-77 later in the experiment, but the chance of it happening seems very low. The success of Llama and failure of Phi with K = 8 show that different LLMs have different sensitivity to the approximation. Figure 5a shows that that with a higher number of contexts K > 32 Phi is able to effectively learn, indicating Phi needs a higher computational budget. Figure 13 in Appendix B shows this sensitivity analysis for Llama. Overall, Llama is robust to the approximation, with most values performing similarly to Explorative, except with the lowest values of K.\nApproximate Reduces Compute Needs We measure the reduction of tokens processed in Approximate compared to Explorative throughout full ICRL runs. We approximate this measure by computing at each step the number of tokens required for a forward call and subtracting the number of tokens of the sequence with the longest common prefix processed in a previous step, as it would be possible to use the KV cache for all the tokens in the common prefix (assuming infinite memory). We find that Explorative processes two orders of magnitude more tokens than Approximate. Table 2 in Appendix B provides numerical results for this analysis.\nICRL is Sensitive to Stochasticity Level Stochasticity in context generation is one of the important components that contribute to both Explorative and Approximate performance. It is modulated by setting \\( p_{keep} \\). Figure 6 shows the sensitivity of Explorative to the value of \\( p_{keep} \\). Without stochasticity (\\( p_{keep} = 1.0 \\)), ICRL struggles on both models, but especially on Phi. However, if \\( p_{keep} \\) is too high, we retain too few examples on the context, and it can hurt performance.\nComparison of Context Subsampling Strategies In practice, we never saturate the LLM context window when using Llama or Phi because our context window is more than 100k and \\( p_{keep} = 0.1 \\). We conduct experiments to evaluate the strategies we presented in Section 2.2 to handle the case of overflowing the context window by limiting the context window of Llama to 4k or 8k tokens. Generally, we observe that start-biased strategy outperforms unbiased, which in turn performs better than end-biased, in all cases, although by only small margins. Figure 7 shows the results of this analysis for Banking-77, and Figure 12 in Appendix B for Clinic-150."}, {"title": "RELATED WORK", "content": "In-Context (Supervised) Learning ICL was first demonstrated by Brown et al. (2020), and since then its causes (Chan et al., 2022; Xie et al., 2022; Olsson et al., 2022; Garg et al., 2022; Von Oswald et al., 2023; Hendel et al., 2023; Wang et al., 2023) and the level of learning it displays (Min et al., 2022; Lyu et al., 2023) have been studied extensively. By now, it is well established that LLMs can learn new tasks in context (Garg et al., 2022; Wei et al., 2023; Pan et al., 2023; Kossen et al., 2024; Li et al., 2024). Our work builds on this line of work, and provides the first evidence that LLMs can perform RL in context, and not only supervised learning (i.e., the standard way it is done).\nOur study would not be possible without recent increases in the context window length of LLMs (Llama Team, 2024; Abdin et al., 2024; Gemini Team, 2024). Recent work showed that model performance can continue to increase when including hundreds or thousands of demonstra- tions (Bertsch et al., 2024; Agarwal et al., 2024). We find similar results, as LLMs can continually improve when learning through ICRL until their context does not saturate. Interestingly, while some work (Zhang et al., 2024; Mo et al., 2024; Shinn et al., 2023) find that models can learn from mistakes, our results do not support this. It is possible that models can learn from mistakes only when explicitly reasoning (Kojima et al., 2022; Wei et al., 2022) about them (Zhang et al., 2024; Shinn et al., 2023) and cannot implicitly leverage negative signals.\nIn-Context Reinforcement Learning Likely the closest work to ours is Krishnamurthy et al. (2024). They investigate whether LLMs can solve multi-armed bandit problems, a state-less simpler RL setting than the one we are focused on. Our experimental setup revolves around contextual bandit problems, where the best action depends on the specific input. We observe similar issues to their findings with the Naive approach. They present a set of negative results, and finally are able to elicit effective learning, but through a prompting strategy that cannot generalize beyond their very simple"}, {"title": "DISCUSSION AND LIMITATIONS", "content": "We study the potential of LLMs to demonstrate ICRL, and propose several algorithms to elicit this behavior: Naive, Explorative, and Approximate. Naive fails miserably, but this allows us to identify exploration as the key missing ingredient. Explorative introduces stochasticity to the prompt construction, and combined with focusing on positive examples, shows consistent ICRL. However, this comes at a high computational cost. Approximate comes to address this cost, by a strict approximation of Explorative. We provide a detailed analysis of the various methods, and show the importance of each of our choices, and the sensitivity of the process to various settings.\nOur work carries several limitations, all of which outline important directions for future work. The first is due to our choice of problems to study. We intentionally selected classification benchmarks to simplify the experiments and evaluation in this early stage of studying ICRL. However, this leaves open the question of applicability to more complex problems, where rewards are more nuanced. For example, summarization and question answering provide much more formidable challenges, albeit with complex evaluation challenges. We believe our work enables future work to study these challenges, and that this is an important direction.\nAnother limitation is our use of a binary reward function. This choice is directly derived from our focus on classification. It is an important aspect in making our benchmark environment straightforward to experiment with. However, it leaves open an important question: can ICRL handle more nuanced reward signals? For example, a reward function that can give all possible real numbers in a specific range. Such a reward function leads to an interesting challenge in decoding it into language. It is a particularly important question given our observations that LLMs do not learn well in-context from negative rewards. The problem we identified with reasoning about episodes with negative rewards poses another limitation, and lays out an important research question for future work.\nOur work also lays out open questions as far as the use of computational resources. Our methods are relatively compute-intensive, especially after the learner observes many episodes. We propose Approximate to address this, and show how it allows to trade-off compute for robustness. However, Approximate left open the problem of working with a limited context window, a critical problem for deploying these methods for extended periods with many interactions. This, again, is a very important direction for future work.\nFinally, not a limitation per se, but we kept prompt optimization to a minimum. This was an intentional choice, because our goal is to find robust behaviors, and not prompt engineer the problem. However, this does leave significant room for development, likely improving on the results we observe.\nWe hope our work helps to shed light on the capabilities of contemporary LLMs, and that it lays out the ground for extensive future work, both research and practice."}, {"title": "EXPERIMENTAL SETUP", "content": "Each experiment is conducted on four NVIDIA A100 GPUs, each with 40GB of memory. For efficient inference, we use vllm (Kwon et al., 2023)."}, {"title": "PROMPT DESIGN", "content": "We report prompt examples from ICL and ICRL experiments. We show the prompts for both Llama and Phi, because Transformers library (Wolf et al., 2020), which we use for the tokenizers, automatically injects the cut-off and current dates in Llama's system prompt, making it slightly different from that of Phi. In all cases, we show the prompts with two in-context examples."}, {"title": "CONTEXT WINDOWS AND EPISODE CAPACITY", "content": "For each task and model combination, we conservatively estimate the maximum number of examples that could fit within the context window. This is done by including all observed examples in descending order of token count in the prompt, assuming the model consistently responds with the longest label and that the formatted reward message is at its maximum length. We perform this calculation using the maximum context window for both Llama and Phi. Additionally, for Llama, we repeat the process with context windows of 4096 and 8192 tokens specifically for the Banking-77 and Clinic-150 tasks. Table 1 reports episode capacity."}]}