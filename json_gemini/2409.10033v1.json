{"title": "Can GPT-01 Kill All Bugs?", "authors": ["Haichuan Hu", "Ye Shang", "Guolin Xu", "Congqing He", "Quanjun Zhang"], "abstract": "ChatGPT has long been proven to be effective in automatic program repair (APR). With the continuous iterations and upgrades of the ChatGPT version, its performance in terms of fixes has already reached state-of-the-art levels. However, there are few works comparing the effectiveness and variations of different versions of ChatGPT on APR. In this work, we evaluate the performance of the latest version of ChatGPT (01-preview and O1-mini), ChatGPT-40, and historical version of ChatGPT on APR. We study the improvements of the Ol model over traditional ChatGPT in terms of APR from multiple perspectives (repair success rate, repair cost, behavior patterns), and find that Ol's repair capability exceeds that of traditional ChatGPT, successfully fixing all 40 bugs in the benchmark. Our work can serve as a reference for further in-depth exploration of the applications of ChatGPT in APR.", "sections": [{"title": "Introduction", "content": "With the development of automated program repair (APR) technology, researchers have proposed many effective methods to fix bugs automatically. Traditional template-based APR (Xia et al., 2023; Xia and Zhang, 2022) performs well for known bugs but are powerless against previously unseen bugs. Learning-based APR (Chen et al., 2022; Chi et al., 2022) is able to learn the bug-fixing patterns automatically from existing code repositories, thus having good generalization capability. As a further development of learning-based APR, large language model (LLM) based APR methods are receiving increasing attention.\nChatGPT (OpenAI, 2023), which is widely regarded as the most representative application of LLMs, has proven to perform well across various code-related tasks (Sun et al., 2023; Liu et al., 2023; Yuan et al., 2023). Moreover, ChatGPT has been proven to be effective in program repair as well. Sobania et al. (Sobania et al., 2023) evaluate the bug-fixing capabilities of ChatGPT and find ChatGPT is able to fix 31 out of 40 bugs on QuixBugs (Lin et al., 2017) benchmark. Xia et al. (Xia and Zhang, 2023) employ ChatGPT in a conversational manner to fix 114 and 48 bugs on the Defects4J-v1.2 and Defects4J-v2.0 benchmark.\nHowever, few studies have compared the effectiveness and differences of various versions of ChatGPT in APR tasks. Most people understand ChatGPT in terms of the fact that newer versions have more parameters and larger training datasets. As a result, it can better understand code, leading to improved performance in APR. Such perspective may be challenged with occurrence of the latest version of ChatGPT, ChatGPT-01 (01). Different from previous versions of ChatGPT, O1 leverages reinforcement learning (RL) and chain of thought (COT) techniques. Before answering a question, it first takes time to think, organizes a chain of thought, and then arrives at a final answer. By practice, Ol is more suitable for fields with complex logic and relatively definitive answers, such as mathematics and programming.\nIn our work, we evaluates Ol's performance in APR through dialogues, given the limited understanding of O1. We conduct experiments on the two currently released trial versions of the Ol model (01-mini and O1-preview), and GPT-40 as a representative of the most popular version of ChatGPT. We design a two-step repair process. First, we provide a basic prompt template for ChatGPT to directly repair the bugs. For bugs that fail the test cases, we further provide ChatGPT with the error information from the test cases to perform a second round of repair. After all repairs are done, we collect data on repair success rate, time spent on repairs, and behavior patterns of the model during the repair process, to further analyze Ol's performance in APR. The results show that Ol outper-"}, {"title": "Methodology", "content": "In this section, we present the method for evaluating ChatGPT's ability to fix bugs. We use different versions of ChatGPT to fix benchmark bugs and evaluate the repair behavior from multiple dimensions."}, {"title": "Two-step Fix", "content": "The process of bug fixing by ChatGPT consists of two steps. First, we use a basic repair template to ask the model whether there are bugs in the target code snippet. After making a judgment, the model is asked to fix the bugs."}, {"title": "Evaluate 01 and Other ChatGPT models", "content": "We evaluate ChatGPT's performance in fixing program bugs from the following dimensions.\n\u2022 Repair success rate, which is the most important metric for evaluating the effectiveness of ChatGPT in program repair.\n\u2022 Time for thinking. Compared to earlier versions of ChatGPT, O1 has added a \"thinking\" phase. We record and analyze the time spent by O1-preview and O1-mini in thinking phase.\n\u2022 Response length. We record and compare the output lengths of different models to evaluate the monetary cost of repairing.\n\u2022 Model behavior pattern. We analyze behavior patterns of O1 and other ChatGPT models to explore their mind when fixing bugs."}, {"title": "Experiment and Results", "content": null}, {"title": "Benchmark and Baselines", "content": "We use QuixBugs (Lin et al., 2017) as our benchmark. For each of the 40 benchmark problems, we take the erroneous python version. These programs have complete context and corresponding test cases, and they are relatively short, making them suitable for bug fixing through dialogue with ChatGPT. We compare our results with previous work (Sobania et al., 2023). In prompt designing, we keep consistent with previous work (Sobania et al., 2023) and carefully review the comments in the original program to ensure that they do not reveal the solution, retaining only the relevant parts of the test cases. Thus, we integrate the baseline method into the first step of the repair method presented in Section 2.1."}, {"title": "Results", "content": "In this section, we first compare the repair results of the latest ChatGPT model Ol, previous versions of ChatGPT, and the baseline method on QuixBugs. We then analyze and summarize the details of the O1 model's repair behavior."}, {"title": "Comparison of O1, Previous Versions of ChatGPT, and Baseline Results", "content": "We present the main results of comparison in Table 1. In terms of baseline results, despite the significant improvement in repair effectiveness after introducing additional information, with a repair success rate reaching 31/40, it is still far from the new version of ChatGPT (ChatGPT-40, O1-mini, Ol-preview). It can be seen that ChatGPT has made significant improvements in program repair capabilities through iterations.\nCompared to the current mainstream version of ChatGPT (ChatGPT-40), 01 shows some improvement in program repair capabilities. Before providing test case error information, O1-mini and O1-preview can repair 37 and 38 bugs respectively, which is 2 and 3 more than ChatGPT-40. After providing test case error information, both 01-mini and O1-preview are able to repair all 40 bugs, whereas ChatGPT-40 can only repair 38 bugs.\nTo further investigate the improvements of the Ol model compared to previous versions of ChatGPT in program repair, we also conduct a case analysis on the programs that the Ol model is able to repair but ChatGPT-40 and previous versions of ChatGPT can not. We find that these programs are relatively complex, often involving structures like recursion and nested loops, and they are likely to correspond to solutions for real-world problems rather than simple utility methods (such as bit-count). Take hanoi as instance, both O1-preview and O1-mini successfully repair the bug on the first attempt, whereas neither ChatGPT-40 nor the baseline version of ChatGPT are able to fix it. In the baseline method, ChatGPT fails four times before finally succeeding after being provided with the correct test case answers. With only error information and no test case answers, ChatGPT-40 is still unable to repair it correctly. ChatGPT-4o incorrectly interpret the boundary condition, assuming that the source rod cannot be equal to the destination rod. In order to solve the hanoi problem, the Ol model spends 15 seconds thinking and forms a chain of thought (analyze functionality, check code logic, correct steps, optimize move steps). This chain of thought helps it correctly understand the logic of the problem, avoiding falling into incorrect logical branches. From this, we can infer that the reasoning pattern from chain of thought to solution is crucial for repairing buggy programs with more complex logic."}, {"title": "Evaluation of O1 in Terms of Response Time, Response Length, and Behavior Pattern", "content": "In terms of response length, we conduct a token count analysis of Ol-preview, O1-mini, and ChatGPT-40 on all benchmark programs using the tool tiktoken. We found that the average response lengths for the three models are 1450, 1086, and 654 tokens, respectively. This indicates that the O1 model's response length is over 50% longer compared to previous versions of ChatGPT, leading to more comprehensive and complete answers, albeit at a higher cost.\nIn terms of response time, since the generation time is related to the length of the output, we only measure the time taken by the Ol model for thinking. We found that the average thinking time for Ol-preview reaches 19.82 seconds, which is about three times longer than that of O1-mini (7.02 seconds).\nIn terms of model behavior patterns, we observe that the Ol model typically begins by performing logical analysis on the buggy program, generating a solution, and then gradually proceeding with repairs and testing before providing the complete fixed code. In contrast, ChatGPT-40 tends to provide the repaired code first, followed by an explanation of the code. Please refer to Appendix A for detailed evaluatin."}, {"title": "Conclusion", "content": "This paper evaluates the capabilities of the latest ChatGPT model Ol in program repair. By comparing it with previous versions of ChatGPT and baseline results, the paper demonstrates the advancements of the O1 model. Additionally, we conduct case analysis and conclude the advantages of the O1 model in repairing more complex bugs. The paper also provides a comprehensive analysis of other characteristics of the Ol model, including thinking time, response length, and behavior patterns, offering a reference for future in-depth research on utilizing O1 for program repair."}, {"title": "Limitations", "content": "Currently, Ol is in the trial phase, with usage limits and high API costs. Understanding of Ol is still quite insufficient. The benchmark used in our study is relatively small, and further research is needed with larger datasets to explore the Ol model's capabilities in APR more comprehensively."}]}