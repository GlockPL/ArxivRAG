{"title": "Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing", "authors": ["Miriam Alber", "Christoph H\u00f6nes", "Patrick Baier"], "abstract": "One of the most promising use-cases for machine learning in industrial manufacturing is the early detection of defective products using a quality control system. Such a system can save costs and reduces human errors due to the monotonous nature of visual inspections. Today, a rich body of research exists which employs machine learning methods to identify rare defective products in unbalanced visual quality control datasets. These methods typically rely on two components: A visual backbone to capture the features of the input image and an anomaly detection algorithm that decides if these features are within an expected distribution. With the rise of transformer architecture as visual backbones of choice, there exists now a great variety of different combinations of these two components, ranging all along the trade-off between detection quality and inference time. Facing this variety, practitioners in the field often have to spend a considerable amount of time on researching the right combination for their use-case at hand. Our contribution is to help practitioners with this choice by reviewing and evaluating current vision transformer models together with anomaly detection methods. For this, we chose SotA models of both disciplines, combined them and evaluated them towards the goal of having small, fast and efficient anomaly detection models suitable for industrial manufacturing. We evaluated the results of our experiments on the well-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing a suitable model architecture for a quality control system in practice, considering given use-case and hardware constraints.", "sections": [{"title": "1 Introduction", "content": "In industrial manufacturing, early detection of defective products saves material and costs and enhances public trust in the manufacturer. Automating this process increases scalability, saves labour costs and reduces human error due to the monotonous nature of visual inspections [27]. To this end, the possibility of automating this process using machine learning methods has been subject of extensive research [24]. Anomaly detection (AD) in machine learning addresses"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Vision Backbone", "content": "Vision transformers have become a powerful alternative to CNNs for various computer vision tasks, with many studies highlighting their proficiency in capturing global dependencies, which are essential for AD and AL workloads [20,30,6,21,31]. Monolithical vision transformers such as ViT follow the architecture of NLP"}, {"title": "2.2 Anomaly Detection and Localization", "content": "There are several different categories of approaches for implementing AD and AL in practice [24]. Reconstruction-based methods use an auto-encoder with the objective to compress the input to a lower dimensional latent space and subsequently reconstruct the original image, using the reconstruction error as anomaly score. They tend to deliver weaker results than other categories [30]."}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Backbone Architectures", "content": "Because of their performance advantages, our analysis primarily focuses on transformer-based architectures. Given the importance of hardware efficiency in practical applications, the lightweight and efficient hierarchical transformers are of particular interest in our study. For a comprehensive perspective, we also evaluate a classical ResNet-50 model as a baseline. We chose this ResNet variant since it is a common choice when making a trade-off between the quality of representations and computational cost [11,31]. The DeiT architecture was chosen as an example of a monolithic vision transformer that closely follows the architecture of the original ViT proposed in [9], but achieves better performance by using knowledge distillation [25]. We used the largest variant DeiT-base in our experiments since it has the highest performance and was also used in [31]. Moreover, we selected the EsViT approach to investigate the performance of hierarchical transformer versions. We used the variant based on the Swin-T architecture since it has a relatively small number of parameters which is comparable to ResNet-50. A window size of 14 achieved the best results in [16], hence we adopted this choice for our experiments. EfficientFormer is another efficient transformer suitable for settings with limited computational resources. To have a comparable parameter size to ResNet-50 and make a good trade-off between efficiency and performance, we picked the medium-sized model variant L3. A comparison of the number of parameters of the different image encoder models is shown in Table 1."}, {"title": "3.2 Anomaly Detection Architectures", "content": "Based on the feature vector produced by the vision backbone a classifier needs to distinguish normal examples from defective anomalies. We focused our experiments on GMMs, which were one of the first approaches using a vision transformer backbone and NFs, which showed promising performance in AD tasks (as discussed in Section 2.2).\nThe performance of a GMM usually improves with a higher number of Gaussians but the use of a fully-connected MLP for every component can make them"}, {"title": "3.3 Datasets", "content": "We evaluate our methods on two datasets that are as close to real-life as possible.\nThe BTAD dataset consists of three different types of real-world industrial products and has a total of 2830 RGB images with a resolution of between 600 \u00d7 600 and 1600 \u00d7 1600 [21]. Each product category is split into a train set, consisting of only normal images and a test set, which includes normal and"}, {"title": "3.4 Implementation Details", "content": "Since both datasets do not provide a separate validation set for hyperparameter tuning and model selection we split the training set into 80% train and 20% validation data in both cases. We applied Min-Max scaling to every image. The minimum and maximum values were computed separately for every channel on the training set. We scaled the input images to an image size of 224 x 224 pixels which is the default training size of most transformer models [9].\nHyperparameters were optimized once for each model using the hazelnut class of the MVTecAD dataset and the best configuration according to validation loss is adopted for all other experiments. We chose this class because it has the largest amount of training samples in the dataset. A summary of the considered hyperparameters and the best values found for them are shown in Table 2. Note that the batch size of the GMM is relatively small due to hardware limitations. We trained each model separately on every object class for a maximum of 500 epochs using early stopping based on the validation loss with a patience of 30 epochs. Only the parameters of the AD models were updated, the image-backbones were pre-trained on ImageNetlk [7] and kept frozen during training. For evaluation the model checkpoint with the best validation loss was selected. All models were trained on a single NVIDIA GeForce RTX 2080 Ti TURBO GPU with 11GB VRAM."}, {"title": "3.5 Metrics", "content": "For the evaluation of our approach, we use the AUROC since it is a common metric for visual quality control [31,2,22,13]. For evaluating AD we consider the AUROC on image and for AL on pixel level.\nAs a second metric we use the Per-Region-Overlap (PRO), which measures the overlap between ground truth and predicted anomalies on a pixel level. To"}, {"title": "3.6 Experiments", "content": "Our first goal was to assess if similar outcomes can be achieved as the VT-ADL architecture by Mishra et al. [21]. While they trained a vision transformer with a GMM and a CNN from scratch on the BTAD and MVTecAD datasets, we used a pre-trained DeiT model as a frozen image backbone and trained only a GMM with the likelihood loss. Chosen for its enhanced performance over ViT, our DeiT configuration includes twelve layers and twelve heads, versus the six layers and eight heads used by VT-ADL. We used an image resolution of 224 \u00d7 224 instead of 500 x 500 to match the requirements of our pre-trained image encoders and employed a smaller GMM with only 100 instead of 150 Gaussians according to our empirical hyperparameter search. Additionally, we attempted to reproduce Yu et al.'s [31] promising FastFlow results for MVTecAD, despite the absence of their source code. We followed their experimental details. While the authors use the 7th DeiT encoder block's embedding we additionally evaluate a model version which uses DeiT's last layer. For compatability with our pre-trained backbones we scale the images to a size of 224 \u00d7 224 instead of 384 \u00d7 384 pixels.\nA second goal was to study the behavior of GMMs and NFs on a more fine-grained level to find out if there are differences in the performance depending on the object class. For this experiment we used our setup with a pre-trained DeiT image encoder and trained and evaluated the 15 classes of the MVTecAD dataset"}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Comparison with the VT-ADL and FastFlow models", "content": "Table 3 shows that our GMM model performs better than VT-ADL in two of three classes of the BTAD dataset in the localization task and one class in the detection class. The bad detection performance on class three may result from the model highlighting anomalies correctly but also producing spots with high anomaly scores in normal images (see Figure 3). This results in a high false positive rate when using the maximum of the patch anomaly scores to classify the image. Considering also the composition of the test dataset for this class can explain the gap between detection and localization performance, since it has about ten times more normal than anomalous samples. The overall localization performance of our model on the MVTecAD dataset is higher than the one of VT-ADL. These results show, that using a pre-trained transformer encoder in"}, {"title": "4.2 Comparison of GMMs and NF models", "content": "Table 4 shows, that our implementation of the NF model could not reach the values reported for FastFlow in [31]. However, the use of only 80% of the training data and a lower image size may have negatively affected the performance."}, {"title": "4.3 Performance of the Backbones", "content": "Table 5 shows that for the GMM the overall localization performance is the best with the DeiT backbone. It outperforms the hierarchical backbones on all classes except for the cable class. In contrast, the Es ViT model performs best in three of five classes in the detection task and has the overall best detection performance. A possible reason for the gap between localization and detection performance can be seen on the generated anomaly maps in Figure 4. The EsViT model does locate the anomaly correctly but also highlights large areas in the background. On the normal image there is no area highlighted at all. A possible reason for the bad performance of the ResNet backbone is the usage of only 50 Gaussians and two output layers as discussed in Section 3.6.\nThe results in Table 6 show, that DeiT is the backbone that results in the best detection performance for the NF model. Nevertheless, Es Vit follows with the second best detection performance. Interestingly, while in general the NF achieved the best results, for Es ViT the GMM resulted in a better performance. The ResNet backbone outperforms DeiT in two localization tasks but performs"}, {"title": "4.4 Considerations and Limitations for Practical Application", "content": "Anomaly maps can be used in various manufacturing scenarios such as explanation of a model's choice, making decisions based on the anomaly size or to double check a model's decision [6]. However, it is important to notice that the experiments in this work are conducted on benchmark datasets that have high quality, which is hard to achieve in a real-world scenario. In manufacturing, metrics should be selected considering the composition of training data and the"}, {"title": "5 Conclusion", "content": "In our work, we provided a comprehensive overview of SoTA vision transformer models and evaluated different paradigms for visual anomaly detection in industrial visual quality control. We implemented two anomaly detection methods with four different image encoding backbones, all of them pre-trained on ImageNetlk. We trained our anomaly detection models on the datasets MVTecAD and BTAD and compared our results with two existing approaches from the literature. Finally, we discussed important aspects to consider when applying these approaches to production. We showed that using transformer models can improve the performance of anomaly detection models and reduce the overall size compared to ResNet. Moreover, we showed that using pre-trained transformer models can have an advantage over training from scratch. Hierarchical"}]}