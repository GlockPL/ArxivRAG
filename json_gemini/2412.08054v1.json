{"title": "Federated In-Context LLM Agent Learning", "authors": ["Panlong Wu", "Kangshuo Li", "Junbao Nan", "Fangxin Wang"], "abstract": "Large Language Models (LLMs) have revolutionized intel-ligent services by enabling logical reasoning, tool use, andinteraction with external systems as agents. The advancementof LLMs is frequently hindered by the scarcity of high-qualitydata, much of which is inherently sensitive. Federated learning (FL) offers a potential solution by facilitating the collaborative training of distributed LLMs while safeguarding privatedata. However, FL frameworks face significant bandwidthand computational demands, along with challenges from heterogeneous data distributions. The emerging in-context learning capability of LLMs offers a promising approach by aggregating natural language rather than bulky model parameters. Yet, this method risks privacy leakage, as it necessitatesthe collection and presentation of data samples from variousclients during aggregation. In this paper, we propose a novelprivacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm, which to our best knowledge for thefirst work unleashes the power of in-context learning to traindiverse LLM agents through FL. In our design, knowledgecompendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods. Apart from that, an incredibleRetrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools. We conducted extensive experiments and the results show that FICAL has competitive performance compared to other SOTA baselines with a significant communication cost decrease of 3.33 \u00d7 105 times.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) has introduced a revolutionary approach to address the growing demands for advanced intelligent services. Different from traditional smaller neural networks, LLMs are trained on massive diverse data with billions of parameters thus enabling them to have emergent abilities (Wei et al. 2022) that traditional neural networks do not have. These emergent abilities enable LLMs to have the ability to carry out logical reasoning and thinking as well as interact with external tools from the open world thus can help them deal with more diverse and complex tasks as agents.\nDespite the rapid development of LLMs and the extraordinary abilities they have, the abilities of LLM agents in downstream tasks are often restricted by the amount of high-quality data. However, most data is stored locally and privately thus preventing LLMs from absorbing more data to improve their performance. Federated learning (FL) has emerged as a promising approach, enabling the collaborative improvement of models across multiple clients without the direct exchange of private data. In FL, knowledge sharing among diverse clients is facilitated through the aggregation of model weights thus protecting privacy.\nHowever, training LLM agents in FL leads to significant challenges for real-world deployment. The first challenge is the mismatch between the high bandwidths consumption and modern communication system. Popular LLMs like LLaMA3.1-405B (Dubey et al. 2024) need more than ten hours to transmit between two distributed nodes under A typical 100 Mbps communication network. The LLMs' parameter sharing in FL between clients and the central server in each communication round is a huge burden for modern communication systems. The second challenge is the mismatch between high computation consumption and modern computation hardware. Popular modern LLMs usually have billions of parameters which are thousands of times larger than traditional models while the development of hardware cannot catch up with it. Training LLMs is computationally intensive, resulting in prolonged training times and significant costs due to the necessity of acquiring GPUs with high processing power and large memory capacities. The third challenge is the heteogeneious data distribution on different clients. Different clients may have non-IID data distribution because of the differences in user geographic location or industry. This can harm the aggregation of model parameters (Zhao et al. 2018) and further complicate the FL process.\nThe in-context learning ability of LLM sheds light on the federated training of LLM agents. With the substantial increase in the parameter size of language models, LLMs demonstrate a remarkable ability to comprehend the provided context and can enhance their performance when additional knowledge is included in the context. This suggests a straightforward approach to enhancing the tool-using capabilities of LLM agents by providing a variety of examples that include instructions, corresponding tools, and input parameters necessary for utilizing these tools within the context, thereby enabling LLM agents to learn from these"}, {"title": null, "content": "instances. The in-context learning capability of LLMs allows them to acquire knowledge extensively from the natural language within the provided context. By integrating this capability into FL, we can transmit natural language context rather than the LLM's cumbersome parameters, thereby significantly reducing communication costs. However, applying in-context learning in FL can lead to the leakage of user privacy as it often requires data samples from different clients to be collected and presented in the context during aggregation. Addressing how diverse LLM agents can access knowledge via in-context learning while safeguarding privacy in FL remains an unsolved challenge in prior research.\nIn this paper, we propose a novel privacy preservative Federated In-Context LLM Agent Learning (FICAL) algorithm to fill this gap which is to our best knowledge for the first time unleashing the power of in-context learning in FL of LLM agents to address these challenges. Different from all previous traditional FL algorithms which transmit model parameters every communication round, in FICAL, we novelly design a Knowledge Compendium Generation (KCG) module to generate knowledge compendiums that contain tool usage knowledge that is transmitted and aggregated. This novel design enables FICAL to have a communication consumption of O(1) while traditional parameter-sharing FL algorithms have a communication consumption of O(N) with respect to model size. The extraordinary communication performance guarantee also demonstrates that FICAL has significant advantages in scalability as the trend towards larger model parameter sizes continues into the future. Furthermore, we design a Retrieval Augmented Generation (RAG)-based Tool Learning and Utilizing (TLU) module to enable the LLM agent to learn how to use tools through a long-context aggregated knowledge compendium. This TLU module addresses the scalability challenges encountered when FICAL supports a substantial number of clients. Such a situation can lead to an excessively long context in the aggregated knowledge compendium, which may adversely affect the performance of the LLM agent and potentially exceed its maximum context length.\nWe consider the FL scenario of multiple clients owning local LLM agents and private data of tool-using instances of different tools cooperatively to train a global LLM agent. Our design consists of the following steps. (1) Each client generates a local knowledge compendium through a novelly designed LLM-enhanced Knowledge Compendium Generation (KCG) module based on their local datasets and transmits it to the central server. The local knowledge compendium contains information such as the usage scenario of the tools, precautions for using the tools, coordination of different tools, etc. (2) The central server receives knowledge compendiums collected from different clients, it aggregates them to form a global knowledge compendium and sends them back to clients. This global compendium contains knowledge that can teach the LLM agent to use tools and is privacy-protective because it is generated to describe the information of tools rather than previous methods that generate synthetic data. These methods often leak the in-"}, {"title": null, "content": "formation of private data distribution and are prone to be attacked (Slokom, de Wolf, and Larson 2022). (3) Clients receive the global knowledge compendium, they use it as teachers, learn how to use corresponding tools for different queries, and invoke tools through a novel RAG-based Tool Learning and Utilizing (TLU) module.\nIn summary, the main contributions of this paper can be summarized as follows:\n\u2022 We propose a novel one-round communication-efficient, computation-efficient, and privacy-preservative FL algorithm called Federated In-Context LLM Agent Learning (FICAL), which is to our best knowledge the first work to unleash the power of in-context learning in FL of LLM agents.\n\u2022 In our design, privacy-preserving local knowledge compendiums which are generated by a novelly designed LLM-enhanced KCG module are transmitted instead of the model parameters in traditional FL. FICAL achieves a communication efficiency of O(1) complexity, irrespective of model size, whereas traditional FL incurs a linear O(N) overhead, scaling with the model size.\n\u2022 We design a Retrieval Augmented Generation (RAG)-based Tool Learning and Utilizing (TLU) module to overcome the long-context issue in knowledge compendium learning and improve the accuracy by 7.6%.\n\u2022 We have conducted extensive experiments on different scenarios, and results show that FICAL can achieve competitive results compared to other SOTA baselines with 3.33 \u00d7 105 times communication costs decrease."}, {"title": "Related Work", "content": "Single LLM Agent Learning\nSeveral works have been done related to LLM agent learning. (Chen, Koenig, and Dilkina 2024) introduce a novel method to enhance LLM's ability to plan within specific domains, which employs \"gradient descent\" to optimize the step-by-step instructions within the prompt of LLM agents, using the chat history from interactions with these agents. (Biderman et al. 2024) aim to predict which sequences will be memorized before the complete training of a large model by extrapolating the memorization behavior from lower-compute trial runs, enabling us to offer equi-compute recommendations to maximize the reliability of these predictions. (Madaan et al. 2024) introduce an approach for enhancing initial outputs from LLMs through iterative feedback and refinement, utilizing a single LLM as the generator, refiner, and feedback provider.\nResource efficient LLM Learning\nWith the advancing capabilities of LLMs, in-context learning has become a new paradigm in natural language processing. (Wei et al. 2023) present a method of fine-tuning language models on in-context input-label pairs where natural language labels (e.g., \"positive/negative sentiment\u201d) are replaced with arbitrary symbols (e.g., \"foo/bar\"). This approach boosts performance on unseen in-context learning tasks and provides greater robustness to unspecified"}, {"title": null, "content": "prompts. (Liu et al. 2022) propose a strategy of selecting in-context learning examples to formulate its corresponding prompt based on similarities between queries and examples where selected examples may serve as more informative inputs intuitively.\nRetrieval Augmented Generation (RAG) (Lewis et al. 2020) enables LLMs to interact with an external large dataset to enhance their performance by retrieving related knowledge. (Asai et al. 2024) propose a self-RAG algorithm that utilizes the self-reflection of the LLM to help them improve their performance to avoid unnecessary information in the RAG process. (Kim et al. 2023) propose a Tree of Clarification (TOC) algorithm that can effectively deal with the problem of ambiguity in the open domain by adopting recursive construction of ambiguity resolution trees and self-validation pruning methods.\nFL with LLMS\nSeveral works have been done related to FL with Large Language Models. (Zhang et al. 2023) explore the performance and the resource consumption of popular parameter efficient tuning methods such as LoRA (Hu et al. 2022), adapter (Houlsby et al. 2019), and prefix tuning (Li and Liang 2021) under various FL settings. (Sun et al. 2024b) improve the LoRA (Hu et al. 2022) method by fixing the randomly initialized non-zero matrices when differential privacy (DP) is added in FL to guarantee user privacy. (Sun et al. 2024a) reduce the communication cost by training optimal prompts and utilizing gradient-free optimization methods. (Peng et al. 2024) propose the sub-FM construction module and the sub-FM alignment module to enhance the performance of FL."}, {"title": "Motivation", "content": "Existing LLMs are often very bulky with a parameter size of over one Billion and have a rapidly rising trend. In Figure. 1, the grey line shows the parameter size of historically"}, {"title": "Methodology", "content": "Problem Statement\nIn this paper, we consider a FL scenario with a total number of N clients and a total number of M types of toolsets, each toolset consists of several tools in a similar domain. Each client i has its own dataset Di which consists of tool-using instances. Different clients' datasets may consist of instances of different tools as in practice different participants in FL may only have access to data from certain areas due to constraints of their respective industries (medical, education, sports, etc.).\nPreliminaries on In-Context Learning\nDifferent from previous neural networks, LLMs can learn knowledge from the natural language context in the prompt given to them without changing the weights of their parameters (Brown et al. 2020). This novel approach to learning, coined \"in-context learning,\" has garnered significant attention as one of the emergent capabilities of LLMs (Wei et al. 2022), setting them apart from traditional neural network architectures. Consequently, the in-context learning approach results in significant reductions in computational power and GPU memory usage, enhancing the accessibility and efficiency of these models for real-world deployment.\nPreliminaries on Retrieval Augmented Generation\nDespite the impressive in-context learning capabilities of LLMs, they encounter significant challenges when processing long contexts. As the context length increases, these models may struggle to allocate attention effectively. The Self-Attention mechanism, which relies on focusing on relevant contextual information, can become compromised in long sequences where pertinent information may be diluted or overshadowed by less relevant data, resulting in a dispersion of the LLM's focus. (Song, Zheng, and Luo 2024). The RAG process is as follows. When receiving a query, first a retrieval model is used to extract the related information in the vector database with a large amount of data and then pass it to LLMs. Finally, the LLM answer the query equipped with the extracted knowledge from the vector database. This methodology enables LLMs to harness the insights of extensive data without grappling with the complexities associated with long-context processing.\nTraditional FL\nIn traditional FL, clients transmit their model parameters to the central server in every communication round. After all the parameters are received, the server aggregates them to form a global model and sends it back to clients. After several communication rounds, the final global model training will be done. The learning goal of traditional FL can be expressed as\n$\\min C = \\frac{1}{N} \\sum_{i=1}^{N} E_{(xi, Yi)~di} Li (Xi, Yi; Wi)$"}, {"title": null, "content": "where Li represents the loss function of client i, (xi and Yi represents the private data and corresponding answer of client i, wi denotes the parameters of the LLM of client i and di denotes the data distribution of the private data of client i.\nFICAL\nThe detail of our method is depicted in Figure 2. The whole process of FICAL can be divided into three parts.\nPart one is the generation and uploading of the knowledge compendium. In this part, each client generates its own local knowledge compendium, denoted as i based on its unique local dataset to extract knowledge about how to use the tools. This knowledge extraction procedure is conducted by a novelly designed (Knowledge Compendium Generation) KCG module whose core is an LLM (In this work, we use the DeepSeek-v2 (DeepSeek-AI et al. 2024) as the generator) which we called Lao-tzu as it generates knowledge compendiums to help LLM agents improve in accordance with the idea proposed by Lao-tzu.\nWe carefully design a knowledge generation prompt and feed it into Lao-tzu to help its' generation. More specifically, Figure 3 shows the design of the knowledge compendium template. In our template, we put the local datasets which consist of instructions from the users, and the corresponding answers that include the name of the tool to be used and input parameters to correctly use the tool as examples. Given these examples, Lao-tzu is going to generate the usage of the tools. This step involves refining knowledge to extract useful data specific to the characteristics of tools from the information observed in examples. The extraction of knowledge about tools offers significant benefits, as the information generated does not contain private user information (such as personal privacy data), yet it aids LLM agents in learning how to use the tools. The first part of the generated usage is the detailed description of the tool, denoted as desi which can help users understand the main functions and uses of the tools clearly, so as to determine whether they meet their needs. The second part denoted as appi is the application scenarios in which the tools should be used. This part can help users understand the effectiveness of tools under certain circumstances, and ensure that the appropriate tools are selected to solve specific problems. By understanding the best times and occasions, users can use tools more efficiently to avoid unnecessary attempts. The third part denoted as prei is the precautions of the usage of the tool that should be noticed. This part guides how to use the API efficiently to avoid wrong requests, thereby improving application performance. Also, it can provide common errors and processing methods to reduce problems that occur when utilizing the tool. The fourth part denoted as cooi, is the coordination of different tools. This part can help the tool user to know how to solve the problem through chain calls to tools. The final knowledge compendium can be represented as\n$\\check{S}_i = \\{des_i, app_i, pre_i, coo_i\\}$"}, {"title": null, "content": "Part two is the global knowledge compendium generation and offloading. After local knowledge compendiums are generated, clients subsequently transmit them to the central server. upon receiving different clients' knowledge compendiums, the central server concatenates them to form a global knowledge compendium g which can be represented as\n$\\check{S}_g = \\{\\check{S}_1, \\check{S}_2, ..., \\check{S}_n\\}$"}, {"title": null, "content": "and then sends it back to clients. This global knowledge compendium contains information on how to use diverse tools collected from different clients.\nPart three is the learning and using of tools. When clients collect the global knowledge compendium they first transform them into a vector form using embedding models and then store it on the vector database. In this work, we use the bge-large-en-v1.5 (Xiao et al. 2023) as the embedding model. This processed knowledge compendium will act as an external teacher to help LLM agents learn how to use tools. The content of the global knowledge compendium is typically extensive, comprising information gathered from all clients, which poses significant challenges for LLM agents attempting to assimilate this data when directly incorporated into the prompt. This highlights the importance of extracting the necessary information from the global knowledge compendium to answer queries. By extracting relevant information, we can avoid interference from redundant information in the allocation of attention in the attention mechanism, ensuring that useful information for answering queries receives adequate attention.\nWe design a novel RAG-based Tool Learning and Utilizing (TLU) module to boost the performance of tool learning. When there is a query to the LLM agent, first the query is converted into a query vector, and then the generated query vector is used to retrieve the most relevant information in the vector database which contains the information on the usage of tools that may be useful to answer the query by calculating the similarity. The extracted related information encompasses knowledge that can instruct the LLM agent in the application of the tools relevant to the query, thereby enhancing its tool learning and utilizing performance."}, {"title": "Experiments and Analyses", "content": "In this section, we compare the performance of FICAL to other SOTA baselines under various settings to prove the effectiveness of our algorithm."}, {"title": "Experiments Settings", "content": "Datasets We generate the dataset utilizing the dataset generation method from (Tang et al. 2023). We select M tool sets from Public APIs2, each tool set consists of a set of tools in a similar domain (for example, a tool set related to dog has tools that can return information of dog types, return how to feed dogs, etc.). The interaction of tools and the LLM agent is in a simulation environment where an external LLM acts as a simulator to simulate the return from the tools. The simulator can get information about the tools and use it as a basis for generating responses. Traces generated from the interaction of the LLM agent and the tools will be fed to a judge LLM (we use the DeepSeek-v2 (DeepSeek-AI et al. 2024) in our experiment) to evaluate whether the LLM agent uses the tools correctly. The default number of clients is 5, and the default number of toolsets is 5. We use the 4-bit NF4 quantization as the default quantization setting and we assert LoRA parameters to the LLM and only fine-tune LoRA parameters in the baselines to better fit the resource-limited scenario. Extra training on an irrelevant dataset is done on FICAL to improve the LLM agent's ability to follow the output format. All baselines are trained for 50 communication rounds and each client conducts 5 epochs of local training"}, {"title": null, "content": "Baselines To verify the effectiveness of the FICAL algorithm, we compare the successful rate of tool-using with the following state-of-the-art (SOTA) FL baselines.\n\u2022 FedACG (Kim, Kim, and Han 2024): FedACG is a FL algorithm that enhances convergence and stability by broadcasting a global model with a lookahead gradient. (CVPR 2024)\n\u2022 FedDecorr (Shi et al. 2023): FedDecorr introduces a regularization term during local training that promotes uncorrelated dimensions in the representations. (ICLR 2023)\n\u2022 FedLoRA (Peng et al. 2024):FedLoRA is a FL approach that offers a comprehensive empirical study of tuning methods for pre-trained language models in federated environments. (ACL 2023 Findings)\n\u2022 FedAdam (Reddi et al. 2021): FedAdam is a FL optimization algorithm that adapts the Adam optimizer for distributed training. (ICLR 2021)\n\u2022 FedYogi (Reddi et al. 2021): FedYogi is an FL optimization algorithm that extends the Yogi optimizer to the federated setting. It balances the stability of adaptive gradi-"}, {"title": null, "content": "ent methods with the robustness of non-iid data distributions and varying client capabilities, (ICLR 2021)\n\u2022 FedProx (Li et al. 2020): FedProx introduces a proximal term in the optimization objective to stabilize training and improve convergence (MLSys 2020)"}, {"title": "Performance Evaluation", "content": "Comparison of Communication Resource Consumption\nTable 3 presents a comparison of the communication resource consumption across various algorithms. From the results, we can conclude that FICAL achieves the least communication resource consumption. It can save the communication resource by 3.33 \u00d7 105 times compared to other baselines. This is because of the novel knowledge compendium transmission instead of the model parameter sharing in traditional FL which is fatal for models like LLMs which have large parameter sizes. Also, FIACL only requires one round of communication while other baselines always require multiple rounds to converge. Moreover, in traditional FL methods, communication overhead continues to increase linearly with the growth of model parameters, rendering these methods increasingly susceptible to the rising communication cost in the future. In contrast, our algorithm demonstrates that communication overhead stays the same with the expansion of model parameters, underscoring the significant potential of the FICAL algorithm in the future.\nResults when each client has one toolset's data We consider the case when each client has one unique toolset and test the accuracy of LLM agent invocating tools. From Figure 4 we can observe that under the default settings, FICAL achieves the highest accuracy of 57.61% while other baselines achieve accuracies from 26.09% to 43.48% which is 14.13% to 31.52% lower than our algorithm.\nResults when each client has multiple toolsets' data\nWe further consider the case that each client has the data of multiple toolsets. More specifically, we consider the case that each client owns two toolsets. Results in Table 2 show that FICAL has an accuracy gain of 13.09%, 14.02%, 16.82%, 9.35%, 14.02%, 13.09% compared to other baselines. From these outcomes, we can conclude that FICAL can perform well under different heterogeneous tool data-owning situations which can prove the robustness of our algorithm.\nResults at different quantization level We conduct experiments on different quantization levels of the LLM to verify the effectiveness of FICAL. More specifically, we test the performance of different algorithms under 4-bit and 8-bit quantization using the data formats NF4 and NF8. We can conclude from Figure 4 that When 4-bit quantization"}, {"title": null, "content": "is used, FICAL has an accuracy of 57.61% and an accuracy of 44.60% when the number of clients is 5 and 8. Other baselines have an accuracy from 25% to 43.47% and from 24.49% to 44.60%. When 8-bit quantization is used, FICAL has an accuracy of 71.74% and 53.24% while other baselines have an average accuracy of 60.32% and 48.80%. We have discovered that the overall performance under the 8-bit quantization is better than that under the 4-bit quantization, which infers that although quantization can lead to saving in memory, it can cause performance drops due to the loss of precision may cause inaccuracy and even gradient vanishing/explosion in the forward propagation.\nImpact of Number of clients To evaluate the performance of the different algorithms under different scales, we test them under different numbers of clients participating in FL. We conduct experiments when there are 5 clients and 8 clients. Results show that FICAL achieves competitive accuracy under different scales.\nPerformance comparison with and without RAG Table 4 presents the accuracy of FICAL when employing the RAG-enhanced tool learning module, as well as its performance without the RAG process, which involves directly incorporating the content of the global knowledge compendium into the prompt to instruct LLM agents on tool usage. From the results, we can observe that the FICAL with RAG has an accuracy gain of 7.6% which demonstrates the effectiveness of the TLU module design."}, {"title": "Conclusion", "content": "In this paper, we propose a novel privacy-preserving Federated In-Context LLM Agent Learning (FICAL) which as far as we know the first work to unleash the power of in-context learning in FL of LLM agents. We design a LLM-enhanced KCG module to generate privacy-preserving knowledge compendiums on clients and send them to the central server to aggregate into a global knowledge compendium. We additionally design a RAG-based TLU module to enable LLM agents to learn and utilize corresponding tools upon receiving a query. FICAL cut down the communication consumption from O(N) with respect to model size in previous FL methods to O(1), which demonstrates its' tremendous potential in future LLM FL research. We have conducted extensive experiments and results show that FICAL has competitive performance with a communication cost decrease of 3.33 \u00d7 105 times."}]}