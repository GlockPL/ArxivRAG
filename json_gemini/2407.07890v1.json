{"title": "Training on the Test Task\nConfounds Evaluation and Emergence", "authors": ["Ricardo Dominguez-Olmedo", "Florian E. Dorner", "Moritz Hardt"], "abstract": "We study a fundamental problem in the evaluation of large language models that we call training on the\ntest task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training\non the test task is not a malpractice. Rather, the term describes a growing set of techniques to include\ntask-relevant data in the pretraining stage of a language model. We demonstrate that training on the test\ntask confounds both relative model evaluations and claims about emergent capabilities. We argue that the\nseeming superiority of one model family over another may be explained by a different degree of training\non the test task. To this end, we propose an effective method to adjust for training on the test task by\nfine-tuning each model under comparison on the same task-relevant data before evaluation. We then show\nthat instances of emergent behavior largely vanish once we adjust for training on the test task. This also\napplies to reported instances of emergent behavior that cannot be explained by the choice of evaluation\nmetric. Our work promotes a new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.", "sections": [{"title": "Introduction", "content": "The machine learning community has long recognized certain clear violations of the benchmarking protocol.\nTraining on the test set is the most notorious among them (Duda and Hart, 1973; Hastie et al., 2017; Hardt\nand Recht, 2022). Data leakage (Kapoor and Narayanan, 2022) and data contamination (Roberts et al., 2023;\nJiang et al., 2024) are closely related problems linked to the rise of massive web-crawled training datasets.\nResearchers can all agree that test data should never appear in the training set.\nBut it's been much less clear what to do about legitimate attempts to bring training closer to evaluation.\nThere is an obvious a gap between next token prediction at training time and tasks, such as reasoning\nand question answering, at test time. Ongoing research and engineering efforts, in fact, aim to narrow\nprecisely this gap (MetaAI, 2024; Lewis, 2024). Why shouldn't training be informed by knowledge about the\ndownstream test tasks? What's an unfair advantage of some may be the feature of others.\nIn this work, we group strategies to utilize task knowledge at training time under the umbrella term of\ntraining on the test task. Examples of training on the test task include the use of instruction-tuning data or\nquestion answering templates during pre-training (Zeng et al., 2022; Bai et al., 2023). We work from the\npremise that training on the test task is acceptable or at least, unavoidable.\nIn a nutshell, we show that training on the test task strongly confounds model comparisons across different\nscales and model families. Moreover, it significantly obscures the study of emergent capabilities of large"}, {"title": "1.1 Our contributions", "content": "We introduce the term training on the test task to group a growing repertoire of practices that utilize knowledge\nabout evaluation tasks at training time. We study its impact on benchmark evaluations by inspecting 53\ndifferent language models in two major active benchmarks, MMLU and GSM8K.\nWe start in Section 2 by dividing models into those trained before November 2023 and those trained\nafter. We find that for the same amount of compute, newer models outperform older models on average by 7\npercentage points in MMLU and 17 points in GSM8K. We then fine-tune all models on the same amount of\ntask-specific data before evaluation. We show that after fine-tuning, newer models no longer outperform"}, {"title": "2 Adjusting for training on the test task", "content": "We choose MMLU (Hendrycks et al., 2020) and GSM8K (Cobbe et al., 2021) as a case study for investigating\ntraining on the test task in active benchmarks. MMLU tests for world knowledge, whereas GSM8K tests\nmulti-step mathematical reasoning. These two benchmarks are very prominent in the literature at present\ntime. For instance, GPT 4 (Achiam et al., 2023), Claude 3 (Anthropic, 2024), Gemini (Gemini et al., 2023)\nand Llama 3 (MetaAI, 2024) all report and highlight MMLU and GSM8K. They are also included in the\nHuggingFace (HF) Open LLM Leaderboard* (Beeching et al., 2023), a popular benchmark leaderboard that\nevaluates and ranks models with publicly available weights. We evaluate models using LM Evaluation Harness\nlibrary (EleutherAI, 2024), in identical fashion to the HF leaderboard."}, {"title": "2.1 Adjusting for training on the test task by training on the test task", "content": "We propose to adjust for differences in test task training by fine-tuning all models on the same, sufficient\namount of task-specific data before evaluation. To do so, we need a source of task-specific data for each of\nthe tasks we consider. For multiple choice questioning answering (MMLU), we use the auxiliary training set\naccompanying the HF MMLU repository. It contains around 100,000 training examples and around 30M\ntokens. For mathematical reasoning (GSM8K), we combine the the MetaMathQA (Yu et al., 2023b) and\nOrca-Math (Mitra et al., 2024) datasets, totalling 600,000 training examples and approximately 200M tokens.\nWe fine-tune models for three epochs using standard hyperparameter choices, with minimal hyperparameter\ntuning, see Appendix A.2. Note that the amount of compute required for fine-tuning is minimal in comparison\nto the compute required for pretraining, since all models considered were pretrained on at least 300B tokens.\nWe plot model scores on MMLU and GSM8K after fine-tuning in Figure 1 (bottom). We observe that\nafter fine-tuning on task relevant data, newer models no longer Pareto dominate in terms of accuracy per\npretraining compute. Instead, benchmark performance is strongly correlated with compute and both newer\nand older models follow remarkably similar scaling trends. That is, newer models no longer appear to\noutperform older models. Moreover, we observe that older models tend to benefit from training on the test\ntask much more than newer models, see Figure 2. The improvements of older models are striking, often\njumping from random chance accuracy to double digit improvements in accuracy. In contrast, fine-tuning\nbrings comparatively little benefit to newer models. This observation suggests that newer models have already\nbeen trained on a substantial amount of task-relevant data."}, {"title": "2.2 Quantifying performance differences between newer and older models", "content": "We draw inspiration from scaling laws (Kaplan et al., 2020) in how we model benchmark accuracy A to\nscale log-linearly with pretraining compute C. To account for emergence (Wei et al., 2022), we assume that\n\\begin{equation}\nA = \\alpha \\max(0, \\log C - c_e) + \\theta N + r + \\epsilon,\\nonumber\n\\end{equation}\nwhere \\( \\alpha, \\theta \\) and \\( c_e \\) are the fit's parameters, and \\( \\epsilon \\) is random noise. We focus on the coefficient \\( \\theta \\), which\ncorresponds to the average difference in benchmark performance between newer and older models after\ncontrolling for pretraining compute. We fit the model in Equation 1, and report the regression coefficient\n\\( \\theta \\) in Figure 1. We obtain \\( R^2 > 0.9 \\) for all model fits. Before adjusting for test task training, the estimated\ndifference in performance \\( \\theta \\) between newer and older models are statistically significant, positive, and large.\nSpecifically, recent models on average outperform older ones by over 7 accuracy points in MMLU and 17\naccuracy points in GSM8K. These are remarkable differences in benchmark performance, as small single digit\nimprovements are typically considered substantial improvements by the literature.\nWe repeat the analysis but using models' adjusted benchmark scores, that is, those obtained after fine-\ntuning on the test task. After adjusting for test task training we find no evidence for a significant difference\nin benchmark performance between newer and older models. That is, the estimated coefficient \\( \\theta \\) is both\nsmall and not statistically significant. Put simply, newer models no longer outperform older ones. Therefore,\nconditioned on all models training on the same, sufficient amount of task-specific data before evaluation,\nthere are no differences in benchmark performance between newer and older models.\nOur findings provide evidence that the differences in benchmark performance between newer and older\nmodels are largely attributable to differences in test task training. We present a causal interpretation of our\nresults in Appendix B, outlining the causal assumptions needed to establish that the improvements of newer\nmodels are attributable to training on the test task. Overall, we find no evidence for the improvements in\nperformance of newer models being attributable to anything other than training more on the test task."}, {"title": "3 Recreating differences in benchmark performance", "content": "Previously, we introduced a way to adjust for training on the test task. Here we systematically test the validity\nof this adjustment method. To do so, we demonstrate how to recreate the observed differences in performance"}, {"title": "3.1 Fine-tuning on the test task", "content": "For this section, we only consider models trained before November 2023, since we hypothesize that older\nmodels do not train on the test task much. We randomly split models into two cohorts: a control group and\na treatment group. We fine-tune the treatment group on the datasets of task-relevant data introduced in\nSection 2. We fine-tune on each dataset independently, for a single epoch. We then evaluate the benchmark\nperformance of the two cohorts, as well as their performance after adjusting for test task training. As in the\nprevious section, we adjust for test task training by fine-tuning all models on the test task before evaluation.\nWe plot in Figure 3 the two cohorts' benchmark performance before and after the adjustment. We repeat\nthe statistical analysis of Section 2.2 and report the estimated coefficient \\( \\theta' \\) indicating the average difference\nin benchmark performance between the two cohorts when controlling for compute.\nFine-tuning the treatment group results in large differences in performance between the control group and\nthe treatment group, see Figure 3 middle. Qualitatively, the differences between the control and treatment\ngroup resemble those observed between newer and older models in Section 2.2. In particular, the fine-tuned\nmodels Pareto dominate the non fine-tuned models. Quantitatively, the estimated increase in performance\n\\( \\theta' \\) due to fine-tuning is statistically significant and large. Importantly, it is also similar to the difference\nin performance \\( \\theta \\) between newer and older models estimated in Section 2.2. Therefore, fine-tuning older\nmodels on the test task gives rise to qualitatively and quantitatively similar confounding to that observed\nbetween newer and older models. This is consistent with our running hypothesis that newer models are\nlargely equivalent to older models that trained on the test task.\nAfter adjusting for test task training by further fine-tuning both the control and treatment groups on the\ntest task, we observe that models in the treatment group are no longer outliers in terms of performance-\nper-compute, see Figure 3 right. Quantitatively, the estimated increase in performance \\( \\theta' \\) is both small and"}, {"title": "3.2 Reformulating the test task", "content": "In this section we consider two additional benchmarks from the HF leaderboard: ARC Challenge (Clark\net al., 2018) and HellaSwag (Zellers et al., 2019). Similarly to MMLU, ARC is comprised of grade-school\nlevel questions. HellaSwag instead tests for commonsense reasoning. Like MMLU, the questions in ARC and\nHellaSwag are accompanied by four possible answers, among which the model must differentiate the correct\none. The standard MMLU evaluation formulates questions as multiple-choice: all four answer choices are\nlisted, and the model is promoted to pick one. In contrast, ARC and HellaSwag use \u201ccloze\u201d evaluations: a\nmodels' answer is taken to be that with the largest completion likelihood given the input question.\nWe evaluate all models on ARC and HellaSwag using the standard cloze evaluation, and plot their\nbenchmark performance in Figure 4 left. We repeat the statistical analysis of Section 2.2, and report the\naverage difference in performance \\( \\theta \\) between newer and older models after controlling for pretraining\ncompute. Qualitatively, we observe that older models and newer models have very similar scaling trends.\nQuantitatively, the estimated difference in performance between newer and older models \\( \\theta \\) is small and not\nstatistically significant. That is, newer models do not outperform older models on ARC and HellaSwag.\nWe then reformulate ARC and HellaSwag as MMLU-style multiple-choice questions, and plot the resulting\nbenchmark performance in Figure 4 center. We observe large differences in performance between newer\nand older models. Qualitatively, these differences in performance resemble those observed for MMLU. In\nparticular, newer models Pareto dominate in terms of performance-per-compute. Quantitatively, we find the\ndifference in performance \\( \\theta \\) between newer and older models to be significant, positive, and large, and to be\nroughly similar in magnitude to that estimated for MMLU in Section 2.2. Therefore, reformulating the test\ntask as multiple choice question answering leads to similar confounding to that observed for MMLU. This\nsuggest that what causes the outliers in MMLU is likely not memorization of specific testing data (i.e., due to\ndata contamination or leakage), but rather an improved ability for MMLU-style prompts.\nWe adjust for test task training by fine-tuning all models on the MMLU auxiliary training set, and plot\ntheir ARC Challenge and HellaSwag scores in Figure 4. We observe that newer models are no longer outliers\nin terms of performance-per-compute. Moreover, we no longer find evidence of a significant difference in\nperformance between newer and older models. The proposed adjustment is therefore effective in removing"}, {"title": "4 Implications for model comparisons", "content": "Our findings indicate that training on the test task acts as a major confounder of LLM benchmark evaluations.\nWe now discuss its implications for the relative comparison of model families (Section 4.1), as well as its\nimplications for measuring progress in model capabilities over time (Section 4.2)."}, {"title": "4.1 Comparing model families", "content": "We compare the MMLU and GSM8K performance of the Pythia, Llama 2, and Qwen 1.5 model families, which\nlikely train on the test task to very different extents. Pythia was trained on the Pile (Gao et al., 2020), a\ncollection of curated datasets that are unlikely to contain much test task data. Llama 2 was trained mostly\non web data, which is reasonable to assume may contain test task data. Lastly, Qwen 1.5 explicitly includes\ninstruction data in its pretraining mixture, thus likely training on the test task to a large extent.\nIn Figure 6 we plot the MMLU and GSM8K scores of the Llama 2, Qwen 1.5, and Pythia families of\nmodels, as well as their adjusted accuracy (i.e., after fine-tuning on task relevant data). Without adjustment,\nQwen 1.5 appears to be the superior model family: it Pareto dominates both the Llama 2 and Pythia models.\nFurthermore, all Pythia models perform no better than random chance, and thus it is unclear what benefit\nscaling Pythia might bring. After fine-tuning the models on the the test task, however, all three model families\nexhibit very similar scaling trends. Therefore, when correcting for the confounding introduced by test task\ntraining it is unclear if any of the model families is superior to the others beyond their pretraining compute.\nInterestingly, recent work equates pretraining data quality with downstream benchmark performance (Penedo\net al., 2024; Li et al., 2024). For example, the Pile (Pythia's pretraining dataset) is thought to be inferior\nto filtered web data. In light of our findings, it is plausible that a major contributing factor to the superior\nperformance of \"higher quality\" pretraining datasets is that they contain a larger share of test task data."}, {"title": "4.2 Progress in model capabilities", "content": "One purpose of benchmarks is to track progress in model capabilities. In Figure 7 we plot the Pareto frontier\nof benchmark accuracy against pretraining compute, both for models trained before November 2023 and\nfor all models. We measure progress by considering the area of improvement of the Pareto frontier since\nNovember 2023, shaded in green. Without adjustment, the difference between the two Pareto frontiers\nis rather large for both MMLU and GSM8K, indicating substantial progress since November 2023. After\nfine-tuning models on the test ask, however, the area of improvement reduces by a sixfold. Therefore, the\nconfounding introduced by test task training leads to substantially overestimating the progress in MMLU and\nGSM8K capabilities per unit of compute achieved by recent model families.\nOn the other hand, recent models tend to be trained on more data. Given the Chinchilla scaling laws (Hoff-\nmann et al., 2022), it is remarkable that newer, smaller models match the performance of older, larger ones for\nthe same amount of pretraining compute. Since inference and fine-tuning of smaller models is substantially\ncheaper, recent models can be much more accessible to less well-resourced institutions, with little cost in\nperformance. For example, we find that Llama 3 8B closely matches the performance of Llama 2 70B."}, {"title": "5 Implications for emergence", "content": "Throughout our evaluations, we observe emergent behaviour for MMLU and GSM8K: models perform at near\nrandom chance up to a certain scale of pretraining compute, followed by relatively sharper improvements\nin performance at larger scales (Wei et al., 2022). After training on the test task, however, emergence for\nMMLU and GSM8K appears to occur at substantially lower scales. We dedicate this section to more closely\ninvestigate the relationship between training on the test task and emergence."}, {"title": "7 Discussion", "content": "The 1968 Olympics took place in Mexico City at the significant altitude of 2340 meters, higher than Australia's\ntallest peak. Runners who had trained at altitude in their home countries were better prepared to compete\nin Mexico City's conditions, as it turned out. But the hotly debated results of the Games did not lead the\norganizers to prohibit training at natural altitude. Instead, they let everyone do it; and athletes came to\nconsider altitude training an excellent way to train.\nThe anecdote holds a lesson for the evaluation of large language models half a century later. Knowledge\nabout the evaluation conditions necessarily influences training practices under competitive pressure. It may\nbe a fool's errand to prohibit the practice. Instead, we propose to adjust for it by giving every model the same\ntask-specific preparation before evaluation. We work from the assumption that training on the test task, in\ngeneral, cannot be effectively detected, disallowed, or disincentivized. Detecting what training data a model\nhas seen is a notoriously difficult problem\u2014existing heuristics achieve partial success at best. Researchers\nroutinely acknowledge the futility of fighting data contamination. Moreover, we anticipate that the ways to\neffectively train on the test task will only grow in scope and adoption.\nOur work demonstrates that comparisons of different models are confounded by the choice of training data\nand training practices. Different model families vary in the degree that they were\u2014implicitly or explicitly-\ntrained on various test tasks. It therefore makes little sense to compare model performance at face value\nwithout accounting for how the training data relate to the test task. The same problem extends to scaling.\nSmaller models can appear unexpectedly performant if they were trained to a greater extent on task data.\nWe can apply the same principles to emergent behavior. After training on the test task, model capabilities\nbecome predictable at smaller model size and grow continuously with scale. This is not to say that emergence\nisn't real; it may well be a real phenomenon for a fixed choice of dataset and evaluation metric. But training\non the test task removes the unpredictability and discontinuity associated with emergence, notably without\nany change in the metric, thus largely disarming the ominous nature of emergence.\nDespite the daunting challenges that training on the test task poses for the fair evaluation of language\nmodels, it's also its own best remedy. Giving each model the same sufficient task-specific fine-tuning\nharmonizes model comparisons, deconfounds scaling laws, and linearizes the relationship between model\ncapabilities and log-scale. We hope that our work informs stronger evaluation standards that address central\nchallenges in the current evaluation ecosystem. Our proposal has the added side benefit of creating incentives\nfor model builders to create models that can be fine-tuned easily and respond well to fine-tuning."}, {"title": "A Additional experimental details", "content": "Model size in billions of parameters is indicated by N and pretraining data size in trillions of tokens is\nindicated by D. Model weights were retrieved from the corresponding HuggingFace (HF) repositories."}, {"title": "B Causal interpretation of our findings", "content": "In Section 2.2 we demonstrated that models trained after November 2023 significantly outperform those trained before\nNovember 2023 for both MMLU and GSM8K. We now seek to determine how much of the benchmark improvements of\nnewer models is attributable to newer models training more on the test task. That is, the extent to which the effect of\nmodel recency N on benchmark accuracy A is mediated by training on the test task T. The key obstacle to our analysis is\nthat test task training T is unobservable. Firstly, because practitioners are typically not transparent about their designs\nchoices (e.g., pretraining data). Secondly, because the extent to which different training practices might amount to test\ntask training is unclear. However, we are able to intervene on T by fine-tuning on the test task.\n\\begin{equation}\n{\\displaystyle f(C,\\alpha) = \\sum_{i=0}^{a} \\alpha_{i} + \\alpha_{i} \\log C \\cdot [c_{i} \\geq C]}\\nonumber\n\\end{equation}\nFor simplicity, we consider three fixed knots at \\( c_1 = 0, C_2 = 10^{22}, \\) and \\( c_3 = 10^{23} \\) FLOPs. We assume all other variable\nrelationships to be linear, resulting in the structural assignments:\n\\begin{aligned}\nT := f(C,\\beta) + \\phi N + \\delta, \\quad \\delta \\sim \\mathcal{N}(0,\\sigma^2)\\\\\nA := f(C,\\alpha) + \\psi N + \\gamma T + \\eta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\n\\end{aligned}\nWe denote benchmark accuracy after fine-tuning as \\( A|do(T=t) \\). To estimate the direct effect N \u2192 A of model recency on\naccuracy, we regress the linear model\n\\begin{aligned}\nA|do(T=t) &= f(C,\\alpha) + \\psi N + \\gamma t + \\eta + \\epsilon \\\\\n&= f(C,\\alpha) + \\psi N + \\eta' + \\epsilon, \\quad \\eta' = \\eta + \\gamma t\n\\end{aligned}\nwhere \\( \\alpha, \\psi, \\eta' \\) are the fit's parameters and \\( \\epsilon \\) is random noise. The coefficient \\( \\psi \\) corresponds to the direct effect N \u2192 A of\nmodel recency on benchmark accuracy. We additionally regress on the difference in accuracy pre and post intervention\n\\begin{aligned}\nA - A|do(T=t) &= (f(C,\\alpha) + \\psi N + \\gamma T + \\eta + \\epsilon_1) - (f(C,\\alpha) + \\psi N + \\gamma t + \\eta + \\epsilon_2) \\\\\n&= \\gamma T - \\gamma t + \\epsilon_1 - \\epsilon_2 \\\\\n&= \\gamma f(C,\\beta) + \\gamma\\phi N + \\gamma\\delta - \\gamma t + \\epsilon_1 - \\epsilon_2 \\\\\n&= f(C,\\beta') + \\phi'N + b + e', \\quad \\text{for } \\beta' = \\gamma\\beta, \\; \\phi' = \\gamma\\phi, \\; b = -\\gamma t, \\; e' = \\epsilon_1 - \\epsilon_2 + \\gamma\\delta\n\\end{aligned}\nwhere \\( \\beta', \\phi', b \\) are the fit's parameters and \\( e' \\) is random noise. The coefficient \\( \\phi' \\) corresponds to the indirect effect\nN\u2192 T\u2192 A of model recency N on benchmark accuracy A mediated by test task training T (Pearl, 2013). That is, the\nimprovements in accuracy of recent models attributable to training on the test task.\nWe fit the models in Equation 5 and Equation 6, and we report the coefficients pertaining to N \u2192 A and N \u2192 T\u2192A\nin Table 2 and Table 3, respectively. We find no evidence of a significant direct effect N \u2192 A of model recency on accuracy.\nOn the other hand, its indirect effect N \u2192 T\u2192 A mediated by test task training T is significant, positive, and large."}, {"title": "C Results for the OpenLLM Leaderboard v2", "content": "HuggingFace released on June 2024 a revision of the OpenLLM Leaderboard (Fourrier et al., 2024a). The HF leaderboard\nv2 differs from v1 in the six benchmarks it considers: MMLU Pro (Wang et al., 2024), GPQA (Rein et al., 2023),\nBBH (Suzgun et al., 2023), MuSR (Sprague et al., 2023), the Level 5 subset of MATH (Hendrycks et al., 2021), and\nIFEval (Zhou et al., 2023a). MMLU and GPQA test for knowledge and are framed as multiple-choice questions. BBH and\nMuSR test for reasoning. MATH tests for mathematical reasoning. IFEval tests the ability of models to follow instructions.\nThe creators of the OpenLLM Leaderboard cite contamination as a key motivation for releasing the v2 revision. They\nnote that a key criteria in choosing the benchmarks of the HF leaderboard v2 was lack of contamination in models as\nof today. In particular, Fourrier et al. (2024b) claim that current models are not contaminated for GPQA, MuSR, and\nMMLU Pro: GPQA due to the gating of the test set, and MuSR and MMLU Pro due to their \"youth\". Fourrier et al. (2024b)\nsuccinctly express their concern as regards to data contamination in the HF leaderboard v1:"}, {"title": "D Additional figures", "content": "In Figure 16 we show that ARC and HellaSwag do not exhibit emergence when using the standard cloze evaluation.\nWhen reformulating the task as multiple choice in the style of MMLU, however, we observe emergence around 1022 to\n1023 FLOPs, similarly to MMLU. Emergence in this range of compute persists even when changing the evaluation metric\nfrom accuracy to Brier score -a continuous metric-, as suggested by Schaeffer et al. (2024a)."}]}