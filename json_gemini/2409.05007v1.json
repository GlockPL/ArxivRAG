{"title": "Audio-Guided Fusion Techniques for Multimodal Emotion Analysis", "authors": ["Pujin Shi", "Fei Gao"], "abstract": "In this paper, we propose a solution for the semi-supervised learning track (MER-SEMI) in MER2024. First, in order to enhance the performance of the feature extractor on sentiment classification tasks, we fine-tuned video and text feature extractors, specifically CLIP-vit-large and Baichuan-13B, using labeled data. This approach effectively preserves the original emotional information conveyed in the videos. Second, we propose an Audio-Guided Transformer (AGT) fusion mechanism, which leverages the robustness of Hubert-large, showing superior effectiveness in fusing both inter-channel and intra-channel information. Third, To enhance the accuracy of the model, we iteratively apply self-supervised learning by using high-confidence unlabeled data as pseudo-labels. Finally, through black-box probing, we discovered an imbalanced data distribution between the training and test sets. Therefore, We adopt a prior-knowledge-based voting mechanism. The results demonstrate the effectiveness of our strategy, ultimately earning us third place in the MER-SEMI track.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, emotion recognition has garnered attention due to its wide range of applications, such as affective computing[5], healthcare[8], human-computer interaction [3, 21], and market research [30]. Traditional emotion recognition methods utilize physical and physiological signals [13]. These methods rely on specialized instruments, which can be time-consuming and labor-intensive to implement. Multimodal emotion recognition [1], on the other hand, extracts emotional representations from speech, visual and text modalities. It employs fusion mechanisms to integrate multimodal features for downstream emotion classification tasks, offering a more flexible and efficient approach [10].In studying emotions, two primary approaches are commonly adopted: the dimensional approach[22] and the discrete approach [25]. The objective of the MER-SEMI track (MER 2024[18]) is to map samples to six correct emotion labels (worried, happy, neutral, angry, surprise, and sad), which fundamentally falls under the task of discrete emotion label classification. This requires us to maximize the retention of emotional information from each modality during feature fusion and simultaneously minimizing the interference of irrelevant information between different modalities [11, 12].\nIn the MER-SEMI track, there are two primary challenges: (1) How to enhance the robustness ability of models using unsupervised or semi-supervised learning methods in the context of scarce labeled data; (2) How to maximally retain the complementary emotional information across different modalities during multimodal feature fusion; To address (1), various methods have been proposed. For instance, Ding et al. [6] introduced the Video-Audio Transformer, which aligns visual and audio modalities and incorporates contrastive loss for semi-supervised learning, achieving significant results[17]. Similarly, Chen et al. [4] proposed a class-balanced pseudo-labeling strategy to select high-confidence pseudo-labeled samples for each category from unlabeled data. Moreover, You et al.[36] designed a Temporal-Alignment Attention mechanism to align speech and textual cues in a common space and used auxiliary self-supervised tasks to ensure the consistency and coherence of unlabeled data. Additionally, Yang et al. [35] integrated inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, thereby simplifying the training process. To tackle (2), Mittal et al. [23] proposed a learning model based on Long Short-Term Memory (LSTM) networks for emotion perception. In Chen et al.'s work [4], large-scale unlabeled emotional video data were used to train a mask autoencoder (Expression MAE[9]). Further, Wang et al. [31] designed three different structures based on"}, {"title": "2 METHOD", "content": "In this section, we will discuss our proposed multimodal emotion recognition system in three subsections. The selection and fine-tuning of feature extractors constitute the first part, the AGT fusion mechanism constitutes the second part, and the pseudo-label and voting mechanism constitutes the third part."}, {"content": "Feature extractors, such as Hubert-large, CLIP-vit-large, and Baichuan-13B, perform well in single-modality scenarios [18]. However, they tend to exhibit poor generalization in multi-modality settings. We hypothesize that this may be due to the fact that these feature extractors are trained on public datasets, where they may learn redundant information that is not conducive to multimodal sentiment recognition [12]. Therefore, for the audio modality, we ultimately selected Hubert-large due to its superior generalization ability. This model effectively extracts semantic information from raw audio that complements other modalities. Additionally, for the visual and textual modalities, we initially selected CLIP-large and Baichuan-13B as the feature extractors. However, since both of these large models are trained on extensive wild datasets, their use for multimodal emotion recognition introduces a substantial amount of irrelevant information (e.g., background information in videos, emotion-irrelevant information in text). Consequently, we fine-tuned these two feature extractors using the labeled data [18, 20, 32, 33]. The specific fine-tuning steps are illustrated in Figure 2.\nRegarding the visual modality, based on [28], better alignment of visual and textual representations in video tasks provides stronger generalization capabilities. Therefore, we performed alignment operations on 5030 labeled data samples using CLIP-vit-large. As"}, {"title": "2.1 Fine-tuning the feature extractor", "content": "L = -log\\sum_{j} \\frac{exp(sim(v_i, t_i)/\\tau)}{exp(sim(v_i, t_j)/\\tau)} (1)"}, {"title": "2.2 AGT Fusion Mechanism", "content": "As illustrated in Figure 1, we extract features from each modality using the selected feature extractors. Subsequently, we use the semantic features extracted by Hubert to guide visual features and textual features independently, using the Context-Based Transformer (CBT) module [11]. This module primarily extracts deep information within channels. it effectively integrates both global and local information using a multi-head self-attention mechanism, leveraging skip connections and feedforward layers to prevent overfitting and enhance the model's linear representation capabilities. During the fusion process, we employed Adaptive Multimodal Fusion (AMF) [11], which addresses the issue of different modalities conveying disparate emotions. This method utilizes a multi-head self-attention mechanism to calculate the similarity between different modalities. AMF can determine whether a particular modality expresses an emotion distinct from the others. If a specific modality exhibits very low similarity with all other modalities, it is considered to express a different emotion, and its features are thus zeroed out to eliminate interfering information.\nX_f = f_{AMF} (f_{CBT} (cat[A_i, V_i]) + f_{CBT} (cat[A_i, T_i])) (2)\nwhere Xf represents the fused feature, while Ai, Vi, and T\u012f denote the audio, visual, and textual modality features of the (i)-th sample, respectively."}, {"title": "2.3 Pseudo-label and Voting Mechanism", "content": "In order to verify if the distributions of the training and testing sets are consistent (which has implications for the necessity of targeted data augmentation), we calculated the quantity of data for each label in the training set (worried: 616, happy: 1038, neutral: 1248, sad: 730, angry: 1208, surprise: 190), and estimated the approximate distribution of the test dataset through F1-scores obtained by black-box probing (worried: 0.0326, happy: 0.0732, neutral: 0.0505, sad: 0.1157, angry: 0.03412, surprise: 0.0094). As shown in Figure3, the statistical results indicate a significant discrepancy between the distributions of the test set and training set, particularly for the 'sad' and 'worried' labels. Consequently, low-confidence labels are prone to misclassification during pseudo-label generation (e.g., The ground truth label is 'sad', but the predicted result is 'worried).\nBased on this, we first generated pseudo-labels with high confidence (p > 0.9) on the test set using Hubert-large, the baseline"}, {"title": "3 EXPERIMENTS AND RESULTS", "content": "In this section, we conduct ablation experiments and analyze the results. First, we perform comparative experiments on features from two and three modalities, using different feature extractors and fusion mechanisms. Second, we validate the effectiveness of various strategies by incorporating them into the experiments.Note that the evaluation metric for the following experiments is the F1-score.\nIn Table 1, we observe that the fine-tuned feature extractors exhibit significant effectiveness when used in combination, but their performance declines when used individually. We propose a possible reason for this phenomenon: the fine-tuning alignment process may enhance the complementary capabilities between the video and text modalities, but it could also result in some modality competition between the audio and video modalities. Additionally, for the same combinations of feature extractors, the AGT fusion mechanism consistently achieves higher F1-scores on both Train&Val and MER-SEMI datasets compared to the baseline fusion mechanism. This indicates that the AGT fusion mechanism is more effective in capturing complementary information across different modalities during cross-modal feature fusion."}, {"title": "4 CONCLUSION", "content": "In this paper, we fine-tune CLIP-large and Baichuan-13B as feature extractors and propose an Audio-Guided Transformer (AGT) architecture. This fusion mechanism eliminates redundant information between modalities while capturing deeper emotional representations. Additionally, we employ self-supervised learning using pseudo-labels. Given the prior knowledge of the imbalanced distribution between training and test data, we assign higher weights to the more generalizable Hubert-large for imbalanced label predictions and use a voting mechanism to improve model prediction accuracy. Ultimately, these methods helped us achieve a score of 89.83% in the MER-SEMI 2024 track."}]}