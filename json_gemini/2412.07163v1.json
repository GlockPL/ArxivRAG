{"title": "Fast Occupancy Network", "authors": ["Mingjie Lu", "Yuanxian Huang", "Ji Liu", "Xingliang Huang", "Dong Li", "Jinzhang Peng", "Lu Tian", "Emad Barsoum"], "abstract": "Occupancy Network has recently attracted much attention in autonomous driving. Instead of monocular 3D detection and recent bird's eye view (BEV) models predicting 3D bounding box of obstacles, Occupancy Network predicts the category of voxel in specified 3D space around the ego vehicle via transforming 3D detection task into 3D voxel segmentation task, which has much superiority in tackling category out-lier obstacles and providing fine-grained 3D representation. However, existing methods usually require huge computation resources than previous methods, which hinder the Occupancy Network solution applying in intelligent driving systems. To address this problem, we make an analysis of the bottleneck of Occupancy Network inference cost, and present a simple and fast Occupancy Network model, which adopts a deformable 2D convolutional layer to lift BEV feature to 3D voxel feature and presents an efficient voxel feature pyramid network (FPN) module to improve performance with few computational cost. Further, we present a cost-free 2D segmentation branch in perspective view after feature extractors for Occupancy Network during inference phase to improve accuracy. Experimental results demonstrate that our method consistently outperforms existing methods in both accuracy and inference speed, which surpasses recent state-of-the-art (SOTA) OCCNet by 1.7% with ResNet50 backbone with about 3\u00d7 inference speedup. What's more, our method can be easily applied to existing BEV models to transform them into Occupancy Network models.", "sections": [{"title": "1 Introduction", "content": "Perception system plays an important role in modern intelligent or autonomous driving, which usually detects various obstacles and road layout information and transmits these surrounding environmental information to downstream planning module. In the early stage, 2D detection models [25, 34], segmentation models [5,46], and lane line detection models [33,37] are leveraged to build the basic perception systems. With LiDAR-based 3D detection methods [14,45] being introduced to intelligent driving, the perception ability has a significant increase, which leads to a giant leap to autonomous driving. Further the sensor fusion methods [40, 43] combine several observations from multiple sensors to obtain more robust and accurate perception results. However, due to high cost of LiDAR sensors, vision-based perception methods attract much attention and almost adopt BEV representation as a mainstream way in the intelligent driving field over the past years.\nVision-based BEV methods usually take multi-view camera images as inputs and leverage various strategies to perform 2D-to-3D transformation [10,19] and forward the BEV features to various task heads, which usually include 3D detection [11, 18, 19, 27], BEV segmentation [31], and high-definition (HD) map construction [16, 21, 22, 26]. The 3D object detection task focuses on representing objects with rigid 3D bounding boxes, which is difficult to depict the various shapes of obstacles in the real world. BEV semantic segmentation only requires to predict the semantic map at the horizontal level, which provides limited semantic information for intelligent driving. The HD map construction task primarily emphasizes the static elements on the road(i.e., lane divider, road boundaries, etc). However, these tasks only represent the 3D environment in a limited perspective without holistic understanding of the 3D scene. The 3D semantic occupancy [12,39,47] has been proposed to represent the surrounding environmental information at a finer granularity, which can depict obstacles with various shapes, road layout information, and other elements with predefined 3D voxels in real driving scenarios.\nThe 3D semantic occupancy is also known as 3D semantic scene completion (SSC) [36] with taking LiDAR point cloud as input. LiDAR-based methods [6,7,15,35] have achieved outstanding performance resulting from explicit depth measurements of point cloud. Owing to the expensive cost of LiDAR sensors, vision-based 3D occupancy has attracted much attention, which takes multi-view camera images as input with various methods to transform multi-view inputs into voxel-based feature representation for 3D occupancy prediction. TPVFormer [12] obtains the voxel features by representing the 3D space by generalizing the BEV to tri-perspective views. Specifically, a 3D point can be projected into three panels and a voxel feature can be acquired by bilinearly interpolating on the tri-perspective features. Such a decoupled representation allows low lifting cost but may show performance decrease when complete environment features are required. Meanwhile, some methods [39, 42, 47] construct the voxel features by directly generalizing the voxel queries. However, due to the huge search space, all the 3D voxel representation incurs high memory costs and significantly impedes inference speed. Therefore, the solution to effective 3D voxel representation lies in acquiring efficient and effective 3D information while minimizing computational cost. OccNet [39] presents a simple and fast BEVNet, which lifts BEV features to 3D voxel features with a simple MLP layer and obtains a not bad performance with 2 points gap to SOTA method [39]. This phenomenon shows the BEV feature contains quite 3D information in vertical direction and has the potential of achieving higher accuracy with more exploration. We verify several methods involving 2D convolution and 2D deformable convolution via expanding receptive field in the lifting process and 3D deformable cross-attention method to find the best method. Finally, we lift the BEV features to 3D voxel features"}, {"title": "2 Related Work", "content": "The BEV has been extensively used for trajectory prediction and control. The general representation capability of BEVs for 3D space has recently led to considerable progress in many 3D perception tasks(e.g., 3D object detection [18,19], BEV semantic segmentation [29], and online high-definition map construction [8, 21]). BEV features have been crucial in advancing camera-based 3D perception methods. The view transformation is a key process in BEV feature construction. Many methods utilize the depth estimation information to transform the perspective features to BEV space. For example, the Lift-Splat-Shoot [32] method splats image features from multi-view cameras onto a BEV grid, determined by the depth distribution of each pixel in the image. The acquired BEV features are capable of executing segmentation or detection tasks in a 3D environment. BEV-Seg [29] performs semantic segmentation and depth estimation for each view before view transformation, and then uses a parse network for BEV semantic segmentation prediction. VPN [30] utilizes a multi-layer perceptron (MLP) for the direct prediction of BEV segmentation from multi-view images. Another kind of view transform method adopts an attention mechanism to directly acquire features on the multi-view images using a set of BEV queries. BEVFormer [19] designs a spatial-cross attention module to encode the multi-camera features into BEV space via pre-defined BEV features. Similarly, MapTR [21] also uses deformable attention in a Transformer encoder to acquire BEV features, which are decoded to a set of map elements by a DETR-like [4] decoder."}, {"title": "2.1 BEV Methods", "content": "The BEV has been extensively used for trajectory prediction and control. The general representation capability of BEVs for 3D space has recently led to considerable progress in many 3D perception tasks(e.g., 3D object detection [18,19], BEV semantic segmentation [29], and online high-definition map construction [8, 21]). BEV features have been crucial in advancing camera-based 3D perception methods. The view transformation is a key process in BEV feature construction. Many methods utilize the depth estimation information to transform the perspective features to BEV space. For example, the Lift-Splat-Shoot [32] method splats image features from multi-view cameras onto a BEV grid, determined by the depth distribution of each pixel in the image. The acquired BEV features are capable of executing segmentation or detection tasks in a 3D environment. BEV-Seg [29] performs semantic segmentation and depth estimation for each view before view transformation, and then uses a parse network for BEV semantic segmentation prediction. VPN [30] utilizes a multi-layer perceptron (MLP) for the direct prediction of BEV segmentation from multi-view images. Another kind of view transform method adopts an attention mechanism to directly acquire features on the multi-view images using a set of BEV queries. BEVFormer [19] designs a spatial-cross attention module to encode the multi-camera features into BEV space via pre-defined BEV features. Similarly, MapTR [21] also uses deformable attention in a Transformer encoder to acquire BEV features, which are decoded to a set of map elements by a DETR-like [4] decoder."}, {"title": "2.2 Occupancy Network", "content": "Occupancy prediction expands the BEV space in the height dimension to explicitly representing each voxel in the 3D space. Occupancy prediction can provide accurate 3D information for planning in autonomous driving, as it accurately depicts an object's geometrical structure. Generally, existing methods achieve occupancy prediction in the BEV space or voxel space.\nBEV-based Methods. Migrating the BEV perception task to occupancy doesn't appear to be a challenging task. It merely requires determining how to lift the BEV feature to 3D and identify the occupation head. MonoScene [3] first employs U-Net to infer dense 3D occupancy with semantic labels from a single monocular RGB image. BEVDet [10] can directly predict occupancy by replacing the detection head with the scene completion head, which is constructed on the BEV feature maps. TPVFormer [12] suggests a tri-perspective view approach for estimating 3D occupancy. They merge the tri-perspective view feature, where each voxel is situated, to accomplish the 2D-to-3D transformation.\nVoxel-based Methods. FB-OCC [20] is a novel method that combines voxel-BEV representation using forward-backward feature projection. By projecting features in both forward and backward directions, FB-OCC can better understand the context and interactions. VoxFormer [17] employs depth estimation to establish voxel queries and then a deformable cross-attention captures the 3D structure of the scene. OccDepth [28] also adopts a depth-aware method to predict semantic occupancy and a distillation is utilized to improve performance. OccFormer [47] uses a dual-path transformer network, the combination of voxel and bev features can provide local and global information along the horizontal plane. OccNet [39] also uses both BEV cross-attention and voxel cross-attention to achieve a trade-off in performance and efficiency. PanoOcc [42] uses 3D voxel queries with a coarse-to-fine learning scheme to reduce the complexity of 3D cross-attention."}, {"title": "2.3 Feature Pyramid Network", "content": "The feature pyramid network (FPN) [23] is proposed to address the multi-scale problem in 2D object detection. FPN fuses the rich semantic information of the higher-level features in the feature pyramid with the location information of the lower-level features. It solves the multi-scale problem in object detection by making predictions on feature maps of different scales separately. Inspired by 2D FPN, Voxel-FPN [13] utilizes multi-layer 3D convolution to extract voxel features into multi-scale features in a bottom-up fashion, and then performs 3D object detection at multiple scales. The multi-scale fusion strategy of FPN can enhance the Occupancy network's voxel segmentation performance as well. In FB-OCC [20], the occupancy prediction header employs a 3D FPN for decoding the voxel features. By employing 3D convolution, it extracts high-dimensional semantic voxel features to enhance the low-dimensional voxel features iteratively.\nOur method belongs to the BEV-based methods, and unlike the existing methods using 3D deformable attention, we directly lift the BEV features to voxel features by deformable convolution in BEV space. This significantly reduces the query number and the computational cost of constructing voxel features. To improve the semantic segmentation performance of Occupancy, we"}, {"title": "3 Method", "content": "The overall pipeline of our Fast Occupancy Network is shown in Fig. 1. First, the image encoder extracts the multi-view features from the raw camera images. The multi-view features consist of a multi-scale feature pyramid that provides semantic features at different resolutions. Then, the image-to-BEV method transforms the multi-view features into the BEV space using deformable cross-attention. Next, the BEV features are lifted to 3D voxel space using a simple but efficient deformable convolutional layer in our occupancy decoder. The occupancy prediction is performed in the 3D voxel space, and the 3D voxel features are further decoded by the proposed Partial Voxel FPN. Last, the voxel predictions are supervised by voxel-wise classification loss during training. The voxel predictions can be transformed into other 3D environmental perception tasks, such as lidar segmentation. Our occupancy network is quite simple but efficient based on BEV features and performs lifting to 3D voxel features with a convolutional layer. Specifically, we first introduce the image encoder and image-to-bev transformation in this section. Next, we will illustrate our proposed module in Sec 3.2 including the BEV lifting module, Partial Voxel FPN, and perspective-view (PV) supervision.\nImage Encoder. Image Encoder is used to extract 2D semantic features from multi-view camera images. We utilize ResNet50 and ResNet101 as backbones to extract deep features in 2D space. Taking any camera images $I \\in R^{N\\times3\\timesH\\timesW}$ as input, the image encoder generates feature pyramids $P_i \\in R^{N\\times C_i,\\times H_i\\times W_i}$, where N denote the number of perspective views, H and W denotes the input size of camera images, C is the channel number, and the subscript i denotes the corresponding variable in the i-th layer.\nImage-to-BEV Transformation. Following [19], we use BEV queries to transform 3D reference points into 2D perspective views. Features that hit in the 2D"}, {"title": "3.1 Overview", "content": "The overall pipeline of our Fast Occupancy Network is shown in Fig. 1. First, the image encoder extracts the multi-view features from the raw camera images. The multi-view features consist of a multi-scale feature pyramid that provides semantic features at different resolutions. Then, the image-to-BEV method transforms the multi-view features into the BEV space using deformable cross-attention. Next, the BEV features are lifted to 3D voxel space using a simple but efficient deformable convolutional layer in our occupancy decoder. The occupancy prediction is performed in the 3D voxel space, and the 3D voxel features are further decoded by the proposed Partial Voxel FPN. Last, the voxel predictions are supervised by voxel-wise classification loss during training. The voxel predictions can be transformed into other 3D environmental perception tasks, such as lidar segmentation. Our occupancy network is quite simple but efficient based on BEV features and performs lifting to 3D voxel features with a convolutional layer. Specifically, we first introduce the image encoder and image-to-bev transformation in this section. Next, we will illustrate our proposed module in Sec 3.2 including the BEV lifting module, Partial Voxel FPN, and perspective-view (PV) supervision.\nImage Encoder. Image Encoder is used to extract 2D semantic features from multi-view camera images. We utilize ResNet50 and ResNet101 as backbones to extract deep features in 2D space. Taking any camera images $I \\in R^{N\\times3\\timesH\\timesW}$ as input, the image encoder generates feature pyramids $P_i \\in R^{N\\times C_i,\\times H_i\\times W_i}$, where N denote the number of perspective views, H and W denotes the input size of camera images, C is the channel number, and the subscript i denotes the corresponding variable in the i-th layer.\nImage-to-BEV Transformation. Following [19], we use BEV queries to transform 3D reference points into 2D perspective views. Features that hit in the 2D"}, {"title": "3.2 Module Details", "content": "BEV lifting methods. The 2D deformable cross-attention method [48] solves the problem of view-transformation from the image feature to the BEV feature very well. Given an input feature map $x \\in R^{C\\times H_f\\times W_f}$, a 2D query q and a 2D reference point p, the deformable attention feature is calculated by\n$\\text{DeformAttn}(q, p, x) = \\sum_{m=1}^{M} \\sum_{k=1}^{K} W_m A_{mqk}W_k x(p + \\Delta p_{mk}),$ (1)\nwhere m indexes the attention head, k indexes the sampled keys, and K is the total sampled key number (K \u226aHfWf). Wm and Wk are the learning weights,\nAmqk is the attention weight, and p+\u2206pmk is the learnable sample point position. The occupancy task requires obtaining the 3D features and classifying each voxel into predefined categories. Some methods [9, 10] lift the bev features obtained by the 2D deformable attention module to 3D features by using MLP to expand the channel and then reshape to the height dimension. Others [17,42] directly obtain 3D features by initializing 3D queries and positional embeddings, then extract 3D features with 3D deformable attention.\nWe find that the difference between 2D deformable cross attention and 3D deformable cross attention is the number of queries and whether or not they collapse height at the end, while the reference points and reference features are the same. However, the 3D deformable attention is too time-consuming due to the huge count of queries. Meanwhile, the BEVNet experiments in OccNet [39] show the BEV features contain quite 3D information and have the potential to achieve higher performance. It makes sense to replace 3D deformable attention by lifting bev features with efficient methods.\nAs shown in Figure 2, we simply obtain the voxel feature from the BEV feature in two steps. First, we 'lift' the BEV feature up by increasing the dimensions of the channels. Then we 'split' the increased channel dimension by the reshape operation to obtain the voxel features. A common method to lift the BEV feature is using MLP. However, it does not consider the relationship between surrounding pixels. To overcome this limitation, we adopt 2D deformable"}, {"title": "Partial Voxel FPN.", "content": "Inspired by FB-OCC [20], the multi-scale voxel feature fusion is key and important for 3D occupancy accuracy. However, using 3D convolution or deconvolution to realize the embedding of upscale and downscale is time-consuming. We propose a Partial Voxel FPN to realize the fusion of multi-scale features efficiently. General voxel FPN downsample xyz three-dimensions equally via 3D convolution. However, we strip out the heights and only perform a 2D convolution on the planar features of each height layer. In addition, in order to speed up voxel embedding, we sample half of the original feature at equal intervals, which will be kept intact, and only downsample and upsample the other half. The original feature will then be added to the upsampled feature. Finally, we use a 3D convolution on the minimum scale resolution to compensate for the lack of interactions on heights at which the cost is negligible.\nConcretely, Figure 3 shows our Partial Voxel FPN framework. Firstly we split the original feature at the height, then downsample the xy-dimension by convolution with stride=2, which results in a feature that is downsampled in all three dimensions. Repeat this operation until you get the minimum scale feature. Secondly, a cheap 3D convolution acts on a feature size of 50 \u00d7 50 \u00d7 4 on our implementation. Thirdly, a multi-scale feature contains two parts. A portion is from the original feature, which is further passed with a 1\u00d71 convolution. The other half is upsampled from the lower low-resolution feature and added to the other part of the original feature. Sec. 4.6 shows our method can dramatically improve inference speed while maintaining performance."}, {"title": "Perspective View Supervision.", "content": "Due to the presence of the view transformation module such as LSS [32] or BEVFormer [19], the perspective image features are first converted into BEV features, which will be passed through a series of complex neural network structures to obtain the final occupancy voxel. As a result, the supervision signals are far from the image backbone, and the learning of parameters in the image backbone may be influenced by the long path of the gradient flow. To mitigate this problem, we introduced a segmentation supervision signal in the perspective view. We found this auxiliary loss can improve performance without additional computational overhead during inference.\nAs demonstrated in Figure 4, we attach a U-Net-like network after the image backbone to predict the segmentation mask of each view in the image space. The weights of the plug-in U-Net are shared by each view image. Due to the nuScenes dataset does not provide image-view segmentation ground truth, as demonstrated in Figure 5, we project the LiDAR points into every view of the image and assign the label of the LiDAR point to the pixel at the corresponding pixel on the image. Consequently, we generate sparse segmentation labels for every view of the image at multi-level features.\nWe utilize focal loss to supervise the training. Suppose there are N cameras. The level I of the output of the additionally inserted U-Net of each view is represented by $I^l$. The binary mask M indicates where the LiDAR points are projected. The sparse image-view segmentation ground truth of each view is represented as S. The loss $L_{pv}$ of perspective views can be expressed as,\n$L_{PV} = \\sum_{i=1}^{N} \\sum_{l=1}^{L} FocalLoss(M * I^l, S)$ (2)"}, {"title": "Visible Mask.", "content": "The generation of 3D occupancy grid ground truth is difficult. Some areas in the scene cannot be observed by onboard sensors, such as behind walls or inside houses. It is blind to predict the occupation of these areas where information cannot be obtained. Therefore, we only consider the visible voxel when training to make the supervision signals more reliable.\nLoss Functions. Since there is a large number of empty voxels in the 3D space, we utilize the focal loss [24] for voxel segmentation to balance the number of positive and negative samples. As shown in Equation 3, the total loss Lis the weighted summation of the PV loss Lpv and the voxel loss Lvox.\n$L = W_{PV} L_{PV} + W_{vox} L_{vox}$ (3)\nHere, Wpv and Wuox are represented as the weights of the weights of the PV loss and the voxel loss, respectively."}, {"title": "4 Experiments", "content": "We prioritize inference latency as the evaluation criterion for computational efficiency over parameters and FLOPs because it offers a more direct reflection of performance. We compare our method with state-of-the-art occupancy methods"}, {"title": "4.1 Dataset Introduction", "content": "OpenOcc. We choose the OpenOcc [39] to facilitate comparison with the SOTA methods. OpenOcc is generated using dense and high-quality occupancy annotations derived from sparse LiDAR data and 3D bounding boxes based on the nuScenes [2] dataset, which comprises 700 training scenes and 150 validation scenes. The occupancy classes are the same as LiDAR segmentation, including 10 foreground objects and 6 background categories.\nSemanticKITTI. The SemanticKITTI [1] dataset, built upon the widely popular KITTI Odometry dataset, emphasizes on semantic understanding of scenes"}, {"title": "4.2 Experiment Setting", "content": "Implementation Details on OpenOcc Dataset. We adopt LiDAR range\n[-50m, 50m] \u00d7 [-50m, 50m] \u00d7 [-5m, 3m] and voxel size [200, 200, 16], to be consistent with OccNet [39]. Following the experimental setting of BEVFormer [19] and OccNet, we use two backbones, which are ResNet50 pretrained on ImageNet and ResNet101-DCN initialized from FCOS3D [41]. The ResNet50 output single feature at the size of 1/32, and ResNet101-DCN output multi-scales features from FPN at sizes of 1/8,1/16,1/32. We use 3 BEV layers in ResNet50 and 6 BEV layers in ResNet101-DCN respectively. The BEV feature dimension is set to 256 and the voxel feature dimension is set to 128. We set the learning rate max to 2e-4, with 0.01 weight decay. The model is trained with 24 epochs.\nImplementation Details on SemanticKITTI Dataset. We adopt LiDAR range [0, 51.2] \u00d7 [-25.6m, 25.6m] \u00d7 [-2m, 4.4m] and voxel size [256, 256, 32], to be consistent with OccFormer. We adopt EfficientNet-B7 [38] as the backbone, following the compared methods. We generate the 3D feature volume of size 192 \u00d7 192 \u00d7 32 with 128 channels and upsampled to 256 \u00d7 256 \u00d7 32 for full-scale evaluation. Unless specified, we set the learning rate max to 10-4, and train the model for 30 epochs."}, {"title": "4.3 Metrics", "content": "The mean intersection over union (mIoU) metric is employed to evaluate the performance of the predicted occupancy results for both the semantic scene completion (SSC) and the LiDAR segmentation tasks. We also provide intersection over union (IoU) scores for individual categories. The LiDAR segmentation results are obtained from the correspondingly predicted 3D semantic occupancy volumes, as detailed in [39]."}, {"title": "4.4 Comparison with State-of-the-Art Methods", "content": "Occupancy Prediction on OpenOcc Dataset. Table 1 shows the results comparing our method with previous SOTA methods for occupancy tasks. We report the results of BEVDet4D [10], BEVDepth [18] and BEVDet [10] reproduced by OccNet [39] via replacing the detection head with the scene completion head built on their BEV feature maps on ResNet50 backbone. Compared with the TPVFormer [12], our method has a substantial boost of 3.55% mIoU on Res101-DCN backbone model. As for the SOTA method OccNet on OpenOcc dataset, we compared it on both ResNet50 and ResNet101-DCN. Our method surpasses OcNet by 1.64% with about 1/3 inference latency on ResNet50 setting, and also obtains a little higher results on Res101-DCN with fewer inference latency shown in Table 4."}, {"title": "4.5 Speed Comparison", "content": "We prioritize inference latency as the evaluation criterion for computational efficiency over parameters and FLOPs because it offers a more direct reflection of performance. We compare our method with state-of-the-art occupancy methods"}, {"title": "4.6 Ablation Studies", "content": "Effect of Our Lifting BEV Feature Method. We investigate the effect of lifting methods by comparing the mIOU and latency of different methods. To facilitate an intuitive sense of the module's size, we also present the proportion of latency in the overall. As shown in Table 5, we utilize BEVNet as the baseline, which is BEVFormer + MLP. We observe that the naive method of employing MLP to expand the channel number and reshape it to the height dimension yields relatively poor performance. The main reason is that the method does not take into account the neighboring pixels, resulting in a receptive field of only 1. This is why we have a 0.7% gain by simply replacing MLP with Conv2D. We continue to expand the receptive field by using a 5\u00d75 kernel of convolution, but the improvement is minimal and comes with significant latency. The Deformable Conv2D can continue to increase the receptive field while having the same dynamically irregular reference points, and achieve 0.9% improvement compared to Conv2D. The deformable offset in Deformable Conv2D is necessary because the shapes of objects at different heights are not consistent. We also utilize the 3D Deformable Attention proposed by OccNet and directly lift the BEV feature, the performance is close to the original OccNet but lower than our method.\nEffect of Partial Voxel FPN. To identify a more efficient voxel FPN method, we compare several voxel FPN methods while using BEVNet as the baseline without voxel FPN. We reproduce the method from FB-OCC [20], which involves downsampling through 3D convolution, upsampling features at different scales, and summing them with the original scale features. Although this method is effective, it is quite time-consuming, accounting for 26.2% of the overall model latency. In our comparison of Partial Voxel FPN with the full Conv2D method, we find that Partial Voxel FPN can achieve nearly equivalent performance while offering faster inference times. The 1 \u00d7 1 Conv2D enables the original preserved features to be efficiently adapted to multi-scale representations, contributing to the speed and effectiveness of our approach. We have also evaluated the impact of using a small 3D convolution on the smallest scale feature of 50 \u00d7 50 \u00d7 4. Our method demonstrates a 0.3% improvement in performance compared to not using a small 3D convolution. Overall, our proposed method can achieve comparable mIOU with a 0.24\u00d7 latency of Voxel FPN with full Conv3D.\nContribution of Our Proposed Methods. To clearly demonstrate the effectiveness of our proposed methods, we conduct ablation experiments as presented"}, {"title": "5 Conclusion", "content": "In this paper, we present a simple yet efficient Occupancy Network method, which leverages 2D deformable convolution to lift the BEV feature to the 3D voxel feature for occupancy prediction, achieving SOTA performance compared with other complex lifting methods with less computational cost. We present a PV segmentation supervision to improve performance without introducing any cost during the inference phase. Further, we present a Partial Voxel FPN module with few computational costs to improve accuracy significantly. Our method can be seamlessly applied to mainstream BEV models to transform them into Occupancy Network models. Experiments show our method can surpass other SOTA methods on both accuracy and inference speed, validating the superiority of our method."}]}