{"title": "REDUCING THE COST OF DROPOUT IN FLASH-ATTENTION BY HIDING RNG WITH GEMM", "authors": ["Haiyue Ma", "Jian Liu", "Ronny Krashinsky"], "abstract": "Dropout, a network operator, when enabled is likely to dramatically impact the performance of Flash-Attention, which in turn increases the end-to-end training time of Large-Language-Models (LLMs). The main contributor to such performance degradation is the Random Number Generation (RNG) phase that is traditionally fused into the Flash-Attention kernel. As RNG and Attention have the same hardware bottlenecks, RNG latency can hardly be hidden within the Attention kernel.\nWe propose overlapping RNG with previous GEMM layers in the network to hide RNG runtime and improve end-to-end performance. RNG and GEMM have distinct resource requirements and hardware bottlenecks, so they can run in parallel without compromising each other's performance. Our fine-grained performance model, cross-validated by silicon results, shows 1.14x speedup on one transformer block (including multi-head attention and feed-forward layers) for Llama2, and up to 1.23x speedup when varying workload sizes, on GH100 GPUs with FP8 precision. Further, we extend our theoretical model to different RNG implementations and hardware architectures, and discuss the widely applicable benefits for overlapping RNG with GEMM layers.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have become important targets for performance optimization due to their ever-increasing workload sizes and corresponding runtime demands. Full-scaled training for models like GPT (OpenAI, 2023) requires several months and thousands of GPUs(II, 2023). Attention dropout (Srivastava et al., 2014)(Zehui et al., 2019) is an optional technique that drops out elements after the Softmax operation in Attention. Dropout is applied in commonly used models such as Llama (Face)(Touvron et al., 2023) because it can make the model focus on relevant features and improve training accuracy (Schumacher)(Shastri) (Xue et al., 2024). However, enabling dropout is costly, which doubles the processing time of the Attention layer with state-of-the-art implementations like Flash-Attention (Dao et al., 2022)(Dao, 2023)(Shah et al., 2024), and in turn increases the end-to-end training time by 1.3x to 1.7x, depending on the network parameters. Optimizing the runtime of dropout can significantly improve the performance of LLM training.\nThe runtime of dropout is dominated by the Random Number Generator (RNG) (Salmon et al., 2011), which generates random numbers to determine which elements within the"}, {"title": "2 BACKGROUND", "content": "2.1 LLM Network Architecture\nLLM networks typically begin with an embedding layer, conclude with a decoding layer, and iteratively call Trans-former Blocks in between. Figure 2 shows the network architecture of one Transformer Block in the forward path. The General Matrix Multiply (GEMM) layers in blue and the Attention layer in green contribute to the majority of the compute time. The purple dashed layers represent communication layers, present only in multi-GPU systems. The white LayerNorm layers perform element-wise operations, and the orange C+T layers handle Conversion (when precision conversion to FP8 is necessary) and Transpose operations for GEMM inputs. These last two types of layers typically require minimal runtime and are omitted from our runtime analysis.\nIn this work, we begin our analysis on single GPU without communication, and focus on the GEMM and Attention layers. We discuss multi-GPU scenarios in Section 5 and draw similar conclusions as in single GPU. With multi-GPU, dif-"}, {"title": "2.2 Dropout", "content": "Dropout (Srivastava et al., 2014)(Wan et al., 2013) is a tech-nique designed to prevent overfitting by randomly setting a small fraction of the elements within a network to zero. It has been proposed to regularize neuron networks in gen-eral. In the context of this paper, we refer specifically to Attention dropout (Schumacher)(Shastri), where the dropout operation is applied to the intermediate outputs of the Atten-tion layer following the Softmax operation, as depicted in Figure 3.\nWhile optional, dropout can substantially improve training accuracy: it prevents the model from relying heavily on cer-tain features, thus making the model only focus on relevant features to prevent overfitting and underfitting (Xue et al., 2024)(Liu et al., 2023). Dropout is used in training widely adapted LLM networks such as Llama2 (Face)(Touvron et al., 2023). However, its huge runtime slowdown has lim-ited its application. We explore dropout optimizations to improve its practicality in future applications, facilitating broader implementation."}, {"title": "2.3 RNG Implementation: Philox", "content": "The computational demand of dropout mostly comes from the Random Number Generator (RNG) used to deter-mine which elements to zero out. Multiple possible RNG implementations exist, and our discussion centers on Philox (Salmon et al., 2011) (NumPy). Philox is a counter-based pseudorandom number generator (PRNG) that relies"}, {"title": "3 IMPLEMENTATION", "content": "In this section, we discuss our experiments performed on sil-icon, as well as the fine-grained performance model which is backed up by silicon results. We first validate our perfor-mance assumptions that are essential for effective overlap-ping on silicon, then utilize the theoretical model to general-ize our findings. Figure 4 shows the setup for baseline and overlap experiments."}, {"title": "3.1 Silicon Implementation", "content": "We tested a CUDA implementation of our proposal on NVIDIA H100 GPUs (NVIDIA, f), the cutting-edge op-tion for data-center training tasks, specifically using the GH100 HBM3 80GB variant. This GPU supports the lat-est High Bandwidth Memory (HBM) and provides ample compute resources and memory capacity for LLM training.\nFor GEMM kernel analysis, we implemented QKV_GEMM, the GEMM layer that immediately precedes the Attention layer. Since GEMM layers have predictable runtime given the M, N and K dimensions, analyzing a single GEMM layer provides sufficient data to predict behavior across all four potential GEMM layers to overlap within a Transformer Block.\nIn terms of implementation, RNG and GEMM can either run as separate kernels or within the same kernel using warp specialization (Li et al., 2023)(NVIDIA, i)(Kerr et al.). We opted for separate kernels to maintain a clear distinction be-tween independent components in the network such as RNG and GEMM in this case. This approach allows other over-lapping strategies to be implemented without deep knowl-"}, {"title": "3.1.1 Performance Assumptions", "content": "To validate our assumptions about performance impacts, we conducted several tests:\n\u2022 GEMM Resource Allocation: Originally, the GEMM kernel utilizes all available Register Files and Shared Memory in each SM. We carved out 6% of the Regis-ters and 7% of the Shared Memory for the RNG kernel, hypothesizing this would not adversely affect GEMM performance. Our silicon measurements confirmed only 0.5% average performance difference across vari-ous GEMM workload sizes.\n\u2022 Processing Time for Dropping Elements: We hy-pothesized that dropping the elements within the At-tention layer would require minimal runtime compared to RNG. Our silicon results confirmed that RNG dom-inated the full dropout, while dropping the elements only increase the original Attention runtime by 12% on average.\n\u2022 RNG and GEMM Interference: We hypothesized that RNG should not noticeably slow down GEMM performance. We observed an average of 4% slow-down in GEMM when running concurrently with RNG, which is acceptable. Conversely, RNG experiences a 50% slowdown when run alongside GEMM, but this is also deemed acceptable since the original runtime of RNG is likely shorter than GEMM."}, {"title": "3.1.2 Baseline and Overlap CUDA Implementation", "content": "The baseline implementation includes a stand-alone QKV_GEMM kernel and dropout (including RNG) fused into the Attention kernel, running serially on the same CUDA stream. Our optimized kernel fusion minimizes syn-chronization overhead and maximizes the overlap of RNG operations with Attention's floating-point computations.\nThe overlapping implementation uses two separate CUDA streams to allow GEMM and RNG kernels to run concur-"}, {"title": "3.2 Theoretical Model", "content": "This subsection explains the construction of our fine-grained theoretical performance model, designed to evaluate the overlapping technique's effectiveness based on hardware bottlenecks. We begin by discussing the hardware bottle-necks taken into account in our model, and then the deriva-tion of baseline and overlapping runtime."}, {"title": "3.2.1 Hardware Limiters", "content": "We identified several hardware limiters critical for calcu-lating kernel runtime in our performance model. For each kernel, we estimate its runtime assuming each limiter as a potential bottleneck and determine the final runtime by selecting the maximum runtime among all considered lim-iters."}, {"title": "3.2.2 Modeled Layers", "content": "Figure 5 illustrates the modeling approach for baseline and overlap runtime, incrementally constructed with individual kernel performance assessments.\nInitially, we model the performance of each individual ker-nel - GEMM, Attention, and RNG calculating their the-oretical runtime based on bounding each by the limiters. For workload sizes within the range of typical networks (GPT (Brown, 2020) and Llama (Touvron et al., 2023)), we found that GEMM runtime is bounded by MMA, Attention by RF bandwidth and the Issue stage, and RNG by the Issue stage and ALU pipe.\nNext, we integrate additional components to derive runtime estimations for composed kernels. For the Attention ker-nel with the element dropping step, we derive its runtime by adding the silicon-measured dropping overhead to the standalone Attention kernel runtime (Figure 5d). We also compute the runtime of the fused Attention kernel with RNG by integrating both sets of instructions and identifying the primary limiter, which typically is the Issue Stage, with the ALU pipe and RF bandwidth as close secondary limiters (Figure 5e)."}, {"title": "4 RESULTS", "content": "This section presents the performance outcomes of our RNG and GEMM overlapping experiments. The analysis is based on the theoretical results, supported by silicon measure-ments. We base our experiments on a GPT-3-like network architecture, using a common hidden dimension per head of 128, and a batch size of 1. We vary the sequence lengths from 2048 to 65536, and number of heads from 48 to 128. These conclusions are broadly applicable across various network architectures.\nWe validated the theoretical model's accuracy with a mere 2% average difference between the theoretical model and silicon measurements for overlapping QKV_GEMM with RNG. The difference is calculated by averaging the absolute difference of speedup between silicon and theoretical results. With such validation, we extend our performance analysis to overlapping RNG with all four GEMM layers between the previous and the current Attention layer. Figure 6 shows the modeled overlap speedup for different sequence length and number of heads.\nThe results demonstrate significant performance improve-ments, with speedups up to 1.23x across the five key layers of a Transformer Block (four GEMM and one Attention layer). Specific speedups include 1.06x for GPT-3, 1.14x for LLAMA2, and 1.13x for MoE. Further analysis reveals that speedup is closely correlated with the ratio of sequence length to the number of heads, given that the batch size and the hidden dimension don't change. The greatest improve-ment occurs within a specific range (Region 2 in Figure 6); the speedup gradually decreases towards Region 1 or Region 3.\nTo understand this trend, we examined the runtime depen-dencies of each kernel on sequence length and number of heads:"}, {"title": "5 DISCUSSION", "content": "In this section, we dive into the stand-alone RNG's HBM capacity requirement, and the overlapping methodology's implication on different RNG implementations and future hardware architecture generations."}, {"title": "5.1 HBM Capacity Requirement for Stand-alone RNG", "content": "An overhead brought by our methodology is storing the RNG bits generated in HBM to be later used by the Attention kernel. For each element in Attention's intermediate matrix, assuming RNG generates 1 bit per element to indicate the dropping status, we require storage of $B * nH * SQ^2$ in HBM, which may initially appear to be critical.\nFigure 9 illustrates the HBM requirements for stand-alone RNG when the entire network is run on a single GPU. If we hypothetically allocate 8GB for RNG data, it becomes apparent that applying this methodology to networks with sequence lengths of 32K or more is not feasible on a single GPU.\nHowever, the typical deployment scenario for LLM training involves multiple GPUs, where the workload is divided into smaller segments distributed across the GPUs. This division significantly reduces the HBM capacity needed for RNG on each GPU. Common strategies for parallelism include Tensor Parallelism (Shoeybi et al., 2019) and Sequence Parallelism (Korthikanti et al., 2023), which split the head and sequence length dimensions, respectively. Figure 9 also demonstrates the reduced HBM requirements when employing these parallelism mechanisms. For the three mainstream LLM training networks, GPT3, LLAMA2 and GPT4-MoE, the required HBM capacity is decreased by a factor of ten or more, depending on the chosen parallelism strategy and dimension.\nIt is worth noting that parallelism does not change the perfor-mance benefit from overlapping RNG and GEMM analyzed"}, {"title": "5.2 Implication of Cheaper RNG", "content": "In our prior discussions, we focused on the use of Philox 7 for RNG implementation. This subsection explores the implications of adopting more cost-effective RNG imple-mentations, namely Philox 5 and Philox 3, which involve fewer computational iterations and shorter runtimes.\nWe implemented all three Philox variants (Philox 3, 5, 7) on silicon using the GH100 with the same experimental setup as previously discussed. We show representative results collected for sequence length = 16K in Figure 11, which has a consistent trend with other configurations not shown on the graph.\nWe observed that the runtime of the Philox 5 RNG kernel is approximately 81% of that required for Philox 7, while Philox 3 operates at 67% of the Philox 7 runtime. These numbers align closely with the expected reduction in Fused Multiply-Add (FMA) operations (71% for Philox 5 and 43% for Philox 3). The difference is because other operations in the RNG process do not scale linearly with the number of iterations.\nOur theoretical performance model was validated against the silicon results on overlapping RNG with QKV_GEMM, showing an error margin consistent with previous valida-tions. Using the validated model, we further analyzed the implications of varying RNG complexities on overlapping RNG with all four GEMM kernels in the Transformer Block.\nAs the complexity of the RNG algorithm decreases, the potential for speedup through overlapping also diminishes"}, {"title": "5.3 Hardware Exploration", "content": "Our analysis so far has been in the context of NVIDIA's GH100 GPUs (NVIDIA, f). With our fine-grained perfor-mance model, we now explore how variations in hardware design might influence the efficiency of our overlapping strategy.\nA key aspect of hardware evolution, particularly with NVIDIA GPUs, is the consistent increase of computational efficiency with each new generation. This trend contin-ues with the newest Blackwell GPUs (NVIDIA, a), which motivates us to model the potential impacts of further ad-vancements in computation power, such as enhanced FP operations or lower FP precisions.\nFigure 15b illustrates the implications of increased com-pute capability on our overlapping technique. We maintain the assumption that computation remains the bottleneck for GEMM, with memory performance keeping pace with MMA improvements through reduced precision or increased memory bandwidth. However, other non-Tensor related lim-iters like the issue pipeline and the ALU pipeline are likely to remain unchanged, meaning the most significant run-time reduction would be observed in the GEMM kernel. This shift will make RNG latency an even more critical bottleneck, proportionally increasing its impact on end-to-end network performance. Although the absolute runtime difference between baseline and overlapped configurations remains similar, the relative speedup should improve.\nWe evaluated the theoretical performance of our overlapping strategy using a hypothetical model of a more advanced GPU, which offers twice the compute capability of the H100. Figure 15 shows that overall speedup increases up to 10% with higher compute efficiency for a variation of se-quence lengths and number of heads. Our findings indicate that while reductions in GEMM runtime boost the overall speedup ratio, this benefit is primarily observed in work-loads with shorter sequence lengths. For longer sequences, where RNG and Attention dominate network runtime, over-lapping a shorter GEMM could exacerbate problems by fully exposing RNG latency once GEMM computation com-"}, {"title": "6 RELATED WORK", "content": "Scheduling layer components within Large Language Mod-els (LLMs) has become a critical area of research due to the increasing demands of improving the efficiency of these models (Ye et al., 2024) (Li et al., 2024). Effective schedul-ing can improve end-to-end network performance when components underutilize GPU resources, if they exhibit distinct resource utilization patterns.\nSeveral studies have explored different aspects of LLM scheduling. For instance, Splitwise (Patel et al., 2024) pro-poses a technique to separate the prefill and decoding phases of LLM inference onto different machines because of their"}, {"title": "7 CONCLUSION", "content": "This paper proposed a strategic overlapping of RNG with GEMM layers to improve end-to-end LLM training effi-ciency. By decoupling RNG from the Dropout process and running it in parallel with the computationally intense GEMM operations, we effectively reduce the latency impact typically associated with RNG if fused with the Attention layer. Our approach optimizes the use of hardware resources by exploiting the distinct hardware demands of RNG and GEMM, and achieves a notable improvement in end-to-end training performance, with speedups ranging from 1.06x to 1.23x within a Transformer Block across various LLM architectures.\nWe also develop a fine-grained theoretical performance model, validated with silicon results, to provide deeper in-sights into overlapping different kernels in the LLM network and highlight potential areas for further improvements. The principles established here can extend to optimizing other network layers, offering a generalized strategy to analyze the implications and enhance the performance of future LLM systems. As LLMs continue to scale and the demand for computational efficiency grows, the theoretical model can serve as a valuable framework for evaluating future overlap-ping strategies maximize resource utilization and minimize training time."}]}