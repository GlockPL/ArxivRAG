{"title": "PFML: SELF-SUPERVISED LEARNING OF TIME-SERIES DATA WITHOUT REPRESENTATION COLLAPSE", "authors": ["Einari Vaaras", "Manu Airaksinen", "Okko R\u00e4s\u00e4nen"], "abstract": "Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar pre-existing SSL method and competitive against the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse. The code is freely available at https://github.com/SPEECHCOG/PFML.", "sections": [{"title": "INTRODUCTION", "content": "Self-supervised learning (SSL) can be described as a data-driven learning paradigm where the training process is guided by the inherent structure of the data itself. Unlike supervised learning that relies on externally provided labels, SSL exploits the intrinsic properties of the data to generate its own supervisory signal (Balestriero et al., 2023). SSL enables the model to learn rich feature representations from large amounts of unlabeled data that can be used as a starting point for downstream tasks, either as such or by fine-tuning the feature extractor to be better suited for solving some specific task (Erhan et al., 2010). Since typically there is an abundance of unlabeled data but a scarcity of labeled data, the use of SSL has been shown to reduce the need for large, manually annotated datasets (van den Oord et al., 2018; Baevski et al., 2020; Chen et al., 2020). In addition to SSL algorithms that have been developed for a single data modality, SSL algorithms that can be applied to multiple different data modalities have gained popularity in recent years (van den Oord et al., 2018; Akbari et al., 2021; Baevski et al., 2022; Wang et al., 2023). These methods and their extensions have shown great success in e.g. audio, image, and text data (van den Oord et al., 2018; H\u00e9naff et al., 2020; Akbari et al., 2021; Baevski et al., 2022; Wang et al., 2023; Baevski et al., 2023; Yoon et al., 2023; Zhu et al., 2023; Lian et al., 2023)."}, {"title": "RELATED WORK", "content": "Most of the advances in SSL have focused on developing new, better-performing algorithms with some specific data modality in mind. For speech data, Baevski et al. (2020) presented an SSL algorithm where the basic idea is to mask speech embeddings and then solve a contrastive task that is defined over a quantization of the embeddings which are simultaneously learned during the pre-training task. Hsu et al. (2021) proposed that instead of solving a contrastive task, they predict cluster targets of masked embeddings. Furthermore, the SSL method by Chen et al. (2022) also uses masking of embeddings, but the authors simulate noisy speech inputs and predict pseudo-labels of the original speech from the masked embeddings.\nSimilar to the advances in SSL for audio data, there have been significant developments in SSL for image data as well (Lee et al., 2017; Gidaris et al., 2018; Caron et al., 2018; Grill et al., 2020; Chen et al., 2020; Radford et al., 2021; He et al., 2022; Bao et al., 2022; Oquab et al., 2024). Grill et al. (2020) presented an SSL method that uses two neural networks that learn from each other's representations of differently augmented views of the same image. He et al. (2022) proposed masked autoencoders (MAE) that try to reconstruct masked patches of input images using an asymmetric encoder-decoder architecture. The SSL algorithm by Bao et al. (2022) tokenizes images into visual tokens, followed by masking some image patches and then trying to recover the original tokens from the masked patches.\nSSL has also excelled in natural language processing (Devlin et al., 2019; Brown et al., 2020; Tay et al., 2023; OpenAI, 2023). Devlin et al. (2019) introduced an SSL method which obtains bidirectional"}, {"title": "METHOD", "content": "One key issue with many SSL methods is the problem of representation collapse, where the model outputs a constant, input-invariant feature representation, leading to a trivial solution of the pre-training task (Jing et al., 2022; Balestriero et al., 2023). This considerably slows down the development process for novel data domains and/or tasks due to the necessity of operating in uncertainty, when it is not clear whether the representation collapse is caused by an ill-posed task or by the SSL algorithm. To avoid this, SSL methods have taken several different countermeasures: Baevski et al. (2020) use the same target representations in their contrastive learning task in a dual manner, i.e. both as a positive and a negative example. Grill et al. (2020) both add an additional predictor to their training regime and use a moving average of their so-called online neural network to avoid representation collapse. Bardes et al. (2022) add a regularization term to their loss function that both maintains variance of the embeddings and decorrelates each pair of variables. In data2vec (Baevski et al., 2022), the authors tackle representation collapse by carefully selecting their model hyperparameters and promoting target representation variance through feature normalization. Also, in the code implementation of data2vec\u00b9, pre-training is stopped if the variance of either model predictions or training targets falls below a predefined threshold."}, {"title": "MOTIVATION", "content": "One key issue with many SSL methods is the problem of representation collapse, where the model outputs a constant, input-invariant feature representation, leading to a trivial solution of the pre-training task (Jing et al., 2022; Balestriero et al., 2023). This considerably slows down the development process for novel data domains and/or tasks due to the necessity of operating in uncertainty, when it is not clear whether the representation collapse is caused by an ill-posed task or by the SSL algorithm. To avoid this, SSL methods have taken several different countermeasures: Baevski et al. (2020) use the same target representations in their contrastive learning task in a dual manner, i.e. both as a positive and a negative example. Grill et al. (2020) both add an additional predictor to their training regime and use a moving average of their so-called online neural network to avoid representation collapse. Bardes et al. (2022) add a regularization term to their loss function that both maintains variance of the embeddings and decorrelates each pair of variables. In data2vec (Baevski et al., 2022), the authors tackle representation collapse by carefully selecting their model hyperparameters and promoting target representation variance through feature normalization. Also, in the code implementation of data2vec\u00b9, pre-training is stopped if the variance of either model predictions or training targets falls below a predefined threshold."}, {"title": "PREDICTION OF FUNCTIONALS FROM MASKED LATENTS", "content": "Figure 1 depicts an overview of the PFML pre-training pipeline. First, a single- or multi-channel signal x is framed into a sequence of short-term frames {x\u2080, x\u2081, ...}, x\u2099 = {x\u209c, x\u209c\u208a\u2081, ..., x\u209c\u208a\u2099\u208b\u2081},"}, {"title": "", "content": "of N samples each. Then, a set of m functionals, F = {F\u2080, F\u2081, ..., F\u2098\u208b\u2081}, is computed for each frame x\u2099 to produce corresponding functional values f\u2099 = {F\u2080(x\u2099), F\u2081(x\u2099), ..., F\u2098\u208b\u2081(x\u2099)}. Here, functionals are defined as mathematical operations which map a time series of arbitrary length into a single value, such as the mean or variance of the signal. The frames x\u2099 are also fed to an encoder model, which converts the framed signals into embeddings z\u2099. Some of these embeddings are masked randomly at time steps M (for example, M \u2208 {1, 2} in Figure 1), after which all z\u2099 are used as an input for a Transformer-based model to obtain outputs y\u2099. Finally, a prediction loss is computed between the outputs of masked time steps y\u2098 and their functional counterparts f\u2098. As a result, PFML pre-training optimizes the prediction of functionals of input signal frames corresponding to the masked embeddings, given the unmasked embeddings from the temporal context of these frames.\nIn PFML, predicting only one or a few functionals of a framed signal can be a trivial task, and will most probably lead to learning feature representations that are not very useful for downstream tasks. However, as the number of functionals that each describe some property of the framed signal grows, a more accurate description of the signal can be obtained (see e.g. McDermott & Simoncelli (2011) for representing perceptual properties of sound textures with functionals). Therefore, as the number of different functionals grows, the PFML algorithm is getting closer to predicting all of the nuances of the input signal.\nLet us assume the following in PFML pre-training:\n\u2022 Assumption 1: There is temporal variability across the frames x\u2099. This assumption is reasonable as real-world data typically exhibits temporal variability.\n\u2022 Assumption 2: Given Assumption 1, a set of non-trivial functionals F computed from x\u2099 also contains variance across the frames. This follows naturally since non-constant functionals derived from variable data also exhibit variability.\nUnder these assumptions, as the model is trying to predict the computed functionals f\u2099 given the embeddings z\u2099, good model predictions y\u2099 that lead to low prediction loss values also inherently contain variance. On the contrary, if y\u2099 were to contain zero variance across the frames while f\u2099 contains variance, the prediction loss would be high. Consequently, PFML pre-training does not converge to collapsed feature representations, as long as Assumptions 1 and 2 hold true. For a more detailed formulation, see Appendix A. Empirical results (see Section 4.4) support this theoretical claim, showing that PFML maintains variance in predictions across various datasets.\nIn the present study, we selected 11 mathematical operations as our set of functionals: mean, variance, skewness, kurtosis, minimum value, maximum value, zero-crossing rate (ZCR), and the mean, variance, skewness, and kurtosis of the autocorrelation function (ACF). The ZCR for a signal x = {x\u2080, x\u2081, ..., x\u2099\u208b\u2081} is defined as\nZCR(x) = \\frac{1}{N-1} \\sum_{k=1}^{N-1} |sgn(x\u2096) - sgn(x\u2096\u208b\u2081)|, (1)\nwhere sgn denotes the sign function (Rabiner & Schafer, 2007). The ACF for a signal x at lag \u03c4 is defined as\nACF(x, \u03c4) = \\frac{1}{(N - \u03c4)\u03c3\u00b2} \\sum_{k=0}^{N - \u03c4 - 1} (x\u2096\u208a\u03c4 - \u03bc) (x\u2096 - \u03bc), (2)\nwhere \u03c4 < N, \u03bc is the mean of x, and \u03c3\u00b2 is the variance of x (Rabiner & Schafer, 2007). Note that Equation 2 returns a vector of measurements when applied to all lags \u03c4 < N.\nFor masking the embeddings, in each training and validation minibatch we randomly select frames with a probability of p\u2098 to be mask starting indices, and we mask the embedding of that frame and l\u2098 - 1 subsequent frames, resulting in a minimum mask length of l\u2098 frames. We replace each embedding that is selected for masking with a vector of ones. Masks can overlap, enabling longer mask spans than l\u2098 frames (especially with high p\u2098). Furthermore, we also define that each training and validation sequence needs to have at least one mask starting index during PFML pre-training.\nNote that the PFML pre-training process is not restricted to any specific type of neural networks. In the present study, we used convolutional neural networks (CNNs) as our encoder model, and T Transformer encoder blocks as the temporal model. However, any type of encoder could be"}, {"title": "EXPERIMENTS", "content": "We evaluate our PFML method using three different datasets of time-series data with complex classification tasks: infant posture and movement classification from multi-sensor IMU data, emotion recognition from speech data, and sleep stage classification from EEG data. For each dataset, we first run SSL pre-training with unlabeled data using PFML, after which we fine-tune our models for downstream classification tasks using labeled data. We compare PFML against three different baselines: MAE (He et al., 2022), data2vec (Baevski et al., 2022), and not using pre-training at all. We selected MAE for our experiments since it is conceptually very similar to PFML, and we chose data2vec since it is the current state-of-the-art data modality agnostic SSL method. In order to make the prediction of functionals directly comparable with predicting the input signal, we use a slightly modified version of MAE where we mask embeddings instead of masking inputs.\nFor PFML pre-training, our models consist of a modality-specific frame-level encoder (detailed in Sections 4.1, 4.2, and 4.3 for IMU, speech, and EEG data, respectively) and a Transformer network consisting of T Transformer encoder blocks. Between the encoder and Transformer networks there is a CNN-based relative positional encoder followed by a GeLU (Hendrycks & Gimpel, 2016) activation and layer normalization (Ba et al., 2016). We frame our input signals before feeding the data into an encoder model, and we compute functionals from these frames as our training targets. For multi-channel data, we compute functionals separately for each channel. The functionals are then z-score normalized across the entire pre-training dataset. For computational efficiency, we pre-compute the functionals of each signal frame before the pre-training process. After the Transformer encoder blocks, we add a linear projection to convert the Transformer outputs into predicted functionals. Pre-training is run until validation loss convergence, and we use the model with the lowest validation loss as our pre-trained model. Starting from an initial learning rate, we gradually reduce the learning rate during model training with a reduction factor of 0.5 based on the plateauing of the validation loss.\nWe pre-train our models using MAE and data2vec in a similar manner as for PFML, and we use the same model architecture for all three pre-training algorithms. MAE pre-training is run in a similar manner as PFML pre-training, with the only exception of predicting the input signal frames instead of functionals. For data2vec pre-training, we used the instance-normalized (Ulyanov et al., 2016) and averaged outputs of each feed-forward part of all Transformer encoder blocks as our training targets. If we observed that a representation collapse occurred during data2vec pre-training, we restarted the pre-training process. For further details on the data2vec algorithm, see Baevski et al. (2022). We used mean squared error loss for all pre-training processes except for PFML with speech data, where we found L1 loss to work better.\nWe fine-tune our pre-trained models in two stages. In the first stage, two randomly initialized fully-connected GeLU layers followed by a softmax function are added after the Transformer model. Then, these layers are fine-tuned separately as the weights of the encoder and Transformer are frozen. In the second stage, the entire model is fine-tuned with the same hyperparameters as in the first fine-tuning stage with one exception: The learning rate LR is linearly increased from 0.001 LR to LR during a warm-up period of 20 training epochs, followed by reduction by a factor of 0.5 based on validation loss plateauing. We use weighted categorical cross-entropy loss by weighting the loss of each sample by its class' inverse frequency.\nWe also test the linear separability of the features learned by our pre-trained models. In this case, we only add one linear layer after the Transformer model, and we fine-tune this single layer while the weights of the encoder and Transformer are frozen. As a baseline, we perform the same linear evaluation for a randomly initialized model without any pre-training.\nFor pre-training and fine-tuning, we use the RAdam (Liu et al., 2020) and Adam (Kingma & Ba, 2015) optimizers, respectively. For the \u201cno pre-training\u201d condition, we simply omit pre-training, the first fine-tuning stage, and the learning rate warm-up period of the second fine-tuning stage. We"}, {"title": "INFANT POSTURE AND MOVEMENT CLASSIFICATION", "content": "For infant posture and movement classification, we use the multi-sensor IMU data from Airaksinen et al. (2022). The data contains 24-channel signals from infants (three gyroscope and three accelerometer channels, four limbs) with a sampling rate of 52 Hz. We window the signals into 120-sample frames (approx. 2.3 seconds) with 50% overlap. For further details about the dataset, see Airaksinen et al. (2022).\nFor model pre-training, we use a 387-hour set of unlabeled IMU data from infant free-form play that has been automatically screened for signal quality (Vaaras et al., 2023b). This subset contains 4669 sequences of 260 consecutive frames, each corresponding to five minutes of data. As the encoder, we use the same four-layer CNN-based encoder architecture as in Airaksinen et al. (2022) with three minor modifications that were found to improve training efficiency and system performance when replicating the experiments of Airaksinen et al. (2022): We added layer normalization after the last two convolutions to make the pre-training process more stable, the kernel size of the second convolutional layer of the CNN encoder was changed from [4,5] to [3,5], and the originally temporally asymmetrical padding was set to [1,2] to make it symmetric. The pre-training data is randomly split into a training and validation set in a ratio of 80:20 sequences, and we input 260-frame sequences into the model.\nFor fine-tuning our pre-trained models, we use a 29-hour (91,449 frames) labeled dataset of IMU data (41 recordings and distinct participants) for two separate tasks: posture classification and movement classification. The data contains nine annotated movement categories (still, roll left/right, pivot left/right, proto/elementary/fluent movement, transition) and seven annotated posture categories (prone, supine, left/right side, crawl posture, sitting, standing) for each 2.3-second frame. For model training, we use all annotated data, but we only use the frames in which all annotators agreed on the label for model testing. We train our models separately for both classification tasks using the so-called iterative annotation refinement labels from Airaksinen et al. (2020).\nModel fine-tuning is run using recording-level 10-fold cross-validation on the 41 distinct recordings of the labeled dataset. We split each training fold into separate training and validation sets in a ratio of 80:20 recordings. The unweighted average F1 score (UAF1) on the validation set is used as the training criterion, and we select the best-performing model based on validation set UAF1 score. We use random sensor dropout (p = 0.3) for data augmentation during model fine-tuning. The final UAF1 score of fine-tuning is computed from an aggregate confusion matrix across all test folds. For further details regarding the pre-training and fine-tuning hyperparameters, see Appendix B."}, {"title": "SPEECH EMOTION RECOGNITION", "content": "We use the 56-hour subset of Finnish speech of the NICU-A corpus (Vaaras et al., 2023a) for our speech emotion recognition experiments. This subset contains 129,007 utterances with a sampling rate of 16 kHz, of which 5198 and 345 belong to annotated training and testing sets, respectively. Each annotated utterance in NICU-A contains binary labels for emotional valence (positive/non-positive) and arousal (high/low). We window each speech signal into 30-ms frames with a 20-ms overlap. Each sequence is z-score normalized, and we zero-pad or truncate each normalized sequence into 3-second segments (301 frames). See Vaaras et al. (2023a) for further details on NICU-A."}, {"title": "SLEEP STAGE CLASSIFICATION", "content": "For sleep stage classification, we use the pre-processed expanded Sleep-EDF Database (Kemp et al., 2000; Goldberger et al., 2000) from a study by Eldele et al. (2021). The dataset contains 30-second segments of the Fpz-Cz channel with a sampling rate of 100 Hz, comprising a total of 195,479 segments of EEG data. Each 30-second segment belongs to one of five annotated categories: wake, rapid eye movement (REM), non-REM stage 1, non-REM stage 2, or non-REM stages 3 and 4 combined. We z-score normalize each 30-second segment, and we window each segment into 4-second frames with 2 seconds of overlap, resulting into 14 frames for each segment.\nWe pre-train our models using all 195,479 EEG segments. We use the 14-frame sequences as our input for a three-layer CNN encoder with output channels [128, 128, 128, 128], kernel sizes [10, 8, 4], strides of [5, 5, 3], and paddings of [3, 2, 1]. Each convolution is followed by layer normalization, a GeLU nonlinearity, and dropout. The third CNN layer is followed by average pooling with a kernel size of 5 before dropout. We randomly split the EEG segments for pre-training into a training and validation set in a ratio of 80:20 segments.\nWe fine-tune our models for sleep stage classification using 10-fold cross-validation at the test subject-level on the 78 test subjects of the dataset. Each training fold is split into training and validation sets at the test subject-level in a ratio of 80:20 test subjects. Similar to Sec. 4.1, we use the validation UAF1 score as our training criterion, and the testing UAF1 score is computed from an aggregate confusion matrix across all test folds. For further details on the training hyperparameters, see Appendix B."}, {"title": "RESULTS", "content": "Table 1 presents the fine-tuning results of the comparison of our PFML method against MAE, data2vec, and not using pre-training at all. Across all three data modalities and five classification tasks, the results show that PFML outperformed MAE and achieved highly comparable results to data2vec. Using pre-training with any SSL method provided superior results as opposed to not using pre-training at all. For the classification of posture from IMU data, there were only minor differences in performance between different SSL methods. In sleep stage classification from EEG data, both MAE and PFML outperformed data2vec by a large margin. The comparison between PFML and MAE showcases that it is more beneficial to predict functionals than to predict the input signal."}, {"title": "ADDITIONAL HYPERPARAMETER EXPERIMENTS", "content": "In order to demonstrate that it is more beneficial during pre-training to mask the latent features instead of masking the input directly, we ran PFML pre-training for all three datasets twice: either by masking the inputs or by masking the embeddings. Subsequently, we fine-tuned our models for all five classification tasks, and the results are shown in Table 6 of Appendix C. As can be observed from the results, it is more beneficial for downstream tasks if we alleviate the complexity of the pre-training task for the encoder by masking the embeddings instead of masking the inputs. The only exception was with EEG data, where it did not make a difference whether inputs or embeddings were masked.\nFor each data modality, we also experimented with different configurations of masking probability p\u2098 and the length of the masks l\u2098. We ran PFML pre-training using different configurations of p\u2098 and l\u2098, and then we fine-tuned the pre-trained models. For IMU and speech data, we only experimented with one classification task each, namely classification of movement from IMU data and classification of valence from speech data. The results for different configurations of p\u2098 and l\u2098 for IMU, speech, and EEG data are shown in Appendix C in Tables 7, 8, and 9, respectively. For IMU data, the differences between different masking strategies are rather small, whereas for speech and EEG data the selection of masking hyperparameters has a notable effect on fine-tuning performance.\nWe also experimented with the effect of discarding some of the functionals in PFML pre-training for IMU data. After pre-training, we fine-tuned our model for movement classification, and the results are presented in Table 10 of Appendix C. The results indicate that using the full set of 11 functionals during PFML pre-training provides the best outcome. As the number of discarded"}, {"title": "CONCLUSION", "content": "In this paper, we presented PFML, a novel SSL algorithm for time-series data that avoids the common SSL issue of representation collapse. PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. We demonstrated the effectiveness of PFML using five different classification tasks across three different data modalities: infant posture and movement classification from multi-sensor IMU data, emotion recognition from speech data, and sleep stage classification from EEG data. Our results show that PFML is superior to a conceptually similar SSL method, MAE. Our results also show that PFML is competitive against the current state-of-the-art data modality agnostic SSL method, data2vec, while being conceptually simpler and without suffering from representation collapse. The fact that PFML matches the performance of data2vec while also avoiding the issue of representation collapse renders PFML more straightforward to apply to new time-series data domains, such as in the case of clinical time-series data. The present work may also be extended to other domains than time-series data, such as images where functionals could be computed of, e.g., image patches.\nLimitations We selected the present set of 11 functionals for their effectiveness across the three data modalities used in the present study, aiming for potential generalizability and a robust starting point to other data domains and downstream tasks. However, carefully selecting the number and type of functionals specifically for different modalities may lead to better results than presented here. Also, we did not include data augmentation in our pre-training processes to save computational time for PFML pre-training, as we wanted to pre-compute the functionals before the model training. As shown in e.g. Chen et al. (2020); Grill et al. (2020); He et al. (2022); Balestriero et al. (2023), data augmentation during pre-training may lead to improved performance on downstream tasks. Nonetheless, performing masking for randomly sampled frames is already a form of data augmentation in itself. Furthermore, other model architectures besides CNN-based encoders or Transformer encoder blocks could also be used, and this may improve PFML pre-training performance. Lastly, we acknowledge that typically SSL pre-training is run with very large minibatch sizes using multiple GPUs, and the results of the present experiments might improve with larger minibatch sizes. However, to promote reproducibility and encourage other researchers to try PFML, we deliberately pre-trained our models using relatively small minibatches so that the pre-training processes could be run on a single GPU with 16 GB of VRAM. As detailed in Appendix D, our method used only a moderate amount of computational resources.\nBroader Impacts Since the main goal of PFML is to make the algorithm straightforwardly applicable to different time-series data domains, our method makes it easier to apply SSL pre-training for time-series data without complex tuning of hyperparameters or the need to profoundly understand the target data domain. As an example, properties of different medical time-series data, such as those obtained with EEG, ECG, or EMG, can be dependent on the clinical environment, the specific measurement equipment and setup, or clinical population being measured (Watson et al., 2019). This limits the applicability of 'universal' pre-trained models predominant in computer vision and speech"}, {"title": "PROOF OF NON-COLLAPSED FEATURE REPRESENTATIONS IN PFML PRE-TRAINING", "content": "This section provides a more detailed mathematical formulation for the proof that PFML pre-training does not converge to collapsed feature representations.\nLet x be a single- or multi-channel time-series signal, framed into a sequence of short-term frames {X\u2080, X\u2081, ...} of N samples each, where x\u2099 = {X\u209c, X\u209c\u208a\u2081, ..., X\u209c\u208a\u2099\u208b\u2081}. We define a set of m func-tionals, F = {F\u2080, F\u2081, ..., F\u2098\u208b\u2081}, to be computed for each frame x\u2099 to produce a set of computed functionals f\u2099. Here, we refer to functionals as mathematical operations which map a time series of arbitrary length into a single value, such as the mean or variance of the signal. Also, let z\u2099 be the output embeddings of an encoder model given the input x\u2099, and let y\u2099 denote the output predictions of a Transformer-based model given the input z\u2099.\nTo formalize the relationships between inputs and outputs, let us define the following functions:\n\u2022 Let F be the set of functionals that maps the input frames x\u2099 to the computed functionals f\u2099, i.e., f\u2099 = F(x\u2099) = {F\u2080(x\u2099), F\u2081(x\u2099), ..., F\u2098\u208b\u2081(x\u2099)}.\n\u2022 Let g be the function that maps the embeddings z\u2099 to the predictions y\u2099, i.e., y\u2099 = g(z\u2099).\nLet us assume the following in PFML pre-training:\n\u2022 Assumption 1: There is temporal variability across the frames x\u2099. Formally, let \u03c3\u00b2(x\u2099) denote the variance of x\u2099 across the frames, and \u03c3\u00b2(x\u2099) > 0.\n\u2022 Assumption 2: Given Assumption 1, the set of non-trivial functionals F computed from x\u2099 also contains variance across the frames. Formally, let o\u00b2 (f\u2099) denote the variance of f\u2099 across the frames, and o\u00b2(f\u2099) > 0.\nUnder these assumptions, we aim to show that the predictions y\u2099 also contain variance across the frames, i.e., \u03c3\u00b2(y\u2099) > 0.\nIn PFML pre-training, the model learns to predict the computed functionals f\u2099 given the embeddings Z\u2099. The prediction loss L is defined as\nL = \\frac{1}{N} \\sum_{n=1}^{N} (f\u2099 - y\u2099)\u00b2 (3)\nfor the MSE loss and\nL = \\frac{1}{N} \\sum_{n=1}^{N} |f\u2099 - y\u2099| (4)\nfor the L1 loss.\nTo minimize either the MSE or L1 loss functions (Equations 3 and 4, respectively), the predictions y\u2099 must closely match the computed functionals f\u2099. If y\u2099 were to contain zero variance across the frames, i.e., \u03c3\u00b2(y\u2099) = 0, while f\u2099 contains variance, i.e., \u03c3\u00b2(f\u2099) > 0, the prediction loss L would be high. This is because the constant predictions y\u2099 would not be able to capture the temporal variability in f\u2099.\nTherefore, to achieve low prediction loss values, the predictions y\u2099 must also contain variance across the frames, i.e., \u03c3\u00b2(y\u2099) > 0. Consequently, PFML pre-training does not converge to collapsed feature representations, as long as Assumptions 1 and 2 hold true.\nGiven that real-world time-series data generally shows temporal variability, and computed functionals derived from such data are expected to reflect this variability, Assumptions 1 and 2 are valid for most real-world datasets."}]}