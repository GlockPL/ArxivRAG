{"title": "Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic Gradients", "authors": ["Zakaria Patel", "Sebastian J. Wetzel"], "abstract": "It has been demonstrated in many scientific fields that artificial neural networks like autoencoders or Siamese networks encode meaningful concepts in their latent spaces. However, there does not exist a comprehensive framework for retrieving this information in a human-readable form without prior knowledge. In order to extract these concepts, we introduce a framework for finding closed-form interpretations of neurons in latent spaces of artificial neural networks. The interpretation framework is based on embedding trained neural networks into an equivalence class of functions that encode the same concept. We interpret these neural networks by finding an intersection between the equivalence class and human-readable equations defined by a symbolic search space. The approach is demonstrated by retrieving invariants of matrices and conserved quantities of dynamical systems from latent spaces of Siamese neural networks.", "sections": [{"title": "1 Introduction", "content": "The current AI revolution is driven by artificial neural networks (ANNs), particularly deep learning models. These models have enabled machines to achieve superhuman performance in a variety of tasks, such as image recognition, language translation, game playing, and even generating human-like text. However, this remarkable power comes at the expense of interpretability, often referred to as the \"black box\" problem. The representational capacity of artificial neural networks relies on interactions between possibly billions of neurons. While each single neuron is easy to describe mathematically, as networks become larger, it becomes increasingly difficult to understand how these interactions give rise to a neural network's overall prediction.\n The black-box nature of neural networks can be acceptable in applications where prediction is the primary goal. However, in science, where the goal is not just prediction but also understanding the underlying phenomena, interpretability is crucial. Moreover, in medicine, it is important to understand why an AI system has made a particular diagnosis or treatment recommendation to avoid risks of dangerous or ethically questionable decisions (Jin et al., 2022; Amann et al., 2022). AI interpretability in the law domain is crucial for understanding and explaining how automated decisions are made, which helps ensure transparency and accountability. It also allows for the identification and correction of biases, compliance with regulations, and maintains the integrity of legal processes (Hacker et al., 2020; Bibal et al., 2020).\n In many scientific applications of neural networks, it can be verified that neural networks often learn meaningful concepts, similar to those that humans use, to describe certain phenomena (Ha and Jeong, 2021; Desai and Strachan, 2021; Nautrup et al., 2022). Unfortunately, without a method to distill this learned concept in a human-interpretable form, the only way to reveal it is by directly comparing it to a set of candidates that the researcher is already aware of. Clearly, it is not possible to make new discoveries in this way.\n To address this problem, symbolic regression techniques have been proposed to interpret neural networks by deriving closed-form expressions that represent the underlying concepts learned by these networks (Cranmer et al., 2020; Mengel et al., 2023). These approaches involve exploring the space of potential mathematical expressions to identify those that best replicate the predictions of a neural network. Unfortunately, such methods are limited to interpreting output neurons of neural networks performing regression, where the concept that is recovered is the global function learned by the neural network.\n Neural networks applied to perform scientific discovery are often tasked with solving problems that cannot be formulated under the umbrella of regression. Further, it is often necessary to interpret a simpler sub-concept encoded in hidden layers. For these reasons, it is desirable to have a framework capable of interpreting concepts encoded in arbitrary intermediate neurons of artificial neural networks.\n Prominent artificial scientific discovery methods are based on networks like autoencoders (Wetzel, 2017; Iten et al., 2020; Miles et al., 2021; Frohnert and van Nieuwenburg, 2024) or Siamese networks (Wetzel et al., 2020; Patel et al., 2022; Han et al., 2023). These networks can conceive meaningful concepts inside their latent spaces without explicit training information in the form of labeled targets. The crucial obstacle is the lack of tools that enable the recovery of such concepts without prior knowledge of them. Removing this bottleneck would hence allow scientists the discovery of potentially unknown scientific insights.\n In this publication, we describe a framework that can be employed to interpret any single"}, {"title": "2 Related Work", "content": "The current manuscript contributes to the domain of artificial neural network interpretability with the focus of enabling new scientific discovery through latent space models. Almost all of the neural network interpretability research works in science manage to confirm or deny whether neural networks learn certain known scientific concepts. While verifying a neural network is an important task, it is unsuitable for gaining novel scientific insights. There has been limited progress toward revealing scientific insights in symbolic form from artificial neural networks that do not require previous knowledge of the underlying concept beforehand (Wetzel and Scherzer, 2017; Cranmer et al., 2020; Miles et al., 2021; Liu and Tegmark, 2021). These cases are rare examples where the underlying concept is encoded in a linear manner or other properties of the concept simplify the interpretation problem. While there are no unified approaches to interpreting latent space models, it might in principle be possible to build such models based on architectures with symbolic layers (Martius and Lampert, 2016; Sahoo et al., 2018; Dugan et al., 2020; Liu et al., 2024)\n Our article aims to interpret existing latent space models. We extend an interpretation framework (Wetzel, 2024) originally developed to interpret neural network classifiers to interpret neural network latent spaces.\n The interpretation method relies on efficiently searching the space of symbolic equations which can be achieved by genetic search algorithms which form the backend of many symbolic regression algorithms. These include Eureqa (Schmidt and Lipson, 2009), Operon C++(Burlacu et al., 2020), PySINDy (Kaptanoglu et al., 2022), Feyn(Brol\u00f8s et al., 2021), Gene-pool Optimal Mixing Evolutionary Algorithm (Virgolin et al., 2021), GPLearn (Stephens, 2022) and PySR(Cranmer, 2023). Other symbolic regression algorithms include deep symbolic regression uses recurrent neural networks (Petersen et al., 2020), symbolic regression with transformers(Kamienny et al., 2022; Biggio et al., 2021) or AI Feynman (Udrescu and Tegmark, 2020).\n An overview of interpretable scientific discovery with symbolic Regression can be found in(Makke and Chawla, 2022; Angelis et al., 2023)."}, {"title": "3 Method", "content": "neuron within an artificial neural network in closed form. We demonstrate the power of our framework by rediscovering the explicit formulas of matrix invariants and conserved quantities from the latent spaces of Siamese networks.\n Concepts encoded in neurons in hidden layers are in general not stored in a human-readable form, but they get distorted and transformed in a highly non-linear fashion. Hence, the interpretation method is based on constructing an equivalence class around a certain neuron that contains all functions encoding the same concept as the target neuron. In practice, we interpret the neuron by searching a closed-form representative function contained in this equivalence class."}, {"title": "3.1 Siamese Neural Networks", "content": "Siamese neural networks (SNN) (Baldi and Chauvin, 1993; Bromley et al., 1993) were originally introduced to solve fingerprint recognition and signature verification problems. SNNs consist of two identical sub-networks with shared parameters, each receiving distinct inputs which are"}, {"title": "3.2 Interpretation Framework", "content": "A neural network which is trained to perform a task requiring knowledge of some concept g does not necessarily encode it in the same, human-readable format that we use. Instead, it is more likely that it learns some function f, which is related to g by some highly nonlinear and uninterpretable transformation \u03c6,\n\n \\begin{equation}\nf(x) = \\varphi\\left(\\begin{array}{c}g(x)\\end{array}\\right).\n\\end{equation}\n\nuninterpretable transformation closed form concept\nDifferent realizations of neural networks f might learn the same concept g and therefore contain the same information. More formally, these realizations are all members of the following equivalence class:\n\\begin{equation}\nH_g = \\{f(x) \\in C^1 (D \\subset\\mathbb{R}^n, \\mathbb{R}) \\mid \\exists \\text{ invertible } \\varphi \\in C^1(\\mathbb{R}, \\mathbb{R}) : f(x) = \\varphi(g(x))\\}.\n\\end{equation}\nWhile each network uses a unique invertible transformation \u03c6, they are functionally equivalent in that they learn the same concept from the data. At this point, we ask the question, whether it is possible to identify the concept g without knowing the function \u03c6.\n\\begin{equation}\ng(x) = \\varphi^{-1} (f(x)).\n\\end{equation}\nIn order to avoid the necessity of knowing \u03c6, we rewrite the equivalence class (2) such that membership can be defined without explicit information about \u03c6. Since all $f \\in \\hat{H}$, are required to be continuously differentiable, we can show that the gradients of the two functions f and g point in the same direction,\n\\begin{equation}\n\\nabla f(x) = \\varphi'(g(x)) \\cdot\\nabla g(x) \\text{ where } ||\\varphi' (g(x))|| > 0.\n\\end{equation}\nHere we used that \u03c6, by construction, is invertible. Since \u03c6'(g(x)) is merely a scaling factor, this equation allows us to define a new equivalence class $H_g \\subseteq H_g$,\n\\begin{equation}\nH_1 = \\left\\{ f(x) \\in C^2 (D \\subset\\mathbb{R}^n, \\mathbb{R}) \\middle| \\nabla_x \\left(\\frac{\\nabla f(x)}{||\\nabla f(x)||} - \\frac{\\nabla g(x)}{||\\nabla g(x)||} \\right) \\cdot \\nabla f(x) = 0, \\forall x \\in D \\right\\}.\n\\end{equation}\nIn the next section, we discuss how to obtain a closed-form expression which is a member of (5)."}, {"title": "3.3 Symbolic Search", "content": "Symbolic regression is a regression analysis technique that has traditionally been used to find closed-form expressions that approximate the relation between target and input variables for a given dataset. Typically, this is done by employing a genetic algorithm, which evolves a population of candidate formulas using genetic operations like selection, crossover, and mutation, aiming to find the least complex tree of operators T that maps inputs X to outputs Y. In the context of neural network interpretation, symbolic regression is employed to convert a complex model into an interpretable tree representation.\n In our case, we search for a simple tree T which represents a function $g \\in H_g$. However, instead of performing regression on a set of prediction targets to find the best fitting function, we search for an analytical expression whose normalized gradients are as close as possible to those of f. Because of this difference, we refer to this approach as symbolic search instead of symbolic regression. Note that this requires that T consists of operators that yield a differentiable function.\n The objective function we choose is the mean-squared-error (MSE), which measures the\ndistance between the normalized gradients $g_T(x) = \\frac{\\nabla T(x)}{||\\nabla T(x)||}$, and $g_f(x) = \\frac{\\nabla f(x)}{||\\nabla f(x)||}$,\n\\begin{equation}\nMSE(g_T(X), g_f(X)) = \\frac{1}{n} \\sum_{i=1}^n ||g_T(x_i) - g_f(x_i)||^2 .\n\\end{equation}"}, {"title": "3.4 Algorithms", "content": "Implementing our framework involves three main algorithms which summarize the preceding sections:\n1. Train the model $F_\\theta$ to learn the invariant. See algorithm 1.\n2. Choose a neuron to interpret. This neuron computes $f_{\\theta'}(x)$, where \u03b8' \u2286 \u03b8. Treat this as the latent model and compute its gradient with respect to the input, i.e., $\u2207_x f_{\\theta'}(x)$. See algorithm 2.\n3. Apply symbolic search to find a symbolic tree T whose gradients point in the same direction as $f_{\\theta'}$. See algorithm 3."}, {"title": "4 Experiments", "content": "We retrieve the invariants of matrices and various physical systems using our method. We consider invariants of matrices under similarity and Lorentz transformations. Additionally, we investigate dynamical systems characterized by a variety of potentials, as well as the invariants in Minkowski spacetime."}, {"title": "4.1 Datasets and Training", "content": ""}, {"title": "4.1.1 Invariants Under the Similarity Transformation", "content": "In experiments 1-3, 5 in Table 1, we search for the trace and determinant of matrices under the similarity transformation. Each data point is a triplet consisting of three matrices of dimension n: an anchor matrix A, a positive example P, and a negative example N. The anchor is sampled by generating a random matrix.\n The positive example shares one or more invariants with the anchor. In the case of the similarity transformation, these invariants are the trace and determinant. To this end, we sample a n\u00d7 n invertible matrix M and apply the similarity transformation $P = MAM^{-1}$. The negative example should not share invariants with the anchor, which is trivially achieved by sampling another matrix N, which is almost certainly characterized by different invariants.\n In practice, we find that the neural network prefers to learn the trace. To discover a second invariant, such as the determinant, we sample triplets in which all matrices have the"}, {"title": "4.1.2 Invariants of Antisymmetric Matrices", "content": "For antisymmetric matrices in experiment 4, we prepare our dataset in the same way as we describe in 4.1.1. We first sample an antisymmetric 3 \u00d7 3 matrix for the anchor A, followed by a similarity transformation for the positive sample P. Finally, we sample a new antisymmetric matrix for the negative sample N. While both the anchor and negative samples are antisymmetric, the positive sample does not inherit this property under the transformation $P = MAM^{-1}$ when M is not orthonormal, because antisymmetry is not preserved under a general change of basis. Hence, we use all 9 entries of the matrix as input, although we acknowledge that one could easily enforce that M is orthonormal, in which case only 3 inputs would be needed from each of A, P, and N.\n We attempt to retrieve (8), which is invariant under the similarity transformation. Since we use the antisymmetric anchor matrix A as input when computing $\u2207xf(x)$, we expect that the result of symbolic search would simplify to (10)."}, {"title": "4.1.3 Invariants Under the Lorentz Transformation", "content": "In experiment 6, we apply the Lorentz transformation to the field strength tensor $F_{\u03bc\u03bd}$, which gives rise to the Lorentz invariants X and Y. Since the antisymmetry of $F_{\u03bc\u03bd}$ is preserved under the Lorentz transformation, each member of a triplet is antisymmetric, so we only use the 6 off-diagonal entries above (or equivalently below) the main diagonal as our input to the neural network. The anchor is a vector of these 6 entries from $F_{\u03bc\u03bd}$,"}, {"title": "4.1.4 Potentials", "content": "The experiments in Table 2 correspond to motion in a potential, where we simulate trajectories by randomly sampling initial positions and velocities, and subsequently evolve these systems according to Hamilton's equations. For each triplet ($x_A, x_P, x_N$), the anchor $x_A$ and positive sample $x_P$ are measurements at two different points along the same trajectory, while the"}, {"title": "4.1.5 Spacetime", "content": "In experiment 12 in Table 3, each triplet again consists of an anchor $x_A$, a positive sample $x_P$, and a negative sample $x_N$. The anchor is a randomly sampled four-vector representing an event in Minkowski spacetime. The positive sample is generated by applying a Lorentz transformation to the anchor, ensuring that the spacetime interval remains invariant. The negative sample, on the other hand, is another randomly generated four-vector that does not share the same spacetime interval as the anchor, allowing the neural network to distinguish between vectors that do and do not preserve this invariant."}, {"title": "4.2 Results", "content": "We summarize the results of our experiments in Tables 1-3. For each experiment, we use the method outlined in section 3 to obtain a set of predicted expressions, which we present as a Pareto plot in Figure 4 and Figure 3. We identify the solution that most closely matches the correct expression, denoted as retrieved expression in tables 1-3. It is interesting to note that the correct expression is often the one corresponding to the steepest drop in the loss. We note that in some cases, the network learned a polynomial approximation to the desired expression, which we rectified by increasing the sampling range used to produce the dataset.\nAll obtained solutions match the correct expressions. It is possible for the symbolic search algorithm to instead return a solution that matches the ground truth one up to a piecewise invertible transformation, although we do not observe this in our experiments. We also note that the symbolic search algorithm may approximate the correct solution, or add simplifications to it. For example, the solution denoted by the striped green bar in Figure 4 (c) a uses $exp(x_1x_1) \u2248 1 + x_1^2 + x_1^2$, which matches the correct solution up to the fourth order in x. In Figure 4 (d), the expression 2 exp(x + 1) was simplified to exp(x + 1 + ln(2)) \u2248 exp(x + 1.684).\n Furthermore, because the network's latent layer consists of only a single neuron, we can directly compare the value it encodes for inputs X to the true underlying concept g(X). We plot these quantities against each other in Figure 2. Note that these correlation plots are not a necessary component of our interpretation framework. We use them only to highlight the non-linear manner in which the neural network encodes the concept. In most experiments, the values encoded in the latent space are highly correlated with some well-known concept. In fact, the correlation plots for the trace are almost linear, which is expected as they can trivially be learned by a single-layer neural network with no non-linearities. In such cases, it may be possible to use other methods to interpret the neural network. However, most invariants are significantly more complex, and the neural network will encode them in a non-linear manner, in which case most other interpretation methods will fail."}, {"title": "5 Conclusions", "content": "In this manuscript, we develop a framework to interpret any single neuron in neural network latent spaces in the form of a symbolic equation. It is based on employing symbolic search to find a symbolic tree that exhibits the same normalized gradients as the examined latent space neuron. We justify this procedure by defining an equivalence class of functions encoding the"}]}