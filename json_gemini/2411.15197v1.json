{"title": "K-means Derived Unsupervised Feature Selection using Improved ADMM", "authors": ["Ziheng Sun", "Chris Ding", "Jicong Fan"], "abstract": "Feature selection is important for high-dimensional data analysis and is non-trivial in unsupervised learning problems such as dimensionality reduction and clustering. The goal of unsupervised feature selection is finding a subset of features such that the data points from different clusters are well separated. This paper presents a novel method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means. We develop an alternating direction method of multipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS model. Extensive experiments on real datasets show that our K-means UFS is more effective than the baselines in selecting features for clustering.", "sections": [{"title": "I. INTRODUCTION", "content": "FEATURE selection aims to select a subset among a large number of features and is particularly useful in dealing with high-dimensional data such as gene data in bioinformatics. The selected features should preserve the most important information of the data for downstream tasks such as classification and clustering. Many unsupervised feature selection methods have been proposed in the past decades. They can be organized into three categories [1], [2]: filter methods [3], wrapper methods [4], and hybrid methods. Filter methods evaluate the score of each feature according to certain criteria, such as Laplacian score (LS) [5], [6] and scatter separability criterion [7]. Wrapper methods utilize algorithms to evaluate the quality of selected features. They repeat selecting a subset of features and evaluating the performance of an algorithm on these features until the performance of the algorithm is desired. Correlation-based feature selection [8] and Gaussian mixture models [9] are representative ones of wrapper methods. Hybrid methods [10], [11] utilize filtering criteria to select feature subsets and evaluate the feature subsets by algorithm performance.\nWithout labels to evaluate the feature relevance, many criteria have been proposed for unsupervised feature selection in recent years. The most widely used one is to select features that can preserve the data similarity using Laplacian matrix. For instance, Multi-Cluster Feature Selection (MCFS) [12] selects features using spectral analysis and a regression model with \\(l_1\\)-norm regularization. Non-negative Discriminative Feature Selection (NDFS) [13] selects features using non-negative spectral analysis and a regression model with \\(l_{2,1}\\)-norm regularization. Robust Unsupervised Feature Selection (RUFS) [14] selects features using label learning, non-negative spectral analysis and a regression model with \\(l_{2,1}\\)-norm regularization. Joint Embedding Learning and Sparse Regression (JELSR) [15] selects features using embedding learning and sparse regression jointly. Li et al. [16] developed a sampling scheme called feature generating machines (FGMs) to select informative features on extremely high-dimensional problems. Non-negative Spectral Learning and Sparse Regression-based Dual-graph regularized feature selection (NSSRD) [17] extends the framework of joint embedding learning and sparse regression by incorporating a feature graph. Wang et al. [18] proposed to select features using an autoweighted framework based on a similarity graph. Embedded Unsupervised Feature Selection (EUFS) [19] used sparse regression and spectral analysis. Li et al. [20] proposed an unsupervised feature selection method based on sparse PCA with \\(l_{2,p}\\)-norm. Sparse and Flexible Projection for Unsupervised Feature Selection with Optimal Graph (SF2SOG) [21] selects features using the optimal flexible projections and orthogonal sparse projection with \\(l_{2,0}\\)-norm constraint. All these methods select features based on data similarity using spectral analysis, but they don't focus on the separability of data points under the selected feature space.\nIn this work, we present a new method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike those spectral analysis based methods, we select features to minimize the K-means objective proposed by [22], [23]. The goal of our method is to select the most discriminative features such that the data points are well separated, that is, have small within-cluster differences and large between-cluster differences. We focus on the separability of data points and derive this new unsupervised feature selection method from K-means. The contributions of this work are as follows.\n\u2022 A novel unsupervised feature selection method is proposed to select the most discriminative features based on the objective of K-means.\n\u2022 An Alternating Direction Method of Multipliers (ADMM) [24] algorithm is developed for the NP-hard problem of K-means UFS model.\n\u2022 We compare K-means UFS with other state-of-the-art unsupervised feature selection methods and conduct experiments on real datasets to demonstrate the effectiveness of our method.\nThe rest of this paper is organized as follows. In Section II, we derive the K-means UFS model from K-means objective. In Section III, we develop an ADMM algorithm to solve"}, {"title": "II. K-MEANS DERIVED UNSUPERVISED FEATURE SELECTION", "content": "In unsupervised feature selection, there is no unique criteria to evaluate the quality of selected features. We choose to select features by minimizing the objective of K-means clustering proposed by [22], [23]. Given a data matrix \\(X = (x_1, x_2,...,x_n) \\in \\mathbb{R}^{p \\times n}\\), where \\(p\\) denotes the number of features and \\(n\\) denotes the number of samples. Each feature (row) of \\(X\\) is standardized to have zero mean and unit variance. In K-means clustering, the \\(k\\) centroids are determined by minimizing the sum of squared errors,\n\\[ J_k = \\sum_{j=1}^k \\sum_{i \\in C_j} ||x_i - m_j||^2, \\] (1)\nwhere \\(m_j = \\sum_{i \\in C_j} x_i/n_j\\) is the centroid of cluster \\(C_j\\) consisting of \\(n_j\\) points, \\(j = 1,...,k\\). According to [22], \\(J_k\\) can be reformulated as\n\\[ J_k = Tr(X^T X) - Tr(G^T X^T X G), \\] (2)\nwhere \\(G = (g_1,..., g_k) \\in \\mathbb{R}^{n \\times k}\\) is a normalized indicator matrix denoting whether a data point is in a cluster or not, namely,\n\\[ g_{ij} = \\begin{cases} 1/\\sqrt{n_j}, & \\text{if } x \\in C_j \\\\ 0, & \\text{otherwise}. \\end{cases} \\] (3)\nLet \\(\\Psi\\) be the feasible set of all possible indicator matrices with \\(k\\) clusters. Since \\(Tr(X^T X)\\) is a constant, [22] pointed out that K-means clustering is equivalent to\n\\[ \\min_{G \\in \\Psi} - Tr(G^T X^T X G). \\] (4)\nOur K-means derived unsupervised feature selection (K-means UFS) seeks to select the most discriminative features. The input data \\(X\\) contains \\(p\\) rows of features. Let \\(X_h \\in \\mathbb{R}^{h \\times n}\\) be the selected \\(h\\) rows of \\(X\\). Our feature selection goals is the following: *Among all possible choice of \\(X_h\\), we selection"}, {"title": "A. K-means UFS model", "content": "the \\(X_h\\) which minimizes the K-means objective. Therefore, K-means UFS solves the following problem\n\\[ \\min_{X_h} [\\min_{G \\in \\Psi} Tr(G^T X_h^T X_h G)] \\] (5)\nIn order to show the intuition of model (5), we generate a toy data matrix \\(X \\in \\mathbb{R}^{4 \\times 30}\\), shown in Figure 1. The left plot shows \\(X_{h1}\\) consisting of the first two rows of \\(X\\), while the right one shows \\(X_{h2}\\) consisting of the last two rows of \\(X\\). We prefer \\(X_{h2}\\) because the within-cluster differences are much smaller than those in \\(X_{h1}\\). It is expected that solving problem (5) can select the last two features. We want to select the most discriminative features in an unsupervised manner.\nIn this section, we show that problem (5) can be formulated into a discrete quadratic optimization problem. Let \\(S \\in \\mathbb{R}^{p \\times h}\\) be a selection matrix defined as:\n\\[ X_h = S^T X, \\text{ s.t. } S^T S = I, S_{ij} \\in \\{0,1\\} \\] (6)\nLet \\(\\Phi\\) be the feasible set of all the selection matrix \\(S\\), we rewrite problem (5) as:\n\\[ \\min_{G \\in \\Psi, S \\in \\Phi} -Tr(S^T X G G^T X^T S) \\] (7)\nTherefore (7) solves both the K-means clustering problem (the optimal \\(G\\)) and the feature selection problem (optimal \\(S\\)) simultaneously.\nProblem (7) is difficult to optimize. Fortunately, the approximate solution to K-means clustering was obtained in [22], [23]. The approximate solution of K-means indicator \\(G^*\\) can be constructed as the following. Let the singular value decomposition (SVD) of \\(X\\) be \\(X = P \\Sigma Q^T\\), where \\(P\\) and \\(Q\\) denote the left and right singular vectors and the singular values in \\(\\Sigma\\) are sorted decreasingly. Let \\(P_k\\) be the first \\(k\\) columns of \\(P\\), \\(Q_k\\) be the first \\(k\\) columns of \\(Q\\) and \\(\\Sigma_k\\) be the first \\(k\\) singular values of \\(\\Sigma\\). [22], [23] showed that \\(Q_k\\) is a good approximation of \\(G^*\\).\nNow, with this approximate optimal solution for \\(G\\), we rewrite the objective of problem (7) as\n\\[ \\min_{S \\in \\Phi} - Tr(S^T P \\Sigma_k Q_k^T Q_k \\Sigma_k P^T S) \\]"}, {"title": "B. Relaxation", "content": "then, the feature selection optimization problem (7) is simplified to\n\\[ \\min_{S \\in \\Phi} Tr(S^T P \\Sigma_k \\Sigma_k P^T S) = -Tr(S^T A S) \\] (8)\nwhere\n\\[ A = P \\Sigma_k \\Sigma_k P^T \\]\nis in fact the largest \\(k\\) rank of the square of data covariance matrix. Problem (8) is our model for K-means UFS.\nProblem (8) is a discrete optimization problem which is typically NP-hard. To solve this problem we use numerical relaxation. From Eq.(6), we infer that \\(S\\) has \\(h\\) nonzero rows. This can be seen as the following. The \\(p\\) rows of \\(X\\) can be reordered such that the selected rows are reshuffled to the top \\(h\\) rows of \\(X\\),\n\\[ X = \\begin{bmatrix} X_h \\\\ X_{-h} \\end{bmatrix} = \\begin{bmatrix} I_{h \\times h} \\\\ 0_{(p-h) \\times h} \\end{bmatrix} \\]\nwhere \\(X_{-h}\\) represents the rest of rows un-selected, \\(I_{h \\times h}\\) is an identity matrix and \\(0_{(p-h) \\times h}\\) is a zero matrix. Therefore, \\(S\\) has three constraints:\n\\[ S^T S = I, ||S^T||_{2,0} = h, S_{ij} \\in \\{0,1\\} \\] (9)\nNow, from the K-means UFS model (8), if \\(S^*\\) is an optimal solution, \\(V = S^* R\\) (where \\(R \\in \\mathbb{R}^{h \\times h}\\) is a rotation matrix, that is, \\(R R^T = I\\).) is also an optimal solution, because \\(Tr(S^T A S) = Tr(R^T S^T A S R)\\). Thus the binary discrete constraint in Eq.(9) is not necessary. Therefore, the relaxed \\(V\\) has only two constraints:\n\\[ V^T V = I, ||V^T||_{2,0} = h \\] (10)\nFinally, we solve the K-means UFS model by the following relaxed optimization problem.\n\\[ \\min_V - Tr(V^T A V) \\]\n\\[ \\text{s.t. } V^T V = I, ||V^T||_{2,0} = h \\] (11)\nNote that once the optimal solution \\(V^*\\) is obtained, the feature selection matrix \\(S^*\\) is determined uniquely by the index of the \\(h\\) nonzero rows of \\(V^*\\). The value of rotation matrix \\(R\\) has no contribution to feature selection."}, {"title": "III. OPTIMIZATION: AN IMPROVED ADMM", "content": "In this section, we elaborate how to solve optimization problem (11) using ADMM.\nWe consider an equivalent form of problem (11):\n\\[ \\min_V - Tr(V^T A V) \\]\n\\[ \\text{s.t. } V = U, U^T U = I \\]\n\\[ V = W, ||W^T||_{2,0} = h. \\] (12)\nThen the augmented Lagrangian function of (12) is\n\\[ L_{\\mu}(V, U, W) = \u2013 Tr(V^T A V) + \\frac{\\mu}{2} ||V \u2013 U + \\Omega/\\mu||_F^2 + \\frac{\\mu}{2} ||V - W + \\Gamma/\\mu||_F^2 + const \\] (13)\nwhere \\(\\Omega, \\Gamma \\in \\mathbb{R}^{p \\times h}\\) are Lagrange multiplier matrices, \\(\\mu > 0\\) is a penalty parameter, \\(U\\) is an orthogonal matrix and \\(W\\) is a row-sparse matrix. Then we update the variables alternately [24]:\n\\[ V^{t+1} = \\arg \\min_V L_{\\mu}(V, U^t, W^t) \\] (14)\n\\[ U^{t+1} = \\arg \\min_U L_{\\mu}(V^{t+1}, U, W^t) \\] (15)\n\\[ W^{t+1} = \\arg \\min_W L_{\\mu}(V^{t+1}, U^{t+1}, W) \\] (16)\n\\[ \\Omega^{t+1} = \\Omega^t + \\mu^t (V^{t+1} \u2013 U^{t+1}) \\] (17)\n\\[ \\Gamma^{t+1} = \\Gamma^t + \\mu^t (V^{t+1} \u2013 W^{t+1}) \\] (18)\n\\[ \\mu^{t+1} = \\mu^t \\times \\rho, \\rho = 1.05. \\] (19)\nStep 1: Update \\(V\\)\nLet \\(B = \\frac{\\mu}{\\mu} U^T\\), \\(C = \\frac{\\mu}{\\mu} W^T - \\frac{\\Gamma}{\\mu}\\), after algebra, the update \\(V\\) step (14) is solving the following problem:\n\\[ \\min_V -Tr(V^T A V) + \\frac{\\mu}{2} ||V \u2013 B||_F^2 + \\frac{\\mu}{2} ||V \u2013 C||_F^2 \\] (21)\nTake the derivative of this objective function to be zero, we can update \\(V\\) in \\(t\\) iteration by:\n\\[ V^{t+1} = \\frac{\\mu}{\\mu} (\\mu I \u2013 A)^{-1} (B + C) \\] (22)\nTo guarantee the minimal solution exists, \\((\\mu I \u2212 A)\\) should be a positive definite matrix. Suppose \\(\\lambda_1\\) is the max eigenvalue of \\(A\\), we select the initial value of \\(\\mu\\) as \\(\\mu^0 = \\lambda_1 + 0.1\\). Step 2: Update \\(U\\)\nThe update \\(U\\) step (15) is solving the following problem:\n\\[ \\min_U \\frac{\\mu}{2} ||V^{t+1} \u2013 U + \\Omega^T / \\mu||_F^2 \\]\n\\[ \\text{s.t. } UU^T = I \\]\nLet \\(D = V^{t+1} + \\Omega^T/\\mu^t\\), it is equivalent to:\n\\[ \\max_U Tr(DU) \\]\n\\[ \\text{s.t. } UU^T = I \\] (23)\nLet \\(P_h\\) be the first \\(h\\) column of \\(P\\) from \\(D = P \\Sigma Q^T\\) (SVD). The solution is:\n\\[ U^{t+1} = P_h Q^T \\] (24)\nStep 3: Update \\(W\\)\nLet \\(F = V^{t+1} + \\Gamma^T/\\mu^T\\), the update \\(W\\) step (16) is solving the following problem:\n\\[ \\min_W ||F \u2013 W|| \\]\n\\[ \\text{s.t. } ||W^T||_{2,0} = h \\] (25)\nNow we split \\(W\\) and \\(F\\) by rows:\n\\[ W^T = [w_1^T, w_2^T, ..., w_p^T] \\]\n\\[ F^T = [f_1^T, f_2^T, ..., f_p^T] \\]"}, {"title": "A. Vanilla ADMM and its limitation", "content": "Select \\(l = \\{l_1, l_2, ..., l_h\\}\\) as the subset of \\(h\\) row indices in \\(F\\) satisfy:\n\\[ ||f_{l_1}||_2 \\geq ||f_{l_2}||_2 > \u00b7\u00b7\u00b7 \\geq ||f_{l_h}||_2 \\geq ||f_i||_2, \\forall j \\notin l \\]\nThus, we update each row of \\(W\\) in \\(t\\) iteration by:\n\\[ w_j^{(t+1)} = \\begin{cases} f_i, & \\text{ if } j \\in l \\\\ 0, & \\text{ if } \\forall j \\notin l \\end{cases} \\] (26)"}, {"title": "B. Bi-Linear ADMM", "content": "The first trick to avoid scale blow-up is to force \\(||V||_F\\) to be a constant in each iteration of ADMM. Considering \\(V\\) is an orthogonal matrix in Eq. (11), the norm of \\(V\\) should satisfy the constraint:\n\\[ ||V||_F^2 = Tr(V^T V) = Tr(I_{h \\times h}) = h \\]\nThe second trick is to change the quadratic objective function (\\(\u2212Tr(V^T AV)\\)) into a bi-linear one (\\(\u2212Tr(V^T AU)\\)). This trick can avoid the matrix inversion in Eq.(22). Using these two tricks, we obtain an equivalent bi-linear form of Quadratic ADMM Eq.(12) as following:\n\\[ \\min_V - Tr(V^T AU) \\]\n\\[ \\text{s.t. } ||V||_F^2 = h, V = U, U^T U = I \\]\n\\[ V = W, ||W^T||_{2,0} = h \\] (27)\nThe \\(||V||_F^2 = h\\) is necessary for Bi-linear ADMM, though it is redundant in math. We can see it in the update \\(V\\) step.\nLet \\(B = \\frac{\\mu}{\\mu} U^T\\) and \\(C = \\frac{\\mu}{\\mu} W^t \u2013 \\frac{\\Gamma}{\\mu}^t\\), the update \\(V\\) step of problem (27) is solving the following problem:\n\\[ \\min_V - Tr[(U^T A + \\mu^t B^T + \\mu^t C^T)V] \\]\n\\[ \\text{s.t. } ||V||_F^2 = h \\]\nLet \\(D = U^t A + \\mu^t B^T + \\mu^t C^T\\), the solution is:\n\\[ V^{t+1} = \\frac{\\sqrt{h}}{||D||_F} D \\] (29)\nThe \\(||V||_F^2 = h\\) constraint is necessary to guarantee the minimal solution for problem (28) exits. In each iteration, updating \\(V\\) by Eq. (29) will force \\(||V||_F\\) to be a constant \\(h\\). In Figure 2, the log10 \\(||V||_F\\) (red) of Bi-linear ADMM is always a constant.\nLet \\(E = V^{t+1} + \\Omega^T / \\mu^t\\), after algebra, the update \\(U\\) step of Bi-linear ADMM is solving the following problem:\n\\[ \\min_U \u2013 Tr(V^{(t+1)T} A U) + \\frac{\\mu}{2} ||U \u2013 E||_F^2 \\]\n\\[ \\text{s.t. } UU^T = I \\] (30)\nLet \\(H = AV^{t+1} + \\mu E\\), \\(P_h\\) be the first \\(h\\) column of \\(P\\) from \\(H = P \\Sigma Q^T\\) (SVD). The solution is:\n\\[ U^{t+1} = P_h Q^T \\] (31)\nThe update \\(W\\) step is exactly the same as Eq. (26), so we omit it here. With these update rules, our K-means UFS using Bi-linear ADMM is summarized in Algorithm 1."}, {"title": "C. Discussion on The Initialization", "content": "\\(V\\), \\(U\\), \\(W\\) should be initialized into the same value because of the \\(V = U\\) and \\(V = W\\) constraint. We initialize \\(V\\) by removing the discrete \\(l_{2,0}\\)-norm constraint in problem (11):\n\\[ \\min_V -Tr(V^T AV) \\text{ s.t. } V^T V = I \\] (32)"}, {"title": "IV. OTHER UNSUPERVISED FEATURE SELECTION METHODS", "content": "Let \\(P_h\\) be the first \\(h\\) column of \\(P\\) from \\(A = P \\Sigma P^T\\) (SVD), then we initialize \\(V = P_h\\). Lagrange multiplier matrices \\(\\Omega = \\Gamma = 0\\). \\(\\mu\\) is empirically set in the range from \\(10^{-4}\\) to \\(10^{-1}\\) depending on the datasets and is updated by \\(\\mu^{t+1} = \\mu^T \\times \\rho\\) in each iteration. If \\(\\mu\\) is larger than \\(\\mu_{max} = 10^7\\), we stop updating \\(\\mu\\). \\(\\rho\\) is empirically set to 1.05 in our algorithm.\nD. Discussion on The Convergence\nThe convergence proof of ADMM can be found in [24]. In ADMM process, \\(W\\) is always a row-sparse matrix with \\(h\\) nonzero rows (Eq. (26)) which represents the \\(h\\) selected features. In \\(t\\) iteration, we can recover a unique feature selection matrix \\(S^T\\) determined by the \\(h\\) nonzero row indices of \\(W^T\\) because of \\(W^T = V^T\\) and \\(V^T = S^T R\\). Our termination criteria is: If \\(S^T\\) doesn't change in 30 iterations, we stop the ADMM and output the selected features. In practice, we set the maximum iteration value as 3000. In experiment, our algorithm converges within 300 iterations for all datasets.\nE. Discussion on The Time Complexity\nThe time complexity of update \\(V\\) step (29) involves the computation of \\(D\\) and its Frobenius norm, which is both \\(O(np)\\). The time complexity of update \\(U\\) (31) involves computation of \\(E\\) and its SVD, which is \\(O(np)\\) and \\(O(np^2)\\). The time complexity of update \\(W\\) Eq.(26) involves computation of \\(F\\) and sort the norm of rows in \\(F\\), which is \\(O(np)\\) and \\(O(p \\log(p))\\). Since \\(n \\gg p\\), the time complexity of each iteration is \\(O(np^2)\\).\nF. Discussion on The Reproducibility\nThere are no adjustable parameters in the relaxed K-means UFS model (10). The two parameters \\(\\mu\\) and \\(\\rho\\) come from the ADMM. If we set \\(\\rho = 1.05\\) and using the initialization settings above. Our algorithm is a deterministic algorithm with reproducibility for all the datasets we used.\nIn this section, we compare K-means UFS with other state-of-the-art unsupervised feature selection (UFS) methods. Most other UFS methods select features using spectral analysis [5], [6] and sparse regression. They tends to select features that can preserve the structure of similarity matrix. Our K-means UFS (5) select features in a totally different way. We select the most discriminative features using the K-means objective which have smaller within-cluster difference and larger between-cluster difference.\nFor convenience, we denote original data \\(X \\in \\mathbb{R}^{p \\times n}\\), similarity matrix \\(\\tilde{S} \\in \\mathbb{R}^{n \\times n}\\), \\(1 = [1, ..., 1]^T\\), degree matrix"}, {"title": "A. Discussion and Comparison", "content": ""}]}