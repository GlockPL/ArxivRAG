{"title": "MedPix 2.0: A Comprehensive Multimodal Biomedical Dataset for Advanced AI Applications", "authors": ["Irene Siragusa", "Salvatore Contino", "Massimo La Ciura", "Rosario Alicata", "Roberto Pirrone"], "abstract": "The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality dataset, mainly due to privacy-related issues. Moreover, the recent rising of Multimodal Large Language Models (MLLM) leads to a need for multimodal medical datasets, where clinical reports and findings are attached to the corresponding CT or MR scans. This paper illustrates the entire workflow for building the data set MedPix 2.0. Starting from the well-known multimodal dataset MedPix\u00ae, mainly used by physicians, nurses and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure where noisy samples were removed, thus creating a MongoDB database. Along with the dataset, we developed a GUI aimed at navigating efficiently the MongoDB instance, and obtaining the raw data that can be easily used for training and/or fine-tuning MLLMs. To enforce this point, we also propose a CLIP-based model trained on MedPix 2.0 for scan classification tasks.", "sections": [{"title": "1 Introduction", "content": "The rise of computer-based applications in recent years has strongly favoured the digitisation of historically analogue processes in the management and analysis of biomedical data. In turn, the emergence of technologies based on Artificial Intelligence (AI) prompted the development of increasingly precise models to support diagnosis for the construction of personalised treatments. In the biomedical domain, in fact, different sub-areas can benefit of AI-based systems, ranging from the administrative field (e.g. better management of queues in emergency areas, centralised integration of medical records) to the clinical one, thanks to the use of AI that can efficiently extract useful features to reliably achieve diagnosis.\nOne of the fundamental requirements of these applications consists in their trustworthiness, which must help the physician with high confidence, being able to provide reliable predictions and classifications. AI models, however, require"}, {"title": "2 Related Works", "content": "Medical data sets for AI applications suffer from diverse problems, related both with the data and the peculiarity of the domain. First, there is a privacy issue, since clinical data contain private information about the patient, thus the process of creation of such type of data set has to start with an anonymization phase. To overcome this problem, researchers rely on either open-access and textbook data or they collaborate with hospitals to create data sets. The first two methods allow for large scalability, relying on anonymous data. On the other hand, anonymization must be carried out from scratch when dealing with hospitals. Moreover, multimodal medical data suffer from scarcity when compared with other data sets related to different domains such as MS-COCO [8].\nOne of the most used open-access multi modal database for developing medical datasets is PubMed Central\u00ae (PMC)2. This is a widely used free archive of biomedical scientific literature, from which it is possible to build one's own datasets via semi-automatic procedures. In PMC, data are anonymized, and high-quality captions can be extracted from the medical research papers the images belong to. The following multimodal data sets were extracted from PMC: ROCO [12], MedICaT [16], PMC-OA [9]. ROCO contains pairs of radiology images and the corresponding captions, and it incorporates an out-of-class set to improve prediction and classification performances. MedICaT is a disjoint dataset from ROCO that is mainly composed by radiology images and provides manually annotations for sub-figures. PMC-OA is the larger than the previous ones, and it keeps a variety of diagnostic procedures, diseases, and findings, while introducing sub-figure separation.\nVQA-RAD [6] is a data set derived from MedPix\u00ae, and it collects a subset of radiological images, while providing Question-Answer (QA) pairs validated by domain experts.\nAnother source of available high-quality data are textbooks: PathVQA [4] is a Visual Question Answering (VQA) data set that collects both closed- and open-ended QA pairs, which are extracted from both pathology textbooks and online digital libraries via a semi-automated pipeline.\nOn the other side, open access data sets like MINIC-CXR [5], IU-Xray [2] and SLAKE [10] are manually annotated by domain experts. Both MINIC-CXR and IU-Xray are chest radiography data sets derived from hospital's clinical cases. Both if them contain a semi-structured radiology report, describing the radiological findings of the images it is related to. SLAKE, on the contrary, collects images from different radiology open-access datasets and provides manual annotations and QA pairs given by experienced doctors in English and Chinese.\nThe aforementioned data sets are multimodal ones, and they are deeply focused on VQA tasks. Also some specialized data sets exist that provide only"}, {"title": "3 MedPix 2.0", "content": "MedPix\u00ae is a free open-access multimodal online database of medical images, teaching cases, and clinical topics, managed by the National Library of Medicine (NLM) of the National Institutes of Health (NIH). It mainly serves as a support system for Continuing Medical Education (CME) of physicians, nurses, and healthcare students. The database collects clinical cases related to more than 12,000 patients. Each case contains at least one medical image, and the corresponding findings, discussion notes, diagnosis, differential diagnosis, treatment, and follow up. Textual information is reported in a semi-structured format. Attached to the clinical case, there is the topic section, where the disease under investigation is discussed in detail from an academic and general perspective.\nDespite the richness of the data set, its free availability, the possibility to add new cases, and the access to clinical cases of interest using wither keywords, the body part or the disease, it is not possible to access to the raw data. This feature limits the usage of MedPix\u00ae for training multimodal AI systems. Therefore we decided to create a brand new structured version of this data set because it represents a high quality source for AI-based medical aplications. MedPix 2.0 has been built essentially as a MongoDB instance that is released along with a suitable GUI aimed both at general purpose querying and extracting training data for AI models. Referring to the Fig.1, a MongoDB version of the data set was built using a semi-automated pipeline to create two kinds of JSON documents: the one collecting the information falling into the screenshots labeled"}, {"title": "3.1 Dataset extraction", "content": "We decided to focus on a part of MedPix\u00ae that involves cases related with two diagnostic modalities, namely Computed Tomography (CT), and Magnetic Resonance Imaging (MRI). First of all, the images in the considered split were downloaded via Open-i\u00ae 3, and noisy samples where manually removed or modified accordingly. The purpose of this data cleaning stage was obtaining images that can be suitable as input data for training a MLLM. An automatic scraping pipeline was implemented to extract the textual data related to the selected images using Selenium and Beautiful Soup. Finally, two kinds of JSON documents were devised to store, respectively, the information strictly connected with the images (descriptions document) and the one related to a clinical case (case-topic document). A one-to-many relation has been created between a clinical cases and images by embedding the U_id defined for a case-topic document in each descriptions document attached to each image related to the"}, {"title": "3.2 MongoDB representation", "content": "In order to process properly the data in MedPix 2.0 we built a MongoDB database to host all the JSON documents along with the images. The architectural choice stems not only from the nature of the data in MedPix 2.0 but also from considerations related to its high flexibility and scalability to distributed environments where also private multimodal medical data could be added to the original collections with the constraint of not being moved away from their generation site as it is the case of hospital generated information.\nWe built a MongoDB instance made by two collections, namely Image_Descriptions containing the descriptions documents, and Clinical_reports which contains all the case-topic documents. In our implementation, the images are stored in a separated folder, and they are accessed using a proper file://URL built starting from their U_id. Finally, a view called Image_Reports allow a direct access to both collections via their U_id."}, {"title": "4 Training a MLLM using MedPix 2.0", "content": "The training-oriented reorganisation of MedPix 2.0 makes it possible to train a multimodal deep neural network without further pre-processing of the data. To validate this claim, we used a data set extracted from MedPix 2.0 to train CLIP[14] that is one of the most recent and widespread multimodal models. In particular, CLIP succeeds in achieving competitive performance in zero-shot contexts on a wide range of classification datasets by learning the relationships between images and their textual descriptions [18]. The structure of the architecture is well known and consists of an image encoder and a text encoder. CLIP's training scheme was kept consistent with what has been reported and involved a contrastive pre-training phase using a large corpus of image-text pairs, a second phase tha is the creation of the classifier from the text labels, and finally the zero-shot prediction. A schematic of this training is shown in Fig. 5.\nThe CLIP's architecture is modular that is one can use specific encoders both for the visual and the textual part other than the default ones. This feature makes CLIP also a framework and not simply a model.\nLeveraging this peculiarity, we started a trial training phase to select the best suited visual encoder for our purposes, while keeping the textual encoder unchanged. Specifically, a ViT-L/14 and a RN50x16 were tested in combination with the CLIP's default textual encoder. The trial phase favoured the use of the RN50x16, which has been tested as a whole with the CLIP's textual encoder, obtaining an accuracy of 58% in the scanning modality identification task, as it is shown in Fig. 6."}, {"title": "5 Conclusions and Future work", "content": "In this paper we presented MedPix 2.0, a multimodal dataset of clinical reports, CT and MR scans. We devised a semi-automated pipeline to download and curate the images in the original data sets, while structuring the textual information as a set of JSON documents collections which had been used to build a proper MongoDB instance. The NoSQL version of the data set can be accessed and queried with a usable GUI that has been developed puprosely. Using the GUI one can browse the data set in the same manner as in the original MedPix\u00ae and can download the structured output of the query that is suitable for training AI models. To demonstrate this point, we developed a CLIP-based model for multimodal classification of CT/MR scans with respect to both the scanning modality and the body part shown in the input slice.\nThe presented dataset and its MongoDB interface, represent in our view a relevant starting point for the development of AI multimodal models in the medical domain such as Information Extraction systems tailored for clinical reports, automated analysis of the medical images, or Generative AI models for clinical report generation as part of a Medical Decision Support System. All these systems can rely on MedPix 2.0 as a structured source of data containing both clinical cases and medical explanations about the found disease.\nThe structured JSON documents encode also implicit knowledge on the domain. We are currently working on a generative model that relies on a Knowledge Graph (KG) built from MedPix 2.0, to generate diagnostic findings given some preliminary information. The information retrieved analyzing the keys of the JSON documents can be easily structured as nodes (patients, diagnosis, treatment, exams, ...) connected via edges symbolizing the relations be between them, and enriched by adding attributes at node or edge level. This re-organization of the data, can show some patterns on relations that otherwise cannot be easy recognized, and the developed graph, apart from AI-based applications, can be browsed by doctors for diagnostic or research purposes using a proper GUI. The scalability of the developed MongoDB database, makes it suitable for future extensions with the possibility to add new clinical cases that have to be compliant with privacy regulations and follow the required information structure. Moreover, the inherent distributed nature of MongoDB allows for creating huge databases across different wards where the data owned by a single institution do not need to be explicitly moved out of the hospital thus violating privacy regulations. The structure of MedPix 2.0 could also serve as a guide to develop suitable connectors to share allowed data in the EHDS. New cases can also be"}, {"title": "5.1 Data and Code Availability", "content": "Source code and is freely available at https://github.com/CHILab1/MedPix-2.0.git.\nThe data used for the test and train can be downloaded free of charge from the zenodo repository at the following link https://zenodo.org/records/12624810?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6ImZmNjdlMjExLTc2NjQtNDRkNy1hZDU0LTRjNmVjN2YxMzE5NSIsImRhdGEiOnt9LCJyYW5kb20iOiJlMzAxNjM4MDdlZjA0Y2JjZGZ hMzQzNTgwMDU5ZGQ3OCJ9.1G5h-jxdM_wy1Oq61UbXhSWVShcIw5-iq4obuX5wFh9xvKrs8LcgkdUEgR3n_YuDdzv59N156f_kILTHpDoHxw"}]}