{"title": "Speculative Decoding with CTC-based Draft Model\nfor LLM Inference Acceleration", "authors": ["Zhuofan Wen", "Shangtong Gui", "Yang Feng"], "abstract": "Inference acceleration of large language models (LLMs) has been put forward\nin many application scenarios and speculative decoding has shown its advantage\nin addressing inference acceleration. Speculative decoding usually introduces a\ndraft model to assist the base LLM where the draft model produces drafts and the\nbase LLM verifies the draft for acceptance or rejection. In this framework, the\nfinal inference speed is decided by the decoding speed of the draft model and the\nacceptance rate of the draft provided by the draft model. Currently the widely\nused draft models usually generate draft tokens for the next several positions in a\nnon-autoregressive way without considering the correlations between draft tokens.\nTherefore, it has a high decoding speed but an unsatisfactory acceptance rate. In\nthis paper, we focus on how to improve the performance of the draft model and\naim to accelerate inference via a high acceptance rate. To this end, we propose a\nCTC-based draft model which strengthens the correlations between draft tokens\nduring the draft phase, thereby generating higher-quality draft candidate sequences.\nExperiment results show that compared to strong baselines, the proposed method\ncan achieve a higher acceptance rate and hence a faster inference speed.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been applied to a wide range of text generation tasks such as\nmachine translation and question answering due to their remarkable performance[1, 21, 6, 11]. In\nmany applications, LLMs is required to produce a long generation to give explanations to its answer\nor perform chain of thought (CoT). Moreover, in some practical scenarios, LLMs have to resort to\nother applications to fulfill a task under the frame of AI agents where LLMs usually generate long\noutputs to communicate with other applications. All of these have a high demand for the inference\nspeed of LLMs. However, most LLMs employ a token-by-token autoregressive generation paradigm,\nbringing on the severe inference delay problem, which obstacles the real applications of LLMs.\n\nTo mitigate the inference latency of LLMs, speculative decoding is proposed and proves to be a\nmore efficient decoding strategy compared with autoregressive generation[12, 4]. Besides the base\nLLM, speculative decoding usually introduces a drafter model in the working flow that the draft\nmodel generates candidates for the next several tokens and the base LLM verifies the candidate and\ndecides to accept or reject at some criterion. Once accept, then the winner candidate will be used as\nthe output, otherwise, the base LLM will decode and generate the output. In this process, the draft"}, {"title": "2 Background", "content": "Recent advancements have emerged from the innovative approach of Blockwise Decoding[17], which\nintroduced the draft-then-verify paradigm, leading to the development of Speculative Decoding[12]\nand Speculative Sampling[4]. These methodologies offer promising avenues for enhancing the speed\nof Large Language Models (LLMs).\n\nSpeculative Decoding predicts multiple future tokens and verifies their accuracy within a single\ndecoding step. Using greedy sampling as an illustration: at step t, given an initial prompt X and\npreviously produced tokens $y_<t>=Y_1, \u2026, Y_{t\u22121}$, a speculative sequence of length n, $Y_t, ..., Y_{t+n}$, is\ngenerated by the draft model with respective probabilities $p_t, \u2026\u2026\u2026, p_{t+n}$. The target LLM then computes\nthe accurate probabilities $p_t, ..., p_{t+n}$ in one pass during verification. Each token y is evaluated\nin sequence, with its acceptance probability given by $min(1, p_i/p'_i)$. Upon rejection of a token y,\nsubsequent tokens are disregarded, and the rejected token is re-sampled using the adjusted distribution\n$P(y_i) = norm(max(0, P(y_i|y_<i, X) \u2013 P'(y_i|y_<i, X)))$.\n\nThe effectiveness of Speculative Decoding significantly depends on designing an intelligent\ndraft model for precise token prediction and devising an optimal strategy for token sequence\nverification[24]. Consequently, current research efforts concentrate on these aspects to further\nexploit the potential of Speculative Decoding for speed acceleration."}, {"title": "2.1 Design of Draft Model", "content": "Many researchers have pursued the strategy of designing a draft model that operates independently of\nthe base model[15, 19, 5, 13], such as employing a non-autoregressive transformer for simultaneous\ntoken drafting[23]. To ensure compatibility with the base model, works like [12] opt for draft models\nwith fewer parameters from the same model series.\n\nHowever, independent draft models necessitate training or fine-tuning, posing flexibility issues when\ntransitioning between base models. Alternatively, some approaches rely on modifying the base model\nitself for token drafting through moderate adjustments [3, 25, 26, 10]. For instance, [3] introduces an\nadditional module comprised of linear layers atop the target LLM for drafting tokens independently\nfor different positions. In contrast, [25] incorporates a bypass within the LLM, allowing for earlier\nexits during the model's layer-by-layer computation."}, {"title": "2.2 Optimization of Verification", "content": "The strategy for compiling drafted tokens into candidate sequences and the criteria for sequence\nselection are vital during the verification stage. Initially, forming a single candidate sequence from the\nmost probable tokens across positions was the prevalent approach[16, 18]. To incorporate a broader\nrange of draft sequences, SpecInfer[14] organizes draft tokens into a tree structure, with paths from\nthe root to leaf nodes representing different candidate sequences. Regarding selection criteria, early\nmethods only accepted sequences matching the target model's greedy decoding output[23]. Later, [4]\nintroduced Nucleus Sampling as a more effective yet complex acceptance criterion."}, {"title": "2.3 Connectionist Temporal Classification", "content": "Connectionist Temporal Classification (CTC) is tailored for sequence prediction tasks, especially ap-\nplicable in speech and handwriting recognition[9]. CTC expands the output space, Y, by introducing\na blank token e denoting 'output nothing', creating an augmented space Y*. It defines a function\nB(y) that maps any sample y \u2208 Y to a subset of Y*, containing all valid alignments. Conversely, \u03b2-1\nprocesses alignments from Y* back to Y by merging adjacent, repeated tokens and removing blanks,\nresulting in the target sentence.\n\nThe sequence-level CTC loss function offers superior context modeling capabilities compared\nto token-level alternatives and effectively manages variable-length outputs without necessitating\nalignment during training. The training objective leverages dynamic programming to aggregate over\nall potential alignments a \u2208 Y*.\n\n$log p(y) = log \\sum_{\\alpha \\epsilon \\beta{y}} p(a)$  (1)\n\nFor inference, the model generates alignment a, from which repeated tokens and blanks are removed\nto yield the final output y = \u03b2-1(a)."}, {"title": "3 CTC-drafter Model", "content": "In this section, we describe our implementation of the proposed model. CTC-drafter improves the\nacceptance rate of draft tokens while keeps extra investment of draft time in a reasonable duration,\nconsequently achieving superior inference speedup. We first give an illustration of CTC-drafter's\nmodel structure, analyzing the functions of involved modules. Subsequently, we clarify CTC-drafter's\ntraining strategies and inference procedure."}, {"title": "3.1 Model Structure", "content": "Our CTC-drafter model structure are displayed in Figure 1. The training strategy is on the upper\npart and the inference process is on the bottom part. The base model on the left side are the LLM\nwe desired to accelerate, which generally is composed of an embedding layer, multiple attention\ntransformer layers and a output LM head that maps the hidden states to probability in vocabulary\ndimension.\n\nFor the draft procedure, we insert an attention draft module which take the hidden states outputted\nfrom base model as input and predict the probability distributions of draft tokens. Here hidden states"}, {"title": "3.2 Training", "content": "The basic training strategy of CTC-drafter is displayed in Figure 1. We fixed the parameters of base\nmodel and trained the transformer layer in Attention Draft Module on ShareGPT dataset, which"}, {"title": "3.3 Inference", "content": "To clarify the inference speedup mechanism of CTC-drafter, we further explain this procedure in\none specific decoding step with the decoding history \u201cUsr: what is your name? Assistant: hello,\" \nas base model input. The input will first be passed through base model producing the hidden states\nand greedy sampling base token \u201cmy\u201d. Attention Draft Module takes the hidden states from last\ntransformer layer as input, outputting probability distributions of different positions after base token\nafter LM Head projection.\n\nFor every position, the top k tokens are selected in descending order of probability, where k is\npredefined. In this instance, Attention Draft Module suggests that \u201cname\u201d is the best candidate token"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation Settings", "content": "We choose open-source Vicuna large language model[6] with different parameter sizes as base model\nto conduct experiments. Vicuna models is fine-tuned on ShareGPT dataset based on LLaMA model,\nwhich are noted below as Vicuna-7b, Vicuna-13b and Vicuna-33b according to different parameter\nsizes. We also conduct training on LLaMA-2-Chat base models, detailed in the Appendix.\n\nWe fix Vicuna model's parameters and train the transformer layer inside draft module on ShareGPT\ndataset. The learning rate is set to 3 \u00d7 10\u22125. To avoid gradient explosion, we adopt gradient clipping,\nsetting the clipping threshold to 0.5. We set the max length of training data to 2048. All training tasks\nwere executed on four 24GB NVIDIA GeForce RTX 3090 devices, taking around two days. To fully\nutilize graphics memory and accelerate training, we load models with FP16 precision for quantization.\nFor comparison, we also implemented Medusa[3] on Vicuna models, following suggested experiment\nsettings and retrain on the same ShareGPT dataset.\n\nTrained models are evaluated on MT-bench and GSM8K datasets to assess the acceleration per-\nformance in various scenarios. MT-Bench is a carefully curated benchmark that includes 80 high-\nquality, multi-turn questions covering 8 primary categories of user prompts such as writing, roleplay\nand extraction[27]. GSM8K contains 8.5K high quality linguistically diverse grade school math\nproblems[7]. Unlike some other datasets that offer base model questions with definitive answers\nsuch as multiple-choice questions, the two selected evaluation datasets contain open-ended questions,\nrequiring base model to output long sequence answers in multiple decoding steps.\n\nFor every question, we record the total number of tokens in its corresponding answer as N, the\ntotal inference time T and the base model decoding steps M. We calculate the average number of\ntokens accepted per decoding step and the inference speedup compared to vanilla base model without"}, {"title": "4.2 Results and analysis", "content": "The performance of different speculation methods on MT-bench and GSM8K are showed in Table\n1. The speedup ratio of Medusa is evaluated following recommended setting in its corresponding\ntechnical report. The results of Hydra[2] on Vicuna models are acquired from its corresponding paper.\nWe also measures the performance of fully auto-regressive decoding with no speculation method as\nbaseline to calculate speedup ratio of other three methods, noted as Vanilla in Table 1.\n\nMT-bench. The results show that our proposed CTC-drafter achieves better draft quality on MT-\nbench compared other works, with more than three tokens been accepted per decoding steps. Higher\npredicting accuracy enables CTC-drafter to achieve speedup ratio of more than 2x, outperforming\nMedusa and Hydra on all types of base model. Besides, the speedup performance of all speculation\nmethod is influenced as base model size increases. Possible explanation is that we can not significantly\nexpand the size of draft module considering extra time consumed, when base model size increase,\nlarger ability gap between base model and draft module makes it more difficult for draft module\nto imitate base model's prediction behavior. Therefore, the average number of accept tokens \u1e9e of\nCTC-drafter decreases from 3.56 to 3.53, influencing the speedup performance.\n\nGSM8K. Compared with MT-bench, questions in GSM8K mainly focus on math category. As is\nshown on the bottom of Table 1, our proposed CTC-drafter keep a superior speedup performance\nover Medusa for all base models. Besides, the speedup performance suffers to some extent for\nCTC-drafter in Vicuna-7B base model, compared with the performance in MT-bench. The main\nreason is that we completely rely on the comprehensive ability of origin base model to offer answers\nwithout fine-tuning on GSM8K training dataset. Fortunately, when the base model size increases to\n13B, CTC-drafter maintains prediction accuracy, achieving 2.66x speedup. However, bridging the\ncapability gap for Vicuna-33B is challenging, leading to a decline in performance."}, {"title": "4.3 Ablation experiments", "content": "In this part, we list the results of ablation experiments and further analyze the working paradigm\nof CTC-drafter. First, we explore how each part of the model structure influences the acceleration\nperformance in Table 2. Then we illustrate how the prediction ability varies across different categories\nof test questions in Figure 2. Besides, to better visualize the trade-off between extra time consumption\nand prediction accuracy, the time consumption of each calculation procedure during inference is\nmeasured in Figure 3.\n\nModel structure. To better utilize the context information, CTC loss is used as the training objective\nas discussed in Section 3.2, which optimizes the draft module under sequence-level supervision.\nBesides, we replace the linear layers of Medusa head with more complex transformer layer to suit"}, {"title": "5 Related Work", "content": "Medusa. After Speculative decoding[23] and Speculative sampling[12], many improvement works\nhas been proposed to optimize the draft model and verification strategy. Among these, Medusa\nexplores a novel and efficient acceleration framework[3]. Instead of using an independent model with\nfewer parameters as the draft model, several Medusa Heads are added on top of the last transformer\nlayer of base model. The i-th Medusa Head is responsible for predicting the i-th token after the base\nmodel decoding token in each step. For each Medusa Head, top k tokens with highest probability\nare selected and combined in tree structure to form candidate sequences. Using tree mask method in\n[14], the multiple sequences are validated in parallel during the next decoding step. Two strategies\nare used for the training of Medusa Head: Medusa-1 fix the parameters of base model, optimize only\nthe Medusa Head on a small dataset, Medusa-2 adopt end-to-end training, fine-tuning base model\nand Medusa Head together on larger datasets. Although Medusa-2 can achieve remarkable inference\nspeedup, the need of large training data and time consumption limits its generality across different\nbase model. In this paper, we only implement and demonstrate Medusa-1 considering fairness.\n\nHydra. Modified from Medusa Head, Hydra designs Hydra Head, a sequentially dependent, drop-in\nreplacement for standard draft heads[2]. A Hydra Head conduct prediction not only based on base\nmodel hidden states but also the decoding output of other Hydra Heads, which significantly improves\nspeculation accuracy. Besides, some other tricks are adopted for further inference speedup including\nadding noise and knowledge distillation."}, {"title": "6 Conclusion and Future work", "content": "Speculation decoding and corresponding improvement works mostly draft candidate tokens without\nconsidering context information and generate fixed-length candidate sequences for verification,\nwhich not only influences the draft quality, bur also lacks generality across different large language\nmodels. In this paper, we propose a novel framework named CTC-drafter based on CTC algorithm.\nSpecifically, we use CTC loss as the training objective to model the context connection instead of\ncross entropy. We reconstruct the structure of draft model, using transformer layer to better fit base\nmodels. Besides, with CTC transform, we achieve adaptive candidate sequence generation which\nmakes it convenient to transfer the framework across different base models. Nevertheless, our current\nwork is subject to certain limitations that requires careful consideration. More training tricks can\nbe explored to further enhance the prediction ability of draft module. Besides, it is still doubtful\nthat whether the current draft model structure is optimal. What's more, different types of large\npretrained language model need to be adopted in our proposed framework to evaluate the acceleration\nperformance of CTC-based draft model.\n\nFor the future work, we attempt to identify techniques to reduce the extra time consumed caused\nby more complex draft operations introduced. Other verification criteria such as Nuclear Sampling\n[4] can be integrated into our framework. What's more, some other methods such as Conditional\nRandom Field(CRF, [20]) and Directed Acyclic Graph(DAG, [8]) can be explored to model the\ncontext information when drafting tokens, we remain these ideas for future work."}, {"title": "A Appendix", "content": "To evaluate the generality of our method across different base models, we add supplementary\nexperiments on LLaMA-2-Chat base models [22]. We select LLaMA-2-Chat 7b and 13b as base\nmodels and evaluate CTC-drafter's performance on MT-bench and GSM8K. For a clear comparison,\nthe evaluation results of various base models, including Vicuna, are documented in Figure 4.\n\nCTC-drafter maintains ideal performance when transferring from Vicuna models to LLaMA-2-Chat\nmodels, only slight decline when compared the evaluation results on Vicuna-7b and LLaMA-2-chat-\n7b. Besides, it should be noted that increasing the size of the LLaMA-2-Chat model to 13b does not\ncompromise draft quality, while enhancing speedup performance. This trend diverges from Vicuna\nbase models, potentially due to distinct inference paradigms inherent in both models."}]}