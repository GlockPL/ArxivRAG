{"title": "No Dataset Needed for Downstream Knowledge Benchmarking: Response Dispersion Inversely Correlates with Accuracy on Domain-specific QA", "authors": ["Robert L Simione II"], "abstract": "This research seeks to obviate the need for creating QA datasets and grading (chatbot) LLM responses when comparing LLMs' knowledge in specific topic domains. This is done in an entirely end-user centric way without need for access to any inner workings of the LLM, so long as it can be prompted and given a random seed to create different generations to the same prompt. The paper does this by, for a given topic domain, defining the \"response dispersion\" of an LLM by repeatedly asking an LLM the same opinion question about that topic domain. Namely, the response dispersion is the count of singular values needed to explain 95% of the variance in the embedding matrix of the LLM's responses. It is found that the response dispersion is inversely correlated with accuracy on relevant QA evaluations (average spearman rank correlation stronger than -.59). A use-case analysis shows that when comparing two different LLMs on the same topic domain, comparing their response dispersion is a suitable replacement for comparing their QA accuracy between 74% and 89% of the time, the range depending on certain reasonable accuracy-difference tolerances that may be acceptable to an end-user in exchange for the labor being saved using response dispersion instead of QA accuracy for comparison. Two response embeddings are studied for creating the embedding matrix in this study, one is from OpenAI's APIs and one is a novel embedding, here named reference sentence similarity embeddings, that can be computed locally and performs very nearly as well in calculating response dispersion. Also in this research, a pre-existing dataset called the IRC-Wiki Trivia dataset, originally developed for trivia games, has been re-purposed, curated, and the curation, called IRC-WikiTriviaQA, is made available for the purpose of this research.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background and Motivation", "content": "When designing an AI-powered application that will use an LLM as part of its processing, a decision needs to be made about which LLM to use for the purpose. There are many benchmarks for comparing the capabilities of LLMs, and, other considerations held equal, it is generally preferable to use an LLM that has more knowledge about relevant topic domains than not. (Chang et al., 2024, Section 4.2) While this seems true just intuitively, there is also experimental evidence verifying this. (Liu et al., 2024, Section 2.3) showed that a model's ability to answer questions without any assistance can, in some circumstances, provide a correct answer more often than even when the answer to the question is itself a"}, {"title": "1.2 End-User Centric Assumptions", "content": "The procedure created is designed with certain assumptions about a working practitioner in mind. This is because while some LLMs such as Meta's LLama series of models are open-source and provide access to their weights so that activations can be analyzed di-rectly, it's also true that many LLMs used in practice are proprietary and thus kept behind remote-access-only API endpoints where neuron-activation-analysis is not possible. Thus in designing this procedure only 3 assumptions are made:\n1.  The chatbot is known to use a probabilistic procedure such that responses can vary depending on a randomizer seed value. (This is true of arguably all LLMs being used today but wouldn't be true of an older chatbot like ELIZA which may have zero response dispersion but would always perform poorly on a QA benchmark dataset.)\n2.  This random seed can be set so that multiple distinct generations to the same prompt can be accumulated.\n3.  The chatbot can be asked a question from a clean context window multiple times.\nAll of these assumptions are valid for nearly all chatbots on the open market today. Specifically, this study tested LLMs available through OpenRouter.ai APIs, a platform on which many proprietary and open-source LLMs are available for prompting through an API endpoint."}, {"title": "1.3 Procedure Overview", "content": "The procedure detailed and validated in this paper is as follows:\n1.  Specify your topic domain, e.g. \"Sports\", \"Movies\", \"Music\"\n2.  For each LLM, ask it multiple times (in this paper 100) with different seeds to answer an opinion question about that topic domain to create a list of responses from that LLM.\n3.  For each list of responses, create an embedding matrix where every row is the embed-ding of a response from the list. (Two different embedding methods are tested in this paper.)\n4.  For each LLM's embedding matrix, count how many singular values are needed to explain 95% of the variance in the matrix's rows. This count is here named the LLM's response dispersion for the specified topic domain.\n5.  The LLM with the lower response dispersion either would perform better than the other LLM on a hypothetical QA benchmark dataset 74% of the time, or it would perform within at-most 10% tolerance on the hypothetical QA benchmark of the other LLM 89% of the time."}, {"title": "1.4 Paper structure", "content": "The rest of this paper is structured as follows:\nSection 2 motivates the study of response dispersion and its relation to QA benchmark accuracy. Subsection 2.1 describes the motivating hypothesis for why this measure-ment was focused on and studied with the expectation that it would correlate with QA benchmark accuracy. Subsection 2.2 defines response dispersion given an embed-ding matrix. Subsection 2.3 discusses the two text embeddings used and introduces a novel text embedding, reference sentence similarity embeddings, which on this task performed nearly as well as OpenAI's text-embedding-3-large embeddings while being faster and cheaper and able to compute locally.\nSection 3 describes how the use of response dispersion was validated as useful for the in-tended LLM-comaring use-case. Subsection 3.1 introduces a repurposed and curated QA dataset of trivia questions across different topic domains called IRC-WikiTriviaQA. Subsection 3.2 and subsection 3.3 describe how the IRC-WikiTriviaQA was used to find QA benchmark measurements for candidate LLMs studied for 12 different topic domains with subsection 3.2 describing how LLMs were prompted to respond to IRC-WikiTriviaQA and subsection 3.3 describing and validating how the LLMs' responses were graded against the IRC-WikiTriviaQA answer key."}, {"title": "2 Response Dispersion", "content": ""}, {"title": "2.1 Motivating Hypothesis", "content": "This section motivates the hypothesis that is later validated experimentally.\nProbabilistic text generation is a stochastic process guided by the weights learned during training, where subsequent tokens in a response are determined by logits output by an LLM. These logits are used to define a probability distribution from which the next token of a LLM's reply is selected. This means that different response generations to the same prompt will generate different responses.\nThe motivating hypothesis of this paper is that sharper contrast in the output logits of the LLM should correlate with the LLM being more sure (measured by probability) in its output, which ought to (assuming the information it was trained on is truthful) correlate to it having more factual knowledge. However, since the goal is to measure this from the point of view of an end-user of an LLM, it is not always possible to measure the logits directly, as in the case with proprietary LLMs that are accessed remotely through API endpoints. Thus this paper analyzes ways to measure how dispersed the responses of an LLM are when given the same prompt about a topic many times.\nIn an effort to give the latent diversity of potential responses the best opportunity to be detected, a prompt template is used that asks the LLM a generic opinion question about a topic category. The topic category is left as a variable so that different domains of discourse can be evaluated by plugging in their name into the category variable. In particular, the prompt is:\nHere is a test for evaluating LLMs. I want to see how well you follow my instructions when constructing your response.\nI want you to respond with a SINGLE WORD, and ONLY THAT. Do not add any other context, other words, notes, explanations, justifications, or objections to my phrasing of this question. Now, please tell me in a single word what is your favorite thing to discuss related to the topic category of \" {category}\". Do not response with the \"favorite\", \"discussions\", or \" {category}\".\nThis was asked of each LLM studied 100 times, with different passed as an argument seeds for each ask. The explicit request to keep the answer succint is there to help in measuring the diversity of answers by keeping most of the response, because it is easier to programmatically measure the difference in answers like [\u201cXYZ\u201d, \u201cXYZ\u201d, \u201cABC\u201d] rather than in answers like [\"I like to talk about XYZ\u201d, \u201cXYZ is nice to discuss\u201d, \u201cI like to talk about ABC\"]."}, {"title": "2.2 Defining Response Dispersion Using Response Embeddings", "content": "The previous subsection motivated the need to measure how dispersed responses to the prompt are when asked multiple times. Assuming you have embeddings for each re-sponse that sufficiently capture the important similarities and differences in responses to the prompt, then a straightforward way to define response dispersion is to take the embedding matrix where each row is the embedding of a response, and count how many singular values of this matrix (starting from the largest and counting in descending order) are required to explain some threshold of variation in the embeddings, say 95%."}, {"title": "2.3 Response Embeddings Used", "content": "Different embedding methods can be dropped-in-place for defining the Response Dispersion as above. This paper looks at two different candidate embedding methods for embed-ding LLM responses. One is simply to use the embeddings provided by OpenAI's text embedding API using the model text-embedding-3-large, currently available over a re-motely accessed API endpoint. The other embedding method is novel to this paper, here called \"reference sentence similarity embeddings\" and abbreviated as rss embeddings. In the use-case analysis described later, their performance is nearly indistinguishable with the text-embedding-3-large embeddings having a marginally better performance compared to rss embeddings; however rss embeddings have benefits in that they capture almost all the same variation while being lower dimension, they are quicker to obtain (compared to retrieving embeddings over a remote API), and are computed locally.\nGiven a set of sentences to be called \u201creference sentences\" $\\{r_j\\}_j$, and a sentence similarity scoring method $s(\\cdot, \\cdot) \\rightarrow [0, 1]$, then a reference sentence similarity embedding for a string t is a vector where the j-th component of the embedding of t is $s(t, r_j)$.\nFrom now on, this paper will use a normalized edit similarity score called the normalized Indel similarity which is a special case of a normalized Levenshtein similarity score.\nThe reference sentences used will be all the responses to be embedded. This means that for a collection of responses $\\{t_i\\}_i$ the embedding matrix is going to its i, j-th component be $s(t_i, t_j)$. Thus the i-th row of the embedding matrix will be the rss embedding of $t_i$ using all $\\{t_j\\}_j$ as reference sentences."}, {"title": "3 Validation Methodology", "content": ""}, {"title": "3.1 Introducing the IRC-WikiTriviaQA Dataset", "content": "To validate the use of response dispersion in comparing LLM's knowledge, this paper presents a use-case analysis where the comparison of response dispersion measurements is evaluated in its suitability as a proxy for comparing between candidate LLMs' percentage of correct answers to trivia questions, calculated on a per-category basis i.e. the result of the gold-standard human labeling assessment described in the introduction. The hand-labeled dataset used, IRC-WikiTriviaQA, is a repurposement and curation of the original IRC-Wiki Trivia dataset which is available at (Bertrum et al., 2012) and was released under a CC-by-SA-3.0 license (Commons). The author's curation of the dataset used for this paper is being released under the same license at (Simione II, 2024). The author kindly requests"}, {"title": "3.2 Prompting LLM responses to the IRC-Wiki Trivia questions", "content": "This study evaluated 10 different chat-tuned foundation models against this trivia dataset. For each trivia question in the dataset, the question and its category (given by the dataset) were variables placed into the following prompt template that was sent to each LLM studied:\nHere is a test for evaluating LLMs. I want to see how well you follow my instructions when constructing your response."}, {"title": "3.3 Grading the LLM responses to the IRC-Wiki Trivia questions", "content": "In total there were 13888 responses to be graded across the different questions and models. In order to evaluate possible automated grading methodologies, the author sampled 1000 responses from the 13888 responses and hand-graded whether, the author being informed by the trivia question and the answer key's answer, if the sampled responses contained the answer key's answer within it. The goal was to have the hand-graded sample be a gold standard in capturing subtleties in responses that mere string comparisons could easily miss, such as when answering the question \"Which two countries share Victoria Falls?\" the answer \"zambia and zimbabwe\u201d provided by an LLM being actually the same as the answer key's \"zimbabwe and zambia\" or that in answer the question \"In which film did Bill Murray drive an ectomobile?\" the LLM's response \"the answer is ghostbusters 1984\" is marked correct against the answer key's \"ghostbusters\". Thus each of the 1000 sampled responses was hand-checked and graded that the LLM in that case had gotten the answer right, wrong, or if it was ambigous to the author whether the LLM's response should be considered right or wrong then it was held out. 30 responses out of the 1000 sampled fell into this last category, and so 970 hand-checked grades were used in evaluating automated grading methodologies.\nThe first methodology evaluated for autograding the LLM responses was to remove punctuation from the answer key's answer and LLM response, lowercase all letters, and check if the post-processed answer key's answer was a substring of the post-processed LLM response. Based on the 970 hand-graded examples, this assigned the correct grade to the LLM's response 90.5% of the time.\nThe second grading methodology was to grade each LLM response by sending the fol-lowing prompt to GPT4 for each question, answer key answer, and LLM response:\nHere is a test for evaluating LLMs. I want to see how well you follow my instructions when constructing your response.\nI want you to respond with a SINGLE WORD, either \"Yes\" or \"No\", and ONLY THAT. Do not add any other context, other words, notes, explanations, justifications, or objections to my phrasing of this question. Now, I am going to show you a trivia question in the category \" {category}\". I am going to show you the question, and then the answer key's answer, and then the answer from an"}, {"title": "3.4 Response Dispersion Use-Case Analysis Defined", "content": "For each possible tolerance parameter, every unique model-pair and topic category combi-nation goes through the following process:\n1.  The response dispersion is calculated for each of the two models on that category. The model with the lower response dispersion is called \u201cthe chosen model\u201d in this process.\n2.  If, within this category, the chosen model has a ground truth accuracy that is greater than the other model's ground truth accuracy minus the tolerance level, then this is scored as a success for the process.\nThe success % of outcomes is calculated per category, since models are only intended to be compared within a given domain of discourse, same as hand-labeled qa datasets would only be defined over a particular domain of discouse of interest.\nA baseline for each category is also defined for comparison. It is calculated using a 100 iteration monte carlo simulation where in one iteration every unique pair of models is compared:\n1.  \"The chosen model\" in this case is chosen at random.\n2.  If, within this category, the chosen model has a ground truth accuracy that is greater than the other model's ground truth accuracy minus the tolerance level, then this is scored as a success.\nThe success % of outcomes for the baseline is calculated per category, averaged over the 100 iterations."}, {"title": "3.5 LLMs studied", "content": "This research studied 10 candidate models in particular, all of whom were queried through the Openrouter.ai API. The following table lists the Openrouter.ai model identifier (which is a cannonical identifier for the model and model-version used) and the short name for the model used later in the results section of this paper."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Success % averaged over all categories", "content": "The success % averaged over all categories is graphed against all tolerance levels in Figure 1."}, {"title": "4.2 Models ranked in each category by QA Benchmark, and Response Dispersion", "content": "The following tables give details, broken down by each topic category found in IRC-WikiTriviaQA, of the accuracy of each model on the QA benchmark, of the orderings of the models given by rank dispersion calculations for both the OpenAI text-embedding-3-large embeddings and the RSS embeddings, and the correlations between the rankings of the models by all three metrics."}, {"title": "5 Discussion and Future Work", "content": ""}, {"title": "5.1 Shortcomings", "content": "This document repurposed a trivia question dataset previously unused for QA benchmark-ing. While there are many already-existing QA benchmark datasets (see, for example, the long but non-exhaustive list in Wang (2022) and Chang et al. (2024, section 4)), few of them attach explicit topic categories to their respective questions. One of the rare exam-ples to do so subset of the Quasar dataset (Dhingra et al., 2017) called Quasar-T-dev, which contains 132 questions over 9 topic categories (math & science 21, arts 10, language 15, food 14, movies & music 36, sports 3, general 24, history & religion 33, people & places 58; note the sum is greater than 132 questions because some questions belong in multi-ple categories). There are, however, some QA datasets intended for specific topic domains and so they can provide at least another data point, such as MultiMedQA, \"a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA\", from (Singhal et al., 2023). It is also study to dropped categories from the original IRC-Wiki Trivia dataset for more datapoints. The author originally dropped them to keep the studied categories roughly comparable in terms of representing general-knowledge trivia categories so that the averages calculated in subsection 4.1 wouldn't be"}, {"title": "5.2 Future Directions", "content": "Since response dispersion is an automated way to gauge topic knowledge, the author feels it would be useful as another automated metric to track when finetuning LLMs for a specific task. Thus response dispersion can be used to track an LLM learning about a task. Furthermore, when incrementally learning from new data, LLMs are at risk of \"catastrophic forgetting\" of prior probability distributions as its weights change (van de Ven et al., 2022). The author believes and would like to see validation in the future that measuring response dispersion can be metric that guards against catastrophic forgetting of tracked topics, since an increase in response dispersion for an already-known topic should indicate a loss of certainty in the LLM's response generations about that topic.\nThis paper also introduces reference sentence similarity embeddings and shows that the response dispersion defined by them performs nearly as well for comparing hypothetical QA benchmarks as do the embeddings produced by OpenAI's text-embedding-3-large text embeddings, all while being fast to compute locally. RSS embeddings seem to make a lot of sense in any context where you are comparing one sentence against other already known relevant sentences. This strongly suggests, theoretically at least, that rss embeddings should perform well for Retrieval Augmented Generation (RAG), where the task is to find documents relevant to a given user query in order to generate a relevant answer from the relevant documents."}]}