[{"title": "Modality-Inconsistent Continual Learning of Multimodal Large Language Models", "authors": ["Weiguo Pian", "Shijian Deng", "Shentong Mo", "Yunhui Guo", "Yapeng Tian"], "abstract": "In this paper, we introduce Modality- Inconsistent Continual Learning (MICL), a new continual learning scenario for Multimodal Large Language Models (MLLMs) that involves tasks with inconsistent modalities (image, audio, or video) and varying task types (captioning or question-answering). Unlike existing vision-only or modality-incremental settings, MICL combines modality and task type shifts, both of which drive catastrophic forgetting. To address these challenges, we propose MoInCL, which employs a Pseudo Targets Generation Module to mitigate forgetting caused by task type shifts in previously seen modalities. It also incorporates Instruction-based Knowledge Distillation to preserve the model's ability to handle previously learned modalities when new ones are introduced. We benchmark MICL using a total of six tasks and conduct experiments to validate the effectiveness of our proposed MoInCL. The experimental results highlight the superiority of MoInCL, showing significant improvements over representative and state-of-the-art continual learning baselines.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs), leveraging the generative capabilities of LLMs, have demonstrated remarkable performance across diverse modality-specific tasks (Li et al., 2022b, 2023; Zhang et al., 2023; Liu et al., 2023;\nPanagopoulou et al., 2023; Liu et al., 2024). MLLMs typically consist of a pre-trained modality encoder, like CLIP (Radford et al., 2021) for visual data, a pre-trained LLM, and a modality adapter that projects modality-specific features into the language token space. During training, the modality encoder is usually frozen to preserve its pre-trained knowledge, while the adapter and, optionally, the LLM are fine-tuned to align cross-modal representations and enhance task performance.\nWhile fine-tuned MLLMs have demonstrated promising performance across various multimodal tasks, including impressive zero-shot capabilities on unseen instructions (He et al., 2023), adapting to novel tasks still requires task-specific fine-tuning. Nevertheless, existing studies (He et al., 2023; Zeng et al., 2024; Zheng et al., 2024) indicate that fine-tuning MLLMs on new tasks can lead to significant performance degradation on previously learned tasks, a phenomenon known as catastrophic forgetting, which remains the key challenge in continual learning. To address this issue, several works explore new approaches to enable continual training of MLLMs while mitigating the catastrophic forgetting issue. For instance, He et al. (2023) introduce the continual instruction tuning task for multimodal large language models, and propose an adapter-based method to handle it. Zheng et al. (2024) further explore the negative forward transfer problem in continual instruction tuning of MLLMs and propose a prompt-"}, {"title": "2 Related Work", "content": "based method to mitigate these problems. Cao et al. (2024) propose a MLLM-based continual learning framework but mainly focusing on class-incremental image classification. While existing methods have demonstrated their abilities in alleviating the catastrophic problem in the continual learning scenario of MLLMs, they primarily focus on image modality, ignoring more general multimodal scenarios beyond image. Recently, Yu et al. (2024) introduced a modality-incremental setting for MLLMs, but treated each modality as a single, non-incremental task, ignoring the incremental nature of task types within modalities.\nTo address these issues, in this paper, we introduce Modality-Inconsistent Continual Learning (MICL), a novel continual learning scenario for MLLMs. In MICL, different task types, such as captioning and question-answering (QA), are introduced incrementally across learning steps incorporated with inconsistent modalities, as illustrated in Fig. 1. Unlike existing incremental learning settings of MLLMS, MICL not only highlights the modality-inconsistent (modality-incremental) scenario but also emphasizes the potential catastrophic forgetting problem arising from task type incrementality combined with modality inconsistency.\nMoreover, we propose MoInCL (Modality- Inconsistent Continual Learning), a novel continual learning approach designed to address the MICL problem. By leveraging the generative capabilities of the LLM backbone, MoInCL introduces a Pseudo Target Generation Module (PTGM) to handle the task type shifts inherent in the task. Additionally, an Instruction-based Knowledge Distillation (IKD) constraint for LLM backbone is incorporated to preserve its ability to understand modality- and task-aware knowledge, preventing the degradation of its learned capabilities.\nWe evaluate our method across image, audio, and video modalities, combined with captioning and question-answering (QA) tasks, resulting in six multimodal incremental tasks (Image Captioning, Image QA, Audio Captioning, Audio QA, Video Captioning, and Video QA). Our experiments demonstrate that MoInCL significantly outperforms representative and state-of-the-art continual learning methods, effectively addressing both modality and task type shifts within MICL. In summary, this paper contributes the following:\n\u2022 We propose the Modality-Inconsistent Continual Learning, a more general and practical continual learning scenario of MLLMs, where different modalities are introduced incrementally combined with different task types.\n\u2022 We propose a novel continual learning approach named MoInCL to tackle the task. In MoInCL, a Pseudo Target Generation Module (PTGM) is introduced to address the task type shift problem of previously learned modalities through incremental steps. Moreover, we propose the Instruction-based Knowledge Distillation (IKD) constraint to prevent the LLM from the forgetting of learned both modality- and task-aware knowledge in old tasks.\n\u2022 We benchmark the proposed MICL across three modalities\u2014image, audio, and video\u2014and two task types: captioning and question-answering, resulting in six incremental tasks. Experimental results demonstrate that our approach, MoInCL, significantly outperforms representative and state-of-the-art continual learning methods, showcasing its effectiveness in mitigating catastrophic forgetting from both modality and task type perspectives."}, {"title": "2.1 Multimodal Large Language Models", "content": "Recent advances have extended Large Language Models (LLMs) to handle multimodal inputs such as images, audio, and video. Early work like CLIP (Radford et al., 2021) demonstrated the effectiveness of aligning textual and visual representations for zero-shot image classification. Flamingo (Alayrac et al., 2022) further integrated vision encoders with LLMs via cross-attention, significantly improving visual question answering (VQA) and image captioning. Subsequent models like BLIP (Li et al., 2022b) and PaLM-E (Driess et al., 2023) scaled multimodal pre-training, with BLIP using a two-stage training strategy and PaLM-E incorporating embodied reasoning. More recently, LLaVA (Liu et al., 2023), InstructBLIP (Dai et al., 2023), and X-InstructBLIP (Panagopoulou et al., 2023) have leveraged instruction tuning to refine the alignment between multimodal inputs and language, pushing the boundaries of multimodal reasoning and generation. Despite this progress, challenges persist as models scale to new modalities or tasks. Effectively integrating each modality without degrading performance on others remains a key issue."}, {"title": "2.2 Continual Learning", "content": "Moreover, robust continual learning strategies are crucial to prevent catastrophic forgetting and maintain knowledge across both previously learned and newly introduced modalities as new modalities or task types are integrated.\nContinual learning aims to enable models to learn incrementally while retaining previously acquired knowledge. Regularization-based methods, such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), assign importance to model parameters to prevent drastic updates (Kim et al., 2023). Knowledge distillation (KD) (Li and Hoiem, 2017; Rebuffi et al., 2017; Pian et al., 2023; Mo et al., 2023; Ahn et al., 2021; Douillard et al., 2020) and memory replay (Rebuffi et al., 2017; Pian et al., 2024; Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017) are other common strategies, where KD-based methods preserve past learned knowledge by aligning the predictions or internal features of a new model with those of an older one, and memory replay-based methods utilize a small memory set to store samples from old tasks, allowing the model to review a small number of old data while training on the current task (Rebuffi et al., 2017; Pian et al., 2024; Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017). Pseudo-rehearsal approaches (Odena et al., 2017; Ostapenko et al., 2019) take this a step further by generating synthetic examples via a generative model, reducing the need to store large amounts of data.\nFor MLLMs, where multiple modalities (e.g., images, audio, video) interact with language models, catastrophic forgetting is especially severe. Recent adapter-based continual instruction tuning (He et al., 2023) and prompt-based strategies (Zheng et al., 2024) help retain previously learned knowledge, yet they mainly target image-text modalities. A modality-incremental scenario (Yu et al., 2024) has been explored, treating each modality as a separate task. However, it does not fully address evolving task types within each modality. To tackle this gap, we propose a new Modality-Inconsistent Continual Learning (MICL) scenario along with a novel approach to handle it effectively."}, {"title": "3 Method", "content": null}, {"title": "3.1 Problem Formulation", "content": "In this subsection, we formalize the definition of our proposed Modality-Inconsistent Continual Learning (MICL). Given a sequence of T tasks {T1, T2, ..., Tr}, MICL aims to train the Multimodal Large Language Model (MLLM) F\u03b8 with parameters \u03b8 across these tasks incrementally. For the i-th task Ti, we have $T_i = \\{(X_{i,j}, t_{i,j}, Y_{i,j})=1, M_i, P_i\\}$, where Mi and Pi denote the modality and task type of task Ti, respectively. xi,j, ti,j, and yi,j present the modality\u2019s input data, the input text, and the target text of the j-th data sample of task Ti. In our setting, the input text ti,j varies depending on the task type. For captioning tasks, it may consist of a simple instruction, such as \u201cDescribe the image/video/audio.\u201d\nFor question-answering (QA) tasks, the input text consists of sample-specific questions tailored to each instance. Moreover, the target text yi,j typically consists of detailed description sentences for captioning tasks, while for QA tasks, it is usually limited to a few answer words. We define Di = {(Xi,j, ti,j, Yi,j)=1} as the available training data when training the model F\u03b8 on task Ti. Please note that training with data from multiple modalities simultaneously may require multiple forward passes, as data from different modalities cannot be combined into a single mini-batch. Therefore, following the settings in modality-incremental learning (Yu et al., 2024), we do not include the memory set for replay in our MICL, resulting in a memory-free continual learning scenario. In summary, the training process on an incremental task Ti can be presented as:\n$\\Theta_{i} = \\underset{\\Theta}{\\operatorname{argmin}} \\mathbb{E}_{(x,t,y)\\sim D_{i}}[\\mathcal{L}(F_{\\Theta_{i-1}}(x, t), y)],$ (1)\nwhere L denotes the cross-entropy loss function between the generated results and the target text for training the MLLM."}, {"title": "3.2 Framework Overview", "content": "To address our proposed Modality-Inconsistent Continual Learning (MICL), we introduce a novel continual learning method, MoInCL, as illustrated in Fig. 2. MoInCL primarily comprises a Pseudo Target Generation Module (PTGM) and an Instruction-based Knowledge Distillation (IKD) constraint. For the MLLM, we adopt the LLaVA- like (Liu et al., 2023) architecture, which contains the same core components as LLaVA (modality encoder, projection layer, and LLM). However, we do not directly use LLaVA or its pre-trained parameters, as it is designed to process only the visual"}, {"title": "3.3 Pseudo Target Generation Module", "content": "modality, and its visual pre-training could introduce biases in the context of continual learning. Please note that, for fair comparison, all the baseline methods use the same model architecture as our method. During training, the modality encoders remain frozen, while the LLM is fine-tuned using LORA (Hu et al., 2022).\nWe now describe the Pseudo Target Generation Module (PTGM). Our key motivation is to leverage the text generation capability of the LLM component in the MLLM to address the task type shift challenge in continual learning. PTGM generates input and target text for different task types based on the modality input data of the current task. By utilizing the generated pseudo input text and pseudo targets, the model can effectively handle both the current task type and previously learned task types within the current modality.\nIn our PTGM, we maintain a set LM = {} to represent all learned modalities. For example, LM = {\"image\", \"audio\"} indicates that the model has been trained on tasks involving image or audio modalities. And for learned modalities, we maintain a modality-specific set LTM = {} to denote the learned task types of modality M. For instance, LTimage = {\u201ccaptioning"}, "if only image captioning task has been learned for image modality. Since different task types have distinct forms, the pseudo target generation process varies accordingly for each task type. Specifically, for a current task Ti with the modality of Mi, if Mi is a learned modality, i.e. Mi \u2208 LM, the PTGM will be used to generate pseudo targets for task types within LTM. If \u201ccaptioning\u201d \u2208 LTMi, the pseudo input text should be a simple instruction guiding the model to generate a description of the input data. In this case, the pseudo input text generation process can be implemented by automatically filling the template to produce the result \u201cDescribe the Mi\u201d. On the other hand, if \u201cQA\u201d \u2208 LTMi, directly applying a template is not suitable, as the pseudo QA pair should be specifically tailored to the modality\u2019s data rather than relying on generic templates. To overcome this issue, we utilize the generation ability of the LLM to generate the pseudo QA pair from the caption text of the current modality\u2019s data. Please note that in our MICL scenario, the task types considered are captioning and question-answering. Therefore, when generating pseudo QA pairs, the current task"]}, {"title": "3.4 Instruction-based Knowledge Distillation", "content": "should correspond to the captioning task of the current modality. To generate QA pairs from captions, we employ a three-round generation process by prompting the pre-trained LLM component of the MLLM F. Details of this process can be found in the Appendix. In summary, we use the following formulation to denote the pseudo target generation process:\n$\\hat{t},\\hat{y} = PTGM(x, y, p),$\ns.t. Mi \u2208 LM, Pi \u2209 LTMi, (2)\nwhere p \u2208 LTMi is a learned task type of modality Mi (please note that p \u2260 P\u2081), \u0217 and \u0177 denote the generated pseudo input text and pseudo target, respectively. x and y are the modality data and target text sampled from Di. Please note that only \u0302 is used for generating pseudo targets, while only y is utilized for generating pseudo QA pairs.\nAfter obtaining the pseudo input text and pseudo target, a dual consistency constraint is applied between (1) the pseudo outputs of the current model F\u03b8i, and the old model F\u03b8i\u22121, and (2) the pseudo target and the pseudo output of the current model. This process is formulated as:\n$\\mathcal{L}_{p} = \\mathbb{E}_{(x,t)\\sim D_{i}} [\\lambda_{i}^{c}\\mathcal{L}_{CE}(\\hat{y}\\'||\\hat{y}) + \\lambda_{i}^{k}\\mathcal{L}_{KL}(\\hat{y}\\'||y\\')],\ns.t. y\\' = F_{\\Theta_{i}}(x,t), y\\' = F_{\\Theta_{i-1}} (x, t),$ (3)\nwhere \u0177' and \u0177' denote the pseudo output from the old model and current model, respectively. \u03bbi and \u03bbi present the weights to balance the two loss values for task Ti.\nIn the previous subsection, we introduced the proposed PTGM to address the task type shift problem in the MICL scenario. However, when new modalities are introduced, the model faces a modality shift, leading to catastrophic forgetting of previously learned modalities. Additionally, as the PTGM generates pseudo targets only for seen modalities, the task type shift problem persists when training on tasks involving novel modalities. Furthermore, different modalities do not share the modality encoder or the modality projection, meaning that the shift problems primarily arise from updates to the LLM component in the MLLM. This results in the degradation of the LLM\u2019s ability to handle previously learned modalities. To address these issues, we propose Instruction-based Knowledge Distillation (IKD), a text instruction-based constraint"}, {"title": "3.5 Overall Training Target", "content": "designed to prevent the LLM from forgetting its learned capabilities in dealing with old modalities. Specifically, as illustrated in Fig. 2, IKD aligns the outputs of the LLM component from both the old and current models by applying a consistency loss, i.e. KL divergence, on their responses to the same text instruction input. In this way, instead of merely learning to handle tasks from new modalities, the current LLM\u2019s generative ability is also aligned with that of the previous LLM, thereby mitigating degradation in its ability to handle previously learned modalities. To achieve this, we introduce a pure text instruction set within IKD, which is maintained throughout the incremental steps. Since this pure text instruction set contains only text and no modality-specific data, it is not considered part of any multimodal tasks in our MIML scenario. As a result, maintaining this set does not violate the continual learning constraint that prohibits access to data from previous tasks during future tasks. This process can be formulated as:\n$\\mathcal{L}_{ins.} = \\mathbb{E}_{t\\prime \\sim \\mathcal{I}} [\\mathcal{L}_{KL}(f_{\\Theta_{i}}(t\\prime)||f_{\\Theta_{i-1}}(t\\prime))],$ (4)\nwhere I denotes the pure text instruction set, f\u03b8i and f\u03b8i\u22121 denote the LLM component of the F\u03b8i and F\u03b8i\u22121, respectively.\nAbove, we present our proposed Pseudo Target Generation Module (PTGM) and Instruction-based Knowledge Distillation (IKD) constraint. When training on a current task Ti, we have the main loss function:\n$\\mathcal{L}_{main} = \\mathbb{E}_{(x,t,y)\\sim D_{i}} [\\mathcal{L}_{CE}(\\hat{y}||y)],\ns.t. \\hat{y} = F_{\\Theta_{i}}(x, t),$ (5)\nwhere \u0177 is the output of the output of the current model F\u03b8i, by taking data samples from current task\u2019s training data Di as input.\nFinally, in our overall training target, the dual consistency constraint for generated pseudo targets Lpseudo and the IKD constraint Lins. are combined with the main training loss of task Ti:\n$\\mathcal{L} = \\mathcal{L}_{main} + \\mathcal{L}_{p.} + \\mathcal{L}_{ins.}$ (6)\nAdditionally, inspired by the parameters/weights fusion mechanism proposed in existing works (Xiao et al., 2023; Sun et al., 2024), which have demonstrated effectiveness in"}, {"title": "4 Experiments", "content": "Algorithm 1 Training of MoInCL on task Ti\nRequire: Old model F\u0398i\u22121, training set Di, pure text instruction set I, current modality Mi, current task type Pi, learned modalities set LM, learned task type for the current modality LTMi (only if M\u00bf \u2208 LM), learning rate \u03b7, scalars \u03bb\u03af, \u03bb\u03af, \u03b1i\n1: Initialize current model F\u0398i from F\u0398i\u22121\n2: if Mi & LM then\n3:\n{} \u2192 LTMi\n4: end if\n5: while not converged do\n6: Sample data (x, t, y) ~ Di\n7: L = LCE(F\u0398i(x,t)||y)\n8: if Mi \u2208 LM and LTMi \u2260 \u00d8 then\n9: \u0302t,\u1ef9 = PTGM(x, y, p), s.t. p \u2208 LTMi\n10: \u0177' = F\u0398i(x,t), \u0177 = F\u0398i\u22121 (x, t)\n11: Lp. = \u03bbiLCE(\u0177'||\u1ef9) + \u03bb\u00bfLKL(\u0177'||Y')\n12: L = L + Lp.\n13: end if\n14: Sample instruction data t' ~ I\n15: Lins. = LKL(f\u0398i(t')||f\u0398i\u22121(t'))\n16: L = L + Lins.\n17: \u0398i \u2190 \u0398i \u2212 \u03b7\u2207L\n18: \u03b8\u03b5 \u2190 \u03b1\u0390\u03b8\u0390 + (1 \u2212 \u03b1\u0390)\u03b8i\u22121\n19: end while\npreserving learned knowledge from previous tasks by applying a weighted sum between the old and current models\u2019 parameters/weights, we also adopt the parameters fusion mechanism on the LLM component of the MLLM to further prevent it from forgetting the capabilities of handling previously learned modalities, which can be denoted as:\n$\\theta_{i} = \\alpha_{i}\\theta_{i} + (1 - \\alpha_{i})\\theta_{i-1},$ (7)\nwhere e denotes the parameters of the LLM component of the MLLM, a\u017c is the weight for balancing the two groups of parameters. The overall algorithm of our MoInCL is presented in Alg. 1."}, {"title": "4.1 Experimental Setup", "content": "Dataset. In our proposed Modality-Inconsistent Continual Learning (MICL), we include six tasks: Image Captioning, Image QA, Audio Captioning, Audio QA, Video Captioning, and Video QA. Each task is represented by a commonly used dataset. Specifically, we use the Flickr30K (Young et al., 2014) dataset for the Image Captioning task, the OK-VQA (Marino et al., 2019) dataset for the Image QA task, the AudioCaps (Kim et al., 2019) dataset for the Audio Captioning task, the Clotho- AQA (Lipping et al., 2022) dataset for the Audio QA task, the MSR-VTT (Xu et al., 2016) dataset for the Video Captioning task, and the MSVD- QA (Xu et al., 2017) dataset for the Video QA task. More dataset details are provided in the Appendix.\nBaselines. In our experiments, we compare our proposed MoInCL with the following continual learning methods: Fine-tuning, LwF (Li and Hoiem, 2017), EWC (Kirkpatrick et al., 2017), EWF (Xiao et al., 2023), and PathWeave (Yu et al., 2024). Among these, LwF, EWC, and EWF are representative traditional continual learning methods, while PathWeave is the most recent state- of-the-art continual learning method designed for MLLMs, which involves a modality-aware adapter-in-adapter mechanism to address the modality-shift problem in modality-incremental learning of MLLMs. Please note that, for a fair comparison, all baseline methods use the same model architecture as our approach, including the Large Language Model (LLM) component.\nEvaluation Metrics. Following (Panagopoulou et al., 2023), we use the CIDEr score (Vedantam et al., 2015) and prediction accuracy as evaluation metrics to evaluate captioning tasks and QA tasks, respectively. For all baselines and our method, we report the average final performance across all learned tasks, i.e., the average performance of all tasks after completing the training of the final task. Since captioning and QA tasks use different evaluation metrics, we separately report the average final performance for each task type: the average final CIDEr score for captioning tasks and the average final accuracy for QA tasks. We formulate them as:\nAvg.CIDer = \\frac{1}{N_{cap.}}\\sum_{i=1}^{T} c_i\\, (8)\ns.t. P_{i} = \\text{``Captioning''},\\ where Ncap. denotes the number of captioning tasks, cf denotes the CIDEr score of task Ti after completing the training of task TT if task Ti is a captioning task. Similarly, the average final accuracy can be formulated as:\nAvg. Acc. = \\frac{1}{N_{QA}}\\sum_{i=1}^{T} a_i\\, (9)\ns.t. P_{i} = \\text{``QA''},\\ where NQA denotes the number of QA tasks, af denotes the accuracy of task Ti after completing the training of task TT if task Ti is a QA task. Furthermore, to evaluate the anti-forgetting capability of each method, we propose two metrics: the forgetting ratio and the average forgetting ratio. The forgetting ratio measures the proportion of performance drop for each task after completing the training of the final task, while the average forgetting ratio represents the mean forgetting ratio across all tasks, which can be formulated as:\nForget.i=(ss)/si, \nAvg. Forget. = \\frac{1}{T}\\sum_{i=1}^{T} Forget.i (10)\nwhere s and st denotes the testing score of task Ti after the training of task Ti and TT, respectively.\nImplementation Details. We implement our experiments using Pytorch (Paszke et al., 2019) and LaVIS (Li et al., 2022a) framework. For the LLM component of the Multimodal Large Language Model (MLLM), we adopt the Llama-3.2- 1B-Instruct (Dubey et al., 2024) architecture and initialize it with pre-trained parameters at the start of the first task. Following the implementation in (Panagopoulou et al., 2023), we apply the EVA-CLIP-ViT-G/14 (Fang et al., 2023) as the Image Encoder and Video Encoder, and the BEATSiter3+ (Chen et al., 2023) as the Audio Encoder. Each video input consists of 4 frames, and the audio input also consists of 4 frames with the sampling rate of 11kHz. For the video and audio modalities, the Video Encoder and Audio Encoder process each frame individually and then concatenate the encoded patches from all frames, following the approach in (Panagopoulou et al., 2023). For the Image Projection, we use a two-layers MLP with the GELU (Hendrycks and Gimpel, 2016) activation function. For the Video and Audio Projection, both of them include a single convolutional"}, {"title": "4.2 Experimental Comparison", "content": "layer as a pooling layer to reduce the total number of patches, followed by a two-layers MLP with the GELU activation function. For each task, we train the model using the AdamW (Loshchilov and Hutter, 2019) optimizer with an initial learning rate of 1e-5, adjusted using the cosine decay strategy, and a weight decay of 5e-2. We train our proposed MoInCL and all baseline methods on a NVIDIA RTX A6000 Ada GPU. During the training of our approach, the pure text instructions in the Instruction-based Knowledge Distillation (IKD) constraint are randomly sampled from the Natural Instructions (Mishra et al., 2022) dataset.\nWe conduct experiments using two random task orders. For Order 1, the tasks are arranged as: Audio Captioning \u2192 Image Captioning \u2192 Video QA \u2192 Audio QA \u2192 Image QA \u2192 Video Captioning. For Order 2, the task sequence is: Image Captioning \u2192 Video Captioning \u2192 Video QA \u2192 Image QA \u2192 Audio Captioning \u2192 Audio QA.\nThe main results are shown in Tab. 1. We can see that our proposed MoInCL achieves state-of-the-art performance compared to all baseline methods. Except the average final accuracy of the Order 2, our method has the best performance on all three metrics across both orders. Specifically, in Order 1, our method surpasses the best baseline results by 16.25, 1.71, and 24.58 in terms of average final CIDEr score, average final accuracy, and average forgetting ratio, respectively. In Order 2, our method outperforms the best baseline results by 37.21 and 37.71 for average final CIDEr score and average forgetting ratio, respectively.\nWe also present the forgetting ratio of each task in both orders in Tab. 2 and 3, from which we can see that, our method outperforms baseline methods significantly, further demonstrating the superiority of our proposed method in mitigating the catastrophic forgetting in our proposed MICL scenario. The testing results of the first three incremental tasks (Image Captioning \u2192 Video Captioning \u2192 Video QA) are shown in Tab. 4. From these results, we observe that when the modality shift occurs from the Image Captioning task to the Video Captioning task, the performance of the previous task (Image Captioning) drops significantly across all baseline methods, with CIDEr score reductions ranging from 8.34 to 23.63. Additionally, when the task type shift occurs from the Video Captioning task to the Video QA task, the performance of the previous task (Video Captioning) also decreases significantly, with CIDEr score reductions ranging from 35.61 to 40.90. These results further vali-"}, {"title": "5 Conclusion", "content": "date our insight that both modality shift and task type shift directly contribute to the catastrophic forgetting problem, underscoring the core challenges of our proposed MICL scenario. For our method, the performance drop for the Image Captioning task is only 3.91 when the modality shift occurs. Moreover, we observe that the performance of the Video Captioning task improves after training on the Video QA task which introduces the task type shift issue. These findings further highlight the effectiveness of our method in mitigating the catastrophic forgetting problem in MICL by addressing both modality shift and task type shift challenges. For detailed results of each task and qualitative analysis, please refer to the Appendix.\nIn this paper, we explore the Modality-Inconsistent Continual Learning (MICL), a novel and practical continual learning scenario of Multimodal Large Language Models (MLLMs). To address the introduced MICL, we propose MoInCL, which incorporates a Pseudo Targets Generation Modul and an Instruction-based Knowledge Distillation constraint to mitigate the catastrophic forgetting caused by the inherent task type shift and modality shift problem in the context of MICL. Experiments on six multimodal incremental tasks demonstrate the effectiveness of our proposed MoInCL. This paper introduces a new direction for the continual learning of MLLMs."}, {"title": "Limitations", "content": "Our Modality-Inconsistent Continual Learning (MICL) introduces a novel and practical continual learning scenario by incorporating inconsistent modalities and varying task types across incremental tasks. However, the scope of our work is constrained by the limited number of modalities (audio, image, and video) and task types (captioning and question-answering) included in the experiments. This restricts the generalizability of MICL to scenarios involving a broader range of modalities and task types. Another limitation lies in the scale of the datasets of each task. While our experimental results demonstrate the effectiveness of our proposed method in MICL scenario, the experiments are conducted on relatively small-scale datasets, which may not fully reflect the complexity and diversity encountered in real-world multimodal tasks.\nIn the future, we plan to enhance our MICL framework by incorporating additional modalities, such as depth, 3D, or even joint inputs like joint audio-visual modalities. We also aim to introduce a broader range of task types, such as reasoning, grounding, decision-making, etc. Furthermore, scaling up MICL to larger datasets within each task is also a key objective to better enable the model to address the complexity and diversity of real-world multimodal tasks in continual learning."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Three-Round QA Pairs Generation from Captions", "content": "Inspired by the question answering text generation process in (Panagopoulou et al., 2023), we adopt a similar three-round QA pair generation process from captions in our proposed Pseudo Targets Generation Module (PTGM). Given a caption from the dataset of the current captioning task Ti, the objective is to generate a QA pair to address the task type shift problem when training on a captioning task within a seen modality. This process relies entirely on prompt engineering, where the caption is used as input to the pre-trained Large Language Model (LLM) component of our Multimodal Large Language Model (MLLM). Please note that, the LLM component employed in this process uses pre-trained weights, i.e., the weights that are not fine-tuned on our incremental tasks.\nIn Round 1, the LLM takes an input with the format of: Given the Mi context: 'y', generate a potential short answer from it. Provide just one or two words. The answer words should be strictly selected from the context. Provide only the answer, nothing else. Answer:,where Mi is the modality of the task Ti, y denotes the sampled caption text. And the output of the LLM is used as the temporal short answer \u1ef9.\nIn Round 2, the LLM takes the following prompt as input: Given the Mi context: \u2018y\u2019 and the answer: \u2018\u1ef9\u2019, generate a question for the answer that can be inferred from the context. Provide only one question and nothing else. Question:. The output of the LLM in Round 2 is the question we aim to generate, which is denoted as t.\nFinally, in Round 3, the LLM processes the following prompt as input: Answer the question using the given context. The answer should be only one or two words. Context: \u2018y\u2019. Question: \u2018\u0142. Answer:, and generates the final short answer \u1ef9.\nBased the above three rounds, the pseudo QA pair is obtained, where t represents the pseudo question and \u1ef9 denotes the pseudo answer."}, {"title": "A.2 Dataset Details", "content": "In our experiments, we use the AudioCaps, Flickr30K, MSR-VTT, MSVD-QA, Clotho-AQA, and OK-VQA datasets for Audio Captioning, Im-"}, {"title": "A.3 Detailed Results of Each Task in Both Orders", "content": "age Captioning, Video Captioning, Video QA, Audio QA, and Image QA tasks, respectively. We summarize the details of these data in Tab. 5.\nWe present the detailed testing results for each task across the incremental steps in both orders in Tab. 6 and 7. These results show that our proposed MoInCL exhibits less performance drop compared to the baseline methods, demonstrating its superior ability to address catastrophic forgetting in the proposed Modality-Inconsistent Continual Learning (MICL) scenario."}, {"title": "A.4 Qualitative Analysis", "content": "We present the qualitative results of the Fine- tuning, LwF (Li and Hoiem, 2017), EWC (Kirk- patrick et al"}]