{"title": "Modality-Inconsistent Continual Learning of Multimodal Large Language Models", "authors": ["Weiguo Pian", "Shijian Deng", "Shentong Mo", "Yunhui Guo", "Yapeng Tian"], "abstract": "In this paper, we introduce Modality-Inconsistent Continual Learning (MICL), a new continual learning scenario for Multimodal Large Language Models (MLLMs) that involves tasks with inconsistent modalities (image, audio, or video) and varying task types (captioning or question-answering). Unlike existing vision-only or modality-incremental settings, MICL combines modality and task type shifts, both of which drive catastrophic forgetting. To address these challenges, we propose MoInCL, which employs a Pseudo Targets Generation Module to mitigate forgetting caused by task type shifts in previously seen modalities. It also incorporates Instruction-based Knowledge Distillation to preserve the model's ability to handle previously learned modalities when new ones are introduced. We benchmark MICL using a total of six tasks and conduct experiments to validate the effectiveness of our proposed MoInCL. The experimental results highlight the superiority of MoInCL, showing significant improvements over representative and state-of-the-art continual learning baselines.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs), leveraging the generative capabilities of LLMs, have demonstrated remarkable performance across diverse modality-specific tasks (Li et al., 2022b, 2023; Zhang et al., 2023; Liu et al., 2023;"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Multimodal Large Language Models", "content": "Recent advances have extended Large Language Models (LLMs) to handle multimodal inputs such as images, audio, and video. Early work like CLIP (Radford et al., 2021) demonstrated the effectiveness of aligning textual and visual representations for zero-shot image classification. Flamingo (Alayrac et al., 2022) further integrated vision encoders with LLMs via cross-attention, significantly improving visual question answering (VQA) and image captioning. Subsequent models like BLIP (Li et al., 2022b) and PaLM-E (Driess et al., 2023) scaled multimodal pre-training, with BLIP using a two-stage training strategy and PaLM-E incorporating embodied reasoning. More recently, LLaVA (Liu et al., 2023), InstructBLIP (Dai et al., 2023), and X-InstructBLIP (Panagopoulou et al., 2023) have leveraged instruction tuning to refine the alignment between multimodal inputs and language, pushing the boundaries of multimodal reasoning and generation. Despite this progress, challenges persist as models scale to new modalities or tasks. Effectively integrating each modality without degrading performance on others remains a key issue."}, {"title": "2.2 Continual Learning", "content": "Continual learning aims to enable models to learn incrementally while retaining previously acquired knowledge. Regularization-based methods, such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), assign importance to model parameters to prevent drastic updates (Kim et al., 2023). Knowledge distillation (KD) (Li and Hoiem, 2017; Rebuffi et al., 2017; Pian et al., 2023; Mo et al., 2023; Ahn et al., 2021; Douillard et al., 2020) and memory replay (Rebuffi et al., 2017; Pian et al., 2024; Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017) are other common strategies, where KD-based methods preserve past learned knowledge by aligning the predictions or internal features of a new model with those of an older one, and memory replay-based methods utilize a small memory set to store samples from old tasks, allowing the model to review a small number of old data while training on the current task (Rebuffi et al., 2017; Pian et al., 2024; Chaudhry et al., 2019; Lopez-Paz and Ranzato, 2017). Pseudo-rehearsal approaches (Odena et al., 2017; Ostapenko et al., 2019) take this a step further by generating synthetic examples via a generative model, reducing the need to store large amounts of data.\nFor MLLMs, where multiple modalities (e.g., images, audio, video) interact with language models, catastrophic forgetting is especially severe. Recent adapter-based continual instruction tuning (He et al., 2023) and prompt-based strategies (Zheng et al., 2024) help retain previously learned knowledge, yet they mainly target image-text modalities. A modality-incremental scenario (Yu et al., 2024) has been explored, treating each modality as a separate task. However, it does not fully address evolving task types within each modality. To tackle this gap, we propose a new Modality-Inconsistent Continual Learning (MICL) scenario along with a novel approach to handle it effectively."}, {"title": "3 Method", "content": null}, {"title": "3.1 Problem Formulation", "content": "In this subsection, we formalize the definition of our proposed Modality-Inconsistent Continual Learning (MICL). Given a sequence of $T$ tasks $\\{T_1, T_2, ..., T_T\\}$, MICL aims to train the Multimodal Large Language Model (MLLM) $F_\\Theta$ with parameters $\\Theta$ across these tasks incrementally. For the $i$-th task $T_i$, we have $T_i = \\{(X_{i,j}, t_{i,j}, Y_{i,j})^n_{j=1}, M_i, P_i\\}$, where $M_i$ and $P_i$ denote the modality and task type of task $T_i$, respectively. $x_{i,j}$, $t_{i,j}$, and $y_{i,j}$ present the modality's input data, the input text, and the target text of the $j$-th data sample of task $T_i$. In our setting, the input text $t_{i,j}$ varies depending on the task type. For captioning tasks, it may consist of a simple instruction, such as ''Describe the image/video/audio.'' For question-answering (QA) tasks, the input text consists of sample-specific questions tailored to each instance. Moreover, the target text $y_{i,j}$ typically consists of detailed description sentences for captioning tasks, while for QA tasks, it is usually limited to a few answer words. We define $D_i = \\{(X_{i,j}, t_{i,j}, Y_{i,j})^n_{j=1}\\}$ as the available training data when training the model $F_\\Theta$ on task $T_i$. Please note that training with data from multiple modalities simultaneously may require multiple forward passes, as data from different modalities cannot be combined into a single mini-batch. Therefore, following the settings in modality-incremental learning (Yu et al., 2024), we do not include the memory set for replay in our MICL, resulting in a memory-free continual learning scenario. In summary, the training process on an incremental task $T_i$ can be presented as:\n$\\Theta_i = argmin E_{(x,t,y)\\sim D_i}[L(F_{\\Theta_{i-1}}(x, t), y)],$ (1)\nwhere $L$ denotes the cross-entropy loss function between the generated results and the target text for training the MLLM."}, {"title": "3.2 Framework Overview", "content": "To address our proposed Modality-Inconsistent Continual Learning (MICL), we introduce a novel continual learning method, MoInCL, as illustrated in Fig. 2. MoInCL primarily comprises a Pseudo Target Generation Module (PTGM) and an Instruction-based Knowledge Distillation (IKD) constraint. For the MLLM, we adopt the LLaVA-like (Liu et al., 2023) architecture, which contains the same core components as LLaVA (modality encoder, projection layer, and LLM). However, we do not directly use LLaVA or its pre-trained parameters, as it is designed to process only the visual"}, {"title": "3.3 Pseudo Target Generation Module", "content": "We now describe the Pseudo Target Generation Module (PTGM). Our key motivation is to leverage the text generation capability of the LLM component in the MLLM to address the task type shift challenge in continual learning. PTGM generates input and target text for different task types based on the modality input data of the current task. By utilizing the generated pseudo input text and pseudo targets, the model can effectively handle both the current task type and previously learned task types within the current modality.\nIn our PTGM, we maintain a set $\\mathcal{L}M = \\{\\}$ to represent all learned modalities. For example, $\\mathcal{L}M = \\{\"image\", \"audio\"\\}$ indicates that the model has been trained on tasks involving image or audio modalities. And for learned modalities, we maintain a modality-specific set $\\mathcal{L}T_M = \\{\\}$"}, {"title": "3.4 Instruction-based Knowledge Distillation", "content": "In the previous subsection, we introduced the proposed PTGM to address the task type shift problem in the MICL scenario. However, when new modalities are introduced, the model faces a modality shift, leading to catastrophic forgetting of previously learned modalities. Additionally, as the PTGM generates pseudo targets only for seen modalities, the task type shift problem persists when training on tasks involving novel modalities. Furthermore, different modalities do not share the modality encoder or the modality projection, meaning that the shift problems primarily arise from updates to the LLM component in the MLLM. This results in the degradation of the LLM's ability to handle previously learned modalities. To address these issues, we propose Instruction-based Knowledge Distillation (IKD), a text instruction-based constraint"}, {"title": "3.5 Overall Training Target", "content": "Above, we present our proposed Pseudo Target Generation Module (PTGM) and Instruction-based Knowledge Distillation (IKD) constraint. When training on a current task $T_i$, we have the main loss function:\n$\\mathcal{L}_{main} = E_{(x,t,y)\\sim D_i} [L_{CE}(\\hat{y}||y)],$ (5)\ns.t. $\\hat{y} = F_{\\Theta_i}(x, t),$\nwhere $\\hat{y}$ is the output of the output of the current model $F_{\\Theta_i}$, by taking data samples from current task's training data $D_i$ as input.\nFinally, in our overall training target, the dual consistency constraint for generated pseudo targets $\\mathcal{L}_{pseudo}$ and the IKD constraint $\\mathcal{L}_{ins.}$ are combined with the main training loss of task $T_i$:\n$\\mathcal{L} = \\mathcal{L}_{main} + \\mathcal{L}_{p.} + \\mathcal{L}_{ins.}.$\nAdditionally, inspired by the parameters/weights fusion mechanism proposed in existing works (Xiao et al., 2023; Sun et al.,"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental Setup", "content": "Dataset. In our proposed Modality-Inconsistent Continual Learning (MICL), we include six tasks: Image Captioning, Image QA, Audio Captioning, Audio QA, Video Captioning, and Video QA. Each task is represented by a commonly used dataset. Specifically, we use the Flickr30K (Young et al.,"}, {"title": "4.2 Experimental Comparison", "content": "We conduct experiments using two random task orders. For Order 1, the tasks are arranged as: Audio Captioning \u2192 Image Captioning \u2192 Video QA \u2192 Audio QA \u2192 Image QA \u2192 Video Captioning. For Order 2, the task sequence is: Image Captioning \u2192 Video Captioning \u2192 Video QA \u2192 Image QA \u2192 Audio Captioning \u2192 Audio QA.\nThe main results are shown in Tab. 1. We can see that our proposed MoInCL achieves state-of-the-art performance compared to all baseline methods. Except the average final accuracy of the Order 2, our method has the best performance on all three metrics across both orders. Specifically, in Order 1, our method surpasses the best baseline results by 16.25, 1.71, and 24.58 in terms of average final CIDEr score, average final accuracy, and average forgetting ratio, respectively. In Order 2, our method outperforms the best baseline results by 37.21 and 37.71 for average final CIDEr score and average forgetting ratio, respectively.\nWe also present the forgetting ratio of each task in both orders in Tab. 2 and 3, from which we can see that, our method outperforms baseline methods significantly, further demonstrating the superiority of our proposed method in mitigating the catastrophic forgetting in our proposed MICL scenario. The testing results of the first three incremental tasks (Image Captioning \u2192 Video Captioning \u2192 Video QA) are shown in Tab. 4. From these results, we observe that when the modality shift occurs from the Image Captioning task to the Video Captioning task, the performance of the previous task (Image Captioning) drops significantly across all baseline methods, with CIDEr score reductions ranging from 8.34 to 23.63. Additionally, when the task type shift occurs from the Video Captioning task to the Video QA task, the performance of the previous task (Video Captioning) also decreases significantly, with CIDEr score reductions ranging from 35.61 to 40.90. These results further validate our insight that both modality shift and task type shift directly contribute to the catastrophic forgetting problem, underscoring the core challenges of our proposed MICL scenario. For our method, the performance drop for the Image Captioning task is only 3.91 when the modality shift occurs. Moreover, we observe that the performance of the Video Captioning task improves after training on the Video QA task which introduces the task type shift issue. These findings further highlight the effectiveness of our method in mitigating the catastrophic forgetting problem in MICL by addressing both modality shift and task type shift challenges. For detailed results of each task and qualitative analysis, please refer to the Appendix."}, {"title": "5 Conclusion", "content": "In this paper, we explore the Modality-Inconsistent Continual Learning (MICL), a novel and practical continual learning scenario of Multimodal Large Language Models (MLLMs). To address the introduced MICL, we propose MoInCL, which incorporates a Pseudo Targets Generation Modul and an Instruction-based Knowledge Distillation constraint to mitigate the catastrophic forgetting caused by the inherent task type shift and modality shift problem in the context of MICL. Experiments on six multimodal incremental tasks demonstrate the effectiveness of our proposed MoInCL. This paper introduces a new direction for the continual learning of MLLMs."}, {"title": "Limitations", "content": "Our Modality-Inconsistent Continual Learning (MICL) introduces a novel and practical continual learning scenario by incorporating inconsistent modalities and varying task types across incremental tasks. However, the scope of our work is constrained by the limited number of modalities (audio, image, and video) and task types (captioning and question-answering) included in the experiments. This restricts the generalizability of MICL to scenarios involving a broader range of modalities and task types. Another limitation lies in the scale of the datasets of each task. While our experimental results demonstrate the effectiveness of our proposed method in MICL scenario, the experiments are conducted on relatively small-scale datasets, which may not fully reflect the complexity and diversity encountered in real-world multimodal tasks.\nIn the future, we plan to enhance our MICL framework by incorporating additional modalities, such as depth, 3D, or even joint inputs like joint audio-visual modalities. We also aim to introduce a broader range of task types, such as reasoning, grounding, decision-making, etc. Furthermore, scaling up MICL to larger datasets within each task is also a key objective to better enable the model to address the complexity and diversity of real-world multimodal tasks in continual learning."}]}