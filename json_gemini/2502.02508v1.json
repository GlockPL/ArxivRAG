{"title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search", "authors": ["Maohao Shen", "Guangtao Zeng", "Zhenting Qi", "Zhang-Wei Hong", "Zhenfang Chen", "Wei Lu", "Gregory Wornell", "Subhro Das", "David Cox", "Chuang Gan"], "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced\u00b9.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks, including mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021a), programming (Chen et al., 2021; Zhuo et al., 2024) and logical reasoning (Han et al., 2024; Liu et al., 2020). One of the key techniques enabling these strong reasoning capabilities is Chain-of-Thought (CoT) prompting (Wei et al., 2022), which allows LLMs to address complex tasks by generating a series of intermediate reasoning steps. As a result, many early efforts focus on fine-tuning LLMs using large-scale, high-quality CoT reasoning chains, either through human annotation (Hendrycks et al., 2021a; Yue et al., 2024) or by distilling synthetic data from more advanced models (Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024). However, human annotation is extremely labor intensive, and distillation often limits the model's reasoning capabilities to certain level.\nApart from scaling up training resources, more recent work has focused on test-time scaling, i.e., allocating additional inference-time compute to search for more accurate solutions. This often involves extensive sampling, either by generating multiple complete solutions (Wang et al., 2023) or by sampling multiple intermediate reasoning steps (Yao et al., 2024; Wan et al., 2024). These methods typically require external feedback to guide the search process, usually through training an auxiliary reward model to rate final solutions or intermediate steps (Sun et al., 2024; Wang et al., 2024a). However, such two-player frameworks incur more model-deployment costs and do not internalize the search capabilities into a single LLM.\nOrthogonal to the above work, our study investigates a new direction that enables LLMs with autoregressive search capabilities, i.e., an extended reasoning process with self-reflection and self-exploration of new strategies. Specifically, we introduce the Chain-of-Action-Thought (COAT) mechanism, which enables LLMs to take various meta-actions during problem solving. Unlike conventional post-training consisting of large-scale supervised fine-tuning"}, {"title": "2. Related Work", "content": "We summarize the literature that is closely aligned with the scope of this paper (refer to Section B for more discussions).\nConcurrent Work. Building on the impact of OpenAI's o1 (OpenAI, 2024), significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. The most common approach relies on distilling knowledge from stronger teacher models (Huang et al., 2024a; Zhao et al., 2024; Min et al., 2024). In contrast, Satori addresses this problem from a reinforcement learning (RL) perspective and requires minimal supervision (only 10K samples in the format tuning stage). The most related concurrent work is DeepSeek's recently released R1 (Guo et al., 2025), which adopts a similar high-level strategy of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms. Additionally, DeepSeek-R1 focuses on training large-scale LLMs (671B), whereas our work provides insights into the development of smaller-scale LLMs (7B) for research purpose. Finally, as an industry-developed model, the technical details of DeepSeek-R1 (Guo et al., 2025) are not fully disclosed, making reproduction difficult, whereas our work is a fully transparent study that aims to open-source training data and training recipes.\nPost-training LLMs for Reasoning. Recent advancements have focused on extensive post-training to enhance reasoning. A line of work focus on constructing high-quality instruction-tuning datasets (Hendrycks et al., 2021a; Yue et al., 2024; Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024), but suffers from expensive annotatoin costs. More recent research has focused on self-improvement approaches, where models are trained on data generated by themselves (Zelikman et al., 2022; 2024; Singh et al., 2024; Zhang et al., 2024a). Additionally, reinforcement learning methods, particularly those based on Proximal Policy Optimization (PPO) (Schulman et al., 2017a; Ouyang et al., 2022), have been demonstrated to be more effective, which typically leverage reward models to guide the learning process (Sun et al., 2024; Wang et al., 2024a; Yuan et al., 2024).\nEnabling LLMs with Searching Abilities. Prompting-based approaches (Yao et al., 2024; Shinn et al., 2024; Hao et al., 2023; Qi et al., 2024a) guide LLMs to search for solutions via error correction and exploring alternative paths. However, such approaches cannot fundamentally enhance the LLM's reasoning abilities. Moreover, recent work has pointed out the difficulties of LLMs in self-correction (Zhang et al., 2024b; Kamoi et al., 2024). Recent research has pivoted toward training LLMs for self-exploration. Some focused on enabling trajectory-level search-iteratively identify errors in previous complete responses and produce improved responses (Saunders et al., 2022a; Kumar et al., 2024; Qu et al., 2024; Havrilla et al., 2024). Another line of research has explored step-level search, which enables LLMs to identify and correct mistakes in a more fine-grained manner. Some achieve this using another model to provide step-level feedback (Xi et al., 2024; Setlur et al., 2024; Zhang et al., 2024c; Guan et al., 2025; Zhang et al., 2024d), but such two-player frameworks suffer from high costs for model deployment. SoS (Gandhi et al., 2024) is another closely related work that attempts to train a single LLM to perform a tree search as a flattened string. However, the effectiveness of SoS has primarily been shown on simple symbolic tasks, and its ability to generalize to more complex problems remains to be explored."}, {"title": "3. Preliminaries", "content": "We address mathematical problem-solving by training a language model \\(\\pi_\\theta\\) to generate a solution \\(\\tilde{y}\\) that matches the ground truth \\(y^*\\), given a problem prompt \\(x\\). All sequences \\(x\\), \\(y\\), and \\(y^*\\) consist of tokens from a predefined dictionary. Since our approach uses reinforcement learning (RL) to train the model for solving math problems, we outline the key RL concepts below.\nReinforcement Learning (RL). RL (Kaelbling et al., 1996) involves an agent making sequential decisions to max-"}, {"title": "4. Method", "content": "We start this section by introducing the formulation of reasoning and how reasoning can be formulated as a sequential decision-making problem. Goal: We want to train LLMs to solve problems by reasoning through multiple steps rather than directly predicting the final answer. Given a problem statement \\(x\\), the model generates a sequence of reasoning steps \\(\\{y_1, y_2,..., y_L\\}\\), where \\(y_L\\) provides the final answer. However, not all intermediate steps are helpful-repeating errors does not improve accuracy. Effective reasoning requires verifying correctness, identifying mistakes, and considering alternative solutions. For instance, given \\(x\\) = \"1 + 1 =?\", the model might initially output \\(y_1\\) = 3, then recognize the mistake with \\(y_2\\) (e.g., \"Wait, let me verify...\"), before correcting it to \\(y_3\\) = 2.\nChain-of-Action-Thought reasoning (COAT). The key challenge is enabling the model to determine when to reflect, continue, or explore alternatives without external intervention. To enable this, we introduce special meta-action tokens that guide the model's reasoning process beyond standard text generation. These tokens serve as hint for the model to determine when to reassess its reasoning before proceeding.\n\u2022 Continue Reasoning (< |continue|>): Encourages the model to build upon its current reasoning trajectory by generating the next intermediate step.\n\u2022 Reflect (< | reflect | >): Prompts the model to pause and verify the correctness of prior reasoning steps.\n\u2022 Explore Alternative Solution (<|explore|>): Signals the model to identify critical flaws in its reasoning and explore a new solution.\nEach reasoning step \\(y_t\\) is a sequence of tokens, with the starting token potentially being one of the designated meta-action tokens. We refer to this formulation as Chain-of-Action-Thought reasoning (COAT). In particular, typical Chain-of-Thought reasoning (CoT) (Wei et al., 2022) can be viewed as a special case of COAT, where each reasoning step in CoT is restricted to continuation, without explicitly incorporating other types of meta-actions.\nLearning to Reason via RL. We formulate reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the model \\(\\pi_\\theta\\) starts with an input context \\(x\\) (initial state \\(z_0\\)), generates a reasoning step \\(y_1\\) (action), updates the context by appending \\(y_1\\) (next state \\(z_{t+1} = z_t \\oplus y_t\\), where \\(\\oplus\\) denotes string concatenation), and repeats this process until it produces a final answer \\(y_L\\). The reasoning terminates when the model signals completion (e.g., omitting EOS token). The simplest reward function can be \\(I\\{\\tilde{y}_L = y^*\\}\\), evaluates whether the final answer \\(\\tilde{y}_L\\) matches the ground truth \\(y^*\\). With this formulation, we could train the model to reason using RL, aiming to generate reasoning steps that maximize the expected reward. However, applying RL to reasoning presents two key challenges:\n1. Unawareness of meta-action tokens: The model doesn't understand the purpose of special tokens and fails to recognize that encountering special meta-action tokens may require reflection or proposing alternatives.\n2. Long horizon and sparse rewards: Reasoning requires long-term decision-making with rewards only at the end, which hinders learning effectiveness (Bellemare et al., 2016). The model must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state (i.e., the problem statement). This makes learning difficult because training data associated with rewards is scarce, yet rewards are essential for driving RL progress."}, {"title": "4.1. Format Tuning Through Imitation Learning", "content": "Training a base LLM \\(\\pi_\\theta\\) to perform COAT reasoning presents a significant challenge: LLMs are typically not pre-trained on COAT reasoning data that incorporates trials and errors, necessitating a post-training stage to inject this capability. To address this, we introduce format tuning (FT), a method designed to train LLMs to emulate expert COAT trajectories through imitation learning. Imitation learning techniques (Hussein et al., 2017) are widely used in the robotics domain, where agents are trained using demonstration trajectories provided by human experts (Ross and Bagnell, 2010; Ross et al., 2011; Ho and Ermon, 2016). However, generating high-quality demonstration trajectories for LLMs is prohibitively expensive for complex tasks. To efficiently construct a demonstration trajectory dataset \\(D_{syn} = \\{(x^{(i)}, \\tilde{y}^{(i)})\\}_{i=1}^n\\), we propose a multi-agent data synthesis framework that leverages three LLMs:\n\u2022 Generator: Given an input problem, a generator \\(\\pi_g\\) generates multiple reasoning paths for a given input problem using classical CoT techniques.\n\u2022 Critic: A critic \\(\\pi_c\\) evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps.\n\u2022 Reward Model: Additionally, a reward model \\(\\pi_r\\) assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory.\nThese three models collaborate to construct high-quality demonstration trajectories (details on the trajectory synthesis are provided in Appendix C). For this work, we adopt the simplest imitation learning approach, behavior cloning, which utilizes supervised fine-tuning to train the LLM policy on the expert COAT demonstration trajectories \\(D_{syn}\\). Notably, we observe that even a small number (10K) of COAT demonstration trajectories is sufficient for \\(\\pi_\\theta\\) to effectively follow the COAT reasoning format."}, {"title": "4.2. Self-improvement via Reinforcement Learning", "content": "After format tuning, the LLM policy \\(\\pi_\\theta\\) adopts the COAT reasoning style but struggles to generalize, particularly in using meta-actions for self-reflection. This limitation arises from the scarcity of demonstrations during format tuning. While collecting more demonstrations could help, it is costly and time-consuming. Instead, we explore whether the model can self-improve its reasoning via RL.\nWe start with the format-tuned LLM and train it using PPO (Schulman et al., 2017b) algorithm, a widely used RL method. In addition to training on problems \\(x\\) from the dataset \\(D\\), we also train the model \\(\\pi_\\theta\\) to begin reasoning from partial trajectories generated by the format-tuned LLM. Since reasoning errors typically arise from minor mistakes rather than fundamental flaws, re-exploring from the start is inefficient. Instead, we allow the model to restart from intermediate steps to correct errors and finally achieve correct answers. Inspired by Go-Explore (Ecoffet et al., 2019), we introduce the Restart and Explore (RAE) strategy."}, {"title": "5. Experiment", "content": "Implementation Details. We employ Qwen-2.5-Math-7B as the base model due to its strong mathematical capabilities. Our training data is sourced from the publicly available math instruction datasets, OpenMathInstruct-2 and NuminaMath-CoT. For the multi-agent data synthesis framework, the generator is required to generate high-quality, step-by-step reasoning trajectories. Therefore, we use Qwen-2.5-Math-Instruct as the generator. Meanwhile, the critic must have robust instruction-following capabilities, so we choose Llama-3.1-70B-Instruct as the critic. To ensure data quality, we filter out problems with invalid questions or incorrect labels, resulting in approximately 550k samples. Additional implementation details can be found in Appendix D.\nBenchmark and Evaluation. We conduct the main evaluation of the models using math benchmarks to assess their problem-solving abilities, including GSM8K, MATH500 (a subset of the MATH test set (Lightman et al., 2023)), AMC2023, AIME2024, and OlympiadBench. Except for GSM8K, all other datasets feature competition-level problems. The evaluation is performed using greedy decoding"}, {"title": "5.2. Out-of-Domain Transferability", "content": "Although Satori-Qwen-7B is trained only on math domain datasets, we observe that it can extrapolate its reasoning capabilities to other domains. In Table 2, we evaluate Satori-Qwen-7B on a diverse set of out-of-domain benchmarks that require reasoning capabilities but are not directly related to math. Similar to the observation on the math domain, Satori demonstrates superior performance on several benchmarks,"}, {"title": "5.3. Results on Iterative Self-improvement", "content": "Finally, we present the results of the second-round training of Satori. As shown in Table 1 and Table 2, compared to Satori-Qwen-7B, Satori-Qwen-7B (Round 2) demonstrates continuous performance gains across most in-domain and out-of-domain benchmarks. This suggests the significant potential of iterative self-improvement to push the limit of LLM's reasoning performance."}, {"title": "6. Analysis", "content": "In this section, we provide a comprehensive analysis of Satori. First, we demonstrate that Satori effectively leverages self-reflection to seek better solutions and enhance its overall reasoning performance. Next, we observe that Satori exhibits test-scaling behavior through RL training, where it progressively acquires more tokens to improve its reasoning capabilities. Finally, we conduct ablation studies on various components of Satori's training framework. Additional results are provided in Appendix E."}, {"title": "7. Concluding Remarks", "content": "The training framework of Satori exhibits significant potential for enhancing LLM reasoning capabilities. The small-scale format tuning stage serves as a warm-up phase, allowing the LLM policy to internalize a specific reasoning format, while large-scale reinforcement learning (RL) plays a crucial role in incentivizing intrinsic reasoning abilities. We believe that this framework can inspire the research community to explore more methods for achieving autoregressive search, such as developing reasoning formats with a"}, {"title": "A. Satori's Demo Examples", "content": ""}]}