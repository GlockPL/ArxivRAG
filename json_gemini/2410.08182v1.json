{"title": "MRAG-BENCH: VISION-CENTRIC EVALUATION FOR RETRIEVAL-AUGMENTED MULTIMODAL MODELS", "authors": ["Wenbo Hu", "Jia-Chen Gu", "Zi-Yi Dou", "Mohsen Fayyaz", "Pan Lu", "Kai-Wei Chang", "Nanyun Peng"], "abstract": "Existing multimodal retrieval benchmarks primarily focus on evaluating whether models can retrieve and utilize external textual knowledge for question answering. However, there are scenarios where retrieving visual information is either more beneficial or easier to access than textual data. In this paper, we introduce a multimodal retrieval-augmented generation benchmark, MRAG-BENCH, in which we systematically identify and categorize scenarios where visually aug-mented knowledge is better than textual knowledge, for instance, more images from varying viewpoints. MRAG-BENCH consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios. With MRAG-BENCH, we conduct an evaluation of 10 open-source and 4 proprietary large vision-language models (LVLMs). Our results show that all LVLMs exhibit greater improvements when augmented with images compared to textual knowledge, confirming that MRAG-BENCH is vision-centric. Additionally, we conduct extensive analysis with MRAG-BENCH, which offers valuable insights into retrieval-augmented LVLMs. Notably, the top-performing model, GPT-40, faces challenges in effectively leveraging retrieved knowledge, achieving only a 5.82% improvement with ground-truth information, in contrast to a 33.16% improvement observed in human participants. These findings highlight the importance of MRAG-BENCH in encouraging the community to enhance LVLMs' ability to utilize retrieved visual knowledge more effectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval-augmented generation (RAG) has emerged as a promising direction in large vision-language models (LVLMs) (OpenAI, 2023; Liu et al., 2023a; Bai et al., 2023; Huang et al., 2023; Chen et al., 2023a; Hu et al., 2024b; Chen et al., 2024; Tong et al., 2024; McKinzie et al., 2024). By incorporating external knowledge during generation, models such as Wiki-LLaVA (Caffagni et al., 2024) have demonstrated improved performance in knowledge-intensive question answering tasks.\nThere are several existing benchmarks evaluating retrieval-augmented LVLMs. For example, OK-VQA (Marino et al., 2019) focused on scenarios where the image content alone is insufficient to answer the questions. A-OKVQA (Schwenk et al., 2022) further extended this dataset to incorporate additional types of world knowledge. More recent works (Chang et al., 2022; Chen et al., 2023b; Mensink et al., 2023) further expanded and curated large-scale knowledge base data to evaluate pre-trained vision and language models in knowledge-intensive and information-seeking visual questions. However, as shown in Table 1, these benchmarks remain text-centric, as their questions can often be resolved with related external textual knowledge. In contrast, retrieving visual information is sometimes more beneficial than retrieving text, as humans often gain greater insights from it. Specifically, we illustrate examples in Figure 1 where retrieving correct textual knowledge can be hard and retrieved textual knowledge can be useless, while retrieving additional images is helpful. For instance, when presented with a top-down view of a car, humans may struggle to accurately identify it; however, with a front-facing view, they can quickly recognize the vehicle and effectively leverage the visual information.\nIn this paper, we introduce MRAG-BENCH, a benchmark specifically designed for vision-centric evaluation for retrieval-augmented multimodal models, with visual questions typically benefit more from retrieving visual knowledge than textual information. MRAG-BENCH consists of 16,130 images and 1,353 human-annotated multi-choice questions spanning 9 distinctive scenarios. Focusing on utilizing visually augmented knowledge in real-world scenarios, we divide our benchmark into two aspects: perspective, where changes in visual entity's perspective requiring visually augmented knowledge; and transformative, where the visual entity undergoes transformative change physically thus requiring visually augmented knowledge. Specifically, MRAG-BENCH requires models to reason about visual entities that undergo perspective changes, such as angle, partial, scope and occlusion, as well as transformative changes, such as temporal, incomplete, biological and deformations. Additionally, MRAG-BENCH includes 9,673 human-selected images, which serves as the ground-truth image knowledge corpus for model evaluation.\nWe conduct extensive experiments on MRAG-BENCH to evaluate 10 open-source and 4 proprietary LVLMs. The results confirm that MRAG-BENCH is vision-centric, as all LVLMs show greater improvements when augmented with images compared to textual knowledge. Our results indicate that the best-performing GPT-40 model only achieve 68.68% and 74.5% of accuracy without RAG knowledge and with ground-truth (GT) RAG knowledge, respectively. This substantially outperforms the best open-source model LLaVA-OneVision by 15.39% and 15.52%, respectively."}, {"title": "2 MRAG-BENCH", "content": "2.1 BENCHMARK OVERVIEW\nOur benchmark is designed for systematic evaluation of LVLM's vision-centric multimodal RAG abilities. To achieve this, we focus on evaluating the model's understanding of image objects that are not commonly associated with its knowledge base, while the collected ground-truth images can help incentivize specific visual concepts within LVLMs' memory. Therefore, we divide our benchmark into two main aspects, as illustrated in the examples in Figure 1:\n\u2022 perspective, refers to the challenges in visual recognition and reasoning that arise when a visual entity is presented from varying viewpoints, scopes, or levels of visibility.\n\u2022 transformative, refers to the challenges that arise when a visual entity undergoes fine-grained physical transformations, making it unfamiliar or not easily associated with the model's prior knowledge.\nMRAG-BENCH consists of 16,130 images and 1,353 multiple choice questions, with key statistics shown in Table 2. MRAG-BENCH adheres to the following design principles: (1) it focuses on real-world scenarios where visually augmented information is useful; (2) it incorporates 9 diverse multimodal RAG scenarios covering various types of image objects; (3) it features cleaned ground-truth images for each question that align with human knowledge; and (4) it provides robust evaluation settings for deterministic evaluations. Unlike previous works focus on retrieving textual knowledge, evaluation on MRAG-BENCH focuses on retrieving vision-centric knowledge, which can be formulated as follows: Given a query tuple Q composed of (query image, textual question), the multimodal retriever R returns a set of relevant images I ([i1, i2, ..., iv]), then the LVLM M take the input (Q, I) and output the final answer."}, {"title": "2.2 BENCHMARK COMPOSITION", "content": "MRAG-BENCH provides a systematic evaluation across 9 distinctive multimodal RAG scenarios, with four scenarios focused on the perspective understanding of visual entities, four on transfor-mative understanding, and one categorized as \u201cothers\u201d. As illustrated in Figure 2, each scenario comprises 7.5% to 23.8% of the whole benchmark. The selected examples of each scenario is shown in Figure 3. The details of each scenario are introduced as follows.\nPerspective understanding aspect. First, we have perspective aspect comprising [ANGLE], [PARTIAL], [SCOPE], and [OCCLUSION] dimensions.\n\u2022 [ANGLE] evaluates the ability of models to utilize visual knowledge of common shooting angles to identify and reason about less common, long-tailed viewpoints of visual entities.\n\u2022 [PARTIAL] evaluates the ability of models to use complete appearance knowledge to identify and reason when only a partial image of the visual entities is available.\n\u2022 [SCOPE] evaluates the ability of models to leverage high-resolution, detailed images for identifying and reasoning about visual entities in longer-scoped, low-resolution images.\n\u2022 [OCCLUSION] evaluates the ability of models to use ground-truth image knowledge to identify and reason when visual entities are occluded or partially hidden in natural scenes."}, {"title": "Transformative understanding aspect.", "content": "On the other hand, the transformative understanding scenarios cover [TEMPORAL], [DEFORMATION], [INCOMPLETE], and [BIOLOGICAL] dimensions.\n\u2022 [TEMPORAL] evaluates the ability of models to use familiar image knowledge to identify and reason about visual entities undergoing temporal changes that may not be represented in the model's knowledge base.\n\u2022 [DEFORMATION] evaluates the ability of models to use intact physical appearance knowledge to identify and reason when visual entities undergo deformation not captured in the model's knowledge base.\n\u2022 [INCOMPLETE] evaluates the ability of models to compare and contrast the complete layout and structure of image knowledge to identify and reason about missing parts and the correct layout of visual entities.\n\u2022 [BIOLOGICAL] evaluates the ability of models to utilize image knowledge after biological transformations of the visual entities.\n[OTHERS] aims to evaluate the ability of models to leverage geographic image knowledge to accurately identify and reason about the correct regions of origin for the visual entities of interest. All these scenarios work in tandem to comprehensively evaluate LVLMs' abilities of leveraging visually augmented knowledge."}, {"title": "2.3 DATA COLLECTION", "content": "As the guidelines discussed in \u00a7 2.1, our benchmark collection involves a clean ground-truth image corpus that can resonate with model's internal knowledge and a query question and image that challenge model's memory according to our definition of 9 diverse scenarios. To collect a dataset for systematic evaluation of vision-centric multimodal RAG scenarios, we manually annotate all multiple-choice question answering (MCQA) data while sourcing images from either publicly available datasets or manually scraping them from the web.\nCollection of perspective aspect. To collect diverse image objects and knowledge that are not extensively represented in LVLMs' memories (Zhang et al., 2024c), we considered three sources of data, ImageNet (Russakovsky et al., 2015), Oxford Flowers102 (Nilsback & Zisserman, 2008), and StanfordCars (Krause et al., 2013). To construct a high quality image corpus, for each of the image class that we included in our benchmark, we examined the validation set and excluded the unqualified images which can't provide sufficient visual information for the recognition of this class. Among the selected corpus, we further humanly picked five representative examples covering the diverse aspects of each class object, as the five ground-truth examples in our experimental results (See \u00a73). For constructing the query images, we adhered to our scenario definitions and manually selected qualified images for the [ANGLE], [SCOPE], and [OCCLUSION] scenarios. For the [PARTIAL] scenario, we randomly cropped images by 50% in both height and width. Then we performed another human inspection to ensure the quality of the cropped images, filtering out examples where the visual object did not occupy the dominant area of the image. We repeated the random cropping process until satisfactory images were obtained, filtering to 20.4 GT images per question on average.\nCollection of transformative aspect. We chose to manually scrape images from the web based on the definitions of the transformative aspect. To construct the image corpus, we employed Bing Image Search for each of the image object keyword predefined by us, please refer to Appendix A.1 for more details. We filtered out image objects that did not form a clear transformative pair between the query image and the ground-truth image, retaining approximately 74% of the keyword names in the process. For ground-truth image examples, we employed automatic scripts to download the top 15 images related to its keyword names and human filtered out the unqualified image. On average, this results to 5.9 images per question and the five ground-truth images used during our evaluation are manually selected same as in perspective aspect.\nAccording to our guidelines, additional related image object knowledge from the same geographic region can assist in identifying that region more effectively. For the [OTHERS] scenario, we source the data from the GeoDE dataset (Ramaswamy et al., 2023). For each distinct image object category,"}, {"title": "3 EXPERIMENTS", "content": "In this section, we first introduce the experimental setup and evaluation metric (\u00a7 3.1). Then, we present a comprehensive evaluation of 14 recent LVLMs (\u00a7 3.2). We demonstrate the importance of visual knowledge and discuss the critical findings revealed by the results from MRAG-BENCH."}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "We evaluate 14 popular LVLMs on MRAG-BENCH, including 4 proprietary models and 10 open-sourced models that can accept multi-image inputs:\n\u2022 Proprietary models: GPT-40 (0513) (OpenAI, 2023), GPT-4-Turbo (OpenAI, 2023), Gemini Pro (Team et al., 2023), and Claude 3.5 Sonnet (Anthropic, 2024).\n\u2022 Open-source models: OpenFlamingo (v2-9B) (Awadalla et al., 2023), Idefics (v2-8B) (Lauren\u00e7on et al., 2024), VILA (v1.5-13B) (Lin et al., 2023), LLaVA-NeXT-Interleave-7B (Li et al., 2024b), LLaVA-OneVision (Li et al., 2024a), Mantis (clip-llama3, and siglip-llama3 versions; 8B) (Jiang et al., 2024a), mPLUG-Owl3-7B (Ye et al., 2024), Deepseek-VL-7B-chat (Lu et al., 2024a), and Pixtral-12B (Team, 2024).\nEvaluation setup. We follow standard MCQA evaluation setup and employ accuracy score as our metric. We adopt default generation hyper-parameters selected by each model. Following Lu et al. (2024b), we employ GPT-3.5-turbo to extract the multiple choice answer in rare cases where our pre-defined automatic extraction rules failed. We refer the readers to Appendix A.1 and B for more details on evaluation prompts for both without multimodal RAG and with multimodal RAG scenarios, answer extraction prompt and human performance evaluation protocol."}, {"title": "3.2 MAIN RESULTS", "content": "As shown in Table 3, the average performance of the most advanced LVLMs is not better than 68.68% without multimodal RAG knowlege, and 74.5% with ground-truth knowledge, which demonstrates MRAG-BENCH to be a challenging benchmark. The mean accuracies of open-source LVLMs are between 26.83% and 53.29% without RAG knowledge and between 28.90% and 59.28% with ground-truth knowledge, which fall behind from advanced proprietary LVLMs. Notably, MRAG-BENCH proves to be knowledge-intensive as average humans achieved 38.47% without RAG knowledge, while proprietary LVLMs generally perform well, suggesting that their extensive training data equips them with a broader knowledge base. However, when provided with either retrieved or ground-truth knowledge, humans achieve the most significant improvements of 22.91% and 33.16%, respectively. This underscore the need of LVLMs to better utilize visually augmented information like humans.\nCan LVLMs utilize retrieved and ground-truth image knowledge well? As illustrated in Table 3, all models demonstrate improvement when ground-truth image RAG knowledge is provided. Among the open-source models, they achieve improvements ranging from 2.07% to 11.31% when using ground-truth RAG knowledge, whereas 5.64% to 9.69% improvements are observed from proprietary LVLMs. Interestingly, when images from the multimodal retriever is provided, almost all open-source LVLMs on average demonstrate a declined performance while proprietary models can still gain improvement. This indicate proprietary models possess emerging abilities to distinguish between good and bad image knowledge sources, which is a critical skill in the multimodal RAG domain. We further conducted a qualitative analysis to investigate the reasons behind this, as detailed in the following paragraphs.\nFine-grained results. We also report fine-grained scores across 9 scenarios on MRAG-BENCH in Table 3. Remarkably, GPT-40 surpasses most other baselines in various categories, with exceptions in problems related to partial, incomplete and biological scenarios. Notably, GPT-40 outperforms human performance on all perspective aspect as well as on temporal and deformation scenarios within the transformative aspect. We conjecture that incomplete and biological scenarios are less likely to be included in the training knowledge. Interestingly, all models exhibit a decline in performance on incomplete scenarios, with only a few exceptions, while humans find this"}, {"title": "4 ANALYSIS", "content": "In this section, we conduct quantitative analysis addressing three important questions: 1) To what extent can LVLMs benefit more from visual knowledge than from textual knowledge on MRAG-BENCH? (\u00a7 4.1) 2) How does the performance of LVLMs vary with examples retrieved from different retrievers? (\u00a7 4.2) 3) How many ground-truth visual knowledge examples are required for LVLMs to continue benefiting? (\u00a7 4.3)"}, {"title": "4.1 How MUCH CAN VISUAL KNOWLEDGE BENEFIT MORE THAN TEXTUAL KNOWLEDGE?", "content": "We used the Wikipedia corpus as of 2023/07/01 as our text knowledge corpus\u00b9. To ensure a fair comparison, we employed the same multimodal retriever (CLIP) for retrieving either text or image knowledge. The top-5 ranked documents or images are used for augmenting the input. We selected one open-source (LLaVA-Next-Interleave) and one proprietary (GPT-4-Turbo) LVLM to examine their preference for textual knowledge versus image knowledge on MRAG-BENCH. As shown"}, {"title": "4.2 HOW DOES RETRIEVER PERFORMANCE AFFECT LVLMS?", "content": "We picked four recent best-performing multimodal retrievers, including CLIP (Radford et al., 2021), MagicLens (Zhang et al., 2024a), E5-V (Jiang et al., 2024b), VISTA (Zhou et al., 2024) and evaluated their performance (Recall@5). The detailed retriever performance can be found at Table 6 in Appendix C. We selected LLaVA-Next-Interleave as the end model to assess its performance. As shown in Figure 5, when retrievers achieve higher Recall@5 scores (i.e., better retrieved examples), the LVLM's accuracy tends to improve, demonstrating a strong 95% positive correlation. Interestingly, despite similar Recall@5 scores from CLIP and VISTA retrievers, LLaVA-Next-Interleave demonstrated a 2.07% gap in overall accuracy. We conjecture that the order of the correctly retrieved examples may also impact the model's final performance. The sensitivity to the order of retrieved examples is a common issue that persists across various models. Although this phenomenon, known as position bias, has been examined in text-based RAG (Lu et al., 2022b; Wang et al., 2023), its impact on visual RAG remains unexplored, presenting a promising direction for future research."}, {"title": "4.3 How MANY GROUND-TRUTH IMAGE EXAMPLES ARE NEEDED?", "content": "For simplicity, all our experiments used five retrieved or ground-truth image examples. However, it is worth exploring how many examples LVLMs can effectively leverage. As noted in \u00a7 2.3, the perspective aspect of our benchmark includes an average of 20.4 ground-truth examples. To investigate further, we perform an analysis focusing on the perspective and others aspects, covering a total of 892 questions. As shown in Figure 5, we evaluated LLaVA-Next-Interleave using 1, 2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64% with just one GT example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which was 0.29% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-Interleave may not able to better leverage visually augmented knowledge in long context scenarios. Moreover, the complexity of questions affects the number of images needed too, one ground-truth example sometimes help the model the most on MRAG-BENCH. We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions."}, {"title": "5 RELATED WORK", "content": "We overview three lines of related work: 1) multimodal retrieval-augmented generation benchmarks (\u00a7 5.1), 2) large vision language models (\u00a7 5.2), and 3) retrieval-augmented mutlimodal large language models (\u00a7 5.3)."}, {"title": "5.1 MULTIMODAL RETRIEVAL-AUGMENTED GENERATION BENCHMARKS", "content": "A number of recent benchmarks have been developed to comprehensively assess the capabilities of LVLMs (Lu et al., 2021; 2022a; Li et al., 2023; Liu et al., 2023c; Lu et al., 2024b; Yue et al., 2023; Zhang et al., 2024b; Ying et al., 2024). There are several benchmarks well-suited for evaluating retrieval-augmented LVLMs. For instance, OK-VQA (Marino et al., 2019) and A-OKVQA (Schwenk et al., 2022) both focus on scenarios where external textual knowledge is required to answer visual questions. More recent works (Chang et al., 2022; Chen et al., 2023b; Mensink et al., 2023) have curated large-scale knowledge bases to evaluate models on knowledge-intensive and information-seeking visual questions. In contrast, MRAG-BENCH focus on scenarios where retrieving visual information is more helpful than retrieving text."}, {"title": "5.2 LARGE VISION LANGUAGE MODELS", "content": "Large Vision Language Models (LVLMs) (Liu et al., 2023b; Zhu et al., 2023a; Dai et al., 2023; Yin et al., 2023; Hu et al., 2024a) have showcased promising results on a wide variety of vision-language tasks. Many works, such as Flamingo (Alayrac et al., 2022), Emu (Sun et al., 2023), Idefics (Lauren\u00e7on et al., 2023), and VILA (Lin et al., 2023), have demonstrated in-context learning capabilities, where multiple image examples can be leveraged to improve text generation. Recent works start training LVLMs with interleaved image-text corpora, such as MMC4 (Zhu et al., 2023b) and OBELICS (Lauren\u00e7on et al., 2023), for pretraining, as well as high-quality instruction tuning in models like Mantis-Instruct (Jiang et al., 2024a), LLaVA-Next-Interleave (Li et al., 2024b), and LLaVA-OneVision (Li et al., 2024a), enabling models to process and understand information from multiple images. Naturally, evaluating the ability of LVLMs to effectively leverage visually augmented knowledge becomes an important task, which is the primary focus of MRAG-BENCH."}, {"title": "5.3 RETRIEVAL-AUGMENTED MULTIMODAL LARGE LANGUAGE MODELS", "content": "Retrieval-augmented generation (RAG) has emerged as a potential solution to overcome limitations in language models by incorporating external knowledge retrieval during the generation process (Lewis et al., 2020; Shi et al., 2024). Reasonably, several works have focused on using multimodal knowledge to enhance the generation capabilities of Large Language Models (LLMs) (Yasunaga et al., 2023; Chen et al., 2022; Zhao et al., 2023; Cui et al., 2024). Recently, more works (Caffagni et al., 2024; Xuan et al., 2024; Du et al., 2024) has incorporated external knowledge to improve LVLMs' general generation abilities and the comprehensiveness of their reasoning. Although some works (Chen et al., 2022; Yuan et al., 2023) have proposed directly using image information from the web, a systematic vision-centric benchmark to evaluate LVLMs' abilities to leverage visually augmented knowledge is lacking, which is the focus of our work."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce MRAG-BENCH, a benchmark specifically designed for vision-centric evaluation for retrieval-augmented multimodal models. Our evaluation of 14 LVLMs highlights that visually augmented knowledge brings more improvements on MRAG-BENCH compared to textual knowledge. Moreover, the top-performing model, GPT-40, struggles to effectively utilize the retrieved knowledge, achieving only a 5.82% improvement when augmented with relevant information, compared to a 33.16% improvement demonstrated by human participants. We further conduct extensive analysis and propose several promising directions for future research. Our findings underscore the significance of MRAG-BENCH in motivating the community to develop LVLMs that better utilize retrieved visual knowledge."}, {"title": "A MRAG-BENCH DETAILS", "content": "A.1 DATASET CURATION DETAILS\nDataset collection of transformative aspect We chose to manually scrape images from the web based on the definitions of the transformative aspect. To construct the image corpus, we employed Bing Image Search for each of the image object keyword predefined by us. We filtered some of the search results where the image objects do not have a clear pair of query image and ground-truth image example, around 74% keyword names were kept during this process. Here we listed all the keywords that are already filtered and used for search of query image except in biological scenario, it's for search of ground-truth image example. Each search keyword is composed of an \"image"}, {"title": "Transformative: Temporal", "content": "- A young kitten image of Himalayan Cat\n- A young kitten image of Chartreux\n- A young kitten image of Burmese\n- A young kitten image of Turkish Van\n- A young kitten image of American Shorthair\n- A young kitten image of British Shorthair\n- A young kitten image of Maine Coon\n- A young kitten image of Burma (Myanmar)\n- A young kitten image of Selkirk Rex\n- A young kitten image of Siberian\n- A young kitten image of Persian\n- A young kitten image of Manx\n- A young kitten image of Ocicat\n- A young kitten image of Russian Blue\n- A young kitten image of Bengal Cat\n- A young kitten image of Devon Rex\n- A young kitten image of American Bobtail\n- A young kitten image of Balinese\n- A young kitten image of LaPerm\n- A young kitten image of Egyptian Mau\n- A young kitten image of Japanese Bobtail\n- A young kitten image of Ragdoll\n- A young kitten image of Abyssinian\n- A young kitten image of American Wirehair\n- A young kitten image of Oriental Shorthair\n- A young kitten image of Cornish Rex\n- A young kitten image of Kurilian Bobtail\n- A young kitten image of Singapura Cat\n- A young kitten image of Birman\n- A young kitten image of Burmilla\n- A young kitten image of Korat\n- A young kitten image of Tonkinese\n- A young kitten image of Somali Cat\n- A young kitten image of Norwegian Forest Cat\n- A young kitten image of Turkish Angora\n- A young kitten image of Siamese\n- A picture of Sainte-Chapelle under construction\n- A picture of Washington Monument under construction\n- A picture of Hearst Castle under construction\n- A picture of Time Square under construction\n- A picture of Wrigley Building under construction\n- A picture of Eiffel Tower under construction\n- A picture of The Arc de Triomphe under construction\n- A picture of Golden Gate Bridge under construction\n- A picture of White House under construction\n- A picture of Palace of Versailles under construction\n- A picture of Op\u00e9ra Garnier under construction\n- A picture of San Simeon under construction\n- A picture of The Louvre under construction\n- A picture of Cath\u00e9drale Notre-Dame de Paris under construction\n- A picture of Sacr\u00e9-C\u0153ur Basilica under construction"}, {"title": "Transformative: Deformation", "content": "- An image of Toyota Camry damaged\n- An image of Ford F-150 damaged\n- An image of Ferrari 458 damaged\n- An image of Audi Q5 damaged\n- An image of Lamborghini LP640 damaged\n- An image of McLaren 675LT damaged\n- An image of Mercedes SLC damaged\n- An image of Lamborghini Aventador damaged\n- An image of Lamborghini LP570 damaged\n- An image of Porsche 911 GT3 RS damaged\n- An image of Audi A6 damaged\n- An image of Audi A4 damaged\n- An image of Lamborghini Aventador SV damaged\n- An image of GMC Sierra 2500 HD damaged\n- An image of Infiniti G37 damaged\n- An image of GMC Yukon damaged\n- An image of Honda Accord damaged\n- An image of Infiniti FX35 damaged\n- An image of Tesla Model 3 damaged\n- An image of Acura RDX 2020 damaged\n- An image of BMW 7 Series damaged\n- An image of Audi A5 Sportback damaged\n- An image of Hyundai IX35 damaged\n- An image of Cadillac XTS damaged\n- An image of BMW M3 damaged\n- An image of Acura MDX damaged\n- An image of Audi A3 damaged\n- An image of BMW X3 damaged\n- An image of Porsche Boxster damaged\n- An image of Mercedes CLA45 AMG damaged\n- An image of Jaguar XJ damaged"}, {"title": "Transformative: Incomplete", "content": "- MacBook Keyboard missing keys\n- Windows Keyboard missing keys\n- Laptop Keyboards (Generic) missing keys\n- Mechanical Keyboard missing keys\n- Ergonomic Keyboard missing keys\n- Compact Keyboard missing keys\n- Gaming Keyboard missing keys\n- Chiclet Keyboard missing keys\n- Tenkeyless (TKL) Keyboard missing keys\n- Virtual Keyboard (On-screen) missing keys\n- Numeric Keypad missing keys\n- ISO Keyboard Layout missing keys\n- ANSI Keyboard Layout missing keys\n- Ortholinear Keyboard missing keys\n- Bluetooth/Wireless Keyboard missing keys"}, {"title": "Transformative: Biological", "content": "- An image of Lime after oxidation\n- An image of breadfruit after oxidation\n- An image of dragonfruit after oxidation\n- An image of starfruit after oxidation\n- An image of Raspberry after oxidation\n- An image of Zucchini after oxidation\n- An image of Pear after oxidation\n- An image of passionfruit after oxidation\n- An image of Blackberry after oxidation\n- An image of durian after oxidation\n- An image of persimmon after oxidation\n- An image of Apple after oxidation\n- An image of bell pepper after oxidation\n- An image of olive after oxidation\n- An image of Mango after oxidation\n- An image of nectarine after oxidation\n- An image of tomato after oxidation\n- An image of quince after oxidation\n- An image of coconut after oxidation\n- An image of soursop after oxidation\n- An image of Kiwi after oxidation\n- An image of cucumber after oxidation\n- An image of apricot after oxidation\n- An image of Honeydew after oxidation\n- An image of Peach after oxidation\n- An image of pomegranate after oxidation\n- An image of carrot after oxidation\n- An image of fig after oxidation\n- An image of Papaya after oxidation\n- An image of Blueberry after oxidation\n- An image of Banana after oxidation\n- An image of jackfruit after oxidation\n- An image of Lemon after oxidation\n- An image of tamarind after oxidation\n- An image of lychee after oxidation\n- An image of Pineapple after oxidation\n- An image of Cantaloupe after oxidation\n- An image of Orange after oxidation\n- An image of Rambutan after oxidation\n- An image of guava after oxidation\n- An image of sweet potato after oxidation\n- An image of Plum after oxidation\n- An image of Avocado after oxidation\n- An image of Watermelon after oxidation\n- An image of potato after oxidation\n- An image of Grapefruit after oxidation\n- An image of Grapes after oxidation\n- An image of pumpkin after oxidation\n- An image of Cherry after oxidation\n- An image of Strawberry after oxidation\n- An image of custard apple after oxidation"}, {"title": "Quality control", "content": "We employ two types of quality control throughout the annotation process: an automatic check with predefined rules and a manual examination of each instance. The automatic check verifies correct MCQA format in which each question should only have one correct answer, metadata values, assesses image validity (checking the accessibility of each image) and filters out redundant images in the corpus (images that are repetitively downloaded). The manual examination"}, {"title": "A.2 HUMAN EVALUATION PROTOCOL", "content": "Three human annotators in domain conducted the human evaluation. The interface for human evaluation without RAG knowledge and with RAG knowledge are shown in Figure 6 and Figure 7."}, {"title": "B EXPERIMENT SETTING DETAILS", "content": "B.1 MODEL PROMPTS\nFollowing Lu et al. (2024b) and Liu et al. (2023a) our prompt consists of four parts, the instruction, question, options, and a prefix of the answer. For images, we insert them into the text to form a coherent prompt as the image placeholder ({Image}) indicated below. The complete prompt is as follows:"}, {"title": "Model Prompts for No RAG Evaluation", "content": "Instruction: Answer with the option's letter from the given choices directly.\n{Image}\nQuestion: {QUESTION}\nChoices:\n(A) {OPTION_A}\n(B) {OPTION_B}\n(C) {OPTION_C}\n(D) {OPTION_D}\nAnswer:"}, {"title": "Model Prompts for RAG Evaluation", "content": "Instruction: You will be given one question concerning several images. The first image is the input image, others are retrieved examples to help you. Answer with the option's letter from the given choices directly.\n{Image}{Image}{Image}{Image}{Image}{Image}\nQuestion: {QUESTION}\nChoices:\n(A) {OPTION_A}\n(B) {OPTION_B}\n(C) {OPTION_C}\n(D) {OPTION_D}\nAnswer:"}, {"title": "B.2 EVALUATION TOOL", "content": "Following Lu et al. ("}]}