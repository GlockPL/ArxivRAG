{"title": "A Pair Programming Framework for Code Generation via\nMulti-Plan Exploration and Feedback-Driven Refinement", "authors": ["Huan Zhang", "Wei Cheng", "Yuhan Wu", "Wei Hu"], "abstract": "Large language models (LLMs) have achieved impressive perfor-\nmance on code generation. Although prior studies enhanced LLMs\nwith prompting techniques and code refinement, they still struggle\nwith complex programming problems due to rigid solution plans.\nIn this paper, we draw on pair programming practices to propose\nPAIRCODER, a novel LLM-based framework for code generation.\nPAIRCODER incorporates two collaborative LLM agents, namely a\nNAVIGATOR agent for high-level planning and a DRIVER agent for\nspecific implementation. The NAVIGATOR is responsible for propos-\ning promising solution plans, selecting the current optimal plan,\nand directing the next iteration round based on execution feed-\nback. The DRIVER follows the guidance of NAVIGATOR to undertake\ninitial code generation, code testing, and refinement. This inter-\nleaved and iterative workflow involves multi-plan exploration and\nfeedback-based refinement, which mimics the collaboration of pair\nprogrammers. We evaluate PAIRCODER with both open-source and\nclosed-source LLMs on various code generation benchmarks. Ex-\ntensive experimental results demonstrate the superior accuracy of\nPAIRCODER, achieving relative pass@1 improvements of 12.00%-\n162.43% compared to prompting LLMs directly.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation aims to automatically generate executable source\ncode that conforms to given requirements, typically expressed in\nnatural language. Recent progress in large language models (LLMs)\nhas significantly improved software development productivity by\nreducing repetitive programming efforts [23, 35]. The success of\ncommercial models like ChatGPT [33] and Claude [1], along with\npowerful open-source models like Code Llama [40] and DeepSeek\nCoder [14], has attracted substantial interest from both academia\nand industry. These advancements demonstrate the remarkable\ncapabilities of LLMs in code generation and have great potential to\ninfluence the field of intelligent software engineering [9, 13, 48].\nAs requirements become more complex, it becomes challenging\nfor LLMs (and even humans) to directly generate code that meets\nthe given requirements [10]. One key focus of existing work is\nprompting techniques, which guide LLMs to produce intermediate\nreasoning steps for problem descriptions. This line of work [22, 25,\n46] focuses on designing different types of prompts to stimulate the\nreasoning abilities of LLMs, enabling them to generate intermediate\nsteps before producing the final code. Another important aspect\nis that generating the correct code is rarely a one-time effort [7].\nSeveral studies [5, 27, 31] employ sampling-based approaches to\nfilter or rank the numerous responses generated by LLMs, relying\non a substantial number of samples. Other works [7, 32, 45] attempt\nto refine the generated code using feedback from LLMs themselves\nor external sources. They introduce a debugging process to make\nthe generated program behave as expected. Furthermore, a few\nworks [10, 16, 37] explore the use of collaborative LLM agents to\nsimulate human software development processes.\nAlthough existing approaches have improved code generation,\nthey still have significant limitations. First, most approaches focus"}, {"title": "2 MOTIVATING EXAMPLE", "content": "Fig. 1 shows a programming problem from the CodeContest bench-\nmark [27]: given an integer sequence $a_1, a_2,..., a_n$, find the mini-\nmum number of insertions to ensure $\\forall 1 \\leq i \\leq n$, $a_i \\leq i$.\nThe prompting approaches [22, 25, 46] generate intermediate\nreasoning steps based on the problem description to guide code\ngeneration, as excerpted in Plan A. Considering an input sequence\n\"1, 3, 4\" in public test cases, it is evident that a single operation is\nsufficient to satisfy the requirement, i.e., inserting '2' between '1'\nand '3'. Guided by Plan A, the first code snippet on the left side of\nFig. 1 incorrectly assumes that two insertions are required. This\nfeedback from code testing can help LLMs repair the generated code\n[7, 32, 45]. However, iterative repairs do not yield the right answers\nas they stubbornly follow the flawed Plan A, which incorrectly\naccumulates operations. For example, LLMs would mistake the\nbug as an incorrect calculation of the difference between $a_i$ and\n$(i + 1)$, or that the sequence has not been updated. We believe this\ndilemma arises from an inherent pitfall of the single-path approach:\nonce an incorrect blueprint is initially established, LLMs struggle\nto identify the root cause of the error and thus mislead subsequent\nrepairs.\nIn contrast, human programmers do not put all eggs in one\nbasket. If the current plan is deemed ineffective, they will explore\nalternative plans. Plan B correctly solves the problem by tracking"}, {"title": "3 FRAMEWORK", "content": "We first formulate the realistic problem of code generation as gener-\nating a program C from a natural language description Q and a set\nof public (visible) test cases $T_v = \\{(I_i, O_i)\\}_{i=1}^{m_v}$, where $O_i$ denotes\nthe desired output for the input $I_i$. Based on the execution feed-\nback from $T_v$, LLMs can iteratively refine the generated program\nC, having a maximum number of iterations r to control cost and\nefficiency. Finally, C is considered correct if its behavior is consis-\ntent with the test oracle [17], which is usually represented by a\nset of private (hidden) test cases $T_h = \\{(I_i, O_i)\\}_{i=1}^{m_h}$, i.e., satisfying\nthat $\\forall (I_i, O_i) \\in T_h, C(I_i) = O_i$. Note that the accessible $T_v$ is not a\ncomplete test, while $T_h$ is invisible during the code generation and\nrefinement stages.\nFig. 2 illustrates the workflow of PAIRCODER. Both the NAVI-\nGATOR and DRIVER agents are powered by an LLM with general-\npurpose capabilities, such as GPT-3.5-Turbo [33]. The NAVIGATOR\nguides the DRIVER by generating solution plans and repair strate-\ngies. Therefore, the DRIVER focuses all its attention on specific code\ntasks, including code generation, code testing, and refinement. The"}, {"title": "3.2 NAVIGATOR Agent", "content": "The NAVIGATOR agent serves as the main controller in deeply un-\nderstanding the problem and providing strategic direction. Its role\nis to propose multiple promising plans (Step 1 in Fig. 2), select the\ncurrently best solution plan (Step 2), and direct the next iteration\nbased on execution feedback and historical memory (Step 5).\nPropose promising plans. The NAVIGATOR first reflects on the\ngiven natural language description Q (Line 1 in Algorithm 1). The\nprompt for LLMs is shown in Fig. 3. It stimulates LLMs to explicitly\nanalyze the details of the problem, consider possible valid inputs\nand edge cases, and explain public test cases. This reflection process\nenables the NAVIGATOR to gain a comprehensive understanding of\nthe core logic, constraints, and requirements for effective problem\nsolving.\nWith the comprehensive reflection on the problem, the NAVI-\nGATOR further comes up with specific solution plans (Line 2). As\nshown in Fig. 4, we include brief examples in prompts to guide the\nproposal and emphasize the functional correctness of the proposed\nplans. Each plan outlines a high-level solution and key implemen-\ntation steps in concise natural language. To obtain diverse solution\nplans, we set a non-zero temperature for multiple nucleus sam-\npling [15] and ask LLMs to generate multiple plans in each batch\n[8], which improves sampling efficiency while reducing duplicate\nplans. After brainstorming through multiple sampling, we select\nk representative plans as candidates. Specifically, we first divide"}, {"title": "3.3 DRIVER Agent", "content": "In contrast to the high-level planning of the NAVIGATOR, the DRIVER\nagent focuses all its attention on specific code tasks, including\ngenerating initial code guided by a new plan (Step 3), testing code\non public test cases (Step 4), and repairing the buggy code (Step 6).\nGenerate initial code. Once a new solution plan is selected, the\nDRIVER first generates an initial code implementation guided by the"}, {"title": "Complexity analysis.", "content": "The time complexity of Algorithm 1 is de-\ntermined by the number of iterations r and the cost of operations\nwithin each iteration. Let c denote the constant factor representing\nthe cost of operations, such as model inference and code testing\nwithin each iteration. Then, the overall time complexity of PAIR-\nCODER is O(r x c). Similarly, the space complexity of PAIRCODER is\nO(r), since the NAVIGATOR needs to store the historical memory\n$H_c$ and $H_f$, which grow linearly with the iteration count. While\nthe multi-plan exploration and iterative refinement introduce addi-\ntional computational overhead, the superior accuracy of PAIRCODER\nin code generation justifies the trade-off in Sect. 4."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "We evaluate PAIRCODER by defining the following research ques-\ntions (RQs) and outlining how we propose to answer them:\n\u2022 RQ1. How does the accuracy of PAIRCODER in code\ngeneration compare to other approaches? We aim to\nevaluate the effectiveness of our PAIRCODER framework in\ncode generation compared to existing approaches. We con-\nduct comprehensive experiments across diverse benchmarks\nand foundation models.\n\u2022 RQ2. How do critical hyperparameters impact the accu-\nracy of PAIRCODER? We thoroughly investigate the effect\nof iteration count on PAIRCODER compared to other iterative\nrefinement-based approaches, as well as the impact of cluster\nnumber on PAIRCODER.\n\u2022 RQ3. What are the individual contributions of the ma-\njor components in PAIRCODER? We aim to analyze the\neffectiveness of two major components in PAIRCODER: multi-\nplan exploration and feedback-driven refinement facilitated\nby NAVIGATOR-DRIVER collaboration. By disabling each com-\nponent in ablation studies, we isolate their effects and vali-\ndate their contributions to the overall accuracy.\n\u2022 RQ4. What are the findings of cost and error analyses"}, {"title": "4.1 Experiment Settings", "content": "Benchmarks. Following the prior works [7, 45], we conduct com-\nprehensive experiments on five widely used benchmarks of code\ngeneration: HumanEval [6], HumanEval+ [28], MBPP [4], MBPP+\n[28], and CodeContest [27]. The statistics of these benchmarks are\nshown in Table 1. HumanEval, MBPP, and their extended versions\n(Plus) aim at simple function-level code generation, while Code-\nContest consists of competition-level programming problems. Both\nthe validation and test sets of CodeContest are considered.\nFurthermore, we put effort into providing public test cases $T_v$ for\nexecution feedback. For the benchmarks [6, 27, 28] where public\ntest cases are provided in problem descriptions, we extract $T_v$ using\nhand-written rules. For the benchmarks lacking public test cases in\nthe descriptions, we follow [7, 31, 54] by treating the first private\ncase as $T_v$ for MBPP, while for MBPP+, we use the original three\nprivate cases before extension as $T_v$.\nMetrics. In line with previous works [7, 10, 22, 30], we use the\ngreedy pass@1 [6, 49] to assess the functional correctness of the\ngenerated program. A program is regarded correct only if it passes\nall private test cases $T_h$. Compared to pass@K with multiple nucleus\nsampling, the greedy pass@1 represents a more realistic scenario,\nwhere developers are not required to review the correct one from\nmultiple solutions.\nComparative methods. We compare PAIRCODER with two main\ncategories of existing approaches for code generation. We briefly\ndescribe them as follows.\nPrompting techniques. This category focuses on prompts to\nsteer LLMs towards generating more accurate code solutions for\nrequirements. Notable approaches include:\n\u2022 Direct prompting [6] takes the original requirements di-\nrectly as inputs to prompt LLMs for code generation.\n\u2022 Chain-of-Thought (CoT) prompting [46] elicits LLMs to\ngenerate a chain of intermediate natural language reasoning\nsteps before producing the final code. We use the classical\nCoT instruction \"Let's think step by step.\" to guide LLMs in\nzero-shot [24].\n\u2022 SCoT prompting [25] asks LLMs using three basic program\nstructures (i.e., sequence, branch, and loop structures) to"}, {"title": "4.2 RQ1: Accuracy Comparison", "content": "The accuracy comparison results are presented in Table 2. Our\nPAIRCODER achieves the best pass@1 scores across all benchmarks\nand foundation models. In comparison to prompting LLMs directly,\nPAIRCODER shows significant relative improvement of 12.00% to\n162.43%. The prompting techniques would accumulate errors in\nintermediate thoughts and single code generation, causing rela-\ntively weak accuracy on code generation. CoT and SCoT prompting\nare even worse than direct generation in some settings, which is\nconsistent with the findings of prior works [20, 26]. The poor perfor-\nmance of Reflexion is likely due to the model generating incorrect\ntest cases, which leads to self-reflections based on false negative\nevaluations of the code [42]. In contrast, other refinement-based ap-\nproaches using provided public test cases achieve overall accuracy\ngains, since the reliable test feedback can guide the refinement in\nmore promising directions. However, they are confined to a single\nsolving path, lacking the flexibility to explore alternative solution\nplans when stuck. To overcome these limitations, PAIRCODER com-\nbines the advantages of multi-plan exploration and feedback-driven\nrefinement.\nAccuracy discrepancies across benchmarks are worth examining.\nMost approaches perform well on the relatively simple HumanEval\nand MBPP benchmarks. However, all approaches exhibit a substan-\ntial accuracy decrease on the challenging CodeContest benchmark.\nThis reflects that current code generation techniques still have\nroom for improvement in tackling complex programming problems.\nNote that the direct prompting with DeepSeek-Coder even outper-\nforms that with GPT-3.5-Turbo on HumanEval, HumanEval+, and\nCodeContest-test. We speculate this may be due to data leakage\nissues, which will be further analyzed in Sect. 4.6.\nWe further evaluate PAIRCODER with one of the most advanced\nLLMs, GPT-4 (gpt-4-0613) [34], and cite the results of several pow-\nerful approaches [10, 16, 42] from their original papers. The com-\nparison results are shown in Table 3. PAIRCODER still significantly\nimproves accuracy over direct prompting and outperforms existing\nmulti-agent approaches."}, {"title": "4.3 RQ2: Hyperparameter Impact", "content": "We investigate the impact of two critical hyperparameters: the\nmaximum number of iterations r and the number of clusters k."}, {"title": "4.4 RQ3: Ablation Study", "content": "To analyze the individual effectiveness of multi-plan exploration\nand feedback-driven refinement in PAIRCODER, we conduct ablation\nstudies in Table 4. \"w/o MP\" disables the capability of multi-plan\nexploration, making the NAVIGATOR always choose to repair the\ncurrent code rather than adjust the solution plan in Step 5. \"w/o\nRF\" is the opposite, which disables the feedback-driven refinement\nprocess, making the NAVIGATOR always choose to attempt another\ncandidate plan. The DRIVER's behavior changes according to differ-\nent directions of the NAVIGATOR.\nThe ablation results demonstrate that the complete PAIRCODER\nachieves the best accuracy, and both multi-plan exploration and\nfeedback-driven refinement play a positive role in code generation."}, {"title": "4.5 RQ4: Cost and Error Analyses", "content": "For this RQ, we further investigate the usage of PAIRCODER.\nCost analysis. We perform a cost analysis for PAIRCODER and\nall reproduced approaches on HumanEval, MBPP, and the test set\nof CodeContest. The cost is measured by two key metrics: the\naverage number of API calls per problem and the average token\nconsumption per problem. For each approach, we record its API\nrequests and responses using GPT-3.5-Turbo. This provides the\nnumber of API calls and token consumption, including input tokens\nand generated output tokens of LLMs. Note that we count API calls\nto assess the efficiency in code generation, since the time spent is\nsusceptible to uncontrollable factors such as network fluctuation.\nFor the fairness of comparison, we extend the comparative meth-\nods to conduct additional experiments using GPT-3.5-turbo: (i) For\nprompting techniques, we repeat sampling with a temperature of 0.8\nuntil the generated code passes all public test cases or 10 attempts\nare reached. (ii) For Self-repair, we also allow up to 10 iterations.\nThis setup ensures that these approaches have the same maximum\nnumber of attempts as the iterative approaches in Sect. 4.1. As\nshown in Table 5, the results demonstrate that simple repetitive\nsampling can indeed enhance these approaches, but PAIRCODER\nremains dominant. It confirms that the effectiveness of PAIRCODER\nbeyond merely increased computation."}, {"title": "4.6 Threats to Validity", "content": "The first potential threat relates to the generalizability of our eval-\nuation. To mitigate this concern, we carefully select five widely\nused and representative benchmarks and both closed-source and\nopen-source LLMs for our experiments. Under all these settings,\nconsistently superior accuracy demonstrates the effectiveness of\nPAIRCODER. In future work, we plan to further validate the gen-\neralizability of PAIRCODER across a broader range of LLMs and\nbenchmarks, such as multilingual code generation [3, 53].\nAnother potential threat is data leakage in pre-trained LLMs.\nFor example, DeepSeek-Coder was released after benchmarks like\nHumanEval were collected, raising the possibility that benchmark\nsamples were unintentionally included in its pre-training corpus.\nHowever, any such leakage would affect all baselines equally since\nthey use the same model. Therefore, while leakage may inflate\noverall accuracy, it does not affect the fairness of our comparative\nanalysis and the relative gains of PAIRCODER, which consistently\nshows the largest improvements across all settings.\nThe third potential threat arises from the zero-shot prompting\nused in our experiments. Although zero-shot prompting achieves\nsuperior accuracy while largely reducing token consumption, we\ndo not rule out the possibility that other instructions or few-shot\ndemonstrations could further improve performance. However, the\nselection of demonstrations for in-context learning poses a signifi-\ncant challenge, which can greatly influence the behavior of LLMs\n[12]. We leave the exploration of effective few-shot prompting\ntechniques in future work."}, {"title": "5 RELATED WORK", "content": "Recent advancements in LLMs have shown remarkable capabili-\nties in code generation tasks by training on vast amounts of code-\ncontaining corpora. Open-source models like InCoder [11], Code\nLlama [40], WizardCoder [29], and DeepSeek-Coder [14], have de-\npicted performance that matches or even surpasses closed-source\ncommercial models like ChatGPT [33], GPT-4 [34], and Claude [1].\nThis development has significantly improved software productivity\n[23, 35] and profoundly affected the progress of intelligent software\nengineering, attracting substantial work focused on enhancing the\ncode generation capabilities of LLMs.\nA key focus is on prompting techniques that guide LLMs to\nproduce intermediate reasoning steps from problem descriptions.\nPrompting techniques have been proven to effectively improve\nthe code generation performance of LLMs in a plug-and-play man-\nner [18, 22, 25, 26, 46]. SCOT [25] and Self-planning [22] design\ndifferent formats of intermediate steps, while BRAINSTORM [26]\ntrains a neural ranker model to select the best thought. They lever-\nage prompting techniques to stimulate the reasoning capabilities\nof LLMs, guiding them to generate more accurate code. However,\ngenerating correct code is rarely a one-time effort [7]. Some ap-\nproaches [5, 21, 27, 31, 41, 52] first generate multiple code solutions\nand then filter or rank them based on consistency or execution\nresults to obtain the final code. They require substantial computa-\ntional resources to generate code candidates, which is inefficient\nand orthogonal to our framework."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose the PAIRCODER framework, which is the\nfirst to adapt pair programming practices into LLM-based code gen-\neration. It comprises a NAVIGATOR agent for high-level planning\nand a DRIVER agent for specific implementation, collaborating on\ncode generation via multi-plan exploration and feedback-driven\nrefinement. The NAVIGATOR explores multiple plans based on execu-\ntion feedback from the DRIVER and historical memory. The DRIVER\nfollows the guidance of the NAVIGATOR to undertake initial code\ngeneration, code testing, and refinement. Extensive experiments\non diverse benchmarks and LLMs demonstrate the superior accu-\nracy of PAIRCODER. Our work represents a promising step towards\nleveraging collaborative agents to facilitate intelligent software\ndevelopment. In future work, we plan to integrate human feedback\nor external knowledge sources to further enhance the high-level\nplanning capabilities of the NAVIGATOR. We will also explore ap-\nplications of the PAIRCODER framework to other domains beyond\ncode generation."}]}