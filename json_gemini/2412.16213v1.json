{"title": "AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF Models", "authors": ["Tommy Nguyen", "Mehmet Ergezer", "Christian Green"], "abstract": "The increasing deployment of AI models in critical applications has exposed them to significant risks from adversarial attacks. While adversarial vulnerabilities in 2D vision models have been extensively studied, the threat landscape for 3D generative models, such as Neural Radiance Fields (NeRF), remains underexplored. This work introduces AdvIRL, a novel framework for crafting adversarial NeRF models using Instant Neural Graphics Primitives (Instant-NGP) and Reinforcement Learning. Unlike prior methods, AdvIRL generates adversarial noise that remains robust under diverse 3D transformations, including rotations and scaling, enabling effective black-box attacks in real-world scenarios. Our approach is validated across a wide range of scenes, from small objects (e.g., bananas) to large environments (e.g., lighthouses). Notably, targeted attacks achieved high-confidence misclassifications, such as labeling a banana as a slug and a truck as a cannon, demonstrating the practical risks posed by adversarial NeRFs. Beyond attacking, AdvIRL-generated adversarial models can serve as adversarial training data to enhance the robustness of vision systems. The implementation of AdvIRL is publicly available at https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean, ensuring reproducibility and facilitating future research.", "sections": [{"title": "Introduction", "content": "The widespread integration of artificial intelligence (AI) across fields such as autonomous driving and robotics (Li, Chen, and Shen 2019; Ravindran, Santora, and Jamali 2020; Zhang et al. 2020; Hu et al. 2019; Hossain, Capi, and Jindai 2016; Finn et al. 2015) has led to an increase in incidents where AI models misclassify objects (Kurakin, Goodfellow, and Bengio 2017). These errors can have severe consequences, including the loss of life, particularly in safety-critical systems like AI-driven vehicles. To mitigate such risks, research on adversarial AI has expanded, aiming to expose vulnerabilities in AI algorithms and strengthen their robustness against adversarial attacks (Xu et al. 2020; Behzadan and Munir 2017; Goodfellow, Shlens, and Szegedy 2015).\nWhile adversarial attacks on 2D vision models have been widely studied (Li, Lian, and Chen 2023; Huang et al. 2023; Madry et al. 2017), the transition to the physical world introduces new challenges. Many real-world attacks rely on 3D transformations\u2014such as changes in angle, scale, and lighting\u2014which 2D approaches cannot adequately capture. Furthermore, many existing approaches assume white-box access to the model's architecture and gradients (Sarkar et al. 2023; Lin et al. 2020; Andriushchenko and Flammarion 2020; Dong et al. 2018; Hull, Wang, and Chau 2024). In contrast, real-world adversarial attacks are often black-box in nature, where attackers do not have direct access to the model but instead exploit vulnerabilities through inputs captured by external devices like cameras (Athalye et al. 2018; Sharif et al. 2019; Ergezer et al. 2024).\nRecent efforts have begun exploring adversarial attacks on 3D objects (Li, Lian, and Chen 2023), but this domain remains underdeveloped. Applying adversarial noise to 3D objects is more resource-intensive, often requiring high-quality 3D models and complex manipulation of textures. Moreover, existing datasets for 3D adversarial research are sparse compared to their 2D counterparts, creating a bottleneck for advancements. Although recent studies have explored the 3D adversarial landscape using rendering techniques such as Gaussian Splatting (Zeybey, Ergezer, and Nguyen 2024), these investigations remain largely limited to white-box settings. AdvIRL innovatively overcomes these limitations, relying solely on the input and output of the target model to generate effective adversarial attacks. This gradient-free approach simplifies the process and sidesteps challenges associated with adversarial transferability in existing methods.\nIn the context of this research, we focus on digital adversarial attacks targeting machine learning vision systems. These attacks simulate how carefully crafted perturbations can manipulate neural rendering models, potentially compromising perception in critical applications like autonomous vehicles, surveillance systems, and robotic navigation.\nPractical Scenario Example: Consider a virtual reality (VR) training application for first responders, where envi-"}, {"title": "Method", "content": "In this section, we introduce our proposed framework, AdvIRL, designed to generate adversarial noise for 3D models.\nUnlike traditional adversarial methods, our framework operates on a higher dimension leading to more robust results. We describe the components apart of our AdvIRL pipeline in the following subsections.\nThe adversarial noise generation process comprises four primary steps. First, input images are processed through an image segmentation algorithm, generating segmented images X. Secondly, segmented images X are classified using the Contrastive Language-Image Pretraining (CLIP) model to ensure only correctly classified images are used (Radford et al. 2021). Next, Instant-NGP is employed to render the 3D object from various viewpoints, creating snapshots of the NeRF model. Finally, AdvIRL is configured with appropriate parameters to generate adversarial examples from different perspectives. Figure 1 illustrates how AdvIRL adjusts the parameters of Instant-NGP to generate adversarial outcomes."}, {"title": "Image Segmentation", "content": "AdvIRL includes an image preprocessing step before entering the reinforcement learning cycle. As part of this process, an image segmentation algorithm, Detectron2 (Wu et al. 2019), is utilized to generate a mask (M) and class predictions (P) based on the input images. These outputs are then used to create segmented images (X). By leveraging a pre-trained Mask R-CNN model with a ResNet-50 backbone, we achieve strong results without requiring additional training."}, {"title": "Adversarial Generation using Reinforcement Learning", "content": "Once the segmented images X are generated, they serve as the initial observation space. Based on the current state, AdvIRL generates an action A, which is concatenated with the existing Instant-NGP parameters $P_{old}$ to produce adversarial parameters $P_{new}$. Instant-NGP then uses $P_{new}$ to generate adversarial images $X_{adv}$ and the adversarial NeRF model $M_{adv}$. The adversarial images $X_{adv}$ are subsequently processed by our CLIP classifier, producing a nested list where each entry contains the label and its corresponding classification confidence. The nested list is then processed to produce a dictionary D containing the label (key) and a list containing the average confidence and number of images predicted for this label (value).\nOnce preprocessed, dictionary D, input images X and adversarial images $X_{adv}$ will be used to generate reward R formulated as:\n$reward(True, Target, X, X_{adv}) = Target \\cdot \\theta_0 - True \\cdot \\theta_1 - MSE(X, X_{adv}) \\cdot \\theta_2$   (1)\n$MSE(X, X_{adv})$   (2)\nHere, $\\theta_0$ and $\\theta_1$ are hyperparameters that balance the confidence of the true predictions of the target class and the true predictions. The term $MSE(X, X_{adv})$ penalizes excessive modifications, encouraging AdvIRL to focus noise generation within the object's bounds. In our experiments, we"}, {"title": "Experimental Setup", "content": "We target the CLIP Resnet50 model (Dosovitskiy et al. 2021) due to its flexible class labels, allowing us to define any set of string labels for classification. This flexibility is crucial for generating adversarial examples across varied scenarios.\nOur experiments utilize the Tanks and Temples dataset (Knapitsch et al. 2017), as well as an additional banana scene collected by our team. The Tanks and Temples dataset offers diverse 3D scenes, enabling a thorough evaluation of AdvIRL's performance across different settings."}, {"title": "Hyperparameter Selection:", "content": "Our hyperparameters were chosen through grid search and Bayesian optimization, considering computational constraints and attack efficacy."}, {"title": "Policy Gradient Algorithm", "content": "We selected Proximal Policy Optimization (PPO) as our policy gradient algorithm for its simplicity and robustness. The following parameters were used for our experiments:\n\u2022 n_steps: Set to 2.\n\u2022 batch_size: Set to 2.\n\u2022 max_grad_norm: Set to 0.00001.\nOur method's reliance on small n_steps, batch_size, and max_grad_norm were not just hardware limitations caused by the large action space and the memory-intensive nature of rendering NeRF model, but strategic choices to prevent gradient instability in high-dimensional parameter spaces."}, {"title": "Constraints", "content": "Hardware : AdvIRL was trained on a workstation with two GPUs, each with 24 GB of VRAM. Instant-NGP and the CLIP classifier operated on separate GPUs, while the reinforcement learning model ran on the CPU."}, {"title": "Results", "content": "Our experiments evaluated AdvIRL's ability to generate adversarial noise for various scenes from the Tanks and Temples dataset (Knapitsch et al. 2017). Figure 2 showcases the original scenes alongside the adversarially perturbed images produced by AdvIRL from different angles and distances."}, {"title": "Adversarial Results without Segmentation", "content": "Our initial experiments excluded image segmentation from our pipeline to assess the impact of AdvIRL on scenes with unrestricted freedom to modify their entirety. These tests were conducted on two scenes: the lighthouse and the train scene (Knapitsch et al. 2017)."}, {"title": "Lighthouse:", "content": "We first ran AdvIRL to generate untargeted adversarial noise. The average confidence across 13 images was 23.4% for the boathouse class. Of the remaining images, six were classified as a lighthouse (19.4% confidence), and one was misclassified as a church (18.2% confidence). Additionally, targeted adversarial noise generation was applied with the boathouse class as the target.\nFigure 3 shows the resulting adversarial image classified as a boathouse with 50% confidence. Notably, the average confidence across all images was 20%, indicating significant variability in classification confidence across different angles. Additional images of this experiment are shown in Figure 2. This result highlights AdvIRL's capability in generating class-targeted adversarial noise."}, {"title": "Train Scene:", "content": "All 20 images were misclassified as various other objects (breakwater, megalith, couch, etc.) with classification confidences ranging from 4% to 37%. Unlike other scenes, the adversarial noise applied to the train scene significantly altered the object's identifying features, such as edges and text, which may have contributed to the high degree of misclassification."}, {"title": "Adversarial Results For Segmented Input Images", "content": "To prevent substantial alteration of the entire scene, our Segmentation-Reinforcement Learning pipeline guides AdvIRL in selectively modifying only the target object. Our subsequent tests aim to evaluate AdvIRL's ability to focus on objects within scenes. Additionally, these tests also assess the confidence levels achieved when using segmented images."}, {"title": "Banana Scene:", "content": "Our initial tests, using the Segmentation-Reinforcement Learning pipeline, resulted in adversarial noise that misclassified the banana as a flatworm with confidences ranging between 35% and 75%. This demonstrates AdvIRL's effectiveness in applying adversarial noise to 3D-generated models across multiple viewpoints. Some images generated from this experiment are shown in Figure 2."}, {"title": "Truck Scene:", "content": "We conducted a targeted attack with the cannon class as the target. AdvIRL successfully misclassified 15 out of 20 images as a cannon, with confidences ranging from 15% to 70%. This highlights AdvIRL's ability to produce effective targeted adversarial noise, posing a potential risk for vision models in applications like self-driving."}, {"title": "Horse Scene:", "content": "All 20 adversarial images were misclassified as other objects or animals (e.g., scorpion, Kerry Blue Terrier), with confidences ranging from 3% to 70%. Although some misclassifications had low confidence, AdvIRL demonstrated effectiveness in producing adversarial noise in a short time, which may be due to the complex shape of the horse.\nIn a targeted attack, where the target class was a triceratops, AdvIRL successfully misclassified 8 out of 20 images as triceratops, as shown in Figure 6."}, {"title": "Conclusion", "content": "To our knowledge, AdvIRL is the first framework for conducting adversarial AI investigations on generated 3D objects of varying sizes and shapes in black-box scenarios. Central to our approach is the integration of an image segmentation model and a 3D rendering algorithm, such as NeRF, facilitating the generation of robust adversarial noise specifically targeting the texture of selected objects. Unlike conventional adversarial attacks, our model operates by adjusting the parameters of the NeRF algorithm, enabling it to render adversarial noise alongside images or 3D models.\nAdvIRL stands out with its inherently black-box nature, relying solely on the input and output of the target model to generate effective adversarial noise. Unlike existing methods, which depend on knowledge of the target model's architecture or weights, AdvIRL requires no such information to produce robust and transferable adversarial noise. Despite the limited information available in a black-box setting, AdvIRL performs strongly in crafting adversarial noise for both targeted and untargeted scenarios.\nFurthermore, our experiments employing the Segmentation-Reinforcement Learning pipeline (AdvIRL) underscore the efficacy of our approach in generating adversarial noise for both targeted and untargeted scenarios, providing a robust reward system that adequately incentivizes the agent to be implemented. These results suggest the capability of our model for adversarial noise generation and its potential usage in producing adversarial noise for 3D generative models, thus increasing the robustness of vision models against attacks."}, {"title": "Future Works", "content": "While our algorithm performs well in producing both targeted and untargeted attacks, the act of producing targeted noise is highly dependent on the size of the action space. Since our action space represents the 13 million parameters of Instant-NGP, AdvIRL is given numerous potential modifications it can make. Due to this freedom, AdvIRL may take longer than if the action space were smaller. In turn, future works may focus on limiting the action space to a set of crucial parameters to modify, potentially leading to better and more efficient results. Furthermore, due to our usage of a pre-trained Detectron2 model, our segmentation is limited to the available classes in the model. Future work may involve incorporating Segment Anything Model 2 (Ravi et al. 2024) for more generalized segmentation results."}]}