{"title": "Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM", "authors": ["Xiaofeng Liu", "Jonghye Woo", "Chao Ma", "Jinsong Ouyang", "and Georges El Fakhri"], "abstract": "Abstract-Delineating lesions and anatomical structure is important for image-guided interventions. Point-supervised medical image segmentation (PSS) has great potential to alleviate costly expert delineation labeling. However, due to the lack of precise size and boundary guidance, the effectiveness of PSS often falls short of expectations. Although recent vision foundational models, such as the medical segment anything model (MedSAM), have made significant advancements in bounding-box-prompted segmentation, it is not straightforward to utilize point annotation, and is prone to semantic ambiguity. In this preliminary study, we introduce an iterative framework to facilitate semantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt generator (SBPG) module has the capacity to convert the point input into potential pseudo bounding box suggestions, which are explicitly refined by the prototype-based semantic similarity. This is then succeeded by a prompt-guided spatial refinement (PGSR) module that harnesses the exceptional generalizability of MedSAM to infer the segmentation mask, which also updates the box proposal seed in SBPG. Performance can be progressively improved with adequate iterations. We conducted an evaluation on BraTS2018 for the segmentation of whole brain tumors and demonstrated its superior performance compared to traditional PSS methods and on par with box-supervised methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The costly labeling effort in medical image delineation significantly hinder the development of data-driven AI models. There are increasing interests on weakly supervised segmentation to utilize bounding box or even a single point as label for training supervision [5]. However, due to the absence of accurate size and boundary guidance, there is a large performance gap between point-supervised medical image segmentation (PSS) and mask/box-supervised counterparts [2].\nThe recent progress of vision foundational models has achieved breakthroughs in several weakly supervised tasks, benefited by their strong zero-shot generalizability. For instance, the point-prompt natural image segment anything model (SAM) has been applied to enhance pseudo labels for point-supervised segmentation [1], [2]. Recently, the medical image version of SAM (MedSAM) [4] has been trained with 1.5 million segmented medical images to achieve generalizable box-prompt segmentation. MedSAM takes both an image slice and a bounding box, i.e., prompt, to predict the possible segmentation mask within the box. Notably, the natural image SAM itself follows point-prompt design. Therefore, it is not straightforward to integrate box-prompt MedSAM to PSS task as [1]. In addition, a significant limitation of SAM/MedSAM is the lack of classification ability, resulting in class / structure-agnostic segmentation results [1], [2]. They are designed for general use, while fails to accurately delineate the specific lesion or structure as desired [1], [2].\nTo our knowledge, this is the first attempt to integrate MedSAM to facilitate semantic-aware PSS. We adopt an iterative framework to achieve coarse-to-fine progression of the bounding box. The point prompt is first converted to a box-proposal seed. Then, we configure a semantic box-prompt generator (SBPG) to propose and pick the reasonable box according to semantic similarity as [2]. It is followed by a box prompt guided spatial refinement (PGSR) to utilize the generalizable MedSAM to predict the segmentation mask. In addition, the smallest box that covers the mask is further used as the box seed in the next round of SBPG. We do not need to fine-tune the MedSAM to be semantic aware, which is prone to catastrophic forgetting. Notably, only the prompt refiner \\(\\theta\\) with the encoder part of the segmentor model will be trained, which involves about half the parameters of mask-supervised UNet training.\nWe demonstrated its effectiveness in BraTS2018 for PSS"}, {"title": "II. METHODOLOGY", "content": "In PSS, we are given tuplet \\(\\{x^{i}, p^{i}, s^{i}\\}\\), in which \\(x^{i}\\) can be an MR slice, while the point prompt \\(p^{i} = \\{p_{x}^{i}, p_{y}^{i}, c\\}\\) indicates the 2D spatial coordinates and the interested class c, e.g., tumor or normal tissue. We would expect the segmentation mask predictions to approximate the expert annotation \\(s^{i}\\).\nTo catering the box-prompt MedSAM [4], an initial box proposal seed of class c, i.e., \\(b_{c}^{i,0}\\), is generated from \\(p^{i}\\), which is centered on point location \\((p_{x}, p_{y})\\) with the size of XXY. We do not expect \\(b_{c}^{i,0}\\) fit the region of interests very well as there is no prior information about the size in the initial step. Then, the proposal bag \\(B^{i} = \\{b_{c,n}^{i,0}\\}_{n=1}^{N}\\) is created by scaling \\(b_{c}^{i,0}\\) with N different scales. Therefore, the boxes in group \\(B^{i}\\) has strong spatial correlation with \\(p^{i}\\), which avoid the inefficient random proposal in whole image [2].\nTo enable the model be aware of the specific semantic class, for example tumor in our task, a parameterized prompt refiner \\(\\theta\\) with a segmentor encoder and two fully connected layers is applied. Specifically, for the training sample with segmentation mask label \\(s^{i}\\), we convert \\(s^{i}\\) to the corresponding smallest rectangle bounding box label \\(b^{i}\\). Then, we store a set of feature \\(f_{c,n}^{i} = \\theta(x^{i}, b^{i})\\) in the most recent M batches to a memory buffer, and calculate the mean of \\(\\{f_{c,n}^{i}\\}_{n,m=1}^{N,M}\\) as the prototype \\(V_{c}\\). As conventional multiple instance learning (MIL) based methods [5], the instance level probability indicating the likelihood of \\(b_{c,n}^{i}\\) matches \\(V_{c}\\) can be approximated with\n\\[\np(b_{c,n}^{i}, V_{c}) = \\frac{e^{\\cos(f_{c,n}^{i},V_{c})}}{\\sum_{c=1}^{C}e^{\\cos(f_{c,n}^{i}, V_{c})}}, \n\\]\nwhere \\(\\cos()\\) indicates the cosine similarity, and C is the number of segmentation category. With the ground truth \\(b^{i}\\), \\(\\theta\\) is trained with the binary cross entropy loss of\n\\[\n\\mathcal{L} = - \\sum_{c=1}^{C} \\{c_{i} \\log p(b_{c,n}^{i}, V_{c}) + (1 - c_{i}) \\log(1 - p(b_{c,n}^{i}, V_{c}))\\},\\nonumber\n\\]\nwhere \\(c_{i}\\) is the one-hot class label. Therefore, the prompt refiner is optimized to be semantic aware for the specific anatomical structure. The prototype in buffer is also updated with the new model \\(\\theta\\). With \\(p(b_{c,n}^{i}, V_{c})\\) for each \\(b_{c,n}^{i}\\), we pick the highest probability one in \\(B^{i}\\) as our optimal proposal \\(b_{c}^{*}\\) in current stage, which is also our bounding box inference.\nThen, in the PGSR module, the off-the-shelf MedSAM [4] takes both \\(x^{i}\\) and \\(b_{c}^{*}\\) to predict the possible segmentation masks. Notably, no information about the interested class can be directly informed in MedSAM. Although the MedSAM has strong ability of zero-shot segmentation to delineate the relatively accurate mask within the current box prompt \\(b_{c}^{*}\\), the alignment of \\(s^{i}\\) is highly dependent on the precise \\(b_{c}^{*}\\). This motivated us to refine \\(b_{c}^{*}\\) with a more suitable proposal seed \\(b_{c}^{i,0}\\) in SBPG.\nIn practice, we can simply apply the smallest bounding box \\(b_{c}^{i,0}\\) for \\(s^{i,0}\\), and use \\(b_{c}^{*}\\) as our box proposal seed in the next round of SBPG. With T rounds of iteration, we expect that the box-prompt can be progressively refined to inform the MedSAM to predict an accurate segmentation mask.\nIn particular, in the last round of testing, we do not need to generate \\(b_{c}^{i,0}\\). Instead, the T-th round \\(b_{c}^{i,0}\\) is used as our final prediction."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "We evaluated on BraTS dataset for whole tumor segmentation as [3], and use T2-weighted slices as our input. We used the 80%/20% split for training or testing. We generate the smallest bounding box for the mask and use its center as \\(p^{i}\\). Our prompt refiner adopted the encoder part of the segmentor in [3], which is followed by two fully connected layers of 1024 and 256 dimensions, respectively. We empirically set \\(X \\times Y = 21 \\times 21\\) and search proper T\u2208 \\(\\{1,2,3,5,10\\}\\). We followed [2] to compare with the conventional point-supervised method of WISE-Net without the use of the vision fundation model. Also, we directly input the ground truth of the bounding box into MedSAM, which can be \"upper bound\" of the performance of pointwise method. In Tab.I, we can see that our model with T\u2265 5 outperforms WISE-Net by a large margin. The Dice score is close to the box-supervised MedSAM."}, {"title": "IV. CONCLUSION", "content": "In this work, we proposed an efficient iterative framework to enable box-prompt MedSAM to point-supervised medical image segmentation. A lightweight prompt refiner with only encoder is specifically trained for the interested structural class. We show that the fixed off-the-shelf large model can flexibly target the specific downstream task without the need of large scale fine-tuning. The medical PSS performance can be largely improved by the advance of vision foundation model, e.g., MedSAM, which show great promise for facilitating the image-guided interventions."}]}