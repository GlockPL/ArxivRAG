{"title": "Aligner-Guided Training Paradigm: Advancing Text-to-Speech Models with Aligner Guided Duration", "authors": ["Haowei Lou", "Helen Paik", "Wen Hu", "Lina Yao"], "abstract": "Recent advancements in text-to-speech (TTS) systems, such as FastSpeech and StyleSpeech, have significantly improved speech generation quality. However, these models often rely on duration generated by external tools like the Montreal Forced Aligner, which can be time-consuming and lack flexibility. The importance of accurate duration is often underestimated, despite their crucial role in achieving natural prosody and intelligibility. To address these limitations, we propose a novel Aligner-Guided Training Paradigm that prioritizes accurate duration labelling by training an aligner before the TTS model. This approach reduces dependence on external tools and enhances alignment accuracy. We further explore the impact of different acoustic features, including Mel-Spectrograms, MFCCs, and latent features, on TTS model performance. Our experimental results show that aligner-guided duration labelling can achieve up to a 16% improvement in word error rate and significantly enhance phoneme and tone alignment. These findings highlight the effectiveness of our approach in optimizing TTS systems for more natural and intelligible speech generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-speech (TTS) systems have seen significant ad-vancements with the development of state-of-the-art models such as FastSpeech [1], [2] and StyleSpeech [3]. These models feature efficient architectures that enable high-quality and natural-sounding speech generation. FastSpeech uses a non-autoregressive approach to generate speech quickly, while StyleSpeech incorporates style information to produce speech with diverse prosodic variations. However, both models share a common practice in their training paradigm: they rely on duration label to align phonemes with their corresponding speech segments. These durations are typically treated as ground truth and are generated using external tools.\n\nThe Montreal Forced Aligner (MFA) [4] is a widely used tool for phoneme alignment, designed to align phonemes with their corresponding segments in speech using the Kaldi-ASR Toolkit [5]. It automates the alignment process through a model based on Hidden Markov Models (HMMs) [6], which efficiently processes phonetic transcriptions and au-dio recordings to generate time-aligned phoneme boundaries. These boundaries provide the duration labels that serve as the ground truth for training TTS models, ensuring that phonemes are accurately synchronized with the corresponding audio. The use of MFA has become a standard practice in the field, as it offers a convenient and reliable method to produce essential alignment data for TTS training. The MFA approach has facilitated the training of TTS models. However, it presents two significant limitations to the current TTS training paradigm. First, the importance of accurate duration is often underestimated, with much of the focus on improving model architecture or other components. Precise duration alignment is crucial for achieving natural prosody and intelligibility; inaccuracies in duration labelling can negatively impact the quality of the generated speech. Second, the use of MFA, which relies on HMM-based approaches, involves long training times and lacks flexibility. This makes it less adaptable for rapidly evolving TTS development environments.\n\nTo address these limitations, this work proposes a novel approach that emphasizes the importance of duration in the training process. We introduce an Aligner-Guided Training Paradigm, where an aligner is first trained to generate accurate duration. These labels are then used to guide the training of the TTS model, ensuring better alignment and more natural speech generation. Our experimental results demonstrate that this ap-proach significantly improves the performance of TTS models, achieving higher accuracy in phoneme and tone alignment and producing more natural-sounding speech. By leveraging aligner-guided duration labeling, our method provides a more efficient and flexible solution for training state-of-the-art TTS models. The main contributions of this work are:\n\n1) We validate that duration plays a crucial role in TTS"}, {"title": "II. METHOD", "content": "In this section, we present the Aligner-Guided Training Paradigm in detail. Given a speech signal A, a phoneme sequence X, and a target representation Y for the TTS model. We first encode the speech signal into an acoustic feature H, such as a Mel-Spectrogram (MelSpec), Mel-frequency cepstral coefficients (MFCC), or a latent feature. Next, we train an Automatic Speech Recognition (ASR) model to recognise X based on H and employ an aligner module to calculate the duration L for each phoneme in X. Finally, the TTS model is trained using the phoneme sequence X and its corresponding duration L to reconstruct the target output Y. Figure 2b presents an overview of the proposed Aligner-Guided Training Paradigm.\n\nAssume we have a set of phonemes with a totally P distinct phoneme, and a speech signal A. We first encode A into an acoustic feature \\(H \\in \\mathbb{R}^{N \\times T}\\), where T is the number of frames in the acoustic feature and N is the frame dimension. The Automatic Speech Recognition (ASR) model takes H as input and generates a likelihood matrix, \\(C = ASR(H) \\in \\mathbb{R}^{P \\times T}\\), where each element \\(C_{i,j}\\) represents the likelihood of the j-th frame being associated with the i-th phoneme.\n\nWe construct a simple deep learning-based model to imple-ment the ASR model. Figure 2a presents the architecture of the ASR model. It contains two bidirectional Long-Short Term Memory (LSTM) layers followed by a linear layer to capture temporal dependencies in the acoustic feature and map the LSTM outputs to phoneme space to predict the likelihood of each phoneme for every frame.\n\nThe proposed ASR model cannot be directly applied to perform speech recognition tasks. The ASR model outputs a sequence of frame-level probabilities, each corresponding to a particular time step in the input signal. However, the phoneme sequence L is typically much shorter, as each phoneme can span multiple frames. The mismatch between the output size T and target size L creates a significant challenge in aligning the predicted phoneme probabilities with the actual phoneme sequence. It leads to errors in recognizing and predicting the correct phonemes."}, {"title": "A. Automatic Speech Recognition", "content": "We impose Connectionist Temporal Classification Loss (CT-CLoss) [7] to train our ASR model to address the issue of mismatch. It allows the model to learn the most likely alignment between the input frames and the target phoneme sequence without requiring explicit frame-level labels. Specif-ically, CTCLoss considers all possible alignments of H to the target phoneme sequence and then maximizes the probability distribution of correct alignment over all possible alignments. The loss function is formally defined as:\n\n$\\text{CTCLoss} = -\\log \\sum_{\\pi \\in \\text{Align}(X, H)} \\prod_{t=1}^{T} p(\\pi_{t} | H_{t}) $ (1)\n\nwhere \\( \\pi \\) represents a possible alignment between the input sequence H and the target sequence X, \\( \\text{Align}(X, H) \\) is the set of all valid alignments, and \\( p(\\pi_{t} | H_{t}) \\) is the probability of the t-th frame in the alignment \\( \\pi \\) being assigned to the corresponding phoneme in X.\n\nAfter training, the ASR model predicts a likelihood matrix \\(C \\in \\mathbb{R}^{(P+1) \\times T}\\). The additional dimension corresponds to the probability of a blank phoneme, which is typically removed in standard speech recognition tasks. However, in our method, this blank phoneme is preserved and plays a crucial role in the subsequent stages."}, {"title": "B. Phoneme Duration Alignment", "content": "Once the ASR model is well-trained and we have the likelihood matrix C. The next step is to determine the duration of each phoneme in the sequence X. Specifically, we need to calculate a sequence of duration L, where each duration \\(L_{i}\\) is a non-negative integer representing the time span of the i-th phoneme \\(X_{i}\\). This sequence must satisfy the conditions \\(L_{i} > 0\\) and \\(T = \\sum_{i=0}^{N-1} L_{i}\\), where T is the total number of frames in H.\n\nTo achieve this, we define a Phoneme Duration Alignment (PDA) algorithm. The algorithm's objective is to collapse consecutive frames corresponding to the same phoneme and count their occurrences to determine the duration of each phoneme. The first step in this process involves applying a column-wise argmax function to the likelihood matrix C. This operation produces a frame-level phoneme sequence, where each entry corresponds to the phoneme with the highest probability for that frame in the acoustic feature H.\n\nNext, we iterate through the frame-level phoneme sequence. For each phoneme in the sequence, we check if it is the same as the previous phoneme or if it represents silence \\(\\epsilon\\). If the phoneme is the same as the previous one or is a silence, we increase its duration by one. If we encounter a different phoneme, we record the duration of the previous phoneme and then reset the counter to start counting the duration of the new phoneme. This process continues until we have found the duration for each phoneme in the sequence. Figure 1 provides a clearer and more intuitive illustration of the duration alignment process."}, {"title": "C. Aligner Guided Training", "content": "The duration collected by the PDA algorithm are used to guide the training process of the TTS model. During training, the TTS model is provided with the phoneme sequence X, the corresponding duration L, and the TTS feature target \\(Y \\in \\mathbb{R}^{M \\times T}\\), where M represents the feature dimension and T is the number of feature frames, which corresponds to the number of frames in H. The target feature is flexible. In this study, we follow the approach of RVAE [8]. We train an autoencoder to encode speech into a latent feature and use the trained decoder to reconstruct the speech. This speech-encoded latent feature serves as the TTS feature target Y.\n\nWe use the StyleSpeech [3] model as the backbone to build our TTS system. The StyleSpeech model provides a robust architecture that integrates subtle tonal style change into the speech generation process. It allows us to generate speech with diverse prosodic variations. In this model, the phoneme sequence X and the corresponding style sequence S are first encoded into vector embeddings. These phoneme and style embeddings are then fused to produce the acoustic embed-ding E by combining them through element-wise addition: \\(E = \\text{embed}(X) + \\text{embed}(S)\\).\n\nThe sequence of acoustic embeddings E is then passed through a duration adapter. The duration adapter predicts the duration L' for each acoustic embedding and duplicates each embedding \\(E_{i}\\) to match the predicted duration L. This duration-adapted acoustic embedding is subsequently fed into a feature encoder to generate the output H'.\n\nThe TTS model is optimized using two loss functions: (1) Speech Loss, which minimizes the difference between the generated output and the target output using Mean-Square-Error (MSE), and (2) Duration Loss, which minimizes the difference between the predicted duration L' and the aligner-guided duration L using Mean-Absolute-Error (MAE).\n\n$\\text{TTSLoss} = \\text{MSE}(H, H') + \\text{MAE}(L, L') $ (2)\n\nAfter applying the loss optimization, and the TTS model is trained. The generated features H' are passed through the trained decoder and produce the final generated speech A'."}, {"title": "III. EXPERIMENTS AND RESULT ANALYSIS", "content": "Dataset: We use the Baker dataset [9] to train and evaluate this study. We select Chinese as our evaluation language due to its complex phonetic and tonal structure. The Baker dataset contains approximately 12 hours of speech recorded using professional instruments at a frequency of 48kHz. The dataset consists of 10k speech samples from a female Mandarin speaker.\n\nExperimental setup: The phoneme, style, and feature en-coders each consist of four Feed-Forward Transformer (FFT) blocks [2]. We set the dimension of the target feature to 16. For optimization, we employ a learning rate schedule with a warm-up strategy inspired by the Transformer model [10]. \u03a4\u03bf prevent overfitting, dropout rates are set at 0.5 for the FFT blocks and 0.1 for the length adapter.\n\nThe experiment is conducted on an NVIDIA RTX A5000 GPU using a PyTorch implementation. We select 4,000 sen-tences from the Baker dataset for training and 1,000 sentences for testing. The batch size is set to 64, and the model is trained for 600 epochs.\n\nAn ablation study on the training of acoustic features for the ASR model is conducted to evaluate the impact of different types of acoustic features on both the duration prediction and the overall performance of the TTS model. We trained the ASR model using MelSpec, MFCC, and latent features. The source code will be released upon acceptance of this study.\n\nMetrics: We employ Word Error Rate (WER), Mel Cepstral Distortion (MCD) [11], and Perceptual Evaluation of Speech"}, {"title": "A. Analysis of predicted duration", "content": "Table II presents the predicted phoneme duration for a sequence of phonemes (H, AO, H, AO, X, UE, X, I) using TTS models trained with duration generated by aligners trained using different feature types, Origin, Latent, MelSpec, and MFCC. Each row in the table represents the duration predic-tions from one of these features. It highlights how the choice of features impacts the alignment and timing of phonemes.\n\nTTS model trained with Latent features shows a high variability. It tends to predict a very short duration for the initial phoneme (e.g., 'H') and an extremely long duration for the final phoneme (e.g., 'I'), as seen with values of 2 and 22, respectively. This suggests that Latent features may have a tendency to underrepresent the temporal extent of certain phonemes while overemphasizing others. It causes potentially unnatural pacing in the generated speech. The higher variability and extremities in duration predictions with Latent features may indicate difficulties in accurately capturing and aligning the phoneme boundaries.\n\nMFCC feature shows moderate variability, with predicted duration ranging from 2 to 20. While MFCCs provide reason-able accuracy in phoneme duration prediction, the presence of outliers, such as the duration of 20 for the final phoneme, suggests occasional misalignment, which might affect the naturalness of the generated speech. MelSpec feature results in more balanced duration predictions across the phonemes, ranging from 2 to 10. This distribution suggests that MelSpec provides a better representation of phoneme length. It facili-tates more natural and consistent speech output. The consistent duration predictions across different phonemes indicate effec-tive alignment, which likely contributes to MelSpec's superior performance in TTS models."}, {"title": "IV. CONCLUSION", "content": "In this study, we validate the significance of duration in TTS training and propose an effective aligner-guided training paradigm. Our experiments show that Mel-Spectrograms out-perform other acoustic features. These contributions provide valuable insights into optimizing TTS systems for more natural and accurate speech generation."}]}