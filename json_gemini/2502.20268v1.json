{"title": "Large Language Models as Attribution Regularizers for Efficient Model Training", "authors": ["Davor Vukadin", "Marin \u0160ili\u0107", "Goran Dela\u010d"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains. However, effectively leveraging their vast knowledge for training smaller downstream models remains an open challenge, especially in domains like tabular data learning, where simpler models are often preferred due to interpretability and efficiency. In this paper, we introduce a novel yet straightforward method for incorporating LLM-generated global task feature attributions into the training process of smaller networks. Specifically, we propose an attribution-matching regularization term that aligns the training dynamics of the smaller model with the insights provided by the LLM. By doing so, our approach yields superior performance in few-shot learning scenarios. Notably, our method requires only black-box API access to the LLM, making it easy to integrate into existing training pipelines with minimal computational overhead. Furthermore, we demonstrate how this method can be used to address common issues in real-world datasets, such as skewness and bias. By integrating high-level knowledge from LLMs, our approach improves generalization, even when training data is limited or imbalanced. We validate its effectiveness through extensive experiments across multiple tasks, demonstrating improved learning efficiency and model robustness.", "sections": [{"title": "1 Introduction", "content": "The recent expansion in model parameters and training data for large language models (LLMs) has driven a significant breakthrough in natural language processing (NLP) [8,12,24,49]. These models exhibit remarkable performance across various evaluation paradigms, such as zero-shot [32] and few-shot inference, leveraging in-context learning [36,51]. This capability stems from the extensive text corpora used for training, which embed rich prior knowledge into LLMs, allowing them to approximate expert knowledge across diverse domains. Although their strong performance and generalization capabilities have been successfully extended to other modalities, such as images [34] and speech [54], their application in tabular learning settings remains limited."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Standard Machine Learning Approaches", "content": "Inspired by the success of deep learning in other domains, numerous efforts have sought to apply self-supervised learning to tabular data to develop transfer learning-ready models. These approaches include masked feature prediction [4,35], feature corruption correction [6,53], and contrastive pre-training [48]. However, comparative studies indicate that gradient-boosted tree ensembles still outperform these methods [20,45]. More recently, Nam et al. [38] introduced Self-generated Tasks from UNlabeled Tables (STUNT), leveraging self-generated few-shot tasks for tabular learning, though its reliance on large unlabeled datasets may limit practical applicability. Additionally, Hollmann et al. [25] proposed the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model pre-trained on millions of synthetic datasets."}, {"title": "2.2 Large Language Models in Tabular Learning", "content": "Most approaches integrating large language models (LLMs) into tabular learning rely on encoding task and feature descriptions in natural language, serializing the data, and leveraging LLMs for inference either through in-context learning [46] or additional fine-tuning [13,23,50]. However, these methods face significant drawbacks, including the high cost of LLM inference for individual samples and the computational demands of fine-tuning.\nIn sensitive domains such as medicine or finance [44], where transparency is critical, the opaque decision-making of LLMs is less desirable than traditional,"}, {"title": "2.3 Explanation Guided Learning", "content": "A growing line of research explores enhancing model behavior through additional supervision derived from explainable artificial intelligence (XAI) techniques. This field can be broadly categorized into local explanation-guided learning and global explanation-guided learning [17].\nLocal explanation guidance applies supervision signals or regularization terms to individual model explanations, steering learning at the sample level. This approach is more prevalent due to the extensive development of local explanation techniques, particularly in the image domain, such as Grad-CAM [43], Layer-wise Relevance Propagation (LRP) [5], and attention-based attributions [2]. Ross et al. [41] propose regularizing differentiable models by penalizing input gradients, aligning them with expert-defined attribution maps. Dharma et al. [29] use object bounding boxes as explanation supervision signals. In text classification, several studies leverage per-sample human-annotated rationales [11,28,56]. Gao et al. [18] demonstrate the effectiveness of local explanation supervision under limited training data. However, a key limitation of this approach is its reliance on per-sample attribution annotations, which are often difficult and costly to obtain, particularly in expert-driven fields like medicine.\nGlobal explanation guidance, in contrast, does not require instance-level attributions, instead offering a broader, more scalable approach to shaping model behavior. Liu et al. [33] reduce undesired biases by penalizing nonzero attributions on sensitive tokens. Erion et al. [16] aggregate local feature attributions via expected gradients to improve interpretability. Weinberger et al. [52] extract prior knowledge from multiple gene expression datasets to construct meta-features, training a deep global attribution model alongside a predictive model with a regularization loss. However, this method assumes the availability of additional datasets related to the problem, which may not always be feasible."}, {"title": "3 Method", "content": "In contrast to other methods that utilize LLMs for tabular data prediction, we seek to minimize both the computational and price overhead of their use, while simultaneously still effectively using their generalization abilities and providing small, interpretable models that can be readily used in existing pipelines."}, {"title": "3.1 Formulation", "content": "Given a trained binary classification model \\(m_{\\theta} : \\mathbb{R}^{N} \\rightarrow [0, 1]\\) parametrized by \\(\\theta\\), an attribution produced by an attribution method a for an input x is a vector \\(a(x) = (s_1, ..., s_n)\\), where \\(s_i\\) is the attribution score of the input feature \\(x_i\\). We are interested in the expected value of the attribution scores over the entire dataset, given by \\(S_E = \\mathbb{E}_{x \\sim D}[a(x)]\\), where D represents the data distribution.\nFor certain datasets where the expected attribution follows an intuitive pattern that humans can interpret, we hypothesize that this expected value can be approximated using a large language model and thus serve as a valuable local attribution guide during training.\nTaking a step back, given an untrained model along with a task description and feature descriptions, we query an LLM to generate importance scores for each feature, producing a vector \\(S_{LLM}\\). During model training, we then regularize the local attribution scores of the model to align with these LLM-derived scores. This regularization acts as a guiding signal, helping the model maintain behavior that aligns with intuitive, human-understandable reasoning.\nThe final model's loss function consists of two components: the standard binary cross-entropy loss and an attribution regularization term. The regularization term is the mean squared error between the normalized attribution scores and the normalized LLM-derived scores, weighted by \\(\\gamma\\). The overall loss is given as a weighted sum of these terms:\n\\[\\mathcal{L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (BCE(m_{\\theta}(x_i), y_i) + \\gamma l_{MSE}(\\frac{a(x_i)}{\\|a(x_i)\\|_{L_2}}, \\frac{S_{LLM}}{\\|S_{LLM}\\|_{L_2}}))\\]\nFollowing Ross et al. [41], we employ the input gradient as our chosen attribution method."}, {"title": "3.2 LLM Prompting and Scores Parsing", "content": "To enable Large Language Models (LLMs) to generate meaningful feature attribution scores that effectively guide downstream models during training, we developed a comprehensive prompting methodology. This approach incorporates several key components to maximize score accuracy and relevance."}, {"title": "Task and Dataset Contextualization", "content": "We first embed relevant task and dataset information within the prompt structure by incorporating key details. Task descriptions provide a concise overview of the classification objective and possible outcomes. For example: \"Predict whether this patient's breast cancer will reoccur. Yes or no?\" an approach aligned with established methodologies in recent literature [21,23]. Feature descriptions ensure clarity by defining each dataset attribute; for instance, \"Age: The age of the patient at the time of diagnosis.\" To handle categorical features, we apply one-hot encoding and include specific descriptions for each category. This allows the LLM to generate distinct attribution scores for individual categorical levels rather than the overall feature."}, {"title": "Score Generation Protocol", "content": "We implement a structured scoring protocol by instructing The LLM to assign an integer score between -10 and 10 for each feature, creating a standardized scale of feature importance. Following chain-of-thought methodologies [51], we incorporate prompts that require explicit reasoning before score assignment. This approach has demonstrated superior performance in complex reasoning tasks and improves the interpretability of the scoring process. The complete prompt template used for score generation is provided in Appendix 1.1"}, {"title": "Score Extraction and Aggregation", "content": "To transform the LLM's textual responses into structured numerical data we employ a secondary LLM instance configured for function calling to extract numerical scores from the primary LLM's reasoning and output. This parser converts the semi-structured scores into a standardized list format suitable for downstream processing.\nTo mitigate potential variability in LLM outputs, we generate scores multiple times (\\(N_{estimates}\\)) and compute their mean. This ensemble approach produces the final LLM-provided feature-wise attribution vector \\((S_{LLM})\\), enhancing the stability and reliability of the attribution signal.\nThis methodology creates a robust framework for leveraging LLM capabilities to generate informative feature attribution scores that can effectively guide downstream predictive models during their training process."}, {"title": "4 Experiments", "content": "We conducted a comprehensive evaluation of LAAT across diverse tabular datasets, examining its performance in few-shot learning contexts and scenarios where significant bias was present in the training data. Furthermore, we performed supplementary experiments to investigate the impact of various hyperparameter configurations on the proposed methodology."}, {"title": "4.1 Few-shot Learning", "content": "While large volumes of data are readily available in many domains, expert-labeled data remains scarce in fields requiring specialized knowledge, such as"}, {"title": "4.2 Learning on Biased Data", "content": "Many real-world datasets exhibit varying degrees of bias, which can negatively impact machine learning models by introducing misleading correlations. For instance, a model trained on historical recruitment data may inherit biases related to gender and job roles [10]. Similarly, systemic biases in criminal justice datasets often lead to strong correlations between demographic attributes and outcomes,"}, {"title": "4.3 Hyperparameter Analysis", "content": "We investigate the impact of varying the regularization factor \\(\\gamma\\) and the number of importance score estimations on the final performance of LAAT models. Specifically, we assess the performance of a logistic regression model trained using LAAT alignment with importance scores provided by GPT-40 mini. The"}, {"title": "5 Conclusion", "content": "We introduce Large Language Model Attribution Aligned Training (LAAT), a novel approach that leverages importance scores inferred by large language models (LLMs) as local attribution guides. This method effectively harnesses the generalization capabilities of LLMs while employing simple, traditional machine learning models. Although gradient boosting trees have traditionally dominated tabular data learning, LAAT significantly outperforms existing methods in few-shot learning and biased dataset scenarios. Notably, it surpasses FeatLLM, the current state-of-the-art LLM-based few-shot learning approach.\nBeyond its standalone effectiveness, LAAT is highly versatile and can be used in conjunction with more complex models, provided they are differentiable\u2014for example, TabPFN. Additionally, it can be combined with feature preprocessing techniques such as FeatLLM. Given the interpretability of LLM-derived importance scores, LAAT could be incorporated into interactive, chat-based interfaces, enabling human experts to refine these scores before finalizing them. This human-in-the-loop refinement could further enhance model performance"}]}