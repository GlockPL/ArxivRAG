{"title": "Beyond Scores: A Modular RAG-Based System for Automatic Short Answer Scoring with Feedback", "authors": ["MENNA FATEEN", "BO WANG", "TSUNENORI MINE"], "abstract": "Automatic short answer scoring (ASAS) helps reduce the grading burden on educators but often lacks detailed, explainable feedback. Existing methods in ASAS with feedback (ASAS-F) rely on fine-tuning language models with limited datasets, which is resource-intensive and struggles to generalize across contexts. Recent approaches using large language models (LLMs) have focused on scoring without extensive fine-tuning. However, they often rely heavily on prompt engineering and either fail to generate elaborated feedback or do not adequately evaluate it. In this paper, we propose a modular retrieval augmented generation based ASAS-F system that scores answers and generates feedback in strict zero-shot and few-shot learning scenarios. We design our system to be adaptable to various educational tasks without extensive prompt engineering using an automatic prompt generation framework. Results show an improvement in scoring accuracy by 9% on unseen questions compared to fine-tuning, offering a scalable and cost-effective solution.", "sections": [{"title": "I. INTRODUCTION", "content": "Feedback plays a critical role in student learning. Extensive research demonstrates that detailed and timely feedback significantly improves student performance by guiding learning and correcting misunderstandings [1], [2]. However, providing such elaborated feedback, especially for short answer questions, is a time-consuming and labor-intensive task for educators, particularly when managing large numbers of students. To address this, automatic short answer scoring (ASAS) systems have been developed to quickly assess student responses [3], [4]. Despite their utility, ASAS systems primarily focus on scoring correctness, offering holistic scores without the detailed feedback students need to understand their mistakes [5].\nCurrent ASAS systems are typically trained on classification or regression tasks, where the objective is to assign a score to a student's answer. While effective in evaluating answers, these systems lack the explainability needed for two key purposes: (1) helping students understand the reasoning behind their errors and the feedback given and (2) fostering trust in the scoring system [6], [7]. Without this granularity, students miss opportunities to improve, and educators may be reluctant to adopt automated grading systems.\nIn response to these limitations, recent research has focused on developing ASAS systems with feedback (ASAS-F) that not only score student answers but also generate detailed feedback [8]\u2013[10]. These systems aim to provide students with actionable feedback that explains the reasoning behind their scores, helping them identify and correct their mistakes. Existing ASAS-F systems rely on resource-intensive fine-tuning with limited datasets, specific tasks [11], [12] or complex prompt engineering, all of which may not generalize well across contexts. Moreover, evaluating the quality of the textual feedback is usually only done using traditional statistical metrics, which do not capture the main aspects of quality, such as accuracy and clarity.\nTo overcome these challenges, we introduce a novel ASAS-F system that leverages large language models (LLMs) within a Retrieval-Augmented Generation (RAG) [13] framework. Inspired by similarity-based scoring [14], our method retrieves the most similar answers from a short answer scoring feedback dataset using the ColBERT retrieval [15] model. These retrieved answers serve as few-shot examples, allowing LLMs to generate both accurate scores and detailed feedback.\nOur study explores the following research questions:\n\u2022 RQ1: How does the performance of our modular ASAS-F system compare to state-of-the-art models in auto-"}, {"title": "II. RELATED WORK", "content": "A. AUTOMATIC SHORT ANSWER SCORING\nAutomatic scoring has been studied extensively in the field of natural language processing (NLP) and educational technology [5], [16]\u2013[18]. ASAS systems aim to grade students' short answers automatically, offering immediate feedback, reducing teachers' workload, and streamlining the grading process.\nTraditional ASAS systems are often rule-based and rely on handcrafted features and heuristics to score short answers. While these systems are effective and have shown promising results on benchmark datasets, they require significant manual effort to develop [4], [19]. With the rise of deep learning, neural network-based and Transformer-based ASAS systems have been introduced, which automatically learn features from data and outperform traditional rule-based systems [3], [20].\nAmong the newer methods, instance-based approaches have gained traction. These approaches involve using pre-trained models to map student responses directly to scores by pooling token representations across model layers [21]. These methods capture the context and semantics of input text, enabling them to outperform traditional rule-based systems.\nHowever, they often require large amounts of labeled data to achieve high performance and struggle in few-shot settings where data is limited. Comparisons between instance-based and similarity-based approaches have shown the latter to be more effective in zero-shot settings [14], [22], [23]. Additionally, recent advancements in dense information retrieval have proven effective in automatic essay scoring tasks [24]. Building on these insights, our work introduces a similarity-based approach using retrieval-augmented generation to enhance the performance of ASAS-F systems.\nB. ASAS USING GENERATIVE MODELS\nDespite the success of neural network-based ASAS systems, many of the proposed models are computationally expensive and require large amounts of data to achieve good performance. In many real-world scenarios, labeled data for training ASAS systems is scarce, making it challenging to train these models.\nFew studies have explored the use of large language models (LLMs) for ASAS tasks. One study investigated the usage of LLMs for ASAS, focusing on zero-shot and few-shot settings across three diverse datasets [25]. The results showed that LLMs were able to achieve strong performance on tasks involving general knowledge questions but struggled with questions that required domain-specific knowledge or complicated reasoning.\nAnother study explored the use of proprietary LLMs, namely GPT3.5 and GPT4, for ASAS tasks [26], [27]. The findings indicated that while these models showed promise, their performance was negatively correlated with the length of student answers.\nAdditionally, research assessed the possibility of using ChatGPT (GPT-3.5) for automated grading [9]. The results showed a low correlation between the scores given by ChatGPT and human graders, with ChatGPT tending to give middle scores more frequently despite strong variation in student answers. Moreover, the study found that minor changes in answers could lead to significant differences in the scores given by ChatGPT.\nC. ASAS WITH FEEDBACK\nWhile scores provide a general overview of the quality of the answer, they lack the granularity and explainability that students need to improve their answers. Elaborated feedback would also increase the trustworthiness of the scoring system.\nThe Short Answer Feedback (SAF) dataset [10] includes short answer questions in communication network topics, each with a reference answer, given score, label, and content-focused elaborated feedback. The SAF dataset provides a benchmark for evaluating ASAS systems that generate feedback, enabling the development of more informative and actionable systems. It also established a baseline for ASAS systems using the T5 model fine-tuned on the SAF dataset, demonstrating the feasibility of generating feedback for short answers."}, {"title": "III. METHODOLOGY", "content": "In this section, we present our approach for building the ASAS-F system using LLMs and outline the methodologies proposed for different scenarios, including zero-shot ASAS-F, few-shot ASAS-F with automatic optimization and few-shot ASAS-F using RAG.\nA. PROBLEM FORMULATION\nTraditional ASAS systems typically use classification or regression models to score student answers, focusing solely on assigning a numerical score. While effective for grading, these systems lack the ability to provide detailed feedback or explain the rationale behind a score, limiting their usefulness for student learning. In addition to such numeric scores and labels, ASAS-F systems additionally generate detailed elaborated feedback [1] that explains the reasoning behind the score.\nFormally, the ASAS-F system can be expressed as:\n$ASAS-F(q, a, s) \\rightarrow (y, l,f)$       (1)\nwhere given a question q, a reference answer a, and a student answer s, the system assigns:\n\u2022 A score y \u2208 [0, 1] (indicating correctness),\n\u2022 A label l \u2208 {correct, incorrect, partially correct},\n\u2022 Feedback f that explains the reasoning behind the score.\nIn a zero-shot ASAS-F setting, the system utilizes the knowledge embedded in pre-trained LLMs to provide scoring and feedback without requiring labeled training data. When labeled data D is available, we explore two few-shot ASAS-F approaches: one using automatic prompt optimization, and another using RAG for enhanced performance.\nB. ASAS-F-Z: ZERO-SHOT ASAS-F\nThe modular zero-shot ASAS-F approach leverages the extensive domain knowledge embedded in pre-trained LLMs to score student answers and provide feedback without requiring any labeled training data. This approach takes advantage of the generalization capabilities of LLMs, which have been trained on vast amounts of text data and possess a broad understanding of language and context.\nOne of the significant challenges in implementing a zero-shot ASAS-F system is effective prompt engineering. Crafting prompts that elicit the desired responses from LLMs can be complex and often involves intricate string manipulation. This process is not only time-consuming but also prone to errors.\nTo address these challenges, we utilize DSPy [30], a framework designed to automate prompt generation and refinement to realize our ASAS-F-Z and ASAS-F-RAG systems as shown in Figure 1.\nWe first define our base inputs and outputs with their corresponding types and build a signature. A signature replaces a hand-written prompt by specifying what a function should do rather than how to do it. In its most basic form, a signature consists of input and output fields, each with a type. To add more control, we add a basic description of what the signature does, i.e., score a student answer and generate feedback. We also add basic scoring criteria to the signature. Instead of manually crafting prompts with different prompt engineering techniques, we use predefined modules in DSPy such as the 'Predictor' module or the 'Chain-Of-Thought' module that can be easily replaced or modified to generate prompts. These modules allows us to automatically generate prompts by processing the input and output fields, generating instructions, and creating a template for the specified signature.\nIn the few-shot setting, which is discussed in Section III-D, an additional 'Retrieve' module is added to retrieve examples from the training data. A code snippet that shows the basic implementation using DSPy to generate prompts for the zero-shot ASAS-F system is given in Appendix A.\nC. ASAS-F-OPT: AUTOMATIC FEW-SHOT OPTIMIZATION WITH DSPY\nIn scenarios where labeled training data is available, optimizing both prompts and few-shot examples can significantly enhance the performance of ASAS systems. This section introduces ASAS-F-Opt, which leverages DSPy for the automatic optimization of prompts and examples within the ASAS-F framework. Unlike ASAS-F-Z, where DSPy was primarily used to establish a modular system and generate prompts, ASAS-F-Opt extends this functionality. It employs a systematic optimization approach to evaluate and refine combinations of prompts and examples based on defined performance metrics.\nWe start by defining a performance metric for model evaluation. Then, we construct a Bayesian surrogate model that samples and assesses various combinations of prompts and examples. Our approach utilizes the MIPROv2 prompt optimization algorithm [31], ensuring that the most effective prompts and examples are identified to enhance scoring and feedback generation."}, {"title": "D. ASAS-F-RAG: FEW-SHOT ASAS-F USING RAG", "content": "In this section, we first introduce a basic similarity-based majority-vote classifier using ColBERT for ASAS-F. We then extend this approach to incorporate RAG, which enhances the generative process by providing the LLM with contextually rich examples.\n1) Similarity-Based Majority-Vote with ColBERT\nMany existing ASAS systems use similarity-based methods to assign scores to student answers [14]. These systems typically compare a student's response to reference answers or high-scoring examples, relying on similarity metrics to determine the score.\nIn our few-shot ASAS-F system, we build upon this approach by not limiting the comparison to a single reference answer. Instead, we retrieve the most similar examples from the training data to enhance the prompt. We hypothesize that this method will guide the LLM to produce feedback that more closely aligns with human evaluations, resulting in more accurate feedback.\nTo implement this, we utilize the ColBERT model [15], which efficiently retrieves contextually relevant examples from the training data. Unlike traditional models that encode the entire input into a single vector, ColBERT encodes the input into a matrix of contextual token-level embeddings. This approach allows us to capture fine-grained similarities between the student's answer and the training examples. Each word in the training example is represented by a BERT-based embedding, and these embeddings can be pre-stored for efficiency.\nThe relevance score between a student's answer s and an example d is calculated using a sum of maximum similarity (MaxSim) operators between the tokens in s and d. For each token in s, ColBERT identifies the most contextually similar token in d and sums these similarities to compute the overall relevance score. An example is considered more relevant if it has tokens that are highly contextually similar to those in the student's answer.\nFormally, a student's answer s is tokenized, prepended with [CLS] and [Q] (query) tokens, and passed through BERT to produce vectors [s1,...,SN]. The example d undergoes a similar process with [CLS] and [D] (document) tokens. A linear layer then adjusts the output size, and the student answer vectors and example vectors are normalized to unit length, yielding Es and Ed, the final vector sequences. The ColBERT score is computed as:\n$score(s, d) = \\sum_{i=1}^{N} max_{j=1} (E_{s_i}; E_{d_j})$    (2)\nAs a preliminary experiment, we assess the effectiveness of using the ColBERT retriever by employing a similarity-based majority-vote approach to classify student answers. This approach involves two key steps: retrieval and majority-vote classification. Initially, ColBERT encodes the student's answer and the training examples into embeddings and retrieves the top k most similar examples based on these embed-"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. DATASET\nWe evaluate our system on the Short Answer Feedback (SAF) Dataset [10]. The SAF dataset consists of short answer questions in communication network topics, each with a reference answer, given score, label and conent-focused elaborated feedback. There are no other datasets, based on our review, that provide such feedback for short answers. The dataset is split into training (70%), unseen answers (UA) (12%) and unseen questions (UA) (18%). The test split of UA includes new answers to the existing training questions, while the UQ split introduces novel questions. However, it is important to note that in our approach, both splits are considered novel or unseen questions, as we do not fine-tune or train the model on the training split. Even in the few-shot setting, we utilize less than 0.5% of the training data as the provided examples.\nB. EVALUATION METRICS\n1) Scoring Metrics\nTo evaluate the performance of our ASAS-F system in terms of scoring, we used two key metrics for the classification of the generated label: accuracy and macro-averaged F1 score. These metrics assess how well the system assigns correct labels (e.g., correct, incorrect, partially correct) to student answers. For the generated numeric score, we employed Root Mean Squared Error (RMSE) to measure the difference between the predicted scores and the actual scores. These metrics provide a solid foundation for assessing the system's ability to deliver accurate and reliable scores.\n2) Feedback Evaluation\nEvaluating the quality of generated feedback is more complex due to its subjective nature and the need to verify accuracy. We approached this evaluation using both automated metrics and human assessments."}, {"title": "V. RESULTS", "content": "A. SCORING PERFORMANCE ANALYSIS\n1) Similarity-Based Majority-Vote with ColBERT\nWe evaluate the performance of the retrieval-based majority-vote classification approach on accuracy, F1 and RMSE. This serves as a preliminary experiment.  The results indicate that the ColBERT-based majority-vote classifier outperforms the majority class baseline across all metrics showing the efficiency of using our refined approach."}, {"title": "2) ASAS-F-Z", "content": ""}, {"title": "3) ASAS-F-Opt", "content": "In ASAS-F-Opt, we aim to enhance the model's performance by refining the selection and utilization of examples. While the ASAS-F-RAG system dynamically retrieves relevant examples to inform responses, ASAS-F-Opt focuses on optimizing how these examples are integrated into the model's processing. This approach seeks to ensure that the selected few-shot examples are maximally effective in improving accuracy. We use the bigger Llama3:70b model to refine the prompts and few-shot examples, and utilize the Mistral:7b for generating the outputs during testing since it was able to outperform in ASAS-F-Z.\nThe results for the UA split were below expectations, with accuracy of 0.627, F1 Score of 0.41, and RMSE of 0.309. Similarly, in the UQ split, the metrics were accuracy of 0.643, F1 Score of 0.668, and RMSE of 0.317. These results were notably inferior to the performance of ASAS-F-Z using Mistral:7b. This outcome suggests that the automatic few-shot optimization method may not be effective in enhancing the model's performance in this particular scenario. The use of pre-defined, specified few-shot examples may not always be optimal, indicating that automatic optimization strategies require further refinement. This highlights the complexity of automatic optimization and the need for a more nuanced approach to designing these strategies to better align with the requirements of the ASAS-F system."}, {"title": "4) ASAS-F-RAG", "content": "To address the limitations shown by the ASAS-F-Z system especially in the UA split and the low performance shown by ASAS-F-Opt, we explore the ASAS-F-RAG system. This system focuses on automatically retrieving the most similar answers from existing data to augment the prompts used to improve model performance. This indicates that larger models with more examples can leverage their extensive training to provide more accurate predictions in the few-shot setting. The other models also exhibited a similar trend.\nConversely, in the UQ split, the performance improvements with additional examples are less consistent. Notably, the Llama3:8b and Mixtral:8x22b models did not follow the general trend of improved performance with more examples. The Llama3:70b model, while still competitive, did not maintain its leading position as it did in the UA split. Here, the zero-shot Llama3:70b model actually outperformed the few-shot models, suggesting that the examples used in the few-shot setting might not be as effective when dealing with questions that differ significantly.\nThe discrepancies in performance between the two splits can be attributed to the nature of the retrieved examples. In the unseen questions split, the examples retrieved were from different questions, which may have introduced variability that impacted the model's ability to generalize effectively. This variability might explain why the performance with more examples did not consistently improve and why the zero-shot model performed comparably or better in some cases."}, {"title": "B. FEEDBACK QUALITY ANALYSIS", "content": "1) Statistical Analysis\n presents the evaluation of feedback quality on the SAF dataset using SacreBLEU [36], ROUGE-2 [37], and BERTScore [38] metrics. It is evident that the baseline models were able to outperform both the zero-shot and few-shot ASAS-F systems in all metrics. However, the baseline models due to finetuning have been reported to often copy common phrases from the training data. Looking at the second-best performing models, we can see that the ASAS-F-RAG outperforms the ASAS-F-Z in both splits. This suggests that incorporating a small number of labeled examples can improve the quality of the generated feedback in terms of consistency with the reference feedback. However, increasing the number of examples beyond a certain threshold does not necessarily lead to higher similarity to the reference feedback. This can be seen in the UQ split where the second-best performing models used 3 examples.\n2) Human Evaluation\nTo ensure the quality of the feedback generated by our system, we conducted a human evaluation with five expert teachers in Information Technology and Networks. The raters assessed 108 randomly selected samples from 12 models, focusing on accuracy and clarity. Feedback was rated on a 5-point Likert scale ranging from -2 to 2, where -2 indicated strongly inaccurate or unclear feedback, 0 indicated neutral feedback, and 2 indicated strongly accurate or clear feedback. Among the models evaluated, the Llama3:70b model achieved the highest average accuracy and clarity scores in both the UA and UQ splits. The Mistral:7b model followed closely, showing competitive performance. However, a general trend of decreasing accuracy and clarity with an increasing number of examples was observed across all models.\nAs a post hoc analysis, we merged the strongly accurate and accurate categories and the strongly inaccurate and inaccurate categories to create a 3-point scale. This adjustment led to an increase in the Krippendorff's alpha to 0.91 for accuracy and 0.89 for clarity for the no-response answers. For the remaining answers, the Krippendorff's alpha increased to 0.60 for accuracy and 0.64 for clarity, indicating a moderate level of agreement among raters. This suggests that the original scale was too granular and that a simpler scale may have resulted in higher agreement scores.\nIn the few-shot approach, LLMs often mimicked the style of the provided reference answers or examples as shown in the first example in Figure 7. Many raters commented the feedback that was too brief, lacking sufficient context or explanation. Although the feedback was factually correct, it did not provide the depth needed for comprehensive understanding, which could be critical in educational settings. Conversely, in the zero-shot approach, the generated feedback was often longer and appeared more detailed. However, as the second example in Figure 7 demonstrates, while some of this feedback was labeled as accurate, it was actually incorrect when compared to the reference answers. This highlights the challenge of calibrating LLMs, as they may present incorrect feedback with high confidence [39], making it difficult to detect when they are hallucinating [40], especially when evaluating the feedback in isolation."}, {"title": "VI. DISCUSSION", "content": "In this study, we aimed to answer key research questions regarding the performance of an ASAS-F system in zero-shot and few-shot settings. Rather than relying on intricate and labor-intensive prompt engineering to optimize performance for specific datasets, we adopted a modular design for our system using DSPy. We explored two main methods for selecting few-shot examples ASAS-F-RAG and ASAS-F-Opt in comparison with the simpler ASAS-F-Z and fine-tuned baselines. Our evaluation focused on the system's ability to score student answers that align with the reference answers and generate feedback that is accurate and clear."}, {"title": "VII. CONCLUSION", "content": "This study provides a comprehensive evaluation of an ASAS-F system in both zero-shot and few-shot settings. We introduced a novel RAG approach, leveraging ColBERT for automatic short answer scoring with feedback (ASAS-F). Our results demonstrated that the few-shot approach not only outperformed the more computationally expensive fine-tuned baselines in scoring accuracy but also performed effectively in zero-shot settings, especially on unseen questions, highlighting the robustness of our method for automatic scoring.\nHowever, when evaluating feedback quality, we observed an inverse relationship between the number of examples provided and the quality of the feedback generated, as assessed by human experts. This suggests that while our system excels in scoring, generating high-quality, pedagogically sound feedback remains challenging, particularly due to the subjective nature of feedback evaluation. Additionally, the inherent complexity of feedback generation in educational settings, combined with issues like model calibration and potential hallucinations, highlights the difficulty of evaluating feedback produced by large language models.\nDespite these challenges, our approach\u2014combining open-source LLMs with powerful retrieval models in a modular, computationally efficient framework\u2014shows strong potential to enhance the accuracy and efficiency of ASAS systems. Future research should focus on improving feedback quality, exploring model calibration techniques, and developing more reliable methods for evaluating feedback in educational contexts."}, {"title": "APPENDIX A DSPY CODE SNIPPETS", "content": "In this section, we provide code snippets of the DSPy implementation of the ASAS-F system. The code snippets for ASAS-F-Z, ASAS-F-Opt and ASAS-F-RAG are shown in Figures 8, 9, and 10, respectively."}, {"title": "APPENDIX B OUTPUT GENERATION EXAMPLES", "content": "This section provides a comparison of feedback samples generated by the baseline models and our system. Tables 7,"}, {"title": "APPENDIX C TYPED PREDICTOR ERROR ANALYSIS", "content": "In our implementation of the ASAS-F system, we primarily relied on typed predictors generated through the DSPy framework. Typed predictors are advantageous because they enforce type constraints on both input and output, ensuring that the model adheres to the predefined structure and can handle complex, multi-layered data. This structure enhances the reliability and consistency of the model's predictions.\nHowever, during the evaluation phase, we observed an average of 4.13% errors across all models due to formatting issues that led to no output being generated by the typed predictors. These errors arise from strict type constraints, where slight deviations in input formatting\u2014such as unexpected characters or variations in text structure\u2014caused the predictor to fail.\nTo address this issue and ensure comprehensive evaluation, we implemented a fallback mechanism using a normal predictor without typed constraints. In cases where the typed predictor failed to generate an output, the normal predictor was used to obtain the predictions. This approach allowed"}]}