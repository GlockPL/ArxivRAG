{"title": "ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation", "authors": ["Zhiyuan Ma", "Yuxiang Wei", "Yabin Zhang", "Xiangyu Zhu", "Zhen Lei", "Lei Zhang"], "abstract": "By leveraging the text-to-image diffusion priors, score distillation can synthesize 3D contents without paired text-3D training data. Instead of spending hours of online optimization per text prompt, recent studies have been focused on learning a text-to-3D generative network for amortizing multiple text-3D relations, which can synthesize 3D contents in seconds. However, existing score distillation methods are hard to scale up to a large amount of text prompts due to the difficulties in aligning pretrained diffusion prior with the distribution of rendered images from various text prompts. Current state-of-the-arts such as Variational Score Distillation finetune the pretrained diffusion model to minimize the noise prediction error so as to align the distributions, which are however unstable to train and will impair the model's comprehension capability to numerous text prompts. Based on the observation that the diffusion models tend to have lower noise prediction errors at earlier timesteps, we propose Asynchronous Score Distillation (ASD), which minimizes the noise prediction error by shifting the diffusion timestep to earlier ones. ASD is stable to train and can scale up to 100k prompts. It reduces the noise prediction error without changing the weights of pre-trained diffusion model, thus keeping its strong comprehension capability to prompts. We conduct extensive experiments across different 2D diffusion models, including Stable Diffusion and MVDream, and text-to-3D generators, including Hyper-iNGP, 3DConv-Net and Triplane-Transformer. The results demonstrate ASD's effectiveness in stable 3D generator training, high-quality 3D content synthesis, and its superior prompt-consistency, especially under large prompt corpus.", "sections": [{"title": "1 Introduction", "content": "Text-to-3D aims to generate realistic 3D contents from the given textual descriptions, which is particularly useful in many applications such as virtual reality and game design. The main challenge of this task, however, lies in how to generate high-quality 3D contents conditioned on the abstract and diverse textual descriptions. Many existing text-to-3D methods are optimization-based ones, which distill the guidance from the powerful pretrained text-to-image diffusion models via score distillation. In general, these methods employ the KL divergence to reduce the discrepancy between the distribution of rendered images and the desired image distribution embedded in the 2D diffusion prior, while they differ in how to use the pretrained diffusion prior to model the distribution of rendered images. Extensive efforts have been made to explore prompt-specific optimization of various 3D representations, including implicit radiance fields, explicit radiance fields, DmTets and 3D Gaussians. Typically, tens of minutes to hours are needed to optimize a single 3D representation for one prompt to achieve the desired result.\nCompared to the aforementioned optimization-based text-to-3D methods, learning-based methods can largely reduce the compu-"}, {"title": "2 Literature Review", "content": ""}, {"title": "2.1 Text-to-3D with Score Distillation", "content": "Text-to-3D takes text description, a.k.a. text prompt y, as input, and outputs 3D representation @ that renders high-fidelity images at any camera view \u03c0. Thanks to the powerful text-to-image diffusion models, we can optimize e to align with y by computing the objective L(x, y) on the rendered image x = g(0, \u03c0) from camera view \u03c0. Through differential rendering, 0 can be updated with the gradient $\\nabla_{\\theta}L(0,y) = \\frac{\\partial L(x,y)}{\\partial x} \\frac{dx}{d \\theta}$. This technique is generally termed as score distillation. Unlike data-driven techniques, score distillation approaches can produce high-quality 3D content without the need for 3D training datasets."}, {"title": "Prompt-Specific Text-to-3D.", "content": "Existing score distillation methods were originally developed to output a single 3D result 0 for a single text prompt y via online optimization: $ming \u0395\u03c0,x=g(0,\u3160) [L(x, y)]$. The utilized 3D representations, e.g., NeRF , DmTet , and 3D Gaussian , are not designed to render scenes from varying text prompts. Therefore, the optimization has to be conducted again for newly provided text prompts. The optimization process typically costs tens of minutes to hours."}, {"title": "Prompt-Amortized Text-to-3D.", "content": "To mitigate the computational costs in prompt-specific methods, recent studies have attempted to use score distillation to train a text-to-3D generator 0 = G(y), aiming to generate multiple 3D representations from a set of text prompts Sy = {y}. These methods can generate 3D results from queried text prompt in seconds. As proposed by ATT3D , the 3D generator training is performed by minimizing $ming \u0395\u03c0,y\u2208Sy,x=g(G(y),\u03c0) [L(x, y)]$ over all text prompts. Unlike data-driven approaches, score distillation bypasses the scarcity of text-3D data pairs because the 2D diffusion prior can offer the guidance to align the 3D output with the input text prompt. However, its application is currently restricted to training the 3D generator within a limited range of text prompts."}, {"title": "2.2 Representative Score Distillation Methods", "content": "Denote by & the 2D diffusion prior and by $p_\\phi^t(x | y)$ the text-conditioned image distribution embedded within 4, the objectives of most existing score distillation methods can be generally concluded as minimizing the objective\n$L(0,y) = \u0395\u03c0,\u03c4,\u20ac,x=g(0,\u03c0) [w(t) D_{KL} (q_t (X_t | \u03c0) ||p_\\phi^t (X_t | y*))]$:\nwhere DKL denotes KL divergence, $q_t (x_t | \u03c0)$ denotes the distribution of images x rendered at camera view rat diffusion timestep t , and the same for $p_\\phi^t (x_t | y)$. w(t) is a timestep-dependent weight . y* denotes the view-dependent strategy or view-awareness to prompt the different camera views . To minimize"}, {"title": "ScaleDreamer", "content": "this objective, the gradient w.r.t. O can be calculated as per [72]:\n$\\nabla_{\\theta}L(0,y) = \u0395\u03c0,\u03c4\u03b5 w(t) [-\\nabla_{Xt} log p_\\phi^t (X_t|y^*)- (-\\sigma_t\\nabla_{Xt} log q_t^\\theta(X_t|\u03c0))\\frac{dx}{d\\theta}]$\nwhere the first term $ - \\sigma_t\\nabla_{Xt} log p_\\phi^t (X_t | y^*)$ corresponds to the score function of the desired image distribution, and it can be achieved by predicting the noise\n$\u2208 \\sim N (0, I)$ in the noisy image $x_t = a_tx + \\sigma_t\\epsilon$ using the pretrained 2D diffusion model $e_\\phi (x_t;t, y^*)$ . Existing score distillation methods mainly differ in how to model $-\\sigma_t\\nabla_{Xt} log q_t^\\theta(X_t|\u03c0)$, which corresponds to the score function of the distribution of rendered images $q^\\theta(x | \u03c0)$. We denote this term in Eq. 1 as $e_\\theta (x_t; t, \u03c0, y)$ in the following context, since it represents a diffusion model that corresponds to \u03b8.\nThe objective of Score Distillation Sampling (SDS) [48] is $V_{\\theta}L_{SDS}(0,y)\n= \u0395\u03c0,\u03c4,\u03b5 [\u03c9(t) (\\epsilon_\\phi (x_t; t, y) \u2013 \\epsilon)]$, which approximates the term $e_\\theta (x_t; t, \u03c0, y)$ in Eq. 1 as the ground-truth noise \u20ac. That is, SDS assumes that $q^\\theta(x |\u03c0)$ ad-\nheres to a Dirac distribution $\u03b4 (x \u2212 g (\u03b8, \u03c0))$, which is characterized by a non-zero density at the singular point of x = g(\u03b8,\u03c0) and zero density everywhere else. However, updating e under the Dirac distribution might be troublesome . We may need to set the CFG (Classifier Free Guidance) as high as 100 for model convergence, which will produce excessively large gradients and lead to unstable optimization. This problem is alleviated by Classifier Score Distillation (CSD) , which uses the classifier component in SDS as\nthe objective: $V_{\\theta}L_{CSD}(0,y) = \u0395\u03c0,t,e [w(t) (\\epsilon_\\phi (x_t; t, y) - \\epsilon_\\phi (xt; t))]$. CSD\ncan be regraded as straightforwardly using the unconditional term of the diffusion prior $\u2208_\\phi (xt; t)$ to represent $e_\\theta (x_t; t, \u03c0,y)$ in Eq. 1. Unfortunately, in the\ncase of prompt-amortized training, this term may not provide effective gradient, because $\u2208_\\phi (xt;t)$ is unconditional to the provided text-prompts. In con-\ntrast, Variational Score Distillation (VSD) models $e_\\theta (x_t; t, \u03c0,y)$ with\nanother text-aware diffusion model $\u2208_\\phi^\\prime (xt; t, \u03c0,y)$, leading to $V_{\\theta}L_{VSD}(0,y) =\n\u0395\u03c0,\u03c4,\u03b5 [\u03c9(t) (\\epsilon_\\phi (x_t; t, y*) \u2013 \\epsilon_\\phi^\\prime (x_t; t, \u03c0,y))]$, where $\u2208_\\phi^\\prime (xt; t, \u03c0,y)$ is achieved\nby finetuning the pretrained 2D diffusion prior $\u2208_\\phi (xt; t, y^*)$ to align with the\nrendered image distribution $q^\\theta (x | \u03c0)$ via parameter efficient adaptation . In\npractice, this is conducted by alternatively optimizing 0 and finetuning & with\nthe noise prediction objective $||\u2208 (xt; t, y) \u2013 \u20ac||^2$ such that:\n$\u0395\u03c0,\u03c4,\u03b5 [|E_\\phi^\\prime (xt; t, \u03c0,y) \u2013 \u20ac||^2] < \u0395\u03c0,\u03c4,\u03b5 [||E_\\phi (Xt; t, y*) \u2013 \u20ac||^2]$\nThe above equation reveals that a better alignment with the distribution\nof $q^\\theta(x|\u03c0)$ can be achieved by a more accurate noise prediction.\nWhile VSD achieves state-of-the-art results in prompt-specific text-to-3D , it changes the diffusion prior's parameters by alternately optimizing 0 and finetuning 6. This forms a bi-level optimization, known to be problematic in generative adversarial training , and may be troublesome for training prompt-amortized text-to-3D models, because the change of pre-trained diffusion model"}, {"title": "Asynchronous Score Distillation (ASD)", "content": ""}, {"title": "3.1 Objective of ASD", "content": "From the above discussions in Sec. 2.2, it can be seen that one key issue in VSD is to minimize the noise prediction error so that the model output can be aligned with the desired distribution of rendered images. VSD achieves this goal via finetuning the pre-trained 2D diffusion model, which however sacrifices its comprehension capability on text prompts. One interesting question is: can we minimize the noise prediction error without changing the pre-trained diffusion network weights? Fortunately, we find that this is possible and in this section we present a new objective function to achieve this goal.\nRecall that diffusion models solve the stochastic differential equation via reversing the noise added along different stages, a.k.a. diffusion timestep t \u2208\n{Tmax,...,Tmin} via $x_t = a_tx + \\sigma_t\\epsilon$ . The influence of the noise \u2208 ~ N(0, I)\non the image x is incrementally reduced as the process progresses from the initial\ntimestep Tmax to the final timestep Tmin, which is controlled by the scalars at\nand \u03c3\u03c4. Consequently, the diffusion model's noise prediction accuracy will vary\nwith the timestep t, at which the identical noise e is added. To evaluate this,\nwe consider a diffusion model with fixed image x, noise e and condition y, but\nvaried timestep t. We denote such a diffusion model as e(t) and explore how its\nprediction error, denoted by $e(t)=||\u2208(t) \u2013 \u20ac||^2$, changes with t.\nThe model e(t) can be a pre-trained 2D diffusion model (such as Stable\nDiffusion . We denote by EPT(t) such a model, and investigate the behaviour\nof its noise prediction error, denoted by ePT(t). In Fig. 2, we plot the curve (i.e.,\nthe blue colored curve) of ePT(t) versus t. We use a corpus with 15 text prompts\nfrom Magic3D to draw this curve. For each prompt y, we generate 16 images\nwith VSD . Then for each image x, we apply one instance of Gaussian noise e\nand conduct a single diffusion step with 100 distinct timesteps. The average noise\nreconstruction error is then calculated for these timesteps across all prompts and\nimages. We can see from the curve of epy(t) that earlier diffusion timesteps (e.g.,\ntimestep 600) will have lower noise prediction error than later timesteps (e.g.,"}, {"title": "ScaleDreamer", "content": "timestep 200). Such a trend holds for almost every image sample x and noise sample e because the well-trained diffusion model is frozen in our case. Since the noise prediction error declines from Tmin (i.e., late diffusion timestep) to Tmax (i.e., early diffusion timestep), we can conclude that for a given timestep t and a timestep shift 0 < \u2206t < Tmax - t, the following inequality holds:\n$\u0395\u03c0,\u03c4,\u20ac [[[E_\\phi (Xt+\u2206t; t + \u2206t, y*) \u2013 \u20ac||^2] < \u0395\u03c0,\u03c4,\u20ac [||E_\\phi (xt; t, y*) - \u20ac||^2]$\nwhich implies that more accurate noise predictions can be achieved at earlier diffusion timesteps.\nThe above property of diffusion models has also been observed by Yang et al. , who indicated that as the timestep shifts from Tmax towards Tmin, the variance in noise prediction increases, as evidenced by the rising Lipschitz constants, which suggests an increased instability in noise prediction and larger noise prediction errors. Such a behavior can be observed in both e-prediction and v-prediction models, as well as in 2D and 3D diffusion models (please refer to Sec. A.1 for details). This can be intuitively explained as follows. When t \u2192 Tmax, Xt = AtX + \u03c3\u03c4\u03b5 \u2192 \u20ac, then it is easier to achieve $\u2208_\\phi (xt; t, y*) \u2248 \u20ac$ because the model can manage to copy the input as the output.\nThe similarity between Eq. 3 and the fine-tuning objective of VSD in Eq. 2 inspires us to investigate whether simply shifting earlier the timestep could fulfill the fine-tuning requirements of VSD without modifying the pre-trained 2D diffusion network parameters. Specifically, we employ the pretrained 2D diffusion model with shifted timestep to approximate the diffusion model of rendered im-ages in Eq. 1 as $e_\\theta (xt; t, \u03c0,y) = \u2208_\\phi (Xt+\u2206t; t + \u2206t, y*)$, resulting in the following"}, {"title": "Asynchronous Score Distillation (ASD) objective function:", "content": "$\\nabla_{\\theta}L_{ASD}(0,y) = \u0395\u03c0,\u03c4,\u03b5 w(t) (\\epsilon_\\phi (xt;t, y*) - \\epsilon_\\phi (Xt+\u2206t;t + \u2206t, y*)) \\frac{dx}{d\\theta}$.\nWe can see that rather than iteratively fine-tuning the diffusion network as in VSD, ASD achieves similar goal by shifting the timestep t with an interval At in each step, which is much more efficient. One key variable introduced in ASD is the timestep shift \u2206t, which will be discussed in the next subsection."}, {"title": "3.2 The Setting of Timestep Shift At", "content": "Before discussing how to set the timestep shift At, let's plot another curve, i.e., the noise prediction error of $\u0454_\\theta (xt; t, \u03c0, y)$ w.r.t. timestep t. Actually, in the pro-\ncess of generating a with VSD, we will have the fine-tuned model $e_\\phi^\\prime (xt; t, \u03c0, y)$\nas the by-product, which is used to represent $\u2208_\\theta (xt; t, \u03c0,y)$ in Eq. 1. Therefore, with fixed x, e and y, the noise prediction error of the fine-tuned diffusion model, denoted by EFT(t), can be calculated as $e_{FT}(t)= ||\u2208_\\phi^\\prime (t) \u2212 \u20ac||^2$.\nThe curve of eFT(t) w.r.t. t (i.e., the yellow curve) is plotted in Fig. 2 by using the same data as in plotting ep\u0442(t). We can see that the curve of EFT(t) is positioned under ept(t) because eFT(t) is obtained by the fine-tuned diffusion model EFT. However, as mentioned in Sec. 2.2, this fine-tuning changes the weights of pre-trained diffusion model and might damage its ability in compre-hending text-image pairs. Therefore, we propose to fix the pre-trained model EPT(t) but shift it to epr(t + \u2206t) to approximate the desired EFT(t). Referring to Fig. 2, we could shift EPT(t) to an earlier timestep to achieve this goal. For example, at timestep to and with a time shift Ato > 0, we can use EpT(to + Ato) to approximate the noise prediction error of EFT(to).\nOn the other hand, the magnitude of At will vary with t. Let's come to another timestep t\u2081 in Fig. 2, where t\u2081 is earlier than to. Because the decreasing speeds of both epy and EFT will be reduced with t going to Tmax, the magnitude\nof At\u2081 will be increased to approximate eFT (t1). In other words, the magnitude\nof At should grow when t goes from Tmin to Tmax. We heuristically set this relationship as At = n(t - Tmin), where \u03b7 \u2208 [0,1] is a hyper-parameter that controls the length of shift range. Finally, it should be pointed out that the curves\nin Fig. 2 will vary a little for different training iterations, rendered images x and\ntext prompts y. Therefore, At should fall into some range S(t). In practice, we\nset At ~ S(t) = U[0, n(t \u2013 Tmin)], which follows a uniform distribution within 0\nand n(t - Tmin). The pseudo-code of ASD is summarized in Alg. 1, which can\nbe applied to both prompt-specific and prompt-amortized text-to-3D tasks."}, {"title": "Experiments", "content": ""}, {"title": "4.1 Experimental Settings", "content": "Comparison Methods. We compare ASD with state-of-the-art score distilla-tion methods, including SDS , CSD and VSD . We adhere to their official codes for training prompt-amortized text-to-3D networks. For example, the CFG values for SDS, CSD and VSD are configured to 100, 1, and 7.5, respectively. In addition, we compare with existing prompt-amortized method ATT3D  (whose code is not released yet) by replicating its reported results.\nImplementation Details. We employ VolSDF to render images from the 3D generators. For Stable Diffusion, we employ SD-v2.1-base for all score distillation methods for fair comparison. As configured in VSD , we set the CFG value as 7.5 for the pre-trained diffusion model in ASD, and 1 for the"}, {"title": "ScaleDreamer", "content": "diffusion model of rendered images. The resolution of rendered images by Hyper-iNGP is set to 256 \u00d7 256, while that of 3DConv-net and Triplane-Transformer is set to 64 \u00d7 64 for GPU memory considerations.\nPrompt Corpus. To thoroughly evalutate the capability of ASD in prompt-amortized text-to-3D synthesis, we employ multiple datasets encompassing a range of text prompt quantities. MG15 includes 15 prompts from Magic3D ; DF415 comprises 415 prompts from DreamFusion ; and AT2520 contains 2520 compositional prompts of animals from ATT3D . DL17k contains 17k compositional prompts of human with daily activities, proposed by . While AT2520 and DL17k provide a larger number of prompts than DF415, the prompt diversity of them is relatively low due to the predefined templates.\nTo test ASD's performance with an even larger scale of prompts, we intro-duce a novel prompt corpus named CP100k. This corpus consists of 100,000 text prompts filtered from the image descriptions collected by Cap3D , which was developed to test text-to-image model performance. To the best of our knowl-edge, it is the first time to evaluate score distillation methods on such a scale of text prompts. Meanwhile, it should be clarified that this work is focused on ex-amining the score distillation performance rather than prompt generalization, so the test prompts share the same distribution as training prompts.\nEvaluation Metrics. We render 120 surrounding view images as the 3D synthesis result from each prompt. Similar to previous text-to-3D works, we compute the CLIP recall, i.e., the classification accuracy by applying CLIP model to the rendered images to predict the correct text prompt, as one performance metric, denoted by \"R@1\". Additionally, we calculate the CLIP text-image similarity between generated images and input prompts as another metric , denoted by \"Sim\"."}, {"title": "4.2 Evaluation Results", "content": "Results with iNGP/Hyper-iNGP as 3D Representation. The iNGP architecture is designed for prompt-specific text-to-3D generation. Hyper-iNGP has the same spatial encoding as iNGP except that the weights of the decoding layer depend on the text prompt. To eliminate the effect caused by architecture difference as much as possible, we adopt iNGP for prompt-specific text-to-3D tasks, and Hyper-iNGP for prompt-amortized tasks. Our experiments are carried out on the MG15 dataset. For prompt-specific tasks, we optimize an individual iNGP for each MG15 prompt; while for the prompt-amortized tasks, we train a single Hyper-iNGP across all MG15 prompts. We also compare our results with ATT3D , which is among the first to apply Hyper-iNGP to prompt-amortized text-to-3D tasks. ATT3D employs SDS for training and uses soft-shading (denoted as * in Tab. 2) for rendering.\nThe qualitative and quantitative results are shown in Fig. 5 and Tab. 2, respectively. We can see that the existing methods suffer from performance decrease when transiting from prompt-specific to prompt-amortized tasks, as evidenced by the decreased CLIP similarity and recall in Tab. 2."}, {"title": "ScaleDreamer", "content": "mentioning that training Hyper-net with SDS requires turning on the spec-tral normalization in the linear layers, otherwise the training will fail due to numerical instability. This observation is consistent with what reported in ATT3D . This is because SDS suffers from large gradient norm (please also refer to Fig. 3 and the discussions therein), which makes Hyper-iNGP hard to converge. As can be seen in Fig. 5, ATT3D results in wrong geometry by using soft shading and SDS for training. For CSD, we see that it fails to optimize the full geometry, as shown by the shrunk peacock in both prompt-amortized and prompt-amortized results. For VSD, it tends to generate content drifts , resulting in repetitive patterns and abnormal geometry. It may fail to gener-ate reasonable contents in both prompt-specific and prompt-amortized tasks. In contrast, our proposed ASD works very stable across the two tasks, yielding not only outstanding quantitative scores but also high quality 3D contents.\nResults with 3DConv-net as 3D Generator. The issues of existing score distillation methods either persist or become more pronounced when replacing Hyper-iNGP to 3DConv-net as the 3D generator. We find that training SDS with 3DConv-net always fails within several thousand iterations, even using spectral or other normalization techniques. This issue stems from that deeper network is more sensitive to large gradients caused by SDS. Therefore, we only compare the results of other methods in Fig. 6. We see that CSD outputs acceptable results on AT2520, but its results on DF415, which has more varied prompts, are consistently smaller than anticipated. Such a phenomenon has been observed"}, {"title": "4.5 Discussions with Data-Driven Methods", "content": "Our proposed method differs from existing data-driven methods in that we do not require any 3D dataset to train the 3D generator. If the test text prompts fall into the training distribution, these supervised data-driven methods may generate better quality outputs than our unsupervised method. However, by leveraging the strong prior information in pre-trained 2D diffusion models, our method has better generalization capability to the test prompts. By using our 3DConv-net trained on DF415 corpus as an example, we compare our results with open-sourced data-driven 3D generators LGM and Shape-E . Fig. 11 shows the qualitative comparison on some text prompt inputs, which are"}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we presented Asynchronous Score Distillation (ASD), a novel score distillation method that can assist 2D diffusion prior in training 3D generators with a scalable size of text prompts. By shifting the diffusion timestep to earlier stages, our ASD can effectively predict the noise prediction error to align the diffusion model with the distribution of rendered images, while preserving the superior text comprehension capability of pre-trained models, thus facilitating stable training with high-fidelity generation results. Our extensive experiments revealed that ASD performed consistently well on datasets of various sizes, being able to manage as much as 100k prompts.\nThough ASD has shown improvements over earlier score distillation ap-proaches, there remain some limitations. For man-made objects that have very regular shapes, such as chairs or airplanes, the performance of our model will lag behind those data-driven methods, which benefit from an abundance of rel-evant data. We foresee opportunities to combine the advantages of data-driven and score distillation methodologies to improve text-to-3D capabilities in a more comprehensive manner in the future research."}, {"title": "6 Acknowledgement", "content": "This work is supported in part by the Beijing Science and Technology Plan Project Z231100005923033, and the InnoHK program."}, {"title": "Appendix", "content": "In this appendix, we provide the following materials:\nSec. A.1: more illustrations of noise prediction error EFT(t) by different dif-fusion models e(t) (referring to Sec. 3.1 and Fig. 2 in the main paper);\nSec. A.2: more 2D toy experiments of different methods (referring to Sec. 3.2 and Fig. 3 in the main paper);\nSec. A.3: more details of 3D generator architectures (referring to Sec. 3.2 and Fig. 4 in the main paper);\nSec. A.4: more corpus details (referring to Sec. 4.1 in the main paper);\nSec. A.5: more implementation details (referring to Sec. 4.1 in the main paper);"}, {"title": "A.1 More Illustrations of Noise Prediction Error", "content": "In this section, we provide more illustrations of the noise prediction error by various pre-trained diffusion models, including the 2D e-prediction model and the v-prediction model , and the 3D diffusion model . We plot the the noise prediction error against timesteps in Fig. 12. For each text prompt displayed at the top of the sub-figures, we use it as the condition to generate 16 samples. We then introduce a single instance of Gaussian noise to each sample and execute one diffusion step at 100 different timesteps. The DDPM is used as the noise scheduler, as done in VSD . The average noise reconstruction error is then calculated over the timesteps and the 16 data samples.\n2D e-prediction diffusion model. The e-prediction model is widely adopted in the field of text-to-3D synthesis . In our tests, we employ the commonly used SD-v2.1-base model . The noise prediction error curves for four prompts sourced from Magic3D are presented in Fig. 12(a), from which we see a clear decrease of noise prediction error with the timestep going from Tmin to Tmax.\n2D v-prediction diffusion model. The v-prediction model, introduced by Salimans et al. , accelerates the generation process by predicting velocity rather than noise. We test this model using the well-known SD-v2.1 with 4 prompts sourced from Magic3D . To calculate the noise prediction error, we convert the velocity predictions into noise predictions . As depicted in Fig. 12(b), the v-prediction model also exhibits reduced prediction errors as the timestep goes from Tmin to Tmax.\n3D diffusion model. Apart from the above 2D diffusion models, we also conduct experiments on a 3D diffusion model DiffTF , which is a 3D gener-ator trained on 3D object datasets . It is configured with e-prediction and performs the diffusion process on tri-plane . As shown in Fig. 12(c), its noise prediction error e(t) also reduces as timestep t increases, which is similar to 2D diffusion models. In particular, e(t) drops rapidly before t = 200. This is mainly caused by the much smaller scale (e.g., 6k 3D objects) of the 3D dataset compared with the 2D datasets (e.g., 2B text-image pairs). Therefore, the network tends to overfit the 3D data with smaller prediction error."}, {"title": "A.2 More 2D Toy Experiments", "content": "To further validate the effectiveness of the introduced timestep interval At in our ASD, we provide more 2D toy experiments in Fig. 13, covering a wild range of subjects, i.e., plants, objects, animals, and scenes.\nFrom Fig. 13, we can see that SDS  and CSD  do not perform very well. SDS generates high-saturation results because of the large CFG , while CSD shows noisy and blurred patterns so that the subjects are difficult to iden-tify. VSD generates good quality results by fine-tuning the 2D diffusion model. However, as we discussed in the main paper, it hurts the 2D diffusion model's comprehension capability to numerous text prompts, leading to mode collapse when the size of text prompts is extended. Without changing the diffusion prior, our proposed ASD can achieve the same high quality results as VSD.\nWe also ablate the setting of At in this experiment. We see that if we set At = 0, it leads to a noisy pattern similar to CSD. By setting it as a fixed interval, e.g., At = \u03b7Tmax, it would result in poor texture or geometry, such as the panda in Fig. 13. By setting At relevant to t as At = n(t \u2013 Tmin), the results can be much improved. Finally, the results are further enhanced by randomly sampling At via \u2206t ~ U . The detailed explanations can be found in Sec. A.1 of the main paper."}, {"title": "A.3 More 3D Generator Architecture Details", "content": "Hyper-iNGP. We replicate the hypernetwork design from ATT3D , inte-grating it with iNGP to achieve prompt-amortized text-to-3D synthesis. As illustrated in Fig. 14, the hypernetwork projects text prompt embeddings into the weights of linear layers. The HashGrid representation encodes sample points independently, which are then transformed by the hypernetwork-parameterized linear layers into prompt-specific color c and density \u03c3. Following ATT3D , another hypernetwork is implemented to create a prompt-specific background. The ray direction is encoded into a separate HashGrid, which is then projected to the background color cbg, facilitating the creation of high-resolution back-grounds. The spectral normalization can be optionally turned on to stabilize the training with SDS .\n3DConv-net. As illustrated in Fig. 14, our 3DConv-net mirrors the Style-GAN2 model , using modulated convolutions to upscale features directed by"}, {"title": "ScaleDreamer", "content": "the latent code w, which is conditioned on Gaussian noise z ~ N(0, 1) and the text prompt embedding as in text-driven 2D GANs . Transitioning from 2D to 3D, we substitute StyleGAN2's components with their 3D alternatives, mod-ulated by w. The network up-samples a 43 dimensional voxel to 1283 dimension. For quicker convergence, we add 3D bias within blocks for processing voxels with the dimension from 83 to 643. Rendering is accomplished by interpolating voxel features to determine the color and density of each point along the rays. A background module is incorporated as well.\nTriple-Transformer. Recently, the Transformer architecture has gained popularity in 3D generation tasks for its scalability, especially in data-driven methods . However, it has not been applied in re-cent score-distillation-based methods yet . In this paper, we conduct experiments to explore the performance of Transformer architecture in score-distillation-based text-to-3D generation. As shown in Fig. 14, we employ 12 Transformer layers, each comprising self-attention, cross-attention, and feed-forward networks. The text prompt is first processed by the CLIP text encoder and then fed into the cross-attention to set the condition. The query embeddings are passed through these layers, and then reshaped and up-sampled to form a triplane, which is an efficient 3D representation .\nRendering. For prompt-specific optimization, we use the volume rendering in NeRF and keep the configuration in prior arts . For prompt-amortized training, we implement VolSDF , which uses 64 sample points for coarse sampling and 256 sample points for fine sampling . We found that keeping the mean absolute deviation fixed to be 30 can achieve good results. We render 64 \u00d7 64 resolution for 3DConv-net and 256 \u00d7 256 for Hyper-iNGP in the whole training period."}]}