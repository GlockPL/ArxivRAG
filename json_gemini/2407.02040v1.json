{"title": "ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation", "authors": ["Zhiyuan Ma", "Yuxiang Wei", "Yabin Zhang", "Xiangyu Zhu", "Zhen Lei", "Lei Zhang"], "abstract": "By leveraging the text-to-image diffusion priors, score distillation can synthesize 3D contents without paired text-3D training data. Instead of spending hours of online optimization per text prompt, recent studies have been focused on learning a text-to-3D generative network for amortizing multiple text-3D relations, which can synthesize 3D contents in seconds. However, existing score distillation methods are hard to scale up to a large amount of text prompts due to the difficulties in aligning pretrained diffusion prior with the distribution of rendered images from various text prompts. Current state-of-the-arts such as Variational Score Distillation finetune the pretrained diffusion model to minimize the noise prediction error so as to align the distributions, which are however unstable to train and will impair the model's comprehension capability to numerous text prompts. Based on the observation that the diffusion models tend to have lower noise prediction errors at earlier timesteps, we propose Asynchronous Score Distillation (ASD), which minimizes the noise prediction error by shifting the diffusion timestep to earlier ones. ASD is stable to train and can scale up to 100k prompts. It reduces the noise prediction error without changing the weights of pre-trained diffusion model, thus keeping its strong comprehension capability to prompts. We conduct extensive experiments across different 2D diffusion models, including Stable Diffusion and MVDream, and text-to-3D generators, including Hyper-iNGP, 3DConv-Net and Triplane-Transformer. The results demonstrate ASD's effectiveness in stable 3D generator training, high-quality 3D content synthesis, and its superior prompt-consistency, especially under large prompt corpus.", "sections": [{"title": "1 Introduction", "content": "Text-to-3D aims to generate realistic 3D contents from the given textual descriptions [48], which is particularly useful in many applications such as virtual reality [75] and game design [28]. The main challenge of this task, however, lies in how to generate high-quality 3D contents conditioned on the abstract and diverse textual descriptions. Many existing text-to-3D methods [14, 15, 32-35, 39, 42, 44, 48, 50, 71, 72, 92] are optimization-based ones, which distill the guidance from the powerful pretrained text-to-image diffusion models [8,14,32,39,50,53,90] via score distillation [48,72,76,88]. In general, these methods employ the KL divergence to reduce the discrepancy between the distribution of rendered images and the desired image distribution embedded in the 2D diffusion prior, while they differ in how to use the pretrained diffusion prior to model the distribution of rendered images. Extensive efforts have been made to explore prompt-specific optimization of various 3D representations, including implicit radiance fields [48], explicit radiance fields [35,44,72], DmTets [68,91] and 3D Gaussians [12]. Typically, tens of minutes to hours are needed to optimize a single 3D representation for one prompt to achieve the desired result.\nCompared to the aforementioned optimization-based text-to-3D methods, learning-based methods [9, 25, 38, 43, 52, 65, 79] can largely reduce the computational cost by training a text-conditioned 3D generative network. With the availability of 3D object collections [13,77,87], a deep network can be trained in a supervised manner so that 3D outputs can be generated in several seconds. Unfortunately, the size of existing text-3D datasets is far from sufficient compared to text-image datasets [56], limiting the text-to-3D generation performance of trained models. Inspired by the optimization-based text-to-3D methods that use pretrained 2D diffusion models, efforts have been made to train text-to-3D networks by using 2D diffusion models as supervisors [40, 49, 79] without using text-3D pairs. For example, a text-conditioned 3D hyper-network is trained in ATT3D [40] via Score Distillation Sampling (SDS) [48]. Nevertheless, this method suffers from numerical instability, which has been observed in subsequent studies [49,79] that apply SDS to different 3D generator networks.\nDespite the success of score distillation in optimization-based text-to-3D generation [48, 72, 88], its application to learning-based text-to-3D frameworks is rather limited because of the unstable training or unsatisfactory results. We argue that the primary challenge lies in how to efficiently and effectively leverage the pretrained 2D diffusion prior to represent the distribution of images rendered by the 3D generator. For example, SDS [48] forces the rendered images to adhere to the Dirac distribution, which causes numerical instability in 3D generator training [40, 79]. Variational Score Distillation (VSD) [72] finetunes the 2D diffusion prior for distribution alignment via minimizing the noise prediction error. However, the finetuning changes the pretrained diffusion network and hurts its comprehension capability to numerous text prompts, leading to mode collapse when the size of text prompts is extended.\nTo address the above mentioned issues, we propose Asynchronous Score Distillation (ASD). Like VSD, ASD aims to minimize the noise prediction error. Different from VSD, ASD does not finetune the pretrained 2D diffusion network; instead, it achieves the goal by shifting the diffusion timestep. This is based on the observation that diffusion networks will have smaller noise prediction errors in earlier timesteps [83]; therefore, we can shift the timestep to an earlier step to achieve a similar goal to VSD, i.e., reducing the noise prediction error. In this way, the diffusion network can be frozen in training and its strong text comprehension capability can be well-preserved. The shifted timesteps can be well sampled from a pre-defined range for most prompts. To evaluate the performance of ASD, we conduct extensive experiments by using three types of generator architectures, i.e. Hyper-iNGP [40], 3DConv-Net [7] and Triplane-Transformer [21], and two types of 2D diffusion models, i.e., Stable Diffusion [53] and MVDream [59], across various prompt corpus sizes. We conduct extensive experiments to evaluate the superiority of ASD to previous methods, including the stable training of 3D generators, the production of high-quality 3D outputs, the high content fidelity to input prompts, as well as its scalability to larger corpus sizes, e.g., 100k prompts. Some results are shown in Fig. 1."}, {"title": "2 Literature Review", "content": "2.1 Text-to-3D with Score Distillation\nText-to-3D takes text description, a.k.a. text prompt y, as input, and outputs 3D representation \u03b8 that renders high-fidelity images at any camera view \u03c0. Thanks to the powerful text-to-image diffusion models [39,50,53,59,90], we can optimize \u03b8 to align with y by computing the objective L(x, y) on the rendered image x = g(\u03b8, \u03c0) from camera view \u03c0. Through differential rendering, \u03b8 can be updated with the gradient \\nabla_{\\theta}L(\\theta,y) = \\frac{\\partial L(x,y)}{\\partial x} \\frac{\\partial x}{\\partial \\theta}. This technique is generally termed as score distillation. Unlike data-driven techniques [9,25,38,52,65], score distillation approaches [11, 23, 29, 35, 48, 64, 72, 88] can produce high-quality 3D content without the need for 3D training datasets.\nPrompt-Specific Text-to-3D. Existing score distillation methods [48, 72, 88] were originally developed to output a single 3D result \u03b8 for a single text prompt y via online optimization: min_{\\theta} E_{\\pi,x=g(\\theta,\\pi)} [L(x, y)]. The utilized 3D representations, e.g., NeRF [47,48], DmTet [58,91], and 3D Gaussian [24,37,62,64,70,86], are not designed to render scenes from varying text prompts. Therefore, the optimization has to be conducted again for newly provided text prompts. The optimization process typically costs tens of minutes to hours.\nPrompt-Amortized Text-to-3D. To mitigate the computational costs in prompt-specific methods, recent studies [31, 40, 49, 79] have attempted to use score distillation to train a text-to-3D generator \u03b8 = G(y), aiming to generate multiple 3D representations from a set of text prompts S_y = {y}. These methods can generate 3D results from queried text prompt in seconds. As proposed by ATT3D [40], the 3D generator training is performed by minimizing min_{\\theta} E_{\\pi,y\\in S_y,x=g(G(y),\\pi)} [L(x, y)] over all text prompts. Unlike data-driven approaches [21,63,82], score distillation bypasses the scarcity of text-3D data pairs because the 2D diffusion prior can offer the guidance to align the 3D output with the input text prompt. However, its application is currently restricted to training the 3D generator within a limited range of text prompts.\n2.2 Representative Score Distillation Methods\nDenote by \u03c6 the 2D diffusion prior [53,59] and by p_\u03c6(x | y) the text-conditioned image distribution embedded within \u03c6, the objectives of most existing score distillation methods can be generally concluded as minimizing the objective\nL(\\theta,y) = E_{\\pi,t,\\epsilon,x=g(\\theta,\\pi)} [w(t) D_{KL} (q_t (x_t | \\pi) ||p_t (x_t | y^*))]:\nwhere DKL denotes KL divergence, q_t (x_t | \u03c0) denotes the distribution of images x rendered at camera view \u03c0 at diffusion timestep t [18], and the same for p_t(x_t | y). w(t) is a timestep-dependent weight [48]. y* denotes the view-dependent strategy [53] or view-awareness [50,59] to prompt the different camera views [48]. To minimize this objective, the gradient w.r.t. \u03b8 can be calculated as per [72]:\n\\nabla_{\\theta}L(\\theta,y) = E_{\\pi,t,\\epsilon} [w(t) (-\\sigma_t \\nabla_{x_t} log p_t (x_t|y^*)-(-\\sigma_t\\nabla_{x_t} log q_t^\\theta(x_t|\\pi)))\\frac{\\partial x}{\\partial \\theta}] \\quad (1)\nwhere the first term - \u03c3t\u2207xt log p\u03c6 t (xt | y\u2217) corresponds to the score function [61] of the desired image distribution, and it can be achieved by predicting the noise \\epsilon \\sim N (0, I) in the noisy image xt = \u03b1tx + \u03c3t\\epsilon using the pretrained 2D diffusion model \\epsilon\u03c6 (xt;t, y\u2217) [53,59]. Existing score distillation methods [48, 72, 88] mainly differ in how to model - \u03c3t\u2207xt log q\u03b8 t (xt |\u03c0), which corresponds to the score function of the distribution of rendered images q\u03b8 (x | \u03c0). We denote this term in Eq. 1 as \\epsilon_{\\theta} (x_t; t, \\pi, y) in the following context, since it represents a diffusion model that corresponds to \u03b8. A summary of the objectives of major score distillation methods is shown in Table 1.\nThe objective of Score Distillation Sampling (SDS) [48] is \\nabla_{\\theta}L_{SDS}(\\theta,y) = E_{\\pi,t,\\epsilon} [w(t) (\\epsilon_{\\phi} (x_t; t, y) - \\epsilon)], which approximates the term \\epsilon_{\\theta} (x_t; t, \\pi, y) in Eq. 1 as the ground-truth noise \\epsilon. That is, SDS assumes that q\u03b8 (x |\u03c0) adheres to a Dirac distribution \u03b4 (x \u2212 g (\u03b8, \u03c0)) [72], which is characterized by a non-zero density at the singular point of x = g(\u03b8,\u03c0) and zero density everywhere else. However, updating \u03b8 under the Dirac distribution might be troublesome [72]. We may need to set the CFG (Classifier Free Guidance) [19] as high as 100 for model convergence, which will produce excessively large gradients and lead to unstable optimization. This problem is alleviated by Classifier Score Distillation (CSD) [88], which uses the classifier component [19] in SDS as the objective: \\nabla_{\\theta}L_{CSD}(\\theta,y) = E_{\\pi,t,\\epsilon} [w(t) (\\epsilon_{\\phi} (x_t; t, y) - \\epsilon_{\\phi} (x_t; t))]. CSD can be regraded as straightforwardly using the unconditional term of the diffusion prior \\epsilon_{\\phi} (x_t; t) to represent \\epsilon_{\\theta} (x_t; t, \\pi,y) in Eq. 1. Unfortunately, in the case of prompt-amortized training, this term may not provide effective gradient, because \\epsilon_{\\phi} (x_t;t) is unconditional to the provided text-prompts. In contrast, Variational Score Distillation (VSD) [72] models \\epsilon_{\\theta} (x_t; t, \\pi,y) with another text-aware diffusion model \\epsilon_{\\phi'} (x_t; t, \\pi,y), leading to \\nabla_{\\theta}L_{VSD}(\\theta,y) = E_{\\pi,t,\\epsilon} [w(t) (\\epsilon_{\\phi} (x_t; t, y^*) - \\epsilon_{\\phi'} (x_t; t, \\pi,y))], where \\epsilon_{\\phi'} (x_t; t, \\pi,y) is achieved by finetuning the pretrained 2D diffusion prior \\epsilon_{\\phi} (x_t; t, y^*) to align with the rendered image distribution q\u03b8 (x | \u03c0) via parameter efficient adaptation [22]. In practice, this is conducted by alternatively optimizing \u03b8 and finetuning \u03c6 with the noise prediction objective ||\\epsilon_{\\phi} (x_t; t, y) - \\epsilon||^2 [18] such that:\nE_{\\pi,t,\\epsilon} [||\\epsilon_{\\phi'} (x_t; t, \\pi,y) - \\epsilon||^2] < E_{\\pi,t,\\epsilon} [||\\epsilon_{\\phi} (x_t; t, y^*) - \\epsilon||^2] . \\quad (2)\nThe above equation reveals that a better alignment with the distribution of q\u03b8 (x|\u03c0) can be achieved by a more accurate noise prediction.\nWhile VSD achieves state-of-the-art results in prompt-specific text-to-3D [17, 72], it changes the diffusion prior's parameters by alternately optimizing \u03b8 and finetuning \u03c6. This forms a bi-level optimization, known to be problematic in generative adversarial training [66], and may be troublesome for training prompt-amortized text-to-3D models, because the change of pre-trained diffusion model might impairs its comprehension capability on a wide range of text-prompts. In specific, the pre-trained 2D diffusion model may have to sacrifice its generation capability in order to align with the distribution of rendered images, making it fail to produce good gradient for training the 3D generator."}, {"title": "3 Asynchronous Score Distillation (ASD)", "content": "3.1 Objective of ASD\nFrom the above discussions in Sec. 2.2, it can be seen that one key issue in VSD is to minimize the noise prediction error so that the model output can be aligned with the desired distribution of rendered images. VSD achieves this goal via finetuning the pre-trained 2D diffusion model, which however sacrifices its comprehension capability on text prompts. One interesting question is: can we minimize the noise prediction error without changing the pre-trained diffusion network weights? Fortunately, we find that this is possible and in this section we present a new objective function to achieve this goal.\nRecall that diffusion models solve the stochastic differential equation [61] via reversing the noise added along different stages, a.k.a. diffusion timestep t \u2208 {Tmax,...,Tmin} via xt = \u03b1tx + \u03c3t\\epsilon [18]. The influence of the noise \\epsilon ~ N(0, I) on the image x is incrementally reduced as the process progresses from the initial timestep Tmax to the final timestep Tmin, which is controlled by the scalars \u03b1t and \u03c3t. Consequently, the diffusion model's noise prediction accuracy will vary with the timestep t, at which the identical noise \\epsilon is added. To evaluate this, we consider a diffusion model with fixed image x, noise \\epsilon and condition y, but varied timestep t. We denote such a diffusion model as \\epsilon_{\\phi(t)} and explore how its prediction error, denoted by e(t)=\\|\\epsilon_{\\phi(t)} - \\epsilon\\|^2, changes with t.\nThe model \\epsilon_{\\phi(t)} can be a pre-trained 2D diffusion model (such as Stable Diffusion [53]). We denote by \\epsilon_{PT}(t) such a model, and investigate the behaviour of its noise prediction error, denoted by e_{PT}(t). In Fig. 2, we plot the curve (i.e., the blue colored curve) of ePT(t) versus t. We use a corpus with 15 text prompts from Magic3D [48] to draw this curve. For each prompt y, we generate 16 images with VSD [72]. Then for each image x, we apply one instance of Gaussian noise \\epsilon and conduct a single diffusion step with 100 distinct timesteps. The average noise reconstruction error is then calculated for these timesteps across all prompts and images. We can see from the curve of e_{PT}(t) that earlier diffusion timesteps (e.g., timestep 600) will have lower noise prediction error than later timesteps (e.g., timestep 200). Such a trend holds for almost every image sample x and noise sample \\epsilon because the well-trained diffusion model is frozen in our case. Since the noise prediction error declines from Tmin (i.e., late diffusion timestep) to Tmax (i.e., early diffusion timestep), we can conclude that for a given timestep t and a timestep shift 0 < \u2206t < Tmax - t, the following inequality holds:\nE_{\\pi,t,\\epsilon} [||\\epsilon_{\\phi} (x_{t+\\Delta t}; t + \\Delta t, y^*) - \\epsilon||^2] < E_{\\pi,t,\\epsilon} [||\\epsilon_{\\phi} (x_t; t, y^*) - \\epsilon||^2], \\quad (3)\nwhich implies that more accurate noise predictions can be achieved at earlier diffusion timesteps.\nThe above property of diffusion models has also been observed by Yang et al. [84], who indicated that as the timestep shifts from Tmax towards Tmin, the variance in noise prediction increases, as evidenced by the rising Lipschitz constants, which suggests an increased instability in noise prediction and larger noise prediction errors. Such a behavior can be observed in both \\epsilon-prediction and v-prediction models, as well as in 2D and 3D diffusion models (please refer to Sec. A.1 for details). This can be intuitively explained as follows. When t \u2192 Tmax, xt = \u03b1tx + \u03c3t\\epsilon \u2192 \\epsilon, then it is easier to achieve \\epsilon_{\\phi} (x_t; t, y^*) \u2248 \\epsilon because the model can manage to copy the input as the output.\nThe similarity between Eq. 3 and the fine-tuning objective of VSD in Eq. 2 inspires us to investigate whether simply shifting earlier the timestep could fulfill the fine-tuning requirements of VSD without modifying the pre-trained 2D diffusion network parameters. Specifically, we employ the pretrained 2D diffusion model with shifted timestep to approximate the diffusion model of rendered images in Eq. 1 as \\epsilon_{\\theta} (x_t; t, \\pi,y) = \\epsilon_{\\phi} (x_{t+\\Delta t}; t + \\Delta t, y^*), resulting in the following Asynchronous Score Distillation (ASD) objective function:\n\\nabla_{\\theta}L_{ASD}(\\theta,y) = E_{\\pi,t,\\epsilon} w(t) (\\epsilon_{\\phi} (x_t;t, y^*) - \\epsilon_{\\phi} (x_{t+\\Delta t};t + \\Delta t, y^*)) \\frac{\\partial x}{\\partial \\theta} . \\quad (4)\nWe can see that rather than iteratively fine-tuning the diffusion network as in VSD, ASD achieves similar goal by shifting the timestep t with an interval \u2206t in each step, which is much more efficient. One key variable introduced in ASD is the timestep shift \u2206t, which will be discussed in the next subsection.\n3.2 The Setting of Timestep Shift At\nBefore discussing how to set the timestep shift At, let's plot another curve, i.e., the noise prediction error of \\epsilon_{\\theta} (x_t; t, \\pi, y) w.r.t. timestep t. Actually, in the process of generating a with VSD, we will have the fine-tuned model \\epsilon_{\\phi'} (x_t; t, \\pi, y) as the by-product, which is used to represent \\epsilon_{\\theta} (x_t; t, \\pi,y) in Eq. 1. Therefore, with fixed x, \\epsilon and y, the noise prediction error of the fine-tuned diffusion model, denoted by eFT(t), can be calculated as e_{FT}(t)=\\|\\epsilon_{\\phi'} (t) - \\epsilon\\|^2.\nThe curve of eFT(t) w.r.t. t (i.e., the yellow curve) is plotted in Fig. 2 by using the same data as in plotting ePT(t). We can see that the curve of eFT(t) is positioned under ePT(t) because eFT(t) is obtained by the fine-tuned diffusion model EFT. However, as mentioned in Sec. 2.2, this fine-tuning changes the weights of pre-trained diffusion model and might damage its ability in comprehending text-image pairs. Therefore, we propose to fix the pre-trained model ePT(t) but shift it to e_{PT}(t + \u2206t) to approximate the desired eFT(t). Referring to Fig. 2, we could shift EPT(t) to an earlier timestep to achieve this goal. For example, at timestep to and with a time shift Ato > 0, we can use EpT(to + Ato) to approximate the noise prediction error of EFT(to).\nOn the other hand, the magnitude of At will vary with t. Let's come to another timestep t\u2081 in Fig. 2, where t\u2081 is earlier than to. Because the decreasing speeds of both epy and EFT will be reduced with t going to Tmax, the magnitude of At\u2081 will be increased to approximate eFT (t1). In other words, the magnitude of At should grow when t goes from Tmin to Tmax. We heuristically set this relationship as At = \u03b7(t - Tmin), where \u03b7 \u2208 [0,1] is a hyper-parameter that controls the length of shift range. Finally, it should be pointed out that the curves in Fig. 2 will vary a little for different training iterations, rendered images x and text prompts y. Therefore, At should fall into some range S(t). In practice, we set At ~ S(t) = U[0, \u03b7(t \u2013 Tmin)], which follows a uniform distribution within 0 and \u03b7(t - Tmin). The pseudo-code of ASD is summarized in Alg. 1, which can be applied to both prompt-specific and prompt-amortized text-to-3D tasks.\n2D toy experiments. To verify the proposed timestep shift strategy, we follow the paradigm in [72] to test SDS, CSD, VSD and our ASD on 2D toy examples. The left column of Fig. 3 shows the results of SDS, CSD, VSD, and the middle column shows the results of ASD with different sampling strategies of At. One can see that the proposed sampling strategy \u2206t ~ S(t) = U [0, \u03b7 (t \u2212 Tmin)] yields similar results to VSD [72]. Besides, we show the gradient norm produced by these score distillation methods in the right column of Fig. 3. One can see that the range of gradient norm produced by ASD is similar to that of VSD. However, the gradient norm of SDS is more than 10 times larger than ASD and VSD because it needs to set CFG=100 for convergence [48, 72, 88]. Such a large gradient may result in training instability. We append more 2D results in Sec. A.2 to further validate our proposed sampling strategy.\nText-to-3D Synthesis with ASD. As a score distillation method, ASD is open to the selection of 3D generator architectures [7,21, 27,40,47]. The general pipeline of ASD for text-to-3D synthesis is shown in Fig. 4. It takes a rendered image as input and diffuses it in two timesteps t and t+ \u2206t. The noise prediction difference is used as the gradient to optimize the 3D representation of generator. In this work, in addition to prompt-specific generation, as done in most existing score distillation works [19, 34, 48, 72, 78], we focus more on prompt-amortized text-to-3D and conduct thorough experiments to evaluate the effectiveness of ASD with three representative architectures, i.e. Hyper-iNGP, 3DConv-net and Triplane-Transformer, using two types of 2D diffusion models, i.e. Stable Diffusion and MVDream.\nHyper-iNGP is adopted by ATT3D [40], which integrates a prompt-agnostic hash-grid spatial encoding [47] with prompt-conditioned decoding layers to out-"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nComparison Methods. We compare ASD with state-of-the-art score distillation methods, including SDS [48], CSD [88] and VSD [72]. We adhere to their official codes for training prompt-amortized text-to-3D networks. For example, the CFG [19] values for SDS, CSD and VSD are configured to 100, 1, and 7.5, respectively. In addition, we compare with existing prompt-amortized method ATT3D [40] (whose code is not released yet) by replicating its reported results.\nImplementation Details. We employ VolSDF [85] to render images from the 3D generators. For Stable Diffusion, we employ SD-v2.1-base [2] for all score distillation methods for fair comparison. As configured in VSD [72], we set the CFG value as 7.5 for the pre-trained diffusion model in ASD, and 1 for the diffusion model of rendered images. The resolution of rendered images by Hyper-iNGP is set to 256 \u00d7 256, while that of 3DConv-net and Triplane-Transformer is set to 64 \u00d7 64 for GPU memory considerations. Other details are in Sec. A.5.\nPrompt Corpus. To thoroughly evalutate the capability of ASD in prompt-amortized text-to-3D synthesis, we employ multiple datasets encompassing a range of text prompt quantities. MG15 includes 15 prompts from Magic3D [35]; DF415 comprises 415 prompts from DreamFusion [48]; and AT2520 contains 2520 compositional prompts of animals from ATT3D [40]. DL17k contains 17k compositional prompts of human with daily activities, proposed by [31]. While AT2520 and DL17k provide a larger number of prompts than DF415, the prompt diversity of them is relatively low due to the predefined templates.\nTo test ASD's performance with an even larger scale of prompts, we introduce a novel prompt corpus named CP100k. This corpus consists of 100,000 text prompts filtered from the image descriptions collected by Cap3D [41], which was developed to test text-to-image model performance. To the best of our knowledge, it is the first time to evaluate score distillation methods on such a scale of text prompts. Meanwhile, it should be clarified that this work is focused on examining the score distillation performance rather than prompt generalization, so the test prompts share the same distribution as training prompts. More details of the prompt corpus are in Sec. A.4.\nEvaluation Metrics. We render 120 surrounding view images as the 3D synthesis result from each prompt. Similar to previous text-to-3D works [31,40, 40,48], we compute the CLIP recall, i.e., the classification accuracy by applying CLIP model to the rendered images to predict the correct text prompt, as one performance metric, denoted by \"R@1\". Additionally, we calculate the CLIP text-image similarity between generated images and input prompts as another metric [65, 74], denoted by \"Sim\".\n4.2 Evaluation Results\nResults with iNGP/Hyper-iNGP as 3D Representation. The iNGP [47] architecture is designed for prompt-specific text-to-3D generation. Hyper-iNGP has the same spatial encoding as iNGP except that the weights of the decoding layer depend on the text prompt. To eliminate the effect caused by architecture difference as much as possible, we adopt iNGP for prompt-specific text-to-3D tasks, and Hyper-iNGP for prompt-amortized tasks. Our experiments are carried out on the MG15 dataset. For prompt-specific tasks, we optimize an individual iNGP [47] for each MG15 prompt; while for the prompt-amortized tasks, we train a single Hyper-iNGP [40] across all MG15 prompts. We also compare our results with ATT3D [40], which is among the first to apply Hyper-iNGP to prompt-amortized text-to-3D tasks. ATT3D employs SDS for training and uses soft-shading [48] (denoted as * in Tab. 2) for rendering.\nThe qualitative and quantitative results are shown in Fig. 5 and Tab. 2, respectively. We can see that the existing methods suffer from performance decrease when transiting from prompt-specific to prompt-amortized tasks, as evidenced by the decreased CLIP similarity and recall in Tab. 2. It is worth mentioning that training Hyper-net with SDS requires turning on the spectral normalization [46] in the linear layers, otherwise the training will fail due to numerical instability. This observation is consistent with what reported in ATT3D [40]. This is because SDS suffers from large gradient norm (please also refer to Fig. 3 and the discussions therein), which makes Hyper-iNGP hard to converge. As can be seen in Fig. 5, ATT3D results in wrong geometry by using soft shading and SDS for training. For CSD, we see that it fails to optimize the full geometry, as shown by the shrunk peacock in both prompt-amortized and prompt-amortized results. For VSD, it tends to generate content drifts [59], resulting in repetitive patterns and abnormal geometry. It may fail to generate reasonable contents in both prompt-specific and prompt-amortized tasks. In contrast, our proposed ASD works very stable across the two tasks, yielding not only outstanding quantitative scores but also high quality 3D contents.\nResults with 3DConv-net as 3D Generator. The issues of existing score distillation methods either persist or become more pronounced when replacing Hyper-iNGP to 3DConv-net as the 3D generator. We find that training SDS with 3DConv-net always fails within several thousand iterations, even using spectral or other normalization techniques. This issue stems from that deeper network is more sensitive to large gradients [16] caused by SDS. Therefore, we only compare the results of other methods in Fig. 6. We see that CSD outputs acceptable results on AT2520, but its results on DF415, which has more varied prompts, are consistently smaller than anticipated. Such a phenomenon has been observed when Hyper-iNGP is used as the generator, which underlines CSD's inability to reliably guide the 3D generator to produce geometries aligned with the text prompts. As for VSD, it leads to rather abnormal results, failing to match the text prompts. This can be attributed to its fine-tuning of the pre-trained 2D diffusion model, which severely compromises VSD's text-image comprehending ability. In comparison, our proposed ASD, with 3DConv-net as the generator, yields improved outcomes, as evidenced by the visual results in Fig. 6 and the enhanced metric scores in Tab. 3.\nScalability. In this section, we evaluate the scalability of competing methods by using as many as 100k prompts in the CP100k dataset with 3DConv-net as the generator. The results are shown in Fig. 7 and Tab. 3. Due to the issue of numerical instability, SDS is not involved in this experiment. We can see that the outcomes of CSD are significantly diminished with uniformly small-sized shapes across all prompts. There is also a lack of variety since most outputs exhibit similar patterns. The results of VSD are also degenerated, displaying almost identical and anomalous outcomes for the text prompts. This resembles the phenomenon of mode collapse often encountered in bi-level optimization [66], which also highlights the importance of fixing the 2D diffusion model when training with such a large number of text prompts. In comparison, ASD is able to produce much higher quality outcomes across the text prompts, showcasing its capability in large-scale training with numerous text prompts as inputs.\n4.3 Ablation Study\nIn this section, we perform ablation studies to evaluate the settings of timestep shift At ~ S(t) = U [0, \u03b7 (t \u2212 Tmin)] from several aspects. The qualitative and quantitative results are shown in Fig. 8 and Tab. 4, respectively.\nImportance of Timestep Shift. We use \u03b7 = 0 (i.e., no timestep shift) as a baseline to evaluate the necessity of introducing timestep shift At. From Fig. 8 and Tab. 4, we see that while it can generate plausible results, the model is prone to generating shapes that do not make sense, such as the so-called Janus problem [5]. Examples include a frog with an extra eye, robot face with block-like features, and a peacock with tails at both the front and back. This is because the non-shifted diffusion model will align more with the 2D image distribution, tending to generate redundant contents and unreasonable geometry along the training. By introducing a timestep shift, our proposed ASD demonstrates advantages in achieving more coherent and visually pleasing results.\nRange of Timestep Shift. By setting \u03b7 = 0.2, we allow At to be sampled from a large range. However, this might not be a good choice. In the extreme case, for any timestep t we can set a large interval At such that t + At = Tmax, then the noise prediction becomes \\epsilon\u03c6(xt; t, y*) \u2248 \\epsilon, so that ASD is degraded to SDS, which cannot perform well under CFG=7.5 [48]. Inpractice, we find a larger \u03b7 tends to result 3D contents with larger size and rounded shapes, e.g., the peacock with closer views, or the frog with larger size, as shown in Fig. 8. Therefore, we set \u03b7 = 0.1 in all our experiments.\nDeterministic or Random Shift. If we set At = \u03b7 (t \u2212 Tmin), it assumes that the diffusion model of rendered images can be approximated by the pre-trained one with a fixed and deterministic timestep shift. As shown in Fig. 8 and Tab. 4, it reduces the chance to generate correct geometry and colors. Randomly sampling At in a range is more effective, which is adopted in our method.\n4.4 Results with MVDream\nAs a score distillation method, ASD is open to the choice of 2D diffusion models. In this section, we evaluate ASD's compatibility with another representative 2D diffusion model, MVDream [59]. To conduct score distillation, MVDream takes four views as input for rendering, and explicitly uses the camera poses as prompts. We conduct comparison and ablation study in prompt-specific optimization with iNGP as the 3D representation, as well as prompt-amortized text-to-3D with Triplane-Transformer as the 3D generator.\nResults with iNGP as 3D Representation. MVDream officially implements a modified SDS method by incorporating the CFG re-scale technique [36] to alleviate large gradient norms caused by SDS. We refer to this modified SDS as SDS*. We qualitatively compare the performance of SDS* and ASD on prompt-specific text-to-3D. The results are shown in Fig. 9. It can be seen that SDS* produces abnormal geometry with solid matter covering most of the 3D space, and it generates grayish textures. In contrast, ASD generates more natural geometry and textures. More results of ASD can be found in Fig. 1.\nResults with Triplane-Transformer as 3D Generator. We then employ MVDream for prompt-amortized text-to-3D by using Triplane-Transformer as the 3D generator. In addition to the comparison with SDS*, we ablate ASD without timestep shift to further solidify our proposed asynchronous timesteps. The experiments are conducted on DL17k corpus. As shown in Fig. 10, SDS* tends to produce small geometries. By using ASD with a deterministic timestep shift, i.e. At = \u03b7 (t \u2212 Tmin), the results are improved yet still unsatisfactory. Without any timestep shift in ASD, i.e., \u03b7 = 0, the 3D results have some floating patterns. This happens because without a timestep shift, the model fails to align the distribution of rendered images with the prior distribution of pre-trained diffusion model. By using a random timestep shift \u2206t ~ U [0, \u03b7 (t \u2013 Tmin)] and the magnitude of n = 0.1 in ASD, the results are significantly improved, which is also reflected in the metrics shown in Tab. 5.\n4.5 Discussions with Data-Driven Methods\nOur proposed method differs from existing data-driven methods [20,25,63,65,89] in that we do not require any 3D dataset to train the 3D generator. If the test text prompts fall into the training distribution, these supervised data-driven methods may generate better quality outputs than our unsupervised method. However, by leveraging the strong prior information in pre-trained 2D diffusion models, our method has better generalization capability to the test prompts. By using our 3DConv-net trained on DF415 corpus as an example, we compare our results with open-sourced data-driven 3D generators LGM [63] and Shape-E [25]. Fig. 11 shows the qualitative comparison on some text prompt inputs, which are out of the training distribution. We can see that LGM and Shape-E output poor results. In contrast, ASD can still work well by exploiting the powerful diffusion priors in pre-trained 2D models."}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we presented Asynchronous Score Distillation (ASD), a novel score distillation method that can assist 2D diffusion prior in training 3D generators with a scalable size of text prompts. By shifting the diffusion timestep to earlier stages, our ASD can effectively predict the noise prediction error to align the diffusion model with the distribution of rendered images, while preserving the superior text comprehension capability of pre-trained models, thus facilitating stable training with high-fidelity generation results. Our extensive experiments revealed that ASD performed consistently well on datasets of various sizes, being able to manage as much as 100k prompts.\nThough ASD has shown improvements over earlier score distillation approaches, there remain some limitations. For man-made objects that have very regular shapes, such as chairs or airplanes, the performance of our model will lag behind those data-driven methods, which benefit from an abundance of relevant data. We foresee opportunities to combine the advantages of data-driven and score distillation methodologies to improve text-to-3D capabilities in a more comprehensive manner in the future research."}, {"title": "6 Acknowledgement", "content": "This work is supported in part by the Beijing Science and Technology Plan Project Z231100005923033, and the InnoHK program."}, {"title": "Appendix", "content": "In this appendix, we provide the following materials:\nSec. A.1: more illustrations of noise prediction error eFT(t) by different diffusion models \\epsilon(t) (referring to Sec. 3.1 and Fig. 2 in the main paper);\nSec. A.2: more 2D toy experiments of different methods (referring to Sec. 3.2 and Fig. 3 in the main paper);\nSec. A.3: more details of 3D generator architectures (referring to Sec. 3.2 and Fig. 4 in the main paper);\nSec. A.4: more corpus details (referring to Sec. 4.1 in the main paper);\nSec. A.5: more implementation details (referring to Sec. 4.1 in the main paper);\nA.1 More Illustrations of Noise Prediction Error\nIn this section, we provide more illustrations of the noise prediction error by various pre-trained diffusion models, including the 2D \\epsilon-prediction model [2,53] and the v-prediction model [1,54], and the 3D diffusion model [9]. We plot the the noise prediction error against timesteps in Fig. 12. For each text prompt displayed at the top of the sub-figures, we use it as the condition to generate 16 samples. We then introduce a single instance of Gaussian noise to each sample and execute one diffusion step at 100 different timesteps. The DDPM [18] is used as the noise scheduler, as done in VSD [72]. The average noise reconstruction error is then calculated over the timesteps and the 16 data samples.\n2D \\epsilon-prediction diffusion model. The \\epsilon-prediction model is widely adopted in the field of text-to-3D synthesis [34,50,59,72,78]. In our tests, we employ the commonly used SD-v2.1-base model [2]. The noise prediction error curves for four prompts sourced from Magic3D [35] are presented in Fig. 12(a), from which we see a clear decrease of noise prediction error with the timestep going from Tmin to Tmax.\n2D v-prediction diffusion model. The v-prediction model, introduced by Salimans et al. [54], accelerates the generation process by predicting velocity rather than noise. We test this model using the well-known SD-v2.1 [1] with 4 prompts sourced from Magic3D [35]. To calculate the noise prediction error, we convert the velocity predictions into noise predictions [54]. As depicted in Fig. 12(b), the v-prediction model also exhibits reduced prediction errors as the timestep goes from Tmin to Tmax.\n3D diffusion model. Apart from the above 2D diffusion models, we also conduct experiments on a 3D diffusion model DiffTF [9], which is a 3D generator trained on 3D object datasets [77]. It is configured with \\epsilon-prediction and performs the diffusion process on tri-plane [10]. As shown in Fig. 12(c), its noise prediction error \\epsilon(t) also reduces as timestep t increases, which is similar to 2D diffusion models. In particular, \\epsilon(t) drops rapidly before t = 200. This is mainly caused by the much smaller scale (e.g., 6k 3D objects) of the 3D dataset [13] compared with the 2D datasets [56] (e.g., 2B text-image pairs). Therefore, the network tends to overfit the 3D data with smaller prediction error."}]}