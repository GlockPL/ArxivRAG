{"title": "TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps", "authors": ["Arjun Raj", "Lei Wang", "Tom Gedeon"], "abstract": "Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach leverages frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNet, as TrackNetV4.", "sections": [{"title": "I. INTRODUCTION", "content": "Ball trajectory data is a crucial element in sports analy-sis and athlete training. However, accurately detecting and tracking high-speed, small balls in sports competition videos presents significant challenges. The primary difficulties arise from the fact that balls in broadcast videos often appear blurry, tiny, or obscured by afterimages. Additionally, they may be come invisible due to occlusion, extreme visual indistinctness, or simply flying out of the camera's field of view.\nWith recent advances in deep learning, TrackNetV1 is intro-duced in [6] to track tennis balls and shuttlecocks in broadcast match videos. This heatmap-based tracking framework, built on a VGG-16 feature extraction network [9] and an upsam-pling network [8], takes multiple consecutive frames as input, leveraging the ball's trajectory\u00b9 for improved detection. While TrackNetV1 achieves superior tracking performance compared to conventional methods, its processing speed is insufficient for real-time sports analysis, and its network design consumes substantial GPU memory.\nTrackNetV2, presented in [10], offers improvements over TrackNetV1 by: (i) increasing processing speed through re-"}, {"title": "II. APPROACH", "content": "Notation. Let \\( I_t \\) denote the index set 1,2,..., T. We use regular fonts for scalars (e.g., x), lowercase boldface letters (e.g., x) for vectors, uppercase boldface letters (e.g., X) for matrices, and calligraphic letters (e.g., \\( \\mathcal{X} \\)) for tensors. Let \\( X \\) \u2208 \\( \\mathbb{R}^{d_1\\times d_2\\times d_3} \\) denote a third-order tensor. Using Matlab notation, we refer to its t-th slice as \\( X_{:,:,t} \\), which is a \\( d_1 \\times d_2 \\) matrix.\nMotivation. Prompts can extend beyond text or signals; they can also be learnable [12], [16] and take various forms [2], [4], [5], [7], [13]\u2013[15]. Recent work [1] introduces a motion prompt layer with only two learnable parameters that modulate frame differencing maps to produce motion attention maps. These maps spatially highlight regions where motion is relevant (e.g., balls) and suppress irrelevant motion (e.g., video noise and background movement), while also capturing the temporal evolution of these attention maps over time. We explore the application of motion attention maps in tracking and predicting high-speed, small objects in professional ball games (e.g., badminton, tennis, football, golf, etc.)."}, {"title": "B. Motion Attention Maps and Motion-Aware Fusion", "content": "Fig. 2 provides an overview of our motion-aware fusion framework. For simplicity, we use TrackNetV2 for visual-ization, but our motion-aware fusion can be seamlessly inte-grated into any existing heatmap-based detection and tracking framework. Our approach offers a straightforward yet effective solution: (i) using a motion prompt layer to highlight relevant motions as attentions, and (ii) fusing the motion attention maps with high-level visual feature maps to preserve both the visual and motion information of small objects, thereby enhancing detection and tracking. Following best practices in [3], [10], we also use a multiple-input, multiple-output design.\nLearnable attention maps for highlighting motion. Given a video sequence \\( X = [F_1, F_2,\\dots, F_T] \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times 3 \\times T} \\), where \\( F_t \\) (\\( t \\) \u2208 \\( \\mathbb{I}_T \\)) represents the t-th frame, and H and W denote the frame height and width, respectively, we select T' as the number of input frames to form short-term temporal blocks. The t-th temporal block is represented as \\( X_t = [F_t, F_{t+1}, \\dots, F_{T'+t-1}] \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times 3 \\times T'} \\) for \\( t \\) \u2208 \\( \\mathbb{I}_{T-T'+1} \\)."}, {"title": null, "content": "Each temporal block is then converted into a grayscale video sequence, \\( X' = [F'_t, F'_{t+1},\\dots, F'_{T'+t-1}] \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times T'} \\). After normalizing pixel values between 0 and 1, frame differencing maps are computed between consecutive frames, resulting in \\( D_t = [D_t, D_{t+1},\\dots, D_{T'+t-2}] \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times (T'-1)} \\), where \\( D_t = F'_{t+1} - F'_t \\) (\\( t \\) \u2208 \\( \\mathbb{I}_{T'-1} \\)). Positive values in \\( D_t \\) indicate an increase in pixel intensity from frames t to t + 1, while negative values indicate a decrease. Since \\( D_t \\) includes both positive and negative pixel intensity changes, such as minor object movements, we take the absolute values to capture all relevant motions for tracking and prediction, resulting in \\( D_t^+ \\) ranging between 0 and 1. Fig. 3 shows the difference between using original frame differencing maps and absolute frame differencing maps. We apply a Power Normalization (PN) function \\( \\alpha \\) with learnable parameters \\( \\Theta \\) as in [1] to these differencing maps, producing a sequence of motion attention maps for the t-th temporal block:\n\\[A_t = \\alpha_{\\Theta}(D_t^+),\\]\nwhere \\( t \\) \u2208 \\( \\mathbb{I}_{T'-1} \\) and \\( A_t \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times (T'-1)} \\)."}, {"title": "Fusing motion attention maps with visual features.", "content": "Inspired by the performance gains achieved through skip connec-tions that address the gradual loss of tiny object features along the processing pipeline (e.g., in TrackNetV2), we introduce a specialized motion-aware fusion mechanism. This mechanism integrates high-level visual features with our motion attention maps, preserving the ball's location and trajectory.\nSpecifically, we first extract high-level visual feature maps using the tracking network up to the last convolutional block (just before the Sigmoid layer that outputs the heatmaps):\n\\[V_t = \\text{TrackNet}_{visual}(X_t),\\]\nwhere \\( \\text{TrackNet}_{visual}(\\cdot) \\) refers to the TrackNet for extracting the visual features \\( V_t = [V_t, V_{t+1},\\dots, V_{T'+t-1}] \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times T'} \\). We then aggregate these visual feature representations with the motion attention maps generated via Eq. (1) from the motion prompt layer:\n\\[H_t = \\sigma(A_t \\odot V_t),\\]\nwhere \\( \\sigma(\\cdot) \\) is the Sigmoid function, and \\( H_t \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times T'} \\) denotes the motion-attention-enhanced heatmaps. The symbol"}, {"title": null, "content": "represents our fusion operation, which involves element-wise multiplication followed by concatenation:\n\\[A_t \\odot V_t = [V_t, A_t \\odot V_{t+1},\\dots, A_{T'+t-2} \\odot V_{T'+t-1}],\\]\nwhere \\( A_t \\odot V_{t+1} \\) (\\( t \\) \u2208 \\( \\mathbb{I}_{T'+t--2} \\)) denotes the motion-enhanced visual representations, and \\( A_t \\odot V_t \\) \u2208 \\( \\mathbb{R}^{H \\times W \\times T'} \\). As shown in Eq. (3), these representations are then passed through a Sigmoid function to produce the final heatmaps, which highlight the ball's location and trajectory over time.\nWe name the tracking framework incorporating our motion-aware fusion mechanism TrackNetV4. Below, we present our experimental results, comparisons and discussions."}, {"title": "III. EXPERIMENT", "content": "Dataset, protocol, and setup. The tennis ball tracking dataset introduced in [6] lacks a standardized training and test split, as it is initially divided randomly. This random division has led to inconsistencies across different works, making it difficult to fairly compare the results with other models. To address this issue, we develop two evaluation protocols while maintaining the 70/30 frame split:\ni. Game-level: The training set includes 'game5', 'game10', 'game6', 'game2', \u2018game7', \u2018game3', and 'game8',, while the test set comprises 'game1', 'game9', and 'game4'. This"}, {"title": "IV. CONCLUSION", "content": "In this paper, we present TrackNetV4, an advanced tracking framework that integrates motion-aware fusion to enhance the tracking and prediction of fast-moving, small objects in sports videos. TrackNetV4 builds on existing tracking technologies, incorporating a novel fusion mechanism that significantly improves performance in challenging conditions, such as oc-clusions and limited visibility. Our extensive experimental re-sults highlight substantial performance improvements over the previous TrackNetV3, showcasing TrackNetV4's superiority in high-speed object tracking. Notably, TrackNetV4's lightweight and modular design allows for easy integration into various applications."}]}