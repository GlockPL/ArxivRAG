{"title": "Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with Anomaly Aware", "authors": ["Chaoqun Liu", "Xuanpeng Li", "Chen Gong", "Guangyu Li"], "abstract": "Traffic prediction is an indispensable component of urban planning and traffic management. Achieving accurate traffic prediction hinges on the ability to capture the potential spatio-temporal relationships among road sensors. However, the majority of existing works focus on local short-term spatio-temporal correlations, failing to fully consider the interactions of different sensors in the long-term state. In addition, these works do not analyze the influences of anomalous factors, or have insufficient ability to extract personalized features of anomalous factors, which make them ineffectively capture their spatio-temporal influences on traffic prediction. To address the aforementioned issues, We propose a global spatio-temporal fusion-based traffic prediction algorithm that incorporates anomaly awareness. Initially, based on the designed anomaly detection network, we construct an efficient anomalous factors impacting module (AFIM), to evaluate the spatio-temporal impact of unexpected external events on traffic prediction. Furthermore, we propose a multi-scale spatio-temporal feature fusion module (MTSFFL) based on the transformer architecture, to obtain all possible both long and short term correlations among different sensors in a wide-area traffic environment for accurate prediction of traffic flow. Finally, experiments are implemented based on real-scenario public transportation datasets (PEMS04 and PEMS08) to demonstrate that our approach can achieve state-of-the-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Accurate traffic flow prediction can effectively reduce traffic congestion and alleviate traffic pressure [1], which is crucial for the development of smart transportation systems [2]. As shown in Fig 1, traffic prediction is a typical spatio-temporal sequence prediction problem that involves analyzing complex spatio-temporal correlations. Additionally, traffic prediction is influenced by various factors [3], which makes accurate forecasting challenging [4]. Consequently, researchers have extensively studied methods to improve the accuracy of traffic state predictions [5]. Some researchers have employed statistical modeling methods for traffic predictions [6]. However, these approaches only consider historical data from individual road sensors and overlook complex inter-sensor correlations, leading to poor prediction outcomes [7]. In recent years, deep learning-based methods have become increasingly popular in the field of traffic flow prediction [8]. Wang et al. [9] demonstrated the efficacy of encoding road structure information into the latent space using directed-attention neighborhood aggregation and highlighted the potential of combining this approach with Long Short-Term Memory (LSTM) networks to model the temporal evolution of traffic. The Long Short-Term Memory (LSTM) network effectively preserves the nonlinear temporal dependence of node embeddings across successive time steps, leading to improved performance in traffic prediction tasks. Liu et al. [10] proposed a novel spatio-temporal adaptive embedding and integrated it with a periodic embedding into a transformer-based model, achieving excellent results in traffic prediction. Li et al. [11] extended the graph structure into a spatio-temporal graph by combining data from neighboring time steps, and then extracted both long and short term correlations among traffic nodes using graph convolution on this spatio-temporal graph, which improved the accuracy of traffic prediction. However, these methods primarily concentrate on the transient correlations between neighboring or local spatial nodes, and are therefore inadequate for the extraction of complex spatio-temporal correlations in the context of large-scale road environments, which would result in a significant decline in model performance with increasing in prediction time.\nAdditionally, the traffic environment is affected by numerous external factors, such as weather, traffic accidents, and road construction, leading to irregular fluctuations in traffic patterns and complicating traffic prediction [12]. To address this problem, Essien et al. proposed a traffic forecasting algorithm based on a Stacked Autoencoder (SAE) architecture with Long Short-Term Memory (LSTM) to combine information extracted from Twitter messages with weather data [13]. Zhu et al. [14] represented traffic information and various external factors as a heterogeneous semantic network, and then employed knowledge graph representation methods to capture the knowledge structures and semantic relationships between traffic information and external factors to achieve accurate traffic prediction. Han et al. [15] improved prediction accuracy by leveraging human activity frequency data from a household travel survey to capture the relationships between traffic patterns and human activities. However, the existing works treat the anomalous factors in the same way as the original data, and the generalized feature extraction networks are not capable of effectively extracting the special spatio-temporal influences of the anomalous factors on the traffic prediction, which in turn leads to a poor performance of the traffic prediction.\nIn order to effectively capture the global long-time information as well as the personalized characteristics of anomalous factors, we propose a global spatial-temporal fusion traffic prediction algorithm with anomaly awareness. Firstly, based on the anomaly classification network, we design a feasible anomaly influence module, and input the both traffic state information and external anomaly information to obtain the anomaly influence characterization. Secondly, we integrated temporal, location, and spatial embeddings to form a high-dimensional representation and output, so as to obtain richer feature representations. Furthermore, we construct an attention-based multiscale spatio-temporal feature mixing module, to obtain the long/short time feature information of the global traffic state and achieve accurate traffic prediction results. The principal contributions of this study can be summarized in three points.\n\u2022 Based on an anomaly detection network, we construct an anomalous factors impacting module to accurately quantify its complex spatio-temporal impacts on traffic prediction.\n\u2022 By means of the proposed spatio-temporal self-attention mechanism architecture, we design multi-scale spatiotemporal feature fusion module to explicitly capture both short and long spatio-temporal relationships among different sensors across a variety of scenarios.\n\u2022 Quantitative and qualitative experiments are conducted on two benchmark datasets (PEMS04 and PEMS08), demonstrating that our approach outperforms all state-of-the-art comparative methods."}, {"title": "II. PRELIMINARIES", "content": "Traffic prediction, as a type of spatio-temporal prediction, is the process of forecasting the future state of traffic based on historical data. This data includes the impact of external anomalous events, such as traffic accidents, road construction, and other factors. A traffic network is defined as a graph $G = (N, E, A)$, where N represents the set of sensors on a traffic road, E represents the connectivity between traffic sensors (i.e., roads), and A represents the adjacency matrix, which is 1 when two sensors are connected by a road and 0 otherwise. $N = |N|$ represents the number of sensors. Furthermore, we define the traffic state information on the road as a tensor $X \\in R^{T \\times N \\times C}$, where T represents the time interval and C represents the number of traffic states (such as traffic flow, traffic speed). Additionally, we define the external abnormal event impact data as a tensor $Exc \\in R^{T \\times N \\times C}$. In conclusion, the traffic prediction problem can be modeled as\n$F([X_{t-\\alpha+1}, ..., X_t, G, Ex_t]) = [X_{t+1}, ..., X_{t+\\beta}]$  (1)\nwhere F(.) denotes the designed deep learning model, $\\alpha$ is the duration for which the historical data sequence, $\\beta$ indicates the prediction period, $X_t$ refers to the traffic state from all traffic sensors at a specific point in time."}, {"title": "III. METHODOLOGY", "content": "To fully explore the multi-scale spatiotemporal influences between traffic state data (including global/local traffic scenes, long/short time lengths) and capture the impact of external anomalous factors on road nodes, we propose a traffic prediction algorithm based on global spatiotemporal fusion. Our method mainly consists of the following parts: the first part is the Data Embedding Module(DEM), the second part is the Anomalous Factors Impacting Module, and the third part is the Multi-scale Spatiotemporal Feature Fusion Module, and the main framework is illustrated in Fig 2."}, {"title": "A. Anomalous Factor Impact Module", "content": "In real-world road scenarios, various external anomalous factors, such as traffic accidents or on-site construction, can have a significant spatiotemporal impact on traffic conditions. These events can not only affect the state of current road sensors but also, over time, extend their influence to distant connected nodes. To effectively model the impact of anomalous factors, we propose an Anomalous Factor Impact Module based on an anomaly classification network. This module includes three components: Temporal Self-Attention (TSA), Spatial Self-Attention (SSA), and Anomaly Detection Network.\n(1) Embedding Representation of Anomalous Information\nDue to the paucity of datasets containing multiple types of external anomalous events, we utilize a moving average method to identify anomalous data from raw data and label these points as anomalies. If a data point is deemed anomalous, it is marked as 1; otherwise, it is marked as 0. Thus, we obtain labeled data for the anomaly factors, consistent with the shape of the input X, denoted as $Exc \\in R^{\\alpha \\times N \\times C}$. Furthermore, in order to effectively capture the impact of anomaly factors from the original information, we concatenate Exc with the original data and map it to a high-dimensional space to obtain an embedded representation of the anomaly information, denoted as $Exc_{emb} \\in R^{\\alpha \\times N \\times d}$. The specific formula is as follows:\n$Exc\\_emb = BN(FC(cat[X, Exc]))$  (2)\nwhere X stands for the input historical traffic state data, Exc denotes the labeled anomalous factor data, FC signifies the fully connected layer, and BN refers to the batch normalization layer.\n(2) Temporal Impact of External Anomalous Factors\nTo extract the temporal influence of external anomaly factors, we designed a Temporal Self-Attention component (TSA). First, we reshape the obtained $Exc_{emb}$, changing its tensor shape $\\alpha \\times N \\times d$ to $N \\times \\alpha \\times C$, to calculate the influence of anomaly factors along the temporal dimension. Second, using a fully connected layer, we generate the query and key for each node i at each time step, allowing us to compute the similarity between different time points, resulting in a similarity matrix $A_T \\in R^{\\alpha \\times \\alpha}$, which can be expressed as\n$A_T = \\frac{Q_F(K_F)^T}{\\sqrt{d}}$  (3)\nwhere $Q^T = Exc\\_emb_{i::}W_F^T \\in R^{\\alpha \\times d}$ represents the time query matrice obtained through the fully connected layer, $K^T = Exc\\_emb_{i::}W_F^T \\in R^{\\alpha \\times d}$ indicates the time key matrice obtained through the fully connected laye, and d stands for the hidden dimension in the self-attention mechanism.\nFinally, by multiplying the similarity matrix with the value matrix obtained through the fully connected layer, we derive the temporal influence of external anomaly factors. The specific representation is as follows:\n$TSA(Exc\\_emb_{i::}) = FC(softmax(A_T)V^T)$ (4)\nwhere $V^T = Exc\\_emb_{i::}W_V^T \\in R^{\\alpha \\times d}$ represents the time value matrix obtained from the fully connected layer, FC refers to the fully connected layer. By applying the above operations to all sensor data, the output of the TSA component is obtained$Exc_T \\in R^{N\\times\\alpha\\times d}$\n$Exc\\_T = TSA(Exc\\_emb)$ (5)\n(3) Spatial Impact of External Anomalous Factors\nThe influence of external anomaly factors spreads over time to other connected road sensors. Therefore, we design a Spatial Self-Attention component (SSA) based on road links, incorporating a masked mechanism. First, we reshape the obtained tensor $Exc_T \\in R^{N\\times\\alpha\\times d}$, transforming its shape from $N \\times \\alpha \\times d$ to $\\alpha \\times N \\times d$, to compute the impact of anomaly factors on sensor space. Second, using two fully connected layer, we generate the query and key for each sensor at time, allowing us to compute the similarity between different sensors and obtain the similarity matrix $A_s \\in R^{N\\times N}$.\n$A_s = \\frac{Q_s(K_s)^T}{\\sqrt{d}}$ (6)\nwhere $Q^s = Exc\\_T{t::}W_s \\in R^{d\\times d}$ represents the spatial query matrice obtained through the fully connected layer, $K^s = Exc\\_T{t::}W_s \\in R^{d\\times d}$ indicates the spatial key matrice obtained through the fully connected laye.\nNext, we construct a matrix Mask. When the number of hops between two nodes in the adjacency matrix A exceeds a certain threshold, the corresponding Mask values are set to True. The positions in the similarity matrix that are True in the mask matrix are then set to negative infinity to eliminate interference from distant nodes, effectively capturing the spatial influence of anomaly factors.\nFinally, after applying the mask to the similarity matrix, we multiply it by the value matrix $V^s$ obtained from the fully connected layer to derive the temporal influence of external anomaly factors. The specific representation is as follows:\n$SSA(Exc\\_T{i::}, Mask) = FC(softmax(A_s)V^s)$ (7)\nwhere $V^s = Exc\\_T{t::}W_s \\in R^{N \\times d}$ represents the time value matrix obtained from the fully connected layer. By applying the above operations to all sensor data, the output of the SSA component is obtained$Exc_{TS} \\in R^{N\\times\\alpha\\times d}$.\n$Exc\\_TS = SSA(Exc\\_T, Mask)$ (8)\n(4) Anomaly Detection Network\nTo measure the impact of different categories of external anomaly factors on traffic prediction, we designed an anomaly detection network. First, we predefine M (where M is a hyperparameter in the deep classification network), which consists of M learnable d-dimensional variables to store representations of different categories of anomaly factors.\nNext, we calculate the similarity between the input anomaly factors and the predefined M categories of anomaly informa-"}, {"title": "tion, and then normalize the obtained similarity Yim. This can be represented as:", "content": "$\\Upsilon_m = \\frac{e^{Exc\\_TS[i].w_r^{[m]}}}{\\Sigma_M e^{Exc\\_TS[i].w_r^{[m']}}}$ (9)\nFinally, we apply weighted processing to the normalized result and then feed it into a fully connected layer to obtain the enhanced external anomaly impact $Exc\\_inf_i$, which can be represented as:\n$Exc\\_inf_i = FC(Exc_i)$ (10)\nwhere $Exc_i = \\Sigma_m \\Upsilon * w_m$ indicates the weighted value of i-th anomaly category, $w_m$ refers to the vector representation of the m-th anomaly category."}, {"title": "B. Data Embedding Module", "content": "To enrich the information in the input sequence, we introduced various types of embedding data. First, we use the same fixed positional encoding based on sine and cosine functions as in the Transformer model to obtain the embedded data, denoted as $Xt \\in R^{\\alpha \\times d}$.\n$PE(t, 2i) = sin (t/10000^{(2i/d)})$ \n$PE(t, 2i+1) = cos (t/10000^{(2i/d)})$ (11)\nwhere t represents the position in the time step, i is the index of the embedding dimension, and d is the total embedding dimension in the Transformer.\nIn addition, traffic patterns exhibit significant periodicity. To capture this periodicity, we introduced daily embeddings $Xa \\in R^{\\alpha \\times d}$ and weekly embeddings $Xw \\in R^{\\alpha \\times d}$. Here, we index each day from 1 to 1,440 and apply a linear transformation to obtain the mapping for each index, Xa. Similarly, we index each week from 1 to 7 and apply a linear transformation to derive the corresponding mapping, $X_w$.\nMoreover, to represent the graph structure in traffic data, we use the graph Laplacian eigenvectors to retain global information. We take the k smallest non-trivial eigenvectors from the Laplacian matrix and linearly project them to obtain $X_s \\in R^{N\\times d}$\nFinally, we add the obtained embeddings of each category to get the output $Xemb \\in R^{\\alpha \\times N \\times d}$ of the data embedding module. This can be represented as:\n$X_{emb} = X_{origin} + X_t + X_d + X_w + X_s$ (12)\nwhere $X_{origin}$ is the high-dimensional embedding of the original data X."}, {"title": "C. Multi-Scale Spatiotemporal Feature Fusion Module", "content": "Previous methods often capture correlations between sensors by separating time and space, or are limited to short-term spatio-temporal correlations, resulting in an inability to fully capture the long-term correlations between different sensors. To address this issue, we designed a Multi-scale Temporal-Spatial Feature Fusion Module (MTSFFM) based on the Transformer architecture. This module comprises L layers of Multi-scale Temporal-Spatial Feature Fusion Layer (MTSFFL), with each layer of MTSFFL containing three components: SSA, TSA, and Multi-scale Temporal-Spatial Feature Fusion component(MTSFF).\n(1) MTSFF\nTo directly capture the interactions between different sensors over long durations, we constructed the MTSFF component to extract the correlations among different sensors in a long-term context. First, we define the input as tensor $H \\in R^{\\alpha\\times N\\times d}$, then we unfold it along the temporal dimension to obtain $H_{ts} \\in R^{\\alpha N \\times d}$. Using three fully connected layers, we generate query, key, and value tensors corresponding to traffic sensors across different times and spatial locations.\n$Q_{ts} = H_{ts}W_s, K_{ts} = H_{ts}W_k, V_{ts} = H_{ts}W_v$ (13)\nNext, we calculate the influence matrix among all sensors across different time points $A_{st} \\in R^{\\alpha N \\times \\alpha N}$.\n$A_{st} = \\frac{Q_{ts}(K_{ts})^T}{\\sqrt{d}}$ (14)\nFinally, since only a few interactions between nodes are necessary in practice, we expand the original tensor to a size of (E, F) to reduce the influence of weakly correlated nodes. Specifically, the proposed formula for the masked spatiotemporal attention mechanism is as follows:\n$MTSFF(H_{ts}, Mask) = FC(softmax(A_{st} \\cdot Mask)V_{ts})$ (15)\n(2) SSA And TSA components\nAlthough the spatio-temporal hybrid self-attention component can capture the relationships between any sensors, when calculating sensor correlations, it only uses the sensor's own characteristics without considering the trend information of the sensors. To effectively capture the local trend between sensors, we introduced SSA and TSA components before the spatiotemporal hybrid self-attention component to extract local trend information.\nFirst, we use SSA to aggregate local spatial features from the sensors.\n$H^l = SSA(H^{l-1}, Mask)$ (16)\nwhere $H^{l-1}$ represents the input vector of the i-th layer MTSFFL, and $H^l$ denotes the output vector of the SSA in the i-th layer.\nNext, we make use of TSA to aggregate the local temporal features from the sensors:\n$H^l = TSA(Reshape(H))$ (17)\nwhere $H^l$ indicates the spatio-temporal node representation after the local spatio-temporal information has been aggregated.\nFinally, the data containing local trend information is fed into the MTSFF component to obtain the output $H^{out}$ of the i-th layer, and $H^{out} \\in R^{\\alpha N\\times d}$ can be expressed as\n$H^{out} = MTSFF(Reshape(H^l), Mask)$ (18)\n(3) Skip Connection"}, {"title": "To introduce richer multi-level features, we set up skip connections after each MTSFFL. The input to the first MTSFFL is H\u00b9.", "content": "$H^1 = X_{emb} + Exc_{inf}$ (19)\nwhere $X_{emb}$ represents the output from the data embedding module, and $Exc_{inf}$ represents the output of the anomalous factors impacting module.\nAfter processing through the three components, the output for the i-th layer is\n$H^{i+1} = Reshape(H_{out}^i) \\in R^{\\alpha \\times N \\times d}$ (20)\nFinally, after reshaping the output of each layer, we aggregate them to obtain the final hidden state, denoted as X.\n$X = Reshape(H_{out}^1) + ... + Reshape(H_{out}^L)$ (21)\nTo produce the final prediction results, we use two 1 \u00d7 1 convolutional neural networks to transform the temporal dimension and the feature dimension to the desired output size. The exact transformation is illustrated in the following formula:\n$\\hat{Y} = con2(con1(X)) \\in R^{\\beta \\times N \\times C}$ (22)\nwhere $\\hat{Y}$ represents the predicted output, $\\beta$ denotes the number of time steps for the prediction, and C indicates the number of categories in the traffic state."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "A. Environment\nThe efficacy of our method was evaluated on two publicly available datasets, PEMS04 and PEMS08. The datasets were partitioned into training, validation, and test sets in a 6:2:2 ratio. The model was implemented with PyTorch 1.11.0+cu113 and executed on an NVIDIA RTX 4070Ti GPU. The identical parameters were employed for both datasets. As illustrated in Table I: In the anomaly detection network, the number of anomaly categories, denoted by M, is 64. Each category is represented by a learnable 64-dimensional vector. The hidden dimension in the self-attention mechanism is set to 16. The observation step size and prediction horizon are both set to 12. The number of spatio-temporal feature extraction layers is four. Adam optimizer is employed with a learning rate of 0.01, and the batch size is 64. Early stopping occurs if the validation error converges within 50 epochs or if the training process reaches 400 epochs. Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE) are utilized as evaluation metrics.\nB. Quantitative Evaluation\nWe chose a selection of traditional methods (such as HI), typical deep learning methods (e.g., GWNet [2], DCRNN [4], AGCRN [16], STGCN [17], GTS [18]), and state-of-the-art methods (e.g., GMAN [6], MTGNN [8], STID [5], STNorm [19], PDFormer [7], STAEformer [10]). From Table II, we can observe that our proposed model outperforms the latest state-of-the-art methods across all metrics. STAEformer introduced a novel concept of spatio-temporal embedding, comprised"}, {"title": "C. Ablation Study", "content": "To further examine the effectiveness of different modules in our method, we compared the GSTF module with the following variants: (1) PDFormer: This method is based on a transformer architecture, and we use it as a baseline model. (2) w/ST: This variant removes the multi-scale spatiotemporal fusion component from the multi-scale spatiotemporal feature fusion module. (3) w/ext: This variant removes the anomalous factor impact module. Fig 3 shows a comparison of these"}, {"title": "variants on the PEMS08 dataset, we can see that our proposed multi-scale spatio-temporal feature fusion module significantly improves accuracy, which implies that the relationships among different sensors over a long period have a significant impacts on prediction tasks. Additionally, it can be observed that our proposed anomaly factor module effectively reduces the RMSE value. Since the RMSE value is more sensitive to outliers compared to the other two metrics, this suggests that effectively modeling external anomaly factors can greatly reduce the impact of outliers on the model.", "content": "V. CONCLUSION AND FUTURE WORK\nIn this paper, we have proposed a global spatiotemporal fusion-based traffic prediction algorithm. First, based on an anomaly classification network, we designed an anomalous factors impacting module that effectively represented the spatio-temporal impact of external anomalies on traffic prediction. Additionally, we constructed a multi-scale spatiotemporal feature fusion model based on the Transformer architecture, which can capture all potential long-term and short-term correlations among different sensors in a broad traffic environment, enabling accurate traffic flow prediction. Experiments conducted on two real-world datasets validated the superiority of our approach. In the future work, we will further study how to improve the robustness of traffic prediction models and reduce model parameters and training time."}]}