{"title": "Automatic Annotation Augmentation\nBoosts Translation between Molecules and Natural Language", "authors": ["Zhiqiang Zhong", "Simon Sataa-Yu Larsen", "Haoyu Guo", "Tao Tang", "Kuangyu Zhou", "Davide Mottin"], "abstract": "Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA\u00b3, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA\u00b3 by creating an enhanced dataset, LACHEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LACHEBI-20, we train LAMOLT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.\nExperimental results on text-based de novo molecule generation and molecule captioning demonstrate that LAMOLT5 outperforms state-of-the-art models. Notably, incorporating LA\u00b3 leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA\u00b3 notable applications in image, text and graph tasks, affirming its versatility and utility.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has garnered increasing attention due to its transformative potential in broad real-world applications, including biology (Schwaller et al., 2019; Pei et al., 2023; Zhong and Mottin, 2023). Take a recent new trend as an example: researchers intend to jointly model SMILES (Weininger, 1988) strings and scientific text to obtain shared representations across the two\nmodalities. For instance, Edwards et al. (2022) innovatively propose MOLT5, a model based on the T5 architecture (Raffel et al., 2020), pre-trained on ZINC (Sterling and Irwin, 2015) by predicting the masked text parts. Consequently, they fine-tune the model on CHEBI-20 (Edwards et al., 2021) to learn how to map between SMILES representations of molecules and their corresponding annotations (captions) to support molecular tasks.\nHowever, the development of AI faces a fundamental setback: the scarcity of high-quality annotated data. For instance, molecular data annotation is often a costly and time-consuming process (Di-Masi et al., 2016). This limitation restrains the development of AI approaches, as models grow in size and expressiveness, they require larger and more diverse annotated datasets to achieve high performance and generalisability (Devlin et al., 2019; He et al., 2023). Therefore, one viable alternative for AI in practice is to resort to effective data augmentation strategies.\nBack to the example CHEBI-20 dataset, a\nset of studies attempted various solutions to ad-"}, {"title": "2 Related Work", "content": "Molecule Language Models. MLMs have recently seen significant advancements, leveraging NLP techniques to understand and generate molecules. Early works such as ChemBERTa (Chithrananda et al., 2020) and Text2Mol (Edwards et al., 2021) adapt transformer-based architectures for molecular representation learning. MolGPT (Bagal et al., 2022) and MOLT5 (Edwards et al., 2022) demonstrate the ability to predict molecular properties and generate novel compounds, highlighting the potential of language models in biological research. However, the effectiveness of MLMs is often constrained by the limited availability of annotated molecular data. Meanwhile, manual molecular data annotation is often a costly and time-consuming process, necessitating specialised equipment and extensive human labour (DiMasi et al., 2016).\nData Augmentation in MLMs. Data augmentation has emerged as a critical strategy to address the scarcity of high-quality datasets. Take the following studies on CHEBI-20, Christofidellis et al. (2023), Liu et al. (2023b), Pei et al. (2023) and Pei et al. (2024) introduce additional chemical databases (PubChem (Kim et al., 2023), Drugbank (Wishart et al., 2018), UniProt (Consortium, 2023), PubMed (White, 2020), etc) as to enrich model with extra knowledge, and design auxiliary tasks to train advanced models. However, these methods depend on supplemental datasets and the"}, {"title": "3 Methodology", "content": "addition, Section 3.4 describes extensive implementations of LA\u00b3 across broad applications in image, text and graph tasks.\n3.1 Showcasing CHEBI-20\nCHEBI-20. CHEBI-20 contains 33010 molecular entities centred on chemical compounds. Each molecule is represented using a SMILES string (Weininger, 1988) and associated with a high-quality, manually annotated caption supporting various computational and experimental studies. Given a molecule, we formally represent it as M = (S, C), where S and C denote its SMILES string and associated caption. Examples are illustrated in Figure 2-(A). Consequently, CHEBI-20 can be formally represented as D = {M1, M2,..., Mn}. S\n= {S1, S2,..., Sn} represents the SMILES string set and C = {C1, C2, . . ., Cn} the caption (annotation) set. The dataset is publicly available with a fixed split: $D_{TRAIN}$ (80%), $D_{VALID}$ (10%) and $D_{TEST}$ (10%), allowing researchers to consistently train and evaluate their models.\nTasks. CHEBI-20 supports two molecular tasks:\n(1) text-based de novo molecule generation (GEN) and (2) molecule captioning (CAP). The goal of text-based de novo molecule generation is to train a model which can generate a variety of possible new molecules with desired properties as described in the text. Specifically, for CHEBI-20, we aim to learn a model $f_{GEN} : C \u2192 \u015c$ by minimising the loss function value $min_\u03a8 L(\u015c_{train}, S_{train})$, where \u03a8 represents the set of trainable parameters of $f_{GEN}$. The target of molecule captioning is to generate descriptions of the components and chemical functionality of a molecule. Similarly, we aim to learn a model $f_{CAP} : S \u2192 \u0108$, by minimising loss function value $min_\u03a6 L(\u0108_{TRAIN}, C_{TRAIN})$, where \u03a6 represents the set of trainable parameters of $f_{CAP}$.\nMOLT5. The molecule generation tasks can be considered translation tasks, and sequence-to-sequence models serve as solid solutions. One fundamental method in this category is MOLT5 (Edwards et al., 2022), an improved version of T5 (Raffel et al., 2020). MOLT5 initialise an encoder-decoder transformer model using public checkpoints of T5. The model is then pre-trained on a combined dataset of C4 (Raffel et al., 2020) and ZINC (Sterling and Irwin, 2015) for 1 million steps. Finally, it undergoes 50,000 steps of fine-tuning on CHEBI-20 for two molecular tasks. Since these tasks are formulated as sequence-to-sequence tasks,"}, {"title": "3.2 Automatic Annotation Augmentation", "content": "As shown in Equation 1, the number of training instances directly affects the amount of information injected into the model. Edwards et al. (2022) discuss the potential limitations caused by the limited data in CHEBI-20. A recent breakthrough known as In-Context Learning (ICL) has enhanced the adaptability of LLMs by enabling them to acquire contextual knowledge during inference, eliminating the need for extensive fine-tuning (Clark et al., 2020). To harness ICL for CHEBI-20 augmentation, we first formulate a prompt for querying LLMs. The goal in prompt engineering is to find the correct way to formulate a question Q in such a way that an LLM (LLM) will return the corresponding answer A essentially represented as A = $f_{LLM}(Q)$. In this work, we design the prompt as shown in Appendix A. Precisely, the prompt consists of two components: Instruction: Provides general guidance to the LLM, clarifying its role in the conversation; Message: Tasks the LLM to rewrite the molecule caption, considering the chemical expertise and given information.\nGiven an instance $M_i = (S_i, C_i)$ from\n$D_{TRAIN}$, we can generate k rewritten captions\n${Ci,1, Ci,2,..., Ci,k}$ with multiple rounds of\nqueries. This results in an augmented instance,\n$M^{+} = (Si, Ci,o, Ci,1, Ci,2 . . ., Ci,k)$, and an aug-\nmented dataset, LACHEBI-20 ($D^+$). Specifically,\n$D^+$ = ($D_{TRAIN}, D_{VALID}, D_{TEST}$), where $D_{TRAIN}$ = ${M\u207a, M\u207a,... }$. Each SMILES string of $D_{TRAIN}$ is associated with k + 1 captions. This language augmentation process introduces diversity in sentence structure and vocabulary while preserving the essential knowledge about the molecules. In our experiments, we adopt two open-source LLMs (Llama 2-70b (Touvron et al., 2023) and Llama 3- 70b (Touvron et al., 2023)) and two closed-source LLMs (GPT 3.5-turbo (Achiam et al., 2023) and Gemini Pro (Google, 2024)) to generate four rewritten captions. We demonstrate some generated example captions in Table 6 of Appendix B."}, {"title": "3.3 Training on Augmented Dataset", "content": "After generating k new captions for each molecule of the training dataset $D_{TRAIN}$. We proceed to train a model based on LACHEBI-20 to perform the molecular tasks, i.e., text-based de novo molecule generation (GEN) and molecule captioning (CAP). In this work, we initialise encoder-decoder transformer models using the available MOLT5 variants, as introduced in Section 3.1. We then train a novel model, LAMOLT5, using a cross-entropy loss:\n$L^{GEN}_{CE} =\n\\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{k+1}logp(S_{TRAIN,i} | C_{TRAIN,i,j})}{(k+1)n}$\n$L^{CAP}_{CE} =\n\\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{k+1}log p(C_{TRAIN,i,j} |S_{TRAIN,i})}{(k+1)n}$\nwhere $p(S_{TRAIN,i} | C_{TRAIN,i,j})$ is the probability assigned by $f_{GEN}$ to the i-th true SMILES string\n$S_{TRAIN,i}$ given the i-th molecule's j-th caption\n$C_{TRAIN,i,j}$. The critical addition to the MOLT5 is the augmented caption set $C_{TRAIN,i, {0,1,...,k}}$ for each SMILES string $S_{TRAIN,i}$. For the molecule generation task, LAMOLT5 is trained to generate the correct SMILES string $S_{TRAIN,i}$ by giving different caption inputs $C_{TRAIN,i, {0,1,...,k}}$. By introducing diversity into the training data, we enhance the effectiveness and robustness of the model in generating SMILES strings (validated in Section 4.2). Meanwhile, for the molecule captioning task, LAMOLT5 is trained to generate various captions for each SMILES string. Despite these captions having different sentence structures and vocabularies, they preserve the essential knowledge about the molecules. This training enhances the model's ability to generate more semantically"}, {"title": "3.4 Extension to Broad Applications", "content": "To further demonstrate the versatility of LA3, we extend its application to several additional datasets, including ogbg-molbace, ogbg-molhiv, ogbg-molesol (Hu et al., 2020), and CC3M (Sharma et al., 2018). These datasets support a variety of crucial tasks, e.g., image captioning, text understanding, and graph property prediction. Due to the paper length constraints, the detailed implementations of LAMOLT5 on these additional datasets are presented in Appendix F. Experimental results derived from these implementations will be discussed in Section 4.5."}, {"title": "4.1 Experimental Settings", "content": "Dataset. We use our generated LACHEBI-20 dataset for training LAMOLT5. One annotation is the original present in CHEBI-20, and the other two are LLM-generated. We adopt two conventional LLMs, GPT 3.5-turbo and Gemini Pro, to generate annotations in the main dataset from most experiments. We additionally experiment in Section 4.4 with annotations generated by two open-source LLMs, e.g., Llama 2-70b and Llama 3-70b.\nBaselines. We mainly consider three families of methods: (1) Methods included in the benchmark paper (Edwards et al., 2022), including RNN (Cho et al., 2014), Transformer (Vaswani et al., 2017), T5 (Raffel et al., 2020) and MOLT5 (Edwards et al., 2022). (2) The state-of-the-art methods (reported on the leaderboards (den, 2024; cap, 2024)) without additional datasets, including Text+Chem T5 (Christofidellis et al., 2023), TGM-DLM (Gong et al., 2024) and MolReGPT (Li et al., 2024). They rely on the same datasets as LAMOLT5, we report their results in Table 1, 3 (3) The state-of-the-art methods incorporating extra datasets. For the text-based de novo molecule generation task, we include BioT5 (Pei et al., 2023) and BioT5+ (Pei et al., 2024); for the molecule captioning task, we include BioT5, MolXPT (Liu et al., 2023b). For a fair comparison, we report their results in Figure 3. A detailed description of each baseline method can be found in Appendix D."}, {"title": "Training Setup", "content": "We train LAMOLT5-Small and -Base for as little as 1500 epochs and LAMOLT5- Large for 200 epochs. This project used ~ 11500 H100 GPU hours. Detailed hyper-parameter settings and checkpoints are available online2.\nEvaluation Setup. We adopt the benchmark evaluation setup following (Edwards et al., 2022). For the text-based de novo molecule generation task, we employ: SMILES BLEU score (Papineni et al., 2002), Levenshtein distance (Miller et al., 2009), Fr\u00e9chet ChemNet Distance (FCD) (Preuer et al., 2018), MACCS FTS (Durant et al., 2002), RDK FTS (Schneider et al., 2015) Morgan FTS (Rogers and Hahn, 2010), Exact score (Edwards et al., 2022), Validity (Edwards et al., 2022), and Text2Mol (Edwards et al., 2021). For the molecule captioning task, we measure the BLEU score, ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and Text2Mol of the generated annotation compared to the ground-truth. Detailed descriptions can be found in Appendix C."}, {"title": "4.2 Molecule Generation", "content": "Observation 1: LAMOLT5 significantly elevates the performance of MOLT5. The de novo molecule generation results across nine evaluation metrics in Table 1 reveal that LAMOLT5 achieves the highest performance on all measures. In addition, LAMOLT5 consistently delivers substantial enhancements over MOLT5, with improvements up to 301% in terms of Exact score, which measures the number of times the output corresponds to the ground truth. This consistent and notable performance underscores the effectiveness of LACHEBI-20 and LAMOLT5. Moreover, it illustrates the efficacy of our automatic annotation-augmentation pipeline, LA\u00b3, on biomedical datasets.\nObservation 2: The small 77M parameters LAMOLT5 outperforms the 800M MOLT5. The results of Table 1 indicate that the small-size variant of LAMOLT5 (77M) outperforms the large-size variant of MOLT5 (800M) in seven different evaluation metrics (Levenshtein, MACCS FTS, RDK FTS, Morgan FTS, FCD, Text2Mo, Validity) on the molecule generation task. On the other two evaluation metrics (BLEU and Exact), LAMOLT5- Small achieves competitive performance compared to MOLT5-Large. LAMOLT5 achieves impressive results by leveraging the annotation-augmented"}, {"title": "4.3 Molecule Captioning", "content": "Observation 3: LAMOLT5 generates coherent descriptions. The results in Table 3 highlight the superior performance of the LAMOLT5 in the molecule captioning task. LAMOLT5 excels in the Text2Mol metric, which provides a comprehensive assessment of the semantic alignment \u2014throuch cosine similarity\u2013 between generated descriptions and their corresponding molecules. LAMOLT5 variants achieve improvements up to 23% in LAM- OLT5-Small over the corresponding MOLT5 variants. These results underscore LAMOLT5 's enhanced ability to capture the intricate semantics of molecule descriptions, making it a highly effective model for this task.\nObservation 4: LAMOLT5 exhibits lower ROUGE score. LAMOLT5 exhibits lower"}, {"title": "4.4 Analysis", "content": "Comparison with state-of-the-art methods. Figure 3 demonstrates the performance of top-3 leaderboard SOTA models. LAMOLT5 variants take the first two overall ranks. Notably, LAMOLT5 establishes new SOTA results on molecule generation task. Although BioT5 outperforms LAMOLT5 on the molecule captioning task, BioT5 leverages"}, {"title": "4.5 Broad Applications", "content": "Results of image, text and graph tasks. To further demonstrate the versatility of LA\u00b3, we perform extended experiments on several additional datasets, including ogbg-molbace, ogbg-molhiv, ogbg-molesol, and CC3M, which support a variety of crucial tasks, e.g., image captioning, text understanding, and graph property prediction. Due to the page limit, we describe the detailed implementations in Appendix F. Results in Table 5 show that LA\u00b3 significantly enhances performance across these diverse applications. This improvement highlights LA\u00b3 's potential to be a valuable tool in a wide range of AI tasks, offering substantial gains in accuracy and efficiency."}, {"title": "5 Concluding Discussion", "content": "This work proposes an automatic annotation augmentation pipeline, LA\u00b3, designed to enhance annotated datasets and thereby boost the performance of AI approaches with minimal cost. We generate LACHEBI-20, an enriched biomedical dataset featuring diverse sentences and vocabulary while preserving essential molecular knowledge. This increased diversity is crucial for training LAMOLT5 models, leading to remarkable improvements in challenging molecular tasks. A set of ablation studies investigate the impact of LA\u00b3 design and affirm its effectiveness. Furthermore, we expand the application of LA\u00b3 to a wide range of datasets across different domains, including image captioning, text understanding, and graph property prediction. The"}, {"title": "6 Limitations and Ethic Statement", "content": "Limitations. The language augmentation process relies on external LLMs, which introduce uncertainties because their robustness in other applications cannot be guaranteed. Additionally, results shown in Figure 4 demonstrate the impact of caption quality on molecular tasks, suggesting that developing techniques for filtering captions could be a valuable direction for future work. Moreover, while LLMs continue to improve in performance and ICL capabilities, LAMOLT5 can benefit from these advancements. However, the domain-specific knowledge embedded in LLMs remains relatively limited. Thus, exploring practical solutions to incorporate more comprehensive domain knowledge into LLMs for language augmentation is a promising future direction for enhancing LA3.\nEthic Statement. Throughout our work, we did not utilise any private or sensitive information. The involved datasets are open-source, and outputs are available online to the community. However, it's essential to note that if any private information were to be inadvertently exposed to an LLM during internal pre-training and fine-tuning stages, LA\u00b3 does not offer any privacy filtration mechanism. Therefore, there exists the potential for privacy concerns associated with the underlying model to manifest through the output provided by LA\u00b3."}, {"title": "A Proposed Prompt Details", "content": "In this work, we design the prompt as follows:\nInstruction: You are now a chemical specialist in rewriting captions for a molecule in SMILES format. Make sure those captions describe the given molecule correctly and precisely based on your two inputs (SMILES and Caption of it). Also, make sure your rewriting captions do not include the input SMILES. Write the response without using linebreaks, newlines, or special characters such as \"\\t\" or \"\\n\".\nMessage: SMILES string of target molecule: [SMILES].\nDescription of the molecule: [caption].\nTask: Rewrite the following molecule with its SMILES and caption. The newly rewritten caption should be elaborate, descriptive, and concise, highlighting the key structural features and biological activities of the molecule. Only output rewritten caption without any header and linebreak.\nAnswer:"}, {"title": "B Caption Augmentation Details", "content": "This section presents some examples of augmented captions. Particularly, we provide the SMILES string, the original caption of CHEBI-20 and four different captions rewritten by four LLMs, i.e., two open-source LLMs (Llama 2-70b (Touvron et al., 2023) and Llama 3-70b (Touvron et al., 2023)) and two closed-source LLMs (GPT 3.5-turbo (Achiam et al., 2023) and Gemini Pro (Google, 2024)).\n1. SMILES string: [18FH]\nOriginal caption: The molecule is the radioactive isotope of fluorine with relative atomic mass 18.000938. The longest-lived fluorine radionuclide with half-life of 109.77 min.\nGPT 3.5-turbo: The molecule is the radioactive isotope of fluorine with relative atomic mass 18.000938 and atomic number 9. It is the longest-lived fluorine radionuclide with half-life of 109.77 min. It is used in positron emission tomography (PET) for imaging the brain, heart, and other organs.\nGemini Pro: This molecule is a radioactive isotope of fluorine with a relative atomic mass of 18.000938. It is the longest-lived fluorine radionuclide with a half-life of 109.77 minutes.\nLlama 2-70b: This molecule is a radioactive isotope of fluorine, with a relative atomic mass of 18.000938. It is the longest-lived fluorine radionuclide, with a half-life of 109.77 minutes. It has a number of applications in medical imaging and research, including positron emission tomography (PET) scans and fluorodeoxyglucose (FDG) tracers.\nLlama 3-70b: This molecule is fluorine-18 a radioactive isotope of fluorine with a relative atomic mass of 18.000938 and a half-life of 109.77 minutes making it the longest-lived fluorine radionuclide.\n2. SMILES string: CCN(CC)CCN1C(=O)CN=C(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3F\nOriginal caption: The molecule is a 1,4-benzodiazepinone that is 1,3-dihydro-2H-1,4-benzodiazepin-2-one substituted by a 2-(diethylamino)ethyl group, 2-fluorophenyl group and chloro group at positions 1, 5 and 7, respectively. It is a partial agonist of GABAA receptors and used for the treatment of insomnia. It has a role as a sedati gve, an anticonvulsant, a GABAA receptor agonist and an anxiolytic drug. It is a 1,4-benzodiazepinone, an organochlorine compound, a member of monofluorobenzenes and a tertiary amino compound.\nGPT 3.5-turbo: A 1,4-benzodiazepinone with a 1,3-dihydro-2H-1,4-benzodiazepin-2-one core, this molecule features a 2-(diethylamino)ethyl group, 2-fluorophenyl group, and chloro group at positions 1, 5, and 7, respectively. As a partial agonist of GABAA receptors, it exhibits sedative, anticonvulsant, and anxiolytic effects. Its therapeutic applications include the treatment of insomnia, making it a valuable pharmaceutical agent in managing sleep disorders.\nGemini Pro: The molecule is a 1,4- benzodiazepinone derivative with a 2- (diethylamino)ethyl side chain and substitutions of a 2-fluorophenyl group at position 5 and a chloro group at position 7. Known for its partial agonist activity on GABAA receptors, it is commonly used as a sedative, anticonvulsant, and anxiolytic agent for the treatment of insomnia. Its chemical structure includes a diazepine ring fused to a benzene ring with specific substituents, making it a potent thera-"}, {"title": "D Baseline Models Description", "content": "This section presents brief descriptions of baseline models included in this work.\nRNN (Cho et al., 2014). It introduces a novel approach for improving statistical machine translation through the use of Recurrent Neural Networks (RNNs). They propose an encoder-decoder architecture that learns continuous-space representations for phrases. The encoder processes an input phrase and compresses it into a fixed-dimensional vector, while the decoder uses this vector to generate the target phrase. This method allows for better handling of variable-length input sequences and capturing long-term dependencies in phrases, leading to significant improvements in translation quality compared to traditional models.\nTransformer (Vaswani et al., 2017). It introduces the Transformer model, a novel neural network architecture designed for sequence transduction tasks, such as machine translation. The Transformer model relies entirely on attention mechanisms to capture dependencies between input and output without using recurrent or convolutional layers. This self-attention mechanism allows for greater parallelization and better handling of long-range dependencies compared to previous models. They demonstrate that the Transformer achieves state-of-the-art performance on translation tasks, significantly improving both training speed and translation quality.\nT5 (Raffel et al., 2020). It presents the Text-to-Text Transfer Transformer (T5), a model designed to unify various NLP tasks by converting all tasks into a text-to-text format. They explore the capabilities of transfer learning within this framework, demonstrating that the same model, training objective, hyperparameters, and architecture can be applied across a wide range of NLP tasks. By pre-training on a massive and diverse dataset and fine-tuning specific tasks, T5 achieves state-of-the-art performance on numerous benchmarks. Additionally, they propose the CHEBI-20 dataset."}, {"title": "E Additional Experimental Results and Discussion", "content": "Anecdotal molecule generation examples. Table 2 shows some example molecules generated by MOLT5-Small and LAMOLT5-Small, and the ground-truth molecules from LACHEBI-20. From these examples, we can find that LAMOLT5-Small can generate accurate molecules similar to the ground truth, while MOLT5-Small is making mistakes. (1) is an interesting case since (i) its ground truth SMILES string has a long length, 88 characters for which LAMOLT5-Small is able to generate an exact match; (ii) indicates that LAMOLT5- Small can understand crystalline solids, like indolylmethylglucosinolate, in the annotation. In another interesting case, (2), LAMOLT5-Small not only understands chemical compounds but also can understand chemical treatments, e.g., replace-"}, {"title": "F.1 Dataset and Task", "content": "We consider four benchmark datasets, which contain image, text and graph data.\n1. ogbg-molbace. The ogbg-molbace dataset provides quantitative (IC50) and qualitative (binary label) binding results for a set of inhibitors of human b-secretase 1 (BACE-1). All data are experimental values reported in the scientific literature over the past decade, some with detailed crystal structures available. Task: ogbg-molbace merged a collection of 1,522 compounds with their 2D structures and binary labels, built as a classification task.\n2. ogbg-molhiv. The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for 41,127 compounds. Screening results were evaluated and placed into three categories: confirmed inactive (CI), confirmed active (CA) and confirmed moderately active (CM). We further combine the latter two labels, making it a classification task between inactive (CI) and active (CA and CM). Task: As we are more interested in discovering new categories of HIV inhibitors based on the available text and graph structure information.\n3. ogbg-molesol. ogbg-molesol is a small dataset consisting of water solubility data for 1,128 compounds. Task: We intend to estimate solubility directly from chemical graph structures (as encoded in text SMILES strings).\n4. CC3M. CC3M is a large-scale dataset comprising around 3.3 million image-caption pairs. It is designed for automatic image captioning tasks and represents a significant step forward in terms of the variety and volume of data compared to previous datasets like MS-COCO. Task: We follow the settings of (Fan et al., 2023) to train CLIP model (Radford et al., 2021) and test it on ImageNet (Deng et al., 2009)."}, {"title": "F.2 Automatic Annotation Augmentation", "content": "Given ogbg-molbace, ogbg-molhiv, and ogbg-molesol datasets, we first generate descriptions following the instruction of (Zhong et al., 2024a). Consequently, we query LLMs to augment these descriptions as described in Section 3.2. The prompt is designed as follows:\nInstruction: You are now a chemical specialist in rewriting descriptions for a molecule in SMILES format. Make sure those descriptions describe the given molecule correctly and precisely based on your two inputs (SMILES and Description of it). Also, make sure your rewriting captions do not include the input SMILES.\nMessage: SMILES string of target molecule: [SMILES].\nDescription of the molecule: [description].\nTask: Rewrite the following molecule with its SMILES and description. The newly rewritten caption should be elaborate, descriptive, and concise, highlighting the key structural features and biological activities of the molecule. Only output rewritten caption without any header and linebreak.\nAnswer:\nCC3M has available annotations for each image. We leverage LLMs to augment their annotations using this prompt:\nInstruction: You are now a specialist in rewriting descriptions for an image. Make sure those descriptions describe the given image correctly and precisely.\nMessage: Description of the image: [description].\nTask: Rewrite the following description. The newly rewritten caption should be elaborate, descriptive, and concise, highlighting the key knowledge of the molecule. Only output rewritten caption without any header and linebreak.\nAnswer:"}, {"title": "F.3 Training on Augmented Dataset", "content": "After obtaining the augmented datasets (ogbg-molbace, ogbg-molhiv, and ogbg-molesol), we simply combine three annotations of each molecule as the input features and integrate them within the LM and GNN models. Other training implementations follow the instruction of (Zhong et al., 2024a). About the CC3M dataset, we follow the implementation of (Fan et al., 2023) to integrate the augmented annotations with the CLIP model and evaluate them.\nResults in Table 5 show that LA\u00b3 significantly enhances performance across these diverse applications. This improvement highlights LA\u00b3 's potential to be a valuable tool in a wide range of AI tasks, offering substantial gains in accuracy and efficiency."}]}