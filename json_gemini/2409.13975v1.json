{"title": "ProTEA: Programmable Transformer Encoder Acceleration on FPGA", "authors": ["Ehsan Kabir", "Jason D. Bakos", "David Andrews", "Miaoqing Huang"], "abstract": "Transformer neural networks (TNN) have been widely utilized on a diverse range of applications, including natural language processing (NLP), machine translation, and computer vision (CV). Their widespread adoption has been primarily driven by the exceptional performance of their multi-head self-attention block used to extract key features from sequential data. The multi-head self-attention block is followed by feedforward neural networks, which play a crucial role in introducing non-linearity to assist the model in learning complex patterns. Despite the popularity of TNNs, there has been limited numbers of hardware accelerators targeting these two critical blocks. Most prior works have concentrated on sparse architectures that are not flexible for popular TNN variants. This paper introduces ProTEA, a runtime programmable accelerator tailored for the dense computations of most of state-of-the-art transformer encoders. ProTEA is designed to reduce latency by maximizing parallelism. We introduce an efficient tiling of large matrices that can distribute memory and computing resources across different hardware components within the FPGA. We provide run time evaluations of ProTEA on a Xilinx Alveo U55C high-performance data center accelerator card. Experimental results demonstrate that ProTEA can host a wide range of popular transformer networks and achieve near optimal performance with a tile size of 64 in the multi-head self-attention block and 6 in the feedforward networks block when configured with 8 parallel attention heads, 12 layers, and an embedding dimension of 768 on the U55C. Comparative results are provided showing ProTEA is 2.5\u00d7 faster than an NVIDIA Titan XP GPU. Results also show that it achieves 1.3 \u2013 2.8\u00d7 speed up compared with current state-of-the-art custom designed FPGA accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, transformer neural networks have become widely utilized on a diverse range of applications including natural language processing (NLP) [1], [2], neural machine translation [3], and image processing [4]. They are becoming favored over traditional recurrent neural network (RNN) and long short-term memory (LSTM) models for NLP tasks, and convolutional neural networks (CNN) for CV tasks. Their popularity is being driven by their ability to enable high computational parallelism for both the training and inference steps. Their natural exposure of higher levels of parallelism makes them well-suited for acceleration on hardware such as GPUs and FPGAs. There exist many transformer-based models such as full transformers containing both encoder and decoder [2], BERT [5], ROBERTa [6], Swin Transformers [7],\nstructBERT [8] etc. These models incorporate two notable features: a multi-headed attention (MHA) mechanism and feedforward neural networks (FFN) that distinguishes them from traditional CNNs, RNNs, and LSTMs. These MHA and FFN mechanisms are computationally expensive due to intensive matrix-matrix multiplications and complex data flows [9]. They account for a significant portion of runtime in many existing TNNs [10]. Unfortunately, executing TNNS is inefficient on general-purpose platforms such as GPUs and CPUs because of their high power consumption, low computational efficiency, underutilized memory bandwidth, and significant compilation overheads [11]. In addition to GPUs, FPGAs have become popular commercial off the shelf components used to accelerate DNNs. FPGAs offer the ability to exploit high level of parallelism to provide low run time inference latencies with efficient power consumption [12], [13]. Many studies have investigated how to increase the parallelization of CNNs, LSTMs, Graph Convolutional Networks [14]\u2013[17] on FPGAs to enhance performance. Recently, TNNs have been successfully deployed on FPGAS and application-specific integrated circuit (ASIC) hardware accelerators [18]\u2013[20]. Most implementations compress the model by using different weight pruning strategies, and reduce latency by incorporating sparse matrices. Thus, they use a specialized sparse architecture specific to each application. However, different applications require different sparsity patterns, necessitating the redesign of the hardware architecture for optimal results. This comes at the cost of time-consuming synthesis, and requires skills in digital design and computer architecture as well as detailed knowledge of each target logic family. Therefore, there is a need for a versatile accelerator capable of efficiently managing dense matrix computations across a range of TNN applications.\nThe study in [18] uses logic resources to implement\na systolic array for parallelism, which can lead to\nunderutilization of digital signal processing (DSP) units that\nare capable of high-speed computation at higher frequencies.\nDSP utilization also depends on the implementation method.\nFor instance, many accelerators [20]\u2013[23] employ high-level\nsynthesis (HLS) tools, while others use hardware description\nlanguage (HDL) [24]\u2013[26] for design. Although HLS requires\nless implementation time compared to HDL, writing efficient\nHLS code that effectively manages specific FPGA resources,\nsuch as DSPs, for optimal performance remains challenging\n[15]."}, {"title": "II. BACKGROUND", "content": "Transformers consist of several fundamental components, as depicted in Fig. 1. An input sequence of tokens is first converted into embeddings. The positional encoder adds positional information to these embeddings, enabling the model to account for the order of tokens in a sequence. This encoder generates vectors that provide context based on each word's position in a sentence. These vectors are then linearly transformed into three tensors: Q (queries), K (keys), and V (values) by multiplying the embedding matrix with three distinct weight matrices. The encoder block processes these tensors, transforming them into a higher-level representation that captures essential information. This transformation is crucial for accurately capturing features and contextual relationships within the input sequence. The encoder architecture is composed of two primary sub-layers: (1) the self-attention mechanism, and (2) the position-wise feed-forward network.\nThe self-attention mechanism allows the model to simultaneously evaluate different parts of an input sequence, capturing long-range relationships by calculating attention scores and using multi-head projections for various input representations. This capability enables the model to effectively learn complex patterns, dependencies, and relationships. The position-wise feed-forward network (FFN), similar to a multilayer perceptron (MLP), applies linear transformations independently to each position in the input sequence. This network performs two linear transformations, primarily involving matrix-vector multiplication. The first transformation includes activation functions such as the Rectified Linear Unit (ReLU) or Gaussian Error Linear Unit (GeLU), while the second transformation does not.\nAdditionally, each sub-layer incorporates a residual connection combined with layer normalization (LN), addressing the vanishing gradient problem during training. Residual connections and LN layers are added after each MHA and FFN layer, involving the addition of matrix elements and nonlinear functions."}, {"title": "III. RELATED WORK", "content": "Various FPGA and ASIC accelerators have been designed for TNNs. The ASIC design in [19] leveraged parallelism and specialized datapaths to achieve significant gains in performance and energy efficiency. Another ASIC, ELSA [10], employed specialized approximation algorithms to reduce computational demands. The SpAtten [31] ASIC utilized sparsity and quantization to decrease computations and memory access. Additionally, the hardware-software co-design framework Sanger [9] facilitated dynamic sparsity through a reconfigurable ASIC architecture. Despite these advancements, these solutions primarily focus on accelerating sparse attention mechanisms and do not address the deployment of full transformer models. The FPGA accelerator proposed by Lu et al. [18] is the first hardware architecture to accelerate both the MHA and FFN layers of the transformer. However, their implementation was done using HDL for a single attention head. A shared computing architecture is implemented in [32], where a parallel computing array is shared between M\u041d\u0410 and FFNs for a CNN application. A novel structural pruning method was proposed by [33] and the associated accelerator on FPGA was designed to reduce memory footprint. Peng et al. [21] explored column-balanced block-wise pruning for transformers and designed an FPGA accelerator for optimized block-wise matrix multiplication. An algorithm hardware framework [28] utilizes latency and accuracy constraints to determine the optimal sparsity ratio and select an appropriate FPGA platform. The energy-efficient acceleration framework FTRANS [29] features an improved block-circulant matrix method for algorithm-level sparsity, along with a custom-designed accelerator tailored for this approach. Wojcicki et al. [23] deployed a small TNN model on FPGA using HLS for experiments at the Large Hadron Collider. All of the existing hardware architectures are designed for a specific TNN and a specific sparsity pattern. They lack the flexibility to reconfigure the computing structure for different applications during runtime. EFA-Trans [25] is compatible with dense and sparse computing patterns, but it would need resynthesis of the hardware to switch between two options. Furthermore, none of them explored which tile size and what utilization DSPs could achieve optimum parallelism."}, {"title": "IV. ACCELERATOR ARCHITECTURE", "content": "The core of the accelerator is designed in C language on Vitis HLS 2022.2.1 tool. C simulation verifies the correctness of the algorithm, while C/RTL co-simulation ensures the functionality of the synthesized hardware. This section describes the high-level synthesis design technique that generates an optimized architecture utilizing most of the DSPs in the computation engines, ensuring high parallelism. The overall structure of the accelerator contains two main processing modules - the multihead attention (MHA) module and the feedforward network (FFN) module. The overall system was designed in Vivado 2022.1.2 design suite. It contains a custom IP block for the accelerator, which is exported from HLS. The inputs and weights are fetched from off-chip high-bandwidth memory (HBM) using AXI4 master interfaces [34] when the load instruction from the accelerator controller is received according to demand. The accelerator receives control signals from the processor through an AXI-lite slave interface [35]. Each hyperparameters of TNN can be programmed during runtime up to a maximum value by MicroBlaze (\u00b5B) softcore processor [36]."}, {"title": "A. Attention Module", "content": "The attention module (Fig. 3) comprises three computation engines (CE), labeled as QKVCE, QKCE, and SVCE based on their outputs. The number of these engines is determined by the number of attention heads (h). Each engine features an array of processing elements (PE), where each PE includes\na DSP48 for performing multiplication and accumulation\n(MAC) operations. The quantity of PEs denoted as 't' is\ninfluenced by the unrolling factor of the inner loop and\nthe initiation interval of the pipelined outer loop. Since the\ndata access patterns and computational needs vary across\ndifferent engines, each has separate function definition in\nHLS. This ensures that the synthesized RTL modules of\nthe engines contain distinct PE arrays, enabling individual\noptimization. Input data and weights are stored in multiple\nBRAMS/LUTRAMS to support parallel access.\nEach PE operates independently, equipped with its own memories, controller, and computing units. In HLS, the weights ($W_q, W_k, W_v$) for generating the query (Q), key (K), and value (V) matrices are defined as separate two-dimensional arrays of size ($d_{model} \\times TSMHA$). Here, $TSMHA$ represents the tile size in the attention module. It is the dimension of the sub-matrices into which the larger weight matrices are partitioned. The number of heads, tile size, and array partitioning directives in HLS determine how these arrays are divided to create multiple two-port BRAMs. To address the limited ports of BRAMs, array partitioning and data loading are optimized to ensure that data needed simultaneously by a DSP is stored in separate BRAMs. The Q, K, and V matrices, sized (SL \u00d7 dmodel), are stored in intermediate buffers. Here, SL stands for sequence length.\n1) QKVCE engine: QKVCE engine generates the query, key, and value matrices. This engine contains the Wq, Wk, W\u03c5 buffers, and input (X) buffers from which data is accessed in parallel by parallel DSP units. The arrays used in this engine are divided into subarrays using our tiling technique to fit into on-chip memories. The number of loop iterations in the QKVCE engine is determined by $TSMHA$, resulting in\na total of $(d_{model})$ tiles or iterations. During each iteration,\ndistinct data is loaded into the Wq, Wk, Wu, and Xi buffers.\nComputations then commence in the PEs, while biases for\nthe Q, K, and V matrices are simultaneously loaded into\nregisters from off-chip memory. These biases are subsequently\nadded to the Q, K, and V matrices. Algorithm 1 illustrates the\ncomputations of this engine, where the second loop (line 6) is\npipelined, resulting in the full unrolling of the innermost loop\n(line 8) and generating $(\\frac{d_{model}}{TSMHA})$ PEs.\n2) QKCE engine: The QKCE engine performs matrix-matrix multiplication between the Q and K matrices. Since these matrices are relatively small, they are not tiled. Algorithm 2 outlines the operations performed."}, {"title": "B. Feedforward Network Module", "content": "There are three CEs, denoted as $FFN1CE, FFN2CE, and FFN3CE$ in FFN to perform the operations of feedforward networks of different dimensions (Fig. 4). The definitions of the functions representing the CEs have different dimensions of arrays for the inputs and outputs in HLS. These arrays are converted into BRAMs/LUTRAMs after synthesis. The number of computations inside each engine is different, which is why each has a separate function in HLS. They contain a different number of processing elements (PE) after synthesis because of different unrolling factors of the innermost loop.\nThe weights are stored in a two-dimensional array (Wo) of size $(dmodel \\times (4 \\times dmodel))$ in HLS, where $TSFFN$ is tile size in FFN. $FFN1CE$ and $FFN3CE$ are followed by\nlayer normalization (LN) modules. Algorithm 4 describes the\ngeneral coding approach for an FFN engine.\n1) $FFNICE$ engine: FFN1CE engine performs the first linear transformation on the attention scores. The arrays used by the PEs are tiled along both dimensions. Thus, this engine is accessed $TSFFN \\times TSFFN$ times to finish the complete operation. The second for loop of the HLS code is pipelined\ncausing the innermost for loop to be fully unrolled. This\ngenerates TSFFN PES which equals to $ \\frac{4 \\times dmodel}{Tile\\ no.\\ FFN}$.\n2) $FFN2CE$ engine: FFN2CE engine performs second\nlinear transformation on the normalized outputs of FFN1CE\nengine. The arrays used by the PEs are tiled along both\ndimensions. Thus, this engine is accessed 4 \u00d7 TSFFN X\nTSFFN times to finish the complete operation. This engine\nalso contains TSFFN PES which equals to $ \\frac{4 \\times dmodel}{Tile\\ no.\\ FFN}$,\nbecause the trip count of the innermost loop is $Tile\\ no.\\ FFN$\nand it is fully unrolled.\n3) $FFN3CE$ engine: FFN3CE engine performs final linear\ntransformation on the normalized outputs of FFN2CE engine.\nThe arrays used by the PEs are tiled along both dimensions.\nThus, this engine is accessed 4 \u00d7 TSFFN \u00d7 TSFFN times\nto finish the complete operation. The complete unroll of the\ninnermost loop generates 4 \u00d7 TSFFN PEs in it, which equals\nto $ \\frac{4 \\times dmodel}{Tile\\ no.\\ FFN}$."}, {"title": "C. Tiling Technique", "content": "Since transformer models are typically large, tiling is used to manage the utilization of on-chip memory and computing units effectively. It ensures that the HLS tool can efficiently partition arrays and pipeline or unroll loops to minimize latency while keeping compilation time short. The weight matrices are divided into tiles, enabling BRAMs to be loaded with partial data fetched from off-chip memory. Tiling is applied only along the second dimension (columns) of the matrix because the first dimension (rows) is already reduced by the number of heads. Consequently, each matrix is loaded $(d_{model}/TSMHA)$ times. The input buffers for each attention head are defined as a two-dimensional array of size (SL \u00d7 TSMHA), and tiling is similarly applied along the column of the matrix, resulting in $(d_{model}/TSMHA)$ loads. During each iteration, data for one tile is loaded initially. The PEs then compute on this data, storing the results in intermediate buffers, which are accumulated with results from previous iterations. Ultimately, the final output is the cumulative sum of the results computed across all tiles."}, {"title": "D. Runtime Configurable Capability", "content": "The runtime-programmable parameters such as the number of attention heads, number of layers, embedding dimension, and sequence length can be sent to ProTEA via software running on the \u00b5B processor. TNN models are trained using the PyTorch framework, and the resulting models should be saved as '.pth' files. These files are then processed by a Python interpreter to extract key parameters such as the number of attention heads, layers, embedding dimension, and sequence length. While these parameters will vary across applications, ProTEA does not require resynthesis for each model; only minor software modifications are necessary. The software, developed in C++ using the Xilinx SDK tool, utilizes the extracted data to generate instructions and control signals. These signals guide the processor in activating the relevant parts of the accelerator hardware."}, {"title": "E. Tile Size Determination", "content": "In ProTEA, the programmable parameters can be adjusted at runtime, whereas the tile size must be set before synthesis, as it cannot be modified without resynthesizing the entire\nhardware. The graph in Fig. 7 illustrates how variations in\n$TSMHA$ and $TSFFN$ impact system frequency (MHz) and\nlatency (normalized to the minimum value). The number of\ntiles in MHA $(d_{model}/TSMHA)$ was varied from 6 to 48, and for\neach MHA tile count, the number of tiles in FFN $(4 \\times dmodel/TSFFN)$\nranged from 2 to 6. The results indicate that the optimal\nconfiguration for achieving the highest frequency (blue color)\nand lowest latency (green color) was 12 tiles in MHA and\n6 tiles in FFN. This setup achieved a maximum frequency\nof 200 MHz, allowing ProTEA to execute all transformer\nneural network models discussed in Section V. Moreover,\nexperiments showed that TSMHA of 64 and TSFFN of 128\nare optimal for HLS, allowing for efficient array partitioning\nwithin a reasonable compilation time (approximately 36 hours)\nfor a state-of-the-art (SOTA) transformer encoder."}, {"title": "V. EVALUATION AND RESULTS", "content": "Table I presents the runtime programmability, resource utilization, and performance metrics of ProTEA. The reported latency reflects the computation time, accounting for the overlap of data loading and computation. The synthesis was conducted with fixed tile sizes of TSMHA = 64 and TSFFN = 128, as these values are set before synthesis and cannot be altered afterward. Data was quantized to 8-bit fixed-point format; while this might result in accuracy loss depending on the application, it was not a primary focus. For applications requiring a larger bit width, the design can be easily modified in the HLS code, which will impact both resource utilization and latency. The accelerator's design parameters, including the embedding dimension (dmodel), number of heads (h), number of layers (N), and sequence length (SL), were initially configured with fixed values - 768, 8, 12, and 64 respectively based on a variant of BERT [5"}]}