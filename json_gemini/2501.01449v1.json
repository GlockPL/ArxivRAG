{"title": "LS-GAN: Human Motion Synthesis with Latent-space GANs", "authors": ["Avinash Amballa", "Gayathri Akkinapalli", "Vinitra Muralikrishnan"], "abstract": "Human motion synthesis conditioned on textual input has gained significant attention in recent years due to its potential applications in various domains such as gaming, film production, and virtual reality. Conditioned Motion synthesis takes a text input and outputs a 3D motion corresponding to the text. While previous works have explored motion synthesis using raw motion data and latent space representations with diffusion models, these approaches often suffer from high training and inference times. In this paper, we introduce a novel framework that utilizes Generative Adversarial Networks (GANs) in the latent space to enable faster training and inference while achieving results comparable to those of the state-of-the-art diffusion methods. We perform experiments on the HumanML3D, HumanAct12 benchmarks and demonstrate that a remarkably simple GAN in the latent space achieves a FID of 0.482 with more than 91% in FLOPs reduction compared to latent diffusion model. Our work opens up new possibilities for efficient and high-quality motion synthesis using latent space GANs.", "sections": [{"title": "1. Introduction", "content": "Human motion synthesis has recently seen rapid advancements in a multi-modal generative fashion, fueled by various conditional inputs such as music [8, 25, 27], action categories [16, 33], and notably, natural language descriptions [2, 12, 13, 23, 34, 49]. This field significantly enhances industries like gaming, film production, and virtual/augmented reality, with text-based conditioning standing out for its convenience and interpretability. However, learning a probabilistic mapping function from textural descriptors to motion sequences is challenging [47] and this mapping often leads to misalignments and high computational demands due to stark differences in distributions between language descriptors and motion sequences, making the task of probabilistic mapping complex.\nConditional diffusion models [23, 49, 55] address this problem by learning a more powerful probabilistic function from the textual descriptors to motion sequences. However, diffusion models in raw sequential data require computational overhead in both traning and inference. To overcome this, motion latent diffusion (MLD) [7] address these issues by encoding motion in a latent space using a Variational Autoencoder (VAE). However, MLD relied on computationally intensive diffusion processes to achieve high-quality image sampling, especially during the training and inference phases.\nTo efficiently model the motion synthesis, we propose substituting the diffusion model [18] in the latent space with a Generative Adversarial Network (GAN) [10] to capitalize on its efficient adversarial training dynamics. Recognizing the effectiveness of GANs in learning complex representations across diverse modalities [22, 43], and their efficiency in training and inference compared to diffusion models, we propose to utilize them within this latent space. By leveraging GANs, we aim to accelerate the mapping between text embeddings and latent space, thus producing higher-quality motion sequences more efficiently.\nSpecifically, this work undertakes the task of text-to-motion and action-to-motion synthesis using conditional Generative Adversarial Networks [32] in latent space, as depicted in the accompanying figure:1. We employ a Variational Autoencoder (VAE) to transition from motion space to latent space and utilize pre-trained CLIP models from MLD [7] to condition on textual input. We experiment with various GAN architectures, including vanilla GAN, deep GAN, with loss functions such as cross-entropy and Wasserstein [11] to optimize performance and fidelity in generated motion sequences. Our experiment results on HumanML3D [14] benchmark suggest that a simple GAN architecture achieves an FID of 0.482 with 91% in FLOPS reduction compared to MLD. In addition, our method shows competitive performance on action-to-motion HumanAct12 [16] benchmark. This strategic shift of GANs in latent space not only addresses the computational inefficiencies associated with previous diffusion-based models but also leverages the rapid generative capabilities of GANs to enhance the quality and diversity of motion synthesis, suitable for real-time applications."}, {"title": "2. Related work.", "content": "Motion Synthesis is broadly categorized into conditional and unconditional motion synthesis. Unconditional motion synthesis models the entire motion space without requiring specific annotations, is discussed by Raab et al. [38] in an unsupervised setting using unstructured and unlabeled datasets. Conditional motion synthesis, on the other hand, employs inputs from various modalities such as music [28] and text [24] to generate motion sequences. Text-to-motion synthesis, in particular, has become a dominant area of research due to the user-friendly nature of natural language interfaces. Additional recent advancements in the field include the development of joint-latent models like TEMOS [35] and conditional diffusion models [24, 50, 55], which have led to significant progress. TEMOS, uses a VAE architecture to create a shared latent space for motion and text based on a Gaussian distribution.\nMotion diffuse [55] is the first text-based motion diffusion model with fine-grained instructions on body parts.\nMDM [48] proposes a motion diffusion model on raw motion data to learn the relation between motion and input conditions. Our work closely relates to the Motion Latent Diffusion (MLD) model [7] which utilizes a Variational Autoencoder (VAE) to encode human motion sequences into a low-dimensional latent space and decode them back to motion sequences. The MLD model then employs diffusion processes in this latent space, inspired by other latent diffusion models [41]. To condition the motion sequences on specific inputs like text or actions, the model utilizes CLIP encodings [39], demonstrating robust performance on tasks such as text-to-motion and action-to-motion.\nMoreover, approaches like MotionGPT [19] integrates language modeling for both motion and text, treating human motion as a distinct language to construct a generalized model capable of executing various motion tasks through VQ-VAE [51]. T2M-GPT [54] uses a standard 1D convolutional network to map motion sequences to discrete code indices, followed by standard GPT-like model is learned to generate sequences of code indices from pre-trained text embedding. The use of GAN networks for motion synthesis has been done in Ganimator [26] but uses an additional motion sequence as conditional input. On the other hand,"}, {"title": "3. Method", "content": "While diffusion models have shown tremendous promise and exhibit state-of-the-art performance they are expensive to train, requiring a huge corpus of data. The use of latent space in MLD [7] opens up avenues for other architectures such as GANs to also leverage it. Specifically, given an input condition $c$ describing a motion, our Latent space GAN (LS-GAN) aims to generate a human motion $x_{1:L}$ where $L$ represents the motion length."}, {"title": "3.1. VAE and CLIP", "content": "Our VAE architecture is borrowed from the MLD [7], which uses transformer model as Encoder $\\mathcal{E}$ and Decoder $\\mathcal{D}$ with skip connections. The motion encoder $\\mathcal{E}$ encodes the motion sequences, $x_{1:L}$ into a latent $z = \\mathcal{E}(x_{1:L})$, and the decode $z$ into the motion sequences using the decoder $\\mathcal{D}$, i.e., $\\hat{x}_{1:L} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x_{1:L}))$. VAE is trained in a similar fashion as MLD with the MSE and KL divergence loss. After training, the VAE is kept fixed. We use pretrained CLIP-ViT-L-14 [40] text encoder to map text prompt. On the other hand to condition on action, we use the learnable embedding for each action category."}, {"title": "3.2. Latent space GAN", "content": "We chose GANs for 3 reasons (1) their effectiveness in learning complex representations across diverse modalities [22, 43], (2) the flexibility of implementing any architecture for the generator and discriminator and the potential adversarial training offers, (3) reduced training and inference time compared to Diffusion models. We discuss the GAN challenges in section 7\nOur method overview is shown in figure:3, where we adapt the conditional GAN [32] architecture to latent space. In particular, the generator $G$ takes the latent $z$ and conditioned input $c$ and generates the fake motion latent space $z' = G(z, c)$. On the other hand, discriminator $D$ learns to differentiate between real motion latent space $\\mathcal{E}(x)$ and fake motion latent space $z'$ conditioned on $c$. We write the training objective of LS-GAN as a two-player min-max game with: $\\min_{G} \\max_{D} E_{z\\sim \\mathcal{E}(x)}[\\log D(z, c)] + E_{z\\sim p_z(z)}[\\log(1 - D(G(z, c), c))]$. During generation, we decode $z'$ into the motion sequences using the decoder $D$, that is $\\hat{x}_{1:L} = \\mathcal{D}(z') = \\mathcal{D}(G(z, c)).$"}, {"title": "3.3. GAN architectures", "content": "We experiment with two different LS-GAN architectures in the latent space setting.\nVanilla GAN: The Generator comprises three fully connected layers and the Discriminator consists of four fully connected layers. Both models employ leaky ReLU activation to all layers preceding the final layer.\nDeep GAN: We add two residual blocks between the fully connected layers in both the Generator and discriminator architectures. Residual connections [17] helps to train deeper networks by overcoming the vanishing gradients."}, {"title": "4. Dataset, Loss and Evaluation metrics", "content": ""}, {"title": "4.1. Dataset", "content": "Text-to-motion: HumanML3D [14] is a 3D human motion-language dataset which covers a wide range of human actions including human activities like walking, jumping, swimming, playing golf etc. It contains 14,616 motion sequences from AMASS [31] and annotates 44,970 sequence-level textual descriptions. Here, we employ the motion representation as combination of: 3D joint rotations, positions, velocities, and foot contact.\nAction-to-motion: HumanAct12 [16] is a action-to-motion language dataset that provides 1,191 raw motion sequences and 12 action categories."}, {"title": "4.2. Loss", "content": "We experiment with Binary Cross entropy (BCE) and Wasserstein loss [4]. We use sigmoid activation on the discriminator with BCE loss. We use Gradient penality [11] instead of weight clipping in Wasserstein GAN."}, {"title": "4.3. Metrics", "content": "To assess the performance of our models, we utilize metrics as in MLD [7]: FID, R-precision, Diversity, Multimodality, Multimodal Distance (MM Dist), Average position error(APE), Average variance error (AVE). To measure the computational workload, we use FLOPs."}, {"title": "5. Training details and Results", "content": ""}, {"title": "5.1. Implementation details", "content": "We borrow the Motion transformer encoders $\\mathcal{E}$ and decoder $\\mathcal{D}$ from in MLD [7]. Our VAE model consists of 9 layers and 4 heads with skip connections. To train VAE, we follow the same loss configuration as MLD. All our models are trained on A100 GPU with AdamW optimizer using a fixed learning rate of $10^{-4}$. Our batch size is set to 128 during the VAE training stage and 64 during the LS-GAN training stage. We report the test metrics on the training checkpoints with the lowest FID. In all of our experiments, we use the latent dimension $z \\in \\mathbb{R}^{1\\times 100}, z' \\in \\mathbb{R}^{1\\times 256}$. We choose $c\\in \\mathbb{R}^{1\\times 768}$ for text-to-motion task and $c\\in \\mathbb{R}^{1\\times 10}$ for action-to-motion task."}, {"title": "5.2. Text-to-motion", "content": "For text-to-motion, we utilize the VAE checkpoint from iteration 1250 and keep it fixed during GAN training. For detailed evaluation metrics of the VAE, refer table:4. Figures: 1, 2 show the qualitative results for the text-to-motion task with LS-GAN (Refer Appendix 9.1 for more results).\nTable:1 summarizes the test metrics with mean and 95% confidence interval from 20 times running (most of the results are borrowed from MLD [7]). We observe that the vanilla and deep GAN architectures gave the best empirical metrics and qualitative results when used with wasserstein loss with gradient penality. Table:2 depicts the total number of floating-point operations on 2048 motion clips.\nOur Deep WGAN-GP achieves a FID of 0.482 that is near-parity with the state-of-the-art MLD [7] (FID of 0.473) with 91% in FLOPs reduction as shown in Table:2. It outperforms MDM [48] in R precision, FID, MM Dist, MModality. It also achieves state-of-the-art across MModality compared to all the previous models. Furthermore, our LS-GAN outperforms cross-modal models such as Seq2Seq [37], LJ2P [3], T2G [5], Hier [9], TEMOS [36], T2M [15] across all evaluation metrics. This signifies high-quality motion and high text prompt matching while maintaining a rich motion diversity as evident in Figures:1, 2. These results demonstrate that a simple GAN in latent space can achieve impressive results with minimal compute in both training and inference compared to the Diffusion models."}, {"title": "5.3. Action-to-motion", "content": "The action-conditioned task involves generating motion sequences based on an input action label. We compare our Deep WGAN-GP with ACTOR [33], INR [6], MDM [49], and MLD [7]. ACTOR and INR are transformer-based VAE models specifically designed for the action-conditioned task. In contrast, MDM and MLD are diffusion models that utilize the same learnable action embedding module as our method. We report the test metrics as the mean and 95% confidence interval computed from 20 independent runs.\nFrom table:3, we observe that Deep WGAN-GP outperforms all the other models in Diversity while maintaining competitive performance on FID, accuracy and Multi-Modality (MM). These results indicate that GAN in motion latent can also benefit action-conditioned motion generation task."}, {"title": "6. Latent space visualization", "content": "In this section, we present t-SNE visualizations of the latent space on action-to-motion task, illustrating how our LS-GAN effectively captures and separates different actions within the latent space. These results are compared with the MLD [7] in Figure 4.\nFrom the latent space visualization, it is evident that Vanilla GAN and Deep GAN have low MultiModality scores (measures the generation diversity within the same text or action input), while Vanilla WGAN-GP and Deep WGAN-GP have higher MultiModality highlighting the effectiveness of the Wasserstein loss with gradient penalty. This observation even holds true for the text-to-motion generation task, as evidenced by the results presented in Table 1. Furthermore, our approach shows superior separation of latent code clusters at timestep $t = 0$ compared to MLD. This improved clustering at $t = 0$ indicates that our LS-GAN framework captures a more structured motion latent representation, potentially leading to better interpretability and generation fidelity."}, {"title": "7. Discussion", "content": ""}, {"title": "7.1. Addressing GAN challenges:", "content": "Usage of condition information in our model helps us to overcome mode collapse challenge by conditioning the model on additional information. In addition, our generator learns the inherent features of real motion data. This encourages the discriminator to compare the underlying properties instead of the high dimensional real data, similar to Feature Matching [42] that helps to stabilize training. Methods such as regularization, spectral normalization, adaptive learning rates, multiple generators/discriminators, auxiliary loss as discussed by [45] can be further explored to stabilize GAN training."}, {"title": "7.2. Accelerated Diffusion:", "content": "Diffusion distillation [30, 44, 53] is a knowledge distillation task, where a student model is trained to distill the multi-step outputs of the original diffusion model into a single or few steps. These prior works, require a separate pre-training and distillation phase. In addition, one-step diffusion models require a greater attention in choosing the training objectives and scheduling mechanism [53]. On the other hand, recent works on GANs [20,43] shows that StyleGAN-T, Giga-Gan outperforms Distilled diffusion models on text-to-image generation. Considering these, we believe GAN in latent space would serve as a solution to accelerated diffusion."}, {"title": "7.3. Limitations", "content": "First, similar to most motion generation methods, our approach can generate motion sequences of arbitrary lengths, but still below the maximum length in the dataset. Secondly, LS-GAN specifically targets human body motion, in contrast to works focusing on facial motion [21] or hand motion [29]. Lastly, we limited ourselves to simple prompts for text-to-motion. It may be beneficial to consider the impact of motion outputs on edge cases and ambiguous text descriptions."}, {"title": "8. Conclusion", "content": "In this paper, we introduced a novel approach for text-to-motion and action-to-motion synthesis using Generative Adversarial Networks in the latent space. By leveraging the power of GANs and the compact representation of motion sequences in the latent space, our method achieves faster training and inference times compared to previous methods while maintaining high-quality motion synthesis results. Results demonstrate that a simple GAN in latent space is comparable to complex models. This work will open a new direction in exploring latent space GANs that can have faster stable training and inference compared to latent space diffusion."}]}