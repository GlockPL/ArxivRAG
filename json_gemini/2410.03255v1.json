{"title": "Towards a Benchmark for Large Language Models for Business Process Management Tasks", "authors": ["Kiran Busch", "Henrik Leopold"], "abstract": "An increasing number of organizations are deploying Large Language Models (LLMs) for a wide range of tasks. Despite their general utility, LLMs are prone to errors, ranging from inaccuracies to hallucinations. To objectively assess the capabilities of existing LLMs, performance benchmarks are conducted. However, these benchmarks often do not translate to more specific real-world tasks. This paper addresses the gap in benchmarking LLM performance in the Business Process Management (BPM) domain. Currently, no BPM-specific benchmarks exist, creating uncertainty about the suitability of different LLMs for BPM tasks. This paper systematically compares LLM performance on four BPM tasks focusing on small open-source models. The analysis aims to identify task-specific performance variations, compare the effectiveness of open-source versus commercial models, and assess the impact of model size on BPM task performance. This paper provides insights into the practical applications of LLMs in BPM, guiding organizations in selecting appropriate models for their specific needs.", "sections": [{"title": "1. Introduction", "content": "An increasing number of organizations are deploying Large Language Models (LLMs), either by developing their own models (Fedus, Zoph, and Shazeer 2022; Raffel et al. 2023) or utilizing readily available ones through APIs, such as OpenAI's GPT-3 (Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Remi Louf, et al. 2020). These LLMs have demonstrated significant utility across a wide range of tasks. They do, however, make mistakes, which can vary from inaccuracies to hallucinations (Shuster et al. 2021; Zhou et al. 2021). Therefore, to objectively quantify the capabilities of LLMs, so-called performance benchmarks are conducted. Such benchmarks have a long history in Natural Language Processing (NLP) research and typically involve solving a particular task on a dedicated dataset (Marcus, Santorini, and Marcinkiewicz 1993; Pradhan et al. 2012). Common examples include text natural language inference, sentiment analysis, and question answering (Kiela et al. 2021).\nWhat these benchmarks have in common is that they focus on rather generic tasks. As a result, LLMs might perform extraordinarily well in the context of benchmarks and might yet fail on simple real-world tasks that are more specific (Kiela et al. 2021). This raises the question, to what extent the performance results reported from benchmarks can be transferred to more specific domains. One domain that is of particular importance for many organizations is the Business Process Management (BPM) domain. That is, because most organizations employ LLMs in the first place to increase efficiency (Waber and Fast 2024). As this, ultimately, impacts the way how business processes are designed and executed, understanding how LLMs perform on typical BPM tasks, such as process analysis or process prediction, is highly relevant (Grohs et al. 2023). Currently, however, there are no BPM-specific benchmarks available, leaving it unclear which LLMs are suitable for which BPM task (Busch et al. 2023). It also remains unclear whether the widely propagated insight that performance generally increases with model size (Brown et al. 2020; J. Kaplan et al. 2020) can be transferred to the BPM domain.\nRecognizing this, we use this paper to systematically compare the performance of LLMs on four established BPM tasks. We chose both open-source as well as closed-source LLMs. Among others, this allows us to understand whether there are differences across different tasks, whether open-source and commercial models perform similarly well, and to what extent the size of an LLM affects the performance.\nThe rest of the paper is structured as follows. Section 2 elaborates on the use of LLMs in the context of BPM and introduces the analyzed BPM tasks. Section 3 introduces our experimental setup. Section 4 discusses the results and Section 5 the implications before Section 6 concludes the paper."}, {"title": "2. Business process management tasks", "content": "Business Process Management aims to understand, analyze, and improve how work is done in an organization (Dumas et al. 2018). To accomplish this, BPM experts in industry employ a diverse set of means ranging from management methodologies, such as Six Sigma, to specific analytical tools, such as process mining. Since a considerable amount of knowledge pertaining to business processes is captured in data sources that contain text (e.g., work instructions, event logs, e-mails), NLP has become increasingly popular over the last decade in BPM research (Van der Aa et al. 2018). With the introduction of GPT3, the focus has shifted to the use of LLMs (Busch et al. 2023).\nTo develop an understanding of how LLMs can be effectively used in the BPM domain, we evaluate their BPM-related capabilities. Specifically, we chose four representative tasks, each addressing a specific BPM problem:\n1.  Activity recommendation: This task helps us to assess the LLM's ability to understand and predict logical activity sequences in business processes.\n2.  Identifying robotic process automation (RPA) candidates: This task helps us to evaluate the model's capability to differentiate and categorize various types of tasks, highlighting potential automation opportunities.\n3.  Process question answering: This task tests the LLM's comprehension and information extraction skills from textual process descriptions.\n4.  Mining declarative process models: This task allows us to examine the model's proficiency in deriving process constraints from natural language.\nWe briefly explain each BPM task in detail in the following sections. Note that although we use these BPM tasks in this paper, they are only intended as a starting point for various additional analyses in the BPM area."}, {"title": "2.1. Activity recommendation", "content": "Activity recommendation is an important task in the context of Business Process Modeling (Sola, Aa, et al. 2023). It involves predicting a suitable subsequent activity in a process model based on a sequence of already modeled activities. We define each instance in this problem as a tuple (H, A), where H denotes a history of activities already modeled, arranged in a sequence that corresponds to the logical sequence of the process, and A is the next recommended activity to be added to the process model. The objective is to recommend an activity A that suitably follows the sequence H. This task approaches as a sequence prediction problem, where the sequence of past activities (H) is used to predict the next activity (A). For example, consider the already modeled activity sequence H = [receive loan application, check credit history, approve application]. A suitable next activity could be A = send approval."}, {"title": "2.2. Identifying RPA candidates", "content": "In the context of BPM, the task of identifying RPA candidates involves classifying activities based on the nature of their execution. Each activity is categorized as a manual task, a user task, or an automated task (Dumas et al. 2018). A manual task refers to activities where no IT system is involved. A user task involves a human interacting with an IT system, and an automated task is performed entirely by IT systems without human involvement. We define each instance in this problem as a tuple (E, C), where E represents the activity label and C is the classification of the activity into one of the three categories: manual, user, or automated. The objective is to accurately classify the activity label E into its respective category C. This task is approached as a multi-class classification problem, where the input activity label (E) is analyzed to predict its category (C). This classification helps in identifying potential RPA candidates by distinguishing between tasks that are currently manual or user-based and those that are automated. For example, given an activity label E = generate invoice, the model classifies it as C = automated task."}, {"title": "2.3. Process question answering", "content": "In the context of Business Process Modeling, the task of process question answering involves understanding a textual process description and providing accurate answers to questions related to that description. This task is crucial for assessing the extent to which an LLM comprehends the given process description, which is an essential step for effective process modeling. We define each instance in this problem as a triple (P, Q, A), where P represents the textual process description, Q denotes the question related to the process, and A is the expected answer. The objective is to predict the correct answer A given the process description P and the question Q. This task is approached as a reading comprehension problem, where the input process description (P) and question (Q) are analyzed to generate the appropriate answer (A). This evaluation helps in determining how well an LLM can interpret and extract relevant information from process descriptions. For example, consider the simplified order handling process description P = \"The process starts with customer order and ends with product delivery.\" and Q = \u201cWhat is the final step in the process?\". A correct answer is A = \u201cproduct delivery\"."}, {"title": "2.4. Mining declarative process models", "content": "Mining declarative process models from textual descriptions is a task that aims to extract flexible and formal process constraints from natural language. Unlike imperative modeling notations, which specify exact sequences of activities, declarative models use constraints to allow more adaptable and knowledge-intensive process definitions. In this context, each instance of the problem is defined as a tuple (T, D) where T represents a textual description of a process, and D denotes the set of declarative constraints derived from the text. The objective is to accurately generate a set of constraints D that corresponds to the process described in T. A way to employ declarative process modeling is the declarative process language DECLARE (Di Ciccio and Montali 2022). We follow the procedure by Grohs et al. 2023 and use the following constraints:\n\u2022 Initiation constraint: Init(a) \u2013 The process starts with activity a.\n\u2022 Termination constraint: End(a) \u2013 The process ends with activity a.\n\u2022 Precedence constraint: Prec(a, b) \u2013 Activity b occurs in the process instance only if preceded by activity a.\n\u2022 Succession constraint: Succ(a, b) \u2013 Activity a (b) occurs if and only if it is followed (preceded) by activity b (a) in the process.\n\u2022 Response constraint: Resp(a, b) \u2013 If activity a occurs in the process, then activity b occurs after a.\nThis task can be approached as a Natural Language Processing problem where rule-based techniques are applied to sentences to identify and generate these constraints. Consider the textual process description T=\"The process begins with registration and concludes with certification issuance.\". A valid constraint would be D =Init(registration)."}, {"title": "3. Experimental Setup", "content": "The approach pipeline for this benchmark involves three steps: Initially, we collect or create different datasets tailored to each specific BPM task. Then we prompt the LLMs with task-specific instructions. This involves carefully creating prompts that align with the requirements of each BPM task to ensure that the LLMs generate relevant and accurate responses. Finally, we evaluate the performance of the LLMs. This evaluation is based on predefined metrics that are relevant to the respective BPM tasks. The following sections provide a detailed overview of the datasets and LLMs employed, as well as the prompt templates and metrics used."}, {"title": "3.1. Datasets", "content": "To test the LLMs on the different BPM tasks, we use three task-specific datasets.\nDataset 1. To evaluate the activity recommendation task, we construct a new dataset derived from real-world process models available in the SAP Signavio Academic Models (SAP-SAM) collection (Sola, Warmuth, et al. 2022). This collection comprises over one million processes from various domains, represented in different modeling notations and languages. To ensure the dataset's quality, we implement a series of filtering and cleaning operations. Initially, we exclude vendor-provided examples, as these are likely to be duplicates, in accordance with the guidelines established by Sola, Warmuth, et al. 2022. We further refine the dataset by selecting only models that utilize the BPMN 2.0 notation and contain English language labels. Subsequently, we employ several label cleaning techniques to improve label consistency, like removing non-alphanumeric characters, addressing special cases such as line breaks, converting all text to lowercase, and eliminating unnecessary spaces. From the refined collection, we randomly select 300 process models. To facilitate the extraction of sequences, we convert the selected process models to event logs. Whenever possible, we extract a sequence of four consecutive activities from each event log. The first three activities in each sequence represent the modeled situation, while the fourth activity represents the optimal subsequent activity. We thus adopt the procedure described by Sola, Aa, et al. 2023. This results in a final set of 288 test samples suitable for evaluating the activity recommendation task.\nDataset 2. For the BPM tasks RPA candidate identification and declarative process model mining, we use the available datasets from Grohs et al. 2023, which consist of 424 and 104 test samples, respectively.\nDataset 3. For the process question and answer task, we create a data set based on four process descriptions: dispatch of goods, recourse, credit scoring, and self-service restaurant from Camunda 2013. For each process description we create a set of questions and answers that reflect different aspects of the processes. We create a total of 15 questions and answers across three levels of complexity: easy, medium and complex. We define the different level heuristically based on the complexity and depth of information required to accurately answer the question. The easy questions focus on straightforward, factual details, such as \u201cWho writes the package label for small shipments?\". These questions are designed to assess basic understanding and recall of the process steps. Medium questions require a deeper understanding and the ability to sequence events correctly, such as \"What is the sequence of steps if special shipping is required?\". These questions test the ability to integrate multiple steps and understand conditional sequences within the processes. Complex questions involve higher-order thinking and scenario-based problem-solving, such as \u201cIn what scenarios do you close the case without involving a collection agency?\". These questions assess the ability to apply knowledge in dynamic contexts and understand exceptions and special conditions. This procedure results in a dataset of 60 samples."}, {"title": "3.2. Selected language models", "content": "To select suitable LLMs for our benchmark, we make the following heuristic considerations: First, we prioritize models that are recently published to ensure that our benchmark incorporates the latest advancements in LLM technology. Second, we select lightweight models in terms of parameter size to ensure practicality in real-world applications. Models with fewer parameters are generally more resource-efficient, enabling broader accessibility and easier deployment in various BPM environments. Third, a significant criterion is the availability of models under open-source licenses. Open-source models allow for transparency, reproducibility, and customization, which are crucial for research and development. Fourth, to provide a comprehensive evaluation, we include GPT-4 (OpenAI et al. 2024), a well-known closed-source model. This inclusion allows us to benchmark open-source models against a leading, high-performance model. As a result, we select seven LLMs: GPT-4, Phi-3 Medium (Abdin et al. 2024), Claude 2 (Chen et al. 2023), Falcon 2 (TII 2024), Mixtral-8x7b (Jiang et al. 2024), Llama 3 (AI@Meta 2024), and Yi-1.5 (AI et al. 2024)."}, {"title": "3.3. Prompt templates", "content": "To enable LLMs to perform the BPM task effectively, it is essential to develop task-specific prompt templates that outline how prompts should be structured. This aspect can be particularly challenging, as the performance of LLMs is highly sensitive to the prompt templates used (Webson and Pavlick 2021; Perez, Kiela, and Cho 2021; Zhao et al. 2021). We identified three widely employed prompt templates as summarized in Table 2. The Few-shot example prompt (i) provides the LLM with a task description followed by three examples. The inclusion of examples serves as a guide to help the model understand the task more concretely and generate responses that align with the demonstrated patterns. The Persona prompt (ii) involves framing the LLM as a specialist for the forthcoming task. By explicitly defining the model's role, we aim to elicit informed and contextually appropriate answers. Finally, the Step-by-step prompt (iii) encourages the LLM to approach the task methodically by prompting it to think through the problem step by step. This method is intended to enhance the model's logical reasoning capabilities. Additionally, each prompt template includes an output instruction that specifies how the model should format its response, ensuring consistency in the generated outputs.\nFor the purpose of this paper, we initially focus on the few-shot prompting strategy as this has been shown to yield the best results in various contexts (Brown et al. 2020; Perez, Kiela, and Cho 2021). We, however, also conduct a separate robustness analysis to gain detailed insights into the impact of the prompting strategy."}, {"title": "3.4. Metrics", "content": "To evaluate the performance of the LLMs on various BPM tasks, we use several metrics.\nActivity recommendation. For this task, we use the average cosine similarity (Li and Han 2013) between the embeddings of the true next activity and the predicted next activity. This metric assesses how closely the model's recommendation aligns with the expected next activity in the sequence. Let A be the true next activity and the model's recommendation be \u00c2. We embed A and A using a pre-trained BERT model (Devlin et al. 2019) to obtain their vector representations A and \u00c2. Based on the cosine similarity $S_c$ between these two vectors, we calculate the average similarity $S_{avg}$ across all samples in the dataset:\n$S_{avg} = \\frac{1}{N} \\sum_{i=1}^{N} S_c(A_i, \\hat{A_i})$\nwhere N is the total number of samples in the dataset.\nIdentifying RPA candidates. For this task, we use the standard information retrieval metrics precision (prec), recall (rec), and F1-score (F1) to evaluate the classification accuracy. Each activity is classified into one of three categories: manual, user, or automated. Given the number of true positives (TP), false positives (FP), and false negatives (FN), precision and recall are defined as follows:\n$prec = \\frac{TP}{TP + FP}$, $rec = \\frac{TP}{TP + FN}$\nThe F1-Score is defined as the harmonic mean of precision and recall. These three metrics are calculated for each category and also averaged across categories for an overall performance metric.\nMining declarative process models. For this task, we calculate precision, recall, and F1-score for each type of constraint (precedence, response, succession, initiation, termination) as well as overall metrics across all constraint types. Given a textual description T and a set of constraints D, the model predicts a set D. In line with the definitions above, we calculate the metrics for each constraint type \u03c4:\n$prec_\\tau = \\frac{TP_\\tau}{TP_\\tau + FP_\\tau}$, $rec_\\tau = \\frac{TP_\\tau}{TP_\\tau + FN_\\tau}$\nwhere $TP_\\tau$, $FP_\\tau$, and $FN_\\tau$ are the true positives, false positives, and false negatives for each constraint type. The F1-Score for each type $F1_\\tau$ is again given by the harmonic mean of the respective precision and recall values. To quantify the overall performance across all constraint types, we also compute the overall precision, recall, and F1-score based on the total number of true positives, false positives, and false negatives.\nProcess Question Answering. To evaluate the performance of the LLMs on the task of process question answering, we use the ROUGE-L (Lin 2004) score, which is a common metric for assessing the quality of generated text in comparison to a reference text. The ROUGE-L score measures the longest common subsequence (LCS) between the predicted answer and the true answer, capturing both precision and recall aspects of text similarity. Let A be the true answer and A be the predicted answer. We compute ROUGE-L between A and \u00c2 as follows:\n$ROUGE-L(A, \\hat{A}) = \\frac{prec_{LCS} \\cdot rec_{LCS}}{prec_{LCS} + rec_{LCS}}$\nwhere\n$prec_{LCS} = \\frac{LCS(A, \\hat{A})}{|\\hat{A}|}$, $rec_{LCS} = \\frac{LCS(A, \\hat{A})}{|A|}$\nand $LCS(A, \\hat{A})$ is the length of the longest common subsequence between A and \u00c2, and |A| and |\u00c2| refer to the number of words in A and A, respectively. To calculate the ROUGE-L score across all samples in the dataset, we compute the score for each sample and then average these scores:\nAverage ROUGE-L = $\\frac{1}{N} \\sum_{i=1}^{N}ROUGE-L(A_i, \\hat{A_i})$\nwhere N is the total number of samples in the dataset, $A_i$ is the true answer for the i-th sample, and $\\hat{A_i}$ is the predicted answer for the i-th sample. This average ROUGE-L score provides a comprehensive measure of how well the LLM can interpret and generate accurate answers based on the process descriptions.\nNote that we calculate the metrics with strict adherence to the model's ability to generate output exactly as instructed. Any deviation from the given instructions is considered a failure to fulfill the task. Although this approach has limitations\u2014since the output might be correct but not in the desired format\u2014we consider this procedure essential for benchmarking as it allows us to efficiently evaluate a large number of test cases."}, {"title": "3.5. Implementation", "content": "We implement our study in Python, using the following models from huggingface (Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, R\u00e9mi Louf, et al. 2019):\n\u2022 Yi-1.5-9B-Chat-Q4_K_M.gguf\n\u2022 Phi-3-medium-4k-instruct-Q4_K_S.gguf\n\u2022 Falcon2-11B.Q4_0.gguf\n\u2022 Claude2-alpaca-13b.Q6_K.gguf\n\u2022 Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\n\u2022 mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\nWe use quantization techniques to optimize performance and reduce memory usage. We deploy GPT-4 via the API from OpenAI (OpenAI et al. 2024). To ensure reproducibility, we use temperature = 0 for all models. The experiments were conducted using an Nvidia RTX A6000 GPU."}, {"title": "4. Results", "content": "In this section, we discuss the results of our experiments. We start by providing an overview of the overall results. Then, we take a closer look at each BPM task as well as the inference time and token usage. Finally, we analyze the impact of the employed prompt templates.\nOverall results. The overall results of our experiments reveal that there are significant performance differences across the four investigated BPM tasks. Figure 1 provides an overview of the results by showing the normalized performance results for each BPM task as well as the normalized inference time, with a negative sign applied so that higher values represent better performance. In general, we can see that there is no model that performs best in all five dimensions but that the models have different strengths and weaknesses.\nGPT-4 proves to be a robust performer in most tasks and consistently demonstrates high performance, especially in recommending activities and answering process questions. Llama3-7b performs very well, particularly in answering process questions and recommending activities, narrowly outperforming GPT-4. Phi3-14b and Mixtral-8x7b also perform competitively, particularly in identifying RPA candidates. Claude2-13b's performance is more specialized. It outperforms in activity recommendation but shows variability across the other tasks.\nIn summary, all models have unique strengths. However, GPT-4 and Llama3-7b offer the most balanced performance, making them ideal for a wide range of BPM tasks. Phi3-14b and Mixtral-8x7b are strong alternatives for RPA-related tasks, and Claude2-13b outperforms in activity recommendation. Falcon2 and Yi-1.5-9b are more specialized but offer valuable strengths in terms of speed of inference and constraint generation respectively. The results indicate also, that the chosen open-source models can indeed perform comparably to GPT-4 on certain BPM tasks. This finding is noteworthy, given the significant difference in the number of parameters between these models.\nIn the subsequent sections, we take a more detailed look at the investigated BPM tasks as well as the inference time and token usage.\nActivity recommendation. The performance results (i.e., the cosine similarity Sc) for the activity recommendation task are shown in Table 3. We observe that Claude2-13b achieved the highest average cosine similarity with 0.861, indicating its superior ability to recommend the next activity in the process sequence most accurately among the tested models. GPT-4 and Llama3-7b also performed well, with average cosine similarities of 0.855 and 0.853, respectively. By contrast, Yi-1.5-9b yielded the lowest performance, with an average cosine similarity of 0.719, highlighting its relatively lower capability in this task compared to the other models. Overall, the results show that there is a quite a notable performance difference across the tested LLMs. While Claude2-13b exhibited the best performance, models like Phi3-14b and Mixtral-8x7b are, next to GPT-4, only slightly less precise.\nIdentifying RPA candidates. Table 4 shows the results for applying the LLMs on the identifying RPA candidates BPM task. It shows precision, recall, and F1-score across three categories: manual (0), user (1), automated (2) tasks as well as the overall performance for each model. We can see that GPT-4 performs robustly across all metrics, achieving a macro-averaged F1-score of 0.671. This indicates that GPT-4 has a good balance in identifying various task types. Phi3-14b achieves the highest macro-averaged F1-score of 0.714, indicating a balanced performance across all categories. This model shows strong precision and recall for user tasks (0.805 and 0.836, respectively), but struggles with automated tasks, as reflected by a low F1-score of 0.100. Claude2-13b, on the other hand, exhibits a significant discrepancy in its performance. Despite achieving a recall of 0.750 for automated tasks, its overall macro-averaged F1-score is only 0.127, indicating inconsistency in handling other task categories. Falcon2-11b displays an interesting pattern, with a notably high recall for manual tasks (0.992) but an almost negligible performance in other categories, resulting in a low macro-averaged F1-score of 0.138. This suggests that Falcon2-11b might be overly biased towards detecting manual tasks. Llama3-7b and Mixtral-8x7b present competitive results, with macro-averaged F1-scores of 0.636 and 0.650, respectively. Both models show moderate strengths and weaknesses across different categories, suggesting a balanced but not exceptional performance. Finally, Yi-1.5-9b has a strong recall and F1-score for user tasks (0.858 and 0.771, respectively) but fails to identify automated tasks, as indicated by an F1-score of 0. Despite this, it achieves a reasonable macro-averaged F1-score of 0.616. The findings suggest that, while some LLMs are capable of effectively identifying RPA candidates, there remains a need for further optimization to ensure consistent performance across all task categories. However, the results also again show that open source models can compete with GPT-4 in this task.\nProcess Question Answering. Table 5 provides a detailed breakdown of the ROUGE-L scores across different levels of question complexity: easy, medium, and complex. The overall performance shows that the LLMs can handle questions of varying complexity differently. It is noteworthy that Mixtral-8x7b achieves the highest overall ROUGE-L score of 0.600, outperforming both GPT-4 and Llama3-7b. For the simple questions, Mixtral-8x7b again showed superior performance with a score of 0.657, closely followed by Llama3-7b and GPT-4, which scored 0.611 and 0.604 respectively. These results indicate that Mixtral-8x7b and Llama3-7b are particularly good at extracting simple, factual details from process descriptions. For the medium difficult questions, Mixtral-8x7b and Llama3-7b continue to perform good with scores of 0.723 and 0.721 respectively. GPT-4 also performs well with a score of 0.680. This indicates that these models can effectively integrate multiple steps and understand conditional flows within business processes. For complex questions, which involve higher-order thinking and scenario-based problem-solving, GPT-4 stands out with a score of 0.452. It is followed closely by Mixtral-8x7b and Llama3-7b, which score 0.420 and 0.413, respectively. In summary, Mixtral-8x7b consistently performs well across all question complexities, particularly excelling in the easy and medium categories. GPT-4 shows robust performance, especially in handling complex questions, indicating its strong capability in higher-order reasoning within BPM tasks. Llama3-7b also demonstrates strong overall performance, suggesting its effectiveness across various aspects of process comprehension. These results emphasis once again that smaller models are quite capable of competing with GPT-4, at least if the questions are not too complex.\nMining declarative process models. Table 6 shows the performance of the models on mining declarative process models from textual descriptions. We evaluated the models based on overall precision, recall, and F1-score. The model with the highest precision is Llama3-7b, achieving a precision of 0.375. However, this model struggles with recall, recording only 0.143, which results in an F1-score of 0.207. This indicates that while Llama3-7b is precise in identifying correct constraints, it misses a significant number of them, leading to lower overall effectiveness. In contrast, Yi-1.5-9b has a recall of 0.619, the highest of all models, and a precision of 0.126, resulting in an F1 score of 0.210, comparable to that of GPT-4 and Llama3-7b. The high recall suggests that Yi-1.5-9b is more effective in identifying a larger number of relevant constraints, although with a lower precision. GPT-4 shows a precision of 0.135 and a recall of 0.476, leading to the highest F1-score of 0.211. Phi3-14b, Claude2-13b, Falcon2-11b, and Mixtral-8x7b show relatively lower precision and recall compared to other models.\nInference time and token usage. Both inference time and token usage are important factors to consider for the application of LLMs. While inference time may decide about the overall applicability of a model (as users might only be willing to wait for a certain time), token usage is potentially associated with costs (see e.g. GPT-4).\nTable 4 provides an overview of the average inference time and the average tokens produced per model for all BPM tasks. Generally, higher inference times correlate with more tokens generated. For instance, Falcon2 and Yi-1.5 have longer inference times (2.18 and 1.74 seconds) and produce more tokens (49.89 and 32.02) compared to other models. However, more tokens do not necessarily mean better performance, as shown in previous experiments. Llama3 has the fastest inference time at 0.19 seconds while generating 6.18 tokens on average, indicating a balance between speed and token generation. All models fall within an acceptable inference time range, but the trade-offs between speed, token count and performance highlight the importance of choosing models like Llama3 for optimal efficiency and effectiveness. Surprisingly, the smaller open-source models, such as Llama3-7b and Mixtral-8x7b, demonstrated competitive performance.\nThe importance of prompting. Figure 2 shows the performance of the LLMs using different prompt templates across all BPM tasks. Specifically, it shows the percentage gain or loss when comparing persona prompt and step-by-step prompt templates to the few-shot example prompt template. Two main aspects can be observed. First, for models like phi3-14b, yi-1.5-9b, GPT-4, Mixtral-8x7b, and Llama3-7b, the the use of persona or step-by-step prompt templates leads to performance decreases. These models benefit from seeing multiple examples before generating outputs. Second, models like falcon2-11b and Claude2-13b would benefit from using persona or step-by-step prompt templates, whereby the effect is stronger with the former. This highlights that there is substantial variability in how LLMs respond to prompt templates."}, {"title": "5. Implications", "content": "The results reported in this paper have implications for both practice and research. In the following, we highlight these implications by reviewing the three main insights from our paper.\nGPT-4 is not best. While GPT-4 exhibited a stable performance across all BPM tasks, it is not the best model in terms of performance. Taking into account its relatively large inference time and the cost per generated token, we can conclude that it is not necessary for organization to invest into GPT-4 for the purpose of addressing BPM tasks.\nModel selection is important. Our results show that there are significant differences in performance between the models analyzed. Some models, as for instance Claude2, perform extraordinary well on one task but perform poorly on others. It is, hence, important for organizations to reflect on what BPM tasks need to be supported and select a suitable model respectively. One model that turned out to be stable across all tasks (besides GPT-4) is Llama3.\nModel size does not explain performance alone. There is a general consensus that performance of LLMs increases with model size (Brown et al. 2020; J. Kaplan et al. 2020). However, our experiments reveal that model size alone is insufficient to explain performance differences. GPT-4 has over 1,760 billion parameters while the tested open-source models have between 8 and 46.7 billion parameters. While GPT-4 performs generally well across all tasks, also other, much smaller models yield comparable results.\nNote that these insights cannot be easily generalized to the use of LLMs in other contexts. They do, however, highlight the necessity to properly reflect on the specific use cases and better understand when certain LLMs perform well."}, {"title": "6. Conclusion", "content": "In this paper, we investigated to what extent the results of existing LLM performance benchmarks can be transferred to the BPM domain. Specifically, we systematically compared the performance of open-source and close-source LLMs on four established BPM tasks. First, despite showing a stable performance across all BPM tasks, GPT-4 is not the best performing model. Given its relatively large inference time and associated cost per generated token, we do not believe that the use of GPT-4 can be justified in the BPM context. Second, model selection is an important factor as the models exhibit varying strengths with respect to the different BPM tasks. Hence, organizations should properly reflect on their needs and use cases before deploying a particular LLM. Third, the size of an LLM cannot alone explain the performance. We showed that small models"}]}