{"title": "DECOUPLED PROMPT-ADAPTER TUNING FOR CONTINUAL ACTIVITY RECOGNITION", "authors": ["Di Fu", "Thanh Vinh Vo", "Haozhe Ma", "Tze-Yun Leong"], "abstract": "Action recognition technology plays a vital role in enhancing security through surveillance systems, enabling better patient monitoring in healthcare, providing in-depth performance analysis in sports, and facilitating seamless human-AI collaboration in domains such as manufacturing and assistive technologies. The dynamic nature of data in these areas underscores the need for models that can continuously adapt to new video data without losing previously acquired knowledge, highlighting the critical role of advanced continual action recognition. To address these challenges, we propose Decoupled Prompt-Adapter Tuning (DPAT), a novel framework that integrates adapters for capturing spatial-temporal information and learnable prompts for mitigating catastrophic forgetting through a decoupled training strategy. DPAT uniquely balances the generalization benefits of prompt tuning with the plasticity provided by adapters in pretrained vision models, effectively addressing the challenge of maintaining model performance amidst continuous data evolution without necessitating extensive finetuning. DPAT consistently achieves state-of-the-art performance across several challenging action recognition benchmarks, thus demonstrating the effectiveness of our model in the domain of continual action recognition.", "sections": [{"title": "INTRODUCTION", "content": "The widespread deployment of cameras has significantly broadened the scope and influence of action recognition technology across multiple sectors. This technology is essential for boosting safety via security and surveillance, offering vital patient care in healthcare, and providing in-depth performance analyses in sports. It also enables robots to quickly perceive and respond to human actions during human-AI collaborations thereby enhancing the collaboration and efficiency between humans and AI in contexts such as manufacturing and assistive technologies. In this context, the importance of continual learning (CL) emerges, driven by the technological imperative to synchronize with the dynamically evolving nature of human activities and interactions. Defined as the ability of the model to incorporate new information from an ongoing data stream while retaining previously acquired knowledge, continual learning plays an essential role in this arena. It adeptly navigates the complex and heterogeneous spectrum of actions an action recognition algorithm encounters, enabling adaptation to emerging actions and contexts over time without compromising the recognition of previously observed actions. This flexibility is crucial for sustaining the efficacy and relevance of action recognition systems across their extensive applications, thereby establishing continual learning as a foundational element for the advancement and persistent applicability of action recognition technologies.\nWhile continual learning has made significant progress in recent years, the specific challenge of continual action recognition, which involves learning from dynamic video data streams without forgetting previously acquired knowledge, remains a difficult problem. On the one hand, the majority of CL methodologies are primarily devised for static images rendering them inadequately equipped to confront the unique challenges presented by video data. These challenges encompass the high-dimensional nature of video data, temporal"}, {"title": "RELATED WORK", "content": "Continual Learning. Continual Learning endeavors to mitigate the issue of catastrophic forgetting. Inspired by the hippocampus's replay mechanism in the human brain, memory replay methods adopt either the conservation of real samples for future replay or the employment of generative models to create samples that emulate previous task distributions. These methodologies, while effective, are constrained by substantial storage requirements and the complexity inherent in producing high-fidelity synthetic samples, especially when dealing with high-dimensional data such as video content. On the other hand, regularization-based methods present an alternative strategy aimed at preserving essential weight configurations from preceding tasks. This is achieved through an array of analytical instruments, including the Fisher information matrix, gradient assessments, and uncertainty metrics, which are employed to evaluate and rank the significance of weights. These approaches, which obviate the need for the storage of additional data, offer a distinct advantage in terms of efficiency and privacy. However, empirical evidence suggests that excessive regularization can impair the model's ability to generalize effectively across complex tasks. This limitation underscores the delicate balance required in"}, {"title": "PRELIMINARY", "content": "In the domain of action recognition, continual learning frameworks are tailored to incrementally adapt to a series of data streams, denoted as {$D_1, D_2, ..., D_T$}. Each stream $D_t$ at stage t comprises $N_t$ labeled video clips {$(v_b,y_b$)}$_{b=1}^{N_t}$, where $v_i$, a video clip, is characterized within $R^{T\\times H\\times W\\times C}$, and y represents the label of the video clip $v$. In this context, $v_i$ is defined in a four-dimensional space, with T representing the number of frames, H the height, W the width, and C the number of channels. The label y is a categorical variable that identifies the class of the video clip v. In the class incremental learning framework, distinct, non-overlapping class groups are introduced at each stage ($Y_t$), with the constraint that $Y_i \\cap Y_j = \\O$ for any $i \\neq j$. This setup mandates that the model, $f_\\theta$, not only assimilates new data from the current dataset $D_t$ while retaining the ability to accurately classify across an expanding set of classes, encapsulated as $Y_t = \\bigcup_{i=1}^{t-1} Y_i$. The primary objective is to optimize a singular model's average classification accuracy across all tasks."}, {"title": "CONTINUAL LEARNING WITH PREFIX-TUNING", "content": "In the realm of continual learning, Prefix-tuning has become a pivotal strategy for adapting Transformer models to new tasks with minimal retraining effort. Let the input to the Multi-Head Self-Attention (MSA) layer be $h \\in R^{L\\times D}$, and we further denote the input query, key, and values for the MSA layer to be $h_Q$, $h_K$, and $h_V$, respectively. The prompt parameter $p \\in R^{L_p\\times D}$ is divided into {$p_K$,$p_V$} $\\in R^{L_p\\times D}$ and prepended to $h_K$, and $h_V$ in the MSA as:\n$f_{Pre-T}(p, h) = MSA (h_Q, [p_K; h_K], [p_V; h_V])$ (1)\nTo address catastrophic forgetting during Prefix tuning, state-of-the-art methods such as L2P and DualPrompt utilize a key-value pair query strategy for dynamically selecting instance-specific prompts from a pool. Each prompt $p_m$ is linked to a learnable key $k_m$, with M denoting the pool's size, selected based on cosine similarity against an input-conditioned query q(x), thereby identifying the key $k_m$ with the highest similarity $\\gamma(q(x), k_m)$. During the test, the prompt embedding task-specific information is selected by arg min$_m$ $\\gamma(q(x), k_m)$, ensuring that the keys, embedded with task-specific knowledge during training, are precisely matched to the input for inference. However, these approaches were primarily tested for dealing with static images and do not accommodate the temporal information crucial for video action recognition, rendering them inadequate for such applications, whereas our proposed model seeks to address such limitations by incorporating additional adapters."}, {"title": "METHOD", "content": "Our proposed approach is illustrated in Figure 1. We begin by explaining the configuration of the adapter and prompt in our model in Section 4.1, where we discuss how these components work with the MSA layer in the pretrained image encoder. In Section 4.2, we describe our decoupled training process. In Section 4.3, we outline the redesigned query-key matching loss. Finally, we detail the training objectives in Section 4.4."}, {"title": "POSITION OF ADAPTER AND PROMPT", "content": "Figure 1a illustrates our approach, diverging from recent methods that append an additional temporal model atop the ViT backbone to capture temporal dynamics. Instead, we deploy adapters to refine the model's inherent processing capabilities. Specifically, our model incorporates a spatial adapter, Adapter-S, with a spatial prompt $p_S$, and a temporal adapter, Adapter-T, with a temporal prompt $p_T$. This configuration maintains the original functionality of the MSA layer within the pre-trained image encoder, leveraging its inherent strengths while minimizing modifications."}, {"title": "DECOUPLED PROMPT-ADAPTER TUNING", "content": "Despite the theoretical advantage of adapters being less susceptible to catastrophic forgetting compared to traditional fine-tuning, their standalone application does not completely circumvent the challenges associated with rapid task specialization. Conversely, employing Prefix tuning directly, although it enhances model stability and generalizability, it has been observed that Prefix tuning exhibits a slower adaptation rate to new tasks and is prone to homogeneity , thereby constraining the adaptability required for varied and intricate tasks. These observations form the core motivation for our proposed strategy, Decoupled Prefix Prompt and Adapter Tuning, which aims to harness the complementary strengths of both adapter and Prefix tuning in a unified framework.\nFigure 1b illustrates the proposed decoupled training strategy, the training process is delineated into two distinct phases, meticulously designed to balance adaptability with generalizability:\nFirst Stage: Prefix Tuning. Initially, Prefix tuning is employed to provide the model with a stable and generalizable base. Learnable prompts encapsulate the task-specific information, reducing the immediate adaptation pressure for adapter and providing the model with a robust understanding of the task. This stage is crucial for setting the stage for specialized adaptation.\nSecond Stage: Adapter Tuning. Subsequently, the focus shifts to adapter tuning, emphasizing task-specific refinement and adaptation. By maintaining the prompt learned in the initial phase, we preserve the generalization and stability benefits of Prefix tuning, while the adapter module targets task-specific learning. This approach aims to strike a balance between rapid adaptation and maintaining the model's ability to generalize, addressing the limitations of employing either tuning strategy in isolation."}, {"title": "REDESIGNED QUERY-KEY MATCHING LOSS", "content": "In the DualPrompt, the matching loss is formalized as $L_{match}(x, k_t) = \\gamma(q(x), k_t)$, with $\\gamma$ acting as a distance metric and q(x) as a query function. This design aims to minimize the distance between $k_t$ and the query representation of $x$, thus enhancing the affinity between task-specific keys and inputs from corresponding tasks. Nonetheless, this initial formulation overlooks the inter-task relationships, focusing solely on the proximity to a single task key without considering the influence of other task keys. To address this limitation, we propose an enhancement through normalization of similarity scores using a softmax function to ensure that the model's predictions are influenced not only by the nearest task key but also by the relative similarity to all task keys. The revised matching"}, {"title": "TRAINING OBJECTIVE", "content": "The comprehensive training and testing processes are outlined in Algorithm 1 and Algorithm 2, respectively. The objective for the two-stage decoupled training is formulated as follows:\nStage 1:\nmin $L(f_\\phi(f_{p_S,p_T,\\theta_T,\\theta_S}(x)), y)$\n$p_S,p_T,\\phi$\nStage 2:\nmin $L(f_\\phi(f_{p_S,p_T,\\theta_T,\\theta_S}(x)), y) + \\lambda L_{match}(x, k_t)$ (4)\n$\\theta_T,\\theta_S, k_T,\\phi$\nwhere $f_\\phi$ denotes the classification head parametrized by $\\phi$, $f_{p_S,p_T,\\theta_T,\\theta_S}(x)$ represents the forward process of the adapted, frozen pretrained vision model. It integrates spatial-temporal adapters and prompts with parameters $\\theta_T$, $\\theta_S$ and $p_S$, $p_T$, respectively, as described in Equation 2. L is the cross-entropy loss, $L_{match}$ is the matching loss defined in Equation 3, and $\\lambda$ is a scalar balancing factor."}, {"title": "EXPERIMENTS", "content": "In this section, we first study the performance of DPAT compared to the baselines. We then examine the significance of each component of DPAT through ablation studies."}, {"title": "EXPERIMENTS SETTINGS", "content": "Datasets. We evaluate our method across three public datasets: Kinetics-400 and ActivityNet for standard action recognition, alongside EPIC-Kitchens-100 for fine-grained action recognition. Following a strategy akin to the vCLIMB benchmark guidelines, we organize the data into a class-incremental setting. In this arrangement, each dataset's classes are introduced sequentially across a series of 10 tasks, with careful measures taken to prevent class overlap. This structure rigorously evaluates our method's adaptability and learning evolution in a systematic, incremental manner. Further details are provided in Appendix A.1.\nEvluation Metrics. We use the widely recognized evaluation metrics of average accuracy (Acc) and backward forgetting (BWF) to measure the model's overall performance across tasks and the extent to which it retains knowledge from previous tasks after learning new ones, respectively, as defined below:\n$Acc = \\frac{1}{N}\\sum_{i=1}^{N} R_{N,i}$\n$BWF = \\frac{1}{N-1} \\sum_{i=1}^{N-1} (R_{i,i} - R_{N,i})$ (5)\nwhere N represents the total number of tasks, $R_{N,i}$ denotes the accuracy of the model on task i after learning all N tasks, and $R_{i,i}$ represents the accuracy of the model on task i immediately after learning it.\nImplementation. We leverage the ViT-B/16 architecture pre-trained on the ImageNet-21K dataset, as the backbone for DPAT. To adapt to task-specific requirements while maintaining the integrity of the pre-trained features, we freeze the backbone and sequentially fine-tune adapters and prompts. Optimization is performed using the Adam optimizer, with a batch size of 64 across 50 epochs for each task. We set the bottleneck ratios for spatial and temporal adapters at 0.25 and the scaling factor, $\\lambda$, to 1. To ensure robustness and generalizability of our findings, we select benchmark methods that operate under analogous conditions, facilitating a rigorous and fair comparison."}, {"title": "COMPARISON WITH BASELINE", "content": "The comparative analysis incorporates methodologies such as EWC, MAS, iCaRL, and the state-of-the-art PIVOT. To ensure a fair comparison, configurations for EWC, MAS, and iCaRL are enhanced with a tunable adapter. In conducting a comparison with PIVOT, our analysis adopts the CLIP ViT-B/16 model as the underlying architecture, aligning with the image encoder utilized by PIVOT itself.\nResults on Kinetics-400 and ActivityNet. As illustrated in Table 1, DPAT (IN-21K), employing a ViT-B/16 model pre-trained on ImageNet-21K, substantially surpasses traditional rehearsal-free methodologies MAS and EWC. Further analysis reveals that against the state-of-the-art PIVOT, DPAT (CLIP), which also utilizes the same CLIP ViT-B/16 backbone as PIVOT, secures higher prediction accuracy and demonstrates reduced backward forgetting on both Kinetics-400 and ActivityNet datasets, all achieved without relying on replayed video instances. This enhanced per-"}, {"title": "ABLATION STUDIES", "content": "In this section, we conduct ablation studies to scrutinize the characteristics and efficacy of our fundamental design elements.\nEffect of Model Component. In the ablation studies summarized in Table 3, we systematically assess the contribution of each component to our model's proficiency in continual action recognition, utilizing the Kinetics-400 dataset for our experiments. The absence of the Temporal Adapter leads to a substantial decrease in accuracy by 38.2%, underscoring its critical role in capturing temporal dynamics crucial for precise action recognition. The complete removal of all adapters results in a marked decrease in accuracy to 25%, indicating a further decline in learning performance. This underscores the critical role of adapters in adapting to new tasks. Interestingly, this configuration leads to minimal forgetting, suggesting that a combination of prompt learning with a frozen pre-trained model provides a stable foundation, even as it points to the necessity of adapters for effective task adaptation. In contrast, the exclusion of the Prefix Prompt detrimentally impacts accuracy and leads to the highest BWF rate among the configurations tested,"}, {"title": "Effect of Decoupled Training.", "content": "To illustrate the efficacy of our decoupled training strategy, we present the learning curve comparison between decoupled and joint training in Figure 2. The x-axis represents the current task, while the y-axis measures the average accuracy (Acc). Conducting experiments on the Kinetics-400 dataset, our findings reveal that although both strategies commence with comparable accuracy levels, the decoupled training demonstrates a trend of slower degradation in mean accuracy alongside reduced backward forgetting. This highlights our model's enhanced capability to preserve previously learned knowledge over time."}, {"title": "Effect of Query Matching Loss.", "content": "Table 4 highlights DPAT's superiority over the DualPrompt model, with DPAT achieving a Matching Accuracy of 45.6% compared to DualPrompt's 32.8% on Kinetics-400. This improvement underscores our optimized matching loss's efficacy in enhancing task-specific contrast, allowing for more precise task-specific key embeddings. Notably, the elevation in Matching Accuracy corresponds with a significant boost in Prediction Accuracy and a reduction in Backward Forgetting, showcasing the direct impact of improved alignment on model performance and memory retention."}, {"title": "CONCLUSION", "content": "In this paper, we present a simple yet effective rehearsal-free approach for continual activity recognition. Eliminating the necessity for integrating auxiliary temporal architectures, relying on external modal inputs, or undertaking extensive fine-tuning, our architecture employs adapters in conjunction with prompt tuning to excel in the realm of continual action recognition, leveraging a frozen pre-trained model. Furthermore, We introduce a decoupled training strategy that capitalizes on the adapter's generalization capabilities and the stability provided by prompt tuning, effectively"}, {"title": "DATASETS", "content": "Kinetics-400. Kinetics-400 stands as an extensive dataset tailored for action recognition, encompassing roughly 300k video snippets classified into 400 distinct human action categories. Originating from a wide variety of YouTube videos, each clip is meticulously trimmed to approximately 10 seconds to ensure uniformity. For the purposes of this work, the dataset is partitioned into 10 separate tasks, with each task consists of 40 unique action classes, arranged in a sequential, class-incremental manner.\nActivityNet. ActivityNet is a large-scale dataset designed for action recognition, featuring over 20,000 video clips spread across 200 activity classes. Sourced from YouTube, it offers a wide-ranging and diverse collection of real-world scenarios that capture a broad spectrum of everyday human activities. Similar to Kinetics-400, we organize the dataset into a class-incremental setting with 10 tasks, ensuring a neat division of 20 activities per task.\nEpic-Kitchen-100. Epic-Kitchen-100 is a large-scale egocentric video dataset that records over 100 hours of kitchen unscripted activities. It consists of 90K action segments, which are split into train/val/test sets of 67K/10K/13K. Differ from preceding two datasets, Epic-Kitchen-100 defines an action as a combination of a verb and a noun. Given the necessity to match both verbs and nouns for action recognition, this task presents a higher level of complexity compared to action recognition in previous datasets characterized by single label prediction. We employ a class-incremental strategy for verb classification, dividing 97 verb categories into 10 non-overlapping tasks to systematically introduce new classes. Concurrently, noun prediction is approached with a task-incremental strategy, wherein all tasks share a consistent set of total 300 noun classes. This deliberate division, prioritizing verbs for the class-incremental learning setting, stems from our intent to scrutinize the model's ability to discern and learn from the nuanced temporal dynamics across different tasks."}, {"title": "IMPLEMENTATION DETAILS", "content": "Following the ViT architecture guidelines, we sample videos to 16 frames. Each frame is then randomly cropped and resized to 224 \u00d7 224 pixels. Additionally, we enhance the diversity of the training data by applying data augmentation techniques, including mixup (Zhang et al., 2017), label smoothing, horizontal flipping, color jittering, and RandAugment (Cubuk et al., 2020). Specifically, for mixup, an alpha of 0.2 was utilized. Label smoothing was implemented using a factor of 0.1. In the color jittering process, brightness, contrast, and saturation adjustments were uniformly set to 0.4, with hue adjustments at 0.1. We deployed RandAugment with a configuration of 2 transformations at a magnitude level of 10. Finally, to ensure consistency across diverse datasets, we normalize each frame to the range [0, 1].\nAs depicted in Figure 1a, our method, DPAT, integrates spatial and temporal adapters into every ViT block. The architecture of the adapter adheres to a bottleneck design, incorporating two fully connected (FC) layers separated by an activation layer. The design involves diminishing the input dimensionality through the initial FC layer, and subsequently restoring it via the second FC layer. The extent of dimensionality reduction is determined by a bottleneck ratio-defined as the quotient of the bottleneck to the input dimension\u2014with a consistent ratio of 0.25 applied throughout the experiment."}, {"title": "ABLATION STUDIES ON ACTIVITYNET", "content": "To further validate the effectiveness of our proposed DPAT approach and the individual contributions of its components, we conducted additional ablation studies on the ActivityNet dataset. The results are summarized in Table 5, highlighting the importance of both the temporal adapter and the task-agnostic prefixes within our framework."}, {"title": "EXPERIMENTS ON UCF-101", "content": "Further experiments were conducted on the UCF-101 dataset to offer a broader evaluation of our approach. The outcomes, detailed in Table 6, show our method's performance in comparison to existing benchmarks."}, {"title": "ABLATION STUDIES ON PROMPT POSITION", "content": "A parameter search was conducted to fine-tune the positioning of temporal and spatial prompts within the DPAT framework, with the starting position (start) set to 1 and the ending position (end) set to 5. We varied the ending position (endg) and adjusted the starting position of the subsequent element (starte) accordingly. The search results, provided in Table 7, validate our initial configuration choices, though it is acknowledged that due to the expansive search space, these findings may not represent the optimal configuration."}, {"title": "ABLATION STUDIES ON BOTTLENECK RATIO", "content": "The bottleneck ratio, a critical hyperparameter in our DPAT framework, influences the trade-off between model capacity and efficiency. Experiments were conducted on the Kinetics-400 dataset to identify the optimal bottleneck ratio. Various ratios were tested, and their impacts on average accuracy and backward forgetting were assessed. As Table 8 indicates, a bottleneck ratio of 0.25 optimally balances model capacity with efficiency, leading to the best performance on this dataset."}]}