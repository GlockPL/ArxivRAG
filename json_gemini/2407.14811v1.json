{"title": "DECOUPLED PROMPT-ADAPTER TUNING FOR CONTINUAL ACTIVITY RECOGNITION", "authors": ["Di Fu", "Thanh Vinh Vo", "Haozhe Ma", "Tze-Yun Leong"], "abstract": "Action recognition technology plays a vital role in enhancing security through surveillance systems, enabling better patient monitoring in healthcare, providing in-depth performance analysis in sports, and facilitating seamless human-AI collaboration in domains such as manufacturing and assistive technologies. The dynamic nature of data in these areas underscores the need for models that can continuously adapt to new video data without losing previously acquired knowledge, highlighting the critical role of advanced continual action recognition. To address these challenges, we propose Decoupled Prompt-Adapter Tuning (DPAT), a novel framework that integrates adapters for capturing spatial-temporal information and learnable prompts for mitigating catastrophic forgetting through a decoupled training strategy. DPAT uniquely balances the generalization benefits of prompt tuning with the plasticity provided by adapters in pretrained vision models, effectively addressing the challenge of maintaining model performance amidst continuous data evolution without necessitating extensive finetuning. DPAT consistently achieves state-of-the-art performance across several challenging action recognition benchmarks, thus demonstrating the effectiveness of our model in the domain of continual action recognition.", "sections": [{"title": "1 INTRODUCTION", "content": "The widespread deployment of cameras has significantly broadened the scope and influence of action recognition technology across multiple sectors. This technology is essential for boosting safety via security and surveillance, offering vital patient care in healthcare, and providing in-depth performance analyses in sports (Kong & Fu, 2022). It also enables robots to quickly perceive and respond to human actions during human-AI collaborations (Akkaladevi & Heindl, 2015), thereby enhancing the collaboration and efficiency between humans and AI in contexts such as manufacturing and assistive technologies. In this context, the importance of continual learning (CL) emerges, driven by the technological imperative to synchronize with the dynamically evolving nature of human activities and interactions. Defined as the ability of the model to incorporate new information from an ongoing data stream while retaining previously acquired knowledge, continual learning plays an essential role in this arena. It adeptly navigates the complex and heterogeneous spectrum of actions an action recognition algorithm encounters, enabling adaptation to emerging actions and contexts over time without compromising the recognition of previously observed actions. This flexibility is crucial for sustaining the efficacy and relevance of action recognition systems across their extensive applications, thereby establishing continual learning as a foundational element for the advancement and persistent applicability of action recognition technologies.\nWhile continual learning has made significant progress in recent years, the specific challenge of continual action recognition, which involves learning from dynamic video data streams without forgetting previously acquired knowledge, remains a difficult problem. On the one hand, the majority of CL methodologies are primarily devised for static images (Wang et al., 2022b;a; Smith et al., 2023), rendering them inadequately equipped to confront the unique challenges presented by video data. These challenges encompass the high-dimensional nature of video data, temporal"}, {"title": "2 RELATED WORK", "content": "Continual Learning. Continual Learning endeavors to mitigate the issue of catastrophic forgetting. Inspired by the hippocampus's replay mechanism in the human brain, memory replay methods adopt either the conservation of real samples for future replay (Rolnick et al., 2019; Rebuffi et al., 2017; Buzzega et al., 2020) or the employment of generative models to create samples that emulate previous task distributions (Shin et al., 2017; Ostapenko et al., 2019). These methodologies, while effective, are constrained by substantial storage requirements and the complexity inherent in producing high-fidelity synthetic samples, especially when dealing with high-dimensional data such as video content. On the other hand, regularization-based methods (Kirkpatrick et al., 2017; Aljundi et al., 2018; Zenke et al., 2017; Li & Hoiem, 2017) present an alternative strategy aimed at preserving essential weight configurations from preceding tasks. This is achieved through an array of analytical instruments, including the Fisher information matrix, gradient assessments, and uncertainty metrics, which are employed to evaluate and rank the significance of weights. These approaches, which obviate the need for the storage of additional data, offer a distinct advantage in terms of efficiency and privacy. However, empirical evidence suggests that excessive regularization can impair the model's ability to generalize effectively across complex tasks. This limitation underscores the delicate balance required in"}, {"title": "3 PRELIMINARY", "content": "3.1 CONTINUAL ACTION RECOGNITION\nIn the domain of action recognition, continual learning frameworks are tailored to incrementally adapt to a se-Ntries of data streams, denoted as {D1, D2, ..., Dr}. Each stream D\u2081 at stage t comprises Nt labeled video clips{(v,y)}1, where v\u012f, a video clip, is characterized within $R^{T \\times H \\times W \\times C}$, and y represents the label of the videoclip v. In this context, vi is defined in a four-dimensional space, with T representing the number of frames, H theheight, W the width, and C the number of channels. The label y is a categorical variable that identifies the class ofthe video clip v. In the class incremental learning framework, distinct, non-overlapping class groups are introducedat each stage (Y\u2081), with the constraint that Y\u00bf \u2229 Yj = \u00d8 for any i \u2260 j. This setup mandates that the model, f\u03b8, notonly assimilates new data from the current dataset Dt while retaining the ability to accurately classify across an ex-panding set of classes, encapsulated as $Y_\\text{t} = \\cup_\\text{i=1}^{\\text{t}} Y_i$. The primary objective is to optimize a singular model's averageclassification accuracy across all tasks.\n3.2 CONTINUAL LEARNING WITH PREFIX-TUNING\nIn the realm of continual learning, Prefix-tuning (Li & Liang, 2021) has become a pivotal strategy for adapting Transformer models to new tasks with minimal retraining effort. Let the input to the Multi-Head Self-Attention (MSA)layer be h \u2208 RL\u00d7D, and we further denote the input query, key, and values for the MSA layer to be hq, hk, and hv,respectively. The prompt parameter p \u2208 RLp\u00d7D is divided into {pK,pV } \u2208 R\uc46c\u00d7D and prepended to hk, and hvin the MSA as:\n$f_{Pre-T} (p, h) = MSA (h_q, [p^k; h_k], [p^v; h_v]) .$                                                                             (1)\nTo address catastrophic forgetting during Prefix tuning, state-of-the-art methods such as L2P (Wang et al., 2022b) andDualPrompt (Wang et al., 2022a) utilize a key-value pair query strategy for dynamically selecting instance-specificprompts from a pool. Each prompt pm is linked to a learnable key km, with M denoting the pool's size, selected basedon cosine similarity against an input-conditioned query q(x), thereby identifying the key km with the highest similarity\u03b3(q(x), km). During the test, the prompt embedding task-specific information is selected by arg minm y(q(x), km),ensuring that the keys, embedded with task-specific knowledge during training, are precisely matched to the input forinference. However, these approaches were primarily tested for dealing with static images and do not accommodate thetemporal information crucial for video action recognition, rendering them inadequate for such applications, whereasour proposed model seeks to address such limitations by incorporating additional adapters."}, {"title": "4 METHOD", "content": "Our proposed approach is illustrated in Figure 1. We begin by explaining the configuration of the adapter and promptin our model in Section 4.1, where we discuss how these components work with the MSA layer in the pretrainedimage encoder. In Section 4.2, we describe our decoupled training process. In Section 4.3, we outline the redesignedquery-key matching loss. Finally, we detail the training objectives in Section 4.4.\n4.1 POSITION OF ADAPTER AND PROMPT\nFigure la illustrates our approach, diverging from recent methods that append an additional temporal model atop theViT backbone to capture temporal dynamics. Instead, we deploy adapters to refine the model's inherent processingcapabilities. Specifically, our model incorporates a spatial adapter, Adapter-S, with a spatial prompt ps, and a tem-poral adapter, Adapter-T, with a temporal prompt p\u00b9. This configuration maintains the original functionality of theMSA layer within the pre-trained image encoder, leveraging its inherent strengths while minimizing modifications."}, {"title": "4.2 DECOUPLED PROMPT-ADAPTER TUNING", "content": "Despite the theoretical advantage of adapters being less susceptible to catastrophic forgetting compared to traditionalfine-tuning, their standalone application does not completely circumvent the challenges associated with rapid taskspecialization. Conversely, employing Prefix tuning directly, although it enhances model stability and generalizability,it has been observed that Prefix tuning exhibits a slower adaptation rate to new tasks and is prone to homogeneity (Gaoet al., 2023), thereby constraining the adaptability required for varied and intricate tasks. These observations form thecore motivation for our proposed strategy, Decoupled Prefix Prompt and Adapter Tuning, which aims to harness thecomplementary strengths of both adapter and Prefix tuning in a unified framework.\nFigure 1b illustrates the proposed decoupled training strategy, the training process is delineated into two distinctphases, meticulously designed to balance adaptability with generalizability:\nFirst Stage: Prefix Tuning. Initially, Prefix tuning is employed to provide the model with a stable and generalizablebase. Learnable prompts encapsulate the task-specific information, reducing the immediate adaptation pressure foradapter and providing the model with a robust understanding of the task. This stage is crucial for setting the stage forspecialized adaptation.\nSecond Stage: Adapter Tuning. Subsequently, the focus shifts to adapter tuning, emphasizing task-specific refine-ment and adaptation. By maintaining the prompt learned in the initial phase, we preserve the generalization andstability benefits of Prefix tuning, while the adapter module targets task-specific learning. This approach aims to strikea balance between rapid adaptation and maintaining the model's ability to generalize, addressing the limitations ofemploying either tuning strategy in isolation."}, {"title": "4.3 REDESIGNED QUERY-KEY MATCHING LOSS", "content": "In the DualPrompt (Wang et al., 2022a), the matching loss is formalized as Lmatch (x, kt) = \u03b3 (q(x), kt), with yacting as a distance metric and q(x) as a query function. This design aims to minimize the distance between kt andthe query representation of \u00e6, thus enhancing the affinity between task-specific keys and inputs from correspondingtasks. Nonetheless, this initial formulation overlooks the inter-task relationships, focusing solely on the proximityto a single task key without considering the influence of other task keys. To address this limitation, we propose anenhancement through normalization of similarity scores using a softmax function to ensure that the model's predictionsare influenced not only by the nearest task key but also by the relative similarity to all task keys. The revised matching"}, {"title": "4.4 TRAINING OBJECTIVE", "content": "The comprehensive training and testing processes are outlined in Algorithm 1 and Algorithm 2, respectively. Theobjective for the two-stage decoupled training is formulated as follows:\nStage 1:             min C (f (fpt,ps,\u04e9\u0442,05(x)),y),\nps,pT,\nStage 2:             min C (f (fpt,ps,\u04e9\u0442,\u04e9s(x)),y) + ALmatch (x, kt),\n\u03b8\u03c4,\u03b8\u03c2, \u03ba\u03c4,\u03c6\n(4)\nwhere f$ denotes the classification head parametrized by \u00a2, fpT,ps,\u04e9\u0442,\u04e9s(x) represents the forward process of theadapted, frozen pretrained vision model. It integrates spatial-temporal adapters and prompts with parameters \u03b8\u03c4, \u03b8\u03c2and ps, pT, respectively, as described in Equation 2. L is the cross-entropy loss, Lmatch is the matching loss definedin Equation 3, and A is a scalar balancing factor."}, {"title": "5 EXPERIMENTS", "content": "In this section, we first study the performance of DPAT compared to the baselines. We then examine the significanceof each component of DPAT through ablation studies.\n5.1 EXPERIMENTS SETTINGS\nDatasets. We evaluate our method across three public datasets: Kinetics-400 (Kay et al., 2017) and ActivityNet (Caba Heilbron et al., 2015) for standard action recognition, alongside EPIC-Kitchens-100 (Damen et al., 2021)for fine-grained action recognition. Following a strategy akin to the vCLIMB (Villa et al., 2022) benchmark guide-lines, we organize the data into a class-incremental setting. In this arrangement, each dataset's classes are introducedsequentially across a series of 10 tasks, with careful measures taken to prevent class overlap. This structure rigorouslyevaluates our method's adaptability and learning evolution in a systematic, incremental manner. Further details areprovided in Appendix A.1.\nEvluation Metrics. We use the widely recognized evaluation metrics of average accuracy (Acc) and backward for-getting (BWF) to measure the model's overall performance across tasks and the extent to which it retains knowledgefrom previous tasks after learning new ones, respectively, as defined below:\n$Acc = \\frac{1}{N} \\sum_{i=1}^{N} R_{N,i}$,\n$BWF = \\frac{1}{N-1} \\sum_{i=1}^{N-1} (R_{i,i} - R_{N,i}),$\n(5)\nwhere N represents the total number of tasks, RN,i denotes the accuracy of the model on task i after learning all Ntasks, and Ri,i represents the accuracy of the model on task i immediately after learning it.\nImplementation. We leverage the ViT-B/16 architecture (Dosovitskiy et al., 2020), pre-trained on the ImageNet-21K(IN-21K) dataset, as the backbone for DPAT. To adapt to task-specific requirements while maintaining the integrityof the pre-trained features, we freeze the backbone and sequentially fine-tune adapters and prompts. Optimization isperformed using the Adam (Kingma & Ba, 2014) optimizer, with a batch size of 64 across 50 epochs for each task. Weset the bottleneck ratios for spatial and temporal adapters at 0.25 and the scaling factor, \u03bb, to 1. To ensure robustnessand generalizability of our findings, we select benchmark methods that operate under analogous conditions, facilitatinga rigorous and fair comparison."}, {"title": "5.2 COMPARISON WITH BASELINE", "content": "The comparative analysis incorporates methodologies such as EWC (Kirkpatrick et al., 2017), MAS (Aljundi et al.,2018), iCaRL (Rebuffi et al., 2017), and the state-of-the-art PIVOT (Villa et al., 2023). To ensure a fair comparison,configurations for EWC, MAS, and iCaRL are enhanced with a tunable adapter. In conducting a comparison withPIVOT, our analysis adopts the CLIP (Radford et al., 2021) ViT-B/16 model as the underlying architecture, aligningwith the image encoder utilized by PIVOT itself.\nResults on Kinetics-400 and ActivityNet. As illustrated in Table 1, DPAT (IN-21K), employing a ViT-B/16 modelpre-trained on ImageNet-21K, substantially surpasses traditional rehearsal-free methodologies MAS and EWC. Fur-ther analysis reveals that against the state-of-the-art PIVOT, DPAT (CLIP), which also utilizes the same CLIP ViT-B/16 backbone as PIVOT, secures higher prediction accuracy and demonstrates reduced backward forgetting on bothKinetics-400 and ActivityNet datasets, all achieved without relying on replayed video instances. This enhanced per-"}, {"title": "5.3 ABLATION STUDIES", "content": "In this section, we conduct ablation studies to scrutinize the characteristics and efficacy of our fundamental designelements.\nEffect of Model Component. In the ablation studies summarized in Table 3, we systematically assess the contributionof each component to our model's proficiency in continual action recognition, utilizing the Kinetics-400 dataset forour experiments. The absence of the Temporal Adapter leads to a substantial decrease in accuracy by 38.2%, under-scoring its critical role in capturing temporal dynamics crucial for precise action recognition. The complete removalof all adapters results in a marked decrease in accuracy to 25%, indicating a further decline in learning performance.This underscores the critical role of adapters in adapting to new tasks. Interestingly, this configuration leads to min-imal forgetting, suggesting that a combination of prompt learning with a frozen pre-trained model provides a stablefoundation, even as it points to the necessity of adapters for effective task adaptation. In contrast, the exclusion ofthe Prefix Prompt detrimentally impacts accuracy and leads to the highest BWF rate among the configurations tested,"}, {"title": "Effect of Decoupled Training", "content": "To illustrate the efficacy of our decoupled training strategy, we present the learningcurve comparison between decoupled and joint training in Figure 2. The x-axis represents the current task, while they-axis measures the average accuracy (Acc). Conducting experiments on the Kinetics-400 dataset, our findings revealthat although both strategies commence with comparable accuracy levels, the decoupled training demonstrates a trendof slower degradation in mean accuracy alongside reduced backward forgetting. This highlights our model's enhancedcapability to preserve previously learned knowledge over time."}, {"title": "Effect of Query Matching Loss", "content": "Table 4 highlights DPAT's superiority over the DualPrompt model, with DPATachieving a Matching Accuracy of 45.6% compared to DualPrompt's 32.8% on Kinetics-400. This improvementunderscores our optimized matching loss's efficacy in enhancing task-specific contrast, allowing for more precisetask-specific key embeddings. Notably, the elevation in Matching Accuracy corresponds with a significant boost inPrediction Accuracy and a reduction in Backward Forgetting, showcasing the direct impact of improved alignment onmodel performance and memory retention."}, {"title": "6 CONCLUSION", "content": "In this paper, we present a simple yet effective rehearsal-free approach for continual activity recognition. Eliminatingthe necessity for integrating auxiliary temporal architectures, relying on external modal inputs, or undertaking exten-sive fine-tuning, our architecture employs adapters in conjunction with prompt tuning to excel in the realm of continualaction recognition, leveraging a frozen pre-trained model. Furthermore, We introduce a decoupled training strategythat capitalizes on the adapter's generalization capabilities and the stability provided by prompt tuning, effectively"}, {"title": "A.1 DATASETS", "content": "Kinetics-400. Kinetics-400 stands as an extensive dataset tailored for action recognition, encompassing roughly 300kvideo snippets classified into 400 distinct human action categories. Originating from a wide variety of YouTube videos,each clip is meticulously trimmed to approximately 10 seconds to ensure uniformity. For the purposes of this work,the dataset is partitioned into 10 separate tasks, with each task consists of 40 unique action classes, arranged in asequential, class-incremental manner.\nActivityNet. ActivityNet is a large-scale dataset designed for action recognition, featuring over 20,000 video clips spread across 200 activity classes. Sourced from YouTube, it offers a wide-ranging and diverse collection of real-worldscenarios that capture a broad spectrum of everyday human activities. Similar to Kinetics-400, we organize the datasetinto a class-incremental setting with 10 tasks, ensuring a neat division of 20 activities per task.\nEpic-Kitchen-100. Epic-Kitchen-100 is a large-scale egocentric video dataset that records over 100 hours of kitchenunscripted activities. It consists of 90K action segments, which are split into train/val/test sets of 67K/10K/13K.Differ from preceding two datasets, Epic-Kitchen-100 defines an action as a combination of a verb and a noun. Giventhe necessity to match both verbs and nouns for action recognition, this task presents a higher level of complexitycompared to action recognition in previous datasets characterized by single label prediction. We employ a class-incremental strategy for verb classification, dividing 97 verb categories into 10 non-overlapping tasks to systematicallyintroduce new classes. Concurrently, noun prediction is approached with a task-incremental strategy, wherein all tasksshare a consistent set of total 300 noun classes. This deliberate division, prioritizing verbs for the class-incrementallearning setting, stems from our intent to scrutinize the model's ability to discern and learn from the nuanced temporal dynamics across different tasks."}, {"title": "A.2 IMPLEMENTATION DETAILS", "content": "Following the ViT architecture guidelines, we sample videos to 16 frames. Each frame is then randomly cropped andresized to 224 \u00d7 224 pixels. Additionally, we enhance the diversity of the training data by applying data augmentationtechniques, including mixup (Zhang et al., 2017), label smoothing, horizontal flipping, color jittering, and RandAug-ment (Cubuk et al., 2020). Specifically, for mixup, an alpha of 0.2 was utilized. Label smoothing was implementedusing a factor of 0.1. In the color jittering process, brightness, contrast, and saturation adjustments were uniformlyset to 0.4, with hue adjustments at 0.1. We deployed RandAugment with a configuration of 2 transformations at amagnitude level of 10. Finally, to ensure consistency across diverse datasets, we normalize each frame to the range[0, 1].\nAs depicted in Figure 1a, our method, DPAT, integrates spatial and temporal adapters into every ViT block. Thearchitecture of the adapter adheres to a bottleneck design, incorporating two fully connected (FC) layers separatedby an activation layer. The design involves diminishing the input dimensionality through the initial FC layer, andsubsequently restoring it via the second FC layer. The extent of dimensionality reduction is determined by a bottle-neck ratio-defined as the quotient of the bottleneck to the input dimension\u2014with a consistent ratio of 0.25 appliedthroughout the experiment."}, {"title": "A.3 ADDITIONAL EXPERIMENTS AND ANALYSIS", "content": "In this section, we present additional experiments and ablation studies to further validate the effectiveness of our proposed DPAT approach and investigate the impact of various components and hyperparameters on its performance.\n1 ABLATION STUDIES ON ACTIVITYNET\nTo further validate the effectiveness of our proposed DPAT approach and the individual contributions of its components, we conducted additional ablation studies on the ActivityNet dataset. The results are summarized in Table 5, highlighting the importance of both the temporal adapter and the task-agnostic prefixes within our framework."}, {"title": "2 EXPERIMENTS ON UCF-101", "content": "Further experiments were conducted on the UCF-101 dataset to offer a broader evaluation of our approach. Theoutcomes, detailed in Table 6, show our method's performance in comparison to existing benchmarks."}, {"title": "3 ABLATION STUDIES ON PROMPT POSITION", "content": "A parameter search was conducted to fine-tune the positioning of temporal and spatial prompts within the DPATframework, with the starting position (start) set to 1 and the ending position (end) set to 5. We varied the endingposition (endg) and adjusted the starting position of the subsequent element (starte) accordingly. The search results,provided in Table 7, validate our initial configuration choices, though it is acknowledged that due to the expansivesearch space, these findings may not represent the optimal configuration."}, {"title": "4 ABLATION STUDIES ON BOTTLENECK RATIO", "content": "The bottleneck ratio, a critical hyperparameter in our DPAT framework, influences the trade-off between model capacity and efficiency. Experiments were conducted on the Kinetics-400 dataset to identify the optimal bottleneck ratio.Various ratios were tested, and their impacts on average accuracy and backward forgetting were assessed. As Table 8indicates, a bottleneck ratio of 0.25 optimally balances model capacity with efficiency, leading to the best performanceon this dataset."}]}