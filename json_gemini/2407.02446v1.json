{"title": "Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling", "authors": ["Margaret Li", "Weijia Shi", "Artidoro Pagnoni", "Peter West", "Ari Holtzman"], "abstract": "RLHF-aligned LMs have shown unprecedented ability on both benchmarks and long-form text generation, yet they struggle with one foundational task: next-token prediction. As RLHF models become agent models aimed at interacting with humans, they seem to lose their world modeling the ability to predict what comes next in arbitrary documents, which is the foundational training objective of the Base LMs that RLHF adapts.\nBesides empirically demonstrating this trade-off, we propose a potential explanation: to perform coherent long-form generation, RLHF models restrict randomness via implicit blueprints. In particular, RLHF models concentrate probability on sets of anchor spans that co-occur across multiple generations for the same prompt, serving as textual scaffolding but also limiting a model's ability to generate documents that do not include these spans. We study this trade-off on the most effective current agent models, those aligned with RLHF, while exploring why this may remain a fundamental trade-off between models that act and those that predict, even as alignment techniques improve.", "sections": [{"title": "1 Introduction", "content": "Alignment via RLHF (Ivison et al., 2023; Touvron et al., 2023) trains models towards action: completing specific goals and excelling across both short and long-form textual tasks. RLHF works by adapting base LMs that are trained to be world models, accurately predicting the distribution of text that might occur after an arbitrary prefix. While RLHF models tend to excel at complex tasks, in this work we find that they partially lose the world modeling abilities that allow base LMs to simulate documents from the broader distribution of the internet.\nWe propose that this trade-off is a natural result of RLHF models concentrating probability to specific spans, which allows these models to blueprint long-form generation (Figure 1) but reduces their ability to model arbitrary text.\nSpecifically, RLHF models struggle with next-token prediction which directly measures ability to world model, even when they are finetuned to regain these skills (\u00a72). RLHF models seem to concentrate probability on a smaller set of text (\u00a73), which follows past work on distributional collapse (Shumailov et al., 2023). Yet this concentration may have a use: making generation more self-predictable, and helping to blueprint long-form text generations. For example, we find anchor spans (Figure 1) which appear across many samples for the same prompt, and seem to serve as scaffolding for generation (\u00a74).\nIs a trade-off between world modeling and agent modeling fundamental? We argue that self-predictability is likely an inevitable aspect of successful agent models, not just a spurious byproduct of RLHF. In order to generate coherent long-form responses (or act towards goals in general) an agent must guarantee that its future actions are largely predictable to its current self. In effect, agent modeling may require minimizing long-term uncertainty while world modeling requires maintaining the true uncertainty of natural text documents, a fundamental trade-off. We briefly explore this question in \u00a75.\nSuch a trade-off would suggest that methods for adapting models to useful tasks, such as RLHF, will tend to narrow the breadth of a model. In other words, an agent model that takes actions towards a long-term goal may not be fully representative of the broader distribution of all possible agents and goals. General systems covering both sets of abilities might combine agent and world models rather than relying on agent models to both act and predict."}, {"title": "2 Agent models aren't general language models anymore", "content": "Many current language models begin as world models, trained to accurately predict the probabilities of possible events in a medium (i.e. text), and are later adapted as agent models-trained to interact with users towards specific goals. While agent models, particularly those trained with RLHF, are unparalleled as interactive dialogue agents, we show in this section that such adaptation diminishes the original ability to world model, i.e., provide accurate estimated probabilities of text. Our analysis shows that agent models significantly underperform the Base LMs they are trained from on a set of language modeling tasks across diverse domains (\u00a72.1). Even when re-trained towards language modeling (\u00a72.2), they fail to match Base models, suggesting a potential trade-off between the abilities of agent and world models."}, {"title": "2.1 Perplexity of Base vs. RLHF models", "content": "The ability to accurately predict the what will happen next given arbitrary starting conditions (i.e., to world model) can be evaluated in the text domain as performance on next-token prediction with the perplexity metric. We evaluate the perplexity of the Base models against their agent model counterparts. We focus on RLHF as a means of producing agent models, as this is the most popular and effective approach currently being used. Specifically, we analyze two Base models: Llama 2 (Touvron et al., 2023) and OLMO (Groeneveld et al., 2024). For their RLHF adaptations, we employ Tulu 2 (Ivison et al., 2023) and Llama 2 Chat (Touvron et al., 2023) for Base Llama 2, and OLMO RLHF for the Base OLMo. We consider common pre-training corpora such as C4 (Roberts et al., 2019), Arxiv (Clement et al., 2019), and Wikipedia for evaluation. To test generalization to new data, we also use new corpora released post-model development including new Arxiv papers and BBC news stories (Li et al., 2023b). Furthermore, we incorporate instruction finetuning data like Humpback (Li et al., 2023a) and chat assistant data such as OASST1 (K\u00f6pf et al., 2024), Anthropic Harmless and Helpful corpora (Bai et al., 2022).\nResults in Figure 2 (A) show that agent models perform consistently worse than the Base models that they were adapted from at language modeling. On standard test sets crawled directly from the broader internet (e.g., C4) or specific domains (e.g., Arxiv) this is not a surprising result: RLHF models"}, {"title": "2.2 Readaptation via Finetuning", "content": "While RLHF certainly warps the distribution of LLMs to be worse predictors, is this merely a surface-level change? Is the next-token-prediction still hidden within the weights of the adapted model? Perhaps the information for arbitrary next token prediction is simply unused in the output layers of RLHF models. Fully addressing this concern is beyond the scope of this paper, but we present evidence that it is at least not trivial to recover the next-token-prediction capabilities of RLHF models. To test this hypothesis we continue to pretrain both Base models and RLHF models on the training sets of the evaluation corpora used in \u00a72.1.\nThe results in Figure 2 (B) show that it is difficult to recover the original ability of RLHF models to act as world models. Note that this remains true even for instruction tuning and chat assistant corpora, which much more closely match the distribution that agent models have been adapted to. Further evidence that RLHF models are not even good rankers of likely text is shown in Appendix D via the Shannon Game (Hovy and Lin, 1998)."}, {"title": "3 Agent models concentrate probability", "content": "Past work (Shumailov et al., 2023) has suggested that RLHF models collapse their probability distributions, assigning high probability to a small set of tokens rather than a smoother distribution as observed in Base language models. In this section we quantify some properties of this collapse, leading us to propose that RLHF concentrates model probability onto text that is predictable by the model, yet still diverse and high quality. This will be an important point in \u00a74 when we discuss how agent models appear to have implicit blueprints for generating text by narrowing the scope of possible futures, particularly onto self-predictable text that could make structuring long-form text easier.\nFigure 3 shows how concentrated different model distributions are, measured as the probability mass assigned to the most probable k tokens on gold vs. generated data. The most top-heavy distribution is very clearly RLHF on its own sampled generations, e.g., the average probability of the highest probability token in RLHF when conditioning on its own generated text is nearly 0.9"}, {"title": "4 Agent models think ahead", "content": "We have shown that RLHF models are no longer good next token predictors and that they concentrate their probability distributions into more predictable outcomes. What does this probability mass shift lead to? In this section we show evidence that RLHF models make use of a kind of implicit blueprint for long-form generation rather than predicting only one token in advance, effectively using probability concentrated on certain future spans to enable \"thinking ahead\" in long form generation."}, {"title": "4.1 RLHF hidden states are more predictive of future tokens", "content": "As a basic measure of \u201cthinking ahead\", we first estimate the information models have about future timesteps, beyond the next token. Using the methodology of Pal et al. (2023), we study how well we can predict tokens for Llama 2 7B Base and RLHF models by leveraging their own hidden states with a linear probe. Specifically, we perform evaluation on the Pile (Gao et al., 2020) and Anthropic Helpful (Bai et al., 2022) datasets. We train a linear model on the hidden representation from the 28th layer (Llama 2 7B comprises 32 layers), using 100,000 tokens from each dataset, to predict tokens n steps into the future (n = 1,2,3), after the token being predicted at the current timestep. We use this prediction accuracy as a metric to assess the effectiveness of using a model's hidden state to foresee its own generation of future tokens. Linear probes on RLHF models can predict future tokens in their own generations with higher accuracy than Base models (Figure 6). RLHF adapted models find it easier to predict the future because they generate a future that is more predictable, as supported by Figure 5.\nFigure 6 also shows that RLHF models may contain a subset of information that base LLMs do, as base LLMs are more effective at predicting future tokens of RLHF models than the reverse. This supports the idea of concentration or collapse in RLHF models, as their generations fit Base LLMs, but they no longer explain Base LLM generations as effectively."}, {"title": "4.2 Sampled generations contain align-able blueprints", "content": "RLHF models seem to construct implicit blueprints for a given prompt, consistently touching on certain points across different samples. Sampling 100 continuations for the same prompt from an RLHF model leads to sequences with a significant amount of diversity (in terms of unique ngrams), but a surprisingly high amount of overlap between the sequences. While diversity and overlap may seem to be in conflict, Figure 7 shows this is not the case. We can use nucleus sampling (Holtzman et al., 2020) to generate text with similar ngram diversity across Base LLMs and RLHF-adapted models by setting the value of p appropriately (see Appendix E.4). Yet even when diversity statistics are similar,"}, {"title": "4.3 Anchor spans as an implicit blueprint", "content": "Visualizing these alignments suggests that RLHF models seem to blueprint generations, consistently using the same key points across many generations for a given prompt (Figure 1 top). Base LLMs show no such structure (Figure 8 bottom).\nWe visualize these blueprints with Sankey diagrams (Figure 1 top), which characterize the flow through critical nodes over a sequential process. We focus on \"anchor spans\"\u2014substrings that occur across many sampled outputs for the same prompt such that the flow between two nodes A and B represents the set of generations which contain A, followed by B, with (possibly different) text in between. Specifically, we first identify all text spans of a fixed minimum length (30 characters) that occur at the same index in the alignments found in \u00a74.2, for at least some threshold number of generations (20%). If the same span occurs at two different positions in the alignment indexing, they are counted as different spans. We sort the spans in descending order of frequency across unique outputs, and then by span length. We then greedily pick up to a maximum (6 in these visualizations)"}, {"title": "5 Is this a fundamental trade-off?", "content": "Are these differences in world models and agent models fundamental, or just a result of current language model training and RLHF practices? Even if new agent adaptation methods alleviate some of these problems, we propose that, under fixed capacity, there must necessarily be some trade-off between world models and agent models because of the underlying differences in their optimal behavior, which we are more closely approaching with recent state-of-the-art methods."}, {"title": "5.1 Why can't world models just be agent models when we ask them to be?", "content": "Prompts almost never fully specify desired behavior. Base LLMs have been trained to sample from the space of possible documents with a given prefix. In the overwhelming majority of cases, prefixes do not fully specify many of the choices a document can represent. Furthermore, Base LLMs have been shown to be very sensitive to short-term context (Paperno et al., 2016; Wang et al., 2023). This is a feature, not a bug, of world models, as short-term context is generally more predictive than long-term context in human-authored documents. Yet it means that Base LLMs are highly sensitive to sampling even one incoherent token. Dziri et al. (2024) suggest that the probability of generating an error is lower bounded by the 1 \u2013 P(error) where l is the length of a document. The actual error rate is likely much higher, as the model conditions on previously made errors, creating a snowball effect, as with hallucination (Zhang et al., 2023)."}, {"title": "5.2 Planning around randomness", "content": "The ability for agents to plan well is directly related to the amount of randomness in their environment. Reinforcement Learning (RL) is the mathematical language of agent modeling, so we take a moment to conceptualize why RLHF models would collapse"}, {"title": "6 Related Work", "content": "Catastrophic Forgetting Prior work examined models forgetting previously learned distributions. In most continual learning settings, where the model continues to train on new data, this is termed catastrophic forgetting (Kirkpatrick et al., 2017). This phenomenon has been observed in Language Models, and several mechanisms have been proposed to mitigate its effects (Chen et al., 2020; Xu et al., 2020; Vu et al., 2022). More recently, catastrophic forgetting has also been found to impact modern generative LLMs (Luo et al., 2023).\nDistribution collapse of LLMs Distribution collapse is known to occur in models which have been trained on data distributions that include model generations (Shumailov et al., 2023). Such self-consuming models exhibit a degradation in generation quality as well as diversity (Briesch et al., 2023; Alemohammad et al., 2023). Unlike these works, which do not consider preference tuning objectives at all, we focus on collapse in RLHF models. Other works do consider RLHF models, but do not study the nature of this collapse, instead focusing exclusively on methods for alleviating the style of mode collapse which arises from the degenerative overfitting to an imperfect reward model (Perez et al., 2022; Go et al., 2023). These methods are taken from the RL literature (Jaques et al., 2017), and build on studies which find mode collapse common in many other models (Che et al., 2017; Mescheder et al., 2018).\nConcurrent with our work, Lake et al. (2024) show that RLHF appears to be collapsing onto a subset of the Base LLM distribution, as we hypothesize in \u00a74.1, suggesting that RLHF has lower diversity due to an aggregating effect that can be partially (but not fully) mimicked with prompting. Xiao et al. (2024) also studies RLHF models, specifically on collapse at a preference level over all generations. Our work, on the other hand, visualizes and describes, quantitatively and qualitatively, the sequence-level multi-token repetition in generations after RLHF, as characterized by the presence of anchor spans."}, {"title": "7 Conclusion", "content": "We present evidence that: (1) RLHF-adapted LLMs are no longer performant next-token predictors, and thus no longer serve as world models of the textual space. (2) Such models concentrate their probability into a more predictable subdistribution of the Base model. (3) RLHF LLMs consistently produce anchor spans which can serve as a blueprint for long-form generations.\nWe propose that these differences may indicate a deeper trade-off between world models and agent models. An agent model that takes action via sampling might reshape its distribution in such a way that it no longer represents the full set of possibilities that world models capture, ensuring that it doesn't \"go off the rails\u201d as Base LLMs are prone to. Future work could explore strategies that can mitigate the observed trade-off between the predictive capabilities of world models and the action-oriented nature of agent models. For instance, a system that decides when to call a world model vs. an agent model may be able to more reliably switch between these for different goals, using the world model as a probabilistic simulator and predictor that helps the agent model decide how to act."}, {"title": "Limitations", "content": "One key limitation of our work that will need to be addressed in future literature is comprehensiveness across a broader range of models, as well as varied methods for agent-alignment. We focus on RLHF here as this is the most popular and successful method currently being used. We also aimed to test popular and performant models, as these represent the limits of both RLHF and base LM capabilities. In future work, it will be important to study more various models, including more model scales. It will also be useful to include multiple random seeds for training/tuning in all experiments, but the cost of such experiments would be infeasible here.\nIt will also be useful in future work to study the blueprints produced by RLHF models in more detail. This work aims to both demonstrate the trade-off between agent and world modeling, then explore different aspects of this blueprinting. However, dedicated work will be required to fully characterize this process."}, {"title": "Ethical Considerations", "content": "Aligned models, such as those tuned with RLHF, have seen a recent explosion in capabilities, popularity, and deployment compared to traditional LLMs trained primarily on broad text prediction. As part of this shift, aligned models are much more frequently framed as \u201cagents\u201d which can take explicit actions, and take active part in human-facing systems. This framing poses significant risks without a better conceptual understanding of these techniques, particularly with respect to whether these models are indeed robust agents and what underlying learned mechanisms might allow for this.\nOur work seeks to contribute to this understanding. For example, we find some evidence that aligned models may indeed be planning, which supports a notion of \"agentiveness\u201d. Yet we also find that these models lose robust and accessible notions of calibrated text prediction, which could indicate a tendency towards biased heuristics or at least away from the more robust, broad-text understanding of traditional LMs. Overall, our work indicates ongoing concerns with treating aligned models as well-informed agents, and demands further study into these aspects and risks."}, {"title": "D Shannon Game", "content": "What if the ranking information about which tokens are more or less likely is still preserved in RLHF models, even if the probabilities themselves are distorted? The perplexities RLHF adapted LLMs yield are higher, but this could be potentially be a result of the distribution collapse RLHF models undergo (see \u00a73). To evaluate how well LLMs rank the gold next token, we evaluate both Base and RLHF models using the Shannon Game.\nExperimental setup The Shannon Game (Hovy and Lin, 1998), is a next token prediction task, except no probabilities are used. Instead, models are judged by the number of incorrect guesses that a model ranks with a higher score over the target token. The Shannon Game is invariant to relative differences in exactly how much probability is allocated to different strings, and is only sensitive to the ordering that tokens are given in the hypothesis. We evaluate the Base Llama 2 and RLHF Llama 2 (the Chat version) on LAMBADA (Paperno et al., 2016), a collection of narrative passages designed to test the ability of LLMs on predicting the final word of a whole passage.\nResults Table 1 shows that this is not the case via the Shannon Game, revealing that RLHF adapted LLMs are worse at ranking possible next-tokens, not just assigning them probability. Table 1 shows that RLHF models are worse at the Shannon Game, suggesting that the ability to model arbitrary aspects of the textual world are diminished by current agent adaptation techniques.\nWhile it is tempting to assume that this is merely a result of imperfect RLHF methods, we argue that this trade-off is inherent to agent-adaptation. To generate multiple hundreds of tokens towards a singular goal, an agent model must limit the amount of uncertainty about future tokens. Planning a coherent document while marginalizing over all possible paths is an exponentially harder problem then collapsing onto a small set of possibilities. We hypothesize that the long-form generation capabilities of current RLHF models, are a general feature agent models: limiting the subspace of possibilities for any given prompt allows for better planning within this subspace. Further evidence for this hypothesis is given in \u00a74."}, {"title": "5.1 Why can't world models just be agent models when we ask them to be?", "content": "Prompts almost never fully specify desired behavior. Base LLMs have been trained to sample from the space of possible documents with a given prefix. In the overwhelming majority of cases, prefixes do not fully specify many of the choices a document can represent. Furthermore, Base LLMs have been shown to be very sensitive to short-term context (Paperno et al., 2016; Wang et al., 2023). This is a feature, not a bug, of world models, as short-term context is generally more predictive than long-term context in human-authored documents. Yet it means that Base LLMs are highly sensitive to sampling even one incoherent token. Dziri et al. (2024) suggest that the probability of generating an error is lower bounded by the 1 \u2013 P(error) where l is the length of a document. The actual error rate is likely much higher, as the model conditions on previously made errors, creating a snowball effect, as with hallucination (Zhang et al., 2023)."}, {"title": "5.2 Planning around randomness", "content": "The ability for agents to plan well is directly related to the amount of randomness in their environment. Reinforcement Learning (RL) is the mathematical language of agent modeling, so we take a moment to conceptualize why RLHF models would collapse"}, {"title": "6 Related Work", "content": "Catastrophic Forgetting Prior work examined models forgetting previously learned distributions. In most continual learning settings, where the model continues to train on new data, this is termed catastrophic forgetting (Kirkpatrick et al., 2017). This phenomenon has been observed in Language Models, and several mechanisms have been proposed to mitigate its effects (Chen et al., 2020; Xu et al., 2020; Vu et al., 2022). More recently, catastrophic forgetting has also been found to impact modern generative LLMs (Luo et al., 2023).\nDistribution collapse of LLMs Distribution collapse is known to occur in models which have been trained on data distributions that include model generations (Shumailov et al., 2023). Such self-consuming models exhibit a degradation in generation quality as well as diversity (Briesch et al., 2023; Alemohammad et al., 2023). Unlike these works, which do not consider preference tuning objectives at all, we focus on collapse in RLHF models. Other works do consider RLHF models, but do not study the nature of this collapse, instead focusing exclusively on methods for alleviating the style of mode collapse which arises from the degenerative overfitting to an imperfect reward model (Perez et al., 2022; Go et al., 2023). These methods are taken from the RL literature (Jaques et al., 2017), and build on studies which find mode collapse common in many other models (Che et al., 2017; Mescheder et al., 2018).\nConcurrent with our work, Lake et al. (2024) show that RLHF appears to be collapsing onto a subset of the Base LLM distribution, as we hypothesize in \u00a74.1, suggesting that RLHF has lower diversity due to an aggregating effect that can be partially (but not fully) mimicked with prompting. Xiao et al. (2024) also studies RLHF models, specifically on collapse at a preference level over all generations. Our work, on the other hand, visualizes and describes, quantitatively and qualitatively, the sequence-level multi-token repetition in generations after RLHF, as characterized by the presence of anchor spans."}, {"title": "7 Conclusion", "content": "We present evidence that: (1) RLHF-adapted LLMs are no longer performant next-token predictors, and thus no longer serve as world models of the textual space. (2) Such models concentrate their probability into a more predictable subdistribution of the Base model. (3) RLHF LLMs consistently produce anchor spans which can serve as a blueprint for long-form generations.\nWe propose that these differences may indicate a deeper trade-off between world models and agent models. An agent model that takes action via sampling might reshape its distribution in such a way that it no longer represents the full set of possibilities that world models capture, ensuring that it doesn't \"go off the rails\u201d as Base LLMs are prone to. Future work could explore strategies that can mitigate the observed trade-off between the predictive capabilities of world models and the action-oriented nature of agent models. For instance, a system that decides when to call a world model vs. an agent model may be able to more reliably switch between these for different goals, using the world model as a probabilistic simulator and predictor that helps the agent model decide how to act."}, {"title": "Limitations", "content": "One key limitation of our work that will need to be addressed in future literature is comprehensiveness across a broader range of models, as well as varied methods for agent-alignment. We focus on RLHF here as this is the most popular and successful method currently being used. We also aimed to test popular and performant models, as these represent the limits of both RLHF and base LM capabilities. In future work, it will be important to study more various models, including more model scales. It will also be useful to include multiple random seeds for training/tuning in all experiments, but the cost of such experiments would be infeasible here.\nIt will also be useful in future work to study the blueprints produced by RLHF models in more detail. This work aims to both demonstrate the trade-off between agent and world modeling, then explore different aspects of this blueprinting. However, dedicated work will be required to fully characterize this process."}, {"title": "Ethical Considerations", "content": "Aligned models, such as those tuned with RLHF, have seen a recent explosion in capabilities, popularity, and deployment compared to traditional LLMs trained primarily on broad text prediction. As part of this shift, aligned models are much more frequently framed as \u201cagents\u201d which can take explicit actions, and take active part in human-facing systems. This framing poses significant risks without a better conceptual understanding of these techniques, particularly with respect to whether these models are indeed robust agents and what underlying learned mechanisms might allow for this.\nOur work seeks to contribute to this understanding. For example, we find some evidence that aligned models may indeed be planning, which supports a notion of \"agentiveness\u201d. Yet we also find that these models lose robust and accessible notions of calibrated text prediction, which could indicate a tendency towards biased heuristics or at least away from the more robust, broad-text understanding of traditional LMs. Overall, our work indicates ongoing concerns with treating aligned models as well-informed agents, and demands further study into these aspects and risks."}, {"title": "Equation", "content": "V(s) = max \u03b1\u0395\u0391{ R(s, a) + \u03b3 S'ES P(s's, a)V(s') } (1)\nmax \u03b1'\u0395\u0391  P(s's, a)R(s', a') \u2264 max \u03b1'EA R(s', a') (2)"}]}