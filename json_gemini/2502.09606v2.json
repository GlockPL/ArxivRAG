{"title": "Human-LLM Coevolution: Evidence from Academic Writing", "authors": ["Mingmeng Geng", "Roberto Trotta"], "abstract": "With a statistical analysis of arXiv paper ab-\nstracts, we report a marked drop in the fre-\nquency of several words previously identified\nas overused by ChatGPT, such as \"delve\", start-\ning soon after they were pointed out in early\n2024. The frequency of certain other words\nfavored by ChatGPT, such as \u201csignificant\", has\ninstead kept increasing. These phenomena\nsuggest that some authors of academic papers\nhave adapted their use of large language mod-\nels (LLMs), for example, by selecting outputs\nor applying modifications to the LLM outputs.\nSuch coevolution and cooperation of humans\nand LLMs thus introduce additional challenges\nto the detection of machine-generated text in\nreal-world scenarios. Estimating the impact of\nLLMs on academic writing by examining word\nfrequency remains feasible, and more attention\nshould be paid to words that were already fre-\nquently employed, including those that have\ndecreased in frequency due to LLMs' disfavor.", "sections": [{"title": "1 Introduction", "content": "After the launch of ChatGPT at the end of 2022,\nlarge language models (LLMs) began to be widely\nused and are now transforming many aspects of our\nwork and life, with academic writing being one of\nthem. The coevolution of AI and humans has also\nbeen recognized by researchers (Pedreschi et al.,\n2024).\nFor example, empirical studies from April 2024\nobserved that the frequency of certain words used\nin academic papers published in 2023 had changed\nand confirmed a strong correlation between these\nchanges and the use of LLMs (Liang et al., 2024b;\nGeng and Trotta, 2024). Survey results also show\nthat many researchers are utilizing LLMs in their\nwork (Liao et al., 2024).\nThe detection of machine-generated text (MGT)\nhas also attracted a lot of attention (Tang et al.,\n2024; Chowdhury et al., 2024; Wang et al., 2025),\nbut the performance of detectors has also been\nquestioned early on (Sadasivan et al., 2023; Weber-\nWulff et al., 2023; Ghosal et al., 2023). Recent\nstudies continue to show that some methods are not\nsufficiently robust (Zhang et al., 2024b; Wang et al.,\n2024; Creo and Pudasaini, 2025). The effective-\nness of MGT detectors is also related to the model\nof LLMs and the type of text (Liu et al., 2024), and\ntheir accuracy may also be exaggerated (Dough-\nman et al., 2024). The situations likely to arise in\nreality are more complicated and are not limited\nto a binary classification framework (Zhang et al.,\n2024a). Thus, examining and analyzing the ongo-\ning evolution of word usage remains a useful and\nmeaningful task."}, {"title": "2 Data", "content": "arXiv paper metadata Metadata of arXiv pa-\npers updated weekly on Kaggle\u00b9. Our paper used\nversion 214 of this dataset. Between January 2018\nWithdrawn arXiv papers data WithdrarXiv\ndataset (Rao et al., 2024), containing over 14,000\narXiv withdrawn papers up to September 2024."}, {"title": "3 Word Frequency Analysis", "content": "The analysis presented in Figure 2 is based on the\nabstracts of all arXiv papers submitted between\n2018 and 2024. The frequency of words is calcu-\nlated on a monthly basis and normalized per 10,000\nabstracts.\nFigure 2a shows the frequency of the 4 words\nhighlighted by Liang et al. (2024b) and Figure 2b\npresents the frequency of the 6 words emphasized\nby Liang et al. (2024a). The former paper analyzes\nacademic papers, while the latter focuses on Al\nconference peer reviews, and the average frequency\nof these words is shown in Figure 2c. The trend\nis clear: starting from April 2024, the frequency\nof these well-known LLM-style words began to\ndecrease. Some other words show patterns of con-\nsistent growth or a rise followed by a decline, as\nillustrated in Figure 7a of the Appendix.\nA study published in December 2024 also ob-\nserved a decline in the use of certain words, such\nas \"delve\", in some selected arXiv papers (Leiter\net al., 2024). While they suggested that this was\nlikely due to the release of GPT-40 in May 2024,\nwe suggest that the main reason is that LLMs may\nhave given these words a bad reputation. Many re-\nsearchers noticed such kind of words in March and\nApril and quickly changed their arXiv abstracts.\nIf new LLMs were the cause, the drop in word\nfrequency would have been delayed.\nIn addition, the frequency of words like \u201csignifi-\ncant\", specifically pointed out by Geng and Trotta\n(2024), continues to grow. This may be because\nthese terms are relatively common and frequently\nused, their presence alone would not easily lead\none to suspect the text as the product of LLMs.\nBesides, as presented in Table 1, this article has\nattracted less attention than the former, for exam-\nple, in terms of Google Scholar citation counts.\nTherefore, fewer researchers should have noticed\nthe relationship between these words and LLMs.\nWe compared the results with the abstracts of\nthe withdrawn papers, as illustrated in Figures 2e\nand 2f. Given the small number of withdrawn pa-\npers, the 12-month rolling averages of their word\nfrequency are used in the graphs. The frequency\nof some words, such as \"intricate\", is higher in the\nwithdrawn papers, but the difference is not very\nlarge, as is also the case in Figures 7b, 7c and 7d\nof the Appendix.\nTo better compare the changes in word fre-\nquency, we define $R_{ij}(T_1, T_2)$ (the ratio of word i\nin the abstracts of category j between periods $T_1$\nand $T_2$) as follows: $R_{ij}(T_1,T_2) = \\frac{f_{ij}(T_1)}{f_{ij}(T_2)}$, where\n$f_{ij}(T)$ is the frequency of word i in the abstracts\nof category j during the time period T.\nWe also categorized the abstracts into two groups\nbased on the first category of the papers: com-"}, {"title": "4 Challenges in Machine-Generated Text\nDetection", "content": "The first 1000 arXiv papers submitted each year\nfrom 2018 to 2025 were utilized for this part of\nthe analysis. We also used the following two\nsimple prompts to examine the differences be-\ntween original arXiv abstracts and those revised\nby GPT-40-mini (temperature = 1, top-p = 0.9):\n\u2022 (P1) Revise the following sentences: ...\n\u2022 (P2) Don't use the following words in your re-\nsponses: 'realm', 'pivotal', 'intricate', 'show-\ncasing'. Revise the following sentences: ..."}, {"title": "5 Conclusion and Discussion", "content": "The results in Figure 4 reinforce the point\nthat the frequency of certain words increases af-\nter LLMs revision. Using prompt P2, aimed at\nsuppressing them, reduces the frequency of such\nwords, although it does not completely eliminate\nthem.\nHumans and LLMs are coevolving and we can al-\nready conclude that, for this reason, the impact\nof LLMs on academic writing will fully assert it-\nself over the long term. According to recent stud-\nies, people who frequently use ChatGPT for writ-\ning tasks can accurately distinguish AI-generated\ntext (Russell et al., 2025), which implies that they\nare also able to foil MGT detectors.\nGrammarly can sometimes achieve effects simi-\nlar to those of ChatGPT (Rudnicka, 2023), and the\nmix of human-written text and machine-generated\ntext should be very common in academic writing.\nDetecting LLM-generated content with accuracy is\nbecoming more difficult, perhaps impossible on a\ntext-by-text basis.\nOur findings suggest that some researchers may\nintentionally avoid using LLM-characteristic terms,\nbut they are not as sensitive to some relatively com-\nmon words. The gradual decrease in the occurrence\nof \"is\" and \"are\" in arXiv abstracts is an excellent\nexample of such a trend, which we ascribe to a\nmore subtle - and continually increasing\u2013 LLM\ninfluence.\nTherefore, using the frequency of more common\nwords to measure the impact of LLMs on a vast\nnumber of publications will be more reliable, al-\nthough this approach is less suitable for the precise\ndetection of short texts."}]}