{"title": "SCOPE: Sign Language Contextual Processing with Embedding from LLMs", "authors": ["Yuqi Liu", "Wenqian Zhang", "Sihan Ren", "Chengyu Huang", "Jingyi Yu", "Lan Xu"], "abstract": "Sign languages, used by around 70 million Deaf individuals globally, are visual languages that convey visual and contextual information. Current methods in vision-based sign language recognition (SLR) and translation (SLT) struggle with dialogue scenes due to limited dataset diversity and the neglect of contextually relevant information. To address these challenges, we introduce SCOPE (Sign language Contextual Processing with Embedding from LLMs), a novel context-aware vision-based SLR and SLT framework. For SLR, we utilize dialogue contexts through a multi-modal encoder to enhance gloss-level recognition. For subsequent SLT, we further fine-tune a Large Language Model (LLM) by incorporating prior conversational context. We also contribute a new sign language dataset that contains 72 hours of Chinese sign language videos in contextual dialogues across various scenarios. Experimental results demonstrate that our SCOPE framework achieves state-of-the-art performance on multiple datasets, including Phoenix-2014T, CSL-Daily, and our SCOPE dataset. Moreover, surveys conducted with participants from the Deaf community further validate the robustness and effectiveness of our approach in real-world applications. Both our dataset and code will be open-sourced to facilitate further research.", "sections": [{"title": "Introduction", "content": "Sign language is the vital visual language used by the Deaf and hard of hearing. Hence, vision-based sign language understanding provides a communication bridge between the Deaf and hearing communities. Such a bridge should accurately and conveniently convey complex contextual information during communication between us humans, especially for dialogue scenarios.\nCurrently, the two main tasks in vision-based sign language processing include Sign Language Recognition (SLR) (Jiao et al. 2023; Wei and Chen 2023; Zheng et al. 2023) and Sign Language Translation (SLT) (Zhao et al. 2024; Chen et al. 2022b; Yin et al. 2023). SLR converts visual signals into intermediate gloss sequences (Stokoe Jr 2005), while SLT translates visual signals or glosses into natural language. Yet, we notice that most existing meth-"}, {"title": "Related Works", "content": "Sign Language Recognition (SLR) focuses on recognizing glosses from sign videos. While progress has been made in Isolated SLR (ISLR) (Albanie et al. 2020; Tunga, Nuthalapati, and Wachs 2021; Li et al. 2020c; Hu et al. 2021; Li et al. 2020a), current research is shifting to Continuous SLR (CSLR), which converts continuous sign videos into sentence-level gloss sequences. This task involves two main components: feature extraction and translating these features into gloss sequences.\nVisual feature extraction often involves extracting features from RGB images using CNNs(Chen et al. 2022a; Li et al. 2020b; Hu et al. 2023c; Min et al. 2021). These features are then modeled with temporal frameworks like RNNs(Camgoz et al. 2018; Ko et al. 2019), LSTMs(Hu et al. 2023a; Cui, Liu, and Zhang 2019), and Transformers(Camgoz et al. 2020; Voskou et al. 2021; Yin and Read 2020) to capture the connection between visual signals and glosses. Some approaches(Zhou et al. 2021b; Papadimitriou and Potamianos 2020) utilize estimated keypoint sequences to describe motions through spatial coordinates or generate heatmaps(Chen et al. 2022b, 2024). However, many methods require video processing, which can be slow and space-consuming, limiting their practical application.\nDecoding the extracted features into gloss sequences needs temporal modeling. Hidden Markov Models (HMMs)(Koller, Zargaran, and Ney 2017a; Gao et al. 2004; Koller et al. 2016) and Connectionist Temporal Classification (CTC)(Cheng et al. 2020; Zhou et al. 2021b; Min et al. 2021) are commonly used for this purpose. However, most existing methods focus on frame-wise or sentence-wise information, often neglecting the broader linguistic context, resulting in the loss of important language features.\nSign Language Translation (SLT) aims to translate sign language directly into natural language, bridging the gap between the Deaf community and hearing individuals. This task is challenging due to the modality gap between visual signal and text, compounded by the scarcity of context sign language datasets. Many approaches (Camgoz et al. 2020; Zhou et al. 2021b,a) use SLR results to aid translation. Joint training of SLR and SLT modules has also been explored to improve performance. Some researchers(Li et al. 2020b; Camgoz et al. 2018; Zhou et al. 2023) seek to eliminate gloss by directly translating sign language videos into text using techniques like Conditional Variational Autoencoders and Transformers. SLT involves projecting visual features into coherent textual representations, necessitating insights from both computer vision and natural language processing."}, {"title": "Method", "content": "We present SCOPE framework, a novel framework that aligns motion features with LLM-provided sentence embeddings of previous contexts, aiming to fully utilize contextually related dialogues in which sign language conversations mainly occur. To address the often overlooked contextual aspects in data collection, we provide SCOPE dataset that annotates sign videos with additional context information, which our model effectively utilizes. Details of SCOPE dataset will be further presented in the Dataset section.\nFig. 2 demonstrates the structure of our SCOPE framework. Our Embedding Alignment Encoder transforms motion features into an embedding that captures the linguistic information of the whole motion sequence. Aligning embedding space to a frozen LLM enables integrating contextual information of previous sentences to recognize glosses. Finally, Q-LORA fine-tuning fits an LLM for translating predicted glosses into spoken language with the assistance of context information."}, {"title": "Model Details", "content": "Embedding Alignment Encoder. We use a transformer encoder structure to extract information from motion features. For the input keypoints $J = J_1...J_t$, they first pass through the feature extractor linear layer and the temporal sequencer linear layer, which compress the motion information in the spatial and temporal dimensions, respectively, resulting in the intermediate state motion input D, which aligns textual embedding in shape.\nNext, we need to pretrain an Embedding Alignment Encoder to align features from the motion space with the textual embedding space. The key idea is to directly learn the alignment between the linguistic features of sign language motion and the contextual features of text. In this step, we aim to align the sign motion feature D with the embedding vector of the target sentence. We do this by passing the motion features through the Embedding Alignment transformer encoder and then pooling them to compress the time dimension, resulting in an embedding vector that matches the size of the text embedding. The encoding process, in detail, first embeds the input D into a latent space, represented by $h_0$, and then obtains the encoded hidden states $h_n$ through N attention layers. Finally, a feed-forward network is used to obtain the encoded vector. The formulas for the transformer motion encoder process are as follows:\n$Q = W^Qh_i, K = W^Kh_i, V = W^Vh_i,$ \n$h_{i+1} = Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{C}})V,$ (1)\nwhere $W^Q, W^K, W^V$ are trainable weights, C is the number of channels in the attention layer, and $h_{i+1}$ is hidden states before the next layer.\nSupervision by distance to the embedding of the target sentence provided by an LLM.\nThe loss of the motion encoder is the L2 distance between the pooled embedding vector and the target text embedding vector.\n$L_{emb} = E||E_{out} - E_{text}||^2,$ (2)\nwhere $L_{emb}$ denotes the embedding loss, $E_{out}$ is the output of the motion encoder, and $E_{text}$ is the text embedding vector. The text embedding vector is generated by a frozen LLM text embedding model (Neelakantan et al. 2022), which encodes the ground truth sentence meaning of the sign video. Through this process, we align the motion features with the language feature information, enhancing the connections between the isolated sign words.\nGloss Embedding encoder. After aligning the motion features, we obtain an embedding vector that contains both semantic and sign language information. Next, we combine this with the motion features to predict the gloss. For sign language conversations, providing previous language context is crucial for improving the accuracy of recognizing the current target sentence. Therefore, we use a frozen LLM to get the embedding vector for the previous sentences. To minimize irrelevant information, we only keep the last three text embeddings. If there are fewer than three previous sentences, we use a mask to ignore the padding input. The gloss embedding encoder is also a transformer encoder model. The encoding process can be formulated as follows:\n$H, A = Hidden(Cat(E_t, E_A)),$ \n$E_{out} = FFN(Attn(W^H H_A, W^K H_A, W^V H_A)),$ (3)"}, {"title": "", "content": "where Hidden is the hidden layer embedding in the transformer encoder, and Cat is the concatenate operation. $E_t$ is the previous stage encoded sequence, and $E_A$ is the above text embedding vector. FFN is the feed-forward network in the transformer encoder. Passing the output $E_{out}$ through a linear classifier layer, we get the output logic of the glosses.\nCTC Decoding. We use connectionist temporal classification (CTC) (Graves et al. 2006) loss to optimize the embedding encoder:\n$L_{CTC} = -logp(I|y) = \u2212log\\sum_{\u03c0\u2208B^{-1}(I)}P(\u03c0|y),$ (4)\nwhere $I = I_1...I_t$ is the gloss sequence corresponds to keypoints sequence J. B is a many-to-one mapping between hypotheses and gloss, and is the alignment path. In addition, we adopt Minimum World Error Rate (MWER) Training (Meng et al. 2021) technique to reduce the mismatch between training objectives and evaluation metrics, boosting the accuracy of hypotheses on top of the beam. We use beam search during training to decode the top 3 possible gloss sequences. While maintaining the top 1 decoded result as the final output of the SLR network, other candidate glosses contribute to optimization with minimum word error rate (MWER) loss:\n$L_{MWER} = \\sum_{i=1}^{N}\\frac{P(Y|J;\\theta)}{\\sum_{i=1}^{N}P(Y|J;\\theta)}R(Y^n, Y^*),$ (5)\nwhere $\\frac{P(Y^n|J; \\theta)}{\\sum_{i=1}^{N}P(Y^n|J; \\theta)}$ is the re-normalized posterior over the N-best hypotheses, $\\theta$ is model parameters, and $R(Y^n, Y^*)$ is the number of word errors in a hypothesis $Y^n$ compared to the reference $Y^*$.\nFurthermore, the top 3 decoded results also serve as the input of the LLM model in SLT task.\nContextual LLM Fine-tuning. Inspired by (Gong et al. 2024), we adopt the idea by using Q-LoRA to fine-tune an LLM as a sign language translator. We adopted the Qwen2 LLM model as our translator. To fine-tune Qwen2, we need to set the LLM using the scenario as a \"Sign language translator\" and design prompts to guide the model. In the prompts, we provide the top 3 gloss sequences mentioned in and all the above text related to the current sign language sequence, and ask the LLM to summarize the top 3 glosses and guess the correct words to use by checking previous texts. We also provide some summarized task examples to help the LLM understand translation procedures. We use the previous top 3 gloss outputs as input and use the designed prompt along with the above text as auxiliary input, jointly fine-tuning the LLM model. We optimize the model using the cross-entropy loss function:\n$L_{llm} = -\\sum_{i=1}^{|T|}(Y_{i}) log(\\hat{Y}_{i}), i \\in N_{tok}.$ (6)\n$Y_t$ is the ground truth textual output, and $\\hat{Y}_{t}$ is the predicted textual output. $N_{tok}$ is the number of classes in the tokenizer."}, {"title": "Data Processing", "content": "Iris Normalization. To fetch keypoint sequences, we utilized DWPose (Yang et al. 2023) to obtain whole-body keypoints (COCO-WholeBody (Jin et al. 2020)) from sign language videos. Each keyframe contains 133 keypoints $J = {J_{1,i},..., J_{T,i} i = 1...133}$. However, such keypoints are often influenced by the input video resolution and the distance between the person and the camera. A scaling process is needed to mitigate the impact of input distortions on motion. Inspired by (Lugaresi et al. 2019), we choose the length of the lower eyelid in humans as the golden standard, comparing the eyelid length differences to get the scale factor and scale motions to the standard size:\n$J^{scaled} = \\frac{(J_x, J_y)}{|(J_{x_{63}} - J_{x_{64}})|},$ (7)\nWhere $J^{Scaled}$ are scaled joints under frame t, ($J_x$, $J_y$) are 2D coordinates of joints. $J_{x_{63}}$ - $J_{x_{64}}$ is eyelid length; 63 and 64 are indexes of left and right wings of the eyelid in COCO-WholeBody.\nData Centralizing. After that, we followed (Jiao et al. 2023) by selecting 77 keypoints and dividing them into 5 groups, then applied group-specific centralization to decouple multi-grained motion information in skeleton data:\n$J_{t,k} = J_{t,k} - J_{t,r_g}, k \\in G,$ (8)\nwhere $J_{t,k}$ denotes joints under the t frame, group k, G are 5 groups, and $r_g$ is the root keypoint of group g.\nData Standardizing. Finally, we standardize all input motions to make their distribution more closely conform to a standard distribution, which eliminates the difficulties that motion corner cases bring to training:\n$\\hat{J}^{std} = \\frac{J_i}{N} \\sum_{i=1}^{N} \\frac{\\hat{J}_i - \\mu}{\\sigma}, i = 1...77,$ (9)\nWhere $J_i$ denotes the i \u2013 th joints, and $\\sigma^{std}$ is the standard deviation of joint i."}, {"title": "SCOPE dataset", "content": "A sign language dataset with contextual information is essential to fully leverage the power of context in implementing our context-aware approaches. We propose SCOPE, a large-scale Chinese sign language dataset that includes contextual dialogue information. Our data information and dataset comparison can be found in Tab.1.\nData Collection. Our dataset primarily focuses on daily conversations within the Deaf community, as well as dialogues involving specialized terminology in more professional settings(Bragg et al. 2019). Our dataset includes daily subjects such as school experiences, shopping, and social interactions. Glosses encompass specific products and brands, titles of audiovisual works, and other relevant terms. For more details, please refer to the supplementary material.\nData collection is carried out by a team whose primary members are several professional Deaf signers and three sign language linguistics experts. The team also includes a diverse group of Deaf individuals across various ages, genders, occupations, and educational backgrounds to cap-"}, {"title": "Experiments", "content": "Experimental Setup\nDatasets and Evaluation Metrics. For the SLR task, we evaluate our proposed method on PHOENIX14, PHOENIX14-T, CSL-Daily, and SCOPE dataset; the latter three datasets are also utilized in experiments on SLT task. For the number of samples and other details of datasets, please refer to supplementary materials. Train/dev/test splits of the existing datasets are maintained. For our SCOPE dataset, we follow (Zhang et al. 2024) to use widely adopted split ratios to randomly split our dataset by 80%, 5% and 15% into train, dev, and test sets, carefully ensuring that no same sentence appears in different sets and any sentence in the dev set or test set does not appear in context dialogues of the training set.\nFollowing previous works (Chen et al. 2022b), we adopt the Word Error Rate (WER), which measures the percentage of incorrect words in recognized text, for SLR, and BLEU (Papineni et al. 2002) and ROUGE-L (Lin 2004), which assess the quality of translations based on n-gram overlap and longest common subsequences, for SLT as evaluation metrics. Lower WER indicates more accurate recognition results, while higher BLEU and ROUGE-L signify better translations.\nImplementation Details. We obtain sentence embeddings by OpenAI's text-embedding-ada-002 (Neelakantan et al. 2022) model. Body 2D keypoints are collected from videos using DWPose (Yang et al. 2023). Our motion feature extractor block consists of a 2-layer MLP with a temporal Conv1D layer. The embedding alignment encoder and gloss encoder are both 8-head transformer encoders with 2 and 4 layers, respectively, with hidden size 1568 and feed-forward size 3136. We adopt the AdamW optimizer and use cosine annealing schedules, with 20 epochs focusing on alignment embedding, and 60 epochs for gloss encoder training while keeping the previous module frozen. When training without the context module, we do not provide context information by filling context embeddings with zeros and providing empty context input for LLM. All experiments are executed on 8 NVIDIA A800 GPUs. More implementation details are provided in the supplementary materials."}, {"title": "Comparison with State-of-the-art Methods", "content": "Sign Language Recognition (SLR) We evaluate our approach by comparing results on multiple datasets with recent methods SEN-CSLR(Hu et al. 2023c), TwoStream-SLR(Chen et al. 2022b) and CorrNet(Hu et al. 2023b).\nOn our SCOPE dataset, we evaluate their performance by training their open-sourced framework. We perform preprocessing to match the input specifications of each method and train their models adhering as closely as possible to their proposed training setups.\nAs shown in Tab.2, our context-free SCOPE outperforms other SLR methods in WER by 2.7%/2.2% on CSL-Daily dev/test sets and 3.3%/3.1% on SCOPE dataset, respectively. Moreover, adding context information further improves our model's recognition accuracy by 2.2%/3.3% WER, revealing that contextual understanding effectively assists gloss recognition.\nSign Language Translation (SLT) On the SLT task, we compare our approach with state-of-the-art gloss-supervised and gloss-free methods. Similarly, we stick to their respective training configurations when training their models on SCOPE dataset. Results in Tab.3 show that our approach outperforms previous methods by +3.73/+3.57 BLEU and +3.50/+3.14 BLEU in Phoenix-2014T and CSL-Daily dev/test sets. Additionally, our full approach on SCOPE dataset brings another +3.26/+2.79 BLEU improvement, which we attribute mainly to context-aware LLM fine-tuning. Notably, when comparing across datasets, SCOPE dataset generally yields better performance for any fixed method. We primarily attribute this result to our robust data annotation and cleaning process."}, {"title": "Ablation Studies", "content": "We conduct ablation experiments for both SLR and SLT tasks to validate the contributions of each component. The comparison between our full and context-free SCOPE model also suffices as an ablation study to demonstrate the significance of context information, both in recognition and in LLM fine-tuning. When the embedding alignment encoder is removed, the context sentence embeddings are concatenated to motion features directly, and $L_{emb}$ no longer serves as a supervision term. The performance of this model declines by 4.4%WER and 16.01 BLEU, and we note that it takes significantly longer for this model to converge. Thus, we deduce that the model encounters difficulties in aligning motion features with LLM context embeddings and ultimately behaves poorly. The removal of $L_{MWER}$ directly causes more word errors, thus deteriorating the translation results. The distribution of raw keypoints is severely biased without our Iris Normalization process, rendering the model overfit to extreme cases and unsuitable for real-time practical use with different aspect ratios and camera resolutions."}, {"title": "Real-time Application and User Studies", "content": "Authentic feedback from the Deaf community is the gold standard for practical use. We have developed a real-time SLT application to assist Deaf individuals in accessing dental care. Details are provided in the supplementary materials. Authentic feedback from the Deaf community is the gold standard for practical use. We have developed a real-time SLT application to assist Deaf individuals. Details are provided in the supplementary materials. We conducted a survey on their user experience, and questions concerning random SLR or SLT results. We have collected 40 responses, rating our application user experience as 4.15 / 5 on average, accuracy of SLR results as 3.96 / 5, and SLT results as 3.98 / 5. These ratings indicate a positive response from the Deaf community, providing strong evidence of our research's effectiveness."}, {"title": "Conclusion", "content": "We present the SCOPE dataset, the first dialogue-based Chinese Sign Language dataset featuring both gloss and text annotations. This dataset encompasses 72.4 hours of sign language videos collected from professional Deaf groups, complemented by 59,231 text annotations. Building on this dataset, we introduce the SCOPE framework, a robust pipeline specifically designed to address Sign Language Recognition (SLR) and Sign Language Translation (SLT) tasks with rich contextual information. Our comprehensive"}, {"title": "Appendix", "content": "SCOPE Dataset details\nData Collection Details The SCOPE dataset was meticulously curated to ensure high-quality and diverse sign language data. The data collection process involved several steps and considerations:\n\u2022 Participants: Our participants included both professional Deaf sign language teachers and non-professional Deaf individuals. The professional signers comprised three sign language linguistics experts and several experienced Deaf signers. Non-professional signers were selected to represent a variety of ages, genders, occupations, and educational backgrounds, capturing a wide range of signing habits.\n\u2022 Location: Data collection sessions were conducted in a controlled environment designed to mimic real-life scenarios. This setting was equipped with high-resolution cameras and appropriate lighting to ensure clarity and accuracy in the captured sign language videos.\n\u2022 Collection Process:\n1. Professional signers were provided with reference sentences in natural language and asked to perform the corresponding sign language. These sessions were recorded, and the signers annotated the videos to create initial gloss annotations."}, {"title": "", "content": "2. The annotated videos were reviewed by our sign language linguistics experts to ensure accuracy and consistency. Any discrepancies identified during this phase were corrected.\n3. Non-professional signers were given the gloss annotations and reference sentences to replicate the sign language videos. This step ensured the inclusion of diverse signing styles and habits.\n4. Each reference sentence was performed by four different signers, resulting in multiple video samples per sentence to enhance the richness of the dataset.\n\u2022 Data Cleaning: Our data cleaning process consists of two main steps.\n1. Three sign language linguistics experts reviewed all gloss annotations. Utilizing their expertise, they identified equivalent gloss combinations, including one-to-one, one-to-many, and many-to-one relationships. One-to-one relationships were generally synonymous, and we consolidated these synonyms into a single gloss. One-to-many and many-to-one relationships often pertained to phrases, and we determined the use of phrases based on the frequency of gloss occurrences.\n2. We developed a script to automate the identification and correction of inconsistencies in sign language annotations. This script analyzed discrepancies between the test set and the ground truth, identifying common error types derived from the calculation of Word Error Rate (WER), such as \"C-S-I-C\", \"C-I-S-C\", \"C-D-S-C\", and \"C-S-D-C\u201d. In these acronyms, each letter represents a specific error type: Correct (C), Insertion (I), Substitution (S), and Deletion (D). Linguistics experts then examined and corrected frequently confused patterns. Additionally, the script maintained a record of previously processed annotation pairs to avoid redundancy, thereby improving the efficiency and accuracy of the data cleaning process. Through this iterative process, we successfully reduced the vocabulary size from 7,000 to 5,000, significantly enhancing the quality of the dataset.\n\u2022 Database Management: All collected data, including raw videos, annotations, and metadata, were stored in a structured database. This database facilitated easy access and management of the dataset for further processing and analysis.\nData Statistics The SCOPE dataset covers a wide range of scenarios and dialogue contexts, as detailed in Tab.5. It includes professional settings such as medical treatment, work, and education, as well as daily situations like entertainment, family communication, and shopping. The dataset contains 33,154 clips under 5 seconds and 26,077 clips over 5 seconds, with an average length of 5.05 seconds. The average sentence length is 11.84 characters, highlighting the dataset's richness. Fig.4 presents visual examples from the SCOPE dataset, showcasing its diversity."}, {"title": "Experiment Details", "content": "In this section, we provide an in-depth discussion of the experiment's details, including input data quality, preparation specifics, and additional evaluation comparisons.\nInput Details We employed the state-of-the-art Pose Estimation method, DWPose (Yang et al. 2023), to accurately identify keypoint positions for each action frame. Fig.5 compares the 2D keypoints identified by MediaPipe (Lugaresi et al. 2019) from the POENIX-2014 dataset with those identified by DWPose. The keypoints from DWPose demonstrate significantly higher quality than those from previous methods.\nSetup Details The experimental setup was meticulously designed to ensure the robustness and reproducibility of our results. We begin by introducing the datasets used in the experiment:\n\u2022 PHOENIX14 is a German sign language dataset focused on weather forecasts, containing 5,672 training samples, 540 development samples, and 629 test samples.\n\u2022 PHOENIX14-T extends PHOENIX14 with both gloss and translation annotations, including 7,096 training samples, 519 development samples, and 642 test samples.\n\u2022 CSL-Daily is a Chinese sign language dataset with gloss and translation annotations, comprising 18,401 training samples, 1,077 development samples, and 1,176 test samples.\n\u2022 SCOPE, as previously mentioned, is a Chinese sign language dataset featuring gloss annotations, translation annotations, and dialogue context information, with a total of 59,231 samples across training, development, and test sets.\nExperiments were conducted on a cluster of 8 NVIDIA A800 GPUs. Each GPU was assigned a subset of the dataset to facilitate parallel processing and efficient resource utilization. All sign language videos were preprocessed to extract 2D keypoints using DWPose, which were then normalized using our Iris Normalization technique to account for variations in video resolution and camera distance. Training details are provided below:\n\u2022 SCOPE: As mentioned in the previous section.\n\u2022 SCOPE without context: Most settings are similar to the full pipeline, but we use random noise instead of context"}, {"title": "Limitation and Broader Impact", "content": "Limitation. While our SCOPE framework and dataset represent significant advancements in sign language recognition and translation, several limitations must be considered. First, although the SCOPE dataset covers a wide range of scenarios and includes diverse participants, there are still many professional contexts that need to be addressed, such as emergencies (fire, police), insurance consultancy, pet medical treatment, and transportation navigation for the Deaf. Future dataset collection should explore these scenarios. Second, while SCOPE can incorporate contextual information in sign language processing, excessively long contexts may have minimal impact on current sentences. Investigating an attention decay mechanism should be a future research direction. Finally, regarding speed normalization, although input image sizes are scaled using our iris normalization, variations in sign speed among signers can complicate the recognition network's ability to process sign frames effectively. Future explorations should focus on addressing sign speed variations.\nBroader Impact. The development of the SCOPE framework and dataset offers significant benefits to both the Deaf community and the research field. By enhancing the accuracy and context-awareness of sign language recognition, our work fosters better communication between Deaf and hearing individuals, promoting inclusiveness. Additionally, the SCOPE dataset serves as a valuable resource for developing educational tools that teach sign language and raise"}, {"title": "", "content": "awareness in the broader community. Our real-time sign language translation application is particularly advantageous in healthcare settings, where effective communication is essential for improving care quality for Deaf patients. Furthermore, the insights gained from our research can contribute to advancements in areas such as gesture recognition, virtual reality, and augmented reality. By open-sourcing our dataset and code, we aim to stimulate further research in sign language processing, leading to innovations that benefit the community. In summary, our work has the potential to drive positive change and innovation, ultimately contributing to a more inclusive and accessible society."}]}