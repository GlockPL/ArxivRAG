{"title": "COORDINATE IN AND VALUE OUT: TRAINING FLOW TRANSFORMERS IN AMBIENT SPACE", "authors": ["Yuyang Wang", "Anurag Ranjan", "Josh Susskind", "Miguel Angel Bautista"], "abstract": "Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on unstructured data like 3D point clouds. These models are commonly trained in two stages: first, a data compressor (i.e. a variational auto-encoder) is trained, and in a subsequent training stage a flow matching generative model is trained in the low-dimensional latent space of the data compressor. This two stage paradigm adds complexity to the overall training recipe and sets obstacles for unifying models across data domains, as specific data compressors are used for different data modalities. To this end, we introduce Ambient Space Flow Transformers (ASFT), a domain-agnostic approach to learn flow matching transformers in ambient space, sidestepping the requirement of training compressors and simplifying the training process. We introduce a conditionally independent point-wise training objective that enables ASFT to make predictions continuously in coordinate space. Our empirical results demonstrate that using general purpose transformer blocks, ASFT effectively handles different data modalities such as images and 3D point clouds, achieving strong performance in both domains and outperforming comparable approaches. ASFT is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in generative modeling have enabled learning complex data distributions by combin-ing both powerful architectures and training objectives. In particular, state-of-the-art approaches for image (Esser et al., 2024), video (Dai et al., 2023) or 3D point cloud (Vahdat et al., 2022) generation are based on the concept of iteratively transforming data into Gaussian noise. Diffusion models were originally proposed following this idea and pushing the quality of generated samples in many different domains, including images (Dai et al., 2023; Rombach et al., 2022), 3D point clouds (Luo & Hu, 2021), graphs (Hoogeboom et al., 2022) and video (Ho et al., 2022a). More recently, flow matching (Lipman et al., 2023) and stochastic interpolants (Ma et al., 2024) have been proposed as generalized formulations of the noising process, moving from stochastic gaussian diffusion processes to general paths connecting a base (e.g. Gaussian) and a target (e.g. data) distribution.\nIn practice, these iterative refinement approaches are commonly applied in a low-dimensional latent space obtained from a pre-trained compressor model. Therefore, the training process for these approaches is composed of two independent training stages: in the first stage, a compressor (e.g. VAE (Vahdat et al., 2022), VQVAE (Ramesh et al., 2022), VQGAN (Rombach et al., 2022)) model is trained, using architectures that are specific to the data domain (i.e. ConvNets for image data (Rombach et al., 2022), PointNet for point clouds (Vahdat et al., 2022), etc. ) enforcing a bottleneck on the data dimensionality, with the goal of reducing compute cost of training the subsequent stage. In the second stage, general purpose transformer architectures are used for the generative modeling step (Peebles & Xie, 2023; Ma et al., 2024; Esser et al., 2024), where the distribution of latents is learnt. This type of generative modeling in latent space has become popular in the community due to its computational efficiency benefits obtained from compressed data dimensionality."}, {"title": "2 RELATED WORK", "content": "Diffusion models have been the major catalyzer of progress in generative modeling, these approaches learn to reverse a forward process that gradually adds Gaussian noise to corrupt data samples (Ho et al., 2020). Diffusion models are notable for their simple and robust training objective. Extensive research has explored various formulations of the forward and backward processes (Song et al., 2021a; Rissanen et al., 2022; Bansal et al., 2022), particularly in the image domain. In addition, different denoising networks have been proposed for different data domains like images (Nichol & Dhariwal, 2021), videos (Ho et al., 2022a), and geometric data (Luo & Hu, 2021). More recently, flow matching (Liu et al., 2023; Lipman et al., 2023) and stochastic interpolants (Ma et al., 2024) have emerged as flexible formulations that generalized Gaussian diffusion paths, allowing to define different paths to connect a base and a target distribution. These types of models have shown incredible results in the image domain (Ma et al., 2024; Esser et al., 2024) when coupled with transformer architectures (Vaswani et al., 2017) to model distributions in latent space learnt by data compressors (Peebles & Xie, 2023; Ma et al., 2024; Rombach et al., 2022; Vahdat et al., 2022; Zheng et al., 2023; Gao et al., 2023). Note that these approaches train two separate stages/models: first training the data compressor (e.g. VAE (Vahdat et al., 2022), VQVAE (Ramesh et al., 2022), VQGAN (Rombach et al., 2022)) and then the generative model, requiring careful hyper-parameter tuning.\nIn an attempt to unify generative modeling across various data domains, continuous data repre-sentations\u00b9 have shown potential in different approaches: From Data to Functa (Functa) (Dupont et al., 2022a), Generative Manifold Learning (GEM) (Du et al., 2021a), and Generative Adversarial Stochastic Process (GASP) (Dupont et al., 2022b) have studied the problem of generating continuous representations of data. More recently Infinite Diffusion (Bond-Taylor & Willcocks, 2023) and PolyINR (Singh et al., 2023) have shown great results in the image domain by modeling images as continuous functions. However, both of these approaches make strong assumptions about image data. In particular, (Bond-Taylor & Willcocks, 2023) interpolates sparse pixels to an euclidean grid to then process it with a U-Net. On the other hand, (Singh et al., 2023) uses a patching and 2D convolution in the discriminator. Our approach also relates to DPF Zhuang et al. (2023), a diffusion model that acts on function coordinates and can be applied in different data domains on a grid at low resolutions (i.e. 64\u00d764). Our approach is able to deal with higher resolution functions (e.g. 256x256 vs. 64x64 resolution images) on large scale datasets like ImageNet, while also tackling unstructured data domains that do not live on an Euclidean grid (e.g. like 3D point clouds)."}, {"title": "3 METHOD", "content": "We interpret our empirical data distribution $q$ to be composed of maps $f \\sim q(f)$. These maps take coordinates $x$ as input to values $y$ as output. For images, maps are defined from 2D pixel coordinates $x \\in \\mathbb{R}^2$ to corresponding RGB values $y \\in \\mathbb{R}^3$, thus $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$, where each image is a different map. For 3D point clouds, $f$ can be interpreted as a deformation that maps coordinates from a fixed base configuration in 3D space to a deformation value also in 3D space, $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$, as in the image case, each 3D point cloud corresponds to a different deformation map $f$. For ease of notation, we define coordinates $x$ and values $y$ of any given map $f$ as $x_f$ and $y_f$, respectively. Fig. 1(a) shows an example of such maps in the image domain.\nIn practice, analytical forms for these maps $f$ are unknown. In addition, different from previous approaches (Dupont et al., 2022a; Du et al., 2021a), we do not assume that parametric forms of these maps can be obtained, since that would involve a separate training stage fitting an MLP to each map (Dupont et al., 2022a; Bauer et al., 2023; Du et al., 2021a). As a result, we assume we are only given sets of corresponding coordinate and value pairs resulting from observing these maps at a particular sampling rate (e.g. at a particular resolution in the image case). Therefore, we need a to develop an end-to-end approach that can take these collections of coordinate-value sets as training data."}, {"title": "3.2 FLOW MATCHING AND STOCHASTIC INTERPOLANTS", "content": "We consider generative models that learn to reverse a time-dependent forward process that turns data samples (i.e. maps $f$ in our case) $f \\sim q(f)$ into noise $ \\in \\sim \\mathcal{N}(0, I)$.\n$$f_t = a_t f + \\sigma_t \\epsilon$$ (1)\nBoth flow matching (Lipman et al., 2023) and stochastic interpolant (Ma et al., 2024) formulations build this forward process in Eq. 1 so that it interpolates exactly between data samples $f$ at time $t = 0$ and $ \\epsilon$ at time $t = 1$, with $t \\in [0, 1]$. In particular, $p_1(f) \\sim \\mathcal{N}(0, I)$ and $p_0(f) \\approx q(f)$. In this case, the marginal probability distribution $p_t(f)$ of $f$ is equivalent to the distribution of the probability flow ODE with the following velocity field (Ma et al., 2024):\n$$d_t f_t = u_t(f_t) dt$$ (2)\nwhere the velocity field is given by the following conditional expectation,\n$$u_t(f) = \\mathbb{E}[d_t f_t|f_t = f] = d_t a_t \\mathbb{E}[f_0|f_t = f] + d_t \\sigma_t \\mathbb{E}[\\epsilon|f_t = f].$$ (3)\nUnder this formulation, samples $f_0 \\sim p_0(f)$ are generated by solving the probability flow ODE in Eq. 2 backwards in time (e.g.. flowing from $t = 1$ to $t \\rightarrow 0$), where $p_0(f) \\approx q(f)$. Note that both the flow matching (Lipman et al., 2023) and stochastic interpolant (Ma et al., 2024) formulations decouple the time-dependent process formulation from the specific choice of parameters $a_t$ and $ \\sigma_t$, allowing for more flexibility. Throughout the presentation of our method we will assume a rectified flow (Liu et al., 2023; Lipman et al., 2023) or linear interpolant path (Ma et al., 2024) between noise and data, which define a straight path to connect data and noise: $f_t = (1 - t) f_0 + t \\epsilon$. Note that our framework for learning flow matching models for coordinate-value sets can be used with any path definition. Compared with diffusion models (Ho et al., 2020), linear flow matching objectives result in better training stability and more modeling flexibility (Ma et al., 2024; Esser et al., 2024) which we observed in our early experiments."}, {"title": "3.3 FLOW MATCHING FOR COORDINATE-VALUE SETS", "content": "We now turn to the task of formulating a flow matching training objective for data distributions of maps $f$. We recall that in practice we do not have access to an analytical or parametric form for these maps $f$, and we are only given sets of corresponding coordinate $x_f$ and value $y_f$ pairs resulting from"}, {"title": "3.4 NETWORK ARCHITECTURE", "content": "We base our model on the general PerceiverIO design (Jaegle et al., 2022) which provides a flexible architecture to handle coordinate-value sets of large cardinality (i.e. large number of pixels in an image). Fig. 2 illustrates the architectural pipeline of ASFT. At a high level, our encoder network takes a set of coordinate-value pairs and encodes them to learnable latents through cross-attention. These latents are then updated through several self-attention blocks to provide the final latents $z_{f_t} \\in \\mathbb{R}^{L \\times D}$. To decode the velocity field for a given coordinate-value pair we perform cross attention to $z_{f_t}$, generating the final point-wise prediction for the velocity field $v_\\theta(x_{f_t}, y_{f_t}, t|z_{f_t})$.\nThe encoder of a vanilla PerceiverIO relies solely on cross-attention to the latents $z_{f_t} \\in \\mathbb{R}^{L \\times D}$ to learn spatial connectivity patterns between input and output elements, which we found to introduce a strong bottleneck during training. To ameliorate this, we make two key modifications to boost the performance. Firstly, our encoder utilizes spatial aware latents where each latent is assigned a \"pseudo\" coordinate. Coordinate-value pairs are assigned to different latents based on their distances on coordinate space. During encoding, coordinate-value pairs interact with their assigned latents through cross-attention. In particular, the learnable latent $z_{f_t}$ cross-attends to input coordinate-value pairs of noisy data at a given timestep $t$. Latent vectors are spatial-aware, this means that each of the $L$ latents only attends to a set of neighboring coordinate-value pairs. Latent vectors are then updated using several self-attention blocks. These changes in the encoder allow the model to effectively utilize"}, {"title": "4 EXPERIMENTS", "content": "We evaluate ASFT on two challenging problems: image generation (FFHQ-256 (Karras et al., 2019), LSUN-Church-256 (Yu et al., 2015), ImageNet-128/256 (Russakovsky et al., 2015)) and 3D point cloud generation (ShapeNet (Chang et al., 2015) and Objaverse (Deitke et al., 2023)). Note that we use the same training recipe both tasks, adapted for changes in coordinate-value pair dimensions in different domains. See App. A for more implementation details and training settings.\nASFT enables practitioners to define the number of coordinate-value pairs to be decoded during training. In our experiments, we set the number of decoded coordinate-value pairs to 4096 for images with resolution 128\u00d7128, 8192 for images with resolution 256\u00d7256, and 2048 for point clouds unless mentioned otherwise. On image generation, we train models with small (S), base (B), large (L), and extra large (XL) sizes. For 3D point cloud generation we set the parameter count to match the model size in previous state-of-the-art approaches (i.e. LION (Vahdat et al., 2022)). Detailed configuration for all models can be found in Appendix A. During inference, we adopt black-box numerical ODE solver with maximal NFE as 100 for image generation (Song et al., 2021b) and an SDE sampler with 1000 steps for point cloud generation to match the settings in (Vahdat et al., 2022)."}, {"title": "4.1 IMAGE GENERATION IN FUNCTION SPACE", "content": "Given that ASFT is a generative model for maps we compare it with other generative models of the same type, namely approaches that operate in function spaces. Tab. 1 shows a comparison of different image domain specific as well as function space models (e.g. approaches that model infinite-dimensional signals). ASFT surpasses other generative models in function space on both FFHQ (Karras et al., 2019) and LSUN-Church (Yu et al., 2015) at resolution 256 \u00d7 256. Compared with generative models designed specifically for images, ASFT also achieves comparable or better performance. When scaling up the model size, ASFT-L demonstrates better performance than all the"}, {"title": "4.2 IMAGENET", "content": "We also evaluate the performance of ASFT on large scale and challenging settings, we train ASFT on ImageNet at both 128\u00d7128 and 256\u00d7256 resolutions. On ImageNet-128, shown in Tab. 2, ASFT achieves an FID of 2.73, which is a a competitive performance in comparison to diffusion or flow-based generative baselines including ADM (Dhariwal & Nichol, 2021), CDM (Ho et al., 2021), and RIN (Jabri et al., 2023) which use domain-specific architectures for image generation. Besides, comparing to PolyINR (Singh et al., 2023) which also operates on function space, ASFT achieves competitive FID, while obtaining better IS, precision and recall. The experimental results demonstrate the capabilities of ASFT in generating realistic samples on large scale datasets.\nWe report results of ASFT for ImageNet-256 on Tab. 3. Note that ASFT is slightly outperformed by latent space models like DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024). We highlight that these baselines rely on a pre-trained VAE compressor that was trained on datasets that are much larger than ImageNet, while ASFT was trained only with ImageNet data. In addition, ASFT achieves better performance than many of the baselines trained only with ImageNet data including ADM (Dhariwal & Nichol, 2021), CDM (Ho et al., 2021) and Simple Diffusion (U-Net) (Hoogeboom et al., 2023) which all use CNN-based architectures specific for image generation. Note that this is consistent with the results show in Tab. 1, where ASFT outperforms all function space approaches. When comparing with approaches using transformers architectures we find that ASFT obtains performance comparable to RIN (Jabri et al., 2023) and HDiT (Crowson et al., 2024), with slightly worse FID and slightly better IS. However, ASFT is a domain-agnostic architecture that can be trivially applied to different data domains like 3D point clouds (see Sect. 4.3 and 4.4). For completeness, we also include a comparison with very large U-Net transformer hybrid models, Simple Diffusion (U-ViT 2B) and VDM++ (U-ViT 2B) which both use approx. \u00d72.72 more parameters than ASFT-XL, unsurprisingly, these much bigger capacity models outperform ASFT (see App. A for a more detailed comparison including training settings). We highlight that the simplicity of implementing and training ASFT models in practice, and the trivial extension to different data domains (as shown in Sect. 4.3) are strong arguments favouring our model. Finally, comparing with e.g. PolyINR (Singh et al., 2023) which is also a function space generative model we also find comparable performance, with slight worse FID but better Precision and Recall. It is worth noting that (Singh et al., 2023) applies a pre-trained DeiT model as the discriminator (Singh et al., 2023). Whereas our ASFT makes no such assumption about the function or pre-trained models, enabling to trivially apply ASFT to other domains like 3D point clouds (see Sect. 4.3).\nTo demonstrate the scalability of ASFT we train models of different sizes including small (S), base (B), large (L), and extra-large (XL) on ImageNet-256. We show the performance of different model sizes using FID-50K in Fig. 3(a). We observe a clear improving trend when increasing the number of parameters as well as increasing training steps. This demonstrates that scaling the total training Gflops is important to improved generative results as in other ViT-based generative models (Peebles & Xie, 2023; Ma et al., 2024). Due to the flexibility of cross-attention decoder in ASFT, one can easily conduct random sub-sampling to reduce the number of decoded coordinate-value pairs during training which significantly saves computation. Fig. 3(b) shows how number of decoded coordinate-"}, {"title": "4.3 SHAPENET", "content": "To show the domain-agnostic prowess of ASFT we also tackle 3D point cloud generation on ShapeNet (Chang et al., 2015). Note that our model does not require training separate VAEs for point clouds, tuning their corresponding hyper-parameters or designing domain specific networks. We simply adapt our architecture for the change in dimensionality of coordinate-value pairs (e.g. $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ for images to $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ for 3D point clouds.). Note that for 3D point clouds, the coordinates and values are equivalent. In this setting, we compare baselines including LION (Vahdat et al., 2022) which is a recent state-of-the-art approach that models 3D point clouds using a latent diffusion type of approach. Following Vahdat et al. (2022) we report MMD, COV and 1-NNA as metrics. To have a straightforward comparison with baselines, we train ASFT-B with to approximately match"}, {"title": "4.4 OBJAVERSE", "content": "We also experiment ASFT on large-scale 3D point generation conditioned on images. To this end, we train an image-to-point-cloud generation model on Objaverse (Deitke et al., 2023), which contains"}, {"title": "4.5 RESOLUTION AGNOSTIC GENERATION", "content": "An interesting property of ASFT is that it decodes each coordinate-value pair independently, allowing resolution to change during inference. At inference the user can define as many coordinate-value pairs as desired where the initial value of each pair at $t = 1$ is drawn from a Gaussian distribution. We show qualitative results of resolution agnostic generation for both images and point clouds. Fig. 4(a) show images sampled at resolution 5122, 10242, and 20482 (together with their 2562 resolution counterparts generated from the same seed) from ASFT trained on ImageNet-256. Even though the model has not been trained with any samples at higher resolutions, it can still generate realistic images with high-frequency details. Fig. 4(b) shows point cloud with 32k and 128k points from ASFT trained on Objaverse with only 16k points points per sample (we visualize the generated 16k point could generated from the same seed). Similarly, ASFT generates dense and realistic point cloud in 3D without actually being trained on such high density points. These results show that ASFT is not trivially overfitting to the training set of points but rather learning a continuous density field in 3D space from which an infinite number of points could be sampled. Generally speaking, this also provides the potential to efficiently train flow matching generative models without the need to use large amounts of expensive high resolution data, which can be hard to collect in data domains other than images."}, {"title": "5 CONCLUSION", "content": "We introduced Ambient Space Flow Transformers (ASFT), a flow matching generative model designed to operate directly in ambient space. Our approach dispenses with the practical complexities"}, {"title": "A MODEL CONFIGURATION AND TRAINING SETTINGS", "content": "We provide detailed model configurations and training settings of ASFT for image (Tab. 6) and point cloud (Tab. 7) generation. For image generation, we develop model sizes small (S), base (B), large (L), and extra large (XL) to approximately match the number of parameters in previous works (Peebles & Xie, 2023). Similarly, for point cloud generation, we train a base sized model roughly matching the number of parameters in LION (Vahdat et al., 2022) (i.e. 110M parameters), and a ASFT-L which contains about twice the number of parameters as ASFT-B. For image experiments we implement the \"psuedo\" coordinate of latents as 2D grids and coordinate-value pairs are assigned to different latents based on their distances to the latent coordinates. Whereas in point cloud generation, since calculating the pair-wise distances in 3D space can be time consuming, we assign input elements to latents through a hash code, so that neighboring input elements are likely (but not certainly) to be assigned to the same latent token. We found that the improvements of spatial aware latents in 3D to not be as substantial as in the 2D image setting, so we report results with a vanilla PerceiverIO architecture for simplicity. To embed coordinates, we apply standard Fourier positional embedding (Vaswani et al., 2017) for ambient space coordinate input in both encoder and decoder. The Fourier positional embedding is also applied to the \u201cpsuedo\u201d coordinate of latents. On image generation, we found that applying rotary positional embedding (RoPE) (Su et al., 2024) slightly improves the performance of ASFT. Therefore, RoPE is employed for largest ASFT-XL model. For all the models including image and 3D point cloud experiments, we share the following training parameters except the training_steps across different experiments. On image generation, all models are trained with batch size 256, except for ASFT-XL reported in Tab. 2 and Tab. 3, which are trained for 1.7M steps with batch size 512. On ShapeNet, ASFT models are trained for 800K iterations with a batch size of 16. On Objaverse, we follow the configuration ASFT-XL listed in Tab. 7 and the model is trained with batch size 384 for 500k iterations."}, {"title": "B PERFORMANCE VS TRAINING COMPUTE", "content": "We compare the performance vs total training compute of ASFT and DiT (Peebles & Xie, 2023) in Gflops. ASFT-linear denotes the variant of ASFT where the cross-attention in the spatial aware encoder is replaced with grouping followed by a linear layer. We found this could be an efficient variant of standard ASFT while still achieving competitive performance. Fig. 5 shows the comparison of the training compute in Gflops vs FID-50K between ASFT and latent diffusion model DiT (Peebles & Xie, 2023) including the tranining compute of the first stage VAE. We estimate the training cost of VAE based the model card listed in HuggingFace\u00b2. As shown, the training cost of VAE is not negligible and reasonable models with FID \u2248 6.5 can be trained for the same cost.\nAdmittedly, under equivalent training Gflops, ASFT achieves comparable but not as good performance as DiT in terms of FID score (with a difference smaller than 1.65 FID points). We attribute this gap to the fact that DiT's VAE was trained on a dataset much larger than ImageNet, using a domain-specific architecture (e.g. a convolutional U-Net). We believe that the simplicity of implementing and training ASFT models in practice, and the trivial extension to different data domains (as shown in Sect. 4.3) are strong arguments to counter an FID difference of smaller than 1.65 points."}, {"title": "C ARCHITECTURE ABLATION", "content": "We also provide an architecture ablation in Tab. 9 showcasing different design decisions. We compare two variants of Transformer-based architectures ASFT: a vanilla PerceiverIO that directly operates on ambient space, but without using spatial aware latents and ASFT. As it can be seen, the spatially aware latents introduced in ASFT greatly improve performance across all metrics in the image domain, justifying our design decisions. We note that we did not observe the same large benefits for 3D point clouds, which we hypothesize can be due to their irregular structure."}, {"title": "D RESOLUTION AGNOSTIC GENERATION: QUANTITATIVE ANALYSIS", "content": "Due to the fact that ASFT decodes each coordinate-value pair independently given $z_{f_t}$, during inference time one can decode as many coordinate-value pairs as desired, therefore allowing resolution to change during inference. We now quantitatively evaluate the performance of ASFT in this setting. In Tab. 10 we compare the FID of different recipes. First, ASFT is trained on FFHQ-256 and bilinear or bicubic interpolation is applied to generated samples to get images at 512. On the other hand, ASFT can directly generate images at resolution 512 by simply increasing the number of coordinate-value pairs during inference without further tuning. As shown in Tab. 10, ASFT achieves lower FID when compared with other manually designed interpolation methods, showcasing the benefit of developing generative models on ambient space."}, {"title": "E ADDITIONAL IMAGENET SAMPLES", "content": "We show uncurated samples of different classes from ASFT-XL trained on ImageNet-256 in Fig. 6 and Fig. 7. Guidance scales in CFG are set as 4.0 for loggerhead turtle, macaw, otter, coral reef and 2.0 otherwise."}, {"title": "F ADDITIONAL SHAPENET SAMPLES", "content": "We show uncurated samples from ASFT-L trained jointly on 55 ShapeNet categories in Fig. 8."}, {"title": "G ADDITIONAL OBJAVERSE SAMPLES", "content": "We show conditional images and generated samples from ASFT trained on Objaverse in Fig. 9 and 10."}]}