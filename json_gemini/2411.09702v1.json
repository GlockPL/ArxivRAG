{"title": "On the Surprising Effectiveness of Attention Transfer for Vision Transformers", "authors": ["Alexander C. Li", "Yuandong Tian", "Beidi Chen", "Deepak Pathak", "Xinlei Chen"], "abstract": "Conventional wisdom suggests that pre-training Vision Transformers (ViT) improves downstream performance by learning useful representations. Is this actually true? We investigate this question and find that the features and representations learned during pre-training are not essential. Surprisingly, using only the attention patterns from pre-training (i.e., guiding how information flows between tokens) is sufficient for models to learn high quality features from scratch and achieve comparable downstream performance. We show this by introducing a simple method called attention transfer, where only the attention patterns from a pre-trained teacher ViT are transferred to a student, either by copying or distilling the attention maps. Since attention transfer lets the student learn its own features, ensembling it with a fine-tuned teacher also further improves accuracy on ImageNet. We systematically study various aspects of our findings on the sufficiency of attention maps, including distribution shift settings where they underperform fine-tuning. We hope our exploration provides a better understanding of what pre-training accomplishes and leads to a useful alternative to the standard practice of fine-tuning. Code to reproduce our results is at https://github.com/alexlioralexli/attention-transfer.", "sections": [{"title": "Introduction", "content": "Pre-training has emerged as a dominant paradigm in machine learning and has significantly improved performance on a variety of tasks [27, 11, 2, 22]. In computer vision in particular, self-supervised representation learning methods [21, 6, 4, 22] and weakly supervised methods [40, 45] have enabled learning from large amounts of images. It is widely accepted that these methods work because they teach models useful features that are relevant for downstream tasks. But is this story actually true? Perhaps there is another capability learned during pre-training that is sufficient to explain its benefits.\nIn this paper, we present an alternative explanation: pre-training teaches the model how information should be routed between tokens. We specifically focus on Vision Transformers (ViT) [12], not only because they are the most popular architecture for scaling, but also because Transformers explicitly decouple this information flow. Inter-token communication is solely fulfilled by attention, while the remaining bulk of computation are intra-token operations that are applied to each token independently. In contrast, other architectures such as ConvNets [33, 20] simultaneously expand the receptive fields and extract the features, making it difficult to isolate the effect of information flow. We hypothesize that the features computed by the intra-token operations are not essential to explain the benefits of pre-training, and that the pre-trained attention maps are typically sufficient for downstream tasks.\nWe test our hypothesis by introducing a new set of methods called attention transfer. Concretely, we treat a pre-trained ViT as the teacher and train a student model for downstream tasks while"}, {"title": "Attention Transfer", "content": "To work with a Vision Transformer (ViT) [12], an image is first \u201cpatchified\u201d into N tokens. Their intermediate activations are represented as a sequence X= [X1,X2,\u00a8\u00a8\u00a8,XN]T, xi\u2208RC, where C is the embedding dimension. The self-attention [57] mechanism mainly introduces three learnable parameters Wq, Wk,Wv\u2208RC\u00d7C/H (H is the number of heads). Q=XWq is often referred to as the queries, K=XWk as the keys, and V=XWv as the values. Then the attention function is defined as:\nfattn = softmax (QKT) V,\nwhere the softmax function is computed per query for the attention map. Attention maps determine how the values from other tokens are aggregated, and with multiple heads, each token uses multiple attention distributions within the same Multi-headed Self-Attention (MSA) block.\nFor an L-layer Transformer, MSA blocks are interleaved with MLP blocks, and each Transformer layer contains one of each block type. Most operations are intra-token computations, which are applied independently to each token: value and projection matrices, normalization layers [1], and MLPs. The only inter-token computation is applying the attention map softmax(QKT), which is the only way for information to flow between tokens. Transformers are unique because their inter- and intra-token computations are decoupled; however, the relative importance of each type of operation is not well understood, and Transformers are typically trained by jointly fine-tuning all the weights.\nDeviating from the common practice of joint weight tuning, we propose two attention transfer methods with the goal of exploring decoupled training for ViTs, described next."}, {"title": "Attention Copy", "content": "In this setup, we utilize two separate networks: a pre-trained teacher network that only does a forward pass to compute its attention maps, and a student network that directly copies the attention maps from the teacher but computes all of the other activations. The student's weights are randomly initialized and trained via back-propagation, while the teacher's weights are kept frozen. This setting fully isolates the attention maps from the features that they are applied to, and thus is ideal for measuring the utility of pre-trained attention patterns when the student network learns to perform other tasks (e.g., image classification).\nWe call this method Attention Copy, as we \u201ccopy-and-paste\u201d the attention maps from teacher to student. Figure 2 (left) shows a diagram of this approach. Note that it requires forward passes"}, {"title": "Attention Distillation", "content": "In Attention Distillation, the teacher network is only utilized at the training time. Given each training example, we forward both networks in parallel, with the student also computing its own attention maps. But besides the task-driven loss, we also enforce a distillation loss between student's attention maps and the teacher's counterparts as (soft) targets. Formally, using QsKsT for the student and QtKt for the teacher, the loss is then defined as:\nLdist = H [softmax(QsKsT), softmax(QtKt)],\nwhere H computes the cross entropy. As there can be multiple heads and layers in a Transformer, we simply sum up all the losses from wherever attention distillation is applied. Again, the student is trained via back-propagation. Figure 2 (right) shows the diagram of Attention Distillation.\nCompared to Attention Copy, Attention Distillation is much more practical. After training, the teacher is no longer needed, and the student can be used as a standalone model. Compared to training ViTs from scratch, the only addition is the distillation loss, meaning most of the optimization (e.g., learning rate, momentum) and regularization (e.g., weight decay, dropout rate [50]) hyperparameters can follow the scratch recipe with minimal adjustments. It does introduce a new hyperparameter \u03bb, which weights the distillation loss and balances it with the task loss.\nAttention Distillation can be viewed as a form of generalized knowledge distillation, but it has several key differences from the design proposed by Hinton et al. [26]. Attention Distillation trains the student to match the teacher's intermediate attention maps, not the final teacher output. This gives the flexibility of distilling from models trained on any task, not just models trained on the same final task. This property is well-suited for today's \"pre-train and transfer\" paradigm, where the pre-training task (e.g., reconstruction) and the downstream task (e.g., classification) are usually different. However, Attention Distillation does add the constraint that the architecture needs to compute attention maps. We leave experimenting on this idea for other architectures as future work.\nOverall, while fancier designs can be used for both Attention Copy and Attention Distillation, we choose to keep them simple for cleaner assessments of their effectiveness."}, {"title": "Connection to Transformer Training Dynamics", "content": "Our investigation is also linked to recent attempts to theoretically understand the training dynamics of Transformers. Specifically, the inter-token flow encoded in the pre-trained attention maps can be regarded as a discovered latent hierarchy from the dataset. Self-attention can quickly capture frequently co-occurring token pairs [31, 52]. However, more occasional co-occurrences need to be explained by the top-level hierarchy, rather than directly learned in the lower levels [53]. This is due to many potential spurious correlations [30], especially in the over-parameterized setting. Transferring attention maps from a trained teacher reduces these spurious inter-token correlations, so the student can focus on intra-token learning (i.e., computing useful features)."}, {"title": "Main Results", "content": "As featured in Figure 1, attention transfer is highly effective despite its simplicity. Specifically, we demonstrate this with a ViT-L [12] pre-trained with Masked Autoencoding (MAE) [22] for ImageNet-1K classification [10]. Note that this is the signature result that established MAE's effectiveness for pre-training: compared to a ViT-L trained from scratch (with an accuracy score of 83.0), fine-tuning the MAE pre-trained on the same dataset results in a significant improvement to 85.7.\nFor attention transfer, we use the same pre-trained MAE model as our teacher, and since scratch training can be viewed as no transfer, and fine-tuning weights transfers all the weights, the above two results serve as natural lower and upper bounds for the effectiveness of our attention transfer methods."}, {"title": "Analysis", "content": "Next, we provide extensive analyses to better understand the effectiveness of attention transfer. Broadly speaking, the explorations are driven by the following two questions:\n(i) How important are different activations, layers, and heads? (Section 4.1)\n(ii) Is attention transfer re-learning everything from the teacher? (Section 4.2)"}, {"title": "Variants of Attention Transfer", "content": "We study four variants of attention transfer. We use Attention Copy within this section, since it is a fully-decoupled setting well-suited for scientific analysis.\nTransfer a subset of Q, K and V. A natural alternative to transferring attention maps is to transfer different activations that come with self-attention (Eq. 1), namely queries Q, keys K, or values V. Without loss of generality, if we transfer the teacher's Q, the student will compute its own K and V and use them normally. Note that transferring both Q and K is equivalent to transferring the map softmax(QKT). Table 2 shows that transferring Q works surprisingly well, and is actually better than transferring the attention map.\nWe suggest that copying Q gives the model the flexibility to deviate from the teacher attention maps and use attention patterns that are better suited for the downstream task. This is supported by the fact"}, {"title": "Generalization and Limitations", "content": "In this section, we test how well our findings on attention transfer apply across a variety of pre-training and fine-tuning datasets, pre-training methods, model sizes, and tasks."}, {"title": "Pre-training and fine-tuning datasets", "content": "So far, we have focused on a MAE model pre-trained and evaluated on ImageNet-1K. What happens if we pre-train or evaluate on different datasets? We first test this by pre-training MAE ViT-L models"}, {"title": "Out-of-distribution robustness", "content": "One notable aspect of a standard fine-tuned MAE model is that it shows slight \"effective robustness,\" i.e., it achieves slightly better out-of-distribution (OOD) accuracy than expected based on its in-distribution (ID) accuracy [14]. We test whether Attention Distillation, which achieves the same ID accuracy, has the same benefits OOD. Table 6 shows that Attention Distillation still does quite well, but has lower accuracy than fine-tuned MAE on all 4 distribution shifts we tried. These results indicate that the attention maps do not account for the full robustness benefits, and that the features learned by MAE during pre-training are helpful OOD even if they are not ID."}, {"title": "Pre-training methods", "content": "We have so far focused on MAE, a reconstruction-based pre-training method. We now check whether attention transfer still works if the teacher has been pre-trained with a different algorithm. Specifically, we test MoCo-v3 [7], a self-supervised contrastive learning approach, and FLIP [36], which does image-text contrastive learning. Table 7 shows that Attention Copy still achieves most of the performance benefits for each pre-training method. Impressively, ViT-L is even able to reach 86.6 by just transferring attention maps from FLIP. This confirms that learning the proper attention patterns is indeed a significant bottleneck during learning. Note that the FLIP model we used is pre-trained on LAION-2B [48], yet its effectiveness is less affected by distribution shifts to ImageNet-1K."}, {"title": "Model size", "content": "We test whether attention transfer works across model sizes. For all experiments so far, we have used ViT-L; here, we try Attention Copy from a smaller (ViT-B) and larger (ViT-H) model, both pre-trained with MAE. Table 8 shows that Attention Copy continues to improve with scale, even reaching 86.1%"}, {"title": "Object Detection", "content": "Finally, we examine the performance of attention transfer in the standard ViTDet pipeline [35] for COCO object detection. We compare training from scratch against fine-tuning and attention transfer from a MAE ViT-B pre-trained on COCO, which is done to mitigate the effect of distribution shift. For fair comparisons, we use a 448\u00d7448 input to remove the effect from window attention and positional embedding interpolation, and remove the effect of relative positional embeddings. Table 9 shows that Attention Distillation recovers a majority of the gains from pre-training in this dense prediction setting as well. Based on Table 8, we anticipate that the gap between fine-tuning and attention transfer will decrease with ViT-L, but we are limited by computational resources."}, {"title": "Related Work", "content": "Numerous previous works have studied the attention patterns of pre-trained vision transformers [59, 63, 43]. These works present these differences only as qualitative observations, whereas we are able to isolate the attention patterns and show that they are causally responsible for most of the differences in fine-tuning performance. Other methods, such as Lora [28] or Prompt-to-Prompt [25], do rely on the importance of high quality attention patterns within pre-trained networks, but they also utilize pre-trained features and do not provide our insight that these features are typically unnecessary for the tasks we examine. Trockman and Kolter [55] observe diagonal structure within the product of attention layer weights in a trained supervised network. They show that initializing the weights with this structure moderately improves accuracy for small models early in training. The work most similar to us is Zhang et al. [68] in the language domain, which finds that pre-trained BERT models improve length generalization on a few particular synthetic tasks. They attribute it to the attention patterns of a few, specific heads and show that hardcoding these patterns into the network achieves the same benefit. Our work is complementary and emphasizes the importance of attention maps over features.\nGLoMo [64] also attempts to decouple features from the way they should be combined. They use unsupervised pre-training to train a network to output a graph, which is later used to combine task-specific features. We find that there is no need to develop a specialized architecture to achieve this \u2013 Vision Transformers already do this naturally."}, {"title": "Conclusion", "content": "Even as Transformers have surged in popularity, the way we use them has remained stagnant: pre-train, then fine-tune the weights. In this work, we present attention transfer, a simple alternative to ViT fine-tuning that decouples intra-token operations (how to extract more usable features for each token) from inter-token operations (how those features should be combined). Our key finding is that the attention patterns (inter-token operations) are the key factor behind much of the effectiveness of pre-training - our Attention Distillation method completely matches fine-tuning on ImageNet-1K. We do find some limitations: attention transfer does not work well if the pre-training and transfer datasets are different, and it loses a bit of OOD robustness. Nevertheless, our findings provide insights into the role of attention in pre-trained ViTs, and we hope future work fixes attention transfer's shortcomings and explores the advantages of this new transfer method.\nSome directions for future work are particularly interesting. First, a deeper investigation of the queries Q could help us better understand their importance and potentially yield better transfer strategies. Second, attention transfer eliminates the need for tricks that fine-tuning requires, such as layerwise learning rate decay. Layerwise learning rate decay adds the prior that early layers should change less compared to later layers. However, this prior may be overly restrictive for next-generation models, since it prevents early features from changing, and getting rid of it could open up new opportunities. Finally, since attention maps are L \u00d7 L, where L is the sequence length, attention maps could be transferred more easily across model sizes. In contrast, weight tuning is more difficult to apply when the models have different dimensions. Pre-training a smaller model and transferring its attention patterns to a larger downstream model could be more practical than the current practice of fine-tuning."}, {"title": "Key Numbers", "content": "How much information is transferred during attention transfer? Table 10 shows two ways of doing this accounting. If considering the map as 24 layers \u00d7 16 heads \u00d7 197 query tokens \u00d7 197 key tokens, there are about 15 million activations transferred per example. However, QKT is low rank since Q and K are very \u201ctall,\u201d so the attention map softmax(QKT) can be considered 24 layers \u00d7 16 heads \u00d7 197 tokens \u00d7 64 head dim \u00d7 2 matrices, which is about 9.7 million activations."}, {"title": "Computational Cost of Attention Transfer", "content": "Attention transfer has the same computational and memory cost as any other knowledge distillation method that does a forward pass through a teacher. We compare fine-tuning vs attention distillation on a 16GB NVIDIA GP100 with ViT-L and a batch size of 16:\nTraining these large models on ImageNet-1K is quite computationally expensive. 100 epochs of regular fine-tuning is about 2070 GPU-hours per 100 epochs, and 100 epochs of attention transfer is about 2735 GPU-hours. In total, we estimate about 150k GPU-hours are required to reproduce all experiments."}, {"title": "Additional Analysis", "content": "In Section 4.1, we conducted a thorough analysis of which aspects of attention matter the most. Another way to identify key properties of the teacher's attention maps is to average them across some axis during the transfer. For example, one can average the attention maps over all layers of the teacher network, so that the student uses the same map at every layer. Table 12 shows the results with aggregations over several natural axes. Averaging over examples (i.e., using the same attention map, independent of the input) or averaging over query tokens (i.e., each attention distribution is the same, regardless of the query token given an image) does quite poorly. This indicates, unsurprisingly, that these are key elements of self-attention. This also shows that prior work that focuses on aggregate statistics of the attention maps (e.g., averaged over examples) [59] fail to capture the per-example nature of the attention maps that are actually responsible for full fine-tuning performance. Attention copy performance is more reasonable when averaging over heads or layers. This partially corroborates previous findings that attention maps can largely be shared across all layers [58]. However, while the results can be potentially improved with more recipe search, the performance is far short of the full fine-tuning accuracy (85.7)."}, {"title": "Comparison to Knowledge Distillation", "content": "Our central hypothesis has been that the pre-trained attention maps are sufficient, and the pre-trained features are not necessary. Since our attention transfer methods are special instances of knowledge distillation [26], we additionally compare to a baseline of distilling the residual stream features from a pre-trained MAE ViT-L. In Table 13, we obtain a downstream accuracy of 81.3 on ImageNet-1k. This is significantly lower than the 85.7 that can be achieved through fine-tuning or attention distillation. This makes sense: the features learned during self-supervised pre-training are not directly well-suited for classification, so trying to match them can hurt performance. CKA analysis of the features (Figure 5) supports this hypothesis \u2013 the fine-tuned MAE does well by significantly changing the features in the latter half of the network. Overall, transferring attention appears to do much better than distilling the features."}, {"title": "Attention Map Analysis for Transferring Q", "content": "In Section 4.1, we found that copying the queries Q does surprisingly well, almost matching Attention Distillation or fine-tuning the pre-trained weights. Here, we compare the attention maps learned by the copyQ model to those of other models, in hopes of understanding why copy Q does so well.\nEach of the 24 layers within a ViT-L has 16 attention heads, which each compute an L \u00d7 L attention map for an image with L patches. We would like to determine the similarity between the attention heads in two models using some divergence measure; we use the Jensen-Shannon divergence (JSD) because it is symmetric in its arguments. However, there is one caveat. Because the output of the attention layer is invariant to the ordering of its heads, it is insufficient to compare the ith head of one model against the ith head of another. We need to properly match heads up across models. We explored four ways of doing so:\n1. Direct pair: this is the naive approach of computing the JSD between the ith head of the first model and the ith head of the second model. This can fail since similar heads may not be in the same order across models.\n3. Minimum: instead of creating a one-to-one matching, we allow many-to-one matching between heads. We call this Minimum because each head in the first model is paired with the head from the second model with the smallest JSD. This allows our metric to potentially ignore extraneous heads in the second model, but is still susceptible to extraneous heads in the first model.\n4. Averaged maps: we average the attention maps of all heads in a layer and compare the averaged maps across models. This can still be thrown off by extraneous heads."}, {"title": "Attention Map Visualizations", "content": "Section 4.2 provided several results to show that attention transfer does not simply \u201crelearn\" the teacher. Here, we examine one final piece of evidence. We show the attention maps of different networks in Figure 8. We focus on Attention Distillation, since Attention Copy's maps are identical to those in the teacher. Attention Distillation's maps generally match the teacher (pre-trained MAE), but are not completely identical for layers that are distilled (e.g., layer 13). For layer 24, which is not distilled, Attention Distillation looks very different from pre-trained model, instead resembling the attention map of a model trained from scratch. These visualizations also highlight the fact that these attention maps are a very strong prior on what the model should use. While the randomly initialized model attends completely uniformly over all tokens, the pre-trained teacher attention maps already separate the relevant object from potentially spurious correlations (like the branch or background in the example)."}, {"title": "Attention Distillation Hyperparameter Sensitivity", "content": "We show the sensitivity of Attention Distillation to its hyperparameters.\nWe first consider the distillation loss weight \u03bb which is used to compute the overall loss for the student:\nL = Ltask + \u03bb XL dist\nTable 9 shows that a larger weight, \u03bb = 3, does best. This may be because it encourages the student to learn useful attention maps more quickly, letting it guide feature learning earlier in training. We use this value of \u03bb for our main result, where we match the 85.7 accuracy of fine-tuning. However, all other results in this paper use \u03bb = 1 for simplicity.\nPartial distillation: layers Just as we tried for Attention Copy, we also tried distilling various numbers of layers from the MAE teacher network, starting from the bottom of the network. Table 10 shows that there is a \u201csweet spot\u201d when distilling the first 21 out of 24 layers. Distilling all layers may"}, {"title": "Mix and Match, Student and Teacher", "content": "In the main paper, we focused on transferring attention maps from a pre-trained teacher to a randomly initialized student. However, the fact that Transformers have decoupled inter- and intra-token computation means that we can actually initialize the student with a pre-trained network as well. This entails testing whether the attention patterns from one network can improve the features of an already-pre-trained student model. We try Attention Distillation for various combinations of MAE, MoCo-v3, FLIP, and a randomly initialized network. Table 14 shows that this \u201cmix-and-match\" training does better than training from scratch (83.0) but does not match the performance in Table 7, where the students are randomly initialized. These are preliminary results, as the overall training recipe may need to be changed to accommodate the different learning dynamics of a different student model. Further hyperparameter tuning may significantly improve these results."}, {"title": "Implementation Details", "content": "We present the training recipe for Attention Copy in Table 15 and the recipe for Attention Distillation in Table 16. For our partial layer transfer experiments in Figure 3, we set \u03b22 = 0.95 as it helps avoid training instabilities."}]}