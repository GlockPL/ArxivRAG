{"title": "Convergence to the Truth", "authors": ["Hanti Lin"], "abstract": "The epistemology of scientific inference has a rich history. According to the explanationist tradition, theory choice should be guided by a theory's overall balance of explanatory virtues, such as simplicity, fit with data, and/or unification (Russell 1912). The instrumentalist tradition urges, instead, that scientific inference should be driven by the goal of obtaining useful models, rather than true theories or even approximately true ones (Duhem 1906). A third tradition is Bayesianism, which features a shift of focus from all-or-nothing beliefs to degrees of belief (Bayes 1763). It may be fair to say that these traditions are the big three in contemporary epistemology of scientific inference. There is, in fact, a fourth tradition. I am tempted to call it convergentism, although it does not yet have a widely recognized name, as this tradition is nearly lost in contemporary philosophy despite its prominence in statistics and machine learning. The central idea, traceable to Peirce (1878), is that the concept of convergence to the truth should play a significant role in evaluating inference methods. This idea was further developed by Reichenbach (1938) and Putnam (1965), together with more recent contributors from statistics, machine learning, and formal epistemology. That is the story I will unfold below. Toward the end, the convergentist tradition will be briefly compared with the big three-you can expect to see not just competition, but also co-operation.", "sections": [{"title": "1 Introduction", "content": "The epistemology of scientific inference has a rich history. According to the explanationist tradition, theory choice should be guided by a theory's overall balance of explanatory virtues, such as simplicity, fit with data, and/or unification (Russell 1912). The instrumentalist tradition urges, instead, that scientific inference should be driven by the goal of obtaining useful models, rather than true theories or even approximately true ones (Duhem 1906). A third tradition is Bayesianism, which features a shift of focus from all-or-nothing beliefs to degrees of belief (Bayes 1763). It may be fair to say that these traditions are the big three in contemporary epistemology of scientific inference.\nThere is, in fact, a fourth tradition. I am tempted to call it convergentism, although it does not yet have a widely recognized name, as this tradition"}, {"title": "2 Peirce on Enumerative Induction", "content": "Peirce imagined a Greek tackling a certain empirical problem\u2014testing the hypothesis that the tide would never cease to rise every half-day:\n[The Greek] had seen the tide rise just often enough to suggest to\nhim that it would rise every half-day forever, and had proposed\nthen to make observations to test this hypothesis, had done so,\nand finding the predictions successful, had provisionally accepted\nthe theory that the tide would never cease to rise every half-day,\n(CP 7.215)\nBut what justifies the Greek's acceptance of the inductive hypothesis? Peirce's answer is that the inference method in use, enumerative induction, meets a nice standard:\nThe only justification for this would be that it is the result of a\nmethod that, persisted in, must eventually correct any error that\nit leads us into. (CP 7.215)\nThis evaluative standard requires a guarantee of eventual correction of errors. Some important Peircean elements may not be immediately apparent from those quotes. Let me make them explicit.\nA key Peircean element is, in a sense, internalist. That is, when inference methods are evaluated, the kind of evaluation in question can, in principle,"}, {"title": "3 The Long Run and the Short Run", "content": "For concreteness, I will walk you through a case study on a more precisely defined empirical problem, specified by three components:\n(i) The competing hypotheses are \u2018Yes, all ravens are black' and 'No, not all are'.\n(ii) Pieces of evidence are obtained by collecting ravens and observing their colors one by one.\n(iii) The background assumption is that either all ravens are black or a counterexample would be observed sooner or later if the inquiry were to unfold indefinitely.\nCall this the raven problem. The point I want to make can be equally illustrated with a different empirical problem that alters any one of the three elements (i)-(iii), such as weakening the background assumption (Lin 2022); but then the mathematics involved would be much more complex. So, for simplicity, let me continue with the raven problem, which can be represented by the tree in figure 1. The inquiry starts at the bottom, the root of the tree. Moving upward to the right signifies observing a nonblack raven (i.e., a counterexample); moving upward to the left, a black raven (or anything other than a counterexample). So, each node represents a possible body of evidence. Each branch represents a possible world, with the tip marked by the hypothesis true in that world. To clarify: although every branch is depicted as an infinite sequence, it does not represent a world in which the agent is immortal and will observe an infinite number of ravens. For example, the branch that always grows to the left only represents a world in which all ravens are black, and thus every raven observed would be black if the inquiry were to extend indefinitely. Some possible worlds"}, {"title": "4 A Framework for Convergentism", "content": "The above case study on the raven problem actually illustrates a framework, first adumbrated in Putnam (1965) and later developed further by Kelly (1996) and Schulte (1999) through additional case studies. A clear and general statement of the core thesis was given by Lin (2022):\nTHE CORE THESIS. In any empirical problem, a necessary condition for an inference method to qualify as one of the best is that it achieves the highest achievable mode of convergence to the truth-pending a specification of the right hierarchy of modes of convergence as evaluative standards or epistemic ideals.\nThis thesis sets up what may be called the achievabilist framework for convergentism, for lack of a standard name. This framework encourages the exploration of modes of convergence, such as the mode of stable convergence, which is used to address Carnap's worry.\nThis framework also features a kind of context-sensitivity: the appropriate standard for assessing inference methods should be the highest achievable, which is sensitive to a contextual factor: the empirical problem that one tackles in one's context of inquiry. In fact, when one switches to the context of a statistical problem, the epistemic standards presented above all become unachievable, which requires convergentists to explore lower standards\u2014weaker modes of convergence. To illustrate how that may be done, let's think about statistics, which brings us back to Peirce."}, {"title": "5 Peirce on Statistics", "content": "Peirce once studied a classic problem in statistics. An urn contains an unknown number of black and white balls. We ask: what is the true proportion of white balls in the urn? The three components of this problem are as follows:\n(i) Competing hypotheses are rational numbers in the unit interval (i.e., the possible proportions).\n(ii) Evidence is to be collected by randomly drawing balls with replacement.\n(iii) The background assumption is that different draws are probabilistically independent.\nCall this the white ball problem. To evaluate inference methods for this problem, Peirce proposed to employ the following mode of convergence, where, by probability, he meant physical chance:\nDEFINITION. An inference method M for an empirical problem is said to achieve statistical consistency iff M has the following guarantee: (i) in any possible world compatible with the background assumption, there exists n such that M would highly probably produce a guess that gets close to the truth if the sample size were n or larger, and (ii) M has the above property for any threshold of high probability less than 1 and for any nonzero threshold of closeness.\nPeirce studied this mode of convergence as an evaluative standard in an 1878 paper titled \u201cThe Probability of Induction\" (CP 2.669-93). It is unclear whether Peirce influenced any statisticians of his time, but this stochastic mode of convergence was popularized by the statistician Fisher (1925: section 1.3). It is now generally regarded in classical statistics as a minimum qualification for any permissible statistical methods, with different versions for various statistical problems, including estimation, hypothesis testing, and regression (i.e. curve fitting).\nPeirce never explained why he employed different evaluative standards in two different empirical problems, the white ball problem and the Greek's tide problem (which is equivalent to the raven problem). From an achievabilist's hindsight, he had to employ different standards. The Greek's tide problem is easy enough to allow for a guarantee of getting exactly the true answer (at least when the amount of evidence is arbitrarily large). But this standard is too high to be achievable in the white ball problem. So, let's try lowering the bar for that problem: \"getting exactly the truth\" can be downgraded to\""}, {"title": "6 Closing: Comparison with the Big Three", "content": "How does the convergentist tradition fare against the big three mentioned in the introduction?\nFirst of all, the Bayesian tradition need not be a rival to convergentism. A union has been proposed in statistics as a partial solution to Bayesians' perennial problem of the priors\u2014the problem of identifying the correct norms to constrain the pool of the permissible distributions of prior credences (see [add cross references]). A convergentist constraint on priors was proposed by the statistician Freedman (1963):\nDEFINITION. A prior is said to achieve Bayesian consistency in an empirical problem iff it is guaranteed (under just the background assumption of that problem) that this prior, when guided by the diachronic rule of conditionalization, would have a high chance of its posterior credences converging to the truth among the considered hypotheses if the amount of evidence were to accumulate indefinitely for any threshold of high chance less than 1.\nFreedman did not recognize that this is another implementation of the di-achronic trick in reply to Carnap: a combination of long-run convergence with a diachronic constraint, which in this case is conditionalization. The resulting constraint on the priors\u2014and hence on the short run-turns out to be surprisingly strong in some interesting empirical problems, stronger than what traditional Bayesians have to offer. This constraint implies a particular ver-"}]}