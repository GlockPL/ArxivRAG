{"title": "Exemplar-condensed Federated Class-incremental Learning", "authors": ["Rui Sun", "Yumin Zhang", "Varun Ojha", "Tejal Shah", "Haoran Duan", "Bo Wei", "Rajiv Ranjan"], "abstract": "We propose Exemplar-Condensed federated class-incremental learning (ECoral) to distil the training characteristics of real images from streaming data into informative rehearsal exemplars. The proposed method eliminates the limitations of exemplar selection in replay-based approaches for mitigating catastrophic forgetting in federated continual learning (FCL). The limitations particularly related to the heterogeneity of information density of each summarized data. Our approach maintains the consistency of training gradients and the relationship to past tasks for the summarized exemplars to represent the streaming data compared to the original images effectively. Additionally, our approach reduces the information-level heterogeneity of the summarized data by inter-client sharing of the disentanglement generative model. Extensive experiments show that our ECoral outperforms several state-of-the-art methods and can be seamlessly integrated with many existing approaches to enhance performance.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL) (McMahan et al. 2017) enables the collaborative training of a global model across thousands of clients while keeping data decentralized. It effectively addresses the challenges presented by data silos, facilitating a cooperative learning environment without compromising individual privacy, and has been applied in various areas such as smart healthcare (Nguyen et al. 2022) and Internet-of-Things applications (Nguyen et al. 2021; Jiang et al. 2024). However, traditional FL methods assume the application scenario is static, which conflicts with the realistic environment where the data of novel classes can emerge at any time (Yoon et al. 2021; Ma et al. 2022). Simply fine-tuning the pre-trained model to the novel data results in catastrophic forgetting (Li and Hoiem 2017), where the model's performance significantly deteriorates on previously learned tasks. Moreover, the constraints of limited computational resources and data privacy concerns, which allow only a limited selection of previously learned data to be stored, make retraining a model from scratch impractical.\nTo address these challenges, recent work (Dong et al. 2022, 2024; Zhang et al. 2023) has enabled the FL framework to incrementally learn novel classes, known as Federated Class-Incremental Learning (FCIL). Among various strategies proposed for FCIL, rehearsal-based methods (Dong et al. 2022; Qi, Zhao, and Li 2023) that store and replay exemplars from prior tasks stand out as effective approaches in mitigating the forgetting problem. However, these methods are limited by the constrained storage resources of edge devices and privacy concerns, allowing only a limited selection of data. This raises a crucial question: How can a restricted dataset be utilized to encapsulate more information and effectively counteract the forgetting problem without compromising data privacy?\nA straightforward approach involves directly compressing data into a compact format. While this method efficiently condenses information, it is not necessarily ideal for training machine learning models. Even over-compression can lead to excessively complex information, hindering the model's ability to distinguish decision boundaries effectively. Data condensation (DC) (Wang et al. 2018; Zhao, Mopuri, and Bilen 2020) has emerged as a prominent solution that employs sophisticated approaches, such as distribution/feature matching (Wang et al. 2022; Zhao and Bilen 2023) or gradient/trajectory matching surrogate objectives (Zhao, Mopuri, and Bilen 2020; Cazenavette et al. 2022), to synthesize compact datasets. These condensed datasets are meticulously crafted to encapsulate the quintessential characteristics of the original, larger datasets. These condensed datasets are carefully crafted to capture the core characteristics of larger datasets and enable models trained on them to achieve performance comparable to those trained on the full datasets.\nMost recently, several works (Goetz and Tewari 2020; Hu et al. 2022; Liu, Yu, and Zhou 2022; Xiong et al. 2023) have introduced DC into the FL framework, replacing the traditional exchange of model parameters with only a small portion of synthetic data. In contrast, in this work, we focus on leveraging DC to improve exemplar replay efficiency by maximizing the information stored in condensed datasets, thus better mitigating catastrophic forgetting. However, simply implementing such methods (Masarczyk and Tautkute 2020; Rosasco et al. 2021; Yang et al. 2023) into FL can suffer from a significant challenge we refer to as meta-information heterogeneity, where summarizing data from non-IID distributions across clients can disrupt optimization and degrade performance. Without proper handling, such heterogeneity can deviate from the optimization direction, limiting the ability to reduce catastrophic forgetting effectively.\nTo address these challenges, we propose the Exemplar-Condensed federated class-incremental learning (ECoral) framework. ECoral enables the FL model to retain its performance on previously learned knowledge while incrementally learning new classes. It leverages a dual-distillation approach, where exemplars are distilled from the training dataset to capture more informative features, such as detailed contour information, enhanced texture, and richer color attributes, rather than relying on conventional sampling methods, as illustrated in Figure 1. Additionally, the model's prior knowledge is distilled from the model trained on previous tasks, ensuring that new learning does not compromise previously acquired knowledge. The main contributions of this work can be summarized as follows:\n\u2022 A clear definition of the meta-information heterogeneity problem in federated continual learning (FCL) is proposed, identifying it as a significant challenge that affects model performance across different data sources.\n\u2022 A novel method called ECoral is proposed that features a dual-distillation structure specifically designed to address this issue by mitigating catastrophic forgetting in FCL.\n\u2022 The proposed method in ECoral shifts from storing raw data to using condensed exemplars in memory, significantly enhancing privacy protection by abstracting stored information into less recognizable forms and addressing key privacy concerns in federated learning.\n\u2022 Extensive experiments validate the effectiveness of our proposed method. For instance, when the CIFAR-100 dataset is partitioned into ten tasks, our approach achieves an average accuracy of 49.17%, outperforming the best baseline by 5.32%. In a more challenging long-term continual learning scenario, where CIFAR-100 is divided into 50 tasks, our method reaches an average accuracy of 32.42%, surpassing the best baseline by 10.29%."}, {"title": "Related Work", "content": "Dataset Condensation (DC) aims to reduce the size of a dataset while retaining enough representative information to allow machine learning models to perform comparably to training on the full dataset. Early works, such as those by Wang et al. (2018) and Zhao, Mopuri, and Bilen (2020), framed this as a bi-level optimization problem. The objective is to create a smaller, condensed dataset that preserves the original data's characteristics, allowing a model trained on this smaller dataset to achieve high performance. These techniques often focus on matching distributions, features, or gradients between the original and condensed data. This concept has been successfully integrated into the field of continual learning, where DC methods help mitigate catastrophic forgetting by compressing the data of previous tasks into small but representative memory sets (Masarczyk and Tautkute 2020; Gu et al. 2024).\nIn the context of FL, several studies have explored the application of DC to reduce communication overhead by transmitting condensed datasets rather than full models or gradient updates between clients and the server (Goetz and Tewari 2020; Hu et al. 2022). For example, Liu, Yu, and Zhou (2022) employed DC to address the challenges posed by heterogeneous data in FL environments, reducing both communication costs and data bias. These efforts suggest that DC is well-suited for handling the resource constraints and privacy considerations inherent in FL, as synthetic data can be shared while preserving local client privacy. Our approach builds on these foundations by introducing DC into federated class-incremental learning (FCIL) to enhance the informativeness of memory exemplars and mitigate catastrophic forgetting in non-IID data scenarios.\nFederated Continual Learning (FCL) extends the traditional FL framework to dynamic environments, where new tasks or classes emerge incrementally over time. Unlike conventional FL, which assumes static data distributions, FCL must deal with continually evolving data and the challenges of catastrophic forgetting, non-IID data distribution, and communication constraints between clients and the server (Yoon et al. 2021).\nTo address catastrophic forgetting, one of the most common approaches in FCL is rehearsal-based methods. These"}, {"title": "Preliminaries", "content": "Federated Class-incremental Learning. Federated Class-incremental Learning (FCIL) aims to collaboratively train a global model using streaming data that sequentially introduces new classes. In this context, a model training process consists of a series of sequential tasks $T = {T_t}_{t=1}^\\tau$, where $T$ denotes the total number of tasks. The system involves C local clients and a central server Sg. Each task comprises R global communication rounds (where r = 1, ..., R), and in each round r, a subset of the local participants is randomly selected for gradient aggregation. When the l-th client $C_l$ is selected for a given global round in the t-th incremental task, it receives the latest global model $M^{(r,t)}$.\nDrawing inspiration from online learning, each client maintains a fixed-size local memory $M_l = {(x_{l,m}, y_{l,m})}_{m=1}^M$ of size M, storing examples from prior tasks for knowledge replay. In this work, we divide this memory into three parts: $M_{orig}$, which holds original data sampled from the current task's training set; $M_{cond}$, which stores condensed exemplars from prior tasks; and $M_{sum}$, which saves summarizing data from the current task. At each iteration of the current task, a batch of samples $B_m = {(x_{i,m}, y_{i,m})}_{i=1}^{B_m}$ is randomly drawn from the memory and jointly trained alongside the current task data $B_n = {(x_i^t, y_i^t)}_{i=1}^{B_n}$. Here, $B_m \\leq M$ and $B_n$ represent the mini-batch sizes of the replayed data and the current task data, respectively. The joint training objective is expressed as:\n$\\theta^{r,t} = arg \\min_{\\Theta^{r,t}} L_c(\\Theta^{r,t};B_n) + \\lambda L_m(\\Theta^{r,t}; B_m)$,                                               (1)\nwhere $L_c$ and $L_m$ are the loss functions for the current task data and memory data, respectively. $\\lambda$ is a hyper-parameter for regulation.\nThe client trains the global model $\\theta^{r,t}$ on its own t-th incremental task data $D_l \\subset D$, where $D = {(x_i, y_i)}_{i=1}^N \\subset T_t$ represents the training data for new categories specific to the l-th client. The category distribution for the l-th client is denoted by $P_l$. The distributions ${P_l}_{l=1}^C$ are non-independent and identically distributed (non-IID). At the t-th incremental task, the label space $V_l^t \\subset Y^t$ for the l-th local client is a subset of $Y^t = \\bigcup_{i=1}^{\\tau} V_i$, which includes $K_t$ new categories ($K_t < K$), distinct from the previous categories $K_l = \\sum_{t'=1}^{t-1} K_{t'} \\subseteq \\bigcup_{i=1}^\\tau V_i$. After receiving $\\theta^{r,t}$ and performing local training on the t-th incremental task, the l-th client obtains an updated model $\\theta_l^{r,t}$. These locally updated models from selected clients are then uploaded to the central server $S_g$, where they are aggregated to form the new global model $\\theta^{r+1,t}$ for the next round. The central server $S_g$ subsequently distributes the updated parameters $\\theta^{r+1,t}$ to the local clients for the following global round.\nClient increment strategy. To better simulate a real-world federated continual learning application, we adopt the client increment strategy introduced in GLFC (Dong et al. 2022). This strategy divides local participants into three dynamic groups for each incremental task: Old ($G_o$), In-between ($G_b$), and New ($G_n$). The Old group ($G_o$), consisting of $G_o$ participants, only has access to data from classes introduced in previous tasks and does not receive any data for the new task. The In-between group ($G_b$), with $G_b$ members, works with both the new classes from the current task and the classes from the previous task. Finally, the New group ($G_n$), comprising $G_n$ newly added participants, focuses exclusively on data containing new classes from the current task.\nThe group compositions are dynamically updated with the progression of tasks. Specifically, the membership of the groups $G_o$, $G_b$, $G_n$ is redefined randomly at each global round, and new participants are irregularly added to $G_n$ as incremental tasks arrive. This incremental process gradually increases the total number of participants, $G = G_o + G_b + G_n$, as more tasks are introduced, closely mimicking the nature of streaming data in real-world FL applications."}, {"title": "Problem Definition", "content": "Forgetting in FCIL The primary objective of global model optimization at the t-th incremental task is to minimize the classification error across the current category set $K_t$. However, when a new task arises, clients are often constrained by privacy restrictions and limited resources, allowing only restricted access to data from previous tasks. The category imbalance between old and new categories ($T_{t}$ and $M_{t}$) at the local level exacerbates this issue, leading to significant performance degradation during local training. This limitation frequently results in a notable decline in performance on earlier tasks, a phenomenon known as catastrophic forgetting. To mitigate catastrophic forgetting in the global model, our goal is to minimize the classification error on the current category set $K_t$ while simultaneously preserving the knowledge of previously learned categories. The objective function is formally defined as:\n$\\min_{\\Theta^\\tau} \\sum_{k\\in K_t}\\sum_{i=1}^{N_k} L(P(x_i, \\Theta^{r,t}), y_i)$  (2)\nwhere $L$ is a loss function that measures the classification error, and $N_k$ is the number of samples in class k.\nMeta-information Heterogeneity The condensation of data from non-IID sources inherently retains the non-IID characteristics at an information level, leading to what we define as the meta-information heterogeneity problem. Given that each client's original dataset $D_l$ on task t-th is drawn from a unique distribution $P_l(X, Y)$, the resulting condensed exemplar dataset $M_{cond}$, optimized to represent $D_l$, will also reflect this distinct distribution. Mathematically, this is expressed as $P_{l}^{cond}(X, Y) \\neq P_{l'}^{cond}(X, Y)$ for some clients $l \\neq l'$. The divergence in information content between these condensed datasets can be quantified using measures such as Kullback-Leibler (KL) divergence, where $KL(I(T_{l}^{cond}) || I(T_{l'}^{cond})) > 0$ indicates non-identical information content across clients, thus confirming meta-information heterogeneity. Non-IID data has been shown to exacerbate catastrophic forgetting, as explored in (Zhang et al. 2023), further complicating federated continual learning. Similarly, when these heterogeneous condensed datasets are used to train a global model $\\theta^{r,t}$, the model's updates from different clients may conflict due to the diverse information content, leading to suboptimal performance. This degradation is reflected in the global loss function $L(\\theta)$, which generally increases compared to an IID scenario, expressed as $\\Delta L = L_{non-iid}(\\theta^{r,t}) - L_{iid}(\\theta^{r,t}) > 0$. Therefore, condensed datasets from non-IID sources introduce a meta-information heterogeneity problem that adversely affects the global model's performance, mirroring the challenges posed by non-IID data in traditional federated learning."}, {"title": "Exemplar-condensed FCIL with Dual-distillation Structure", "content": "Online Exemplars Condensation\nIn edge devices within FCIL, where memory space is highly restricted, most existing approaches focus on efficient exemplar sampling strategies. Compared to these, our approach enhances the meta-knowledge capacity of each individual image, thereby increasing its information level, and also balances these improvements with memory efficiency. The balance and trade-offs are further explained in the following sections, highlighting comparisons and improvements over existing methods.\nAdjustable Memory. Efficiently managing a fixed memory space for rehearsal typically requires sophisticated selection algorithms that continually update stored examples, as proposed in methods like (Rebuffi et al. 2017; Aljundi et al. 2019). However, these approaches are not well-suited for our objective, which involves distilling meta-knowledge into exemplars from the entire local training dataset.\nIn contrast to conventional online learning methods like SSD (Gu et al. 2024), which employ a fixed exemplar position strategy for summarizing information, our approach addresses several key limitations. SSD assumes balanced class data in each batch and prior knowledge of the total number of classes, both of which are often unrealistic in real-world federated learning (FL) scenarios. Moreover, SSD allocates only a small fraction of memory to old exemplars. For instance, saving just one exemplar per class when training on a task with 100 total classes within a 100-exemplar space. This inefficient use of memory dedicates the majority of space to current data, hindering effective rehearsal. In an FL environment, this issue is further exacerbated by the non-IID nature of the data and class imbalance, as each client typically holds only a subset of the total class data. Additionally, in our scenario, since many clients do not participate in all tasks, SSD's strategy often results in memory slots being predominantly occupied by current task data by the end of the whole training process.\nTo overcome these challenges, we propose a dynamic memory allocation strategy that adjusts the exemplar space by calculating the required number of samples for each class at the beginning of each task. This approach reduces the number of prior exemplars to free up space, ensuring balanced storage of prior task exemplars while incorporating current task data samples. For example, if the first task includes 10 classes and the total memory space is 100 exemplars, 10 samples are initially stored for each class. When the next task introduces 10 new classes, we reduce the number of exemplars for each previous class to 5 and allocate the remaining 50 slots to the new classes. This ensures a balanced distribution of memory across tasks and efficient rehearsal.\nMeta-knowledge Condensation via Gradient Matching. In Federated Class Incremental Learning (FCIL), where each client is limited by constrained exemplar memory, our approach extends beyond simply managing exemplar selection. The aim is to enhance the information capacity of each image, thereby maximizing meta-knowledge to improve the effectiveness of rehearsal. The primary objective of Meta-Knowledge Condensation is to minimize the divergence between the memory set and the client's local task $T_i$ training data distribution, resulting in an optimized memory set, $M_r$. We hypothesize that the global model trained on exemplars with condensed meta-knowledge can perform similarly as it trained on the whole local training dataset in t-th task. Drawing inspiration from dataset condensation methods (Zhao, Mopuri, and Bilen 2020), the process is implemented by sequentially distilling knowledge from mini-batches of real data into summarized exemplars corresponding to each class.\nConsidering the t-th task, let the condensed exemplars for a class k be denoted by $M_k$, with a predefined size of m. The data points of the same class within the current mini-batch are represented by $B_k$. The primary objective of the memory condensation process is to reduce the divergence between $M_k$ and $B_k$. In the DC works by (Zhao, Mopuri, and Bilen 2020), a novel and efficient metric is introduced, which quantifies the distance between the gradients of the training process on the same neural network when comparing condensed samples with original data points. By aligning the gradient-based model update metrics across the stream of data, the condensed samples are progressively refined to approximate the training performance of the entire dataset. The objective function for gradient matching is defined as:\n$L_{cond} = f_{dist} (\\nabla_{\\Theta^{r,t}} L_{ce}(\\Theta^{r,t}; M_k), \\nabla_{\\Theta^{r,t}} L_{ce}(\\Theta^{r,t}; B_k))$                                                                 (3)\n, where $f_{dist}$ is a distance function.\nCurrent Knowledge Condensation via Feature Matching.\nIn conventional dataset condensation (Zhao, Mopuri, and Bilen 2020), gradient matching is often alternated with model updates. The primary goal is to simulate a comprehensive training process that allows for the matching of gradients across various training stages. During each training iteration, when a new batch of stream data $B_n$ is introduced, the parameters $\\omega$ of the condensation model are first randomly initialized and updated as follows:\n$\\omega \\gets \\omega - \\eta \\nabla_{\\omega} L_{ce}(\\omega; B_n),$                                                        (4)\nwhere $\\eta$ is the learning rate for the condensation model, at this stage, only real images participate in the model summarization update, which prevents knowledge leakage from the summarized samples.\nHowever, in the context of continual learning, the number of classes is not fixed throughout the training process. To address this challenge, we draw inspiration from the SSD approach (Gu et al. 2024), which suggests re-initializing the model when new classes are introduced. To prevent the loss of valuable gradient information when the model's decision boundaries are constructed only for current classes, we employ dataset condensation across multiple dataset batches. This approach updates the model using both current stream data and stored real images in memory $M_{orig}$:\n$\\omega \\gets \\omega - \\eta \\nabla_{\\omega} (L_{ce}(\\omega; B_n) + L_{ce}(\\omega; M_{orig}))$                                (5)\nBuilding on the SSD approach, a relationship-matching strategy is implemented to further refine the condensation process. By using the extracted features of previously summarized samples as anchors, consistency is explicitly enforced between the mean features of condensed samples and real images, ensuring they maintain a consistent relationship with these anchors. The objective for relationship matching is defined as:\n$L_{rel} = f_{dist} (\\rho(M_k, M_{cond} \\backslash M_k, \\omega), \\rho(B_k, M_{cond} \\backslash M_k, \\omega))$                \n$\\rho(X_1, X_2, \\omega) = f_{dist} (\\frac{1}{|X_1|} \\sum_{x\\in X_1} \\phi(\\omega; x), \\phi(\\omega; X_2) )$                        (6)\nwhere $\\rho$ represents the relationship calculation, $M_{cond} \\backslash M_k$ refers to the condensed samples excluding $M_k$, $\\phi$ de-"}, {"title": "Meta-knowledge Compensate Matching", "content": "notes the feature extraction function. This relationship consistency helps establish a more balanced distribution of condensed samples within the memory.\nAs defined in Section, the condensed exemplars from non-iid data can still leave the meta-information heterogeneous, which affects replay efficiency. Thus, this problem must be addressed from both data quantity shift and class shift problems in a privacy-preserving way.\nClient-wise Feature Disentanglement. To address the feature and class skew problem, our goal is to enable each client not only to extract and generate features from the local dataset but also to generate features that are not visible in the local data but present in other clients' datasets, such as color, structure, and texture characteristics. By generating these features from random noise for classes unseen locally but present in other clients, the label skew problem can be reduced. The disentanglement approach (Burgess et al. 2018; Higgins et al. 2017) is an efficient method for low-level feature extraction. As previously discussed, the heterogeneity of meta-knowledge arises from two main issues: feature skew and class skew. To tackle both challenges, we proposed the Client-wise Shared Conditional Variational Auto-Encoder (Shared-VAE) model. In this model, both the encoder $E_{p(x)} = q(z|x)$ and the decoder $D_{e(z)} = p_{\\theta}(x|z)$ are updated through FL in each round. This allows the encoder to improve its ability to extract and refine hidden latent based on global knowledge while the decoder generates information that extends beyond the local data distribution by leveraging local latent information. At the beginning of the local model update, both the local encoder $E_l$ and decoder $D_l$ of the l-th client are updated with the latest global parameters $E$ and $D$, respectively. The local training dataset is then directly used to generate a set of disentangled features if the class data is available locally, addressing the feature skew problem. Otherwise, for classes unseen locally, features are generated from random noise to address the label skew problem. The class ID is provided as a condition during feature generation, ensuring that the model generates class-specific features. The resulting feature set is represented as $H$. After each round of local model updates, the Shared-VAE model is further refined using only the local training data.\nUnbiased Representative Feature Prototypes However, a globally updated Shared-VAE tends to generate features that predominantly represent the majority distribution across all clients. For example, while most cats have fur, a smaller subset, like the Sphynx cat, do not. As a result, when these representations are used to guide data condensation, the condensed data may become biased towards this majority distribution, leading to skewed optimization. To address this limitation, we propose the use of unbiased representative feature prototypes. Specifically, each class k can be represented by multiple characteristic features, denoted as $H_k = {h_i | y_i = k}$, where $y_i$ is the class label for feature $h_i$. These fea-"}, {"title": "Meta-knowledge Compensate Matching", "content": "tures are grouped using an unsupervised clustering method, FINCH (Sarfraz, Sharma, and Stiefelhagen 2019), which is parameter-free and suitable for scenarios where the number of clients is uncertain.\nFirst, the data are divided into groups by category, with one group of data for each class, denoted as $G_k$ for class k. Then, FINCH is applied to cluster each group $G_k$ to extract characteristic features for each class. After applying FINCH clustering, each group $G_k$ is divided into $V_k$ clusters, represented as $Q_k = {Q_{k,j}}_{j=1}^{V_k}$, where $Q_{k,j}$ represents the j-th cluster for class k. Next, the average of all feature vectors in each cluster is directly computed to obtain the representative feature prototype for that cluster:\n$u_{k,j} = \\frac{1}{|Q_{k,j}|} \\sum_{h_l \\in Q_{k,j}} h_l$                                                           (7)\nHere, $u_{k,j}$ is the prototype for cluster j in class k, calculated by averaging all the features $h_l$ within that cluster. Each class k is represented by the set of prototypes from all its clusters:\n$U_k = {u_{k,j}}_{j=1}^{V_k}$                                                            (8)\nFinally, the overall set of representative prototypes for all classes is given by:\n$U = {U_k}_{k=1}^K$                                                             (9)\nThis ensures that each class k is represented by the averaged prototypes of its clusters, providing a balanced and representative set of features for further processing.\nMeta-knowledge Compensate Matching. By incorporating more class-specific characteristic features in the condensed exemplars while minimizing class-irrelevant features, we hypothesize that more discriminative representations can be created, resulting in clearer decision boundaries between different classes. To achieve this, for the current task's condensed data $M_{cond}$, we ensure that the condensed data is similar to its corresponding class prototypes $P_k$, and dissimilar to prototypes of other classes, represented as $N_k = \\overline{U} - P_k$.\nThe similarity between the embedding of a query sample $z_i$ and the corresponding cluster prototypes $u \\in U$ is calculated using cosine similarity. For two feature vectors $z_i$ and u, the cosine similarity is defined as:\n$sim(z_i, u) = \\frac{z_i u}{||z_i|| ||u|| / T}$                                                   (10)\nwhere $z_i = f(x_i)$ is the embedding of sample $x_i$, and T is a temperature parameter that controls sensitivity to similarities.\nTherefore, the aim is to optimize the characters of each data sample to bring the local features of the current class closer to the global set of features for that class while distancing them from the characters of other classes. This optimization is intended to maintain a clear decision boundary, allowing the model to perform replay efficiently. By doing so, the feature distribution of this class can remain balanced across all clients in FL at the level of the character. Thus, we propose Meta-knowledge Contrastive Learning (MKCL) for compensating matching, which contrasts cluster proto- types of the same class for each query sample against those of other classes with differing semantics. This approach naturally results in the following optimization objective term:\n$L_{MKCL} = log ( \\frac{\\sum_{u \\in P_k} e^{sim(z_i, u)}}{\\sum_{u \\in P_k} e^{sim(z_i, u)} + \\sum_{u \\in N_k} e^{sim(z_i, u)}} )$                                                                   (11)\n, finally, we can define as\n$log ( \\sum_{u \\in P_k} e^{sim(z_i, u)} ) - log ( \\sum_{u \\in \\overline{U}} e^{sim(z_i, u)} )$                                                               (12)\nHere, we can summarise the total objective of exemplar condensation as follows:\n$L_{mem} = L_{cond} + L_{rel} + \\beta L_{MKCL},$                                                                 (13)\nwhere $\\beta$ is weighting coefficients for meta-knowledge contrastive learning.\nPrior Knowledge Supervision.\nAnother important part of the dual-distillation structure is knowledge distillation, which is used to transfer knowledge from previous tasks to new tasks, thereby mitigating the problem of catastrophic forgetting. We directly implement Knowledge Distillation (Rebuffi et al. 2017; Li and Hoiem 2017; Wu et al. 2019) widely used in continual learning, which aims to leverage the soft output of a previously trained global model (named teacher model) as a regularization term for the training of the current task global model (named student model).\nMathematically, let $p_{t-1}(x)$ denote the probability distribution (softmax output) of the teacher model after training on the previous task t \u2212 1, and $p_t(x)$ denote the distribution of the student model being trained on the current task t. The goal is to minimize the following objective function:\n$L_{KD} = L_{ce} + \\lambda \u00b7 L_{KL}$                                                                                (14)\nwhere $L_{KL} = KL(p_{t-1}(x) || p_t(x))$\nHere, $L_{ce}$ represents the task-specific cross-entropy loss for the current task t, such as cross-entropy loss, and $L_{KL}$ is the loss function of Kullback-Leibler divergence $KL(p_{t-1}(x) || p_t(x))$ between the teacher's and student's output distributions, which serves as the distillation loss. The parameter $\\lambda$ controls the balance between the task loss and the distillation loss. By minimizing this objective, the student model learns to perform well on the new task while retaining knowledge from previous tasks, thus reducing catastrophic forgetting."}, {"title": "Experimental Setup", "content": "Implementation details.\nIn this work, all methods were implemented using PyTorch (Paszke et al. 2019) and executed on a single NVIDIA RTX 4090 GPU paired with an AMD 7950X CPU, utilizing ResNet18 (He et al. 2016) as the backbone for feature extraction in our classification models. The FedAvg algorithm (McMahan et al. 2017) was employed for global model aggregation. Each task involved training the model over R = 50 communication rounds, with each client performing E = 30 local epochs per round. A learning rate of 0.003 was used across all datasets to achieve optimal performance, and Stochastic Gradient Descent (SGD) served as the optimizer in all experiments. Unless specified otherwise, the constraint factor $\\lambda$ in the Elastic Weight Consolidation (EWC) method was set to 300. For knowledge distillation, the temperature parameter was set to 2, and the distillation loss weight $\\lambda$ was set to 3. And based on our experimental results and a thorough grid search, we set $\\beta = 0.5$ for all experiments. To simulate non-IID data distributions, we partitioned the datasets among clients using the Latent Dirichlet Allocation (LDA) method, where the concentration parameter \u03c3 controls the degree of data skew; we varied \u03c3 to simulate different levels of non-IID data. For experiments on the CIFAR-100 and TinyImageNet datasets, we started with 20 clients and selected 10 clients per round, incrementing the total number of clients by 5 with each new task for the CIFAR-100 experiments with 10 and 20 tasks. However, due to data quantity limitations, only 1 client was incremented per new task in the 50-task CIFAR-100 experiment. Conversely, for the Caltech256 dataset, due to the limited number of samples per class, we started with 5 clients, selected 50% of the total clients in each round, and incremented the total number of clients by 1 with each new task. In all experiments, for each new task, 90% of the existing clients transitioned to the new task. To clearly illustrate the data distributions across clients under varying degrees of non-IID settings (controlled by \u03c3), Figure 3 presents the data distribution for the final task in the CIFAR-100 dataset experiment with a total of 10 tasks. A further detailed breakdown of the data distribution across all clients for each task is illustrated in Fig 3."}, {"title": "Evaluation Metrics", "content": "Accuracy (A): This metric computes the accuracy for a given task. We report the final accuracy after all tasks have been trained as $A_{last"}, "and the average accuracy across the last round of every task as $A_{avg}$.\nAveraged Incremental Accuracy $A_{incre}$ (Rebuffi et al. 2017): This metric calculates the average accuracy after the completion of each task, emphasizing the model's performance throughout the incremental learning process. We denote the overall averaged accuracy across all tasks as $A^{incre}_{avg}$, and the accuracy after the last task as $A^{incre}_{last}$.\nAccuracy $A$ ($A^\\Theta$) (D\u00edaz-Rodr\u00edguez et al. 2018): As defined by D\u00edaz-Rodr\u00edguez et al., this metric differs from standard accuracy by assigning equal weight to the accuracy of each task, regardless of the number of samples. For instance, in a scenario where Task 1 has 50,000 images and Task 2 has 1,000 images, standard accuracy would give more weight to Task 1, whereas Accuracy A treats both tasks equally. We denote the overall averaged Accuracy A as $A^{\\Theta}_{avg}$ and the Accuracy A after the last task as $A^{\\Theta}_{last}$.\nBackward"]}