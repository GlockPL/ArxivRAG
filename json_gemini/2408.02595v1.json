{"title": "Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection", "authors": ["Sajal Aggarwal", "Ananya Pandey", "Dinesh Kumar Vishwakarma*"], "abstract": "Sarcasm is a type of irony, characterized by an inherent mismatch between the literal\ninterpretation and the intended connotation. Though sarcasm detection in text has been\nextensively studied, there are situations in which textual input alone might be insufficient to\nperceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to\nrecognize sarcasm in social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of these triplets\ncomprise the input text and its associated image, as provided in the datasets. Additionally, a\nsupplementary modality is introduced in the form of descriptive image captions. The\nmotivation behind incorporating this visual semantic representation is to more accurately\ncapture the discrepancies between the textual and visual content, which are fundamental to the\nsarcasm detection task. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual feature extraction\nbranch that incorporates a self-regulated residual ConvNet integrated with a lightweight\nspatially aware attention module; (3) an additional modality in the form of image captions\ngenerated using an encoder-decoder architecture capable of reading text embedded in images;\n(4) distinct attention modules to effectively identify the incongruities between the text and two\nlevels of image representations; (5) multi-level cross-domain semantic incongruity\nrepresentation achieved through feature fusion. Compared with cutting-edge baselines, the\nproposed model achieves the best accuracy of 92.89% and 64.48%, respectively, on the Twitter\nmultimodal sarcasm and MultiBully datasets.", "sections": [{"title": "1 Introduction", "content": "Sarcasm is a kind of wit used to poke fun at or make disparaging remarks about a person or\nsituation. The literal interpretation of sarcastic remarks frequently suggests the polar opposite\nof their intended meaning. This makes sarcasm detection a somewhat onerous task. For\ninstance, \u201cOh, I love it when people chew loudly. It's such a pleasant sound.\u201d is a remark\nwherein words indicative of a positive sentiment, namely \u2018love' and 'pleasant', are used to\nexpress an unfavourable opinion for the \u2018unpleasant' behaviour of \u2018chewing loudly'. Sarcasm\ncan be communicated by an amalgamation of verbal and nonverbal cues, including but not\nlimited to alterations in vocal tone, exaggeration of a particular word, elongation of a syllable,\nor a neutral facial expression while delivering a seemingly positive remark. Traditional\nmethods for detecting sarcasm have primarily relied on analyzing either textual [1], [2] or audio\ninputs [3] alone. However, the fact that sarcasm is fundamentally characterized by linguistic\nand contextual incompatibility has necessitated the shift to multimodal datasets [4]\u2013[7] so as\nto capture meaningful information from multiple modalities and contextual history.\nThe sarcasm in the tweet \u201cI love commuting\", for example, is lost if the accompanying\nphotograph of a congested traffic situation is disregarded. The satirical tone of the tweet can\nonly be fully appreciated when both text and image are taken into account. Ignoring the visual\ncues could lead one to mistakenly interpret the comment as sincere, missing the satirical\nundertones entirely. Studies have been conducted to extract image attributes [6] or adjective-noun pairs (ANPs) [8]\nfrom images for multimodal sarcasm detection in image-text pairs. The objective of these\nstudies is to incorporate an additional modality to establish a correlation between textual and\nvisual information, thereby enhancing the understanding of visual semantics. However, the\nincorporation of such elements in the analysis can, at times, lead the sarcasm detection model\nastray. This occurs when these elements generate attributes or objects that fail to capture the\nfundamental essence of the visual semantics. In other words, they may refer to objects or\nattributes that do not effectively support the sarcasm detection task or highlight the inherent\nincongruity between the visual and the text. As a result, this constraint has the capacity to\nimpede the efficacy of sarcasm detection. In order to upgrade the accuracy and coherence of\nimage descriptions and to bring out the disconnect between textual and visual content, we\nintroduce a novel framework that utilises a pre-trained encoder-decoder framework to generate\ndetailed image captions. These captions serve as an additional modality for detecting sarcasm\nin multimodal contexts. Major Contributions\nThe major contributions of the present study are laid out as follows:\n1. A cross-lingual language model is employed as the textual tokenizer and encoder to\nacquire the feature representations for both the text data and the image captions. The\nefficacy of this model is notable as it has undergone extensive pretraining on a wide\narray of data encompassing more than 100 languages, including Hindi. This is\nparticularly advantageous as the MultiBully dataset consists of code-mixed English-\nHindi statements.\n2. In contrast to the majority of prior research studies [6], [9]\u2013[14], which typically utilize\na variant of the ResNet architecture for generating image feature representations, the\nproposed framework employs a more robust self-regulated residual ConvNet [15]\narchitecture to encode images into feature vectors. In order to enhance the obtained\nfeature maps and prioritize the most important regions in the image, we incorporate a\nlightweight spatially aware attention module into our image feature extraction branch.\n3. As opposed to other studies [6], [8], [14], which utilize either image attributes or objects\nas an additional modality, our proposed approach offers a more efficient method by\nintegrating descriptive captions generated through a generative image-to-text\ntransformer model. This integration allows for the incorporation of an additional source\nthat emphasizes the discrepancy between modalities, ultimately enhancing the accuracy\nof sarcasm detection.\n4. To encapsulate the contextual incongruity between the input text and the high-level\nimage description generated through captions, a co-attention layer is integrated on top\nof the transformer-based textual feature extraction and caption feature extraction\nbranches. The inter-modality discrepancy between the textual and low-level image\nrepresentations is effectively addressed through the utilization of a multi-head self-\nattention layer. Therefore, we are able to capture the incongruity between the textual\nand visual data in two distinct manners, operating at separate levels. To the best of our\nknowledge, we are the first to propose a resilient and improved multi-level\nrepresentation of inter-modal semantic incongruity based on descriptive image captions\nfor multimodal sarcasm detection.\n5. Extensive empirical investigations on two publicly accessible datasets [6], [16]\ndemonstrate that the proposed framework exhibits a consistent enhancement when\ncompared to baseline approaches and achieves a competitive level of performance in\ncomparison to several established multimodal sarcasm detection techniques.\nOrganization: The ensuing sections of this document are organised in the following format:\nSection 2 classifies the prior research carried out in the domain of sarcasm detection according\nto the various data modalities employed. In this section, various subsections elaborate on the\ncutting-edge techniques utilized for sarcasm recognition in both unimodal and multimodal\ncontexts. Section 3 provides an in-depth explanation of the proposed framework for detecting\nsarcasm using image-text pairs. Section 4 provides an overview of the dataset characteristics\nfor the two datasets employed in this research. Section 5 offers a detailed description of the\nexperimental setups and presents the findings obtained from the application of the proposed\nframework. Section 6 provides a summary of the current study and offers suggestions for\npotential areas of investigation in future research.\""}, {"title": "2 Literature Review", "content": "2.1\nUnimodal Sarcasm Detection\nIn this section, several state-of-the-art sarcasm detection approaches based on a single\nmodality have been discussed.\n2.1.1 Textual Data\nRule-based methodologies endeavor to recognize sarcasm in textual data by means of\ndiscerning particular pieces of evidence. According to Maynard and Greenwood [17], the\nsentiment expressed through hashtags serves as a pivotal determinant for detecting sarcasm.\nHashtags are frequently employed by authors of tweets to accentuate sarcasm. Consequently,\nif the emotion conveyed by a hashtag failed to align with the overall content of the tweet, it\nwas suggested that the tweet was intended to convey sarcasm. For statistical sarcasm detection,\nthe majority of methodologies utilized bag-of-words as their primary feature representation.\nAs demonstrated by Joshi et al. in [18], a crucial component of sarcasm detection is the use of\nword embeddings as a basis for similarity evaluation. Sarcasm detection studies based on\nconventional techniques extracted a myriad of features, including emoticons, unigrams [2], n-\ngrams [19], POS sequences [20], semantic similarity [21], sentiment [22], and word frequency\n[23].\nWith the increasing prevalence of deep learning-based architectures in the realm of NLP,\nresearchers have employed these methodologies to tackle the intricate challenge of automatic\nsarcasm detection [18], [24]\u2013[29]. These studies delved into the intricate nuances of contextual\nelements, such as the complex interaction between the content of tweets and user reactions. In\n[25], Amir et al. introduced a novel architecture that utilized CNNs to acquire user embeddings\nalongside utterance-based embeddings to acquire user-specific contextual information. In [26],\nPoria et al. examined the use of deep convolutional networks as a means of discerning sarcasm\nin tweets. Goel et al. [30] trained an ensemble model comprising CNN, LSTM and GRU to\neffectively identify sarcastic statements in two benchmark datasets. Misra and Arora [29]\nproposed an attention-based hybrid neural network architecture to detect sarcasm in news\nheadlines. Given that an ample supply of annotated data necessitates a substantial expenditure\nof human effort, Wang et al. [31] examined sarcasm detection from an unsupervised\nstandpoint. The authors proposed a masking and generation paradigm to extract inconsistencies\nand acquire knowledge pertaining to the expression of sarcasm.\n2.1.2 Acoustic Data\nTepperman et al. [3] conducted an experiment to examine the recognition of sarcasm in\nspoken dialogue. They analyzed 131 instances of the phrase \u201cyeah right\u201d, which were obtained\nfrom the Switchboard and Fisher corpora [32], [33]. The authors investigated multiple cues,\nencompassing prosodic, spectral, and contextual factors, and provided evidence that the\ninclusion of prosodic features is dispensable in the detection of sarcasm as long as spectral and\ncontextual cues are considered.\n2.2 Multimodal Sarcasm Detection\nMultimodal sarcasm detection seeks to identify the sarcastic expression across several\nmodalities, particularly image-text pairs [4] or videos [7], as opposed to relying solely on text-\nbased or audio-based sarcasm recognition. The following subsections provide an elaboration\nof various approaches utilized for the detection of sarcasm in multimodal contexts.\n2.2.1 Text and Visual Data\nSarcasm detection systems that depend solely on textual analysis may struggle to distinguish\nbetween genuine and sarcastic speech. Given the prevalence of text-image amalgamations\nwithin contemporary social media platforms, multimodal approaches that capture the disparity\nbetween the two modalities seem to be more promising for sarcasm prediction. Schifanella et\nal. [4] were the first to approach sarcasm identification as a multimodal classification problem\nby compiling a dataset comprising text-image posts from three social media platforms\nTwitter, Tumblr and Instagram. They performed sarcasm recognition by concatenating image\nand text features that were either manually handcrafted or deep-learning-based. Using a\ncombination of text, numeric, and visual information taken from Facebook posts, Das and\nClark [34] trained multiple ML classifiers to identify sarcasm. Sangwan et al. [5] created two\nmultimodal sarcasm recognition datasets: the Silver-Standard dataset, consisting of 10K\nsarcastic and non-sarcastic Instagram posts each, categorized on the basis of hashtags, and the\nGold-Standard dataset, which includes 1600 randomly selected sarcastic posts from the first\ndataset and annotated first using only the text modality, and then reannotated using both\nmodalities. The authors developed an RNN-based deep learning system to identify sarcasm by\ncapitalizing on the interdependence of text and visuals.\nAn extensive dataset for multimodal Twitter sarcasm detection was introduced by Cai et al.\nin [6]. The authors put forward a hierarchical fusion framework that combined text, image, and\nattribute feature representations extracted using BiLSTM, ResNet-50, and ResNet-101 models,\nrespectively. Pandey and Vishwakarma [35], [36] employed attention mechanisms in\nconjunction with convolutional neural networks to carry out sentiment analysis on multimodal\ndatasets comprising image-text pairs. In [10], Wang et al. utilized the pre-trained BERT and\nResNet models for extracting the text and image feature representations, respectively and\nestablished a connection between the vector spaces of BERT and ResNet using a bridge layer.\nIn order to represent the differences between the text and the images, the authors used multi-\nhead attention and devised a 2D-intra-attention mechanism to draw focus to the disparities. Xu\net al. [8] proposed a novel D&R Net (Decomposition and Relation Network) to model cross-\nmodality incongruity and semantic association for sarcasm detection. While the decomposition\nnetwork focused on contextual contrast between the text and image in high-level spaces, the\nrelation network used a variant of multi-head attention to capture the degree of semantic\nassociation between the input text and adjective-noun pairs extracted from the image.\nZhao et al. [11] utilized BERT and Bi-GRU to extract text features and pre-trained ResNet-\n152 to extract image features. They then employed textual and visual attention mechanisms to\nderive context vectors, prioritizing specific keywords and image patches for enhanced focus.\nThe coupled-attention model, as proposed, proceeded to calculate fusion memory vectors for\nk iterations. In this process, a memory vector, indexed by iteration i, stored the combined visual\nand textual information that had been gathered up to that particular iteration. Ultimately, the\nfinal memory vector underwent processing via a single-layer softmax classifier in order to\ndetect sarcasm.\nIn [37], Liang et al. explored the use of graph convolution networks to jointly learn the\nsentiment incongruity within and across the text and image modalities as captured by in-modal\nand cross-modal graphs, respectively. In [38], Liang et al. presented a novel cross-modal graph\nconvolutional network (CMGCN) to identify key parts of an image and then correlate them\nwith the textual tokens to detect sarcasm.\nLiu et al. [39] presented an innovative neural model that incorporated logic-based techniques\nto detect instances of multimodal rumors and sarcasm. The model utilized interpretable logic\nclauses to effectively articulate the reasoning process involved in the target task. A new\nframework called MuLOT (Multimodal Learning using Optimal Transport) was proposed by\nPramanick et al. in [12] to detect sarcasm and humor. This framework uses self-attention to\nconsider associations within a single modality, optimal transport to utilize cross-modal\nrelationships, and multimodal attention fusion to account for inter-dependencies between\nmodalities.\nA multimodal meme dataset - \u201cMultiBully\u201d, annotated with labels for bullying, sentiment,\nsarcasm, and emotion, was developed and provided by Maity et al. in [16]. The authors\nconcluded that the proposed multimodal multitask frameworks for recognition of cyberbullying\nand the three supplemental classification problems (sarcasm detection, sentiment analysis, and\nemotion recognition) prevailed over their unimodal and single-task variants, thus boosting the\naccuracy of cyberbullying detection.\n2.2.2 Text and Acoustic Data\nBedi et al. [40] proposed a new Hindi-English code-mixed dataset, MaSaC, for multimodal\nsarcasm detection in conversational dialogue comprising 15K utterances from 400 scenes of\nthe Indian TV comedy show \u2018Sarabhai v/s Sarabhai'. The authors also developed MSH-\nCOMICS, an attention-based multimodal classification model consisting of a hierarchical\nutterance-level attention module in order to acquire a more detailed linguistic representation of\neach statement.\n2.2.3 Text, Visual and Acoustic Data\nThe first video dataset for sarcasm detection, MUSTARD, was released by Castro et al. in [7].\nThe authors collected and annotated 690 audio-visual utterances from popular TV shows and\nused an SVM classifier to predict sarcasm using a combination of textual, audio and visual\nfeatures. Chauhan et al. [41] used emotion and sentiment for sarcasm recognition after updating\nthe MUSTARD dataset to incorporate relevant emotion and sentiment labels. By adding fresh\nutterances from similar sources, Ray et al. [42] doubled the size of the extended MUSTARD\ndataset provided by Chauhan et al. Labels for arousal, valence, and sarcasm type were added\nto each phrase, and some incorrectly assigned emotions were also fixed. This revised dataset\nwas released by the authors under the name MUSTARD++. The dataset was benchmarked using\ndifferent combinations of pre-trained feature extraction algorithms and multimodal fusion\ntechniques. Chauhan et al. [43] introduced an enhanced version of the MUSTARD dataset,\nknown as SEEmoji MUSTARD, wherein each utterance was annotated with a pertinent emoji,\nalong with the associated emotion and sentiment conveyed by the emoji."}, {"title": "2.3 Research Limitations and Motivation", "content": "Despite the extensive study of sarcasm recognition in textual data, multimodal sarcasm\ndetection is still a developing discipline. To evaluate their approach, most studies based on\nmultimodal sarcasm recognition in image-text pairs use the Twitter multimodal sarcasm\ndetection dataset. In order to stimulate more research on similar datasets, this paper\ndemonstrates its findings on both the Twitter dataset and another less-studied dataset,\nMultiBully. The incorporation of this supplementary dataset in our research prompted us to\nutilize a multilingual variant of a transformer module to handle code-mixed statements. This is\nin contrast to previous studies that predominantly utilized the standard BERT model to extract\nfeatures for English-language statements in the Twitter dataset. The majority of studies\ndiscussed in section 2.2.1 have employed one or the other variant of the ResNet architecture\nas the conventional approach for visual feature extraction. However, there has been a lack of\nexploration into alternative methods for enhancing the robustness of visual feature extraction.\nInspired by this observed pattern, we propose a more robust visual feature extraction pipeline.\nThis pipeline incorporates a self-regulated ConvNet combined with a spatially aware attention\nmodule. This integration imposes minimal computational burden while simultaneously\nimproving the quality of the extracted feature maps. Image attributes have been utilized in\nmultimodal sarcasm detection research to incorporate an extra modality for improving the\ndetection of incongruity. The inclusion of supplementary external knowledge is crucial for\nemphasizing the disparity between the textual message and the visual data. However, it is\nimperative that this knowledge source is more comprehensive and provides clearer\nexplanations. In certain instances, attributes of images can provide meaningful information,\nparticularly when the images depict simple scenes and standard objects. However, these\nattributes are insufficient in cases where the image holds a deeper meaning, particularly when\nthere is text embedded within the image that must be understood in order to accurately interpret\nthe semantic information conveyed by the image. Motivated by this shortcoming, this study\nsuggests utilizing a more reliable external source of knowledge, which involves using\ndescriptive image captions generated by an advanced encoder-decoder architecture. This\narchitecture is capable of understanding text embedded in images and producing meaningful\ncaptions that accurately represent the visual semantics. This presents an additional opportunity\nfor us to capture the differentiation between the given text and image, thereby improving the\nprecision of automated multimodal sarcasm detection."}, {"title": "3 Proposed Methodology", "content": "In this section, we present an extensive account of the proposed framework for multimodal\nsarcasm detection in image-text pairs. depicts the basic building blocks of the\nproposed architecture, which include the following: 1) A transformer-based textual feature\nextraction branch to capture the latent representation of the text modality; 2) An attentional\nvisual feature extraction branch comprising [15] combined with a lightweight attention module\nto uncover the image modality's latent representation; 3) An encoder-decoder architecture to\ngenerate descriptive caption corresponding to each image, followed by conversion to feature\nrepresentation via a cross-lingual language model; and 4) Two distinct intermodal attention\nmodules in order to effectively capture and model the inconsistencies that exist between textual\ninformation and two different levels of image representation. The pseudocode for the proposed\napproach has been outlined in Table 1."}, {"title": "3.1 Task Definition", "content": "In the context of this study, multimodal sarcasm detection seeks to ascertain whether a\nparticular text-image pair is intended to convey sarcasm or not. In the selected dataset D, each\nsample $d \\in D$ comprises an input text sequence of words, denoted as $W =\\$\n{W_{1}, W_{2}, W_{3}, ..., w_{t} }, along with a corresponding image $I$. To enhance the comprehension of\nimage semantics, we employ a more effective approach by generating a descriptive caption\ncorresponding to each image. Therefore, our model is capable of accepting additional input as\nan image caption, $W'$. Thus, the input triplets accepted by the model can be represented as\nshown in Equation (1).\nMultimodal Input, M = {Text (W), Image (I), Image Caption (W')} \t\t\t(1)\nOur objective is to assign a label $y \\in Y$ to each input triplet M, where_Y = {non\nsarcastic, sarcastic}."}, {"title": "3.2 Transformer-based Textual Feature Extraction Branch", "content": "The cross-lingual language model [44] has been employed to get the textual features from\ninput text sequence of words, $W =\\${W_{1},W_{2}, W_{3}, ..., W_{t} }$. The model tokenizer, $X_{tokenizer}$ is\nutilized as an embedding module to generate text embeddings that encompass extensive\nsemantic information. To achieve this, the input sequence is initially tokenized into a specific\nformat, as shown in Equation (2).\nW_{tok} = X_{tokenizer} (W) = {[CLS], W_{1}, W_{2}, W_{3}, ..., W_{t} }\t\t\t\t\t\t\t\t\t(2)\nwhere, $w_{i} \\in \\mathbb{R}^{d}$ and $d$ denotes the embedding size. The sequential output, $S$ obtained from the\nuse of model encoder, $X_{encoder}$ [44] for all tokens in the input $W_{tok}$ can be denoted as shown\nin Equation (3)."}, {"title": "3.3 Attentional Visual Feature Extraction Branch", "content": "The ensuing subsections comprehensively elucidate the proposed methodology for capturing\nvital information from the visual content.\n3.3.1 Self-regulated Residual ConvNet\nUpon receiving an input image, $I$ with dimensions $I_{H} \u00d7 I_{W}$, the initial step involves resizing\nthe image to dimensions 224\u00d7224. Subsequently, the image is partitioned into regions of size\n7\u00d77. Following that, each region is passed through the [15] model, as illustrated in ,\nto obtain a regional feature representation. After eliminating the terminal FC layer, the output\nof final convolutional layer corresponding to input image $I$ can be denoted as depicted in\nEquation (4).\nConvNet_{self-reg} (I) \\in \\mathbb{R}^{r \u00d7 2048} = {r_{i}}^{49}_{i=1}\t\t\t\t\t\t\t\t\t\t\t(4)\nwhere $r_{i} \\in \\mathbb{R}^{2048}$ represents the 2048-dimensional feature representation corresponding to a\nsingle image region.\nThe architecture in [15] introduced a regulator module consisting of convolutional RNNs to\nextract complementary features and control the necessary memory information being passed to"}, {"title": "3.3.2 Lightweight Spatially Aware Attention Module", "content": "Attention modules are extensively employed in tasks related to computer vision. The primary\nintent of this integration is to assist models in discerning particular areas that necessitate\nspecific focus. Nevertheless, the incorporation of attention mechanisms is somewhat\nconstrained as lightweight networks are unable to effectively manage the associated\ncomputational load. Hence, to enhance the accuracy of image feature extraction, this study\nintegrates a simple and flexible attention mechanism [48] that imposes a minimal\ncomputational load. The conventional approach of channel attention enables\nconvolutional networks to acquire about the specific aspects that require emphasis during the\nlearning process. As opposed to channel attention, which uses two-dimensional global pooling\nto reduce a feature tensor to a feature vector and disregards the importance of incorporating\npositional information, [48] splits global pooling operation into separate one-dimensional\nfeature encoding transformations in the horizontal and vertical directions, so as to efficiently\nincorporate spatial coordinate data into the resulting attention maps. As a result, precise\npositioning information can be maintained along one spatial dimension while long-range\ndependencies can be captured along the other. A pair of attention maps that take into account\nboth direction and location are then encoded from the resulting feature maps to further refine\nrepresentations of the input feature map's notable features. The integration of the attention\nmodule, depicted in, with blocks in convolutional networks allows for the\nenhancement of features by zeroing in on the most crucial aspects of the input feature\nrepresentations, owing to its adaptability and low overhead.\nLet the attention module's input feature tensor be $X\\in \\mathbb{R}^{C\u00d7H\u00d7W}$, where C denotes number of\nchannels in $X$, H symbolizes its height, and W stands for its width. Then the output $I' \\E\n\\mathbb{R}^{C\u00d7H\u00d7W}$, represents a transformed tensor of the same size as $X$ but with enhanced\nrepresentations. To precisely capture inter-channel associations and positional information, the\napplied attention mechanism operates through a two-step process. This process involves\ninformation embedding, where the relevant data is encoded, and attention generation, where\nthe attention weights are computed. The first step performs two transformations, one along\neach of the two spatial directions, to model the long-range interactions spatially while\npreserving the positional information. Each channel is encoded along the height and width by\napplying two average pooling kernels, (H,1) and (1,W). The second phase aims at effectively\ncapturing inter-channel relationships from the aggregated feature maps generated in the\npreceding stage.\nThe two feature maps are initially combined and subsequently put through a shared 1\u00d71\nconvolutional transformation, thereby yielding an intermediate feature map, $f\\in \\mathbb{R}^{\\frac{C}{r}\u00d7(H+W)}$\nthat captures spatial information along both horizontal and vertical directions. In this context,"}, {"title": "3.4 Image Captioning", "content": "The following subsections shed light on the architecture utilized for image captioning and the\nprocess of feature extraction for the generated captions.\n3.4.1 Encoder-Decoder Architecture\nThe concept of image captioning pertains to the procedure of generating descriptive captions,\nor textual representations, derived from input images. The generation of a caption necessitates\nthe utilization of both natural language processing and computer vision methodologies [49].\nThe utilization of the vision encoder-decoder model is conducive to the initialization of an\nimage-to-text model, wherein a diverse array of pre-trained transformer-based vision models,\nsuch as [50]-[53], can be employed as the encoder. Similarly, the decoder component can be\ninstantiated using a variety of pre-trained language models, including [46], [47], [54], [55]. For\nour study, we utilized the generative image-to-text transformer [56] that employed a CLIP/ViT-\nL/14 [57] for image encoding and a transformer network for decoding. This framework (Figure\n6) was introduced as a method for consolidating multiple vision-language tasks, namely, image\nand video captioning, as well as question answering. It presents a straightforward model that\nrelies on a single vision encoder and a single language decoder."}, {"title": "3.4.2 Image Caption Feature Representation", "content": "The caption feature representations are derived using the cross-lingual language model, just\nlike the text feature representations as discussed in section 3.2. For the input sequence of\ncaption words, $W' =\\${w_{1}, w_{2},W_{3}, ..., w_{u} }, the tokenized sequence is represented as depicted\nin Equation (7).\nW'_{tok} = X_{tokenizer}(W') = {[CLS], w'_{1}, w'_{2}, w'_{3}, ..., w'_{u} }\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(7)\nwhere $w'_{i} \\in \\mathbb{R}^{d}$ and d denotes the embedding size. The sequential output, C, obtained for all\ntokens in the input $W'_{tok}$ can be denoted as shown in Equation (8).\nC = X_{encoder}(W'_{tok})\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(8)\nwhere, $C \\in \\mathbb{R}^{U \u00d7d}$, where d = 768 stands for the dimension of each token, and U symbolizes\nthe maximum total length of $W'_{tok}$"}, {"title": "3.5 Multi-level Cross-Domain Incongruity Representation", "content": "This section presents a thorough account of the methodologies proposed for capturing multi-\nlevel cross-domain discrepancies. The initial subsection focuses on the extraction of\nincongruity between feature representations obtained from textual and visual feature extraction\nbranches. The subsequent subsection elucidates the modelling of discrepancies arising from\nthe inclusion of the image caption modality.\n3.5.1 Text-Image Incongruity\nThe relationship between each token pair of a sequence is taken into account in the internal\nrepresentation generated using self-attention mechanism [63]. Given the inherent significance\nof discordance in sarcasm, the input tokens will place greater emphasis on image regions that\ndirectly contradict them. We take inspiration from this innate capacity for focused attention to\ncreate a layer, as illustrated in, that can detect discrepancies between text and images.\nThis layer in our model is designed to accept text features $S \\in \\mathbb{R}^{T\u00d7d}$, as queries, and image\nfeatures $I' \\in \\mathbb{R}^{r\u00d7d}$, as keys and values. The model can then be directed to compare the query\nmatrices of the text features to the key matrices of all image regions and allocate greater focus\ntowards the ones that exhibit incongruity. The application of self-attention multiple times in\nparallel allows attending to different token pairs differently. Let us denote the total number of\nheads in the multi-head attention function as h. Then, for the ith attention head, the output $m_{i}$,\ncan be computed as shown in Equation (9).\nm_{i}(S,I) = Softmax(\\frac{(SW^{Q}_{i})(I'W^{K}_{i})^{T}}{\\sqrt{d_{k}}})(I'W^{V}_{i})\t\t\t\t\t\t\t\t\t\t(9)\nwhere $d_{k} \\in \\mathbb{R}, m_{i}(S,I') \\in \\mathbb{R}^{T\u00d7d_{k}}, \\mathbb{T}$ symbolizes the matrix transpose operation, and\n{W^{Q}_{i},W^{K}_{i}, W^{V}_{i}} \\in \\mathbb{R}^{d\u00d7d_{k}}$ stand for the learnable query, key and value weight matrices. The\nmulti-head attention is calculated using the output features of each head, $m_{i}$, as shown in\nEquation (10).\nMulti \u2013 head attention (S, I') = (m_{1} \u2295 m_{2} \u2295 m_{3} \u2295...\u2295m_{n})Wo\t\t\t\t\t\t\t\t\t(10)\nwhere $Wo \\in \\mathbb{R}^{d\u00d7d}$, and \u2295 denotes vector concatenation. Followed by a two-layer MLP and a\nresidual connection, the text-image discrepancy captured can be obtained as illustrated in\nEquation (11).\nIncongurity (S,I') = LNorm (S + MLP(Multi \u2013 head attention (S,I')))\t\t\t\t\t\t\t\t\t(11)\nwhere Incongurity (S,I') \\in \\mathbb{R}^{T\u00d7d}. The final disparity representation, $\\tau_{s1'}$ \\in \\mathbb{R}^{d} is obtained\nvia Equation (12)."}, {"title": "3.5.2 Text-Caption Incongruity", "content": "\\tau_{s1'} = [CLS", "64": "to effectively address the disparities between the original text\nand the generated image captions. As shown in, the co-attention dual input comprises\nthe feature representations of the text and the image captions, as obtained in Equation (3)\nand Equation (8), respectively. The initial step involves the computation of affinity matrix,\n$A\\in \\mathbb{R}^{TU}$ through the utilization of a bi-linear transformation W, as shown in\nEquation (13). The motive of this transformation is to accurately encapsulate the\ninterrelation between the text and the captions of the accompanying images.\nA = tanh(SWC^{T})\t\t\t\t\t\t\t\t\t\t\t\t\t(13)\nHere, $S \\in \\mathbb{R}^{T\u00d7d}$ denotes the text features, $C\\in \\mathbb{R}^{U\u00d7d}$ denotes the image caption features (T and\nU denote the maximum size of these features correspondingly, and d symbolizes the hidden\nsize of [44", "14).\na_{c}[u": "maxi(Ai,u)\t\t\t\t\t\t\t\t\t\t\t\t\t(14)\nThe attention weight vector $v \\in \\mathbb{R}^{U}$ is thus calculated by applying a column-wised max-\npooling operation kernel of dimensions $T\u00d71$ over affinity matrix A, as depicted in\nEquation (15).\nv = MaxPool (A)\t\t\t\t\t\t\t\t\t\t\t\t\t(15)\nThe final text-image caption incongruity representation used to compute the model's prediction\nis obtained as $\\tau_{sc}$ \\in \\mathbb{R}^{d}, using Equation (16).\n\\tau_{sc"}]}