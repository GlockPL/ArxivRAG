{"title": "Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection", "authors": ["Sajal Aggarwal", "Ananya Pandey", "Dinesh Kumar Vishwakarma"], "abstract": "Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primary contributions of this study are: (1) a robust textual feature extraction branch that utilizes a cross-lingual language model; (2) a visual feature extraction branch that incorporates a self-regulated residual ConvNet integrated with a lightweight spatially aware attention module; (3) an additional modality in the form of image captions generated using an encoder-decoder architecture capable of reading text embedded in images; (4) distinct attention modules to effectively identify the incongruities between the text and two levels of image representations; (5) multi-level cross-domain semantic incongruity representation achieved through feature fusion. Compared with cutting-edge baselines, the proposed model achieves the best accuracy of 92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and MultiBully datasets.", "sections": [{"title": "1 Introduction", "content": "Sarcasm is a kind of wit used to poke fun at or make disparaging remarks about a person or situation. The literal interpretation of sarcastic remarks frequently suggests the polar opposite of their intended meaning. This makes sarcasm detection a somewhat onerous task. For instance, \u201cOh, I love it when people chew loudly. It's such a pleasant sound.\u201d is a remark wherein words indicative of a positive sentiment, namely \u2018love' and 'pleasant', are used to express an unfavourable opinion for the \u2018unpleasant' behaviour of \u2018chewing loudly'. Sarcasm can be communicated by an amalgamation of verbal and nonverbal cues, including but not limited to alterations in vocal tone, exaggeration of a particular word, elongation of a syllable, or a neutral facial expression while delivering a seemingly positive remark. Traditional methods for detecting sarcasm have primarily relied on analyzing either textual [1], [2] or audio inputs [3] alone. However, the fact that sarcasm is fundamentally characterized by linguistic and contextual incompatibility has necessitated the shift to multimodal datasets [4]\u2013[7] so as to capture meaningful information from multiple modalities and contextual history."}, {"title": "2 Literature Review", "content": "In this section, several state-of-the-art sarcasm detection approaches based on a single modality have been discussed."}, {"title": "2.1 Unimodal Sarcasm Detection", "content": "In this section, several state-of-the-art sarcasm detection approaches based on a single modality have been discussed."}, {"title": "2.1.1 Textual Data", "content": "Rule-based methodologies endeavor to recognize sarcasm in textual data by means of discerning particular pieces of evidence. According to Maynard and Greenwood [17], the sentiment expressed through hashtags serves as a pivotal determinant for detecting sarcasm. Hashtags are frequently employed by authors of tweets to accentuate sarcasm. Consequently, if the emotion conveyed by a hashtag failed to align with the overall content of the tweet, it was suggested that the tweet was intended to convey sarcasm. For statistical sarcasm detection, the majority of methodologies utilized bag-of-words as their primary feature representation. As demonstrated by Joshi et al. in [18], a crucial component of sarcasm detection is the use of word embeddings as a basis for similarity evaluation. Sarcasm detection studies based on conventional techniques extracted a myriad of features, including emoticons, unigrams [2], n-grams [19], POS sequences [20], semantic similarity [21], sentiment [22], and word frequency [23]."}, {"title": "2.1.2 Acoustic Data", "content": "Tepperman et al. [3] conducted an experiment to examine the recognition of sarcasm in spoken dialogue. They analyzed 131 instances of the phrase \u201cyeah right\u201d, which were obtained from the Switchboard and Fisher corpora [32], [33]. The authors investigated multiple cues, encompassing prosodic, spectral, and contextual factors, and provided evidence that the inclusion of prosodic features is dispensable in the detection of sarcasm as long as spectral and contextual cues are considered."}, {"title": "2.2 Multimodal Sarcasm Detection", "content": "Multimodal sarcasm detection seeks to identify the sarcastic expression across several modalities, particularly image-text pairs [4] or videos [7], as opposed to relying solely on text-based or audio-based sarcasm recognition. The following subsections provide an elaboration of various approaches utilized for the detection of sarcasm in multimodal contexts."}, {"title": "2.2.1 Text and Visual Data", "content": "Sarcasm detection systems that depend solely on textual analysis may struggle to distinguish between genuine and sarcastic speech. Given the prevalence of text-image amalgamations within contemporary social media platforms, multimodal approaches that capture the disparity between the two modalities seem to be more promising for sarcasm prediction. Schifanella et al. [4] were the first to approach sarcasm identification as a multimodal classification problem by compiling a dataset comprising text-image posts from three social media platforms Twitter, Tumblr and Instagram. They performed sarcasm recognition by concatenating image and text features that were either manually handcrafted or deep-learning-based. Using a combination of text, numeric, and visual information taken from Facebook posts, Das and Clark [34] trained multiple ML classifiers to identify sarcasm. Sangwan et al. [5] created two multimodal sarcasm recognition datasets: the Silver-Standard dataset, consisting of 10K sarcastic and non-sarcastic Instagram posts each, categorized on the basis of hashtags, and the Gold-Standard dataset, which includes 1600 randomly selected sarcastic posts from the first dataset and annotated first using only the text modality, and then reannotated using both modalities. The authors developed an RNN-based deep learning system to identify sarcasm by capitalizing on the interdependence of text and visuals.\nAn extensive dataset for multimodal Twitter sarcasm detection was introduced by Cai et al. in [6]. The authors put forward a hierarchical fusion framework that combined text, image, and attribute feature representations extracted using BiLSTM, ResNet-50, and ResNet-101 models, respectively. Pandey and Vishwakarma [35], [36] employed attention mechanisms in"}, {"title": "2.2.2 Text and Acoustic Data", "content": "Bedi et al. [40] proposed a new Hindi-English code-mixed dataset, MaSaC, for multimodal sarcasm detection in conversational dialogue comprising 15K utterances from 400 scenes of the Indian TV comedy show \u2018Sarabhai v/s Sarabhai'. The authors also developed MSH-COMICS, an attention-based multimodal classification model consisting of a hierarchical utterance-level attention module in order to acquire a more detailed linguistic representation of each statement."}, {"title": "2.2.3 Text, Visual and Acoustic Data", "content": "The first video dataset for sarcasm detection, MUSTARD, was released by Castro et al. in [7]. The authors collected and annotated 690 audio-visual utterances from popular TV shows and"}, {"title": "2.3 Research Limitations and Motivation", "content": "Despite the extensive study of sarcasm recognition in textual data, multimodal sarcasm detection is still a developing discipline. To evaluate their approach, most studies based on multimodal sarcasm recognition in image-text pairs use the Twitter multimodal sarcasm detection dataset. In order to stimulate more research on similar datasets, this paper demonstrates its findings on both the Twitter dataset and another less-studied dataset, MultiBully. The incorporation of this supplementary dataset in our research prompted us to utilize a multilingual variant of a transformer module to handle code-mixed statements. This is in contrast to previous studies that predominantly utilized the standard BERT model to extract features for English-language statements in the Twitter dataset. The majority of studies discussed in section 2.2.1 have employed one or the other variant of the ResNet architecture as the conventional approach for visual feature extraction. However, there has been a lack of exploration into alternative methods for enhancing the robustness of visual feature extraction. Inspired by this observed pattern, we propose a more robust visual feature extraction pipeline. This pipeline incorporates a self-regulated ConvNet combined with a spatially aware attention module. This integration imposes minimal computational burden while simultaneously improving the quality of the extracted feature maps. Image attributes have been utilized in multimodal sarcasm detection research to incorporate an extra modality for improving the detection of incongruity. The inclusion of supplementary external knowledge is crucial for emphasizing the disparity between the textual message and the visual data. However, it is imperative that this knowledge source is more comprehensive and provides clearer explanations. In certain instances, attributes of images can provide meaningful information, particularly when the images depict simple scenes and standard objects. However, these attributes are insufficient in cases where the image holds a deeper meaning, particularly when there is text embedded within the image that must be understood in order to accurately interpret the semantic information conveyed by the image. Motivated by this shortcoming, this study suggests utilizing a more reliable external source of knowledge, which involves using descriptive image captions generated by an advanced encoder-decoder architecture. This architecture is capable of understanding text embedded in images and producing meaningful captions that accurately represent the visual semantics. This presents an additional opportunity for us to capture the differentiation between the given text and image, thereby improving the precision of automated multimodal sarcasm detection."}, {"title": "3 Proposed Methodology", "content": "In this section, we present an extensive account of the proposed framework for multimodal sarcasm detection in image-text pairs. Figure 3 depicts the basic building blocks of the proposed architecture, which include the following: 1) A transformer-based textual feature extraction branch to capture the latent representation of the text modality; 2) An attentional"}, {"title": "3.1 Task Definition", "content": "In the context of this study, multimodal sarcasm detection seeks to ascertain whether a particular text-image pair is intended to convey sarcasm or not. In the selected dataset D, each sample $d \\in D$ comprises an input text sequence of words, denoted as $W = \\{W_1, W_2, W_3, ..., w_t \\}$, along with a corresponding image I. To enhance the comprehension of image semantics, we employ a more effective approach by generating a descriptive caption corresponding to each image. Therefore, our model is capable of accepting additional input as an image caption, W'. Thus, the input triplets accepted by the model can be represented as shown in Equation (1).\nMultimodal Input, $M = \\{Text (W), Image (I), Image Caption (W')\\}$  (1)\nOur objective is to assign a label $y \\in Y$ to each input triplet M, where $Y = \\{non - sarcastic, sarcastic\\}$."}, {"title": "3.2 Transformer-based Textual Feature Extraction Branch", "content": "The cross-lingual language model [44] has been employed to get the textual features from input text sequence of words, $W = \\{W_1,W_2, W_3, ..., W_t \\}$. The model tokenizer, $X_{tokenizer}$ is utilized as an embedding module to generate text embeddings that encompass extensive semantic information. To achieve this, the input sequence is initially tokenized into a specific format, as shown in Equation (2).\n$W_{tok} = X_{tokenizer} (W) = \\{[CLS], W_1, W_2, W_3, ..., W_t \\}$   (2)\nwhere, $w_i \\in R^d$ and d denotes the embedding size. The sequential output, S obtained from the use of model encoder, $X_{encoder}$ [44] for all tokens in the input $W_{tok}$ can be denoted as shown in Equation (3)."}, {"title": "3.3 Attentional Visual Feature Extraction Branch", "content": "The ensuing subsections comprehensively elucidate the proposed methodology for capturing vital information from the visual content."}, {"title": "3.3.1 Self-regulated Residual ConvNet", "content": "Upon receiving an input image, I with dimensions $I_H \\times I_W$, the initial step involves resizing the image to dimensions 224\u00d7224. Subsequently, the image is partitioned into regions of size 7\u00d77. Following that, each region is passed through the [15] model, as illustrated in Figure 4, to obtain a regional feature representation. After eliminating the terminal FC layer, the output of final convolutional layer corresponding to input image I can be denoted as depicted in Equation (4).\n$ConvNet_{self-reg} (I) \\in R^{r \\times 2048} = \\{r_i\\}_{i=1}^{49}$   (4)\nwhere $r_i \\in R^{2048}$ represents the 2048-dimensional feature representation corresponding to a single image region."}, {"title": "3.3.2 Lightweight Spatially Aware Attention Module", "content": "Attention modules are extensively employed in tasks related to computer vision. The primary intent of this integration is to assist models in discerning particular areas that necessitate specific focus. Nevertheless, the incorporation of attention mechanisms is somewhat constrained as lightweight networks are unable to effectively manage the associated computational load. Hence, to enhance the accuracy of image feature extraction, this study integrates a simple and flexible attention mechanism [48] that imposes a minimal computational load. The conventional approach of channel attention enables convolutional networks to acquire about the specific aspects that require emphasis during the learning process. As opposed to channel attention, which uses two-dimensional global pooling to reduce a feature tensor to a feature vector and disregards the importance of incorporating positional information, [48] splits global pooling operation into separate one-dimensional feature encoding transformations in the horizontal and vertical directions, so as to efficiently incorporate spatial coordinate data into the resulting attention maps. As a result, precise positioning information can be maintained along one spatial dimension while long-range dependencies can be captured along the other. A pair of attention maps that take into account both direction and location are then encoded from the resulting feature maps to further refine representations of the input feature map's notable features. The integration of the attention module, depicted in Figure 5, with blocks in convolutional networks allows for the enhancement of features by zeroing in on the most crucial aspects of the input feature representations, owing to its adaptability and low overhead.\nLet the attention module's input feature tensor be $X\\in R^{C\\times H\\times W}$, where C denotes number of channels in X, H symbolizes its height, and W stands for its width. Then the output $I' \\in R^{C\\times H\\times W}$, represents a transformed tensor of the same size as X but with enhanced representations. To precisely capture inter-channel associations and positional information, the applied attention mechanism operates through a two-step process. This process involves information embedding, where the relevant data is encoded, and attention generation, where the attention weights are computed. The first step performs two transformations, one along each of the two spatial directions, to model the long-range interactions spatially while preserving the positional information. Each channel is encoded along the height and width by applying two average pooling kernels, (H,1) and (1,W). The second phase aims at effectively capturing inter-channel relationships from the aggregated feature maps generated in the preceding stage.\nThe two feature maps are initially combined and subsequently put through a shared 1\u00d71 convolutional transformation, thereby yielding an intermediate feature map, $f\\in R^{\\frac{C}{r}\\times(H+W)}$ that captures spatial information along both horizontal and vertical directions. In this context,"}, {"title": "3.4 Image Captioning", "content": "The following subsections shed light on the architecture utilized for image captioning and the process of feature extraction for the generated captions."}, {"title": "3.4.1 Encoder-Decoder Architecture", "content": "The concept of image captioning pertains to the procedure of generating descriptive captions, or textual representations, derived from input images. The generation of a caption necessitates the utilization of both natural language processing and computer vision methodologies [49]. The utilization of the vision encoder-decoder model is conducive to the initialization of an image-to-text model, wherein a diverse array of pre-trained transformer-based vision models, such as [50]-[53], can be employed as the encoder. Similarly, the decoder component can be instantiated using a variety of pre-trained language models, including [46], [47], [54], [55]. For our study, we utilized the generative image-to-text transformer [56] that employed a CLIP/ViT-L/14 [57] for image encoding and a transformer network for decoding. This framework (Figure 6) was introduced as a method for consolidating multiple vision-language tasks, namely, image"}, {"title": "3.4.2 Image Caption Feature Representation", "content": "The caption feature representations are derived using the cross-lingual language model, just like the text feature representations as discussed in section 3.2. For the input sequence of caption words, $W' = \\{w'_1, w'_2, w'_3, ..., w'_u \\}$, the tokenized sequence is represented as depicted in Equation (7).\n$W'_{tok} = X_{tokenizer}(W') = \\{[CLS], w'_1, w'_2, w'_3, ..., w'_u \\}$   (7)\nwhere $w_i \\in R^d$ and d denotes the embedding size. The sequential output, C, obtained for all tokens in the input $W'_{tok}$ can be denoted as shown in Equation (8).\n$C = X_{encoder}(W'_{tok})$  (8)\nwhere, $C \\in R^{U\\times d}$, where d = 768 stands for the dimension of each token, and U symbolizes the maximum total length of $W'_{tok}$."}, {"title": "3.5 Multi-level Cross-Domain Incongruity Representation", "content": "This section presents a thorough account of the methodologies proposed for capturing multi-level cross-domain discrepancies. The initial subsection focuses on the extraction of incongruity between feature representations obtained from textual and visual feature extraction branches. The subsequent subsection elucidates the modelling of discrepancies arising from the inclusion of the image caption modality."}, {"title": "3.5.1 Text-Image Incongruity", "content": "The relationship between each token pair of a sequence is taken into account in the internal representation generated using self-attention mechanism [63]. Given the inherent significance"}, {"title": "3.5.2 Text-Caption Incongruity", "content": "We utilize a co-attention mechanism, as demonstrated in the field of visual question answering by Lu et al. in [64], to effectively address the disparities between the original text and the generated image captions. As shown in Figure 8, the co-attention dual input comprises the feature representations of the text and the image captions, as obtained in Equation (3) and Equation (8), respectively. The initial step involves the computation of affinity matrix, $A\\in R^{TU}$ through the utilization of a bi-linear transformation W, as shown in Equation (13). The motive of this transformation is to accurately encapsulate the interrelation between the text and the captions of the accompanying images.\n$A = tanh(S W C^T)$   (13)\nHere, $S \\in R^{T\\times d}$ denotes the text features, $C\\in R^{U\\times d}$ denotes the image caption features (T and U denote the maximum size of these features correspondingly, and d symbolizes the hidden size of [44]), and $W \\in R^{d\\times d}$ stands for a learnable parameter comprising weights. The affinity matrix that has been acquired serves to transform the attention space of the text into the attention space of the caption. In order to highlight the words that most significantly contribute to the incongruity characterizing the sarcastic expression, the affinity matrix is maximized over the text feature's locations to obtain caption attention, $a_c$, as illustrated in Equation (14).\n$a_c[u] = max_i(A_{i,u})$  (14)"}, {"title": "3.5.3 Final Fused Incongruity Representation and Model Prediction", "content": "After capturing both, the incongruity between the text and low-level image feature representation, $\\tau_{sI'}$, and that between the text and high-level image description in the form of generated captions, $\\tau_{sc}$, we concatenate the two disparity descriptors to obtain fusion vector, F for final model predictions, as shown in Equation (17), where $\\oplus$ signifies vector concatenation.\n$F = \\tau_{sI'} \\oplus \\tau_{sc}$   (17)\nThe fusion vector, F is put through the softmax function for classification purpose. The final prediction y is computed as shown in Equation (18), where $W_f \\in R^{2d}$ is a trainable parameter and b denotes the bias.\n$y = Softmax(W_f * F + b)$   (18)"}, {"title": "4 Dataset Description", "content": "Our model for multimodal sarcasm detection is evaluated using two publicly available datasets constructed by Cai et al. [6] and Maity et al. [16]."}, {"title": "4.1 Twitter Dataset (Cai et al. [6])", "content": "Every individual instance within this dataset comprises a combination of textual and corresponding visual information that has been acquired from the social media platform Twitter. Specific hashtag queries, such as #sarcasm, #sarcastic, #irony, #ironic, and so forth, were employed to gather positive samples which pertain to instances of sarcasm. Conversely, the negative samples, denoting non-sarcastic content, were obtained from tweets lacking the aforementioned hashtags. The dataset has been partitioned into three distinct subsets: the training set, which comprises 80% of the data; the development set, which accounts for 10%; and the test set, which also represents 10% of the dataset. The pertinent information has been presented in a tabular format, as outlined in Table 2."}, {"title": "4.2 MultiBully (Maity et al. [16])", "content": "The authors released a dataset collected from publicly available sources such as Twitter and Reddit to examine the influence of sentiment, emotion, and sarcasm in the detection of cyberbullying from multimodal memes within a code-mixed language context. They incorporated three supplementary tasks, namely sarcasm detection, sentiment analysis, and recognition of emotions, aiming to achieve better accuracy with respect to the primary objective, i.e. cyberbullying detection. For this study, we focus only on the subtask of sarcasm detection from multimodal data, i.e. text-image pairs. Table 3 provides an overview of the statistics regarding the number of sarcastic and non-sarcastic memes held by the train, development, and test sets."}, {"title": "5 Experimental Results", "content": "This section provides elaborate information regarding the experimental settings and hyperparameters utilized in the proposed framework, as well as performance evaluations in comparison to baseline models."}, {"title": "5.1 Experimental Settings and Hyperparameters", "content": "Our model is implemented in PyTorch [65] using Google COLAB Pro Plus, which featured an NVIDIA V100 GPU, CUDA version 11.2, driver version 460.32.03, 40 GB of graphics memory, 100 GB of hard disk space, and 80 GB of RAM. We used softmax as a classifier for recognizing sarcasm in image-text pairs. The loss function employed to optimize the presented model was the L2-regularized binary cross-entropy, as represented in Equation (19).\n$L^{JBCE}_{2} = - \\frac{1}{Train size} \\sum^{Train size}_{i=1} y^i log(\\hat{y_i}) + (1 - y^i)log(1 - \\hat{y_i}) + \\frac{w}{Train size}$   (19)\nIn the present study, the Adam optimizer [66] was employed as the optimization algorithm to update all the trainable parameters of the model. The constituted model was trained with a learning rate of 5 \u00d7 10-5, using a batch size of 32. Additionally, a warmup rate of 0.1 was used to control the variation of learning rate and facilitate the optimization process. To mitigate the issue of overfitting, we implemented the dropout technique with a probability value of 0.5. Further, to save time and computational resources, an early stopping patience value of 5 was enforced. By employing this technique, we were able to effectively monitor the training process and terminate it once the model's performance began to plateau, thereby ensuring that the model does not become overly specialized to the training data and could exhibit good generalization capabilities towards novel data. The proposed model was fine-tuned for 15 epochs."}, {"title": "5.2 Evaluation Metrics", "content": "In accordance with the evaluation methodology employed by [6], we employ four widely used performance metrics, namely Accuracy (A), Precision (P), Recall (R), and F1-score (F1), to evaluate the efficacy of the proposed model. The mathematical formulae of these metrics are provided in Table 4."}, {"title": "5.3 Baselines", "content": "The presented framework is being compared to the baselines that are discussed below.\nText-only"}, {"title": "5.4 Experimental Results", "content": "Table 5 and Table 6 present a comparative analysis between our proposed methodology and other cutting-edge benchmarks. Our proposed solution presented in this study demonstrates superior performance compared to the existing state-of-the-art approaches across each of the four evaluation metrics, namely Accuracy, Recall, Precision, and F1 score. Our model indicates a significant enhancement of 2.07% in Accuracy and 0.51% in F1 score compared to the current state-of-the-art MuLOT (with OCR) [12] for Twitter dataset and 1.49% in Accuracy and 0.47% in F1 score compared to Maity et al. [16] for MultiBully dataset. This outcome serves as empirical evidence supporting the efficacy of our proposed model.\nIt can be observed from Table 5 and Table 6 that the independent treatment of images or text does not yield satisfactory results in addressing the sarcasm detection problem. From a qualitative perspective, it can be observed that models relying solely on images tend to exhibit lower performance compared to models relying solely on text. This is due to the fact that an image alone lacks the necessary information to accurately identify the presence of sarcasm. Furthermore, it is evident that unimodal approaches exhibit inferior performance in sarcasm identification, highlighting the need for multimodal approaches to be employed for more effective sarcasm detection. It can be inferred from Table 5 and Table 6 that our proposed approach exhibits superior performance in comparison to other multimodal approaches by effectively capturing the disparities between the image and text pairs."}, {"title": "5.5 Ablation Study", "content": "In this section, a set of ablation trials are carried out on the two datasets, namely the Twitter multimodal sarcasm dataset and the MultiBully dataset. The objective is to thoroughly evaluate the effectiveness of each proposed component for the multimodal input. As per the suggested framework, we have omitted the lightweight spatially aware attention module from the attentional visual feature extraction branch, referred to as the proposed model w/o visual attention. Additionally, we have also eliminated the incongruity representation $\\tau_{sI'}$ from the proposed model, denoted as the proposed model w/o ts\u0131. Likewise, the removal of $\\tau_{sc}$ from the proposed model has been represented as proposed model w/o tsc. The results obtained from the ablation trials are succinctly displayed in Table 7."}, {"title": "6 Conclusion and Future Scope", "content": "In this study, we propose a novel approach for multimodal sarcasm recognition by utilizing image captions to create a multi-tier cross-modal incongruity representation. The inclusion of captions in this particular context provides a valuable opportunity to improve our understanding of the meaning conveyed by images and draw attention to the difference in emotional tone between the message conveyed in a tweet and the visual content that accompanies it. This methodology improves the efficacy of the suggested framework in identifying instances of sarcasm within two openly available datasets. The experimental results indicate that our proposed framework consistently improves upon state-of-the-art baselines for multimodal sarcasm detection. The effectiveness of each module within the provided framework is additionally supported by the ablation study. In future investigations, our objective is to analyze the feasibility of adapting our model to accommodate supplementary multimodal tasks. Future research endeavors may delve into improved fusion methodologies in order to better capture the interconnections among various modalities. Furthermore, researchers may also aim to integrate low-parameter or domain-adaptive models into their studies. This is due to the fact that models with a reduced number of trainable parameters provide notable benefits by facilitating real-time deployment."}]}