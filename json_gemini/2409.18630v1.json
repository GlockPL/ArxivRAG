{"title": "Entropy, concentration, and learning: a statistical mechanics primer", "authors": ["Akshay Balsubramani"], "abstract": "Artificial intelligence models trained through loss minimization have demonstrated significant success, grounded in principles from fields like information theory and statistical physics. This work explores these established connections through the lens of statistical mechanics, starting from first-principles sample concentration behaviors that underpin AI and machine learning. Our development of statistical mechanics for modeling highlights the key role of exponential families, and quantities of statistics, physics, and information theory.", "sections": [{"title": "1 Foundations: entropy, concentration, and learning", "content": "The basic ideas underpinning modern AI and machine learning involve recognition of statistical patterns from sampled training data, which generalize usefully to a test setting. The broad probabilistic questions involved here are universal:\n\u2022 How much can be learned about a probability distribution from a finite sample of it?\n\u2022 What happens when the test-time distribution being evaluated is different from the training distribution?\n\u2022 Which patterns and representations usefully generalize to other distributions and datasets?\nTo begin understanding these basic principles of machine learning, we need to understand concentration of samples from a distribution. This understanding has been assembled progressively over time by a succession of quantitative fields encountering these problems, going back to the heart of probability and information theory. It was first central to statistical mechanics, a field studying the collective (macroscopic) behavior of large populations of (microscopic) atoms.\nIn the late 19th century, as scientists tried to relate properties of bulk matter to its individual atoms, they sought a quantitative theory of this collective behavior. Development of the field was stalled at a basic level of understanding for many decades, in which unexplained observations were plentiful and unifying explanations scarce the problem was daunting because of the huge numbers of microscopic entities involved.\nProgress came from a major insight by Boltzmann. When he considered a discretized probability-calculation scenario in trying to develop a molecular theory of gas behavior in 1877 [Bol77, SM15], he launched the field of statistical mechanics [Ell99]. These arguments are extremely foundational in an AI context even today, when models learned by loss minimization like deep neural networks - dominate.\nSo we first sketch Boltzmann's core statistical mechanics understanding. With this entry point, we ultimately describe modern-day (loss-minimization-based) AI modeling in statistical mechanics terms. This particular link between modeling and statistical mechanics is foundational, and contains basic insights into learning. It allows us to see the essential unity between the basic concepts underlying loss minimization, information theory, and statistical physics."}, {"title": "1.1 Context among related work", "content": "Starting from Boltzmann, these ideas have been explored in depth and detail, from their discovery in physics in the early 20th century to modern AI/ML. Here is a brief discussion to begin.\nThese ideas have a long history in the statistical mechanics of Boltzmann and Gibbs. Though it was originally developed for physics-centric reasons, early pioneers like Gibbs realized that the math behind it applied to any system of constituent \"particles,\" like a dataset of samples: \"The laws of statistical mechanics apply to...systems of any number of degrees of freedom, and are exact\" [Gib02]. In that spirit, very similar ideas would be repeatedly rediscovered and developed in various quantitative fields leading up to modern AI.\nIn the postwar years of the 1940s, the basic concepts resurfaced for discrete alphabets with the flowering of probability theory and information theory [Sha48]. The engineering focus of information theory, with real-life consequences built on semiconductor advances in computing, resulted in a different approach and focus to areas such as coding and processing of distributions.\nSimilar ideas had been developed in statistics as well, through the concept of sufficient statistics [Fis22, NP36, Koo36, Dar35, Pit36, HS49]. The different focus there is reflected in the primary role of exponential families, particularly for easy-to-conceptualize sufficient statistics. Interestingly, it took longer to appreciate the connections between this branch of statistics and the ongoing work in information theory. Methods of information geometry have more recently developed"}, {"title": "1.2 Scope", "content": "From all that we have discussed, the purpose here is to comprehensively lay out the statistical mechanics view of loss minimization from first principles. Such scenarios are everywhere in modern AI systems. This viewpoint enables us to cross-fertilize ideas between our world of data modeling on one hand, and statistical physics and information theory on the other.\nFor the most part, these are well-established concepts and do not require complex technical machinery to handle real-world modeling scenarios. So we can prove many general results, in a self-contained way with little needed technically. Self-contained derivations of these results are helpful for several reasons:\n\u2022 The results have well-understood interpretations in the real world, as describing the behavior of bulk observed properties emerging from simply applying some basic underlying principles. These interpretations can be translated into data-modeling situations.\n\u2022 The methods used for the derivations, from optimization, game theory, and convexity, are familiar tools to AI researchers. But in the context of the results in physics and information theory, they lead to sometimes nonstandard proofs and interpretations of known results.\nThe proofs' broad scope and applicability, to new problems and loss functions, gives the results significant new power and broadens their applications."}, {"title": "2 Concentration: Boltzmann's \"probability calculation\"", "content": "Start with the first basic question from earlier: how much can be learned about a probability distribution from a finite sample of it? This question has a precise quantitative answer, which is what Boltzmann found in what he called a \"probability calculation\" [Bol77].\nIn probability terms, given a distribution P over a set of outcomes X (we write $P \\in \\Delta(X)$), we are looking to understand the consequences of repeatedly sampling from P. Boltzmann's insight was that if the set of outcomes X is finite, this question can be answered by explicitly counting the possibilities.\nSuppose X is finite: $X = {x_1, x_2, ..., x_D}$. We draw n samples from this set independently according to a probability distribution $P = (P_1, ..., P_D)$, and we observe the frequencies of each outcome. Let $n_i$ be the number of times we observe outcome $x_i$, so that $E n_i = n$. The observed probability of outcome $x_i$ is then $Q_i := n_i/n$, so Q is another probability distribution - the empirical histogram of the data over X.\nBoltzmann calculated the probability of observing a particular set of frequencies ${n_1,...,n_D}$ in this situation:\n$\\frac{1}{n} \\log \\text{Pr}(x_1 = n_1,..., x_D = n_D) = -D(Q || P) + \\frac{1}{2n} \\log \\bigg[ \\frac{2 \\pi n}{\\prod_{i=1}^D (2\\pi n P_i)} \\bigg] + \\Theta(1/n^2)$\nwhere the relative entropy (or divergence) of P with respect to Q is $D(Q || P) := E_{x \\sim Q} \\log \\frac{Q(x)}{P(x)}$.\nThis is an extremely accurate and powerful approximation for even moderate sample sizes n, which tells us the likelihood of observing any specific configuration of outcomes.\n\u2022 We've calculated the chance of observing a particular set of frequencies Q given P. If we instead view Q as given, it makes sense to calculate the P which makes the observed Q most likely.\n\u2022 The real distribution P only enters the picture through its divergence -D(Q || P) from the observed distribution Q.\n\u2022 This -D(Q || P) is also by far the dominant term, as all the others are O(1/n).\nIn short, observing the histogram Q alone does not determine P, but it does give us enough information to precisely quantify the likelihood of deviations of Q from P."}, {"title": "2.1 Boltzmann's reasoning", "content": "Boltzmann's reasoning is the most direct one even after over a century, and it is worth going over the highlights.\nBoltzmann reduced the problem to essentially a generalized balls-in-bins problem by discretizing the probability space, and discretizing quanta of probability (at resolution $\\frac{1}{n}$). The calculation is simply a matter of accounting for the differently weighted \"bins\" (outcomes), and"}, {"title": "2.2 Consequences", "content": "What Boltzmann called his \"probability calculations\" [Bol77] launched the field of statistical mechanics, and inspired the field of information theory decades later. This is because discretizing the space is a fully general technique, with all the essential elements used to study concentration and collective behavior in statistical mechanics. The major quantities of information theory entropy H(P), cross entropy H(P,Q), and relative entropy (divergence) D(P || Q) \u2013 all emerge directly from the calculation, as the evident quantities of interest.\nThe calculation shows the degeneracy in observing the histogram Q- the \"macrostate\" of the n-sample dataset - from a particular \"microstate,\" i.e. the individual outcomes of each of the n samples. This was the idea that allowed physicists to quantify observable bulk properties of matter (macrostates) from unobservable configurations of each of its atoms (microstates).\nIn AI and data science, the system being studied (the \"matter\") is a dataset comprising n examples, whose state is the microstate. And the macrostate consists of our coarse-grained observations about the dataset, as we develop more in the following sections."}, {"title": "3 Enter entropy", "content": "In this calculation, the log-multinomial coefficient $\\log \\binom{n}{(n P_1)!...(n P_D)!}$ is $\\approx nH(P)$, with the approximation being very accurate for even moderate n. In fact, this is the only approximation that we have made in the calculation. How accurate is it?\nWe can quantify the relative probability, i.e. the exp-difference between the log-multinomial coefficient and nH(P) (Fig. 1). This shows that the approximation is very accurate (and that extreme accuracy is achieved when the first-order correction is applied), even for distributions over 50,000 outcomes, comparable to modern LLM token vocabularies.\nThis multinomial coefficient is the number of ways to get the same observed macrostate (the histogram Q) from particular microstates (the individual values of all the examples in the dataset). In other words, microstates can be counted in terms of the entropy of the observed macrostate. Putting all this together, we arrive at some powerful insights.\n\u2022 Entropy is a measure of the microscopic multiplicity, or degeneracy, associated with a set of limited macroscopic/average observations into the underlying microstate. High-entropy configurations are exponentially more likely than other configurations they dominate observed configurations for large n.\n\u2022 Therefore, the macrostate maximizing entropy is the \"most likely\" state. Entropy maximization accounts for the many microstates that are consistent with a set of macroscopic observations."}, {"title": "4 Learning: generalizing Boltzmann's scenario", "content": "Until this point, we have followed the clarifying insight of Boltzmann in looking only at discrete outcome spaces X, which has allowed us to do explicit probability calculations. Only a finite set of outcomes can happen there - in the AI context, it is like requiring each training sample to be a member of some discrete space. This is itself very useful in practice for models over text and similar discrete spaces.\nHowever, practical modern scenarios are also full of continuous spaces like $X = \\mathbb{R}^d$. This makes it impossible to assign probabilities pointwise and count their combinatorics discretely. How, then, can we quantify observations about the distribution P?\nFirst, we no longer observe a histogram Q over the discrete X, as Boltzmann did. Instead, the natural extension of the observed Q is the \"empirical measure\" $P_n$, which puts weight on any event according to the event's frequency over the n samples.\nAlso, each observation corresponds to a function of the outcome $f_i(x)$, which associates a real number to any outcome $x \\in X$. In AI / machine learning, this is a feature function we observe d of them. For any feature function $f_i$, our observation over the data is the empirical average over the sampling distribution $P_n$: $E_{x \\sim P_n}[f_i(x)]$. We observe $\\forall i$ that $E_{x \\sim P_n}[f_i(x)] = a_i$ for some $a_i \\in \\mathbb{R}$, i.e. that $P_n \\in A$, where\n$A := {P \\in \\Delta(X) : E_{x \\sim P}[f_i(x)] = a_i \\forall i = 1,...,d}$\nWith these concepts in mind, the situation is a natural extension of Boltzmann's calculations arising above for discrete X. The discrete Boltzmann scenario can be fully generalized, in a beautiful way that retains all the insights observed before."}, {"title": "5 Concentration: general \"probability calculations\"", "content": "The quantity we're interested in is still the probability of seeing the observations, just like in Boltzmann's case. In our new scenario with the generalized notation, this is\n$Pr (P_n \\in A)$\nThe behavior of this probability - or rather the normalized log probability of the observation $\\frac{1}{n} \\log Pr (P_n \\in A)$ - has been extensively bounded by a series of results (the theory of large deviations)."}, {"title": "5.1 The general calculation", "content": "Now we are in a new setting of general X, with the concepts of $P_n$, ${f_i(x)}_{i=1}^d$, A. For the first time we encounter an extremely important definition."}, {"title": "5.1.1 Information projection", "content": "The information projection of P on A is the distribution $P^*_A \\in A$ that is closest to P, according to divergence with P used as a prior.\n$P_A := \\arg \\min_{Q \\in A} D(Q || P)$\nThis is almost always the target distribution that we try to learn, changing X, A to suit the situation. $P^*_A$ is desirable for modeling data under observations A \u2013 in fact, it is essentially universal and unique because of its many favorable and fully general properties:\n\u2022 Admissibility: $P^*_A$ meets the constraints A.\n\u2022 Highest probability / \"likelihood\": $P^*_A$ is the distribution that is most likely to have generated the observed data.\n\u2022 Axiomatic justification: $P^*_A$ is essentially the only distribution that could be generating the data, in which measuring the data with the given features does not lose relevant information.\n\u2022 Robustness: $P^*_A$ is the most robust distribution to predict with, given the expected feature values across the dataset.\n\u2022 Convenience: It has many favorable properties for approximation of data, and for being conveniently learnable. Learning problems are typically convex, and the error decomposes readily in convenient ways.\nWe discuss all these properties in Section 7."}, {"title": "5.1.2 The result", "content": "For any set A that is an intersection of expected-value constraints as defined in Section 4, we can rewrite the probability of observing $P_n$ in A as\n$\\frac{1}{n} \\log \\text{Pr} (P_A) = -D(P_A || P) - \\frac{1}{n} D(\\mu_A || P_n)$\nwhere the conditional distribution $\\mu_A$ is the data $P_n$ conditioned on the empirical measure falling in A. We emphasize that this is an identity, not a bound so all concentration bounds in this setting are essentially approximating this one. It can be proved in great generality using only a short argument with basic techniques [Bal20].\nThe identity, only involving the fundamental quantities related to A, is bounded and approximated by a line of \"Sanov-type\" results, following the main result of Sanov [San57]:\nTheorem 1 (Sanov's Theorem).\n$\\lim_{n \\to \\infty} \\frac{1}{n} \\log \\text{Pr} (P_n \\in A) = - \\min_{Q \\in A} D(Q || P) = -D(P_A || P)$\nThis is the appropriate way of generalizing the probability calculation for finite X. Again, the message is that the dominant term in the log-probability is $\u2013D(P^*_A || P)$; note the similarities with Boltzmann's result for discrete X."}, {"title": "5.2 Discussion: the nature of concentration in A", "content": "Sanov's theorem is a very general result that is well known to describe many concentration phenomena and subsume many concentration inequalities. The relative entropy term ($-D(P^* || P)$) on the right-hand side typically dominates, and is the pivotal quantity studied by the theory of large deviations [Ell99].\nAs such a general statement, Sanov's theorem too has a long history out of the immediate scope here, dating back to the origins of information theory on bit-strings [CT06]. The modern literature [Tou09, DZ09] develops many other consequences and ideas of this kind."}, {"title": "5.2.1 Gibbs conditioning principle", "content": "In conjunction with the finite-n identity $\\frac{1}{n} \\log \\text{Pr} (P_n \\in A) = -D(P_A || P) - \\frac{1}{n} D(\\mu_A || P_n)$, we see that\n$\\lim_{n \\to \\infty} D(\\mu_A || P_n) = 0$\n(This fact can be shown independently [VCC81, Tju74, Csi84]. If it is taken alternatively as a starting point, it can be combined with the finite-n identity to prove Sanov's theorem!)\nSo the conditional distribution $\\mu_A$ behaves like $P_n$, as if it were n i.i.d. samples from $P^*_A$. This originates from the roots of statistical mechanics over a century ago [Gib02], often known as the Gibbs conditioning principle [LN02]. This is quite a profound idea \u2013 the approximating distribution $P_n$ completely removes the interdependences between the n samples of conditioning in $\\mu_A$.\nIn effect, we can \"pretend\" the data are i.i.d. generated from $P^*_A$, a topic we return to later in describing the properties of exponential families."}, {"title": "5.2.2 The impact of further information", "content": "We can actually go much further in outlining the role of $P^*_A$ in the tail probability of additional information, $B \\subseteq A$.\nIt is possible to prove a fully general formula for the relative probability of a subset $B \\subseteq A$ [Bal20]:\n$\\frac{1}{n} \\log \\text{Pr} (P_n \\in B | P_n \\in A) = \u2013 (D(\\mu_B || P_n) \u2013 D(\\mu_A || P_n))$\nThis shows the role of the n-sample exponential family $P_n$ in measuring the impact of further information $B \\subseteq A$ on the log-probability of an n-sample from P. The added information affects the probability in a way that depends on how much it affects the regret of predicting with $P_n$. Among product distributions when A is known, this regret is minimized by predicting with $P_n$; augmenting the knowledge to $B \\subseteq A$ only increases the regret."}, {"title": "6 Entropy: priors and perturbations", "content": "We have seen that maximum-entropy microstates have the highest multiplicity under the macroscopic observations, when the outcome space X is discrete. What is the probability of observing some other microstate of the system (dataset), even if it's a perturbation away from having maximal entropy?\nThe modern treatment of such \"fluctuations\" constitutes some of the foundations of statistical mechanics (see [Sch48, LL69]), typically attributed to Einstein [Ein10] and by him to Boltzmann. The same ideas are extremely useful in motivating the meaning of entropy."}, {"title": "6.1 The prior as a carrier measure", "content": "First, we need to discuss the role of the base \"carrier\" measure over the microstate space X. This is something we specify here, equivalent to the prior we put over our inferences. As [Gr\u00fc07] says, the carrier measure \"...represents the symmetries of the problem, which amounts to determining how outcomes should be counted.\" Reflecting this, there are more sophisticated and general group-theoretic arguments for how to encode ignorance in different spaces [Jay68].\nUltimately, this is a free and arbitrary choice, dictated by philosophy [Jay86] and the practical situation at hand. The choice matters to the performance of any downstream inference, in a way that's well-studied by information theory - the more the choice reflects the test-time reality, the better it performs at test-time inference.\nIn learning, this corresponds to choosing a prior, which evidently affects the loss. The learning literature is full of development of the relationship between the loss (often some version of log loss), the regularization, and the prior [Mur12]. Learning scenarios therefore use the prior flexibly, defining it differently for each situation. But it's useful to discuss a basic default option first, which shows how the prior determines \"how outcomes should be counted.\""}, {"title": "6.2 A uniform prior: entropy counts probabilities", "content": "Let's look at the consequences of making this measure uniform over known outcomes, as an expression of our prior indifference between them.\nWriting this prior as $P_0$, we can denote the resulting empirical measure and information projection as $U_n$ and $U_A$ respectively, and write using Sanov's theorem:\n$\\frac{1}{n} \\log \\text{Pr} (\\hat{U}_n \\in A) = \u2212D(U_A || P_0) \u2013 \\frac{1}{n} D(\\mu_A || U^*_n)$\n$\\leq \u2212D(U_A || P_0) = H(U_A) \u2013 H(U_A, P_0)$\nIn this case, H(x, P0) is the same for all x; call this value $\\ell(P_0)$. Then we have concluded that:\n$Pr (\\hat{U}_n \\in A) \\leq \\frac{e^{nH(U_A)}}{e^{n \\ell(P_0)}} \\propto \\exp (nH(U_A))$\nSo if we implicitly assume that the carrier measure over the data is uniform over the outcome space X, the log-probability of any macrostate A is determined by the entropy H(UA).\nThis was a very early discovery of Boltzmann in the context of physics, where the uniform carrier measure is typically justified by Liouville's theorem characterizing how physical systems evolve in \"phase space.\" Boltzmann was so pleased with it that he had it inscribed on his tombstone. Via Einstein [Ein10], it made its way into standard treatments of statistical mechanics [Sch48, LL69].\nIn learning scenarios, uniformity over the outcome space makes sense as well. It is often a default choice because of computational convenience, and a desire to avoid ruling out any regions of the outcome space with high enough n, commonly used learning procedures converge to the correct answer regardless of prior. And when the n constituents are i.i.d. sampled data, uniform weights are extremely natural. (However, learning scenarios also suggest non-uniform prior distributions, which generalize the applications of statistical mechanics.)\nIn short, both statistical mechanics and learning scenarios use a uniform prior in certain situations, for different reasons."}, {"title": "6.3 Fluctuations", "content": "Since A does not constrain any feature directly but only its average, we expect fluctuations in the observed features, and can precisely quantify them. In statistical physics, this has been studied for a long time on a basic level [LL69, Kar07], in the context of fluctuations in observed energy and other quantities.\nThe idea is that any system comprised of many separately observed units shows it upon even a bulk observation. The many teeming units lead to predictable probabilistic behavior, as predicted by statistical mechanics. When n is large, the fluctuations are negligible, and for gigantic $n \\sim 10^{23}$ as in physically observable systems, the fluctuations can be unobservably small. Physics handles this situation through many approximations, which only hold in the large-n limit. Statistical physics for small-n is typically confined to relatively exotic systems in the observable world. But learning scenarios are very different n could be any size.\nThere is no exact parallel between the loss-minimization / probability-theory scenario and the energy-based one of statistical physics. Energy is an observation which happens to have a privileged status in physics, compared to other observations like volume and particle number. In our view of statistical mechanics, the observations are the features $f_i$, which are all kept on the same footing. Each feature function corresponds to just one constraint, just like energy does SO any feature could be considered to play the role of energy.\nIn general, feature fluctuations happen with frequencies governed approximately by $P^*_A$; an extreme case of this is the uniform prior, as we have shown in Section 6.2. The exact picture is given by P, and makes exponential families important in general."}, {"title": "7 Learning: a prescription", "content": "In all this the core problem of concentration, and the natural role of entropy in counting combinations of microstates - we're motivated by learning the data distribution P from observations Q.\nIt's interesting to highlight some very differently motivated ways to proceed with this learning problem. It turns out that they are all equivalent, so they provide complementary perspectives that we'll describe, all of which amount to maximizing entropy in the correct context.\nThe maximum-entropy method can be viewed directly as a prescription for learning from data, in a straightforward scenario we've outlined previously in Section 4. To summarize the scenario:\n\u2022 We know the data space X, and we have a set of feature functions $f_i : X \\to \\mathbb{R}$, i = 1, . . ., d that we can observe over X.\n\u2022 We sample n elements from X using an unknown distribution P, giving an empirical measure $P_n$.\n\u2022 We observe the expected values of these features $f_i$ over some data distribution $P_n$, $E_{x \\sim P_n}[f_i(x)] = a_i$, i = 1,...,d. So $P_n \\in A$, where again remember that\n$A := {P \\in \\Delta(X): E_{x \\sim P}[f_i(x)] = a_i \\forall i = 1, ...,d}$\nTo model the data distribution P in this scenario given the observations $P_n \\in A$, we can view the extended Boltzmann calculation above in some more general ways for learning."}, {"title": "7.1 Some equivalent perspectives", "content": "It's useful to show some completely complementary perspectives on learning. They inevitably all suggest the same common inference method, but they seem on the surface to be very different from each other. Each illuminates a different aspect of learning, as we discuss in Section 7.2."}, {"title": "7.1.1 Prescription I: minimizing log loss in a model class", "content": "Start with a familiar justification for learning:\nPredict with a distribution Q that minimizes the log loss to the data $P_n$ (cross-entropy H($P_n$, Q)).\nSince $P_n$ is the empirical distribution of the data we have, we are trying to learn the distribution Q that best fits the data. If the data distribution $P_n$ is known to match P exactly, then the learning problem is:\n$\\min_{Q \\in \\Delta(X)} H(P_n, Q)$\nThe naive solution here is a trivial one: $Q = P_n$, recapitulating the empirical observations perfectly. Why is this trivial?\n\u2022 Generalization: In real situations, where X is continuous or high-dimensional, we cannot expect $P_n$ to totally generalize to P no two samples $P_n$ are exactly alike. This is a common and general situation we are faced with in machine learning.\n\u2022 Regularization: The typical solution is to guide the modeling by restricting Q within $\\Delta(X)$, with various powerful model classes available for Q like deep architectures. These build inductive bias into the modeling, in a way which we wish will generalize to test sets.\nSo in this formulation, something has to change; typically, we introduce some model assumptions on Q. By restricting Q to a still-expressive family of modeling distributions, we can hope that the eventually learned Q will generalize past the sample $P_n$ to P.\nWe will consider a common and universally used type of modeling distribution, where Q is a member of an exponential family, an easy-to-work-with distribution that uses ${f_i}_{i=1}^d$ and a prior P(x) over X. This means that Q is in the form $Q \\in Q$, where:\n$Q := {Q(x | \\lambda) \\in \\Delta(X) : \\exists \\lambda \\in \\mathbb{R}^d : Q(x | \\lambda) \\propto P(x) \\exp \\bigg( \\sum_{i=1}^d \\lambda_i f_i(x) \\bigg)}$\nDepending on the definitions of the features ${f_i}_{i=1}^d$, this can be extremely powerful and expressive. A choice of Q follows easily once ${f_i}_{i=1}^d$ is chosen. Now the task at hand is modified to include Q:\nPredict with a distribution Q that minimizes the log loss to the data $P_n$ (cross- entropy H($P_n$, Q)) within the appropriate exponential family model class Q.\nThis amounts to:\n$\\min_{Q \\in Q} H(P_n, Q)$\nwhich is a very standard and universal way of prescribing learning. This precisely describes unsupervised learning; supervised learning corresponds to using conditional distributions Pr(y|x) in this same formalism, with P(x) known. Other learning scenarios map similarly on to this framework, only changing the space X and model class Q.\nThis exponential family formalism will be central to us for many reasons that will be discussed - for one thing, the information projection $P^*_A$ is in Q. For now, it's enough to realize it as a model class that dictates the learning problem, in this prescription of learning.\nThis formulation can be interpreted as minimizing the description length of the model on the data. The minimum description length principle (philosophically, \"Occam's Razor\") has been useful for learning for a long time, with roots ranging from likelihood maximization to Kolmogorov complexity and compressibility [Gr\u00fc07]."}, {"title": "7.1.2 Prescription II: minimizing \"robust Bayes\" log loss", "content": "Another way to think about learning from limited observations is to accept that our observations about $P_n$ don't uniquely determine P. So what is our optimization function? The best we can do is an upper bound on loss, over distributions in A that satisfy our observations. This leads to another guiding justification for learning:\nPredict with a distribution Q that minimizes the log loss to the data-generating distribution P (cross-entropy H(P,Q)), given $P \\in A$ (i.e., given our observations about $P_n$ hold for P).\nThis amounts to behaving as if the objective function is $\\max_{P \\in A} H(P,Q)$, the upper bound on loss we have described. Therefore, the loss minimization problem faced by the learner becomes:\n$\\mathcal{V}_A := \\min_{Q \\in \\Delta(X)} \\max_{P \\in A} H(P,Q)$\nClearly, our loss minimization problem is $\\min_{Q \\in \\Delta(X)} H(P_n, Q) \\leq \\mathcal{V}_A$, so $\\mathcal{V}_A$ is a tight bound on our log loss. This is known as a \"robust Bayes\" approach [GD04]. The optimal Q here is not trivial, depending sensitively on the structure of A.\nInformation theory studies these ideas with a slightly different focus, relating similar robustness concepts in information and coding theory. What robust Bayes and statistical mechanics aim to optimize is the worst-case loss, which is deeply related to the information-theory notion of redundancy of a communication channel. A milestone theorem of information theory says that this is equal to the capacity of the channel - the \"redundancy-capacity theorem\" [MF95, Hau97]."}, {"title": "7.1.3 Prescription III: maximizing probability of the observations", "content": "An intuitively appealing principle guiding modeling is to predict with the distribution that makes the data most likely.\nThis is the principle behind maximum-likelihood modeling, but there the modeling restrictions are expressed in terms of a model class. In our situation, we are instead guided by the information A restricting the data. We can formulate the learning principle directly:\nPredict with a distribution that maximizes the probability of the given observa- tions, i.e. a distribution $P_n$ maximizing $\\log \\text{Pr} (P_n \\in A)$.\nWe have seen that this amounts to solving\n$\\min_{Q \\in A} D(Q|P)$\nbecause of Sanov-type behavior (Section 5). The KL divergence, which emerges from Boltzmann's calculation as intimately linked to probability, is the objective function for learning here."}, {"title": "7.2 Equivalence of these prescriptions", "content": "All these perspectives lead to exactly the same learning problem! This is the key observation of this section.\nWe show this by showing that the commonly used log loss minimization with Q is exactly following probability maximization, then showing that robust Bayes and probability maximization each require maximizing entropy in the same manner."}, {"title": "7.2.1 Probability maximization < Log loss minimization with Q", "content": "The probability calculation-based learning problem is equivalent to the log loss minimization with the exponential family model class.\nStarting with the latter, the log loss minimization problem is:\n$\\arg \\min_{Q \\in Q} H(P_n, Q)$\nIt is clear because of the definition of divergence $D(P_n||Q) = H(P_n, Q)-H(P_n)$ that: $\\arg \\min_{Q \\in Q} H(P_n, Q) = \\arg \\min_{Q \\in Q} D(P_n||Q)$.\nFinally, we use the fact that\n$\\arg \\min_{Q \\in Q} D (P_n ||Q) = \\arg \\min_{Q \\in A} D(Q||P)$\nwhich we prove later in Section 10.3. The right-hand side $\\arg \\min_{Q \\in A} D (Q||P)$ is exactly the probability calculation.\nThis shows the equivalence of log loss minimization over Q and probability maximization."}, {"title": "7.2.2 Probability maximization \u2192 Maximum entropy", "content": "The probability-maximization principle is equivalent to maximizing entropy under a uniform prior.\nAs we have seen, a uniform distribution over X is a common and intuitive choice of prior (Section 6.2). Writing this prior as $P_0$, the learning problem is\n$\\min D(P||P_0) = \\min [H(P, P_0) \u2013 H(P)] \\leftrightarrow \\max H(P)$\nbecause H(P, P0) is the same for all P. This key property of the uniform prior makes it the special, unique prior under which max-entropy is equivalent to the probability calculation."}, {"title": "7.2.3 Robust Bayes \u2192 Maximum entropy", "content": "The robust Bayes prescription for learning is to solve\n$\\mathcal{V"}, "A := \\min_{Q \\in \\Delta("]}