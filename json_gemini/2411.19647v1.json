{"title": "CADAM: CONFIDENCE-BASED OPTIMIZATION FOR ONLINE LEARNING", "authors": ["Shaowen Wang", "Anan Liu", "Jian Xiao", "Huan Liu", "Yuekui Yang", "Cong Xu", "Qianqian Pu", "Suncong Zheng", "Wei Zhang", "Jian Li"], "abstract": "Modern recommendation systems frequently employ online learning to dynamically update their models with freshly collected data. The most commonly used optimizer for updating neural networks in these contexts is the Adam optimizer, which integrates momentum (mt) and adaptive learning rate (vt). However, the volatile nature of online learning data, characterized by its frequent distribution shifts and presence of noises, poses significant challenges to Adam's standard optimization process: (1) Adam may use outdated momentum and the average of squared gradients, resulting in slower adaptation to distribution changes, and (2) Adam's performance is adversely affected by data noise. To mitigate these issues, we introduce CAdam, a confidence-based optimization strategy that assesses the consistence between the momentum and the gradient for each parameter dimension before deciding on updates. If momentum and gradient are in sync, CAdam proceeds with parameter updates according to Adam's original formulation; if not, it temporarily withholds updates and monitors potential shifts in data distribution in subsequent iterations. This method allows CAdam to distinguish between the true distributional shifts and mere noise, and adapt more quickly to new data distributions. Our experiments with both synthetic and real-world datasets demonstrate that CAdam surpasses other well-known optimizers, including the original Adam, in efficiency and noise robustness. Furthermore, in large-scale A/B testing within a live recommendation system, CAdam significantly enhances model performance compared to Adam, leading to substantial increases in the system's gross merchandise volume (GMV).", "sections": [{"title": "1 INTRODUCTION", "content": "Modern recommendation systems, such as those used in online advertising platforms, rely on online learning to update real-time models with freshly collected data batches (Ko et al., 2022). In online learning, models continuously adapt to users' interests and preferences based on immediate user interactions like clicks or conversions. Unlike traditional offline training-where data is pre-collected and static online learning deals with streaming data that is often noisy and subject to frequent distribution changes. This streaming nature makes it challenging to effectively denoise and reorganize training samples (Su et al., 2024; Zhang et al., 2021).\nA widely adopted optimizer in these systems is the Adam optimizer (Kingma & Ba, 2015), which combines the strengths of parameter-adaptive methods and momentum-based methods. Adam adjusts learning rates based on the averaged gradient square norm ($v_t$) and incorporates momentum ($m_t$) for faster convergence. Its ability to maintain stable and efficient convergence by dynamically adjusting learning rates based on the first and second moments of gradients has made it a reliable"}, {"title": "2 RELATED WORK", "content": "Adam Extensions Adam is one of the most widely used optimizers, and researchers have proposed various modifications to address its limitations. AMSGrad (Reddi et al., 2018) addresses Adam's non-convergence issue by introducing a maximum operation in the denominator of the update rule. RAdam (Liu et al., 2019) incorporates a rectification term to reduce the variance caused by adaptive learning rates in the early stages of training, effectively combining the benefits of both adaptive and non-adaptive methods. AdamW (Loshchilov, 2017) separates weight decay from the"}, {"title": "3 DETAILS OF CADAM OPTIMIZER", "content": "Notations We use the following notations for the CAdam optimizer:\n\u2022 $f(\\theta) \\in \\mathbb{R}, \\theta \\in \\mathbb{R}^d$: $f$ is the stochastic objective function to minimize, where $\\theta$ is the parameter vector in $\\mathbb{R}^d$.\n\u2022 $g_t$: the gradient at step $t$, $g_t = \\nabla_{\\theta_{t-1}} f(\\theta_{t-1})$.\n\u2022 $m_t$: exponential moving average (EMA) of $g_t$, calculated as $m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$.\n\u2022 $v_t$: EMA of the squared gradients, given by $v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$.\n\u2022 $\\hat{m}_t, \\hat{v}_t$: bias-corrected estimates of $m_t$ and $v_t$, respectively, where $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$ and $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$.\n\u2022 $\\alpha, \\epsilon$: $\\alpha$ is the learning rate, typically set to $10^{-3}$, and $\\epsilon$ is a small constant to prevent division by zero, typically set to $10^{-8}$.\n\u2022 $\\beta_1, \\beta_2$: smoothing parameters, commonly set as $\\beta_1 = 0.9$, $\\beta_2 = 0.999$.\n\u2022 $\\theta_t$: the parameter vector at step $t$.\n\u2022 $\\theta_0$: the initial parameter vector."}, {"title": "4 EXPERIMENT", "content": "In this section, we systematically evaluate the performance of CAdam across various scenarios, starting with synthetic image data, followed by tests on a public advertisement dataset, and concluding with A/B tests in a real-world recommendation system. We first examine CAdam's behaviour under distribution shift, and noisy conditions using the CIFAR-10 dataset(Krizhevsky et al., 2009) with the VGG network(Simonyan & Zisserman, 2014). Next, we test CAdam against other popular optimizers on the Criteo dataset(Jean-Baptiste Tien, 2014), focusing on different models and scenarios. Finally, we conduct A/B tests with millions of users in a real-world recommendation system to validate CAdam's effectiveness in large-scale, production-level environments. The results demonstrate that CAdam consistently outperforms Adam and other optimizers across different tasks, distribution shifts, and noise conditions."}, {"title": "4.1 NUMERICAL EXPERIMENT", "content": "Distribution Change To illustrate the different behaviours of Adam and CAdam under distribution shifts, we designed three types of distribution changes for both L1 and L2 loss functions: (1) Sudden change, where the minimum shifts abruptly at regular intervals; (2) Linear change, where"}, {"title": "5 CONCLUSION", "content": "In this paper, we addressed the inherent limitations of the Adam optimizer in online learning environments, particularly its sluggish adaptation to distributional shifts and heightened sensitivity to noisy data. To overcome these challenges, we introduced CAdam (Confidence Adaptive Moment Estimation), a novel optimization strategy that enhances Adam by incorporating a confidence-based mechanism. This mechanism evaluates the alignment between momentum and gradients for each parameter dimension, ensuring that updates are performed judiciously. When momentum and gradients are aligned, CAdam updates the parameters following Adam's original formulation; otherwise, it temporarily withholds updates to discern between true distribution shifts and transient noise.\nOur extensive experiments across synthetic benchmarks, public advertisement datasets, and large-scale real-world recommendation systems consistently demonstrated that CAdam outperforms Adam and other well-established optimizers in both adaptability and robustness. Specifically, CAdam showed superior performance in scenarios with sudden and continuous distribution shifts, as well as in environments with significant noise, achieving higher accuracy and lower regret. Moreover, in live A/B testing within a production recommendation system, CAdam led to substantial improvements in model performance and gross merchandise volume (GMV), underscoring its practical effectiveness.\nFuture work may explore further refinements of the confidence assessment mechanism, its integration with other optimization frameworks, and its application to a broader range of machine learning models and real-time systems. Ultimately, CAdam represents a promising advancement in the development of more resilient and adaptive optimization algorithms for dynamic learning environments."}, {"title": "A PROOFS OF THEOREM 1", "content": "Given a stream of objectives $f_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, $t = 1, 2, . . ., T$, online learning aims to minimize the regret w.r.t. the optimum; that is,\n$R_T := \\sum_{t=1}^T f_t(x_t) - \\sum_{t=1}^T f_t(x^*), \\quad x^* = \\underset{x}{\\text{argmin}} \\sum_{t=1}^T f_t(x)$.\nRecall that each update in CAdam can be characterized as follows:\n$m_t = \\beta_{1,t} m_{t-1} + (1 - \\beta_{1,t}) g_t$,\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$,\n$\\hat{m}_t = \\begin{cases} m_{t,\\Xi_t}, & i \\in \\Xi_t \\\\ 0, & \\text{else} \\end{cases}$,\n$\\hat{v}_t = \\text{max}(\\hat{v}_{t-1}, v_t)$,\n$x_{t+1} = x_t - \\alpha_t \\hat{m}_t / \\sqrt{\\hat{v}_t}$,\nwhere $\\Xi_t := \\{i \\in [d]: m_{t,i} g_{t,i} \\geq 0\\}$ indicates the set of active entries at step $t$. For notation clarity, let $x_{t,\\Xi_t}$ be the vector of which the entries not belonging to $\\Xi$ are masked. Following the AMSGrad (Reddi et al., 2018), we are to prove that the sequence of points obtained by CAdam satisfies $R_T / T \\rightarrow 0$ as $T$ increases.\nWe first introduce three standard assumptions:\nAssumption 1. Let $f_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, $t = 1,2,...,T$ be a sequence of convex and differentiable functions with $\\|\\nabla f_t(x)\\|_{\\infty} < G_{\\infty}$ for all $t \\in [T]$.\nAssumption 2. Let $\\{m_t\\}$, $\\{v_t\\}$ be the sequences used in CAdam, $\\alpha_t = \\alpha / \\sqrt{t}$, $\\beta_{1,t} = \\frac{\\beta_1}{1 - \\beta_1 x^{t-1}} < 1, \\gamma = \\beta_1 / \\sqrt{\\beta_2} < 1$ for all $t \\in [T]$.\nAssumption 3. The points involved are within a bounded diameter $D_{\\infty}$; that is, for the optimal point $x^*$ and any points $x_t$ generated by CAdam, it holds $\\|x_t - x^*\\|_{\\infty} < D_{\\infty} / 2$.\nWe present several essential lemmas in the following. Given that some of these lemmas have been partially established in prior works (Kingma & Ba, 2015; Reddi et al., 2018), we include them here for the sake of completeness.\nLemma 1. For a convex and differentiable function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, we have\n$f(x) - f(y) \\leq \\langle \\nabla f(x), x - y \\rangle$.\nLemma 2. Under Assumption 1 and 2, we have\n$\\langle g_{t,\\Xi_t}, x_{t,\\Xi_t} - x^* \\rangle \\leq \\frac{1}{2 \\alpha_t (1 - \\beta_{1,t})} \\left( \\|V_t^{1/4} (x_{t,\\Xi_t} - x^*)\\|^2 - \\|V_{t+1,\\Xi_t}^{1/4} (x_{t+1,\\Xi_t} - x^*)\\|^2 \\right) + \\frac{\\beta_1}{1 - \\beta_1} \\langle \\sqrt{v_t}, \\hat{m}_{t-1} \\sqrt{v_t} \\rangle + 2 \\frac{G_{\\infty}}{V_t} \\|V_t^{1/4} \\langle x_{t,\\Xi_t} - x^* \\rangle\\|.$\nwhere $V_t := \\text{diag}(\\hat{v}_t)$.\nProof. CAdam updates the parameters as follows\n$x_{t+1,\\Xi_t} = x_{t,\\Xi_t} - \\alpha_t \\frac{\\hat{m}_{t,\\Xi_t}}{\\sqrt{\\hat{v}_t}} = x_{t,\\Xi_t} - \\alpha_t \\hat{v}_t^{-1/2} (\\beta_{1,t} m_{t-1,\\Xi_t} + (1 - \\beta_{1,t}) g_{t,\\Xi_t})$.\nSubtracting $x^*$ from both sides yields\n$\\|V^{1/4} (x_{t+1,\\Xi_t} - x^*) \\|^2_2 = \\|V^{1/4} (x_{t,\\Xi_t} - x^*) - \\alpha_t V^{-1/4} \\hat{m}_{t,\\Xi_t} \\|^2_2$\n$\\qquad = \\|V^{1/4} (x_{t,\\Xi_t} - x^*)\\|^2 - 2 \\alpha_t \\langle V^{-1/4} \\hat{m}_{t,\\Xi_t}, V^{1/4} (x_{t,\\Xi_t} - x^*) \\rangle + \\|\\alpha_t V^{-1/4} \\hat{m}_{t,\\Xi_t} \\|^2$\n$\\qquad = \\|V^{1/4} (x_{t,\\Xi_t} - x^*)\\|^2 - 2 \\alpha_t \\langle \\beta_{1,t} m_{t-1,\\Xi_t} + (1 - \\beta_{1,t}) g_{t,\\Xi_t}, x_{t,\\Xi_t} - x^* \\rangle + \\|\\alpha_t V^{-1/4} \\hat{m}_{t,\\Xi_t} \\|^2$."}]}