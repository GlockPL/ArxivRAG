{"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training", "authors": ["Ameera Bawazir", "Kebin Wu", "Wenbin Li"], "abstract": "Recent advancements in vision-language pre-training via contrastive learning have significantly improved performance across computer vision tasks. However, in the medical domain, obtaining multimodal data is often costly and challenging due to privacy, sensitivity, and annotation complexity. To mitigate data scarcity while boosting model performance, we introduce Uni-Mlip, a unified self-supervision framework specifically designed to enhance medical vision-language pre-training. Uni-Mlip seamlessly integrates cross-modality, uni-modality, and fused-modality self-supervision techniques at the data-level and the feature-level. Additionally, Uni-Mlip tailors uni-modal image self-supervision to accommodate the unique characteristics of medical images. Our experiments across datasets of varying scales demonstrate that Uni-Mlip significantly surpasses current state-of-the-art methods in three key downstream tasks: image-text retrieval, image classification, and visual question answering (VQA).", "sections": [{"title": "Introduction (Uni-Mlip)", "content": "Vision-and-Language Pre-training (VLP) techniques, such as the Contrastive Language Image Pre-training (CLIP) model [35], have set a foundational approach for leveraging self-supervision with language guidance to integrate visual and textual data effectively. By aligning image and text representations through contrastive learning, CLIP improves the model's ability to interpret complex data by linking visual concepts to linguistic descriptions. This method significantly enhances the understanding of multimodal representations by pre-training on extensive datasets and subsequently fine-tuning on targeted downstream tasks. This paradigm shift is particularly vital in the medical domain, where the acquisition of multimodal medical data presents significant challenges due to concerns over data privacy, sensitivity, and the complex, domain-specific knowledge required for annotation.\nMedical Vision-and-Language Pre-training (Med-VLP) aims to address such challenges inherent to the medical imaging field. The exploration of self-supervised learning (SSL) techniques in medical VLP, as seen in models like ConVIRT [45] and GLORIA [17], highlight the shift towards leveraging unlabelled data, indicating a growing emphasis on models"}, {"title": "2 Related work", "content": "The CLIP model [35] pioneers vision-language pre-training by aligning image-text pairs by maximizing the similarity between image and text features in a shared latent space. Subsequent models build upon CLIP by enhancing learning objectives [42, 43], modifying pre-training architecture [20, 39], and expanding downstream tasks [25, 44], which allow for better representation learning and widen the scope of vision-language pre-training.\nMedical Vision-and-Language Pre-Training (Med-VLP). The models above optimized for natural images often fall short in medical settings: medical images necessitate a meticulous focus on fine-grained details, and critical diagnostic information is often embedded in the absolute pixel intensity, the scale of abnormalities, and their precise locations within the image. Hence, domain-specific Med-VLPs have been proposed to overcome these challenges. Medical VLP models such as GLORIA [16], LoVT [32], and PRIOR [5] introduced global-local image-text alignment, enabling the learned models to capture fine-grained information lying in the medical images. PMC-CLIP [27] presents a CLIP-like model architecture that is trained on a large-scale dataset of medical image-caption pairs, highlighting the critical role of extensive datasets in effective pre-training. A significant contribution of their work is the introduction of PMC-OA, a newly curated dataset of image-caption pairs sourced from biomedical literature available on PubMedCentral's OpenAccess.\nSelf-Supervised learning (SSL). Self-Supervised Learning [1, 2, 10] addresses the issue of data annotation scarcity by training models to learn meaningful representations with unlabelled data. For natural images, adding traditional SSL to the CLIP framework for joint training can significantly improve the performance, whereas the study in [15, 31] found the opposite when directly trained on medical images. Firstly, the inherent assumptions of conventional SSL, i.e., image augmentations do not alter the semantic information of an image, may not hold when applied to medical images. This requires careful adaptation of augmentations to maintain the diagnostic integrity of the image. Secondly, conventional SSL generally applies augmentation techniques to the input space only. We argue the feature-level"}, {"title": "3 Uni-Mlip model", "content": "In this section, we present a comprehensive overview of our proposed methodology Uni-Mlip, encompassing a detail explanation of our model architecture and pre-training objectives. As shown in Figure 1, in our pre-training model design, we follow a CLIP-like architecture that separately encodes each modality with a modality-specific encoder, and then jointly optimize both of them through image-text contrastive (ITC) loss, ensuring an alignment of image and text embedding. Let V be an input image, and C be the corresponding caption, given a batch of image-caption pairs $(V_1,C_1),....,(V_B,C_B)$, where $V_i \\in R^{H\\times W \\times C}$ is the i-th image and $C_i$ is the paired caption, we encode the input image via the image encoder $E_V$ and the input text via the text encoder $E_T$ to get the image and text features respectively. These features are then projected to a shared embedding space and normalized to get the final image embedding $I \\in R^{B\\times D}$ and text embeddings $T \\in R^{B\\times D}$, where B is the batch size and D is the shared embedding dimension.\nIn Uni-Mlip, we integrate several complementary Self-Supervised Learning (SSL) techniques into the architecture reminiscent of CLIP. This involves incorporating cross-modality SSL at both the input and feature levels to align the embedding of images and text. Additionally, we introduce fused-modal and uni-modal SSL for text and images respectively to enhance robustness even further."}, {"title": "3.1 Cross-modal input level self-supervision", "content": "We perform an input level SSL by contrasting the image and text embeddings using the Image-Text Contrastive (ITC) loss from CLIP [35]. The ITC loss is defined as a sum of image-to-text $L_{I2T}$ and text-to-image $L_{T2I}$ NT-Xent losses [37], which is formulated as:\n$L_{ITC} = \\frac{1}{2B} \\sum_{i=1}^{B} log \\frac{e^{sim(I_i, T_i) / \\tau}}{\\sum_{j=1}^{B} e^{sim(I_i, T_j) / \\tau}} +  \\frac{1}{2B} \\sum_{i=1}^{B} log \\frac{e^{sim(T_i, I_i) / \\tau}}{\\sum_{j=1}^{B} e^{sim(T_i, I_j) / \\tau}}$\nThe goal of the ITC loss is to maximize the similarity between the embedding of the B image-text pairs i.e., $(I_i, T_i)$ while minimizing the similarity with the rest of the $\\frac{B^2}{B}$ \u2013 B non-pair samples i.e., $(I_j, T_i)$ in the batch, where $i \\neq j$. The temperature $\\tau$ is a learnable parameter for scaling the measured similarity. The image embedding I and text embedding T are obtained via the image and text encoder, and sim is the cosine similarity metric."}, {"title": "3.2 Cross-modal feature level self-supervision", "content": "To further boost the cross-modality alignment, we also introduce a feature-level self-supervision by performing a feature perturbation. The motivation is that image/text features after some minor perturbations are expected to remain semantically aligned with the corresponding pair of text/image embeddings, ensuring robust and better representation learning. The feature perturbation can be implemented by a simple Dropout [14] or its variant such as DropBlock [9]. Perturbation applied to the image features is done after the last residual layer and before the attention pooling layer in the vision encoder, where a DropBlock method is utilized. On the other hand, to generate the perturbed text features, a dropout is applied after getting the"}, {"title": "3.3 Uni-modal self supervision - image", "content": "Based on the observation [31] that combining image-only self-supervised learning with language supervision can effectively improve the representation quality of the images, we move one step further and add uni-modal, image-only or text-only, self-supervision on top of cross-modal supervision. While reconstructing masked images is a popular approach for natural images [13], this method may be less suitable for medical imaging. This is because medical images often depict localized anomalies associated with diseases, and masking these parts could hinder the model's ability to accurately represent these localized anomalies. Thus, we adopt the contrastive self-supervised learning objective from SimCLR [2], which enables better capturing of subtle patterns and anomalies in medical images.\nGiven a batch size B of images, we create two strongly augmented views for each image. Then the uni-modal contrastive loss $L_{12I}$ is defined in Eq. 2 to maximize the similarity between the embedding of the positive pairs (views of the same image) while minimizing the similarity between the negative pairs (views of different images). Such loss term promotes the model to learn semantically robust representations for images with different augmentations.\n$L_{I2I} =  \\frac{1}{2B} \\sum_{i=1}^{B} log \\frac{e^{sim(I_i^a, I_i^b) / \\tau}}{\\sum_{j=1}^{B} e^{sim(I_i^a, I_j^b) / \\tau}} +  \\frac{1}{2B} \\sum_{i=1}^{B} log \\frac{e^{sim(I_i^b, I_i^a) / \\tau}}{\\sum_{j=1}^{B} e^{sim(I_j^b, I_i^a) / \\tau}}$\nwhere $I_i^a$ is the feature embedding of the augmented image $V_i^a$, and $I_i^b$ is the feature embedding of the augmented image $V_i^b$.\nWhile SLIP [31] empirically demonstrates SimCLR's enhancement of vision-language pre-training with natural images, simply adding Eq. 2 worsens the CLIP performance in the medical domain [15]. One reason may be the fundamental differences in characteristics between natural and medical images. Unlike natural images, for which relative intensity information is key in recognition, medical images rely heavily on precise absolute intensity values. When integrating SimCLR into CLIP-like architecture, the same vision encoder will process three different views for each input image: one weakly augmented view used for CLIP, and two views with strong augmentation for SimCLR. When the vision encoder, such as a ResNet, includes batch normalization layers, strong image augmentations will significantly alter the mean and variance statistics in batch normalization layers, harming the intensity information critical for medical images. To address this issue, we propose to train without SimCLR for some epochs first, and then we freeze the mean and variance in the batch normalization layers while keeping the weight and bias as learnable parameters when incorporating the SimCLR loss (Eq. 2). This approach ensures that strong augmentations introduced by SimCLR will not alter the mean and variance in batch normalization layers, al-"}, {"title": "3.4 Fused-modal self-supervision", "content": "To enhance text representation, we integrate fused-modal self-supervision for the textual modality. Motivated by MMBERT [22] and PMC-CLIP [27], we utilize masked language modeling (MLM) self-supervision task for the input captions. In masked language modeling, the model predicts the masked text token by learning from the medical text features while using the image feature as a supplementary context. Before being fed to the text encoder, the captions are masked randomly with a probability of 15%. Following BERT [6], our masked tokens are replaced with a unique token [MASK]. After that, the visual and masked linguistic features obtained from the image and text encoder are fused by a multimodal transformer-based fusion module. The fusion module generates the full caption by replacing the [mask] token with the predicted text token. Hence the MLM loss is defined as:\n$L_{MLM} = E_{(V, C) \\sim D}[CE(y_{mask}, p_{mask}(V, C)]$\nwhere $y_{mask}$ denotes the ground truth and $p_{mask}$ is the predicted masked token generated by the fusion module. The masked language modeling objective enables the model to focus on learning fine-grained details from the medical text to improve the representation and understanding of language-specific information."}, {"title": "3.5 Total training loss", "content": "The total loss is a weighted sum of all of the individual cross-modal, uni-modal and fused-modal losses and therefore is formulated as:\n$L_{Total} = \\lambda_{cm} \u00b7 (L_{ITC} + L_{\\hat{I}TC} + L_{I\\hat{T}C}) + \\lambda_{um} \u00b7 (L_{I2I}) + \\lambda_{fm} \u00b7 (L_{MLM})$\nwhere $\\lambda_{cm}, \\lambda_{um}, and \\lambda_{fm}$ refer to the weight value assigned to the cross-modal, uni-modal, and fused-modal losses respectively during training."}, {"title": "4 Experiment settings", "content": "We pre-train the Uni-Mlip on a large-scale dataset and then evaluate it on three downstream tasks. We provide the dataset and implementation details below."}, {"title": "4.1 Dataset", "content": "For pre-training, we use PMC-OA [27], a vast open-source medical image-caption dataset comprising 1.65M pairs sourced from PubMedCentral's OpenAccess.\nFor image-text retrieval evaluation, including both image-to-text (I2T) and text-to-image (T2I) retrieval tasks, we employ the Radiology Objects in COntext (ROCO) dataset [34], where 2000 samples are randomly selected in evaluation following [3, 27, 38]. As there is no overlapping between ROCO and PMC-OA, such a task also serves as a zero-shot evaluation for Uni-Mlip. For classification, we assess our model on three datasets: NIH [41], CheXpert [19] and MIMIC [21], consisting of 112,120, 224,316, and 377,110 chest X-ray images"}, {"title": "4.2 Implementation details", "content": "Implementation Details of Model Pre-training. Following [27], we adopt a modified ResNet50 [12] architecture as a vision encoder $E_V$, and PubMedBERT [11] as our text encoder $E_T$. Our fusion module employs four transformer layers to concatenate image and text embedding. Perturbation applied to image features uses DropBlock [9] with a 50% probability with a block size of 3x3, while perturbation on text features are carried out by using dropout [14] with a 75% probability. For the image-only self-supervision branch, the implementation of the SimCLR [2] augmentation includes color jittering, random gray-scale conversion, random Gaussian blurring, and horizontal flipping. All images are resized to 224x224 and trained with a batch size of 768 across 8 NVIDIA V100 GPUs for 100 epochs. The training employs a learning rate of 1e-4, with cosine scheduling and AdamW optimizer. For the training loss weights, we set $\\lambda_{cm} =  \\frac{3}{6} = 0.167, \\lambda_{um} = 0.5, and \\lambda_{fm} = 0.5$ as default values, which are empirically determined.\nAs discussed in Section 3.3, freezing batch normalization is required to incorporate image-only self-supervision into the framework. Therefore, we initialize the three primary components\u2014vision encoder, text encoder, and fusion module\u2014with pre-trained weights from the 100th epoch of the PMC-CLIP model and freeze the mean and variance in the batch normalization layers during subsequent training. To ensure a fair comparison with PMC-CLIP, we re-implement PMC-CLIP by extending the model training to 200 epochs, thus doubling the original duration while retaining identical hyper-parameters.\nImplementation Details of Downstream Tasks. For cross-modal retrieval, we randomly select 2000 test image-text pairs following [3, 27] and resize the images to 224x224. For image-to-text retrieval, we calculate the inner product between the image feature and each text feature in the test set. The text with the highest inner product value is deemed the predicted text label for the image. The recall is then computed for the top 1, 5, and 10 captions. The text-to-image retrieval process is similar.\nRegarding the classification task, we follow [36] for dataset splits to mitigate comparison bias. With the pre-trained modified ResNet50 in Uni-Mlip, we add a linear classification layer on top for predictions. Input images are resized to 224x224, with batch sizes of 190 for CheXpert, and 150 for MIMIC and NIH. We perform end-to-end fine-tuning with binary cross-entropy loss and Adam optimizer, and the training is up to 64 epochs with early stopping criteria. The performance is evaluated with the macro Area Under the ROC Curve (AUC) metric.\nFor visual question answering (VQA), we cast the VQA as a classification task following the works in [3] and [7]. Instead of appending lots of new self-attention layers as in [3] and [7] to fuse the vision and text representations, we simplify the model framework with four components: a vision encoder, a text encoder, a one-layer transformer to fuse vision and text features, and a two-layer MLP to serve as the classification head. In addition, we append 20 learnable tokens in encoding questions, which can improve the representation power of text embedding effectively. The vision and text encoders are initialized with our pre-trained models and then trained with a learning rate of 5e-5. Meanwhile, the fusion and classification modules are randomly initialized and trained with a higher rate of 5e-4."}, {"title": "5 Empirical results", "content": "In this section, we quantitatively demonstrate Uni-Mlip's superiority in image-text retrieval, image classification, and VQA, and provide comprehensive ablation studies to highlight the effectiveness of each component in our unified self-supervised framework."}, {"title": "5.1 Downstream task results", "content": "Image-Text Retrieval For image-text retrieval, we compare our results with ViTL[23], and METER [7], M3AE [3], ARL [4], and our own reproduction of PMC-CLIP [27]. As shown in Figure 2, Uni-Mlip consistently outperforms the others under both image-to-text and text-to-image tasks across Recall@1, Recall@5, and Recall@10 metrics. In particular, in image-to-text retrieval, Uni-Mlip surpasses the previous state-of-the-art by 3.17% in Recall@10 and achieves a 2.76% improvement in Recall@5. Similarly, in text-to-image retrieval, the model outperforms PMC-CLIP with gains of 1.46% in Recall@5 and 1.54% in Recall@10.This task, regarded as a zero-shot evaluation, shows that our method with cross-modality, uni-modality, and fused-modality SSL objectives at both the input and feature levels achieves superior alignment between medical image and text, facilitating better retrieval performance.\nMedical Image Classification For image classification, we compare the performance of our solution with previous state-of-the-art medical vision-language models. As shown from Table 1, the AUC scores indicate that Uni-Mlip achieves the best results for different data splits (1%, 10%, and 100%) on MIMIC [21], CXP [19] and NIH [41] datasets. Notably, when using 10% of the data, Uni-Mlip achieves an AUC of 79.1% on MIMIC, a gain of 1.7% over the previous best performance of 77.41% by PMC-CLIP. Our model consistently outperforms baselines that do not employ image uni-modal self-supervision, which further validates the importance of incorporating uni-modal image self-supervision to improve the visual representations, resulting in a better performance on medical image classification tasks.\nMedical Visual Question Answering For the VQA task, we report each dataset's open-ended, close-ended, and overall accuracy. Table 2 demonstrates the superiority of Uni-Mlip in the overall evaluations. Moreover, Uni-Mlip gains consistently on closed-ended VQA with substantial improvement of 1.48% on VQA-RAD and 1.44 % on Slake over PMC-CLIP. Hence, Uni-Mlip excels at leveraging medical text and image data to answer visual questions accurately."}, {"title": "5.2 Ablation studies", "content": "To accelerate our analysis and reduce computational demands, we conducted ablation experiments using the ROCO dataset [34] instead of PMC-OA for pre-training, focusing on image-text retrieval tasks. Performance evaluation was conducted on both image-to-text (I2T) and text-to-image (T2I) retrieval tasks, reporting the top 1 recall value.\nIn Table 3, we illustrate the impact of freezing the mean and variance in the batch normalization layer when integrating SimCLR into the CLIP-based model for medical data. Consistently, V represents the input image, while Va and Vb depict strongly augmented images used in SimCLR. Notably, the second row reveals a significant performance drop upon the naive addition of SimCLR, consistent with the observation in [15]. We further analyze this by feeding the augmented images in the forward pass while setting the weight of $L_{12I}$ loss to zero (third row). The still low performance reveals that the drop is due to feeding strongly augmented images in the forward pass. Consequently, we propose freezing batch statistics in batch normalization layers when incorporating the image-only SSL loss $L_{12I}$, resulting in a noticeable improvement over the baseline as shown in the last row. This study emphasizes the importance of freezing batch normalization when integrating the SimCLR"}, {"title": "6 Conclusion", "content": "We introduce Uni-Mlip in this paper, a unified self-supervision framework tailored for enhancing medical vision-language pre-training. Our simple yet effective approach proves the advantage of integrating cross-modality, uni-modality, and fused-modality self-supervision techniques. Additionally, our framework also unifies self-supervision from both the input level and the feature level. Moreover, our technique of freezing the mean and variance in the batch normalization layer is shown to be essential for integrating image-only contrastive learning in VLP on medical data.\nExtensive experiments validate the effectiveness of our method, as it achieves SOTA performance across various downstream medical tasks. Uni-Mlip not only holds promise for advancing vision-language pre-training in clinical applications but also presents potential benefits for domains characterized by limited multimodal data, such as satellite and agriculture imagery analysis."}]}