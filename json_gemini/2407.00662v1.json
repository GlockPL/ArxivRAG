{"title": "Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach", "authors": ["Nhat-Minh Huynh", "Hoang-Giang Cao", "I-Chen Wu"], "abstract": "Pommerman is a multi-agent environment that has received considerable attention from researchers in recent years. This environment is an ideal benchmark for multi-agent training, providing a battleground for two teams with communication capabilities among allied agents. Pommerman presents significant challenges for model-free reinforcement learning due to delayed action effects, sparse rewards, and false positives, where opponent players can lose due to their own mistakes. This study introduces a system designed to train multi-agent systems to play Pommerman using a combination of curriculum learning and population-based self-play. We also tackle two challenging problems when deploying the multi-agent training system for competitive games: sparse reward and suitable matchmaking mechanism. Specifically, we propose an adaptive annealing factor based on agents' performance to adjust the dense exploration reward during training dynamically. Additionally, we implement a matchmaking mechanism utilizing the Elo rating system to pair agents effectively. Our experimental results demonstrate that our trained agent can outperform top learning agents without requiring communication among allied agents.", "sections": [{"title": "1 Introduction", "content": "In 1983, Bomberman was released for the Nintendo Entertainment System (NES) platform. In 2018, Pommerman [1] was introduced; it is a well-organized open-source environment that is implemented in Python. This version enhances the original Bomberman with a battle scenario where players are allowed to communicate in a team.\n\nPommerman is an appropriate multi-agent benchmark because the environment offers various challenges, including multiple agent interaction, sparse rewards, false-positive rewards, delayed action effects, and complex exploration [2]. There are four agents in a game, either playing in 2vs2 team mode or individually competing against each other in Free-For-All mode. The sparse reward is a problem related to the time that an agent receives a reward. In Pommerman, a game reward of win or loss is given to each agent at the end of an episode, which can take up to 800 timesteps. The false-positive reward occurs when rewards are not derived from appropriate actions by an agent but from the opponent's mistake of committing suicide. Another challenging problem in Pommerman is the delay action effect. In the game, the only way to explore the board and eliminate an enemy is by placing bombs, but the effect of bomb placement does not produce a direct result but delays 10 timesteps. In addition, as the vision of an agent is limited to a 9x9 grid around it, the environment turns into a partially observable environment, which makes it hard for an agent to explore the board or find enemies."}, {"title": "2 Approaches", "content": "In this study, we introduce a multi-agent training system to play the 2vs2 team mode of Pommerman. Our system includes two stages: curriculum learning and population-based self-play. The curriculum learning, consisting of three incremental difficulty phases, assists the agent in learning essential skills to handle the games, such as exploring the map, picking up items, hiding from an explosion, and using defensive strategies to stay safe and survive while simultaneously fighting to eliminate enemies. Once agents acquire those necessary skills, we design a population-based self-play system wherein a population of agents compete against each other, naturally evolving their own strategies to improve performance. Figure 1 shows the overview of our system.\n\nDeploying a multi-agent self-play training system for a competitive game poses two challenges [3]: (1) addressing exploration problems in a competitive game with sparse reward; and (2) designing a suitable matchmaking mechanism for effective agent pairing in training, enabling progressive learning. To address the exploration problem with sparse reward, we introduce an adaptive annealing factor, which dynamically anneals the dense exploration reward during training based on the agent's performance. To design a suitable matchmaking, we implement the matchmaking probability based on the Elo rating system, ensuring incremental learning.\n\nThe main contributions of this paper are summarized as follows: 1) We present a multi-agent training system to play Pommerman, which includes two stages: curriculum learning and population-based self-play. 2) We propose an adaptive annealing factor based on agents' performance to dynamically adjust the dense exploration reward during training. 3) We implement a matchmaking mechanism utilizing the Elo rating system to pair agents effectively 4) In our experiments, we demonstrate that our trained agent can outperform learning agents, even without requiring communication among allied agents."}, {"title": "2.1 Curriculum Learning Stage", "content": "Pommerman is a complex environment that requires multiple skills to handle the game. To encourage the agent to acquire different skills, we designed a curriculum learning with three phases: exploring the map and locating opponents, eliminating opponents, and surviving encounters while fighting opponents. Three phases correspond to three rule-based agents (inspired by Chao Gao et al. [4]), denoted as static_agent, simple_moving_agent and simple_bomb_agent.\n\nIn the first phase, the training agent plays with the static_agent, who idle and are waiting to be eliminated by our agent. In this phase, the agent learns how to explode wooden walls to open up the passages, pick up items, hide from an explosion of its own bombs, and finally, eliminate the static opponents.\n\nIn the second phase, the training agent plays with simple_moving_agent who move randomly on the board but are not allowed to place bombs. Furthermore, the opponents are programmed to dodge the bomb explosion safely. As the difficulty increases, our agent is required to find strategies to place bombs more effectively.\n\nIn the third phase, the simple_bomb_agent are able to move and place bombs randomly. In this scenario, approaching the opponents becomes more challenging for our agent. Therefore, our agent is required to learn defensive strategies to stay safe and survive while simultaneously fighting to eliminate enemies.\n\nThe next phase of the curriculum learning stage is activated when the training agent achieves a 55% win rate over the current rule-based agent."}, {"title": "2.1.2 Adaptive Exploration Reward by Performance", "content": "The main objective of Pommerman is to eliminate the opponents. However, players are initially located in the corners of the map and are blocked by wooden walls and stones. Thus, the gameplay can be divided into two subtasks: (1) explore the map to navigate to the opponent; and (2) strategically place bombs to engage in fighting and eliminate them. These two sub-tasks correspond to two types of reward: exploration reward and game reward, denoted as \\(e\\) and \\(R\\), respectively. The details of reward setting are detailed in C.\n\nThere is a conflict between using exploration rewards and game rewards. The dense exploration reward is necessary at the beginning of training to encourage the agent to explore the map and open up the passages. However, using dense exploration rewards distracts the purpose of the self-evolving strategy, which benefits from the sparse game reward. Conversely, directly using the sparse game reward is excessively challenging for agents to learn gameplay effectively due to the complexity of exploration in the game environment.\n\nTo solve this problem, a work [3] calculated the reward function with an annealing factor, denoted as \\(a\\), to linearly reduce the dense exploration reward to zero during the training. In the work [3], the reward function at timestep t is defined as follows:\n\\[r_t = a_t e_t + (1 - a_t)R\\]\nwhere \\(r_t\\) is the total reward agent received, \\(e_t\\) is the exploration reward, R is a game reward, and \\(a_t\\) is the annealing factor, controlling the impact of \\(e_t\\) and \\(R\\). Generally, deciding when the annealing factor should equal zero is a manual fine-tuning process.\n\nTo dynamically adjust the annealing factor a for adapting to strategy changes during the training, we propose an adaptive annealing factor based on the agent's performance as follows:\n\\[a = 1 - tanh(k *x)\\]"}, {"title": "2.2 Population-based Self-play Stage", "content": "where k is the tuning parameter, and x is the current performance of an agent. In our work, an agent's performance is measured by the average number of deaths of the enemy so that x is in the range of [0, 2] and k is set to 1.2 to obtain a suitable curve of tanh function (shown in Figure 2).\n\nAt the beginning of training, the agent is not able to approach and eliminate the opponent; the current performance of an agent x = 0. Thus, the exploration reward fully impacts the reward, motivating the agent to place bombs to explore the map. Subsequently, the agent eventually locates the opponents to eliminate them, increasing the performance. As performance increases, the impact of the dense exploration rewards and sparse game rewards shifts. This encourages the agent to gradually prioritize self-evolving strategies aligned with the primary game objective.\n\nAfter the curriculum learning stage, the dense exploration reward annealed to zero, entirely replaced by the sparse game reward. So, in the self-play stage, the agent is trained only on game rewards."}, {"title": "2.2.1 Population-based Self-play System", "content": "After training in the curriculum learning stage, the agent knows the properties of the Pommerman environment and learns the essential skills to handle the game. To improve the agent's strength, we designed a population-based self-play system where a population of agents plays against each other for self-improvement.\n\nWe designed a population-based self-play system with eight agents. Three agents are rule-based agents used in the curriculum learning stage, to prevent from forgetting learned skills The remaining five learning-based agents are initialized by the trained agent from the curriculum learning stage.\n\nDuring the self-play training, if an agent has a win rate below 45% over the population, it is considered to be replaced by a stronger agent. A stronger agent is selected randomly from the four remaining agents. Figure 1 shows the design of our population-based self-play stage.\n\nTo ensure effective progressive learning during self-play, selecting an appropriate opponent for pairing is crucial. In the next subsection, we will introduce a matchmaking mechanism based on Elo rating to tackle this problem."}, {"title": "2.2.2 Match Making Probability", "content": "During the self-play stage, each agent gains experience by interacting with other agents. Therefore, implementing a suitable matchmaking system is necessary to effectively pair agents for training, ensuring they facilitate progressive learning, adapt to new strategies introduced by other agents, and avoid getting stuck [3]."}, {"title": "3 Experiment Results", "content": "In this research, we implement the matchmaking probability based on the Elo rating system. The Elo rating system is commonly used in board games like Chess and Go [5]. In competitive games, the Elo rating is adopted to measure players' strength and calculate the win rate of a match between two players.\n\nLet the Elo ratings of agent A and agent B be denoted as \\(R_A\\) and \\(R_B\\), respectively. The formula to calculate the expected win rate of Agent A against Agent B is defined as follows:\n\\[E_A = \\frac{1}{1 + 10^{(R_B-R_A)/400}}\\]\nThen, after a match between agent A and agent B, the Elo rating of agent A is updated as follows:\n\\[R'_A = R_A + K(S_A \u2013 E_A)\\]\nwhere K is the maximum adjustment per game, and \\(S_A\\) is the actual score of agent A after a match: 1, 0.5, or 0 if the match results in a win, tie, or loss, respectively.\n\nIn a multi-agent training system, the matchmaking probability of other agents to a selected agent is calculated by applying the softmax function to the expected win rate between those agents and the selected one. As a result, a higher Elo rating agent has a higher chance of being selected as an opponent of the current training agent.\n\nFor example, a population-based self-play system with 4 agents, A1, A2, A3, and A4, with 1010, 1020, 920, and 986 Elo points corresponding. If agent A4 is the current training agent to play a match, the expected win rate of A1, A2, and A3 are 0.53, 0.54, and 0.4 (Equation 3). We then apply the soft-max function to these expected win rates to obtain the matchmaking probabilities of agents A1, A2, and A3 as follows: 0.346, 0.35, and 0.304."}, {"title": "3.1 Curriculum Learning Stage", "content": "All training agents are implemented using the actor-critic algorithm with Proximal Policy Optimization (PPO) [6]. The network architecture is detailed in Appendix D.\n\nAs described in Section 2.1, in this stage, the agent will learn essential skills by training with three different rule-based agents in three phases. The adaptive exploration reward by performanc is also applied in this stage.\n\nIn the beginning, the agent does not know how to eliminate the opponents, so the annealing factor a in Equation 1 is equal to 0. In this way, the agent prefers to do actions to explore the area as well as pick up items while it is not punished by the negative reward of accidental suicide.\n\nAfter about 5 million timesteps, the agent is able to locate and eliminate static_agent, reaching the win rate of 55%. Then, the second phase with simple moving_agent of curriculum learning is activated. Due to the increase in difficulty in the second phase, the performance of the agent quickly drops. Following the decrease in performance is the increase in the annealing factor, which forces the agent to pick up items and place more bombs to find better strategies to eliminate the enemies.\n\nFinally, in the third phase, the simple_bombs_agent starts to place bombs randomly, making it harder for the training agent to approach the opponent. As a result, the training agent learned defensive strategies to effectively dodge the bomb's explosion while simultaneously threatening and eliminating opponents.\n\nThe two Figure 3 and Figure 4 illustrate the contrast between the annealing factor and the performance of the training agent.\n\nIn comparison with the performance annealing factor, we train another model with a linear annealing factor to show the difference between the two methods. Figure 5 illustrates the average number of"}, {"title": "3.2 Self-play Stage", "content": "enemy deaths together with the linear annealing factor. After around 5 million timesteps, the training agent can also finish the first phase and activate the second phase, the same as in the performance of the adaptive exploration reward method. In the next phase, the performance of the training drops immediately. The problem is that the annealing factor cannot adjust to encourage the training agent to keep exploring. Ultimately, the training agent takes a long time to play against the second rule-based agent and cannot pass the second phase. Note that adjusting the value of the linear annealing factor can lead to improved results. However, it will introduce another problem of manually fine-tuning the hyperparameters.\n\nAfter the curriculum learning stage, the training learned useful behaviors such as exploding wood walls, picking up revealed items, dodging, hiding from the explosion, and finding enemies. Also, the dense exploration reward is annealed to zero. The reward function in the self-play stage now contains only sparse game rewards, allowing the training agent to self-develop their strategy aligned with the main game objective.\n\nAs described in Section 2.2, the self-play stage utilizes a population-based training system with eight agents: three rule-based agents (static_agent, simple_move_agent, simple_bombs_agent), and five training agents (initialized from the trained agent in the curriculum stage). We evaluate the agent using the Elo rating. Initially, every agent is given 1000 Elo rating points and continuously updates after every match. This Elo rating point is also used to obtain matchmaking probability between two agents, using the mechanism detailed in Subsection 2.2.2, which ensures the selection of suitable opponents for facilitating progressive learning and avoids getting stuck.\n\nFigure 6 demonstrates the Elo rating of our training agent during the self-play training stage. After the curriculum stage, our agents start with 1040 Elo rating points. The Elo rating keeps increasing gradually to 1160 after 200 million training timesteps.\n\nAfter finishing self-play stage training, we arranged 100 round-robin matches to evaluate all methods, including a baseline agent developed by Pommerman, nine agents from the 2018 and 2019 competitions, and our agent. As shown in Figure 7, our method has 982 Elo rating points, which is greater than the top two learning agents in 2018 and a robust rule-based agent (Neoteric in 2019). Besides, our Elo rating almost equals to dypm, a tree search-based agent.\n\nWe also compare the win rate of our agent against other agents. As shown in Table 1, our agent has a win rate of 98.85% against the baseline of Pommerman, which is a heuristic agent using the Dijkstra algorithm to find the opponents and place bombs next to enemies. Also, our agent"}, {"title": "4 Conclusions", "content": "outperforms the top learning agents in 2018, Skynet955 and Navocado, with 96.23% and 88.33% win rate, respectively. Furthermore, we defeated Neoteric a ranked 4th agent in 2019, using robust rule-based strategies. Our agent is defeated when facing tree-search-based agents or methods that have communication in their team (shown in Table 2), which is out of scope in this study.\n\nThis research introduces a training system for multi-agent learning in Pommerman with two stages: curriculum learning and population-based self-play. After the curriculum learning stage, the agent learned essential skills such as placing bombs to explode wooden walls and picking up revealed items. After the self-play stage, the agents' strength is improved, and they autonomously learn a more effective and secure strategy, which includes strategic behaviors such as kicking bombs toward enemies or trapping enemies with bombs. We also address two challenges when deploying a multi-agent self-play training system for competitive games: sparse reward and suitable matchmaking. Specifically, we propose an adaptive annealing factor based on agents' performance to dynamically adjust the dense exploration reward, gradually prioritizing the game reward. Furthermore, we implement a matchmaking mechanism utilizing the Elo rating system to pair agents effectively, ensuring incremental learning during the self-play training. Finally, without communication, our trained agent is able to defeat the top learning agent and the top four rule-based agents in the 2019 competition."}]}