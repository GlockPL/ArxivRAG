{"title": "Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization", "authors": ["Sanghyeob Song", "Jaihyun Lew", "Hyemi Jang", "Sungroh Yoon"], "abstract": "Estimating the homography between two images is crucial for mid- or high-level vision tasks, such as image stitching and fusion. However, using supervised learning methods is often challenging or costly due to the difficulty of collecting ground-truth data. In response, unsupervised learning approaches have emerged. Most early methods, though, assume that the given image pairs are from the same camera or have minor lighting differences. Consequently, while these methods perform effectively under such conditions, they generally fail when input image pairs come from different domains, referred to as multimodal image pairs. To address these limitations, we propose AltO, an unsupervised learning framework for estimating homography in multimodal image pairs. Our method employs a two-phase alternating optimization framework, similar to Expectation-Maximization (EM), where one phase reduces the geometry gap and the other addresses the modality gap. To handle these gaps, we use Barlow Twins loss for the modality gap and propose an extended version, Geometry Barlow Twins, for the geometry gap. As a result, we demonstrate that our method, AltO, can be trained on multimodal datasets without any ground-truth data. It not only outperforms other unsupervised methods but is also compatible with various architectures of homography estimators.", "sections": [{"title": "1 Introduction", "content": "Homography is defined as the relationship between two planes when a 3D view is projected onto two different 2D planes. Many mid- or high-level vision tasks, such as image stitching [1], multispectral image fusion [2], and 3D reconstruction [3, 4], require low-level vision tasks such as image registration or alignment as preprocessing steps. Image registration is the process of aligning the coordinate systems of a given pair of images by estimating their geometric relationship. If the relationship between the image pair is a linear transformation, it is called homography.\nHomography estimation for image alignment is known as reducing the geometric gap between a pair of images as depicted in Figure 1. It has been an active area of research since the pre-deep learning era, with prominent algorithms such as SIFT [5], SURF [6], and ORB [7]. The advent of deep learning ushered in the exploration of end-to-end approaches utilizing supervised learning, beginning with DHN [8]. These studies demonstrated the effectiveness of deep learning in homography estimation. However, supervised learning assumes the availability of ground-truth data for the relationship between image pairs or pre-aligned image pairs. The assumption is often unrealistic in real-world scenarios. Therefore, the rise of unsupervised learning has become essential to overcome these practical challenges."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Hand-Crafted Homography Estimation", "content": "Homography estimation has advanced significantly with the development of the feature-based match-ing pipeline, especially after the introduction of SIFT [5]. This pipeline typically consists of feature detection, feature description, feature matching, and homography estimation. Various methods, such as SURF [6] and ORB [7], have been developed as variants for specific steps within this process. However, these methods do not account for the modality gap and are thus unsuitable for multi-modal environments. Recently, feature-based matching methods that handle multimodality, such as RIFT [13] and POS-GIFT [14], have been introduced. Despite these advances, their overall perfor-mance remains insufficient compared to learning-based methods. One of the main reasons is that most of them are based on HOG [15], which assumes planar cases without considering perspective effects."}, {"title": "2.2 Supervised Learning Homography Estimation", "content": "The first method to employ an end-to-end approach in the deep learning era is DHN [8]. This method trains a VGG [16]-based backbone network from a given pairs of images to predict offsets between four corresponding point pairs. The predicted offsets are then converted into a homography using the DLT [17] algorithm. Furthermore, the paper proposes a method for synthesizing datasets for training, which has been continually cited in subsequent research. Another supervised learning-based method is IHN [18], which, compared to DHN, uses a correlation volume to iteratively predict offsets, gradually reducing error. In addition, this method also experiments with a multimodal dataset and has demonstrated only minor error of alignment. This approach was also adopted in RHWF [19], which replaces some convolution layers with a transformer [20] blocks."}, {"title": "2.3 Unsupervised Learning Homography Estimation", "content": "Supervised learning requires ground-truth data, which is the homography between two images, for training. In practice, collecting such datasets, including the ground-truth labels, is nearly impossible. To overcome this limitation, unsupervised learning-based methods have emerged. The first proposed unsupervised learning method is UDHN [9]. It predicts offsets between a pair of input images, like DHN [8], and converts them into homography using the DLT [17]. It then warps one of the input images with this homography and compares it at the pixel level with the other image to measure similarity. Another method, biHomE [10], addresses the challenge of lighting variations through unsupervised learning. It utilizes a ResNet-34 [21] encoder that has been pre-trained on ImageNet [22], benefiting from the variety of lighting conditions in the dataset. This feature makes biHomE less susceptible to differences in lighting. However, it struggles when the input distribution significantly diverges from that of ImageNet, such as with multimodal image pairs. To address these cases, MU-Net [23] has been introduced and adopts CFOG [24]-based loss function. Although this demonstrates that CFOG can be integrated into an unsupervised learning framework and handle modality gap, CFOG still has weaknesses in handling geometric distortions such as rotation and scale."}, {"title": "3 Background", "content": null}, {"title": "3.1 Homography", "content": "Homography is a 3\u00d73 transformation matrix that maps one plane to another. It has a form of projection transformation and maps an arbitrary point p in the image to p' as follows:\n$$p' \\sim Hp \\Rightarrow w \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}$$\nIn Equation (1), \u0397 represents the homography matrix and the symbol ~ denotes homogeneous coordinates, and w indicates the scale factor that allows the use of equality in the equation. In the matrix H, h13 and h23 are related to translation, while h11, h12, h21, and h22 are associated with rotation, scaling, and shearing. Additionally, h31 and h32 are related to perspective effects and are set to 0 when perspective effects are not considered. Generally, when all effects are considered, it has 8 degrees of freedom (DOF), while in special cases where perspective effects are not considered, it has 6 DOF or fewer."}, {"title": "3.2 Trivial Solution Problem", "content": "As mentioned earlier, our goal is to address both the geometry and modality gaps together using unsupervised learning. One straightforward approach is to map each image into a shared space via an encoder and perform registration, similar to UDHN [9], using a reconstruction loss. Another approach, as in biHomE [10], places the encoder after the registration network, where registration is performed first, followed by loss calculation through the encoder. However, when all modules are trained simultaneously, both approaches collapse into a trivial solution: the encoder outputs a constant value, and the registration network predicts the identity matrix as the homography. It appears as if the encoder and registration network collaborate to minimize the loss in a trivial way. To avoid this, special techniques are needed to prevent collapse into trivial solutions."}, {"title": "3.3 Metric Learning", "content": "Metric learning refers to a method that trains models to increase the similarity between data points. Specifically, it is well-known within the field of self-supervised learning, which includes methods such as NPID [25], SimCLR [26], Simsiam [27], Barlow Twins [12], and VIC-Reg [28]. These methods commonly involve multiple input data and Siamese networks, and they focus on how to compare each output so that the model can learn good representations.\nBarlow Twins, which we adopt, requires further explanation for the subsequent sections. Figure 2 illustrates the conceptual diagram of the Barlow Twins method. The encoder extracts the representations rA and rB from inputs A and B, then produces embedding vectors VA and vB by passing these rA and rB through the projector. At this point, the dimension of both embedding vectors becomes (N, D\u2082), where N is the batch size and D\u2081 is the feature dimension. By calculating the similarity matrix between them, we obtain a matrix of size (Dv, Dv). In this process, the similarity is calculated using the correlation coefficient Cij in Equation (2), for the i-th element of C from va and the j-th element of C from vB. The bar over each vector (VA and B) denotes that the vectors have been mean-centered. The objective of the loss function L is to make the computed similarity matrix as close to the identity matrix as possible. This involves a balancing factor between the diagonal and off-diagonal elements. Especially, the term related with off-diagonal is called as 'Redundancy Reduction'. The formula is expressed as follows:\n$$L = \\sum_{i} (1 - C_{ii})^2 + \\lambda \\sum_{i} \\sum_{j \\neq i} C_{ij}^2, \\text{ where } C_{ij} = \\frac{\\sum_{n} \\overline{v_A^{(n,i)}} \\overline{v_B^{(n,j)}}}{\\sqrt{\\sum_{n} (\\overline{v_A^{(n,i)}})^2} \\sqrt{\\sum_{n} (\\overline{v_B^{(n,j)}})^2}}$$"}, {"title": "4 Method", "content": null}, {"title": "4.1 Alternating Optimization Framework", "content": "We present a visualized overview of our training framework at Fig. 3. Our objective is to train a registration network R that takes a moving image IA and a fixed image IB as input, each from modalities A and B. It predicts a homography matrix HAB that could warp (w) image IA to \u0128A = \u03c9(\u0399\u0391, \u0124AB) which aligns with IB.\nOur training framework goes through two phases of optimization per mini-batch of data given, which are 'Geometry Learning' (GL) phase and the \u2018Modality-Agnostic Representation Learning' (MARL) phase. This framework optimizes alternatively, similarly to the Expectation-Maximization (EM) [29] algorithm and it can be expressed as follows:\n$$GL Phase : \\theta_t \\leftarrow \\text{argmin}_{\\theta} [GeometryGap(\\theta_{t-1}, \\eta_{t-1}, \\phi_{t-1})]$$\n$$MARL Phase : \\eta_t, \\phi_t \\leftarrow \\text{argmin}_{\\eta,\\phi} [ModalityGap(\\theta_t, \\eta_{t-1}, \\phi_{t-1})]$$\nIn expressions (3) and (4), t represents the time step, and 0, \u03b7, and 4 denote the parameters of the registration network, encoder, and projector, respectively. The GL phase aims to maximize the similarity of local features of \u0128A and IB, enabling the registration network to learn how to align the two images. The MARL phase is designed to learn a representation space that is modality-agnostic, so that the corresponding features of \u0128A and IB can optimally match. The two phases of optimization will be further detailed in the following sections."}, {"title": "4.2 Geometry Learning", "content": "In this phase of optimization, we fix \u03b7 and 4, the weights of encoder & and projector P, and then train the registration network R. Our ultimate goal is to train the network R that predicts a homography matrix HAB which could warp a moving image 14 to align with a fixed image IB. Explicitly following this objective, we warp (w) image IA to acquire \u00ceA = \u03c9(IA, \u0124AB) and maximize the geometric similarity between the warped image \u0128A and the fixed image IB.\nFor the Geometry loss to use in training, we propose Geometry Barlow Twins (GBT) loss, a modified version of Barlow Twins [12] objective for 2-dimensional space. The features of two images \u0128A and IB are extracted with a shared encoder E, and the output feature maps are used to compute the GBT loss. The GBT loss Lg, is different from the original Barlow Twins in that we consider the spatial axis of the features as the batch dimension of the original Barlow Twins formula:\n$$L_g = \\frac{1}{N} \\sum_{n} [ \\sum_{i} (1 - C_{(n,i,i)})^2 + \\lambda \\sum_{i} \\sum_{j \\neq i} C_{(n,i,j)}^2 ]$$\nwhere $$C_{(n,i,j)} = \\frac{\\sum_{h,w} (\\overline{F_A^{(n,i,h,w)}} \\overline{F_B^{(n,j,h,w)}})}{\\sqrt{\\sum_{h,w} (\\overline{F_A^{(n,i,h,w)}})^2} \\sqrt{\\sum_{h,w} (\\overline{F_B^{(n,j,h,w)}})^2}}$$\nwhere $$f_A^{(n,i,h,w)}$$ and $$f_B^{(n,i,h,w)}$$ are feature vectors in RN\u00d7D\u00d7H\u00d7W corresponding to \u0128A and IB, respectively, mean-normalized along the spatial dimensions, by subtracting the spatial mean from each unit. n and h, w each denote the batch and the spatial (horizontal and vertical) index and i, j denote the index of channel dimension. Our objective can be considered as applying redundancy reduction on the spatial dimension of an image pair, maximizing the similarities of the local features at corresponding regions. By minimizing the geometric distance, or in other words, maximizing the geometrical similarity, the network R learns to geometrically align the given image pair."}, {"title": "4.3 Modality-Agnostic Representation Learning", "content": "Images of different modalities are susceptible to form a different distribution in the deep feature space. Maximizing the similarity of images from different modalities in such feature space may not lead to our desired result. Hence, for the model to successfully perform homography estimation given two images from different modalities, it should extract geometric information independent of modality. To help the GL phase to successfully work, the goal is to train an encoder which could form a modality-agnostic representational space of image features, so that our Geometry loss could properly work as intended. This process is referred to as Modality-Agnostic Representation Learning (MARL). During this phase, the registration network is fixed, and the encoder & and a projector P are trained. The standard form of the Barlow Twins loss [12] is adopted as the training loss, defined as follows:\n$$L_m = \\sum_{i} (1 - C_{ii})^2 + \\lambda \\sum_{i} \\sum_{j \\neq i} C_{ij}^2, \\text{ where } C_{ij} = \\frac{\\sum_{n} v_A^{(n,i)} v_B^{(n,j)}}{\\sqrt{\\sum_{n} (v_A^{(n,i)})^2} \\sqrt{\\sum_{n} (v_B^{(n,j)})^2}}$$\nwhere (mi) E RNXD and j) 2,j) \u2208 RNXD are the projected vectors of f(n,i,h,w) and fm,j,h,w), respectively, mean-normalized along the batch dimension, by subtracting each unit with the batch-mean value. n and i, j denote the batch and the index of channel dimension. The Modality loss aims to constrain z Zn,i) and zen, z\u00edn,j) to share a similar representational space, agnostic to the image's modality. To capture global feature information, we employ global average pooling (GAP) [21, 30], which removes the spatial dimensions to produce a feature vector."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Multimodal Datasets", "content": "Google Map is a multimodal dataset proposed in DLKFM [11]. It consists of pairs of satellite images and corresponding maps, which have different representation styles. There are approximately 9k training pairs and 1k test pairs of size 128\u00d7128.\nGoogle Earth is another DLKFM dataset that provides multimodality by consisting of images of the same area taken in different seasons. The amount of data is about 9k for training and 1k for test. The input image size is also 128\u00d7128.\nDeep NIR is a dataset proposed in [31]. It is an extension of the RGB-NIR scene dataset proposed by M. Brown et al. [32] through cropping and image-to-image translation. The RGB-NIR scene dataset consists of pairs of images, one captured by an RGB camera and the other by an NIR sensor at the same location. In this experiment, we use the oversampled \u00d7100 Deep NIR dataset. The dataset consists of approximately 14k training pairs and 2k test pairs. In this experiment, we resize these images to 192\u00d7192 and then apply dynamic deformation as proposed by DHN [8]. Finally, we obtain image pairs with a size of 128\u00d7128."}, {"title": "5.2 Implementation Details", "content": "Registration networks are selected to aim at demonstrating that our method can universally operate with various types of registration networks. To cover unique modules such as CNN, RNN, and Trans-former [20], we chose DHN [8] (plain CNN), RAFT [33] (CNN+RNN), IHN [18] (CNN+iterative process), and RHWF [19] (Transformer+iterative process). In the case of DHN, following bi-HomE [10], the backbone network was switched from VGG [16] to ResNet-34 [21]. Additionally, IHN and RHWF employed a 1-scale network, which we refer to as IHN-1 and RHWF-1, respectively. In iterative process-based networks like RAFT, IHN, and RHWF, our AltO framework was applied at every time step, replacing the use of ground-truth labels."}, {"title": "5.3 Evaluation Metric", "content": "The evaluation metric used is Mean Average Corner Error (MACE). Corner error is the Euclidean distance between the positions warped by the correct homography HAB and the predicted homogra-phy HAB for a corner of IA. ACE is the average corner error across four corners, and MACE is the overall mean of ACE across the dataset.\n$$MACE(H_{AB}, \\hat{H}_{AB}) = E_{N \\in N} E_{C \\in C} [\\frac{1}{4} ||\\omega(c, H_{AB}) - \\omega(c, \\hat{H}_{AB})||_2]$$"}, {"title": "5.4 Evaluation Results", "content": "The experimental results are shown in Table 1. When image pairs are unaligned, or if the identity matrix is used as the homography, MACE typically exceeds 23 px. Thus, any method yielding MACE above this threshold can be considered unsuccessful in training. Most conventional unsupervised methods fail to train effectively on the benchmark datasets. UDHN [9], which directly compares IB and \u0128A in pixel space, is highly sensitive to modality changes, resulting in failed training on most datasets except Google Earth [11]. Although the Google Earth dataset is multimodal, its image pairs have similar intensity distributions, allowing only limited training in pixel space. Similarly, CAU [36], which addresses only minor lighting variations, suffers from modality gaps and large displacements,"}, {"title": "6 Ablation Study", "content": null}, {"title": "6.1 The Effectiveness of Alternating", "content": "In our AltO, the key technique is alternating, designed to address the trivial solution problem from unintended collaboration. This ablation study demonstrates its effectiveness by comparing cases with and without alternating. The experiment was conducted on the Google Map dataset [11], measuring the MACE values described in Section 5.3. Table 2 presents the experiment conducted with and without Modality-Agnostic Representation Learning (MARL) when alternating is absent. As expected, the result shows that effective learning occurs only with alternating."}, {"title": "6.2 The Necessity of Global Average Pooling", "content": "In Section 4.3, we stated that global average pooling (GAP) [30] is applied at the end of the projector P. This ablation study compares performance with and without GAP. The experiment was conducted using the Google Map dataset [11], with the result presented in Table 3. As shown by the result, effective learning occurs only with GAP. This is because, without GAP, local features are forced to be similar by comparing fine details before the registration network is fully trained, which leads to the trivial solution problem discussed in Section 3.2."}, {"title": "6.3 Combination of Loss Functions", "content": "Our proposed AltO requires both Geometry and Modality losses. While we suggest using Bar-low Twins [12] for both, this section examines the performance impact of alternative losses. For the experiments, the registration network R is set to IHN-1 [18], and the dataset used is Google Map [11]. We compare with Info NCE [37] and VIC-Reg [28], widely used contrastive losses in Siamese self-supervised learning, and additionally test Mean Squared Error (MSE) for the Geometry loss. Modality loss is used in its original form, while Geometry loss is expanded to include spatial dimensions, as described in Section 4.2.\nTable 4 presents the MACE results for different combinations of loss functions. It is known from SimCLR [26] that using Info NCE or Patch NCE benefits from a larger number of contrasting in-stance pairs. However, in our implementation, the batch size affecting the Modality loss is limited to 16, and the spatial resolution impacting the Geometry loss is only 32 by 32 (1024 pairs), which is relatively small. This likely leads to a decline in performance. Additionally, VIC-Reg exhibited instability due to the three separate balance factors required for its components: variance, invariance, and covariance. MSE is another choice that could be used as a distance metric to compute the geometrical gap. This option of loss leads to a fair performance, but short in comparison to our Barlow Twins-based objective."}, {"title": "6.4 Architecture of Encoder and Projector", "content": "As mentioned in Section 5.2, we have adapted ResNet-34 [21] by dividing it into an encoder and a projector. In doing so, there were several cases regarding which stages to use as the encoder and which to designate as the projector. This ablation study aims to measure the performance for each of these configurations. The Google Map dataset [11] is used, with IHN-1 [18] as the registration network R.\nThe result indicates that the encoder performs best when at least two stages are utilized. While using three stages can enhance the encoder's mapping capabilities, the resolution is halved, leading to decreased precision. Conversely, using only one stage results in better resolution compared to two stages, but the mapping capabilities are inadequate."}, {"title": "7 Conclusion", "content": "We propose AltO, a new learning framework, capable of training on multimodal image pairs through unsupervised learning. By alternating optimization, like Expectation-Maximization [29], AltO handles both geometry and modality gaps separately. This framework employs two types of loss functions, using the Barlow Twins [12] and its extended version, to effectively address both gaps. Experimental results demonstrate that the registration network is stably trained within our framework across various backbones. Furthermore, it outperforms other unsupervised learning-based methods in MACE evaluations and achieves performance close to that of supervised learning-based methods across multiple datasets."}, {"title": "8 Limitation and Future Work", "content": "Despite its advantages, our method encounters some limitations. Firstly, applying the same registration network results in slightly reduced performance compared to supervised learning, which directly utilizes ground-truth labels. Secondly, our framework requires training additional modules to replace ground-truth labels, which slows down the training process.\nTo address these limitations, future work could explore new designs, such as integrating Transform-ers [20] into the encoder and projector to improve performance. Additionally, reducing training time is essential, motivating efforts to develop a single-phase framework that avoids the trivial solution problem mentioned in Section 3.2."}, {"title": "A.1 Additional Visualization of the Results from the Main Experiment", "content": "We provide additional results of our method and baseline methods for comparison as below."}, {"title": "A.2 Integration of Iterative Registration Networks into AltO", "content": "In this section, we provide a detailed explanation of integrating a registration network with an iterative process into AltO. In the main experiment, we used DHN [8], RAFT [33], IHN-1 [18], and RHWF-1 [19] as the registration network R. Among these, RAFT, IHN-1, and RHWF-1 employ an iterative process, predicting intermediate homographies over multiple time steps for each homography estimation. When supervised learning is applied, the loss function is used for homographies output at each time step. Since AltO is also intended as a replacement for supervision, we similarly apply the Geometry loss to the homography output at each time step. For RAFT, originally designed for optical flow estimation, we implicitly incorporate DLT [17] to convert optical flow into homography."}]}