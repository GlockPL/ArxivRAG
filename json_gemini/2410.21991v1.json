{"title": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence Monitoring System", "authors": ["Wen-Dong Jiang", "Chih-Yung Chang", "Hsiang-Chuan Chang", "Diptendu Sinha Roy"], "abstract": "Recently, research based on pre-trained models has demonstrated outstanding performance in violence surveillance tasks. However, these black-box systems face challenges regarding explainability during training and inference processes. An important question is how to incorporate explicit knowledge into these implicit models, thereby designing expert-driven and interpretable violence surveillance systems. This paper proposes a new paradigm for weakly supervised violence monitoring (WSVM) called Rule base Violence monitoring (RuleVM). The proposed RuleVM uses a dual-branch structure for different designs for images and text. One of the branches is called the implicit branch, which uses only visual features for coarse-grained binary classification. In this branch, image feature extraction is divided into two channels: one responsible for extracting scene frames and the other focusing on extracting actions. The other branch is called the explicit branch, which utilizes language-image alignment to perform fine-grained classification. For the language channel design in the explicit branch, the proposed RuleCLIP uses the state-of-the-art YOLO-World model to detect objects and actions in video frames, and association rules are identified through data mining methods as descriptions of the video. Leveraging the dual-branch architecture, RuleVM achieves interpretable coarse-grained and fine-grained violence surveillance. Extensive experiments were conducted on two commonly used benchmarks, and the results show that RuleCLIP achieved the best performance in both coarse-grained and fine-grained detection, significantly outperforming existing state-of-the-art methods. Moreover, interpretability experiments uncovered some interesting rules, such as the observation that as the number of people increases, the risk level of violent behavior also rises.", "sections": [{"title": "I. INTRODUCTION", "content": "The primary goal of violent surveillance is to detect abnormal events in the real world to prevent violent behavior and maintain social order. [1] With the rapid advancement of artificial intelligence, researchers are exploring the use of deep learning technologies in surveillance systems to replace the inefficiencies and high costs associated with traditional manual monitoring [2][3]. Weakly-supervised violence surveillance (WSVM) has emerged as an important research area in recent years. Unlike traditional methods, which require labeling every frame of a video and using supervised learning for frame-level training, WSVM achieves frame-level anomaly detection based on video-level annotations [4][5]. In current approaches, pre-trained models such as Inflated 3D ConvNet (I3D) [6], Transformer [7], and Contrastive Language-Image Pretraining (CLIP) [8] are often used to extract features from full-resolution frames. These features are subsequently processed using temporal sequence models, and finally, a Multiple Instance Learning (MIL) mechanism is applied to train classifiers for predicting anomalies at the frame level [9][10][11]. Although these methods have demonstrated strong performance. However, the increasing complexity of systems has raised growing concerns about interpretability [12]-[16].\nFigure 1 highlights some of the interpretability issues found in benchmark datasets of current violence surveillance systems. As shown in fig.1, while the model successfully detects the presence of violence in certain scenes, the interpretability machine learning method [13] reveals that the system made errors in its decision-making. It incorrectly identifies certain elements, such as scenes, colors, or even non-violent bystanders, as being associated with violence. This poses significant risks when applied in real-world contexts.\nFrom a system design perspective, the root cause of this issue lies in the tendency of researchers to employ tightly coupled training processes rather than decoupled representation training. They often package the scene and abnormal behavior together as a fixed input to the system. This approach makes it difficult for the system to distinguish whether it should focus on behavior changes or scene changes during the temporal sequence and classifier training phases. As a result, the system may produce correct predictions but with flawed decision logic when operating in real-world scenarios.\nAdditionally, in many existing applications, traditional CCTV cameras typically rely solely on visual data, making it difficult to detect certain potential violent events that cannot be captured through visual information alone. For instance, when someone holds up a sign or sends messages containing threatening content, it is challenging to discern intent or threats purely from video footage. These text-based signals might reveal signs of potential violence or dangerous behavior.\nRecently, studies using pre-trained models that incorporate both image and text modalities, such as CLIP [17][18], have demonstrated excellent performance in violence surveillance tasks. The core idea of CLIP [8] is to align images and text through contrastive learning, pairing images with corresponding textual descriptions while excluding mismatched pairs in a shared embedding space. This alignment is achieved by calculating the distance between image and text embeddings: a smaller distance indicates a strong association, while a larger distance suggests the two are unrelated.\nEven in scenarios where only visual modality is available, and no additional modality sensors are present to capture other types of data, surveillance systems can still leverage these pre-trained models to perform multimodal training. This helps address problems that a single visual modality alone might not be able to resolve.\nTo address these issues, this paper proposes a violence surveillance system called RuleVM, specifically designed for WSVM. As shown in Fig. 2, unlike previous studies that directly adopt end-to-end coupled designs, the proposed RuleVM employs a differentiated, decoupled design for both image and text modalities and introduces a dual-branch structure to implement an end-to-end violence monitoring system.\nOne branch, referred to as the implicit branch, focuses on coarse-grained binary classification using visual features. In this branch, image feature extraction is divided into two channels: one for scene frame extraction and another specifically for behavior extraction. The second branch, known as the explicit branch, performs fine-grained classification by leveraging the alignment between text and images. In the design of the explicit branch's text channel, the proposed RuleVM incorporates the state-of-the-art YOLO-World [20] to detect objects and behaviors in video frames and uses data mining techniques to identify association rules, which serve as rule base interpretable descriptions for the video content. In addition, a lightweight time series module is proposed in RuleVM to enable the deployment of the system in scenarios involving edge computing.\nWith the advantages of dual-branch architecture, the proposed RuleVM achieves both coarse-grained and fine-grained interpretable violence surveillance. Extensive experiments conducted on the XD-Violence [21] and UCF-Crime [6] dataset demonstrate the superiority of the proposed method. In practice, this paper aims to solve the following three research challenges:\n1.  How can training data be effectively guided to the designed branches, ensuring the system collaborates to achieve both coarse-grained and fine-grained monitoring of violent behavior?\n2.  The explicit branch using the YOLO-World and data mining techniques to extract frequent itemset, how can the system's computational efficiency be optimized without compromising accuracy?\n3.  How can data patterns be extracted from training results to facilitate model optimization and the discovery of potential underlying trends?\nIn real-world applications, such as in large-scale sports events where thousands of spectators gather in stadiums, the environment is often filled with excitement and tension, which greatly increases the likelihood of conflicts or violent incidents. Traditional surveillance systems often rely solely on video footage to detect abnormal behavior, making it difficult to accurately distinguish between potential violent acts and normal intense interactions within the crowd. This limitation can result in real threats being overlooked or false alarms being triggered. In summary, the contributions of this paper are as follows:\n4.  The proposed RuleVM employs dual-branch architecture, consisting of an implicit and explicit branch, to decouple the originally integrated surveillance tasks into two parts responsible for coarse-grained and fine-grained monitoring of violent behavior. The implicit branch performs classification based solely on visual features, enabling the understanding of potential behaviors and the surrounding environment in the images. The explicit branch aligns text with video, utilizing rules and video to perform more detailed comparisons and classifications, making the model's decision-making process more transparent and easier to understand.\n5.  In the explicit branch, the proposed RuleVM uses the YOLO-World model to detect objects and behaviors in the video and applies data mining techniques to mine association rule for video content descriptions. These descriptions provide a natural language-level supplementary explanations for surveillance outcomes, enabling the model to explain its classification decisions in a readable manner, further improving the system's interpretability.\n6.  Extensive experiments conducted on benchmark datasets show that the proposed RuleVM has a significant advantage over state-of-the-art competing methods. What is even more noteworthy is that, unlike"}, {"title": "II. RELATED WORK", "content": "This chapter presents a review of some relevant studies in Violence Monitoring. These studies are categorized into two parts: Weakly Supervised Violence Monitoring and Visual Language Pre-Training Models.\n\nA. Weakly Supervised Violence Monitoring\nSultani et al. [6] and Hasan et al. [22] were the first to introduce the MIL model for violence monitoring in supervised violence surveillance (SVM). The proposed approaches packaged surveillance videos into a package and then input them into I3D and C3D networks for binary classification of violent and non-violent events. Ji and Lee [23] proposed the One-Class Support Vector Machine (OCSVM) model for violence detection. However, due to the limitations of 3D network-based designs in terms of real-time performance and hardware requirements in practical applications, research gradually shifted towards combining video frame extraction and time series modules for violence monitoring. Subsequent research began to focus on using self-attention mechanisms, Transformer, or Graph ConvNet (GCN) to capture temporal and contextual relationships in video content. Zhong et al. [24] proposed a GCN-based method to calculate feature similarity and temporal consistency between monitored video segments for the detection of violent behavior. While Tian et al. [25] utilized a self-attention network to capture the global temporal context of violent behavior in monitoring videos. Their proposed model, referred to as Robust Temporal Feature Magnitude (RTFM), effectively leverages temporal information to enhance the detection of violent actions. Wu et al. [4] proposed Hierarchical Learning Network (HL-NET), which developed a global and local attention module to identify temporal dependencies and thereby obtain more expressive embeddings from monitoring videos.\nAll the above methods are based on an end-to-end coupled system design, which detects abnormal events by predicting the probability of abnormal frames. However, this approach, which compresses each frame into a package and then inputs it into the time series module for an end-to-end MIL system, presents significant interpretability issues. Additionally, in the design of the time series module, these systems only consider application scenarios in a unified cloud environment and do not consider lightweight requirements in some edge computing scenarios.\nB. Visual language pre-training Models\nIn some practical application scenarios, only single-modal cameras were typically available, and traditional multi-modal designs were often not applicable to these single-modal scenarios. Thanks to certain pre-trained models, such as CLIP [8], which aligned text and images to accomplish multi-modal tasks, it became possible to achieve multi-modal tasks even in scenarios where only a single modality was present. For instance, Zhou et al. [26] introduced an enhanced image classification approach leveraging the CLIP, which significantly improved performance in complex visual recognition tasks. Mokady et al. [27] made a major advancement in automatic image captioning by refining the integration of vision and language models. Zhou et al. [28] applied CLIP to object detection, improving the model's ability to distinguish between similar objects. Yu et al. [29] made notable progress in scene text detection, particularly in enhancing the accuracy and efficiency of text localization in cluttered environments. Additionally, Rao et al. [30] conducted valuable research on dense prediction tasks, focusing on improving the precision of pixel-level predictions in dense visual data. Lv et al. [31] proposed Uncertainty-aware Multiple Instance Learning (UMIL), which incorporates uncertainty estimation into the multiple instance learning framework to improve the system's robustness and accuracy. Joo et al. [32] proposed CLIP with Temporal Shift Attention (CLIP-Tsa), which introduces a temporal shift attention mechanism to better capture temporal dependencies in video data, enhancing the model's performance in tasks. In violence monitoring, Luo et"}, {"title": "III. ASSUMPTIONS AND PROBLEM FORMULATION", "content": "This section introduces the assumptions and problem statements of this study. Given a monitoring video sequence V of duration t, partitioned into N equal-length temporal segments. The primary objective of this research is to ascertain the presence of anomalies within V.\nLet $V = \\{\\Phi_i\\}_{i=1}^N$ denote the video sequence, where each segment $\\Phi_i$, may consist of non-violent content, violent content, or a combination there of. Let $V_i$ and $\\bar{V_i}$ denote the non-violent and violent subsets of $\\Phi_i$, respectively. Each segment is defined as the union of its nonviolent and violent components $\\Phi_i = V_i \\cup \\bar{V_i}$.\nLet $C = \\{c_k\\}_{k=1}^K \\cup \\{\\hat{c}\\}$ denote the set of all possible classes, where $c_k$ denotes the k-th violence classes for $1 \\leq k \\leq K$, and $\\hat{c}$ denotes the non-violence class. Correspondingly, let $L = \\{l_k\\}_{k=1}^K \\cup \\{\\hat{l}\\}$ be the set of labels, where $l_k$ is the label associated with class $c_k$, and $\\hat{l}$ is the label for the non-violence class $\\hat{c}$, and $\\hat{l}$ is the label for the non-violence class $\\hat{c}$.\nThis paper assumes that each $\\bar{V_i}$ of segment $\\Phi_i$, belongs to a specific violence class $c_k$ for $k \\in \\{1,2, ..., K\\}$. Consider a violence monitoring mechanism $M \\in M$, where $M$ denotes the space of all admissible monitoring mechanisms. The mechanism M aims to monitor the occurrence of violent events within V.\nFor each segment $\\Phi_i$, mechanism M assigns a confidence score $P_i \\in [0,1]$, indicating the likelihood that $\\Phi_i$ contains violence. Let $\\theta \\in [0,1]$ denote a predetermined prediction threshold. Define the prediction indicator $\\delta_M(\\theta)$ for segment $\\Phi_i$, as:\n$\\delta_M(\\theta) = \\begin{cases} 1, & \\text{if } P_i \\geq \\theta; \\\\ 0, & \\text{if } P_i < \\theta. \\end{cases}$ \nDefine the ground truth indicator $\\delta_i$ for segment $\\Phi_i$, as:\n$\\delta_i = \\begin{cases} 1, & \\text{if } V_i \\neq \\emptyset; \\\\ 0, & \\text{if } V_i = \\emptyset. \\end{cases}$\nLet TP, TN, FP and FN, represent True Positive, True Negative, False Negative and False Positive respectively, of the prediction result of input $V$ by applying mechanism M. The values can be calculated using $TP = \\delta_i \\times \\delta_M(\\theta)$, $TN = (1 - \\delta_i) \\times (1 - \\delta_M(\\theta))$, $FP = (1 - \\delta_i) \\times (1 - \\delta_M(\\theta))$ and $FN = \\delta_i \\times (1 - \\delta_M(\\theta))$.\nLet TP, TN, FP and FN represent the cumulative True Positive, True Negative, False Positive, and False Negative, respectively, of the prediction results by applying mechanism M to all segments $\\Phi_i$, for $1 \\leq i \\leq N$. The value can be calculated by $TP = \\sum_{i=1}^N TP_i$, $TN = \\sum_{i=1}^N TN_i$, $FP = \\sum_{i=1}^N FP_i$, and $FN = \\sum_{i=1}^N FN_i$.\nLet $P_M$, $R_M$ denote the Precision and Recall of the predictions by applying mechanism M to predict a given video V. The values of $P_M$, $R_M$ can be calculated by $P_M = \\frac{TP}{TP+FP}$ and $R_M = \\frac{TP}{TP+FN}$ respectively.\nLet $\\theta_j$ denote the j-th prediction threshold, let $AP_M$ denote the Average Precision of mechanism M. The $AP_M$ can be calculated by Exp. (1) :\n$AP_M = \\sum_{k=1}^{K-1} (R_M(\\theta_{k+1}) - R_M(\\theta_k)) \\times P(\\theta_{k+1})_M$.\nSimilar to previous works [4], [8], [17], [18] the first objective of this paper is to develop mechanism $M^{best}$ that satisfies $M^{best} = arg \\underset{M \\in M}{Max}(AP_M)$.\nLet $A_M$ denote the AUC of mechanism M. The value of $A_M$ can be calculated by Exp. (2):\n$A_M = \\int TPR(\\theta) d(FPR(\\theta)) = \\int_0^1 \\frac{TP(\\theta)}{TP(\\theta) + FN(\\theta)} d\\left(\\frac{FP(\\theta)}{FP(\\theta) + TN(\\theta)}\\right)$.\nSimilar to previous works [4], [8], [17], [18] the second objective of this paper is to develop mechanism $M^{best}$ that satisfies $M^{best} = arg \\underset{M \\in M}{Max}(A_M)$. This section introduced the assumptions and problem formulation of this paper. The next section will introduce the proposed RuleVM."}, {"title": "IV. THE PROPOSED RULEVM", "content": "This section presents the proposed RuleVM system which aims to develop an interpretable violence monitoring system. Unlike previous works [4][26][31][32], which designed a coupled end-to-end black-box system by integrating multiple pre-trained models, the proposed RuleVM adopts a decoupling design strategy, designs implicit and explicit branches and employs a multi-channel approach for achieving an interpretable violence detection system. This allows each branch to handle its respective task independently during the training process without interference from other noise.\nFurthermore, regarding the design of the textual branch, in contrast to previous works [17][18][31][32], which directly used LLMs to generate descriptions for each frame and then obtained video embedding through a temporal module, the proposed RuleVM combines a pre-trained object detection model with data mining techniques to derive the rules for video descriptions. This enables RuleVM to maintain strong interpretability, during both training and deployment phases. Finally, compared to previous works [4][6][17][18][22][25], the proposed RuleVM uses a lightweight temporal module, allowing the model to consume significantly fewer computational resources in both training and deployment phases compared to other systems.\nThe purpose of designing scene channels is to obtain the representation of each frame in the video. Let $V = \\{f_1, f_2,..., f_n\\}$ denote the video sequence consisting of n frames, where each frame $f_i \\in F$ for $j \\in \\{1,2, ..., n\\}$. Each frame is processed using a frozen CLIP Image Encoder to extract frame-level features. Let F represent the space of frames and $F_{clip}$ represent the feature space derived by the CLIP Image Encoder. Define $\\Phi: F \\rightarrow F_{clip}$ as the feature extraction function implemented by the CLIP Image Encoder. Therefore, the frame-level feature representations are given by:\n$V_{clip} = \\{\\Phi(f_1), \\Phi(f_2), ..., \\Phi(f_n) \\} = \\{g_1^{scene}, g_2^{scene}, ..., g_n^{scene}\\}$,\nwhere $g_j^{scene} = \\Phi(f_j) \\in F_{clip}$ denotes the scene channel feature extracted from the j-th frame by frozen CLIP Image Encoder. After the scene channel feature extraction, Next is behavior channels.\n\nThe purpose of designing behavior channels is to capture changes between frames content to guide the model in identifying action changes within a scene. Previous studies typically utilized optical flow methods [34][35] to detect action change. However, these methods have two main issues: first, optical flow may not be accurate in some violent scenes occurring at dusk. Second, the computational cost of optical flow is relatively high. To address this problem, an Efficient Behavior Detection Module (EBDM) is proposed to achieve fast and effective action change detection.\nThe proposed EBDM operates a two-stage detection mechanism. In the first stage, it captures motion changes by computing differences between consecutive frames. In the second stage, it removes background information by calculating changes between neighboring patches within the current frame. Let $p_k^i$ denote the k-th patch in the i-th frame, for a subset of the feature maps $V_{clip} = \\{g_1^{scene}, g_2^{scene},..., g_n^{scene}\\}$. The proposed EBDM divides each frame into m patches, resulting in a corresponding patch set $P_i = \\{p_1^i, p_2^i, ..., p_m^i\\}$. In the first stage, the proposed EBDM computes the motion difference for each patch by calculating the change between consecutive frames: $\\Delta p_k^i = p_k^i - p_k^{i-1}$, where i = 2, ..., n and k = 1, ..., m. In the second stage, the proposed EBDM removes background effects by comparing each patch with the average of its neighboring patches within the same frame. That is,\n$\\delta p_k^i = p_k^i - \\frac{1}{|N(k)|} \\sum_{l \\in N(k)} p_l^i$,\nwhere $N(k)$ denotes the set of neighboring patches of the k - th patch. The total difference for each patch is then given by\n$S_k^i = |\\Delta p_k^i| + |\\delta p_k^i|$.\nLet $A_i$ denote the weight scores computed by applying an exponential kernel function over the total differences $S_k^i$. The calculation of $A_i$ as shown in Exp. (4):\n$A_i = \\frac{e^{\\alpha S_k^i}}{\\sum_{j}e^{\\alpha S_k^i}}$\nHerein, the $\\alpha$ denotes the Hyperparameters. The weight scores are expanded to match the original patch structure. These expanded scores are then used to adjust the total difference scores $S_k^i$ for all patches in the frame. Let $S^{Behavior}$ denote the final representation, the calculated as shown in Exp. (5):\n$S^{Behavior} = A_i \\cdot S_i$\nAccording to Exp. (4), the $S^{Behavior}$ can receive higher weights, while background patches are weighed close to zero. Unlike $g^{scene}$, where all pixels in each frame have nearly equal influence in violent surveillance scenes, $S^{Behavior}$ places a strong emphasis on locations of violent behavior. Since the relationship between $S^{Behavior}$ and $g^{scene}$ is complementary, they are added together and fed into a lightweight temporal sequence module for further processing.\nTill now, the proposed scene channel and behavior channel extract frame-level background and action features using the CLIP encoder. These features contain instantaneous information but lack the global temporal context crucial for WSVM tasks. In this section, a lightweight temporal sequence module is proposed, which is similar to the standard Receptance Weighted Key Value (RWKV) [36] encoder, including self-attention, layer normalization (LN), and feed-forward network (FFN). Since the positions of frames in sequence data are already fixed, there is no need to apply positional encoding when training the temporal sequence module. The main difference between this approach and traditional RWKV or other Transformer-like encoders lies in the computation of self-attention, which is based on relative distance rather than feature similarity. This mechanism makes the model more lightweight, as it effectively reduces the complexity of attention computation by considering only the relative distance between frames, without the need to evaluate high-dimensional feature similarity. This helps reduce computational costs and makes training and inference more efficient.\nSpecifically, in the design of self-attention, let u denote adjacency matrix. Since the similarity between i-th and j-th frames is only determined by their relative temporal distance, the value of u is calculated as $u = e^{\\frac{-|i-j|}{\\sigma}}$, where $\\sigma$ is a hyper-parameter to control the range of influence of distance relation. In this work, the input of Lite time-series module is the summation feature of $f^{scene}$ and $S^{Behavior}$.\nThe operation of the time series module can be treated as a two-stage process. In the first stage, the time series model utilizes the attention mechanism of u, computed through Softmax and Layer Normalization (LN), to incorporate temporal information into the features, thereby analyzing the input time series. Let $\\Psi$ denote the feature with temporal information fused after the first step. The operation of the first stage is represented as shown in Exp. (6):\n$\\Psi = LN(Softmax(u)(S^{Behavior} + f^{scene}))$,\nIn the second stage, $\\Psi$ is processed through a Feed-Forward Network (FFN), followed by a residual connection, and then through Layer Normalization to obtain the result. Let $\\psi$ denote the output after the non-linear transformation and residual connection in the second step. The operation of the second stage is represented as shown in Exp. (7):\n$\\psi = LN(FFN(\\Psi) + \\Psi)$.\nAfter completing the time series processing, in the Implicit Branch, a coarse-grained binary classifier is integrated. The classification objective function is adopted for the classification branch, which is presented as shown in Exp. (8):\n$L_{BCE} = \\frac{1}{N} \\sum_{i=1}^N [y_i log(\\hat{y_i}) + (1-y_i) log(1 - \\hat{y_i})]$,\nwhere $L_{BCE}$ represents the cross-entropy loss, N is the number of samples, $y_i$ is the true label of the i-th sample and $\\hat{y_i}$ is the predicted probability for the i -th sample. The above provides the complete content of the implicit branch; next, the explicit branch will be introduced.\n\nThe goal of the explicit branch is to perform fine-grained multi-class classification by utilizing contrastive learning between text and video. This branch is divided into two parts: rule generation based on YoloWorld and data mining, and contrastive learning between text and video."}, {"title": "V. MODEL PERFORMANCE", "content": "In this section, relevant experimental performance and analysis are presented.\n\nThe experiments utilized two popular datasets from the WSVM domain: UCF-Crime [6] and XD-Violence [21]. XD-Violence, the largest publicly accessible dataset in violence monitoring, contains 4,754 videos amounting to 217 hours and spans six categories of violent incidents: verbal abuse, car accidents, explosions, fights, riots, and shootings. This dataset is randomly split into a training set with 3,954 videos and a test set with 800 videos, where the test set includes 500 violent and 300 non-violent videos. The UCF-Crime dataset comprises 1,900 real-world surveillance videos, with 1,610 videos designated for training and 290 for testing.\nIn terms of evaluation metrics, the experiment follows the setup described in Chapter III. Specifically, in the XD-Violence dataset, the evaluation metric $AP_M$ from Exp. (1) is used, while in the UCF-Crime dataset, the metric $A_M$ from Exp. (2) is applied. This configuration is consistent with previous studies [4][6][8][17][18][22][23][32].\nIn practice, this study follows previous works [17][18][32] by using a frozen CLIP (ViT-B/16) to extract features from video frames. For the XD-Violence and UCF-Crime datasets, the experiment process 1 out of 16 frames on UCF-Crime dataset. During the training stage, the maximum length of training videos is set to 256; videos exceeding this limit are sampled down to this length. For all datasets, the length of the learnable prompts prefixed to text labels is set to 8. For each image patch $p_i$, the image is resized to 224 \u00d7 224, and a sliding window of 32 \u00d7 32 with a stride of 32 is used to generate multiple patches.\nFor hyperparameter settings, $\\alpha$ in Exp. (3) is uniformly set to 0.25. The $\\lambda_1$ in $L_{align}$ is 5 \u00d7 10-4 for the XD-Violence dataset and 1 \u00d7 10-3 for the UCF-Crime dataset. The $\\lambda_2$ in $L_{contrast}$ is 6 \u00d7 10-4 for the XD-Violence dataset and 2 \u00d7 10-3 for the UCF-Crime dataset.\nFor model optimization, the AdamW optimizer with a learning rate of 1e-4 is used to train the model on an NVIDIA A100 SXM4, with a batch size set to 256. In the data mining practice, an Intel Core i7-10700K is utilized.\nThe proposed RuleVM can simultaneously realize coarse-grained and fine-grained violence detection. The experiment presents the performance of the proposed RuleVM and compares it with several state-of-the-art methods on coarse-grained and fine-grained WSVM tasks."}, {"title": "VI. CONCLUSION", "content": "This paper presents an interpretable RuleVM system for violence surveillance tasks. The system utilizes a dual-branch structure to achieve accurate violence detection in both coarse-grained and fine-grained classifications, successfully integrating explicit and implicit knowledge representations and significantly enhancing the system's interpretability.\nIn the implicit branch, the system uses only image features for coarse-grained binary classification, with image feature extraction divided into two channels: one responsible for scene frame extraction and the other focusing on action extraction. These are then combined and fed into a lightweight temporal attention module for sequence analysis, enabling effective feature extraction across different violent scenarios. In the explicit branch, the system performs fine-grained classification using language-image alignment. For the text modality design, the system uses the YOLO-World model to detect objects and actions in video frames, identifying high-frequency itemset through data mining methods as video descriptors. This dual-branch design provides RuleVM with strong interpretability in violence surveillance, balancing the requirements of coarse-grained and fine-grained detection.\nUsing XD-Violence and UCF-Crime as experimental datasets, results show that the proposed RuleVM significantly outperforms existing methods in terms of AP and AUC evaluation metrics. Additionally, the system reveals an interesting pattern: as crowd size increases, the level of violence also rises. This further demonstrates the system's powerful interpretability and its substantial potential for applications in areas such as intelligent surveillance and traffic monitoring."}]}