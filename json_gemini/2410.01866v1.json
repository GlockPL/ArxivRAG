{"title": "HOUSE OF CARDS: MASSIVE WEIGHTS IN LLMS", "authors": ["Jaehoon Oh", "Seungjun Shin", "Dokwan Oh"], "abstract": "Massive activations, which manifest in specific feature dimensions of hidden states, introduce a significant bias in large language models (LLMs), leading to an overemphasis on the corresponding token. In this paper, we identify that massive activations originate not from the hidden state but from the intermediate state of a feed-forward network module in an early layer. Expanding on the previous observation that massive activations occur only in specific feature dimensions, we dive deep into the weights that cause massive activations. Specifically, we define top-k massive weights as the weights that contribute to the dimensions with the top-k magnitudes in the intermediate state. When these massive weights are set to zero, the functionality of LLMs is entirely disrupted. However, when all weights except for massive weights are set to zero, it results in a relatively minor performance drop, even though a much larger number of weights are set to zero. This implies that during the pre-training process, learning is dominantly focused on massive weights. Building on this observation, we propose a simple plug-and-play method called MacDrop (massive weights curriculum dropout), to rely less on massive weights during parameter-efficient fine-tuning. This method applies dropout to the pre-trained massive weights, starting with a high dropout probability and gradually decreasing it as fine-tuning progresses. Through experiments, we demonstrate that MacDrop generally improves performance across zero-shot downstream tasks and generation tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), such as GPT (Achiam et al., 2023) and Llama (Touvron et al., 2023; Dubey et al., 2024), have achieved remarkable success across diverse natural language tasks (Roziere et al., 2023; Mitra et al., 2024; Labrak et al., 2024; Wu et al., 2023). Their success is largely attributed to the pre-training phase, during which they are trained on extensive high-quality corpora datasets to predict the next token (Longpre et al., 2024; Zhao et al., 2024; Shen et al., 2023). However, despite the impressive achievements of LLMs, a crucial gap remains in our understanding of the underlying mechanisms that drive their remarkable performance.\nRecently, Xiao et al. (2024) uncovered an intriguing phenomenon in LLMs, referred to as attention sinks: an unexpectedly large portion of attention is directed toward the initial tokens, regardless of their semantic context, after a small number of early layers. They demonstrated that under a restricted budget, focusing attention solely on recent window leads to poor performance, and that performance is recovered when initial tokens are included. Based on this observation, they proposed StreamingLLM, which retains the key-value caches of the initial sink tokens and the recent tokens for streaming use of LLMs. Yu et al. (2024) further investigated the attention sinks phenomenon, finding that attention sinks can appear both in the initial tokens and in later tokens with less semantic importance (e.g., \u2018.' and \u2018\\n'). They showed that when sink tokens appear later in a sequence, sink tokens can potentially result in performance degradation. Inspired by this observation, they proposed a head-wise attention calibration technique without requiring additional training. Concurrently, Sun et al. (2024) discovered the existence of massive activations in the hidden states of LLMs, with magnitudes substantially larger than the others. Massive activations are jointly identified based on their sequence and feature dimensions within the hidden states. Specifically, massive activations occur at the initial tokens and weak semantic tokens according to the model, and are consistently present in only a few fixed feature dimensions. Moreover, they connected massive activations with attention sinks, suggesting that massive activations inject implicit bias into the self-attention mechanism throughout the pre-training phase."}, {"title": "2 MASSIVE WEIGHTS", "content": "In this section, we review the key observations on massive activations reported by Sun et al. (2024) and extend the analysis by exploring various states using the bos token, which was not covered. Based on this expanded analysis, we formally define top-k massive weights in a specific layer and investigate their importance through two opposite types of attacks."}, {"title": "2.1 PREREQUISITE: MASSIVE ACTIVATIONS", "content": "Autoregressive Transformers (Vaswani et al., 2017) are structured with L decoding layers. Each layer $l \\in [1, L]$ includes an attention (ATTN) module and a feed-forward network (FFN) module. These modules are connected via residual connections (He et al., 2016), each following a layer normalization (LN) layer (Ba, 2016). The previous hidden state $h_{l-1}$ is fed into layer l and processed to produce the subsequent hidden state $h_l$:\n$h_l = h'_l + FFN(LN(h'_l)), where h'_l = h_{l-1} + ATTN(LN(h_{l-1})) \t(1)$\nSun et al. (2024) primarily concentrated on the activations within hidden states, identifying that certain activations exhibit exceptionally large magnitudes, which they termed massive activations. Massive activations are observed at the starting position (i.e., input-agnostic) or at the delimiter tokens, depending on the model. Furthermore, these activations are confined to a small number of fixed feature dimensions, even within these tokens. These activations initially emerge after passing through several early layers and then decreases as they near the last layer.\nMassive activations are strongly tied to the attention sinks phenomenon, as identified by Xiao et al. (2024), in which attention is abnormally concentrated on a small subset of tokens. In detail, a given query state tends to have positive cosine similarity with the key states of the tokens exhibiting massive activations, and negative cosine similarity with those of other tokens. Consequently, attention is heavily skewed toward the tokens associated with massive activations."}, {"title": "2.2 FURTHER ANALYSIS ON MASSIVE ACITVATIONS", "content": "We primarily utilize the Llama-3-8B model (Dubey et al., 2024) and explicitly specify other models when necessary."}, {"title": "2.3 MASSIVE WEIGHTS", "content": "Massive weights are defined based on massive activations in the intermediate state at layer l, denoted as $h^{inter}$, when the bos token is fed into LLMs. To elaborate, we define the rows in the projection matrix $W^{J}_{up}$ (and $W_{gate}$, if it exists) that correspond to the indices of the top-k magnitudes in $h^{inter}$ as top-k massive weights, depicted in Figure 1(a). It is noted that massive weights are defined within one specific layer, which means the number of massive weights is significantly smaller compared to the total number of parameters in LLMs. For example, in Llama-3-8B, the number of top-k massive weights is calculated as $2 \\times k \\times 4096$, where 4096 represents the dimensions of hidden state. If k is set to 5, massive weights account for approximately 0.0005% of the total parameters in Llama-3-8B, approximately 0.0001% in Llama-3-70B, and approximately 0.00004% in Llama-3.1-405B.\nMassive weights are extremely small in quantity, their impact is tremendous. To assess the significance of massive weights, we conduct two types of attacks: top-k zeroing and top-k retaining. Note that these attacks only affect the $W_{up}$ and $W_{gate}$ projection matrices in layer l, where massive weights are present. The first attack is to set the top-k massive weights to zero (i.e., darker orange weights in Figure 1(a)). In essence, this attack is very similar to the one proposed in Sun et al. (2024), where massive activations in the hidden state are zeroed out in a single layer. The difference is that their attack targets the hidden state, while our attack targets the intermediate state. The second attack is to set all weights to zero except for top-k massive weights (i.e., lighter orange weights in Figure 1(a)). That is, the number of rows being damaged in each attack is k and the dimensions of intermediate state $\\times k$, respectively."}, {"title": "3 MASSIVE WEIGHTS CURRICULUM DROPOUT", "content": "In this section, we propose a straightforward plug-and-play method, termed massive weights curriculum dropout (MacDrop), during parameter-efficient fine-tuning such as low rank adaptation (Hu et al., 2022). This method applies dropout to the pre-trained massive weights with a curriculum that gradually reduces the dropout probability. The reason for applying dropout to weights (Wan et al., 2013) instead of activations (Srivastava et al., 2014) is that the number of massive activations is only k, but that of massive weights is $k \\times d$, where d is the dimension of the hidden states. Note that our method is not applied to the trainable parameters of adapters. Therefore, MacDrop can be applied orthogonally to the process of training the adapter. Algorithm 1 describes MacDrop in a pseudo PyTorch (Paszke et al., 2019) style, and is implemented within the trainer code of transformers. Initially, massive weights are identified using the bos token before fine-tuning (Lines 1-3). Subsequently, an adapter is trained while the pre-trained massive weights are dropped. Meanwhile, a curriculum strategy is applied to progressively enable the use of the original pre-trained weights without masking. Note that in Line 8, 'model' includes both the masked pre-trained network and a trainable adapter.\nMacDrop is motivated by the observation that massive weights are predominantly learned during the pre-training phase, and that zeroing them can severely undermine LLMs. Therefore, at the early stages of fine-tuning, the objective is to reduce the reliance on massive weights, as their excessive dominance may lead the model to over-rely on specific patterns. Moreover, considering that the undamaged pre-trained model is used after fine-tuning is finished, we develop a strategy to adjust the dropout rate using a curriculum."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 ZERO-SHOT DOWNSTREAM TASK", "content": "We fine-tune the Llama-3-8B and Mistral-7B using the alpaca_gpt4_en dataset (Peng et al., 2023) for 3 epochs (579 steps), and evaluate on five zero-shot tasks. We use two parameter-efficient fine-tuning methods, LoRA (Hu et al., 2022) and DoRA (yang Liu et al., 2024). DoRA decomposes the pre-trained weights into two components, magnitude and direction, and applies LoRA to the direction component. Our method is based on the implementation of Llama-Factory (Zheng et al., 2024). For MacDrop, k and $p_0$ are set to 5 and 0.8, respectively. Details for implementations are explained in Appendix A. Table 2 presents the results on zero-shot downstream tasks. For both the models and methods, MacDrop consistently leads to performance gains, especially in ARC-Easy and ARC-Challenge tasks."}, {"title": "4.2 GENERATION TASK", "content": "We evaluate on the generated texts of the same models in Section 4.1 using the Spec-Bench dataset (Xia et al., 2024). This benchmark includes six subtasks, each containing 80 instances: multi-turn (MT) conversation from MT-bench (Zheng et al., 2023), translation from WMT14 DE-EN (Bojar et al., 2014), summarization from CNN/Daily Mail (Nallapati et al., 2016), question answering (QA) from Natural Questions (Kwiatkowski et al., 2019), mathematical reasoning from GSM8K (Cobbe et al., 2021), and retrieval-augmented generation (RAG) from Natural Questions (Kwiatkowski et al., 2019). We utilize the direct assessment of Prometheus-2-7B (Kim et al., 2024) to evaluate the generated texts using a 5-point Likert scale. Prometheus-2-7B is an open-source language model specifically designed for evaluation purposes. Table 3 presents the results on generation tasks. MT-1 and MT-2 indicate the first turn and second turn, respectively. Unfortunately, MacDrop shows limited performance improvements in generation tasks. Examples of the generated texts and judgements are provided in Appendix E."}, {"title": "4.3 ABLATION STUDY", "content": "We further provide ablation studies related to MacDrop using Llama-3-8B. Unless otherwise stated, for MacDrop, k and $p_0$ are set to 5 and 0.8."}, {"title": "4.3.1 DROPOUT SCOPE AND PROBABILITY", "content": "We investigate the effect of dropout scope and probability compared to the original performance achieved through LoRA without dropout. This ablation study is also conducted on the $W_{up}$ and $W_{gate}$ projection matrices in layer l. The dropout scope is divided into three categories: all weights, massive weights, all weights except for massive weights. Additionally, to assess the impact of dropout probability, it is kept constant throughout the fine-tuning process, without using a curriculum."}, {"title": "4.3.2 CURRICULUM METHODS AND INITIAL DROPOUT PROBABILITY", "content": "We investigate the effect of curriculum methods and initial dropout probability $p_0$ in MacDrop, when LoRA is applied. We compare four curriculum methods: step-wise linear (Step), before epoch-wise linear (Epoch(before)), after epoch-wise linear (Epoch(after)), and exponential (Exp.). In formula, Step is defined as $p = p_0 \\times (1 - \\frac{t}{T})$. Epoch(before) and Epoch(after) are defined as $p = p_0 \\times (1 - \\frac{t_{epoch}-1}{T_{epoch}})$ and $p = p_0 \\times (1 - \\frac{t_{epoch}}{T_{epoch}})$, respectively. Exp. is defined as $p = p_0 \\times exp(-\\alpha t_{step})$. Figure 7 describes dropout probability p according to curriculum methods, when $p_0$ is 1.0. The distinct difference between Epoch(before) and Epoch(after) is that at the final epoch, the former continues to apply dropout to the pre-trained massive weights with a probability of $p_0 \\times \\frac{1}{T_{epoch}}$, while the latter fully utilizes the pre-trained massive weights."}, {"title": "5 RELATED WORK", "content": "The attention sinks phenomenon and their importance, uncovered by Xiao et al. (2024), have been widely used to compress key-value caches. For quantization, KVQuant (Hooper et al., 2024) applies attention sink-aware quantization, which retains only the first token in fp16. CushionCache (Son et al., 2024) inserts sink tokens into the prefix to mitigate massive activations in the middle of the sequence, enhancing the performance of quantized models. For token eviction and token merging, sink tokens are never evicted or merged; they remain unchanged (Xiao et al., 2024; Ge et al., 2024; Li et al., 2024; Zhang et al., 2023; Wang et al., 2024; Zhang et al., 2024).\nNevertheless, there has been limited in-depth research on the phenomenon itself. In fact, the idea of global attentions, such as [CLS] and [SEP] tokens-similar to attention sinks-was introduced and emphasized even before the LLM era (Zaheer et al., 2020; Beltagy et al., 2020). In the LLM era, Yu et al. (2024) showed that sink tokens can appear not only at the beginning of a sentence but also in the middle, and they are often shown to be nonsemantic (e.g., '.'). Sun et al. (2024) discovered the presence of massive activations in the hidden state space of sink tokens, demonstrating that massive activations trigger the attention sinks phenomenon. Meanwhile, in vision transformers, similar phenomenon is observed (Darcet et al., 2024). They showed that training with register tokens, which is additional meaningless tokens similar to sink tokens, resulted in improved dense prediction and interpretability. Different from previous work, we explore this phenomenon in the weight space."}, {"title": "6 CONCLUSION", "content": "In this paper, we explore the weight space of LLMs and identify the presence of massive weights within a FFN module in an early layer, which are predominantly pre-trained and have a significant impact on the performance of LLMs. Based on our observation, we propose a plug-and-play fine-tuning method called MacDrop, which applies dropout to the pre-trained massive weights, rather than to the parameters of adapters, during parameter-efficient fine-tuning. We hope that our findings will inspire future research in weight space of LLMs, including model merging (Li et al., 2023) and model editing (Yao et al., 2023)."}]}