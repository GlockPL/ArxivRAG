{"title": "A Review on the Applications of Transformer-based language models for Nucleotide Sequence Analysis", "authors": ["Nimisha Ghosh", "Daniele Santoni", "Indrajit Saha", "Giovanni Felici"], "abstract": "In recent times, Transformer-based language models are making quite an impact in the field of natural language processing. As relevant parallels can be drawn between biological sequences and natural languages, the models used in NLP can be easily extended and adapted for various applications in bioinformatics. In this regard, this paper introduces the major developments of Transformer-based models in the recent past in the context of nucleotide sequences. We have reviewed and analysed a large number of application-based papers on this subject, giving evidence of the main characterizing features and to different approaches that may be adopted to customize such powerful computational machines. We have also provided a structured description of the functioning of Transformers, that may enable even first time users to grab the essence of such complex architectures. We believe this review will help the scientific community in understanding the various applications of Transformer-based language models to nucleotide sequences. This work will motivate the readers to build on these methodologies to tackle also various other problems in the field of bioinformatics.", "sections": [{"title": "1. Introduction", "content": "Starting from the 1940s, scientists began to study the amino acid composition of proteins in order to characterize tissues and species by their amino acid frequencies [1]. Insulin was the first protein to be sequenced in 1951 and 1952 (chains of bovine insulin B and A, respectively) by Sanger [2] that led to him winning the Nobel in chemistry (1958). In the following years, as new technologies were developed, many other sequences became available thereby opening new frontiers in Biology and Chemistry. In fact, in the 70s a consistent number of sequences was made available, and several new issues related to protein sequences arose. Many works show that classification methods are able to separate proteins into different families [3\u20137]) based exclusively on their sequences. Also at genomic level, as a significant number of fully sequenced genomes was made available, scientists started to study and compare genome features in terms of similarity, complexity, information content and statistical properties. In the late 90s, the first whole-genome studies provided insights on genome composition in terms of subsequences or k-mers [8, 9]. Recently, Natural Language Processing (NLP) approaches have been showing a significant impact in bioinformatics ranging from DNA/RNA sequence analysis to computational biology [10, 11]. Since biological sequences can be seen as words on given alphabets (the four nucleotides for genomic sequences) or as texts (where words are k-mers) an NLP approach seems to be particularly suitable and effective to investigate and extract information in this context.\nFor more than two decades now, the abilities of neural networks in NLP tasks have been exploited, and series of word embedding technologies have been used for unique representation of text [12, 13].\nAs an example, Word2Vec [14, 15] can be considered as a representative model which uses shallow neural network for obtaining vector representation of words. Word2vec utilises either Continuous Skip-Gram (CSG) or Continuous Bag-of-Words (CBOW). Given a current word, CSG predicts the surrounding words while CBOW predicts the current word based on the context. The evolution of language models in NLP can be accredited to the rapid development of deep learning technologies. Conventional RNN-based models such as Bidirectional Recurrent Neural Network (Bi-RNN) [16], Long-Short Term Memory (LSTM) [17] and Gated Recurrent Unit (GRU) [18] attempt to encode the entire sequence into a finite length vector. Such deep learning models have been extensively used in bioinformatics, e.g., for the prediction of Transcription Factor Binding Sites [19]. However, such models suffer from vanishing gradient (gradients become too small during backpropagation) as well as low-efficiency problems. Subsequently, Transformer [20] models came into existence which did not have any RNN-based network structures and was dependent on multi-head attention mechanism (described later in Section 2) which focuses on computing similarity scores between words in a sentence. The advantage of Transformers is that they are not dependent on past hidden states to understand the dependency on previous words. To avoid vanishing gradient problem and performance deterioration caused by long-term dependency, Transformers process a sentence as a whole to enable parallel computation. Considering the aforementioned advantages of parallel computation and multi-head attention, this review article focuses on the use of Transformer-based language models in bioinformatics, in particular their applications in processing nucleotide sequences.\nTransformer-based language models can be categorised into either trained from scratch or pre-trained models. Models which are trained from scratch are based on the direct training of all model parameters from the initial stages using datasets specific to a task, and takes many iterations to completely converge. One such example is Transformer-XL [21] which uses segment-level recurrent mechanism and a novel position encoding scheme to model long text. Sparse Transformers [22] by Zhao et al. have introduced a variation in architecture of traditional Transformers to train deeper networks as well as have recomputed attention matrices to save memory. To improve the efficiency of Transformers, Kitaev et al. [23] have proposed Reformer which uses locality-sensitive hashing and reversible residual layers. Although self-attention is a very powerful component in Transformers, its computational and memory"}, {"title": "2. The Essence of Transformers", "content": "Moving forward with our description, we continue with the example of natural language processing: we assume to have a corpus made of a number of sentences, composed of words; such words belong to a finite vocabulary. Such sentences may be in a special relationship, i.e., one sentence may be connected to another because it is the answer to a question posed by the first sentence, or its translation; when such relations are present, we will consider the first sentence to be the input and the second to be the output. Changing the alphabet to nucleotides and sentences to DNA sequences respectively would, with a sufficient level of approximation, allow to use Transformers in bioinformatics.\nFor the ease of understanding and readability, words and tokens are used equivalently to describe the workings of Transformers."}, {"title": "2.1. PreTraining and FineTuning", "content": "As mentioned earlier, Transformers are a learning mechanism; their learning proceeds with trial and error. The many parameters that define the system are initialized at random, then an input-output pair is passed through it, and an output is predicted; if it is too distant from the actual output, backpropagation is used to modify the parameters (weights) of the system to try and reduce such an error. This is done iteratively for all the training samples, several times, until the error is small enough. But how does this process declinate for the Transformer and language approach? The training is mainly of two types, according to the objective of the application: pre-training and fine tuning. The easiest way to look at it, is that pre-training is unsupervised learning, while finetuning is supervised learning. In pre-training, some words in the input are masked; the system then aims to predict a masked word according to the words that precede them; this way, a prediction error may arise and such an error is backpropagated to adjust the weights. In pre-training, one may assume that the network, through adjusting its weight to reduce errors on masked words, learns the structure of the language, the similarity among words, their relations and the like. Once the system has been pre-trained, finetuning for a specific task may take place: according to a series of input-output training pairs, the weights in the Transformers are further adjusted to generate an output when provided with an input. This is the case of question answering, translation, or text classification: for example, the system may be provided with a large"}, {"title": "2.2. Encoding and Decoding", "content": "One common way of describing a Transformer is as something composed of an Encoder and a Decoder. These are two paired machines: the Encoder takes a sentence and embeds it into a set of numbers; the Decoder has the purpose of generating text, one word at a time, based on the embedding of an input sentence. Fig. 1(a) and 1(b) respectively show the different components of Encoders and Decoders. As shown in Fig. 1(c), there are multiple blocks of Encoder and Decoder in a Transformer.\nWe start from describing the main steps of the Encoder when one sentence is presented to it:\n1. each word in the sentence is associated with an id; the same word will receive the same id also when it appears in other sentences. If the number of words in the sentence exceeds a maximum value m, the sentence will be divided into chunks, and each chunk is considered as a separate sentence; if the length is below m, the sequence of ids associated to the words of the sentence will be filled up with padding.\n2. the sentence is now represented by a vector of length m. Such vector is then expanded into a matrix of n rows of dimension m. The parameter n represents the number of additional features that are attached to each word. Such expansion may be viewed as an enrichment of the role of each word, that is allowed to be described with an additional set of n numbers. The dimension of such enrichment is a design decision; the values of these n numbers to be associated with each word are initialized randomly and refined in the training process.\n3. It is important here to note that the words of a sentence are fed in parallel to an Encoder, thus the order in which the words appear in the sentence is not directly taken into account. This is taken care of by positional encoding. Furthermore, the main essence of Transformers lies in introducing self-attention mechanism, where the relative position of the words in a sentence are explored to identify the relationship among such words. We skip the details for brevity. Interested readers are requested to look into [20] for further clarification.\n4. At this point, the matrix representing the sentence, also referred to as Q for Query, is processed via a number of transformations based on different types of neural networks, that have the purpose of extracting the relevant information from each matrix, aligning their dimensions, and combining them with typical neural network activation function. Here, two additional matrices of that same dimensions as Q:K and V (for Key and Value) are introduced; the three matrices Q, K, V are each passed through a linear network, that combines their values with additional weights. Subsequently, these matrices are combined together with a non-linear network that uses a RELU activation and a softmax function. The main purpose of K and V is to add additional degrees of freedom to the system, with additional parameters that can be modified in the training;\n5. At the end of the encoding process, which encompasses several stacked layers of Encoders, the output is an embedding that, through the many transformation undertaken by the network layers, incorporates the original"}, {"title": "3. Applications of Transformer-based language models for Nucleotide sequences", "content": "In this section, a summary of different applications of Transformer-based language models for nucleotide sequences is provided along with their main advantages. As reported in Table 1 the considered studies are partitioned into six areas according to the biological context they refer to: Promoter and Enhancer, Methylation, Reads, Binding and Miscellaneous."}, {"title": "3.1. Promoter and Enhancer", "content": "A promoter region is a DNA sequence located upstream of the Transcription Starting Site of a gene containing specific motifs that proteins like Transcription Factors can bind to in order to regulate the gene transcription. It is a challenging task to identify promoter regions in order to understand the mechanisms that regulate gene expression. Although several deep learning models based on CNN [37] and LSTM [38] have been proposed, the prediction performance is still not satisfactory. To address this problem, [39] has proposed a predictor based on BERT and SHapley Additive exPlanations (SHAP) [40] analysis along with machine learning algorithms. Initially, they have used BERT to extract features from DNA sequences and subsequently SHAP is used for reducing the dimension of features. Next, such features are used as inputs for different machine learning algorithms to predict DNA promoters as well as their strength. Among the several learning algorithms as used in this work, XGBoost provides the best results and is therefore selected as the desired machine learning algorithm (Fig. 2(a)). In [41], Wang et al. have proposed miProBERT ((Fig. 2(b))) which predicts miRNA promoters directly from gene sequences without considering any structural or biological signal. They have used the pretrained model DNABERT and subsequently finetuned it on the gene promoter dataset to scan the upstream regions of miRNA, thereby identifying 665 miRNA promoters. They have further used a random substitution strategy to create a negative dataset. miProBERT shows better performance than many other miRNA promoter prediction methods [42, 43] showing 78.13% precision and 75.76% recall. The authors have also verified the predicted promoter regions by analysing conservation, CpG content and histone marks. However, this work has certain weaknesses such as no biological experimental validation has been performed for the identified miRNA promoters. Also, due to the presence of alternative promoters it is possible for the same miRNA to have multiple promoters. Thus, the role of alternative promoters needs to be explored as well. Transformer models have also been used in [44] for accurate promoter prediction of freshwater cyanobacterium Synechocystis sp. PCC 6803 and the fastest growing cyanobacterium Synechococcus elongatus sp. UTEX 2973. The promoters and non-promoters from Synechococcus elongatus sp. UTEX 2973 have been used to train the model which achieved an F1 score of 0.92 and an AUROC of 0.97. The validation has been performed with promoters from Synechocystis sp. PCC 6803, during which F1 score of 0.91 and AUROC score of 0.96 are achieved. The authors have further developed TSSNote-CyaPromBERT which is an integrated platform for dataset extraction, model training and also prediction of promoter from dRNA-seq datasets. Although the work shows quite competitive performance, it suffers from certain drawbacks. The model does not show very good results when tested on datasets of some other species. Also, the model works with sequential information thereby failing to capture complex interactions during transcription thereby leading to false positives in the regional scanning mode. Most of the works discussed so far have used a typical BERT model for the tasks. Such models rely on token prediction and may ignore domain knowledge such as repetitive elements, DNA binding sites etc. In [45], the authors have incorporated motif prediction along with token prediction to take care of such domain knowledge by using ELECTRA as the Transformer model. Promoter and transcription factor binding site predictions have also been explored in [45] where the authors have proposed a self-supervised Motif-oriented DNA (MoDNA) pre-training framework that can be finetuned for many downstream tasks like promoter and TFBS prediction.\nOn the other hand, enhancers are important cis-regulatory elements regulating various biological functions as well as enhancing the transcription of target genes. In this regard, iEnhancer-ELM [46] which is based on BERT-like enhancer language models, have been used to tokenize DNA sequences with multi-scale k-mers to extract contextual information through a multi-head attention mechanism. Initially, they have evaluated the performance of different scale k-mers and later merged them for improved enhancer identification. The final classification is performed using a 2-layer perceptron network. The results show that iEnhancer-ELM scores better than methods which does not use BERT.But, this work does not differentiate between typical enhancers and super enhancers. Super enhancers are used for defining cell identity and regulating transcription of gene as well as they are associated with several known diseases. Usually, they span a large genomic domain by aggregating multiple typical enhancers together based on a specific stitching distance. Typical enhancers made up of a super enhancer are similar to normal enhancers on the genome and are involved in gene transcriptional regulation as well. Thus, it is important to distinguish super-enhancers from typical enhancers. In this regard, Luo et al. [47] proposed SENet which is based on deep neural network model to discriminate between the two categories by using only sequence information. SENet uses dna2vec feature embedding, convolution for extracting local features, attention pooling for retaining finer features, and Transformer for extracting contextual information. More recently Ligeti [48] and colleagues proposed ProkBERT a family of language models for microbiome applications. It was successfully applied to promoter prediction and phage identification, demonstrating strong performances. ProkBERT models leverage transfer learning and self-supervised approaches to effectively handle complex microbial data. The innovative Local Context-Aware (LCA) tokenization enhances traditional transformers by preserving local context, enabling adaptability across diverse bioinformatics tasks."}, {"title": "3.2. Methylation", "content": "DNA methylation is a crucial biological process that primarily involves adding a methyl (CH3) group to the fifth carbon atom of a cytosine ring, but it can also occur through other mechanisms. It is often associated with many processes such as genomic imprinting, X-chromosome inactivation, repression of transposable elements, aging, and carcinogenesis.\nMethylations drive epigenetic regulation of gene expression and have been also used as markers in metagenomic binning. DNA methylation maps can be obtained either by using next-generation sequencing (NGS) approaches such as whole-genome bisulfite sequencing (WGBS) [49] or reduced-representation bisulfite sequencing (RRBS) [50]. However, these techniques are often costly and time-consuming [51]. Also, short-read sequencing prevents bisulfite sequencing from profiling DNA methylation in repetitive genomic regions [52, 53]. These shortcomings have led to the development of computational based approaches such as Transformer-based language models to predict DNA methylation sites. In this regard, iDNA-ABF [54] which is a multi-scale deep biological language learning model has been used to predict DNA methylations based on only genomic sequences. Most importantly, iDNA-ABF captures both sequential and functional semantics information from background genomes. For model construction, the authors have adopted DNABERT [55]. Although, the work shows competent results, the authors neither considered multiple DNA modification sites for prediction nor used taxonomic information as explicit features. These shortcomings have been overcome in [56] where the authors propose model viz. MuLan-Methyl. It is based on five popular Transformer-based language models and aims to identify three types of DNA methylation sites; N6-adenine, N4-cytosine, and 5-hydroxymethylcytosine. The five models (BERT [29], DistilBERT [57], ALBERT [34], XLNet [31] and ELECTRA [58]) are combined together to collectively predict the DNA methylation status (Fig. 2(c)). Furthermore, post-transcriptional 2'-O-methylation (Nm) RNA modification is crucial in various cellular tasks and related to a number of diseases such as hepatocellular carcinoma, lung adenocarcinoma and congenital muscular dystrophy. Techniques such as PCR-based approaches, RNaseH-based approaches [59] and reverse transcription-based approaches [60] are few of the experimental approaches used to infer additional Nm modifications. However, they are slow thus paving the way for further computational methods such as Support Vector Machines [61, 62], Light Gradient Boosting Machines [63], BiLSTM with CNNs [64] etc. BERT2OME has been proposed by [65] et al. to predict Nm modification sites from RNA sequences where the model has been pretrained with RNA segments as input language. Once the pretraining is over, this high dimensional embedding dataset is fed into a 2D convolutional neural network to extract more attributes in order to finally predict RNA 2'-O-methylation modification sites over RNA sequences. Stanojevi\u0107 [66] et al. have proposed Rockfish, a deep learning algorithm that significantly improves read-level 5-methylcytosine detection by using Nanopore sequencing. Rockfish demonstrates strong agreement with whole-genome bisulfite sequencing, requiring fewer reads while providing high confidence in CpG-rich regions. Its efficiency and performance in human and mouse samples make it a versatile tool for studying 5-methylcytosine methylation across species and diseases. Additionally, its adaptable design supports compatibility with evolving sequencing technologies and modification types."}, {"title": "3.3. Reads", "content": "In Next-Generation Sequencing (NGS) short sequenced fragments are called \"reads\"; in most of the cases they are the product of DNA random shearing. Nowadays, the increase in NGS technologies provides a huge amount of data and the need for new methodologies to classify sample reads through machine learning approaches has become a priority. Finding an association between the microbiome and human health is an intriguing yet difficult topic that is feasible with the advent of next-generation sequencing (NGS) and metagenomic methods. Accurate identification is necessary to understand the connection between disease and the microbiome because even species that belong to the same genus might have diverse functions and pathogenicities. In this regard, Gwak et al. [67] have applied Transformer-based embedding consisting of 12 embedding blocks to bacterial genomes in order to accurately identify species at the read-level. The model has been pretrained with Staphylococcus genomes and was finetuned for classifying species within the Staphylococcus genus. With the use of simulated reads of 151 and 251 bp, two distinct models have been trained as well wherein both models achieved an ROC-AUC values of over 0.98. Identification of Human Papillomavirus (HPV) reads in human host genomic data by using Transformer-based pipeline DeepViFi has been explored in [68]. DeepViFi has three components: (i) a Transformer for the production of latent representations of the short-reads, (ii) Classification of the latent representations using Random Forest classifier to determine if the read is HPV positive and (iii) a Light Gradient Boosting model for the sub-family identification of HPV. DeepViFi has achieved a precision-recall AUC of 0.94, 0.94, 0.91, and 0.16 for detecting HPV reads on the easy, intermediate, hard and non-human test sets respectively. It is important to note that though DeepViFi shows better performance than many existing methods, it does not consider domain-specific knowledge of viruses. In [69], a hierarchical BERT model named ViBE has been used for the detection of eukaryotic viruses from metagenome sequencing data and classify them at the order level. ViBE has been adapted from BERT and DNABERT architecture and consists of an embedding and a classification layer. Both pre-training and finetuning are performed to classify the viruses. The pre-training step includes unsupervised learning of a large set of virus genomic sequences while the fine tuning involves the paired end reads generated from the viral and bacterial genome sequences. In their study, the authors have used three classifiers encompassing a domain-level classifier and two order-level classifiers for DNA and RNA viruses. Domain-level classifier is responsible for the screening of viral sequences while the order-level classifiers are two separate predictors for DNA and RNA viruses. With an AUROC of 0.98 ViBE performed much better when compared to other methods such as CHEER [70] and DVF [71].\nExtracting information from short contigs is yet another area where Transformers can be effectively used. Tang et al. [72] have developed PLASMe (Fig. 2 (d)) for plasmid detection using order-specific Transformer models. In this work, the authors have treated plasmids as a language defined on a vocabulary of proteins, thereby leveraging the use of Transformer to learn the importance of proteins and their associations for plasmid identification. To maximise the capacity of feature learning, 35 Transformer-based models are designed for plasmid detection. The authors have conducted the experiments both at the nucleotide and protein levels. Despite the desirable results shown by the authors in [72], the work has certain shortcomings. This work has considered only proteins of plasmids but not proteins"}, {"title": "3.4. Binding", "content": "Binding between molecules is a thermodynamically driven process, that in the case of nucleic acids can be investigated through primary sequences by applying a machine learning approach. Cell functions which include mRNA modification, splicing, localisation and translation are regulated by interactions between RNA sequences and RNA-binding proteins (RBPs). In order to predict such interactions, statistical models especially support vector machine are widely used. Moreover, many deep learning models are also explored. However, the existing models lack interpretability and are mostly of complex nature leading to the development of new models. In this regard, Yamada et al. [75] have proposed BERT-RBP (Fig. 2 (e)) model which is pretrained on a human reference genome to predict the RBP-binding property of RNA sequences. Furthermore, attention analysis is applied on the finetuned model which shows that BERT-RBP is able to translate biological contexts from only RNA sequences (Fig. 2(e)).\nIdentifying DNA-protein binding is another aspect where BERT can be specifically used. In [76], Luo et al. have proposed TFBert which considers DNA sequences as natural sentences and k-mer nucleotides as words, thereby extracting upstream and downstream nucleotide information for 690 unlabeled ChIP-seq datasets. Experiments show that the average AUC is 94.7% which outperforms most of the existing popular methods. Wang et al. [77] have proposed SA-Net which uses k-mer (4-mer provided the best results) embedding to encode RNA sequences and thereafter utilises self-attention based neural network to extract sequence features. The experimental results show that SA-Net outperforms CNN and CNN-BLSTM models in sequence feature extraction. In the context of binding prediction, it is also a challenging task to predict which antigens a T-cell receptor may bind to. TCR-BERT has been proposed in [78] which is a deep learning model with self-supervised transfer learning. TCR-BERT uses unlabeled TCR sequences to learn about TCR sequences and thereby may help in many downstream applications as well. Apart from being a useful tool for T-cell scientists, TCR-BERT also helps in solving challenging problems such as designing novel TCR sequences with engineered binding affinities. TF-DNA binding is explored in [79] where the authors have developed GHTNet (General Hybrid Transformer Network), a Transformer-based model to predict TF-DNA binding specificity. Transcription Factors (TFs) are proteins that bind to DNA and regulate gene expression. Prediction of TF-DNA binding is crucial for understanding how gene expression is controlled by TFs. GHTNet uses self-attention"}, {"title": "3.5. Miscellaneous", "content": "A generalised pretrained large language model DNAGPT which is trained on over 200 billion base pairs from all mammals is proposed in [82]. The authors have improved upon the classical GPT (Generative PreTrained Transformer) model by incorporating binary classification task of DNA sequence order and a numerical regression task of guanine-cytosine content prediction. The model behaves as a comprehensive token language as well. Thus, DNAGPT can work with different DNA analysis tasks while processing both sequence and numerical data. By comparing with many state-of-the-art models, the authors have proven the superiority of DNAGPT in genomic signal, region recognition, mRNA abundance regression and artificial genome generation task. Fishnman et al. [83] have proposed GENA-LM which is a suite of Transformer-based foundational DNA language models that can handle input lengths of up to 36000 base pairs. GENA-LM is made up of bert-base, bert-base-t2t, bert-base-lastln-t2t, bert-base-t2t-multi and bert-large-t2t. For the sequence tokenisation, the authors have used byte-pair encoding (BPE) where the dictionary size is set to 32000. Once the model is ready, the authors have used it for several downstream tasks such as prediction of promoters, splice site, drosophila enhancers and polyadenylation sites as well as chromatin profiling. However, the authors have tested their model against those sequences which can exploit the benefits of a longer context. Such tests show that GENA-LM outperforms other pretrained models including task-specific CNNs. Nucleotide Transformer is proposed in [84] which is pretrained on DNA sequences while integrating information from 3202 diverse human genomes as well as 850 genomes from a diverse range of species. The experimental results show that the model has commendable performance in 15 out of 18 downstream tasks using finetuning. DNABERT-2 which is an improvement over DNABERT is proposed in [85]. DNABERT-2 uses byte-pair encoding and is pretrained with masked language modelling loss with a mask ratio of 15%. It has then been used for core promoter detection as well as other downstream tasks. The authors have used Genome Understanding Evaluation (GUE) benchmark, which includes 7 genome sequence classification problems with 28 datasets. The results indicate that DNABERT-2 outperforms both DNABERT and nucleotide Transformers. TIS Transfomer has been proposed by Clauwaert et al. [86] to determine translation start sites by considering the information in the transcript nucleotide sequence. They showed that the limitations in the performance of TIS Transformer are due to the absence of high-quality annotations and not due to the model itself. However, the advantages of the model are the ability to detect key features of the translation process and multiple coding sequences on a transcript. For the prediction of cross-immunity between viral strains, Du et al. [87] have proposed DNA Pretrained Cross-Immunity Protection Inference (DPCIPI) model. Such prediction is important for public health surveillance as well as vaccine development. In this regard, the authors have used DNABERT to create the initial encoding which is then followed by BiLSTM to capture the sequence meaning. Finally, a multi-layer perceptron neural network (MLP) is considered to get the final classification result. The authors have compared their model with several other statistical methods and DPCIPI outperfoms all of them for both binary and multi-level cross immunity prediction. Bai et al. [88] have proposed IdentificatioN of bacteriopHagEs using deep RepresentatIon model with pre-Training (INHERIT) to identify bacteriophages which are viruses that infect and replicate within bacteria and archaea and rich in human body. Researchers can more effectively investigate phages by employing a technique that can accurately discriminate between phages and bacteria. INHERIT is an integrated model based on DNABERT and with an F1 score of 0.9932 has shown best performance when compared to state-of-the-art methods like VIBRANT [89], VirSorter2 [90], Seeker [91] and DeepVirFinder [71]. A Transformer-based, three-dimensional chromatin conformation-aware deep learning architecture Chromoformer is proposed in [92] for the quantitative decipher of histone codes in gene regulation. Gene expression is controlled by diverse group of regulators encompassing transcription factors, coactivators and corepressors and histone modifications (HMs) play the key role in the interplay among these factors. Chromoformer is trained on seven major HMs including H3K4me1, H3K4me3, H3K9me3, H3K27me3, H3K36me3, H3K27ac, and H3K9ac. It consists of three independent modules each accepting input features at different resolutions and producing an embedding vector. These three embeddings are then concatenated and fed into fully-connected layers to predict gene expression. Prediction of miRNAs is yet another area where the application of Transformers cannot be overlooked. miRe2e (Fig. 2(f)) as proposed in [93] is the first full end-to-end deep learning model for pre-miRNA prediction and is based on Transformers (Fig. 2(f)). The model can accept raw genome-wide data as input, without any pre-processing or feature engineering. miRe2e has been tested with many experimental setups with human genome and has shown much improved performance as compared to state-of-the-art methods. Prediction of RNA modifications is carried out in [94] by employing MRM-BERT (Multi-"}, {"title": "4. Challenges and Future Directions", "content": "Transformer-based language models have been proven to provide effective and significant results when applied to biological sequences, especially for their ability to define and handle a huge number of context-dependent features. Nevertheless, in order to build even more reliable and better performing Transformer models, some issues are still to be addressed; a huge amount of computational resources is needed to build Transformer models. Thus, the scientific community is devoting many efforts in order to reduce the computational load both in terms of time and space complexity. Also, some features of the models can be customized and particular attention could be paid to overcome the common limitation of deep learning model in interpreting and reading the intrinsic meaning of the models. There is still a large margin of improvement that could be achieved by developing models tailored for specific contexts as well as focus can be given on decoder-based models. Moreover, features apart from DNA sequences can be explored in order to come up with better performance results.\n\u2022 Computationally expensive: Although the main advantage of Transformers come from the self-attention module, it also leads to a very high computational expense which increases quadratically with the input sequence length, thereby making Transformer not able to model long sequences. Some of the works which have tried to mitigate or improve the Transformer for this problem are discussed below:\nImprovement in self-attention module: Beltagy et al. [24] have introduced local windowed attention with task motivated global attention in place of standard self-attention. This enables processing of longer sequences. BIGBIRD as proposed in [99] have used a sparse attention mechanism to reduce the quadratic dependency on sequence length to linear. Performers introduced in [100] have used linear attention by replacing softmax with another approach, thereby using only linear space and time complexity. However, all these works have focussed on customising the self-attention module, thus retraining the whole Transformer model.\nDividing long sequences into chunks: Xie et al. [101] have proposed dividing the long input sequences into a batch of chunks of feasible lengths and then selecting the most relevant tokens for decoding.\nFew-shot learning: Few-shot learning is used for learning new tasks when provided with only a handful of data. In this regard, Transformers models may be adapted to new tasks using only a small set of parameters for nucleotide sequences, thereby being computationally efficient.\n\u2022 Model interpretability: Deep learning models are known for their lack of interpretability. However, in the field of computational biology and bioinformatics, interpretability is of utmost important in order to gain insights from the model. In this regard, the self-attention mechanism of Transformers is very important. For example, the attention mechanisms in DNABERT [55] and DNABERT-2 [85] can focus on important areas for decision-making, thus providing an interpretability to the models by conveying semantic meaning. Both the aforementioned models can rank the input nucleotide molecules as well as find the relationship between the input sequence contexts. Clauwaert et al. [86] have built a model from Transformer-XL to detect and characterise transcription factor starting sites, thereby showing the potential of Transformers in extracting biological meaning.\n\u2022 Creating specific pretrained models for different contexts (at genus, family or species level or for metagenomics analysis): One very important direction for future work can be developing specific models that can handle data at genus, family, species level for metagenomic analysis. Metagenomic reads have shown to perform well for eukaryotic viral taxa at the order level [67]. Such work can also be extended for prediction at genus level. Moreover, most of the works have used k-mer representations for the sequence data. However, as pointed out in [85], overlapping k-mer tokenisation may result in information leakage in masked language modelling."}, {"title": "5. Conclusion", "content": "The revolution of Transformer-based models is unprecedented in the field of natural language processing. Such models have also brought new waves in bioinformatics as well, particularly for the processing of nucleotide sequences. In this regard, BERT-promoter with an accuracy of 85.5% for promoter identification, miProBERT showing recall of 75.76% for predicting miRNA promoters etc. are some of the examples which show how Transformer-based models can be utilised for several purposes in bioinformatics. Such effective and reliable performances pave the way for the application of Transformer-based models to many different fields encompassing nucleotide sequences. For instance, they can be applied for the recognition of stable nucleosome forming nucleotide sequences, as well as in immunoinformatics applied for the identification of potential epitopes. Transformer-based models seem to be particularly suitable for designing models that are able to identify Transcription Factor Binding Sites for an individual TF. However, application of Transformers in bioinformatics and related fields are still in the nascent stage. Better pretrained models, combining Transformer models with other newly developed deep learning techniques as well as improving model flexibility are some areas which can be focused on. We hope that this review article helps the researchers working in the field of bioinformatics to propose new and improved methodologies to mitigate various problems pertaining to the diagnosis and treatment of human diseases."}]}