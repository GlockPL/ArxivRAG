{"title": "wav2sleep: A Unified Multi-Modal Approach to Sleep Stage Classification from Physiological Signals", "authors": ["Jonathan F. Carter", "Lionel Tarassenko"], "abstract": "Accurate classification of sleep stages from less obtrusive sensor measurements such as the electrocardiogram (ECG) or photoplethysmogram (PPG) could enable important applications in sleep medicine. Existing approaches to this problem have typically used deep learning models designed and trained to operate on one or more specific input signals. However, the datasets used to develop these models often do not contain the same sets of input signals. Some signals, particularly PPG, are much less prevalent than others, and this has previously been addressed with techniques such as transfer learning. Additionally, only training on one or more fixed modalities precludes cross-modal information transfer from other sources, which has proved valuable in other problem domains. To address this, we introduce wav2sleep, a unified model designed to operate on variable sets of input signals during training and inference. After jointly training on over 10,000 overnight recordings from six publicly available polysomnography datasets, including SHHS and MESA, wav2sleep outperforms existing sleep stage classification models across test-time input combinations including ECG, PPG, and respiratory signals.", "sections": [{"title": "1. Introduction", "content": "Quantitative analysis of sleep stages is important for applications such as the diagnosis of sleep disorders, the validation of sleep disorder medications, and the discovery of sleep biomarkers. This is typically done through a polysomnography (PSG) study following the American Academy of Sleep Medicine (AASM) guidelines (Iber, 2007), during which a subject will undertake one or more nights of sleep whilst being continuously monitored using a variety of sensors.\nAfter the study, a human expert will review the recording and assign each 30-second window of data (a sleep epoch) to one of 5 discrete sleep states defined by the AASM: Wake, N1 (light), N2 (intermediate), N3 (deep), or rapid eye movement (REM) sleep. This is traditionally done using signals such as the electroencephalogram (EEG), electrooculogram (EOG), and chin electromyogram (EMG), which are measured using electrodes attached to the subject's head.\nThe goal of sleep stage classification (sleep staging) algorithms is to automate this process using one or more input signals. Accurate sleep staging from inputs such as the photoplethysmogram (PPG), electrocardiogram (ECG), abdominal (ABD) and/or thoracic (THX) respiratory signals (see Figure 1) is of particular interest since acquiring these signals is typically more comfortable for the subject and easier to set up than a full PSG study.\nSleep staging models have commonly been trained and evaluated using one or more fixed input modalities. However, the recorded signals often have high mutual information, meaning that information available from one input modality may be useful for learning to classify sleep stages from another.\nTo better leverage this information, we introduce wav2sleep, a unified model for sleep stage classification that operates on sets of physiological signals. During training, the model can use all available input signals from each night of data, enabling it to be"}, {"title": "2. Background and Motivation", "content": "Prior work has shown that deep learning models can classify stages of sleep from brain signals recorded using the EEG with expert-level accuracy (Phan and Mikkelsen, 2022). However, EEG signals are typically measured using electrodes attached to the patient's scalp. To overcome this, other work has investigated classifying sleep stages using alternative, less obtrusive modalities such as the ECG (Sridhar et al., 2020), PPG (Kotzen et al., 2023)), and combinations of cardiac and respiratory signals (Bakker et al., 2021).\nTwo-step learning Automatic sleep staging from modalities such as the ECG or PPG is possible because these signals encode measures of physiological activity, such as heart rate variability (HRV), and these are known to be predictive of the sleep stage (Shinar et al., 2001). Sleep staging from these signals can therefore be formulated as a two-step learning problem. First, we must find a mapping $f: x \\to z$ from the input signals $x$ to relevant physiological features $z$, then a mapping $g: z \\to y$ from physiological features $z$ to sleep stages $y$. For models trained end-to-end to classify sleep stages from input signals, these mappings $f$ and $g$ are jointly and implicitly learnt by a deep neural network. This has been shown to outperform methods where the physiological features $z$ are derived using a manual feature engineering approach (Kotzen et al., 2023), i.e. where the mapping $f$ is explicit and human-designed."}, {"title": "Transfer learning", "content": "Sleep staging from modalities such as the PPG is of particular interest since they can be measured from ubiquitous wearables such as smartwatches (Charlton et al., 2023). However, PPG signals are less common in historical PSG datasets (Radha et al., 2021), hindering the use of deep learning methods which are notoriously data-hungry. To overcome this issue, transfer learning has been used to improve the performance of PPG-based models, by first pre-training using ECG data (Radha et al., 2021; Kotzen et al., 2023).\nTransfer learning approaches from ECG to PPG signals work because these signals have high mutual information and morphological similarity. For example, both signals encode HRV and respiratory rate variability (RRV) information. This fact is explicitly used by Radha et al. (2021), who first pre-train a model using HRV features derived from the ECG, then fine-tune using HRV features derived from PPG data. Using the two-step learning formulation introduced at the start of this section, this approach can be thought of as explicitly mapping each input signal $x_i$ to a modality-agnostic feature space $z$ via a modality-dependent mapping $f_i$."}, {"title": "Modality-agnostic learning", "content": "Ideally, modality-agnostic relationships such as the link between HRV and deep sleep (Shinar et al., 2001), or between RRV and REM sleep (Kantelhardt et al., 2003), should not need to be learnt from a specific modality, a fact which is already partially exploited by transfer learning approaches. However, transfer learning is prone to the problem of catastrophic forgetting (Kemker et al., 2018) i.e. where after training on new data, information learnt from the old dataset is 'forgotten'. Hence, even if our end goal is to produce a specialised model for a particular modality, we will show that it is beneficial to use a model that is jointly trained across multiple modalities, rather than directly training or using a transfer learning approach."}, {"title": "Multi-modal learning", "content": "Multi-modal sleep staging methods have varied in their approaches to combining cross-modal information. For example, the model proposed by Chambon et al. (2018) independently turns each input signal into features before passing them to a classifier (late fusion). In contrast, models such as SeqSleepNet (Phan et al., 2019) concatenate modalities at the input level (early fusion). In the middle ground, separately extracting features from each modality for each 30-second sleep epoch, then fusing and jointly modelling sequential informa-"}, {"title": "Objective", "content": "Our aim is to learn a modality-agnostic intermediate representation of the input signals because we hypothesise this can enable more robust representation learning, improving sleep staging performance. However, we want to maintain the ability to perform end-to-end deep learning from the raw input signals, since this has led to improved performance not just in sleep staging, but across numerous problem domains (Goodfellow et al., 2016). Finally, we desire a model that can be jointly trained across heterogeneous datasets, improving the diversity of input data, and avoiding the problem of catastrophic forgetting. In Sections 3 and 4, we describe the wav2sleep model which addresses these challenges, and discuss its advantages over prior methods. In Section 5, we empirically validate its effectiveness in sleep staging across a range of test-time modalities."}, {"title": "3. Model Architecture", "content": "In this section, we describe the architecture of the wav2sleep model, which turns sets of time-series signals spanning multiple hours into sleep stage classifications for each 30-second sleep epoch. The model architecture, illustrated in Figure 2, consists of three high-level components:\n1. Signal Encoders, which independently extract features for each signal in the input set $X_{1:T}$.\n2. Epoch Mixer, which fuses cross-modal information into a unified representation $z_t$ for each sleep epoch.\n3. Sequence Mixer, which mixes information temporally to classify sleep stages $y_{1:T}$."}, {"title": "3.1. Signal Encoders", "content": "The model first turns the set of continuous 1D input signals $X_{1:T} = \\{x_{1:k}^i | i \\in S \\}$ into a set of feature vector sequences $Z_{1:T} = \\{z_{1:k}^i | i \\in S \\}$, where $z_t^i$ denotes the feature vector for modality $i$ for sleep epoch $t$, $k$ denotes the relative sampling rate of each signal, and $S$ denotes the set of available modalities e.g. ECG and PPG signals. We use separate CNN encoders for each input modality, which follow the design of the early layers of SleepPPG-Net (Kotzen et al., 2023). These consist of a stack of residual layers (He et al., 2016), each containing three convolutional layers followed by a max pooling layer to downsample the signal by a factor of 2. The residual layers are followed by a reshape operation and a time-distributed dense layer to produce the sequence of feature vectors $z_{1:T}^i$."}, {"title": "3.2. Epoch Mixer", "content": "Having independently transformed each modality $i$ into a sequence of feature vectors $z_{1:T}^i$, we next fuse information from the set of modalities to provide a single unified representation $z_t$ for each sleep epoch i.e. to complete the mapping $f$ described in Section 2. We use a transformer encoder (Vaswani et al., 2017) to do this, providing the transformer with an extra learnable vector, i.e. a CLS token (Devlin et al., 2019; Dosovitskiy et al., 2020), and using the output at that position as our unified feature vector. This design straightforwardly handles a varying number of input modalities during training and inference whilst keeping the dimensionality of the fused feature sequence $z_{1:T}$ fixed."}, {"title": "3.3. Sequence Mixer", "content": "The feature vectors $z_{1:T}$ are passed to the sequence mixer, which mixes sequential information to produce sleep stage outputs $y_{1:T}$. This is a desirable property since sleep exhibits long-range time-series structures such as sleep cycles (Patel et al., 2022). We use a dilated CNN design as previously used by Sridhar et al. (2020); Kotzen et al. (2023). This consists of multiple blocks of dilated convolutional layers where the dilation doubles at each layer, meaning that the size of the model's receptive field increases exponentially with network depth."}, {"title": "3.4. Advantages", "content": "Returning to the two-step learning formulation ($f$ and $g$) introduced in Section 2, the wav2sleep architecture has two key advantages:\n1. Because it operates on sets of input signals, the model can be trained on heterogeneous datasets, increasing the variety of data available in terms of both the input modalities (for learning $f$) and physiology (for learning $g$).\n2. By training on all available modalities jointly, this should lead to more robust learning in the presence of noise, by avoiding shortcut learning when one or more modalities are corrupted.\nUsing a unified model also has practical advantages, since only a single model needs to be trained, validated and deployed, reducing operational complexity for real-world applications."}, {"title": "4. Experimental Set-up", "content": "We use 7 PSG datasets available from the National Sleep Research Resource (Zhang et al., 2018): SHHS (Quan et al., 1997), MESA (Chen et al., 2015), CFS (Redline et al., 1995), MROS (Blackwell et al., 2011), CHAT (Marcus et al., 2013), CCSHS (Rosen et al., 2003), and WSC (Young et al., 2009). Demographic information for the datasets used is provided in Table 1. Collectively, these datasets contain over 15,000 pairs of overnight polysomnography recordings and expert-annotated sleep stages. Notably, there is significant variation in patient demographics. For example, the SHHS, MESA and WSC datasets are mostly comprised of recordings from older adults with high apnea-hypopnea indices (sleep-disordered breathing). In contrast, the CCSHS and CHAT datasets both contain PSG recordings from children. Joint training across all datasets exposes the model to a wider variety of contact sensors (makes, models etc.) and individual physiological variations.\nThere is also variation in the availability of signals between the datasets. For example, recordings from MESA and CCSHS, and some from the CFS and CHAT datasets, contain a PPG signal, but recordings from the other datasets do not. Where available, we used the ABD and THX respiratory signals, the PPG signal, and the ECG signal from each recording."}, {"title": "Dataset splits", "content": "Although numerous prior works have explored the problem of sleep staging on the datasets used, there are no widely-established fixed training, validation and test partitions. We therefore establish new splits for all data sets, excluding nights that have not been annotated with multiple sleep stages as done in prior work (Phan et al., 2022). For datasets that contain multiple recordings from a single participant, we ensured that no participant appeared in both the test set and either the training or validation sets. No other exclusion criteria-such as signal quality heuristics (Jones et al., 2024)-were explicitly used since one of the key aims of training on multiple modalities jointly is to improve robustness to noise on any particular channel.\nThe size of our training, validation and test set splits for each dataset are listed in Table 1. These were chosen to be in line with those used in prior work, e.g. (Sridhar et al., 2020). Our splits were carefully constructed to additionally allow evaluation on the aggregated test set proposed by Jones et al. (2024), which uses multiple PSG datasets to create a test set that approximately matches the 2022 US census demographics. Throughout the remainder of this paper, we refer to this as the 'Census' test set. More detail on the construction of our training, validation and test sets is provided in Appendix B."}, {"title": "Preprocessing", "content": "We minimally processed all signals using a similar process to that described by Kotzen et al. (2023), padding or truncating each recording to 10 h (i.e. sequence length $T = 1200$), re-sampling each signal to the same frequency across recordings, and applying unit normalisation. The ECG and PPG signals were resampled such that each 30-second sleep epoch consisted of $k = 1024$ data points ($\\approx 34$ Hz), which simplifies temporal alignment during pooling operations within the convolutional layers of the signal encoders. Since respiratory signals are generally sampled at a lower frequency during PSG recordings (e.g. 5-10 Hz in SHHS), the ABD and THX signals were resampled to a lower frequency of $k = 256$ data points per sleep epoch ($\\approx 8$ Hz), reducing the computational and memory requirements of the model during training and inference."}, {"title": "4.2. Model training", "content": "All models were trained to minimise the cross-entropy loss between expert-annotated sleep stages and model outputs using the AdamW optimiser (Loshchilov and Hutter, 2019) with a batch size of 16 and weight decay of $10^{-2}$. For the learning rate schedule, we used a linear warm-up of 2000 steps to a maximum learning rate $\\epsilon = 10^{-3}$ followed by an exponential decay to zero. Training continued until there was no decrease in the loss on the validation set for 5 epochs, which typically required around 30 epochs in total. Further training details can be found in Appendix D. The checkpoint which resulted in the lowest validation loss was restored for evaluation. Model hyper-parameters were tuned using the validation sets before evaluation on the test sets took place."}, {"title": "Augmentation", "content": "As noted by Jones et al. (2024), signals such as the ECG are sometimes inverted due to electrodes being connected the wrong way around. To improve robustness, all signals were randomly inverted (multiplied by -1) with a 50% probability during training."}, {"title": "4.4. Stochastic masking", "content": "During training, to handle differences in the available modalities within a batch, we padded unavailable signals and added a mask to the attention matrices of the epoch mixer. To aid test-time generalisation to a subset of modalities, we randomly sampled a subset of the available signals for each recording via additional masking of the attention matrix. Where available, the input signals were masked with the following probabilities:\n$p(M_{ABD}) = 0.7$\n$p(M_{ECG}) = 0.5$\n$p(M_{THX}) = 0.7$\n$p(M_{PPG}) = 0.1$\nThese values were intuitively chosen so that the higher frequency ECG and PPG signals were less likely to be masked, and to increase the prevalence of the scarcer PPG signal."}, {"title": "5. Results and Discussion", "content": "In this section, we report the performance of our model in four-class sleep staging, merging N1 and N2 into a single 'Light' sleep class as commonly done in prior work. We report total Cohen's $\\kappa$ ($\\kappa_T$) and accuracy (Acc) calculated over all sleep epochs in each test set. All results are averages over three training runs using different random seeds. Our full set of results for all dataset-modality combinations evaluated can be found in Appendix A.3."}, {"title": "5.1. Cross-modal learning", "content": "In Table 3, we compare the performance of three approaches to PPG-based sleep staging:\n1. Direct training on (scarce) PPG signals.\n2. Transfer learning from ECG to PPG signals.\n3. Joint training on all available modalities.\nWe additionally compare with a re-implementation of SleepPPG-Net (Kotzen et al., 2023), trained using the same splits and learning procedure as our model. Using transfer learning ($S_{Train}$ = ECG $\\rightarrow$ PPG), we pre-train using the ECG signal, then fine-tune using the PPG signal, resuming the learning rate schedule. Across datasets, we find that our joint training approach with stochastic masking consistently leads to better performance than either direct training or transfer learning for the scarce PPG modality.\nSimilarly, Table 4 compares the performance of direct and joint training for sleep staging using the (abundant) ECG signal. For the SHHS and WSC datasets, and for the completely held-out MROS"}, {"title": "5.2. Varying modalities", "content": "Figure 4 shows confusion matrices between expert-annotated sleep stages and model outputs for different test-time modalities using the Census test set, aggregated over all sleep epochs. Here we see that the addition of breathing signals (ABD, THX) is particularly helpful in distinguishing both Wake and REM from Light (N1+N2) sleep. Using just the ECG and THX signals, we obtain a Cohen's $\\kappa$ of 0.812. Whilst caution should be taken when interpreting $\\kappa$ values, notably, using the rule-of-thumb proposed by Landis and Koch (1977) this corresponds to 'almost perfect' agreement with the expert-annotated sleep stages.\nIn Figure 5, we plot the performance of the wav2sleep model for different age ranges and test-time modalities $S_{Test}$. We observe good performance across age ranges, and that using more modalities consistently leads to improved performance, particularly by reducing the quantity and severity of outliers."}, {"title": "5.3. Stochastic masking", "content": "Figure 6 shows the performance of wav2sleep for various dataset-modality combinations with and without the use of stochastic masking during training. Here we can see that stochastic masking is essential for generalisation to subsets of modalities at test-time, whilst maintaining equivalent performance when using all modalities."}, {"title": "5.4. Comparison with prior work", "content": "In Table 5, we compare the performance of wav2sleep after training on all modalities with prior models trained on specific modalities. We follow the exclusion criteria of Jones et al. (2024) and compare against prior work that has explicitly reported the use of distinct training, validation and test sets. Across multiple datasets and combinations of test-time modalities, the wav2sleep model outperforms existing methods for sleep staging from cardiorespiratory signals."}, {"title": "Future Work", "content": "We have focused on learning from cardio-respiratory signals since sleep staging from these modalities is of particular interest. However, using additional signals such as the EEG may help to further improve the quality of the learnt representations. Finally, the generalised architecture of wav2sleep, particularly the ability to jointly train it on heterogeneous, multi-modal time-series, means that it could be used to complement unsupervised approaches, e.g. (Thapa et al., 2024)."}, {"title": "6. Conclusions", "content": "In this paper, we have introduced wav2sleep, a deep learning model for automated sleep stage classification that can operate on a variable number of input modalities during training and inference. After joint training on over 10,000 nights of publicly available data from six heterogeneous datasets, this single, unified model leads to improved performance compared to direct training and transfer learning methods across a range of test-time modalities and datasets. Our work further improves the accuracy of sleep staging across a range of important modalities, such as ECG, PPG and respiratory signals, bringing accurate, low-cost sleep monitoring from less obtrusive contact sensors closer to clinical practice."}, {"title": "Appendix A. Additional results and discussion", "content": "As shown in Table 4, we observed a small tradeoff between ECG and PPG performance using our stochastic masking approach. During training, there are a finite number of optimisation steps before the model begins to overfit. This results in a small performance trade-off between different test-time modality combinations depending on the masking parameters, which determine the relative frequency of modalities observed during training. For example, by increasing the PPG masking probability $p(m_{PPG})$ to 0.2 we found that the kappa values slightly decreased by 0.01-0.02 across datasets when using only the PPG at test-time, but increased by around the same amount using just the ECG.\nA similar effect was noted in the similar approach of Hierarchical Channel Sampling (Bao et al., 2024), where performance was best for combinations of channels that were most frequently sampled during training. Our stochastic masking procedure means that ECG-only examples are infrequently sampled during training, accounting for less than 1 example"}, {"title": "A.2. Signal noise", "content": "Figure 7 shows the performance of wav2sleep on the MESA test set for different test-time modalities, grouped by ECG signal quality. Here we can observe how the use of multiple input modalities provides improved redundancy. When the ECG signal is of poor quality, the use of additional signals, e.g. THX, helps to maintain good performance."}, {"title": "A.3. Varying test-time modalities", "content": "After joint training on all datasets and input modalities ($S_{Train}$ = All), the performance of the wav2sleep model for different test-time modalities $S_{Test}$ is listed in Table 6."}, {"title": "A.4. Example hypnograms", "content": "Figure 8 shows example sleep hypnograms generated by the wav2sleep model using different test-time modalities. This night of data corresponds to an elderly male with diagnosed cardiac arrhythmia and mild sleep apnea. Visually, the ECG is of good quality, however, using the ECG as the sole input to the model results in poor agreement with expert-annotated sleep stages. The addition of the thoracic signal results in a significant performance improvement.\nNotably, we found that when using the ECG as the sole input, performance improves with apnea severity for subjects with cardiac arrhythmia (see Figure 9). This is in contrast to the general trend seen in prior work that performance tends to decrease with apnea severity (Korkalainen et al., 2020). We hypothesise that, for subjects with arrhythmia, the model may mistake heart rate variability (HRV) caused by arrhythmia for HRV caused by the more common condition of sleep apnea (Penzel, 2003). In turn, this may confound the learnt mapping between physiological features and sleep stages i.e. the mapping $g$ described in Section 2."}, {"title": "Appendix B. Dataset processing", "content": "Because of signal quality issues, some of the recordings in each dataset only have binary sleep-wake annotations, rather than full AASM (Wake, N1, N2, N3, REM) sleep stages. For these recordings, all sleep stages are typically assigned to the same integer as 'N2' sleep. This means that these labels should not be used for training or evaluation of multi-class sleep staging models. Where available, we used the harmonised 'nsrr_flag_spsw' metadata variable produced by the National Sleep Research Resource to exclude these recordings. Otherwise, we checked for the existence of either N1, N3 or REM sleep labels."}, {"title": "B.2. Construction of test sets", "content": "From the CFS and CHAT datasets, we created our validation and test sets using recordings where the PPG signal was available, to enable evaluation across all combinations of modalities. The remaining recordings (with and without PPG available) were used for training. We used recordings from the non-randomised (single night per participant) arm of the CHAT dataset for our test set. Similarly to Carter et al. (2024), we selected 1000 nights for our SHHS test set by randomly choosing 500 participants who participated in both visits. For the WSC dataset, we selected 2 recordings from 250 participants who had undertaken at least 2 visits to form our test set"}, {"title": "Appendix C. Model design", "content": "Here we describe additional experiments and observations that informed the design and hyper-parameters of the wav2sleep model. Hyper-parameter search was informed by the minimum validation loss $L_{Min}$ during initial experiments."}, {"title": "C.1. Signal Encoders", "content": "We found that using instance normalisation (Ulyanov et al., 2017) within the signal encoders and layer normalisation (Ba et al., 2016) in the sequence mixer improved training stability and performance. Because of our stochastic masking procedure, the number of examples of a signal within a batch will often be much smaller than the actual batch size, increasing the variance of statistics used by the more common approach of batch normalisation (Ioffe and Szegedy, 2015)."}, {"title": "C.2. Epoch Mixer", "content": "We evaluated two designs for the epoch mixer:\n1. A small transformer encoder (TE) i.e. our best-performing approach.\n2. A linear concatenation and projection layer, handling variation in the available inputs with zero-padding.\nThe attention-based epoch mixer achieved a lower validation loss and higher Cohen's $\\kappa$ values across multiple datasets and modalities."}, {"title": "C.3. Sequence Mixer", "content": "We evaluated two designs for the sequence mixer:\n1. A dilated convolutional (DCNN) design, as originally proposed by Sridhar et al. (2020).\n2. A transformer encoder (TE) with sliding window attention (Beltagy et al., 2020) and rotary positional embeddings (Su et al., 2024).\nWe found that the DCNN design consistently achieved better performance across different modalities, and converged after fewer training epochs. The hyper-parameters of the dilated convolutional design have been carefully tuned through extensive hyperparameter search in prior work (Sridhar et al., 2020; Kotzen et al., 2023). Though we did perform a basic search over transformer hyper-parameters, such as the number of encoder layers and the context"}, {"title": "Appendix D. Model training", "content": "Model parameters were found by minimising the unweighted cross-entropy loss between one-hot encoded labels $Y_{1:T} \\in \\mathbb{R}^{C \\times T}$ and output probabilities $P_{1:T} \\in \\mathbb{R}^{C \\times T}$. For each night of data, the total cross-entropy loss is given by:\n$L_O(Y_{1:T}, P_{1:T}) = \\sum_{i=1}^{C} \\sum_{j=1}^{T} (Y_{1:T})_j \\odot log (P_{1:T})_j$  (1)\nwhere $\\odot$ denotes the Hadamard product.\nGPU training Experiments were performed using a computing cluster containing multiple GPU architectures. Gradient accumulation was used to ensure a consistent effective batch size of 16, using the largest batch size that could fit on the particular GPU(s) used in a given experiment. Using a single NVIDIA A100, the actual batch size was 4 samples, and each epoch took 21 minutes, resulting in an average training time of around 10 hours."}]}