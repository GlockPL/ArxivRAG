{"title": "MMVU: MEASURING EXPERT-LEVEL MULTI-DISCIPLINE VIDEO UNDERSTANDING", "authors": ["Yilun Zhao", "Lujing Xie", "Haowei Zhang", "Guo Gan", "Yitao Long", "Zhiyuan Hu", "Tongyan Hu", "Weiyuan Chen", "Chuhan Li", "Junyang Song", "Zhijian Xu", "Chengye Wang", "Weifeng Pan", "Ziyao Shangguan", "Xiangru Tang", "Zhenwen Liang", "Yixin Liu", "Chen Zhao", "Arman Cohan"], "abstract": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark\nfor evaluating foundation models in video understanding. MMVU includes 3,000\nexpert-annotated questions spanning 27 subjects across four core disciplines: Sci-\nence, Healthcare, Humanities & Social Sciences, and Engineering. Compared to\nprior benchmarks, MMVU features three key advancements. First, it challenges\nmodels to apply domain-specific knowledge and perform expert-level reasoning\nto analyze specialized-domain videos, moving beyond the basic visual perception\ntypically assessed in current video benchmarks. Second, each example is anno-\ntated by human experts from scratch. We implement strict data quality controls\nto ensure the high quality of the dataset. Finally, each example is enriched with\nexpert-annotated reasoning rationals and relevant domain knowledge, facilitating\nin-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal\nfoundation models on MMVU. The latest System-2-capable models, 01 and Gem-\nini 2.0 Flash Thinking, achieve the highest performance among the tested models.\nHowever, they still fall short of matching human expertise. Through in-depth error\nanalyses and case studies, we offer actionable insights for future advancements in\nexpert-level, knowledge-intensive video understanding for specialized domains.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models have demonstrated remarkable capabilities in reasoning across various domains,\nyet their ability to handle expert-level knowledge remains a critical area of evaluation (Hendrycks\net al., 2021; Yue et al., 2024a). In recent years, researchers have developed numerous benchmarks\nto assess these models' proficiency in specialized domains, primarily focusing on text-based reason-\ning (Hendrycks et al., 2021; Wang et al., 2024d; Feng et al., 2024; Sun et al., 2024) and image-based\ncontexts (Lu et al., 2024; Yue et al., 2024a;b; Zhang et al., 2024a; Li et al., 2024g). However, as ca-\npabilities of foundation models expand across multiple modalities, there is a significant gap in eval-\nuating expert-level reasoning over specialized-domain videos. This gap is particularly concerning\nas video is one of the most information-rich and naturalistic modalities, and is widely used to con-\nvey complex, dynamic information in specialized fields like healthcare, engineering, and scientific\nresearch (He et al., 2024). Unlike static text or images, expert-level videos often capture temporal\ndynamics, procedural knowledge, and complex interactions that are essential in many specialized\ndomains. For example, in science, expert-level and knowledge-intensive reasoning might involve\nanalyzing a chemical reaction video (Figure 1). A model must identify key reaction stages based\non subtle visual cues like color changes or the formation of precipitates, which requires integrating\nchemical knowledge in addition to recognizing visual patterns.\nTo bridge this gap, we introduce MMVU, a comprehensive benchmark measuring Multimodal foun-\ndation models in expert-level, Multi-discipline Video Understanding and reasoning. MMVU con-\nsists of 3,000 expert-annotated QA examples over 1,529 specialized-domain videos, spanning 27\nsubjects across four key disciplines: Science, Healthcare, Humanities & Social Sciences, and En-\ngineering. To ensure both the breadth of domain knowledge and the depth of reasoning required\nfor MMVU, we implement a textbook-guided data annotation process. Expert annotators first lo-\ncate key concepts from textbooks in their fields, then source relevant videos and create correspond-\ning questions that require domain knowledge and expert-level reasoning to comprehend the videos.\nEach example also includes expert-annotated reasoning rationale and relevant domain knowledge,\nfacilitating fine-grained evaluation of model performance. Thorough data quality controls are im-\nplemented to ensure high quality of MMVU.\nWe conduct an extensive evaluation on MMVU, covering 32 frontier multimodal foundation models\nfrom 17 organizations. Notably, the latest o1 model demonstrates the highest performance among all\ntested models, approaching the expertise of human experts. Despite this progress, other models still\nfall noticeably short of human-level capabilities. For instance, GPT-4o achieves a score of 66.7%,\nwhich is substantially lower than the benchmark set by human experts (i.e.,, 86.8%) in the open-\nbook setting. Our analysis highlights the effectiveness of CoT reasoning, which generally enhances\nmodel performance compared to directly generating final answers without intermediate reasoning\nsteps. To deepen understanding of the current models' limitations, we perform an in-depth error\nanalysis of frontier models, including numerous case studies reviewed by human experts. These\ninsights provide valuable guidance for future advancements in the field."}, {"title": "2 RELATED WORK", "content": "Video Understanding Benchmark. Existing video understanding benchmarks primarily focus on\ngeneral-purpose video comprehension tasks, such as action recognition (Heilbron et al., 2015; Sig-\nurdsson et al., 2016; Liu et al., 2020; Deng et al., 2023), captioning and description (Xu et al., 2016;\nKrishna et al., 2017; Li et al., 2024c; Takahashi et al., 2024; Wu et al., 2021), grounding (Lei et al.,\n2018; Wang et al., 2022; Chen et al., 2023c; Kesen et al., 2023), temporal reasoning (Jang et al.,\n2017; Liu et al., 2024b; Shangguan et al., 2024; Cores et al., 2024; Cai et al., 2024; Kesen et al.,\n2024; Li et al., 2024e), and long video understanding (Zhang et al., 2023b; Wang et al., 2024b;\nNagrani et al., 2024; Ataallah et al., 2024; Fang et al., 2024). The rise of video-based foundation\nmodels (Tang et al., 2023; Zhang et al., 2023a; Fei et al., 2024; Huang et al., 2024b) has driven the\ndevelopment of new benchmarks that include diverse video comprehension tasks for more compre-\nhensive evaluation (Xiao et al., 2021; Ning et al., 2023; Li et al., 2024d; Fu et al., 2024; Li et al.,\n2024f; Khattak et al., 2024; Yang et al., 2024b). However, these benchmarks remain predominantly\nfocused on natural scenes and general-purpose tasks. A significant gap persists in benchmarks tar-\ngeting expert-level and knowledge-intensive reasoning over specialized-domain videos, where both"}, {"title": "3 MMVU BENCHMARK", "content": "We present MMVU, a comprehensive evaluation benchmark that focuses on measuring progress\non knowledge-intensive, expert-level reasoning in the video modality. MMVU has the following"}, {"title": "3.1 PRELIMINARY SETUP", "content": "We first discuss the preliminary setup for data construction.\nSubject Selection. To ensure a broad and accurate representation of expert-level video understand-\ning across diverse disciplines, we conduct a user study involving 133 college and graduate students\nfor subject selection. We ask them to curate two QA examples requiring expert-level video under-\nstanding in subjects relevant to their field of study, and provide feedback on their experiences during\nthe curation process. Such a user study-guided approach helps us identify subjects within each disci-\npline that may not be obvious from a top-down selection process. It also offers insights into the chal-\nlenges of designing expert-level video examples, helping us design and refine the textbook-guided\nQA annotation process (detailed in \u00a73.2). The authors manually analyze the collected examples and\nselect 27 subjects (as listed in Figure 1) across four disciplines that align best with our benchmark's\nconstruction desiderata discussed earlier.\nExpert Annotator Recruitment and Training. For each subject, we assign at least two anno-\ntators with relevant expertise. We include a total of 67 expert annotators (detailed biographies are\npresented in Appendix A.1), comprising 22 third- or fourth-year undergraduate students, 36 grad-\nuate students, and nine of the authors. All the annotators also participated in our initial user study.\nEach annotator is required to finish a training session to learn the annotation protocol (detailed in\nAppendix A.3) before official annotation."}, {"title": "3.2 TEXTBOOK-GUIDED QA EXAMPLE ANNOTATION", "content": "Constructing a high-quality, expert-level, multi-disciplinary benchmark for video-based tasks is\nmore challenging than the ones for text- or image-based, as there is no existing resources (e.g.,, text-\nbooks or exam questions) that can adapted from and each example has to be curated from scratch.\nTherefore, it is crucial to establish a structured approach that ensures the quality and comprehen-\nsiveness of the benchmark. We employ a textbook-guided example annotation pipeline designed to\ncapture both the breadth of knowledge and depth of reasoning. In brief, annotators first identify key\nconcepts from the textbook and locate relevant videos that align with these concepts. The textbooks\nfor each subject (listed in Appendix A.2) are selected by expert annotators and are recognized as\nauthoritative references in their respective fields. Annotators then curate QA examples and detailed\nsolution rationales. We detail the annotation procedure as follows:\nConcept-Driven CC-Licensed Video Collection. Annotators are instructed to first review each\nchapter of the textbook to identify key concepts that inherently require dynamic visual representa-"}, {"title": "3.3 DATA QUALITY CONTROL", "content": "We next discuss our methods to ensure high data quality.\nTime-Based Annotation Compensation. As discussed earlier, annotating examples for MMVU\ncan be particularly time-intensive, especially when there is limited availability of videos with Cre-"}, {"title": "3.4 MMVU BENCHMARK ANALYSIS", "content": "Data Statistics. Table 2 presents the key statistics of MMVU. It consists of 3,000 examples, which\nare randomly divided into two subsets: validation and test. The validation set contains 1,000 exam-\nples, and is intended for model development and validation. The test set, comprising the remaining\n2,000 examples, is strictly reserved for standard evaluation to prevent data contamination (Jacovi\net al., 2023; Deng et al., 2024; Glazer et al., 2024). To further promote fair benchmarking, the test\nset remains hidden. We are developing an online evaluation pipeline on a public platform, enabling\nresearchers to benchmark their models and participate in a public leaderboard.\nHuman Performance. To provide a rough but informative estimate of human-level performance\non MMVU, we randomly sampled 30 questions per discipline from the test set, resulting in a total\nof 120 questions for evaluation. Five participants-three graduate students specializing in biology,\nanesthesiology, and East-Asian literature, along with two of the authors\u2014individually answered\nthese questions. The evaluation proceeded in three phases: (1) Closed-book Setting: In the first"}, {"title": "4 EXPERIMENTS", "content": "This section discusses the experiment setup and our key findings.\n4.1 EXPERIMENT SETUP\nEvaluated Multimodal Foundation Models. To establish a comprehensive understanding of the\nchallenges posed by MMVU and provide reference points for future research, we evaluate a broad\nrange of frontier multimodal foundation models that support video or multiple images as input.\nSpecifically, we evaluate 16 series of open-source models, including InternVL-2 & 2.5 (Chen\net al., 2023b; 2024b), Qwen2-VL (Wang et al., 2024a; Yang et al., 2024a), LLaVA-NeXT (Liu et al.,\n2024a), Pixtral (MistralAI, 2024), DeepSeek-VL2 (Wu et al., 2024), H2OVL Mississippi (Galib\net al., 2024), Idefics2 (Lauren\u00e7on et al., 2024), Aria (Li et al., 2025), LLaVA-NeXT-Video (Li\net al., 2024b), LLaVA-OneVision (Li et al., 2024a), Llama-3.2-Vision (Dubey et al., 2024), Phi-3.5-\nVision (Abdin et al., 2024), InternVideo2 (Wang et al., 2024c), and VideoLLaMA2 & 2.1 (Cheng\net al., 2024). We also evaluate eight series of proprietary models, including OpenAI 01 (Ope-\nnAI, 2024a) and GPT-4o (OpenAI, 2024b), Gemini-1.5 & 2 and Gemini-Thinking (Google, 2024),\nGLM-4V-Plus (GLM et al., 2024), Grok-2-Vision (xAI, 2024), and Claude-3.5 (Anthropic, 2024).\nFor open-source models, we prioritize the vLLM pipeline (Kwon et al., 2023) for model inference;\notherwise, we use the Transformers pipeline (Wolf et al., 2020). We use the official API service\nfor proprietary models. For models without native video support, following VideoMME (Fu et al.,\n2024), we provide visual input using the maximum number of images that fits within the model's\ncontext window. \u00a7B.1 details the parameter settings and model configurations. We evaluate the mod-\nels with both Direct Answer and Chain-of-Thought prompts (presented in appendix B.2), which\nis adapted from the versions used in MMMU-Pro (Yue et al., 2024b).\nAccuracy Evaluation. We use accuracy as the primary metric to evaluate model performance on\nMMVU. Following recent benchmarks for foundation model evaluation (Wang et al., 2024e; Lu\net al., 2024; He et al., 2024), we employ GPT-4o to assess accuracy. Specifically, given a question,\nits ground truth answer, and the model's response, GPT-4o is instructed to extract the final answer\nfrom the model response and determine its correctness. The evaluation prompts for both multiple-\nchoice and open-ended questions are presented in Appendix B.3."}, {"title": "4.2 MAIN FINDINGS", "content": "Table 3 presents the evaluated models' CoT performance on the MMVU benchmark, while Figure 4\nillustrates a comparison between the model performance in CoT reasoning and direct answering.\nOur key findings are as follows:\nMMVU presents substantial challenges for current multimodal foundation models. Even the\ntop-performing model falls well short of human expert performance. For instance, GPT-4o achieves\n66.7% accuracy with CoT prompting, significantly lower than the 86.8% accuracy achieved by hu-\nman experts in an open-book setting. Notably, while GPT-4o has narrowed the performance gap with\nhuman experts in text-based expert-level reasoning on MMLU (88.7% vs 89.8% (Hendrycks et al.,\n2021)) and image-based expert-level reasoning on MMMU (69.1% vs 82.6% (Yue et al., 2024a)),\nthe gap remains large on MMVU. This disparity underscores MMVU's critical role in advancing\nand evaluating multimodal foundation models' capabilities in video-based expert reasoning across\nspecialized domains."}, {"title": "4.3 QUALITATIVE ANALYSIS", "content": "To gain a deeper understanding of the capabilities and limitations of frontier models on MMVU,\nwe perform comprehensive case studies and error analysis by humans. The inclusion of expert-\nannotated reasoning rationales and domain knowledge for each example in MMVU facilitate a more\neffective analysis compared to datasets that provide only answers. We focus on four top-performing\nmodels, GPT-4o, Qwen2-VL-72B, Llama-3.2-90B-Vision, and DeepSeek-VL2, for human evalua-\ntion. From the MMVU validation set, we randomly sample 50 error cases for each model. These\ncases are analyzed by the authors using ground-truth features (i.e., expert-annotated reasoning ratio-\nnales and required domain knowledge) as references. We identify following six primary errors:\nVisual Perception Error (18%): The model fails to accurately interpret spatial, temporal, or se-\nmantic aspects of visual information within a video. Additionally, it might \u201challucinate\u201d, detecting\nobjects or events that are not actually present in the video. Figure 5 (left) is a typical related in-\nstance where the model fails to correctly perceive the traversal order of the binary tree. Similarly,\nFigure 18 shows that the model mistakenly identifies the device shell in the video as water, leading\nto completely wrong reasoning about the device's function.\nMisuse or Lack Domain Knowledge in Visual Perception (20%): The model fails to apply the\ndomain-specific expertise required to accurately interpret specialized concepts or elements within\nthe video. For example, in a medical video, it may identify objects but fail to recognize their tech-\nnical terms or misunderstand their importance within the procedure being demonstrated. Moreover,\nas shown in Figure 20, the model correctly perceives the ascending numbers (array indices), but\nmisuses its pretrained knowledge and misidentifies them as the numbers to be sorted. It leads to the\nwrong conclusion that the video demonstrates a sorting algorithm. This limitation underscores a gap\nin the model's ability to integrate domain knowledge with visual perception effectively.\nMisuse or Lack Domain Knowledge in Reasoning (27%): The model fails to effectively recall and\napply domain knowledge during its reasoning processes. For instance, when addressing questions\nover chemistry videos, it may fail to correctly apply relevant chemical equations, leading to errors\nin computing the reaction mass. A notable example is Figure 5 (right), where the model misuses the\ndomain knowledge that bats often live in unsanitary environments and makes the wrong inference\nthat poor hygiene conditions are the cause of virus outbreaks. Besides, in Figure 25, the model\nlacks the domain knowledge about relevant chemical equations, so that it cannot correctly answer\nthe question. This limitation underscores the model's inability to integrate domain knowledge into\nits reasoning processes effectively.\nHeavy Reliance on Textual Information (20%): The model predominantly depends on textual\ninformation for problem-solving, especially when addressing multiple-choice questions, as it eval-\nuates each option individually without leveraging the actual video content. For instance, Figure 26\nshows the model ignores the video information about the reason of the disease and overly focuses\non the textual question. Similar limitations have been observed in other multimodal benchmarks (Fu"}, {"title": "5 CONCLUSION", "content": "We introduce MMVU, a high-quality, multi-disciplinary benchmark designed to assess the expert-\nlevel, knowledge-intensive reasoning capabilities of multimodal foundation models on specialized-\ndomain videos. Each example in MMVU is annotated by human experts from scratch. We employ\na textbook-guided example annotation pipeline designed to capture both the breadth of knowledge\nand depth of reasoning. In our evaluation of 32 frontier multimodal foundation models, we find that\nwhile the latest ol model achieves the highest performance among all tested models-approaching\nhuman expert-level proficiency-a notable performance gap remains between other models and hu-\nman experts. Additionally, models employing CoT reasoning consistently outperform those that\ngenerate final answers directly. Through comprehensive error analysis and case studies, we iden-\ntify persistent challenges of MMVU, offering valuable insights for advancing foundation models'\ncapabilities to achieve expert-level video understanding in specialized domains."}, {"title": "A MMVU PRELIMINARY SETUP", "content": "A.1 ANNOTATOR BIOGRAPHY\nThe detailed biographies of the annotators involved in MMVU construction are presented in Table 4.\nAll annotators are from universities ranked in the Top 500 of the 2024 QS Global Rankings\u00b3 and are\nfluent in English."}, {"title": "A.2 TEXTBOOK FOR EACH SUBJECT", "content": "As discussed in Section 3.2, we design a textbook-guided example annotation pipeline to encompass\nboth the breadth of knowledge and the depth of reasoning. The textbooks used for each subject\nare detailed in the following tables. They are selected by expert annotators and are recognized as\nauthoritative references in their respective fields."}, {"title": "\u0391.3 \u0391\u039dNOTATION GUIDELINE AND INTERFACE", "content": "With the goal of ensure the high quality of data, MMVU adheres to the following four bench-\nmark construction desiderata, we develop the following annotation interface based on Turkle (HLT-\nCOE@JHU, 2024), an open-source clone of Amazon's Mechanical Turk:"}, {"title": "A.4 VALIDATION GUIDELINE AND INTERFACE", "content": "To ensure that the final dataset remains high-quality and meets expert-level standards without intro-\nducing unnecessary bias, each example in MMVU undergoes expert review by one of the authors or\ntop-performing annotators to verify the accuracy of its annotations, following the annotation guide-\nline detailed in Appendix A.3. The examples of validation interface are presented as follows:"}, {"title": "A.5 DATA ANNOTATION AND VALIDATION PAYMENT", "content": "The annotation and validation process for MMVU spans three months. As outlined in Section 3.2,\nannotating examples for MMVU can be particularly time-intensive, especially when there is limited\navailability of videos with Creative Commons licenses in the required subjects. To accommodate this\nand ensure a high-quality dataset, we compensate annotators based on the time they spend rather than\nthe number of examples completed, preventing them from rushing through tasks. Annotators are\nrequired to record their screens throughout the annotation process, which enables us to verify time\nreporting accuracy and maintain productivity standards. This also helps us identify any distractions\nand precisely track the total time spent on each task. We offer a base rate of 6 USD per hour for both\nannotation and validation work, with an additional 2 USD per completed annotation and 0.40 USD\nper validated example. On average, annotating a single question for MMVU takes 20 minutes and 17\nseconds, while validation requires 4 minutes and 12 seconds. This compensation structure ensures\nthat annotators earn wages that are competitive with the average payment for teaching assistants at\ntheir respective universities. To reduce pressure and maintain a comfortable pace, we recommended\nthat annotators limit their work to a maximum of 10 QA example annotations or 50 QA example\nvalidations per day."}, {"title": "B EXPERIMENT SETUP", "content": "B.1 CONFIGURATION OF EVALUATED MODELS\nTable 10 detail the configuration of each evaluated models. We use the default settings from the offi-\ncial implementation of each model to process vision input. Across all experiments, the temperature\nis set to 1.0, with a maximum output length of 1024 tokens. However, for Gemini-2-Flash-Thinking,\nthe maximum output length is set as 8192 tokens to accommodate its long CoT reasoning mecha-\nnism. All inferences are reproducible on a workstation equipped with two NVIDIA A100-80G\nGPUs."}, {"title": "B.2 CHAIN-OF-THOUGHT AND DIRECT ANSWER PROMPTS", "content": "The following figures illustrates the CoT reasoning and Direct Answer prompts applied in this study\nfor answering multiple-choice and open-ended questions, respectively.\nQuestion:{question}\nA: {option_a}\nB: {option_b}\nC: {option_c}\nD: {option_d}\nE: {option_e}\nVisual Information: {processed_video}\nAnswer the given multiple-choice question step by step. Begin by explaining your reasoning process\nclearly. Conclude by stating the final answer using the following format: \"Therefore, the final answer\nis: $LETTER\" (without quotes), where $LETTER is one of the options. Think step by step before\nanswering."}, {"title": "B.3 PROMPTS FOR ACCURACY EVALUATION", "content": "[Instruction]\nEvaluate whether the model's final answer is correct by comparing it to the ground-truth answer pro-\nvided for the given question.\nYou should first extract the final answer from the model's response, and then compare the extracted\nanswer with the ground-truth answer to determine its accuracy. Output your response in the following\nstructured format:\n{\n'extracted_answer': // str value \"A\" \"B\" \"C\" \"D\" \"E\", should be a single character\n'correct': // boolean value, True if the answer is correct, False otherwise\n}\n[User]\nQuestion: {question}\nA: {option_a}\nB: {option_b}\nC: {option_c}\nD: {option_d}\nE: {option_e}\nGround Truth Answer: {ground_truth}\nModel Response to the Question: {model_response}"}, {"title": "C EXPERIMENT", "content": "C.1 COMPARISON BETWEEN COT REASONING AND DIRECT ANSWERING"}, {"title": "C.2 ERROR CASE ANALYSIS: VISUAL PERCEPTION ERROR", "content": "Thermodynamics\nQuestion: Which process is shown in the animation?\nOptions:\n(A) Isothermal compression (B) Adiabatic compression (C) Isobaric compression\n(D) Adiabatic expansion (E) Isothermal expansion\nAnswer: B\nRelevant Knowledge (Wikipedia page title): Adiabatic process, Isothermal process, Isobaric process, Ideal gas\nlaw\nReasoning Rational:\nFrom the video, it can be seen that the gas is thermally isolated and returns to its initial state through compression. This\nphysical process is known as adiabatic compression.\nLlama-3.2-90B-Vision:\nTo determine the process shown in the animation, we need to analyze the provided graph and the printed values.\nThe graph plots pressure (P) against volume (V), and the curve is descending, indicating that the volume is increasing\nwhile the pressure decreases. This is characteristic of expansion.\nAdditionally, the absence of a thermostat indicates that no temperature regulation is involved. This suggests the process\nis not isothermal (constant temperature), but rather adiabatic (no heat exchange with the surroundings).\nSince the volume is increasing, it is likely that the gas is expanding. (Human Evalutor's Comment: This\nobservation contradicts with the v\u00eddeo content, where the gas is being compressed.) Adiabatic expansion\noccurs when a gas expands without gaining or losing heat, and its temperature decreases.\nTherefore, the final answer is: D. Adiabatic expansion.\nError Category: Visual perception error\nError Reason: The model wrongly identifies the process of gas compression as the process of gas expansion, which is\nnot consistent with the human observation and leads to the wrong final answer."}, {"title": "C.3 ERROR CASE ANALYSIS: MISUSE OR LACK DOMAIN KNOWLEDGE IN VISUAL\nPERCEPTION", "content": "Computer Science\nQuestion: What is the algorithm shown in the video?\nAnswer: Binary Search\nRelevant Knowledge (Wikipedia page title): Binary search, Search algorithm, Computer Science\nReasoning Rational:\nThe character in the video always locates data from the middle and then discards a part of the data, ultimately finding\nthe target value. From the entire process, we can infer that this sequence is an ordered sequence, which aligns with the\nprocess of Binary Search.\nLlama-3.2-90B-Vision:\nTo determine what algorithm is shown in the video, let's analyze the steps and their characteristics:\n1. The video starts by showing a set of numbers.\n2. It then rearranges the numbers in a specific order.\nThe observed steps suggest that the algorithm is sorting the numbers in ascending order. The algorithm sorts an array\nby repeatedly finding the minimum element from the unsorted part and putting it at the beginning. It involves selecting\nthe smallest element and moving it to the correct position, which matches the steps shown in the video.\nBased on the analysis, the algorithm shown in the video involves selecting the minimum element and moving it to the\ncorrect position repeatedly, which is characteristic of the selection sort algorithm. (Human Evalutor's Comment:\nMisuse of domain knowledge in visual perception. The indices of the array are mistakenly identified as\nthe values.)\nTherefore, the final answer is: Selection Sort.\nError Category: Misuse or Lack Domain Knowledge in Visual Perception\nError Reason: The model wrongly recognized the numerical sequence in the video as values (when in fact they are the\nindices of the sequence), leading to the incorrect conclusion that the algorithm shown in the video is a sorting\nalgorithm."}, {"title": "C.4 ERROR CASE ANALYSIS: MISUSE OR LACK DOMAIN KNOWLEDGE IN REASONING", "content": "Computer Science\nQuestion: The algorithm demonstrated in the video is which common algorithm?\nOptions:\n(A) Bubble Sort (B) Merge Sort (C) Insertion Sort (D) Selection Sort (E) Heap Sort\nAnswer: C\nRelevant Knowledge (Wikipedia page title): Insertion sort, Bubble sort, Merge sort, Selection sort, Heapsort\nReasoning Rational:\nThe video ultimately sorts these blocks in ascending order, thus it is clearly a sorting algorithm. Observing the video\nprocess, there is no need to scan the sorting process from beginning to end, and there is an obvious insertion action,\nwhich allows us to eliminate Selection Sort, indicating it is Insertion Sort.\nDeepSeek-VL2:\nThe given video shows computer bars that increase and decrease according to their size. The algorithm being\ndemonstrated is a sorting algorithm, as it reorganizes the bars in order of their size. Among the given options, options\n(B) and (C) are not suitable, as Merge Sort and Insertion Sort are not typically demonstrated through visual bar\nrepresentations. Option (F) is also not suitable, as Heap Sort is not typically demonstrated through visual bar\nrepresentations. (Human \u00c9valutor's Comment: Biased misuse of pre-trained domain knowledge in\nreasoning. The sorting algorithms can be demonstrated through visual bar representations and the\nform of presentation should not be the main focus.) Although option (D) Selection Sort is a possibility, the\ngiven algorithm does not repeatedly find the minimum element of the unsorted part and put it at the front, which\ncharacterizes Selection Sort. Therefore, the given algorithm is most likely a Bubble Sort, as this algorithm uses visual\nbar representations.\nTherefore, the final answer is: A. Bubble Sort\nError Category: Misuse or Lack Knowledge in Reasoning\nError Reason: The model considers Insertion Sort not to be the correct answer because it believes that Insertion Sort\nis not typically demonstrated through visual bar representations. This is a typical case of reasoning errors caused by the\nmisuse of pretrained domain knowledge."}, {"title": "C.5 ERROR CASE ANALYSIS: HEAVY RELIANCE ON TEXTUAL INFORMATION", "content": "Clinical Medicine\nQuestion: When the phenomenon in the video occurs, which of the following options is the most important to adjust?\nOptions:\n(A) Renin-angiotensin system\n(B) Hypothalamic-pituitary-thyroid axis\n(C) Adrenaline and norepinephrine\n(D) Melatonin secretion\n(E) Sodium-potassium pump activity\nAnswer: A\nRelevant Knowledge (Wikipedia page title): Renin-angiotensin system, Homeostasis, Assessment of kidney\nfunction, Osmoregulation\nReasoning Rational:\nThe issue presented in the video is related to the imbalance of water within the body, causing cells to swell and\nresulting in symptoms such as headaches. Among the options provided, the regulation of water balance is primarily\nmanaged through the renin-angiotensin system.\nLlama-3.2-90B-Vision:\nThe phenomenon in the video likely refers to the effects of overhydration on the body. To adjust this, the most\nimportant physiological response would be to regulate the body's sodium level, which is primarily managed by the\nsodium-potassium pump activity. (Human Evalutor's Comment: The model ignores the video information\nabout water balance.) This pump plays a crucial role in maintaining the balance of electrolytes and fluids within the\nbody, which is crucial for the proper functioning of cells.\nTherefore, the final answer is: {E. Sodium-potassium pump activity}.\nError Category: Heavy Reliance on Textual Information\nError Reason: The model ignores the video information about water excretion in the human body. It relies solely on\nthe text and chooses the incorrect option E simply because cellular ion balance is crucial to the body's internal\nenvironment, thereby missing the actual disorder of fluid overload mentioned in the video."}, {"title": "C.6 ERROR CASE ANALYSIS: LOGICAL REASONING ERROR", "content": "Mechanical Engineering\nQuestion: What kind of mechanical transformation is not represented in the video?\nOptions:\n(A) It may convert gravitational energy into elastic energy.\n(B) It may convert one force to another force.\n(C) It may convert one speed to another speed.\n(D) It may convert linear motion into rotational motion.\n(E) It may convert gravitational energy to kinetic energy.\nAnswer: D\nRelevant Knowledge (Wikipedia page title): Mechanical energy", "Rational": "nIn the video", "D.\nLlama-3.2-90B-Vision": "nTo determine which type of mechanical transformation is not represented in the video", "Motion": "The video shows a mechanical linkage where a linear motion is converted\ninto rotational motion.\n2. Rotational Motion into"}]}