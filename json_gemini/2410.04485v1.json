{"title": "Exploring the Potential of Conversational Test Suite Based Program Repair on SWE-bench", "authors": ["Anton Cheshkov", "Pavel Zadorozhny", "Rodion Levichev", "Evgeny Maslov", "Ronaldo Franco Jaldin"], "abstract": "Automatic program repair at project level may open yet to be seen\nopportunities in various fields of human activity. Since the SWE-\nBench challenge was presented, we have seen numerous of solu-\ntions.\nPatch generation is a part of program repair, and test suite-based\nconversational patch generation has proven its effectiveness. How-\never, the potential of conversational patch generation has not yet\nspecifically estimated on SWE-Bench. This study reports experi-\nmental results aimed at evaluating the individual effectiveness of\nconversational patch generation on problems from SWE-Bench.\nThe experiments show that a simple conversational pipeline\nbased on LLAMA 3.1 70B can generate valid patches in 47% of cases,\nwhich is comparable to the state-of-the-art in program repair on\nSWE-Bench.", "sections": [{"title": "1 INTRODUCTION", "content": "The grows of AI capabilities opens new horizons for automation\nof software engineering tasks. As an illustration, the SWE-Bench\nbenchmark [3] was recently published to challenge and evaluate\nAI ability to accomplish Automatic Issue Resolving (AIR) task.\nThe majority of AIR approaches explicitly or implicitly leverage\nprogram repair techniques such as fault localization, patch genera-\ntion, and patch validation. LLM-based Conversational Patch Gener-\nation (CPG), that runs tests and uses failing information in dialog,\nhas shown its advantage [7, 12] over repetitive patch generation.\nTo date, the leading AIR approaches\u00b9 may resolve at least 43%\nof tasks in SWE-Bench Lite. However, what is not yet known is the\npotential of CPG as a part of AIR system design. If one had oracle\nfault localization\u00b2 and a unit test, what would the effectiveness of\nthe repair be? Knowing the potential of each individual component\nwithin the system allows planning the improvement of the entire\nsystem.\nThe aim of this study is to estimate the potential of LLM-based\nconversational patch generation. We conduct an experiment us-\ning problems from SWE-Bench benchmark. In our experiment the\nfunction-level fault localization is known and the failure informa-\ntion of running test suite is used in the repair dialog with LLM.\nWe want to answer the following research questions:\n(1) What is the effectiveness of conversational patch generation\nwith a test failure information feedback on SWE-Bench\n(2) Is conversational patch generation with a test failure in-\nformation feedback more effective than regular repetitive\nLLM-base patch generation at the same budget of requests\nto the LLM"}, {"title": "2 METHODOLOGY", "content": "In order to assess the potential of LLM-based conversational pro-\ngram repair in the context, present study adopts Automatic Issue\nResolving (AIR) problems from SWE-Bench dataset. For each SWE-\nBench record, the fault localization\u00b3 and failing tests are used to\nstart conversational repair pipeline. The repair process is considered\nsuccessful when all previously failing tests pass.\nIn the original AIR task, the fault localization and reproducible\nexample are not given. The proposed experiment allows to estimate\nthe potential of conversational program repair, but can not be used\nfor comparison with state-of-the-art approaches in AIR task.\nThe first step in our experiment is to prepare dataset of programs,\ndefect descriptions, and failing test suites. After, conversational\nprogram repair is run for each program. Finally, the effectiveness\nmetrics are calculated.\nTask Definition. Let P is a GitHub project, that have an issue i\ndescribed in natural language, and there is a set of failing unit-tests\nT because of the issue I, and a function s from P. The task is to find\na \u015d the new version of s, that if replace s with \u015d it ensures that all\ntests in T pass.\nThe defined task is similar to test suite based repair except the\nprecise fault localization s is provided, along with the issue descrip-\ntion i.\nData. The SWEBench is a collection of GitHub issues and corre-\nsponding patches from 11 well-known open-source python projects.\nThe Lite subset of SWEBench contains 300 issues where the gold\nrepair patch affects only a single project's file4.\nInitially, we pick out 192 problems from SWEBench Lite where\nthe gold patch is localized in a single function. We exclude all\nDjango problems5. Additionally, we manually validated each prob-\nlem environment to be sure that all unit tests mentioned in the\nbenchmark can be unambiguously found in test framework output\nlogs. Finally, it results into 92 problems; the full list can be found\nit the Appendix [].\nFrom each of the chosen problems we extract the following\ninformation: (1) issue description i; (2) faulty function s; (3) public\ntest site T; (4) hidden test site T*.\nThe faulty function is obtained from the oracle patch. The test\nsuites T and T* corresponds to FAIL_TO_PASS7 and PASS_TO_PASS8\nfields in SWE-Bench."}, {"title": "3 RESULTS & DISCUSSION", "content": "RQ-1. The conversational experiment includes 6 consecutive con-\nversation rounds; each round is limited by 5 requests to LLM. The\nFigure 1 plots the value of both metrics for both models. Metrics\nare calculated after each attempt.\nThe number of valid patches increases as total number of requests\ngrows and reach 62% for llama3.1 and 56% for gpt4o-mini. The\nfastest grows takes place at the beginning of the experiment.\nThe number of valid patches that pass both public and hidden\ntest suites drops to 47% and 46% for llama3.1 and gpt4o-mini. The\ntop 2 approaches for SWE-Bench Codestory Aide + Mixed Models\nand Honeycomb have solved 44% among 92 problems participated\nin the experiment.\nThe simple conversational patch generation pipeline allows to\ngenerate patches that are valid in 62% cases; and in 47% of cases\nthe generated patches also pass hidden test suite.\nThe experiment version where both public T and private T*\nsuites are used simultaneously shown worse results. This result\nmay be explained by the fact that private test suite size usually\nmultiple times larger than public, and its failure output significantly\nincreases the LLM request size.\nRQ-2. The second experiment includes 30 consecutive rounds with\na single request to LLM. It does not suppose of using any test failure\nfeedback. The Figure 2 plots experiment data.\nThe number of valid patches increases as total number of requests\ngrows and reach 46% for llama3.1 and 54% for gpt4o-mini; but drops\nto 34% and 47% correspondingly to pass both test suites.\nOne interesting finding is that repetitive pipelines produced valid\npatches for six problems that were not addressed with conversa-\ntional pipeline.\nThe experimental data shows that llama3.1 based patch gen-\neration with failure information (62%) significantly outperforms\nrepetitive design (46%). Surprisingly, no differences were found for\ngpt4o-mini. A possible explanation for this might be that gpt4o-\nmini is over-fitted on the experimental data and generates patches\nwith equal effectiveness regardless of failure test information."}, {"title": "4 RELATED WORK", "content": "Test Suite Based Program Repair. Automatic Program Repair (APR)\nis a general task aims to help developers fix software defects. A test\nsuite based program repair supposes to have at least one failing\ntest. These tests can be helpful for all common repair steps (i) to\nlocalize the defect [2, 4, 9] in the program; (ii) to generate a patch\n[11]; (iii) and to validate the generated patch [2, 4]. This study uses\nfailing tests for the last two steps.\nAutomatic Program Repair with pretrained LLMs. Previous studies\nhas shown that pre-trained LLMs are able to outperform state-of-\nthe-art learning-based APR tools on both Java and Python [5, 8].\nThe study [7] firstly introduces conversational-driven APR para-\ndigm. A usage of LLM and unit test failure information outperforms\nlearning-based and traditional APR tools. Similarly, the study [12]\ndemonstrates that conversational APR might be useful for a difficult-to-fix bugs. The current study evaluates potential of conversational\npatch generation on problems from SWE-Bench [3].\nAutomatic GitHub Issue Resolving. Automatic GitHub Issue Resolv-\ning was introduced in [3] to evaluate and challenge AI abilities. It\nis APR task at project level, with provided issue description and\npublic and hidden test suites. This challenge got an attention and\nmultiple approaches have been proposed [6, 10, 13]. The majority\nof them exploit pretrained LLM for fault localization and patch\ngeneration steps.\nRepeated Sampling. Today's state-of-the-art on SWE-Bench Lite\nis 43%. The repetitive sampling [1] allows to achieve up to 56%.\nAuthors sample with a high temperature 250 solutions for each\nSWE-Bench problem to enhance solution diversity and increase\nthe chance of obtaining the correct one. In like manner, including\ntest failure information in LLM request results in greater solution\ndiversity."}, {"title": "5 CONCLUSIONS", "content": "The results of this study demonstrates that LLM-based conversa-\ntional patch generation with a failing unit test feedback has po-\ntential to improve effectiveness of automatic program repair at\nproject-level.\nThe naive conversational patch generation pipeline, being ap-\nplied to problems from SWE-Bench Lite, allows to generate patches\nthat pass 47% of public and hidden tests. This is 7% higher than cur-\nrent state-of-the-art. These findings opens future questions about\nthe importance of precise fault localization, failing unit-tests, and\nmore advanced patch generation pipelines."}, {"title": "B PROMPT TEMPLATES", "content": "B.1\nPrompt template A\nYou need to fix the issue:\n<ISSUE_DESCRIPTION>\nYou are professional Python programmer. You are asked to fix code. Write the answer ONLY in the following format:\n<replace>\n{fixed python code}\n</replace>,\nwhere the code after <replace> should be the code which was already rewritten.\nFix the following code, insert fixed code of the full function, do not shorten it.\n<FUNCTION_TO_FIX>\nB.2\nPrompt template B\nYour code has errors, make reasoning then fix function using <replace></replace> tags. Replace function which was sent to you.\nYou need to change only provided function, you must not add new functions or new classes.\n<FAILURE LOG>\nB.3\nPrompt C\nError: Failed to create or apply patch. Use <replace></replace> tags. Rewrite what you fixed in the last iteration using the required\nformat. Replace function which was sent to you. You need to change only provided function, you must not add new functions or\nnew classes!"}]}