{"title": "Leveraging Estimated Transferability Over Human Intuition for Model Selection in Text Ranking", "authors": ["Jun Bai", "Zhuofan Chen", "Zhenzi Li", "Hanhua Hong", "Jianfei Zhang", "Chen Li", "Chenghua Lin", "Wenge Rong"], "abstract": "Text ranking has witnessed significant advancements, attributed to the utilization of dual-encoder enhanced by Pre-trained Language Models (PLMs). Given the proliferation of available PLMs, selecting the most effective one for a given dataset has become a non-trivial challenge. As a promising alternative to human intuition and brute-force fine-tuning, Transferability Estimation (TE) has emerged as an effective approach to model selection. However, current TE methods are primarily designed for classification tasks, and their estimated transferability may not align well with the objectives of text ranking. To address this challenge, we propose to compute the expected rank as transferability, explicitly reflecting the model's ranking capability. Furthermore, to mitigate anisotropy and incorporate training dynamics, we adaptively scale isotropic sentence embeddings to yield an accurate expected rank score. Our resulting method, Adaptive Ranking Transferability (AiRTran), can effectively capture subtle differences between models. On challenging model selection scenarios across various text ranking datasets, it demonstrates significant improvements over previous classification-oriented TE methods, human intuition, and ChatGPT with minor time consumption.", "sections": [{"title": "Introduction", "content": "Ranking relevant documents based on user queries has gained prominence as an essential component for Information Retrieval (IR) (Li et al., 2024), Retrieval-Augmented Generation (RAG) (Cuconasu et al., 2024), etc. As the volume of online content continues to grow, developing efficient and effective text ranking techniques has become increasingly imperative. Fortunately, the advancements in Pre-trained Language Models (PLMs) have spurred the development of dual-encoder capable of meeting these demands (Zhao et al., 2024). Usually, practitioners select the PLM to be employed by their intuition. However, such decision-making becomes unreliable as the number of PLMs rapidly grows (Lin et al., 2024), introducing a new challenge: the accurate and automatic selection of the most potent PLM to achieve the desired performance on given datasets, namely Model Selection (MS) (Bai et al., 2023b).\nEarly approaches to MS adopted a brute-force method, fine-tuning each model to identify the optimal one (Choi et al., 2020), which suffers from huge computing costs. Hence, it is preferable to estimate model fine-tuning result with computational efficiency. In pursuit of this goal, Transferability Estimation (TE) has garnered increasing attention in recent years, merely leveraging model-encoded features and labels (Agostinelli et al., 2022). Currently, advances in TE have primarily been developed for classification tasks (Bassignana et al., 2022). These methods approximate the log-likelihood from features to labels, aligning with the classification objectives well and demonstrating promising results on classification datasets (Bai et al., 2023b). However, the text ranking task fundamentally differs from classification task in which the matching scores for relevant query-document pairs are expected to be higher than those for irrelevant pairs (Luan et al., 2021). As a result, applying current classification-oriented TE methods directly to text ranking is sub-optimal (Bai et al., 2023a).\nTypically, a PLM well-suited to a dataset is expected to generate well-distributed sentence embeddings, where matched pairs exhibit high matching scores and vice versa, even before fine-tuning (Gao et al., 2021). Hence, the expected rank of documents relevant to queries resulted from initial model can serve as explicit and consistent indicators of model fine-tuning performance in text ranking, in contrast to the classification-oriented TE methods. However, a challenge arises from the anisotropy that usually exists in PLMs from small size to large size, manifesting as entangled"}, {"title": "Related Work", "content": "feature dimensions and a biased distribution of sentence embeddings (Liu et al., 2024). In this case, even if two sentences are very similar in semantics, their vector representations may be far apart or exhibit directional differences. As a consequence, it leads to inaccurate document rank (Su et al., 2021) and hinders the alignment of expected rank scores with ranking transferability. In solving the anisotropy problem, previous research has proposed isotropization methods such as BERT-flow (Li et al., 2020), BERT-whitening (Su et al., 2021), and SimCSE (Gao et al., 2021), aiming to transform sentence embeddings into an isotropic space. Considering BERT-flow and SimCSE both need careful training (Gao et al., 2021), we select BERT-whitening in this work since it only involves efficiently whitening the sentence embeddings. However, the isotropization overlooks the training dynamics involved in adapting model to the text ranking task (Sasaki et al., 2023), hindering the full expression of the fine-tuning ranking capabilities. To address this issue, we introduce an adaptive scaling mechanism after obtaining isotropic sentence embeddings, aiming to simulate the training dynamics by a simple but effective scaling operation, whose scaling weight can be efficiently solved by the least square method (Ding, 2023). Then, our approach allows for a more precise reflection of the model's true transferability through improved expected rank scores.\nIn summary, we present a promising alternative to human intuition for MS in text ranking: the Adaptive Ranking Transferability (AiRTran). In this approach, the expected rank is computed and further enhanced by the proposed Adaptive Isotropization (AdaIso) to estimate ranking transferability. On five widely studied datasets with two challenges pools of small and large candidate PLMs, covering various text ranking scenarios and dual-encoder backbones, our method shows superior efficiency compared to brute-force fine-tuning and show promising performance compared to previous classification-oriented TE methods, human intuition, and ChatGPT.\nEarly text ranking methods were primarily based on bag-of-words functions (Aizawa, 2003; Robertson and Zaragoza, 2009), which fall short of comprehensively understanding semantic relevance. In recent years, with the advancements in PLMs, dual-encoder has demonstrated superiority in capturing semantic interaction that goes beyond simple word overlap (Zhao et al., 2024). Prior research on dual-encoder has predominantly focused on improving aspects such as sentence representations (Gao and Callan, 2021; Tang et al., 2022; Wu et al., 2023), negative document sampling (Xiong et al., 2021; Formal et al., 2022; Chen et al., 2023), matching score computation (Khattab et al., 2021; Lassance et al., 2022; Wang et al., 2023), and supervisory signals construction (Wang et al., 2021; Zeng et al., 2022; Lin et al., 2023), etc.\nUnlike the above research focus on improving the network structure or training approach, our work explores selecting the most powerful PLM to achieve superior fine-tuning performance. The most direct approaches to MS involve either exhaustively trying all available models (Ni et al., 2022), or training a performance predictor to estimate how well a given model will perform on a dataset (Zhang et al., 2023; Meng et al., 2023), while they are less practical due to the large demands for model training. To address this issue, several TE methods have been developed for efficient MS, primarily in classification tasks (Bai et al., 2023b). These methods, when provided with model-encoded features and corresponding labels, quantify the degree of compatibility between them, treating this as a measure of transferability. Subsequently, they rank candidate models based on their transferability scores to identify the best-performing model. Some of these approaches are grounded in the assumption that an effective model should produce features with a high degree of class separability (Puigcerver et al., 2021; Kumari et al., 2022; P\u00e1ndy et al., 2022; Xu and Kang, 2023). Another line of research focuses on the approximation of likelihood from features to labels (Bao et al., 2019; You et al., 2021; P\u00e1ndy et al., 2022; Huang et al., 2022; Shao et al., 2022; Ding et al., 2022). Inspired by these remarkable works, this paper focuses on the selection of text ranking model which is an underexplored and challenging area."}, {"title": "Preliminaries", "content": "Before delving into the details of our proposed method, we begin by providing foundational knowledge, including the problem definition, dual-encoder model, and an examination of the challenges that previous TE methods have faced."}, {"title": "Problem Definition", "content": "In the context of MS for text ranking, we are presented with a pool of $M$ candidate PLMs, denoted as $\\{\\phi_i\\}_{i=1}^{M}$, and a dataset $D = \\{(q_i, d_i, y_i)\\}_{i=1}^{N}$ containing $N$ pairs of (query $q_i$, document $d_i$), where the label $y_i$ is 1 if $d_i$ is relevant to $q_i$ and 0 otherwise. To evaluate the performance of MS, we are also provided with the fine-tuning performances $\\{T(D)_i\\}_{i=1}^{M}$ of all candidate PLMs on $D$ (e.g., the R@10 score). The goal of MS is to assign scores to all candidate PLMs, denoted as $\\{S_{\\phi_i}(D)\\}_{i=1}^{M}$, which can well correlate with $\\{T(D)_i\\}_{i=1}^{M}$, allowing us to select the best-performing model."}, {"title": "Dual-encoder", "content": "The dual-encoder is commonly employed to rank desired results from large-scale candidate documents, which consists of PLM-based query and document encoders. The query $q$ and document $d$ are mapped into sentence embeddings by conducting mean pooling on the last layer's outputs of corresponding encoders ($e_q = \\phi(q)$ and $e_d = \\phi(d)$), where $e_q \\in \\mathbb{R}^{1\\times D}$, $e_d \\in \\mathbb{R}^{1\\times D}$, and $D$ represents the dimension of the embeddings. Subsequently, the matching score between the query and the document is computed by the dot-product\u00b9 between their sentence embeddings ($e_q e_d^\\top$).\nFor each query, the training objective of the dual-encoder is to ensure that its matching scores for relevant pairs are higher than those for irrelevant pairs. To achieve this goal, the probability of the relevant document $d^+$ is typically optimized:\n$p(d^+|q, d^+, I_q) = \\frac{\\exp(e_q e_{d^+}^\\top)}{\\exp(e_q e_{d^+}^\\top) + \\sum_{d^- \\in I_q}\\exp(e_q e_{d^-}^\\top)}$                                        (1)\nwhere $d^+$ is from $R_q$ that is the set of documents relevant to $q$, $d^-$ is the document irrelevant to $q$, and $I_q$ is the set of documents irrelevant to $q$."}, {"title": "Classification-oriented TE Methods", "content": "Previous research has been devoted to MS for classification tasks. Given the PLM-encoded feature $f$ and label $y$, these methods estimate the expected log conditional probabilities, denoted as $E[\\log(p(y|f))]$. This objective aligns with the training objective of the classification, establishing consistency between the estimated transferability and model fine-tuning performance."}, {"title": "Methodology", "content": "As illustrated in Figure 1, we introduce the AiR-Tran approach, motivated by the observation that expected rank score is inherently well-aligned with text ranking performance. To further improve this alignment, we propose AdaIso to mitigate the anisotropy problem and adapt raw sentence embeddings to the downstream text ranking task. As a result, the resultant expected rank effectively captures the true ranking transferability of the model. In the following sections, we will first describe how to compute expected rank and then elaborate on how AdaIso further achieves the enhancement."}, {"title": "Expected Rank as Transferability", "content": "The computation of expected rank score necessitates the determination of matching scores between queries and candidate documents. Fortunately, the dual-encoder can readily perform inference once initialized by a PLM, allowing for the direct acquisition of matching scores and resultant expected rank score. Consequently, the computation incurs minimal cost, merely involving a simple forward propagation process of the PLM on the dataset. After obtaining sentence embeddings and corresponding matching scores, we can further compute the ranks of documents relevant to queries and also the expected rank (as Eq. 2):\n$S(D) = E_{q\\sim p(q)}[E_{d^+ \\in R_q, d^- \\in Z_q} [\\frac{1}{\\text{rank}(d^+)}]]$                    (2)"}, {"title": "Adaptative Isotropization", "content": "Due to the inherent disparity between the pre-training task and the text ranking task, sentence embeddings encoded by PLMs often exhibit anisotropy issue that renders them unsuitable for direct use in text ranking. Anisotropy manifests as severe entanglement among the feature dimensions and distortion of the sentence embeddings' distribution, leading to matching scores that inadequately capture sentence semantics (Li et al., 2020). When working with these perturbed sentence embeddings, the computed expected rank score fails to genuinely reflect the model transferability.\nNumerous approaches have been introduced to address this issue, such as BERT-flow (Li et al., 2020), SimCSE (Gao et al., 2021), and BERT-whitening (Su et al., 2021). BERT-flow and SimCSE both necessitate careful model training, where BERT-flow involves the training of a flow-based calibration model, and SimCSE optimizes the contrastive objective to achieve isotropization. However, these methods run counter to the objective of efficient MS, and their training is also unstable. Hence, in this work, we opt for BERT-whitening as our primary isotropization method, which exclusively post-processes sentence embeddings through an efficient whitening operation, bypassing the need for model training (Huang et al., 2021).\nIn BERT-whitening, the mean sentence embedding $\\mu \\in \\mathbb{R}^{1\\times D}$ (Eq. 3) and the covariance matrix $\\Sigma \\in \\mathbb{R}^{D\\times D}$ (Eq. 4) are initially estimated based on all sentence embeddings $E \\in \\mathbb{R}^{2N\\times D}$:\n$\\mu = \\frac{1}{2N} \\sum_i E_i$                               (3)\n$\\Sigma = \\frac{1}{2N-1}(E - \\mathbb{1}_{2N}\\mu^\\top)(E - \\mathbb{1}_{2N}\\mu^\\top)^\\top + \\epsilon I_D$                       (4)\nwhere $\\mathbb{1}_{2N} \\in \\mathbb{R}^{2N\\times 1}$ is the 2N-dimensional ones vector, $I_D \\in \\mathbb{R}^{D\\times D}$ is the identity matrix, and $\\epsilon > 0$ is a small positive number to prevent a singular $\\Sigma$. Then the sentence embeddings are centered and transformed as Eq. 5:\n$\\hat{E} = (E - \\mathbb{1}_{2N}\\mu^\\top) U \\sqrt{\\Lambda^{-1}}$                                                                         (5)\nwhere $\\Sigma = U \\Lambda U^\\top$, $\\hat{E}$ is the whitened embeddings.\nHowever, isotropization does not account for the adaptation of sentence embeddings to the downstream task, which restricts its ability to accurately reveal the true ranking performance of the model. To address this limitation, we propose an adaptive scaling mechanism partially simulating the task adaptation by further scaling the isotropic sentence embeddings, expressed as $\\hat{E} \\odot (\\mathbb{1}_{2N} \\gamma^\\top)$, where $\\odot$ denotes the Hadamard product, and $\\gamma \\in \\mathbb{R}^{D\\times 1}$ represents the scaling weight. The crucial question is how to determine the optimal $\\gamma^*$. Fortunately, this weight vector can be derived by solving an ordinary least squares problem that minimizes the squared difference between predicted matching scores and ground truth labels. To begin, given the whitened sentence embeddings for all queries $(\\hat{E}_q \\in \\mathbb{R}^{N\\times D})$ and for all documents $(\\hat{E}_d \\in \\mathbb{R}^{N\\times D})$, the predicted matching scores $\\hat{Y} \\in \\mathbb{R}^{N\\times 1}$ can be calculated as:\n$\\hat{Y} = (\\hat{E}_q \\odot (\\mathbb{1}_N \\gamma^\\top)) \\odot (\\hat{E}_d \\odot (\\mathbb{1}_N \\gamma^\\top))\\mathbb{1}_D \\\\= \\mathbb{1}_N (\\gamma^2)^\\top \\hat{E}_q \\hat{E}_d^\\top \\mathbb{1}_D \\\\= (\\hat{E}_q \\hat{E}_d^\\top) \\gamma^2$                                                            (6)\nwhere $\\mathbb{1}_N \\in \\mathbb{R}^{N\\times 1}$ and $\\mathbb{1}_D \\in \\mathbb{R}^{D\\times 1}$. Next, given all ground truth labels $Y \\in \\mathbb{R}^{N\\times 1}$, the squared"}, {"title": "Experiments", "content": "To validate the AiRTran, the experiments are conducted on five datasets: SQUAD (Rajpurkar et al., 2016), NQ (Kwiatkowski et al., 2019), BioASQ (Bai et al., 2023a), SciFact (Wadden et al., 2020), and MuTual (Cui et al., 2020), covering typical text ranking scenarios (For dataset details, see Appendix A). To conduct TE, we randomly sample 1,000 queries from the training set of each dataset. The irrelevant documents for each query are then randomly sampled, since constructing irrelevant documents by human annotation or other sophisticated methods can be time-consuming, while this simple strategy also introduces a variable number of samples. Increasing the number of irrelevant documents usually creates a more challenging scenario, effectively highlighting differences in the ranking abilities of models. As a result, our experiments are carried out with varying sizes of candidate documents (consisting of one relevant document and multiple irrelevant documents) for each query, ranging from 2 to 10."}, {"title": "Candidate Model Pools", "content": "We have curated two model pools from Hugging Face's model hub (Jiang et al., 2023), consisting of 25 small PLMs (11M~140M parameters) and 25 large PLMs (1B~8B parameters), respectively. We fully fine-tuned the small PLMs and parameter-efficiently fine-tuned the large ones to obtain their fine-tuning results. Each fine-tuning is conducted using 5 different random seeds, and the results are averaged. Subsequently, the best fine-tuning performances are recorded to form $\\{T(D)_i\\}_{i=1}^{M}$ (We record R@10 for SQuAD and NQ since they are text retrieval tasks that focus on recall performance, and record P@1 for BioASQ, SciFact, and MuTual since they are text matching tasks that focus on matching accuracy.) The training details and fine-tuning results are presented in Appendix \u0412."}, {"title": "Evaluation Metric", "content": "Given the target of MS, when $S_i$ is higher than $S_j$, the corresponding $T_i$ is expected to be higher than $T_j$. To evaluate TE approaches, we employ Kendall's $\\tau$ to measure the degree of agreement between $\\{S_i(D)\\}, and $\\{T_i(D)\\}_{i=1}^{M}$. It is particularly valuable when working with ordinal or ranked data, without any assumptions about the underlying data distribution. The formula for calculating Kendall's $\\tau$ is as follows:\n$\\tau = \\frac{2\\sum_{1\\le i < j \\le M} I(T_i - T_j)I(S_i - S_j)}{M(M-1)}$                                          (11)\nwhere $\\tau \\in [-1, 1]$; the indicator function $I$ returns 1 for positive number and return -1 otherwise. For each TE method, we compute $\\tau_{Small}, \\tau_{Large}$ that denote the Kendall's $\\tau$ computed when $\\tau$ corresponds to the fine-tuning results of small model pool and large model pool, respectively."}, {"title": "Compared Approaches", "content": "We compare the following competitive TE methods which have proven effective when facing classification tasks: TMI (Xu and Kang, 2023), GBC (P\u00e1ndy et al., 2022), TransRate (Huang et al., 2022), NLEEP (Li et al., 2021), LinearProxy (Kumari et al., 2022), SFDA (Shao et al., 2022), PACTran (Ding et al., 2022), H-score (Bao et al., 2019), LogME (You et al., 2021), whose details are presented in Appendix C. We run each method using 5 random seeds and report averaged Kendall's $\\tau$. Note that these methods assume each data sample corresponds to one feature vector. To adapt"}, {"title": "Model Selection Performance", "content": "Table 1 presents the best results of all methods. The anisotropy issue leads to substantial overlap between features of matched and mismatched pairs, which hinders the accurate computation of Gaussian distance and entropy, resulting in inferior performances for TMI, GBC, and TransRate. The methods that simulate training dynamics including LinearProxy, SFDA, PACTran, and LogME exhibit strong performance. H-score also demonstrates notable improvements, attributed to its consideration of feature redundancy in addition to the inter-class variance. Note that NLEEP involves the feature transformation as well, while it doesn't consider"}, {"title": "Effect of Candidate Document Size", "content": "As depicted in Figure 2, we visualize the performance variations of AiRTran, and the visualization results of other methods are presented in Appendix F. It is observed that TMI, TransRate, and NLEEP show significant fluctuation. The other methods indicate relatively regular patterns of performances with the candidate document size increases, while they behave differently when facing different model pools. Both of the observations are attributed to the lack of alignment between their estimation score and model ranking ability."}, {"title": "Scoring Time Comparison", "content": "The scoring time, i.e., the time required for each TE method to score the transferabilities of all models given the model-encoded features, serves as a reflection of the efficiency of model selection. In Figure 3, we have visualized the trends in scoring time for all TE methods on two pools as the candidate document size increases. Broadly, there exists a linear relationship between the scoring time of all TE methods and the size of the candidate document set. Though the scoring time for all TE methods is significantly lower than the time required for the brute-force fine-tuning (In our case, fine-tuning all PLMs on all datasets took nearly three months using a single NVIDIA GeForce RTX 3090 24G.), the proposed AiRTran consistently operates with a fast runtime, thanks to the efficient whitening operation and the solution to the scaling weight."}, {"title": "Exploration of AdaIso", "content": "Though AiRTran is agnostic to isotropization method, we select whitening due to its efficacy. To verify this choice, we also instantiate AiRTran by BERT-flow and SimCSE, i.e., AiRTran (flow) and AiRTran (simcse), and compare them with AiRTran (whiten). As shown in Table 2, since the training of BERT-flow and SimCSE both need to be carefully tuned, the resultant AiRTran performances are unstable. Specifically, AiRTran (flow) performs best only in the SciFact with the large model. Though AiRTran (simcse) performs well in several datasets (e.g., NQ with small model and SciFact with small model), but overall, it is slightly outperformed by the whitening variant. In contrast, AiRTran (whiten) consistently shows the highest performance across most cases, generally being the most effective instantiation. Moreover, AiRTran (whiten) requires only seconds of CPU runtime to perform the whitening, whereas the other methods require costly GPU resources. We also conducted an ablation study for AiRTran (whiten) by removing isotropization (w/o Iso), adaptive scaling (w/o Ada), and full AdaIso (w/o Adalso), the results revealed that both adaptive scaling and isotropization play a crucial role, where removing any of them led to a significant drop. Additionally, our observations highlight that isotropization does not consistently yield improvements, e.g., \u201cw/o Ada\u201d performed worse than \"w/o AdaIso\u201d on MuTual, primarily due to its unsupervised nature, underscoring the necessity of adaptive scaling."}, {"title": "Relation to Alignment and Uniformity", "content": "We further use the following properties to investigate the inner workings of AdaIso: (1) Alignment: It quantifies the expected affinity between sentence embeddings of paired sentences. (2) Uniformity: It measures how uniformly the embeddings of different sentences are distributed (Wang and Isola, 2020). These two properties align well with the objective that matched pairs should remain close in embedding space, while embeddings for random instances should be widely scattered in the context of text ranking. As Eq. 12, we combine them to form a quality score, denoted as Q.\n$Q = E_{(q,d^+) \\sim P_{pos}} e_q^\\top e_{d^+} + E_{(x,x') \\sim P_{data}} -e_x^\\top e_{x'}                             (12)$\nIntuitively, a powerful model should exhibit high alignment and uniformity scores. Consequently, Q should exhibit a strong correlation with the model's fine-tuning results, serving as a valuable indicator for transferability, i.e., Quality score as Transferability (QTran). However, this correlation is challenging to fully establish when the anisotropy problem persists, and training dynamics are not considered. Fortunately, the proposed Adalso can solve this problem. As demonstrated in Table 3, it is observed that QTran computed on raw sentence embeddings only exhibits a weak correlation with model fine-tuning results. After whitening isotropization and adaptive scaling are conducted on the raw sentence embeddings, the correlations improve significantly, facilitating more accurate MS. We also observed that when isotropization is conducted, the further employment of adaptive scaling doesn't always bring improvement (i.e., $\\tau_{Small}$ in SQUAD), probably because the sentence embeddings lose too much original information after whitening and adaptive scaling weight is over-fitted on such perturbed features."}, {"title": "Comparison with Human and ChatGPT", "content": "Currently, the most prevalent method for MS often relies on human intuition or, alternatively, prompts to ChatGPT (OpenAI, 2022). Although such processes can be completed in a few minutes, the corresponding performances become unreliable when facing challenging MS scenarios. To demonstrate AiRTran's potential as a viable replacement for these methods, we conducted a comparative study among AiRTran, human intuition, and ChatGPT on SQUAD and BioASQ datasets. For MS by human intuition, we engaged 5 NLP practitioners (doctoral and master's students specializing in NLP) who were provided with metadata about the dataset and models. Similarly, ChatGPT was guided using instructions that utilized the same information (see Appendix G), and we parsed the ranking results of candidate PLMs from its responses. This process was repeated 5 times to capture the inherent variability in ChatGPT's responses. The results, depicted in Figure 4, emphasize the underperformance of human intuition and ChatGPT in terms of relevance and stability. This is due to their limited understanding of the dataset and models. In contrast, AiRTran excels in demonstrating transferability by effectively capturing the compatibility between model-encoded features and labels, thus showcasing a significant advantage."}, {"title": "Conclusion", "content": "Given the limitations of classification-oriented TE methods, we propose the use of AiRTran in this study. It demonstrates superior MS performance compared to competitive TE methods, human experts, and the ChatGPT MS agent, indicating its potential as a solution for MS in text ranking tasks. We hope that our work will offer text ranking practitioners valuable guidance and inspiration when selecting models for their datasets of interest in addition to their intuition."}, {"title": "Limitations", "content": "To construct irrelevant documents for each query, this study employs a random sampling strategy. While easy to implement and efficient in generating irrelevant pairs, it often results in documents that are too easy to discriminate. In such an un-competitive ranking scenario, both proficient and average models may perform well and achieve high transferability scores, complicating model selection. Therefore, there is a pressing need to explore more effective strategies for negative sampling in MS tasks. Additionally, although human experts and ChatGPT exhibit lower performance compared to AiRTran, their knowledge demonstrates a sophisticated understanding of model-to-dataset transfer. However, this study does not investigate the potential effectiveness of integrating their insights as a complement to the feature-label interactions captured by TE approaches."}]}