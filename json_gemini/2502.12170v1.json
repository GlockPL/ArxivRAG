{"title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections", "authors": ["Da Xiao", "Qingye Meng", "Shengping Li", "Xingyuan Yuan"], "abstract": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with ~1.8\u00d7-2.4\u00d7 compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/ Caiyun-AI/MUDDFormer.", "sections": [{"title": "1. Introduction", "content": "Residual connections (He et al., 2016), which help mitigate the vanish gradient problem, have become indispensable to training deep learning architectures from CNNs to Transformers, the latter becoming the de facto backbone for foundation models. Though being very simple and effective, residual connections still have limitations to be solved, especially with deep Transformers with dozens of layers made common by prevailing Transformer-based LLMs.\nOn one hand, although theoretical (Merrill et al., 2022) and experimental (Tay et al., 2021b) work have suggested that adding layers increases the expressive capacity and generalization performance of Transformers, it is observed that increasing depth beyond a certain point yields diminishing returns (Petty et al., 2023). The common practice of using Pre-Norm to stabilize training leads to the issue of representation collapse (Liu et al., 2020b), where hidden features in deeper layers become highly similar, and for popular families of open-weight LLMs, a large fraction of the layers can be removed with minimal degradation of performance (Gromov et al., 2024).\nOn the other hand, mechanistic interpretability studies reveal that Transformers do in-context learning tasks by composing model components (attention heads and MLPs) across different layers to form circuits, (Elhage et al., 2020; Wang et al., 2023; Merullo et al., 2024; Ni et al., 2025), where layers communicate with each other by writing to and reading from different subspaces of the residual stream. The residual stream as the shared communication channel may be overloaded and become the bottleneck for very deep models, hindering the formation of sophisticated circuits spanning distant layers necessary for complex tasks.\nDense connections (Huang et al., 2017) was proposed as a promising solution to the above issues, by allowing subsequent layers to directly access outputs of all preceding layers (Figure 2 (a)). It has shown effectiveness for CNNs with DenseNet (Huang et al., 2017), and recently for Transformers with DenseFormer (Pagliardini et al., 2024). However, these dense connection approaches use static (either fixed (Huang et al., 2017) or learnable (Pagliardini et al., 2024)) dense connection weights that are shared across sequence positions and different input streams of a Transformer block. As will be shown, this rigidity severely limits their expressive capacity in Transformers.\nIn this work, we propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective approach to address the shortcomings of residual connections. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input (the query, key, value or residual) of a Transformer block. These weights are used by depth-wise aggregate modules to combine outputs from all preceding layers, creating multiple input streams for the current layer. MUDD connections can be seen as depth-wise multi-head attention (Vaswani et al., 2017) and the cross-layer communication bandwidth is expanded far beyond the restriction of the residual stream.\nMUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer models. We conduct extensive experiments focusing on language model pretraining to evaluate MUDDFormer's effectiveness, efficiency and scalability. MUDDFormer significantly outperforms Transformer across various model architectures and scales (from 405M model on 7B tokens to 2.8B models on 300B tokens), achieving performance of Transformers trained with ~1.8\u00d7-2.4\u00d7 compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining perplexity and downstream tasks and even rivals Pythia-12B in five-shot in-context learning settings (Figure 1), while adding only 0.23% parameters and 0.4% computation. We also evaluate MUDD connections on vision Transformers and analyze the trained models to elucidate why MUDD connections work."}, {"title": "2. Method", "content": "We begin with a standard Transformer decoder with L layers on input sequence X = {x0, ..., xT}:\n$\\begin{aligned} \\mathbf{X}_{0} &=\\text { Embedding }(\\mathbf{X}) \\\\ \\mathbf{X}_{i} &=B_{i}\\left(\\mathbf{X}_{i-1}\\right), \\quad i \\in[1, L] \\\\ \\text { Transformer }(\\mathbf{X}) &=\\mathbf{X}_{L} \\end{aligned}$\nwhere Bi() is the ith Transformer block\u00b9 composed of a multi-head attention (MHA) module followed by a fully connected feed-forward network (FFN), both wrapped with Pre-LayerNorm (LN) residual connections:\n$\\begin{aligned} \\mathbf{X}_{A} &=\\text { MHA }(\\text { LN }(\\mathbf{X}), \\text { LN }(\\mathbf{X}), \\text { LN }(\\mathbf{X}))+\\mathbf{X} \\\\ B(\\mathbf{X}) &=\\operatorname{FFN}\\left(\\text { LN }\\left(\\mathbf{X}_{A}\\right)\\right)+\\mathbf{X}_{A} \\end{aligned}$\nIn this paper we use \u201clayer\u201d and \u201cblock\u201d interchangeably."}, {"title": "2.1. Static Dense Connections", "content": "In the simplest case, the DA module aggregates previous layers' outputs by taking a weighted sum of them (Figure 2 (b), also equivalent to DenseFormer (Pagliardini et al., 2024)):\n$\\overline{\\mathbf{X}}_{i}=\\mathrm{DA}^{\\text {static }}\\left(\\mathbf{X}_{: i} ; \\theta\\right)=\\mathrm{wsum}\\left(\\mathbf{a}_{i},\\left\\{\\mathbf{X}_{j}\\right\\}_{j=0}^{i}\\right) :=\\sum_{j=0}^{i} a_{i j} \\mathbf{X}_{j}$\nwhere wsum() is the weighted sum function taking the sequences of weights and values as inputs. Scalar aij is the jth value of the dense connection weight vector ai \u2208 Ri+1 which is trainable parameters, i.e., \u03b8 = {az}."}, {"title": "2.2. Dynamic Dense Connections", "content": "In Transformer-like sequence models, each layer processes information from multiple sequence positions and may benefit from differentiated and input-dependent dense connectivity for each position. Dynamic dense connections expand the connection weight for Xj from a static scalar aij to a vector Aij \u2208 RT, allowing Xj to contribute differentially to each position t \u2208 [1,T] of X\u2081 based on the hidden state X\u2081[t] \u2208 RD at that position. The i + 1 weight vectors stack into a matrix A\u1d62 \u2208 RT\u00d7(i+1) which is generated dynamically by a function A\u1d62(\u00b7) depending on X\u1d62 (Figure 2 (c), cf. Eq. (4)):\n$\\begin{array}{l} \\mathbf{X}_{i}=\\mathrm{DA}^{\\text {dynamic }}\\left(\\mathbf{X}_{: i} ; \\theta\\right) \\\\ =\\mathrm{wsum}\\left(\\mathbf{A}_{i}=\\mathbf{A}_{i}\\left(\\mathbf{X}_{i}\\right),\\left\\{\\mathbf{X}_{j}\\right\\}_{j=0}^{i}\\right) \\\\ :=\\sum_{j=0}^{i} \\mathbf{A}_{i j} \\mathbf{X}_{j} \\text { (with broadcasting) } \\end{array}$"}, {"title": "2.3. Multiway Dynamic Dense Connections", "content": "In a Transformer block, a single input is reused simultaneously as the query, key, value and residual of the MHA module (Figure 2 (e) top). These input streams play divergent roles and we hypothesize that they benefit from differentiated dense connectivity. To enable this, we first turn a normal Transformer block B(X) into a multi-input one B'(XQ, XK, XV, XR) by decoupling its input into four streams for query, key, value and residual, respectively (Eq. (7), Figure 2 (e) bottom), and then instantiate four DA modules, each specializing in one stream's dense connectivity (Eq. (8), Figure 2 (d))2:\n$\\begin{aligned} &\\mathbf{X}_{A}=\\operatorname{MHA}\\left(\\operatorname{LN}\\left(\\mathbf{X}^{Q}\\right), \\operatorname{LN}\\left(\\mathbf{X}^{K}\\right), \\operatorname{LN}\\left(\\mathbf{X}^{V}\\right)\\right)+\\mathbf{X}^{R} \\\\ &B^{\\prime}\\left(\\mathbf{X}^{Q}, \\mathbf{X}^{K}, \\mathbf{X}^{V}, \\mathbf{X}^{R}\\right)=\\operatorname{FFN}\\left(\\operatorname{LN}\\left(\\mathbf{X}_{A}\\right)\\right)+\\mathbf{X} \\end{aligned}$\n$\\begin{aligned} &\\mathbf{X}^{Q}=\\mathbf{X}^{K}=\\mathbf{X}^{V}=\\mathbf{X}^{R}=\\mathbf{X}=\\text { Embedding }(\\mathbf{X}) \\\\ &\\mathbf{X}_{i}=B\\left(\\mathbf{X}_{i-1}^{Q}, \\mathbf{X}_{i-1}^{K}, \\mathbf{X}_{i-1}^{V}, \\mathbf{X}_{i-1}^{R}\\right) ; \\\\ &\\mathbf{X}_{i}^{Q}, \\mathbf{X}_{i}^{K}, \\mathbf{X}_{i}^{V}, \\mathbf{X}_{i}^{R}=\\operatorname{DA}^{Q}\\left(\\mathbf{X}_{: i}\\right), \\ldots, \\operatorname{DA}^{R}\\left(\\mathbf{X}_{: i}\\right), i \\in[1, L] \\\\ &\\text { MUDDFormer }(\\mathbf{X})=\\mathbf{X}_{L}^{R} \\end{aligned}$"}, {"title": "2.4. Parameter Re-allocation", "content": "Due to the dense connections, MUDDFormer's upper layers have the opportunity to process more information than lower layers and thus may need more parameters. We re-allocate the parameters of a standard Transformers to make the size of FFN sub-layers grows with depth. Specifically, let Df be the hidden dim of the original FFN, we compute D'f(i), the hidden dim of FFN at layer i for MUDDFormer using linear interpolation:\n$\\mathbf{D}_{f}^{\\prime}(i)=\\frac{0.5(L-i)+1.5(i-1)}{L-1} D_{f}$\ni.e., the FFN hidden dim D'f grows linearly from 0.5Df to 1.5Df. The total number of parameters remains unchanged."}, {"title": "2.5. Optional Normalization", "content": "To stabilize training models with large depth/width ratios, we propose a variant of MUDDFormer by applying RMSNorm before and after DA module, and adding a residual connection to DA module after the post-RMSNorm:\n$\\begin{array}{l} \\mathbf{X}_{: i}=\\left{\\operatorname{Norm}\\left(\\mathbf{X}_{0}\\right), \\ldots, \\operatorname{Norm}\\left(\\mathbf{X}_{i}\\right)\\right} \\quad (\\text { PreDANorm }) \\\\ \\mathbf{X}_{i}=\\operatorname{Norm}\\left(\\mathrm{DA}_{i}\\left(\\mathbf{X}_{: i}\\right)\\right)+\\mathbf{X}_{i} \\quad (\\text { PostDANorm }) \\end{array}$\nIt is similar to the hybrid-norm strategy used by recent models such as Gemma 2 (Team et al., 2024) and Grok-1 (xai org, 2024), though we apply it to DA modules instead of MHA/MLP modules. We use this PrePostDANorm variant to train the DeepNarrow models in scaling law experiments in Section 3.1 and the MUDDViT model in Appendix D."}, {"title": "2.6. Complexity Analysis", "content": "Table 1 shows the ratios of extra parameters and computation introduced by MUDD connections with both analytical results and typical concrete values. The derivations are in Appendix B. The ratio of extra parameters, i.e. parameter count of W\u2081 and W2 of DA modules divided by that of the whole model, is proportional to the rectified depth/width ratio \u03b7 = L+3/D. The ratio of extra computation, i.e. FLOPs of generating MUDD connection weights and cross-layer aggregation divided by FLOPs of the whole forward pass, besides proportional to \u03b7, decreases with \u03c1 = T/D. Both ratios are negligible for commonly used settings."}, {"title": "3. Experiments", "content": "Implementation Details We implement MUDDFormer model and training in JAX. We initialize the MUDD connection weight generating parameters W\u2081 and W2 with N(0, 1/D) and 0 respectively, and initialize the static weight vector ai with 1 at air and 0 elsewhere. This reduces MUDDFormer to Transformer at the beginning of training, which is found to be critical for good performance. If PrePostDANorm is used, we initialize the scale parameters of Pre-DA and Post-DA RMSNorms with 1 and 1e-3, respectively, and initialize a\u017e to 0 because X\u2081 is added as the residual after DA modules (Equation (10)). For the other parameters outside DA modules, we use Xavier normal initializer.\nOrganization Our evaluation focuses on language modeling with decoder-only Transformer architecture, analyzing both pretraining scaling laws (Section 3.1) and downstream task performance (Section 3.2) with large scale training on the Pile dataset (Gao et al., 2020). Section 3.3 elucidates why MUDDFormer works through analyzing trained models, followed by efficiency analysis (Section 3.4) and ablations (Section 3.5). Extended vision experiments are provided in Appendix D."}, {"title": "3.1. Scaling Laws", "content": "Settings Table 2 (top half) specifies the model sizes and hyperparameters for scaling experiments, which are mostly taken from GPT-3 specifications (Brown et al., 2020). We untie input and output embedding matrices. We train with context length 2048 and set the number of training tokens to roughly match Chinchilla scaling laws (Hoffmann et al., 2022). The other hyperparameters are in Appendix C.1. In another set of scaling experiments we trade off some width for depth to train DeepNarrow models (Table 2 (bottom half)) to see if MUDD connections offer any benefits for this style of scaling.\nBaselines We compare MUDD with two recently proposed approaches to enhancing residual connections in Transformers: DenseFormer (Pagliardini et al., 2024) (same as the static dense connections described in Section 2.2) and Hyper-Connections (Zhu et al., 2025). We also compare to Transformer with dynamic dense connections (DDFormer) as described in Section 2.2. All these approaches are applied to an improved and now widely adopted Transformer architecture (Touvron et al., 2023) with rotary positional encoding (ROPE) (Su et al., 2024), SwiGLU MLP (Shazeer, 2020), etc. (often called Transformer++). We also include the plot for the original Transformer architecture used by GPT-3 as a comparison. The details for these baselines are in Appendix C.2."}, {"title": "3.2. Large Scaling Training and Downstream Evaluations", "content": "Settings We compare MUDDFormer with the open source Pythia model suit (Biderman et al., 2023) at large scale training on 300B tokens of Pile. Specifically, we train two models, MUDDPythia-1.4B and MUDDPythia-2.8B, and compare them with Pythia models ranging from 1.4B to 12B. For fair comparison and clear quantification of the gain brought by MUDD, except adding MUDD connections as described in Section 2, MUDDPythia uses exactly the same architecture choices (e.g. parallel attention and MLP, rotary embedding with 1/4 head dim) and training hyperparameters (e.g. optimizer settings, learning rate schedule, batch size, context length, initialization methods) as Pythia (refer Biderman et al. (2023) Appendix E for details). To evaluate if MUDD connections also work well with more advanced Transformer++ architecture and training recipe at this large scale, we also train MUDDFormer-2.8B based on Transformer++ instead of Pythia architecture with a larger learning rate of 3.2e-4 cosine decayed to 3.2e-6. Except these two changes, the other architectural and training hyperparameters are kept the same as MUDDPythia-2.8B.\nEvaluation Datasets Besides the datasets used by Pythia for downstream evaluation (LAMBADA (Paperno et al., 2016), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018), SciQ (Welbl et al., 2017), LogiQA (Liu et al., 2020a)), we also include BoolQ (Clark et al., 2019) and HellaSwag (Zellers et al., 2019) for commonsense reasoning, RACE (Lai et al., 2017) for reading comprehension, all of which are widely used benchmarks. We evaluate zero-shot and five-shot results using LM evaluation harness (Gao et al., 2023).\nResults As shown in Table 3 and Figure 1, besides lower Pile validation ppl, MUDDPythia also significantly outperforms Pythia at 1.4B and 2.8B scales on downstream task accuracies. Notably, MUDDPythia-2.8B matches Pythia-6.9B (2.46\u00d7 compute) on both pretraining ppl and downstream evaluation. Augmented with better Transformer++ architecture and training recipe, MUDDFormer-2.8B even outperforms Pythia-12B."}, {"title": "3.3. Analyzing and Understanding", "content": "We analyze and compare Pythia-2.8B and MUDDPythia-2.8B trained in Section 3.2 to elucidate why MUDD connections work. The analysis is done on 1024 randomly sampled sequences of length 2048 from Pile validation set.\nRepresentation Collapse Figure 5 quantifies representation collapse through cosine similarity between the inputs of adjacent layers. While Pythia exhibits progressive collapse with >0.97 similarity in later layers, MDDDPythia maintains more distinct input representations, particularly in the value stream. Thanks to input stream decoupling and stream-specific aggregation, DA modules can freely aggregate distinct value input streams for MHA modules of each layer at each sequence position. These values will then be moved to other positions by MHA at this layer without polluting the residual stream of the current position. The relative importance of dense connections for the value stream is also evidenced by ablation studies (Section 3.5). We compare illustrative V-composition circuits\u2074 in Transformer and"}, {"title": "3.4. Training and Inference Efficiency", "content": "Besides theoretical complexity analysis in Section 2.6, we assess training and inference efficiency of MUDDFormer compared with Transformer in real-world settings.\nSettings Though we use Pythia's architecture in large scale training, the evaluation in this section is done on untrained models with Transformer++ (Llama) architecture, which is of more practical relevance due to better performance. We measure on three model sizes: 1.3B, 2.8B and 6.9B, which are \"Llama-ed\" version of Pythia 1.4B, 2.8B and 6.9B respectively. We train on Google Cloud TPU v5p-128 pods with context length of 2048, batch size of 2M tokens and measure training throughput. We do inference on NVIDIA A100 80G GPU with prompt length of 4096, batch size of 1 and measure the speed to generate 128 tokens. We repeat the measurements 3 times and take the average. We implement training and inference in pure JAX and PyTorch respectively without writing any Pallas/Triton-based custom kernels. We use torch.compile (PyTorch 2.5.1) to accelerate both Transformer++ and MUDDFormer.\nResults As shown in Table 4, the training and inference overheads, while larger than the theoretical estimates in Table 1 and not negligible, are entirely acceptable considering the significant performance gain. The overheads primarily stem from the series of small operations and additional I/O introduced by DA modules. We believe that kernel fusion techniques offer potential for further acceleration and leave it for future work."}, {"title": "3.5. Ablations and Variants", "content": "We conduct ablation studies with the 405M Transformer++/MUDDFormer models in scaling law experiments for language modeling in Section 3.1.\nAblation settings We do two groups of experiments and report the perplexity results in Table 5. In the first group, we progressively add the four components, i.e. static (Section 2.1), dynamic (Section 2.2), multiway (Section 2.3) dense connections and parameter re-allocation (2.4)) to Transformer++ to finally obtain MUDDFormer to compare the contribution of each component. In the second group, we focus on the multiway aspect and study the effect of dense connections for the four decoupled inputs streams by replacing each of them with normal residual connection respectively.\nResults All three ingredients of dense connections, i.e. static, dynamic and multiway, make contributions. While parameter re-allocation is effective on MUDDFormer, it deteriorates Transformer++. Removing dense connections for each of the four streams hurts performance and the value stream benefits most from dense connections.\nVariants with Sparse Connectivity We design MUDDFormer variants by approximating its dense connections with two sparse connectivity patterns: 1) dilation and periodicity (MUDDFormer-k\u00d7p, also used in (Pagliardini et al.,"}, {"title": "4. Related Work", "content": "Enhancing Residual Connections Despite the pervasive use of residual connections (He et al., 2016) in modern deep architectures, various approaches have been proposed to address their issues such as representational collapse and diminishing return for deeper models by strengthening cross-layer communication. Huang et al. (2017) introduced DenseNet for CNNs. Inspired by it, Pagliardini et al. (2024) proposed DenseFormer for Transformers, which uses Depth Weighted Averaging modules to aggregate outputs from all preceding layers with static, learnable weights. Most recently, Zhu et al. (2025) proposed Hyper-Connections (HC), an alternative to residual connections that uses both static and dynamic weights to adjust inter-layer dependencies. Other research has explored different forms of cross-layer attention (ElNokrashy et al., 2022; Fang et al., 2023; Wang et al., 2024) which retrieve or update representations across different layers in a more flexible manner. MUDDFormer is closely related to DenseFormer and HC but differs in critical ways. First, unlike DenseFormer, our MUDD connections dynamically compute per-position weights conditioned on the hidden states. Second, although HC uses a combination of static and dynamic weights to expand the hidden states, it does not employ explicit all-to-all cross-layer dense connectivity. Moreover, none of existing approaches consider decoupling the four input streams of a Transformer block by a multiway design, which is shown to bring significant performance gain in MUDDFormer.\nMechanistic Interpretability Research in this field employs various attribution methods (Conmy et al., 2023; Hanna et al., 2024) to uncover the circuits within Transformers that underlie specific capabilities (Elhage et al., 2020; Wang et al., 2024; Ni et al., 2025). These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning a key insight motivating MUDD connections' design, which explicitly facilitates such interactions.\nCross-Layer KV Cache Optimization Brandon et al. (2024) proposes Cross-Layer Attention (CLA) to reduce Transformer KV cache size by sharing keys and values between adjacent layers, trading expressiveness for efficiency. Our MUDD connections enable cross-layer information flow between KV caches via dense connections on key and value streams. This enhances KV cache expressiveness and utility, improving in-context learning as evidenced by experiments. OmniNet (Tay et al., 2021a) achieves a fully global KV Cache receptive field by allowing each token to attend to all tokens in all layers. MUDDFormer attains a similar effect in a much more efficient way via composition of cross-layer dense connections and within-layer attention.\nIntra-Layer Architectural Innovations Many other studies attempt to enhance the performance or efficiency of foundational sequence models with individual layers, including attention mechanisms (Ye et al., 2024; Leviathan et al., 2024; Liu et al., 2024), sub-quadratic linear attention or RNN/SSM architectures (Gu & Dao, 2024; Dao & Gu, 2024; Peng et al., 2024; Yang et al., 2024) and sparse Mixture-of-Experts (MoE) (Fedus et al., 2022; Dai et al., 2024). By contrast, MUDD connections focus on cross-layer communication, making it orthogonal and complementary to these approaches. We leave the exploration of combining MUDD connections with these within-layer optimizations for future work."}, {"title": "5. Conclusion", "content": "We introduced Multiway Dynamic Dense connections to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Experimental results showed that MUDDFormer is effective, efficient and scalable. It significantly outperforms Transformer baselines in language and vision tasks and improves emergent abilities such as in-context learning with minimal overhead. MUDD connections have the potential to become an indispensable component of next-generation foundation models."}, {"title": "A. PyTorch Style Pseudo-code for MUDDFormer", "content": "# B = batch_size; T = seq_len; D = model_dim\n# L = layer_index; C = num_ways = 4; K = DA_hidden_dim = C*(L+1)\ndef generate_dw(x, mudd_theta): # x: BxTxD\nw1, w2, a = mudd_theta # w1: DxK, w2: Kx(C*(1+1)), a: Cx(1+1)\ndw = GELU(RMSNorm(x) @ w1) @ w2 + a\ndw = rearrange(dw, 'B T (CL)-> B T L', C=4)\nreturn dw\ndef DA(Xs, mudd_theta): # Xs: List(1+1)x[BxTxD]\ndw = generate_dw(Xs[-1], mudd_theta)\nxs = []\nfor c, way in enumerate(('Q', 'K', 'V', 'R')):\nx = sum([dw[c, :, :, j:j+1] * Xs[j] # BT1,BTD->BTD\nfor j in range(len(Xs))])\nxs.append(x)\nreturn xs\ndef muddformer(x, model):\nX = model.embedding(x)\nXs = [x]\nxq, xk, xv, xr = X, X, X, X\nfor block in model.blocks:\nattn_out = block.attn(LN(xq), LN(xk), LN(xv)) + xr\nX = block.ffn(LN(attn_out)) + attn_out\nXs.append(x)\nxq, xk, xv, xr = DA(Xs, block.mudd_theta)\nreturn xr"}, {"title": "B. Details of Complexity Analysis", "content": "Compared to Transformer++, extra compute and parameters in MUDDFormer are introduced by DA modules, increasing from bottom layer to top layer, due to varied hidden dim Ki of DA at layer i. We approximately calculates Roparams and RAFLOPS, the ratios of extra parameters and computation by considering an average layer in the middle of MUDDFormer. In this layer, K = 4(L+1) = 4(L/2 + 1) = 2(L + 3). We assume D \u00bb K in a typical Transformer architecture. We denote p = T/D and \u03b7 = (L+3)/D. To simplify, we ignore RMSNorm because it is negible in terms of parameters and computation.\nRatio of extra parameters\n$R_{\\text{aparams}} = \\frac{\\sum_{l=1}^{L} (DK + K^2)}{12LD^2} \\approx \\frac{DK + K^2}{12D^2} = \\frac{DK}{12D^2} \\approx \\frac{K}{12D} = \\frac{(L+3)/2}{6D} = \\frac{\\eta}{6}$\nRatio of extra FLOPs.\n$R_{\\text{aFLOPs}} = \\frac{\\sum_{l=1}^{L} (TDK + TK^2 + TDK)}{LDT(12D + T)} = \\frac{2TDK + TK^2}{DT(12D + T)} \\approx \\frac{2DK + K^2}{D(12D + T)} \\approx \\frac{2K}{12D + T} = \\frac{4(L+3)}{12D + T} = \\frac{\\eta}{3 + \\rho/4}$"}, {"title": "C. Hyperparameters and Baselines for Scaling Law Experiments", "content": "C.1. Hyperparameters\nWe use the AdamW optimizer with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.95, gradient clip value of 1.0, weight decay of 0.1, 1% learning rate warmup steps followed by cosine decay to 10% of its maximal value, and no dropout. These hyperparameters are mostly taken from the GPT-3 paper (Brown et al., 2020) and are also used by all the baseline models listed below.\nC.2. Baseline Models\n\u2022 Transformer: The standard Transformer based on GPT-3.\n\u2022 Transformer++: An improved Transformer architecture adopted by Llama (Touvron et al., 2023) etc. with rotary positional encoding (RoPE) (Su et al., 2024), SwiGLU MLP (Shazeer, 2020), RMSNorm instead of LayerNorm and no linear bias.\n\u2022 DenseFormer: The DenseFormer model from Pagliardini et al. (2024) without dilation, which has the best performance according to the paper. We implemented the model in JAX based on the PyTorch code released by the authors\u2076.\n\u2022 Hyper-Connections: The dynamic hyper-connections with expansion rate n = 4 (DHC\u00d74) from Zhu et al. (2025), which achieves superior results on language model pre-training and is the recommended configuration in the paper. We implemented the model in JAX based on the PyTorch Implementation given in Appendix J of the paper.\n\u2022 DDFormer: Transformer with dynamic dense connections but without multiway splitting as described in Section 2.2."}, {"title": "D. Image Classification with ViT", "content": "Besides decoder-only transformer for language modeling, we apply MUDD connections to Vision Transformer (ViT, an encoder-only Transformer) (Dosovitskiy et al., 2020) for image classification on the ImageNet-1k dataset (ILSVRC-2012). Implementation and experimental settings (e.g. the use of RandAugmention+MixUp, fixed 2D sincos position embedding and global average pooling) are based on the Big Vision codebase\u2077. We use AdamW with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.999, gradient clip value of 1.0, weight decay of 0.3, learning rate of 0.003 with 10000 warmup steps followed by cosine decay to 0, batch size of 4096, RandAugment of level 10, Mixup of 0.2 and dropout rate of 0.1. We use ViT-S/16 as the baseline model and equip it with MUDD connections to obtain MUDDViT-S/16. We also compare with a 1.72\u00d7 larger model ViT-M/16 (Table 6). We report validation loss and top-1 accuracy results on 90 and 300 epochs in Table 7. As can be seen, the gain from MUDD connections decreases a bit during the training progress, probably because many epochs of repeated passes over the same dataset diminish the additional expressive capacity brought by MUDD connections. Despite this, MUDDViT-S/16 still outperforms ViT-S/16 by 2% on epoch 300, also surpassing ViT-M/16."}, {"title": "E. Visualization", "content": "Head activation from attention patterns In Section 3.3, we show that the ratio of head activations in MUDDPythia-2.8B is larger than that in Pythia-2.8B. Here we draw the actual attention patterns on a randomly sampled sequence of length 32 from Pile validation set for the 32 heads in layer 25 of these two models in Figure 9 and 10. It is clear that attentions in Pythia mainly concentrate on the sink token (inactive) while attentions in MUDDPythia disperse on various tokens (active).\nCross-layer dynamic weights To better understand MUDDPythia, we visualize dynamic dense connection weights. Due to the high variance of the norm of hidden states Xi, we scale those weights by the average norm of each layer, rectifying the importance of weights. The rectified mean and standard deviation of dynamic connection weights in MUDDPythia-2.8B are shown in Figure 11 and 12, respectively. It is evident that the patterns of the four streams (query, key, value, residual) differ from each other, validating the necessity of separating them from the standard residual"}]}