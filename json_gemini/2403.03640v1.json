{"title": "Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People", "authors": ["Xidong Wang", "Junying Chen", "Nuo Chen", "Yan Hu", "Yidong Wang", "Xiangbo Wu", "Anningzhe Gao", "Xiang Wan", "Haizhou Li", "Benyou Wang"], "abstract": "Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion. We will open-source training corpora, code, model weights and evaluation benchmark.", "sections": [{"title": "1 Introduction", "content": "The integration of medical knowledge and artificial intelligence has always been a focal point of research communities, with each incremental improvement potentially enhancing patient experiences and healing rates\u2014serving as a direct manifestation of technology for good. Although medical large language models are promising, existing works are mainly in Chinese or English. The multilingual adaption of medical LLMs, as part of the democratization of large models, seeks to extend the benefits of cutting-edge LLMs to a broader spectrum of users, including those from underrepresented communities. This movement is akin to the historical endeavors to disseminate transformative technologies like electricity and vaccines to wider communities, positing LLMs as the modern equivalents of these essential innovations.\nRationale of Multilinguality in Medical LLMs The rationale of multilingual adaption in medical LLMs could be twofold. Firstly, non-native English-speaking doctors often engage in bilingual learning through their native language and English from the outset, naturally introducing multilingual challenges in the learning process . Secondly, to better serve local communities, especially in countries and regions with scarce medical resources, medical aid based on local languages often achieves higher communication efficiency and acceptance. Meanwhile, local medical knowledge can complement mainstream medical knowledge, fostering mutual benefits to accelerate medical development. Our pilot study in Sec. 2 also reveals that joint training of multiple languages enhances performance in the medical LLMs, indicating a beneficial complementarity among languages.\nThe Corpora: ApolloCorpora Towards building multilingual medical LLMs, the first step is to build high-qaulity corpora. We select the six most populous languages: English, Chinese, Hindi, Spanish, French, and Arabic for experiments 1. As shown in Fig. 2, it encompasses a diverse array of linguistic backgrounds, particularly in regions often characterized by limited medical resources (e.g., areas with lower average life expectancies). We collect and process data from data sources including books, clinical guidelines, encyclopedias, papers, online forums and examinations, obtaining ApolloCorpora with 2.5B tokens.\nThe Lite LLM: Apollo The resulted multilingual medical LLMs trained by ApolloCorpora are named Apollo, to commemorate the Greek deity associated with healing, disease, the Sun, and light; this symbolizes the democratization of medical LLMs to 6 billion people, illuminating global healthcare. We explore a new domain adaption method that rewrite the pre-training corpora into QA pairs using ChatGPT and adaptively sample training data, resulting in a smoother transition compared with the typical paradigm with continued pretraining and instruction tuning. Apollo ranges from 2B to 7B parameters. The advantage of the relatively-small model scale includes potential use as draft models for speculative decoding or as proxy models for ProxyTuning. In particular, we apply proxy tuning on top of Apollo to larger general LLMs, enhancing its multilingual medical capabilities. This is achieved without the need to directly train the general model using sensitive medical corpora, thereby underscoring the practical significance of Apollo in terms of protecting the privacy of medical training data against centralized training methods."}, {"title": "2 Pilot Study on the Multilinguality of Medical LLMs", "content": "This section presents two contrasting hypotheses regarding the nature of medical knowledge and its representation in LLMs.\nLanguage-neutral Hypothesis It is commonly believed that knowledge, whether medical or general, should be independent of language. For example, the fact that the sun rises in the east remains unchanged whether expressed in English or Chinese, suggesting that knowledge might be considered language-neutral. Consequently, medical corpora in various languages could serve as an augmentation for training, thereby enhancing the efficacy of the resulting medical LLMs.\nLanguage-dependent Hypothesis However, due to historical, cultural, and regional political influences, medical knowledge can vary significantly across different cultural contexts, especially as reflected in language. The integration of medical knowledge across languages might dilute the local specificity of medicine due to differences in lifestyle and constitution across regions. For instance, in traditional Chinese medicine, colds are classified into types caused by heat or cold, and treatments may vary locally, relying on unique medical recipe that uses local herbs or substances. This indicates that historical and cultural traditions can shape medical knowledge to some extent."}, {"title": "3 ApolloCorpora, Multilingual Medical Dataset", "content": "In this chapter, we first introduce the organizational philosophy of the data set, then introduce the structure of the data set and the data collection process, then introduce the further processing of the data set, and finally analyze the local medical characteristics reflected in the data set.\nOur goal is to enhance the multilingual medical capabilities of large-scale pre-trained models at an affordable computational cost, provide the community with a lightweight reproducible solution, and promote further exploration by the scientific research community. Considering that the model pre-training was most likely trained on PubMed-related data sets, we did not follow the mainstream method and repeat training on all Pubmed data.\nIf the model that has undergone large-scale pre-training is regarded as a medical student who is about to enter college with certain basic knowledge, in order to obtain more professional capabilities, we only need to replicate the high-quality data learned on the growth path of medical students. Starting from this point, after extensive communication with doctors and medical students, we identified six high-quality medical data collections: medical books, medical encyclopedias, medical clinical guidelines, medical papers, medical examinations, and professional doctor-patient dialogues. At the same time, in order to simulate the data input in the growth process of medical students in addition to medical professional knowledge, we also sampled medical-related parts of the Internet and collected instruction following data for some general tasks (including mathematical tasks of code).\nFor the dimension of multilingual medical expertise, we insist on using only medical data sets entirely from local languages and do not translate any medical-related data. This is done out of the following two considerations. First, there are many related works that prove that medical translation is a very complex task that cannot be simply solved by translation software; second, the expression habits of different languages, effective drugs, and even culture and Taboo terms arising from faith need to come from the local community intact, so as to maximize communication efficiency and avoid conflicts."}, {"title": "3.3 Data processing", "content": "Rewrite If we continue to train plain text, it is likely to destroy the original output format and knowledge. We want to explore whether rewriting the original pre-training corpus into QA pairs in the context of continuing training can help increase its medical capabilities without destroying the original model's capabilities. We use ChatGPT\u00ba to generate questions and answers for a certain paragraph. For paragraph interception, we divide it according to the basic semantic units in the data set, such as sections in books and guides, paragraphs in website data, single wiki entry and abstracts of papers. For basic semantic units that are too long, we comprehensively consider the knowledge expression density of the language and subdivide different languages into blocks of different lengths to ensure that the semantic information covered by a single paragraph does not exceed the amount of information that can be included in a question and answer pair. For Spanish, French, English and Hindi we use 2048, for Chinese we use 256 and for Arabic we use 128. Prompts for generating QA pairs are detailed in the Appendix A.2.\nData Leakage Checking The issue of data leakage is a recent focus of the academic community, which largely determines whether the results of the paper are convincing. For knowledge embedding tasks, data leakage screening with different stringency often leads to different performance. We follow Med-PaLM2 and adapt a more stringent deletion strategy. Specifically, we define a data item as leaked data if the entire question or at least 64 consecutive characters overlap with the data item. Regarding the exam exercise data source, there were 580,645 exercises before screening, and 3,041 exercises were deleted, with a screening rate of 0.52%. For other data sources, since they are not exam questions, there is no difference before and after filtering."}, {"title": "3.4 Localized features", "content": "As shown in the Fig. 3, we illustrate the local language features in the dataset by language:\nIn terms of symptom diagnosis, local languages retain the terminology of traditional medicine, and due to different geographical environments and living habits, the possibility that a certain symptom corresponds to different diseases is also different: for Chinese, a disease has two aspects: \u201cb\u00ecng\u201d and \u201czh\u00e8ng\u201d. The former is often translated as \"disease entity\u201d. The latter, and more important one, is usually translated as \"pattern\u201d. For example, the disease entity of a common cold might present with a pattern of wind-cold in one person, and with the pattern of wind-heat in another"}, {"title": "4 XMedBench, Multilingual Medical Knowledge Assessment", "content": "Construction We focus on assessing multilingual medical knowledge, select multiple-choice questions as assessment tasks, and collect common data sets with local medical characteristics as much as possible. For English, we use the MedQA-USMLE, MedMCQA, and medical-related parts of MMLU; for Chinese, we used the of MedQA-MCMLE and medical-related parts CMMLU ; for Spanish, we used HEAD-QA ; for French, we used FrenMedMCQA ; For Arabic and Hindi, which lack local assessments, we make a compromise by applying translated versions of MMLU11. Specifically, we follow Med-PaLM2 and select six subcategories in MMLU: Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, and College medicine. For MedQA, we choose the 4-options version. For CMMLU, we select seven subdirectories: Anatomy, Clinical knowledge, College medicine, Genetics, Nutrition, Traditional chinese medicine, and Virology.\nSettings We adapt 3-shot evaluation, and the specific evaluation prompt is shown in the Fig. 4. To determine the answer, we use regular matching from the answers generated by the model. For the generation strategy, we do not perform sampling and set the maximum and minimum number of generated tokens to 128 and 2. For model loading, except for the 0.5B size model which uses full precision loading, we uniformly use half precision loading. Please see the Appendix for details of the models.\nResults As shown in Tab. 3, GPT-4 and Qwen-72B rank first in closed source and open source with accuracy rates of 73.37 and 68.74 respectively. The gap between closed source and open source is gradually decreasing. The Apollo series models achieve the best performance of models of the same size. Apollo-7B achieve the same performance as GPT-3.5, Apollo-1.8B achieve the same performance as Mistral-7B, and Apollo-0.5B achieve the same performance as Llama2-7B."}, {"title": "5 Apollo, the Lite Multilingual Medical LLM", "content": "We have two main starting points for training the small model. First, medical equipment usually cannot call network services due to its strict privacy protection settings. For local services, the small model can achieve offline inference on the PC side, which is completely Ensure that the data is localized to help improve the efficiency of medical staff; secondly, the original intention of our article is to explore a reproducible technical solution at an affordable computing cost, and promote the exploration of the field and the raising of new questions. Small models are useful for The training is very friendly for academic researchers who lack sufficient computing power.\nTraining models in the medical field usually involves continuing pre-training on the corpus. However, some scholars believe that although training on the original corpus gives the model domain knowledge, it greatly damages its ability to prompt question answers. We consider exploring ways to rewrite the pre-training corpus into the form of question-and-answer pairs to alleviate this problem. At the same time, we use priority sampling"}, {"title": "6 The application of the lite Apollo: Proxy Tuning for Larger Models", "content": "Preliminaries Inspired by Liu et al. , we introduce a lightweight model-agnostic decoding method in medical senarios. We leverage the logits from both pre and post fine-tuned small models to indirectly steer the larger base model's adjustments, thereby eschewing the need for direct parameter fine-tuning. Let $M_{raw}$ denote the smaller pre-trained model, and $M_{tuned}$ denote its fine-tuned counterpart. We compute the logit offset as \"proxy\" for each token, corresponding to the anti-expert and expert roles as delineated in Liu et al. . This offset is then applied to the base model $M_{base}$ to synchronize the predictive distributions of the smaller and larger models. The modified probability distribution is given by:\n$P'_{M_{base}}(x_t | x_{<t}) = \\text{softmax} [l_{M_{base}} + \\Delta l_M] \\propto \\frac{P_{M_{tuned}}(x_t | x_{<t})}{P_{M_{raw}}(x_t | x_{<t})} \\cdot P_{M_{base}}(x_t | x_{<t})$\nwhere $\\Delta l_M = l_{M_{tuned}} - l_{M_{raw}}$ represents the logit offset of the expert model $M_{tuned}$ and the anti-expert pre-trained model $M_{raw}$.The logit output for M at t is denoted by $l_{M_t}$ for the current timestep t. The probability distribution of M refers to $p_{M_{base}}(x_t | x_{<t})$.\nSettings $M_{base}$ is designated as the subject of investigation for Qwen-7B. Apollo-1.8B and Qwen-1.8B are appointed as the $M_{tuned}$ and $M_{raw}$ respectively.\nResults As shown in the Tab. 6, the overall effect of the model improves a lot without changing the parameters after Proxy Tuning. From language perspective, except English, all other languages increase, and French has the most obvious increase. Excitingly, for French and Spanish, the model after Proxy Tuning performs better than both $M_{tuned}$ and $M_{base}$, indicating that new accurate knowledge is generated after Proxy Tuning. We also notice a decline in English proficiency. This may be because there is a gap between the distribution of difference and the probability itself, which leads to over-strengthening of the second option and requires further exploration and optimization."}, {"title": "7 Related Work", "content": "The integration of Large Language Models (LLMs) into the medical domain has sparked both enthusiasm and concern. These models demonstrate a remarkable ability to respond accurately to free-text queries using domain-specific knowledge. For instance, Google's Med-PaLM 2 stands out as the first medical LLM to achieve an 'expert' level on the USMLE2-style questions in the MedQA dataset, boasting an accuracy exceeding 85%. Other significant contributions include HuatuoGPT , and Visual Med-Alpaca. These models have been specifically tailored to address the unique needs of healthcare, enhancing their relevance and applicability.\nNotable early developments in healthcare-focused LLMs include GatorTron , which explores the use of unstructured Electronic Health Records (EHRs) in clinical settings, and Codex-Med , which assesses the efficacy of GPT-3.5 models in healthcare. Galactica targets the challenge of scientific information overload by storing, combining, and reasoning about scientific knowledge, including in healthcare.\nPrivacy and confidentiality concerns have prompted innovative solutions like DeID-GPT , a de-identification framework using GPT-4, and MedAlpaca , which offers an open-source, on-site implementation approach. ChatDoctor , Meditron and PMC-LLaMA address limitations in medical knowledge accuracy of existing LLMs, tuning base models on millions of biomedical papers.\nGatorTronGPT , a clinical LLM with a GPT-3 architecture, and MedAGI , a model aiming to unify domain-specific medical LLMs, illustrate the trend towards more specialized models. Finally, Med-Flamingo represents an advance in vision-language models, handling multimodal data that includes both images and text.\nThere have been some outstanding works focusing on multilingual topics recently. BioMistral introduce the perspective of a multilingual evaluation system for the first time. MMedLM is the first large medical model trained on multilingual corpus. We believe that our work, together with the formers, will bring a multilingual perspective into the medical artificial intelligence community and help more people with Medical AI."}, {"title": "8 Conclusion", "content": "In order to serve more people and larger community, we carefully collect and organize a high-quality medical corpus covering most populous languages in the world, open sourcing multi-language Dataset ApolloCorpora and evaluation set XMedBench. Based on these, we explore suitable methods for multilingual training and interrelationships between languages in the medical field, and finally obtains a series of models named after Apollo, with SOTA performance from 0.5B to 7B. At the same time, the Proxy Tuning method is also used to improve larger models' multilingual medical capabilities without changing the parameters.\nWe propose the question of exploring the relationship between languages in the medical field. While open-sourcing a series of small models with comparable performance, we expand their application scenarios and keep the entire process open-source and reproducible. We provide a foundation for researchers with limited resources to explore questions such as datasets sampling optimization, Proxy Tuning optimization, and different language models combination optimization in the multilingual medical field."}]}