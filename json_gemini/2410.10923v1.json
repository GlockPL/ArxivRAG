{"title": "ATLAS: Adapter-Based Multi-Modal Continual Learning with a Two-Stage Learning Strategy", "authors": ["Hong Li", "Zhiquan Tan", "Xingyu Li", "Weiran Huang"], "abstract": "While vision-and-language models significantly advance in many fields, the challenge of continual learning is unsolved. Parameter-efficient modules like adapters and prompts present a promising way to alleviate catastrophic forgetting. However, existing works usually learn individual adapters for each task, which may result in redundant knowledge among adapters. Moreover, they continue to use the original pre-trained model to initialize the downstream model, leading to negligible changes in the model's generalization compared to the original model. In addition, there is still a lack of research investigating the consequences of integrating a multi-modal model into the updating procedure for both uni-modal and multi-modal tasks and the subsequent impacts it has on downstream tasks. In this paper, we propose an adapter-based two-stage learning paradigm, a multi-modal continual learning scheme that consists of experience-based learning and novel knowledge expansion, which helps the model fully use experience knowledge and compensate for novel knowledge. Extensive experiments demonstrate that our method is proficient for continual learning. It expands the distribution of representation upstream while also minimizing the negative impact of forgetting previous tasks. Additionally, it enhances the generalization capability for downstream tasks. Furthermore, we incorporate both multi-modal and uni-modal tasks into upstream continual learning. We observe that learning from upstream tasks can help with downstream tasks. Our code will be available at: https://github.com/lihong2303/ATLAS.", "sections": [{"title": "1 Introduction", "content": "Continual learning provides a viable approach to enhance a model's capability by sequentially updating it with incoming tasks. In recent years, there has been an increased focus on multi-modal continual learning [6, 23], primarily driven by the rise of multi-modal foundation models. This approach tackles scenarios where incoming tasks can originate from diverse modalities, thereby receiving more and more attention in the research community. The intuition of involving multiple modalities is based on the human learning process, where successfully acquiring knowledge from one modality task significantly enhances proficiency in performing other tasks. However, there is usually a trade-off between acquiring new knowledge and maintaining old knowledge in continual learning, where there will be a (catastrophic) forgetting of older information when learning new tasks. Moreover, continual learning in a multi-modal model is more challenging because it additionally requires the ability to update the model for each individual uni-modal task and may suffer from a mismatch between different modalities [12].\nRecently, parameter-efficient finetuning has shown promise in alleviating catastrophic forgetting by learning a unique module for each task. This suits continual learning in multi-modal models, as multi-modal and uni-modal tasks often have different distributions, and generalizing to one can hurt the performance on others. Recent prompt-based studies [19, 32, 33, 34] address continual learning in class-incremental and domain-incremental settings by learning prompts for each task. Similarly, the adapter-based study [21] learns individual adapter modules for each task. Prompt-based methods and adapter-based methods usually insert small, task-specific modules into the pre-trained models. However, neither method substantially expands the model's existing knowledge when encountering many tasks. Moreover, most research in multi-modal learning tends to explore the integration of different modalities encoders (e.g., text, image) to solve tasks that require understanding and processing multiple modal inputs. Therefore, the impact of employing a multimodal model [9] that jointly handles multimodal inputs for individual unimodal tasks remains relatively less explored in continual learning.\nTherefore, in this paper, we introduce Adapter-based Multi-modal ConTinual Learning with A Two-stage Learning Strategy (ATLAS). ATLAS follows a two-stage learning paradigm: experience-based learning and novel knowledge expansion (Figure 1), utilizing multimodal models that directly process multimodal inputs. The goal of experience-based learning is to learn how to effectively utilize previously-seen task knowledge when encountering new tasks. Based on experience-based learning, the novel knowledge expansion further compensates for the knowledge outside of the previously seen task. This not only ensures that the knowledge is thoroughly explored for each task but also avoids the redundancy of knowledge between tasks sequentially. Our method enhances the diversity of distribution and improves downstream generalization capability during upstream sequential tasks. Moreover, we consider tasks that include both multi-modal and individual uni-modal tasks for both upstream and downstream processes. This helps us analyze the influence of utilizing a multimodal model that jointly processes multimodal inputs for separate unimodal tasks.\nThe main contributions of this work can be summarized as follows: (1) We introduce a two-stage learning paradigm for multi-modal continual learning to enhance the richness of distribution and improve the general-ization capability; (2) We propose a knowledge vector learning method that can combine the knowledge from different adapters based on the cosine similarity between the previous-seen tasks and the new task; (3) We systematically analyze the effect of learning the multi-modal model on individual uni-modal tasks."}, {"title": "2 Related work", "content": null}, {"title": "2.1 Continual Learning", "content": "The methods of continual learning can be mainly classified into three categories: regularization-based [2, 5, 10, 11, 29, 39], rehearsal-based [4, 15, 27, 28, 30, 35], and architecture-based [17, 18, 31, 37]. The regularization-based methods mitigate catastrophic forgetting [14] by stabilizing the modification of previous model parameters when learning new tasks. The regularization in weight space penalizes changes in model parameters, while the prediction space penalizes changes in model predictions. The rehearsal-based methods require a memory buffer to store previous task samples, which are utilized to recover previously encountered distributions and prevent catastrophic forgetting. The architecture-based methods add task-specific architec-tures for each new task and expand the overall capacity of the model, which is significantly important for improving the capability to learn new knowledge.\nIn this paper, we focus on enhancing the richness of model representation and improving its generalization capability using architecture-based methods."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 Continual Learning", "content": "Assume that we are given a multi-modal model Mo, which is pre-trained on an initial task. The primary focus is on task-incremental continual learning, which involves the sequential arrival of tasks. In this setting, the model only has access to the current training data and is not able to access any data from previous tasks. At time t, the model updates its parameters based on the loss function Lt and training data D\u0142. We assume that a multi-modal model undergoes continuous training on a sequence of T distinct upstream tasks.\nAn effective continual learning paradigm should exhibit a minimal amount of forgetting about previously encountered tasks. This means that after training on subsequent tasks, the average accuracy of a previously trained task should remain close to its initial accuracy. In other words, the model should retain its knowledge and performance on earlier tasks even as it learns new ones. Another crucial criterion for a reliable continual learning method is that its generalization ability on downstream tasks (which have no intersections with upstream tasks) should improve as the training process progresses. In other words, the method should exhibit an increasing capability to generalize and perform well on unseen tasks as it continues to learn and accumulate knowledge from (upstream task) learning. This indicates that the model becomes more proficient in applying its learned knowledge to a diverse range of unseen tasks over time.\nRegardless of whether we consider tasks in the upstream or downstream context, our investigation encompasses a diverse set of tasks that involve both multiple modalities and uni-modalities. By doing so, we aim to understand the effects of training a multi-modal model on tasks that rely on individual modalities, while also exploring the potential benefits that arise for both downstream multi-modal tasks and tasks focused on individual modalities."}, {"title": "3.2 Adapters", "content": "The adapter, introduced in the paper by [8], presents an alternative approach for adapting a pre-trained model to a specific task. In contrast to resource-intensive fine-tuning, an adapter module consists of a considerably smaller number of parameters compared to the original model. Despite this reduction, adapter-based models generally achieve performance that is comparable to fine-tuning the entire model.\nIn practical, the parameters of the pre-trained model stay fixed, while the adapter modules are trained for a given task. This enables quick adaptation to new tasks without the need for extensive retraining of the entire model.\nIn this work, the initialized model for upstream continual learning is a pre-trained vision-language model Mo. For each task at time t and its corresponding dataset Dt, the model Mt is initialized from the previous upstream updated step checkpoint Mt-1. We continuously learn a task-specific adapter module Pt for each new task based on all previous task modules as:\n$\\Phi_{t} = arg min L_{t} (\\Phi' | D_{t}; M_{0}, \\Phi_{0}, ..., \\Phi_{t-1}), $ (1)\nwhere Lt refers to the loss function used for the t-th task and Po = 0."}, {"title": "4 Method", "content": "While using adapters has shown a promising way to alleviate catastrophic forgetting, the existing works lack the investigation of enhancing the generalization ability of the foundation model for downstream tasks. Specifically, the adapter-based method [21] usually trains an individual adapter module for each task during upstream training. However, the original model is still used for initialization when seeing new tasks, and the upstream learned knowledge is not involved in the new incoming task. One potential way is to leverage the existing knowledge in the upstream learned task adapters and the original pre-trained model for the incoming task. Intuitively, to tackle a novel task, the model needs to leverage prior knowledge from the initialized checkpoint and assimilate new knowledge specific to the current task. Additionally, we desire an approach that constantly improves the modal capability, which we conjecture is helpful for the generalization capability for adapting to new tasks.\nWe propose a two-stage learning paradigm for multi-modal continual learning that utilizes adapters to expand knowledge, enabling the model to continuously enhance its generalization ability for new tasks. Rather than training a separate adapter for each task, we introduce an adapter weighting method that incorporates the knowledge of different adapters to tackle new tasks. This allows us to enhance the capability of the foundation model and capture different task-specific knowledge, which can be beneficial for new incoming tasks."}, {"title": "4.1 Two-Stage Learning Paradigm", "content": "Crucial to expanding knowledge is utilizing the knowledge involved in the previous task adapters. Empirical studies [26] propose that various knowledge sources, representations, and integration methods can boost the learning of new knowledge. This is intuitive for advanced intelligent beings like humans, who naturally associate previously learned knowledge when learning new knowledge. However, in the current process of continual learning for the multi-modal model, the new downstream task still relies on the original pre-trained model for initialization, and the knowledge from the previous task is not incorporated into the learning process for the new task. They independently learn a new architecture for the incoming task, resulting in redundant knowledge among task-specific architectures and limiting the reserve of knowledge for new tasks. This leads to an urgent need for methods that effectively utilize experience-based learning for incoming tasks.\nTo better illustrate the working process of the paradigm, we will first briefly present the schematic two-stage learning paradigm consisting of experience-based learning and novel knowledge expansion as follows, detailed implementations will be discussed in section 4.1.1, 4.1.2, 4.1.3, and 4.1.4. Figure 2 presents the illustration of the proposed two-stage learning paradigm. Experience-based learning hopes to fully use the previous comprehensive knowledge to tackle new tasks. Such knowledge may include original pre-trained model knowledge Mo as well as task-specific knowledge \u03a6\u2081 gained from previously observed upstream tasks. Moreover, we introduce the knowledge coefficient vector to control the degree of participation of different knowledge in dealing with new tasks. For the upstream t-th task (t > 1), we optimize the knowledge coefficient within the framework of experience-based learning as follows:\n$\\bar{\\alpha}_{-t} = arg min L_{t} (\\bar{\\alpha}_{-t} | D_{t}; M_{0}, \\Phi_{1}, \\dots , \\Phi_{t-1}),$ (2)\n$\\bar{\\alpha}_{-t}$"}, {"title": "4.1.1 Knowledge Vector Learning", "content": "Based on a set of learned task-specific adapters, we desire to produce the knowledge coefficient vector \u03b1 via correlating the sample-level latent representation with the corresponding key of the adapter. The underlying intuition is that the involvement degree of each adapter to a new task is determined by the similarity between the sample-level latent representation h(x) and corresponding indicator ki for the i-th adapter. More importantly, the different adapter keys need to connect with the different aspects of latent representation, which indicates the demand for each adapter to be involved in the current task. However, the network's latent representation for similarity computation requires a key with the same dimension, which is more computationally expensive. Additionally, it is challenging to focus on the knowledge corresponding to the adapter.\nTo solve the above challenges, we propose to compute a sample-level description vector di(xt) for t-th task input xt with respect to the i-th adapter, which captures the different components of the latent representa-tion. It boosts our method's computation efficiency and enhances the accuracy of making use of different adapter modules. Then, for each adapter \u03a6\u00bf, we assign a learnable reference vector ki \u2208 RD as its abstract description.\nDrawing inspiration from the query-based feature extraction method [1], we introduce a learnable query to capture the sample-level description corresponding to each adapter in the latent representation of the network. For any sample xt \u2208 Dt, we compute cross-attention between the latent representation h(xt) within the t-th task input \u00e6t and a learnable query qi \u2208 RD that corresponds to each adapter to produce the sample-level description as follows:\n$d_{i}(x_{t}) = CrossAtt(h(x_{t}), q_{i}),$ (4)\nwhere CrossAtt refers to the computation of cross-attention. di(xt) \u2208 RD is the sample-level description for i-th task input corresponding to adapter \u03a6t. Notice that our cross-attention does not involve the proj\u0435\u0441-tion operation and only utilizes the multiplying operation, which helps the multi-modal model to alleviate catastrophic forgetting.\nThen we calculate the cosine similarity between the description vector di(xt) and the reference vector ki as:\n$\\alpha_{i}^{t} = (d_{i}(x_{t}), k_{i}),$ (5)\nwhere \u03b1i is the weighting coefficient for the i-th adapter in the t-th task. This captures the overlapping of the task input \u00e6t and the adapter \u03a6\u2081. We use \u03b1 as the weight of the output of \u03a6\u00bf for the t-th task adapter weighting. The loss on the whole dataset Dt is just the average loss of all the samples."}, {"title": "4.1.2 Stage1: Experience-Based Learning", "content": "Experience-based learning solves new tasks by decomposing task knowledge into previously saved knowledge. In our work, we adopt the adapter module \u03a6\u2081 as the basic structure for knowledge increment in our two-stage learning paradigm. Specifically, for the upstream t-th task, we attain adapter-based knowledge through insert a weighted summation and form a decomposed adapter into the training to effectively utilize the knowledge of the previous task adapter: \n$\\Phi_{stage 1} = \\sum_{i=1}^{t-1} \\alpha_{i}^{t}\\Phi_{i}, where \\Phi_{i}$ is the output of the i-th adapter module. The \u03b1 is the weighting coefficient for the i-th adapter that determines the degree of each adapter involved in the current tasks, see Figure 2 for better understanding.\nThus, the optimization problem for experience-based learning can be described as follows:\nq-t, K\u2212t = arg min Lt (q'-t, K'-t | Dt; M0, \u04241, ..., \u03a6t\u22121) (6)\nqtKt\nwhere q\u2212t = [q\u0142, . . ., q\u2021\u00af\u00b9] is the learned collection of task-specific queries for previously seen t 1 tasks and K-t = [k1, ..., kt\u22121] is the learned set of adapter description vectors."}, {"title": "4.1.3 Stage2: Novel Knowledge Expansion", "content": "With experience-based learning as a foundation, we are ready to expand new knowledge that has not been covered in the previous adapters. This ensures that we can fully learn the knowledge carried by new tasks and continuously expand the non-redundant knowledge of the foundation model. The model is initialized using experience-based generalization learning. This includes the original pre-training model Mo, a collection of previous task adapters \u03a61, ..., \u03a6t\u22121 and their corresponding reference vector k1, ..., kt\u22121, as well as queries q,..., q\u00b9 specific to t-th upstream task. Then we introduce a set of learnable parameters: adapter \u03a6t and its corresponding adapter reference vector kt, along with a query qt to compensate for new knowledge.\nSimilar to the experienced-based learning stage, we use \n$\\Phi_{stage 2} = \\sum_{i=1}^{t} \\alpha_{i}^{t}\\Phi_{i}$ in the training of the novel knowledge expansion, then the novel knowledge expansion is an optimization problem as follows:\n\u0424t, qt, kt = arg min Lt (\u00de', qt, k\u2081 | Dt; M0, \u03a61, . . ., \u0424t\u22121, q\u2212t, K\u2212t).\n\u03a6\u03b1\u03ba\nThe performance of the above novel knowledge expansion reflects how much the multi-modal model attains knowledge for the current task. Additionally, the compensated new task knowledge may be useful for future tasks."}, {"title": "4.1.4 Orthogonality & Optimization", "content": "The two-stage learning paradigm is focused on improving the richness of distribution and enhancing gen-eralization capability. For the t-th task, its task-specific query qt = [q],\u2026\u2026,q\u2021] and adapter-specific key Kt = [k1,..., kt] are used to determine the weighting value. The empirical study [19] proposes that orthogo-nal vectors have less interference between existing and new knowledge. In our work, we add the orthogonal constraint to qt and Kt. Specifically, we introduce the orthonormal regularization as:\n$L_{ortho}(X) = ||X^{T}X \u2013 I||_{F},$\nwhere || || F represents Frobenius norm and X is qt or Kt.\nCombined with task-specific loss Lt, the total loss is defined as:\n$L_{t} + \\lambda (L_{ortho} (q_{t}) + L_{ortho} (K_{t})),$ (7)\nwhere \u03bb is a hyper-parameter to control the degree of orthogonal constraint."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experimental Setting", "content": "In this paper, we apply our adapter-based two-stage learning paradigm approach to task-incremental multi-modal model continual learning that covers upstream continual learning and downstream generalization,"}, {"title": "5.2 Upstream Continual Learning", "content": "In this subsection, we focus on the training process of upstream continual learning and demonstrate that our two-stage learning paradigm is an effective continual learner without forgetting while having powerful upstream experience-based learning capability. In upstream continual learning, we use a fixed upstream task order: Multi-First (SNLI-VE\u2192PIQA\u2192iNaturalist2019\u2192VQAv2\u2192SST-2).\nIn our experiments, we use the trained parameters of the task-specific query for additional adapter modules, this allows us to analyze the impact of the current task update on subsequent tasks. Notably, CLIMB [21] can be seen as a special case of our method by setting the task-specific query of additional adapter to zero vector, which produces that the corresponding adapter coefficient is zero. The Forgetting displayed in our results is similar to prior work like CLIMB [21]. See Supplementary materials for more detailed discussions.\nFigure 3 presents the results of the training process during upstream continual learning. Following the fixed task order, we estimate the experience-based learning performance on unseen tasks while estimating forgetting on tasks that have been previously learned. Experience-based learning optimizes additional task-specific parameters compared to linear probing updates, which is negligible given that each task-specific parameter for one adapter is 18.4k compared to 3.1M adapter parameters. Novel knowledge expansion learns task-specific adapters based on experience-based learning in the current task. We only learn the task-specific query for the newly added adapters when estimating forgetting. To better estimate the performance of our algorithm, we introduce the linear probing as our lower bound and the fine-tuning update as our upper bound.\nOur approach is an effective continuous learner, achieving performance close to fine-tuning across the entire dataset and demonstrating a strong ability to mitigate catastrophic forgetting. Adding the compensation knowledge of tasks after the current task does not improve performance, except for a subtle improvement in the iNaturalist2019 dataset. It is intuitive that our two-stage learning paradigm fully explores the knowledge in each task. Experience-based learning decomposes the new task into knowledge from previous tasks, while novel knowledge expansion compensates for the knowledge beyond previously saved knowledge. The improvement of experience-based learning over linear probing reflects the additional task knowledge that assists in new task learning."}, {"title": "5.3 Downstream Generalization", "content": "The capability of downstream generalization reflects the richness of distribution and the adaptability of a new task. In the following, we access the downstream learning capability based on experience-based learning and the low-shot capability in each step of upstream continual learning.\nTable 1 shows that our method significantly improves the capability of downstream experience-based learning compared to prior works. It also maintains a constantly increasing trend as knowledge constantly expands. For prior work CLIMB [21], the experience-based learning capability of the model requests the use of the full individual adapter. If there is no relevance between the upstream task and the downstream task, the generalization ability of the downstream model will greatly decrease. For instance, the upstream iNaturalist image dataset adapter-based model adapts to the downstream natural language CommonsenseQA dataset, with accuracy decreasing from 28.75 to 25.92. Opposed to CLIMB, our method can learn the effective combination of different adapter knowledge. If a downstream task is unrelated to one of the upstream tasks, our method can learn a knowledge coefficient close to zero for this adapter. For example, the downstream VCR dataset is unrelated to the upstream PIQA dataset, so the accuracy remains unchanged when adding PIQA adapter knowledge.\nTable 2 and 3 present the result of downstream low-shot capability. With low-shot samples, downstream experience-based learning also significantly enhances the model's performance, and as knowledge expands, the model's performance continues to increase consistently. It is noticed that with low-shot samples, the adapter-learning performance is lower than linear probing in some cases, i.e., NLVR2. One possible reason is that adapter learning overfits low-shot samples since adapter learning has more parameters than linear probing. This reflects that low-shot samples contain less task knowledge. Experience-based learning decomposes the new task knowledge over previously saved knowledge, improving the adaptation of the pre-training models to new tasks. Combining our two-stage learning paradigm, downstream adapter-based novel knowledge expansion has enhanced performance compared to task-specific adapter learning. This indicates that our method is data-efficient with low-shot samples as our method ensures non-overlapping among different task knowledge, which can release more capability to learn new task knowledge.\nWe next show the impact of expanding multi-modal and uni-modal task knowledge on downstream tasks. Figure 4 compares the accuracy and knowledge coefficient at each step of upstream continual learning on the VCR and Places365 datasets, respectively. We discovered that empirical-based learning effectively utilizes adapter knowledge according to the similarity between new tasks and adapters. In addition, we observe that the expansion of knowledge over multi-modal tasks typically enhances performance in both uni-modal and multi-modal tasks. However, the expansion of knowledge from uni-modal tasks offers little assistance for multi-modal tasks. Intuitively, language task knowledge does not always translate to improvements in image tasks. Our experiments have shown that in certain cases, knowledge from uni-modal tasks can benefit uni-modal tasks involving different modalities. For example, knowledge of PIQA language tasks enhances performance on Places365 tasks. We suspect that knowledge from a single-modal adapter contributes to the utilization of multi-modal models for single-modal tasks. Furthermore, we have observed that the utilization of specific task knowledge is progressively diminishing as the saved knowledge continues to grow. We attribute this phenomenon to the presence of low-level competition in the integration of diverse tasks."}, {"title": "6 Conclusion", "content": "We propose a two-stage learning strategy for multi-modal continual learning: experience-based learning and novel knowledge expansion. Experience-based learning leverages old knowledge to decompose the task and fully utilize it. Novel knowledge expansion compensates for knowledge that goes beyond the saved knowledge. Our method continuously enhances the distribution of the updated model and improves experience-based learning and low-shot capability in downstream tasks. Analyzing different modalities enhances understanding of multi-modal continual learning. Future work includes applying our method to backbones like CLIP [16] and exploring class-incremental learning settings [19, 33, 34]."}, {"title": "A Implementation Details", "content": null}, {"title": "A.1 Two-Stage Learning Paradigm Algorithm", "content": "Algorithm 1 shows the details of our Two-Stage Learning Paradigm, including Experience-Based Learning and Novel Knowledge Expansion. For each time step t, Experienced-Based Learning aims to discover new task knowledge in already completed tasks knowledge. While Novel Knowledge Expansion compensates for the extra knowledge of a new task beyond the old task's knowledge."}, {"title": "A.2 Evaluation of Forgetting", "content": "In our experiments, for each task when we continue to expand the knowledge for subsequent tasks, there will exist some extra adapter modules after the novel expansion knowledge. We can discard the extra adapter modules for each task, which is the same as CLiMB [21]. In this case, the forgetting is zero because we do not use the knowledge of the additional adapter after the corresponding step. To implement this in our method, we can simply set the task-specific query of the additional adapters to zero vector, which produces the knowledge coefficient for the corresponding adapter as zero. However, this implementation comes with a price, how do the subsequent adapters continue to improve the performance of the current task? To overcome this, we also investigate another solution by continue training the task-specific query in the upstream t-th step model on old task k (k \u2208 {1, . . ., t \u2212 1}), which will create a testbed for evaluating the influence of the additional adapter. The optimization is as follows:\n$q_{k} = arg min L_{k} (q_{k}' | D_{k}; M_{0}, \\Phi_{1},..., \\Phi_{t}).$ (8)\nq_{k}"}, {"title": "B Ablation Study", "content": null}, {"title": "B.1 Ablation Study for Different Task Order", "content": "We conducted experiments with different upstream task orders, i.e., Uni-First (PIQA\u2192SNLI-VE\u2192VQAv2\u2192iNaturalist2019\u2192SST-2), to validate that the effectiveness of our method is unrelated to the task order. Table 4 shows the comparison of accuracy and forgetting with different task orders. We discov-ered that the performance achieved after each task's novel knowledge expansion is quite similar. Different sequences contain different amounts of adapter knowledge in experience-based learning, so there are some subtle differences in performance. For instance, Uni-First has added the SNLI-VE adapter knowledge than Multi-First, and accordingly, the performance has somewhat improved. In addition, the Forgetting results show that for different task sequences, our method is a continuous learner without forgetting.\nWe conduct experiments to ablate the computation of knowledge coefficients, testing various combinations of adapter-level weighting and token-level weighting. We find that the performance of adapter-level weighting is the same as that of token-level weighting, while adapter-level weighting has fewer learnable parameters. Therefore, we use adapter-level weighting in our methods."}, {"title": "B.2 Ablation for Knowledge Vector Learning", "content": "We devise a knowledge vector learning method that incorporates the knowledge from different adapters, where each adapter's output contains a set of tokens with the same dimensions. To explore effective methods for fusing different adapter knowledge, we conducted experiments with different linear combination methods, including adapter-level fusion and token-level fusion. The adapter-level fusion uses one knowledge coefficient value for all tokens in an adapter, while each token has a specific coefficient value at the token-level fusion.\nTable 5 shows the results of the comparison of different linear combination methods. We observe that adapter-level fusion consistently outperforms token-level fusion in all cases. We speculate that there is consistency among tokens in the adapter output, and using a global knowledge coefficient for all tokens usually produces decent results. In addition, the token-level fusion introduces more learnable parameters for learning the knowledge coefficient. Specifically, an adapter that uses token-level fusion requires 441.6k learnable parameters for the knowledge coefficient, while the adapter itself has a parameter size of 883.2k. This makes the token-level fusion is less parameter-efficient. However, at the adapter-level fusion, only 18.4k parameters are involved in learning knowledge coefficients, which is negligible compared to the 883.2k parameters required for an adapter. Moreover, involving more parameters can make optimization difficult, and it becomes challenging to learn the optimal knowledge coefficients.\nIn our experiments, we utilize adapter-level fusion to calculate the knowledge coefficient, which is not only parameter-efficient but also effectively combines different adapter knowledge."}, {"title": "C Additional Results", "content": null}, {"title": "C.1 Upstream Continual Learning with Uni-First Upstream Task Order", "content": "Fig. 5 shows that the process of upstream continual learning with various task sequential, Uni-First (PIQA\u2192SNLI-VE\u2192VQAv2 iNaturalist2019\u2192SST-2), which involves training uni-modal tasks first and immediately following multi-modal tasks. Before the stage of novel knowledge expansion in each task, we access the ability to learn from experience-based learning and evaluate forgetting afterward. Similarly, we introduce Linear probing and Fine-tuning as references, representing our lower and upper bounds."}, {"title": "C.2 Additional Adapter Weighting Analysis", "content": "To further explore the influence of expanding multi-modal and uni-modal task knowledge on downstream tasks, we calculate the knowledge coefficient of the adapter during downstream experience-based learning on the VCR and CommonsenseQA datasets. Fig. 6 is the comparison of accuracy and knowledge coefficient at each step of upstream continual learning on VCR and CommonsenseQA datasets. We find that the performance improvement on each task is attributed to the utilization of novel incremental knowledge. For instance, the accuracy of the NLVR2 dataset increased by 0.25 from t = 2 to t = 3 as a result of incorporating knowledge from the VQAv2 dataset. For multi-modal tasks, the expansion of uni-modal tasks has little effect on performance improvement and even has a negative impact in some cases, while the expansion of multi-modal task knowledge usually improves performance. For uni-modal tasks, knowledge expansion from uni-modal tasks with the same modality or multi-modal tasks usually improves performance, while uni-modal tasks with different modalities often do not improve or have a negative impact that may be attributed to competition between tasks. In addition, we have observed that as novel knowledge constantly expands, the coefficient of previous knowledge decreases, which means that the decomposition of new tasks over previously saved knowledge decreases when expanding novel knowledge. We speculate that this may be attributed to the misalignment and competition among different task knowledge, and we will study this in our future work."}]}