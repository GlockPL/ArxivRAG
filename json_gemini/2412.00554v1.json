{"title": "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective", "authors": ["Yue Zhou", "Barbara Di Eugenio", "Lu Cheng"], "abstract": "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs in healthcare fairness and the urgent need for specialized research in this area. \\WARNING: This paper contains model outputs that may be considered offensive in nature.", "sections": [{"title": "1 Introduction", "content": "The application of Artificial Intelligence (AI) in healthcare is almost as old as AI itself\u00b9. Over the years, the penetration of AI techniques in healthcare has increased, from early expert systems like MYCIN (Shortliffe, 1976) to NLP techniques applied to clinical notes (Friedman et al., 1999) to the current proliferation of applications of Large Language Models (LLMs) (He et al., 2024). The assumption is that LLMs will be equally successful in healthcare as they have been in other domains (Srivastava et al., 2023; Rae et al., 2022; Liang et al., 2023), especially given emerging learning frameworks such as chain-of-thought, parameter-efficient fine-tuning, and LLM as autonomous agents to address in-context reasoning, data scarcity and factual knowledge (Wei et al., 2022; Kojima et al., 2023; Zhou et al., 2024c; Yao et al., 2023; Shinn et al., 2023; Wang et al., 2024b).\nHealthcare applications present unique challenges due to the complexity of knowledge involved, limited data resources, and inherent ethical considerations, including how to mitigate health disparities and achieve health equity (Pereira, 1993; LaVeist, 2005; Waters, 2000; Braveman, 2006; Lane et al., 2017; Ndugga and Artiga, 2021). While recent studies have begun exploring LLMs in medical QA, bio-medicine understanding, and disease diagnosis (Singhal et al., 2023; Tian et al., 2023; Zhou et al., 2024a; He et al., 2024; Wang et al., 2024a), there remains a significant gap in comprehensive evaluations of LLM performance on real-world healthcare tasks, particularly as concerns their potential to reinforce health disparities, a crucial consideration given LLM known biases and their potential impact on patient care (Schick et al., 2021; Weidinger et al., 2021; Sun et al., 2024; Gallegos et al., 2023).\nTo address this gap and provide insights into best practices for utilizing LLMs on low-resource healthcare tasks, we present a comprehensive study examining the performance of LLMs across diverse healthcare benchmarks. Concretely, we formulate six benchmarks, including mortality, readmission, health coaching outcome prediction, and mental health diagnosis. We evaluate three state-of-the-art LLMs, GPT-4 (OpenAI, 2023), Claude-3 (Anthropic, 2024), and LLaMA-3 (AI@Meta, 2024), with prevalent frameworks: in-context learning with chain-of-thought reasoning (Wei et al., 2022; Kojima et al., 2023), parameter-efficient fine-tuning (Hu et al., 2021; Dettmers et al., 2023), and LLM-as-agent leveraging external factual knowledge. We employ two standard fairness metrics, Demographic Parity Difference (DPD) and Equal Opportunity Difference (EOD) (Zemel et al., 2013; Wang et al., 2023;"}, {"title": "2 Related Work", "content": "LLMs in Healthcare. This domain has recently seen a surge in the application of LLMs. Google proposed PalmMed2 (Singhal et al., 2023), an LLM in the medical domain. Zhou et al. (2024a); Clusmann et al. (2023); Tian et al. (2023) discuss the current applications and future landscape of LLMs in medicine. He et al. (2024) offers a review of healthcare data and applications with LLMs. Wang et al. (2024a) explore utilizing LLMs for rare case diagnosis. Hu et al. (2024); Monajatipoor et al. (2024) study named entity recognition with LLMs in clinical and biomedicine settings. However, limited work exists on demographic fairness in LLMs across multiple healthcare applications.\nLLMs and Fairness. Large language models have demonstrated considerable in-context learning abilities (Wei et al., 2022; Kojima et al., 2023) and parameter-efficient fine-tuning possibilities such as Low-Rank Adaptation (Hu et al., 2021; Dettmers et al., 2023). Recently, the use of LLMs as autonomous agents equipped with tool usage capabilities shows promising results (Yao et al., 2023; Shinn et al., 2023; Wang et al., 2024c). Nonetheless, LLMs can exhibit limitations on generating unbiased and faithful output, with performance deterioration among underrepresented"}, {"title": "3 Datasets and Task Formulation", "content": "To facilitate evaluating LLM performance in healthcare, especially in demographic fairness, we formulate six tasks based on four healthcare datasets containing demographic information, such as age, gender, and ethnicity. We point out that public healthcare datasets with demographic information are scarce due to potential ethical and privacy concerns. We employ MEDQA (publicly available), MIMIC, which is available upon request, and two others that are either partially available (a subset of the Health Coaching Dataset is available, but not the demographic information) or not available (Bipolar Disorder and Schizophrenia Interviews). Our results on the publicly available ones should attest to the generalizability of our approach.\nMIMIC-IV (Johnson et al., 2020) is a large, publicly available healthcare dataset containing hospital health records. Taking the patients' clinical notes as input, we formulate two tasks: one-year mortality prediction and 90-day readmission prediction, which emulate patient outcome prediction in real-world settings. The input note and label pairs are created by joining three tables in the MIMIC database: 'patients,' \u2018admissions,' and 'discharge.' The note mainly contains sections including chief complaint, history of present illness, past medical history, and lab results. We removed the discharge instruction from the note in the input to our models since they can reveal direct information on mortality/readmission (e.g., Hospice).\nHealth Coaching Datasets Dialogues are inherently unique in fairness evaluation since they often contain implicit demographic cues. This raises questions about how LLMs handle these subtleties and whether their responses might exhibit unfairness. The health coaching datasets (Gupta et al., 2020; Zhou et al., 2024b) comprise SMS conversations between patients and certified health coaches over several weeks, focusing on creating and accomplishing S.M.A.R.T. goals to promote health behavior changes (Doran, 1981). Each week, the conversation starts with a goal setting stage, where"}, {"title": "4 Baselines", "content": "Our study aims to evaluate the performance of trendy frameworks utilizing large language models in real-world, low-resource healthcare settings. We seek to provide insights into best practices for leveraging LLMs when building applications under these constraints. To this end, we evaluate the performance of LLMs using three representative frameworks:\n\u2022 In-Context Learning (ICL) with Chain-of-Thought enhances LLM inherent reasoning capabilities by prompting the LLM to provide a step-by-step reasoning chain (Wei et al., 2022; Kojima et al., 2023; Zhou et al., 2024c). We implement two schemes: (1) Zero-Shot Chain-of-Thought (CoT), which appends \u201cLet's think step by step.\u201d to the question text; and (2) N-Shot CoT, where we append four to eight-shot in-context examples with CoT to the LLM when solving the problems. The examples are demographically balanced fol-"}, {"title": "5 Metrics for Fairness", "content": "Existing literature predominately adopts two metrics to evaluate the demographic fairness of the model prediction (Zemel et al., 2013; Wang et al., 2023; Liu et al., 2023). The first metric is called Statistical Parity or Demographic Parity. Statistical parity is achieved when favorable decision outcomes are unrelated to the protected attributes. The rationale is to test whether the model treats various subgroups similarly. Take fraud detection as an example; the model should output \"good credit\" with a similar chance for both males and females. Note that it does not consider the ground truth label. Consider the sensitive/demographic attribute Z and the predicted outcome \u00dd, the (one-vs-all) Demographic Parity Difference (DPD) for subgroup zi can be defined as:\n$PDP = P(\u0176 = 1|Z = z\u2081) \u2013 P(\u0176 = 1|Z \u2260 zi)$\nThis metric may pose challenges when assessing model performance in healthcare applications, as the attribute Z could be a prior factor influencing model predictions. For instance, when predicting a patient's one-year mortality, age may significantly influence risk, with individuals above the age of 90 facing greater risk compared to those below. Consequently, an LLM which obtained such knowledge during pre-training may be more likely to predict mortality for patients above 90 years old. Nonetheless, we include this metric as it provides valuable insights into the model's prediction tendencies across different demographic groups in healthcare contexts and is crucial for understanding potential biases.\nThe second metric, Equality of Opportunity, evaluates model fairness based on the ground truth labels. It indicates that different subgroups should have an equal likelihood of being accurately classified by the model. One way to formulate the metric is to measure the true positive rates of class Y across various subgroups. We report the Equal Opportunity Difference (EOD) as:\n$EOD = P(\u0176 = 1|Y = 1, Z = zi)\n\u2013 P(\u0176 = 1|Y = 1, Z \u2260 zi)$\nNote the definition of favorable attributes in healthcare is more nuanced than in other domains like fraud detection or tweet classification. While \"good credit\u201d or \u201cnon-toxic\" are straightforward favorable attributes in those fields, healthcare scenarios often have context-dependent favorable classes. However, for clarity and consistency in our main experiments, we define favorable attributes by any positive health indicators across different tasks. These include, for example, Low Mortality Risk out of {Low Mortality Risk, High Mortality Risk} in mortality prediction"}, {"title": "6 Experiments", "content": "In this section, we describe our experiment results evaluating the effectiveness of LLMs in solving real-world healthcare tasks with various frameworks and settings, as well as additional discussions on demographic awareness and qualitative examples.\n6.1 Experimental Settings\nLanguage Models We utilize three state-of-the-art large language models for evaluation, including two closed-source models, OpenAI GPT-4 (OpenAI, 2023) and Claude-3 (Sonnet) (Anthropic, 2024), and one open-source model, LLaMA-3 (8b) (AI@Meta, 2024). In compliance with the responsible use guidelines for MIMIC data with online services, we utilize the Azure OpenAI service for GPT-4 and opt out of human data review.\u00b2 We have also ensured that our usage of Claude-3 adheres to the agreement.\u00b3 The LLaMA-3 model is run locally on our machines. The Schizophrenia and Bipolar dataset is the only dataset in our study that is not publicly available and requires approval from an Institutional Review Board (IRB).\nImplementation Details For fine-tuning, we employed LoRA with a rank of 8 across all trainable layers. We use a dropout rate of 0.1, a learning rate 1e-5, and a batch size of 8 for all experiments. Our implementation adheres to the recommendations outlined in QLORA (Dettmers et al., 2023), except for the LoRA scaling factor (Alpha), which is set equal to the LoRA rank. We choose the temperature T = 0.3 for all three language models for inference. The full implementation details and prompt templates used in the experiments are available in Appendix A.\n6.2 Main Results\nTable 2 shows the accuracy results for six healthcare tasks using different LLM frameworks. The"}, {"title": "6.3 Additional Studies", "content": "Demographic Awareness and Biases An intriguing question arises regarding the risk that LLMs infer irrelevant or wrong demographic information from conversational data and hence, that they introduce bias into health outcome predictions. To explore this, we ask three LLMs to identify the patient's race, given the conversation. Table 6 shows that GPT-4 achieved the highest accuracy in predicting race information, followed by LLaMA-3, outperforming random guessing. Notably, Claude-3 refused to make race predictions, stating, \u201cI would prefer not to speculate about the participant's race or ethnicity.\u201d GPT-4 exhibited similar reluctance for the health coaching"}, {"title": "7 Conclusions and Future Work", "content": "We explored LLM performance and demographic fairness across diverse healthcare tasks. Our experimental results highlight LLM difficulties in solving real-world healthcare tasks and significant disparities across demographic groups. We showed that explicitly providing demographic information to LLMs does not guarantee improved performance or fairness. While LLM-as-agent can retrieve factual knowledge, the reasoning processes may still lead to inaccurate conclusions. We also observed potential biases in race inference, which could influence health outcome predictions. These findings highlight the urgent need for future research addressing LLM fairness and reliability in healthcare."}, {"title": "Limitations", "content": "While our study provides valuable insights into the performance disparities of LLMs across demographic groups in healthcare tasks, we still need to identify a systematic strategy to mitigate biases and improve fairness across different healthcare scenarios. Additionally, investigating the relationship between LLM demographic awareness and the fairness of health outcome predictions could yield noteworthy insights, particularly given our observations of biases in demographic inference and demographic parity differences."}, {"title": "Ethics Statement", "content": "This study examined LLM performance across diverse healthcare tasks, revealing disparities in accuracy and fairness among demographic groups. Our findings highlight the potential risks of propagating LLM unfairness and the need for rigorous fairness assessments and bias mitigation in healthcare. Future work should focus on developing LLMs for equitable outcomes in healthcare."}, {"title": "A Prompt Templates", "content": "We list the prompt templates used in the paper below.\nQuestion Formulation Prompt\nMortality / Readmission: Assess the 1-year mortality risk (low or high) / 90-day readmission chance for the patient based on the given de-identified clinical note. Using medical evidence only. If uncertain, provide your best evidence-based estimate. [Note]\nHealth Coaching: Given the following conversation between the patient and the health coach, predict the patient's outcome regarding goal achievement in the following week. Will the patient be most likely to complete the goal in the following week based on the conversation history? [Conversation]\nMedQA: Answer the following USMLE question with medical evidence only. No assumptions. If unsure, give your best evidence-based guess. [question] [options]\nNeighbor Scene: The following is a transcribed conversation from an audio recording between a participant and an interviewer, who is a trained psychologist. [scene]: In this scenario, the participant imagines they have just moved into a new neighborhood and must introduce themselves to a new neighbor. The purpose of this exercise is to gather information about the participant's mental status through their linguistic cues. The participant belongs to one of three groups: individuals with schizophrenia, those with bipolar disorder, or healthy controls. Based on the dialogue, which group is the participant most likely to belong to? [Conversation]\nLandlord Scene: Same as above except for [scene]: In this confrontational scenario, the participant imagines having a leaky pipe in their apartment that has not been fixed for a while, and they need to complain to their landlord and get it fixed."}, {"title": "B Additional Results", "content": "We show more fairness discrepancy across demographic subgroups in mortality prediction in Figure 3. The Demographic Parity Difference (DPD) and Equal Opportunity Difference (EOD) are calculated using a one-vs-all approach. Both GPT-4 and LLaMA-3 exhibit similar bias patterns: they are more likely to predict high mortality risks for the geriatric age group and African Americans. Additionally, these models demonstrate lower prediction performance (True Positive Rate) for these groups. These findings highlight the persistent challenges in achieving LLM fairness across different demographic subgroups in healthcare settings."}, {"title": "C Sociolinguistic Consultation", "content": "We consulted with a sociolinguist regarding language model outputs that attempt to infer demographic characteristics from conversational patterns for diagnosis. The consultation revealed significant concerns about linguistic stereotyping in current LLMs.\nMisattribution of Common Linguistic Features Features like \"ain't\" and \"gonna,\" which LLMs often flag as African American Vernacular English (AAVE), are prevalent across multiple dialects. The expert notes that while \"ain't\" showed some demographic correlation in specific contexts (e.g., Oak Park school study, Chicago area), it is not unique to AAVE. Similarly, \u201cgonna\u201d is a common informal contraction across all English dialects.\nProblematic Behavioral Assumptions The models demonstrate concerning biases in attributing emotional expressions (e.g., frustration, anger) to cultural norms of specific demographic groups. The expert emphasized that such reactions are universal human responses to situations like unresolved maintenance issues, not characteristics of any particular group.\nMisinterpretation of Speech Patterns The models incorrectly classify common features of verbal communication (e.g., short sentences, informal tone) as dialect-specific markers. However, these are typical characteristics of spoken language across all demographics.\nUnfounded Assumptions About Language Sophistication The models exhibit bias in equating informal language with a lack of sophistication, particularly problematic when associating this with specific demographic groups. As referenced by the expert, this misconception has been thoroughly addressed in seminal sociolinguistic works (?)."}]}