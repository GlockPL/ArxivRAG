{"title": "LLM-AGENT-UMF: LLM-BASED AGENT UNIFIED MODELING FRAMEWORK FOR SEAMLESS INTEGRATION OF MULTI ACTIVE/PASSIVE CORE-AGENTS", "authors": ["Amine B. Hassouna", "Hana Chaari", "Ines Belhaj"], "abstract": "The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity. Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent. This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives. Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security\u2014the latter often neglected in previous works. Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types. Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents. For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects. Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems. This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) excel in tasks like language modeling, text generation, question answering, sentiment analysis, natural language understanding, and commonsense reasoning [1]. However, standalone LLMs lacks other skills such as accessing external knowledge, information retrieval, mathematical reasoning, code evaluation, and numerous others. These functional shortcomings can be managed by AI agents by leveraging external tools and human feedback. An autonomous agent is a system interacting with an environment, sensing it, and acting on it over time following a certain agenda [2]. There are different classes of agents but in this paper, we will be focusing on LLM-based agents which, by combining the capabilities of LLMs and autonomous agents, can achieve a broad range of tasks [1]."}, {"title": "2 Related Work", "content": "We will start off this section by providing a comprehensive overview of the foundational concepts upon which our work is based. First, Tool-augmented LLMs are a major advancement in NLP that combine the language understanding and generation capabilities of LLMs with the ability to interface with external tools and APIs. For instance, TALM [5] introduces models which can leverage a wide range of functionalities, from information retrieval to task planning and execution."}, {"title": "3 LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF)", "content": ""}, {"title": "3.1 Core-Agent: Keystone component of an LLM-based agent", "content": "Several works have aimed to establish a well-defined framework for building LLM-based agents. For instance, the paper [15] presents a framework for designing educational problem-solving simulations using LLM-powered agents. The authors emphasize that separating the AI agent from the environment is important in the design process. However, the paper does not provide a clear framework for the agent's components, making it challenging for future work to identify specific points of modification or reuse. Without a transparent and detailed outline of the agent's architecture and component interactions, it is difficult to pinpoint where changes can be made to alter the agent's behavior or how its components could be reused and integrated in other systems.\nThis issue arises from a lack of modularity in the software design, which can be mitigated by adhering to the Single Responsibility Principle (SRP). As emphasized in the paper [16], the SRP is crucial in software development because it provides granularity at different levels of the software, both in terms of code and functionalities. This granularity facilitates the implementation of future improvements and enhances reusability opportunities. Additionally, applying this principle prevents various code smells, ensuring the modularity of the code and thus making it easier for practitioners to manage and maintain the software.\nPaper [17] attempts to establish boundaries between LLM-based agents, the tools they utilize, and their surrounding environment. Although the proposed LLM-based agent system offers an abstract separation of the agent's components, it does not clearly delineate them from a software engineering perspective. Understanding the theoretical contributions of each component within a unified agent is valuable; however, from a practical development standpoint, it is essential to identify the location, functionality, and role of each internal component within the agent."}, {"title": "3.2 Modeling the Core-Agent Internal Structure", "content": "The human brain exhibits remarkable modularity, with distinct regions and functionalities working in coordination to facilitate cognitive processes, decision-making, and behavior. Inspired by this organized design, our proposed framework aims to emulate the brain's modular structure thought the incorporation of five internal modules: the planning module, the memory module, the profile module, the action module and the security module. By integrating"}, {"title": "3.2.1 Planning Module", "content": "In our proposed framework, the planning module is a pivotal element that enables the agent to break down the complex problems to generate effective plans, the same as in the original framework [4]. However, in our solution the planning module becomes part of the core-agent and collaborates with the other sibling modules to empower the agent to achieve specific goals.\nThe planning module requires complicated understanding and reasoning [20] which could leverage the capabilities of an LLM. In fact, the LLM's ability to comprehend nuanced instructions, interpret implicit information, and adapt to various problem domains renders it an invaluable asset in the planning process. Consequently, the planning module can formulate more comprehensive, context-aware, and adaptable plans, significantly enhancing the core-agent's decision-making process. Furthermore, the planning module works in close collaboration with all other modules within the"}, {"title": "Planning process:", "content": "While generating the procedures to undertake, the planning module follows an incremental approach. Therein, the steps dictate how tasks are decomposed, how the planning procedure unfolds, and how alternative solutions are generated and evaluated. Inspired by the human capacity to decompose complex tasks into simpler ones to achieve overarching goals, this process comprises two main steps [21].\n\u2022 Task decomposition:\nThis initial phase involves breaking down a complex task into simpler subtasks, thereby establishing a structured hierarchy of intermediate goals. The decomposition of complex tasks can adopt two primary approaches: In the non-iterative decomposition approach, the complex task is broken down into simple subtasks all at once. The planning module defines all subtasks, creating a complete task hierarchy in a single step. This method provides a comprehensive overview of the entire task structure upfront. However, the iterative decomposition approach involves a step-by-step breakdown of the task. In fact, the planning module first defines the initial subtask and goal to reach, establish a proper plan and generates the procedures to undertake. After performing the procedures and completing the plan, the output is considered, and the next subtask is defined. This process is repeated, with each new subtask being planned and executed, contributing to defining the next subtask to achieve. The key advantage of this method is its flexibility and ability to adapt the plan based on the outcomes of each subtask. An exemplary application of the iterative approach is the Decomposition-Alignment-Reasoning Agent (DARA) framework [22]. DARA [22] demonstrates how iterative decomposition can lead to more precise and context-aware planning, particularly for complex tasks with multiple interdependent subtasks.\n\u2022 Plan generation:\nFor each subtask derived from the decomposition step, a specific plan is established outlining the procedures to achieve the task's goal while defining the different tools and parties involved. Depending on the chosen planning strategy, the module can generate either a single plan or multiple candidate-plans for each subtask which will be further detailed in the following section."}, {"title": "Planning strategies:", "content": "To guide the planification, organization and execution of complex tasks, the planning module must adhere to a specific strategy when elaborating the procedures during the plan generation step. Selecting a planning strategy can substantially influence the effectiveness, efficiency, and robustness of the resulting plans. Within the LLM-Agent-UMF, we define two primary strategies:\n\u2022 Single-path strategy:\nThis approach involves generating a singular path or sequence of procedures to achieve the goal, adhering to the plan step-by-step without exploring alternatives, thereby providing a straightforward, deterministic approach to planning. Chain of thought (CoT) [23] is an example that outlines such a strategy. Indeed, it uses sequential reasoning as it involves breaking down complex problems into multiple procedures, each built on the previous ones [23].\n\u2022 Multi-path strategy:\nConsequently, in single path strategy, any error in one procedure can lead the subsequent procedures or the overall plan to be suboptimal or infeasible [21], negatively impacting the entire strategy. A straightforward approach to mitigate such failures is the Multi-path strategy, which involves two major steps: The first phase involves leveraging the LLM to generate multiple plans for the complex task. Indeed, each intermediate step holds multiple subsequent paths [4]. As for the second phase, it deals with the evaluation and the selection of the most suitable path.\nAs a matter of fact, the Tree of Thoughts (ToT) [24] and Graph of Thoughts (GoT) [24] are two frameworks that utilize this multi-path approach. They both operate by leveraging an LLM as a thought generator to produce intermediate procedures, that are structured either as a hierarchical tree in case of ToT or as a more"}, {"title": "Planning Techniques:", "content": "The planning module follows planning techniques as methodological approaches to form executable plans. These techniques are chosen based on criteria such as the complexity of the task, and the need for contextual comprehension. Our framework present two primary techniques:\n\u2022 Rule-based technique:\nWithin our architectural framework, rule-based methodologies encompass what is commonly referred to in literature as symbolic planners [21]. These techniques proved to be valuable especially in contexts characterized by complex constraints, such as mathematical problem-solving or the generation of plans within highly problematic situations [21].\nSymbolic planners, leveraging frameworks like PDDL (Planning Domain Definition Language), utilize formal reasoning to delineate optimal trajectories from initial states to targeted goal states [25]. These methodologies entail formalizing problem scenarios into structured formats, subsequently subjecting them to specialized planning algorithms.\n\u2022 Language model powered technique:\nLanguage Model powered (LM-powered) methodologies leverage the vast knowledge and reasoning ca-pabilities inherent in LMs to orchestrate planning strategies. Within our framework, this category also encompasses neural planners [21], who are adept at addressing intricate and vague tasks necessitating nuanced comprehension and adaptive problem-solving abilities."}, {"title": "Feedback Sources:", "content": "Planning without feedback may pose several challenges as feedback plays a crucial role in optimizing the performance of the planning module within the core-agent. For instance, in iterative task decomposition, feedback has an influential impact on the next generated step and enhances the agent's alignment with the user's expectations.\nTo effectively address these challenges, the planning module relies on diverse feedback sources. As outlined in Figure 3, the core-agent engages with tools within its system boundaries, as well as entities outside its scope, such as external systems and humans. Consequently, interactions with these components can offer valuable feedback:\n\u2022 Human Feedback:\nHuman feedback may be an essential source of information for aligning the planning module with human values and preferences. This feedback results from direct interactions between the core-agent and humans. For example, when the core-agent proposes a plan, humans may provide feedback on its appropriateness, effectiveness, or ethical implications. This feedback could come in various forms, such as ratings, or comments.\n\u2022 Tool Feedback:\nThe core-agent often utilizes various tools, which can be internal components of the system or external applications. For instance, an internal calculator tool might raise an exception upon receiving an illegal operation like division by zero. Likewise, external tools provide feedback in the form of error messages, or performance indicators. Indeed, if the core-agent uses a weather prediction remote API, the accuracy of the prediction serves as feedback. This tool-provided feedback helps the core-agent refine its tool selection and usage strategies.\n\u2022 Sibling Core-Agent Feedback:\nIn multi-core agent systems, as will be discussed in section 3.4, feedback from sibling core-agents becomes a valuable source of information. This type of feedback results from interactions and information exchanges"}, {"title": "3.2.2 Memory Module", "content": "The memory module is responsible for the storage and retrieval of information pertinent to the core-agent's activities, thereby enhancing the core-agent's decision-making efficiency and task execution capabilities. In [26] and [4], the memory module in an LLM-based agent was approached from an abstract functional perspective, neglecting its analysis from a software architectural viewpoint. This led to some overlap between the memory module and the other modules defined in the framework suggested by [4]. Consequently, we propose a more comprehensive and well-defined presentation of this module based on three perspectives: Memory Structure, Memory Location, and Memory Format.\nThe framework presented in the survey [4] classified the memory structure into Unified Memory, intended to emulate human short-term memory via in-context learning, and Hybrid Memory, which represents both short-term and long-term memory functionalities. While these names aimed to differentiate types of agent systems, they deviate unnecessarily from the well-established terminology in the field. A more conventional and widely recognized categorization is introduced through the first perspective, Memory Structure, which includes short-term and long-term memory, aligning more with the human memory structure as established by [27].\nAs opposite to how it was defined in framework [4], our short-term memory definition diverges to focus more on the impact of the core-agent rather than on the LLM. From a content perspective, various sources of information contribute to short-term memory. As pointed out in paper [26], data can be derived from one trial of a given task or from previous trials of the same task, which is referred to as short-term memory. This data has a narrow scope that focuses on a specific task and primarily communicated to the LLM for the purpose of in-context learning and enhancing the LLM capability with more information related to the task at hand. Conversely, long-term memory refers to the ability to store and recall information over extended periods, beyond the scope of a specific task. This type of memory enables the core-agent to maintain coherence and context over prolonged interactions, learning and adapting from past experiences. Namely, MemoryBank stores all interactions with user in a large symbolic memory and processes experiences into high-level summaries to reflect upon future similar tasks [28].\nHowever, there is always a limit to the amount of memory a core-agent can retain. Therefore, techniques such as forgetting mechanisms must be implemented to decide which memories to discard and which to retain [26]. As mentioned earlier, survey [4] treats memory from an abstract functional aspect, which is inconsistent with our framework that emphasizes a clear delineation of distinct modules. Our approach aims to provide a more precise and structured understanding of core-agent components from a software perspective. For instance, according to survey [4], the memory module includes a \"reflection\" operation with a cognitive aspect that we believe belongs to the planning module. The planning module focuses on achieving the agent's goals and is responsible for decision-making and synchronizing among all other modules. Thus, it makes it more suitable for handling reflections and memory optimization by collaborating with the memory module to access stored data. Likewise, extracting useful information from memory and reflecting upon it to produce a solid plan belongs to planning module responsibilities. For example, Voyager [29] considers environmental feedback, handled as short-term information, and leverages the LLM capability to adjust its plan and make more efficient and rational decisions in the scope of planning module."}, {"title": "3.2.3 Profile Module", "content": "Similarly to the approach presented by the paper [4], our framework defines the function of the profile module as to establish the role of the LLM and adopts more diverse methods. This template explicitly separates the roles of both the core-agent and the LLM. Indeed, the profile module facilitates the dynamic adaptation of various profiles tailored to specific use cases and strategies employed by the planning module.\nThe Profile module features four methods for defining profiles: the Handcrafted In-Context Learning Method, the LLM-generation method, the Dataset Alignment Method, and the newly introduced Fine-tuned Pluggable Modules method.\nThe Handcrafted In-Context Learning Method, previously referred to as the Handcrafting Method in the survey [4], involves deputing the core-agent to set the LLMs profiles through in-context learning techniques that employ pre-configured prompts. This method allows for fine-grained control over the LLM's personality and behavior. While it is a straightforward method to implement, it necessitates the use of LLMs that are well-suited for in-context learning and often possess a cumbersome size.\nThe LLM-Generation Method facilitates the automatic creation of profiles for agents using LLMs. This method begins by specifying the profile's characteristics which include detailed information such as age, gender, and interests. Optionally, several seed agent profiles can be selected to serve as few-shot examples, as outlined in the paper [4]. Once these seed profiles are established, LLMs are employed to generate additional profiles by referring to the initial seed examples. For instance, RecAgent [32] suggests designing appropriate prompts that encourage a GPT model to generate comprehensive profile descriptions by referring to a table of attributes corresponding to various samples and generate additional profiles. The LLM-Generation Method, while offering significant time-saving advantages, is constrained by its reliance on LLMs. This dependence can lead to potential biases [33] or inaccuracies in generated agent profiles.\nAdditionally, the Dataset Alignment Method derives profiles from real-world datasets [4], which consists of data about actual individuals. This approach starts by organizing the information in these datasets into natural language prompts that describe the characteristics of the role of the LLM. These structured data are then used to create the profiles. In the study conducted by [34], researchers utilized GPT-3 alongside real world demographic data from ANES to assign roles based on characteristics such as state of residence. Subsequently, they evaluated whether GPT-3 could mimic real human behavior reliably. The Dataset Alignment Method ensures that the LLM profile accurately reflects real-world attributes and behaviors, thereby making it meaningful and realistic. However, the effectiveness of this method relies heavily on the accuracy and representativeness of the underlying real-world datasets."}, {"title": "3.2.4 Action Module", "content": "While we have enriched the approaches applied in this module, we define it within the LLM-Agent-UMF similarly to the methodology described in [4]. This module serves as the convergence point where contributions from all other modules and their latent inputs are visualized as outcomes. It translates the agent's decisions into executable actions, conceptualized from four perspectives: Action Goal, Action Production, Action Space, and Action Impact.\nThe first perspective, the Action Goal, represents what the core-agent intends to accomplish by performing actions based on various objectives such as Task Completion, Communication, or Environment Exploration. The Action module maintains this Action Goal and guides the core-agent in selecting appropriate actions accordingly. The second aspect, Action Production, focuses on how the actions are produced and the catalysts behind them. In fact, the core-agent can produce actions through three primary methods: Action via Memory Recollection, Action via Plan Following, and Action via API Call Request, newly introduced in our framework.\nIn the case of Action via Memory Recollection, the core-agent generates actions by extracting relevant information from its memory based on the current task. For instance, in GITM, if the agent needs to achieve a sub-goal, it checks its memory for similar past experiences and uses successful actions from those experiences to handle the current task [31].\nSimilarly, through the Action via Plan Following method, the core-agent can execute actions by adhering to pre-generated plans. In GITM and DEPS (Describe, Explain, Plan, and Select) [36], the agent breaks down the task into subtasks and sub-procedures, acting sequentially to achieve each procedure and complete the overall task. In case of plan failure, both agents have replanning capabilities, so the action module executes the new plan. These two methods, Action via Memory Recollection and Action via Plan Following, align with the framework described in [4].\nBy exploring other state-of-the-art techniques like TALM [5] and ToolFormer [37] that exemplify LLMs' capacity to improve performance across diverse tasks through incorporating external tools, we introduce a new method named Action via API Call Request. This novel approach empowers the core-agent to execute actions in response to API call requests initiated by the LLM, facilitating smooth integration and effective utilization of external resources.\nThe third perspective, Action Space, defines the set of possible actions that can be performed by the core-agent, remaining consistent with the original framework [4]. These actions can leverage internal knowledge, where the core-agent acts on internal information, and/or utilizes tools when comprehensive expert knowledge is required. These tools can encompass APIs such as HuggingGPT, which leverages AI models via the HuggingFace API to accomplish complex user tasks [38]. Alternatively, the core-agent can rely not just on its memory but extend its operational scope and knowledge by communicating with other read-only data sources like databases and knowledge repositories. It is useful to note here that data repositories can have the same formats discussed in the memory module (section 3.2.2). For instance, LLamaIndex [39] stores data as vector embeddings at the indexing stage to leverage semantic search, where the similarity between embeddings is used to rank documents by their relevance to a query. In contrast, ReAct [40] uses a textual data repository, like Wikipedia, to mitigate error propagation in chain-of-thought reasoning.\nFinally, Action impact refers to the consequences resulting from an action. Numerous impacts can be cited, such as changing the environment by moving to different locations, gathering resources, or constructing buildings [41]. Actions can also alter the internal state of the core-agent by updating its memory or acquiring new knowledge which may affect the planning module. Additionally, impacts can trigger new actions, creating a chain of actions during task completion."}, {"title": "3.2.5 Security Module", "content": "As LLMs continue to advance and being implemented in widespread application, addressing the potential risks and unintended consequences associated with their use becomes imperative. These concerns are mainly around unauthorized and unethical use, data biases and privacy [3]. This has led to the introduction of guardrails recently in LLMs field as algorithms to identify and prevent the misuse of LLMs [12]. To enhance our framework, we propose a fifth module: the Security Module. This module aims to provide a more capable and responsible core-agent. Its role is monitoring the action module specifically in production environments to ensure the safety and responsible use of LLMs.\nThe Security Module operates within the parameters of the Confidentiality, Integrity, Availability (CIA) triad [42], a crucial model which encompasses three pivotal principles in the security field. Confidentiality is centered around protecting sensitive information from unauthorized access or disclosure. Within LLMs-based agents, this principle is critical in safeguarding user data and ensuring the non-divulgence of sensitive information. Integrity is concerned with the accuracy, consistency, and trustworthiness of data throughout its lifecycle. For agents, this principle involves maintaining the reliability of the model's outputs and preventing any unauthorized modifications to the system or its data. Lastly, availability focuses on ensuring that information and resources are accessible to authorized users whenever required. In the context of LLM applications, this entails maintaining system uptime and providing safe responses to user inquiries. By adhering to these principles, the Security Module aims to establish a robust and trustworthy environment for the operation of the agent, effectively addressing critical concerns related to their deployment and usage.\nWhile conducting a thorough research around the Security Module, our approach will involve exploring multiple facets: Identifying and securing critical assets and data within the core-agent modeling framework, implementing strategies and mechanisms to protect these assets from potential threats, ensuring the core-agent's ability to effectively respond to and mitigate any security incidents, and maintaining the privacy of user data while adhering to relevant data protection regulations."}, {"title": "Security measures:", "content": "Regardless of the guardrail type deployed, the Security Module encompasses three fundamental axes: Prompt safe-guarding, response safeguarding, and data privacy safeguarding.\n\u2022 Prompt Safeguarding:\nPrompt safeguarding necessitates employing measures to detect and mitigate unauthorized access to Large Language Models through prompt injection attacks [43]. Techniques for enhancing the security of LLM-based agents can be integrated directly into the LLM itself, with Adversarial Training (AT) being a prominent example [44] AT enhances an LLM's defense mechanisms by fine-tuning it with augmented training data containing adversarial examples, thereby increasing the model's ability to safeguard against malicious prompts and improving its robustness. However, AT faces significant limitations, such as the challenges in efficiently selecting adversarial examples and the model's exposure to adversarial perturbations such as HOUYI [45], a black-box prompt injection attack. Moreover, such training-based security techniques may impact the generative performance of the LLM which necessitates additional evaluation steps.\nOther techniques address these limitations by decoupling the security measures from the LLM and delegating them to a distinct entity that we identify as a core-agent supplemented with a security module as illustrated in Figure 3. This decoupling is essential for improved protection and enables the implementation of more advanced security protocols on the LLM input, which can evolve independently of the LLM's training process. An example of this approach is Nvidia NeMo [46], which functions as an intermediary layer between users and LLMs, employing advanced techniques such as vector databases and comparison with stored canonical forms to filter and process user inputs before they reach the model, thereby providing robust prompt safeguarding without directly modifying the underlying LLM.\nThese approaches are essential to address scalability challenges, enable proactive defense, and facilitate continuous learning in LLM security, ensuring that protection mechanisms can adapt to new threats and maintain the integrity of LLM interactions.\n\u2022 Response safeguarding:\nThe recent survey [47] has demonstrated that despite the implementation of prompt safeguarding techniques, the overall resilience of Large Language Models (LLMs) against advanced attacks, known as jailbreaks, may not experience significant improvement. These jailbreaks are designed to exploit biases or vulnerabilities within language models by manipulating their responses. Notable examples include white-box attacks AutoDAN-Zhu [48], which generate stealthy prompts to avoid triggering the model protective mechanisms. Additionally, black-box attacks leverage manually crafted prompts to deceive the LLM [47]. The effectiveness of these"}, {"title": "Guardrail types:", "content": "To implement these security axes effectively, various guardrail methodologies have been developed. These guardrails act as the operational layer of the security module, translating high-level security objectives into actionable safeguards. The paper [47] delves into diverse guardrail methodologies and solutions offered by LLM service providers and the open-source community. Through meticulous analysis of these methodologies, two primary types emerge, rule-based guardrails and LLM-based guardrails.\n\u2022 Rule-based guardrails:\nThese guardrails operate based on a predetermined set of rules and regulations aimed at screening and preventing potentially detrimental or undesirable inputs/outputs from LLMs. To elucidate the process, users define the content necessitating protection. Subsequently, the guardrails assess the inputs/outputs against these predefined regulations, and custom rules [47], to ascertain compliance. In instances where the content is deemed unsafe, it may be obstructed, or a cautionary alert may be issued. For instance, the Adversarial Robustness Toolbox (ART) [52] is specifically designed to bolster the security and robustness of models against adversarial attacks. It offers tools and methods to defend against and adapt to malicious inputs, thereby safeguarding AI applications from potential vulnerabilities.\n\u2022 LLM-powered guardrails:\nWhile rule-based guardrails provide a solid foundation for safeguarding LLM operations, they face limitations in adaptability and maintenance. The need for manual, continuous improvement and intervention to upgrade rules can be time-consuming and may struggle to keep pace with rapidly evolving threats and diverse use cases. LLM-powered guardrails offer a compelling solution to these challenges. A prevalent design approach for constructing these guardrails involves the usage of neural-symbolic agents [47]. These agents, functioning akin to core-agents from a security standpoint, undertake the critical task of analyzing input and output, ensuring"}, {"title": "3.3 Active/Passive Core-Agent Classification", "content": "As discussed in the preceding sections, the core-agent represents a distinct entity within the LLM-Agent-UMF. While the LLM excels in cognitive tasks such as understanding, reasoning, and generating responses, it lacks the capability to directly interact with the environment or external tools. This is where the core-agent plays a crucial role. It bridges the gap between the LLM's cognitive abilities and the need to engage with external sources, enabling seamless integration with various tools and systems. The core-agent is thus characterized by its action capabilities and its ability to respond to user requests through interaction with these diverse tools. In fact, ToolLLM [54] is a general tool-use framework that enhances LLMs capabilities enabling agents to use external tools and APIs. It uses a neural API retriever to recommend appropriate APIs for each instruction. Then they employ a depth-first search-based decision tree algorithm to evaluate multiple reasoning traces and expand the search space. Consequently, it enhances the planning ability of the retriever and empowers the finetuned LLM, ToolLlaMA, to generate adequate instructions. The retriever here, in association with the search-based decision tree algorithm, satisfies our definition of a core-agent. In this case where the LLM-based agent conducts cognitive tasks, memory and planning modules are essential in the core-agent to ensure reasoning capabilities because they enable the agent to retain and recall past experiences, plan and synchronize actions, reason and make decisions [26] [21].\nIn other cases, such as Toolformer [37], we identify entities that fit our definition of a core-agent but lack both planning and memory modules. Indeed, Toolformer fine-tunes its LLM on function calling, enabling it to generate API requests within natural language as needed. Consequently, the LLM determines when to make an API call, which API to use, and how to integrate the results, while the actual execution of the API request is delegated to an entity that we identify as a core-agent. In this case, the planning module of the core-agent is obsolete because planning is handled solely by the LLM. Yet, its action module is present because it is still responsible for executing API calls systematically. For example, if the model suggests using a calculator API, the core-agent retrieves the arguments for the mathematical operation, performs the calculation, and returns the computed result to the LLM.\nThe inspection of the state-of-the-art led to the conclusion that the action module is always indispensable in a core-agent as it is responsible for producing the executive steps to achieve their goals. However, the architectural disparities in core-agents and the absence of some modules in some proposed agent systems highlight the need to introduce a"}, {"title": "3.3.1 Active Core-Agents", "content": "Active core-agents encompass all five modules described in section 3.2 and illustrated in Figure 3, but what differentiates an active from a passive core-agent is its managerial aspect. An active core-agent is characterized by its leading position in the agent as the orchestrator of other components, so naturally, it requires a planning module to divide tasks into subtasks and collaborates with the memory module to provide the necessary context, analyze information, and make decisions. Consequently, we consider an active core-agent to be stateful, meaning it can maintains information about its past interactions and states over time. This is facilitated by an adaptive memory that captures and stores various aspects of the agent's lifecycle, allowing it to use this historical data to inform future actions and decisions. The profile module role is emphasized in the active core-agent category, because it guides the LLM's behavior in a certain direction. Furthermore, the security module plays a prominent role in safeguarding the communication between the LLM and the human, ensuring a reliable exchange; Acting as an intermediary, the core-agent safeguards the LLM from threats such as jailbreak attempts and protects user data privacy by implementing safety measures as outlined in Table 1.\nThroughout our research on the state of the art, we observed that LLM-based agents are recently built upon active core-agents performing tasks from planning to execution [4]. As highlighted in [17], active core-agents are more effective because they incorporate planning and memory modules, which enable them to reason, plan, and execute tasks efficiently. This structure allows the agent to adapt to changing situations and make informed decisions, making the system more robust and capable.\nHowever, relying solely on active core-agents would increase the complexity of the agent, which can lead to scalability issues and negatively impacts the maintainability of the agent as it will hinder and complicate future improvement efforts. As noted in [17], \"the complexity of the agent system grows exponentially with the number of tasks it needs to perform\". Therefore, rather than centralizing responsibilities on one entity, it would be more beneficial and adhering to the Single Responsibility Principle, if we leverage other core-agents to granulate task execution and reduce the complexity of the agent system. This approach is supported by the concept of \"separation of concerns\" in software engineering, which emphasizes the importance of dividing responsibilities among multiple components to improve system modularity and maintainability. By distributing tasks among multiple core-agents, we can reduce the cognitive load on individual core-agents, improve system efficiency, and enhance overall performance."}, {"title": "3.3.2 Passive Core-Agents", "content": "Passive core-agents are employed when LLMs cover all cognitive tasks of the agent such as planning and taking decisions, while passive core-agent's role is mainly to execute specific procedures. As a direct consequence, the planning module becomes unnecessary and likewise the memory needed in reasoning. Unlike active core-agents, passive core-agents are stateless, and the short-term memory is handled by the LLM, covering only the current task's state. In LLM-based agents, passive core-agents, which always follow instructions from domain-specific LLMs, lack the ability to control the profile of the LLM thus do not possess a Profile module. The LLM profile may be statically defined during the system setup or dynamically defined by another entity, which will be discussed in the next section.\nThe most essential module in a passive core-agent is the action module. Our framework posits that the function of a passive core-agent is limited to specific task execution. Actions are often triggered by API call requests, which are not decision-based nor self-generated by the passive core-agent but provided by another entity (e.g., LLM or an active core-agent) as shown in Figure 4. The actions do not alter the internal state of the agent or change a predetermined plan. This again points to the absence of a planning module in passive core-agents. Furthermore, we introduce another distinction between passive and active core-agents: the communication between humans and core-agents is interactive and bidirectional in both categories, aiming to gather information and/or feedback. However, as pointed out by Figure 4, the communication between passive core-agents and humans can only be initiated from the passive core-agent part, unlike active core-agents, where communication can be initiated by either party.\nDespite not being directly responsible for handling prompts from humans and providing generated text responses, passive core-agents should still possess a robust security module. This component is crucial in ensuring privacy during their interactions with other humans or third-party systems by preventing leakage of sensitive data while minimizing potential threats and breaches. Consequently, this bolsters the overall trustworthiness and reliability of LLM-based agent applications. It is also important to note that in this setup, as illustrated in Figure 4, the LLM ensures by itself the safety of the prompts by implementing one of the mechanisms previously discussed in section 3.2.5 such as adversarial training."}, {"title": "3.4 Multi Active/Passive Core-Agent Architecture", "content": "Handling complex tasks often necessitates the use of multiple agents, as a single agent may not possess the requisite capabilities or expertise to tackle diverse domains. However, LLM-based multi-agent systems face considerable challenges, including scalability, integration, management of inter-agent relationships, and ensuring interpretability in managing intricate tasks [55"}]}