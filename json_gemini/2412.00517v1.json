{"title": "LAMBDA: Covering the Multimodal Critical Scenarios for Automated Driving Systems by Search Space Quantization", "authors": ["Xinzheng Wu", "Junyi Chen", "Xingyu Xing", "Jian Sun", "Ye Tian", "Lihao Liu", "Yong Shen"], "abstract": "Scenario-based virtual testing is one of the most significant methods to test and evaluate the safety of automated driving systems (ADSs). However, it is impractical to enumerate all concrete scenarios in a logical scenario space and test them exhaustively. Recently, Black-Box Optimization (BBO) was introduced to accelerate the scenario-based test of ADSs by utilizing the historical test information to generate new test cases. However, a single optimum found by the BBO algorithm is insufficient for the purpose of a comprehensive safety evaluation of ADSs in a logical scenario. In fact, all the subspaces representing danger in the logical scenario space, rather than only the most critical concrete scenario, play a more significant role for the safety evaluation. Covering as many of the critical concrete scenarios in a logical scenario space through a limited number of tests is defined as the Black-Box Coverage (BBC) problem in this paper. We formalized this problem in a sample-based search paradigm and constructed a coverage criterion with Confusion Matrix Analysis. Furthermore, we propose LAMBDA (Latent-Action Monte-Carlo Beam Search with Density Adaption) to solve BBC problems. LAMBDA can quickly focus on critical subspaces by recursively partitioning the logical scenario space into accepted and rejected parts. Compared with its predecessor LaMCTS, LAMBDA introduces sampling density to overcome the sampling bias from optimization and Beam Search to obtain more parallelizability. Experimental results show that LAMBDA achieves state-of-the-art performance among all baselines and can reach at most 33 and 6000 times faster than Random Search to get 95% coverage of the critical areas in 2- and 5-dimensional synthetic functions, respectively. Experiments also demonstrate that LAMBDA has a promising future in the safety evaluation of ADSs in virtual tests.", "sections": [{"title": "I. INTRODUCTION", "content": "HE past few years have witnessed the rapid development of highly automated driving technology, which has shown great potential in terms of accidents reduction, energy savings and congestion mitigation [1], [2]. However, the safety of au- tomated driving systems (ADSs) remains a concern hindering"}, {"title": "II. RELATED WORKS", "content": "In this section, we first review the BBO algorithms proposed in the current literature and choose several algorithms as base- lines for the benchmarks. The studies on the safety evaluation of ADSs based on optimization are then summarized and research gaps are discussed."}, {"title": "A. The black-box Optimization Methods", "content": "Current BBO algorithms can be divided into 3 categories: population-based, surrogate-based, and Monte Carlo Tree Search (MCTS)-based. Population-based methods are mainly inspired by the behavior of the biological population. Genetic Algorithm (GA) [12] simulates evolution and selection by using cross and mutation operators to propose new samples. Differential Evolution (DE) [13] is similar to GA but uses vec- tor differences to perturbate current samples. Besides, Particle Swarm Optimization (PSO) [14], CMA-ES [15], Simulated Annealing [16] are also widely known in population-based optimization methods. In this paper, we choose the most classic GA and DE as baselines in benchmarks from the population-based methods.\nSurrogate-based methods maintain a surrogate model of the objective function during optimization and determine candi- dates to be evaluated obeying to the surrogate. Bayesian Opti- mization (BO) [17] is a typical surrogate-based method widely used in many fields such as Neural Architecture Search (NAS) [18], structure design [19], and hyper-parameter tunning [20]. The most significant limitation of BO is the $O(n^3)$ com- plexity of the Gaussian Process Regressor (GPR) surrogate,"}, {"title": "B. Optimization-based Safety Evaluation Methods of ADS", "content": "Safety evaluation of ADSs has been a popular research topic for years, mainly conducted through scenario-based tests, in the form of finding or generating critical scenar- ios [32]. Critical scenarios can be generated through data-driven [33], adversary-based [34], and sample-based methods, while this paper focuses on sample-based methods, especially optimization-based sampling methods.\nCurrently, numerous studies are introducing the BBO algo- rithms mentioned above into scenario-based testing of ADSs to find the most critical scenario and conduct the pressure test. For population-based methods, Klischat et al. [35] ap- plied DE and PSO to generate critical scenarios in com- plex situations (for example, intersections). Kl\u00fcck et al. [36] compared the performance of GA with simulated annealing and random testing for optimization of test parameters. Li et al. [37] combined GA with a local fuzzer to perturb the parameters of the trajectories of other traffic participants to create safety-hazardous situations. When it comes to surrogate- based methods, Gladisch et al. [38] used Systems Theoretic Process Analysis (STPA) to initiate a test function scenario and then applied BO to find critical combinations of test parameters and their values. Sun et al. [9] compared six surrogate models with two logical scenarios and found that Extreme Gradient Boosting (XGB) stood out among them. Based on an information-theoretic approach, Gong et al. [39] proposed a novel acquisition function for a GPR-based multi- fidelity sampling framework to reduce the cost of CAV safety evaluation. As for the MCTS-based methods, Koren et al. [40] used MCTS and Deep Reinforcement Learning (DRL) to find collision scenarios, respectively, and then compare their efficiency.\nAs discussed in Section I, the studies reviewed above are essentially falsification methods, which means that they focus only on one or several optimal solutions and the efficiency of the applied algorithm, leading to a biased search result that cannot validate the comprehensive safety performance of ADSs. To fill the gap, researchers have taken into account test coverage, with the goal of balancing exploration and exploitation. Feng et al. [41] modified PSO to enhance the exploration ability and adopted the index F1 to quantify the coverage of critical scenarios. Wang et al. [42] proposed a modified expected improvement criterion that emphasized more on the coverage of failure mode as an acquisition function for a GPR-based adaptive sampling, while utilizing contextual information to encourage exploration. From another perspective, Zhu et al. [43] attempted to identify a safety performance boundary (SPB) with the sampled data using the Gaussian distribution method to cover all hazardous parameter spaces. Following the same idea, Wang et al. [44] utilized an optimized gradient descent searching algorithm to quickly and accurately identify SPB and further proposed the concept of safety tolerance performance boundary (STPB). However, it is still a concern to guarantee both the efficiency and coverage of these algorithms under multimodal and high-dimensional BBC problems. This paper proposes a metric for evaluation, an algorithm for solution, as well as a synthetic function for verification to address this issue."}, {"title": "III. DEFINITION AND METRIC OF THE BBC PROBLEM", "content": "The BBC problem can be defined as follows. Given a black-box function f which represents method to obtain the criticality of a scenario and a criterion 8, the goal is to figure out the solution set \u00e6 satisfying the inequality $f(x) > \\delta$ and achieve as much coverage of x as possible. In this paper, only deterministic functions are considered. The key question of BBC problem is that no information else except the input and output of f can be obtained, and the number of times to evaluate f is limited. It is a typical search-based optimization problem that might be solved within an optimization paradigm. Table I shows the denotations in this paper.\nTo be specific, in a BBC problem, the optimization agent II has a limited optimization budget N, which means that the agent only has N opportunities to sample from f. At each sampling from f, the agent decides where to place the sampling point \u00e6 according to the historical sample records D, then gets an output y = f(x) through evaluating f, and appends the new pair (x, y) to D, as shown in Eq. (2). After running out the optimization budget, agent II gets sample records D of size N."}, {"title": "A. Definition", "content": "The BBC problem can be defined as follows. Given a black-box function f which represents method to obtain the criticality of a scenario and a criterion 8, the goal is to figure out the solution set \u00e6 satisfying the inequality $f(x) > \\delta$ and achieve as much coverage of x as possible. In this paper, only deterministic functions are considered. The key question of BBC problem is that no information else except the input and output of f can be obtained, and the number of times to evaluate f is limited. It is a typical search-based optimization problem that might be solved within an optimization paradigm. Table I shows the denotations in this paper.\nTo be specific, in a BBC problem, the optimization agent II has a limited optimization budget N, which means that the agent only has N opportunities to sample from f. At each sampling from f, the agent decides where to place the sampling point \u00e6 according to the historical sample records D, then gets an output y = f(x) through evaluating f, and appends the new pair (x, y) to D, as shown in Eq. (2). After running out the optimization budget, agent II gets sample records D of size N."}, {"title": "B. Metric", "content": "As mentioned above, different from that in a BBO problem which is to evaluate whether the optimal value is found, the goal of the metric in a BBC problem is to evaluate how many critical values are covered. For this purpose, we use a classifier C trained from sample records D to predict whether x meets $f(x) > \\delta$, and further divide the search space into subspace & and subspace 8. Note that is the set of validation points generated through gird sampling in \u03a9, and s is the subspace predicted by C that satisfies $f(x) > \\delta$, namely the set of critical scenarios, while \u03a9\u00acs is \u03a9 \u2013 \u03a9\u03b4. With the actual Os and \u03a9\u00acs known, the performance of C, which is directly related to the quality of D, can be assessed with Confusion Matrix Analysis [45]. Since D is the sample records of optimization agent II, the ability to cover all the critical scenarios of II can be reflected this way. The confusion matrix of the BBC problem is illustrated in Fig. 2.\nAs shown in Fig. 2, in the Confusion Matrix, recall represents the coverage of C's prediction to the solution set of $f(x) > \\delta$. However, an intentional classifier can decide that all points in the search space must be satisfying $f(x) > \\delta$, then the recall would be 100%. For all practical purposes, it does not make sense. Thus, the precision must be taken into account. In this paper, the $F_2$ score is used to balance recall and precision, with recall being more emphasized. Fig. 2 illustrates the calculation methods for recall, precision, and $F_2$ as well as their relationships.\nIn practice, the key point of the BBC problem is how to acquire information about the search space with a limited optimization budget. Therefore, this paper mainly focuses on the optimization agent II, which decides where to sample the search space and acquire information about f. As for the classifier C, appropriate methods can be chosen according to the domain knowledge of each practical issue. In this paper, for generality and fairness, a basic configuration of C was used in all experiments. A regressor f of f is built from D with the linear interpolation method of SciPy [46]. Thereby the classifier can be constructed with f simply, as shown in Eq.(3). An illustration of the evaluation process can be seen in Fig. 3.\n$C(x) = True IF \\hat{f}(x) > \\delta ELSE False$ (3)"}, {"title": "IV. LAMBDA ALGORITHM", "content": "In this section, we first give a brief introduction to LaMCTS, the predecessor of our proposed algorithm. Then the limitation of LaMCTS is demonstrated and our solutions are provided. Finally the framework and details of our LAMBDA algorithm are depicted."}, {"title": "A. Brief Introduction of LaMCTS", "content": "MCTS is an algorithm to solve discrete sequential decision problems, widely known for its surprising performance in Go [25]. However, MCTS is not originally applicable to continu- ous optimization because it requires a fixed and limited action space, which is usually arbitrary and infinite in continuous optimization problems. LaMCTS [30] breaks this limitation by learning partitions of the search space to form latent actions and adapts MCTS to optimization problems in continuous search space.\nThe main steps of LaMCTS are summarized in Fig. 4, where A represents the entire search space (i.e., the parent node), and other different letters represent the child nodes obtained as the partition proceeds. As depicted in Fig. 4, after initial sampling from the entire search space, LaMCTS builds a tree by recursively splitting the search space into regions with high/low function values. Then, the UCB of each leaf node is calculated to guide the selection of the most promising node, within which new samples will be created. Finally, the function values of the newly created samples are obtained and back- propagated to the historical data set before the next iteration.\nAmong the above steps, the calculation of the UCB, which directly determines where to sample next, is of great impor- tance. In [30], the formula for calculating UCB (denoted as UCB1) from a parent node A to a child node B is as shown in Eq. (4).\n$UCB_1(A\\rightarrow B)=\\frac{\\sum_{x\\in D_B}\\hat{f}(x)}{n_B} + 2 \\cdot C_P \\cdot \\sqrt{\\frac{2 \\ln n_A}{n_B}}$ (4)"}, {"title": "B. Sampling Bias of LaMCTS and Solutions", "content": "Despite the outstanding performance in the BBO problem, LaMCTS is faced with a further question called sampling bias when it comes to the BBC problem. As the optimization pro- ceeds, samples concentrate towards more hopeful subspaces. Hence, the record set suffers from an imbalance of samples, leading to the following: A) the partition learned from D overemphasizes these hopeful regions and cuts them into many fragmentary subspaces. B) the UCB score based on the mean value and the number of samples in a subspace loses efficacy. The two impacts above make the optimization agent trapped in one local modality of the search space. Fig. 5 shows an intuitive example of this phenomenon.\nAs shown in Fig. 5, assuming VB > vc, then B is the hopeful region of the search space. Compared to \u03a9, with the same number of samples (n\u0432 = nc = 6), \u03a9\u0392 has a higher mean value of samples but a smaller volume. Based on Eq. (4), the exploration terms constructed with the number of samples turn out to be equal and no longer encourage sampling in the under-explored subspace \u03a9. To handle this issue, in this paper, density information is introduced to overcome the sampling bias. The Inverse Probability Weighting (IPW) technique is used to rebalance the sample records in search space partitioning and UCB calculation, which will be detailed later.\nIn order to estimate the sampling density efficiently, we developed an adaptive Kernel Density Estimator (KDE) with Approximate Nearest Neighborhood (ANN). The kernel band- width is dynamically calculated by querying the k-th nearest neighbors' minimal bounding sphere so that the KDE can adapt to different problems without tuning hyper-parameters. In addition, faiss [47] was utilized to improve retrieval efficiency. The speedup structure (or Index in faiss) needs to be retrained periodically, because ANN assumes that data are drawn from one fixed distribution while the distribution of samples gradually concentrates to target subspaces during the optimization."}, {"title": "C. Framework and Improvements of LAMBDA", "content": "The framework of LAMBDA is similar to that of LaMCTS, consisting of extension (treeification), selection, simulation, back-propagation, etc. The structure of LAMBDA is shown in Fig. 6. Note that the F2 score calculation module for evaluation is an independent module decoupled from the entire search process. The calculation can be performed at any search stage of any optimization algorithm as long as the historical samples and the black-box objective function values of the validation points are available.\nInitial Sampling. Sample an initial D to construct the partition tree. Sobol sequence [48] was used in sampling to get more uniformity.\nTreeification. Treeify the search space into subspaces recursively to form a quantization of the search space. Each leaf node in the partition tree represents a subspace. In this paper, we used linear partitions. More complex forms of partitions can be used, but with a greater cal- culation burden in the treeification and simulation stages. Sampling density estimated by KDE is introduced into this stage to get unbiased partitions.\nSelection. Select a batch of subspaces to sample accord- ing to the UCB scores of the nodes in the partition tree. A new UCB calculation method is proposed based on the sampling density to overcome sampling bias."}, {"title": "V. EXPERIMENTS", "content": "In this section, we first verify LAMBDA's performance and illustrate the advantages of LAMBDA over other baseline al- gorithms in terms of efficiency and coverage on synthetic func- tions. Baseline algorithms are chosen as mentioned in Section II, namely Random Search (RS) and Sobol, GA and DE, \u0412\u041e and TuRBO, and LaMCTS, which belong to random search algorithms, population-based algorithms, surrogate-based al- gorithms and MCTS-based algorithms, respectively. As for synthetic functions, we choose Holder-Table as the black-box function in low-dimensional BBC problems, while we can obtain a better visualization about the performance of each algorithm in low-dimensional cases. For high-dimensional BBC problems, we have designed a multimodal black-box function named Ripples (which we call a global optimum as a modality in this paper) so as to better demonstrate the superiority of our algorithm. Finally, a practical issue for the safety evaluation of certain ADSs based on SiL (Software-in- Loop) testing is demonstrated.\nIn addition, hyper-parameters of our algorithm and base- lines are as demonstrated in Appendix A, noting that hyper- parameters of baselines are set according to the suggestion of their authors. If no suggestions are found, these hyper- parameters are set similarly to those of LAMBDA to ensure the"}, {"title": "A. Holder-Table Function Benchmarks", "content": "1) Test Function: Holder-Table is a classical 2- dimensional synthetic function to verify the performance of optimization algorithms. As shown in Eq. (9) and Fig. 12, it has 4 global optima distributed at the corners of the search space, namely f(x*) = 19.2085, at x* = (8.05502, 9.66459), (8.05502, -9.66459), (-8.05502, 9.66459), and (-8.05502, -9.66459), while many local maxima to trap the optimizer. It should be noted that in this paper we invert the function by multiplying -1 by the original function, since LAMBDA is a maximization algorithm. The black-box inequality is defined as f(x) > 18, whose solution set is lined with red. Moreover, as described above, LAMBDA uses Sobol as the local sampler for this low-dimensional case, and so does LaMCTS for fair comparison.\n$f(x)=\\left| sin(x_1)cos(x_2)exp\\left( -\\frac{\\sqrt{x_1^2 + x_2^2}}{\\pi} \\right) \\right|$ , $X_i \\in [-10, 10]$ for all i = 1,2 (9)\n2) Experimental Process and Results: After every 10 sam- ples, the F2 score of each algorithm is calculated under current historical sampling records as described in Section III-B. With the search progressing, the F2 score will gradually increase and eventually reach 1 if all four modalities are covered. The benchmark results are shown in Fig. 13. The solid lines represent the average F2 scores of algorithms while the light- colored areas lie between test cases with the algorithms' best and worst performances, indicating the stability of the algorithms. What is more, it is worth noting that BO (with"}, {"title": "B. Ripples Function Benchmarks", "content": "1) Test Function: To further validate the performance of LAMBDA, high-dimensional experiments are required. Recall that the purpose of our experiments is to test the algo- rithm's coverage of critical subspaces, high-dimensional syn- thetic functions with multiple global optimums are preferred. However, to the best of the authors' knowledge, there exist few functions satisfying the above requirement. Therefore, in this paper, we have designed a high-dimensional multimodal synthetic function named Ripples that specifically serves as a black-box objective function for the BBC problem. The formula of Ripples is shown in Eq. (10).\n$f(x) = \\sum_{i=1}^{dim} \\left(\\frac{x_i^2}{\\sigma^2} + kcos(\\omega x_i - \\mu_i)\\right)$ , where $x_i = ||x + M_{i,:||^2}$ , for all i = 1, 2, ..., dim (10)\n$M = I_{dim}bias$ (11)\nwhere dim is the dimension of the function. $I_{dim}$ is an identity matrix with order dim. $M_{i,:}$ represents the i-th row of the matrix \u039c. \u03c3, k, w and bias are parameters that determine the shape of the function and the locations of the optimal points. As depicted in Eq. (10), there are two parts inside the \u03a3() function. The former part is actually a Gaussian function with a default \u03bc = 0. The latter part is a cos() function to generate multiple local maxima, which shares the same idea with the classical Rastrigin function. It should be noted that win the cos() function must be \u221a2 or a multiple of \u221a2 to ensure that the global optimums appear at the expected positions. With the above settings, there are a total of dim modalities in the dim-dimensional Ripples, namely f(x*) = 1, at x1 = (-bias,0,0,..., 0), X2 = (0,-bias, 0, ..., 0), X3 = (0,0,-bias, ..., 0), Xdim = (0,0,0,..., -bias), respectively.\nSpecifically in this paper, we set bias = 3, \u03c3 = 1, \u03c9 = 2\u221a2 and k = 0.1. In addition, the input domain is chosen to be [-5,5] in every dimension. Fig. 15 demonstrates the profiles of Ripples under the dimension of 1 to 3, respectively. As described above, in Fig. 15a, there is only one modality located at x = -3 in 1-d Ripples. While in 2-d Ripples, two modals are obtained with locations at (-3,0) and (0, -3). Moreover, the solution sets that satisfy f(x) > 0.7 are lined by red lines in Fig. 15b. As for 3d-Ripples in Fig. 15c, we only illustrate the points whose function values are higher than 0.7 to obtain a clearer visualization result. And the three modalities right appear at the expected locations, namely (-3,0,0), (0, -3, 0) and (0, 0, -3). From the above observations, we can conclude that Ripples perfectly satisfies our requirements with multiple global optimums independently distributed at known positions, as well as a low percentage of critical areas and a large number of local maxima that are challenging enough for optimization algorithms.\n2) Experimental Process and Results: In the experiment, we benchmark the algorithms with a 5-dimensional Ripples us- ing the parameters mentioned above. The black-box inequality is defined as f(x) > 0.7. Considering the high-dimensional feature, we use BO and TuRBO as local samplers for LaM- CTS and LAMBDA, respectively. The experimental results are shown in Fig. 16a. Unfortunately, neither random search"}, {"title": "C. Practical Issue for Safety Evaluation of ADSs", "content": "The above two experiments have well verified the superior- ity of LAMBDA over other baseline algorithms on low- and high-dimensional synthetic functions, respectively. For practi- cal applications, safety evaluation of an ADS in a particular logical scenario [6] is a typical BBC problem. Here, we give a practical example for safety evaluation of a certain ADS through SiL testing based on Virtual Test Drive (VTD) [49].\n1) Test Scenario: Car-following behavior is one of the basic driving behaviors and the core of traffic flow theories [50] and the car-following scenario is a typical test scenario for the safety evaluation of ADSs, especially when there exists a rear vehicle in the adjacent lane, in which the risk of the scenario stems from different vehicles, resulting in a multimodal risk distribution as discussed in Section I. The parameterized car- following scenario in this experiment is shown in Fig. 18. The ego vehicle (EGO) is driven by the built-in ADS of VTD (decision-making, planning, and control, with ideal perception) with an initial velocity Vo. While the background vehicle BV1 drives right in front of the ego vehicle with an initial distance S\u2081 and speed V\u2081, which keeps driving at a constant"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose the Black-Box Coverage problem for scenario-based virtual testing of ADSs, which means covering the solution set that satisfies an inequality contain- ing a black-box function. To achieve as high coverage as possible within a limited optimization budget, we develop the LAMBDA algorithm based on the idea of search space quantization. LAMBDA utilizes density information to over- come sampling bias and introduces Beam Search to obtain more parallelizability. Moreover, to verify and evaluate the performance of our proposed algorithm under multimodal and high-dimensional situations, a metric F2 score to measure the coverage and a synthetic function Ripples to challenge the algorithm are proposed. The experimental results demonstrate the outstanding performance of LAMBDA on Black-Box Coverage problems. According to the benchmarks, LAMBDA can be 33 times on the 2-dimensional Holder-Table function and 6000 times on the 5-dimensional Ripples function faster than Random Search to get 95% coverage. Furthermore, the experiment in practical application shows that LAMBDA has great potential in the safety evaluation of ADSs. In future work, we plan to adopt LAMBDA with high-dimensional and more challenging logical scenarios [52]. Moreover, instead of TTC, more comprehensive scenario risk indicators, such as DNDA [53] will be used as the objective function to obtain a more reasonable critical scenario set."}]}