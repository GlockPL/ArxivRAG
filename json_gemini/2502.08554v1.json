{"title": "Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies", "authors": ["Sunnie S. Y. Kim", "Jennifer Wortman Vaughan", "Q. Vera Liao", "Tania Lombrozo", "Olga Russakovsky"], "abstract": "Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such over-reliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are powerful tools, capable of a wide range of tasks from text summarization to sentence completion to code generation. Technology companies have leapt at the unprecedented opportunity to build LLM-infused applications that help users with information retrieval and search, learning new things, and performing everyday tasks more efficiently. Many such applications, such as LLM-infused search engines and chatbots, are predicated on LLMs' ability to provide intricate responses to complex user questions. Already millions of people use LLMs to find answers to their questions about health, science, current events, and other domains, and the use of LLMs is widely predicted to grow [54, 106, 136]. However, the responses produced by LLMs are often inaccurate, sometimes in subtle ways [27, 51, 103, 108]. Inaccurate LLM responses have the potential to mislead users, raising the risk that users will take actions based on the assumption that responses are correct [60, 95, 114, 120, 122]. While such over-reliance on Al systems is not a new problem [6, 96, 97, 117, 132], it may be exacerbated by the introduction of LLMs, since LLM responses are often fluent and convincing even when wrong and public excitement around LLMs is high.\nWhen asked to answer a question, LLMs and systems based on them typically provide a response that contains both an answer to the question and some supporting details or justification for this answer [71, 127]. For example, when asked a math question, an LLM may provide a step-by-step derivation for its answer [25, 44]. In line with everyday usage and much of the psychology literature [56, 78, 80], we refer to such supporting details as an explanation of the answer. (We note that this differs from how the term explanation is often used within the explainable AI community in that we do not make any assumptions about the extent to which it faithfully describes the way that the model arrived at its answer. That is, the explanation describes why the answer is correct, not necessarily why the model output the answer that it did.) Some authors have argued that such explanations should help users spot incorrect answers, potentially mitigating overreliance [17, 40, 69, 114]. However, prior work suggests that in many settings, the very presence of an explanation can increase trust and reliance, whether or not it is warranted [6, 33, 93, 97, 117, 132]. \u03a4\u03bf avoid such unintended negative consequences, it is necessary to understand how users interpret and act upon explanations from LLMs, and how explanations and other features of LLM responses might be adjusted to encourage appropriate reliance.\nTo explore these questions, we first conduct a think-aloud study with 16 participants with varying knowledge of and experience"}, {"title": "2 Related Work", "content": "2.1 Appropriate Reliance on AI\nDespite the rapid progress of technology, AI systems still frequently and unexpectedly fail. Without knowing when and how much to rely on a system, a user may experience low-quality interactions or even safety risks in high-stakes settings. Prior work has investigated how providing information about an Al system's accuracy [43, 128, 129] and (un)certainty [6, 14, 17, 41, 132], explanations of outputs [6, 14, 17, 40, 41, 69, 132], and onboarding materials [18, 68] impact user reliance, as well as the roles played by human intuition [20], task complexity [101, 102], and other human, AI, and context-related factors [63]. However, fostering appropriate reliance on AI remains difficult. Findings on the effectiveness of proposed methods are mixed, and more research is needed on how reliance is shaped in real-world settings.\nWhile most prior work on Al reliance has been in the context of classical Al models (e.g., specialized classification models), there is a growing body of work looking at reliance on systems based on LLMs or other modern generative AI models [60, 71, 109, 110, 113, 135]. For example, several recent studies explored the effect of communicating (un)certainty in LLMs by highlighting uncertain parts of LLM responses [110, 113] or inserting natural language expressions of uncertainty [60, 135], finding that some but not all types of (un)certainty information help foster appropriate reliance.\nContributing to this line of work, we first take a bottom-up approach to identify the features of LLM responses that impact user reliance in the context of answering objective questions with the assistance of a popular LLM-infused application ChatGPT (Section 3). In line with findings from prior work [109], we see that reliance is shaped by the content of explanations provided by the system, particularly whether or not these explanations contain inconsistencies. We also observe that participants seek out sources to verify the information provided in responses. We then design a large-scale, pre-registered, controlled experiment to isolate and study the effects of these features (Section 4). We discuss the relevant literature on these features and their impact on Al reliance next.", "2.2": "Explanations and Inconsistencies"}, {"content": "The impact of explanations on human understanding and trust of Al systems has been studied extensively within the machine learning and human-computer interaction communities, often under the names explainable AI or interpretable machine learning [7, 62, 74, 99, 125]. Explanations are often motivated as a way to foster appropriate reliance and trust in Al systems, since in principle they provide clues about whether a system's outputs are reliable. However, empirical studies have shown mixed results, with a large body of work suggesting that providing explanations increases people's tendency to rely on an Al system even when it is incorrect [6, 97, 117, 132]. One potential reason for this is that study participants do not make the effort to deeply engage with the explanations [16, 37, 55, 73, 114]. That is, instead of encouraging deep, analytical reasoning (System 2 thinking [52, 53]), study participants may resort to heuristics, such as the explanation's fluency or superficial cues to expertise [111], and defer to the system's response on this basis. People may also be more likely to assume an Al system is trustworthy simply because it provides explanations [30]. Further, some clues of unreliability may be difficult to pick up on without existing domain knowledge [20].\nAdopting the broad definition of an explanation as an answer to a why question [13, 35, 80, 123], LLMs often provide explanations by default; when asked a question, LLMs rarely provide the answer alone. For factual questions, they provide details supporting the answer [71, 127], and for math questions, they provide detailed steps to derive the answer [25, 44]. This default behavior is likely due to human preference for verbose responses [21, 100, 134]. Research in psychology has shown that explanations are often sought spontaneously [36, 86], favored when they are longer, more detailed, or perceived to be more informative [4, 8, 75, 121, 131], and used to guide subsequent judgments and behaviors [81, 82]. Since LLMs are often fine-tuned on human preference data via approaches such as Reinforcement Learning from Human Feedback (RLHF) [23, 91, 137], such preferences would shape the form of their outputs. We note that the default explanations that LLMs present typically provide evidence to support their answers, but do not necessarily reflect the internal processes by which the LLM arrived at the answer. This distinguishes these explanations from those traditionally studied in the explainable AI literature.\nExplanations generated by LLMs are widely known to contain inaccurate information and other flaws [27, 51, 103, 108]. We direct readers to recent surveys for comprehensive overviews [48, 115]. In our studies, we found inconsistencies in explanations to be an important unreliability cue that shapes participants' reliance. As documented in prior work, inconsistencies can occur within a response; they are sometimes referred to as logical fallacies or self-inconsistency in the NLP community [47, 116]. Inconsistencies can also occur between responses; many studies have demonstrated that LLMs often change their answer to a question when challenged, asked the question in a slightly different way, or re-asked the exact same question [32, 66, 71]. Such inconsistencies, when noticed, may impact people's evaluation of explanations and reliance on LLMs.\nWe contribute to this line of work in several ways. We first conduct a qualitative, think-aloud study to understand what features of LLM responses shape people's reliance, and find that reliance is shaped by explanations, inconsistencies in explanations, and sources. We then conduct a larger-scale, pre-registered, controlled experiment to quantitatively examine the effects of these features. While a previous work by Si et al. [109] has studied the effects of LLM-generated explanations and inconsistencies on people's fact-checking performance through a small-scale study (16 participants per condition), our work provides a more holistic picture by studying what (else) might contribute to reliance and how the identified features affect a wider range of variables including people's"}, {"title": "2.3 Sources", "content": "The final feature of LLM responses that we study is the presence of sources, i.e., clickable links to external material. Sources are increasingly provided by LLM-infused applications, including general-purpose chatbots (e.g., ChatGPT, Gemini) and search engines (e.g., Perplexity AI, Copilot in Bing, SearchGPT). Sources are commonly sought by users, as found in prior work [64] and supported in our studies. Similar to explanations, however, sources in LLM responses can be flawed in various ways [2, 77]. For instance, Liu et al. [77] conducted a human evaluation of popular LLM-infused search engines and found that their responses frequently contain inaccurate sources and unsupported statements. Alkaissi and McFarlane [2] conducted a case study of ChatGPT in the medical domain and found that it generates fake sources. These issues were observed in our studies as well. Currently there is active research on techniques such as Retrieval Augmented Generation (RAG) [38, 72] to help LLMs provide more accurate information and sources.\nIt is well known that the presence and quality of sources impact how credible people find given content in other settings [98, 118]. However, there has been little work studying how people make use of and rely on sources in the context of LLM-infused applications. On the one hand, the presence of sources might reduce overreliance if people click on the provided links to verify the accuracy of the LLM's response. On the other hand, the presence of sources might increase reliance if people interpret them as signs of credibility and defer to the system without verifying the answers themselves. Indeed, in one study of uncertainty communication in LLM-infused search, participants were found to rarely click on source links [60]. Through a large-scale, pre-registered, controlled experiment (Section 4), we study how the presence of clickable sources impacts people's reliance, task accuracy, and other measures, and how this interacts with the presence of explanations and inconsistencies. In our studies, we use realistic explanations and sources, generated by state-of-the-art LLM-infused applications ChatGPT and Perplexity AI, and provide insights for fostering appropriate reliance on LLMs."}, {"title": "3 Study 1: Think-Aloud Study", "content": "Towards the goal of identifying features of LLM responses that can help foster appropriate reliance, we first take a bottom-up approach and conduct a think-aloud study in a relatively natural setting. Specifically, we observe how participants solve question-answering tasks with ChatGPT in multi-turn interactions, and explore how they perceive ChatGPT's responses and what helps them arrive at correct answers despite incorrect answers from ChatGPT."}, {"title": "3.1 Study 1 Methods", "content": "In this section, we describe our study methods, all of which were reviewed and approved by our Institutional Review Board (IRB) prior to conducting the study."}, {"title": "3.1.1 Procedure.", "content": "The study session had two parts. In Part 1 (Base), participants were introduced to the study and asked to complete three question-answering tasks while thinking aloud. Each task involved determining the correct answer to an objective question using ChatGPT\u00b3 and reporting confidence in their final answer on a 1-7 scale. As in natural settings, participants could exchange as many messages with ChatGPT as they wished. Participants could also check the sources provided in ChatGPT's responses, but were asked not to conduct their own internet search.\nEach participant was given three questions: a general domain factual question (e.g., \"Has Paris hosted the Summer Olympics more times than Tokyo?\"), a health or legal domain factual question (e.g., \"Is it illegal to collect rainwater in Colorado?\"), and a math question (e.g., \"Sue puts one grain of rice on the first square of a Go board and puts double the amount on the next square. How many grains of rice does Sue put on the last square?\"). The factual questions were binary questions. The math questions were not binary, but had one correct"}, {"title": "3.1.2 Participant recruitment and selection.", "content": "To recruit participants, we posted a screening survey on Mastodon, X (previously Twitter), and various mailing lists and Slack workspaces within and outside the first author's institution. The survey included questions about the respondent's knowledge and use of LLMs. Based on the survey responses, we selectively enrolled participants to maximize the diversity of the study sample's LLM background. See below for a summary of participants' knowledge and use of LLMs. We manually reassigned two participants to different categories than what they selected in their survey when their survey responses did not line up with their described experience (high to low knowledge for one participant and low to high knowledge for another). We refer to individual participants by identifier P#.\n\u2022 Low-knowledge: \"Slightly familiar, I have heard of them or have some idea of what they are\" (P6, P9, P13, P15) or \"Moderately familiar, I know what they are and can explain\" (P2, P3, P11, P14).\n\u2022 High-knowledge: \"Very familiar, I have technical knowledge of what they are and how they work\" (P1, P4, P8, P10, P16) or \"Extremely familiar, I consider myself an expert on them\" (P5, P7, P12).\n\u2022 Low-use: \"Never\" use LLMs (P5, P13, P15, P16) or use LLMs \"Rarely, about 1-2 times a month\" (P4) or use LLMs \"Sometimes, about 3-4 times a month\" (P3, P6, P8).\n\u2022 High-use: Use LLMs \"Always, about once or more a day\" (P1, P2, P7, P9, P10, P11, P12, P14)."}, {"title": "3.1.3 Conducting and analyzing studies.", "content": "We collected data from 16 participants in June 2024, each over a Zoom video call. The study lasted one hour on average, and participants were paid $20 for their participation. All sessions were video recorded and transcribed for data analysis. We used a mix of quantitative and qualitative methods to analyze the study data. On the quantitative side, we analyzed the accuracy of participants' answers and their self-reported confidence in their answers measured on a 1-7 scale for each task. Since each participant solved three tasks, once in Part 1 and again in Part 2, there are 6 accuracy and 6 confidence numbers for each participant. On the qualitative side, we conducted a thematic analysis [10, 12] of participants' think-aloud data and their responses to interview questions to identify features of LLM responses that shaped participants' reliance. The first author performed the initial coding, discussed the categories with other authors, and then refined the coding."}, {"title": "3.2 Study 1 Results", "content": "We first provide some descriptive statistics about participants' accuracy, over- and underreliance, and confidence across the two parts of the study (Section 3.2.1). We then discuss which LLM response features participants reported as influences on their reliance (Section 3.2.2). We emphasize that this study was not intended to provide statistically significant results, but to identify features that may help foster appropriate reliance. Given the small sample size, we report the quantitative results only to provide context."}, {"title": "3.2.1 Accuracy, reliance, and confidence.", "content": "In Part 1 (Base), we collected data on 48 task instances (16 participants \u00d7 3 tasks). For 34 of these instances, ChatGPT gave a correct answer in its first response. (ChatGPT sometimes changed its answer over the course of the interaction, either due to stochasticity or in response to participants' follow-up messages.) Among these, participants' final answer agreed with ChatGPT's correct answer in 33 instances (average confidence 5.97 on the 1-7 scale) and disagreed in only a single instance (confidence 4.5), indicating that underreliance was not prevalent. In 13 instances, ChatGPT gave an incorrect answer in its first response. Among these, participants' final answer agreed with ChatGPT's incorrect answer in 9 instances (average confidence 6.15) and disagreed in only 4 instances (average confidence 5.61), indicating widespread overreliance. In a single instance, ChatGPT did not answer the question in its first response, and the participant submitted an incorrect answer with a confidence of 2.\nWe did not find meaningful differences in participants' accuracy between the two parts of the study. That is, follow-up prompting did not increase participants' accuracy, at least based on our small sample of quantitative data. For 44 out of 47 instances in which the participant completed Part 2 (Prompting) (one participant had to skip a task instance due to lack of time), the participant submitted the same answer in both parts. In 3 instances, participants submitted an incorrect answer in Part 1 and a correct answer in Part 2. In 2 of these 3 instances, ChatGPT gave an incorrect answer in Part 1, but gave a correct answer in Part 2. In the other instance, ChatGPT gave incorrect answers in both parts, but the participant arrived at the correct answer in Part 2 after engaging in multiple rounds of interaction with ChatGPT.\nFinally, we compared participants' confidence in their answers for the same task between the two parts, finding that it increased in Part 2 in 19 instances, decreased in 8 instances, and stayed the same in 20 instances. However, changes in confidence do not correspond to changes in answers. As mentioned above, participants changed their answers in only 3 out of 47 instances. In these 3 instances, participants' confidence stayed the same or increased slightly as their answer changed from being incorrect to correct. Participants' self-described reasons for increased confidence included seeing and checking sources, seeing ChatGPT give the same answer multiple times, and receiving more information in general. Reasons for decreased confidence included experiencing issues with sources (e.g., links were broken or sources were not reputable) and seeing ChatGPT change answers."}, {"title": "3.2.2 LLM response features shaping reliance.", "content": "From a thematic analysis of participants' think-aloud data and responses to interview questions, we found explanations, inconsistencies, and sources to be key features of LLM responses that participants reported as influences on reliance. First, consistent with our discussion in Sections 1 and 2.2, we observed that ChatGPT provided explanations of its answers by default. Participants found these explanations important for judging the reliability of ChatGPT's answers. For example, P14 (low-knowledge, high-use) described explanations as \u201cvery important for having reliability on the answer\u201d and said \u201cthe more explanation it [ChatGPT] can provide me about the answer [...] the more I would be able to rely on it.\u201d P11 (high-knowledge, high-use) added that they judge the response by \u201chow well ChatGPT explains the answer.\u201d This participant judged ChatGPT's explanation in one task to be very high quality, noting \u201cI would put this on my homework and submit it [...] the quality is very high\u201d.\nHowever, in another task, P11 submitted a different answer from ChatGPT after observing inconsistencies: \u201cSince it [ChatGPT] doesn't answer these simple questions consistently, I don't trust it as much.\u201d Sometimes inconsistencies occurred within a response (e.g., ChatGPT saying Paris hosted the Summer Olympics more times than Tokyo while also saying both have hosted twice). At other"}, {"title": "4 Study 2: Large-scale, Pre-registered, Controlled Experiment", "content": "Based on the insights from Study 1, we designed a large-scale, pre-registered, controlled experiment to study the effects of different features of LLM responses on people's reliance, task accuracy, and other measures including confidence, source clicking behavior, time on task, evaluation of LLM responses, and asking of follow-up questions. The goal of the study was to test whether the findings from Study 1 apply at scale and identify which combinations of features help people achieve appropriate reliance and high task accuracy."}, {"title": "4.1 Study 2 Methods", "content": "In this section, we describe our study methods. Before collecting data, we obtained IRB approval and pre-registered our experimental design, analysis plan, and data collection procedures.\u2074"}, {"title": "4.1.1 Procedure.", "content": "We designed a within-subjects experiment in which participants completed a set of question-answering tasks with LLM responses. Each task involved determining the correct answer to a binary factual question with access to a response from a hypothetical LLM named \"Theta\" (hereafter we occasionally refer to it as \"the LLM\"). See Figure 2 for an example. Our experiment had a 2 x 2 x 2 design where we varied three variables in Theta's responses: accuracy of Theta's answer to the question (correct/incorrect), presence of an explanation (absent/present), and presence of clickable sources (absent/present). In total, there were 8 types of responses. Participants completed 8 tasks in the experiment and saw one of each type. This makes Theta's accuracy 50%, but participants were not given this information: participants did not receive feedback on whether their answer or Theta's answer was correct after solving a task. See Figure 3 for examples of different types of responses.\nThe experiment had three parts. In the first part, participants were introduced to the study and to Theta. Theta was described as an LLM-based Al system prototype that uses similar technology to OpenAI's ChatGPT, is connected to the internet, and can answer a wide range of questions. In the second part, participants answered a total of eight questions. For each question, participants were provided with a response from Theta and were asked to submit their answer, report their confidence in their answer, and rate Theta's response. They were told that they could click on source links in Theta's responses, but asked not to conduct their own internet search. Participants could also optionally write a follow-up question, but they did not see Theta's response to it. We made this choice to fully control the number and content of responses, while being able to collect data on when and what types of follow-up questions participants ask. We acknowledge that showing one controlled response instead of allowing free-form interaction has limitations (see Section 5.3). However, we adopt this method from prior work studying LLMs [60, 71, 109] as a valid approach for capturing user perceptions and behaviors around LLM responses with the advantage of controlling unwanted noise from free-form interactions (for instance, LLMs making different mistakes across participants in follow-up interactions).\nWe randomized the order in which questions were presented, as well as the assignment of the 8 response types to the questions. In the final part, participants filled out an exit questionnaire about their experience with and perception of Theta, their background on LLMs, and basic demographic information. Lastly, participants"}, {"title": "4.1.2 Dependent Variables.", "content": "We formed a set of dependent variables (DVs) using a mix of behavioral and self-reported measures to capture participants' reliance and accuracy, as well as related behaviors and judgments. First, we measured the agreement between a participant's answer and that of Theta; this is a commonly used behavioral measure of reliance [14, 19, 69, 76, 84, 89, 128, 132]. Second, we measured the accuracy of a participant's answer to assess the task outcome. These are our main two DVs. To complement them, we also examined participants' confidence and source clicking behavior as indirect measures of reliance, as well as time on task, since efficiency is also an important aspect of task outcome. These complementary measures have also been commonly studied in prior work [19, 22, 60, 61, 84, 97, 113].\nAdditionally, we had participants evaluate the individual LLM responses. First, we had participants evaluate the justification quality of a response, i.e., whether it offers a good justification for its answer. Based on prior work in psychology, we expected this to be correlated with reliance and confidence [29, 81], as well as whether participants ask follow-up questions [36, 75]. Second, we had participants evaluate the actionability of a response, as incorrect responses or responses with low justification quality can still be useful if they are actionable; recall that in Study 1, we observed that participants often treated an LLM response as a starting point for determining what action to take next to arrive at the correct answer. Finally, we measured whether participants wrote a follow-up question they would like to ask to Theta. This is in part a proxy for satisfaction: prior work in psychology has found that children are less likely to re-ask a question when they are satisfied with an initial response [11, 65, 88]. On the other hand, greater satisfaction with a response can increase curiosity about related content [75]. Formally, we measured the following DVs based on participants' observed behavior:\n\u2022 Agreement: TRUE if the participant's final answer is the same as Theta's answer; FALSE otherwise.\n\u2022 Accuracy: TRUE if the participant's final answer is correct; FALSE otherwise.\n\u2022 SourceClick: TRUE if the participant clicked on one or more sources; FALSE otherwise.\n\u2022 Time: Number of minutes from when the participant saw the question to when they clicked next.\nWe additionally measured the following DVs based on participants' self-reported ratings or selections:\n\u2022 Confidence: Rating on the question \"How confident are you in your answer?\" on a 7-point scale.\n\u2022 Justification Quality: Rating on the statement \"Theta's response offers good justification for its answer\" on a 7-point scale.\n\u2022 Actionability: Rating on the statement \"Theta's response includes information that helps me determine what my final answer should be\" on a 7-point scale.\n\u2022 Followup: TRUE if the participant wrote a follow-up question they would like to ask instead of selecting \"I'm satisfied with the current response and would not ask a follow-up question.\"\nAll DVs were measured once for each of the 8 tasks. See Figure 2 for screenshots of an example task."}, {"title": "4.1.3 Analysis.", "content": "We hypothesized that the three features of LLM responses that we manipulated - the accuracy of the answer, the presence of sources, and the presence of an explanation - would affect each of the DVs. To examine this hypothesis, we used a mixed-effects regression model (logistic or linear depending on the data type), where each participant has a unique ID and each task question has a unique ID. Specifically, for each DV except SourceClick, we fit the model  $DV \\sim AI\\_Correct * AI\\_Sources * AI\\_Explanation + (1 | participant) + (1|question)$. For SourceClick, we fit the model  $DV \\sim AI\\_Correct * AI\\_Explanation + (1 | participant) + (1|question)$ only looking at data points for which participants were provided with sources. AI_Correct, AI_Sources, and AI_Explanation are binary variables with Correct Answer, No Sources, and No Explanation as the reference levels.\nWe complemented the main analysis with several additional analyses. First, we conducted two pre-registered analyses exploring how participants reacted to inconsistencies in explanations (Section 4.3.1) and how participants' source clicking behavior relates to other DVs (Section 4.3.2). Analysis details and results are presented in the respective sections. Second, we conducted a thematic analysis [10, 12] of participants' free-form answers in the exit questionnaire. The results are presented in Section 4.2 alongside the quantitative results from the main analysis."}, {"title": "4.1.4 Materials.", "content": "To simulate a realistic LLM usage scenario of users seeking answers to questions they don't know the answer to, we selected task questions according to the following criteria: (1) most lay people should not know the answer off the top of their head so that they will likely engage with the LLM response and (2) the answer can be objectively and automatically assessed. To satisfy the criteria, we first created 32 binary factual questions based on facts from the books Weird But True Human Body [58] and Weird But True World 2024 [59] by National Geographic Kids. We then ran a short pilot study (N = 50) in which we asked participants to answer the 32 questions based on their knowledge and without consulting external sources. This allowed us to assess how commonly known the answers to the questions are in our sample. We selected questions with less than 50% accuracy (i.e., worse than random guessing) as our final set of task questions (12 in total) to satisfy our first selection criterion. However, we acknowledge that focusing on difficult questions may affect the generalizability of our results. See Figures 2 and 3 for an example question and the appendix for the full set.\nTo create LLM responses that are realistic and reflect the state-of-the-art, we used ChatGPT-40 with a Plus subscription and with Browsing allowed, Memory disallowed, and a new chat for each prompt. Initially, we inputted the selected task questions to ChatGPT without any system prompts. Consistent with prior work [71], we observed that ChatGPT's responses generally follow the same"}, {"title": "4.1.5 Participants.", "content": "We aimed to collect a minimum of 300 responses post-exclusions. This number was determined based on a power analysis on pilot data using the simR package in R [42]. We conducted data collection using Qualtrics and Prolific in August 2024. Specifically, we collected responses from 320 U.S.-based adults on Prolific who had completed at least 100 prior tasks with a 95% or higher approval rating. We excluded 12 responses (3.75%) based on three pre-registered exclusion criteria (3 for response time under 5 minutes, 9 for less than 80% accuracy on the post-task attention check, and 1 for off-topic free-form response; 1 response was caught on multiple criteria). Our final sample consists of 308 responses. Regardless of inclusion or exclusion in the final sample for analysis, we paid all participants $3.75. The median study duration was 15.3 minutes, so on average, participants were paid $14.70 per hour. See the appendix for more information about participants."}, {"title": "4.2 Study 2 Results: Main Analysis", "content": "We begin with the main analysis results. We report the raw data means (M) and standard deviations in Table 1 and the regression results (\u03b2, SE, p) in the text. We use significance to refer to statistical significance at the level of p < .05. Recall that we fit mixed-effects regression models with three variables and all possible interactions (see Section 4.1.3 for details). We did not find a significant three-way interaction for any DVs. Given our interest in the effects of explanation and sources in LLM responses, we report significant main effects and two-way interactions in the following order: main effects of explanation and interactions with LLM accuracy (Section 4.2.1), main effects of sources and interactions with LLM accuracy (Section 4.2.2), interactions between explanation and sources (Section 4.2.3), and additional effects of LLM accuracy (Section 4.2.4)."}, {"title": "4.2.1 Main effects of explanation and interactions with LLM accuracy.", "content": "We find a significant main effect of explanation on most DVs (all except SourceClick and Time). Specifically, provided that the LLM answer is correct and there are no sources, providing an explanation leads to higher participant agreement with the LLM answer (M = 78.2% vs. 67.2%, \u03b2 = .60, SE = .19, p = .002), accuracy (M = 78.2% vs. 67.2%, \u03b2 = .65, SE = .19, p < .001), confidence in the final answer (M = 5.26 vs. 4.55, \u03b2 = .74, SE = .10, p < .001), rating of the LLM response's justification quality (M = 5.52 vs. 2.58, \u03b2 = 2.94, SE = .13, p < .001), and rating of its actionability (M = 5.14 vs. 2.56, \u03b2 = 2.59, SE = .13, p < .001). On the other hand, the likelihood of asking a follow-up question is lower when an explanation is provided (M = 28.2% vs. 71.4%, \u03b2 = -2.38, SE = .21, p < .001).\nFor participants' accuracy, however, we find a significant interaction between the presence of an explanation and the accuracy of the LLM answer (\u03b2 = -1.00, SE = .28, p < .001). In the absence of sources, when the LLM answer is correct, participants' accuracy is higher when an explanation is provided (M = 78.2% vs. 67.2%). In contrast, when the LLM answer is incorrect"}]}