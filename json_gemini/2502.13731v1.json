{"title": "Robust Counterfactual Inference in Markov Decision Processes", "authors": ["Jessica Lally", "Milad Kazemi", "Nicola Paoletti"], "abstract": "This paper addresses a key limitation in existing counterfactual inference methods for Markov Decision Processes (MDPs). Current approaches assume a specific causal model to make counterfactuals identifiable. However, there are usually many causal models that align with the observational and interventional distributions of an MDP, each yielding different counterfactual distributions, so fixing a particular causal model limits the validity (and usefulness) of counterfactual inference. We propose a novel non-parametric approach that computes tight bounds on counterfactual transition probabilities across all compatible causal models. Unlike previous methods that require solving prohibitively large optimisation problems (with variables that grow exponentially in the size of the MDP), our approach provides closed-form expressions for these bounds, making computation highly efficient and scalable for non-trivial MDPs. Once such an interval counterfactual MDP is constructed, our method identifies robust counterfactual policies that optimise the worst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate our method on various case studies, demonstrating improved robustness over existing methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Markov Decision Processes (MDPs) are a fundamental model of computation for representing sequential decision-making processes under uncertainty, including reinforcement learning (RL) problems. However, evaluating RL-learnt policies can be challenging, especially in safety-critical domains like healthcare, where testing these policies directly on patients would be risky and unethical.\nCounterfactual inference of MDPs enables offline policy evaluation, i.e., without \"deploying\" the alternative policy into the environment. Given an observed sequence of actions and outcomes (e.g., from an existing clinician's policy), counterfactual inference estimates what the outcome would have been if different actions had been taken. Counterfactual outcomes yielding higher rewards than the observation can serve as explanations for how the observed policy could be improved.\nThere is a rapidly growing body of literature on counterfactual inference for MDPs [Oberst and Sontag, 2019, Lorberbom et al., 2021, Tsirtsis et al., 2021, Benz and Rodriguez, 2022, Noorbakhsh and Rodriguez, 2022, Killian et al., 2022, Zhu et al., 2020, Tsirtsis and Rodriguez, 2024]. Most of these papers assume a specific causal model of the system (i.e., the Gumbel-max structural causal model (SCM) [Oberst and Sontag, 2019]) to identify counterfactuals, but, in general, counterfactuals in MDPs are non-identifiable: many causal models would fit the given observational and interventional data, each yielding different counterfactual probabilities [Zhang et al., 2022]. In the MDP context, the observational distribution will be a single observed path of the MDP, and the interventional distribution is the MDP's transition probabilities for a given state and action. As a result, any counterfactual analysis based on a presumed causal model may not be valid, which is particularly concerning in safety-critical domains.\nPartial counterfactual inference methods derive bounds (as opposed to sharp values) on counterfactual probabilities, accounting for all causal models compatible with given observational and interventional data, avoiding reliance on a fixed (and possibly erroneous) model. Our work is inspired by the canonical SCM approach developed by Zhang et al. [2022], which formulates partial counterfactual inference as an optimisation problem. However, this optimisation procedure is very inefficient even for small causal graphs (with variables and constraints that are exponential in the size of the MDP), which has encouraged the development of approximate algorithms [Zhang et al., 2022, Duarte et al.,"}, {"title": "2 BACKGROUND", "content": "In this section, we provide background on counterfactual inference and an overview of related work on counterfactual inference in MDPs and partial counterfactual inference.\nMarkov Decision Processes MDPs are a class of stochastic models for representing sequential decision-making processes. In an MDP M, at each step t, an agent in state $s_t$ performs some action $a_t$ determined by a policy $\\pi$, ending up in state $s_{t+1} \\sim P(. | s_t,a_t)$ and receiving reward $R(s_t, a_t)$. Formally, an MDP is a tuple $M = (S, A, P, P_1, R)$ where S is the discrete state space, A is the set of actions, $P : (S \\times A \\times S) \\rightarrow [0, 1]$ is the transition probability function, $P_1 : S \\rightarrow [0,1]$ is the initial state distribution, and $R : (S \\times A) \\rightarrow \\mathbb{R}$ is the reward function. A (deterministic) policy for M is a function $\\pi: S \\rightarrow A$. A path $\\tau$ of M under policy $\\pi$ is a sequence $\\tau = (s_0, a_0),...,(s_{T-1},a_{T-1})$ where $T = |\\tau|$ is the path length, $P_1(s_0) > 0$, $a_t = \\pi(s_t)$ for all $t = 0, ..., T - 1$, and $P(s_{t+1} | s_t, a_t) > 0$ for all $t = 0, . . ., T - 2$.\nCounterfactual Inference Structural causal models (SCMs) [Pearl, 2009, Halpern and Pearl, 2005] provide a mathematical framework for causal inference. Formally, a SCM is a tuple $C = (U, V, F, P(U))$, where V is the set of endogenous (observed) variables, U is a set of exogenous (unobserved) variables, $P(U)$ is a joint distribution over the possible values of each $U \\in U$, and F is a set of structural equations where each $f_v \\in F$ determines the value of endogenous variable V, given a fixed realisation of $U_v \\in U$ and set of direct causes (parents) $PA_y \\subset V^1$.\nUsing SCMs, we can apply an intervention $X \\leftarrow x$ on an endogenous variable X, and measure the effect on the value of some other endogenous variable Y. We can also perform counterfactual inference to estimate what causal effect applying an intervention in the past would have had on an observation. This involves inferring the value of the exogenous variables present in the observation v, i.e., $P(U | v)$, then evaluating the structural equations, replacing $P(U)$ with the inferred $P(U | v)$, and applying the intervention. Counterfactual inference can be challenging because many SCMs may fit the observed and interventional distributions but yield different counterfactual probabilities [Zhang et al., 2022]. As a result, many approaches require additional assumptions about the causal model for real-world application.\nCounterfactual Inference in MDPs The Gumbel-max SCM was developed by Oberst and Sontag [2019] to perform counterfactual inference in MDPs and has been widely used in existing work [Lorberbom et al., 2021, Benz and Rodriguez, 2022, Noorbakhsh and Rodriguez, 2022, Killian et al., 2022, Zhu et al., 2020, Kazemi et al., 2025, 2024] due to its desirable property of counterfactual stability (see Definition 3.3). The Gumbel-max SCM for an MDP is expressed as:\n$S_{t+1} = f(S_t, A_t, U_t = (G_{s,t})_{s\\in S})$\n$= arg max {log (P_M(s | S_t, A_t)) + G_{s,t}}$                                                                                                             (1)\nwhere $P_M$ is the MDP's transition probabilities, $S_t, A_t$ and $S_{t+1}$ are endogenous variables representing the state, action and next state, and the values of the exogenous variable $U_t = (G_{s,t})_{s\\in S}$ are sampled from the standard Gumbel distribution. In this model, the Gumbel values $G_{s,t}$ represent the exogenous noise. We can perform (approximate) posterior inference of $P((G_{s,t})_{s\\in S} | S_t,a_t, S_{t+1})$ through"}, {"title": "3 PARTIAL COUNTERFACTUAL INFERENCE VIA CANONICAL SCMS", "content": "Zhang et al. [2022] introduced a family of canonical SCMs that can model all possible counterfactual distributions for any causal graph. Their partial counterfactual inference approach optimises over these canonical SCMs to identify counterfactual probability bounds. In this section, we explain how their approach can be applied to MDPs, and how additional assumptions can be incorporated to ensure counterfactuals are realistic, addressing issues in existing work.\n3.1 OPTIMISATION PROCEDURE\nMDPs can be represented as SCMs with the following structural equations:\n$S_{t+1} = f(S_t, A_t, U_t); A_t = \\pi(S_t); S_0 = f_0(U_0)$                                                                                                                               (3)\nWe can convert this SCM to its equivalent canonical SCM to perform partial counterfactual inference. To this end, we need to find the c-component of the exogenous variable $U_t$ in the MDP causal graph (see Figure 1), which is defined as follows:\nDefinition 3.1 (c-component [Tian and Pearl, 2002]) Given a causal graph G, a subset of its endogenous variables $C\\subset V$ is a c-component if any two variables $V_i, V_j \\in C$ are connected by a sequence of bi-directed edges $V_i \\leftarrow U_k and V_j \\leftarrow U_k$, where each $U_k$ is an exogenous parent shared by $V_i$ and $V_j$.\nDefinition 3.2 (Canonical SCM [Zhang et al., 2022]) A canonical SCM is a tuple $C = (V,U,F,P)$, where:\n1. For every endogenous variable $V\\in V$, its values $v \\in V$ are determined by a structural equation $v \\leftarrow f_v(pa_v, u_v)$ where, for any $pa_v$ and $u_v$, $f_v(pa_v, u_v)$ is contained within a finite domain $\\Omega_v$."}, {"title": "3.2 INCORPORATING EXTRA ASSUMPTIONS", "content": "While this optimisation correctly considers all SCMs consistent with the interventional and observational data, this can result in wide or trivial [0, 1] bounds. We now explain how assumptions can be added to the optimisation problem to tighten the counterfactual probability bounds, and produce more realistic counterfactual probabilities. The first assumption is counterfactual stability [Oberst and Sontag, 2019], which, in the context of an MDP, is defined as:\nDefinition 3.3 (Counterfactual stability) An MDP SCM (3) satisfies counterfactual stability if, given we observed the next state $S_{t+1} = s_{t+1}$ after observing the state-action pair $(s_t, a_t)$, the counterfactual outcome under a different state-action pair $(\\tilde{s}, \\tilde{a})$ will not change to some $S_{t+1} = \\tilde{s}$:  \n$P(S_{t+1} | \\tilde{s}, \\tilde{a})$\n$\\tilde{s}' \\neq s_{t+1}$ unless\n\nHowever, even with counterfactual stability, the Gumbel-max SCM yields unrealistic counterfactual probabilities. In the example in Figure 2, the counterfactual probability of transition $s = 1,a = 0 \\rightarrow s' = 2$ (highlighted in Table 2) is greater than its nominal probability, even though state $s' = 2$ was reachable from the observed state $s_t = 0$, but not observed (see Appendix A for further discussion). Arguably, a state that was possible but did not unfold in the factual world should not be more likely in the counterfactual world. To formalise this, we introduce monotonicity:"}, {"title": "4 DERIVING ANALYTICAL BOUNDS", "content": "While the optimisation in (4) and (5) yields exact bounds, it is very inefficient as the number of constraints grows exponentially with the sizes of the state and action spaces. However, for MDPs, we have proven this linear optimisation problem always reduces to exact closed-form solutions.\n4.1 ANALYTICAL BOUNDS\nGiven the observed transition $s_t, a_t \\rightarrow s_{t+1}$, the counterfactual probability of any transition $\\tilde{s}, \\tilde{a} \\rightarrow \\tilde{s}'$ depends only on whether the state-action pair $(\\tilde{s}, \\tilde{a})$ is equal to the observed state-action pair $(s_t, a_t)$, or if the support of its nominal transition probabilities, $P(\\cdot | \\tilde{s}, \\tilde{a})$, is disjoint from or overlaps with the support of the observed state-action pair, $P(\\cdot | s_t, a_t)$. The lower and upper bounds, denoted by $P^{LB}$ and $P^{UB}$, for these cases are given in Theorems 4.1-4.3 (with proofs provided in Appendix B.1).\nTheorem 4.1 For the observed state-action pair $(s_t,a_t)$, the linear program will produce the following bounds:\n$P^{LB}(s_{t+1} | s_t, a_t) = P^{UB}(s_{t+1} | s_t, a_t) = 1$\n$\\forall \\tilde{s}' \\in S\\backslash{s_{t+1}}, P^{LB}(\\tilde{s}' | s_t, a_t) = P^{UB}(\\tilde{s}' | s_t, a_t) = 0$"}, {"title": "4.2 EQUIVALENCE WITH EXISTING WORK", "content": "The most similar work to ours is by Li and Pearl [2024], who derived tight counterfactual probability bounds for categorical models, given observational and interventional distributions. In contrast, our bounds are defined for one specific observation. In Appendix C, we prove our bounds are equivalent, but only when the counterfactual stability and monotonicity assumptions are removed, or equivalently, when the counterfactual state-action pair has disjoint support with the observed state-action pair. This equivalence"}, {"title": "5 METHODOLOGY", "content": "With our analytical bounds, we can compute a non-stationary interval counterfactual MDP for any MDP M and observed path $\\tau$. Formally, an interval Markov decision process (IMDP) [Givan et al., 2000] is a tuple $(S, A, P_I, R)$, extending MDPs with an uncertain transition probability function $P_I$ that maps each transition to a probability within its bounds $P_I(s' | s,a) = [P^{LB}(s' | s,a), P^{UB}(s' | s, a)]$. To construct an interval counterfactual MDP, we compute the counterfactual probability bounds for every transition in the MDP, given every observed transition in $\\tau$.\nDefinition 5.1 (Interval Counterfactual MDP (ICFMDP)) Given an observed path $\\tau$ and MDP M, the counterfactual probability for each transition in the interval counterfactual MDP (ICFMDP) $M_{\\tau}$ is defined, for $t = 0, . . ., T - 1$, as $P(\\tilde{s}' | \\tilde{s},\\tilde{a}) = [P^{LB}(\\tilde{s}' | \\tilde{s},\\tilde{a}), P^{UB}(\\tilde{s}' | \\tilde{s},\\tilde{a})]$.\nTo derive optimal counterfactual policies for this ICFMDP, we apply robust value iteration [Mathiesen et al., 2024], which pessimistically optimises the expected reward\n$V^*(s) = max min E_{s'\\sim P_I(\\cdot|s,\\pi(s))} [R(s, \\pi(s)) + V^*(s')]$.\nIn Appendix B.2, we prove that any CFMDP entailed by the ICFMDP will be valid (i.e., satisfying (4) + (5)), so this policy will be robust across all possible causal models (i.e., the ICFMDP does not introduce spurious models)."}, {"title": "6 EVALUATION", "content": "We apply our methodology to derive counterfactual policies for various MDPs, addressing three main research questions: (1) how does our policy's performance compare to the Gumbel-max SCM approach; (2) how do the counterfactual stability and monotonicity assumptions impact the probability bounds; and (3) how fast is our approach compared with the Gumbel-max SCM method?\n6.1 EXPERIMENTAL SETUP\nTo compare policy performance, we measure the average rewards of counterfactual paths induced by our policy and the Gumbel-max policy by uniformly sampling 200 counterfactual MDPs from the ICFMDP and generating 10,000 counterfactual paths over each sampled CFMDP. Since the interval CFMDP depends on the observed path, we select 4 paths of varying optimality to evaluate how the observed path impacts the performance of both policies: an optimal path, a slightly suboptimal path that could reach the optimal reward with a few changes, a catastrophic path that enters a catastrophic, terminal state with low reward, and an almost catastrophic path that was close to entering a catastrophic state. When measuring the average probability bound widths and execution time needed to generate the ICFMDPs, we averaged over 20 randomly generated observed paths 7."}, {"title": "6.2 GRIDWORLD", "content": "The GridWorld MDP is a 4 \u00d7 4 grid where an agent must navigate from the top-left corner to the goal state in the bottom-right corner, avoiding a dangerous terminal state in the centre. At each time step, the agent can move up, down, left, or right, but there is a small probability (controlled by hyper-parameter p) of moving in an unintended direction. As the agent nears the goal, the reward for each state increases, culminating in a reward of +100 for reaching the goal. Entering the dangerous state results in a penalty of -100. We use two versions of GridWorld: a less stochastic version with p = 0.9 (i.e., 90% chance of moving in the chosen direction) and a more stochastic version with p = 0.4.\nGridWorld (p = 0.9) When p = 0.9, the counterfactual probability bounds are typically narrow (see Table 3 for average measurements). Consequently, as shown in Figure 3, both policies are nearly identical and perform similarly well across the optimal, slightly suboptimal, and catastrophic paths. However, for the almost catastrophic path, the interval CFMDP path is more conservative and follows the observed path more closely (as this is where the probability bounds are narrowest), which typically requires one additional step to reach the goal state than the Gumbel-max SCM policy.\nGridWorld (p = 0.4) When p = 0.4, the GridWorld environment becomes more uncertain, increasing the risk of entering the dangerous state even if correct actions are chosen. Thus, as shown in Figure 4, the interval CFMDP policy adopts a more conservative approach, avoiding deviation from the observed policy if it cannot guarantee higher counterfactual rewards (see the slightly suboptimal and almost catastrophic paths), whereas the Gumbel-max SCM is inconsistent: it can yield higher rewards, but also much lower rewards, reflected in the wide error bars. For the catastrophic path, both policies must deviate from the observed path to achieve a higher reward and, in this case, perform similarly."}, {"title": "6.3 SEPSIS", "content": "The Sepsis MDP [Oberst and Sontag, 2019] simulates trajectories of Sepsis patients. Each state consists of four vital signs (heart rate, blood pressure, oxygen concentration, and glucose levels), categorised as low, normal, or high. and three treatments that can be toggled on/off at each time step (8 actions in total). Unlike Oberst and Sontag [2019], we scale rewards based on the number of out-of-range vital signs, between \u20131000 (patient dies) and 1000 (patient discharged). Like the GridWorld p = 0.4 experiment, the Sepsis MDP is highly uncertain, as many states are equally likely to lead to optimal and poor outcomes. Thus, as shown in Figure 5, both policies follow the observed optimal and almost catastrophic paths to guarantee rewards are no worse than the observation. However, improving the catastrophic path requires deviating from the observation. Here, the Gumbel-max SCM policy, on average, performs better than the interval CFMDP policy. But, since both policies have lower bounds clipped at -1000, neither policy reliably improves over the observation. In contrast, for the slightly suboptimal path, the interval CFMDP policy performs significantly better, shown by its higher lower bounds. Moreover, in these two cases, the worst-case counterfactual path generated by the interval CFMDP policy is better than that of the Gumbel-max SCM policy, indicating its greater robustness."}, {"title": "6.4 INTERVAL CFMDP BOUNDS", "content": "Table 3 presents the mean counterfactual probability bound widths (excluding transitions where the upper bound is 0) for each MDP, averaged over 20 observed paths. We compare the bounds under counterfactual stability (CS) and monotonicity (M) assumptions, CS alone, and no assumptions. This shows that the assumptions marginally reduce the bound widths, indicating the assumptions tighten the bounds without excluding too many causal models, as intended."}, {"title": "6.5 EXECUTION TIMES", "content": "Table 4 compares the average time needed to generate the interval CFMDP vs. the Gumbel-max SCM CFMDP for 20 observations. The GridWorld algorithms were run single-threaded, while the Sepsis experiments were run in parallel. Generating the interval CFMDP is significantly faster as it uses exact analytical bounds, whereas the Gumbel-max"}, {"title": "7 CONCLUSION", "content": "This paper addresses a key limitation in existing counterfactual inference methods for MDPs, introducing the first partial counterfactual inference approach that can efficiently handle large state and action spaces in MDPs, eliminating the need for an assumed causal model. A major contribution of this work is deriving exact analytical counterfactual probability bounds, which makes our method significantly faster than the (widely-used) Gumbel-max SCM method. Moreover, counterfactual policies derived from the interval CFMDP offer greater robustness to uncertainty about the true counterfactual MDP, especially in environments with high stochasticity. As a result, our approach provides a more reliable explanation of how an observed policy could be improved, which is crucial for safety-critical applications.\nFuture Work Like most existing methods, we assume access to the MDP's true transition probabilities and full state observability. Future work will focus on relaxing these assumptions by learning transition probabilities from data (with uncertainty bounds) and developing approaches for partially observable and continuous-state MDPs."}, {"title": "D TRAINING DETAILS", "content": "Our algorithm was executed on a 128-core machine with an Intel Xeon CPU and 512 GB RAM. The interval CFMDP generation part of the algorithm is implemented in Python 3.10, and the value iteration algorithm over the interval CFMDP was implemented in Julia 1.9.4, using the Julia Interval MDP package (v.0.2.1) [Mathiesen et al., 2024]. Most of the algorithm was run single-threaded, except for generating the Sepsis interval CFMDP and Sepsis Gumbel-max SCM CFMDP, which were run in parallel. 10 threads were needed to generate the Sepsis interval CFMDP, and 32 threads were required to calculate the Gumbel-max SCM CFMDP."}, {"title": "A EXPLANATION OF UNINTUITIVE COUNTERFACTUAL PROBABILITIES PRODUCED BY GUMBEL-MAX SCMS", "content": "Take the example MDP from Figure 2, which contains three states and a single action. In this example, we observe the transition $s_t = 0, a_t = 0 \\rightarrow s_{t+1} = 1$. Neither state 0 nor state 2 was observed, although both were possible. This means that the observed transition gives us no information as to whether state 0 or state 2 would have been more likely where, counterfactually, $s_t = 1$, so intuitively these counterfactual probabilities should be equal to the nominal probabilities. However, the Gumbel-max SCM scales the probabilities, making state 2 even more likely than in the nominal MDP, and vice versa for state 0. This happens because the Gumbel distributions that the Gumbel values are drawn from for each transition are truncated in proportion to the log probability of the transition's nominal probability (instead of in proportion to their nominal probabilities)."}, {"title": "B PROOFS", "content": "Optimisation Problem For a given counterfactual transition s, a \u2192 s' and given observed transition $s_t, a_t \\rightarrow s_{t+1}$, the optimisation problem is defined as follows:\n$min/max \\sum_{u_t=1}^{\\left | U_t \\right |} \\mu_{s,a,u_t,s'}\\cdot \\mu_{s_t,a_t,u_t,S_{t+1}}\\cdot \\theta_{u_t}$                                                                                                                                                   (6)\ns.t. $\\sum_{u_t=1}^{\\left | U_t \\right |} \\mu_{\\tilde{s},\\tilde{a}, u_t, \\tilde{s}'} \\cdot \\theta_{u_t} = P(\\tilde{s}' | \\tilde{s},\\tilde{a}), \\forall \\tilde{s},\\tilde{a}, \\tilde{s}'$                                                                                                                                                                          (7)\n$P_t(s_{t+1} | \\tilde{s}, \\tilde{a}) \\geq P(s_{t+1} | \\tilde{s}, \\tilde{a})$ if $P(s_{t+1} | \\tilde{s}, \\tilde{a}) > 0, \\forall \\tilde{s}, \\tilde{a}$ (Mon1)                                                                                                                                                                    (8)\n$P_t(\\tilde{s}' | \\tilde{s}, \\tilde{a}) < P(\\tilde{s}' | \\tilde{s}, \\tilde{a})$ if $P(\\tilde{s}' | \\tilde{s}, \\tilde{a}) > 0$ and $P(\\tilde{s}' | s_t, a_t) > 0, \\forall \\tilde{s}, \\tilde{a}, \\tilde{s}' \\neq s_{t+1}$ (Mon2)                                                                                                                                                           (9)\n$P_t(\\tilde{s}' | \\tilde{s}, \\tilde{a}) = 0$ if $\\frac{P(s_{t+1} | \\tilde{s}, \\tilde{a})}{P(s_{t+1} | s_t, a_t)} > \\frac{P(\\tilde{s}' | \\tilde{s}, \\tilde{a})}{P(\\tilde{s}' | s_t, at)}$ and $P(\\tilde{s}' | s_t, a_t) > 0, \\forall \\tilde{s}, \\tilde{a}, \\tilde{s}'$ (CS)                                                                                                                                                                (10)\n$0\\leq \\theta_{u_t} \\leq 1, \\forall u_t$                                                                                                                                                                                            (11)\n$\\sum_{u_t=1}^{\\left | U_t \\right |} \\theta_{u_t} = 1$                                                                                                                                                                                                                                    (12)\nwhere $\\theta$ and $\\mu$ are defined as follows:\n$\\theta \\in \\mathbb{R}^{\\left | U_t \\right |}$\n$\\mu \\in {0,1}^{S\\times A\\times \\left | \\Omega_{U_t} \\right |\\times S}$   \n$\\forall s \\in S, a \\in A, s' \\in S, U_t \\in U_t, \\mu_{s,a,u_t,s'} = \\begin{cases}1 & if f(s, a, u_t) = s'\\\\0 & otherwise\\end{cases}$   \nCounterfactual Probabilities For any transition s, a \u2192 s' and observed transition $s_t, a_t \\rightarrow s_{t+1}$, the counterfactual transition probability $P_t(s' | s, a)$ can be calculated as follows:\n$P_t(s' s, a) = \\frac{\\sum_{u_t=1}^{\\left | U_t \\right |} \\mu_{s,a,u_t,s'}\\cdot \\mu_{s_t,a_t,u_t,S_{t+1}}\\cdot \\theta_{u_t}}{P(S_{t+1} St, at)}$                                                                                                                                                                         (13)"}, {"title": "4 DERIVING ANALYTICAL BOUNDS", "content": "B.1 COUNTERFACTUAL TRANSITION PROBABILITY BOUNDS\nIn this section, we prove that the above optimisation problem reduces to the exact analytical bounds for counterfactual transition probabilities given in Section 4, for any MDP M.\nTheorem B.1 For any MDP M and path $\\tau$, the optimisation problem defined in (6)-(12) reduces to the exact analytical bounds given in Theorems 4.1-4.3 for the counterfactual transition probabilities of every transition in M.\nFor any state-action pair $(\\tilde{s}, \\tilde{a})$ in M, the counterfactual probabilities for all transitions from $(\\tilde{s}, \\tilde{a})$ depend only on the other transitions from $(\\tilde{s}, \\tilde{a})$, and the observed state-action pair $(s_t, a_t)$ (as shown in the equation for calculating counterfactual probabilities, (13)). Since $P(S_{t+1} | S_t, a_t)$ is fixed, the counterfactual probability of each transition $\\tilde{s}, \\tilde{a} \\rightarrow \\tilde{s}'$ only depends on\n$\\sum_{u_t=1}^{\\left | U_t \\right |} \\mu_{\\tilde{s},\\tilde{a},u_t,\\tilde{s}'} \\cdot \\mu_{s_t,a_t, u_t, S_{t+1}} \\theta_{u_t} = \\sum_{\\begin{subarray}{c}u_t \\in U_t\\\\f(S_t,a_t,u_t)=S_{t+1}\\\\f(\\tilde{s},\\tilde{a},u_t)=\\tilde{s}' \\end{subarray}} \\theta_{u_t}$"}, {"title": "B.1.4 CF Bounds for Disjoint (\u00a7, \u0101)", "content": "To enhance readability, we split the proof for Theorem 4.2 into two separate theorems: Theorem B.3 proves the upper counterfactual probability bound, and Theorem B.4 proves the lower counterfactual probability bound.\nTheorem B.3 For outgoing transitions from state-action pairs $(\\tilde{s}, \\tilde{a})$ which have completely disjoint support from the observed $(s_t, a_t)$, the linear program will produce an upper bound of:\n$P^{UB} (\\tilde{s}' |\\tilde{s}, \\tilde{a}) = \\begin{cases}\\frac{P(\\tilde{s}' | \\tilde{s}, \\tilde{a})}{P(S_{t+1} | S_t, a_t)} & if P(\\tilde{s}' | \\tilde{s}, \\tilde{a}) < P(S_{t+1} | S_t, a_t)\\\\1 & otherwise\\end{cases}$                                                                                                                                                                                                       for all next states $\\tilde{s}' \\in S$.\nProof B.2 Take an arbitrary transition $\\tilde{s},\\tilde{a} \\rightarrow \\tilde{s}'$ where $(\\tilde{s}, \\tilde{a})$ has disjoint support with the observed state-action pair $(S_t, a_t)$. By definition:\n$P^{UB}(\\tilde{s}'s, \\tilde{a}) = max_{\\theta} \\frac{\\sum_{u_t=1}^{\\left | U_t \\right |} \\mu_{\\tilde{s},\\tilde{a},u_t,\\tilde{s}'} \\mu_{s_t,a_t,u_t,S_{t+1}}\\cdot \\theta_{u_t}}{P(S_{t+1} St, at)}$\nBecause P($S_{t+1} | S_t, a_t)$ (the interventional probability of the observed transition) is fixed, to find the upper bound for $P_t(\\tilde{s}' | \\tilde{s}, \\tilde{a})$ we need to maximise $\\sum_{ut=1}^{\\left | U_t \\right |} \\mu_{\\tilde{s},\\tilde{a}, u_t,\\tilde{s}'} \\cdot \\mu_{s_t,a_t,u_t,S_{t+1}} \\cdot \\theta_{u_t}$. From Lemma B.2, we have:\n$\\sum_{ut=1}^{\\left | U_t \\right |} \\mu_{\\tilde{s},\\tilde{a}, u_t,\\tilde{s}'} \\mu_{s_t,a_t,u_t,S_{t+1}}\\cdot \\theta_{u_t} \\leq min (P(S_{t+1} | S_t, at), P(\\tilde{s}' | \\tilde{s}, \\tilde{a}))$\nTherefore, if we can show that there exists a $\\theta$ such that\n$\\sum_{ut=1}^{\\left | U_t \\right |} \\mu_{\\tilde{s},\\tilde{a}, u_t,\\tilde{s}'} \\cdot \\mu_{s_t,a_t,u_t,S_{t+1}}\\cdot \\theta_{u_t} = min ((P(s_{t+1} | S_t, at), P(\\tilde{s}' | \\tilde{s}, \\tilde{a}))$\nand $\\theta$ satisfies the constraints of the optimisation problem, we know that this results in the maximum possible counterfactual transition probability $P_t(\\tilde{s}' | \\tilde{s},\\tilde{a})$. Since $\\forall s, s' \\in S,\\forall a \\in A, \\forall u_t \\in U_t, \\mu_{s,a,u_t,s'} = 1 \\Leftrightarrow f (s,a,u_t) = s'$ by definition of $\\mu$, this is equivalent to showing that there exists a $\\theta$ such that:"}, {"title": "C EQUIVALENCE WITH EXISTING WORK", "content": "Li and Pearl [2024"}]}