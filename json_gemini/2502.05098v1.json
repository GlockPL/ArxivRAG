{"title": "Learning Temporal Invariance in Android Malware Detectors", "authors": ["Xinran Zheng", "Shuo Yang", "Edith C.H. Ngai", "Suman Jana", "Lorenzo Cavallaro"], "abstract": "Learning-based Android malware detectors degrade over time due to natural distribution drift caused by malware variants and new families. This paper systematically investigates the challenges classifiers trained with empirical risk minimization (ERM) face against such distribution shifts and attributes their shortcomings to their inability to learn stable discriminative features. Invariant learning theory offers a promising solu- tion by encouraging models to generate stable representations crossing environments that expose the instability of the train- ing set. However, the lack of prior environment labels, the diversity of drift factors, low-quality representations caused by diverse families make this task challenging. To address these issues, we propose TIF, the first temporal invariant train- ing framework for malware detection, which aims to enhance the ability of detectors to learn stable representations across time. TIF organizes environments based on application obser- vation dates to reveal temporal drift, integrating specialized multi-proxy contrastive learning and invariant gradient align- ment to generate and align environments with high-quality, stable representations. TIF can be seamlessly integrated into any learning-based detector. Experiments on a decade-long dataset show that TIF excels, particularly in early deploy- ment stages, addressing real-world needs and outperforming state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "In open and dynamic environments, even the most effective malware detectors encounter significant challenges due to natural distribution drift\u00b9, leading to performance degrada- tion [5, 37]. This degradation arises from the evolution of malware behavior and the emergence of new malware fami- lies that detectors have not previously encountered [26]. These\nFor clarity, we use the terms drift, shift, concept drift/shift, natural drift/shift, and distribution drift/shift interchangeably throughout the text. This is in contrast to adversarial drift or adversarial evasive attacks, which are caused by identifying optimal input perturbations specifically crafted to cause a model's misclassification.\nnew variants and families change the underlying statistical properties of the test samples [5, 11, 12, 27], weakening de- tectors that rely on features derived from past training data, which lack sufficient discriminative power for the new vari- ants [31].\nRecent studies have explored incremental training tech- niques to address distribution drift. These techniques involve detecting new distributions during testing [5, 37] and fre- quently updating models through active [9,27] or online learn- ing [6, 19, 36]. However, such approaches incur significant labeling and model update costs. Although pseudo-labeling has been employed to alleviate labeling burdens, it introduces noise that can lead to self-poisoning of models [19, 39], fur- ther complicating the learning process. Therefore, enhancing model robustness in drifting scenarios is essential to reduce the frequency of updates. Some studies have attempted static feature augmentation or selecting features that are less sensi- tive to malware evolution [11,40,45]. However, these methods typically require tailored selection strategies for specific fea- ture spaces and depend heavily on the stability of these spaces, limiting their adaptability to diverse drift sources. Ideally, a drift-robust detector should extract invariant features that im- prove performance and generalize to drifted samples without relying on specific feature space assumptions.\nIn this paper, we conducted a comprehensive investiga- tion into the brittleness of learning-based malware detectors against natural distribution drift. Our findings indicate that, although invariant features representing shared malicious be- haviors exist within malware families or variants, detectors trained using empirical risk minimization (ERM) tend to cap- ture unstable features instead of these invariant ones. This inefficiency arises from the inherent limitations of the ERM training paradigm, which operates under the assumption that training and testing data share the same distribution. ERM typically requires data shuffling or the construction of k-fold cross-validation sets. However, in real-world scenarios, test- ing data is often collected after the training period [27]. As a result, shuffling the training data disrupts the temporal evolu- tion of malware samples, preventing the model from learning"}, {"title": "2 Background", "content": "In this section, we review the development of malware de- tectors robust to drift and the key components of invariant learning, laying the groundwork for proposing a temporal invariant representation learning solution tailored for Android malware detection."}, {"title": "2.1 Drift-robust Malware Detectors", "content": "Existing approaches to enhancing model robustness against drift primarily focus on refining feature representations. One approach identifies features resilient to distribution changes through manual or automated engineering, enabling models to learn patterns less affected by malware evolution [45]. Another approach anticipates future distribution shifts us- ing episode-based learning [29, 42], where pseudo-unknown families are constructed in the training set to simulate low- sample tasks. Each episode replicates a specific learning sce- nario, fostering robust feature representation for open envi- ronments [25]. While effective for addressing drift from new family emergence, this method requires managing numer- ous episodes to cover all families in the training set, signifi- cantly increasing complexity. Yang et al. [40] propose feature de-entanglement techniques aiming to reduce spurious cor- relations during training, preserving invariant information. However, these methods may overlook critical feature corre- lations, such as specific API calls or intent combinations (in Android apps) directly indicative of malicious behavior and only focus on target feature space. Similarly, feature stability metrics for linear classifiers offer insights for drift robust- ness but have limited applicability to non-linear models and complex feature interactions [2].\nThese prior methods seek to enhance invariant feature in- formation by mitigating instabilities, which aligns with our approach. However, due to the diverse evolution and complex- ity of malware, pinpointing the precise cause of drift remains challenging. Strategies that rely on assumptions, such as drift\n\u00b2We will open source the dataset metadata and code repository to foster reproducibility studies."}, {"title": "2.2 Invariant Learning", "content": "Assume that the training data $D_{tr}$, is collected from multiple environments $e \\in E$, i.e., $D_{tr} = {D_e}_{e \\in E}$. Let the input space be $x \\in X$ and the target space be $y \\in Y$, and for the sample observations from each environment denoted $x, y \\sim p(x,y|e)$, the samples within the environments obey an independent and identical distribution. Suppose a classification model $f$, denoted as a composite function $f = c \\circ \\phi$. where $\\phi: X \\rightarrow H$ denotes a feature encoder that maps the input samples into a feature representation space $H$, and $q(x) \\in H$ is the \u201crepre- sentation\" of sample $x$. The $c: H \\rightarrow Y$ denotes a classifier that maps the feature representation to the logits space of $Y$."}, {"title": "2.2.1 Learning Invariant Representation", "content": "In test environments where distribution drift exists, the test data $D_{te}$ may come from a distribution $p(x,y|e_{te})$ that does not appear in the training set, i.e. $e_{te} \\notin E$. Robustness to drift yields lower error rates on unknown test data distributions.\nInvariant learning enhances the generalization ability of a model to unknown distributions by learning label distributions that are invariant across training environments. Its goal is to develop a classifier $c(\u00b7)$ that satisfies the environmental invariance constraint (EIC) [13]:\n$\\mathbb{E} [y | \\phi(x), e] = \\mathbb{E} [y | \\phi(x), e'], \\forall e,e' \\in E$, (1)\nwhere $e$ and $e'$ denote different environments to which the samples belong. This constraint is integrated into the train- ing objective through a penalty term. Thus, the goal can be formalized as:\n$\\min_{f \\in E} \\sum_{e} R_{erm} (f) + \\lambda \\cdot penalty (\\{S(f)\\}_{e \\in E})$, (2)\nwhere $R_{erm}(f) = \\mathbb{E}_{p(x,y|e)} [l(f(x), y)]$ represents the expected loss on environment $e$. Empirical risk minimization (ERM) is to minimize this expected loss within each environment. $S_e (f)$ is some statistic of the model in $e$ (see next), and the penalty is to constrain the change in this statistic to control the degree of deviation from the EIC. Optimizing this objective prevents mapping all $x$ to the same value to satisfy environmental invariance, as it encourages the predictive utility of $\\phi$ by mini- mizing empirical risk loss. In addition, the form of the penalty term is variable to achieve constraints for different objectives. Krueger et al. [20] proposed V-Rex such that $S_e (f) = R_{erm} (f)$, to minimize the variance of $S_e (f)$ in different environments. In CLOVE [33], the penalty is defined as the sum of the cali- bration errors of the model in each environment. One widely used scheme is Invariant Risk Minimization (IRM) and its practical variant IRMv1 proposed by Arjovsky et al. [3]. The"}, {"title": "2.2.2 Split Environments for Invariant Learning", "content": "Invariant learning relies on segmenting environments to high- light differences across them [8]. Early methods assumed prior knowledge of environment labels, which is often un- available in practice [1, 14, 22, 30]. Recent approaches focus on invariant learning without predefined labels. Creager et al. [13] proposes estimating environment labels via prior clus- tering before applying invariant learning (EIIL), while others explore segmentation strategies such as natural clustering us- ing dataset-provided labels or unsupervised clustering [32]. Experiments show natural clustering outperforms unsuper- vised methods. Regardless of the approach, effective segmen- tation requires environments to expose unstable information that can be ignored during invariant learning [8]."}, {"title": "3 Motivation", "content": "Learning-based malware detectors deliver outstanding perfor- mance in identifying potential threats; however, maintaining consistent performance under drift scenarios remains chal- lenging. Figure 1 supports this claim, depicting the monthly proportion changes of the top-10 dominant families in our dataset (Section 5.1.1) across the training and testing phases. Notably, some families that dominate during training, such as Dowgin, gradually disappear in the testing phase, while new families, like Artemis, emerge. This dynamic evolution poses challenges for malware detectors' generalization across fami- lies. Even for families like Airpush, present in both phases, its feature distribution is also influenced by multiple factors in- cluding temporal fluctuations in proportions and API changes."}, {"title": "3.1 Threat Model", "content": "Malware natural drift refers to the gradual evolution of mal- ware distributions over time, driven by the emergence of new"}, {"title": "3.2 Invariance in Malware Evolution", "content": "The challenges highlighted in Section 3.1 motivate the search for characteristics from training set which can be generalized to test samples. Indeed, definitions of malware families and types have inspired the exploration of such invariance by cat- egorizing malware according to code structure, behavioural patterns and malicious intent. While specific implementa- tions may vary due to evolutionary or obfuscation techniques, members of the same family typically exhibit behavioral pat- terns consistent with their overall goals. Furthermore, mal- ware types similarly represent a wide range of operational intentions that remain stable across families. These internal consistencies form the basis of invariance, which we cate- gorize as inter-family invariance and intra-family invariance: Intra-family invariance: Variants within a malware family maintain consistent operational behaviors and attack strate- gies, despite differences in implementation. Inter-family in- variance: Common malicious patterns observed across mal- ware families, such as resource abuse or evasion techniques.\nTo illustrate invariant malicious behaviors in drift scenar- ios, we select APKs from Androzoo\u00b3 and decompile them using JADX4 to obtain .java files. Our analysis focuses on core malicious behaviors in the source code. For intra-family invariance, we use versions of the Airpush family, known for intrusive ad delivery, from different periods. For inter-family invariance, we examine the Hiddad family, which shares ag- gressive ad delivery and tracking tactics but uses broader permissions, increasing privacy risks. Figure.2 shows code snippets with colored boxes highlighting invariant behaviors across samples. While Airpush uses asynchronous task re- quests, Hiddad relies on background services and scheduled tasks to evade detection.\n\u00b3https://androzoo.uni.lu\n\u2074https://github.com/skylot/jadx"}, {"title": "3.3 Failure of Learning Invariance", "content": "Let $f_i \\in \\mathbb{R}$ be a sample in the data space with label $y \\in Y = \\{0,1\\}$, where 0 represents benign software and 1 represents malware. The input feature vector $x \\in X$ includes features F extracted from $f_i$, according to predefined rules. The goal of learning-based malware detection is to train a model $M$ based on F, mapping these features into a latent space $H$ and passing them to a classifier for prediction. The process is formally described as follows:\n$\\underset{\\theta}{\\text{arg min}} R_{erm} (\\mathcal{F})$, (3)\nwhere $\\theta$ is the model parameter to be optimized and $R_{erm}(\\mathcal{F})$ represents the expected loss based on features space $\\mathcal{F}$, de- fined as:\n$R_{erm} (\\mathcal{F}) = \\mathbb{E}[l(\\hat{y},y)]$. (4)\n$l$ is a loss function. By minimizing the loss function, $M$ achieves the lowest overall malware detection error."}, {"title": "3.3.1 Stability and Discriminability of Features", "content": "To investigate the drift robustness in malware evolution from the feature perspective, we define two key feature properties: stability and discriminability. Stability refers to a feature's ability to maintain consistent relevance across distributions, while discriminability reflects a feature's capacity to effec- tively distinguish categories. To avoid model-induced biases, we propose a modelless formal definition applicable to diverse architectures.\nLet $f_j$ represent the $j$-th feature in the feature set $F$, and $S$ denote the set of all samples. To capture the behavior of feature $f_j$ under different conditions, we compute its active ratio over a subset $S' \\subseteq S$, representing how frequently or to what extent the feature is \u201cactive\u201d within that subset. Specifi- cally, for a binary feature space, feature $f_j$ takes values 0 or 1 (indicating the absence or presence of the feature, respec- tively), the active ratio of $f_j$ in the subset $S'$ is defined as the proportion of samples where $f_j$ is present, which is defined as Eq. 5:\nr(fj,S') = \\frac{1}{|S'|} \\sum_{s \\in S'}f_j(s). (5)\nThe ratio measures how frequently the feature is activated within the subset $S'$ relative to the total number of samples in the subset. At this point, we can define the stability and discriminability of features.\nDefinition 1. Stable Feature: A feature $f_j$ is defined as sta- ble if, for any sufficiently large subset of samples $S' \\subseteq S$, the active ratio $r (f_j,S')$ remains within an $\\epsilon$-bound of the overall active ratio $r (f_j,S)$ across the entire sample set, regardless of variations in sample size or composition. Formally, $f_j$ is"}, {"title": "3.3.2 Failure Due to Learning Unstable Discriminative Features", "content": "The malware detector's strong performance within the same period indicates effective learning of discriminative features to separate benign software from malware. However, perfor- mance degradation over time stems from the model's inability to capture stable discriminative features from the training set. To illustrate this, we randomly sample 110,723 benign and 20,790 malware applications from the Androzoo plat- form (2014-2021). Applications are sorted by observation date, with 2014 samples used for training and the remainder divided into 30 test intervals. We extract Drebin [4] features, covering nine behavioral categories such as hardware com- ponents, permissions, and restrict API calls, and select the top 10 discriminative features based on active ratio differ- ences to track over time. The model configuration follows DeepDrebin [17], a three-layer fully connected neural net- work with 200 neurons per layer. We evaluate performance in each interval using macro-F1 scores. As shown in Figure 3, although the top 10 discriminative features maintain stable active ratios, the detector's performance consistently declines. We further examine feature importance over time using In- tegrated Gradients (IG) with added binary noise, averaging results across five runs to ensure robustness, as recommended by Warnecke et al. [34].\nhttps://androzoo.uni.lu\nble features with fluctuating effectiveness, leading to poor generalization under drift.\napi_calls::java/lang/Runtime;->exec\nasandGET_TASKS, are often linked to high-permission opera- tions and potential malicious activity. These features, rarely seen in legitimate applications, reflect malware invariance, where core malicious intents persist despite evolving implementations."}, {"title": "3.4 Create Model to Learn Invariance", "content": "This discussion emphasizes the importance of learning sta- ble, discriminative features for drift-robust malware detection. ERM captures both stable and unstable information corre- lated with the target variable [10], often relying on unstable features when they are highly correlated. The key challenge is to isolate and enhance stable features, aligning with the principles of invariant learning in Section 2.2.\nInvariant learning methods face challenges, as their suc- cess depends on effective environment segmentation to reveal unstable information [23, 30]. In malware detection, iden- tifying variants that trigger distribution shifts is uncertain. High-quality representations from the encoder are also essen- tial for invariant predictors [38], yet Figure 3 shows that even"}, {"title": "4 Methodology", "content": "In this section, we present our Temporal Invariant Training framework (TIF), which comprises two main components: a Multi- Proxy Contrastive Learning module for generating modular, discriminative representations, and an Invariant Gradient Alignment module for aligning the model across environments, achieving temporal invariance."}, {"title": "4.1 Preliminary", "content": "In this section, we provide essential notations and definitions necessary to understand the proposed approach clearly."}, {"title": "4.1.1 Notations", "content": "We use uppercase and lowercase letters to denote matrices and vectors, respectively; $\\mathbb{D}$ represents the total number of elements in set $\\mathbb{D}$."}, {"title": "4.1.2 Problem setting", "content": "In Android malware detection, samples $x \\in \\mathbb{R}^d$ are divided into two parts: the labeled training data $D_{tr} = \\{(x_i,y_i)\\}_{i=1}^{|D_{tr}|}$, where $y_i \\in \\{0,1\\}$ represents the labels for benign and mali- cious applications, and the unlabeled test data $D_{te}$. Training data $D_{tr}$ comes from a specific period $T_r$, with each sample $x_i$ ontains its corresponding observation timestamp $t_i \\in T_r$. Test data $D_{te}$ comprises future samples $t_i > T_r$ that appear pro- gressively over time. We assume that the training and test data share the same label space and have unknown semantic simi- larities. The model is defined as $M = c \\circ \\phi$, where $c$ denotes the classifier and $\\phi$ is the feature encoder. This work aims to enhance the encoder in capturing invariant semantic features from training data that generalize to unseen test data."}, {"title": "4.2 Overall Architecture", "content": "Section 3.3.2 highlights the need for learning temporally stable and discriminative features to improve robustness against distribution drift. We propose a temporal invariant training method, adaptable to any malware detector, which enhances robustness by extracting rich features and sup- pressing unstable factors through invariant risk minimiza- tion. The method comprises two components: Multi-Proxy Contrastive Learning (MPC): representation diversity for benign and malware samples with dynamic proxies, encod- ing complex multi-family malware semantics into compact,"}, {"title": "4.3 Training Environments Segmentation", "content": "Segmenting the training environment requires exposing unsta- ble information. Malware distribution drift arises from multi- ple factors, such as application market demands or Android version updates. Dividing the training environment based on application observation dates effectively captures this mix of unknown causes. For a dataset with timestamps from $T_{min}$ to $T_{max}$, and a chosen time granularity $\\Delta$, samples are divided into $t$ time windows, each representing an environment $e \\in E$, where $|E| = t$. The resulting training set is an ordered multiset, $\\mathbb{D} = \\{D_1,D_2, ...,D_t\\}$, with each environment $D_i$ containing $|D_i|$ samples. A sample $x_i$ with timestamp $t_i$ is assigned to an environment using:\n$\\mathbb{E}(x_i) = \\left \\lfloor \\frac{t_i - T_{min}}{\\Delta} \\right \\rfloor$. (10)\nAs the environment index increases, timestamps approach the present. Time granularity impacts representation learning, balancing detailed distribution patterns against sample suffi- ciency. Finer granularity risks sparse samples, while coarser granularity may obscure shifts. Achieving balance requires careful consideration of label distribution to avoid misaligned"}, {"title": "4.4 Multi-Proxy Contrastive Learning", "content": "Malware families have distinct distributions and imbalanced sample sizes, complicating malware category modeling in the embedding space. Benign samples may also display complex feature distributions due to factors like geographic location or user behavior. Treating all relationships within a single family equally can exacerbate imbalances, while overly homogeniz- ing samples overlook valuable information. Thus, effective"}, {"title": "4.5 Invariant Gradient Alignment", "content": "Section 3.4 highlights the prerequisites for learning invari- ant features: environment segmentation to expose unstable information and rich feature representations. Building on the definition of Invariant Risk Minimization (IRM) in Sec- tion 2.2, our goal is to guide encoders to focus on the stable aspects of these representations. To achieve this, the encoder must ensure that representations of the same class across dif- ferent environments produce similar classifier gradients. We introduce an invariant gradient alignment module based on IRMv1 [3] to enhance the model's focus on invariant features. The objective function is shown in Eq.18.\n$L_{IGA} = \\frac{1}{|E|} \\sum_{e \\in E} ||s_e|_{s_e=1.0} R^e (s_e \\circ \\phi)||^2$. (18)\nHere, $s_e$ acts as a scalar, serving the role of a dummy classi- fier, set to 1.0 and updates through gradient backpropagation. $L_{grad}$ evaluates how adjusting $s_e$ minimizes the empirical risk in environment $e$. $R^e (s_e \\circ \\phi)$ is represented as Eq. 19:\n$R^e (s_e) = \\mathbb{E}[L_{CLS}(s_e (\\phi(x)), y)]$, (19)\nwhere $L_{CLS}$ denotes the classification loss function used in the current environment, such as binary cross-entropy for malware detection. This loss is computed following standard ERM training. The term $\\phi(x)$ represents the output of the shared encoder for samples $x, y \\sim p(x, y|e)$ from environment $e$. The gradient penalty term encourages uniform feature rep- resentation by aligning gradients of classifiers across environ- ments, thereby promoting consistent model performance."}, {"title": "4.6 Invariant Training Framework", "content": "Gradient adjustment for invariant learning classifiers relies heavily on the encoder's ability to learn rich representa- tions [44]. Starting from random initialization often leads to suboptimal convergence. To overcome this, we propose a two-stage training strategy to first capture diverse features before applying invariant learning."}, {"title": "4.6.1 Discriminative Information Amplification", "content": "In the first stage, ERM training is conducted independently for each environment to initialize the encoder. The multi- proxy contrastive learning module is then applied to each environment to maximize the exploitation of discriminative features. The optimization objective is defined in Eq.20:\n$L_{ERM} = \\frac{1}{|E|} \\sum_{e \\in E} (L_{CLS}+\\alpha \\cdot L_{MPC}),$ (20)\nwhere $L_{CLS}$ and $L_{MPC}$ denote the classification loss and multi- proxy contrastive loss for environment $e$, respectively. Jointly minimizing the empirical risk across all environments equips the encoder with diverse feature representations, forming a robust foundation for invariant training."}, {"title": "4.6.2 Unstable Information Suppression", "content": "To mitigate overfitting to environment-specific features from the earlier stage, we reset the optimizer's parameters before the second training phase. This reset allows the model to refo- cus on the objectives of invariant learning. In this phase, we first apply a multi-proxy contrastive loss across all samples to enhance class representation learning. Next, invariant gra- dient alignment is used to harmonize classification gradients across environments. The updated optimization objective is defined in Eq. 21:\n$L_{irm} = L_{CLS} + \\alpha \\cdot L_{MPC} + \\beta \\cdot L_{IGA},$ (21)\nIn training, the hyperparameters $\\alpha$ and $\\beta$ balance the contri- butions of each loss term. This two-stage approach enables the model to first capture a broad feature set, then refine it for cross-environment invariance, enhancing generalization under"}, {"title": "5 Evaluation", "content": "This section evaluates the effectiveness of our proposed method in improving drift robustness for Android malware detectors across various feature spaces. We also analyze the contributions of each component in the invariant learning framework. The evaluation addresses the following research questions:\nRQ1. Can our framework mitigate detector aging across dif- ferent feature spaces?\nRQ2. Can our framework stabilize detectors in different drift scenarios?\nRQ3. Does our framework effectively learn invariant features of applications?\nTo ensure reliability, experiments were conducted using ran- dom seeds (1, 42, 2024), with results averaged. All experi- ments ran on an RTX A6000. The dataset and code will be released upon paper acceptance."}, {"title": "5.1 Evaluation Settings", "content": "This section provides details on the experiment settings, dataset construction, and candidate detectors used to evaluate the effectiveness of our approach in mitigating detector degradation due to malware evolution."}, {"title": "5.1.1 Dataset", "content": "To evaluate the effectiveness of our approach in the context of long-term malware evolution, we select Android APKs from the Androzoo platform and construct datasets based on the time when each software was discovered (i.e., when the sample was submitted to VirusTotal for detection)\n\u2079https://androzoo.uni.lu\n\u00b9\u2070https://www.virustotal.com\n\u00b9\u00b9Due to the modifiable or randomly generated release time of Android applications, some timestamps (dex date) are unreliable."}, {"title": "5.1.2 Candidate Detectors", "content": "In Android malware detection, an APK file serves as input, containing the codebase (e.g., .dex files) and configuration files (e.g., manifest.xml), which offer behavioral insights such as API calls and permissions then organized as different for- mats. We select three representative feature spaces: Drebin [4] (vector-based), Malscan [35] (graph-based), and BERTroid [7] (string-based), each using distinct feature sources (details in Appendix B). For fair evaluation, we apply a two-layer linear classifier to each feature representation, with the final layer performing binary classification."}, {"title": "5.1.3 Baseline", "content": "To evaluate the robustness of our scheme across different fea- ture spaces, we compare it with two non-linear baselines: (1) APIGraph [45], which enhances robustness using API-based features, and (2) Guided Retraining [16], which improves de- tector performance across multiple feature spaces. We also include T-stability [2], a feature stability method for linear classifiers in the Drebin feature space, to demonstrate the"}, {"title": "5.1.4 Metrics", "content": "We evaluate different approaches using static and dynamic metrics. For static evaluation, the macro-F1 score is used to address the class imbalance in the Android dataset, offering a balanced assessment of precision and recall on a fixed-time test set. For dynamic evaluation, we employ the Area Under Time (AUT), a metric proposed by TESSERACT [27], which measures performance over time. AUT is calculated as:\n$AUT(m,N) = \\frac{1}{N-1}\\sum_{t=1}^{N-1} \\frac{(m(x_{t+1})+m(x_t))}{2},$ (22)\nwhere $m$ represents the performance metric, with the F1-score selected as its instance. $m(x_t)$ denotes the performance metric at time $t$, and $N$ is the number of monthly testing slots. AUT ranges from [0, 1], with a perfect classifier achieving an AUT of 1 across all test windows."}, {"title": "5.2 Enhance Different Feature Space (RQ1)", "content": "This section evaluates the Temporal Invariant Learning Frame- work (TIF) for mitigating detector degradation across feature spaces. The classifier is trained on 2014 samples and tested monthly from 2015 to 2023, with annual results (AUT(F1,"}, {"title": "5.3 Robustness in Different Drift Scenarios (RQ2)", "content": "This section evaluates how TIF aids malware detectors in learning stable representations under different drift scenarios."}, {"title": "5.4 Effective Invariant Feature Learning (RQ3)", "content": "We evaluate the framework's ability to learn invariant features. Section 3.3.2 shows that ERM models underperform by rely- ing on unstable features. To quantify stable and discriminative feature learning, we define the Feature Contribution Score (FCS) for each feature $f_j$ as follows:\n$FCS_j = |r (f_j, S_m) - r (f_j, S_b)| \\cdot IS_j, (23)"}, {"title": "5.5 Ablation Study", "content": "This section evaluates the robustness of each component through an ablation study. Five configurations are tested: the"}, {"title": "5.6 Environment Granularity Selection", "content": "Environment segmentation is crucial in invariant learning to expose uncertainty while ensuring sufficient samples per environment for effective training. We evaluate three seg- mentation granularities\u2014monthly, quarterly, and equal-sized splits (n = 4, 8, 12). Table 5 reports AUT(F1, 12m) scores across the test phase. Equal-sized splits increase label im- balance, hindering robust learning, while fine-grained splits improve feature discrimination but reduce per-batch samples, complicating cross-environment alignment. Therefore, when"}, {"title": "6 Discussion", "content": "This section discusses key aspects of the results, providing insights into the benefits and limitations of our approach, as well as potential directions for future work."}, {"title": "6.1 Influence of Malware Ground Truth", "content": "Malware labels are determined by multiple security engines, often with conflicting decisions. Researchers set thresholds to balance sensitivity and specificity; lower thresholds include borderline software, blurring classification boundaries. In this"}, {"title": "6.2 Comparison to the Regularization Method", "content": "ERM-trained models are easy to overfit to unstable features, limiting test set generalization [21]. Regularization methods, such as early stopping, 12 regularization, and dropout, help mitigate this by constraining model parameters [43]. Unlike regularization, invariant learning focuses on stable features"}, {"title": "6.3 Application Scope of Invariant Learning", "content": "Invariant learning enhances a model's generalization to un- seen distributions, but keeping stable to all possible shifts is unrealistic. With significant distribution changes, its effective- ness diminishes, and current research indicates that addressing concept drift typically requires costly model updates, includ- ing data labeling, retraining, and redeployment. Additionally,"}, {"title": "7 Conclusion", "content": "Android malware detectors suffer performance degradation due to natural distribution changes caused by malware evolu- tion. We identify learnable invariant patterns among malware samples with similar intent, enabling a drift-stable feature"}]}