{"title": "TOWARDS COUNTERFACTUAL AND CONTRASTIVE EXPLAINABILITY AND TRANSPARENCY OF DCNN IMAGE CLASSIFIERS", "authors": ["Syed Ali Tariq", "Tehseen Zia", "Mubeen Ghafoor"], "abstract": "Explainability of deep convolutional neural networks (DCNNs) is an important research topic that\ntries to uncover the reasons behind a DCNN model's decisions and improve their understanding\nand reliability in high-risk environments. In this regard, we propose a novel method for generating\ninterpretable counterfactual and contrastive explanations for DCNN models. The proposed method\nis model intrusive that probes the internal workings of a DCNN instead of altering the input image\nto generate explanations. Given an input image, we provide contrastive explanations by identifying\nthe most important filters in the DCNN representing features and concepts that separate the model's\ndecision between classifying the image to the original inferred class or some other specified alter\nclass. On the other hand, we provide counterfactual explanations by specifying the minimal changes\nnecessary in such filters so that a contrastive output is obtained. Using these identified filters\nand concepts, our method can provide contrastive and counterfactual reasons behind a model's\ndecisions and makes the model more transparent. One of the interesting applications of this method\nis misclassification analysis, where we compare the identified concepts from a particular input image\nand compare them with class-specific concepts to establish the validity of the model's decisions. The\nproposed method is compared with state-of-the-art and evaluated on the Caltech-UCSD Birds (CUB)\n2011 dataset to show the usefulness of the explanations provided.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep convolutional neural networks (DCNNs) have achieved state-of-the-art performance in many\ncomputer vision applications such as medical imaging and diagnostics Gu et al. [2019], Shakeel et al. [2019], biometrics\nidentification Liu et al. [2019a], object detection Wang et al. [2017], scene segmentation Fu et al. [2019], image\ninpainting Yu et al. [2019], etc. DCNNs are a type of deep learning (DL) method that automatically learns generalized\nrepresentations from the input data useful for image classification, segmentation, or recognition tasks. Most of the\ncurrent research on DCNNs is focused on architectural improvements He et al. [2016], Chen et al. [2017], Tan and\nLe [2019], Touvron et al. [2019], Zhang et al. [2020], Mohan and Valada [2020]. Although this aspect of research is\nnecessary, one essential aspect that is mostly overlooked, missing, or not focused upon while developing new CNNs is\ntheir explainability. Since DCNNs are trained in an end-to-end manner, their inner workings are not well understood,\nwhich makes them a black-box Arrieta et al. [2020].\nThe explainability or transparency of DCNN models is extremely important for certain applications where the impact of\nincorrect or unjustifiable predictions can have a significant impact on the outcome Samek et al. [2017], Goebel et al.\n[2018], Rudin [2019]. This is especially true for those security-critical applications that involve endangerment of human\nlife or property, such as in medical imaging and diagnostics Tjoa and Guan [2020], Holzinger et al. [2017], autonomous\nvehicles Zablocki et al. [2021], military applications Svenmarck et al. [2018], etc. If a DCNN model's decision cannot\nbe interpreted or it cannot explain how or why it made its decision, then using such a model in high-risk environments\nwill carry some risks. For example, DCNNs are known to be affected by dataset bias Zhang et al. [2018b] due to which\nthey may rely on unrelated or out-of-context patterns to classify images. Zhang et al. Zhang et al. [2018b] demonstrated\nan example where a CNN incorrectly relied on eye features to identify the 'lipstick' attribute of a face image. Similarly,\nif a DCNN model is trained to detect lung diseases from chest X-rays annotated with doctors' pen marks, it may learn to\nrely on them to make predictions. Another issue with DCNNs is that they are prone to adversarial attacks where subtle\nchanges to the input may lead to the CNN producing incorrect results Akhtar and Mian [2018]. Adversarial attacks pose\nthreats to many security-critical applications, such as for self-driving cars where minor obstructions on traffic signs\ncan cause incorrect decisions Eykholt et al. [2018] or in surveillance systems where malicious users can cause harm\nThys et al. [2019]. These are some of the many aspects that make DCNNs un-trustworthy and require explainable AI\ntechniques to identify their weaknesses and train robust, trustworthy, and transparent models Rudin [2019], Ghorbani\nand Zou [2020], Fong and Vedaldi [2017].\nIn the literature, several types of explainability or interpretability methods exist. At the top level, most types of\nexplainable techniques can be broadly divided into two main categories: intrinsic/inherent interpretability or post-hoc\ninterpretability Du et al. [2019]. Inherently interpretable techniques attempt to introduce explainability built into the\nstructure of the model itself that makes the model self-explanatory, for example, decision trees. Whereas post-hoc\ninterpretable techniques usually consist of a separate explanation of a pre-trained DCNN black-box model. These\ntechniques may use a second model to explain the black-box either by probing the internal structure of the black-box\nor by altering the input. An example of post-hoc explainability is a visual explanation technique such as GradCAM\nSelvaraju et al. [2017]. Counterfactual and contrastive explanations are two popular types of post-hoc explanation\nmethods. Contrastive explanations generally try to find the critical features in the input that lead to the model making\nits decision to the inferred class Dhurandhar et al. [2018]. While in counterfactual explanations, the goal is to alter\nthe input features (pixels) such that the model changes its decision to some other counter class Goyal et al. [2019].\nSuch explanations are natural to humans since they mimic human thought processes. For example, a contrastive\nexplanation can have the form \u201cif some event X had not occurred, then event Y would not have occurred", "if X is changed to X' then would Y' happen instead of Y?\". Such\ntypes of explanations are human-friendly and easy to understand. If a DCNN model can provide such explanations, it\ncan be considered a reliable or trustworthy model, and we can predict its behavior.\nRecently, several counterfactual and contrastive explanation methods have been proposed Goyal et al. [2019": "Hendricks\net al. [2018], Dhurandhar et al. [2018], Liu et al. [2019c], Luss et al. [2019]. These methods generally perturb the\ninput pixels to alter the model prediction. One of the drawbacks of such an approach is that it generally does not\nidentify semantically meaningful features or high-level concepts useful in explaining the model decisions. Another\nissue with pixel-based perturbation methods is that the search space for finding the optimal combination of pixels that\ncan cause the network to change its prediction is large, which makes such methods computationally expensive Wang\nand Vasconcelos [2020]. A better approach would be to identify the major concepts that a model learns and relies on to\nmake predictions. In a recent study, Akula et al. Akula et al. [2020] tried to address this issue by using super-pixels to\nidentify critical concepts that, when added or removed from the input image, alter the model's decision. Although this\nmethod provides a useful way to generate explanations, it still operates on pixel data to generate explanations that do\nnot make it fully transparent. In another work, concept activation vectors (CAV) Kim et al. [2018] were introduced\nthat essentially measure the sensitivity of the model towards a particular high-level concept. The concepts can be\nlearned from either training or user-provided data. However, this method only identifies whether a particular concept is\nimportant or not. It does not investigate if it is actively used in the decision-making process for a particular input.\nThe problem with these works is that they are primarily non-intrusive. They only look at model behavior by altering\nthe input and do not thoroughly investigate the internal workings or reasoning behind the model predictions. Such\ninterpretability methods may not be aligned with how the model is making decisions. In this study, we propose a\npost-hoc explainability method that predictively identifies counterfactual and contrastive filters in a DCNN model.\nAlthough in this work, we restrict the identification of filters from the top convolution layer only, the proposed method\ncan be used to identify counterfactual and contrastive filters from any layer of a DCNN and can also be used to identify\nsuch filters in other networks. It has been shown that filters in the top convolution layer of a DCNN tend to learn more"}, {"title": "3 Proposed methodology", "content": "Given a pre-trained DCNN model, M, and the dataset, D, consisting of images belonging to C distinct classes, our\nobjective is to provide two types of explanations for each image xi \u2208 D. Firstly, our model predicts the minimum\nset of filters in the top convolution layer of M necessary for M to maintain its prediction of image xi to the original\ninferred class (source class) c\u2081 \u2208 C. We call these filters as minimum correct (MC) for image xi to be classified to class\nci, denoted as FMC; \u2208 [0,1]1\u00d7n, where n is the number of filters in the top convolution layer of M. In the VGG-16\nDCNN model, the number of filters in the top convolution layer is 512, i.e., n = 512. Values of '1' and '0' indicate\nwhether the corresponding filter is predicted to be active or disabled, respectively.\nSecondly, our model predicts the minimum set of filters that, if they were altered by a larger magnitude, would have\nresulted in the DCNN classifying the input to some target class c\u00f3 \u2208 C. We call these filters as minimum incorrect (MI)\nfor image xi with respect to the target class c', denoted as FMI\u2081 \u2208 [R+]1\u00d7n. Non-zero indexes in FM1, correspond to\nthe MI filters, and the values at these indexes indicate the magnitude by which the original filter activations are altered\nto modify the DCNN's decision. The overall diagram depicting the proposed approach is shown in Fig. 2 which is\ndescribed in the following sub-sections."}, {"title": "3.1 CFE model for MC filters", "content": "To achieve the first objective, we train the CFE model to predict MC filters from the pre-trained model M that explain\neach image with respect to its inferred class. The MC CFE model is designed with partially similar architecture as M\nby sharing the feature extraction layers up to the top convolution layer of M, as shown in Fig. 2. Given an input image\nxi, model M generates two outputs: (1) the feature maps gi \u2208 [R+]1\u00d7n produced at the last convolution layer of M\nafter the global average pooling layer, and (2) the source or inferred class ci. The MC CFE model takes as input the\nfeature maps gi and generates a binary filter map FMC, corresponding to class ci. With these definitions, the MC CFE\nmodel can be represented as:\nFMC=CFEMC(gi)\n= ReLU+(A(d\" + (gi))), (1)\nwhere gi represents the feature maps after the global average pooling layer of M(xi), dn is a dense layer with n units,\nA represents sigmoid activation function, and ReLUt is thresholded-ReLU layer with threshold t set to t = 0.5 that\noutputs the approximately\u00b9 binarized MC filter map FMC\u2081. The thresholded-ReLU function sets all values below the\nthreshold to zero and keeps other values unchanged. The shared feature extraction layers between the CFE model\nand the pre-trained model M are kept frozen for the training of the CFE model. Additionally, all layers of M after\nthe top convolution layer are frozen. Only the dense layer dn weights are updated during training of the CFE model.\nTo generate explanations, CFEMC model predicts the MC filters matrix for the corresponding input xi. FMC; is\nmultiplied with gi (Hadamard product) to disable all but the MC filters from the top layer of M. The DCNN model M\nmakes the alter prediction with the disabled filters as \u0109i:\n\u0109i = h(gi FMC), (2)\nwhere h represents the classification (fully-connected and softmax) layers of M.\nThe MC CFE model is trained to predict the optimal filter maps FMC. to reduce the loss between the predicted\nclasses \u0109i and the source class ci. The optimal MC filters FMC; are learned by minimizing the following three losses\nsimultaneously: 1) cross-entropy (CE) loss, LCE, for classifying each input image x\u2081 by modified model M to the\nspecified class c, 2) sparsity loss, L11, that ensures FMC, is sparse so that minimal filters remain active, and 3) negative\nlogits loss, Llogits, that ensures that the predicted sparse filters have higher contribution towards the chosen class c:\nLMC = LCE(\u0108i, Ci) + AL11(FMC) \u2013 Llogits, (3)\nwhere X is the weight assigned to the sparsity loss. LCE(\u0109i, ci) is computed using the output of the modified model M\nand the desired class ci for each training example i:\nLCE(\u0108i, Ci) = 1/m * sum([ci log \u0109i + (1 \u2212 ci) log(1 \u2013 \u0109i))]. (4)\nThe L11 (FMC) loss minimizes the sum of the activated filters that push the CFE model to predict minimally sufficient\nfilters:\nL11(FMC) = sum(sum(|FMC\u2081(k)|)). (5)\nwhere n is the number of filters in the top convolution layer of M.\nThe negative logits loss - Llogits is necessary for the proposed methodology to ensure that the sparse filters predicted by\nthe MC CFE model are contributing maximally towards the source class ci. This loss is applied on the logits (weighted\nsum) computed after disabling the filters using FMC; and before applying the final activation function to get M's\noutput:\nLlogits (FMC) = - sum(|(FMC;(k) * gi(k)) * Wk,ci |), (6)\nwhere gi represents the feature maps after the global average pooling layer of model M, and Wk,c represents the\npre-trained weights of model M connecting the GAP layer with the output layer's class ci. The negative sign of the loss\nis for ensuring that the model chooses filters that have higher weight for the desired class, and hence their activation\ncontributes more towards it and results in a larger logits score. The results section shows that if this loss is not included,\nthe CFE model is more likely to predict less important filters that may still classify the images to the desired class but\nwith lower confidence.\nWhen all three of these losses are minimized, the CFE model learns to predict minimally sufficient or MC sparse filters\nusing which the inputs are classified to the source class ci. For example, if the pre-trained model M classified a given\nimage xi to class ci, then to identify the MC filters, we train the CFE model with respect to the source class Ci. This\nCFE model is then used to predict the most important FMC; filters necessary for classifying input xi to class Ci. The\nprocedure for MC CFE model training is summarized in Algorithm 1\""}, {"title": "3.2 CFE model for MI filters", "content": "To achieve our second objective of predicting MI filters, we follow a similar methodology used for MC filters but with\nkey differences. The MI filters are those filters that, if they were altered to have higher magnitude, would have resulted\nin the model classifying the input to some other target class c\u00f3 \u2208 C, instead of the initially inferred class ci (or source\nclass). For this purpose, we train the MI CFE model to predict MI filters from M that explain each image with respect\nto some target class c. The CFE model for MI filters is designed similarly to the CFE model for MC filters but with\nminor changes. The MI CFE model shares the feature extraction layers of M up to the top convolution layer, as shown\nin Fig 2. Given an input image xi, model M generates two outputs: (1) the feature maps gi \u2208 [R+]1\u00d7n produced at the\nlast convolution layer of M after the global average pooling layer, and (2) the source or inferred class ci. The MI CFE\nmodel takes as input feature maps gi and learns to generate non-binary MI filter map FMI, corresponding to target class\nc. This map is combined with the output of GAP layer of M to modify the activation magnitudes with the objective to\nclassify each input xi to the target class c. With these definitions, the MI CFE model can be represented as:\nFMI:=CFEMI(9i)\n= ReLU(d\" (gi)), (7)\nwhere gi represents the feature maps of M(xi) after global average pooling layer, dn is a dense layer with n units, and\nReLU is the ReLU activation function that produces the non-binary MI filter map FMI\u2081.\nThe key difference between this equation and Eq. (1) is the absence of sigmoid activation and the usage of standard\nReLU instead of thresholded-ReLU. Similar to the MC CFE model, the feature extraction layers shared between the\nMI CFE model and the pre-trained model M are kept frozen during training of the CFE model. Only the dense layer\ndn weights are updated during training of the CFE model. To generate explanations, CFEMI model predicts the MI\nfilters matrix for the corresponding input Xi. FMI, is added to g to alter the filters from the top layer of M after global\naverage pooling. The DCNN model M makes the alter prediction with the altered filters as \u0109i:\n\u0109i = h(gi + FMI\u2082), (8)\nwhere h represents the classification (fully-connected and softmax) layers of M.\nThe MI CFE model is trained to predict the optimal MI filter map FMI, to reduce the loss between the predicted\nclasses \u0109i and the target class c. Optimal MI filters, FMI\u2081, are learned by minimizing the following two losses: 1)\ncross-entropy (CE) loss, LCE, for classifying each input image xi by modified model M to the target class c\u00ed, and 2)\nsparsity loss, L11, that ensures FMI, is sparse so that minimal filters are modified with minimal additive values such\nthat the model M classifies each input to class :\nLMI = LCE(\u0108i, c\u2081) + AL11 (FMI, C). (9)\nThis equation is similar to Eq. 3 with the difference that the logits loss is not used to find the MI filters. Logits loss, in\nthis case, is not necessary as our objective can be achieved with just cross-entropy and sparsity losses. The first term of\nthe loss function is the cross-entropy loss using which error between modified model M's output and the target class\nc is minimized, which is computed using Eq. 4. The second term minimizes the sum of the MI filter matrix FMI\nthat pushes the CFE model to choose the least number of filters whose activation magnitude is increased minimally to\npredict each input x\u2081 to the target class c. This loss can be computed using Eq. 5 by replacing FMC with FMI\u2081. The\nprocedure for MI CFE model training is summarized in Algorithm 2\nWith both these CFE models for MC and MI filters, we can explain each decision of the pre-trained model M in terms\nof finding the minimum required critical filters that maintain the model's decision to the inferred class or to finding the\nminimum set of filters that if they were altered with higher magnitude, would have classified the input to c\u00bf instead of\nCr. We show the importance of these filters by highlighting the features that activate them the most. The results of the\nproposed methodology are presented in the following section.\""}, {"title": "4 Results", "content": "This section presents the results and discussion of the proposed counterfactual explanation (CFE) method. We evaluate\nthe explanations generated by the proposed CFE method qualitatively and quantitatively. In qualitative analysis,\nwe provide visualization of the proposed method and show how to interpret these explanations. We compare these\nvisualizations with existing counterfactual and contrastive explanation methods. Additionally, we conduct a user-study\nto evaluate the usefulness of the explanations provided based on the Explanation Satisfaction (ES) qualitative metric\nHoffman et al. [2018].\nThe results section is structured as follows. In Section 4.1, we discuss the experimental setup describing the dataset,\nthe pre-trained model used for testing, and the training details of the proposed CFE model for the explanation of the\npre-trained model. In Section 4.2, we provide visualization of how the CFE method works and how it's output can be\ninterpreted, and in Section 4.3, we qualitatively compare the CFE method with Selvaraju et al. [2017] and Wang and\nVasconcelos [2020]. In Section 4.4, we present the quantitative evaluation of the proposed method where we measure\nthe impact of disabling the MC filters for different classes on the overall model accuracy and class recall. We also\ninclude an analysis on using different weights for the sparsity loss and measure the effectiveness of logits loss. Finally,\nin Section 4.5, we compare our method with state-of-the-art explanation methods."}, {"title": "4.1 Experimental setup", "content": "For the evaluation of the proposed CFE method, we used the Caltech-UCSD Birds (CUB) 2011 Wah et al. [2011]\ndataset. We train a VGG-16 Simonyan and Zisserman [2014] model on this dataset and train our CFE model to\nprovide explanations for the trained model's decisions. The VGG-16 model was initially trained by removing the dense\nclassification layers and adding a GAP layer after the top convolution layer, followed by a dropout and the output\nsoftmax layer, as discussed in Section 3. The VGG-16 model was trained in two steps. First, we performed transfer\nlearning to train the newly added output softmax layer with stochastic gradient descent (SGD) optimizer using imageNet\nRussakovsky et al. [2015] pre-trained weights. Transfer learning was performed for 50 epochs with a 0.001 learning\nrate, 0.9 momentum, 32 batch size, and 50% dropout without data augmentation. In the second step, we fine-tuned all\nmodel layers for 150 epochs at a 0.0001 learning rate with standard data augmentation and kept all other parameters the\nsame. The VGG-16 model achieved the final training and testing accuracy of 99.0% and 69.5%, respectively."}, {"title": "4.1.1 Counterfactual explanation model training details", "content": "The CFE model provides contrastive and counterfactual explanations of the decisions of the pre-trained VGG-16 model\nby predicting the minimum correct (MC) and minimum incorrect (MI) filters for each decision by the model with\nrespect to the original inferred class (source class) and some target alter class. The CFE models for MC and MI filter\nprediction are comprised of similar architectures as the pre-trained model but with some differences, as discussed in\nSection 3. For the MC CFE model, the feature extraction layers are frozen. The output layer consists of the same\nnumber of units as the number of filters in the top convolution layer, with a sigmoid activation function followed by\nthresholded-ReLU. The weights of the feature extraction layers are shared with the pre-trained model being explained.\nThe CFE model is trained for a given alter class by minimizing the three losses, namely, cross-entropy loss, sparsity\nloss, and logits loss, as discussed in Section 3. The MI CFE model follows similar architecture but with the difference"}, {"title": "4.2 Qualitative analysis", "content": "In this section, we qualitatively discuss the results of the proposed CFE approach by providing the visualization of\nthe MC and MI filters, comparing explanations with existing methods, performing misclassification analysis, and\nconducting a user evaluation to assess the usefulness of different explanation methods."}, {"title": "4.2.1 Explanation visualization and interpretation", "content": "Fig. 3 shows the CFE model results for a sample image from the CUB dataset that was correctly predicted as \"Red-\nwinged blackbird\" with 99.9% probability by the VGG-16 model. Our model highlights the MC and MI filters necessary\nfor classifying the input image either to the source class or to the target class, respectively. The contrastive explanation\nin Fig. 3, identifies the most important MC filters plotted as a graph of their activation magnitudes (y-axis) against\nfilter number (x-axis). Using these filters, the model maintains the prediction of the input image to its inferred class, i.e.\n\"Red-winged Blackbird\" with 99.8% probability.\nWe visualize the concepts represented by the top-3 filters (based on activation magnitude) by drawing the filter's\nreceptive field (RF) Zhou et al. [2014] on the input image. RF is the image-resolution feature map of the selected filter\nfrom the top convolution layer that allows us to understand where the filter pays most attention. RF for filters 295 and\n399 in Fig. 3 show that they focus around the \"red spot\" of the bird, while filter 5 focuses on the belly of the bird.\nThe counterfactual explanation in Fig. 3 identifies the MI filters with respect to the \"Bronzed cowbird\" class (top-3\npredicted class) shown as a graph of filter activation magnitudes against filter number. The MI filters are highlighted in\nred. The CFE model predicts the minimum additive values to modify these MI filters such that it results in the input\nimage being classified to the target class, i.e., \u201cBronzed cowbird\". The RF visualization of the top-3 MI filters show\nthe most important features for the target class. Filter 15 activates the most on the red-colored eye of the \u201cBronzed\ncowbird\". In contrast, filter 158 activates the most on the blueish tinge on the bird's wings. These two filters identify the\ncritical features that if they were present in the input image, the model would have been more likely to classify the bird\nas \"Bronzed cowbird\" instead of the \"Red-winged blackbird.\" Filter 297, on the other hand, activates the most on the"}, {"title": "4.2.2 Misclassification analysis", "content": "One of the useful applications of the proposed explanation method is misclassification analysis. Since our model\npredicts the MC and MI filters for an input sample with respect to different classes, it is possible to analyze why the\nmodel has classified an input to the inferred class and why not to some other, maybe top-2 or 3 class. In Fig. 6, we\nshow a misclassification case where an image of class \"Red-winged blackbird\" was incorrectly classified as \"Myrtle\nwarbler\" with a probability of 92%. In Fig6a, we show the top-3 MC filters for the inferred class with RF drawn on the"}, {"title": "4.3 User evaluation", "content": "To show the effectiveness of the explanations provided by the proposed CFE method, we conducted a user-study to\nqualitatively evaluate different counterfactual and contrastive explanation methods including Selvaraju et al. [2017]\nand Wang and Vasconcelos [2020] in terms of the Explanation Satisfaction (ES) Hoffman et al. [2018] qualitative\nmetric. This metric was previously used by Akula et al. [2020] to perform user-study, and we have followed their\nprotocol closely in this work as well. ES metric measures the user's satisfaction at achieving an understanding of the\nDCNN model based on different explanations provided in terms of metrics such as usefulness, understandability, and\nconfidence Hoffman et al. [2018].\nThe user-study is conducted by creating two expert and non-expert human subjects groups. The non-expert group\nconsists of 30 subjects with a limited understanding of the computer vision field, whereas the expert group consists of\n10 subjects that routinely train and evaluate DCNN models. Subjects in each group go through a familiarization phase\nwhere they are first shown a sample query image and the DCNN model's classification decision for that image. The users\nare then shown various images from the predicted class, and some alter class selected from top-2 or top-3 classes to help\nthem understand the differences between the two classes. The users are then shown explanations generated by Selvaraju\net al. [2017], Wang and Vasconcelos [2020], and the proposed method for why the model classified the image to the\ninferred class and not to the alter class, along with a brief description on how to interpret the generated explanations.\nFinally, the users are shown the DCNN model's decisions on 10 test images, along with the explanations for these\nprovided by the explanation techniques regarding the inferred and alter classes. At the end of the test examples, the"}, {"title": "4.4 Quantitative analysis", "content": "In this section, we quantitatively discuss the results of the proposed CFE methodology in terms of finding the most\ncommonly activated MC filters predicted by the CFE model for explaining the pre-trained VGG-16 model with respect\nto different classes. We show the importance of these MC filters by demonstrating the effect of disabling them on\nthe class recall metric compared to the effect on the overall model accuracy. Furthermore, we also discuss the effect\nof different training parameters on the MC filters predicted and CFE model accuracy for explaining the pre-trained\nVGG-16 model."}, {"title": "4.4.1 Activated filter statistics", "content": "First, we discuss the filter activation statistics of the MC filters predicted by the CFE model for explaining the pre-trained\nVGG-16 model with respect to different classes. For a given class, we use the CFE model to predict MC filters for\nall test images of that class. We then accumulate the predicted MC filters to find the number of times each filter is\npredicted and compute the normalized activation magnitude for those filters. Fig. 9 shows the activated filter analysis of\nMC filters for \"Red-winged blackbird\" class. There are 5794 test images in the CUB dataset, of which 30 belong to this\nclass. Fig. 9a shows that filters 44, 147, and 364 are predicted as part of MC filters for nearly all of the test images.\nFig. 9b shows the normalized activation magnitudes of these filters. It can be seen that filters 147 and 364 have, on\naverage, the highest activation magnitude of these filters. In other words, these filters are globally the most important\nfilters for this class and represent crucial features/concepts relevant to this class. We show the importance of these\nglobally significant filters for the \"Red-winged blackbird\" class by disabling them from the pre-trained VGG-16 model\nand reporting the decrease in the model's ability to accurately classify images of this class as compared to the overall\naccuracy of the model. Interestingly, disabling all 31 MC filters predicted by the CFE model for the \u201cRed-winged\nblackbird\" class reduced the class recall from 93.3% to 30%. In contrast, the overall model accuracy is decreased\nby less than 2%. Similarly, we carry out this analysis for a few other classes and summarize the findings in Table 2.\nIn these cases, disabling around 31-44 globally critical MC filters results in a significant decrease in the class recall,\nwhereas the overall model accuracy is reduced by just 2-3%. On the contrary, it can be seen that randomly disabling"}, {"title": "4.4.2 Trade-off between CFE model accuracy and predicted filter sparsity", "content": "In this section, we will discuss the effect on CFE model training and testing accuracy by varying the losses and training\nparameters and also report the average number of MC filters predicted by the CFE model with these changes. Table 3\npresents the effect of different weights assigned to the sparsity loss in Eq. 3 for explaining the VGG-16 model with\nrespect to the \"Red-winged Blackbird\" class using MC filters. It can be seen that as the sparsity loss weight is increased\nfrom X = 1 to X = 4, the average number of filters predicted for classifying all train or test images in the CUB dataset\nto the \"Red-winged Blackbird\" decreases from 20.5 and 21.9 to 10.1 and 11.4, respectively. Although fewer predicted\nfilters are desirable, it comes at the cost of higher cross-entropy loss that results in low confidence predictions.\nTable 4 presents a similar analysis where we show the effect of training the MC CFE model with and without the logits\nloss. Without using logits loss, the CFE model suffered higher training and testing loss but better accuracies. This\nmeans that the sparse filters predicted by the CFE model had a lower impact or were less critical towards the specified\nclass, leading to low confidence predictions. With logits loss, on the other hand, the CFE model predicted on average\n0.8 and 1.1 more filters, respectively, for training and testing sets, resulting in lower CE loss but slightly decreased\naccuracy. However, the predicted filters are more likely to be relevant to the specified alter class."}, {"title": "4.5 Comparison with the state-of-the-art", "content": "Quantitative evaluation and comparison of state-of-the-art counterfactual explanation models is challenging as the\nground truths are unavailable. Recent works have mostly provided qualitative or human-based evaluations Goyal et al.\n[2019", "2020": "."}, {"2020": "nproposed a way to synthetically generate ground truths based on part and attribute annotations present in the CUB\ndataset Wah et al. [2011"}, {"2020": "."}]}