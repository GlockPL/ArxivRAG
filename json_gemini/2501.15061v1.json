{"title": "POLAFORMER: POLARITY-AWARE LINEAR ATTENTION FOR VISION TRANSFORMERS", "authors": ["Weikang Meng", "Yadan Luo", "Xin Li", "Dongmei Jiang", "Zheng Zhang"], "abstract": "Linear attention has emerged as a promising alternative to softmax-based attention, leveraging kernelized feature maps to reduce complexity from quadratic to linear in sequence length. However, the non-negative constraint on feature maps and the relaxed exponential function used in approximation lead to significant information loss compared to the original query-key dot products, resulting in less discriminative attention maps with higher entropy. To address the missing interactions driven by negative values in query-key pairs, we propose a polarity-aware linear attention mechanism that explicitly models both same-signed and opposite-signed query-key interactions, ensuring comprehensive coverage of relational information. Furthermore, to restore the spiky properties of attention maps, we provide a theoretical analysis proving the existence of a class of element-wise functions (with positive first and second derivatives) that can reduce entropy in the attention distribution. For simplicity, and recognizing the distinct contributions of each dimension, we employ a learnable power function for rescaling, allowing strong and weak attention signals to be effectively separated. Extensive experiments demonstrate that the proposed PolaFormer improves performance on various vision tasks, enhancing both expressiveness and efficiency by up to 4.6%.", "sections": [{"title": "INTRODUCTION", "content": "Transformers have demonstrated remarkable success across a broad range of vision tasks (Yuan et al., 2021b; Cai et al., 2022). The core component, dot-product attention with softmax normaliza-tion, enables transformers to capture long-range dependencies effectively. However, this comes at the cost of quadratic complexity O(N2) in relation to the sequence length N, resulting in consider-able computational overhead particularly when processing long-sequence videos or high-resolution images. This limits their efficiency in resource-constrained environments, making practical deploy-ment difficult in such scenarios."}, {"title": "RELATED WORK", "content": "Efficient Vision Transformers. By cutting images into smaller patches and processing them as a sequence, Vision Transformers (ViT) (Dosovitskiy et al., 2021) successfully transfer transformer models (Vaswani et al., 2017) from language tasks to vision tasks, and have achieved remarkable results. However, the quadratic complexity of the self-attention mechanism in ViT makes it expen-sive to train. Existing works have made various improvements to ViT for computational efficiency. Swin-Transformer (Liu et al., 2021) introduces a shifted windows scheme to limit self-attention computation to local windows. Pyramid Vision Transformer (PVT) (Wang et al., 2021) uses a pro-gressive shrinking pyramid to reduce the computations of feature maps. Deit (Touvron et al., 2021) enables models to achieve competitive performance without pretraining on large datasets by utiliz-ing designed tokenization mechanisms and training strategies. However, these improvements do not solve the bottleneck of the self-attention mechanism, and quadratic complexity, thus the train-ing cost is still unaffordable as the model scale increases. To address this issue, VMamba (Liu et al., 2024) extracts the information of the picture based on the spatial state model (SSM) encod-ing through serializing and scanning the picture, at the same time it inherits the linear complexity of SSM. VHeat (Wang et al., 2024) conceptualize image patches as heat sources and simulate the conduction process, and utilize DCT and IDCT operations to reduce the complexity to O(N1.5). These methods have just been proposed and are not yet as widely validated and deployed at scale as Transformers, their model performance is also not significantly higher than the other models.\nLinear Attention. Sub-quadratic transformers focus on alleviating the inefficiency of the standard self-attention mechanism due to the softmax function and its quadratic complexity. A preferable solution is to use kernel-based similarities to reduce the complexity by approximating the softmax operator. The initial linear attention (Katharopoulos et al., 2020) proposes to substitute the Softmax function with a linear dot-product of kernel feature maps, which facilitates reducing the complexity from O(N2) to O(N). Following this Softmax-free scheme, some variants of linear attention have been proposed by employing different kernel functions, such as ReLU(\u00b7) (Shen et al., 2021) and 1 + ELU(\u00b7) (Katharopoulos et al., 2020). Moreover, tFollowingo fulfill the non-negative and dis-tribution properties of attention matrix, Cosformer (Qin et al., 2022) combines the ReLU function and cos-based re-weighting mechanism to enhance the self-attention weighs with locality inductive biases. FLatten Transformer (Han et al., 2023a) extends ReLU(\u00b7) with power operation to maintain both properties of attention weights, i.e., non-negative and low-entropy. It is a practical way to use power function to calculate the inner product to approximate exp, which is similar to the use of power function to approximate max-pooling proposed in R-MAC (Tolias et al., 2016). Recently, Agent Attention (Han et al., 2023b), a claimed generalized linear attention, introduces n agent to-kens to aggregate features based on a combination of Softmax and linear attention with O(Nnd) complexity. As both N and n increase simultaneously with the model size, the complexity of the generalized linear attention is not absolutely linear with respect to N. Notably, the balanced perfor-mance still relies on the softmax operator and additional agent tokens, which violates the original premise of linear attention, i.e., softmax-free and linear complexity. Current kernel functions either suffer from performance degradation or introduce excessive computational overhead. We observed significant information loss in comparison to original query-key dot products due to the non-negative constraint on attention weights and the intricate kernel designs aimed at achieving low entropy. This issue will be further addressed in the following sections of this work."}, {"title": "PRELIMINARY", "content": "In this section, we first highlight the inefficiency of the standard self-attention mechanism, followed by a discussion of the variants of existing linear attention methods."}, {"title": "Low EFFICIENCY OF SELF-ATTENTION MECHANISM", "content": "Consider a sequence x \u2208 \\mathbb{R}^{N \\times D} of token length N and dimension D. x is devided into h heads, the dimension of each head is d. In each head, tokens at various positions are collectively attended to capture long-range dependencies. The output O = {ot}t=1N \u2208 \\mathbb{R}^{N \\times d} can be formulated as:\nO = Softmax(\\frac{Q K^T}{\\sqrt{d}})V, ot = \\frac{\\sum_{i=1}^{N} exp(q_t k_i / \\sqrt{d})}{\\sum_{i=1}^{N} exp(q_t k_i / \\sqrt{d})} V_i.\nHere, the query, key, and value vectors of dimension d are obtained by linearly projecting the inputs with three learnable matrices Q = {qt}t=1N, K = {k_t}t=1N, V = {vt}t=1N \u2208 \\mathbb{R}^{d}. For each head, the complexity of self-attention is O(N\u00b2d), making the mechanism inefficient for long sequences."}, {"title": "KERNEL-BASED LINEAR ATTENTION", "content": "To mitigate the efficiency bottlenecks of standard self-attention, kernel-based linear attention mech-anisms (Katharopoulos et al., 2020) have been proposed, which decompose the similarity function into dot products of feature maps. Following the notations in (Choromanski et al., 2021; Chen et al., 2021), we define SM(q, k) = exp(q\u2081k) as the softmax kernel function. Mathematically, linear attention aims to use $(qi)$(kj)T to approximate SM(\u00b7,\u00b7), where the feature map $(\u00b7) : Rd \u2192 Rd' is applied row-wise to the query and key matrices. As a result, the t-th row of attention output ot can be rewritten as,\nOt = \\frac{\\sum_{i=1}^{N} \\Phi(q_t) \\Phi(k_i)^T v_i}{\\sum_{i=1}^{N} \\Phi(q_t) \\Phi(k_i)^T} = \\frac{\\Phi(q_t) \\sum_{i=1}^{N} \\Phi(k_i) v_i}{\\Phi(q_t) \\sum_{i=1}^{N} \\Phi(k_i)}\nBy leveraging the associative property of matrix multiplication, the complexity per head is reduced to O(Nd'\u00b2), which scales linearly with the sequence length.\nChoices of Feature Map (\u00b7). The primary distinction between various linear attention methods lies in the choice of feature maps (\u00b7). Considering SM(\u00b7, \u00b7) is a PSD kernel function and the chosen feature map & must satisfy two properties:\n1. Non-negativity. To preserve the non-negative values in the approximation of SM, previous methods utilize activation functions like $(x) = 1 + ELU(x) (Katharopoulos et al., 2020) or (x) = ReLU(x) (Qin et al., 2022; Han et al., 2023a). Other approaches connect SM with Gaussian kernel that uses $(x) = exp(\\frac{x}{||x||^{2}}), incorporating trigonometric or random positive features.\n2. Low Entropy. It has been observed the attention-weights distribution in standard Trans-formers tends to be more \"spiky\" in linear ones, exhibiting lower entropy (Zhang et al., 2024a). To rescale the query-key dot products back to the original magnitudes, techniques such as Taylor expansion (Keles et al., 2023) or higher norms on the numerical value of query and key (Han et al., 2023a) have been employed.\nHowever, using non-negative feature maps inherently results in the loss of information from the original negative values, which may carry important information in the original dot product calcu-lation. This leads to discontinuities in the linear attention map compared to the standard attention. Furthermore, existing rescaling strategies (Han et al., 2023a) manually select a fixed norm across all dimensions, i.e., \u0444(x) = fp(ReLU(x)), where fp(x) = \\frac{x}{||x||^{p}}. This fixed norm p may not be optimal across different datasets."}, {"title": "PROPOSED APPROACH", "content": "In this section, we present a novel polarity-aware attention mechanism that accurately captures query-key interactions without incurring additional computational overhead. Our method incorpo-"}, {"title": "POLARITY-AWARE ATTENTION", "content": "The key idea behind polarity-aware attention is to address the limitations of existing linear attention mechanisms, which often discard valuable information from negative components. We start by decomposing the query vector q = {qi}i\u2208[d] \u2208 Rd and key vector k = {ki}i\u2208[d] \u2208 Rd element-wise into their positive and negative components:\nq=q^+-q^-, k=k^+-k^-,\\\\\nwhere q+ = max(qi, 0) and q\u2212 = max(\u2212qi, 0), representing the positive and negative parts of q, respectively, and similarly for k. Substituting these decompositions into the inner product of q and k gives:\n<q, k> = (q^+, k^+) + (q^-,k^-) - (q^+, k^-) - (q^-,k^+)\nThe first two terms capture the similarity between same-signed components, while the latter two terms represent interactions between opposite-signed components. Previous linear attention ap-proaches, such as ReLU-based feature maps, eliminate negative components by mapping them to zero, resulting in significant information loss when approximating query-key dot products.\nTo address this, our polarity-aware attention mechanism separates query-key pairs based on their polarity, computing their interactions independently. The attention weights are calculated as follows:\nSM(q, k) = exp(q^kT)\n\u2248 (\u03a6(q+)\u03a6(k+)^T + (\u03a6(q\u2212)\u03a6(k\u2212))) - (\u03a6(q+)\u03a6(k\u2212)^T + \u03a6(q\u2212)\u03a6(k+)).\nThis formulation recovers the information embedded in both positive and negative components.\nLearnable Polarity-aware Mixing. While this formulation captures key information carried by both same-signed and opposite-signed components, directly subtracting opposite-signed similarities can violate non-negativity constraints, leading to unstable training and suboptimal performance. To avoid the pitfalls of subtractive operation, we instead resort to a learnable mixing mechanism that weighs the contributions of same-signed and opposite-signed query-key similarities.\nMore concretely, we split each value vector v \u2208 \\mathbb{R}^{N \\times d} along the d dimension into two halves to separately handle same- and opposite-signed response, i.e., v = [vs; v\u00b0], where both vs and v\u00ba have a dimensionality of d/2. The output attention is then computed as:\nOt = \\frac{ ([q^+; q^-]) \\sum_{i=1}^{N} \\Phi([k_i^+; k_i^-])^T v^s G^s  } {([q^+; q^-]) \\sum_{i=1}^{N} \\Phi([k_i^+; k_i^-])^T} \\\\\n+  \\frac{([q^+; q^-]) \\sum_{i=1}^{N} \\Phi([k_i^-; k_i^+])^T v^o G^o}{([q^+; q^-]) \\sum_{i=1}^{N} \\Phi([k_i^-; k_i^+])^T},\nwhere [, ] denotes concatenation operation. GS \u2208 RN and G\u00b0 \u2208 RN are two learn-able polarity-aware coefficients matrices ap-plied with element-wise multiplication, which are expected to learn the complementary re-lationship between same-signed and opposite-signed values. As shown in Figure 3, there is a clear negative correlation and value dis-crepancy between the weights learned in G and G\u00b0, which evidences our learnable mixing strategy compensates for the relaxed subtraction operation in Equation (5).\nLow-Rank SM. Previous theoretical work (Verma, 2021) has shown that SM is inherently low-rank, particularly in higher layers where the spectrum distribution becomes more skewed. This property can lead to degenerate solutions when learning value vectors, especially when compact representations are required to accommodate polarity-aware information in our case. We explore various techniques such as depthwise and deformable convolutions to increase the rank, which can refer to the ablation study in Section 5.4."}, {"title": "REDUCING ENTROPY IN LINEAR ATTENTION VIA LEARNABLE POWER FUNCTIONS", "content": "Softmax-free linear attention mechanisms often exhibit higher entropy compared to softmax-based attention, leading to less sharp value vector attention, which is detrimental to tasks requiring precise attention. To recover the low entropy characteristics observed in softmax-based attention, we rein-terpret each row in SM(q, k\u00af) as a generalized unnormalized positive sequence x = (x1, ..., XN) and analyze its entropy using our proposed positive sequence entropy (PSE) measure, defined as:\nDefinition 1 (Positive Sequence Entropy (PSE)). Let a sequence x = (x1, ..., XN), in which xi \u2265 0, i = 1, . . ., N, and s = \u2211i=1 xi > 0. Then the entropy of this positive sequence is defined by:\nPSE(x) = E(x) = - \\sum_{i=1}^{N} \\frac{X_i}{S} log(\\frac{X_i}{S}), S = \\sum_{i=1}^{N} Xi.\nWith PSE(\u00b7) defined, we now seek a function g(\u00b7) that can be applied element-wise to $(q\u00b2) and $(K) = [(k\u00b9), . . ., (kN)] such that the PSE of the i-th row of the linear attention map is reduced. The following theorem formalizes the conditions under which this reduction in PSE can be achieved.\nTheorem 1. Let x, yn \u2208 Rd for n = 1, . . . N, and let g : [0, +\u221e) \u2192 [0, +\u221e) be a differentiable function satisfying the condition g'(x) > 0 and g'(x) > 0 for all x > 0. Then, there exists such a function g such that the PSE of the transformed sequence is strictly less than that of the original sequence. Specifically, we have:\nPSE((g(x), g(y\u00b9)),..., (g(x), g(y^~))) < PSE((x, y\u00b9), . . ., (x, y)).\nThis theorem also provides insights into why commonly used feature maps & such as ReLU or ELU +1 fail to reduce entropy effectively, as they do not satisfy the necessary conditions of having both a positive first and second derivative across their entire domain.\nTo select a suitable function g, There exists a wide variety of functions g that meet these condi-tions. However, for the sake of model simplicity and efficiency, we opt for the most straightforward choice: a power function with an exponent greater than 1. Additionally, as different dimensions may contribute unequally to the similarity computation, we design learnable exponents to capture the varying importance of each dimension, formalized as follows:\np = 1 + a  sigmoid(w1,...,wd), g(x; p) = (x^1,...,x^d)\nwhere a > 0 is a hyper-parameter scaling factor and [w1, ..., wa] are learnable parameters. There-fore, the feature map in our linear attention can be expressed as $(x+) = g(ReLU(x); p) and (x) = g(ReLU(-x); p), where x refers to either q or k.\nComplexity Analysis. We now analyze the complexity complexity of PolaFormer and demonstrate its linear complexity. Let d denote the number of channels, d' the dimensionality after kernelized, and k the kernel size of convolution (d' = d since g() does just a element-wise mapping). The computational cost for query, key, value, coefficients G\u00ba and G\u00ba and outputs projections is 5Nd2. Performing matrix multiplication for (Q, K, V) across each head requires 4Ndd'. The convolution operation contributes k2Nd, while the element-wise multiplication of polarity-aware coefficients Gs and Go requires Nd computations. Summarizing these components, the total complexity of PolaFormer is given in Equation (10), confirming its linear complexity w.r.t. sequence length N.\n\u03a9 = 5Nd\u00b2 + 4Ndd' +k\u00b2Nd+ Nd"}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate our PolaFormer model on three tasks: image classification on ImageNet-1K (Deng et al., 2009), object detection and instance segmentation on COCO (Lin et al., 2014), and semantic segmentation on ADE20K (Zhou et al., 2019), comparing its performance with previous efficient vision models. Additionally, we assess PolaFormer on the Long Range Arena (LRA) task (Tay et al., 2021) to compare against other linear attention models. We first train PolaFormer from scratch on the image classification task, then fine-tune the pre-trained model on ADE20K dataset for segmentation and COCO dataset for detection. The models were pretrained on 8 NVIDIA A800 GPUs and fine-tuned on 8 NVIDIA RTX A6000 and 8 NVIDIA RTX 3090 GPUs."}, {"title": "IMAGENET-1K CLASSIFICATION", "content": "The ImageNet-1K (Deng et al., 2009) dataset is the widely used dataset for image classification tasks, containing 1,000 categories and over 1.2 million training images. We comprehensively as-sess our model's performance using Top-1 accuracy, and compare it against recent state-of-the-art efficient Vision Transformer (ViT) models. Specifically, we selected four representative ViT back-bones: DeiT (Touvron et al., 2021), PVT (Wang et al., 2021), PVTv2 (Wang et al., 2022) and Swin-Transformer (Liu et al., 2021). We replaced their self-attention modules with the our proposed polarity-aware attention module and trained these Pola-variants from scratch on ImageNet-1K.\nResults. The experimental results are presented in Table 1 and Table 5, consistently showing that our model outperforms the baseline models. For instance, in Table 1, our DeiT-T-PolaFormer surpasses other DeiT variants from 0.5% to 6.3%. In Table 5, the PVT-T/S-PolaFormer obtain an increase of 3.7% and 2.1% comparing with the corresponding baseline with comparable FLOPs. Additionally, our method integrated in Swin and PVTv2 achieves a better balance between performance and ef-ficiency. These results demonstrate that the PolaFormer enhances the expressive capability of the attention mechanism and can be widely applied in various attention-based models.\nEfficiency Analysis. We visualize the efficiency comparison between the proposed PolaFormer and other linear attention approaches with similar FLOPs in the first two plots of Figure 4. The results show that our model can achieve comparable performance with significantly less compu-tation. Furthermore, we evaluate the inference speed of PolaFormer. To be specific, we test the PVT-PolaFormer and Swin-PolaFormer on RTX3090 and RTXA6000 platforms, as shown in the third and forth plots of Figure 4. PVT-PolaFormer achieves 1.15\u00d7 and 1.12\u00d7 faster inference speed and Swin-PolaFormer achieves 1.32\u00d7 and 1.29\u00d7 faster, both with comparable or higher accuracy. These figures highlight the excellent trade-off between accuracy and latency that our model provide."}, {"title": "OBJECT DETECTION AND INSTANCE SEGMENTATION", "content": "We further validate the effectiveness of the proposed approach across various vision tasks, including object detection task on the COCO dataset (Lin et al., 2014), which contains over 118K training images and 5K validation images. We integrate Pola-Swin and Pola-PVT separately as the back-bone into Mask-RCNN (M) (He et al., 2017), RetinaNet (R) (Lin et al., 2017) and Cascade Mask"}, {"title": "SEMANTIC SEGMENTATION", "content": "A similar phenomenon was observed when fine-tuning our pre-trained model for pixel-wise se-mantic segmentation tasks on the ADE20K dataset. ADE20K (Zhou et al., 2019) provides a di-verse set of annotations for scenes, objects, and object parts, containing 25,000 images of complex scenes with various objects in natural spatial environments. We integrate Pola-Swin and Pola-PVT with ImageNet-1K pre-trained weights into two segmentation models, SemanticFPN (Kirillov et al., 2019) and UperNet (Xiao et al., 2018), using mIoU as the evaluation metric. The results, shown in Table 2 (right), demonstrate a performance improvement in mIoU ranging from 1.2% to 2.6%. These findings further highlight the versatility of our model, showing that it can be effectively fine-tuned and adapted to a wide range of vision tasks."}, {"title": "ABLATION STUDY", "content": "Impact of Components. We evaluate the effectiveness of each component in PolaFormer. As shown in Table 3, to address the low-rank issue of the attention map, we exam-ine the impact of incorporating deformable convolutions (DCN) and depth-wise convolutions (DWC) in row 1 and row 4, respectively. DWC demonstrates better adaptabil-ity, achieving an accuracy of 74.6%. It is important to note that our model is agnostic to the choice of convolution modules. Furthermore, adopting polarity coefficients Gs and G\u00ba yields a 1.8% improvement in row 3 and 4, indicating that the model effectively learns the complementary rela-tionship between same-signed and opposite-signed values."}, {"title": "CONCLUSION", "content": "In this work, we presented PolaFormer, a novel efficient transformer with linear complexity. Our PolaFormer is built on two properties of the original softmax attention: (i) making each element of the attention weight non-negative and (ii) making attention weight spikier. To fulfill these properties, we computed the similarity in a polarity-aware form to avoid neglecting negatives; theoretically, we proposed a family of element-wise functions to lower the entropy and employ a learnable power function for simplicity and rescaling. Besides, we used convolution to alleviate the problem of de-generate solutions caused by the low-rank property of SM and introduced polarity-aware coefficient matrices to learn the complementary relationship between same-signed and opposite-signed values. We validated the effectiveness of the proposed PolaFormer in a series of vision tasks and additionally benchmarked on the LRA testbed to fairly compare with mainstream linear attention models. The experimental results demonstrated that our model has good compatibility with most attention-based models and measures up to a better balance between performance and efficiency."}, {"title": "APPENDIX", "content": "This Appendix provides proof and supporting lemma for Theorem 1, followed by implementation details for various vision tasks. The source code is available in the supplementary material for reference.\n\u2022 Proof. A.1: The mathematical proof and supporting lemmas of Theorem 1\n\u2022 Implementation Details. A.2: Training settings for all experiments\n\u2022 Long Sequence Efficiency\n\u2022 Comparison of the results of models with different G initializations\n\u2022 Visualization of Attention Probability Distribution's Entropy\n\u2022 Visualization of Attention Maps"}, {"title": "PROOF OF THEOREM 1", "content": "Theorem. Let x, yn \u2208 Rd for n = 1, . . . N, and dimensions are independently distributed. Given that g: [0,+\u221e) \u2192 [0, +\u221e) is a differentiable function satisfying the condition g'(x) > 0 and g''(x) > 0 for all x > 0. Then, there exists such a function g such that the PSE of the transformed sequence is strictly less than that of the original sequence. Specifically, we have:\nPSE((g(x), g(y\u00b9)),..., (g(x), g(y^~))) < PSE((x, y\u00b9), . . ., (x, y)).\nProof. We establish two lemmas to facilitate the proof of the main theorem.\nLemma 1. Let f be a function induced by g : [0, +\u221e) \u2192 [0, +\u221e) with the conditions of g'(x) > 0 and g''(x) > 0, for all x > 0, defined as:\nf((x, y)) := (g(x), g(y))\nwhere x, y \u2208 Rd\u207a, g(x) = (g(x1),..., g(xa)). Then f(x) > 0, f'(x) > 0 and f''(x) > 0, for all x \u2265 0.\nProof. Consider the element-wise function g for pairs of (x, y) with dimension d:\ng(x) = (g(x1),...,g(x)),\ng(y) = (g(y1),...,g(ya))\nThen, the inner-product between g(x) and g(y) is given by,\n{g(x), g(y)) = \u2211g(xi)g(Yi).\nBecause of the independence across dimensions, we apply Jensen's inequality, leveraging g'(x) > 0 and g''(x) > 0, yielding:\nE[f((q, k))] = E[(g(q), g(k))] = E[\u2211g(qi)g(ki)]\n=\n\u2211E[g(qi)g(ki)] = \u2211E[g(qi)]E[g(ki)]\n<\u03a3g(E[qi])g(E[ki])\n= (g(E[q]), g(E[k])) = f((E[q], E[k]))\n= f(E[(q, k)])"}, {"title": "", "content": "where E[q] = (E[q1], ..., E[qa]) denotes a vector. Consequently, we have the following results, i.e.,\nE[f((q, k))] \u2264 f(E[(q, k)]),\nindicating that f is concave function having a positive second derivative. Also, according to the definition of x and y, f is obviously mapping from [0, +\u221e) to [0,+\u221e) with a positive first derivative."}, {"title": "", "content": "Lemma 2. Given two positive values (a,b), and function f : [0, +\u221e) \u2192 [0, +\u221e) with the condi-tions of f'(x) > 0 and f''(x) > 0, we have PSE(f(a), f(b)) < PSE(a, b).\nProof. Consider the case N = 2 (extendable to N > 2). Without loss of generality, we assume a > b, c :=, then c > 1, and PSE(a, b) can be calculated as\nf(a)\nf(b)\na\nH\u2081 = -(log(  ) +log( ) ) =-(\\frac{c}{c+1}log(\\frac{c}{c+1}) + \\frac{1}{c+1}log(\\frac{1}{c+1})  )\n= -(log( ) + log( -)) = log(c+1) -log(c)\nc+1\nc+1\nThen, we apply the kernel function f on (a,b), and it is mapped to (f(a), f(b)). Then, we define d by d := , and it is easy to prove that d > c > 1. Followed by Eq. (17), we can compute PSE(f(a), f(b)) as:\na\nH2 = log(d+1) - log(d)\nThrough defining h(x) = log(x + 1) \u2212 log(x), x > 1, we have\nh'(x) = -,\nh''(x) \u2264 0, x > 1\nindicating that H\u2081 = h(c) > H2 = h(c) for all x > 1, i.e., H2 < H1. Therefore, all functions that satisfy the conditions have the effect of entropy decrease."}, {"title": "", "content": "Now come back to the theorem. Firstly, we define f induced by g that\nf((x,y)) = (g(x), g(y))\nFrom Lemma 1, we know that f is a function with positive first and second derivative. Then by using Lemma 2, we have,\nPSE(f((x, y\u00b9)), f((x, y\u00b2))) < PSE((x, y\u00b9), (x, y\u00b2))\nTherefore, the scaling effect can be achieved by the element-wise computation based on a function g with positive first and second derivative. This allows for the removal of the softmax function, enabling linear complexity and lower entropy in the attention mechanism."}, {"title": "IMPLEMENTATION DETAILS", "content": "Classification. In this task, we use the AdamW optimizer (Loshchilov & Hutter, 2019) to train all of our models for 400 epochs, including 20 epochs for linear warm-up. The basic learning rate is set to le-3 for 1024 batch size. Additionally, we use a weight decay of 5e - 2. The training framework is developed on the top of the official Swin Transformer implementation made by Microsoft.\nObject Detection. In this task, we utilize pretrained PVT models and Swin models on as the backbone and connect them to various detectors. Specifically, for the PVT model, we select from RetinaNet and Mask R-CNN as detectors, with the schedule set to 1x. For the Swin model, we"}, {"title": "", "content": "choose the detector from Mask R-CNN and Cascade Mask R-CNN as detectors, where models us-ing Mask R-CNN are experimented with under both 1\u00d7 and 3\u00d7 schedule settings, while models using Cascade Mask R-CNN case are trained under the 3\u00d7 schedule. All experiments follow themmcv-detection (Contributors, 2018) project. The training epoch is set to 12 per schedule and we use the AdamW optimizer with a learning rate of 1e - 4 and a weight decay of 1e - 4.\nSemantic Segmentation. we employ pretrained PVT models and Swin models on two rep-resentative segmentation models, SemanticFPN and UperNet. The task is conducted basedmmcv-segmentation (Contributors, 2018) project. The training interation is set to 40000 for PVT-SFPN models, 160000 for Swin-UperNet models by using AdamW optimizer with a learning rate of 2e - 4 and a weight decay of le - 3.\nLong Range Arena. We evaluate the PolaFormer based on the official implementation of Skyformer (Chen et al., 2021). For Listops and Text Classification, we set batch size to 32 with 1e - 4 learning rate. For Pathfinder, we set batch size to 128 with 5e - 4 learning rate. For Image Classification, we set batch size to 256 with le - 4 learning rate. For Retrieval sub-task, we set batch size to 16 with 2e - 4 learning rate. All models are trained from scratch using the AdamW optimizer."}, {"title": "LONG SEQUENCE EFFICIENCY", "content": "To evaluate the scalability of our model in such settings, we performed experiments on the Long-Range Arena (LRA) benchmark. These results demonstrate PolaFormer's efficiency and scalability for both high-resolution vision tasks and long-sequence NLP applications."}, {"title": "COMPARISON OF THE RESULTS WITH DIFFERENT INITIALIZATIONS OF COEFFICIENTS MATRICES", "content": "To assess the impact of G initialization on downstream tasks, we conducted additional experiments using five distinct initialization methods. These experiments were performed on a text classification (TEXT) task in Long Range Arena (LRA) with a sequence length of 4k, maintaining the same exper-imental setup as described in Table4. The initialization strategies tested included Kaiming uniform, zero initialization, normal distribution (N(0, 1)), uniform distribution (U(0, 1)), and constant ones. The results are summarized in the table below:\nInit Comparison Kaiming Uniform Zeros Normal(0,1) Uniform(0,1) Ones\nAcc 73.06 72.17 74.30 74.40 70.70"}]}