[{"title": "A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration", "authors": ["Yingqian Cui", "Pengfei He", "Xianfeng Tang", "Qi He", "Chen Luo", "Jiliang Tang", "Yue Xing"], "abstract": "Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.", "sections": [{"title": "1 Introduction", "content": "Few-shot Chain-of-Thought (CoT) prompting has emerged as a highly effective technique to enhance the reasoning capabilities of large language models (LLMs) [27]. Given a few examples of step-by-step reasoning at the inference stage, the model can generalize the reasoning process to new tasks, demonstrating significant improvement in solving complex problems, particularly in mathematical reasoning and common-sense inference [27, 12, 18].\nAside from these empirical successes, recent efforts have provided valuable theoretical insights into CoT. In the literature, two main approaches are typically used to analyze the ability of transformers in CoT tasks (and more broadly, ICL). In the first approach, to show the existence of transformers that are capable of performing CoT, people explicitly construct a transformer by specifying its parameters and assigning each component to perform a specific task. The second approach defines a specific data format to organize different reasoning steps and then trains a transformer model to learn a step-by-step prediction process. Subsequent analysis focuses on the properties of this trained model. Using these approaches, previous studies are able to explain the expressiveness power of the transformer in CoT tasks and understand how different components of the model contribute to the multi-step reasoning [16, 17, 11, 23].\nHowever, a key limitation of these analyses is that they often overlook the connections among multiple reasoning steps. In the approach where a transformer is directly constructed to perform CoT, predictions at each reasoning step are based solely on the result from the previous step. Similarly, when training a transformer from scratch on CoT tasks, to simplify the analysis, a \"filtering\" process is used to retain only the information from the most recent step [16]. In summary, when constructing/training the transformer from either way mentioned above, the prediction process for a training sample (a CoT prompt) involves multiple reasoning steps, and each step only focuses on the immediate task at hand without incorporating earlier reasoning steps. We refer to this prediction process as \"Stepwise"}, {"title": "ICL\" (formally defined in Definition 3.1).", "content": "However, in real-world scenarios, e.g., next token prediction, the LLM takes into account all the previous tokens in the context window, rather than treating each step in isolation (referred to as \"Coherent CoT\", defined in Definition 3.2). Observing this discrepancy between Stepwise ICL and Coherent CoT, we extend the theoretical framework in [31] and study the properties of a model trained using Coherent CoT and compare it with the model trained with Stepwise ICL. Based on our result, using Coherent CoT instead of Stepwise ICL during the training stage provides better prediction performance. Intuitively, when treating CoT as a holistic process where later steps integrate the reasoning from earlier steps the transformer will consider the potential errors in previous predictions and adjust subsequent predictions accordingly, which provides a form of self-correction and enhances the prediction performance.\nWhile Coherent CoT potentially outperforms Stepwise ICL as elaborated in our first contribution, it requires training the model on the entire reasoning chain, which alters the model's optimal parameters and changes the behavior of the model. This change introduces uncertainty about which steps in the reasoning chain are most sensitive to errors, highlighting the necessity for sensitivity analysis. This leads to our second contribution, which focuses on the sensitivity of the trained Coherent CoT model to perturbations in the demonstration examples at the inference stage. To quantify the sensitivity, we examine how the Coherent CoT model reacts with random perturbations at different reasoning steps. We reveal that, during inference, the Coherent CoT model is more sensitive to noise in the intermediate reasoning steps of the demonstration examples than to inaccuracies in their final outcomes.\nInspired by the sensitivity analysis, our third contribution is to propose a prompt composing method to enhance CoT performance at the inference stage. Based on our sensitivity result, CoT is more sensitive to possible incorrectness at the intermediate reasoning steps, thus improving the accuracy of these steps can better enhance the overall CoT performance. We propose to incorporate both correct and incorrect reasoning paths in the demonstrations to enhance the accuracy of the intermediate reasoning steps. Experiments are conducted to validate the effectiveness of the proposed method."}, {"title": "2 Related Works", "content": "Empirical Insights in CoT. Chain-of-Thought (CoT) prompting, introduced by [27], has proven to be highly effective in enhancing the reasoning capabilities of LLMs by breaking complex tasks into step-by-step processes. This technique has been expanded into various variants and extensions, including Zero-shot COT [15], Self-Consistency [26], Auto-CoT [32], Tree-of-Thought [30], and Graph-of-Thought [4], to further improve efficiency or model performance.\nBuilding upon the success of CoT, recent studies seek to deepen the understanding behind CoT by empirically exploring its mechanisms. For example, [28] finds that CoT enables the model to maintain the attention robust and stable on the semantically relevant tokens in the prompt. [19] defines the key components of a prompt as symbols, patterns, and text, and investigates how each of these elements and their interaction contribute to the superior performance of CoT. Additionally, [25] validates that the relevance of the demonstration example to the query and the correct ordering of reasoning steps are key factors for the effectiveness of CoT, and [14] examines the relationship between CoT's effectiveness and reasoning step length. Based on [14], simpler tasks require fewer reasoning steps, while more complex tasks benefit greatly from more detailed inference sequences.\nTheories in ICL and CoT. The mechanism of ICL has been extensively studied in the theoretical literature. For example, studies such as [24, 1, 2, 31, 13] have explained how ICL learns to perform linear regression using gradient descent. The work by [8] extends these analyses by investigating how transformers can apply ICL to non-linear functions, while [3] focuses on generalized linear models, ridge regression, and LASSO. Additionally, [10, 7] explains why multi-head attention is preferred than single-head attention when performing ICL.\nBesides ICL, recent research also starts to establish theoretical frameworks to understand CoT. For example, Li et al. [16] offers a specific framework that views CoT as a series of ICL components, each addressing a smaller subproblem. [11] constructs a transformer that solves arithmetic and linear equation tasks using CoT. [20] offers a Bayesian perspective on how intermediate steps improve reasoning. Additionally, [17] provides a TC\u00b0 upper bound for the expressiveness of constant-precision transformer, and [23] introduces a two-level hierarchical graphical model to explain how LLMs generate sequences of reasoning steps."}, {"title": "3 Theoretical Results", "content": "We extend the existing theory of ICL in transformers, e.g., [31], to a CoT scenario to conduct our theoretical investigation. Briefly speaking, there are two key observations: (1) Compared to Stepwise ICL, using"}, {"title": "Coherent CoT at the training stage results in a model\nwith better inference performance. (2) When noises\nexist in the demonstration examples at the inference\nstage, the model with Coherent CoT is more sensitive\nto perturbations in the reasoning steps than the inac-curacies in the final response.", "content": "In the following, we introduce some setups in Section\n3.1 for data generation and the transformer architec-\nture, then present the theoretical results for (1) and\n(2) respectively in Section 3.2 and 3.3.\n3.1 Model Setup\nData generation process. Since demonstration\nexamples are needed in the prompt in CoT, we de-\nfine how the examples as well as the query data are\ngenerated as follows.\nAssumption 3.1 (Data Generation Process) In\neach prompt, the examples $(x_i, z_i, y_i)$ and the query\ndata $(x_q, z_q, y_q)$ are i.i.d sampled from the following\n\"two-layer\" noisy regression:\n\u2022 The independent variable $x \\in \\mathbb{R}^d \\in \\mathcal{N}(0, I_d)$.\n\u2022 The intermediate response $z = \\beta^T x$.\n\u2022 The final response $y = z + \\epsilon$, $\\epsilon \\in \\mathcal{N}(0, \\sigma^2)$.\nFor each prompt, all the examples and the query data\nshare the same $\\beta$. In different prompts, $\\beta$ is i.i.d. uni-\nformly sampled from unit sphere, i.e., $|\\beta|_2 = 1$.\nAssumption 3.1 is primarily based on [16] with some\nmodifications about the relation between $y$ and $z$. Fol-\nlowing [16], we assume $x$ follows a Gaussian distribu-\ntion to simplify the analysis. In terms of the final\nresponse $y$, we assume $y = \\beta^T x + \\epsilon$, a linear mapping\nof $x$ with added noise. To improve the prediction of $y$,\nan intermediate response $z = \\beta^T x$ is introduced. As\na noise-free representation of the relationship between\n$x$ and $y$, introducing $z$ helps to mitigate the impact of\nnoise $\\epsilon$ and guide a more accurate prediction of $y$. An\nadditional discussion on potential relaxations of the\nassumptions can be found in Remark 3.1.\nCoherent CoT and Stepwise ICL. We define\n\"Coherent CoT\" and \"Stepwise ICL\" as follows:\nDefinition 3.1 (Stepwise ICL) There are two sep-\narate ICL steps involved in the Stepwise ICL process,\nand each is performed by a different model. First, the\nintermediate response $z_q$ is predicted using the model\n$f_1$ (to be defined later) as $z_q = f_1(E_{ICL}^{(1)})_{d+1,D+1}$,\nwhere the input prompt is formatted as\n$E_{ICL}^{(1)} = \\begin{pmatrix}\nx_1 & x_2 & ... & x_D & x_q \\\\\nz_1 & z_2 & ... & z_D & 0\n\\end{pmatrix} \\in \\mathbb{R}^{(d+1) \\times (D+1)}$."}, {"title": "(1)", "content": "Obtaining $z_q$, the input prompt of the second step is\n$E_{ICL}^{(2)} = \\begin{pmatrix}\nz_1 & z_2 & ... & z_D & z_q \\\\\ny_1 & y_2 & ... & y_D & 0\n\\end{pmatrix} \\in \\mathbb{R}^{2 \\times (D+1)}$. (2)\nThe final prediction is obtained using the model $f_2$ as\n$\\hat{y}_q = f_2(E_{ICL}^{(2)})_{d+1,D+1}$.\nDefinition 3.2 (Coherent CoT) An Coherent\nCoT process involves two steps. In the first step, the\ninput prompt is formatted as:\n$E_{CoT}^{(1)} = \\begin{pmatrix}\nx_1 & x_2 & ... & x_D & x_q \\\\\nz_1 & z_2 & ... & z_D & 0 \\\\\ny_1 & y_2 & ... & y_D & 0\n\\end{pmatrix} \\in \\mathbb{R}^{(d+2) \\times (D+1)}$. (3)\nThe intermediate response $z_q$ is predicted as $z_q =$\n$f(E_{CoT}^{(1)})_{d+1,D+1}$. After $z_q$ is obtained, it is plugged\ninto the input prompt, forming:\n$E_{CoT}^{(2)} = \\begin{pmatrix}\nx_1 & x_2 & ... & x_D & x_q \\\\\nz_1 & z_2 & ... & z_D & z_q \\\\\ny_1 & y_2 & ... & y_D & 0\n\\end{pmatrix} \\in \\mathbb{R}^{(d+2) \\times (D+1)}$. (4)\nThe final prediction of the query input is made by $\\hat{y}_q =$\n$f(E_{CoT}^{(2)})_{d+2,D+1}$ using the same transformer.\nFor both Stepwise ICL and Coherent CoT, we con-\nsider that the input prompt follows a structured data\nformat. While some existing literature attempts to re-\nlax the assumption on the restrictive structured data\nformat condition, e.g., [29], it is observed that the per-\nformance of ICL with structured data serves as a lower\nbound for the best possible performance.\nThe main difference between Coherent CoT and Step-\nwise ICL is that, in the second step of Stepwise ICL,\nthe input prompt only includes the intermediate and\nfinal responses, $z_i$s and $y_i$s, of the in-context examples.\nIn contrast, Coherent CoT plugged the predicted $z_q$\nback into the original input prompt to form the input\nprompt for the second step and retains the initial in-\nputs $x_i$s. This allows Coherent CoT to leverage both\nthe intermediate reasoning and the original input when\nmaking the final prediction.\nModel architecture. We follow [31] and use trans-\nformers with one single-head linear attention layer as\nthe model, which is defined as\n$f(E) = W_{out} W_V E \\cdot ((W_K E)^T (W_Q E))$, (5)\nwhere $E$ denotes the input prompt, $W_Q, W_K, W_V \\in$\n$\\mathbb{R}^{m \\times m}$ refer to the key, query and value matrix of the\nattention node and $W_{out} \\in \\mathbb{R}^{m \\times m}$ refers to a fully-\nconnected layer conducted to the output of the atten-\ntion node. For $f_1(\\cdot)$ in Stepwise ICL, $m = d + 1$; for\n$f_2(\\cdot)$, $m = 2$. For CoT, $m = d + 2$."}, {"title": "Training objectives. For both Stepwise ICL and\nCoherent CoT, to train a model, we minimize the fol-lowing loss function:\n$L(\\Theta) = E_{\\{x\\},x_q} (y_q - \\hat{y}_q)^2$,\n(6)\nwhich represents the mean squared error (MSE) be-tween the predicted response $\\hat{y}_q$ and the true response\n$y_q$ of the query example. Here, $\\Theta$ denotes the set of\nparameters in the transformer.", "content": "3.2 Coherent CoT vs Stepwise ICL\nThis section presents the main results of comparing\nCoherent CoT and Stepwise ICL. To simplify the\nderivation, we assume a specific format for the opti-\nmal attention parameters as follows:\nAssumption 3.2 We considerthe following specific\nformulations of the matrices $W_K, W_Q, W_{out}$ and $W_V$\nfor Stepwise ICL and Coherent CoT.\n\u2022 For Stepwise ICL, the specific format for the pa-\nrameters of $f_1(\\cdot)$ is: $(W_K)^T W_Q = \\begin{bmatrix}\nu_x I_d & 0 \\\\\n0 & u_z\\end{bmatrix}$,\nand $(W_{out} W_V)_{d+1,:} = (0,...,0,1/u_x)$. For\n$f_2(\\cdot)$, we assume $(W_K)^T W_Q = \\begin{bmatrix}0 & 0 \\\\\n0 & u_x\\end{bmatrix}$\nand $(W_{out} W_V)_{2,:} = (0,1/u_z)$.\n\u2022 For Coherent CoT, we assume\n$(W_K)^T W_Q = \\begin{bmatrix}\nu_x I_d & 0 & 0 \\\\\n0 & u_z & 0 \\\\\n0 & 0 & u_y\\end{bmatrix}$,\nand $(W_{out} W_V)_{d+1,:} = (0,..., 0,1/v_x, 0)\n$(W_{out} W_V)_{d+2,:} = (0, . . ., 0, 1/v_y).\nand\nand\nAssumption 3.2 is built upon the observations in [31,\n10]. Based on these studies, in order to optimize the\ntask of $x \\rightarrow z$ in ICL, the corresponding off-diagonal\nelements of $(W_K)^T W_Q$ and the first $d$ elements in the\n$(W_{out} W_V)_{d+1,:}$ vector should be all zero when $x$ fol-\nlows $\\mathcal{N}(0, I_d)$. Since each step of the Coherent CoT\nprocess utilizes the same transformer model, we con-\nsider the same format for the $z \\rightarrow y$ process, and as-\nsume all off-diagonal elements of $(W_K)^T W_Q$ are zero\nand $(W_{out} W_V)_{d+1,:} = (0,..., 0, v_y)$. With Assump-\ntion 3.2, we can rewrite $L(\\Theta)$ to $L(u_x, u_z)$ for Step-\nwise ICL or $L(v_x, v_y, v_z)$ for Coherent CoT to highlight\nthese parameters.\nGiven Assumption 3.2, we figure out the optimal solu-\ntion of Stepwise ICL in Theorem 3.1.\nTheorem 3.1 Under Assumption 3.1 and 3.2, the op-\ntimal expected loss of Stepwise ICL is achieved when"}, {"title": "ux \u2260 0 and uz \u2260 0 are satisfied. The corresponding\noptimal loss is\n$L(u_x^*, u_z^*) = \\sigma^2 + \\frac{7+d}{D} \\sigma^2 + \\frac{7}{D} + o(\\frac{1}{D})$.", "content": "The proof of Theorem 3.1 can be found in Ap-\npendix A.1. In short, we separate the MSE loss of\nthe prediction into multiple terms and taking the ex-\npectation of each term considering $D \\rightarrow \\infty$.\nRemark 3.1 In Assumption 3.1, we assume that $x \\sim$\n$\\mathcal{N}(0, I_d)$. The exact Gaussian distribution is used to\nderive the closed-form expression of the loss. If we\nrelax it to other distributions, there will be no closed-\nform expression of the loss to exactly compare Coher-\nent CoT and Stepwise ICL. Nonetheless, the high-level\nintuition on why Coherent CoT outperforms Stepwise\nICL still holds: Stepwise ICL misses information on\nthe previous reasoning steps, thus the prediction is\nworse than Coherent COT.\nTo compare with the optimal results of Stepwise ICL,\nwe derive the optimal solution for Coherent CoT and\npresent the results in Theorem 3.2.\nTheorem 3.2 Under Assumption 3.1 and 3.2, the op-\ntimal parameters $v_x, v_y$ and $v_z$ that minimize the Co-\nherent CoT's loss satisfying $v_y = (u_x + z)$. The cor-\nresponding loss for Coherent CoT becomes\n$L(v_x, v_y, u_z) = \\sigma^2 + \\frac{d \\sigma^2}{D} + \\frac{1+d}{D} + \\frac{4 u_x u_z}{D v_y} + \\frac{6 v_z^2}{D v_y^2} +\\frac{4 u_x u_z^2}{D v_y^2}$.\nTo minimize the above loss, it is require that $v_y =$\n$\\frac{(d - 1)^2 + 2 v_x}{(d - 1) \\sigma^2 - 2 u_z} \\sigma^2 + \\frac{(d - 1)^2 + 2}{4 u_x u_z} (where $u_z, v_x, v_y \\neq 0$).\nThen the optimal expected loss of Coherent CoT is\n$L(v_x^*, v_y^*, u_z^*) = \\sigma^2 + \\frac{d \\sigma^2}{D} + \\frac{1+d}{D} - \\frac{((d - 1)^2 + 2) \\sigma^4}{D ((d - 1)^2 + 2)}$.\nThe proof of Theorem 3.2 is similar to that of Theo-\nrem 3.1, whose details are shown in Appendix A.2.\nBuilding on the above results, Proposition 3.1 below\ndirectly compares the expected loss of Coherent CoT\nand Stepwise ICL.\nProposition 3.1 Given that $d \\geq 2$, the expected loss\nof Coherent CoT equals to the expected loss of Step-\nwise ICL when $v_x = 0$ and $v_z = v_y$. In addition, the\nminimal expected loss of Coherent CoT is smaller than\nthe one of Stepwise ICL."}, {"title": "The proof of Proposition 3.1 is in Appendix A.3.\nProposition 3.1 indicates that, regardless of the values\nof $d$ and $\\sigma$, the optimal expected loss of Coherent CoT\nis smaller than that of Stepwise ICL. To explain this,\nwhen $v_x = 0$ and $v_z = v_y$, Coherent CoT is equivalent\nto Stepwise ICL, and this condition also aligns with\nthe optimal solution of Stepwise ICL. On the other\nhand, since $(v_x = 0, u_z = v_y)$ is not the optimal so-\nlution of Coherent CoT, Coherent CoT can achieve a\nsmaller loss given its corresponding optimal solution.", "content": "Insights from the theory. To further investigate\nhow the $x_i$s and $z_i$s contribute to the final prediction\nof $\\hat{y}_q$ in CoT, we present the following proposition:\nProposition 3.2 For any $\\sigma$ and $d$, $u_z$ and $v_y$ always\nshare the same sign for optimal Coherent CoT. In ad-\ndition, the ratio $\\frac{v_x}{v_y}$ for the optimal Coherent CoT is\nconsistently smaller 1. Meanwhile, when $v_x, u_z$ and\n$v_y$ are set such that Coherent CoT reduces to Stepwise\nICL, the ratio $\\frac{u_z}{v_y}$ satisfies $\\frac{u_z}{v_y} = 1$.\nBased on Theorem 3.2 and Proposition 3.2, the pre-\ndiction of the final response can be formulated as\n$\\hat{y}_q = \\frac{1}{D} \\sum_i \\frac{u_x}{v_y} (x_i^T x_q) + \\frac{u_z}{v_y} (z_i z_q)$. (7)\nThe above formulation indicates how $x_i$s and $y_i$s im-\npact the final prediction of $\\hat{y}_q$. According to Proposi-\ntion 3.2, in Coherent CoT, the initial inputs $x_i$s always\npositively contribute to the final prediction $\\hat{y}_q$. Addi-\ntionally, the final prediction $\\hat{y}_q$ in Coherent CoT relies\nless on the $z_i$s and $z_q$ compared to Stepwise ICL, in-\ndicating a reduced dependency on these intermediate\nvalues when using the optimal Coherent CoT.\nBased on Proposition 3.2, the consequence of leverag-\ning Coherent CoT to train a model is that, when there\nis an error in the prediction of $z_q$, Coherent CoT's re-\nduced reliance on $z_q$, along with its attention to $x_q$,\nensures that the error in the prediction of $z_q$ has a\nsmaller impact on the final prediction. Moreover, since\nthe final prediction of $y_q$ incorporates both $x_i$s and $z_i$s,\nwhen $z_q$ is inaccurate, the model can better adjust its\nprediction using the $x_i$s values. This inherently pro-\nvides a form of self-correction by leveraging the com-\nbined information from both $x_i$s and $z_i$s values.\nNotably, the theorems in Section 3.2 and the later Sec-\ntion 3.3 focus on different stages of the model's applica-\ntion. The theorem in Section 3.2 highlights that com-\npared with Stepwise ICL, using Coherent CoT during\nthe training stage provides a better inference perfor-\nmance of the model. The theorem in Section 3.3, which\nexamines the sensitivity of Coherent CoT to random\nnoise, focuses on the inference stage."}, {"title": "3.3 Sensitivity against Random Perturbation", "content": "While Section 3.2 investigates how Coherent CoT\ngains a better prediction performance compared to\nStepwise ICL by considering all the previous steps dur-\ning the reasoning process, this holistic process may\nalso lead to a different sensitivity to potential er-\nrors/corruptions in each reasoning step. Therefore, in\nSection 3.3, with a model trained with Coherent CoT,\nwe investigate the model's sensitivity and quantify the\nimpact of random perturbations at different reasoning\nsteps $y_i, x_i$, and $z_i$ at the inference stage. The results\nare summarized in the following theorems respectively.\nTheorem 3.3 Under Assumption 3.1 and 3.2, when\nthere is random perturbation $\\delta_i \\sim \\mathcal{N}(0, \\sigma^2)$ added to $y_i$,\nwe denote the loss for Coherent CoT as $L'_y(v_x, v_y, u_z)$.\nWhen $v_x, v_y$ and $u_z$ takes the optimal values, we have\n$L'(0) - L(v^*_x, v^*_y,v^*_z) = o(\\frac{1}{D})$."}, {"title": "4 Improving CoT through\nError-Aware Demonstrations", "content": "The analysis in Section 3.3 reveals that, at the infer-\nence stage, CoT is more sensitive to errors in the in-\ntermediate reasoning steps of the demonstration than\nto errors in the final outcome. Inspired by this, we\nconjecture that it is beneficial for the model to learn"}, {"title": "4.1 Methodology", "content": "We use a date understanding problem as an example to\nexplain the proposed method in detail, and present the\ndemonstration format in Figure 3. In standard CoT\nprompting, few-shot examples with correct reasoning\npaths are provided in the prompt demonstration. In\ncontrast, in the proposed demonstration format, after\npresenting the question, we first provide a potentially\nincorrect reasoning path, clearly labeled as a wrong\nsolution. We then provide a detailed explanation of\nwhy this reasoning is incorrect, pointing out specific\nmissteps or logical errors. Both the incorrect reason-\ning path and the analysis of why it is flawed are cre-\nated manually. After analyzing the incorrect path, we\nprovide a step-by-step correct reasoning process that\nleads to the correct answer.\nBy incorporating both correct and incorrect reason-\ning paths in the demonstrations, the proposed method\nteaches the model not only the correct reasoning path\nbut also how to identify and handle potential reason-\ning errors. This error-aware approach helps improve\nthe model's ability to adjust predictions and enhances\nits overall performance in reasoning tasks."}, {"title": "4.2 Experiments", "content": "In this section, we evaluate our proposed method with\nvarious LLMs on multiple benchmarks.\n4.2.1 Experimental setup\nLanguage Models. Our experiments involve four\nLLMs: GPT-3.5-Turbo [6], GPT-40-mini [6]3, Gem-\nini Pro [22]4 and DeepSeek 67B [5] 5. For generation,\nwe set the temperature to 0 to ensure deterministic\noutputs.\nBenchmarks. We use five datasets from two bench-\nmarks for the experiments: the BBH benchmark [21]\nand the GSM8k benchmark [9]6. The BBH bench-\nmark focuses on reasoning tasks, and we select Disam-\nbiguation QA, Tracking Shuffled Objects (7 objects),\nDate Understanding, and Penguins in a Table from\nthis benchmark. Each of these datasets contains 250\nexamples. Besides the BBH benchmark, we also use"}, {"title": "4.3 Main Results", "content": "In Table 1, we demonstrate the performance of various\nLLMs across different datasets when using standard\nCoT prompting (w/o IR) and our proposed method,\nwhich incorporates Incorrect Reasoning (IR) in the\ndemonstrations (w/ IR). From Table 1, we can see that\nin most cases, adding handcrafted incorrect reasoning\npaths to CoT demonstrations improves the models'\nperformance. In some settings, our proposed method\nbrings a significant improvement exceeding 5%. For\nexample, in the Tracking Shuffled Objects dataset,\nGemini Pro shows a 6.60% improvement (from 58.20%\nto 64.80%), and in Penguins in a Table, DeepSeek 67B\ndemonstrates an increase of 6.17% (from 73.97% to\n80.14%). These results highlight the positive impact\nof exposing models to incorrect reasoning paths."}, {"title": "4.4 Additional Experiments", "content": "Necessity of including error explanation. We\npresent an ablation study to evaluate our proposed"}, {"title": "5 Conclusion", "content": "This paper provides a theoretical analysis of Coherent\nCoT by investigating its advantages over Stepwise ICL,\nshowing that treating CoT as a holistic process leads\nto improved error correction and prediction accuracy."}, {"title": "In addition, we examine Coherent CoT's sensitivity to\nerrors in different reasoning steps of the demonstration\nexamples during the inference stage. We observe that\nCoherent CoT is more sensitive to the error in the in-termediate reasoning process than the inaccuracies in\nthe final response. Inspired by this result, we propose\nto incorporate both correct and incorrect reasoning\npaths in demonstrations to improve the accuracy of\nthe intermediate steps to enhance CoT performance.\nExperimental results validate the effectiveness of this\napproach.", "content": "A Proofs\nA.1 Proof of Theorem 3.1:\nThe proof begins by decomposing the mean square loss into distinct components involving attention scores. Each\nterm's expectation is then calculated separately", "f_1(E_{ICL}^{(1)})_{d+1,D+1}$\n$=(W_{out}W_V)_{d+1,": ""}, "E \\begin{pmatrix}(E^T (W_K)^T W_Q & 0)\n\\end{pmatrix}$\n$=\\frac{1}{(Dux)} [z_1, z_2,..., z_D, 0"], "f_2(E_{ICL}^{(2)})_{2,D+1}$\n$=(W_{out}W_V)_{2,": ""}, {"expressed\nas": "n$L(u_x, u_z) = E (y_q - \\hat{y}_q)^2$\n$= E_{\\{x_i\\},\\epsilon_i\\} \\in [D]} (\\beta^T x_q + \\epsilon_q - \\frac{1}{D^2}(\\sum (\\beta^T x_i + \\epsilon_i) ("}]