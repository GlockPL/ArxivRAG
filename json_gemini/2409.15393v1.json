{"title": "APPROXIMATED ORTHOGONAL PROJECTION UNIT: STABILIZING\nREGRESSION NETWORK TRAINING USING NATURAL GRADIENT", "authors": ["Shaoqi Wang", "Chunjie Yang", "Siwei Lou"], "abstract": "Neural networks (NN) are extensively studied in cutting-edge soft sensor models due to their feature\nextraction and function approximation capabilities. Current research into network-based methods\nprimarily focuses on models' offline accuracy. Notably, in industrial soft sensor context, online\noptimizing stability and interpretability are prioritized, followed by accuracy. This requires a clearer\nunderstanding of network's training process. To bridge this gap, we propose a novel NN named\nthe Approximated Orthogonal Projection Unit (AOPU) which has solid mathematical basis and\npresents superior training stability. AOPU truncates the gradient backpropagation at dual parameters,\noptimizes the trackable parameters updates, and enhances the robustness of training. We further\nprove that AOPU attains minimum variance estimation (MVE) in NN, wherein the truncated gradient\napproximates the natural gradient (NG). Empirical results on two chemical process datasets clearly\nshow that AOPU outperforms other models in achieving stable convergence, marking a significant\nadvancement in soft sensor field.", "sections": [{"title": "1 Introduction", "content": "Deep learning methods have achieved recent success in many regression areas such as natural language processing,\nprotein structure prediction, and building energy consumption forecasting. However, for these methods to be useful in\nthe industrial soft sensor field, which demands higher immediacy and stability, further research into model structure\nand the stability of the training process is necessary [1, 2, 3]. The safety and economic impact of factory impose\nstringent requirements on soft sensor models deployed online [4, 5]. For example, each mini-batch update must not\ncause significant performance fluctuations to ensure that downstream controllers and monitors do not execute erroneous\nactions; soft sensor models must be deployed online to avoid fluctuations due to changes in operating conditions and\nmodel switching [6]. Common network's training tricks are not suitable for soft sensor contexts. For example, it's not\nfeasible to use checkpoints for early stopping but to always use the latest updated checkpoint; there wouldn't be adaptive\nlearning rate changes but rather a constant learning rate maintained throughout. Experimental results demonstrate\nthat such differences lead to a substantial decline in performance. These constraints necessitate the development of\nbetter-suited network architectures for regression task that ensure more stable optimization during training [7, 6].\nMVE is the best unbiased estimator under the Mean Squared Error (MSE) criterion, essentially representing the\nperformance ceiling for regression models [8]. Unfortunately, directly applying MVE to NN is challenging due to the\ndifficulty in obtaining the likelihood distribution p(y|x) of inputs x and outputs y [9]. Traditional research on MVE has\nfocused on techniques like Kalman filtering [10, 11], algorithms based on Ordinary Least Squares (OLS) [12, 13], and\nother system identification research [14, 15, 16] which operate under linear and convex conditions. These methods,\nwhile effective within their scope, have limited expressive power [17]."}, {"title": "2 AOPU:Methodology", "content": ""}, {"title": "2.1 Trackable vs. Untrackable", "content": "Parameters within NN that can be decoupled from input data x and computed through an inner product are defined as\ntrackable parameters. Conversely, parameters that cannot be decoupled are classified as untrackable parameters.\n$f(x) =WTx = \\langle W,x \\rangle, g(x) = M(x)x = \\langle M(x),x \\rangle$\n(1)"}, {"title": "2.2 Natural Gradient vs. Gradient", "content": "Fig. 2 vividly presents the major difference between NGD and GD using a simple GPR in experiment. This GPR had\nonly two parameters, the bias of the mean and the coefficient of the kernel matrix, both constant values. We sampled\n100 instances from this GPR and updated these two parameters 100 times using these samples. It was observed that NG\nrequire a higher learning rate, while conventional gradients only need a smaller one. The major difference between NG\nand conventional gradients lies in their directions. Conventional gradients ignore the parameter manifold and treat every\nparameter equally. NG, by dividing the gradient by its second derivative, treat sensitive parameters cautiously (low\ngradient) and non-sensitive parameters boldly (high gradient). This adjustment results in different gradient directions\nand contributes to better convergence.\nNevertheless, the calculation of NG involves considering the inverse of the Fisher Information matrix, thereby introduc-\ning computational complexity cubic to the number of parameters, making it entirely infeasible for neural networks.\nExisting research on NG is almost entirely focused on conventional machine learning, e.g., considering more complex\ndistributions (such as the product of multiple exponential family) for computing NG. Research in the neural network do-\nmain on NG mostly centers on second-order optimizers (such as AdamW), which are merely first-order approximations\nof second-order NG."}, {"title": "2.3 Network's structure", "content": "We wish to emphasize that within AOPU framework, the truncated gradient of the dual parameters serves as an\napproximation of the NG of the trackable parameters, while AOPU's structured output approximates the MVE.\nThe fidelity of these approximations is measured by the RR, the closer RR is to 1, the more precise the approximation;\nconversely the closer RR is to 0, the more precision loss occurs. Furthermore, it can be demonstrated that the output of\nAOPU fundamentally differs from traditional neural networks: instead of explicitly modeling a mapping from x to y, it\nimplicitly models a mapping from x to xy. To ensure that y can be recovered from xy, it is imperative that RR equals 1.\nAOPU also guarantees the convergence of the dual parameters if the input-output relationship can be characterized by\nspecific system. The proof of above is intricate and comprehensive, and one may refer to Appendix A, B and C for\nmore detailed information. This section focuses on the implementation of AOPU.\nAOPU utilize data augmentation to replace stacked activation structures to enhance the nonlinear modeling capabilitie.\nIn such designs, the choice of the data augmentation module forms a crucial model prior. For ease of implementation,\nAOPU adopts a random weight matrix approach for its data augmentation module [51, 52]. Specifically, suppose the"}, {"title": "3 Experiments and Analysis", "content": "In this section, we detail the experimental results of the AOPU model, analyze the impact of hyperparameters on AOPU,\nits robustness regarding changes in hyperparameters, its advantages over other comparative algorithms, and some\ninherent limitations of the model. Comprehensive and detailed experiments and comparisons have been conducted on\ntwo publicly available chemical process datasets, Debutanizer and Sulfur Recovery Unit (SRU). For more information\nof the dataset please refer to Appendix D."}, {"title": "3.1 Baselines", "content": "We choose seven different NN models as baselines: Autoformer, Informer, DNN, SDAE, SVAE, LSTM, and RVFLNN,\ncovering four major domains including RNN-based networks, auto-encoder-based networks, attention-based networks,"}, {"title": "3.2 Experiment Implementation", "content": "Apart from AOPU, which is trained using the approximated minimum variance estimation loss function as previously\ndescribed, all other deep learning algorithms are trained using the Mean Squared Error (MSE) loss. AOPU's learning\nrate for gradient updates is set at 1.0, while for all other deep learning algorithms, it is set at 0.005, with the Adam\noptimizer used for gradient updates. The learning rates of all models remain static throughout the training process.\nThe experimental setup differs based on the requirements of various models regarding input dimensions. Models such\nas Autoformer, Informer, and LSTM necessitate an input that includes an additional dimension for 'sequence length'.\nThis dimension is preserved as part of the input structure for these models. Conversely, models like DNN, SDAE,\nSVAE, AOPU, and RVFLNN do not require this additional dimension. For these models, the sequence length and input\ndimensions are combined and flattened to serve as the feature dimensions in the input. AOPU's latent space size is set\nat 2048. Autoformer, Informer, SDAE, and SVAE utilize two layers each for their encoder and decoder layers; LSTM\nuses two layers of LSTM layers; RVFLNN and AOPU share identical settings. All models except AOPU and RVFLNN\nhave their latent space sizes set at 16 to ensure the trainable parameters size across all models are comparable."}, {"title": "3.3 Main Result", "content": ""}, {"title": "3.3.1 How certain we are about the inverse", "content": "According to the previous discussion, the existence of the inverse of \u017e\u017e is crucial as it does not only impact the\nnumerical stability of the model but also directly determines whether it is possible to recover y from the approximated\nmapping relationship x \u2192 xy. Clearly, the input feature dimensions d + h and the batch size b significantly affect\nwhether the inverse of \u017e\u017e exists. Specifically, the larger the batch size, the more columns z has, and the less likely\nis to be column-full-rank; conversely, the longer the sequence length and the larger the input feature dimensions, the\nmore likely \u017e is to be linearly independent and thus column-full-rank. From the following experimental results, it will\nbe clearly observed the impact of batch size and sequence length on RR."}, {"title": "3.3.2 Is the training stable", "content": "Stability is a crucial characteristic for the online deployment of deep learning models in actual production processes.\nSpecifically, the incremental updates to model parameters following the observation of new mini-batch data should have\na smooth impact on model performance. However, experimental results indicate that most networks in the soft sensor\nfield fail to achieve stable convergence. Fig. 6 provides a detailed display of how the MSE metrics for different networks\nchange with training iterations on the SRU validation dataset, with blue solid circles marked every 50 iterations. It is\nevident that all models, except Autoformer, Informer, LSTM, and AOPU, exhibit significant performance fluctuations\nas training iterations progress. The density of the blue solid circles can to some extent represent the likelihood of\ncorresponding performance fluctuations."}, {"title": "3.3.3 Quantitative analysis", "content": "To verify the reliability of the AOPU model's performance and to quantify its comparison with other methods, we\nimplemented two different training strategies. Strategy one involved an early stopping trick and used the best checkpoint\nto validate the model's performance on the test dataset. Strategy two involved training all models for 40 epochs and"}, {"title": "3.4 Ablation Study", "content": "In this section, we further investigate the effects of structural designs for augmentation through some ablation studies,\nexamining the impacts of the ReLU piecewise activation function, the Tanh smooth activation function, and normaliza-\ntion on AOPU's performance. It is important to note that if AOPU is trained using direct gradient descent without dual\nparameter updates, it actually degenerates to an RVFLNN model, and this part of the ablation study has been detailed in\nsection 3.3.3.\nFrom Table 3 we can draw two conclusions: The first is normalization significantly impairs AOPU's model performance.\nThe second it ReLU piecewise non-linear activation function suits worse for AOPU than the Tanh activation function.\nAs previously analyzed in A where both the input data x and y should to be zero mean, hence reducing the covariance\noperator R to an inner product operator. However, piecewise linear functions like ReLU and LeakyReLU are not\nzero-mean, which violates such assumptions.\nTo validate the analysis regarding the effects of zero-mean and non-zero-mean activation functions on AOPU's\nperformance, an additional comparative experiment was conducted. This experiment included 20 independent repetitions"}, {"title": "4 Conclusion and Limitation", "content": "This paper introduces a novel NN regression model, AOPU, which is grounded in solid mathematics basis and validated\nthrough extensive experiments. The results demonstrate its superior performance, robustness, and training stability. The\ndevelopment of AOPU lays the foundation for the practical implementation of deep learning soft sensor techniques\nin industrial processes and provides guidance for subsequent control, monitoring, and optimization management of\nthese processes. The introduction of RR also illuminates a promising and valuable direction for exploring the design of\naugmentation models. Such prospective topics of value encompass how to reduce the sensitivity of AOPU to batch size\nand sequence length, how to derive the NG optimization of the augmentation model, and how to bolster the nonlinear\nmodeling capability of the augmentation model.\nWe note that AOPU is not a \"plug-and-play\" model; it requires adjustments based on actual data conditions. AOPU\nnecessitates a clear understanding of the RR distribution of data intended for application to guide the selection of\nbatch size and sequence length hyperparameters. This requirement stems from the inherent matrix inversion operations\nin AOPU. When the RR value is too low, noise during the AOPU training process can greatly exceed the effective\ninformation, potentially leading to model divergence as Appendix E discusses."}, {"title": "A Network's mechanism", "content": "In this section, we first demonstrate that AOPU implements a MVE through NN, explain the relationship between data\naugmentation, minimum variance, and orthogonal projection. We then discuss the physical significance and necessity\nof the dual parameter from NGD perspective."}, {"title": "A.1 From MVE perspective", "content": "We start by giving the definition of the MVE,\nDefinition 1. Given the independent variable x and the dependent variable y, $f^*(y|x)$ is said to be the MVE for y if\nthe following hold,\n$E[(y - f^*(y|x))^2] \\le E[(y - f(y|x))^2]$\n(7)\nwhere E[] denotes the expectation operator, and f(y|x) represents any unbiased arbitrary estimation function for y\ngiven x.\nAccording to definition 1, it is straightforward that the MVE is the optimal unbiased estimator under the Mean Squared\nError loss metric, providing a performance boundary for all regression networks. However, the solution to the MVE,\ndetailed in Appendix C.1, represented as $\\int yp(y|x)dy$ where p(\u00b7) denotes the probability operator, is challenging to\ndetermine. Since the prior knowledge of the likelihood distribution p(y|x) is not accessible, this integral is difficult to\nsolve. Instead of delving into modeling the likelihood, AOPU turns to referencing solvable linear MVE operators for\nnetwork design. Specifically, when the function form of f(y|x) is constrained to be linear with respect to x, it can be\nset as $f(y|x) = W_{mve}x + b$, with the solution $W_{mve} = R_{yx}R_{x}^{-1}$ and $b = E[y] - W_{mve}E[x]$, where Rab represents the\ncovariance matrix $E [(a \u2013 E[a])(b \u2212 E[b])^T]$. For clarity, the proof is listed in Appendix C.2.\nGiven that variables x and y have been normalized to have zero mean, i.e., E[x] = 0 and E[y] = 0, it follows that b = 0,\nand the covariance operator R degenerates into an inner product operation. Revisiting the loss function of AOPU, it\nis evident from Eq. 6 that AOPU essentially estimates the parameters $W_{mve}$ of the linear MVE. Here the $(x^Tx)^{-1}$\nis aligned with the $R_{x}^{-1}$, and TD(W) ought correspond to the estimation of the input-output covariance matrix. It\nis noteworthy that the parameter $W_{mve}$ is not the estimator's output. Therefore, by approximating $W_{mve}$ in the loss\nfunction, AOPU implies an important assumption: unlike other regression algorithms that explicitly model the mapping\nrelationship from x to y, i.e., $x \\rightarrow y$, AOPU implicitly models the relationship $x \\rightarrow xy$. Given that $x \\in R^{d+h,b}$ and\n$y \\in R^{b,1}$, the key to deriving y from known \u017e and \u017ey lies in the requirement that \u017e must be column-full-rank. This\nrequirement aligns with the numerical stability needs during the computation process of AOPU, thereby establishing a\nself-consistent mathematical framework for the unit.\nThe geometric interpretation of the linear minimum variance estimator as orthogonal projection underpins the naming\nof the AOPU. AOPU differs from the orthogonal projection in threefold: (1) Orthogonal projection is a non-parametric\nbatch algorithm, whereas AOPU operates as a parametric, gradient-based mini-batch optimization algorithm. (2)\nOrthogonal projection requires the covariance matrix's inverse to exist definitively, whereas AOPU can employ\napproximate inverses for its computations. (3) Orthogonal projection strictly adheres to linear minimum variance\nestimation, but AOPU introduces non-linearity through data augmentation, allowing it to serve as a versatile minimum\nvariance estimator. The data augmentation techniques illustrated in Fig. 1 are critical for enhancing the expressive\ncapabilities of AOPU. We initially improve model expressiveness through a fixed, randomly initialized Gaussian matrix\nG. However, this approach remains confined within the linear transformation. Consequently, in subsequent experiments,\nthe inputs are further augmented using LeakyRelu($G^{T}$x), pushing the model beyond linear transformations."}, {"title": "A.2 From NGD perspective", "content": "In this section, we introduce NGD, including the computation of FIM, how to reduce the computational complexity of\nNGD through EM, and ultimately demonstrate that the truncated gradient of AOPU is an approximated NG.\nWe begin with an introduction to the most basic optimization algorithm used in neural network training, Gradient\nDescent (GD). Assuming the network parameters are represented as \u039b (\u039b represents W in AOPU), and \u2207 denotes the\ngradient, GD can be defined by the following equation:\nGD: $\\Lambda_{t+1} = \\Lambda_{t} \u2013 \\alpha \\nabla_{\\Lambda_{t}}\\mathcal{L}(\\ell_{t})$\n(8)\nwhere \u03b1 > 0 represents the network's learning rate. Many optimization algorithms assist the network in escaping local\noptima and accelerating convergence by incorporating momentum gradients and adaptive learning rates. However,\nfundamentally, these are first-order optimization methods (considering only first-order derivatives) and typically exhibit"}, {"title": "B Convergence Analysis", "content": "In this section, we are going to analyze the convergence of AOPU referencing the conclusions from [53, 54]. We\neventually demonstrate that under the condition \u017e\u017e is column-full-rank, AOPU converges to the optimal solution almost\nsurely. Firstly, thanks to the trackability of parameters, AOPU is capable of being proven a coherent optimization\nproblem under a strict assumption, which agrees on previous analysis, is made about the distribution of the observed\nsamples y during the proof; AOPU's truncated gradient is then proven to structurally ensure consistency with the\nStochastic Mirror Descent (SMD), specifically that the dual parameters correspond directly to the mirror map; the\nassumptions in [53] about regularity (assumption 3), differentiability (assumption 1), and bounded second moments\nwith Lipschitz continuity (assumption 2) are also proven to be met under the condition \u017e\u017e is column-full-rank. Finally,\nby referencing theorem 3.4 from [53], we prove that AOPU can enter arbitrarily small neighborhood of the optimal\nparameter solution W*."}, {"title": "C Mathematic Proof", "content": ""}, {"title": "C.1 Solution to General Minimum Variance Estimator", "content": "In this subsection we are about to prove that given x, the solution to the general minimum variance estimator of y\nis $\\int yp(y|x)dy$, i.e., $E_{y|x}[y]$. Since it is the expectation of likelihood, this result is intuitive to prove. Rewrite the\ncovariance calculation in the following,\n$E [(y \u2013 f(y|x))(y \u2013 f(y|x))^T]$\n$=E [(y + E_{y|x} [y] \u2013 E_{y|x} [y] \u2013 f(y|x))(y + E_{y|x} [y] \u2013 E_{y|x} [y] \u2013 f(y|x))^T]$\n$=E [(y \u2013 E_{y|x}[y])(y \u2013 E_{y|x}[y])^T] + E [(E_{y|x}[y] \u2013 f(y|x))(E_{y|x}[y] \u2013 f(y|x))^T] +$\n$E [(y \u2013 E_{y|x}[y])(E_{y|x}[y] \u2013 f(y|x))^T] + E [(E_{y|x}[y] \u2013 f(y|x))(y \u2013 E_{y|x}[y])^T]$\n(15)\nNoting that f(y|x) is not a conditional probabilistic distribution representation, it denotes a function that takes x\nas input, and the output of such function is regarded as an estimator of y. In conclusion, f(y|x) is fundamentally\nindependent of y, therefore, for the term $E [(E_{y|x}[y] \u2013 f(y|x))(y \u2013 E_{y|x}[y])^T]$ we can rewrite it into,\n$E [(E_{y|x}[y] \u2013 f(y|x))(y \u2013 E_{y|x}[y])^T]$\n$\\frac{}{\\int(E_{y|x}[y] \u2013 f(y|x))(y \u2013 E_{y|x}[y])^Tp(x,y)dxdy}$\n$\\frac{}{\\int(E_{y|x}[y] \u2013 f(y|x)) (\\int(y \u2013 E_{y|x}[y])p(y|x)dy) p(x)dx}$\n$=(E_{y|x}[y] \u2013 f(y|x))(E_{y|x}[y] \u2013 E_{y|x}[y])^Tp(x)dx$\n$=0$\n(16)\nThe conclusion also applies to the term $E [(y \u2013 E_{y|x}[y])(E_{y|x}[y] \u2013 f(y|x))^T]$. Consequently, the last two terms in Eq.\n16 consistently equal zero. Given that $E [(E_{y|x}[y] \u2013 f(y|x))(E_{y|x}[y] \u2013 f(y|x))^T]$ is semi-positive definite, it follows\nthat $E [(y \u2013 E_{y|x}[y])(y \u2013 E_{y|x}[y])^T]$ establishes a lower bound for $E [(y \u2013 f(y|x))(y \u2013 f(y|x))^T]$. Equality holds if\nand only if $f (y|x) = E_{y|x}[y]$ which completes the proof."}, {"title": "C.2 Solution to Linear Minimum Variance Estimator", "content": "In this subsection, we are about to prove that the solution to the linear minimum variance estimator is $W_{mve} = R_{yx}R_{x}^{-1}$\nand $b = E[y] \u2013 W_{mve}E[x]$. Initially, it is straightforward to see that the value of b renders the estimator unbiased. By\nsimply taking the expectation, we can complete the proof. Again we rewrite the covariance calculation in the following,\n$E [(y \u2013 f(y|x))(y \u2013 f(y|x))^T]$\n$=E [(y \u2013 E[y] + W_{mve}E[x] \u2013 W_{mvex})(y \u2013 E[y] + W_{mve}E[x] \u2013 W_{mvex})^T]$\n(17)\nIncorporating the covariance matrices $R_{yy} = E [(y \u2013 E[y])(y \u2013 E[y])^T], R_{xx} = E [(x - E[x])(x \u2013 E[x])^T]$, and\n$R_{xy} = E [(x - E[x])(y \u2013 E[y])^T]$. We can reformulate Eq. 17 as $R_{yy} + W_{mve}R_{xx}W_{m} - RyxW_{mve} - W_{mve}R_{xy}$.\nUpon simplification, this equation transforms to,\n$R_{yy} + W_{mve}R_{xx}W_{mve}^T - R_{yx}R_{x}^{-1}R_{xy}$ ==$(W_{mve} - R_{yx}R_{x}^{-1}) R_{xx}(W_{mve} - R_{yx}R_{x}^{-1})^T$\n(18)\nNoting that $(W_{mve} - R_{yx}R_{x}^{-1}) R_{xx}(W_{mve} \u2013 R_{yx}R_{x}^{-1})^T$ is again semi-positive definite, indicating that the optimal\n$W_{mve}$ is identical to $R_{yx}R_{x}^{-1}$, which completes the proof."}, {"title": "C.3 Proof to Proposition 1", "content": "Suppose there exists an operator T independent of x such that for a given W and any inputs $x_{1}$ and $x_{2}$, the Eq. 2 holds.\nFrom the linearity property of operators, it follows that,\nacti($Wx_1$) + acti($Wx_2$) = T(W)($x_1+x_2$)\nacti($Wx_1$) + acti($Wx_2$) = acti(W($x_1+x_2$))\n(19)"}, {"title": "C.4 Proof to Proposition 2", "content": "We are about to give concise and precise proof in this section, starting by proving the connection between the\nexpectation-parameter and the log-partition function.\nProposition 6. The expectation-parameter equals to the gradient of log-partition function with respect to natural\nparameter.\nProof. Since EM represents a probability distribution, the log-partition function acts as a normalizing factor, thus the\nfollowing identity holds true,\n$\\mathcal{A}(\\lambda) = log \\int h(z) exp (\\langle\\phi(z), \\lambda \\rangle)dz$.\nTherefore, expectation-parameter could be derived from differentiating A(\u03bb) with respect to \u03bb.\n(20)\n$\\nabla_{\\lambda}\\mathcal{A}(\\lambda) = \\nabla_{\\lambda} log \\int h(z) exp (\\langle\\phi(z), \\lambda \\rangle)dz$\n$\\frac{\\nabla_{\\lambda} \\int h(z) exp (\\langle\\phi(z), \\lambda \\rangle)dz}{\\int h(z) exp (\\langle\\phi(z), \\lambda \\rangle)dz}$\n$\\frac{\\nabla_{\\lambda} (\\langle\\phi(z), \\lambda \\rangle) (\\int h(z) exp (\\langle\\phi(z), \\lambda \\rangle)dz)}{\\int h(z) exp (\\langle\\phi(z), \\lambda \\rangle)dz}$\n$=E_{p(z/\\lambda)}[\\phi(z)]$\n(21)\nClearly, A(\u03bb) is the only term that is second-order derivable in the score function log p(z|\u03bb). The FIM can then be\nintuitively derived from its definition.\n$F(\\lambda) = -E_{p(z|\\lambda)} [\\nabla^{2}log p(z|\\lambda)]$\n$=-E_{p(z|\\lambda)} [-\\nabla^{2}\\mathcal{A}(\\lambda)]$\n$=\\nabla m(\\lambda)$\n(22)"}, {"title": "C.5 Proof to Proposition 3", "content": "We first reiterate that treating the output g(y|x) as a Gaussian distribution is merely a prior assumption and does not alter\nthe structure or computation of AOPU. Representing this Gaussian distribution as the minimal EM can be expressed as\nfollows,\n$\\mathcal{N}(\\hat{y}|W^T\\tilde{x}, \\mathcal{I}) = (2\\pi)^{-\\frac{d+h}{2}}|\\mathcal{I}|^{-\\frac{1}{2}} exp [-\\frac{1}{2} (\\hat{y} - W^T\\tilde{x})^T(\\hat{y} - W^T\\tilde{x})]$\n$=(2\\pi)^{-\\frac{d+h}{2}}|\\mathcal{I}|^{-\\frac{1}{2}} exp [-\\frac{1}{2} (\\hat{y}^T\\hat{y} + W^T\\tilde{x}\\tilde{x}^TW \u2013 2(\\tilde{x}\\hat{y}, W)]$\n(23)\nUnder this representation, the sufficient statistics and the natural parameter are respectively \u017e\u0177 and W. From this, by\nthe definition of the expectation parameter, we can calculate m(W) = $E_{\\mathcal{N}(\\hat{y}|W^T\\tilde{x}, \\mathcal{I})}[\\tilde{x}\\hat{y}]$ equals to $\\tilde{x}\\hat{y}^TW$ which is\nexactly identical to D(W). According to proposition 2, the FIM with respect to W is equivalent to the gradient of\nD(W) with respect to W. Thus, by introducing D(W), we can accelerate the NGD computation with respect to W as\nshown below,\nNote that $\\mathcal{L}(W)$ is equivalent to MSE. To compute the gradient of $\\mathcal{L}$ at D(W) using automatic differentiation tools and\navoid complex algebraic operations, we design $\\mathcal{L}(D(W))$ as,\n$\\mathcal{L}(D(W)) =E_{\\mathcal{N}(\\hat{y}|(\\tilde{x}^T\\tilde{x})^{-1}\\tilde{x}^TD(W), \\mathcal{I})} [(\\hat{y} - y)^T (\\hat{y} - y)]$\n(24)\nClearly, $\\mathcal{L}(D(W))$ is identical to the L introduced in section 2.3."}, {"title": "C.6 Proof to Proposition 4", "content": "We now assume the observed sample y is fully characterized by dual parameter D addition with a zero-mean random\nvariable \u03f5. Such constraint implies that there exists an optimal parameter set D* which fully captures the mean trend of\ny, i.e., $y = (x^Tx)^{-1}x^TD^* + \\epsilon$. The coherence definition could be rewritten as follows,\n$E[(\u2207L(D)), D \u2013 D^*]$\n$=E [(\u2207\\mathcal{L}(D), D - D^*)]$\n$=E [(((x^{T}x)^{-1}x^{T}D \u2013 y, ((x^{T}x)^{-1}x^{T} (D \u2013 D^*))]$\n$E [(((x^{T}x)^{-1}x^{T}D \u2013 y, ((x^{T}x)^{-1}x^{T} (D \u2013 D^*) \u2013 y + y)]$\n$=E [\\mathcal{L}(D)] + E [(((x^{T}x)^{-1}x^{T}D \u2013 y, y - ((x^{T}x)^{-1}x^{T}D^*)]$ \n$=E [\\mathcal{L}(D) - \\mathcal{L}(D^*)] + E [(((x^{T}x)^{-1}x^{T} (D \u2013 D^*), \\epsilon]$ \n(25)\nIn Eq. 25, the first equation arises due to the linear invariance of the gradient with respect to expectation. The second\nequation is derived by expanding the objective function and calculating its gradient, followed by reorganization. The\nthird equation results from adding and subtracting the same variable y on the right-hand side of the second equation.\nThe fourth equation reconstructs the objective function and cross-terms from the third equation. The fifth equation\nreconstructs the objective function under optimal parameter settings from the fourth equation.\nNote that the objective function under globally optimal parameter settings is necessarily less than or equal to the\nobjective function under any other parameter settings, thus term $E[\\mathcal{L}(D) - \\mathcal{L}(D^*)] \\geq 0$. Given the previous assumption\nthat each instance within the optimal parameter set perfectly captures the trend in y, the second term on the right-hand\nside of the fifth equation is equivalent to the previously defined zero-mean random variable \u03f5, and hence the second\nexpected value is identically zero. Thus, it is proven that AOPU's optimization is coherent."}, {"title": "C.7 Proof to Proposition 5", "content": "Using $dist_M$ and $dist_E$ represent Mahalanobis distance and Euclidean distance respectively we have $dist_E(x, y; \\Sigma) =$\n$\\sqrt{x^T\\Sigma^{-1}y}$ and $dist_E(x,y) = \\sqrt{x^Ty}$. The major difference between them is that the former adjusts for the distribution\nof data across different dimensions. Euclidean distance is essentially the Mahalanobis distance when the covariance\nmatrix is I (i.e., when dimensions are independent and identically distributed). Both Mahalanobis and Euclidean\ndistances are strictly convex functions with respect to the input, making them suitable for use as regularizer terms in\nmirror maps. Referring to Algorithm 1, we have revised the training strategy for AOPU, presented in Algorithm 2. It is\nevident that both share a consistent optimization structure, thus structurally ensuring that AOPU's optimization process\naligns with SMD. The key to the proof lies in establishing the relationship between the mirror map in SMD and the dual\nparameter in AOPU."}, {"title": "D Dataset Description", "content": ""}, {"title": "D.1 Debutanizer", "content": "The Debutanizer column is part of a desulfuring and naphtha splitter plant. It is required to maximize the C5 (stabilized\ngasoline) content in the Debutanizer overheads(LP gas splitter feed), and minimize the C4 (butane) content in the\nDebutanizer bottoms (Naphtha splitter feed) [57]. However, the butane content is not directly measured on the bottom\nflow, but on the overheads of the downstream deisopentanizer column by the gas chromatograph resulting in a large\nmeasuring delay, which is the reason soft sensor steps in.\nThe dataset comprises 2,394 records, each featuring 7 relevant sensor measurements. The flowchart of the Debutanizer\ncolumn, detailing the locations of these sensors and their respective descriptions, is presented in Fig. 7 (b). The\ncorresponding details can also be found in Table 5."}]}