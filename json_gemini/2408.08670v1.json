{"title": "Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning", "authors": ["Alessio Devoto", "Federico Alvetreti", "Jary Pomponi", "Paolo Di Lorenzo", "Pasquale Minervini", "Simone Scardapane"], "abstract": "Recently, foundation models based on Vision Transformers (ViTs) have become widely available. However, their fine-tuning process is highly resource-intensive, and it hinders their adoption in several edge or low-energy applications. To this end, in this paper we introduce an efficient fine-tuning method for ViTs called ALaST (Adaptive Layer Selection Fine-Tuning for Vision Transformers) to speed up the fine-tuning process while reducing computational cost, memory load, and training time. Our approach is based on the observation that not all layers are equally critical during fine-tuning, and their importance varies depending on the current mini-batch. Therefore, at each fine-tuning step, we adaptively estimate the importance of all layers and we assign what we call \"compute budgets\" accordingly. Layers that were allocated lower budgets are either trained with a reduced number of input tokens or kept frozen. Freezing a layer reduces the computational cost and memory usage by preventing updates to its weights, while discarding tokens removes redundant data, speeding up processing and reducing memory requirements. We show that this adaptive compute allocation enables a nearly-optimal schedule for distributing computational resources across layers, resulting in substantial reductions in training time (up to 1.5x), FLOPs (up to 2x), and memory load (up to 2x) compared to traditional full fine-tuning approaches. Additionally, it can be successfully combined with other parameter-efficient fine-tuning methods, such as LoRA.", "sections": [{"title": "Introduction", "content": "Recently, large-scale Vision Transformers (ViTs, (Dosovitskiy et al. 2021)) have become the leading paradigm in computer vision. By leveraging the self-attention mechanism, ViTs can capture long-range dependencies in images at every layer, leading to superior results compared to traditional convolutional neural networks (CNNs). ViTs are at the core of a wide array of applications, ranging from vision-language models (Radford et al. 2021; Matthias Minderer 2022; Liu et al. 2023) to resource-constrained embedded devices (Cai, Gan, and Han 2022; Cai et al. 2020; Mehta and Rastegari 2022; Laskaridis et al. 2020).\nThe impressive capabilities of ViTs come with the drawback of a resource-intensive training process. The high computational demands arise from the large number of parameters and the quadratic complexity of the self-attention mechanism. Moreover, ViTs are extremely data-hungry, requiring large datasets to achieve optimal performance, which in turn prolongs training times.\nTo address these constraints, a common practice for the deployment of ViTs is to leverage a pre-trained foundation model and then perform fine-tuning for specific tasks. By updating only a subset of the model parameters, fine-tuning makes it feasible to achieve high performance on specialized tasks without the prohibitive costs associated with training a model from scratch (Xin et al. 2024). However, fine-tuning ViTs introduces additional complexity: the choice of which parameters to update is critical, as it can significantly impact performance (Sarkar et al. 2024). Identifying the optimal layers to fine-tune often requires extensive experimentation, which is not always feasible in real-world scenarios due to time and computational constraints. Parameter-efficient"}, {"title": "Background on Vision Transformer", "content": "A Vision Transformer (ViT) processes an image $X \\in \\mathbb{R}^{C \\times H \\times W}$ (where C, H, W are channels, width, and height respectively) through a series of L transformer layers. The transformation pipeline can be formalized as follows:\n$y = C \\circ F_L \\circ F_{L-1} \\circ ... \\circ F_1 \\circ E(X),$\t(1)\nwhere $E(\\cdot)$ denotes the encoding network, $F^i$ represents the i-th transformer layer, and $C(\\cdot)$ is the classification head. The encoding network $E(\\cdot)$ splits the image into smaller, non-overlapping patches. Each patch is then flattened and linearly projected into a lower-dimensional embedding space, forming tokens. Suppose the image is divided into N patches, each of size P \u00d7 P, resulting in a sequence of N tokens. This can be represented as:\n$E(X) = [t_1, t_2,...,t_N],$\t(2)\nwhere $t_i \\in \\mathbb{R}^{E}$ is the embedding of the i-th patch, and $E$ is the embedding dimension. To enable classification, a special trainable token, known as the class token (CLS token), is prepended to the sequence of patch embeddings. Additionally, since transformers lack an inherent sense of order, positional encodings are added to each token to retain spatial information. The transformer layers $F(\\cdot)$ process this sequence of tokens through a series of operations involving multi-head self-attention and feed-forward neural networks. A generic transformer block at layer l transforms each token from layer l 1 via:\n$t_l = F'(t_{l-1}),$\t(3)\nwhere $t_{l-1}$ denotes the token embedding at layer l 1, $t_l$ the updated token embedding, and $F'$ represents a standard transformer encoder block with multi-head attention and a feed-forward MLP. The self-attention operation has a quadratic complexity with respect to the sequence length, i.e. it has a cost of $O(N^2)$, where N is the number of tokens. This quadratic cost can be significant for devices with limited resources. Efficient implementation techniques or approximations are often required to make ViTs faster and more memory efficient for inference on downstream tasks, especially in low resource settings (Meng et al. 2022; Yin et al. 2022; Bolya et al. 2023)."}, {"title": "2.1 Layer Contributions during Fine-tuning", "content": "We analyze the transformer architecture from the perspective of the residual stream, as described by Elhage et al. (2021). In this framework, the set of tokens flows through the model, with token embeddings being updated via vector additions from the attention and feed-forward blocks in each layer. This perspective allows us to isolate and examine the individual contributions that each layer adds to the residual stream.\nMultiple studies (Samragh et al. 2023; Zhang, Bengio, and Singer 2022; Gromov et al. 2024) have demonstrated that not all layers contribute equally to the updates in the residual stream. This phenomenon is particularly evident in pre-trained models, where some layers function almost like identity mappings, providing minimal updates to the token embeddings in the residual stream."}, {"title": "3 Adaptive Layer Selective Fine-tuning", "content": "Because different layers affect the final prediction in different ways (Dodge et al. 2020; Samragh et al. 2023), predetermining which layers to fine-tune can result in suboptimal outcomes. Previous approaches have attempted to identify the most critical layers through extensive search methods (Samragh et al. 2023; Sarkar et al. 2024; Gromov et al. 2024). However, these methods are highly dependent on the specific dataset and model being used, leading to inconsistencies when applied to different data distributions or model scales. For example, an exhaustive search might determine that layers 4, 5, and 6 are the most important when fine-tuning on the Flower-102 dataset, leading to the decision to freeze the other layers. Yet, this configuration may not be effective for a different dataset, necessitating another round of exhaustive searching. Similarly, the important layers in ViT-B may differ from those in DeiT-T, requiring a separate search for each model.\nTo overcome these challenges, we propose a simple strategy that adaptively estimates the importance of each layer during fine-tuning, leading to improvements in memory usage, FLOPs, and wall-clock training time. This approach is especially valuable in low-resource scenarios, where it can be integrated requiring minimal modifications to existing training pipelines. In such scenarios where resources are limited, we argue that not all layers should be trained with the same computational effort; some layers should receive a reduced computational budget and therefore be trained with fewer resources.\nTo optimize resource allocation, we focus on two key parameters: the number of tokens processed by each layer and which layers are actively trained. Adjusting these parameters directly influences computational load and memory consumption. For the first parameter, we follow Meng et al. (2022) and Rao et al. (2021) by reducing the number of tokens processed, selectively removing redundant ones during the forward pass. For the second parameter, we freeze less critical layers, saving memory and preventing unnecessary updates to their weights during the backward pass (Figure 1). We highlight that the proposed method can be integrated into existing fine-tuning frameworks with minimal overhead. We provide an overview of ALaST in Figure 3. To implement the budget allocation, we first need a reliable method to estimate the importance of each layer."}, {"title": "3.1 Estimating the importance of each layer", "content": "In the following, we explain how we assign a budget to each transformer layer. We assume that each fine-tuning step comprises a forward and a backward pass. We use i to indicate the current step and CLS to indicate the embedding of the class token at layer l for step i. Given a fine-tuning step i, we estimate the importance of each layer l and assign it a training budget $b_i \\in (0,1)$, representing the computational resources we can afford to spend on that layer. The budget should be high if the layer's contribution to the final prediction is high, low otherwise. Notably, the budget must be computed adaptively at each step.\nDuring the forward pass, the class token aggregates information from all other tokens and it is passed through the classification head. This token is crucial for capturing rich semantic information about the input, making it a reliable indicator for evaluating each layer's contribution to the final prediction. The class token's role in capturing essential features for downstream tasks has already been investigated in previous works, such as Raghu et al. (2021), which studied the correlation between the CLS token representation and each layer's contribution to the final prediction. This correlation can be intuitively understood by noting that the class token, being the only one that is passed through the final classifier, must capture information about the whole input image (Liang et al. 2022; Raghu et al. 2021). As a result, layers that contribute less to updating the class token are less critical, and we assign them lower compute budgets without impacting overall performance."}, {"title": "3.2 Applying the budget", "content": "Based on the distribution of budgets $D_i$, we allocate our memory and FLOPs resources leveraging two degrees of freedom: the number of processed tokens and the trainable layers. In the two following paragraphs, we explain how we select which tokens to discard and which layers to freeze.\nAdaptive Token selection As pointed out in several works (Jain et al. 2024; Meng et al. 2022; Bolya et al. 2023) not all tokens carry equally valuable information for the task at hand. Given an input sequence $T \\in \\mathbb{R}^{N \\times E}$, where N is the sequence length and E is the embedding dimension, we only allow $b \\cdot N$ tokens to flow to the next layer. Here, $b \\in [0, 1]$, meaning that this approach selects b% of the tokens from the input sequence. To determine which tokens to discard, we adopt a strategy similar to that of Liang et al. (2022). We rank the tokens based on their attention scores from the CLS token and retain only the top $b \\cdot N$ tokens.\nThis approach leverages the fact that tokens with higher attention scores are more influential in determining the class prediction. By retaining only the most important tokens - those that contribute significantly to the class token's representation - we reduce computational overhead and memory usage while preserving the most relevant information for classification. Initially, we allowed unattended tokens to proceed to the next layer. However, analysis of attention score distributions revealed that tokens excluded at one layer remained excluded in subsequent layers. Consequently, we opted to discard the least attended tokens, preventing their progression to the next layer. This approach further reduced batch size and memory consumption without performance loss.\nAdaptive Layer Freezing In addition to token selection, we also optimize resource allocation by freezing certain layers. Given a budget b, we freeze the less critical layers to conserve memory, which is crucial for efficient on-device"}, {"title": "4 Experimental Setup", "content": "We fine-tune our models on Flower-102 Nilsback and Zisserman, Cifar-100 Krizhevsky, Nair, and Hinton and the more challenging Food-101 Bossard, Guillaumin, and Van Gool dataset. We test a larger pre-trained Vision Transformer ViT-B (Dosovitskiy et al. 2021) along with the DeiT-S and DeiT-T (Touvron et al. 2021) models, that are smaller and more parameter efficient, thus more likely to be used in resource-constrained scenarios. We download pre-trained weights from timm-models (Wightman 2019). In all the runs, we set K (number of trainable layers) to 9, but we show results for other values in Appendix B. We use mixed precision training (Micikevicius et al. 2018) for all our experiments to keep memory usage as small as possible and simulate a real-world on-device training. During training, we keep track of FLOPs (Floating Point Operations), memory load and wall-clock time. While memory and time are important metrics to assess practical performance and resource utilization, they can be partially influenced by hardware. Therefore, we also use FLOPs to provide a hardware-independent measure of computational complexity, enabling consistent cross-system comparisons. For reproducibility, we provide detailed descriptions of our experimental setup, including code, hyperparameters, and training procedures in Appendix E."}, {"title": "5 Results & Analysis", "content": "We evaluate our method against a diverse array of baselines, including both traditional fine-tuning approaches and state-of-the-art techniques, including Low-Rank Adaptation (LoRA) (Hu et al. 2022), Token merging (Bolya et al. 2023) and Block Selective Reprogramming (Sarkar et al. 2024). Compared to standard fine-tuning procedures, ALaST achieves optimal performance at a fraction of the cost. While fine-tuning all layers typically results in high final accuracy, it comes at the expense of significantly more expensive training. In Figure 5, we demonstrate the normalized gains our method achieves compared to full fine-tuning. With ALaST, we observe only a minimal reduction in accuracy while using just 60% of the FLOPs, 50% of the memory, and 80% of the fine-tuning wall-clock time relative to the full fine-tuning baseline, without adding any additional parameters. An alternative to full fine-tuning is training only the classification head, which is computationally and memory efficient since only the final linear layer is updated via back-propagation. However, this approach typically results in substantially lower final accuracy. ALaST strikes a balance between these two extremes, maintaining low memory usage and training FLOPs, while losing only minimal accuracy compared to full fine-tuning. In low-resource scenarios, it is critical not only to reduce the total resources consumed during fine-tuning but also to manage the distribution of these resources throughout the training process. Often, training until full convergence is not feasible, making the initial \"speed\" of convergence particularly important. In other words, a method that requires fewer FLOPs but converges slowly is impractical for such constrained environments."}, {"title": "6 Related Works", "content": ""}, {"title": "6.1 Parameter Efficient Fine-Tuning", "content": "In the last years, the field of parameter efficient fine-tuning (PEFT) has seen increased interest. One of the first approaches in PEFT was the use of adapters, Houlsby et al., that insert and fine-tune small neural network modules into each layer of a pre-trained model. Prefix-tuning Li and Liang and prompt-tuning Lester, Al-Rfou, and Constant learn soft prompts that are added to the input tokens. Perhaps the most popular PEFT approach is LoRA (Low-Rank Adaptation), proposed in (Hu et al. 2022), that reduces the number of additional parameters that need to be fine-tuned by decomposing the weight updates into a product of two low-rank matrices. Most of the PEFT methods were introduced for fine-tuning large language models, and subsequently applied to computer vision with varying degrees of success (Xin et al. 2024). Typically, these approaches emphasize optimizing the number of additional parameters rather than FLOPs or training time. However, parameters memory is only a fraction of the memory used during training. Unlike PEFT, ALaST does not add any parameters, and also reduces FLOPs and memory. Additionally, ALaST can be combined with LoRA"}, {"title": "6.2 On-device Training", "content": "Our work is perhaps most similar to those papers that optimize resources for on-device training. Among these, (Cai et al. 2020) was the first to highlight that memory is a major bottleneck in CNN training and devise a way to reduce memory load. Along the same line, some approaches proposed to recompute activations for a subset of the layers to reduce memory consumption (Chen et al. 2016; Gruslys et al. 2016). While this strategy can be useful for low memory budgets, it sacrifices the compute (FLOPs), and is not suitable for devices with limited computational resources."}, {"title": "6.3 Efficient Transformers", "content": "A substantial body of research has focused on enhancing the efficiency of ViTs, particularly during the inference phase. For example, (Rao et al. 2021; Meng et al. 2022) proposed token halting strategies that select the most important tokens at runtime and speed up inference. Touvron et al. (2021) employed distillation from CNNs to maintain model accuracy while reducing the number of parameters and FLOPs. Lin et al. (2022) introduced a quantized version of the Vision Transformer to decrease model inference complexity. More recently, W\u00f3jcik et al. (2023) proposed to control the computational load by activating sub-modules of the transformer, based on input difficulty. In contrast to these methods, we do not focus on inference, but address the challenge of fine-tuning available foundation models when resources are limited."}, {"title": "7 Conclusions", "content": "We introduced ALaST, a simple and effective method to fine-tune ViTs in low-resource scenarios, saving computational budget, memory load and training time, with minimal modifications to the training pipeline. Although we test ALaST on ViTs, its principles are generalizable to other transformer-based architectures, which we plan to explore in future work."}, {"title": "A Additional Results", "content": "We show additional results on the Flower-102 dataset in table Table 7. Flower-102 is the smallest dataset that we use for our experiments."}, {"title": "B Number of trainable blocks", "content": "At each iteration, we select K transformer layers for fine-tuning. We showed experiments with K = 9. Here we report results for different values of K. In Figure 8 we show the accuracy and FLOPs trade-off for different values of K. We see that incresing the number of trainable layers leads to better performance in general. The improvement becomes smaller when K is greater than 8."}, {"title": "C Combination with PEFT", "content": "As we showed in Table 3 and Table 2, LoRA performs rather poorly when applied to smaller models, while ALaST converges faster and to higher accuracy. On larger ViT-B, on the other hand, LoRA achieves similar accuracy to our method, at a higher memory cost to store the activations for all layers. Because the two methods are orthogonal, we integrate them and test LoRA + ALaST on ViT-B.\nIn order to combine LoRA with ALaST, we apply the budget schedule to LoRA's additional parameters and keep the transformer layers frozen, except for the classfication head."}, {"title": "D Compute Budget Allocation", "content": "In Figure 9 we show the total number of times each layer is chosen during fine-tuning. We see that for DeiT-S and ViT-B the compute budget is more evenly distributed across layers, and initial layers are usually picked more frequently. In the smaller DeiT-T, on the other hand, the budget is allocated mainly to initial and last layers, and the distribution results more peaked. In Figure 10 we show the budget assignment during fine-tuning. We observe that central layers usually exhibit a higher variance of assigned compute budget. In DeiT-S and ViT-B we observe a pattern where deeper layers are assigned high budget in the first iterations, and the budget is then decreased along fine-tuning. For the initial layers on the other hand, the budget is usually constantly high across the entire fine-tuning."}, {"title": "E Implementation details", "content": "In the following, we provide details on the implementation and fine-tuning hyper-parameters. We fine-tune the three models using Adam optimizer (Kingma and Ba 2015) with a batch size of 128. After an initial grid search, we set the learning rate to 0.0001 for all methods but LoRA, where we found 0.001 to be more effective. We use two random augmentations picked between horizontal or vertical flipping and random cropping for all datasets and runs. All images are resized to 224 \u00d7 224, that is the required input size for the pretrained models. We download the weights of the models (pre-trained on ImageNet (Russakovsky et al. 2015)) from Timm Image models (Wightman 2019). During fine-tuning we keep track of FLOPs, peak memory and training time on an NVidia RTX4090 GPU. To measure FLOPs and memory load, we use PyTorch MACs counter and memory allocation tools (Paszke et al. 2017). Finally, we provide the Python code used for experiments."}]}