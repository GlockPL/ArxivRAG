{"title": "Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning", "authors": ["Alessio Devoto", "Federico Alvetreti", "Jary Pomponi", "Paolo Di Lorenzo", "Pasquale Minervini", "Simone Scardapane"], "abstract": "Recently, foundation models based on Vision Transformers (ViTs) have become widely available. However, their fine-tuning process is highly resource-intensive, and it hinders their adoption in several edge or low-energy applications. To this end, in this paper we introduce an efficient fine-tuning method for ViTs called ALaST (Adaptive Layer Selection Fine-Tuning for Vision Transformers) to speed up the fine-tuning process while reducing computational cost, memory load, and training time. Our approach is based on the observation that not all layers are equally critical during fine-tuning, and their importance varies depending on the current mini-batch. Therefore, at each fine-tuning step, we adaptively estimate the importance of all layers and we assign what we call \"compute budgets\" accordingly. Layers that were allocated lower budgets are either trained with a reduced number of input tokens or kept frozen. Freezing a layer reduces the computational cost and memory usage by preventing updates to its weights, while discarding tokens removes redundant data, speeding up processing and reducing memory requirements. We show that this adaptive compute allocation enables a nearly-optimal schedule for distributing computational resources across layers, resulting in substantial reductions in training time (up to 1.5x), FLOPs (up to 2x), and memory load (up to 2x) compared to traditional full fine-tuning approaches. Additionally, it can be successfully combined with other parameter-efficient fine-tuning methods, such as LoRA.", "sections": [{"title": "Introduction", "content": "Recently, large-scale Vision Transformers (ViTs, (Dosovitskiy et al. 2021)) have become the leading paradigm in computer vision. By leveraging the self-attention mechanism, ViTs can capture long-range dependencies in images at every layer, leading to superior results compared to traditional convolutional neural networks (CNNs). ViTs are at the core of a wide array of applications, ranging from vision-language models (Radford et al. 2021; Matthias Minderer 2022; Liu et al. 2023) to resource-constrained embedded devices (Cai, Gan, and Han 2022; Cai et al. 2020; Mehta and Rastegari 2022; Laskaridis et al. 2020).\nThe impressive capabilities of ViTs come with the drawback of a resource-intensive training process. The high computational demands arise from the large number of parameters and the quadratic complexity of the self-attention mechanism. Moreover, ViTs are extremely data-hungry, requiring large datasets to achieve optimal performance, which in turn prolongs training times.\nTo address these constraints, a common practice for the deployment of ViTs is to leverage a pre-trained foundation model and then perform fine-tuning for specific tasks. By updating only a subset of the model parameters, fine-tuning makes it feasible to achieve high performance on specialized tasks without the prohibitive costs associated with training a model from scratch (Xin et al. 2024). However, fine-tuning ViTs introduces additional complexity: the choice of which parameters to update is critical, as it can significantly impact performance (Sarkar et al. 2024). Identifying the optimal layers to fine-tune often requires extensive experimentation, which is not always feasible in real-world scenarios due to time and computational constraints. Parameter-efficient fine-tuning (PEFT) methods, such LoRA (Hu et al. 2022) or adapters (Houlsby et al. 2019) aim to address some of these challenges, but they often focus on optimizing the count of additional parameters rather than reducing overall computational load. As a result, PEFT methods might still be unsuitable for highly constrained environments where minimizing computation and memory are limited (Sarkar et al. 2024). This is the case for mobile and edge devices, drones or next-generation 6G networks that face strict constraints on memory and computational resources (Cai, Gan, and Han 2022; Cai et al. 2020; Calvanese Strinati et al. 2024). These devices require lightweight models that can be quickly fine-tuned on a low budget without compromising performance. Similarly, in situations where privacy concerns prevent data from being sent to a server, on-device processing and fine-tuning becomes essential.\nIn this work, we propose a simple method to accelerate the fine-tuning of ViTs in low-resource settings by leveraging two key insights. First, it is known that not all tokens are equally useful during training, due to redundant information present in input images. Various techniques have been developed to estimate token importance and either discard, merge, or halt redundant tokens to enhance inference (not training) speed, showing promising results (Meng et al. 2022; Bolya et al. 2023; Rao et al. 2021). Second, not all layers are equally important during training. Some layers receive minimal updates due to small gradients, making associated computation inefficient in both the forward and backward pass. Building on these observations, we propose Adaptive LAyer Selective fine-Tuning for Vision Transformers (ALaST) - during fine-tuning, we allocate a so-called scalar budget to each layer, and we control resource consumption along two axes: the number of discarded tokens and the selection of trainable layers. Specifically, we adaptively determine (a) how many tokens to forward through each layer and (b) which layers to freeze based on the budget allocation. Discarding tokens significantly reduces FLOPs \u2013 due to the quadratic cost of multi-head attention \u2013 and accelerates training, especially on mid-range GPUs, enabling models to reach higher accuracy in shorter time. Freezing layers enhances energy efficiency and substantially reduces memory usage, which is critical for on-device training.\nWe are the first, to the best of our knowledge, to introduce an adaptive framework that systematically optimizes both token and layer selection during fine-tuning, addressing the computational challenges of ViTs in resource-constrained environments. We validate our method against a comprehensive set of baseline approaches, ensuring that our results are robust and demonstrating that ALaST achieves superior efficiency while maintaining competitive accuracy."}, {"title": "Background on Vision Transformer", "content": "A Vision Transformer (ViT) processes an image $X \\in \\mathbb{R}^{C \\times H \\times W}$ (where C, H, W are channels, width, and height respectively) through a series of L transformer layers. The transformation pipeline can be formalized as follows:\n$y = C \\circ F_L \\circ F_{L-1} \\circ ... \\circ F_1 \\circ E(X)$,\nwhere $E(\\cdot)$ denotes the encoding network, $F^i$ represents the i-th transformer layer, and $C(\\cdot)$ is the classification head. The encoding network $E(\\cdot)$ splits the image into smaller, non-overlapping patches. Each patch is then flattened and linearly projected into a lower-dimensional embedding space, forming tokens. Suppose the image is divided into N patches, each of size P \u00d7 P, resulting in a sequence of N tokens. This can be represented as:\n$E(X) = [t_1, t_2,...,t_N]$,\nwhere $t_i \\in \\mathbb{R}^E$ is the embedding of the i-th patch, and E is the embedding dimension. To enable classification, a special trainable token, known as the class token (CLS token), is prepended to the sequence of patch embeddings. Additionally, since transformers lack an inherent sense of order, positional encodings are added to each token to retain spatial information. The transformer layers $F(\\cdot)$ process this sequence of tokens through a series of operations involving multi-head self-attention and feed-forward neural networks. A generic transformer block at layer l transforms each token from layer l 1 via:\n$t_l = F'(t_{l-1})$,\nwhere $t_{l-1}$ denotes the token embedding at layer l 1, $t_l$ the updated token embedding, and $F'$ represents a standard transformer encoder block with multi-head attention and a feed-forward MLP. The self-attention operation has a quadratic complexity with respect to the sequence length, i.e. it has a cost of $O(N^2)$, where N is the number of tokens. This quadratic cost can be significant for devices with limited resources. Efficient implementation techniques or approximations are often required to make ViTs faster and more memory efficient for inference on downstream tasks, especially in low resource settings (Meng et al. 2022; Yin et al. 2022; Bolya et al. 2023).\nThe CLS token, which aggregates information from all patches, is extracted after the final transformer layer and passed to the classification head $C(\\cdot)$. This head typically consists of a linear layer followed by a softmax function to produce the final classification output. We show an overview of the Vision Transformer architecture adapted to our method in Figure 3."}, {"title": "Layer Contributions during Fine-tuning", "content": "We analyze the transformer architecture from the perspective of the residual stream, as described by Elhage et al. (2021). In this framework, the set of tokens flows through the model, with token embeddings being updated via vector additions from the attention and feed-forward blocks in each layer. This perspective allows us to isolate and examine the individual contributions that each layer adds to the residual stream.\nMultiple studies (Samragh et al. 2023; Zhang, Bengio, and Singer 2022; Gromov et al. 2024) have demonstrated that not all layers contribute equally to the updates in the residual stream. This phenomenon is particularly evident in pre-trained models, where some layers function almost like identity mappings, providing minimal updates to the token embeddings in the residual stream."}, {"title": "Adaptive Layer Selective Fine-tuning", "content": "Because different layers affect the final prediction in different ways (Dodge et al. 2020; Samragh et al. 2023), predetermining which layers to fine-tune can result in suboptimal outcomes. Previous approaches have attempted to identify the most critical layers through extensive search methods (Samragh et al. 2023; Sarkar et al. 2024; Gromov et al. 2024). However, these methods are highly dependent on the specific dataset and model being used, leading to inconsistencies when applied to different data distributions or model scales. For example, an exhaustive search might determine that layers 4, 5, and 6 are the most important when fine-tuning on the Flower-102 dataset, leading to the decision to freeze the other layers. Yet, this configuration may not be effective for a different dataset, necessitating another round of exhaustive searching. Similarly, the important layers in ViT-B may differ from those in DeiT-T, requiring a separate search for each model.\nTo overcome these challenges, we propose a simple strategy that adaptively estimates the importance of each layer during fine-tuning, leading to improvements in memory usage, FLOPs, and wall-clock training time. This approach is especially valuable in low-resource scenarios, where it can be integrated requiring minimal modifications to existing training pipelines. In such scenarios where resources are limited, we argue that not all layers should be trained with the same computational effort; some layers should receive a reduced computational budget and therefore be trained with fewer resources.\nTo optimize resource allocation, we focus on two key parameters: the number of tokens processed by each layer and which layers are actively trained. Adjusting these parameters directly influences computational load and memory consumption. For the first parameter, we follow Meng et al. (2022) and Rao et al. (2021) by reducing the number of tokens processed, selectively removing redundant ones during the forward pass. For the second parameter, we freeze less critical layers, saving memory and preventing unnecessary updates to their weights during the backward pass (Figure 1). We highlight that the proposed method can be integrated into existing fine-tuning frameworks with minimal overhead. We provide an overview of ALaST in Figure 3. To implement the budget allocation, we first need a reliable method to estimate the importance of each layer."}, {"title": "Estimating the importance of each layer", "content": "In the following, we explain how we assign a budget to each transformer layer. We assume that each fine-tuning step comprises a forward and a backward pass. We use i to indicate the current step and CLS to indicate the embedding of the class token at layer l for step i. Given a fine-tuning step i, we estimate the importance of each layer l and assign it a training budget $b_i \\in (0,1)$, representing the computational resources we can afford to spend on that layer. The budget should be high if the layer's contribution to the final prediction is high, low otherwise. Notably, the budget must be computed adaptively at each step.\nDuring the forward pass, the class token aggregates information from all other tokens and it is passed through the classification head. This token is crucial for capturing rich semantic information about the input, making it a reliable indicator for evaluating each layer's contribution to the final prediction. The class token's role in capturing essential features for downstream tasks has already been investigated in previous works, such as Raghu et al. (2021), which studied the correlation between the CLS token representation and each layer's contribution to the final prediction. This correlation can be intuitively understood by noting that the class token, being the only one that is passed through the final classifier, must capture information about the whole input image (Liang et al. 2022; Raghu et al. 2021). As a result, layers that contribute less to updating the class token are less critical, and we assign them lower compute budgets without impacting overall performance."}, {"title": "Applying the budget", "content": "Based on the distribution of budgets $D_i$, we allocate our memory and FLOPs resources leveraging two degrees of freedom: the number of processed tokens and the trainable layers. In the two following paragraphs, we explain how we select which tokens to discard and which layers to freeze.\nAdaptive Token selection As pointed out in several works (Jain et al. 2024; Meng et al. 2022; Bolya et al. 2023) not all tokens carry equally valuable information for the task at hand. Given an input sequence $T \\in \\mathbb{R}^{N \\times E}$, where N is the sequence length and E is the embedding dimension, we only allow $b \\cdot N$ tokens to flow to the next layer. Here, $b \\in [0, 1]$, meaning that this approach selects b% of the tokens from the input sequence. To determine which tokens to discard, we adopt a strategy similar to that of Liang et al. (2022). We rank the tokens based on their attention scores from the CLS token and retain only the top $b \\cdot N$ tokens.\nThis approach leverages the fact that tokens with higher attention scores are more influential in determining the class prediction, as we show in Figure 4, where we plot some attention maps with respect to the class token for DeiT-S. By retaining only the most important tokens - those that contribute significantly to the class token's representation - we reduce computational overhead and memory usage while preserving the most relevant information for classification. Initially, we allowed unattended tokens to proceed to the next layer. However, analysis of attention score distributions revealed that tokens excluded at one layer remained excluded in subsequent layers. Consequently, we opted to discard the least attended tokens, preventing their progression to the next layer. This approach further reduced batch size and memory consumption without performance loss.\nAdaptive Layer Freezing In addition to token selection, we also optimize resource allocation by freezing certain layers. Given a budget b, we freeze the less critical layers to conserve memory, which is crucial for efficient on-device training. By freezing these layers, we reduce the number of trainable parameters, thereby lowering memory consumption and improving computational efficiency. When a layer is frozen, we save the memory which is necessary to store the activations for the backpropagation. After determining the budget distribution $D_i$, we sample layers $x \\sim D_i$ without replacement, ensuring that the same layer is not selected twice. We then train only the K layers with the highest budget allocation, and freeze the rest. The dual approach of token selection and layer freezing enables us to manage resources effectively and optimize the performance of our model within the constraints of the available device memory, by leveraging two different degrees of liberty."}, {"title": "Experimental Setup", "content": "We fine-tune our models on Flower-102 Nilsback and Zisserman, Cifar-100 Krizhevsky, Nair, and Hinton and the more challenging Food-101 Bossard, Guillaumin, and Van Gool dataset. We test a larger pre-trained Vision Transformer ViT-B (Dosovitskiy et al. 2021) along with the DeiT-S and DeiT-T (Touvron et al. 2021) models, that are smaller and more parameter efficient, thus more likely to be used in resource-constrained scenarios. We show the tested model architectures in Table 1. We download pre-trained weights from timm-models (Wightman 2019). In all the runs, we set K (number of trainable layers) to 9, but we show results for other values in Appendix B. We use mixed precision training (Micikevicius et al. 2018) for all our experiments to keep memory usage as small as possible and simulate a real-world on-device training. During training, we keep track of FLOPs (Floating Point Operations), memory load and wall-clock time. While memory and time are important metrics to assess practical performance and resource utilization, they can be partially influenced by hardware. Therefore, we also use FLOPs to provide a hardware-independent measure of computational complexity, enabling consistent cross-system comparisons. For reproducibility, we provide detailed descriptions of our experimental setup, including code, hyperparameters, and training procedures in Appendix E."}, {"title": "Results & Analysis", "content": "We evaluate our method against a diverse array of baselines, including both traditional fine-tuning approaches and state-of-the-art techniques, including Low-Rank Adaptation (LoRA) (Hu et al. 2022), Token merging (Bolya et al. 2023) and Block Selective Reprogramming (Sarkar et al. 2024). Compared to standard fine-tuning procedures, ALaST achieves optimal performance at a fraction of the cost. While fine-tuning all layers typically results in high final accuracy, it comes at the expense of significantly more expensive training. In Figure 5, we demonstrate the normalized gains our method achieves compared to full fine-tuning. With ALaST, we observe only a minimal reduction in accuracy while using just 60% of the FLOPs, 50% of the memory, and 80% of the fine-tuning wall-clock time relative to the full fine-tuning baseline, without adding any additional parameters. An alternative to full fine-tuning is training only the classification head, which is computationally and memory efficient since only the final linear layer is updated via back-propagation. However, this approach typically results in substantially lower final accuracy. ALaST strikes a balance between these two extremes, maintaining low memory usage and training FLOPs, while losing only minimal accuracy compared to full fine-tuning. A more detailed comparison can be found in Figure 6. In low-resource scenarios, it is critical not only to reduce the total resources consumed during fine-tuning but also to manage the distribution of these resources throughout the training process. Often, training until full convergence is not feasible, making the initial \"speed\" of convergence particularly important. In other words, a method that requires fewer FLOPs but converges slowly is impractical for such constrained environments."}, {"title": "Related Works", "content": "In the last years, the field of parameter efficient fine-tuning (PEFT) has seen increased interest. One of the first approaches in PEFT was the use of adapters, Houlsby et al., that insert and fine-tune small neural network modules into each layer of a pre-trained model. Prefix-tuning Li and Liang and prompt-tuning Lester, Al-Rfou, and Constant learn soft prompts that are added to the input tokens. Perhaps the most popular PEFT approach is LoRA (Low-Rank Adaptation), proposed in (Hu et al. 2022), that reduces the number of additional parameters that need to be fine-tuned by decomposing the weight updates into a product of two low-rank matrices. Most of the PEFT methods were introduced for fine-tuning large language models, and subsequently applied to computer vision with varying degrees of success (Xin et al. 2024). Typically, these approaches emphasize optimizing the number of additional parameters rather than FLOPs or training time. However, parameters memory is only a fraction of the memory used during training. Unlike PEFT, ALaST does not add any parameters, and also reduces FLOPs and memory. Additionally, ALaST can be combined with LoRA, as we show in Appendix C.\nOur work is perhaps most similar to those papers that optimize resources for on-device training. Among these, (Cai et al. 2020) was the first to highlight that memory is a major bottleneck in CNN training and devise a way to reduce memory load. Along the same line, some approaches proposed to recompute activations for a subset of the layers to reduce memory consumption (Chen et al. 2016; Gruslys et al. 2016). While this strategy can be useful for low memory budgets, it sacrifices the compute (FLOPs), and is not suitable for devices with limited computational resources.\nRecently, Sarkar et al. (2024); Samragh et al. (2023) explored smart initialization by identifying the best subset of layers to fine-tune. Sarkar et al. (2024) identifies the most important layers and discards redundant tokens during fine-tuning. Samragh et al. (2023) uses the relative magnitude of outputs to estimate the importance of different blocks and initialize a smaller model from a larger one. Unlike these methods, ALaST learns the importance of individual layers during fine-tuning and therefore does not need extensive research of which layers to train in advance."}, {"title": "Efficient Transformers", "content": "A substantial body of research has focused on enhancing the efficiency of ViTs, particularly during the inference phase. For example, (Rao et al. 2021; Meng et al. 2022) proposed token halting strategies that select the most important tokens at runtime and speed up inference. Touvron et al. (2021) employed distillation from CNNs to maintain model accuracy while reducing the number of parameters and FLOPs. Lin et al. (2022) introduced a quantized version of the Vision Transformer to decrease model inference complexity. More recently, W\u00f3jcik et al. (2023) proposed to control the computational load by activating sub-modules of the transformer, based on input difficulty. In contrast to these methods, we do not focus on inference, but address the challenge of fine-tuning available foundation models when resources are limited."}, {"title": "Conclusions", "content": "We introduced ALaST, a simple and effective method to fine-tune ViTs in low-resource scenarios, saving computational budget, memory load and training time, with minimal modifications to the training pipeline. Although we test ALaST on ViTs, its principles are generalizable to other transformer-based architectures, which we plan to explore in future work."}]}