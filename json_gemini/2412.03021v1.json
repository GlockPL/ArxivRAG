{"title": "PEMF-VVTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm", "authors": ["Tianyu Chang", "Xiaohao Chen", "Zhichao Wei", "Xuanpu Zhang", "Qing-Guo Chen", "Weihua Luo", "Xun Yang"], "abstract": "Video Virtual Try-on aims to fluently transfer the garment image to a semantically aligned try-on area in the source person video. Previous methods leveraged the in-painting mask to remove the original garment in the source video, thus achieving accurate garment transfer on simple model videos. However, when these methods are applied to realistic video data with more complex scene changes and posture movements, the overly large and incoherent agnostic masks will destroy the essential spatial-temporal information of the original video, thereby inhibiting the fidelity and coherence of the try-on video. To alleviate this problem, we propose a novel point-enhanced mask-free video virtual try-on framework (PEMF-VVTO). Specifically, we first leverage the pre-trained mask-based try-on model to construct large-scale paired training data (pseudo-person samples). Training on these mask-free data enables our model to perceive the original spatial-temporal information while realizing accurate garment transfer. Then, based on the pre-acquired sparse frame-cloth and frame-frame point alignments, we design the point-enhanced spatial attention (PSA) and point-enhanced temporal attention (PTA) to further improve the try-on accuracy and video coherence of the mask-free model. Concretely, PSA explicitly guides the garment transfer to desirable locations through the sparse semantic alignments of video frames and cloth. PTA exploits the temporal attention on sparse point correspondences to enhance the smoothness of generated videos. Extensive qualitative and quantitative experiments clearly illustrate that our PEMF-VVTO can generate more natural and coherent try-on videos than existing state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Video Virtual Try-On, which aims to transfer the provided garment into a specific area in the source person video while maintaining the inter-frame coherence, has garnered significant attention especially for the E-commerce and fashion design fields. This technology greatly reduces the related costs and brings more convenient shopping and work experience to consumers and fashion designers.\nRecently, based on the powerful diffusion models [23, 33, 39, 50] and existing image-based virtual try-on training paradigm [7, 26, 34, 47, 48, 55], many video virtual try-on methods [13, 21, 43, 45, 54] have been proposed to leverage the inpainting mask to ensure the try-on area and the temporal attention module to keep the video coherence, obtaining natural and impressive try-on results on the simple in-shop model videos. However, in realistic scenarios, person videos may contain more complex body movements and scene changes (e.g., street dance videos). The pre-acquired agnostic mask of such hard videos will lead to 1) the loss of spatial information on human postures and 2) temporal inconsistency in try-on areas between adjacent frames, causing mask-based methods to generate distorted and incoherent try-on video, as Fig. 1 shown. Therefore, to learn a more general video virtual try-on model, it is necessary to design a more reasonable framework to eliminate the inherent deficiencies of mask-based try-on methods.\nIn the image-based virtual try-on area, several attempts [35, 47, 52, 53] have been made to alleviate the negative impact of the inpainting mask by either 1) correcting the initial inaccurate agnostic mask [47, 52] or 2) constructing the large-scale paired training data to learn a mask-free try on model [35, 53]. However, the correlation of the initial agnostic mask doesn't completely get rid of the dependencies of the mask guidance. Besides, as Fig. 1 shown, directly transferring the mask-free paradigm to a video virtual try-on task will bring confusing inconsistencies between reference garments and generated video frames. The core reason is the person video data contain more complex actions or movements at the temporal level, compared to image data. Based on the above analysis, it is necessary to design an innovative video virtual try-on paradigm that can avoid the deficiencies of agnostic masks while providing explicit and flexible guidance on the specific garment try-on area.\nIn this work, we propose a novel Point-Enhanced Mask-Free Video Virtual Try-On (PEMF-VVTO) framework, which exploits the sparse frame-cloth and frame-frame point alignments to explicitly guide the desirable try-on area while enhancing the coherence of generated try-on video. Specifically, we first choose the open-sourced mask-based video virtual try-on method ViViD [13] as our baseline framework. Then, we apply it to construct large-scale paired data to learn a more powerful mask-free try-on paradigm. Inadequately, the pure mask-free model can not completely meet the requirements to maintain the temporal consistency of the desirable try-on area, especially for realistic videos with diverse body movements. To further boost our mask-free video try-on method, by employing the reliable point matching model DIFT [40] and point tracking model TAP-Net [9, 10], we introduce sparse frame-cloth and frame-frame correspondences (matching point pairs) to enhance the consistency and coherence of garment transfer to source video. Concretely, a Point-enhanced Spatial Attention (PSA) is designed to further strengthen the garment transfer to specific area. Besides, a Point-enhanced Temporal Attention (PTA) is also proposed to increase the coherence of the generated video. In this way, our PEMF-VVTO can simultaneously meet the three important and challenging requirements of the video virtual try-on task: 1) the accurate transfer of reference garment 2) the preservation of the non-try-on area and 3) the continuity of generated video frames. Extensive qualitative and qualitative experiments clearly show the effectiveness of our method.\nOur contributions can be briefly summarized as follows:\n\u2022 We explore the deficiencies of existing mask-based video virtual try-on methods, and propose a more user-friendly and intelligent point-enhanced mask-free video virtual try-on paradigm than previous methods.\n\u2022 We design the Point-enhanced Spatial Attention (PSA) module and the Point-enhanced Temporal Attention (PTA) module to respectively enhance the garment transfer ability and frame continuity of generated try-on video.\n\u2022 To the best of our knowledge, our method is the first one to achieve the video-based mask-free virtual try-on paradigm while maintaining superior coherence. Extensive qualitative and quantitative experiments further illustrate the effectiveness of our method."}, {"title": "2. Related Work", "content": "Image Virtual Try-On. Given a source person image and a reference garment image, image virtual try-on aims to synthesize an identity and background preserved and cloth changed image. Previous image virtual try-on methods [6, 12, 14, 19, 20, 42, 46, 49] mainly leveraged Generative Adversarial Networks (GANs) [15] to first warp the reference garment to fit the person's body and then transfer the deformed garment into the source person. However, the limited generational capacity of GANs significantly influences the quality of try-on images for these GAN-based methods.\nRecently, based on the amazing performance of diffusion models in generating high-quality images at high resolutions, many diffusion-based virtual try-on methods [1,\n5, 7, 16, 26, 29, 56] have been proposed to generate natural and fidelity try-on images. For instance, StableVITON [26] employed a ControlNet-like [50] encoder to ensure the fine-grained garment transfer to person data. IDMVTON [7] designed a dual U-Net architecture to respectively encode the person feature and cloth feature, then conduct the cross attention between them to achieve high-quality garment transfer in wild realistic scenarios. While significant progress has been made, these methods heavily depended on the quality of the inpainting mask to determine the try-on area. When evaluating more complex try-on data that contains diverse foreground occlusions and person poses, they always fail to restore the non-try-on area.\nTo alleviate the above issue, TPD [47] and Betterfit [52] proposed the mask prediction or correlation module to dynamically obtain a more precise try-on area. However, these methods don't completely get rid of the dependencies of the mask guidance. Furthermore, BooW-VTON [53] and AnyDesign [35] performed a mask-free virtual try-on training paradigm. Though achieving impressive progress, they sometimes become confused to perceive accurate try-on areas, especially dealing with ambiguous cloth types or changing with different types of clothes.\nVideo Virtual Try-On. Most recently, following the training paradigm of image virtual try-on methods, some diffusion-based video virtual try-on methods [13, 21, 43, 45, 54] have been proposed. They utilized the inpainting mask to ensure the try-on area and employed the temporal attention module to maintain coherence in the generated video. However, these methods also inherited more severe deficiencies of the mask-based training paradigm due to the more complex video data. It is urgent and challenging to design a more effective and general video virtual try-on paradigm.\nDifferent from the above mask-based paradigm, we propose a point-enhanced mask-free video virtual try-on method to simultaneously achieve 1) controllability of desirable try-on area, 2) preservation of the non-try-on area and 3) coherence of generated video frames. To the best of our knowledge, PEMF-VVTO is the first method to achieve mask-free and point-guided controllability for the video virtual try-on task."}, {"title": "3. The Approach", "content": "3.1. Preliminary\nStable Diffusion. Our PEMF-VVTO leverages the Stable Diffusion (SD) [37], one of the most widely applied generation models based on the Latent Diffusion Model (LDM). LDM performs the denoising process in the latent space to conduct a more effective image generation. Specifically, a VAE encoder $E(\\cdot)$ first converts the image $x$ into a latent embedding $z_0 = E(x)$. Then the forward diffusion process is exploited by adding the noise to the latent embedding:\n$q(z_t | z_0) = \\mathcal{N}(z_t; \\sqrt{\\bar{\\alpha}_t} z_0, (1 - \\bar{\\alpha}_t)I),$ (1)\nwhere $t \\in \\{1, ..., T\\}$ represents the number of diffusion timesteps and ${\\bar{\\alpha}_t}_{t=1}^T$ determines the diffusion schedule. Finally, the denoising model $\\epsilon$ is trained to predict the added noise of the noisy latent $z_t$ by the loss constraints of LDM:\n$\\mathcal{L}_{LDM} = \\mathbb{E}_{E(x), y, \\epsilon \\sim \\mathcal{N}(0,1), t} [||\\epsilon - \\epsilon_\\theta(z_t, t, y)||^2]$, (2)\nwhere $\\epsilon$ represents the denoising model and $y$ is the conditional embedding to control the content of generation. In"}, {"title": "3.2. Our Proposed Method", "content": "Given a source person video and a reference garment image, our target is to fluently transfer the garment to a desirable try-on area in the source video, thus synthesizing a natural and coherent try-on video. Existing mask-based methods [7, 13, 26, 43] first employ the inpainting mask to determine the try-on area, and then adhere the provided garment to the try-on area. However, these pre-acquired agnostic masks not only destroy the original spatial information of person actions and postures, but also lead to the temporal inconsistency of the try-on area, which has a high risk of generating conflicting and disjointed try-on videos.\nIn this work, we conduct the video virtual try-on task through a new point-enhanced mask-free video virtual try-on paradigm. Specifically, we first exploit the mask-based method ViViD to construct the large-scale paired pseudo-person training samples, thus learning a powerful mask-free virtual try-on model. Then, to further boost the try-on performance, we leverage the pre-acquired sparse semantic frame-cloth and frame-frame alignments to integrate a novel Point-Enhanced Transformer (PET) into the mask-free model, thereby greatly enhancing the garment transfer ability and temporal coherence on more complex realistic human videos. The rest of this section will introduce our proposed method in detail. The pipeline of our proposed PEMF-VVTO and PET is shown in Fig. 2.\n3.2.1 Mask-free Video Virtual Try-on\nIt is intuitive that the overly large and incoherent agnostic mask will cause the lack of spatial information on actions and postures and the temporal inconsistency in try-on areas. Therefore, a natural idea is to train a mask-free try-on model. The main challenge of this idea is the lack of paired training data, that is, the same person with the same actions wears two different sets of clothes. However, it is not easy or even impossible to collect these paired training data from realistic scenarios. Fortunately, we can leverage the pre-trained mask-based try-on model to construct the paired pseudo-person samples as the training data of the mask-free model. Then, we will learn our mask-free video virtual try-on model by training on these synthetic data.\nPseudo-Person Data Preparation. Given a source person video $\\{x_t\\}_{t=1}^T$ and a garment image $g$, we should first obtain the cloth-agnostic video $\\{a_t\\}_{t=1}^T$, agnostic mask sequence $\\{m_t\\}_{t=1}^T$ and human pose $\\{p_t\\}_{t=1}^T$ of $\\{x_t\\}_{t=1}^T$. Following [13], the $\\{a_t\\}_{t=1}^T$ and $\\{m_t\\}_{t=1}^T$ are extracted through the human parsing model SCHP [2, 28]. The pose detection model DensePose [17] is employed to acquire the pose information $\\{p_t\\}_{t=1}^T$. Besides, the garment mask $m_g$ of $g$ is segmented by powerful semantic segment model SAM [27]. After acquiring all necessary inputs, we apply the pre-trained mask-based model to perform the virtual try-on operation on the publicly available image and video virtual try-on datasets with randomly selected same-type clothes. Then, the large-scale paired training data is successfully collected.\nMask-Free Video Virtual Try-on. To ensure the correctness of the learning objectives, the generated pseudo try-on data $\\{x_{ps}\\}_{t=1}^T$ and the original realistic data $\\{x_{gt}\\}_{t=1}^T$ are respectively determined as the input and the ground truth of our mask-free model. The mask-free model possesses the same network architecture as the mask-based model. In the inference stage, users can only provide the person video and reference garment image to obtain a reliable try-on result. Without the agnostic mask, our mask-free model can effectively preserve the complex human movements and poses of the source video with reasonable and natural modifications of the try-on area, showing a superior performance than mask-based methods."}, {"title": "3.2.2 Point-Enhanced Transformer", "content": "Mask-free try-on model have eliminated the spatial occlusion of the agnostic mask on the movements and postures of the human body in the source video. However, different from image data, video exists significant scenes and poses changes in the temporal level, which may lead the model to perceive semantic confusing try-on area between different video frames, thereby causing sub-optimal try-on performance. Therefore, it is worth of considering the other explicit guidance to enable the model to better achieve controllable and reasonable video try-on results.\nInspired by recent works [4, 31, 32, 38], we choose flexible and powerful matching point pairs to explicitly strengthen the accurate garment transfer and video coherence. Specifically, we first acquire the sparse frame-cloth and frame-frame point alignments through the diffusion-based matching model DIFT and point tracking model TAP-Net. Then, as Fig. 2 shown, a Point-Enhanced Transformer (PET), which adds the Point-enhanced Spatial Attention (PSA) and Point-enhanced Temporal Attention (PTA) compared to the transformer layer of denoising U-Net D in baseline model, has been designed to make full use of the explicit guidance of these point alignments.\nPoint Sampling and Alignment. The pipeline of our Point Sampling and Alignment (Point SA) is shown in Fig. 3. In the training stage, we first randomly select a frame $x_{gt}^t$ from the realistic ground truth video $\\{x_{gt}\\}_{t=1}^T$. Then M sparse points is randomly sampled from the agnostic mask (try-on area) of $x_{gt}^t$. In this work, the number of selected points $M \\leqslant K$, where $K$ denotes the maximum number of matching correspondences. After that, DIFT will calculate the semantic-aware frame-cloth point alignments of these sparse points from the garment image $g$. Finally, we exploit the point tracking model TAP-Net to acquire the frame-frame correspondences based on the select points in $x_{gt}^t$. In the inference stage, after selecting a frame $x_i$ from the source video $\\{x_t\\}_{t=1}^T$, users can mark the matching points of $x_i$ and $g$ to achieve a spatially controllable and temporally smooth virtual try-on process.\nAfter acquiring the point alignments, we construct two 0-1 binary masks $\\{c_x\\}_{t=1}^T$ and $c_g$ to respectively represent the positions of frame points in $\\{x_{gt}\\}_{t=1}^T$ and garment points in $g$. The values at the selected point positions on $\\{c_x\\}_{t=1}^T$ and $c_g$ are set to 1. A max pooling operation MaxPool is then applied to obtain the masks in lower resolutions, which align with the denoising U-Net D. Besides, a 3 \u00d7 3 channel-wise convolution is applied to $\\{c_x\\}_{t=1}^T$ and $c_g$ to expand the receptive field of matching points, thus obtaining a soft alignment mask $m$.\nPoint-enhanced Spatial Attention. Referring to the powerful Diffusion Transformer (DiT) [36], in Fig. 4 (a), we design our point-enhanced spatial attention (PSA) to provide explicit guidance of the try-on area without destroying any spatial information on human movements and postures. Specifically, the input of j-th PET layer $\\{d_j\\}_{t=1}^T$ is employed as the query $Q^* \\in \\mathbb{R}^{T \\times N \\times C}$ of PSA, where $N$ is the pixel number of $d_j$ and $C$ is the number of latent channel. Then, based on the point position representations $\\{c_x\\}_{t=1}^T$ and $c_g$, we acquire the sparse frame-level point latents $\\{d_x\\}_{t=1}^T$ and garment point latents $r_g$ as the key $K \\in \\mathbb{R}^{1 \\times M \\times C}$ and value $V_g \\in \\mathbb{R}^{T \\times M \\times C}$ of PSA. Before entering the PSA, following [36], we exploit the timestep embedding $f_t$ to regress the dimension-wise scale parameters $\\gamma_1, \\gamma_2$ and shift parameters $\\beta_1$ and $\\beta_2$ of query $\\{d_j\\}_{t=1}^T$ and the dimension-wise scale parameters $\\alpha_1$ and $\\alpha_2$ of residual connections by MLP layers. The formula of PSA is given as:\n$Attn = softmax((\\gamma_1 \\cdot LN(Q^*) + \\beta_1) K^\\top + W),$ (3)\n$Q = Q^* + \\alpha_1 \\cdot Attn \\cdot V_g,$\nwhere the $W \\in \\mathbb{R}^{T \\times N \\times M}$ is a point-wise attention bias to adaptively adjust the similarities between $\\{d_x\\}_{t=1}^T$ and $r_g$ in the try-on area. The $W = m \\cdot FFN(Cat(K_x, V_g, f_t))$ as Fig. 4 (c) shown. Besides, to further enhance the controllability of the try-on area, the soft alignment mask $m$ explicitly constrains the updated residual to only work in the surrounding area of point pairs by the following formula:\n$Q^* = Q^* + m \\cdot \\alpha_2 \\cdot FFN(\\gamma_2 \\cdot LN(Q^*) + \\beta_2)$ (4)\nPoint-enhanced Temporal Attention. In [18, 24], it employed a temporal attention module to ensure the coherence of the generated video. However, this method assumes that the pixels between different frames with the same coordinates are matching points, which is inexact or even wrong, especially for human videos with complex actions or movements. To this end, as shown in Fig. 4 (b), we leverage the pre-acquired frame-frame alignments $\\{c_x\\}_{t=1}^T$ to achieve a more reasonable point-enhanced temporal attention (PTA) to further improve the coherence of the try-on video. Concretely, we employ the sparse frame-level point latents $\\{d_x\\}_{t=1}^T$ as the query $Q$ of the PTA, while utilizing the sparse frame-level point latents $\\{d_x\\}_{t=1}^T$ in conjunction with garment point latents $r_g$ as the key $K$ and value $V$. Then, the multi-head attention is employed to $Q, K$ and $V$ to enhance the consistency between frames of the surrounding area of these matching points $\\{c_x\\}_{t=1}^T$, thus promoting the overall video smoothness. The formula is given as:\n$Q^\\prime = Q + FFN(LN(Softmax(LN(Q) \\cdot K^\\top) \\cdot V))$ (5)"}, {"title": "3.2.3 Training and Inference", "content": "Training scheme. The training scheme of PEMF-VVTO comprises three stages: (1) Stage 1: the denoising U-Net D is initialized as a 2D inpainting model, and only a single frame of pseudo-person data is taken as training data, which enables the model to perceive and transfer the reference garment to a reasonable try-on area. The parameters of P, R and D are updated in this stage. (2) Stage 2: we initialize the temporal attention module with the parameters in [18] and exploit both image and video data to only finetune it, thus enhancing the temporal coherence of generated try-on results. (3) Stage 3: since most of the training samples have been well learned through the first and second stages, we first leverage the mask-free model of the second stage to construct hard-paired pseudo-person training samples with lower generation performance (i.e. SSIM < 0.75). Then, to ensure the PSA and PTA can really promote the temporal consistent and controllable garment transfer, we only train the PSA module and the PTA module with these hard training samples. The learning objectives of the three training stages are the same LDM loss in Eq. (2).\nInference scheme. In the inference stage, according to most human videos with simple action postures, our mask-free model can achieve accurate and desirable try-on results without the guidance of point alignments. However, when dealing with more complex human videos (e.g., street dance videos) or aiming at more controllable try-on results, users can manually click on the matching point pairs between a randomly selected single video frame and reference garment image, thereby acquiring a more coherent and natural generation. A more detailed inference scheme can be seen in supplementary materials."}, {"title": "4. Experiments", "content": "4.1. Dataset and Experimental Setting\nDatasets. Following our baseline backbone [13], we train our model on two image datasets, VITON-HD [6] and Dress Code [30], and one video dataset, ViViD [13]. The commonly used VVT [11] dataset is selected as the evaluation dataset to obtain a fair performance comparison. Besides, we also choose a more challenging StreetVTON image dataset [8] and TikTok video dataset [25] to show our visualization results. All datasets are open source for wide research purposes. The detailed introduction of all datasets is shown in the supplementary materials.\nMetrics. Following previous methods [13, 43], both frame-level metrics LPIPS [51], SSIM [44], FID [22] and video-level metric VFID [41] metric with I3D [3] backbone are adopted for comprehensive evaluation.\nImplementation Details. We initialize the denoising U-Net D and reference U-Net R with the weights from Stable Diffusion-1.5. The temporal module is initialized with the"}, {"title": "4.2. Quantitative Results", "content": "To achieve a fair and comprehensive comparison with previous methods, we exploit two different evaluation settings. Specifically, in Tab. 1 and Tab. 2, we evaluate our proposed Mask-Free video virtual try-on paradigm (MF) and Point-Enhanced Transformer (PET) by using the same agnostic data as the previous mask-based method. However, this approach does not fully showcase the abilities"}, {"title": "4.3. Qualitative Results", "content": "As shown in Fig. 5, we show the visualization results for both ViViD and our PEMF-VVTO models. Specifically, compared to the baseline model, our generated results achieve significant visual fidelity to the reference garment in the spatial dimension and content consistency in the temporal dimension in all datasets. It is worth noting that our model is only trained on the simple model images and videos and obtains superior try-on results on more challenging StreetVTON and TikTok datasets, which demonstrates the effectiveness and generalization of our method.\nBesides, in Fig. 6, we also find that our PEMF-VVTO can leverage the point alignments to determine the desirable try-on areas. It further illustrates that our designed PET can effectively transfer the garment details to the specific area to achieve a more flexible and controllable try-on process."}, {"title": "4.4. Ablation Studies", "content": "Different maximum number of points. In Fig. 7, we investigate the effects of the maximum number of points $K$ on the performance of two synthetic pseudo datasets. As $K$ increases, the FID and VFID metrics first improve and then stabilize. It can be attributed to the repetition of more sampling points. Therefore, we choose $K$ = 16 in this paper.\nPoint-Enhanced Spatial Attention. Based on the frame-cloth point alignments, the PSA is designed to provide explicit guidance to transfer the garment into desirable try-on area, which significantly improves the performance of our mask-free model as Tab. 4 shown. Besides, in the upper part of Tab. 4, we analyze the effectiveness of soft alignment mask m and point-wise attention bias W. Our PSA obtains performance decrease when removing the two designs, which illustrates the reasonability of PSA.\nPoint-Enhanced Temporal Attention. The complex human actions at the temporal level always lead to the discontinuity of the try-on video. To alleviate this issue, we design the PTA to enhance the coherence of the try-on area between different frames. As Tab. 4 shown, the PTA brings clear improvements to the mask-free model. Besides, the performance of PTA decreases when only selecting the frame-level point latents $\\{d_x\\}_{t=1}^T$ as the key K and value V of PTA, which further verifies the effectiveness of our PTA."}, {"title": "5. Limitation", "content": "Although our proposed PEMF-VVTO has strongly enhanced the accurate garment transfer ability in both spatial and temporal dimensions, our method may generate incorrect try-on results when dealing with longer input video with more ambiguous garment types. Besides, the ability to control and edit the try-on area should be further improved. In the future, we will construct more diverse person-pseudo training data to continually train our model, thereby promoting the continuous upgrading and evolution of our video virtual try-on model to achieve better performance on more challenging and realistic video data."}, {"title": "6. Conclusion", "content": "To prevent the existing mask-based methods from destroying the spatial-temporal information of the virtual try-on process, in this work, we propose a novel Point-Enhanced Mask-Free Video Virtual Try-On method (PEMF-VVTO). Concretely, we first leverage the pre-trained try-on model to construct paired pseudo-person training samples to learn a more powerful mask-free try-on framework. Then, based on the pre-acquired sparse alignments, The Point-enhance Spatial Attention (PSA) and Point-enhance Temporal Attention (PTA) are designed to improve the controllability and coherence of more complex realistic human video. Compared to existing methods, our PEMF-VVTO can simultaneously achieve 1) the accurate transfer of reference garment 2) the preservation of non-try-on areas and 3) the continuity of generated video frames. Extensive quantitative and qualitative experimental results clearly show the effectiveness of our method."}]}