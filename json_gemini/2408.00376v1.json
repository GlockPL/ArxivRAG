{"title": "On the Limitations and Prospects of Machine Unlearning for Generative AI", "authors": ["Shiji Zhou", "Lianzhe Wang", "Jiangnan Ye", "Yongliang Wu", "Heng Chang"], "abstract": "Generative AI (GenAI), which aims to synthesize realistic and diverse data samples from latent variables or other data modalities, has achieved remarkable results in various domains, such as natural language, images, audio, and graphs. However, they also pose challenges and risks to data privacy, security, and ethics. Machine unlearning is the process of removing or weakening the influence of specific data samples or features from a trained model, without affecting its performance on other data or tasks. While machine unlearning has shown significant efficacy in traditional machine learning tasks, it is still unclear if it could help GenAI become safer and aligned with human desire. To this end, this position paper provides an in-depth discussion of the machine unlearning approaches for GenAI. Firstly, we formulate the problem of machine unlearning tasks on GenAI and introduce the background. Subsequently, we systematically examine the limitations of machine unlearning on GenAI models by focusing on the two representative branches: LLMs and image generative (diffusion) models. Finally, we provide our prospects mainly from three aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and conscientiously advocate for the future development of this field.", "sections": [{"title": "1. Introduction", "content": "In an era marked by the burgeoning influence of Generative AI (GenAI) (Baidoo-Anu & Ansah, 2023), we are rapidly progressing toward a digital future dominated by AI-generated content. This technological advancement has become a cornerstone in various domains, including natural language processing, image synthesis, audio generation, and graph-based applications. While GenAI heralds an era of innovation and efficiency, it simultaneously raises pressing concerns about data privacy, security, and ethical implications (Carlini et al., 2023).\nThe training datasets employed in GenAI often contain sensitive information encompassing private, copyrighted, or potentially harmful content (Dubi\u0144ski et al., 2024). This situation raises significant risks of sensitive data leakage (Wu et al., 2022), directly conflicting with the growing legislative emphasis on the \"right to be forgotten\" (Rosen, 2011). Instances such as the proliferation of copyright infringement cases post the release of models like Stable Diffusion (Rombach et al., 2022), and The New York Times's lawsuit against OpenAI for content leakage\u00b9, underscore the urgency of addressing these issues.\nIn response to these challenges, Machine unlearning (Bourtoule et al., 2021) has emerged as a potentially promising solution. Machine unlearning aims to compel models to forget sensitive information, thereby fundamentally eliminating the risk of content leakage. This approach, which seeks to erase sensitive memories directly, stands in contrast to filtering-based solutions that are often susceptible to bypassing or direct attacks. Current research on Machine unlearning spans various generative models, including Large Language Models (LLMs) (Yao et al., 2023), image generative models (Mishkin et al., 2022), and multi-modal generative models (Suzuki & Matsuo, 2022). These studies have demonstrated machine unlearning's potential in removing elements like copyrighted styles (Gandikota et al., 2023), fictional characters (Eldan & Russinovich, 2023), and private data (Tarun et al., 2023).\nHowever, the exploration of MU in the context of GenAI is still nascent, with several limitations hindering its full potential. As illustrated in Figure 1, we summarise the urgent limitations of the current machine unlearning methods on GAI into three perspectives. Firstly, as an emerging technique, machine unlearning for GenAI is still distant from achieving a level of efficacy requisite for practical applications. Secondly, the evaluation metrics currently employed in MU research are insufficiently robust and fail to capture"}, {"title": "2. Background", "content": "Through this paper, we seek to contribute to the growing body of knowledge in GenAI, specifically in the area of machine unlearning, and catalyze further research that addresses the critical challenges identified.\n2.1. Unlearning Formulation\nLet $D_{tr} = \\{(x_i)\\}_{i=1}^n$ be the training data where $x_i \\in X$ is the training input. Suppose we have the original model by optimizing towards the training data:\n$\\theta_0 = \\arg \\min_{\\theta \\in \\Theta} E_{x \\sim D_{tr}}L(\\theta, x)$.\nLet $D_f \\subseteq D_{tr}$ be a subset of training data that is harmful to the model and needs to be forgotten, and $D_r = D_{tr} \\setminus D_f$ be remaining training data of which information we want to retain. The goal of machine unlearning is to successfully unlearn $D_f$, and the exact unlearning is to obtain the gold standard model retrained from scratch with only $D_r$:\n$\\theta^* = \\arg \\min_{\\theta \\in \\Theta} E_{x \\sim D_r} L(\\theta, x)$.\nExact unlearning can be obtained by retraining, which causes tremendous cost that is not affordable to frequently updating models. In practice, approximate unlearning aims to finetune the original model to obtain the unlearned model $\\theta_u$ whose output distribution $P_{\\theta_u}(\\cdot)$ approximates the distribution of gold standard model $P_{\\theta^*}(\\cdot)$. Unless otherwise indicated, we only discuss the approximate unlearning in the next context in this paper.\n2.2. Unlearning for LLM\nMachine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into:\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations."}, {"title": "2.3. Unlearning for Image Generative Model", "content": "Image generative models have various applications, such as image editing, style transfer, super-resolution, and data augmentation (Iqbal & Qureshi, 2022). However, image generative models also face challenges and risks, such as violating data privacy, infringing data ownership, and generating inappropriate or misleading images. Based on the degree of influence removal achieved, the existing methods for machine unlearning for image generative models can be broadly classified into two categories (Xu et al., 2023):\nExact Unlearning Methods. These methods focus on removing the influence of targeted data points from the model through retraining at the algorithmic level completely. It usually involves censoring images from the training dataset such as removing all people's images (Nichol et al., 2021) or excluding undesirable classes of data and then performing model retraining (Mishkin et al., 2022). The retraining process of large models is often costly, which makes this dataset removal-based approach less practical.\nApproximate Unlearning Methods. These methods aim to minimize the influence of target data points in an efficient manner through limited parameter-level updates to the model. This approach is post-hoc and efficient to test and deploy. Among them, (Fan et al., 2023) introduced a new concept of weight saliency and used a gradient-based approach to estimate the influential weights and then conduct unlearning accordingly.(Lin et al., 2023) defined the unlearning process from the knowledge perspective and proposed an entanglement-reduced mask (ERM) structure to reduce the knowledge entanglement during training. (Gandikota"}, {"title": "3. Limitations", "content": "Through this paper, we seek to contribute to the growing body of knowledge in GenAI, specifically in the area of machine unlearning, and catalyze further research that addresses the critical challenges identified.\n3.1. The Efficacy of Unlearning\nWe divide the limitations on the efficacy of the current methods into four subsections. These include the discussions on the overarching efficacy limitations, followed by specific efficacy of the LLMs and the image generative models.\n3.1.1. GENERAL WEAKNESSES\nDependence on the Original Training Data. Many of the existing unlearning methods assume that the unlearning targets are a subset of the training set, and therefore require access to the original training data (Anonymous, 2024; Bae et al., 2023; Yao et al., 2023). However, it can be difficult to obtain the original training data at times. Training data can be confidential when they are concerned with privacy or contractual issues. For models trained through distributed learning, the model and data are decentralized and can be aggregated from complex sources. Moreover, the training data can often be lost or corrupted due to the limit of storage space, cyber-attacks, or hardware failures. Unlearning methods that are dependent on the training data can not fit into these scenarios, which hinders the generalization of unlearning methods to more real-world applications.\nScalability Issue. The parameter optimization technique is still the major fundamental idea of the existing unlearning methods for generative models, which involves the iterative updating of parameters (Si et al., 2023; Anonymous, 2024; Moon et al., 2023). This updating process can be computationally expensive and time-consuming, especially when dealing with large-scale models or datasets. Besides, even though the unlearning of target instances can be conducted in a batch-wise manner to improve the efficiency (Xu et al., 2023), experiments show that a larger batch size brings more degradation to the performance of the model (Jang et al., 2022). An alternative way is to feed the unlearning instances sequentially, which is beneficial for maintaining the capability of the model (Jang et al., 2022; Chen & Yang, 2023a). The sequential approach compromises system efficiency and poses significant scalability challenges. Particularly in practical applications, there is a frequent necessity to consecutively apply unlearning on the large generative models within constrained timeframes to maintain the relevance and accuracy of models in dynamic environments. Current"}, {"title": "3.1.2. FOR LLMS", "content": "Task Dependence. LLMs learn general representations of both syntactic and semantic knowledge during their pre-training on the large corpus. This enables them to serve as a more general-purpose tool to solve different tasks. The patterns in the representations are intertwined and can be vulnerable when combined with downstream tasks. Catastrophic forgetting is a typical example of such vulnerability, which often occurs during transfer learning. The model can lose its generalization ability and overfit to the target domain in a catastrophic forgetting (Luo et al., 2023; Zhai et al., 2023; Wang et al., 2023a). Thus, when testing the efficacy of an unlearning method for the LLMs, it is important to conduct comprehensive fidelity experiments on datasets from various domains. Nevertheless, most of the existing work tests the retaining and forgetting performance of the unlearning methods on specific datasets (Chen & Yang, 2023a; Yao et al., 2023). The impact of the unlearning process on the model's generalization ability to other tasks is rarely verified. Although the model preserves its performance on the current task, it remains uncertain whether the nuanced modifications in parameters during unlearning force the model to compromise its capability in other tasks. We argue that an effective unlearning method should minimize the performance degradation of the target model on diverse tasks.\nForget or Lie? Nowadays, we hold a higher expectation of generative models than before. This gap of expectation is more prominent in terms of LLMs. In the previous era, we mainly focused on improving the fluency and stability of the generation. Therefore, as the unlearning results from (Eldan & Russinovich, 2023) shown in Figure 2, a worse performance or a fabricated description for the sensitive instances could be viewed as a successful unlearning result. However, simply generating incorrect outputs can no longer be enough when we are expecting factual and reliable generation. LLMs are being transformed to function as knowledge bases (AlKhamissi et al., 2022) consisting of structural representations of facts and relations. Besides, substantial efforts have been devoted to reducing hallucinations (Rawte et al., 2023). Fake outputs have become increasingly unacceptable after we force the LLMs to forget target knowledge. Contrary to the conventional methods"}, {"title": "3.1.3. FOR IMAGE GENERATIVE MODELS", "content": "Visual Perception Inconsistencies. Human visual systems exhibit high sensitivity to inconsistencies in images. Conversely, image generative models after unlearning tend to generate images with nuanced discrepancies, which are noticeably contradictory to human perceptual norms. For models with deletion of features (Moon et al., 2023), the discrepancies can be subtle changes in the details of the image including abnormal colors, shapes, or textures of objects and backgrounds. For models trained to unlearn a large block of an image (Anonymous, 2024), imbalanced foreground and background, unrealistic patterns, or visual artifacts can occur in the images. This kind of abnormal factor is a critical obstacle to the landing of the image generation unlearning techniques and can also be potentially risky for the privacy of users. Thus, it is significant for unlearning methods to minimize the perception inconsistencies in the generated images to become more reliable and effective.\nCreation of Visual Bias. Some of the unlearning methods for image generation aim to remove certain features or concepts (Moon et al., 2023) in the images. This process requires the unlearned model to recover the part that was occupied by the patterns related to the concept. The limitation is that the recovery can introduce undesired biases that may not have existed in the original image. These biases could manifest in the form of gender-specific disparities or patterns indicative of racial biases. Assuming the primary goal of employing concept-removing unlearning is to eliminate biased features, the introduction of other types of biases could bring unpredictable deficiencies to the data, which might be even more challenging to detect and address.\nAmbiguity in Prompts. Image generative models, especially text-to-image models, generate images that share closely matched representations with the corresponding texts in the same latent space. In the unlearning of text-to-image models, textual concepts usually serve as the prompt to guide the removal of visual features. Under this setting, one of the key challenges is to retain the intrinsic similarities between text and image representations. However, some textual prompts can be ambiguous and some concepts may possess vague boundaries with others. The ambiguity may"}, {"title": "3.2. The Side Effect of Unlearning", "content": "Impact on Utility. During the process of machine unlearning, particularly in text-to-image generative models, it becomes evident that forgetting specific concepts can have some negative consequences on the performance of associated concepts (Gandikota et al., 2023; Kumari et al., 2023; Wu et al., 2024b). This phenomenon is especially pronounced when considering the intricate relationship between artistic styles. For instance, unlearning the style of Van Gogh may inadvertently impact the style of Claude Monet. Previous studies largely overlooked the inclusion of mechanisms to mitigate these negative outcomes and propose proper methods to control these effects (Zhang et al., 2023).\nImpact on Generalization. Machine unlearning methods usually inversely process the forget set samples, which potentially influences the test performance. On the one hand, a majority method tries to decay the performance of the unlearned model, by flipping the label (Pawelczyk et al., 2023) or updating with inverse gradient (Yao et al., 2023). However, the forget set performance can be viewed as the test performance of the unlearned model. On the other hand, some methods (Chen & Yang, 2023b) aims to align the output distribution of the forget set and an unseen set of the unlearned model. However, the inherent difference between the forget set and the unseen set may lead to mismatching between the two targets. Therefore, forcing the model to inversely learn the forget set samples may impair the generalization of the test set."}, {"title": "3.3. The Difficulty of Measuring", "content": "Addressing the complexities of evaluating the efficacy of unlearning across different generative scenarios, such as Language Models and image generative models, poses a multifaceted challenge. Each scenario presents unique obstacles, necessitating a tailored approach. After examining individual scenarios, we will explore the overarching challenges that pervade the field of generative model unlearning.\nEvaluation for Large-scale Models. Evaluating unlearning in LMs, particularly large-scale models, encounters the"}, {"title": "4. Future Prospects", "content": "Addressing the complexities of evaluating the efficacy of unlearning across different generative scenarios, such as Language Models and image generative models, poses a multifaceted challenge. Each scenario presents unique obstacles, necessitating a tailored approach. After examining individual scenarios, we will explore the overarching challenges that pervade the field of generative model unlearning.\n4.1. Towards More Effective Unlearning for GenAI\nWe provide three directions where future research can focus in terms of improving the efficacy of unlearning for GenAI.\nTransferable and Scalable Unlearning. In the forthcoming period, with the extremely increasing number of data being pooled into the training of large generative models, the demand for the right to be forgotten will inevitably intensify. This can induce the massive use of the unlearning methods on a significant amount of data. In this case, we emphasize that future unlearning methods should be able to effectively adapt to large-scale applications. The unlearning should be agnostic to the statistics of the original data to enable transferability between different tasks. Moreover, instead of retraining the whole model, more attention needs to be paid to parameter-efficient methods that only adjust a small portion of weights. Only lightweight unlearning methods can become prevalent and permeate into any downstream areas where sensitive data could potentially be located. When designing an unlearning algorithm, researchers may need to care more about a fast and accurate tackling of the privacy problem instead of sacrificing efficiency for a minor performance improvement.\nUnlearning for General Generative Models. As we are pacing into the era of Artificial General Intelligence(AGI), we have to consider the demand from general generative models when designing unlearning methods. Since the emergence of generalisability usually comes from extreme scaling up at a very high cost, the loss of such ability will not be affordable. We need unlearning methods that maintain the general ability of the target model on all tasks. This certainly should be accompanied by a comprehensive evaluation benchmark, which will be discussed in later sections. Additionally, the unlearning method should also be curated to prevent hallucinations. As aforementioned in 3.1.2, existing unlearning methods can lead the generative models to output fabricated facts which may require more effort in cleaning. To address this problem, different from knowledge editing which performs precise alterations of the knowledge, in unlearning we may avoid the injection of new knowledge and seek alternative ways to represent the forgetting of knowledge.\nSafe Unlearning The unlearned model should not be- come more vulnerable to the attack of privacy or bias. Even though, as we mentioned in 3.1.1, most of the existing methods ignore the necessity to minimize the revealing of privacy"}, {"title": "4.2. Utility-Unlearning Trade-off", "content": "or the creation of bias under hostile attacks. We must consider whether the unlearning algorithm will amplify the bias or privacy issues in the original training data and whether the models after unlearning will exhibit excessive reactions to the modified data points. We may consider integrating differential privacy techniques during the unlearning phase, which can provide a mathematical guarantee of privacy protection. Additionally, fairness-aware algorithms could be adapted to monitor and adjust the model's outputs, ensuring that unlearning does not unfairly impact certain groups. By prioritizing the development of such comprehensive approaches, the field can move towards unlearning methods that not only remove data effectively but also uphold the ethical standards required for responsible AI development.\nAs delineated in the preceding section, extant unlearning algorithms substantially impair the performance of the original model. Consequently, in the design of unlearning algorithms, it is imperative to consider the trade-off between model performance and the efficacy of unlearning. In this section, we envisage several potential solutions that may address this Utility-Unlearning Trade-off in the future.\nRegularization. Owing to the small size of the forget set, the disparity in parameters between the retrained model and the original model is typically minimal. However, in current unlearning algorithms, there is often a substantial deviation of the unlearned model from the original model due to an overemphasis on unlearning efficacy. This deviation results in the loss of a considerable amount of useful information, thereby substantially diminishing the utility of the model. Consequently, a very direct approach would be to employ various regularization methods to constrain the changes to the parameters during the unlearning process. Techniques from parameter-efficient fine-tuning methods, such as those employed in LoRA (Hu et al., 2021) or Adapter (Houlsby et al., 2019) methods, could be adapted for this purpose. This approach achieves a more optimal Utility-Unlearning Trade-off and facilitates a faster unlearning efficiency.\nMulti-Objective Optimization. Merely simplistically applying regularization constraints could yield unforeseen outcomes, and due to the presence of conflicting training objectives, this approach may result in suboptimal unlearning efficiency. An alternative method worth exploring involves the employment of gradient surgery techniques, commonly employed to address conflicting gradients present in the multi-task learning framework (Yu et al., 2020; Zhu et al., 2023; Wang et al., 2023b). By pruning gradients that conflict with the direction of knowledge preservation, it becomes feasible to retain the performance manifestations of the samples in the retained set intact."}, {"title": "4.3. Evaluating Metrics", "content": "In the pursuit of advancing the field of machine unlearning in generative models, it is imperative to address the notable limitations in current evaluative practices. The intricate nature of generative models, ranging from language to image generation, demands a nuanced approach to metric development. This endeavor is not merely a technical challenge but a fundamental requirement to ensure that unlearning processes align with ethical standards, maintain utility, and adapt to diverse applications. The following directions are proposed not only as responses to identified gaps but as strategic advancements that acknowledge the evolving landscape of GenAI.\nHolistic and Standardized Metrics. The development of holistic and standardized metrics is crucial for creating a uniform framework for evaluating unlearning across different generative models. This approach will facilitate comparative studies and benchmarking, enabling a clearer understanding of the effectiveness of various unlearning methods. By integrating measures of residual information, utility retention, and model integrity, these metrics can provide a comprehensive assessment that is currently lacking in fragmented and scenario-specific evaluations.\nAdvanced Residual Information Assessment. Advanced methods for residual information assessment are essential to address the limitations of current metrics, which often fail to capture the nuanced effects of unlearning. This necessitates the exploration of novel approaches, such as utilizing AI interpretability techniques, to trace and quantify the lingering influences of unlearned data. Such methods could provide deeper insights into the effectiveness of unlearning processes, bridging the gap between theoretical unlearning and its practical implications.\nIntegrated Efficacy-Utility Metric. A nuanced approach involves formulating an integrated metric that encapsulates both unlearning efficacy and utility preservation. This could be operationalized as a composite score, merging domain-specific measures of unlearning with utility indicators (e.g., FID for image models, ROUGE for text models). Adjusting the weightage in the composite score would allow for flexibility depending on domain-specific requirements.\nEmphasizing Condition-Output Alignment. In conditional generative models, maintaining the fidelity of outputs to their conditions post-unlearning is critical. Developing metrics that rigorously evaluate this alignment is imperative, especially considering the subjective nature of outputs in these models."}, {"title": "4.4. Unlearning Benchmark", "content": "Benchmarking is indeed an important aspect of evaluating and comparing different machine unlearning methods for GenAI. However, there are not many existing benchmarks that specifically address this problem. Therefore, there is a need to develop more comprehensive, reproducible, and interpretable benchmarks for machine unlearning in GenAI.\nBenchmarking LLMs. It was only recently that the urgent of benchmarking machine unlearning in LLMs aroused the attention of researchers. As the pioneering research, TOFU (Task of Fictitious Unlearning) benchmark (Maini et al., 2024) involves a dataset of 200 synthetic author profiles, each with 20 question-answer pairs, and a subset known as the 'forget set' for unlearning. TOFU allows for a controlled evaluation of unlearning with a suite of metrics, offering a dataset specifically designed for this purpose with various task severity. While TOFU is a significant contribution to the field of machine unlearning in GenAI by introducing the first comprehensive benchmark for unlearning in the context of LLMs, it also has some limitations.\nThese limitations inspire us for future directions on further benchmarking LLMs from different perspectives: (i) The synthetic nature of the data and the scenarios may not capture the real-world challenges and risks of machine unlearning in LLMs, such as the diversity and complexity of the data sources, the ambiguity and subjectivity in data contents, and the ethical and legal implications of the data ownership and consent. Therefore, real-world datasets especially where the exact retrain set is inaccessible would be a valuable add-on for effective evaluation. (ii) The evaluation metrics and datasets may not be sufficient or representative of the forget quality and model utility, as they only cover a limited range of tasks and domains with a small quantity. Therefore, more complex tasks that account for the tradeoffs and interactions between different metrics and datasets, such as directly forgetting a specific person rather than a set of people, are important to demonstrate the forget quality and model utility. (iii) The baseline methods may not be adequate or competitive for machine unlearning in LLMs, as they only include four methods that are based on parameter optimization. In the future, more LLM-native techniques such as parameter merging or in-context learning should be considered to make the baselines more comprehensive.\nBenchmarking Image Generative Models While there have been some efforts to address the issue of unlearning in image generative models, a clearly defined benchmark for evaluating this process is yet to emerge (Gandikota et al., 2023; Kumari et al., 2023; Gandikota et al., 2024).\nTo develop a comprehensive benchmark for the unlearning capabilities of image generative models, it is essential to consider multiple dimensions: (i) The diversity of unlearning goals. These should encompass tasks such as eradicating distinct image aesthetics, excising particular objects from scenes, and filtering out content that is not suitable for all users. (ii) The situations involving the simultaneous unlearning of multiple concepts. Past endeavors in dataset construction primarily focused on unlearning individual concepts. Yet, it holds paramount importance and possesses practical significance to carry out examinations that evaluate the eradication of multiple concepts concurrently. (iii) The unlearning objective of synonym concepts. For example, the model's competency in unlearning the artistic style of Van Gogh should be tested. This should include the model's ability to dissociate from works like \u201cStarry Night,\" which, despite not explicitly naming Van Gogh, could still be indicative of his distinctive style.\""}, {"title": "5. Conclusions", "content": "In this position paper, we have systematically examined the challenges and limitations in the field of machine unlearning within GenAI. Our analysis reveals critical areas requiring attention: efficacy limitations in LLMs and image generative models compounded by issues of scalability, potential data leakage, the side effects on model utility and safety, and the difficulty of measurement design. We posit more effective unlearning for GenAI, the establishment of robust and nuanced evaluation metrics, and a balanced approach to the utility-unlearning trade-off. These steps are crucial for the development of sophisticated benchmarks, and realizing GenAI's full potential while adhering to ethical standards."}, {"title": "Impact Statements", "content": "Our contribution lays a foundation for future research, emphasizing the need for innovative, responsible strategies in machine unlearning. We assert that addressing these challenges is imperative for the ethical advancement of GenAI, ensuring its alignment with societal values and legal norms. This paper serves as a call to action for the research community to prioritize responsible and effective unlearning methods in the rapidly evolving landscape of GenAI."}]}