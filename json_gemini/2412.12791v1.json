{"title": "Implicit Location-Caption Alignment via Complementary Masking\nfor Weakly-Supervised Dense Video Captioning", "authors": ["Shiping Ge", "Qiang Chen", "Zhiwei Jiang", "Yafeng Yin", "Liu Qin", "Ziyao Chen", "Qing Gu"], "abstract": "Weakly-Supervised Dense Video Captioning (WSDVC) aims\nto localize and describe all events of interest in a video\nwithout requiring annotations of event boundaries. This set-\nting poses a great challenge in accurately locating the tem-\nporal location of event, as the relevant supervision is un-\navailable. Existing methods rely on explicit alignment con-\nstraints between event locations and captions, which in-\nvolve complex event proposal procedures during both train-\ning and inference. To tackle this problem, we propose a\nnovel implicit location-caption alignment paradigm by com-\nplementary masking, which simplifies the complex event pro-\nposal and localization process while maintaining effective-\nness. Specifically, our model comprises two components: a\ndual-mode video captioning module and a mask generation\nmodule. The dual-mode video captioning module captures\nglobal event information and generates descriptive captions,\nwhile the mask generation module generates differentiable\npositive and negative masks for localizing the events. These\nmasks enable the implicit alignment of event locations and\ncaptions by ensuring that captions generated from positively\nand negatively masked videos are complementary, thereby\nforming a complete video description. In this way, even un-\nder weak supervision, the event location and event caption\ncan be aligned implicitly. Extensive experiments on the pub-\nlic datasets demonstrate that our method outperforms existing\nweakly-supervised methods and achieves competitive results\ncompared to fully-supervised methods.", "sections": [{"title": "Introduction", "content": "Dense Video Captioning (DVC) is a challenging task that\naims to generate a series of temporally localized captions\nto describe the various events in a video (Krishna et al.\n2017). It extends traditional video captioning by providing\na more comprehensive understanding of the video content,\nmaking it particularly useful for applications such as video\nunderstanding, video summarization, and video search (Li\net al. 2018; Wang et al. 2021; Yang et al. 2023). Recently,\nthe Weakly-Supervised Dense Video Captioning (WSDVC)"}, {"title": "Related Work", "content": "Dense Video Captioning Dense Video Captioning is a\nchallenging multi-task problem that involves event localiza-\ntion (Buch et al. 2017; Lin et al. 2018; Zeng et al. 2019; Zhao\net al. 2024) and event captioning (Gao et al. 2017; Seo et al.\n2022; Nie et al. 2022). A lot of existing methods follow the\n'detect-then-describe' paradigm, which first localizes a set\nof event proposals and then generates captions for the event\nproposals (Krishna et al. 2017; Wang et al. 2018; Mun et al.\n2019; Iashin and Rahtu 2020). Krishna et al. (2017) first in-"}, {"title": "Proposed Method", "content": "Task Definition\nIn Dense Video Captioning (DVC), a video is represented as\nv = {vi}\\_1, where vi denotes the i-th frame, and N is the\ntotal number of frames. The objective is to generate captions\n{Si} for temporally localized events within the video.\nEach captioning event S\u2081 encompasses a tuple (t, t, Ci),\ndetailing the start time, end time, and the associated caption.\nUnlike the DVC task, the Weakly-Supervised Dense\nVideo Captioning (WSDVC) requires the model to gener-\nate these temporally localized captioning events without re-\nlying on explicit annotations for the start and end times of"}, {"title": "Full Video Captions Generation", "content": "Our proposed DVC module leverages a spatial-temporal\nvideo encoder and a pretrained language model to gener-\nate multiple content-continuous captions for a given video.\nAs shown in Figure 2(a), the process can be divided into\ntwo steps: (1) spatial-temporal video encoding, which cap-\ntures both spatial and temporal information from the video\nframes, and (2) prompt-based caption decoding, which fine-\ntunes the language model using video embeddings to gener-\nate contextually relevant captions.\nSpatial-Temporal Video Encoding To fully capture vi-\nsual information, we design a spatial-temporal encoder with\na frame-level spatial encoder Ea and a video-level temporal\nencoder Eb. Given a video $v \\in \\mathbb{R}^{N\\times 3\\times H\\times W}$, where H and\nW represent the height and width of each video frame,the\nspatial encoder Ea extracts visual embeddings from each\nframe, resulting in frame-level embeddings $v^a \\in \\mathbb{R}^{N\\times d}$.\n$v^{a}=\\left\\{E^{a}\\left(v_{i}\\right)\\right\\}_{i=1}^{N_{v}},$\nwhere d is the dimension of the embeddings. We formal-\nize Ea using the pretrained CLIP ViT-L/14 model (Radford\net al. 2021) and keep the parameters of E\u00ba frozen during\ntraining and testing. Next, the temporal encoder Eb pro-\ncesses these embeddings va to capture temporal informa-\ntion, generating contextualized video embeddings vb:\n$v^{b}=E^{b}\\left(v^{a}+\\Theta_{p}\\right) \\in \\mathbb{R}^{N_{v} \\times d},$\nwhere \u03b8p \u2208 RN\u00d7d are position embeddings and Eb is a\nrandomly initialized Transformer encoder. The output vb en-\ncodes frame characteristics and their temporal relationships,\nessential for generating accurate captions.\nPrompt-Based Caption Decoding Inspired by advance-\nments in multimodal language models (Li et al. 2023; Zhu\net al. 2023; Liu et al. 2024), we adapt a pretrained GPT-\n2 (Radford et al. 2019) to serve as a prompt-based caption\ndecoder. This model processes video embeddings to gener-\nate sequential captions for all events in a video. Given cap-\ntions {$C_i$}\\_1, we concatenate a prompt P\u201c[FULL] Ns\nevents:\" with all captions into a paragraph, tokenized and\nembedded into a sequence r:\n$r=\\left\\{r_{1}, \\ldots, r_{N_{r}}\\right\\}=\\text { tokenizer }\\left(\\left\\{P, C_{1}, \\ldots, C_{N_{s}}\\right\\}\\right),$\nwhere [FULL] signals the model to generate all captions,\n\"Ns events:\" specifies the number of captions, and Nr\nis the number of all tokens. We use contextualized video em-\nbeddings vb as prefix visual tokens, concatenated with cap-\ntion token embeddings:\n$Z=\\left\\{v^{b} ; r_{1}, \\ldots, r_{r}\\right\\}.$\""}, {"title": "Captioning-Guided Event Localization", "content": "We introduce a Captioning-Guided Event Localization\nmethod to ground the generated captions to the correspond-\ning video segments. As shown in Figure 2(b)&(c), this\nmethod consists of two main steps: (1) Mask Prediction for\nEvents, predicting the center and width of each caption to\ncreate a differentiable Gaussian mask representing its tem-\nporal location, and (2) Complementary Masked Captioning,\napplying these masks to video embeddings for training on\npositive and negative masked captioning tasks.\nMask Prediction for Event As shown in Figure 3, the\nmask prediction component consists of the event caption\nprediction and the differentiable mask construction pro-\ncesses. The goal is to generate a differentiable mask for each\ncaption to represent its temporal location within the video\nand encourage diversity among the masks.\nEvent Location Prediction. We first use a randomly initial-\nized learnable embedding as a event embedding ei \u2208 Rd for\neach caption proposal Ci. Then, for all event embeddings\ne\u2208 RNsxd and the frame embeddings va \u2208 RN\u00d7d, we\nuse a Transformer decoder to generate the embeddings h:\n$h=\\text { TransformerDecoder }\\left(e, v^{a}+\\Theta_{1}\\right) \\in \\mathbb{R}^{N_{s} \\times d}.$\nNext, we utilize two linear layers to predict the center \u03bc\u2081 and\nwidth \u03c3\u03b5 of the proposal i:\n$ \\begin{aligned}\\mu_{i} &=\\text { Sigmoid }\\left(F C_{1}\\left(h_{i}\\right)\\right), \\\\\n\\sigma_{i} &=\\text { Sigmoid }\\left(F C_{2}\\left(h_{i}\\right)\\right) .\\end{aligned} $\nDifferentiable Mask Construction. Inspired by previous\nworks (Zheng et al. 2022a,b; Kim et al. 2024), we use \u03bci\nand \u03c3\u03b5 to generate a Gaussian mask Mi \u2208 R for each\nevent, representing its temporal location:\n$M_{i}(t)=\\exp \\left(-\\frac{\\left(t / N_{v}-\\mu_{i}\\right)^{2}}{2\\left(\\sigma_{i} / \\tau\\right)^{2}}\\right), \\forall t \\in\\{1, \\ldots, N_{v}\\},$\nwhere Mi(t) is the mask value at frame vt, and 7 is a hyper-\nparameter that controls the steepness of the Gaussian curve.\nNote that other functions generating soft masks can be used\nas alternatives to the Gaussian mask.\nDiversity Loss. To ensure masks cover different video parts,\nwe introduce a diversity loss based on cosine similarity,\nserving as a regularization term:\n$L_{d i v}=\\frac{1}{N_{s}\\left(N_{s}-1\\right)} \\sum_{i=1}^{N_{s}} \\sum_{j=1, j \\neq i}^{N_{s}} \\max \\left(\\mathbf{s}(i, j)-\\gamma, 0\\right)$,\nwhere $\\mathbf{s}(i, j)=\\frac{M_{i}^{T} M_{j}}{\\left|\\left|M_{i}\\right|\\right|\\left|\\left|M_{j}\\right|\\right|}$ and y is the hyperparameter\nthat controls overlap between masks."}, {"title": "Complementary Masked Captioning", "content": "Since the supervi-\nsion for event localization is unavailable, we propose to\nguide event localization by complementary masked caption-\ning, which mainly consists of two subtasks: positive masked\ncaptioning and negative masked captioning.\nPositive Masked Captioning. In this task, we apply the\nGaussian mask M\u2081 to the video frame embeddings v' in the\ntemporal dimension and input them into the video-level tem-\nporal encoder E6:\n$\\hat{v}^{b}=E^{b}\\left(M_{i} \\cdot v^{a}\\right).$\nWe then concatenate a prompt sentence P\u201c[MASK] 1\nevent:", "sequence\nri": "n$\\hat{r}_{i}=\\left\\{\\hat{r}_{i, 1}, \\ldots, \\hat{r}_{i, N_{r}}\\right\\}=\\text { tokenizer }\\left(\\left\\{P, C_{i}\\right\\}\\right),$\nwhere [MASK] is a special prompt token that indicates the\nmodel to generate part of the captions.\nFinally, the optimization objective for Positive Masked\nCaptioning is defined as:\n$L\\left(\\hat{v}^{b}, \\hat{r}\\right)=\\frac{1}{N_{s}} \\sum_{n=1}^{N_{s}} \\sum_{i=2}^{N_{\\hat{n}}} \\log p\\left(\\hat{r}_{n, i} \\mid \\hat{r}_{n, 1}, \\ldots, \\hat{r}_{n, i-1} ;\\right.\\left.\\Theta_{E, G, T}\\right),$\nwhere N is the total number of the tokens and Tn,i denotes\nthe i-th token of the tokenized caption Cn and OE,G,T de-\nnotes the parameters of the video encoder, the caption de-\ncoder and the event localization model, respectively.\nNegative Masked Captioning. This task further explores the\nalignment between captions and video frames by predicting\nthe remaining captions {$C_j$}\\_1,j\u2260i using the inverse mask\non video embeddings. Specifically, we compute the negative\nGaussian mask M\u2081 and inverse masked video embedding v\nas:\n$\\begin{aligned}\\widetilde{M}_{i} &=1-M_{i}, \\\\\n\\widetilde{v}^{b} &=E^{b}\\left(\\widetilde{M}_{i} \\cdot v^{a}\\right) .\\end{aligned}$"}]}