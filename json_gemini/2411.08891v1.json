{"title": "CALIBRATED DECISION-MAKING THROUGH LARGE LANGUAGE MODEL-ASSISTED RETRIEVAL", "authors": ["Chaeyun Jang", "Hyungi Lee", "Seanie Lee", "Juho Lee"], "abstract": "Recently, large language models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead humans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of generated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, traditional RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user's decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by the retrieved documents are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs; Jiang et al., 2023; Touvron et al., 2023; Dubey et al., 2024; Achiam et al., 2023) have demonstrated remarkable performance on numerous downstream natural language processing (NLP) tasks, leading to their widespread integration into various decision-making processes (Bommasani et al., 2021; Band et al., 2024; Zhou et al., 2024). However, even with significant increases in model size and the expansion of training datasets, it remains infeasible for LLMs to encode all possible knowledge within their parameters. As a result, the outputs produced by LLMs may not consistently be reliable for important human decision-making processes, potentially overlooking key or hidden details. Additionally, LLMs frequently provide inaccurate or misleading information with a high degree of confidence, a phenomenon referred to as hallucination (Zhuo et al., 2023; Papamarkou et al., 2024), which can lead humans to make flawed decisions. In addition, Zhou et al. (2024) have empirically demonstrated that human users often over-rely on LLM outputs during decision-making processes, and this over-reliance tends to increase in proportion to the model's confidence. Here, the model's confidence refers to the verbalized expression of how certain the model is when asked how confident it is in its answer. Specifically, they have found that for answers with high confidence, users show strong over-reliance regardless of whether the answer is correct or not. These findings highlight that utilizing LLMs without proper calibration of their responses and addressing the frequent occurrence of hallucinations can lead to incorrect decisions in high-stakes tasks like medical diagnosis and legal reasoning, potentially resulting in severe consequences (Li et al., 2019; 2022b; Han et al., 2024).\nRetrieval Augmented Generation (RAG) (Lewis et al., 2020; Li et al., 2022a; Wang et al., 2024) has emerged as a promising method to address hallucinations, which is one of the two key issues when using LLMs in decision-making (Shuster et al., 2021; Li et al., 2024). Instead of generating answers directly, RAG retrieves relevant documents from external databases and uses them as an additional context for response generation. This approach supplements the information that LLMs lack, resulting in more accurate and reliable responses. However, the database cannot encompass all information, and the world knowledge is continuously being updated. In such cases, the retriever may retrieve irrelevant documents, which can distract the LLM and lead to the generation of incorrect answers to the question (Shi et al., 2023). Moreover, as described in Section 2.2, due to"}, {"title": "PRELIMINARIES", "content": ""}, {"title": "2.1 DECISION CALIBRATION OF LONG FORM GENERATION", "content": "As discussed in Section 1, since human decision-makers tend to over-rely on the outputs of LLMs during the decision-making process, it is crucial to ensure that the confidence in LLMs' outputs is well-calibrated. To address this problem, Band et al. (2024) propose decision calibration, which aims to align the confidence of the model's predicted output with the accuracy of the user's decision based on the model output. This allows the user to make a reliable decision based on the model's confidence. Therefore, to achieve this goal, we need to ensure that the model not only generates factual information but also that its confidence in the generated responses accurately reflects the likelihood of correctness.\nTo formalize the problem, we introduce the following notations. Let x \u2208 X represent the question or task for which a user needs to make a decision (e.g., \"What was the name of the 1996 loose adaptation of William Shakespeare's Romeo & Juliet written by James Gunn?\"), and let y \u2208 Y denote the corresponding true answer (e.g., \u201cTromeo and Juliet\"). Here, X and Y are the set of all possible questions and answers, respectively. Given the question x, the user provides an open-ended"}, {"title": "2.2 RETRIEVAL AUGMENTED GENERATION (RAG)", "content": "Retrieval augmented generation (RAG) is first proposed by Lewis et al. (2020) and uses dense passage retrieval (DPR; Karpukhin et al., 2020) to retrieve and rank relevant paragraphs in question-answering (QA) tasks. The bi-encoder structure of the DPR model embeds questions and documents separately, enabling to precompute document embeddings and cache them in a vector database. A question is only embedded when presented and the similarity between the question and document embeddings is computed. The most relevant documents are retrieved and provided as additional context for the question to an LLM. The retrieved documents can guide the LLM to generate more reliable answers, rahther than solely relying on the knowledge encoded in its parameters.\nAlthough RAG improves accuracy, retrieval models can still produce errors. First, since retrieval models are typically trained in an unsupervised manner (Izacard et al., 2021; Jin et al., 2023), the order of query-document similarities they produce does not necessarily align with how helpful those documents are for downstream user decisions. As shown in Figure 1a, the top-1 document retrieved"}, {"title": "3 CALIBRAG: RAG FOR DECISION CALIBRATION", "content": ""}, {"title": "Overview.", "content": "We summarize our method and describe it in more detail in the following section. Given a task x on which users make a decision and an open-ended query q about the task, a retriever model gets a document d relevant to the query from an external database. Based on the query and retrieved document, an LLM generates a guidance z in the form of long-form generation that can help the user make an informed decision and outputs confidence c for its response. To allow the LLM to express its uncertainty, we prompt the model to respond using either an integer number between 0 and 10 or linguistic terms of certainty (e.g., \"Ambiguous\"). Finally, the user makes a final decision about the task, using both the guidance response z and the LLM's confidence c. Our goal is to align the model confidence with accuracy of the user's decision based on the guidance.\nTo this end, we train a forecasting function f(q, d) that gets the query and retrieved document as input predicts the probability of the decision being correct, and uses it as a ranking function of the retriever model. The overall pipeline of our method is illustrated in Figure 2."}, {"title": "3.1 PROBLEM SETUP", "content": "Following Band et al. (2024), to train and evaluate the forecasting function, we first use an LLM surrogate model, denoted as U, to mimic human decision-making when making decisions, instead of relying on actual human users. However, unlike Band et al. (2024), we leverage the human evaluation results from Zhou et al. (2024) to design a prompt that steers the surrogate LLM model U to exhibit more human-like behavior. Specifically, the prompt is crafted to lead the surrogate U to place strong belief in the confidence of the LLM, denoted as M, generating the guidance in its responses. For further details on the prompt, please refer to Appendix E. Additionally, since the user's decision (mimicked by U) is usually given as a free-form text rather than being a simple class label, we use GPT-40-mini (Achiam et al., 2023) model, denoted as G, to evaluate the correctness of the user's decision compared to the true answer y."}, {"title": "3.2 MODELING AND TRAINING", "content": "To model the forecasting function f, it is essential to have the capacity to sufficiently analyze the relationship between the query q and the retrieved document d. For this reason, we use a pre-trained LLM encoder ffeat as the base feature extractor model. Additionally, to model the probability of whether U(x, z, f(q, d)) is correct or not, we attach a linear classifier head after ffeat. This head uses a sigmoid function on the logits to generate the probability values. For efficient learning during supervised fine-tuning, we keep the weights of the pre-trained ffeat fixed and employ Low-Rank Adaptation (Lora; Hu et al., 2021) to train the feature extractor. This allows us to adapt the model efficiently with minimal additional parameters. Then our overall forecasting function f is formulated as follows:\nPr(G(y, Uf) = 1) = f(q, d) := sigmoid (Whead ffeat(concat[q, d]; WLORA) + bhead)                                                                                             (3)\nwhere sigmoid and concat denote the sigmoid function x \u2192 1/(1 + exp(-x)) and the concatenate operation, respectively. Whead, bhead, and the LoRA weight WLORA are learnable parameters, and Uf is the shorthand for U(x, z, f(q, d)). Here, the reason f can model p(G(y,Uf) = 1) using only the query q and document d is that q and d depend on x, and z also depends on both q and d. This enables f to acquire enough information from the query and the retrieved document to forecast the distribution of correctness of the decision y. To train our forecasting function f, we employ a synthetic dataset whose construction will be described in the next section. The model is trained with the following binary cross-entropy loss,\nL = 1/T \u2211 (blog f(q, d) + (1 \u2212 b) log(1 \u2212 f(q, d)))                                                                  (4)\n(q,d,b) ET\nwhere T represents the synthetic training dataset, and b \u2208 {0, 1} is a binary label indicating the correctness of the user's decision. Here, through supervised learning using various combinations of q, d, and b, the trained function f can analyze the relationship between unseen combinations of q and d using the learned feature map, enabling it to predict the probability of the decision."}, {"title": "3.3 SYNTHETIC SUPERVISION DATA GENERATION", "content": "To conduct the supervised learning discussed in Section 3.2, it is essential to construct an appropriate synthetic training dataset T consisting of the triples (q, d, b). We first extract the (x, y) (e.g., (\u201cIn which county is Ascot\u201d, \u201cBerkshine, England\u201d)) decision-making task pairs from the following three Question Answering datasets: 1) TriviaQA (Joshi et al., 2017), 2) SQUAD2.0 (Rajpurkar et al., 2018), and 3) WikiQA (Yang et al., 2015) datasets. Then, for every x in the training dataset, we generate an open-ended query q (e.g., \u201cWrite a paragraph about the county where Ascot is located.\") based on each x, using the GPT-40-mini model. At this point, it is important to note that instead\""}, {"title": "3.4 INFERENCE", "content": "After finishing the training of the forecasting function f, we perform inference for a new decision task x* through the following four stage process:\nStage 1: Initial retrieval of documents. Given an open-ended query q*, derived from the original question x*, we begin the document retrieval process using the retrieval model. Similar to the training data generation process, we retrieve the top K relevant documents from the external database, denoted as D* := {d}K1. The goal of this stage is to construct a diverse set of candidate documents that may contain valuable information for producing the correct answer y.\nStage 2: Scoring and selection of documents. Once the K candidate documents are retrieved, we predict the decision confidence level for each document using our trained forecasting function f. At this point, regardless of the similarity score from the retrieval model, each document is assigned a new rank based on its confidence level predicted with f. Specifically, the ranking is determined based on the probability that the user will make a correct decision when provided with the guidance generated from each document, with documents arranged in descending order of the forecasted probabilities {f(q*,d)}=1. The document with the highest ranking is selected for the next stage. Here, if the predicted probability for the highest-ranked document d* is lower than a pre-defined threshold e, we determine that none of the currently retrieved K documents are useful for assisting with the decision task x*. Consequently, in this case, we proceed to Stage 3 to retrieve a new set of K candidate documents. If this condition is not met, we move forward to Stage 4.\nStage 3: Reformulating the query. If the predicted probability for the highest-ranked document d* is lower than a pre-defined threshold e in Stage 2, to retrieve a new set of K candidate documents, we reformulate our open-ended query q* into q** by emphasizing more important content from the question x. This reformulation focuses on extracting key aspects of the original task, ensuring that the next retrieval attempt targets more relevant and helpful documents. After reformulating the query, we repeat Stage 1 and Stage 2 once again. Stage 3 is only performed once for each task x* to prevent excessive repetition, ensuring the process remains efficient.\nStage 4: Final decision. After retrieving the document d*, we generate the guidance z* using the RAG model M. The user model U then makes a decision U(x*, z*, f(q*, d*)). This decision is compared with the correct answer y* by G to determine its accuracy."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 SETUP", "content": "Implementation detail. For all experiments, following Section 3.3, we collect a total of 20,870 samples for training and 4,125 for validation. We employ the Llama-3.1-8B (Dubey et al., 2024) model as both the RAG model M and decision model U. For evaluating the long-form generated answers, we utilize the GPT-40-mini API as an evaluation model G. Additionally, we used Contriever-msmarco (Izacard et al., 2021) as the base retrieval model. In all tables, the"}, {"title": "4.2 MAIN RESULTS", "content": "Comparison with uncertainty calibration baselines. Table 1 presents a comparison of uncertainty-based baselines across four QA datasets. Our CalibRAG achieves both a lower 'No Answer' rate and higher accuracy compared to other baselines, achieving the accuracy of 35.03 and 39.91 on BioASQ and HotpotQA, respectively, representing over a 3% improvement over the best-performing baseline. Additionally, its confidence level is better calibrated than the other baselines, demonstrating the lowest ECE and BS. CalibRAG\u2020, which regenerates the query for documents that do not exceed the threshold, consistently shows performance improvements. However, while it correctly answers more challenging questions, it also makes accurate decisions with lower confidence, causing some variation in the calibration metrics.\nComparison with reranking baselines. For a fair comparison with the reranking baselines, we assume a scenario where the surrogate user U makes decisions using only the question x* and the guidance z* without leveraging the confidence prediction c*, i.e., U(x*, z*). In the case of CalibRAG, although the confidence predicted by the forecasting function f is not provided to the user, the reranking is based on f's prediction. This means that, unlike the other baselines, CalibRAG takes the confidence of f into account for reranking. Table 2 highlights the reranking capability of CalibRAG, achieving an average accuracy improvement of 5.19% over the reranking with cross-encoder. Notably, CalibRAG\u2020 once again results in further performance improvement, similar to the previous experiment. In contrast, the LLM-rerank method even underperforms HotpotQA and NQ compared to the cross-encoder baseline due to cases where the LLM either refuses to answer or generates incorrect tokens. These findings demonstrate the superior performance of CalibRAG in reranking for RAG."}, {"title": "4.3 ABLATION STUDIES", "content": "In this section, we provide ablation studies to demonstrate the performance of CalibRAG."}, {"title": "Does an LLM approximate human decision making?", "content": "Since it is impractical to directly hire human annotators to generate and evaluate large amounts of data, as mentioned earlier, we follow the setup of Zhou et al. (2024) by crafting prompts to encourage the LLM to mimic human decision behaviors. As illustrated in Fig. 3a, we ask each of 10 human annotators to answer 10 questions based on the guidance z as well as the confidence level e corresponding to specific confidence bins. The agreement rate exceeded 50% in all confidence bins, achieving an average agreement rate of 81.33%. This indicates that our LLM serves as an effective surrogate through prompting, which is consistent with the results reported by Zhou et al. (2024)."}, {"title": "Does CalibRAG generalize to utilize unseen RAG models?", "content": "The way CalibRAG constructs the synthetic dataset T, used for training the forecasting function, depends on the RAG model M, which is responsible for generating the guidance z. In this experiment, we study how well our CalibRAG can generalize to utilize an unseen LLM as the RAG model for decision-making task. We use Mistral-7B for the RAG model and plot its performance improvement over the best-performing baseline. As shown in Fig. 3b, our CalibRAG with Mistral-7B still improves the accuracy and ECE, indicating the effectiveness of CalibRAG with the unseen RAG model. Compared to Llama-3.1-8B, it slightly underperforms due to inherent performance disparities between the two models."}, {"title": "What is the effect of directly using retrieved documents for prediction?", "content": "In this experiment, we study the effectiveness of utilizing the guidance generated by the RAG model M. To this end, instead of generating the guidance z* with respect to the query q*, we directly provide the retrieved document d* to the surrogate user U for prediction of the task x*, i.e., U(x*, d*, f(q*, d*)) instead of U(x*, z*, f(q*, d*)), and evaluate its performance. As illustrated in Fig. 3b, prediction without generating the guidance z*, denoted as \"w/o Generation\", significantly degrades both accuracy and ECE. This degradation is attributed to irrelevant parts of the retrieved document that distract the surrogate user U, leading to an incorrect decision (Shi et al., 2023)."}, {"title": "How does the number of retrieved passages (K) impact reranking?", "content": "We use K = 20 documents for reranking in all the experiments, considering the trade-off between its computational cost and the performance of the decision-making task. To validate our choice, we plot accuracy as a function of the number of documents for reranking in Fig. 3c. The results show that performance improves up to 20 documents, but the gains diminished beyond 40 documents, supporting our choice of 20 documents. This indicates that the retrieval model gets most of the relevant documents in the initial stage, and a more advanced reranking would be necessary for further improvement."}, {"title": "4.4 QUALITATIVE RESULTS", "content": "While quantitative metrics alone may not fully capture all the benefits of CalibRAG, we present examples highlighting its ability to identify relevant documents and assign calibrated confidence scores. Given the query \"Write a paragraph about the kind of bug that uses the American Sweetgum as a host plant.\u201d, the base retriever focuses only on the keyword \u201cAmerican Sweetgum,\", retrieving loosely relevant content and marking its confidence as \u2018Confident' (10/11) as illustrated in Fig. 4. This led to the incorrect conclusion that the sweetgum is the host plant of Parcoblatta divisa, the southern wood cockroach. In contrast, CalibRAG captures the full context, retrieving documents specifically about the gypsy moth, which uses the sweetgum as a host plant, and correctly assigns a confidence level of 81.41. This demonstrates the capability of CalibRAG to find a relevant document and assign a confidence level correlated with the accuracy of the downstream surrogate user. Additional examples can be found in Appendix C.\""}, {"title": "5 RELATED WORKS", "content": ""}, {"title": "5.1 UNCERTAINTY CALIBRATION IN LANGUAGE MODELS", "content": "Traditional calibration techniques primarily rely on token-level log probabilities (Guo et al., 2017). However, many modern LLMs are autoregressive, allowing the generation of token sequences through the chain rule of probability by multiplying the conditional probabilities of each token (Achiam et al., 2023). To estimate the concept-level probability within such generated sen-"}, {"title": "5.2 RERANKING FOR RETRIEVAL AUGMENTED GENERATION", "content": "RAG leverages external knowledge to produce accurate answers in Open-Domain QA. However, not all documents retrieved by the retrieval model hold the same importance, and many contain noise, making reranking essential to select the most relevant documents (Glass et al., 2022). LLM-based reranking is an effective approach as it captures complex semantic relationships between documents and queries to reorder the retrieved documents appropriately (Sun et al., 2023). Another prominent reranking method uses cross-encoders, which take both the question and document as input, considering their interactions to perform more precise reranking (Li et al., 2022c). These diverse reranking approaches help RAG systems minimize noise from retrievers and select the most pertinent information to generate optimal answers."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced CalibRAG, a simple yet effective framework designed to improve confidence calibration and ensure more reliable document retrieval. Our experiments demonstrated that CalibRAG significantly enhances QA performance within the RAG setting across various benchmark datasets. Moreover, ablation studies showed that CalibRAG effectively aligns model confidence with factual correctness, resulting in improved decision-making accuracy and calibration. Overall, CalibRAG stood out as a robust solution for enhancing the reliability of RAG-based LLM guidance in decision-driven scenarios. However, creating synthetic datasets and training the forecasting function for decision calibration may introduce some overhead. Nonetheless, accurately calibrating language model confidence is crucial, making this approach both valid and worthwhile."}, {"title": "Reproducibility statement.", "content": "We present the overall dataset generation and training procedure in Fig. 2. Additionally, we further present all the details regarding experimental environments, datasets, hyperparameters, and evaluation metrics in Appendix A."}, {"title": "Ethics statement.", "content": "In this paper, we proposed a method that enables well-calibrated decision-making based on the guidance provided by RAG. During the synthetic data generation process, we did not create or use datasets containing personal or sensitive information; instead, we processed existing publicly accessible document datasets to create new datasets, thus avoiding ethical issues. On the other hand, as various human users increasingly utilize LLMs in different aspects of daily life, the trustworthiness of LLM outputs is becoming increasingly important. We specifically enhanced the model's guidance by providing additional confidence in situations where users rely on LLMs for decision-making. This approach helps users trust the accuracy of the guidance, thereby offering a positive societal impact by increasing users' confidence in LLMs."}, {"title": "A EXPERIMENTAL DETAILS", "content": "Our implementation builds on key libraries such as PyTorch 2.1.2 (Paszke et al., 2019), Huggingface Transformers 4.45.1 (Wolf et al., 2019), and PEFT 0.7.11, providing a robust foundation for experimentation. We employ the Llama-3.1-8B-Instruct model, a state-of-the-art open-source multilingual LLM available from Hugging Face models.2 Our experiments are executed on high-performance NVIDIA RTX 3090 and RTX A6000 GPUs, ensuring efficient and scalable model training. Additionally, we utilize the official facebookresearch-contriever repository for our retrieval model3. For training baselines, we reference the calibration-tuning repository.4"}, {"title": "A.1 DATATSETS", "content": "Train Datasets SQUAD2.0 (Rajpurkar et al., 2018) is a reading comprehension dataset sourced from Wikipedia, containing questions answered by text spans from the articles, including some unanswerable ones. WikiQA (Yang et al., 2015) is a question-sentence pair dataset from Wikipedia, designed for open-domain question answering and includes unanswerable questions for research on answer triggering. TriviaQA (Joshi et al., 2017) is a reading comprehension dataset with questions authored by trivia enthusiasts, paired with evidence documents from Wikipedia and other web sources. We randomly sampled 10,000 data points each from TriviaQA and SQuAD, and collected all 873 training samples from WikiQA, resulting in a total of 20,873 training data. For the validation set, we gathered 2,000 samples each from TriviaQA and SQUAD, along with 126 samples from WikiQA, resulting in 4,126 validation data points. After removing null values, we compiled 20,870 training and 4,125 validation data. For CalibRAG, we retrieved the top 20 documents for each query from a set of 21,015,300 Wikipedia articles. we downloaded all these datasets in Hugging Face datasets.5 For construction of labeled dataset T used to train the forecasting function of CalibRAG, we collect positive and negative documents for each query as follows. If the first correct document is ranked at position k, the top k - 1 documents are labeled as negative and the correct document is labeled positive. Each k documents are paired with corresponding query and added to the dataset T. If we find the correct document ranked at position 1, only the correct document is added to the dataset. This process resulted in a total of 27,220 training data points and 6,271 validation data points.\nEvaluation Datasets For zero-shot evaluation, we employ several datasets covering diverse domains and question types. BioASQ (Krithara et al., 2023) is a biomedical QA dataset containing factoids, lists, and yes/no questions derived from PubMed articles. HotpotQA (Yang et al., 2018) is a multi-hop question-answering dataset requiring reasoning across multiple supporting documents from Wikipedia to find answers, emphasizing a more complex retrieval and reasoning process. WebQA (Chang et al., 2022) is an open-domain question-answering dataset consisting of natural, conversational questions paired with web documents, targeting real-world, context-rich scenarios. Natural Questions (NQ) (Kwiatkowski et al., 2019) is another large-scale question-answering dataset, designed to answer questions based on Wikipedia articles, containing both long-form and short-form answers. These datasets are used without additional training, providing a robust evaluation of the generalization capabilities of CalibRAG across different domains and question types."}, {"title": "A.2 HYPERPARAMETERS", "content": "Table 3 outlines the hyperparameters used for training the base model and LoRA, including key parameters such as learning rate, batch size, and LoRA-specific settings like rank and alpha."}, {"title": "A.3 EVALUATION METRICS", "content": "To evaluate long-form text, we utilized gpt-40-mini to compare the ground-truth answers with the predicted answers in all cases. Based on this comparison, we labeled each instance as correct or incorrect accordingly."}, {"title": "A.3.1 CALIBRATION METRICS", "content": "\u2022 Expected Calibration Error (ECE; Naeini et al., 2015):\nECE = \u2211Mm=1 |Bm|/M |acc(Bm) - conf(Bm)|\nwhere Bm is the set of predictions in bin m, acc(Bm) is the accuracy, and conf(Bm) is the average confidence of predictions in that bin. ECE measures how well the model's predicted probabilities are calibrated.\n\u2022 Brier Score (BS; Brier, 1950):\nBS = 1/N\u2211Ni=1(fi-Yi)2\nwhere fi is the predicted probability and yi is the true label. BS combines both the accuracy and confidence of predictions, penalizing overconfident and underconfident predictions."}, {"title": "A.3.2 HUMAN EVALUATION", "content": "We recruited 10 participants to answer 10 questions from each confidence bin, with the survey formatted as shown in Fig. 5. The survey was conducted anonymously, ensuring that no ethical concerns were raised during the process.\nAdditional findings of human evaluations. In Fig. 3a, the 0-20 confidence bin exhibits the lowest agreement between human and user models. Our qualitative analysis revealed that, for the question, \"Rex Riot is known for a remix of the Kanye West song from which album?\", the model generated the answer, \"All of the Lights by Kanye West.\" with a confidence score of only 0.09. Despite this low confidence, participants trusted the model's output due to the retrieval-augmented guidance that made the response sound convincing. This suggests that \u201cplausible-sounding LLMs\u201d with retrieval-based support can significantly influence people, even when their numerical confidence is low. We leave further exploration of this phenomenon to future research.\""}, {"title": "B ADDTIONAL EXPERIMENTS", "content": "Table 4, Table 5, Table 6, and Table 7 present the complete results from the primary experiments. For the Base model, we utilized a pretrained model, sampling sentences across three different seeds. For the other methods, training was conducted across three random seeds to ensure robust evaluation.\nTable 8 and Table 9 present results demonstrating how the existing baselines perform without the application of RAG. It can be observed that RAG generally increases accuracy while also leading"}, {"title": "E PROMPT EXAMPLES", "content": "f\"\"\"The task is to answer questions based on a context generated by a\nlanguage model in response to a question about relevant information,\nalong with the model's confidence level in the provided answer.\nFollow the behavior instructions below:\n1. Overconfidence: You feel that the answers provided are very accurate.\nTrust the provided answers with confidence.\n2. Avoidance of Uncertainty: If any uncertainty is expressed in the\nanswer, do not trust the answer and seek other information.\n3. Consistent Trust: If the provided answer shows clear confidence or\nlacks any explicit uncertainty, accept the answer as it is.\n4. Trust in Incorrect Information: You believe that answers provided with\nconfidence are correct, even if they are actually incorrect.\nIf you cannot provide an answer or if you do not trust the information,\nrespond with 'no answer'.\nAdditionally, consider the language model's confidence level when\nassessing the answer.\nContext: {context}\nQuestion: {question}\nModel Confidence: {confidence}\nAnswer:\"\"\"\nTable 1: Human decision prompt. This prompt is crafted to force the user model to imitate according to the human evaluation results of Zhou et al. (2024). This prompt encourages the user model U to over-rely on the guidance provided by the LLM.\nIn this section, we present prompt examples used during training and inference. Table 1 shows the prompt that encourages the user model U to act like a human decision-maker, leading it to over-rely on the guidance provided by the LLM. Table 2 displays the prompt that generates the open-ended query q from the decision task x. Table 3 presents the prompt that induces the generation of guidance z from M based on the retrieved document d. Table 4 is used when grading the user model U's decision against the true answer using G. Table 5, Table 6, and Table 7 are prompts used to instruct M to generate confidence in terms of linguistic or numerical calibration. Lastly, Table 8 is the prompt used during Stage 3 of the inference process."}, {"title": null, "content": "f\"\"\"You are an automated assistant tasked with rephrasing specific\nquestions into open-ended queries to encourage detailed exploration\nand discussion of the key topics mentioned.\nYour goal is to prompt someone to write a paragraph exploring the topic\nwithout directly revealing the answer.\nYou will be given an original question, labeled as 'Question 1.' Your\ntask is to rephrase this into a new question, labeled as 'Question\n2.' This new question should encourage someone to provide a\ncomprehensive exploration of the key topic from the original question\n.\nExamples for Guidance:\nExample 1:\nQuestion 1: Which sea creature is the world' largest invertebrate?\nQuestion 2: Write a paragraph about the world's largest invertebrate.\nExample 2:\nExample 3:\nQuestion 1: In which century was the printing press established in\nBritain?\nQuestion 2: Write a paragraph about the century in which the printing\npress was established in Britain.\nExample 4:\nQuestion 1: What type of creature is Chewbacca?\nQuestion 2: Write a paragraph about the type of creature that Chewbacca\nis.\nNow, please rephrase the following question:\nQuestion 1: {question}\nQuestion 2:\"\"\"\nTable 2: Prompt that generates open-ended query q from the decision task x. This prompt was first suggested by Band et al. (2024), and we have modified part of the proposed prompt for our use here. We use this prompt as an input when generating the query q based on the decision task x."}, {"title": null, "content": "f\"\"\"You are an expert who responds with concise", "the correct answer is\n.": "nIf you cannot provide an answer or if you do not trust the information", "no answer.": "nGiven the provided context", "context.\nContext": {"title};{context}\nQuestion": {"query}\nAnswer": "\"\"\nTable 3: Guidance z generation prompt. This prompt guides the language model to provide direct, concise guidance z based on a given retrieved document d, avoiding unnecessary phrases. It emphasizes providing an"}}}]}