{"title": "DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations", "authors": ["Anis Bourou", "Saranga Kingkor Mahanta", "Thomas Boyer", "Val\u00e9rie Mezger", "Auguste Genovesio"], "abstract": "In recent years, deep learning models have been extensively applied to biological data across various modalities. Discriminative deep learning models have excelled at classifying images into categories (e.g., healthy versus diseased, treated versus untreated). However, these models are often perceived as black boxes due to their complexity and lack of interpretability, limiting their application in real-world biological contexts. In biological research, explainability is essential: understanding classifier decisions and identifying subtle differences between conditions are critical for elucidating the effects of treatments, disease progression, and biological processes. To address this challenge, we propose DiffEx, a method for generating visually interpretable attributes to explain classifiers and identify microscopic cellular variations between different conditions. We demonstrate the effectiveness of DiffEx in explaining classifiers trained on natural and biological images. Furthermore, we use DiffEx to uncover phenotypic differences within microscopy datasets. By offering insights into cellular variations through classifier explanations, DiffEx has the potential to advance the understanding of diseases and aid drug discovery by identifying novel biomarkers.", "sections": [{"title": "1. Introduction", "content": "Image classification is a fundamental task in deep learning that has achieved remarkable results (Li et al., 2020; He et al., 2016; Huang et al., 2017; Dosovitskiy et al., 2020; Liu et al., 2022). The success of classifiers is primarily due to their ability to extract patterns and features from images to distinguish between classes. However, these patterns can often be difficult to discern (Li et al., 2020; Zeiler & Fer-"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Classifiers Explainability", "content": "Class Activation Maps (CAMs) (Selvaraju et al., 2017; Chattopadhay et al., 2018) are a well-known technique for explaining classifier decisions, as they highlight the most influential regions in an image that affect the classifier's output. However, these methods typically require access to the classifier's architecture and all its layers, as they involve computing gradients of the outputs with respect to the inputs. Additionally, CAMs only indicate important regions in images without explicitly identifying the affected attributes, such as shape, color, or size. This can be limiting, particularly in microscopy images where subtle variations are of interest. Counterfactual visual explanations represent another family of methods aimed at explaining classifier decisions. These methods seek to identify minimal changes that would alter the classifier's decision. Generative models have been widely used to generate such counterfactual explanations. Generative Adversarial Networks (GANs), for instance, have been employed for this purpose (Singla et al., 2020; Lang et al., 2021a; Goetschalckx et al., 2019). While some approaches generate counterfactual explanations all at once (Singla et al., 2020; Goetschalckx et al., 2019), the work in (Lang et al., 2021a) identifies a set of attributes that influence the classifier's decision. However, GANs suffer from training instability due to the simultaneous optimization of two networks: the generator and the discriminator. Recently, diffusion models have demonstrated more stable training, superior generation quality, and greater diversity (Dhariwal & Nichol, 2021; Nichol & Dhariwal, 2021). They have also been adopted for generating visual counterfactual explanations (Augustin et al., 2022; Jeanneret et al., 2024; Sobieski & Biecek, 2024)."}, {"title": "2.2. Diffusion Models", "content": "Generative models have recently achieved significant success in various tasks (Goodfellow et al., 2014; Song & Ermon, 2020; Dhariwal & Nichol, 2021; Kingma & Welling, 2014). Diffusion models (Ho et al., 2020; Song et al., 2022; Dhariwal & Nichol, 2021; Nichol & Dhariwal, 2021), a class of generative models, have been applied to different domain (Dhariwal & Nichol, 2021; Guo et al., 2023; Rombach et al., 2022a). These models consist of two processes: a known forward process that gradually adds noise to the input data, and a learned backward process that iteratively denoises the noised input. Numerous works have proposed improvements to diffusion models (Dhariwal & Nichol, 2021; Rombach et al., 2022a; Nichol & Dhariwal, 2021), enhancing their performance and making them the new state-of-the-art in generative modeling across different tasks. Recently, it has been shown that diffusion models can be used to learn meaningful representations of images that facilitate image editing tasks (Preechakul et al., 2022; Kwon et al., 2023). In (Preechakul et al., 2022), the authors proposed adding an encoder network during the training of diffusion models to learn a semantic representation of the image space. This approach enables the model to capture high-level features that can be manipulated for various applications. In (Kwon et al., 2023), the authors modified the reverse process-introducing an asymmetric reverse process to discover semantic latent directions in the space induced by the bottleneck of the U-Net (Ronneberger et al., 2015) used as a denoiser in the diffusion model, which they refer to as the h-space. By exploring this space, they were able to identify directions corresponding to specific semantic attributes, allowing for targeted image modifications. These advancements demonstrate the potential of diffusion models not only for high-quality data generation but also for learning rich representations that can be leveraged for downstream tasks."}, {"title": "2.3. Detecting phenotypes in microscopy images", "content": "Capturing the visual cellular differences in microscopy images under varying conditions is essential for understanding certain diseases and the effects of treatments (Moshkov et al., 2022; Chandrasekaran et al., 2021; Lotfollahi et al., 2023; Bourou & Genovesio, 2023; et al., 2022; Bourou et al., 2024). Historically, hand-crafted methods were employed to measure changes between different conditions (et al, 2006). However, these tools have limitations, especially when the observed changes are subtle or masked by biological variability (et al., 2022; Bourou et al., 2024). Recently, generative models have been proposed to alleviate these limitations. In (Bourou & Genovesio, 2023), CycleGAN (Zhu et al., 2020) was used to perform image-to-image translations, aiming to discard biological variability and retain only the induced changes. By translating images from one condition to another, the model focused on the specific alterations caused by the experimental conditions, effectively highlighting phenotypic differences. In (et al., 2022), a conditional StyleGAN2 (Karras et al., 2020) was trained to identify phenotypes by interpolating between classes in the StyleGAN's latent space. This approach enabled the generation of high-fidelity images that represent different phenotypic expressions, facilitating the study of subtle cellular variations and providing insights into the underlying biological processes. Furthermore, recent advancements have seen the use of conditional diffusion models in image-to-image translation (Bourou et al., 2024). In this method, an image from the source condition is first inverted into a latent code, that is used to generate corresponding the image from the target condition. This technique leverages the strengths of diffusion models in capturing complex data distributions and performing realistic translations between conditions. All of these methods have proven effective in uncovering phenotypes and enhancing the understanding of cellular differences. However, they rely solely on generative models and do not integrate classifiers that can extract patterns from images and assess how a given image would be transferred to another class. Incorporating discriminative models alongside generative approaches could enhance pattern recognition and provide a more comprehensive analysis of cellular changes, ultimately improving the assessment of disease progression and treatment effects."}, {"title": "2.4. Contrastive learning", "content": "Contrastive learning is a powerful self-supervised framework that has achieved remarkable success across various domains, including computer vision and natural language processing (Chen et al., 2020; Radford et al., 2021; Gao et al., 2021; Fang et al., 2020). By contrasting positive and negative pairs, it learns rich feature representations, maximizing similarity for positive pairs while minimizing it for negative ones using a contrastive loss (Chen et al., 2020; van den Oord et al., 2019; Schroff et al., 2015; Wang & Liu, 2021). This versatile approach has been integrated into diverse architectures, enabling the extraction of robust and generalizable features for a wide range of downstream tasks. Beyond traditional applications, contrastive learning has also been leveraged in generative modeling. It has been employed to enhance conditioning in GANs (Kang & Park, 2020) and to improve style transfer in diffusion models (Yang et al., 2023). Discovering interpretable directions in generative models is fundamental to various image generation and editing tasks (Y\u00fcksel et al., 2021; Dalva & Yanardag, 2024; Kwon et al., 2023). In this context, contrastive learning has proven highly effective. For instance, LatentCLR (Y\u00fcksel et al., 2021) identifies meaningful transformations by applying contrastive learning to the latent space of GANs, while NoiseCLR (Dalva & Yanardag, 2024) uncovers semantic directions in pre-trained text-to-image diffusion models like Stable Diffusion (Rombach et al., 2022b)."}, {"title": "3. Method", "content": "In this section, we introduce DiffEx, a method designed to explain a classifier by generating separable and interpretable attributes. As illustrated in Fig 1, our method leverages diffusion models to provide insights into the classifier's behavior. First, we construct a latent semantic space that is aware of the classifier specific attributes. Then, using a contrastive learning approach, we identify separable and interpretable directions within this space. Finally, we rank the importance of the discovered directions and modify the image accordingly to highlight the critical features influencing the classifier's predictions."}, {"title": "3.1. Building a classifier-aware semantic latent space", "content": "GANs benefit from a well-structured semantic latent space, which allows for easy control over different attributes of generated samples (Karras et al., 2019; 2020; Brock et al., 2019; Voynov & Babenko, 2020). This property has been leveraged in various applications, such as counterfactual visual explanations (Lang et al., 2021b). However, due to the iterative nature of diffusion models, they lack such a readily accessible latent space. In this work, we follow an approach similar to (Preechakul et al., 2022), where we construct a semantic latent space for our diffusion model by incorporating an encoder network. The encoder generates a latent code from a given input image, which is subsequently used to condition the diffusion process. To ensure that the generated samples maintain classifier-relevant attributes, we concatenate the classification score with the latent vector, forming a semantic code to condition the diffusion model, we denote it as Zsem.\n\nLdiffusion = E_{z_0, \\epsilon_t} [\\lVert \\epsilon - \\epsilon_{\theta}(x_t, t, z_{sem}) \rVert^2]  (1)\n\nIndeed, our goal is not only to generate images using this semantic code, but also to ensure that the generated image retains the same classification score as the original input. To achieve this, we introduce a classifier loss, which in our case is a KL divergence between the classification scores of the input image x and the reconstructed one x', an approach similar to (Lang et al., 2021b), the classifier loss is given by:\n\nL_{cls} = D_{KL} [C(x')||C(x)]  (2)\n\nThe total loss to optimize is then:\n\nL_{sem} = L_{diffusion} + \\lambda_1 L_{cls} (3)\n\nwhere \\lambda_1 is a hyperparameter."}, {"title": "3.2. Finding interpretable directions in the latent space", "content": "After training our semantic encoder, we introduce a contrastive learning approach to identify distinct and interpretable directions within its latent space. Contrastive learning has shown strong potential in exploring the latent spaces of GANs (Y\u00fcksel et al., 2021) and has been adapted recently to discover latent directions in the noise space of text-to-image diffusion models (Dalva & Yanardag, 2024). Unlike these prior methods, which locate semantic directions within either an intermediate GAN layer or the noise space of a diffusion model, our approach focuses on identifying meaningful directions directly within the latent space of the learned encoder.\n\nh = MLP_2(z_i, \\alpha) (6)\n\nf = h_i^k - MLP_2(z_i) (7)\n\n\\ell_{cont}(z) = -log{\\frac{\\sum_{k=1}^N  \\mathbb{1} [l=k]  \\sum_{i,j=1}^{N}  \\exp{  (sim(f_i^k , f_j^k)/\\tau) } }{\\sum_{k=1}^N  \\sum_{l=1}^{N} \\sum_{i,j=1}^{N}  \\mathbb{1} [i\\neq k]  \\exp{  (sim(f_i^k , f_j^l)/\\tau) } }} (8)\n\n\\ell_{reg} =  \\sum_{i\\neq j} Cov (D_i(z), D_j(z))^2 (9)\n\n\\ell_{dir} = \\ell_{cont} + \\lambda \\ell_{reg} (10)\n\nwhere \\lambda_1 is a hyperparameter."}, {"title": "3.3. Ranking the identified direction according to their importance", "content": "After obtaining the directional models, the next step is to identify those that significantly influence the classifier's probabilities. To do this, we first select a sample of images and compute their initial classification scores. For each discovered direction, we shift all images in the sample along that direction by a specific value of a and then calculate the new classification scores for the shifted images. If the average change in classification scores exceeds a predefined threshold, we retain that direction. Once a direction is selected, the images used to explain it are removed from the sample to avoid redundancy. This process is repeated iteratively until we identify the desired number of directions or exhaust the available images. The detailed pseudo-code for this procedure is provided in the Supplementary. B."}, {"title": "4. Results", "content": ""}, {"title": "4.1. Datasets", "content": "We used the following datasets to evaluate our method:\nFFHQ: The FFHQ (Karras et al., 2019) dataset is a high-quality image collection containing 70,000 high-resolution face images with diverse variations. Given its combination of high resolution and diversity, FFHQ has become a benchmark in the field."}, {"title": "4.2. DiffEx encodes natural and biological images", "content": "We trained a classifier on the FFHQ dataset to distinguish between male and female classes, we also trained classifiers on BBBC021 and Golgi datasets to classify untreated and treated images. As shown in Table 1, the proposed framework effectively encodes both biological and natural image features. Indeed, the different metrics used to assess the reconstruction quality demonstrate very low values for the datasets utilized in the experiments. Furthermore, the classification metrics across the three classifiers perform well on the generated images. This consistent classification accuracy suggests that the generated images are not only visually coherent but also maintain key distinguishing features necessary for correct classification, most importantly, the absence of adversarial artifacts that could alter the classifier's decisions."}, {"title": "4.3. Explaining a Classifier trained on natural and biological images", "content": "First, we applied DiffEx to explain a classifier trained on natural images. In Fig. 2, some directions identified by the method on the FFHQ dataset are shown. Specifically, short haircuts tend to push the classification toward the \"male\" class, while the presence of lipstick pushes the classification toward the \"female\" class, more examples are shown in Supplementary. A\nWe then applied DiffEx to a classifier trained on the BBBC021 images. In Fig.4, we illustrate the three most significant directions identified by our method for transition-"}, {"title": "4.4. Comparing to existing methods", "content": "Comparing our method to existing approaches is inherently challenging, as many of the current methods for detecting phenotypes rely solely on generative models. Among the most closely related methods, GCD(Sobieski & Biecek, 2024) stands out, although it was not proposed to identify phenotypes, it uses diffusion models to explain a classifier. Similar to our approach, they utilize a latent space constructed with DiffAE(Preechakul et al., 2022). However, GCD does not incorporate the classifier during training, and it identifies counterfactual directions using a single image optimized to minimize a counterfactual loss.\nFor comparison, we identified the first principal direction that most significantly shifts the classification score of the trained classifier. In Fig. 7, we present the generated explanation using our method and GCD. It is evident that the explanations produced by our method are visually superior and more disentangled compared to those obtained using GCD. Specifically, our method focus on modifying a single attribute-primarily shortening the hairstyle\u2014while GCD introduces changes to multiple attributes simultaneously, leading to less interpretable results. Additionally, we observe that the classification shifts are more pronounced in the examples generated by GCD compared to those produced by our method. This can be attributed to GCD's optimization of the counterfactual loss with respect to shifts in latent space. While GCD can identify a direction that reduces the classifier's confidence, the resulting counterfactuals are often of poor visual quality, as evident in some of the generated samples. In Fig. 4, we further evaluate the performance of GCD and our method on biological images. Notably, GCD fails to generate meaningful images when applied to this domain. This limitation is likely due to GCD's reliance on a single image to identify directions in the learned latent space. While this approach works well in datasets with inherent class similarities, such as FFHQ, it struggles in scenarios where there is high variability between classes, as is the case with biological images. Furthermore, in Table 2, we compare the quality of the generated explanations using the Kernel Inception Distance (KID) (Bi\u0144kowski et al., 2021), as well as the similarity between the original and generated images. The results show that our method consistently outperforms GCD across various datasets. This indicates that our method produces images that are not only closer to the target dataset distribution but also retain higher similarity to the original images, demonstrating its effectiveness and robustness."}, {"title": "5. Conclusion", "content": "In this work, we introduced DiffEx, a versatile framework for explaining classifiers using diffusion models. By identifying meaningful directions in the latent space, DiffEx produces high-quality and disentangled attributes that maintain fidelity to the original data while effectively shifting classification outcomes. An important application of DiffEx is its ability to detect phenotypes. We validated this capability across multiple datasets, demonstrating that DiffEx can reveal fine-grained biological variations and enhance our understanding of cellular and phenotypic differences. This highlights the method's potential to be a valuable tool in advancing research in biology and related fields, where uncovering subtle variations is essential. Moreover, DiffEx can be extended to other applications where it is critical to explain classifier outputs, making it a versatile framework for enhancing model interpretability across diverse domains."}, {"title": "A. More examples", "content": "In the following examples, we trained DiffeEX to identify 10 different directions in the semantic space. As we can see, these directions alter various attributes, but not all of them lead to changes in the output probabilities. To address this, we apply our ranking algorithm to rank the directions based on their ability to modify the classification output. For instance, in this case, the most important attribute is direction 5 (positive), which shortens the haircut of images belonging to the female class. Conversely, direction 6 (negative) adds makeup to images of males, increasing the probability of classification into the female class."}, {"title": "B. Ranking Algorithm Pseudo-code", "content": "Algorithm 1 Ranking algorithm\nData: Model f, Diff-Ex Generator G, Number of samples nsamples, Number of directions ndirs, Alpha a, Threshold T,Max coordinates M\nResult: Best coordinates C and directions D\nInitialize empty lists C\u2190[], D\u2190 Initialize used coordinates U\u2190 Sample samples images from the datasetpertaining to a particular class\nwhile length(C)\u00a1 M do\nmax_effect\u21900, best_coordinate \u2190 None, best_direction \u2190 None\nfor each image batch B of size BS from the remaining images do\nEncode B to obtain the semantic space: sem \u2190 G.encode(B)\nEncode the stochastic latent code: x\u2190 G.encode_stochastic(B, Zsem)\nfor each latent direction k \u2208 {1,2,..., ndirs} and k & U do\nModify latent codes: Zpos\u2190 edit(zsem, a, k,+1) zneg\u2190 edit (zsem, a, k, -1)\nGenerate positive and negative images: Ipos G.render(XT, Zpos) Ineg\u2190 G.render(xT, Zneg)\nCompute the average probability change for target class c: Apos pe (Ipos | c) -pe(B\u2758c) Aneg\u2190 po(Ineg | c) - \u0440\u043e (\u0412\u0441)\nif Apos > max_ef fect then\nbest_coordinate \u2190k best_direction \u2190 +1 max_effect \u2190 Apos\nif Aneg > max ef fect then\nbest_coordinate k best_direction-1 max_effect Aneg\nAppend best_coordinate to C Append best_direction to D Add best_coordinate to U Update remaining images byremoving sufficiently explained ones (low probability change) Decrease threshold: \u0442\u2190 min(\u0442, \u0442\u0430x_effect)\nreturn Best coordinates C and directions D"}]}