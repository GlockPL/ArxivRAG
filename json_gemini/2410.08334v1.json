{"title": "Exploring Natural Language-Based Strategies for Efficient Number Learning in Children through Reinforcement Learning", "authors": ["Tirthankar Mittra"], "abstract": "This paper investigates how children learn numbers using the framework of rein-forcement learning (RL), with a focus on the impact of language instructions. The motivation for using reinforcement learning stems from its parallels with psy-chological learning theories in controlled environments. By using state of the art deep reinforcement learning models, we simulate and analyze the effects of various forms of language instructions on number acquisition. Our findings indicate that certain linguistic structures more effectively improve numerical comprehension in RL agents. Additionally, our model predicts optimal sequences for present-ing numbers to RL agents which enhance their speed of learning. This research provides valuable insights into the interplay between language and numerical cog-nition, with implications for both educational strategies and the development of artificial intelligence systems designed to support early childhood learning.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning has been proven to be successful in multiple tasks like Atari games [1] and robotic manipulation [2]. There are also significant correspon-dences between reinforcement learning (RL) and the experimental study of animal learning in psychology. One notable example is how the temporal-difference (TD) algo-rithm generalizes the Rescorla-Wagner model, whose main idea is that animals only learn when events violate their expectations [3]. Another example is of reward shap-ing, a technique used in RL [4] to provide additional rewards to guide agents toward desired behaviors, this is an effective tool in both training RL agents and animals [5]. By systematically shaping the reward structure, trainers can accelerate the learning process. Furthermore, model based reinforcement learning, which involves the use of environmental models to predict future states and rewards, shares commonalities with what psychologists refer to as cognitive maps, these are mental representations that animals and humans use to navigate and understand their environment. Similarly, in RL the creation of an internal model of the environment allows agents to plan and make decisions based on predicted outcomes. [6].\nNumber learning in children is a crucial cognitive process that has been extensively studied both to understand artificial intelligence and to develop innovative pedagogical strategies [7]. We aim to study this process using reinforcement learning (RL) drawing inspiration from parallels that exist between RL and psychological learning theories. As per our knowledge, there haven't been any efforts to model or gain insights into number learning in children using RL and deep learning techniques. Additionally, we have developed a robust RL environment adhering to OpenAI API standards, which can be easily extended to further study various aspects of this cognitive learning process or other similar relational and hierarchical tasks. Using our reinforcement learning (RL) and language instruction framework, we discovered that agents learn significantly better when language instructions include explicit guidance on how to solve the task, compared to instructions that merely describe the state to the agent. Moreover, agents perform substantially worse and often fail to solve the task when only visual information is provided. Our model also uncovered an optimal ordering of numbers that consistently improves the agent's performance. Based on these findings, we propose that utilizing similar instructional strategies could greatly benefit children in their learning processes."}, {"title": "2 Related Works", "content": "Reinforcement learning (RL) agents face significant challenges when inferring abstract relational and causal structures within their environments. Recent studies have demon-strated that integrating language can significantly improve RL agent's understanding of environmental dynamics, thereby improving their ability to navigate and manipu-late complex systems [8] [9]. Moreover, language has been shown to assist RL agents in generalizating across different environments, facilitating the transfer of learned knowl-edge to new and varied contexts [10]. For humans, language explanations also play an important role in knowledge transfer and help us understand the causal structure of the world [11]. In this paper we model learning of numbers in children using reinforce-ment learning and investigate how and which type of language instructions help RL agents acquire numerical concepts.\nMany studies have investigated the learning of numbers in children, but these stud-ies have not typically employed machine learning and language modeling approaches, for instance, [12] explored core numerical skills in early childhood. Recognizing this gap, we decided to investigate this cognitive task using a deep RL framework. Our approach involves grounding language in RL tasks, which entails mapping linguis-tic elements to actionable knowledge that agents can use in their decision making processes. By leveraging these techniques, we aim to gain deeper insights into the mech-anisms underlying number learning and improve the efficacy of the learning process in educational contexts."}, {"title": "3 Methods", "content": "In this paper, we introduce a new reinforcement learning (RL) environment designed to study how language and visual cues affect children's ability to learn numbers. We model the child as an RL agent, and the agent's task is to build numbers using blocks that represent hundreds, tens, and ones. The agent has six possible actions. Three actions correspond to choosing blocks from the pool one for hundreds, one for tens, and one for ones. The other three actions involve placing these blocks in the correct positions to ultimately form the number displayed by the environment. The agent receives visual information about the current state of the system (as shown in Figure [1]) and uses different types of language instructions to complete the task. The above task is nontrivial because, although end-to-end trained deep learning has demonstrated strong performance in representation learning and pattern recognition, its reasoning capabilities remain relatively weak[13]. To build numbers, the agent must exhibit advanced reasoning abilities. If we observe the image, the number that the agent needs to construct is displayed at the center. In Figure [1], the agent is tasked with constructing the number one hundred and twenty-one. The blue-colored box corresponds to the hundredth block, the pink box corresponds to the tenth block, and the yellow block corresponds to the unit block. Beneath the number, there are three rectangles, each indicating the correct position where the agent should place the block.\nFor instance, the agent must place one cyan (hundredth) bock, two pink (tenths) blocks and one yellow (unit) block below the digits one, two and one respectively to complete the task. For simplicity, an agent is only allowed to carry one type of block at a time. In Figure [1], the agent has already picked up a hundredth block, which is represented by a black box indicator. If the agent is currently carrying any block, it will be represented by the black indicator box. This indicator makes the task a Markov decision process for agents trying to solve it using only visual cues.\nIn our experimental setup, we provide our agent with two distinct types of language instructions to guide its decision. The first type, known as policy based instructions, provides explicit directives on what actions the agent could take to accomplish the task at hand. These instructions essentially prescribe a strategy or policy that the agent can adhere to in order to navigate and solve the task efficiently. The second type of instructions, which we call state based instructions, offers descriptions of the current state of the environment to the agent. These descriptions convey the same information that the agent could otherwise gather by visually inspecting the environment.\nTo understand how the environment operates, we will discuss some state transi-tions within our environment. The environment generates policy based instructions as shown in Figure [2]. At the onset of each episode, the RL agent is provided with an initial image and an accompanying instruction. This instruction informs the agent of the displayed number and the action it should take to initiate solving the task. A proficiently trained agent will utilize both the visual information and the instruction to identify and pick up the hundredth block, transitioning to the subsequent state depicted in the second image of Figure [2]. However, it is not mandatory for the agent to strictly adhere to the actions suggested by the instruction. The detailed instructions provided to the agent are displayed in Figure[2].\nIn state-based instructions, as illustrated in Figure [3], the environment provides a detailed description of the current state to the agent. For instance, in the example of forming the number one hundred and twenty-one, the initial instruction presents the number in words and prompts the agent to begin the task. A proficient agent, drawing from its experience, will select the correct block. In the subsequent instruction in the next state, the environment again informs the agent of the current number and that no blocks are currently placed in the hundredth, tenth, or unit place and that the agent is holding a hundredth block. One might question the necessity of repeating the first part of the instruction, \"This is one hundred and twenty-one.\". The repetition serves a crucial purpose, it provides the agent with an indirect signal of when to cease its actions and how many blocks it should still pick up, thereby making the task a Markov Decision Process (MDP), which is essential for using traditional reinforcement learning algorithms. The dynamics of an MDP adhere to the Markov property, stipulating that the system's future behavior depends solely on the present state and action. By reiterating the aforementioned instruction phrase, the agent is relieved of the burden of tracking past states, allowing it to make optimal decisions based solely on the current state. Agents solely utilizing language to solve the task needs to know the magnitude to each number it's trying to solve in each state so it knows how many blocks to put for each digit and when to stop building the number."}, {"title": "3.1 Reinforcement Learning PPO", "content": "In the reinforcement learning paradigm, agents interact with an environment to solve tasks and receive rewards based on their performance. Among the commonly used reinforcement learning algorithms are deep Q-learning[1], vanilla policy gradient[14], and actor-critic methods such as Trust Region Policy Optimization (TRPO)[15] and Proximal Policy Optimization (PPO)[16]. Deep Q-learning, despite its popularity, suf-fers from several limitations: it is not well understood, often fails on simple tasks, and exhibits instability during training. Additionally, deep Q-learning requires a large number of training episodes to achieve satisfactory performance and involves tun-ing numerous hyper parameters, including the learning rate, target network update frequency, and discount factor. A significant challenge with deep Q-learning is catas-trophic interference, where new learning disrupts previously acquired knowledge. Our preliminary experiments with DQN did not yield promising results, prompting us to explore policy gradient methods. However, vanilla policy gradient methods, such as REINFORCE, are known for high variance, which can impede learning. Conse-quently, we opted for the actor-critic method, specifically PPO, due to its ease of implementation and stable learning characteristics with comparatively lower variance. PPO is a model-free, off-policy learning algorithm that alternates between sampling data through interaction and optimizing a surrogate objective. Being an off-policy method, PPO is significantly more sample-efficient than traditional policy gradient methods like REINFORCE. The choice of PPO as our reinforcement learning algo-rithm due to its reduced number of hyper parameters and enhanced stability also allowed us to concentrate on experimenting with deep neural network architectures rather than investing significant time in hyper parameter tuning. The most common policy gradient objective used for PPO is shown in Equation[1].\n$J^{P\\theta}(\\theta) = E_\\tau \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)} A_t$ (1)\nwhere, $A_t$ is called the advantage function, which quantifies how much better an action is compared to other actions in a given state $s_t$. The ratio $\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$ is the importance sampling term used in typical off policy learning methods. Typical advantage estimates are calculated using the formula in Equation[2]\n$A_t = r + \\gamma * V(s_{t+1}) - V(s_t)$ (2)\nbut we have used a generalized advantage estimate which reduces the variance of policy gradient estimates even more, Equation[3].\n$A_t = \\delta_t + (\\gamma \\cdot \\lambda) \\cdot \\delta_{t+1} + ... + (\\gamma \\cdot \\lambda)^{T} \\cdot \\delta_{t+T}$\nwhere,\n$\\delta_t = r_t + \\gamma \\cdot v_\\pi(s_{t+1}) - v_\\pi(s_t)$ (3)\nIn the above Equation[3], (0 \u2264 x \u2264 1) is used to control bias and variance, higher value of A means higher variance. In our reinforcement learning framework, we incor-porate an entropy term into the policy gradient objective (Equation[4]) to facilitate exploration within the environment. By maximizing entropy, the policy tends to select actions with lower probability, thereby promoting exploration, especially during initial training stages. This emphasis on maximizing entropy encourages the explo-ration of diverse actions, which is crucial for discovering optimal strategies in complex and uncertain environments. As training progresses, the policy gradually refines its focus based on accumulated experience, striking a balance between exploration and exploitation which achieves robust performance over time.\n$H(\\pi(.|s)) = - \\sum_a \\pi(a|s) \\cdot log(\\pi(a|s))$ (4)\nPutting it all together, the final loss function, which we minimize using off-the-shelf optimizers, is shown below in Equation(5).\n$L_t = \\alpha \\cdot (r_t + v_\\pi(s_{t+1}) \u2013 v_\\pi(s_t))^2 +$\n$- \\beta \\cdot E_\\tau [A_t \\cdot \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)} ] +$\n$- \\gamma \\cdot \\sum_a \\pi(a | s_t) \\cdot log(\\pi(a | s_t))$ (5)\nThe first term in the Equation(5) is the critic loss, the second term is the actor loss and the third term is the entropy term which encourages exploration."}, {"title": "3.2 Neural Network Architectures", "content": "In our study, we explore various neural network architectures to identify the archi-tecture that most accurately replicates the learning process observed in children who are proficient at forming numbers. The goal here is to determine which neural network architecture achieves superior performance."}, {"title": "3.2.1 Model 1", "content": "With model 1 as its neural network, the agent only used visual observations to find the optimal policy. Figure[4] shows the neural network architecture, we use a pre-trained ResNet [17] and made the top layers trainable, we then connect this layer to two dense neural networks, one dense neural network outputs the value of the state($v_\\pi(s_t)$ whereas the other dense neural network predicts action the agent needs to perform."}, {"title": "3.2.2 Model 2", "content": "The agent having model 2 as it's neural network architecture used both visual and language instruction to make a decision. This model used pre-trained Neural Network architecture BERT[18] and ResNet. The order in which we put the pre-trained models ensures the agent is forced to use whatever it learns from the pre-trained BERT and ResNet models. The idea to use pre-trained BERT and ResNet was to find out if there is a knowledge transfer from tasks on which these Neural Networks were trained on and the current task."}, {"title": "3.2.3 Model 3", "content": "In the third Neural Network model we only use a simple Dense Neural Network, this was to check if attention layers and pre-trained Neural Networks have any added benefit on how agents learn. The number of trainable parameters in Model 3 were approximately kept similar to the number of trainable parameters in Model 2 for easier comparisons."}, {"title": "3.2.4 Model 4", "content": "In this last model, we use the attention layer and train it from scratch. Its resource and time intensive to train LLMs and ResNet from scratch, so we created our own attention model with reduced size to study it's performance."}, {"title": "3.3 Description of our Experiments", "content": "We conducted three broad experiments. In the first experiment, we do something similar to curriculum learning[19], where we put different numbers in the training set and test set based on various strategies to understand which strategy of building the training set provides the best result. For our subsequent experiments we use the strategy of building training set that yields the best result. Additionally, it would be interesting to see if children exhibit similar learning patterns to our agent when presented with the differently constructed training sets.\nIn our second experiment, we trained different Deep Neural Network Architectures to understand their relative performance in our new environment. We took three different seeds and trained them independently to account for variance in results. After this experiment, we select the top-performing model as the one that most accurately mimics a child who is proficient at number building.\nThe final experiment was to check which form of instruction provides better learn-ing for our agent. Our RL environment can have different constructs of instructions for example, the agent can be provided instructions in other languages. To assess the rela-tive effectiveness of various forms of instruction, but to limit the scope of this paper we tried experimenting with two types of instructions. In the first kind, we tell an agent what to do and in the other, we describe what is happening. For data visualization we use bar plots across various ranges of numbers the height of the bar plot represents the average reward the agent earned in solving the task in that number range.\nThe code is made publicly available at code link."}, {"title": "4 Results", "content": "The following section presents the outcomes of our experiments. We examined four dif-ferent neural networks as described in the previous section. For all the results obtained from the RL algorithm, we used the different seeds and plotted the 66% confidence interval in the learning curves along with their mean, as illustrated in Figures [9] and [11]. Some of our experiments were used to validate our model by drawing parallels between children's learning processes, while others attempt to make predictions that can be used as pedagogical strategies for children."}, {"title": "4.1 Curriculum Learning", "content": "During our experiments, we observed that the ordering of numbers in the training set significantly impacted the performance of the agents on both the training and test datasets. We explored three strategies for arranging data in the training set:\n1. Ascending Order: Numbers were arranged in ascending order.\n2. Task-Ease Order: Numbers were sorted based on the ease with which an agent could solve the task. For example, an optimal agent expends the same number of actions to construct the number 1 and the number 100; for the number 1, the agent picks up and places the unit's block, and for the number 100, the agent does the same with the hundredth's block.\n3. Descending Order: Numbers were arranged in descending order.\nOur results indicated that the second strategy, where numbers were sorted by task-ease, consistently yielded the best performance across all models and types of language instructions. Figure [8] illustrates the performance of our attention based deep RL model with different instruction types for numbers between 0-99. Note that here we used a smaller subset of the data for hyper parameter optimization(HPO) which have been proven to be effective[20]. The second strategy outperformed the first strategy (ascending order), followed by the third strategy (descending order). The poor performance of the third strategy can be attributed to the difficulty agents faced when it was required to build larger numbers. Conversely, the first strategy allowed agents to solve numbers more easily at the beginning but ultimately performed worse than the second strategy. This was due to the development of a bias towards picking up the unit's block, as all numbers at the start of training can be solved using this strategy. This bias was detrimental when constructing numbers like 100, which do not contain unit digits. Given that agents performed better using the second type of ordering, and considering that we are modeling the learning of numbers, we can infer that human children might also benefit from being taught numbers in this order. Additionally, we experimented with training models using numbers arranged in a random order. The performance of models with a random order fluctuated between that of the descending and task-ease orders. Due to this variance, we did not include the random strategy results in Figure [8], as consistent results are preferred over randomness."}, {"title": "4.2 Performance On Policy Based Instructions", "content": "In policy based instructions, the agent receives language instructions to guide its actions. Figure 9 displays the learning/training curves of our four neural network models. The learning curve plots the average cumulative reward on the entire training set periodically after a fixed number of frames, with each episode composed of multiple frames or states. The agent powered by the attention-based neural network(model 4) and pre-trained ResNet plus BERT architecture (model 2) achieved the highest average cumulative reward in the shortest number of frames, followed by model 3. The worst-performing model was the agent that focused solely on image information. The bar plot provides a breakdown of the average cumulative reward in number ranges 0-99, 100-199, 900-999. The average cumulative reward increases as the numbers on the x-axis increase because if an agent builds a bigger number, it receives a higher reward. Figure 10 shows the average cumulative reward for the test set. Agents with model 2, model 3 and model 4 were able to complete the tasks for numbers it had never seen before, indicating that those models were able to generalize to unknown numbers. Conversely, the agent that ignored the language input and focused only on visual cues performed the worst."}, {"title": "4.3 Performance On State Based Instructions", "content": "In state based instructions, the agent is given language instructions describing the current state. Figure[11] shows the learning curve. Cumulative reward on the entire training set is calculated periodically after a certain number of frames and plotted on the training curve. Figure[10] show the performance of all four models on the test set respectively. Compared to results in policy based instructions all models fail to show good performance except the attention model. Compared to the policy based instruction in the previous experiment, the agents faced difficulty in solving the task because state based instruction doesn't provide the agent with the actions it can take to solve the task; the agent has to figure that out. The model that only uses visual information performs the worst. Therefore, from these two experiments, we can conclude that language instruction plays a critical role for the agent to learn, language can encode information more compactly compared to images, and helper instructions significantly speed up the learning process. Our observation shows that training on a larger dataset for a longer period will led to improved performance."}, {"title": "4.4 Policy based v/s State based instructions", "content": "Figures 13 and 14 compare the performance of our best model i.e. the attention model, under two different types of instructions. This trend holds for all our models. The agent performs significantly better with policy based language instructions, and this difference is especially pronounced in the test set. The reason for this is that with policy based instructions, the agent doesn't need to figure out what actions to take to solve the task; it can simply follow the language instructions provided by the environment. In contrast, with state based instructions, the agent doesn't know the optimal actions it needs to take. It must infer these actions by interacting with the language environment. This setup is much more challenging for the agent to solve, resulting in the type of performance seen in Figures 13 and 14. The policy-based instructions also achieved optimal performance with only 4-6% of the training we did for the state based instructions."}, {"title": "5 Future Work", "content": "One of our key contributions is a novel reinforcement learning environment, which can be utilized to explore how children grasp numerical concepts or one can study hierarchical and relational reasoning capabilities of a generic deep RL algorithm. With this innovative environment, our future aim is to investigate how different languages, such as Chinese, German, and Indian, among others, influence children's numerical learning. It's worth noting that we cannot utilize the pre-trained models for different languages due to their unavailability or discrepancies in quality, based on the corpora they were trained on. To ensure equitable comparisons, we need to train models from scratch. Moreover, we intend to delve into the impact of various language structures. While our current study focused on two types of instructions, those describing the state (state-based instructions) and those providing a policy for the agent to potentially follow (policy-based instructions), there is a third type of language structure which involves instructions that convey the value of the state. For instance, instructions generated from the environment can indicate whether the agent is in a favorable or an unfavorable state."}, {"title": "6 Conclusion", "content": "In conclusion, we have observed numerous parallels between our model and the learn-ing process of a child. For instance, if one instructs a child to construct numbers by simply following actions, they can easily comply. However, if you describe the situation or state using words and ask the child to solve the task, they may struggle to under-stand how to proceed. The same thing happens with our agent, if we give policy based instruction to our agent meaning we are telling it what to do, it will have an easier time building the numbers but the agent doesn't learn anything about the underlying structure of the numbers or how to build them, it only learns to follow instructions blindly. Another parallel we observed was that when children are provided with more positive feedback, they learn faster compared to receiving sparse feedback. There was a similarity here with our model: when given dense rewards, it performed well, akin to how a child would perform under similar circumstances. We also observed that the order in which we arrange numbers in our training set has a different impact on the final model and its performance. It would be interesting to explore whether children also exhibit similar learning patterns. That is, if we teach them numbers in the order that yields the best performance for our model, would the child learn the numbers faster? In fact, we noticed that curriculum learning, where models learn more quickly with easier examples, is not the sole determining factor for optimal performance. It is also essential to ensure diversity among the easier examples to reduce model over-fitting. In our case, putting a proper mix of single-digit, two-digit, and three-digit numbers which can be formed easily in the training set resulted in better outcomes. Our experiments also revealed that language plays a more pivotal role in number learn-ing in children as our models couldn't solve the task only with visual cues. It seems for our task of building numbers, language has the capacity to encode more information efficiently and in a more compact manner compared to visual information."}]}