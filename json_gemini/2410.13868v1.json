{"title": "Stars, Stripes, and Silicon: Unravelling the ChatGPT's All-American, Monochrome, Cis-centric Bias", "authors": ["Federico Torrielli"], "abstract": "This paper investigates the challenges associated with bias, toxicity, unreliability, and lack of robustness in large language models (LLMs) such as ChatGPT. It emphasizes that these issues primarily stem from the quality and diversity of data on which LLMs are trained, rather than the model architectures themselves. As LLMs are increasingly integrated into various real-world applications, their potential to negatively impact society by amplifying existing biases and generating harmful content becomes a pressing concern. The paper calls for interdisciplinary efforts to address these challenges. Additionally, it highlights the need for collaboration between researchers, practitioners, and stakeholders to establish governance frameworks, oversight, and accountability mechanisms to mitigate the harmful consequences of biased LLMs. By proactively addressing these challenges, the AI community can harness the enormous potential of LLMs for the betterment of society without perpetuating harmful biases or exacerbating existing inequalities.", "sections": [{"title": "Introduction", "content": "Bias, toxicity, unreliability and lack of robustness are interrelated issues that plague large language models (LLMs). Given that LLMs are utilised in various real-world applications, including language translation [51,13,45,34], search engines [2,1], and scientific literature summarisation [50], it is crucial that production-ready LLMs exhibit minimal bias and do not generate harmful content. However, the current state of language models faces significant challenges in this regard.\nIn this work, we analyse the issues that affect state-of-the-art language models such as ChatGPT, an RLHF-augmented [5] chatbot based on GPT-3.5 [11]. These problems stem less from the model architectures themselves and more from the fact that the models are trained on massive collections of uncurated data from the Internet [54]. While LLMs gain much of their knowledge and capabilities from the scale of data, we argue for the use of high-quality, curated datasets over stronger content filters as a solution. We discuss why this superficial approach is problematic.\nAddressing problems like bias is crucial not only to ensure theoretical soundness but also to implement practically before LLMs become integrated into daily"}, {"title": "The Bias Bazaar", "content": "LLMs generate responses with a coherent and fluent natural language structure, creating an illusion of authority and credibility. This presentation format implies an intelligence that encourages users to accept the outputs at face value, exacerbating the human tendency to trust autonomous systems that reduce cognitive load [49]. These factors could impede the ability to distinguish facts from falsehoods and rational from irrational reasoning in LLM outputs. In this context, accepting fake news, toxic content and bias is easy and seen as normal by a inexperienced user.\nIt is crucial to note that LLMs like chatbots currently generate all responses as text, though recent models such as GPT-4 experiment with multi-modal approaches [43]. Each step towards humanising these interfaces makes it increasingly difficult to approach them with a critical perspective. Despite this, humans tend to view text as more credible and accurate than other media, given vision's dominance as a sense [53,41].\nBiases represent the discrepancy between rational and heuristic behaviour [52,3]. As of 2023, cognitive science has identified innumerable cognitive biases [7,8,32,47,55] which can lead to flawed reasoning, irrationality, and potentially harmful consequences [49]. Prominent biases studied in the literature include cultural, gender, nationality, political, and ethical biases.\nWhile recent work has examined the presence of specific biases in various models [49], the scale and complexity of LLMs today make comprehensive auditing and remediation challenging. As models continue to increase in capability and adoption, governance frameworks, oversight, and accountability are urgently needed. Reliance on biased algorithms and data can directly and negatively impact marginalised groups through unfair treatment, discrimination, or by influencing consequential decisions."}, {"title": "ChatGPT Waves the American Flag", "content": "Recent progress in LM design has led to increasingly large models that demonstrate strong capabilities in various natural language tasks [9]. However, larger models also introduce and amplify biases present in their training data [56,9]. For models trained primarily on raw data ingestion, the characteristics of the training data have significant influence on model performance and biases."}, {"title": "The Larger They Get, the Larger Their Shadow is", "content": "The representation of viewpoints in large language models (LLMs) is primarily governed by frequency, which is an inherent aspect of their architecture and not easily modifiable [40]. Biases often originate from extensive, unfiltered corpora and can persist even when safety filters are employed in the architecture [43].\nThe dominance of frequency in LLMs gives rise to a critical issue: the under-representation of less frequent data. Consequently, majority viewpoints tend to overshadow minority perspectives.\nFor instance, Wikipedia is frequently among the most representative sources in corpora used for LLM training. However, its content is predominantly authored by males, with female contributors constituting less than 15% of the total [6]. Furthermore, training datasets such as Common Crawl\u00b9 and The Pile [16] have been found to contain high levels of toxic, racist, or sexist content [17]. This underscores the significance of carefully selecting and curating training data to mitigate biases in LLMs.\nThe widespread adoption of the \"bigger is better\" paradigm in the context of large language models presents both ethical and computational challenges. While it is evident that larger training datasets yield improved performance for LLMs, the necessity of human involvement in dataset curation or generation cannot be ignored. This labour is frequently carried out by crowdworkers who receive inadequate compensation, lack essential protections, and are exposed to harmful content throughout their workday [20,23,28,29]. Moreover, due to the sheer size and continuous growth of these datasets, assessing their quality in terms of bias identification and toxicity presence becomes increasingly difficult."}, {"title": "Complete the Sentence: All you Need is... [Violence]", "content": "The current trend in LLMs for secure human-chatbot interactions involves reinforcement learning with human feedback (RLHF) [22,14] and standard safety mechanisms. RLHF ensures safety in typical \"naive\" interactions [57,21], while safety mechanisms have been found to be less reliable against prompt injection [21]. A majority of prompt injection techniques utilise storytelling, an effective method capable of diverting LLMs like ChatGPT from generating innocuous content and instead producing harmful narratives. The underlying motivations behind this phenomenon remain unclear; however, some attribute it to a semiotic-simulation theory known as \"The Waluigi Effect\"2, wherein the creation of an ideal simulation environment allows the LLM greater freedom to improvise. As larger and more sophisticated models increasingly exhibit a tendency to reproduce human common misconceptions [35], it is anticipated that this issue will continue to exacerbate unless more effective countermeasures are developed and implemented.\nRevising the harmful content in primary training datasets is the most effective approach for mitigating the generation of toxic output from large language models. It is widely acknowledged that amidst the vast data sources used for training GPT-x models, harmful and toxic content can be found; these models' initial datasets encompass data from unreliable news sites as well as quarantined and banned subreddits [17]. Even when present in smaller quantities, such data has been shown to be more salient for the model and considerably more challenging to \"unlearn\" [30,12].\nWith minimal or no prompting, models have been observed to generate potent and offensive content targeting minority groups and LGBTQIA+ individuals [42], thereby supporting the aforementioned hypothesis."}, {"title": "All Bias is Language Bias", "content": "A prevalent misconception is that these models exhibit cognitive bias, akin to those found in human decision-making [49]. However, it is essential to clarify that large language models do not possess cognitive bias, as they lack cognitive abilities [37]. Instead, the biases observed in these models stem from language"}, {"title": "Words are Powerful: Unintended Consequences of Real-World AI Misadventures", "content": "In light of the growing concerns about bias in LLMs like ChatGPT, it is essential that these models are developed to be explainable, transparent, unbiased, fair, verifiable, and accountable for every decision [39,15]. Despite these requirements, many current LLMs are deployed without fully addressing these issues, raising questions about whether our expectations are too high or if these models are not yet ready to be products. The accelerated release of unsafe models by companies may be contributing to the difficulty in refining these models to meet these criteria, necessitating further research.\nFurthermore, the extended use of unsafe models in daily life has the potential to jeopardise crucial sectors such as healthcare, medicine, code safety, journalism, online content, and spam prevention, among others [24,26]. Models like GPT-43 have been shown to be capable of creating misinformation scenarios and spreading toxic and biased content with ease [43]. It is imperative that the development and deployment of LLMs prioritise safety and responsibility to prevent adverse consequences in these vital areas."}, {"title": "Healthcare and Medicine", "content": "Positive Impacts: ChatGPT can streamline the healthcare industry by providing quick and accurate responses to common medical questions [31,18], thereby saving time for medical professionals. Additionally, it can aid in the analysis of medical records and help identify patterns or trends that might otherwise go unnoticed.\nNegative Impacts: If ChatGPT is not properly trained or its knowledge base is outdated, it may provide incorrect or potentially harmful medical advice. This could lead to dangerous consequences for patients and healthcare providers."}, {"title": "Code Safety", "content": "Positive Impacts: ChatGPT can serve as an effective tool for code review [25], detecting potential bugs, and suggesting improvements to existing code. This could improve overall software quality and reduce the likelihood of security vulnerabilities.\nNegative Impacts: For now, ChatGPT generates flawed or unsafe code suggestions [27], and it could inadvertently introduce security risks or software bugs in the future, compromising the safety and reliability of the developed software."}, {"title": "Journalism", "content": "Positive Impacts: ChatGPT can assist journalists in drafting articles quickly and efficiently. It can also help in the generation of news summaries, translations, and content personalisation for readers [44].\nNegative Impacts: The potential for ChatGPT to generate biased or misleading content poses a risk to journalistic integrity [44,46,33]. If unchecked, it could contribute to the spread of misinformation and undermine public trust in news sources."}, {"title": "Online Content", "content": "Positive Impacts: ChatGPT can help generate engaging and relevant content for websites, blogs, and social media platforms, assisting content creators and marketers in their efforts to reach and captivate audiences.\nNegative Impacts: The ease with which ChatGPT can generate content may result in an oversaturation of low-quality or misleading information online. Additionally, it could be used to create and spread fake news, deepfakes, and other manipulative content [46,15,4]."}, {"title": "Spam Prevention", "content": "Positive Impacts: ChatGPT can be utilised to develop advanced spam filters, capable of identifying and blocking spam messages more effectively by understanding the semantic meaning of text, rather than relying solely on keywords or patterns.\nNegative Impacts: Conversely, ChatGPT can also be employed by malicious actors to generate sophisticated spam messages that bypass existing filters [38,10], leading to an increase in unwanted and potentially harmful content in users' inboxes."}, {"title": "Conclusion: the Avalanche Effect", "content": "In conclusion, the concerns surrounding bias, toxicity, unreliability, and lack of robustness in large language models (LLMs) such as ChatGPT are multifaceted and significant. This paper highlights that the primary challenge lies in the diversity of data on which LLMs are trained. As these models become increasingly integrated into daily technologies and real-world applications, their potential to negatively impact society by amplifying existing biases and generating harmful content is intensified.\nOne particularly noteworthy concern we should raise is the possible \"avalanche effect\", wherein future LLMs could inadvertently include generated content from previous LLMs in their training data. This effect could result in a self-perpetuating loop of biased and potentially harmful content being propagated across generations of models, exacerbating the issues that already plague these systems. Consequently, it is crucial for researchers and practitioners to develop methods for mitigating these problems and ensuring the development of fair, ethical, and trustworthy AI systems."}]}