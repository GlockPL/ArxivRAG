{"title": "Generalizing Constraint Models in Constraint Acquisition", "authors": ["Dimos Tsouros", "Senne Berden", "Steven Prestwich", "Tias Guns"], "abstract": "Constraint Acquisition (CA) aims to widen the use of constraint programming by assisting users in the modeling process. However, most CA methods suffer from a significant drawback: they learn a single set of individual constraints for a specific problem instance, but cannot generalize these constraints to the parameterized constraint specifications of the problem. In this paper, we address this limitation by proposing GENCON, a novel approach to learn parameterized constraint models capable of modeling varying instances of the same problem. To achieve this generalization, we make use of statistical learning techniques at the level of individual constraints. Specifically, we propose to train a classifier to predict, for any possible constraint and parameterization, whether the constraint belongs to the problem. We then show how, for some classes of classifiers, we can extract decision rules to construct interpretable constraint specifications. This enables the generation of ground constraints for any parameter instantiation. Additionally, we present a generate-and-test approach that can be used with any classifier, to generate the ground constraints on the fly. Our empirical results demonstrate that our approach achieves high accuracy and is robust to noise in the input instances.", "sections": [{"title": "Introduction", "content": "Constraint Programming (CP) is considered one of the main paradigms for solving combinatorial problems in AI. It provides powerful modeling languages and solvers for decision-making, with many successful applications (Wallace 1996; Simonis 1999). In CP, the user declaratively states the constraints over a set of decision variables, thereby defining the feasible solutions to their problem. A solver is then used to generate a solution. However, modeling a new application as a constraint problem requires significant expertise, which is a barrier to the wider use of CP (Freuder and O'Sullivan 2014; Freuder 2018). This has motivated the development of methods to assist the user in the modeling process (De Raedt, Passerini, and Teso 2018; Freuder 2018; Kolb 2016; Lombardi and Milano 2018). This is the focus of the research area of Constraint Acquisition (CA) (Bessiere et al. 2017), which has been identified as an important topic for CP (De Raedt, Passerini, and Teso 2018) and as progress toward the \u201cHoly Grail\" of computer science (Freuder 2018).\nIn CA, constraints are either learned passively from a set of known solutions and (optionally) non-solutions (Beldiceanu and Simonis 2012; Berden et al. 2022; Prestwich et al. 2021) or actively through interaction with a user (Bessiere et al. 2023; Tsouros and Stergiou 2020, 2021). Recent advancements in both passive and active acquisition systems show significant potential (Prestwich et al. 2021; Prestwich and Wilson 2024; Tsouros, Berden, and Guns 2024). For instance, a recent application of interactive CA in a real-world scheduling problem was presented in (Barral et al. 2024).\nHowever, a significant limitation of most (passive and active) CA systems is that they only learn the ground constraints of a specific problem instance (Arcangioli, Bessiere, and Lazaar 2016; Bessiere et al. 2017; Prestwich 2021; Prestwich et al. 2021; Tsouros, Stergiou, and Sarigiannidis 2018), while in practice it is common for the instance at hand to change over time. To accommodate these changes, well-known constraint modeling languages like MiniZinc (Nethercote et al. 2007) and CPMpy (Guns 2019) allow the use of parameters in the constraint model. An assignment of values to the parameters then instantiates the ground constraints for a given instance of the problem. For example, in exam timetabling with the requirement 'exams of courses that are given in the same semester should be scheduled on different days', the actual ground constraints will be instantiated based on the following parameters: which semester each course belongs to, and how many timeslots are available per day. Achieving the same ability to generalize the constraint models learned using CA over different instances of the same problem class has been identified as a key challenge for CA (Simonis 2023).\nIn the literature on CA, there are only a few works that support generalization. The first approach for generalizing constraints was introduced in interactive CA (Bessiere et al. 2014; Daoudi et al. 2015), to enhance the acquisition process within a single instance. Although it targets within-instance generalization, this approach can be used across instances. Another approach, called extrapolation, was recently explored (Prestwich 2022). This method requires"}, {"title": "Background", "content": "We now introduce the necessary concepts used in the paper.\nConstraint Satisfaction Problems A constraint satisfaction problem (CSP) is a triple P = (V, D, C), defining:\n\u2022 a set of n decision variables V = {v_1, v_2, ..., v_n }, representing the entities of the problem,\n\u2022 a set of n domains D = {D_{v1}, D_{v2}, ..., D_{vn} }, with D_{vi} \\subset Z being the domain of v_i \\in V,\n\u2022 a constraint set C = {C_1, ..., C_t}. over the variables in V.\nA constraint c is a pair (rel(c), var(c)), where var(c) \\subset V is the scope of the constraint, and rel(c) is a relation over the domains of the variables in var(c), restricting their allowed value assignments. The arity of the constraint, denoted as |var(c)| = arity(rel(c)), indicates the number of variables involved. The set of solutions of a constraint set C is denoted by sol(C).\nConstraint Acquisition In Constraint Acquisition (CA), the pair (V, D) is called the vocabulary of the problem at hand and is common knowledge shared by the user and the system. Besides the vocabulary, the learner is also given a language \u0393 consisting of a broad range of fixed arity constraint relations that may exist in the problem at hand. Using the vocabulary (V, D) and the constraint language \u0393, the system generates the constraint bias B, which is the set of all expressions that are candidate constraints for the problem.\nThe (unknown) target constraint set CT is a constraint set such that for every example e it holds that e \\in sol(CT) iff e is a solution to the problem the user has in mind. The goal of CA is to learn a constraint set CL that is equivalent to the unknown target constraint set \u0421\u0442.\nMachine Learning Classification ML classification is a supervised learning task that involves learning a function over a given dataset. The dataset, denoted as E, is a collection of N training examples, E = {(X_1, y_1), (X_2, y_2), ..., (X_N, y_N)}. Each example is a pair (x_i, y_i), where x_i is a feature vector from the input space X and y_i is the corresponding class label from the output space Y. The feature vector x is composed of m features, x_i = (\u03a6_{i1}, \u03a6_{i2},...,\u03a6_{im} ), with each feature \u03a6_{ij} being a (quantifiable) property of example i. In the case of classification, Y is a set of possible class labels. An ML classifier aims to learn a function f_\u03b8 : X \u2192 Y, using a set of learnable parameters \u03b8. These parameters are adjusted during training to minimize a loss function L(f_\u03b8(x), y) measuring the error between the predicted and actual class labels.\nDecision Rules With the rising importance of explainable AI (XAI) and interpretable ML, various approaches focus on extracting decision rules from ML models (Gilpin et al. 2018). These approaches represent the function f_\u03b8 : X \u2192 Y with if-then rules, denoted as a set R = {r_1, r_2, ..., r_k}. Each decision rule r_i is a pair (Q_i, y_i), with Q_i being a set of conditions and y_i a class label. Each condition q_{ij} \\in Q_i is a function q_{ij} : X \u2192 {0, 1} that maps an example x to a binary value indicating whether the given example satisfies the condition. For a rule r_i = (Q_i, y_i) to be satisfied, all of its conditions need to be satisfied, i.e., q_{ij}(x) = 1  \u2200q_{ij} \u2208 Q_i."}, {"title": "Problem Definition", "content": "Constraint problems are often not thought of as a single CSP, but as a set of requirements, with the specific instantiation of the ground CSPs depending on the values of some input parameters P. We illustrate this with the following example, which we will also use as a running example."}, {"title": "Constraint Specifications", "content": "Being able to generalize constraint models involves finding such a function F (definition 2). Typically, in constraint problems, such a function F can be decomposed into several inner functions \u2013 which we model as constraint specifications (CSs) - each corresponding to a specific requirement of the problem; for example \"all courses must be scheduled in a different timeslot\". The complete set of constraints CT of an instance T is then the union of the sets of constraints produced by each inner function.\nEach requirement is modeled by a constraint specification, which defines how to derive the pairs (rel(c), var(c)) for any target instance T, using the parameter values Pr and the corresponding vocabulary (VT, DT). We consider the following three key elements of a CS:\n1. Constraint relation. The relation rel(c) of each constraint in this CS, which may optionally depend on parameters that determine constant values involved in the relation.\n2. Variable partition(s). Typically, a pattern that appears in a constraint model concerns certain partitions of variables of the problem and is applied to sequences of variables in this partition. Such partitions can be the dimensions (e.g., rows and columns) of the (multi-dimensional) matrix the variables are given in, or based on latent dimensions in this tensor.\n3. Sequence conditions. These define which scopes within a partition of variables to apply the constraints to. It is common to have a constraint apply to all possible scopes in a partition. This is done by using the sequence all_pairs for binary constraints, or more generally, the sequence combinations, to find combinations of size arity(r) (the arity of the given relation). However, there may also be sequence conditions, restricting the variable combinations that should be taken as scopes.\nUsing these three key elements, we now formally define constraint specifications:\nDefinition 3. A constraint specification (CS) is a triple (r, G, S), defining\n\u2022 a relation r, along with any parameters defining its constants,\n\u2022 a variable partition function G : Vr \u2192 P(VT), that partitions a given set of variables VT into subsets based on certain characteristics,\n\u2022 a set of sequence conditions S restricting the scopes to which the constraints are applied.\nA CS can generate the corresponding ground constraints of an instance T using the following generator template:\nForeach Y \u2208 P(V_T):\nForeach scope \u2208 combinations (Y, arity(r),\nS):\nc \u2190 (rel (c) = r, var (c) = scope)\nThis is equivalent to our formally defined CS generator, where P(VT) corresponds to all_rows and consecutive_pairs corresponds to combinations with arity 2, and sequence condition column (varl) = column (var2) = 1."}, {"title": "Generalizing Constraint Models", "content": "To generalize beyond one or more known instances and learn the CSs of a problem, we propose an approach named GENCON. The key idea is to use ML to identify patterns in the constraints of the known instance(s) and reconstruct the CSs of the problem. This is especially promising because, recently, an approach using probabilistic classification during active CA (Tsouros, Berden, and Guns 2024) demonstrated that ML classifiers can effectively detect patterns within the learned constraint network.\nGENCON is shown in Figure 1. The given set of ground constraints in the input instance(s) is used in order to train a classifier to predict for any constraint whether it belongs to the set of constraints of any target instance of the problem. For this, we use a (parameterized) feature representation of the constraints, whose design is inspired by the different elements of CSs discussed in the previous section. For classes of classifiers allowing the extraction of decision rules, we directly translate these rules to the CSs of the problem, which can produce the ground constraints of any target instance. When decision rules cannot be extracted, a generate-and-test approach is used instead."}, {"title": "Building the Dataset", "content": "We build a dataset on the constraint level, i.e., its examples correspond to individual constraints. Given a set of constraints CA for a problem instance A, and a distinct set of constraints C, consisting of constraints that are not part of the model, we define a dataset E, wherein each training example is represented as a tuple (xi, Yi), corresponding to a constraint ci\u2208 C. For each example (xi, Yi), we have xi = \u03a6(ci, P), which is a parameterized feature representation of constraint ci, and yi = [ci \u2208 CA], a Boolean label that indicates whether ci is part of the set of true constraints or not.\n$E = {(\\mathbf{x}_i, Y_i) | \\mathbf{x}_i = \\phi_\\sigma(c_i, P) \\wedge Y_i = [c_i \\in C_A], \\forall c_i \\in {C_A \\cup C^{-}_A} }$  (1)\nHowever, realistically, we may only have access to the set of true constraints CA for each problem instance. But we also need a set of constraints C consisting of constraints that will have a negative label, for the classifier to learn how to distinguish between the classes. To produce this set, we first generate a set of constraints BA, using as a language \u0393 all relations detected in the given set of constraints CA, i.e, \u0393 = {rel(c) | c \u2208 CA}. The bias BA is created by applying each relation in \u0393 to all possible scopes in VA. The set C then consists of all constraints in BA that are not part of the given instance(s), i.e., C = BA\\CA."}, {"title": "Parameterized Feature Representation", "content": "In our approach, we propose a framework for the (parameterized) feature representation of constraints, targeted at learning patterns based on the different elements of CSs discussed above. For any constraint c, the classifier expects a fixed-size feature representation \u03c6(c) as input. As shown in the middle of Figure 1, this feature representation \u03c6(c) is then transformed to a parameterized version, denoted by \u03c6\u03c3(c), to be able to learn patterns across instances with different parameter values.\nFeature Representation. The feature representation \u03c6(c) must be designed based on the different elements of CSs that we want the classifier to detect in the problem, i.e., the relations, partitioning functions, and sequence conditions. Hence, it must contain features that describe the constraint relations, variable characteristics that can be used to recognize partitions of the variables, and other attributes that can play a role in the sequence conditions. Based on this, we construct a feature representation consisting of three groups of features:\n1. Relation features: Features that capture properties of the relation rel(c) of a given constraint c, along with numerical values of the constants present in the constraint.\n2. Partitioning features: Features describing whether the variables in the scope of constraint c have characteristics in common that can be used in the partitioning function. These characteristics can be problem-specific variable properties (e.g., in what semester a course takes place in exam timetabling), or based on information regarding the structure the variables were given in. For example, in many cases, the variables V are given in the form of a matrix or tensor, and the position of each variable in this tensor often plays a crucial role in the partitioning function of the CSs.\n3. Conditioning features: These features describe how the variables in the scope of the constraint relate in different ways, to capture sequence conditions that may exist in the CSs of the problem. For example, a constraint may only apply to pairs of variables that are a certain distance away from each other in the variable tensor. Thus, the distance between the variables in a constraint's scope may be included as a conditioning feature. Note that the partitioning features can also be used to capture the sequence conditions, since they describe whether the variables share a certain property or not. For example, a sequence condition may state that the variables must not be part of the same row in the variable matrix.\nThe grammar of relations, partitioning functions, and sequence conditions used can be considered as the inductive bias of our method. The feature representation needs to be able to capture the CSs existing in the problem at hand. In our implementation, a proof-of-concept feature representation was used based on structural properties of the variables matrix, as matrix modeling is common and beneficial in CP (Flener et al. 2001), with no problem-specific variable attributes.\nFrom numerical to categorical features over parameters. As the goal is to generalize beyond a single problem instance, the feature representation of the constraints should capture the characteristics of the constraints in a generic, parameterized way. Numerical attributes of the constraints typically are not static across instances but depend on parameters of the problem; e.g., in our running example, the constant present in the different_day constraints depends on the timeslots-per-day parameter and is not a static value. We want the classifier to be able to capture that. In this step of our approach, we thus replace numerical features with categorical features over the parameters. We do so using a numerical-to-categorical parameter mapping function \u03c3: R \u2192 {\u201cNaN\u201d} UPA (where PA is the list of named parameters and their value), defined as follows:\n$\\sigma(\\upsilon) = \\begin{cases} p_i & \\upsilon = \\upsilon_i, (p_i, \\upsilon_i) \\in P_A\\\\\\ \"NaN\" & otherwise. \\end{cases}$  (2)\nFor any numerical feature value that corresponds to a parameter value of the problem instance, function \u03c3 replaces the feature by the corresponding parameter's name.\nNote that, in a constraint model, it is sometimes not the value u of parameter p that comes up directly in the features. Instead, a trivial arithmetic adaptation of the parameter value may be used, e.g., u \u2212 1, u + 1 or the multiplication of two parameter values. To capture this, we extend the set of parameters P with these adaptations, along with the common basic constants 0 and 1, as is also done in COUNT-CP (Kumar, Kolb, and Guns 2022). For the new categorical features, there are thus |P + 1| categories (where P is the extended set of parameters): one for every parameter, plus a \u201cNaN\u201d in case none of the parameters match the given value. Our categorical features will thus be able to represent the constraint in a parameterized way. Although arbitrary constants cannot be captured this way, we make the assumption that every constant present is related to the parameters of the problem.\nAlso note that, when parameterizing the feature representation of a constraint, a single numerical feature value might correspond to multiple parameter values. When this occurs, one example is included in the dataset for each possible matching. Although this could add noise to the dataset, due to examples with a wrong parameter replacing the numerical feature, it ensures that the correct feature representations will definitely be included."}, {"title": "Extracting Constraint Specifications", "content": "Decision rules continue to be popular due to their interpretability, with methods existing to extract rules from various classes of classifiers (Barakat and Bradley 2010; Iqbal 2012). In this process, the goal is to derive a set of rules R = {r_1, r_2,...,r_k} that represent the classification function. Each rule ri specifies some conditions Qi on a subset of features, and a class label yi, such that Q_i \\rightarrow y_i. In this context, rules that lead to a positive classification define the conditions for a constraint to be part of the target problem. Thus, these conditions can be converted into the CSs of the problem. We now propose a method for extracting the interpretable CSs of the problem from such a set of extracted decision rules. The extracted CSs can then be used to generate the constraints of any given target instance.\nIn this context, rules that lead to a positive classification define the conditions for a constraint to be part of the target problem. Thus, these conditions can be converted into the CSs of the problem. As a result, our approach only operates on such positive-classification rules, iterating over their conditions to identify the relation, partitioning function and sequence conditions of each CS.\nOur method is shown in more detail in Algorithm 1. First, the rules leading to a positive classification are extracted in Rpos (line 1). Then, for each ruler \u2208 Rpos (line 3), a CS is constructed. The elements of the CS are first initialized (lines 5-7), and then the algorithm iterates over the rule conditions in Q to construct the CS (line 8) as follows:\n1. Relation Extraction: Identify conditions in related to relation features (RF) (lines 10-11). These conditions determine which relations from \u0393 are used, and which constants are used in these relations, if any.\n2. Partitioning Function (lines 12-14): Identify the partitioning function of the CS using conditions in Q involving partitioning features (PF). Use conditions that require certain characteristics to be equal in the constraint's variables, and thus can be used to partition the variables based on them.\n3. Sequence Conditions (lines 15-18): Sequence conditions can be derived from both partitioning features and sequence condition features (SF). More concretely, if a condition Q involves a partitioning feature, and requires certain characteristics to not be equal, then this requirement is added to the sequence condition (lines 15-16). If a condition in Q involves a sequence condition feature, it is also added to the sequence conditions (lines 17-18)."}, {"title": "Generate-and-Test", "content": "To enable the use of our method even when decision rules cannot be extracted, we now present a generate-and-test approach that can be used with any classifier, as an alternative to extracting the CSs from interpretable classifiers.\nThe intuition of this approach is the following: Even if we cannot extract the CSs from the learned classifier f\u03b8 explicitly, we know that it implicitly represents them. Thus, we can use the classifier itself to recognize the true constraints for any problem instance. Our generate-and-test approach does so by generating a set of candidate constraints BT for the target problem T, using the language \u0393 as described above. For relations with constants, the set P provides candidate values. To decide which of the constraints from BT to use, each of them is featurized, and the function f\u03b8 predicts whether it should be part of the model. We keep all constraints with positive classification:\n$C_T = {c | c \\in B_T \\wedge f_\\theta(\\phi_\\sigma(c,P)) = True}$  (3)"}, {"title": "Experimental Evaluation", "content": "We now experimentally evaluate GENCON, using ground CSPs of different instances on a variety of benchmarks. We evaluate our approach both when the given sets of constraints are correct and when noise exists. Noisy CSPs can result when the ground CSPs were themselves acquired using passive CA, on a noisy dataset of solutions and non-solutions, or on a dataset containing too few examples. We recognize two different types of noise in our setting:\n1. False positive (FP) noise, where the input set of constraints is not sound, also including wrong constraints.\n2. False negative (FN) noise, where the input set of constraints is not complete, missing some true constraints.\nWe aim to answer the following experiment questions:\n(Q1) To what extent does GENCON effectively generalize ground CSPs?\n(Q2) What is GENCON's performance when the input set of constraints also includes wrong constraints.\n(Q3) What is GENCON's performance when the input set of constraints does not include all true constraints.\nExperimental Setup\nBenchmarks. We focused on using benchmarks that have different constraint specifications so that our method is evaluated in distinct cases.\nMetrics. We evaluate each method by identifying the correctly generated constraints and the number of constraints missing from the model of the target instance(s). Based on that, we define as True Positives (TP) the correctly identified constraints, as False Positives (FP) the incorrectly identified constraints, and as False Negatives (FN) the missing constraints. Using the defined concepts, our evaluation is based on the following metrics that are common in ML:\n\u2022 Precision (Pr): It measures the accuracy of the identified constraints in the target instance. A high precision score signifies a low rate of false positives. When the precision score is 100%, the predicted set of constraints is sound.\n\u2022 Recall (Re): It measures the method's ability to identify all relevant constraints. A high recall score indicates a low rate of false negatives. When the recall score is 100%, the predicted set of constraints is complete.\nComparison.\nResults\nQ1: To what extent does GENCON effectively generalize ground CSPs? Figure 3 shows the results of GENCON using different classifiers, and of COUNT-CP's generalization. Both the extraction of CSs for DT and CN2, as well as generate-and-test for the other classifiers, achieve high precision and recall in all benchmarks. The only exception is NB, which gets lower recall in ET and NR, and significantly lower precision in Sudoku. We believe that this is due to its feature independency assumption, which makes it hard for the classifier to recognise the relationship of the different features in the CSs. The benchmark that turned out to be the most difficult for all methods was ET, with the difficulty being recognizing the parameter of the CS regarding the different_day constraints. The problem occurred in instances with many parameters with the same value, with all of them being recognized as part of the different_day\nQ2: What is the impact of FP noise in the performance of GENCON?\nQ3: What is the impact of FN noise on the performance of GENCON?"}, {"title": "Conclusions", "content": "CP models are typically defined by parameterized specifications, rather than a flat list of ground constraints. However, most CA methods focus on learning a single ground CSP for a specific instance. Our work addresses this limitation by generalizing ground CSPs to parameterized models using a constraint-level classification approach named GENCON. We showed how interpretable CSs can be derived from decision rules, and introduced a generate-and-test method for non-interpretable classifiers. Our evaluation indicates that GENCON achieves high accuracy and robustness, even for high levels of noise, highlighting the potential of ML-based techniques for generalizing constraint models and making CA more robust. We recommend using decision trees as the classifier of choice, as they facilitate the extraction of interpretable CSs while presenting strong performance.\nPromising avenues for future work include exploring active learning to enhance generalization; and using generalization during interactive constraint learning to reduce queries, leveraging also the noise robustness demonstrated here. Additionally, GENCON can be applied during passive CA, enabling the learning of constraint models from a limited amount of solutions and non-solutions across various instances, a scenario common in real-world applications."}, {"title": "Appendix", "content": "Feature representation used\nAn overview of the feature representation we used in our implementation is given in Table 1.\nRelation features The first feature represents the constraint's relation, such as equality, inequality, or a specific function name. The second feature indicates whether at least one constant value is present in the constraint. The remaining relation features represent numerical values of the constants. Since the list of features must be of fixed size, we include as many features for constants as the largest number of constants present in any constraint from the input set of constraints CA. For constraints with fewer constants, the unnecessary features get \"NaN\" values.\nPartitioning features In many cases, the variables V are given in the form of a matrix or tensor. The dimensions of such a tensor, and the index of each variable in them, often play a crucial role in the partitions within the problem. For example, the same constraint may be defined on all groups of variables that are part of the same row or column of tensor. Thus, we include information about the indices of the variables in these dimensions. We include as many index features as the largest number of dimensions over all variables, again using \"NaN\" values for constraints whose variables have fewer dimensions.\nBesides using only the dimensions present in the variables tensor, we also aim to identify \"latent dimensions\" that can be indirectly derived from the problem parameters P.\nConditioning features There may be sequence conditions that restrict which combinations of variables the constraints should apply to within the variable partition.\nBenchmark details\nSudoku\nGolomb rulers.\nExam Timetabling\nNurse rostering"}]}