{"title": "Hypothesis Testing the Circuit Hypothesis in LLMs", "authors": ["Claudia Shi", "Nicolas Beltran-Velez", "Achille Nazaret", "Carolina Zheng", "Adri\u00e0 Garriga-Alonso", "Andrew Jesson", "Maggie Makar", "David M. Blei"], "abstract": "Large language models (LLMs) demonstrate surprising capabilities, but we do\nnot understand how they are implemented. One hypothesis suggests that these\ncapabilities are primarily executed by small subnetworks within the LLM, known\nas circuits. But how can we evaluate this hypothesis? In this paper, we formalize\na set of criteria that a circuit is hypothesized to meet and develop a suite of\nhypothesis tests to evaluate how well circuits satisfy them. The criteria focus on\nthe extent to which the LLM's behavior is preserved, the degree of localization\nof this behavior, and whether the circuit is minimal. We apply these tests to\nsix circuits described in the research literature. We find that synthetic circuits\ncircuits that are hard-coded in the model \u2013 align with the idealized properties.\nCircuits discovered in Transformer models satisfy the criteria to varying degrees.\nTo facilitate future empirical studies of circuits, we created the circuitry package,\na wrapper around the TransformerLens library, which abstracts away lower-level\nmanipulations of hooks and activations. The software is available at https:\n//github.com/blei-lab/circuitry.", "sections": [{"title": "Introduction", "content": "The field of mechanistic interpretability aims to explain the inner workings of large language models\n(LLMs) through reverse engineering. One promising direction is to identify \u201ccircuits\u201d that correspond\nto different tasks. Examples include circuits that perform context repetition [Olsson et al., 2022],\nidentify indirect objects [Wang et al., 2023], and complete docstrings [Heimersheim and Janiak, 2023].\nSuch research is motivated by the circuit hypothesis, which posits that LLMs implement their\ncapabilities via small subnetworks within the model. If the circuit hypothesis holds, it would be\nscientifically interesting and practically useful. For example, it could lead to valuable insights about\nthe emergence of properties such as in-context learning [Olsson et al., 2022] and grokking during\ntraining [Stander et al., 2023, Nanda et al., 2023b]. Moreover, identifying these circuits could aid in\nexplaining model performance and controlling model output, such as improving truthfulness.\nIn this work, we empirically study the circuit hypothesis to assess its validity in practice. We begin\nby defining the ideal properties of circuits, which we posit to be: 1. Mechanism Preservation:\nThe performance of an idealized circuit should match that of the original model. 2. Mechanism"}, {"title": "Mechanistic Interpretability and LLMS", "content": "In this section, we define the necessary ingredients for mechanistic interpretability in LLMs.\n2.1 LLMs as computation graphs\nA Transformer-based LLM is a neural network that takes in a sequence of input tokens and produces\na sequence of logits over possible output tokens. We define it as a function $M : \\mathcal{X} \\rightarrow \\mathcal{O}$, where\n$\\mathcal{X} = \\{(x_1,...,x_L) | x^\\nu \\in \\mathcal{V}, L \\in \\mathbb{Z}_{>1}\\}$ is the space of sequences of tokens, $\\mathcal{V}$ is the space of\npossible tokens, called the vocabulary, and $\\mathcal{O} = \\{(o^1, . . .,o^L)| o^\\nu \\in \\mathbb{R}^{|\\mathcal{V}|}, L \\in \\mathbb{Z}_{>1}\\}$ is the space of\nsequences of logits over the vocabulary."}, {"title": "Tasks: measuring the performance of a model", "content": "To measure whether a particular model performs a specific function, we define a task, $r$, as a tuple\n$\\mathcal{T} = (\\mathcal{D}, s)$ of a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ and a score $s : \\mathcal{O} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$. The dataset $\\mathcal{D}$ contains pairs\nof inputs $x \\in \\mathcal{X}$ and output $y_i \\in \\mathcal{Y}$. The score maps a sequence of logits, such as the output of the\nmodel $M(x_i)$, and the ground truth information $y_i$ to a real number indicating the performance of the\nmodel's output on that particular example: a higher score indicates better performance.\nExample 1 (Greater-Than). An example task is the greater-than operation [Hanna et al., 2023],\nwhere we evaluate whether the model can perform this task as it would appear in natural language.\nThe dataset $\\mathcal{D}$ contains inputs from $x_i =$ \"The noun lasted from the year XXYY to the year XX\u201d where\nnoun is an event, e.g \"war\", XX is a century, e.g. 16, and YY is a specific year in the century. The\nscore function is the difference in assigned probabilities between the years smaller than $y_i = \\text{YY}$ and\nthe years greater than or equal to $y_i$. The implied task is to predict the next token $YY'$ as any year\ngreater than YY so as to respect chronological order.\nA circuit is a subgraph $\\mathcal{C} = (\\mathcal{V}_c, \\mathcal{E}_c)$ of the computational graph $\\mathcal{G}$. It includes the input and output\nnodes and a subset of edges, $\\mathcal{E}_c$, that connect the input to the output. We let $\\mathfrak{C}$ denote the space\nof all circuits. Fig. 1 depicts one such circuit in a simplified computational graph of a two-layer\nattention-only Transformer. Given a circuit, we define its complement $\\overline{\\mathcal{C}}$ to be the subgraph of $\\mathcal{G}$\nthat includes all edges not in C and their corresponding nodes."}, {"title": "Circuits of an LLM", "content": "A circuit specifies a valid subgraph, but it is not sufficient to specify a runnable model. Recall that a\nnode $v$ in the circuit is a function with a collection of inputs $a_u$ corresponding to each $(u, v)$ present\nin E. If the edge $(u, v)$ is removed, then what input $a_u$ should be provided to node $v$?\nOne solution, called activation patching, is to replace all inputs $a_u$ with an alternative value $a'_u$, one\nfor each edge $(u, v) \\in E$ that is absent from the circuit.\nThere are various ways to choose a value for $a'_u$. Two common approaches are zero ablation, which\nsets $a'_u$ to 0 [Olsson et al., 2022], and Symmetric Token Replacement (STR) patching [Chan et al.,\n2022, Geiger et al., 2024, Zhang and Nanda, 2024]. STR sets $a'_u$ differently for each input $x_i$ and\nproceeds as: First, create a corrupted input $x'_i$, which should be like $x_i$ but with key tokens changed\nto semantically similar ones. For example, in the greater-than task with input $x_i =$ \u201cThe war lasted\nfrom the year 1973 to the year 19", "The war lasted from the year 1901\nto the year 19": "The meaning is preserved but the $\\geq 73$ constraint is removed. Then, run the model\non $x'_i$ and cache all the activations $a'_u$. Finally, run the circuit on $x_i$, replacing the input $a_u$ of $v$ with\nthe cached $a'_u$ for all edges $(u, v) \\in E \\setminus E_c$ iteratively until reaching at the output node.\nWe use the notation $\\mathcal{C}'(x)$ to denote the output of the circuit $\\mathcal{C}$ on the input $x$, where the ablation\nscheme is implicit. When we compute the output of the complement of the circuit, namely $\\overline{\\mathcal{C}}(x)$, we\nsay that we knock out the circuit $\\mathcal{C}$ from the model $M$."}, {"title": "Evaluation metric: faithfulness", "content": "Given a circuit $\\mathcal{C}(x)$ and a task $\\tau = (\\mathcal{D}, s)$, we can use the score function to evaluate how well\nthe circuit performs the task. However, in mechanistic interpretability, the goal is often to evaluate\nwhether the circuit replicates the behavior of the model, which is known as faithfulness.\nWe define a faithfulness metric, $F : \\mathfrak{C} \\times \\overline{\\mathfrak{C}} \\times \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$, that maps two circuits and an example\nin $\\mathcal{D}$ to a real number measuring the similarity in behavior of these two circuits with respect to this\nparticular example. We then define the faithfulness of circuit $\\mathcal{C}$ to model $M$ on task as\n$F_\\mathcal{T}(M, \\mathcal{C}) := \\mathbb{E}_{(\\mathcal{X}, \\mathcal{Y}) \\sim \\mathcal{D}} [F(M, \\mathcal{C}, \\mathcal{X}, \\mathcal{Y})].$  (1)\nWe call $F_\\mathcal{T}(M, \\mathcal{C})$ the faithfulness score of circuit C. For example, a faithfulness metric could be the\n$l^k$ norm between the score of the model and the score of the circuit,\n$F(M, \\mathcal{C}, x, y) = |s(M(x), y) - s(\\mathcal{C}(x), y)|^k,$\nwith $k \\in \\{1, 2\\}$. However, F can be more general and non-symmetric, such as the KL divergence\nbetween the logits of M and $\\mathcal{C}$ [Conmy et al., 2023]. Following convention, a lower value for\n$F_\\mathcal{T}(M, \\mathcal{C})$ means the circuit is more faithful to the model."}, {"title": "Hypothesis Testing on Circuits", "content": "In this section, we develop three tests that formalize the following idealized criteria for a circuit: 1.\nMechanism Preservation: The circuit should approximate the original model's performance on the\ntask. 2. Mechanism Localization: The circuit should include all information critical to the task's\nexecution. 3. Minimality: The circuit should be as small as possible.\nIn \u00a7 3.1, we discuss three idealized but stringent hypotheses implied by these criteria and develop tests\nfor each. Then, in \u00a7 3.2, we develop two flexible tests for Mechanism Localization and Mechanism\nPreservation, allowing users to design the null hypotheses and determine to what extent a circuit\naligns with the idealized properties.\nStandard hypothesis testing has 5 components:\n1. A variable of interest, $Z^*$, e.g., the faithfulness of the candidate circuit.\n2. A reference distribution $\\mathbb{P}_Z$ over other Z that we wish to compare $Z^*$ to, along with $n$\nsamples $(z_i)_{i=1}^n$ from it, e.g., the faithfulness of n randomly sampled circuits.\n3. A null hypothesis $H_0$, which relates $Z^*$ and $\\mathbb{P}_Z$ and which we assume holds true. E.g., $H_0$:\n\"the candidate circuit is less faithful than 90% of random circuits from $\\mathbb{P}_Z$.\""}, {"title": "Idealized tests", "content": "We develop three tests, Equivalence, Independence, and Minimality, which are direct implications of\nthe idealized criteria. These tests are designed to be stringent: if a circuit passes them, it provides\nstrong evidence that the circuit aligns with the idealized criteria.\nWe assume we have a model M, a task $\\tau = (\\mathcal{D}, s)$ with a score function s, and a faithfulness metric\nF. We are then given a candidate circuit $\\mathcal{C}^*$ to evaluate.\nEquivalence. Intuitively, if $\\mathcal{C}^*$ is a good approximation of the original model M, then $\\overline{\\mathcal{C}}^*$ should\nperform as well as M on any random task input. Hence, the difference in task performance between\nM and $\\overline{\\mathcal{C}}^*$ should be indistinguishable from chance. We formalize this intuition with an equivalence\ntest: the circuit and the original model should have the same chance of outperforming each other.\nWe write the difference in the task performance between the candidate circuit and the original model\non one task datapoint $(x, y)$ as $\\Delta(x, y) = s(\\mathcal{C}^*(x); y) - s(M(x); y)$, and let the null hypothesis be\n$\\mathcal{H}_0: \\mathbb{P}_{(\\mathcal{X}, \\mathcal{Y})\\sim \\mathcal{D}}\\left( \\Delta (\\mathcal{X}, \\mathcal{Y}) > 0 \\right) - \\frac{1}{2} < \\epsilon,$ (2)\nwhere $\\epsilon > 0$ specifies a tolerance level for the difference in performance.\nTo test this hypothesis, we use a nonparametric test designed specifically for null hypotheses like\n$\\mathcal{H}_0$. The test statistic is the number of times $\\mathcal{C}^*$ and M outperform each other. We provide a detailed\ndescription of the test in \u00a7 B.1.\nSince $\\mathcal{H}_0$ is in the idealized direction, if we reject the null, we claim with confidence $1-\\alpha$,\nNon-Equivalence: $\\mathcal{C}^*$ and $M$ are unlikely to be equivalent on random task data.\nIndependence. If a circuit is solely responsible for the operations relevant to a task, then knocking\nit out would render the complement circuit unable to perform the task. An implication is that the\nperformance of the complement circuit is independent of the original model on the task.\nTo formalize this claim, we define the null hypothesis as\n$\\mathcal{H}_0: s(\\overline{\\mathcal{C}^*}(\\mathcal{X}); \\mathcal{Y}) \\perp\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!s(M(\\mathcal{X}); \\mathcal{Y}),$ (3)\nwhere the randomness is over X and Y.\nTo test this hypothesis, we use a permutation test. Specifically, we measure the independence between\nthe performance of the complement circuit and the performance of the original model by using the\nHilbert Schmidt Independence Criterion (HSIC) [Gretton et al., 2007], a nonparametric measure of\nindependence. We provide a formal definition of HSIC and describe the test in \u00a7 B.3.\nIf the null is rejected, it implies that the complement circuit and the original model's performances\nare not independent. We claim with confidence $1-\\alpha$,\nNon-Independence: Knocking out the candidate circuit does not remove all the information\nrelevant to the task that is present in the original model."}, {"title": "Flexible tests", "content": "\u00a7 3.1 presents stringent tests that align with the idealized versions of circuits. Passing any of these tests\nis a notable achievement for any circuit. Here, we consider two flexible ways of testing mechanism\npreservation (sufficiency) and mechanism location (partial necessity).\nInstead of comparing the candidate circuit to the original model, we compare $\\mathcal{C}^*$ against random\ncircuits drawn from a reference distribution. Different definitions of the reference distribution\nmodulate the difficulty of the tests. We demonstrate that by varying the definition of the reference\ndistribution, we can determine the extent to which the circuit aligns with the idealized criteria.\nSufficiency. For the sufficiency test, we ask whether the candidate circuit is particularly faithful to\nthe original model compared to a random circuit from a reference distribution.\nThe variable of interest is the faithfulness of $\\mathcal{C}^*$ to $M$, $Z^* = F_\\mathcal{T}(M, \\mathcal{C}^*)$. We define the reference\ndistribution $\\mathbb{P}_Z$ as the distribution of $Z = F_\\mathcal{T}(M, \\mathcal{C}_r)$ induced by sampling random circuits $\\mathcal{C}_r$ from\na chosen distribution $\\mathfrak{R}$. The null hypothesis is\n$\\mathcal{H}_0: \\mathbb{P}_{\\mathcal{C}_R \\sim \\mathfrak{R}}(F_\\mathcal{T}(M, \\mathcal{C}^*) < F_\\mathcal{T}(M, \\mathcal{C}_r)) \\leq q^*,$ (6)\nwhere $q^*$ is a prespecified quantile.\nPartial necessity. If the candidate circuit is responsible for solving a task in the model, then\nremoving it will impair the model's ability to perform the task. However, this impairment may not be\nso severe as to make the model entirely independent of the complement circuit's output as tested in\nthe independence test.\nInstead, we define partial necessity: compared to removing a random reference circuit, removing the\ncandidate circuit significantly reduces the model's faithfulness. The null hypothesis is\n$\\mathcal{H}_0: \\mathbb{P}_{C_r \\sim \\mathfrak{R}}(\\text{C}^* \\text{ is worse to knock out than } C^r) < q^*,$\nwhere $q^* \\in (0, 1)$ is a user-chosen parameter and where \u201c$\\text{C}^* \\text{ is worse to knock out than } C^r$\u201d is short-\nhand for $F_\\mathcal{T}(M, \\overline{\\mathcal{C}^*}) > F_\\mathcal{T}(M, \\overline{\\mathcal{C}^r})$.\nSimilar to the sufficiency test, this hypothesis test is highly flexible in its design. An easier version\ninvolves using a reference distribution over circuits from the complement $\\overline{\\mathcal{C}^*}$ distribution. This\nallows us to determine whether the edges in the candidate circuit are particularly important for task\nperformance compared to a random circuit. Another approach is to define the reference distribution by\nsampling from the original model M, enabling us to assess whether the significance of a knockdown\neffect could have occurred by chance.\nThe test statistic is the proportion of times that $\\mathcal{C}^*$ is less faithful than $\\mathcal{C}_r$. Similar to the sufficiency\ntest, we apply a binomial test to get the p-value. If $\\mathcal{H}_0$ is rejected, we claim with confidence $1-\\alpha$,\nPartial necessity: The probability that knocking out $\\mathcal{C}^*$ damages the faithfulness to M more\nthan knocking out a random reference circuit is at least $q^*.$"}, {"title": "Empirical Studies", "content": "We apply hypothesis tests to six benchmark circuits from the literature: two synthetic and four\nmanually discovered. The synthetic circuits align with the idealized properties, validating our criteria."}, {"title": "Experimental setup", "content": "We use the experiment configuration from ACDC [Conmy et al., 2023] for all tasks and circuits and\nperform the ablations using TransformerLens [Nanda and Bloom, 2022]. Below, we briefly describe\neach task, with detailed explanations in \u00a7 D. We omit the greater-than (G-T) task as it was detailed in\n\u00a7 2. Both IOI and greater-than use GPT-2 small, while the other tasks use various small Transformers.\nIndirect Object Identification (IOI, Wang et al. 2023): The goal is to predict the indirect object in\na sentence containing two entities. For example, given the sequence \"When Mary and John went\nfor a walk, John gave an apple to\", the task is to predict the token \"Mary\". The score function is\nlogit(\" Mary\") \u2013 logit(\" John\u201d).\nInduction (Olsson et al. 2022): The objective is to predict B after a sequence of the form AB... A.\nFor example, given the sequence \u201cVernon Dursley and Petunia Durs", "2023)": "The objective is to predict the next variable name\nin a Python docstring. The score function is the logit difference between the correct answer and the\nmost positive logit over the set of alternative arguments.\nTracr (Lindner et al. 2024): For tracr-r, the goal is to reverse an input sequence. For tracr-p,\nthe goal is to compute the proportion of x tokens in the input. The score function is the l\u00b2 distance\nbetween the correct and predicted output. Both of these tasks have \u201cground truth"}, {"title": "Results", "content": "Below we report and analyze key findings across tests. Additional results are reported in \u00a7 E.\nIdealized tests. Table 1 presents the results for the six circuits across the three idealized hypothesis\ntests. The synthetic circuits (highlighted in grey) align well with our hypotheses, supporting the\nvalidity of these tests. Among the discovered circuits, alignment with the idealized hypotheses varies:"}, {"title": "Equivalence", "content": "The equivalence test evaluates whether the candidate circuit outperforms the original model at least\nhalf of the time. As shown in Table 1, none of the natural circuits passed the equivalence test. Table 3\nshow the test statistics \u2013 the proportion of inputs where the candidate circuit outperforms the original\nmodel - of all tasks. All circuits except IOI are much worse than the original model at the task. This\nmay be because circuits are only a small proportion of the original model. We omit the Tracr-based\ntasks because their performance is identical to the original model by design (they are ground truth\ncircuits). Thus, in their case, although the null hypothesis is true, the sign test can't be applied."}, {"title": "Independence", "content": "For the independence test, we consider retaining the null as passing the test. As shown in Table 1,\nthe only natural circuit that pass the independence test is induction heads, Tracr, the ground truth,\ncircuit does. Table 4 reports the results."}, {"title": "Minimality", "content": "To produce the results in Table 1, we set $q^* = 0.9$ and if there exist any edges deemed insignificant,\nwe reject the null hypothesis that the candidate circuit is minimal. We find that only the Induction\nand Tracr circuits pass the minimality test.\nIn Fig. 5, we plot the scores for knocking out each edge for each circuit. As the Tracr circuits are\nground truth circuits, all edges are significant relative to the reference distribution.\nFor the Induction circuit, all edges are also significant relative to the reference distribution. However,\nfor the other circuits, we find that a significant portion of the edges are insignificant. This is especially\nprevalent for the DS circuit, where less than half of the edges are significantly different from the\nreference distribution. This suggests that other than the Induction circuit, these circuits are not\nminimal. Unsurprisingly, for the IOI circuit, we see a few edges that can be removed with little\nimpact to performance, in agreement with Wang et al. [2023]."}, {"title": "Discussion & Limitations", "content": "Do existing circuits align with the circuit hypothesis? We develop a suite of idealized and flexible\ntests to empirically study this question. The results suggest that while existing circuits do not strictly\nadhere to the idealized hypotheses, they are far from being random subnetworks.\nOur tests successfully differentiate circuits by their alignment with the idealized properties, identifying\nthe Induction circuit [Conmy et al., 2023] as the most aligned. We also demonstrate the limitations\nof existing evaluation criteria, showing that the knockdown effect alone is insufficient to determine\ncircuit quality and that some benchmark circuits are not minimal.\nOur tests and empirical studies have several limitations. The idealized tests are stringent, while the\nflexible tests are sensitive to circuit size measurements and require careful null hypothesis design.\nFor the non-equivalence and non-independence tests, the desired direction is to retain the null, but we\ndid not empirically study the Type II error associated with these tests. Furthermore, the empirical\nstudy uses the original experimental setup, whereas existing work and our ablation studies show that\ncircuits are not robust to changes in the experimental setup.\nDespite these limitations, we believe the study provides an overview of the extent to which existing\ncircuits align with the idealized properties. We also believe that the tests will aid in developing new\ncircuits, improving existing circuits, and scientifically studying the circuit hypothesis."}, {"title": "Impact Statement", "content": "We present a suite of statistical tests to assess whether existing circuits align with the idealized version\nof circuits. When utilized appropriately, these tests can help identify new circuits, improve existing\ncircuits, and compare the quality across circuits. Thus, we expect these tests to improve the quality of\ncircuits reported in the mechanistic interpretability literature, making them more aligned with the\nidealized criteria.\nWe anticipate that the overall effect of this work will be to accelerate progress in mechanistic\ninterpretability and consequently improve our understanding of how LLMs work. This should\nfacilitate explaining and steering model behavior, and possibly \u201cdebugging\u201d learned models. At the\nsame time, an improved understanding of model internals may enhance architectures and accelerate\ncapabilities. Additionally, it can open the door to more sophisticated attacks and defenses for various\nthreat models.\nIt is important to note that our methodology is based on a hypothesis testing framework. Similar\nto other hypothesis-based tests, there is a potential for misuse or engagement in practices such\nas p-hacking by practitioners. Misapplication of these tests can lead to misleading assurances of\nrobustness that the circuits might not genuinely possess. Furthermore, we assume a fixed experimental\nsetup rather than considering generalization across different setups. The inferences we draw across\nexperimental setups can differ significantly.\nIf these tests are used to check mechanistic interpretability results for an application of AI, they may\ngive users or developers a misplaced sense of confidence in a faulty hypothesis about neural network\ninternals. However, we believe that this is already a danger with present results, and our work is an\nimprovement in this regard."}, {"title": "Equivalence Test", "content": "The null hypothesis for the equivalence test is defined as\n$\\mathcal{H}_0 : \\left| \\mathbb{P}(\\Delta (\\mathcal{X}, \\mathcal{Y}) > 0) - \\frac{1}{2} \\right| < \\epsilon,$ (8)\nwhere $\\epsilon > 0$ is a user-chosen tolerance parameter.\nGiven that $\\mathbb{1} \\{ \\Delta (\\mathcal{X}, \\mathcal{Y}) > 0 \\}$ is Bernoulli-distributed under the null hypothesis, we use the test\nstatistic\n$t= \\sum_{i} \\frac{1}{n} \\mathbb{1} \\{ \\Delta (x_i, y_i) > 0 \\} / \\frac{1}{2} ,$ (9)\nand choose rejection regions of the form $\\mathcal{R}_\\alpha = \\{ t > c(\\alpha) \\}$, where $c(\\alpha)$ is a yet-to-be defined\nfunction of $\\alpha$ ensuring that $\\mathbb{P}(\\mathcal{T} \\in \\mathcal{R}_\\alpha) \\leq \\alpha$. Intuitively, $c(\\alpha)$ increases (or remains constant) as\n$\\alpha$ decreases. Moreover, because $\\mathbb{P}(\\mathcal{T} \\in \\{t > C'\\}) = 1$ if $C' = 0$, and $\\mathbb{P}(\\mathcal{T} \\in \\{t > C'\\}) \\rightarrow 0$ as\n$C'\\rightarrow \\infty$, we know it must be possible to construct at least one function $c(\\alpha)$ so that the regions $\\mathcal{R}_\\alpha$\nsatisfy the requirements of a hypothesis test.\nLet $\\theta = \\mathbb{P} \\left( \\Delta (\\mathcal{X}, \\mathcal{Y}) > 0 \\right)$. By the definition of the hypothesis test and the null hypothesis, we\nrequire $\\mathbb{P}(\\mathcal{T} \\in \\mathcal{R}_\\alpha) \\leq \\alpha$ for all $\\theta \\in \\left[ \\frac{1}{2} - \\epsilon, \\frac{1}{2} + \\epsilon \\right]$. However, notice that for a fixed rejection region\n$\\mathcal{R}_\\alpha$ and any value $\\theta' \\in \\left[ \\frac{1}{2} - \\epsilon, \\frac{1}{2} + \\epsilon \\right]$,\n$\\mathbb{P}(\\mathcal{T} \\in \\mathcal{R}_\\alpha \\mid \\theta = \\theta') \\leq \\mathbb{P} \\left( \\mathcal{T} \\in \\mathcal{R}_\\alpha \\mid \\theta = \\frac{1}{2} + \\epsilon \\right).$ (10)\nHence, if we have a set $\\mathcal{R} = \\{t > C'\\}$, where $C'$ is some constant, R is a valid rejection region for\nany $\\alpha$ such that\n$\\alpha \\geq \\mathbb{P} \\left(\\mathcal{T} \\in \\mathcal{R} \\mid \\theta = \\frac{1}{2} + \\epsilon \\right).$ (11)\nNow, construct a function $c(\\alpha)$ such that $\\mathbb{P}(\\mathcal{T} \\in \\mathcal{R}_\\alpha) \\leq \\alpha$, and ensure that $c(\\alpha_p) = t_\\text{obs}$, where\n$\\alpha_p = \\mathbb{P} \\left(\\mathcal{T} \\geq t_\\text{obs} \\mid \\theta = \\frac{1}{2} + \\epsilon \\right),$(12)\nand $c(\\alpha) > c(\\alpha_p)$ for any $\\alpha < \\alpha_p$. We can construct one such function, for example, by letting\n$c(\\alpha) = t_\\text{obs}$ for $\\alpha > \\alpha_p$, which is admitted by Eq. 11, and by choosing valid values for any\n$\\alpha < \\alpha_p$, which is feasible because $\\mathbb{P}(\\mathcal{T} > C') \\rightarrow 0$ as $C' \\rightarrow \\infty$. Under this setup, the p-value is\n$\\alpha_p$. This follows from the fact that $t_\\text{obs} \\in \\mathcal{R}_\\alpha_p = \\{t > t_\\text{obs}\\}$, but for any $\\alpha < \\alpha_p$, $t_\\text{obs} \\notin \\mathcal{R}_\\alpha$ as\n$c(\\alpha) > c(\\alpha_p) = t_\\text{obs}$.\nFinally, we can compute the p-value analytically by using the Bernoulli distribution for $\\mathbb{1} \\{ \\Delta(x_i, y_i) >\n0 \\}$ with parameter $\\theta = 1/2 + \\epsilon$,\n$\\alpha_p = \\sum_{k \\in [n] : \\frac{k}{n} \\geq t_\\text{obs}} \\binom{n}{k} \\left(\\frac{1}{2} + \\epsilon \\right)^k \\left(1 - \\left(\\frac{1}{2} + \\epsilon \\right) \\right)^{n-k}.$ (13)\nAn important clarification is that we could have chosen the test statistic to be the estimated value of $\\theta$\nnamely, $\\sum_i \\mathbb{1} \\{ \\Delta(x_i, y_i) \\} /n$ and change the rejection region. In the main text we choose to express\nit this way for clarity of exposition."}, {"title": "Quantile Test", "content": "We provide the details of the quantile test used for testing sufficiency, partial necessity, and minimality\nin Algorithm 1. We state it generally but assume that it would be instantiated for each of the above"}, {"title": "Independence Test", "content": "We provide details for the independence test used for the partial necessity test. To measure the\nindependence between two variables, we use the Hilbert Schmidt Independence Criterion (HSIC)\n[Gretton et al., 2007"}]}