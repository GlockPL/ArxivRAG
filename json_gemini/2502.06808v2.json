{"title": "ON THE BENEFITS OF ATTRIBUTE-DRIVEN GRAPH DOMAIN ADAPTATION", "authors": ["Ruiyi Fang", "Bingheng Li", "Zhao Kang", "Qiuhao Zeng", "Nima Hosseini Dashtbayaz", "Ruizhi Pu", "Boyu Wang", "Charles Ling"], "abstract": "Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. Specifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscore the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmark verify the effectiveness of our method.", "sections": [{"title": "INTRODUCTION", "content": "In the area of widespread internet data collection, graph vertices are frequently associated with content information, referred to as node attributes within basic graph data. Such graph data can be widely used in prevalent real-world applications, with data suffering from label scarcity problems in annotating complex structured data is both expensive and difficult (Xu et al., 2022). To solve such a challenge, transferring abundant labeling knowledge from task-related graphs is a method considered (Chen et al., 2019). Giving labeled graphs as a source to solve unlabeled graph targets has been proposed as graph domain adaptation (GDA) as a paradigm to effectively transfer knowledge across graphs by addressing distribution shifts (Shi et al., 2024).\nEarly works on GDA apply deep domain adaptation (DA) techniques directly, thereby (Shen et al., 2020b; Shui et al., 2023; Wu et al., 2020; Shen et al., 2020a; Dai et al., 2022) without considering the topological structures of graphs for domain alignment. To address this issue, several recent works have been proposed to leverage the inherent properties of graph topology (e.g., adjacency matrix). While these methods (Yan & Wang, 2020; Shi et al., 2023; Shen et al., 2023; Wu et al., 2023) have achieved substantial improvements by alleviating the topological discrepancy between domains, they overlook the importance of node attributes, a fundamental aspect of GDA. To verify our argument, we investigate the projected feature values\u00b9 of graph topology and attribute on"}, {"title": "RELATED WORK", "content": "Unsupervised domain adaptation is a wildly used setting of transfer learning methods that aims to minimize the discrepancy between the source and target domains. To solve cross-domain classification tasks, these methods are based on deep feature representation (Zhu et al., 2022; Zeng et al., 2024a), which maps different domains into a common feature space. Some recent studies have overcome the imbalance of domains and the label distribution shift of classes to transfer model well (Jing et al., 2021; Xu et al., 2023; Pu et al., 2025; Zeng et al., 2024b). Some novel settings in domain adaption have also gotten a lot of attention, like source free domain adaption(SFDA) (Yang et al., 2021; Xu et al., 2025), test time domain adaption(TTDA) (Wang et al., 2022). As for graph-structured data, several studies have been proposed for cross-graph knowledge transfer via GDA setting methods (Shen & Chung, 2019; Dai et al., 2022; Shi et al., 2024). ACDNE (Shen et al., 2020a) adopt k-hop PPMI matrix to capture high-order proximity as global consistency for source information on graphs. CDNE (Shen et al., 2020b) learning cross-network embedding from source and target data to minimize the maximum mean discrepancy (MMD) directly. GraphAE (Yan & Wang, 2020) analyzes node degree distribution shift in domain discrepancy and solves it by aligning message-passing routers. DM-GNN (Shen et al., 2023) proposes a method to propagate node label information by combining its own and neighbors' edge structure. UDAGCN (Wu et al., 2020) develops a dual graph convolutional network by jointly capturing knowledge from local and global levels to adapt it by adversarial training. ASN (Zhang et al., 2021) separates domain-specific and domain-invariant variables by designing a private en-coder and uses the domain-specific features in the network to extract the domain-invariant shared features across networks. SOGA (Mao et al., 2024) first time uses discriminability by encouraging the structural consistencies between target nodes in the same class for the SFDA in the graph. GraphAE (Guo et al., 2022) focuses on how shifts in node degree distribution affect node embeddings by minimizing the discrepancy between router embedding to eliminate structural shifts. SpecReg (You et al., 2022) used the optimal transport-based GDA bound for graph data and discovered that revising the GNNs' Lipschitz constant can be achieved by spectral smoothness and maximum frequency response. JHGDA (Shi et al., 2023) studies the shifts in hierarchical graph structures, which are inherent properties of graphs by aggregating domain discrepancy from all hierarchy levels to derive a comprehensive discrepancy measurement. ALEX (Yuan et al., 2023) first creates a label shift enhanced augmented graph view using a low-rank adjacency matrix obtained through singular value decomposition by driving contrasting loss. SGDA (Qiao et al., 2023) enhances original source graphs by integrating trainable perturbations (adaptive shift parameters) into embeddings by conducting adversarial learning to simultaneously train both the graph encoder and perturbations, to minimize marginal shifts."}, {"title": "THEORETICAL ANALYSIS", "content": "In this subsection, we provide a discussion on the PAC-Bayesian analysis with the graph domain adaptation.\nNotations. An undirected graph $G = {V,E, A, X, Y}$ consists of a set of nodes $V$ and edges $E$, along with an adjacency matrix $A$, a feature matrix $X$, and a label matrix $Y$. The adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$ encodes the connections between $N$ nodes, where $A_{ij} = 1$ indicates an edge between nodes $i$ and $j$, and $A_{ij} = 0$ means the nodes are not connected. The feature matrix $X \\in \\mathbb{R}^{N \\times d}$ represents the node features, with each node described by a $d$-dimensional feature vector. Finally, $Y\\in \\mathbb{R}^{N\\times C}$ contains the labels for the $N$ nodes, where each node is classified into one of $C$ classes.\nIn this work, we explore the task of node classification in a unsupervised setting, where both the node feature matrix $X$ and the graph structure $A$ are given before learning. We assume that all key aspects of our analysis are conditioned on the fixed graph structure $A$ and feature matrix $X$, while the uncertainty arises from the node labels $Y$. Specifically, we assume that the label $y_{i}$ for each node $i \\in V$ is drawn from a latent conditional distribution $P_r(Y_{i} | Z_{i})$, where $Z = f(X, G)$, with $f$ being an aggregation function that combines features from the local neighborhood of each node within the graph. Additionally, we assume that the labels for different nodes are independent of each other, given their respective aggregated feature representations $Z_{i}$. With a partially labeled node set $V_{0} \\subseteq V$, our objective in the node classification problem is to learn a model $h : \\mathbb{R}^{N\\times d} \\times G^{N} \\rightarrow \\mathbb{R}^{N\\times C}$ from a family of classifiers $H$ that can predict the labels for the remaining unlabeled nodes. For a given classifier $h$, the predicted label $Y_{i}$ for node $i$ is determined by: $Y_{i} = \\arg \\max_{k\\in {1,...,c}} h_{i}(X, G)[k]$,"}, {"title": "ATTRIBUTE-DRIVEN ALIGNMENT", "content": "To make the attribute view fully learnable, we design the attention attribute module to dynamically utilize the important attribute. Specifically, we design learnable domain adaptive models for alignment embeddings in topology and attribute views.\nAttention-based Attribute To guide the network to take more attention to the important node attributes and make attributes learnable, we design attention-based embedding models. Specifically, we map the node attributes into three different latent spaces. By given an example in source graph attribute embedding: $Q = W_{q}Z_{s}^{att} , K = W_{k}Z_{s}^{att}, M = W_{v}Z_{s}^{att}$, where $W_{q} \\in \\mathbb{R}^{d \\times d}, W_{k} \\in \\mathbb{R}^{d \\times d}, W_{v} \\in \\mathbb{R}^{d \\times d}$ are the learnable parameter matrices. And $Q \\in \\mathbb{R}^{d\\times N}, K\\in \\mathbb{R}^{d\\times N}$ and $M\\in \\mathbb{R}^{d\\times N}$ denotes the query matrix, key matrix and value matrix, respectively.\nThe attention-based attribute matrix $att^{s}$ can be calculated by:\n$att^{s} = softmax(\\frac{K^{T}Q}{\\sqrt{d}}) M^{T}$,\nLikewise, we can obtain a similar objective of each learnable graph embedding $att^{s}, att^{t}$ and $att^{T}$.\nCross-view Similarity Matrix Refinement\nSubsequently, the cross-view similarity matrix $S^{s}$ represents the similarity between the source attribute and topology graph. $S^{T}$ represents the similarity between the target attribute and topology graph. $S^{s}$ as formulated:\n$S^{s} = \\frac{Z^{t} (Z_{s}^{att})^{T}}{||Z^{t}||_{2} ||Z_{s}^{att}||_{2}}$\nLikewise, we can obtain a similarity matrix of the target graph by:\n$S^{T} = \\frac{Z^{T} (Z_{t}^{att})^{T}}{||Z^{T}||_{2} ||Z_{t}^{att}||_{2}}$\nwhere $S^{s}$ and $S^{T}$ is the cross-view similarity matrix, and $(\u00b7)$ is the function to calculate similarity. Here, we adopt cosine similarity Qian et al. (2024). The proposed similarity matrix $S^{s}$ and $S^{T}$ measures the similarity between samples by comprehensively considering attribute and structure information. The connected relationships between different nodes could be reflected by $S^{s}$ and $S^{T}$. Therefore, we utilize $S^{T}$ and $S^{s}$ to refine the structure in augmented view with Hadamard product, $att^{s}$ can be formulated as:\n$att^{s} = att \\odot S^{s}$\nSimilarly we can get $att^{s}$ by $att \\odot S^{s}$, $att^{T}$ by $att \\odot S^{T}$, $att^{T}$ by $att^{T} \\odot S^{T}$, which respectively represent source graph and target graph in both topology view and attribute view embedding.\nAttribute-Driven Domain Adaptive\nThe proposed framework follows the transfer learning paradigm, where the model minimizes the divergence of the two views. In detail, GAA jointly optimizes two views of GDA alignment. To be specific, $L_{A}$ is the Mean Squared Error (MSE) loss between the source graph $att^{s}$ and $att^{t}$ and the target graph $att^{t}$ and $att^{T}$, which can be formulated as:\n$L_{A} = (||att^{s} - att^{t}||_{2} + ||att^{t} - att^{T}||_{2})$\nWe adapt the domain in two views, domain classifier loss in the topology view is $||att^{s} - att^{t}||_{2}$ enforces that the attribute graph node representation after the node feature extraction and similarity matrix refinement from source and target graph $G_{s}^{f}$ and $G_{t}^{f}$. Similarly, we get $||att^{s} - att^{t}||_{2}$"}]}