{"title": "How Expressive are Knowledge Graph Foundation Models?", "authors": ["Xingyue Huang", "Pablo Barcel\u00f3", "Michael M. Bronstein", "\u0130smail \u0130lkan Ceylan", "Mikhail Galkin", "Juan L Reutter", "Miguel Romero Orth"], "abstract": "Knowledge Graph Foundation Models (KGFMS) are at the frontier for deep learning on knowledge graphs (KGs), as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMS remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the motifs that are used to learn the relation representations. We then observe that the most typical motifs used in the existing literature are binary, as the representations are learned based on how pairs of relations interact, which limits the model's expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, which necessitate learning relation representations based on, e.g., how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.", "sections": [{"title": "1. Introduction", "content": "Knowledge Graph Foundation Models (KGFMs) gained significant attention (Galkin et al., 2024; Mao et al., 2024) for their success in link prediction tasks involving unseen entities and relations on knowledge graphs (KGs). These models aim to generalize across different KGs by effectively learning relation invariants: properties of relations that are transferable across KGs of different relational vocabularies (Gao et al., 2023). KGFMs learn representations of relations by relying on their structural roles in the KG, which can be transferred to novel relations based on their \"structurally similarities\" to the observed relations.\nExample 1.1. Consider the two KGs from Figure 1 which use disjoint sets of relations. Suppose that the model is"}, {"title": "2. Related work", "content": "Transductive and inductive (on node) link prediction.\nLink prediction on KGs has been extensively studied in the literature. Early approaches like TransE (Bordes et al., 2013), RotatE (Sun et al., 2019), and BoxE (Abboud et al., 2020) focus on the transductive setting, where learned entity embeddings are fixed, and thus inapplicable to unseen entities at test time. Multi-relational GNNs such as RGCN (Schlichtkrull et al., 2018) and CompGCN (Vashishth et al., 2020) remain transductive as they store entity and relation embeddings as parameters. To overcome this limitation, Teru et al. (2020) introduce GraIL, which enables inductive link prediction via the labeling trick. NBFNet (Zhu et al., 2021), A*Net (Zhu et al., 2023), RED-GNN (Zhang & Yao, 2022), and AdaProp (Zhang et al., 2023), provide improvements by leveraging conditional message-passing, which is provably more expressive (Huang et al., 2023). These models, once trained, can only be applied to KGs with the same relational vocabulary, limiting their applicability to graphs with unseen relations.\nInductive (on node and relation) link prediction.\nINGRAM (Lee et al., 2023) was one of the first approaches to study inductive link prediction over both new nodes and unseen relations by constructing a weighted relation graph to learn new relation representations. Galkin et al. (2024) extended this idea with the ULTRA architecture, which constructs a multi-relational graph of fundamental relations and leverages conditional message passing to enhance performance. ULTRA was among the first KGFMs to inspire an entire field of research (Mao et al., 2024). Concurrently, RMPI (Geng et al., 2023) explored generating multi-relational graphs through local subgraph extraction while also incorporating ontological schema. Gao et al. (2023) introduced the concept of double-equivariant GNNs, which establish invariants on nodes and relations by leveraging subgraph GNNs in the proposed ISDEA framework to enforce double equivariance precisely. MTDEA (Zhou et al., 2023a) expands this framework with an adaptation procedure for multi-task generalization. Further, TRIX (Zhang et al., 2024) expands on ULTRA with recursive updates of relation and entity embeddings. Finally, KG-ICL (Cui et al., 2024) introduced a new KGFM utilizing in-context learning with a unified tokenizer for entities and relations.\nLink prediction on relational hypergraphs. Relational hypergraphs are a generalization of KGs used to represent higher-arity relational data. Work on link prediction in relational hypergraphs first focused on shallow embeddings (Wen et al., 2016; Liu et al., 2020; Fatemi et al., 2020), and later G-MPNN (Yadati, 2020) and RD-MPNNs (Zhou et al., 2023b) advanced by incorporating message passing. Recently, Huang et al. (2024) conducted an in-depth expressivity study on these models and proposed HCNets, extending conditional message-passing to relational hypergraphs and achieving strong results on inductive link prediction."}, {"title": "3. Preliminaries", "content": "Knowledge graphs. A knowledge graph (KG) is a tuple $G = (V, E, R)$, where V is a set of nodes, R is the set of"}, {"title": "4. MOTIF: A general framework for KGFMs", "content": "We present MOTIF, as a very general framework for KGFMs. Given a KG $G = (V, E, R)$, MOTIF computes an encoding for each potential link q(u, v) in G following three steps:\n1. LIFT: Use a set F of motifs to compute a relational hypergraph $LIFT_F(G) = (V_{LIFT}, E_{LIFT}, R_{LIFT})$, using a procedure called LIFT.\n2. RELATION ENCODER: Apply a relation encoder on the hypergraph $LIFT_F(G)$ to obtain relation representations.\n3. ENTITY ENCODER: Use the relation representations from Step 2 and apply an entity encoder on the KG G to obtain final link encodings."}, {"title": "4.1. LIFT", "content": "A graph motif is a pair $P = (G_M, r)$, where $G_M = (V_M, E_M, R_M)$ is a connected KG and r is a tuple defining an order on the relation set $R_M$. The information extracted by motifs is defined via homomorphisms.\nDefinition 4.1. Let G be a KG and $P = (G_M, r)$ be a motif with $r = (r_1,..., r_n)$. The evaluation $Eval(P, G)$ of P over G is the set of tuples $(\\phi(r_1), ..., \\phi(r_n))$, for each homomorphism $h = (\\pi, \\phi)$ from $G_M$ to G.\nThe LIFT operation. Given a KG, LIFT outputs a relational hypergraph induced by a set of motifs. Let F be a set of motifs, and G = (V, E, R) a KG. Then $LIFT_F(G)$ is the relational hypergraph $(V_{LIFT}, E_{LIFT}, R_{LIFT})$, where $V_{LIFT} = R$, $R_{LIFT} = F$, and $E_{LIFT}$ is constructed as follows: for each motif $P \\in F$, and for each tuple $(r_1, ..., r_n)$ in the evaluation $Eval(P, G)$ of P over G, $P(r_1, ..., r_n) \\in E_{LIFT}$."}, {"title": "4.2. Relation encoder", "content": "A relation encoder is a tuple $(F, Enc_{LIFT})$, where F is a set of motifs and $Enc_{LIFT}$ computes relation representations for the set $R = V_{LIFT}$ of nodes of $LIFT_F(G) = (V_{LIFT}, E_{LIFT}, R_{LIFT})$. Given that our aim is to encode links of the form q(u, v), the encoding computed by $Enc_{LIFT}$ for relation $r\\in R$ is conditioned on the relation $q \\in R$. These encodings are determined by a sequence of features $h^{(t)}_{rq} \\in R^{d(t)}$, for 0 \u2264 t \u2264 T, defined iteratively as follows:\n$h^{(0)}_{rq} = INIT_1(q,r)$, $h^{(t+1)}_{rq} = UP_1 (h^{(t)}_{rq}, AGG_1(M))$,\nwhere $INIT_1$, $UP_1$, $AGG_1$ are differentiable initialization, update, and aggregation functions, respectively, and:\n$M = \\underset{(e, i) \\in E_{LIFT}(r)}{MSG_{p(e)}} ({ (h^{(t)}_{r'q}, j) | (r', j) \\in N^i(e)} )$\nIn this setting, $MSG_{p(e)}$ is a motif-specific message function and we further define $E_{LIFT}(r) = { (e, i) | e(i) = r, e \\in E_{LIFT}, 1 < i < ar(p(e)) }$ as the set of edge-position pairs of a node r, and $N^i(e)$ as the positional neighborhood of a hyperedge e with respect to a position i: $N^i(e) = { (e(j), j) | j \\neq i, 1 \u2264 j \u2264 ar(p(e)) }$.\nWe use $Enc_{LIFT, q}[r]$ to denote the final relation encoding of a relation r in G when conditioned on q, that is, the last layer vector $h^{(T)}_{rq} \\in R^{d(T)}$.\nRemark 1. Observe that the relation encoder computes binary relation invariants provided $INIT_1$ is an invariant."}, {"title": "4.3. Entity encoder", "content": "MOTIF uses an entity encoder to compute a link-level representation of a KG, regardless of its relation vocabulary. This works as follows. MOTIF is defined as a tuple $(Enc_{KG}, (F, Enc_{LIFT}))$, where $(F, Enc_{LIFT})$ is a relation encoder and $Enc_{KG}$ is an entity encoder that computes node representations over KGs of the form G = (V, E, R), conditioned on a node u \u2208 V and the relation q, according to the following iterative scheme:\n$h^{(0)}_{vu,q} = INIT_2(u, v, q)$, $h^{(l+1)}_{vu,q} = UP_2 (h^{(l)}_{vu,q}, AGG_2(N))$,\nwhere $INIT_2$, $UP_2$, $AGG_2$, and MSG are differentiable initialization, update, aggregation and message functions, respectively, v is an arbitrary node in V, and:\n$N = { MSG(h_{ENCLIFT,q}[r]) | w \u2208 N_q(v), r \u2208 R }$.\nWe use $Enc_{KG,u,q}[V]$ to denote the encoding of v, conditioned on relation q and source node u, which corresponds to $h^{(L)}_{vu,q} \\in R^{d(L)}$ (L being the number of layers). Finally, a unary decoder $Dec : R^{d(L)} \u2192 [0, 1]$ is applied on $Enc_{KG,q,u}[V]$ to obtain the probability score for the existence of the potential link q(u, v).\nRemark 2. Observe that the entity encoder computes link invariants provided $INIT_2$ is an invariant."}, {"title": "5. KGFMs captured by MOTIF", "content": "We revisit the ULTRA architecture (Galkin et al., 2024) (See details in Appendix E). The four fundamental relations used in ULTRA can be interpreted as the graph motifs illustrated in Figure 4. Hence, any ULTRA architecture can be represented as a MOTIF architecture, with $Enc_{LIFT}$ and $Enc_{KG}$ being specific variants of NBFNets (Zhu et al., 2021).\nMOTIF can also represent a slight variant of the INGRAM architecture (Lee et al., 2023) when we replace the constructed weighted relation graph with a KG. Here the construction of LIFT uses a graph motif set $F = {h2h, t2t}$, with $Enc_{LIFT}$ and $Enc_{KG}$ being variants of GATv2 (Brody et al., 2022).\nNote that the definition of MOTIF allows for unconditional message passing (Huang et al., 2023) when both $INIT_1$ and $INIT_2$ are agnostic to the query q.\nWe also note that, while other KGFMs like RMPI (Geng et al., 2023) and TRIX (Zhang et al., 2024) do not technically fall under the framework of MOTIF, these models do rely on a LIFT operation to construct a graph of relations based on a predefined set of motifs, followed by relation and entity encoders. In particular, RMPI constructs an alternative relation graph inspired by the line graph, incorporating the motifs used in ULTRA along with two additional motifs, PARA and LOOP. In turn, TRIX considers the same set of motifs as ULTRA but employs alternating updates on entity and relation representations."}, {"title": "6. Expressive power", "content": "MOTIF computes encodings for links in KGs. In this section, we aim to understand which links MOTIF can separate and whether equipping MOTIF with more graph motifs increases its separation power. Hence, our focus is on the potential power obtained when using a particular set F of motifs. To that extent, we define MOTIF(F) as the set of all MOTIF instances that use a relation encoder built over F, that is, the set of all MOTIF instances of the form $(Enc_{KG}, (F, Enc_{LIFT}))$."}, {"title": "Definition 6.1.", "content": "Let F and F' be sets of graph motifs. Then F refines F', denoted F < F', if for every KG G and potential links q(u, v), q' (u', v') over G, whenever every MOTIF instance in MOTIF(F) encodes q(u, v) and q' (u', v') in the same way, then every MOTIF instance in MOTIF(F') also encodes q(u, v) and q' (u', v') in the same way. Furthermore, F and F' are said to have the same separation power if both F < F' and F' < F hold. We denote this as F ~ F'.\nDefinition 6.1 captures the idea of the potential to separate, or distinguish, pairs of links in a KG using different sets of graph motifs. Specifically, whenever F < F', we know that using F' will not result in architectures that can separate links that cannot be separated by any MOTIF instance in MOTIF(F). On the other hand, if F ~ F', then we know that F and F' can be used interchangeably without altering the potential for separating links in our architectures.\nWe start by observing a simple fact: isomorphic graph motifs do not make a difference in terms of separation power.\nProposition 6.2. Let $P = (G_M, \u0159)$ and $P' = (G'_M, \u0159')$ be two graph motifs such that $G_M$ is isomorphic to $G'_M$. Then, for any set F of graph motifs:\n$(F\\cup{P}) ~ (F\\cup{P'}) ~ (F\\cup{P, P'})$"}, {"title": "Proposition 6.2.", "content": "Returning to Figure 4, it is easy to see that the motifs h2t and t2h are isomorphic. Hence, $F = {h2t, t2h, h2h, t2t}$ has the same separation power as {h2t, h2h, t2t} or {t2h, h2h, t2t}.\nBut although these sets of motifs have the same separation power, this does not imply that the ULTRA architecture can drop either h2t or t2h. Indeed, while the set of links distinguished by any instance in MOTIF(F) is the same as those in MOTIF({h2t, h2h, t2t}) or MOTIF({t2h, h2h, t2t}), the ULTRA architecture is a strict subset of MOTIF(F), so we cannot directly transfer these results to ULTRA. From a practical standpoint, the difference lies in the support for inverse relations: ULTRA bases its relation encoding on NBFNets (Zhu et al., 2021), while MOTIF uses HCNets (Huang et al., 2024), which supports inverse relations by design (see Appendices A and E)."}, {"title": "6.1. When a new motif leads to more links separated?", "content": "As a tool to assert when adding a new motif P increases the number of links that can be separated using a set of graph motifs F, we provide a necessary condition for the refinement of sets of motifs F and F'. We need some terminology. A homomorphism $h = (\u03c0, \u03c6)$ from a KG $G = (V, E, R)$ to a KG $G' = (V', E', R')$ is relation-preserving if $R \u2286 R'$ and $\u03c6(R) = R. Further, a graph H is a relation-preserving core if every relation-preserving homomorphism h from H to H is onto, that is, h(H) = H.\nIt is possible to show the following:\nProposition 6.3. For every KG G, up to isomorphism, there is a unique KG H such that:"}, {"title": "Proposition 6.3.", "content": "\u2022 H is a relation-preserving core, and\n\u2022 there are relation-preserving homomorphisms from G to H and from H to G.\nThe KG H is called the relation-preserving core of G.\nOur condition is laid in terms of a specific notion of homomorphism that we call core-onto homomorphisms. Formally, a homomorphism $h = (\u03c0, \u03c6)$ from G to G' is core-onto if the KG h(G) is isomorphic to the relation-preserving core of G'. If such a homomorphism exists, we write $G \u2192^{c\u00ba} G'$. Let us also say that a motif is trivial if its relation-preserving core is isomorphic to the graph with a single fact r(u, v).\nWe are now ready to state the main result of this section.\nTheorem 6.4. Let F, F' be two sets of motifs, and assume that F < F'. Then, for every non-trivial motif P' \u2208 F' there is a motif P \u2208 F such that $P \u2192^{c\u00ba} P'$."}, {"title": "6.2. Consequences for the design of MOTIF models", "content": "Consider MOTIF models in MOTIF(F). Theorem 6.4 implies that if $P \u2209 F$ cannot be covered via a core-onto homomorphism from any motif already in F, then incorporating P into the set of graph motifs used by the architecture should lead to more expressive encodings. This has consequences for the design of MOTIF architectures.\nk-path motif. Take a motif representing a path of length k:\n$r_1(u_1,u_2), r_2(u_2, u_3),..., r_k(u_k, u_{k+1})$.\nFor n \u2265 1, let $F^{path}_n$ denote the set of motifs formed from changing the orientation of any number of edges $r_i$ in the path of length k, with k \u2264 n, up to isomorphism. Theorem 6.4 tells us that $F^{path}_n \u226f F^{path}_m$ whenever n < m, since there is no core-onto homomorphism from any motif with k \u2264 n edges to a motif with m edges. Hence, every"}, {"title": "6.3. Comparison with existing link prediction models", "content": "Existing inductive link prediction models, such as C-MPNNs and R-MPNNs (Huang et al., 2023), can be viewed as special instances of MOTIF with an empty motif set and $INIT_1$ being a one-hot encoding of relations. However, this configuration breaks relation invariance on relational hypergraphs. Somewhat counter-intuitively, we note that an instance of MOTIF that preserves relation invariance is inherently less expressive than its corresponding entity encoder $Enc_{KG}$, which acts as a node invariant (but not a relation invariant). This implies, for example, that NBFNet (Zhu et al., 2021) is strictly more expressive than ULTRA when measured only over graphs with the same relations types. This increased expressive power comes at the cost of limited generalization to unseen relations."}, {"title": "7. Experimental analysis", "content": "We evaluate MOTIF over a broad range of knowledge graphs to answer the following questions: (Q1) Does MOTIF equipped with more expressive graph motifs exhibit a stronger performance? (Q2) Does ULTRA have the same expressive power as MOTIF($F^{path}_2$)? (Q3) How does a more refined relation invariant help link prediction tasks? (Q4) What is the trade-off between expressiveness and scalability for MOTIF? (See Appendices G and H.)\nIn the experiments, we consider a basic architecture which replaces the relation encoder $Enc_{LIFT}$ by HCNet (Huang et al., 2024) and the entity encoder $Enc_{KG}$ by a slight modification of NBFNet (Zhu et al., 2021) (see Appendix D)."}, {"title": "7.1. Synthetic experiments: ConnectHub", "content": "We construct synthetic datasets ConnectHub(k) to validate MOTIF's enhanced expressiveness with richer motifs (Q1).\nTask & Setup. The dataset ConnectHub(k) consists of multiple synthetic KGs, where in each KG the relations are partitioned into a positive class P, a negative class N, both of size k + 1, and a query relation q. Each KG contains a (k+1)-star hub with positive relations, and for each possible subset of relations of P and N of size k, the KG contains a positive and negative k-star community, respectively. An example of such KG is shown in Figure 7 with k = 2. We consider the following classification task: predict whether there exists a link of relation q from the hub center to positive communities but NOT to negative communities. The key to solving this task is successfully differentiating between relations from positive and negative classes. We consider Ultra and MOTIF($F^{star}_m$) for varying m, and show the empirical results on these datasets in Table 1. (Further details in Appendix K.)\nULTRA & MOTIF with inexpressive motifs. We claim both ULTRA and MOTIF($F^{star}_m$) with m \u2264 k cannot solve the task on ConnectHub(k). Since the only difference between P and N is the presence of the (k + 1)-star hub of relations in P, these models cannot detect this higher-order motif. Thus, $LIFT_F(G)$ will contain two disjoint isomor-"}, {"title": "7.2. Pretraining and fine-tuning experiments", "content": "Datasets & Evaluations. For pretraining and fine-tuning experiments, we follow the protocol of Galkin et al. (2024) and pretrain on FB15k237 (Toutanova & Chen, 2015), WN18RR (Dettmers et al., 2018), and CoDEx Medium (Safavi & Koutra, 2020). We then apply zero-shot inference and fine-tuned inference over 51 KGs across three settings: inductive on nodes and relations (Inductive e, r), inductive on nodes (Inductive e), and Transductive. The detailed information of datasets, model architectures, implementations, and hyper-parameters used in the experiments are presented in Appendix L. Following convention (Zhu et al., 2021), on each knowledge graph and for each triplet r(u, v), we augment the corresponding inverse triplet $r^{\u22121}(v, u)$ where $r^{\u22121}$ is a fresh relation symbol. For evaluation, we follow filtered ranking protocol (Bordes et al., 2013) and report Mean Reciprocal Rank (MRR), and Hits@10 for each dataset. We report both head and tails prediction results for each triplet on all datasets except three from Lv et al. (2020), where only tails prediction results are reported. The code is available at https://anonymous.4open.science/r/MOTIF.\nSetup. To evaluate the impact of graph motifs, we construct four variants of MOTIF models with different F: 3-path ($F^{path}_3$), 2-path ($F^{path}_2$), {h2t} only, and no motifs (\u00d8), defined in Section 6. We present the average zero-shot and fine-tuned results of MOTIF over 51 KGs in Table 2, along with the corresponding results for ULTRA taken from Galkin et al. (2024). The detailed per-dataset results of pretrained, zero-shot, and finetuned inference are shown in Tables 7 to 10, respectively. Note that in the zero-shot inference setting, Inductive e, r, Inductive e, and Transductive are in principle indifferent, as all models encounter unseen relations in the inference graph.\nResults. First of all, note that MOTIF($F^{path}_3$) outperforms MOTIF($F^{path}_2$) over all 51 datasets on average in zero-shot inference. In the case of fine-tuned inference, a similar trend is present: MOTIF ($F^{path}_3$) consistently outperforms MOTIF($F^{path}_2$). It is also worth noting that both MOTIF($F^{path}_3$) and MOTIF($F^{path}_2$) achieve further performance improvements after fine-tuning on the training set compared to zero-shot inference. These findings demonstrate that the additionally introduced motifs help the model to learn richer relation invariants (Q1), which are then leveraged for better predictions on new KGs with unseen nodes and relations. This observation is true on both zero-shot and fine-tuned inference.\nSecondly, recall that MOTIF($F^{path}_2$) and ULTRA have the same expressive power as shown in Theorem 6.5. This theoretical finding is supported in our empirical study, as we see that ULTRA performs similarly with MOTIF($F^{path}_2$) in these experiments (Q2).\nLastly, when it comes to comparing MOTIF($F^{path}_3$) and MOTIF({h2t}), we observe a gradual decrease in all metrics on all datasets as the motifs are removed. This aligns with Theorem 6.4: adding 3-paths on top of 2-paths yields more expressive power on MOTIF since there is no core-onto homomorphism from any 2-path to 3-path motifs. Similarly, adding h2h or t2t on top of h2t also yields more expressive power as there is no homomorphism from h2t to h2h or t2t. In the extreme case, we observe that removing all of the motifs in MOTIF((\u00d8) prevents the model from learning any non-trivial relation invariants, thus exhibiting a complete failure in generalization to unseen KGs."}, {"title": "7.3. End-to-end experiments", "content": "We also conduct end-to-end experiments where for each dataset, we train a MOTIF($F^{path}_3$) on the training set from scratch and evaluate its corresponding validation and test sets. We present the average results over all 54 KGs in Table 3 (Full results in Table 11 and Table 12). We continue to see improved performance of MOTIF($F^{path}_3$) over ULTRA due to the additional motifs, allowing models to capture a richer set of information over relations (Q1).\nDegradation of relation learning (Q3). As a case study, we focus on the end-to-end experiments with WN-v2 (Teru et al., 2020), an inductive (on node) dataset with only 20 relations after inverse augmentation. The MRR for ULTRA is 0.296, severely lower than that of MOTIF($F^{path}_3$) (0.684). To investigate, we plot the cosine similarity\u00b9 among the computed relation embeddings when queried relations are $r_0$ = derivationally_related_form and $r_2$ = hypernym in Figure 8. ULTRA produces highly similar relation representations, whereas MOTIF with richer motifs generates distinguishing relation representations, aiding the entity encoder in link prediction.\nNevertheless, we notice that such degradation does not appear in the pretraining experiments, suggesting that either enriching relation learning with diverse motifs or training over multiple KGs could help avoid such degradation."}, {"title": "8. Conclusions", "content": "We introduced MOTIF, a general framework that extends existing KGFMs via incorporating arbitrary motifs. In the context of this framework, we studied the expressive power of KGFMs and identified sufficient conditions under which adding new motifs improves MOTIFS' ability to compute finer relation invariants. We thoroughly discussed the implications of our results on existing KGFMs. The theoretical findings are empirically validated on a diverse set of datasets.\nOur work provides key insights and concrete recipes for designing more expressive KGFMs. However, the typical trade-off between expressive power and computational complexity clearly also applies to our study. While using richer motifs provably increases MOTIF's expressive power, this may come at the cost of computational efficiency in terms of both time and memory (please see the discussion in Appendices G and H). A promising avenue for future work is to enhance MOTIF's computational efficiency, thus enabling scalability to real-world large-scale graphs."}, {"title": "Impact Statement", "content": "This paper presents work aimed at advancing the field of Machine Learning. Our methods have potential applications in recommendation systems and knowledge graph completion, among others. While these applications can have significant societal impacts, including improving information retrieval and personalization, as well as potential risks such as bias amplification and misinformation propagation, we do not identify any specific, immediate concerns requiring special attention in this work."}, {"title": "A. C-MPNNs and HC-MPNNS", "content": "In this section, we follow Huang et al. (2023) and Huang et al. (2024) to define conditional message passing neural networks (C-MPNNs) and hypergraph conditional message passing neural networks (HC-MPNNs). We then follow Huang et al. (2024) to define hypergraph conditional networks (HCNets) as an architecture from HC-MPNNs. We also present their corresponding relational Weisfeiler Leman test to study their expressive power rigorously. For ease of presentation, we omit the discussion regarding history functions and readout functions from Huang et al. (2023). Also, some notations are adapted to simplify the exposition."}, {"title": "A.1. Model definitions", "content": "C-MPNNS. Let G = (V", "follows": "n$h^{(0)"}, {"h^{(l)}_{vu,q}": "V \u00d7 V \u2192 R^{d(l)"}, "the function $h^{(l)}_q(u, v) := h^{(l)}_{vu,q}$ and denote $z_q$ to be a learnable vector representing the query q \u2208 R. We can then interpret a C-MPNN as computing representations of pair of nodes. Given a fixed number of layers L > 0, a C-MPNN computes the final pair representations as $h^{(L)}_{vu,q}$. In order to decode the likelihood of the fact q(u, v) for some q \u2208 R, we consider a unary decoder $Dec : R^{d(L)} \u2192 R$. We additionally require INIT(u, v, q) to satisfy the property of target node distinguishability: for all q \u2208 R and v \u2260 u \u2208 V, it holds that INIT(u, u, q) \u2260 INIT(u, v, q).\nHC-MPNNs. Given a relational hypergraph H = (V, E, R) and a query $q = (q, \\tilde{u},t) := q(u_1,\u00b7\u00b7\u00b7, u_{t-1}, ?, u_{t+1}, \u00b7\u00b7\u00b7, u_m)$, for l \u2265 0, an HC-MPNN computes a sequence of feature maps $h^{(l)}_{vq}$ as follows:\n$h^{(0)}_{vq} = INIT(v, q)$,\n$h^{(l+1)}_{vq} = UP(h^{(l)}_{vq}, AGG(h^{(l)}_{wq}, MSG_{p(e)}({ (h^{(l)}_{r'q}, j , j) | (w, j) \u2208 N^i(e)}, q),  | (e, i) \u2208 E(v)))$,\nwhere INIT, UP, AGG, and $MSG_{p(e)}$ are differentiable initialization, update, aggregation, and relation-specific message functions, respectively. An HC-MPNN has a fixed number of layers L \u2265 0, and the final conditional node representations are given by $h^{(L)}_{vq}$. We denote by $h^{(L)}_{vq} : V \u2192 R^{d(l)}$ the function $h^{(L)}_q(v) := h^{(L)}_{vq}$.\nHCNets. HCNets can be seen as an architecture of HC-MPNN frameworks by choosing appropriate initialization, aggregation, and message functions. Let H = (V, E, R) be a relational hypergraph. For a query $q = (q, \\tilde{u},t) := q(u_1,..., u_{t-1}, ?, u_{t+1},\u00b7\u00b7\u00b7, u_m)$, an HCNet computes the following representations for all l > 0:\n$h^{(0)}_{vq} = \\sum_{i\\neq t} 1_{v=u_i} * (p_i + z_q)$,\n$h^{(l+1)}_{vq} = \u03c3(W^{(l)}  \\sum_{(e,i) \u2208 E(v)} [g^{(e)}_{p(e),q}  \\sum_{j \\neq i} (\u03b1^{(l)}h^{(l)}_{e(j)q} + (1 - \u03b1^{(l)})p_j)"]}