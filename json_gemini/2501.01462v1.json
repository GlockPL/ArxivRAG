{"title": "Pan-infection Foundation Framework Enables Multiple Pathogen Prediction", "authors": ["Lingrui Zhang", "Haonan Wu", "Nana Jin", "Chenqing Zheng", "Jize Xie", "Qitai Cai", "Jun Wang", "Qin Cao", "Xubin Zheng", "Jiankun Wang", "Lixin Cheng"], "abstract": "Host-response-based diagnostics can improve the accuracy of diagnosing bacterial and viral infections, thereby reducing inappropriate antibiotic prescriptions. However, the existing cohorts with limited sample size and coarse infections types are unable to support the exploration of an accurate and generalizable diagnostic model. Here, we curate the largest infection host-response transcriptome data, including 11,247 samples across 89 blood transcriptome datasets from 13 countries and 21 platforms. We build a diagnostic model for pathogen prediction starting from a pan-infection model as foundation (AUC = 0.97) based on the pan-infection dataset. Then, we utilize knowledge distillation to efficiently transfer the insights from this \"teacher\" model to four lightweight pathogen \"student\" models, i.e., staphylococcal infection (AUC = 0.99), streptococcal infection (AUC = 0.94), HIV infection (AUC = 0.93), and RSV infection (AUC = 0.94), as well as a sepsis \"student\" model (AUC = 0.99). The proposed knowledge distillation framework not only facilitates the diagnosis of pathogens using pan-infection data, but also enables an across-disease study from pan-infection to sepsis. Moreover, the framework enables high-degree lightweight design of diagnostic models, which is expected to be adaptively deployed in clinical settings.", "sections": [{"title": "Introduction", "content": "Infections are well-documented causes of numerous diseases, ranging from mild to severe. For instance, staphylococcus aureus bacteremia (SaB) caused by staphylococcal infections, presents a substantial challenge to healthcare systems, with mortality rates as high as 20% to 30%. Critically, the incidence of SaB infections is increasing in recent years, which further exacerbates its severity as a public health threat. Streptococcal infections also pose serious risks, as these can infect various host organisms and tissues and are transmissible between humans and animals. Most streptococci cause problems in the functioning of the human respiratory or gastrointestinal tract and cause some of the most critical infections, such as sepsis. Sepsis is a life-threatening reaction to a variety of infections that causes inflammation and organ damage (Fig.la), kills about 270,000 people each year in the USA. Timely and accurate prediction and treatment of infection and disease are critical for patients. Infection induces measurable changes in gene expression within the host, providing valuable insights into the immune response to various pathogens. By analyzing gene expression profiles in blood samples, researchers can implement targeted therapeutic interventions based on the host's response. Recently, a number of approaches have emerged that utilize this gene expression profile for the prediction of infections, such as the prognostic models proposed by. Network-based approaches and moonlighting long noncoding RNAs (lncRNAs) have also been used for prediction. However, these existing models face two significant limitations: 1) the small size of training datasets, which hinders their generalizability and increases the risk of overfitting; 2) the coarse classification of infections, typically distinguishing only between viral and bacterial infections without specific types of pathogens involved. This lack of specificity can lead to inappropriate treatment decisions, as different pathogens may require distinct therapeutic approaches, resulting in the misuse of antibiotics. Fortunately, the increasing availability of larger datasets and advancements in Artificial Intelligence (AI) in recent years present a great opportunity to apply more sophisticated analytical methods on expansive datasets, paving the way for the development of more precise and robust diagnostic models for different types of infections.\nKnowledge Distillation (KD) is a sophisticated AI method that performs the lightweight model (known as the student) to learn informative knowledge from the well-trained yet cumbersome model (known as the teacher). This approach possesses the potential for effective infection diagnosis, as it allows models to learn from diverse infections, enhancing their accuracy even with limited data. Additionally, the reduced complexity of the student model enables its deployment in real-world clinical applications with limited computational resources, facilitating timely and accurate diagnostics."}, {"title": "Results", "content": "To construct a most comprehensive host transcriptome dataset, we collected 88 distinct datasets from the Gene Expression Omnibus (GEO) database, which collectively encompass 11,247 samples from 13 countries and 21 diverse platforms. These samples are categorized into three primary groups, 1,505 bacterial infection samples, 5,113 viral infection samples, 1,809 sepsis samples, and 2,326 healthy controls (Fig.1b).\nThe datasets encompass a diverse array of diseases stemming from 67 distinct pathogens, including bacteria, such as staphylococcal infection, streptococcal infection, HIV infection, and RSV infection, as well as viruses, such as HIV and RSV. We integrated and processed the collected data using intra-pathway pairing and PAGE. We selected 35 differential gene pairs (DGPs) for the pan-infection dataset and then selected five DGP sets with the same size corresponding to sepsis, staphylococcal infection, streptococcal infection, HIV, and RSV respectively. This comprehensive dataset enables training of model from a wide range of pathological conditions, thereby improving its ability to generalize and accurately classify novel infections."}, {"title": "The structure of TSGPS.", "content": "We proposed a novel method, Teacher-Student Gene Pair Signature (TSGPS), to parallelly execute multiple tasks for pathogen identification and disease diagnosis using the teacher-student architecture. The workflow of TSGPS consists of two sections, one pan-infection \"teacher\" model construction and multiple pathogen \u201cstudent\u201d model constructions (Fig.2). Firstly, we used pan-infection data to train a pan-infection \"teacher\" model, which constructed based on transformer modules with a proven ability in modeling global feature. Equipped with this foundation teacher model, we constructed two different simplified student models based on lightweight transformer and MLP, respectively. Then, the well-trained pan-infection foundation model was performed in distilling specific student models for four pathogen infections, as well as for cross-disease prediction. During the training, we designed a distillation loss to transfer knowledge from the foundation teacher model to each pathogen student models. This process enables feature transfer between various infections, making TSGPS highly scalable. Finally, our proposed TSGPS distills five highly specialized lightweight student models, including staphylococcal infection, streptococcal infection, HIV infection, RSV infection, and sepsis, which can be easily deployed in clinical scenarios for infection diagnosis."}, {"title": "Pan-infection foundation model.", "content": "Integrating diverse infection data for pan-infection infrastructures faces challenges due to limited training data, impacting classifier accuracy. Biologically, infections trigger immune responses, altering RNA. A foundation model aids early bacterial/viral diagnosis and supports the training of specific infection models. Given the sensitive host response to the infection caused by different pathogens, several studies concentrated on analyses best-informed by host transcriptome data. To this end, we integrated 11,247 samples using PAGE and identified 35 DGPs in infection. Samples of the same infection type tend to cluster together after PAGE, while these samples were scattered before PAGE (Fig.3a-b), demonstrating the effectiveness of PAGE in data integration across platforms. The curated pan-infection data was used to construct six infection-related models (Table 1).\nTeacher-student architecture was used to build six infection-related models, where the pan-infection foundation model (PIFM) served as the \"teacher\" and pathogen models served as \"students\". The PIFM is a crucial determinant for the subsequent performance of the other student models. We trained the \"teacher\" model with 80% of the pan-infection samples. The PIFM applied a deep learning neural network with self-attention mechanisms, layer normalization, fully connected layers, and MLP.\nTo rigorously assess the performance of the PIFM, we reserved 20% of the pan-infection samples as validation set to evaluate its performance using five metrics, including Accuracy (ACC), Area Under Curve (AUC), F1 Score, Precision, and Area Under the Precision-Recall Curve (AUPRC). PIFM demonstrates superior performance, achieving an AUC of 0.97 for both bacterial and viral infections (Fig.3d & 3e). We compared PIFM with bacterial-viral-noninfected GPS (bvnGPS), Random Forest (RF), Gradient Boosting Decision Tree (GBDT), and Support Vector Machine (SVM). PIFM surpassed RF by at least 0.03 and SVM by at least 0.09 in AUC. Overall, it outperforms these methods across all evaluation metrics, highlighting the effectiveness of TSGPS (Fig.3e)."}, {"title": "Pan-infection foundation model enhances pathogen prediction.", "content": "After building pan-infection foundation model, we transferred this \"teacher\" model to establish simple and small pathogen student models for specific pathogen prediction. The student models applied fewer transformer layers and network layers. We conducted a series of rigorous experiments utilizing specific infection datasets to demonstrate the capability of our proposed framework to enhance the performance of diverse models.\nWe concentrated on four common infections, i.e., staphylococcal infection, streptococcal infection, HIV, and RSV. TSGPS was separately used for training the corresponding prediction model. For comparison, we also train four vanilla models without using TSGPS. The model with the assistance of the teacher model consistently showed higher AUC scores compared to the vanilla model. The AUCS of TSGPS in the Staphylococcus infection model, Streptococcus infection model, HIV model, and RSV model were 0.99, 0.94, 0.93, and 0.94, respectively, while the AUCs of the vanilla model were 0.95, 0.93, 0.88, and 0.89, respectively. Additionally, we also trained RF, GBDT, and SVM models for comparison, the AUCs of which were 0.93 to 0.95, 0.86 to 0.91, 0.84 to 0.92, and 0.86 to 0.92 for staphylococcal infection, streptococcal infection, HIV, and RSV respectively. (Fig.5c).\nThe comprehensive evaluation of our model and structure encompasses various indicators, providing a multi-faceted assessment of their performance (Fig.5d). Our result shows the performance of the vanilla model improves significantly when harnessed TSGPS, even the vanilla model is not the optimal one across all metrics. For instance, in models trained with staphylococcal infection data, the Random Forest model outperforms our original model in terms of the AUPRC, achieving a score of 0.96 compared to our original model's 0.94. However, the student model, learning from the PIFM, outperforms all the infection models, achieving an AUPRC score of 0.99. A similar trend is observed in the HIV data, where TSGPS outperforms other models across all evaluation metrics. Specifically, TSGPS consistently enhances the performance of the original student model, leading to improvements in ACC, F1 Score, precision, AUPRC, and AUC. These findings underscore the effectiveness of our structure in enhancing the capabilities of various modeling paradigms, thereby demonstrating its potential to yield superior results across the analysis of different infections."}, {"title": "Pan-infection foundation model enhances sepsis diagnosis.", "content": "Besides pathogen prediction, the pan-infection foundation can also be applied to enhance disease diagnosis such as sepsis. Given the high mortality of sepsis and its potential to be triggered by bacterial or viral infections, early screening is crucial for timely treatment. Host immune system has an extreme response during the development from infection to sepsis, indicating host immune response is a common knowledge across the two diseases. Similarly, the expression pattern of DGPs shares common knowledge between teacher and student models (Fig.6e). In this work, transmission of cross-disease features from teacher model to student model corresponds to the transition from pan-infection to sepsis.\nThe constructed sepsis student model contained fewer attention layers and heads with distillation loss and cross-entropy loss (see Methods). Validation on independent datasets demonstrated the performance of the sepsis student model enhanced by the pan-infection model (Fig.6b-c). Compared to the original model without KD, the AUC of the student model improves by 0.02 and the ACC metric improves by 0.03 (reaching 0.99 and 0.96, respectively). Furthermore, we compared it to the existing biomarkers, SeptiCyte and SNIP, which are rapid molecular detection methods that can differentiate between healthy individuals and those with sepsis. The performance on four-fold cross validation was evaluated using AUPRC, ACC, and AUC, where TSGPS demonstrates superior robustness and consistently achieves higher AUPRC, ACC, and AUC scores compared to SeptiCyte and sNIP (Fig.6d). The effectiveness of TSGPS for cross-disease demonstrates the potential for early diagnosis of disease."}, {"title": "Characterization of TSGPS.", "content": "With the assistance of TSGPS, the five \"student\" models demonstrated superior performance utilizing 35 DGPs. We investigated the DGPs we screened across pan-infection, staphylococcal infection, streptococcal infection, HIV, RSV, and sepsis (Fig.7a). The DGPs in different circumstances have few intersects, which explains the high efficiency of the model as they may focus on different aspects. The DGPs we screened for pan-infection and various other infections share the same gene ontology (GO) terms at the functional level (Fig.4), which shows the plausibility of TSGPS in terms of knowledge transfer. The number of these DGPs is much higher in the infected population than in the healthy population, the probability of the abnormal DGPs being present in the population with the disease is essentially 70% or more (Fig.3c, Fig.5a, Fig.5b). All these results explain our model from a bioinformatics point of view.\nTo explore the effectiveness of KD in pathogen and disease prediction, we analyzed the pathways and functions of the KD \u201cteacher\u201d and \"student\" models. Streptococcal and staphylococcal infections, which are both classified as bacterial infections, share 10 common genes. Beyond this, only a limited number of genes were identified as overlapping across the various diseases (see Fig. 7a). However, the pathways enriched by these genes were found to be consistent across the various diseases. Pan-infection shares 24 pathways with other infections or diseases, mainly focused on the production of inflammatory cytokines, which are crucial for immune response. Notable pathways include the JAK-STAT signaling pathway, the WNT signaling pathway, and the T cell receptor signaling pathway. Infections caused by different pathogens share several pathways, particularly those related to innate immunity and inflammation, such as leukocyte transendothelial migration. Additionally, pathways associated with cell adhesion, including tight junction and cell adhesion molecules (CAMs), are also commonly shared (Fig. 7f).\nEach infection is characterized by distinct DGPs that serve specific functions. For instance, DGPs associated with HIV are enriched in pathways that regulate cellular behavior and communication, including extracellular matrix organization, syndecan interactions, and the E2F pathway. In the case of streptococcal infections, DGPs are notably enriched in pathways that regulate cell differentiation, proliferation, and immune responses. Key pathways include CTLA4 inhibitory signaling, MAPK family signaling cascades, and NOTCH signaling. Staphylococcal infections exhibit DGPs enriched in immune responses and inflammation, such as the STAT4 pathway, TLR4 signaling and tolerance mechanisms, as well as interleukin 37 signaling. Additionally, DGPs in RSV are enriched in pathways related to immune responses and cellular activation, including the CD40 pathway, T cell receptor signaling, and negative regulators of DDX5-IFIH1 signaling.\nAdditionally, higher semantic similarity scores indicate that diseases share related biological functions or pathways, while a greater number of shared connections implies stronger interactions within the PPI network (Fig.7b-e). The semantic similarity score and the number of connections shared between each disease in a PPI network suggest a potential transfer of features among diseases."}, {"title": "Discussion", "content": "This study implements a pan-infection pretrained foundation and fine-tune framework for predicting multiple pathogens. We proposed and validated the effectiveness of the teacher-student model architecture in categorizing diverse infections by training five independent student models of distinct pathogens and diseases. This approach yields impressive performance with AUCs of 0.99, 0.94, 0.93, 0.94, and 0.99 for staphylococcal infections, streptococcal infections, HIV, RSV, and sepsis, respectively. Clinicians can utilize the predictions from the pan-infection model to optimize the allocation of medical resources, provide accurate early-stage treatments, minimize the resources waste and ultimately reduce patient mortality.\nThe KD architecture adopted by TSGPS facilitates knowledge transfer between different models. The teacher-student architecture we employed offers several advantages. First, it leverages a high-accuracy foundational teacher model with strong generalization capabilities to effectively guide the training of the student model. This allows for the transfer of not only output distributions but also intermediate, relational, and structural features, resulting in a more comprehensive knowledge transfer. Second, the student models consume fewer resources for storage and inference, which is important for deploying deep learning models in resource-constrained environments.\nWe curated a pan-infection dataset comprising 11,247 samples from 88 sources, covering 13 countries and 21 platforms. The samples of the established pan-infection data come from various sources, presenting significant challenges in extracting cross-platform information using conventional methodologies. To address this problem, we utilize PAGE to integrate these cross-platform data sets, which is a sophisticated method we previously proposed for feature selection and data fusion. With-in sample comparison of genes enables a more comparable quantification analysis for datasets from various platforms.\nAs the immune system has similar response to different pathogen infections, training the pan-infection teacher model can benefit the diagnosis of the specific pathogens. Besides, sepsis is a disease caused by infection therefore the pan-infection teacher model can help the student model in diagnosing sepsis. The integrated large-scale pan-infection dataset facilitates the training of deeper neural networks, specifically teacher-student architecture in this study, which can assist in parallelly and accurately detecting multiple diseases. Our results demonstrate the substantial potential of our framework in accurately detecting a broad spectrum of diseases. This framework is also characterized by ease of deployment. However, the scarcity of certain pathogen data has prevented TSGPS from providing accurate predictions for some cohorts with extremely small sample size. We anticipate that this limitation will improve as more data becomes available in the future.\nWe found that simpler models outperform complex models when trained on small datasets. For instance, in the streptococcal infection model, the one using the self-attention mechanism achieved an AUC of 0.89, while the one without self-attention mechanism achieved an AUC of 0.93.\nAlthough TSGPS has modeled four pathogens and one infection disease, a wider range of pathogen screening and infection diagnosis are critical for further investigation, which calls for a more powerful pan-infection foundation model using a larger volume of data. On the other hand, although TSGPS has achieved an AUC of 0.94 in cases of streptococcal infections, the accuracy of models can be further improved through other technical methods, such as incorporating few-shot learning methods. In this case, TSGPS can be used in some pathogens with fewer data. Moreover, the pathogen student models in TSGPS operate independently while sharing a common pan-infection model for knowledge acquisition. To further extend the application of this framework, we are planning to propose a novel scenario focused on differentiating between various bacterial and viral infections in the near future.\nWhile the results obtained from TSGPS are promising, such as achieving an AUC of 0.94 for streptococcal infections, there is potential for further improving model accuracy through additional technical methods, including the incorporation of few-shot learning techniques. This approach would enable TSGPS to be effectively applied to pathogens with limited data. Meanwhile, a wider range of pathogen screening and infection diagnosis are critical for further investigation, which calls for a more powerful pan-infection foundation model using a larger volume of data.\nOverall, we proposed a novel framework TSGPS, which has demonstrated strong performance in four cross-infection and one cross-disease studies, with limited number of parameters in the student models to ensure the deployment in clinical settings. The experiments exemplify the potential of our framework to accurately identify pathogens and predict diseases, underscoring its promising applications in clinical practice."}, {"title": "Methods", "content": "KD is a technique used in machine learning, particularly within the domain of neural networks and deep learning, where the knowledge from a larger model (often referred to as the \"teacher\" or \"mentor\" model) is transferred to a smaller model (often called the \"student\" model). The primary aim of this process is to enable the student model to mimic the performance and behavior of the teacher model while being more compact, faster, and less resource-intensive.\nExisting distillation methods can be categorized into online distillation, self-distillation and offline distillation. The student model was designed in four ways: a quantitative version of the teacher model that preserves the network structure, a simplified teacher model, a simple model with good performance and an optimized global network. However, the substantial amount of data required for these techniques limits its widespread application in bioinformatics. Building on the offline destillation, we propose a framework aimed at obtaining fine-grained and easy-to-deploy infection models. This framework revolves around a comprehensive teacher model that possesses the capability to discern between three fundamental categories: health, bacterial infection, and virus infection. The teacher model transferred the feature information to multiple student models, allowing them to achieve higher accuracy in their respective disease prediction tasks (Fig.2)."}, {"title": "TSGPS", "content": "The collected cohorts can be divided into six categories, including, pan-infections, sepsis, streptococcal infection, staphylococcal infection, HIV, and RSV (Table 1). To integrate samples from different cohorts and platforms, we employed the PAGE algorithm to identify differences between gene pairs. Each pair was constructed within the pathways in the Molecular Signatures Database (MSigDB). \u03a4\u03bf build foundation model we assigned the samples with health status, bacterial infections, and virus infections. Then, PAGE identified pairs of genes that exhibit significant differences. We quantified the dissimilarity between DGPs with a metric denoted $rdis$, which captures the degree of difference between the expression profiles of DGPs. We then perform logarithmic operations on these values and subtract them to arrive at the difference between them. After determining the differences between individual DGPs using the PAGE algorithm, the next step is to validate the significance of these differences statistically. Subsequently, we employ Fisher's exact test to ascertain the p-value statistically, thereby identifying the 35 groups of DGPs that exhibit the most pronounced differential expression according to the following equation 1 and Table 2:\n$p = \\frac{(a+b)! (c+d)!(a+c)!(b+d)!}{a!-b!-c!-d! (a+b+c+d)!}$"}, {"title": "Model design.", "content": "In this model, we applied deep neural network with multiple self-attention layers. Self-attention mechanisms serve as an integral component in filtering pivotal information, thereby fostering the establishment of a comprehensive global dependency relationship, which in turn enhancing information acquisition. Multiple head self-attention mechanism is also the key for transformers. To emphasize the discrimination between DGPs and attain superior performance, a transformer layer was strategically incorporated into our model architecture. In single head self-attention mechanism, we use $X\\in R^{N*d_k}$ to represent our input infection data, where N is the total amount of data and $d_k$ to represent the feature dimension. Q, K, and V stand for Query, Key and Value, respectively. the output Z can be obtained using equation 2..\n$Z = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}) V$\nwhere $Q = W^Q \\cdot X, K = W^K \\cdot X, V = W^V \\cdot X$ stand for Query, Key and Value, respectively. $W^Q$, $W^K$, and $W^K$ are learnable parameters.\nThe output of multiple self-attention mechanisms $Z_f$ is a splice of multiple Z then mapped to the original space by a learnable linear transformation as follows with a weight matrix $W_f$.\n$Z_f = W_fUo Z_i$\nOur framework comprises a meticulously designed teacher model, accompanied by two distinct student model structures. The teacher model comprises two transformer layers and a Multi-Layer Perceptron (MLP) component. The formula for updating the weights of the MLP is as follows:\n$w_{ji} \\leftarrow w_{ji} + Aw_{ji} = w_{ji} + \\eta d_{i}x_{ji}$ (4)\nwhere $w$ is the learnable weight, $x$ is input, $\\eta$ is learning rate, and $\\delta$ is local gradient.\nSpecifically, the first transformer layer incorporated a multi-head attention mechanism with 5 attention heads, the dropout rate is 0.1, and utilized the Gaussian Error Linear Unit (GELU) activation function as follows:\n$Gelu(x) = 0.5x (1 + tanh ( (\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)))$ (5)\nWe completed the layer specification before the attention and feedforward operations and set up 4 encoder layers in the encoder. The second transformer layer, mirrored the structure of the first but with tailored configurations. It uses two attention heads in its multi-head attention module and two encoder layers in the encoder. Notably, it adopted the Rectified Linear Unit (ReLU) activation function, catering to potential non-linearity requirements as following:\n$ReLU(x) = max (0, x)$\nAdditionally, a fully connected layer is interposed between the two transformer layers, aiming to augment the representational capabilities of the model by extending the feature space. Then we applied MLP with GELU activation within each linear layer and layer normalization is applied to facilitate learning stability.\nWe devised two distinct student models to address specific objectives and constraints. The first student model has one transformer layer which is the same as the teacher model's first transformer layer, including the multi-head attentional mechanism (5 heads), GELU activation, and layer normalization. It also has an MLP part. The second student model adopts a more streamlined structure, only comprising solely linear layers with GELU activation. The rationale behind designing these two student models is twofold: firstly, to validate the generality and adaptability of our Teacher-student structure across a range of datasets and tasks. Secondly, to accommodate the practical challenges posed by limited data availability, particularly in the context of streptococcal infection, where light models can have better performance. In conclusion, we designed the second student model for streptococcal infection data and used the first student model for other data."}, {"title": "Training TSGPS.", "content": "Throughout the entire training process, we consistently utilize AdamW as the optimizer, owing to its effectiveness in handling large-scale and complex optimization tasks while incorporating weight decay for regularization. Initially, we train the teacher model on pan-infection data excluding specific conditions such as sepsis, HIV, RSV, etc. Subsequently, we use data of staphylococcal infection, streptococcal infection, HIV, RSV, and sepsis to train student models. During the training of the student models, which is tasked with predicting a distinct set of diseases, we present the teacher model with data identical to the student. The soft targets are obtained by applying a softmax function with a temperature parameter $t$ to smooth the distribution predicted by the teacher model. t is set to 5 in the implementation process. To facilitate knowledge transfer from the teacher to the student, we construct a distillation loss function, denoted as $L_{distill}$ which quantifies the discrepancy between the teacher's prediction $P^T$ and the student's predicted soft prediction $P^S$ as the following function:\n$q_i = \\frac{exp(P_i^T/\\tau)}{\\sum_j exp(P_j^T/\\tau)}$\n$P_i = log(\\frac{exp(P_i^S/\\tau)}{\\sum_j exp(P_j^S/\\tau)})$\n$L_{distill} = \\frac{\\sum_i q_i (q_i-P_i)}{\\eta \\cdot \\tau^2}$\nWhere n is the number of $p_i$.\nAdditionally, we use the cross-entropy loss $L_{ce}$ to supervise the training of the student model based on the data-specific annotations Y to preserve the specificity of the student model:\n$L_{ce} =-Y_i \\cdot log P_i$\nThe total loss $L_{Total}$ for the student network training is composed of the weighted combination of distillation loss $L_{distill}$ and cross-entropy loss $L_{ce}$:\n$L_{Total} = W_{ditill} \\cdot L_{distill} + W_{ce} L_{ce}$\nwhere $W_{ditill}$ and $W_{ce}$ are loss weight for distillation loss and cross-entropy loss, and set to 0.2 and 0.8 in practice, respectively.\nTSGPS enables the student model to efficiently distill the knowledge in the teacher's predictions. Through this process, the student model achieves better performance even when dealing with limited data or specific disease prediction tasks.\nTo comprehensively evaluate the performance of these models after training, we employ a diverse set of evaluation metrics including AUC with the Receiver Operating Characteristic (ROC) curve, ACC, AUPRC, ROC, precision, and F1. The AUC, offering a holistic assessment of the classification model's overall efficacy, can be obtained through the False Positive Rate (FPR) and True Positive Rate (TPR) These metrics were calculated using functions as follows:\n$ACC = \\frac{TP+TN}{TP+TN+FP+FN}$\n$Recall = TPR = \\frac{TP}{TP+FN}$\n$FPR = \\frac{FP}{TN+FP}$\n$Precision = \\frac{TP}{TP+FP}$"}, {"title": "Model deployment.", "content": "Our structure aimed at achieving an optimal balance between model quality enhancement and extreme parameter compression. To validate the ease of deployment of the student model, we conducted a thorough analyze their parameter count. Our analysis revealed that the teacher model boasts 18,142,949 parameters, the student model with transformer has 8,178,842 parameters (compression ratio 54.9%), and the student model after extreme compression has 797,925 parameters (compression ratio 95.6%). Notably, despite this substantial parameter reduction, the student model maintains a commendable performance in classification tasks. TSGPS has good performance in both normal and extreme compression and it makes real-world deployment possibilities."}]}