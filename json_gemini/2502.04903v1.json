{"title": "Wavelet-Assisted Multi-Frequency Attention Network for Pansharpening", "authors": ["Jie Huang", "Rui Huang", "Jinghao Xu", "Siran Peng", "Yule Duan", "Liangjian Deng"], "abstract": "Pansharpening aims to combine a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to produce a high-resolution multispectral (HRMS) image. Although pansharpening in the frequency domain offers clear advantages, most existing methods either continue to operate solely in the spatial domain or fail to fully exploit the benefits of the frequency domain. To address this issue, we innovatively propose Multi-Frequency Fusion Attention (MFFA), which leverages wavelet transforms to cleanly separate frequencies and enable lossless reconstruction across different frequency domains. Then, we generate Frequency-Query, Spatial-Key, and Fusion-Value based on the physical meanings represented by different features, which enables a more effective capture of specific information in the frequency domain. Additionally, we focus on the preservation of frequency features across different operations. On a broader level, our network employs a wavelet pyramid to progressively fuse information across multiple scales. Compared to previous frequency domain approaches, our network better prevents confusion and loss of different frequency features during the fusion process. Quantitative and qualitative experiments on multiple datasets demonstrate that our method outperforms existing approaches and shows significant generalization capabilities for real-world scenarios.", "sections": [{"title": "Introduction", "content": "High-resolution multispectral (HRMS) images are vital for applications like environmental monitoring and urban planning. Due to hardware constraints, satellites typically capture low-resolution multispectral (LRMS) and high-resolution panchromatic (PAN) images. Pansharpening fuses these to produce HRMS, combining their strengths to enhance spatial and spectral resolution.\nTo obtain high-resolution multispectral (HRMS) images, pansharpening methods are categorized into traditional and deep learning-based approaches. Traditional methods are divided into three groups (Meng et al. 2019): Component Substitution (CS) (Vivone 2019), Multi-Resolution Analysis (MRA) (Vivone, Restaino, and Chanussot 2018), and Variational Optimization-based (VO) (Tian et al. 2022) techniques. In recent years, with the rapid development of deep learning, many deep learning methods (Wang et al. 2021; Zhang et al. 2023) have been proposed for pansharpening using convolutional neural networks (CNN), as shown in Fig. 1 (a), such as PNN (Masi et al. 2016), DiCNN (He et al. 2019), and LAGConv (Jin et al. 2022b). These methods underscore deep learning's potential to improve pansharpening accuracy and efficiency. However, most existing methods do not process images in different frequency domains but instead operate in the original single spatial domain, thereby limiting the potential for improving fusion quality.\nDirect fusion in the spatial domain methods can often result in detail loss or blurring due to the imprecise separation of frequency information. In contrast, frequency-based methods can separate different frequencies for targeted processing, which better preserves hard-to-capture high-frequency information while effectively preventing interference between different frequencies. Consequently, pro-"}, {"title": "Proposed Method", "content": "This section introduces the proposed WFANet, detailing its two key components: Multi-Frequency Fusion Attention (MFFA) and the Spatial Detail Enhancement Module (SDEM), followed by the overall multi-scale framework. Fig. 3 illustrates the workflow of WFANet."}, {"title": "Multi-Frequency Fusion Attention (MFFA)", "content": "To fuse information across frequencies, we propose the MFFA, which is composed of two phases: Frequency Attention Triplet Generation (FATG) and Attention-Driven Frequency Reconstruction (ADFR). Details are shown in Fig. 4.\n(I) Frequency Attention Triplet Generation As in the typical attention mechanism (Vaswani et al. 2017; Soydaner 2022), Query, Key, and Value are the three components of attention. Query represents the information we seek, Key is the index of this information, and Value is the specific content. To better adapt to different frequency domains, we design the Frequency Attention Triplet. Specifically, different frequency features are the information we seek, the overall spatial features are the indices for querying different frequency features, and the specific content is the fusion of spectral and spatial information. Therefore, we design Frequency-Query, Spatial-Key, and Fusion-Value with specific physical meanings. We first perform a DWT on P, which is the feature of the panchromatic (PAN) image after convolution, as shown below:\n$P_{LL}, P_{LH}, P_{HL}, P_{HH} = DWT(P)$ (1)\nwhere $P_{LL}$ represents the low-frequency details of P, and $P_{LH}, P_{HL}$, and $P_{HH}$ represent the high-frequency details of P in the horizontal, vertical, and diagonal directions, respectively. For convenience, i = LL, LH, HL, HH cor-"}, {"title": "Spatial Detail Enhancement Module (SDEM)", "content": "The core component, MFFA, achieves the fusion of information across different frequency domains. In contrast, SDEM focuses on extracting and enhancing spatial detail information within these frequency domains. First, we decompose P according to Eq.1 to obtain $P_i$, features containing information from different frequencies, helping to prevent interference between them. Next, we extract $f_i$, representing spatial information for different frequency features, separately using several Frequency Adaptation Blocks (FABs). An FAB is a block capable of adapting to different frequencies and is composed of a linear layer and a sigmoid activation function. This process is illustrated below:\n$f_i = FABS(P_i)$ (6)\nwhere i = LL, LH, HL, HH correspond to the four different frequency features, respectively. As illustrated in Fig. 9, we do not choose the Convolution Block. Given that convolutional networks struggle with extracting high-frequency information and perform poorly when processing different frequency domains (Xu et al. 2019; Yedla and Dubey 2021), we opt for linear layers, which, as demonstrated by our ablation experiments, better adapt to different frequency domains. After extracting features in different frequency domains, we use the lossless IDWT to recover the complete spatial details $F_s$, as illustrated below:\n$F_s = IDWT(f_{LL}, f_{LH}, f_{HL}, f_{HH})$ (7)"}, {"title": "Network Framework and Loss", "content": "This section describes how to utilize the wavelet pyramid to construct the multi-scale network architecture of WFANet with MFFA and SDEM. Our network employs a multi-scale structure with N layers (limited by the dataset, we use two scales in this paper). First, we construct a wavelet pyramid by repeatedly applying DWT, as follows:\n$P_k = DWT(P_{k+1})$ (8)\nwhere $P_{k+1}$ represents the four different frequency features of the previous larger scale, and $P_k$ represents the frequency features of the next smaller scale. Then, we progressively fuse from the smallest scale. $P_k$ and $M_k$ are the inputs at the k-th scale. The output of each layer is obtained by adding the output $F_M$ from the MFFA and the output $F_s$ from the SDEM. The output of the k-th layer serves as the input $M_{k+1}$ for the next layer, which in turn enables the process of progressive reconstruction, expressed as follows:\n$M_1 = f(M_0, P_0)$\n$M_2 = f(M_1, P_1)$\n:\n$M_n = f(M_{n-1}, P_{n-1})$ (9)\nwhere n represents the number of scales. $M_n$ is then convolved to get the final high-resolution multispectral (HRMS) image M. We choose the simple $l_1$ loss function since it is sufficient to yield consistently good outcomes:\n$L = \\frac{1}{K} \\sum_{i=1}^{K}||\\{I^{(i)}\\}-\\{\\hat{I}^{(i)}\\}||_1$ (10)\nwhere K is the number of training data, $I^{(i)}$ denotes the i-th ground truth image, and $|| \u00b7 ||_1$ represents the $l_1$ norm."}, {"title": "Experimental Supplementary", "content": "Comparison of Parameter Numbers\nIn this section, we compare the parameter numbers of various DL-based pansharpening methods, as illustrated in Table 5. We divide DL-based pansharpening methods into two categories based on their number of parameters. Models with no more than 0.10M parameters are designated as lightweight networks, whereas those exceeding 0.10M parameters are classified as heavyweight networks. WFANet belongs to the heavyweight category, and we also designed a lightweight version, WFANet-L. To reduce network parameters while maintaining performance, we decreased the common channel size from 32 to 24 and simplified several MLP layers. To ensure a fair comparison, Fig. 9 shows the results of the lightweight networks in the left half and the heavyweight networks in the right half, with PSNR representing the model performance(Yuan et al. 2018). Both WFANet-L and WFANet achieve strong performance while maintaining a relatively low number of parameters. These results demonstrate that our method effectively balances model performance with manageable complexity."}, {"title": "Further Validation of the Frequency Attention Triplet", "content": "To further validate the effectiveness of the Frequency Attention Triplet design, we conducted experiments where we systematically swapped the roles of Frequency-Query, Spatial-Key, and Fusion-Value as the Query, Key, and Value in the attention mechanism. This resulted in six different configurations. The original configuration is labeled as Ours, while the alternative configurations, named V1 through V5, each represents a specific permutation of Frequency-Query, Spatial-Key, and Fusion-Value serving as Query, Key, and Value, respectively. As shown in Table 6, the experimental results clearly demonstrate that our method achieves the best performance across all metrics, which can be attributed to the thoughtful design based on their physical significance."}, {"title": "Ablation Settings Details", "content": "This section provides a more detailed description of some ablation experiments.\nFrequency Attention Triplet First, without any ablation, the generation process of our Frequency Attention Triplet is as follows:\n$Q = MLP(LN(P_i))$\n$K = MLP(LN(P_{LL}))$\n$V = MLP(LN(f_v(M, P_{LL})))$ (16)\nWe separately altered the generation method of one component within the Frequency Attention Triplet while keeping the others unchanged. Fig. 9 (a)-(c) correspond to the three different ablation settings, where each substitutes Q, K, or V for the original component while keeping the other two components unchanged. The process can be obtained using the following equations:\n$\\tilde{Q} = MLP(LN(Conv(P)))$\n$\\tilde{K} = MLP(LN(Conv(P)))$\n$\\tilde{V} = MLP(LN(Conv(M)))$ (17)\nMulti-Frequency Fusion Attention As illustrated in Fig. 9 (d), we respectively concatenate the convolved M with the features in different frequency domains and then extract features through a convolutional network. The design of this convolutional network follows the classical PNN approach (Masi et al. 2016)."}, {"title": "Additional Results", "content": "In this section, we present additional qualitative and quantitative results. Table 7 presents the results on the GF2 full-resolution dataset. Fig. 11 illustrates the visualization results on the QB reduced dataset. Fig. 12 and Fig. 13 present the visualization results of the WV3 and GF2 full-resolution datasets. As depicted in the second row, the redder areas indicate better performance, while the bluer areas indicate poorer performance. Among the methods compared, ours shows the largest and deepest red area, indicating the best performance."}]}