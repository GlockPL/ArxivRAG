{"title": "Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection", "authors": ["Shanu Kumar", "Saish Mendke", "Karody Lubna Abdul Rahman", "Santosh Kurasa", "Parag Agrawal", "Sandipan Dandapat"], "abstract": "Chain-of-thought (CoT) prompting has significantly enhanced the capability of large language models (LLMs) by structuring their reasoning processes. However, existing methods face critical limitations: handcrafted demonstrations require extensive human expertise, while trigger phrases are prone to inaccuracies. In this paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method, a novel approach that improves CoT prompting by utilizing uncertainty estimates to select effective demonstrations without needing access to model parameters. Unlike traditional methods, ZEUS offers high sensitivity in distinguishing between helpful and ineffective questions, ensuring more precise and reliable selection. Our extensive evaluation shows that ZEUS consistently outperforms existing CoT strategies across four challenging reasoning benchmarks, demonstrating its robustness and scalability.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable performance in a wide range of natural language processing tasks (Brown et al., 2020; Touvron et al., 2023; Thoppilan et al., 2022). However, they often struggle with tasks that require complex reasoning (Rae et al., 2021; Liang et al., 2022). The \"chain-of-thought\" (CoT) prompting technique (Wei et al., 2022; Feng et al., 2024) has been proposed to address this limitation by generating intermediate rationales (r) along with the final answer (a) for a given question (q). In this context, few-shot in-context examples, referred to as demonstrations D = (qj, rj, aj)=1, consist of k example questions qj, manually crafted rationales rj, and answers aj. This approach, known as Manual-CoT, relies on handcrafted rationales to guide the model.\nBuilding on Manual-CoT, Zero-Shot-CoT (Kojima et al., 2022) presents a novel prompting method where LLMs generate rationales using a trigger phrase t (e.g., \"Let's think step by step\") appended to the input question q, without requiring manually crafted demonstrations. While Zero-Shot-CoT is cost-effective, its performance often falls short compared to Manual-CoT due to the absence of effective demonstrations.\nCrafting rationales manually is typically labor-intensive and time-consuming, particularly for tasks demanding intricate reasoning. To mitigate this, Auto-CoT (Zhang et al., 2022) combines Manual-CoT and Zero-Shot-CoT, thereby reducing the performance gap while minimizing manual effort. Auto-CoT employs self-supervised learning on a set of unlabeled questions Q = {qj}=1 to generate rationales and answers. Demonstrations are created by clustering Q into k groups and selecting a representative question, rationale, and answer from each cluster. This clustering approach aims to maintain diversity in the demonstrations, which can help mitigate the impact of any errors in the generated rationales.\nIn this work, we seek to enhance the creation of demonstrations that improve LLM performance solely using unlabeled questions Q without any rationale and answer. The selection process of examples qj in demonstrations D has been to significantly influence LLM performance (Wan et al., 2023), and generating consistent rationales (Wang et al., 2022) is crucial. Recent CoT prompting methods (Diao et al., 2024; Bayer and Reuter, 2024) have utilized Active Learning (AL) (Fu et al., 2013; Settles and Craven, 2008; Rotman and Reichart, 2022) to identify examples for human annotation, showing that annotating the most uncertain examples yields the best performance. Drawing on these principles, we propose several selection strategies based on the uncertainty of unlabeled questions.\nTo estimate uncertainty, we adopt perturbation-based methods (Ribeiro et al., 2020; ?; Gao et al., 2024; Tomani et al., 2024), which operate on the"}, {"title": "2 Related Work", "content": "Chain-of-Thought (CoT) prompting has significantly influenced various advanced techniques designed to enhance reasoning capabilities. These include Tree of Thoughts (Yao et al., 2023), Role Play (Kong et al., 2024), and Collaborative Prompting (Zhu et al., 2023; Yin et al., 2023; Liang et al., 2023; Wang et al., 2023), each building on the CoT methodology to improve model performance in complex reasoning tasks. Concurrently, Active Learning (AL)-based methods have gained traction in few-shot prompting scenarios. Diao et al. (2023) enhance CoT prompting within an AL framework by actively selecting questions based on an uncertainty metric and manually constructing demonstrations. Shum et al. (2023) work with labeled questions devoid of rationales, generating rationales through pruning and using an AL-inspired variance-reduced policy gradient strategy to select the most informative examples. Similarly, Bayer and Reuter (2024) apply uncertainty-based AL methods to identify the most valuable questions for annotation. Unlike these studies, our work addresses a"}, {"title": "3 ZEUS: Zero-shot Uncertainty-based Selection", "content": "We propose the ZEUS method, which aims to construct useful demonstrations containing a specific level of required uncertainty. It comprised of three stages: (i) uncertainty estimation, (ii) uncertainty-based question selection, and (iii) demonstration construction. We have illustrated all the stages of ZEUS in Figure 1."}, {"title": "3.1 Uncertainty Estimation (Stage 1)", "content": "In the ZEUS method, uncertainty estimation is a critical step and is performed using perturbation. We exploit three distinct types of perturbations to estimate uncertainty for each unlabeled question in the set Q. These perturbations include temperature adjustments, trigger phrase variations, and question rephrasing.\nTemperature Perturbation: This perturbation technique is based on the principle that a question can be answered in multiple ways, and these variations can be explored by adjusting the temperature parameter during decoding. Temperature perturbation helps in simulating different reasoning paths within the LLM. When the temperature is set to a higher value, the model's outputs become more diverse, while a lower temperature typically results"}, {"title": "Trigger Phrase Perturbation:", "content": "This factor leverages the sensitivity of LLM performance to trigger phrases. Kojima et al. (2022) demonstrated that appending different trigger phrases to a question can affect the LLM's output. By introducing variations in trigger phrases, we can assess whether the LLM's responses remain consistent. If the LLM provides the same answer across different trigger phrases, it suggests a high level of confidence in its response. Conversely, varying answers across trigger phrases indicate that the question qj is challenging or that the LLM is uncertain. To apply this perturbation, we append a set of t different trigger phrases to the original question qj and generate a corresponding set of responses {r}}=1."}, {"title": "Rephrasing Perturbation:", "content": "The third technique utilizes rephrasing of the input question to explore the impact on the LLM's responses. We hypothesize that if the LLM is confident about its answer, rephrasing the question should not significantly alter the generated answer. On the other hand, if the LLM's answer is influenced by specific biases or ambiguities in the original question, rephrasing may lead to a different response. To estimate uncertainty using rephrasing, we generate v rephrased versions of the question qj and obtain the sets of responses {r}}=1\nBy integrating these three types of perturbations-temperature adjustment, trigger phrase variation, and question rephrasing, we generate a diverse set of responses for each question qj. Specifically, we produce a total of nxt\u00d7v responses. This pool of answers reflects variations due to different decoding settings, trigger phrases, and question rephrasing, serving as Monte Carlo samples from the LLM's likelihood distribution (Hastings, 1970). From these responses, we identify C (\u2264 n) unique answers y,..., yf for the question qj. The confidence score p\u00e5 for each unique answer yf is computed based on the consistency of responses across the different perturbations. This score quantifies the degree of certainty associated with each answer and serves as a basis for selecting informative demonstrations in subsequent stages of the ZEUS method. The confidence score p\u00e5 for a unique answer y is defined as:\nP(yj|qj) = \\frac{1}{n} \\sum_{l=1}^n \\mathbb{1}(y^j = a_l),\nwhere 1(\u00b7) is the indicator function that evaluates to 1 if yj matches a; and 0 otherwise.\nTo represent the uncertainty of the LLM regarding the question qj, we use predictive entropy (PE) (Kumar et al., 2022). PE is maximized when confidence scores are uniformly distributed across many unique answers and increases as the number of unique answers grows. It reaches zero when all answers are identical. The PE for the question qj is computed as follows:\nU_j = - \\sum_{c=1}^C p(y_j^c|q_j) \\log(p(y_j^c|q_j)),"}, {"title": "3.2 Uncertainty-based Selection (Stage 2)", "content": "We define the LLM's overall understanding of the task using the mean uncertainty \u00b5 and the standard deviation o of the uncertainty estimates from the unlabeled set Q. A higher mean \u00b5 indicates a more challenging task for the LLM, while a higher standard deviation o reflects greater variability in question difficulty within Q. These two parameters provide insight into the usefulness of a question for improving the LLM's performance.\nFor instance, we hypothesize that when the mean uncertainty \u03bc is low (indicating the LLM is performing well on the task), selecting questions with uncertainties lower than u would not contribute valuable information. On the other hand, when the mean \u00b5 is high (suggesting the LLM struggles with the task), selecting questions with uncertainties significantly higher than u may lead to less informative or erroneous rationales.\nBased on these assumptions, we propose selecting a subset of questions Qs that fall within a specific uncertainty range, as defined by the following condition:\nQ_s = \\{q_j | U_{min} \\leq U_j < U_{max}\\}\nHere, Umin and Umax represent the minimum and maximum uncertainty thresholds used to select questions. In the subsequent section, we will detail the specific ranges (cf. Table 1) based on \u00b5 and \u03c3 for constructing demonstrations."}, {"title": "3.3 Demonstration Construction (Stage 3)", "content": "We adopt the demonstration construction methodology from Auto-CoT, which emphasizes diversity to mitigate the influence of incorrect rationales generated by the Zero-Shot-CoT method. The selected questions Qs are first encoded into vector representations using Sentence Transformers (Reimers and Gurevych, 2019). These vectors are then clustered using k-Means++ (Arthur and Vassilvitskii, 2007), forming k distinct clusters. From each cluster, the question closest to the cluster centroid is selected. The associated rationale and answer, generated by the Zero-Shot-CoT method, are then combined to form the demonstration set D. During inference, a test question q is appended to the constructed demonstration D and passed to the LLM for final predictions."}, {"title": "4 Experimental Setup", "content": "Datasets: We evaluate our proposed method on four challenging reasoning datasets. GSM8K (Cobbe et al., 2021) comprises arithmetic reasoning problems. StrategyQA (Geva et al., 2021) is a question-answering benchmark requiring implicit multi-hop reasoning. Logical Fallacy (referred to as Fallacy) (Jin et al., 2022) involves reasoning about arguments and detecting formal and informal fallacies. Epistemic Reasoning (EPR) (Sileo and Lernould, 2023) is a natural language inference task that challenges LLMs to reason about human mental states. For a fair comparison, we split all datasets, except GSM8K, into two sets using stratified sampling: (i) an unlabeled set (70%) for demonstration creation, and (ii) a test set (30%) for zero-shot performance evaluation. GSM8K already contains train and test sets, so no further split was needed.\nImplementation: We conduct experiments using five LLMs: GPT-40 (OpenAI, 2024), Mistral-7B-Instruct-v0.2 (Mistral) (Jiang et al., 2023), Phi-3-mini-4k-instruct (Phi3) (Abdin et al., 2024), text-davinci-002 (GPT3-XL), and text-davinci-003 (GPT3.5) (Brown et al., 2020). Note that this models are including both open-source (Phi3, Mistral) and proprietary models (GPT-40, GPT3.5, GPT3-XL). To ensure consistency with prior work such as Auto-CoT, we use k = 8 demonstrations for all datasets, except for StrategyQA, where we use k = 6. Additionally, during the evaluation of the LLMs, we set the temperature to 0 to ensure deterministic outputs, and report the average performance across three runs to maintain consistency in predictions.\nUncertainty Estimation in ZEUS: Uncertainty in ZEUS is estimated using a combination of three perturbation methods: (1) non-zero temperature decoding, (2) trigger phrase variation, and (3) question rephrasing. We use five trigger phrases: (Empty), \"Let's think step by step.\" (SS), \"Let's"}, {"title": "5 Results & Qualitative Analysis", "content": "5.1 Uncertainty Distribution Analysis\nIn this subsection, we present an analysis of the mean (\u00b5) and standard deviation (\u03c3) of uncertainty estimates for different LLMs across various reasoning datasets. In Figure 3, we illustrate the distribution of uncertainty estimates for GPT-3.5 on the GSM8K dataset. We have provided the comprehensive plots of the distributions in the appendix (see Figures 7\u201311). The mean \u00b5 and standard deviation \u03c3 of the uncertainty estimates using the unlabeled set Q has been shown through an error bar graph in Figure 2. Notably, LLMs such as GPT3-XL and Mistral show higher uncertainty in GSM8K, particularly with a larger deviation, whereas for tasks like StrategyQA and EPR, the uncertainty is generally more consistent across models, with GPT40 displaying the lowest variation. The trend highlights that model uncertainty is highly task-dependent, with complex reasoning tasks eliciting higher variability in predictions.\n5.2 Sensitivity of Uncertainty Estimates\nTo assess the sensitivity of uncertainty estimates in distinguishing between helpful and redundant questions, we investigate the relationship between confidence scores and answer accuracy. This is done by fitting a linear regression (LR) model between the confidence score of the most common answer"}, {"title": "5.3 Analysis of Selection Strategies", "content": "We present the normalized accuracy values for all selection strategies, including AutoCoT, in Figure 4. Our analysis reveals that AutoCoT was consistently outperformed by at least one other strategy across all LLMs and datasets. This indicates that leveraging uncertainty-based demonstration creation can more effectively identify valuable questions that enhance model performance. To provide a clearer perspective, Table 2 details the best and worst selection strategies for each model and dataset.\nFrom our observations, LLMs can be categorized into two groups: Advanced models and Simpler models. The advanced models, GPT-40, Phi3, and GPT-3.5, do not benefit from Trivial or Very Easy questions. Instead, they perform best with Hard and Challenging questions. This is because their higher capabilities allow them to handle more complex queries effectively. Conversely, the simpler models, including Mistral and GPT-3 XL, perform better with Trivial and Easy strategies. These models, with their lower Zero-Shot-CoT capabilities, gain more from even low-uncertainty questions. However, they struggle with Hard and Very Hard strategies, likely due to the increased presence of incorrect or less informative questions.\nAmong the selection strategies, Trivial and Very Hard tend to yield poorer performance across most models. This suggests that extremes in task difficulty\u2014whether too easy or too hard\u2014are generally detrimental to model accuracy. The Hard strategy generally improves performance for GPT-40, whereas the Challenging strategy appears to be optimal for Phi3, Mistral, and GPT-3.5. These findings align with the overall performance trends observed for these models.\nHowever, performance variations still exist across tasks and models. For instance, the Mistral model's performance declines with Moderate and harder strategies on the EPR task, while it improves with higher uncertainty estimates on other tasks. This indicates that selecting the optimal strategy can be complex and task-dependent. To address this, the next subsection will explore methods for determining the most effective selection strategy."}, {"title": "5.4 Choosing Optimal Selection Strategy", "content": "Upon constructing the demonstration for each strategy, we need to identify the optimal strategy for a given task and model. We calculate the average uncertainty on the unlabelled set Q while keeping the demonstration unchanged. The optimal strategy is the one with the lowest entropy, as this tends to strongly correlate with higher accuracy. Temp-Perb provides well-calibrated uncertainty estimates, although it lacks the sensitivity required to effectively differentiate between similar questions. Despite this limitation, its well calibration makes Temp-Perb suitable for selecting the best-performing strategy based on uncertainty estimates. Therefore, we use Temp-Perb for uncertainty estimation to determine the optimal selection strategy for a given model and task.\nIn Figure 6, we illustrate the accuracy of various selection strategies for GPT-40 in relation to Temp-Perb based uncertainty estimates. The data indicates that the accuracy is inversely correlated with uncertainty across all four datasets. This inverse relationship allows us to identify the optimal selection strategy as the one associated with the lowest uncertainty. We have included similar analyses for other models in the appendix (cf. Figures 12\u201316)."}, {"title": "5.5 Comparison with Baselines", "content": "The selection strategy with the lowest uncertainty is denoted as ZEUS (LU), while the strategy with the highest accuracy is represented by ZEUS (HA)."}, {"title": "6 Conclusion", "content": "This paper introduces the zero-shot uncertainty-basedZEUS method for evaluating and selecting optimal strategies based on uncertainty estimates. Our analysis reveals that ZEUS provides highly sensitive and reliable uncertainty estimates, outperforming temperature-based perturbation approaches (Temp-Perb) in distinguishing between helpful and redundant questions.\nOur findings classify models into two groups based on their optimal strategies. Advanced models like GPT-40, Phi3, and GPT3.5 perform best with Hard and Challenging example selection strategies, effectively leveraging their greater capabilities to tackle complex queries. In contrast, simpler models such as Mistral and GPT3-XL benefit more from Trivial and Easy strategies, where even low-uncertainty questions yield valuable information. By selecting the strategy with the lowest uncertainty estimates, ZEUS(LU) (recommended) achieves performance comparable to the best-performing strategies ZEUS(HA), without requiring manual annotations. Overall, ZEUS consistently matches or surpasses baseline accuracy, demonstrating its robustness and sensitivity in improving model performance."}, {"title": "7 Limitation", "content": "While our work demonstrates the effectiveness of the ZEUS method, there are several limitations and avenues for future research. First, the selection strategies in our current approach require exhaustive exploration to find the optimal strategy, which can be time-consuming and computationally expensive. This process could be automated by incorporating a greedy search algorithm based on uncertainty estimates, allowing for more efficient strategy selection. Another limitation is our reliance on uncertainty estimates from unlabeled questions, without examining the impact of dataset attributes like diversity or size. These factors could affect the estimates and lead to suboptimal strategy selection. Future work should explore these effects to improve robustness."}]}