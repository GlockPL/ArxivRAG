{"title": "SPIKING VISION TRANSFORMER WITH SACCADIC ATTENTION", "authors": ["Shuai Wang", "Malu Zhang", "Dehao Zhang", "Ammar Belatreche", "Yichen Xiao", "Yu Liang", "Yimeng Shan", "Qian Sun", "Enqi Zhang", "Yang Yang"], "abstract": "The combination of Spiking Neural Networks (SNNs) and Vision Transform-ers (ViTs) holds potential for achieving both energy efficiency and high perfor-mance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN counter-parts. Here, we first analyze why SNN-based ViTs suffer from limited perfor-mance and identify a mismatch between the vanilla self-attention mechanism and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. To address these issues, we draw inspiration from biological saccadic attention mechanisms and introduce an innovative Sac-cadic Spike Self-Attention (SSSA) method. Specifically, in the spatial domain, SSSA employs a novel spike distribution-based method to effectively assess the relevance between Query and Key pairs in SNN-based ViTs. Temporally, SSSA employs a saccadic interaction module that dynamically focuses on selected vi-sual areas at each timestep and significantly enhances whole scene understanding through temporal interactions. Building on the SSSA mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive experiments across various visual tasks demonstrate that SNN-ViT achieves state-of-the-art performance with linear computational complexity. The effectiveness and efficiency of the SNN-ViT highlight its potential for power-critical edge vision applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Vision Transformers (ViTs) (Dosovitskiy, 2020) revolutionize the traditional computer vision field, achieving higher performance in many vision tasks such as image classification (Chen et al., 2021; Han et al., 2023) and object detection (Fang et al., 2021c; Touvron et al., 2021). However, ViTs always demand significant computational and memory resources, which greatly restricts their de-ployment in resource-constrained edge vision environments (Wu et al., 2022; Graham et al., 2021). Consequently, the development of energy-efficient and high-performance solutions remains a sig-nificant area of research that necessitates further investigation (Cai et al., 2019; Han et al., 2020b).\nSpiking Neural Networks (SNNs), as the third generation of neural networks (Maass, 1997; Ger-stner & Kistler, 2002; Izhikevich, 2003; Masquelier et al., 2008), mimics biological information transmission mechanisms using discrete spikes as the medium for information exchange. Spiking neurons fire spikes only upon activation and remain silent at other times. This event-driven mech-anism (Caviglia et al., 2014) promotes sparse synapse operations and avoids multiply-accumulate (MAC) operations, which significantly boost the energy efficiency of these models (Zhang et al., 2023). However, the architectures of most SNN-based models still revolve around traditional struc-tures such as CNNs (Fang et al., 2021b; Xing et al., 2019) and ResNets (Fang et al., 2021a; Hu et al., 2024), which exhibit a significant performance gap compared to ViTs.\nIn recent years, numerous researchers have dedicated efforts to develop SNN-based ViT models. However, most studies (Zhou et al., 2023b; Wang et al., 2023b) retain energy-intensive MAC op-erations in self-attention computational paradigm and not fully take advantage of SNNs' energy"}, {"title": "2 RELATED WORK", "content": "Vision Transformers: ViTs segment images into patches and apply self-attention (Vaswani, 2017; Kenton & Toutanova, 2019) to learn inter-patch relationships, outperforming CNNs across multiple vision tasks (Mei et al., 2021; Bertasius et al., 2021; Guo et al., 2021). Nevertheless, ViTs face chal-lenges like high parameter counts (Liu et al., 2021b), and increased computational complexity pro-portional to token length (Pan et al., 2020; Liu et al., 2022). To enhance the computational efficiency of ViTs, many researchers (Jie & Deng, 2023; Li et al., 2023) are focused on exploring lightweight improvement methods. For example, LeViT (Graham et al., 2021) incorporates convolutional el-ements to expedite processing, and MobileViT (Mehta & Rastegari, 2021) combines lightweight MobileNet blocks with MHSA, achieving lightweight ViTs successfully. However, these enhance-"}, {"title": "3 PROBLEM ANALYSIS IN SPIKING SELF-ATTENTION", "content": "In this section, we analyze the mismatches between vanilla self-attention mechanisms and SNNs in both the spatial and temporal domains. The detailed discussion is provided in the following sections."}, {"title": "3.1 DEGRADED SPATIAL RELEVANCE", "content": "The vanilla self-attention measures the spatial relevance between Q and K through Dot-Product operation. For a given query $Q_i$ and key $K_i$ vector, the relevance between them are as follows:\n$Dot-Product (Q_i, K_i) = \\sum_{j=1}^{D} Q_{ij} K_{ij}$                                                                 (1)\nwhere $D$ is the dimension of both vectors, $Q_{ij}$ and $K_{ij}$ refer to the $j$-th elements of these vectors, respec-tively. Notably, the relevance based on Dot-Product takes into account both the angle and magnitude of the vectors (Kim et al., 2021). When there is a significant difference in magnitude between vec-tors, the Dot-Product may not accurately measure their spatial relevance.\nIn ANNs, continuous input $X$ is first normalized using layer normalization (Dosovitskiy, 2020) and then be processed through linear transformations $W_Q$ and $W_K$ to derive the matrices $Q$ and $K$. This ensures that the magnitudes of $Q$ and $K$ are closely matched (Xu et al., 2019), preventing large variations between vectors. As shown in the left part of Fig.1, the distribution between $Q$ and $K$ across various datasets remains nearly identical, allowing effective measuring of the spatial relevance for attention score in ANNS.\nDue to the discrete activation characteristics of spiking neurons, the continuous distribution of the normalized membrane potentials in $Q$ and $K$ is disrupted. As shown in the right part of Fig. 1, the magnitude of $Q$ and $K$ in SNNs shows significant variability, which leads to the failure of the Dot-Product in measuring spatial relevance. Moreover, despite $Q$ and $K$ following identical distributions, the sparsity of binary spikes significantly reduces their stability compared to ANNs. We provide a detailed analysis of this assertion in Appendix. A. Therefore, developing more effective methods to measure the spatial relevance between spike trains could be a viable approach to enhancing the performance of SNN-based ViTs."}, {"title": "3.2 LIMITED TEMPORAL INTERACTION", "content": "As shown in Fig. 2(a), vanilla self-attention in ViTs operates independently of timesteps, thereby preventing the need for temporal interaction in self-attention designs. Conversely, SNNs rely on multiple timesteps to enrich their information representation capabilities (Fang et al., 2021b). How-ever, as shown in Fig. 2(b), most spike self-attention mechanisms (Yao et al., 2024b; Zhou et al., 2023b; Shi et al., 2024) lack dedicated modules for the temporal domain. The only temporal in-teraction in those methods is the accumulation of historical information by spiking neurons (LIF neurons), whose dynamics can be described as:\n$U[t+1] = H[t] + X[t + 1]$, (2)\n$S[t+1] = \\Theta(U[t+1] \u2013 V_{th})$,(3)\n$H[t+1] = V_{reset}S[t+1] + \\tau U[t+1](1 \u2013 S[t + 1])$.(4)\n$X[t+1]$ denotes the spatial input current, while $H[t]$ and $U[t]$ represent the pre-synaptic and post-synaptic membrane potentials, respectively. The Heaviside function $\\Theta (\u00b7)$ is employed for spike generation. If a spike occurs ($S[t + 1] = 1$), $H[t]$ resets to $V_{reset}$; otherwise, $U[t + 1]$ decays with a time constant $$\\tau$ and feeds into $H[t+1]$. However, due to the reset and decay mechanism, the residual membrane potential cannot sustain long-range dependencies, resulting in a significant loss of historical information. To solve this problem, (Wang et al., 2023b) proposes a spatio-temporal spike self-attention method as shown in Fig. 2(c). But this method has $O(T^2N^2D)$ computational complexity, significantly restricting the training efficiency of SNNs and increasing the complexity of deployment. Therefore, achieving more effective spatio-temporal interactions without increasing computational overhead remains a pressing challenge."}, {"title": "4 SACCADIC SPIKING SELF-ATTENTION MECHANISM", "content": "We introduce a Saccadic Spiking Self-Attention (SSSA) method tailored for the spatio-temporal characteristic of SNNs. Spatially, SSSA enhances relevance measurement between spike vectors Q and K based on their distribution forms. Temporally, it incorporates a dedicated saccadic interaction module for dynamic contextual comprehension of the visual scene. Additionally, we advance SSSA to version V2, which retains the high performance of SSSA and reduces computational complexity from $O(N^2)$ to $O(D)$."}, {"title": "4.1 SPATIAL RELEVANCE COMPUTATION FROM SPIKE DISTRIBUTION", "content": "To mitigate the issue of degraded spatial relevance caused by Dot-Product operations, we introduce a novel distribution-based approach. It directly measures the relevance between two vectors using cross-entropy, unaffected by their magnitudes. Further details can be found in Appendix B.\nFor a patch $x \\in R^D$ in either Q or K, it can be treated as a D-dimensional {0,1} random spike train, where p represents the spike firing rate. The cross-entropy between patches $q \\in Q$ and $k \\in K$ is given by:\n$H (q, k) = \u2212 [p_q log p_k + (1 \u2212 p_q) log (1 \u2212 p_k)]$,(5)\nwhere $p_q$ and $p_k$ denote the firing rates for vectors q and k, respectively. Since both q and k are spike trains, our focus shifts to the distribution of spikes rather than silent states. Consequently, we primarily consider the first term of Eq. 5, allowing us to simplify $H (q, k)$ to $-p_q log p_k$. Given that both log(x) and x maintain the same monotonicity, substituting log(x) with x is a feasible simplification that preserves the effectiveness of H(q, k), while avoiding nonlinear computations. Detailed analysis is provided in Appendix \u0412.\nSince cross-entropy H(q, k) measures negative relevance, we take its negative as our attention result. As a result, the cross-attention between Q and K, denoted as CroAtt (Q, K) = \u2212H(Q, K), can be further expressed as:\n$CroAtt (Q, K) = Q'K'^T, Q' = \\sum_{j=1}^{D} Q, K' = \\sum_{j=1}^{D} K, Q,K \\in R^{T\u00d7ND}$(6)\nAs illustrated in Fig. 2(c), Q' and K' represent the sum of spikes across the dimension D. This approximation allows for more efficient parallel computation of spatial relevance between Q and K. By employing this distribution-based method, we more accurately assess the relevance between vectors with non-standard distributions, thereby addressing the issue of degraded spatial relevance."}, {"title": "4.2 SACCADIC TEMPORAL INTERACTION FOR ATTENTION", "content": "Biological saccadic mechanisms do not process all visual information at once. Instead, they progres-sively focus on key visual areas within a scene Guadron et al. (2022). This ensures that biological systems can efficiently achieve contextual understanding of the entire visual scene. Inspired by this mechanism, we have designed an effective temporal interaction module that incorporates two critical processes: salient patch selection and saccadic context comprehension. The first process selectively computes only a subset of patches at each timestep, while ignoring the others. It can significantly reduce the computational complexity of the SSSA method. This process can be described as:\n$Patch = \\sum_{j=1}^{n} CroAtt (Q, K), CroAtt (Q, K) \\in R^{T\u00d7N\u00d7N}$,(7)\nCroAtt(Q, K) represents the spatial relevance between patches in Q and K. By summing the rows of the CroAtt(Q, K) matrix, the Patch represents the spatial salience of patches. Subsequently, the saccadic interaction module makes contextual understanding based on Patch. To ensure the asyn-chronous characteristics of SNNs, we aim to integrate the interaction process into spiking neurons. However, the significant historical forgetting caused by the resetting and decay mechanism of LIF neurons prevents efficient interaction. Therefore, we introduce a plug-and-play saccadic spiking neuron, whose dynamic during training and inference phases can be described as follows:\n$Training \\begin{cases} H = M_w Patch \\\\ S = \\Theta (H - V_{th}) \\end{cases}$\n$Inference \\begin{cases} H[t] = Patch[t] \\\\ S[t] = \\Theta (H[t] \u2013 M_w^{-1}V_{th}[t]) \\end{cases}$(8)\nHere, H, S, Patch \u2208 RT\u00d7N represents the data format for parallel training, encompassing the entire temporal dimension. Mw is a learnable lower triangular matrix that precisely regulates con-tributions from each timestep, facilitating efficient temporal interactions. Utilizing Mw to compute membrane potentials, saccadic spiking neurons avoid decay or resetting disruptions. As shown in Fig.3, we depict the dynamic process of saccadic spiking neurons. During training, the membrane potential of saccadic spiking neurons is represented as $\\sum w_{it}Patch[t]$, $w_{it} \\in M_w$. However, all timesteps are processed simultaneously via matrix multiplication, which requires substantial com-putational resources. To maintain SNNs' energy efficiency, we propose an asynchronous inference decoupling method. By incorporating the inverse of $M_w$ into the threshold levels of the saccadic spiking neurons, we ensure temporal decoupling between H and S. The spike firing process depends solely on the current values of H[t] and $V_{th} M_w^{-1}[t]$, eliminating the need for historical information. Thus, saccadic spiking neurons ensure the capability for asynchronous inference. Notably, the tem-poral complexity of saccadic spiking neurons is only O (T), significantly superior to the O (T2). The dynamics of saccadic spiking neurons are detailed in Appendix.C."}, {"title": "4.3 LINEAR COMPLEXITY AND SPIKE-DRIVEN COMPUTATION", "content": "Building on the aforementioned components, SSSA is specifically designed for the spatio-temporal characteristics of SNNs. It enables a more effective comprehensive understanding of the entire visual scene with lower time complexity. Its formulation is described as follows:\n$SSSA (Q, K, V) = \\Theta (M_w Patc h[0,\u2026\u2026, t] \u2013 V_{th}) \u00b7 V = \\Theta (M_w(Q' \u00d7 K'^T) L \u2013 V_{th}) \u00b7 V$,(9)\nwhere L represents a column vector [1, 1, . . ., 1] with dimension N, facilitating the summation of row elements. However, as depicted in Fig. 2(c), SSSA includes integer multiplication operations within $Q' \u00d7 K'$, compromising the energy efficiency of the SNNs. Moreover, the quadratic com-plexity of $Q' \u00d7 K'$ still indicates potential for optimization. Given that the matrix multiplications in Eq.9 do not involve nonlinear operations, they allow for free association of matrices without al-tering the computational sequence. Consequently, to avoid the need for integer multiplication and further reduce computational complexity, we conduct an linear scaling mapping of Eq.9, which can be described as follows:\n$SSSA (Q, K, V) = \\Theta ((M_w \u00d7 Q') (K'^T \u00d7 L) \u2013 V_{th}) \u2022 V$(10)\nIn SSSA-V2, computations begin with the calculation of Q' and K' based on Qand K, each with a complexity of O(D). Then, instead of calculating $Q' \u00d7 K'$, SSSA-V2 treats $(K'^T \u00d7 L)$ as a learnable scaling factor \u03b1, applied to the threshold Vth of the saccadic neuron. Subsequently, $M_w \u00d7 Q'$ as Patch[i] input into the saccadic neurons. During the inference process, $M_w$ can be integrated into the thresholds of saccadic neurons to maintain a fully spike-driven system.\n$Inference \\begin{cases} H[t] = Q'[t], \\\\ S[t] = \\Theta (\\frac{H[t]}{\\alpha} - (M_w^{-1}V_{th}) [t]). \\end{cases}$(11)\nMathematically, SSSA-V2 is linear scaling mapping to SSSA, preserving all the advantages of SSSA while significantly reducing the need for integer multiplication operations. Additionally, SSSA-V2 achieves a linear spike self-attention mechanism with total computational complexity of $O(2D+N)$, offering significant benefits in resource-constrained environments."}, {"title": "5 SNN-BASED SACCADIC VISION TRANSFORMER", "content": "As illustrated in Fig.4, we introduce a novel SNN-ViT based on the proposed SSSA method. It primarily consists of GL-SPS blocks and SSSA-based transformer blocks. The following section will provide detailed descriptions of these components."}, {"title": "5.1 GL-SPS: GLOBAL-LOCAL SPIKING PATCH SPLITTING MODULE", "content": "Currently, existing SPS methods primarily rely on shallow spiking convolution modules to capture local information, which prevents the effective extraction of multi-scale features. This limitation leads to degraded performance in processing wide-field image features. To address this issue, we design the Global-Local convolutional SPS block, described as follows:\n$GL-SPS (X[t]) = BN (Conv(X[t]) + BN (DConv (X[t])),$(12)\nwhere $X[t]$ is the result of a convolution operation with a stride of 2. Conv2d and D-Conv2d rep-resents standard and dilated convolution operations. BN(\u00b7) is Batch Normalization. The GL-SPS utilizes both Conv2d and D-Conv2d to extract features. Combining layers with different dilation rates effectively gathers context from various visual scales. Consequently, SNN-ViT employs the GL-SPS method as its embedding module, enhancing efficiency and scalability in feature extraction."}, {"title": "5.2 OVERALL ARCHITECTURE", "content": "Building upon the pyramid structure (Liu et al., 2021b; Yu et al., 2023), we propose a novel SNN-ViT that incorporates the GL-SPS block and the SSSA method. GL-SPS part encodes the input image through downsampling operation and various convolutions operation. Specifically, the down-sampling operation is defined as a convolution operation with a kernel size of 7 and a stride of 2. The GL-SPS method follows the previous section. The whole block is defined as follows:\n$U_o = GL-SPS (I), I \\in R^{T\u00d7C\u00d7H\u00d7W}$(13)\nSubsequently, $U_o$ is inputted into the SSSA-based block, which consists of SSSA method and MLP Layer. To further reduce the computational complexity of the model, we adopt the SSSA-V2 version as the paradigm for self-attention computation in the architecture. Subsequently, the output from the SSSA-based Transformer blocks is fed into the Global Average Pooling (GAP) module, followed by a Classification Head (FCH) that generates the prediction Y. These parts can be defined as:\n$U_1 = U_o + BN(Conv([SSSA(SN(U_o))])), U_o \\in R^{T\u00d7NXD}$(14)\n$U_2 = U_1 + BN(Linear[SN(U_1)]), U_1 \\in R^{T\u00d7NXD}$(15)\n$Y = FCH(GAP(SN(U_2))),$(16)\nwhere Y denotes the predicted outcome. For different types of datasets, we can integrate the GL-SPS component with varying numbers of SSSA decoding blocks. The details of the network architecture and the parameter count are presented in Appendix.F."}, {"title": "6 EXPERIMENTS", "content": "6.1 IMAGE CLASSIFICATION\nSNN-ViT is evaluated on both static and neuromorphic datasets, including CIFAR10, CI-FAR100 (Krizhevsky et al., 2009), ImageNet (Deng et al., 2009) and CIFAR10-DVS (Li et al., 2017). Specifically, for ImageNet, the input image size is 3 \u00d7 224 \u00d7 224, with batch sizes of 128, and training epoch is conducted over 310. Our experimental results are summarized in Table.1 and 2."}, {"title": "6.2 REMOTE OBJECT DETECTION", "content": "Given the high adaptability of biological saccadic mechanisms to dynamic visual tasks, we aim to apply SNN-ViT to object detection tasks to demonstrate its advantages. As SNNs are often em-ployed in resource-constrained edge computing scenarios, we select two remote sensing datasets: NWPU VHR-10 Cheng et al. (2017) and SSDD (Wang et al., 2019). The NWPU VHR-10 dataset comprises very high-resolution (VHR) images across ten categories, including airplanes, ships, stor-age tanks, baseball diamonds, tennis courts, basketball courts, ground track fields, harbors, bridges, and vehicles. The SSDD dataset focuses on ship detection using Synthetic Aperture Radar (SAR)"}, {"title": "6.3 ABLATION STUDY", "content": "To verify the effectiveness of each component in the SNN-ViT, we perform a comprehensive ablation study in the CIFAR100 dataset. The Spikformer (Zhou et al., 2023b) is selected as the baseline for comparison. Subsequently, we replace the corresponding modules in the base-line with SSSA blocks and GL-SPS blocks to assess their impact on performance. As shown in Table 4, replacing our SSSA method im-"}, {"title": "7 CONCLUSION", "content": "This work provides a detailed analysis of the mismatch between the vanilla ViT and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. Inspired by the biological saccadic attention mechanism, this work proposes a SSSA method tailored to the SNNs. In the spatial dimension, SSSA employs a more efficient distribution-based approach to compute the spatial relevance between Query and Key in SNNs. In the temporal domain, SSSA utilizes a dedicated saccadic interaction module, calculating only a subset of patches at each timestep to dynamically understand the context of the entire visual scene. Building on SSSA method, we de-velop a SNN-ViT structure, which achieves state-of-the-art performance across various visual tasks with linear computational complexity. SNN-ViT effectively integrates advanced biological cogni-tive mechanisms with artificial intelligence techniques, providing a promising avenue for exploring high-performance, energy-efficient edge visual tasks."}, {"title": "A LIMITATIONS OF DOT-PRODUCT FOR SPIKE TRAINS", "content": "The Dot-Product is the operation to measure relevance between two vectors u and v in an n-dimensional space, which is defined as:\n$u v = \\sum_{i=1}^{n} u_i v_i = ||u|||| v || cos \u03b8$(17)\nwhere $u_i$ and $v_i$ are the components of vectors u and v respectively, $||u||$ and $||v||$ denote the mag-nitudes (norms) of the vectors, and \u03b8 is the angle between them. This expression clearly illustrates that the Dot-Product is influenced by both the magnitudes of the vectors and the cosine of the angle between them. Variations in either magnitude or angle will affect the result of the Dot-Product, thus affecting the measure of relevance between the vectors.\nProblem: If the Q and K are controlled to be similar distributions in SNNs, would the effec-tiveness of the Dot-Product still be influenced by magnitude differences?\nAnalysis: To deepen our investigation, we present the following mathematical assumptions: assum-ing query Q and the key vector K in SNNs are independent and share the same firing rate. Then we examine the\nLet $x = (x_1,x_2,...,x_D) \\in \\{0,1\\}^D$ represent q or k, where each element $x_i$ takes the value 1 with probability p. The square of the magnitude follows a binomial distribution:\n$||x||^2 = \\sum_{i=1}^{D} x_i^2 = \\sum_{i=1}^{D} x_i ~ B(D, p)$,(18)\nIts probability is given by:\n$P(||x||^2 = k) = \\binom{D}{k} p^k (1-p)^{D-k}, k = 0, 1, 2, ..., D$.(19)\nWe randomly select a q and a k from this distribution and calculate their magnitude ratio $R = \\frac{||q||}{||k||}$. Without considering the case when ||k|| = 0, the calculation proceeds as follows:\n$P(R = r) = P(R^2 = r^2)$$\n$= \\sum_{k=0}^{n} \\sum_{l=1}^{n} I(\\frac{k}{l} = r^2) P(X = k) P(Y = l)$$\n$= \\sum_{k=0}^{n} \\sum_{l=1}^{n} I(\\frac{k}{l} = r^2) \\binom{n}{k} \\binom{n}{l} p^{k+l}(1 - p)^{2n-k-l}$(20)\nwhere $I(\\frac{k}{l} = r^2)$ is the indicator function, which equals 1 when $\\frac{k}{l} = r^2$ and 0 otherwise.\nGiven the complexity of this distribution, we employ experimental simulation for approximation. Referencing the data shown in Fig. 1, we set p = 0.15 and D = 128. As the Dot-Product opera-tion is symmetric, we adjust our calculation to ensure that the numerator is always greater than or equal to the denominator, enhancing the clarity of our visualization. Specifically, we compute $\\frac{||q||}{||k||}$ when ||q|| > ||k||, and $\\frac{||k||}{||q||}$ otherwise. The simulation results are shown in Fig.6(b). Clearly, the distribution of $\\frac{||q||}{||k||}$ is notably disordered. For comparison, we conduct the same assumptions and simulations in the self-attention module of ANN. Let $x = (x_1, x_2, ..., x_D) \\in R^D$ represent q or k. Then its magnitude is given by $||x|| = \\sqrt{\\sum_{i=1}^{D} x_i^2}$. Similarly, we randomly select a q and a k and simulate the distribution of their magnitude ratio R'. Based on the data in Fig. 1, we approximate each element $x_i$ as independently normally distributed with x ~ N(35, 10). The results are shown in Fig.6(c). By calculating the variance of $\\frac{||q||}{||k||}$, it is found to be approximately 0.2322 in SNNs and only around 0.00844 in ANNs. This indicates significant magnitude fluctuations in SNNs, re-vealing a high degree of instability. As a result, the efficiency and effectiveness of the Dot-Product computation are negatively impacted."}, {"title": "B CROSS-ENTROPY FOR BETTER RELEVANCE COMPUTATION", "content": "We calculate the relevance in spatial dimensions separately for the Q and K vectors at different moments. The vectors $q \\in Q$ and $k \\in K$, and $p_q$ and $p_K$ represents the spike firing rate for them. Here, we introduce a cross-entropy method to more effectively compute the relevance between q and k vectors:\n$H (q, k) = \u2212 [p_q log p_k + (1 - p_q) log (1 \u2212 p_k)]$.(21)\nThe former term quantifies the degree of relevance when predictions are positive, while the latter reflects the relevance of negative predictions. When using cross-entropy as a measure of relevance, spike trains are first normalized and then transformed into probability distributions. Spike trains typically comprise only two states: spike and silence. Therefore, the probability distribution primar-ily reflects the spike firing rate. This measurement approach focuses on comparing the differences between two probability distributions, disregarding their magnitudes. In summary, cross-entropy is an effective distribution-based tool for assessing the relevance of spike trains, allowing for more precise comparisons and evaluations of similarities and differences between Q and K.\nApproximation Methodology: Although cross-entropy is an effective method for measuring rele-vance, a comprehensive analysis of spike states and silent periods may reduce the system's sensitiv-ity. This is primarily because excessive focus on inactive silent periods can obscure critical informa-tion present during active spike periods when dealing with sparse spike trains. Given our focus on the spike states rather than silent periods in subsequent analyses, we can neglect the (1 \u2013 pq) log(1-Pk) component. Therefore, H (q, k) can be simplified as follows:\n$H (a, b) \u2248 -p_q log p_k$,(22)\nwhere $p_a$ and $p_b$ respectively represent the spike firing rates. However, the log(\u00b7) function introduces non-linear operations that compromise the energy efficiency of SNNs. To address this, we propose a further approximation and simplification.\nAs described in the previous section, within the Transformer blocks of the SNNs, the spike firing rate of the Q vector and K vector primarily range from 10% to 20%. Consequently, we perform a Taylor expansion of log(x) at x = 0.15. This can be expressed as:\n$log(x) \u2248 log(0) (0.15) + \\frac{log^{(n)} (0.15)}{n!} (x - 0.15)^n$(23)\nHere, log(n) function denotes the result of the n-th order derivative of the log(\u00b7) function. Given that x is essentially between 0.1 and 0.2, The terms (PQ \u2013 0.15)\u00b2 and higher-order terms are very small, which can be neglected. Consequently, H(A, B) can consider only the first term of the expansion:\n$log(x) \u2248 log(0) (0.15) + \\frac{log^{(1)} (0.15)}{1!} (x - 0.15) \u2248 kx + b$(24)\nIn the training process of SNNs, since k and b can be learned as weights and biases, we use x to replace log(x) to simplify computations. Although this may introduce slight errors, it avoids nonlinear operations and significantly enhances the network's energy efficiency."}, {"title": "C SACCADIC TEMPORAL INTERACTION", "content": "Saccadic mechanism in biologic Vision: Numerous neuroscience findings(Melcher & Morrone, 2003; Binda & Morrone, 2018; Guadron et al., 2022) confirm that the eyes do not acquire all details of a scene simultaneously. Instead, attention is focused on specific regions of interest (ROIs) through a series of rapid saccadic called saccades. Each saccade lasts for a very brief period-typically only tens of milliseconds\u2014allowing the retina's high-resolution area to sequentially align with different visual targets. This dynamic saccadic mechanism enables the visual system to process information efficiently by avoiding redundant processing of the entire visual scene.\nOther similar works inspired by visual mechanisms: Zhao et al. (2021) introduces a model uti-lizing a retina-inspired spiking camera to enhance image clarity in high-speed motion scenarios. McIntosh et al. (2016) explores how deep convolutional neural networks can model the retina's response to natural scenes. Tanaka et al. (2019) discusses the use of deep learning models to un-derstand the computational mechanisms of the retina. These advanced features of biological vision effectively inform the rational design of deep neural networks, promoting the efficient integration of biological and machine intelligence.\nLeaky Integrate-and-Fire (LIF) neuron model: In the LIF models, resetting and decay mech-anisms significantly impair the neuron's ability to retain long-term historical information. The model's dynamics are described by the differential equation:\n$\\tau_m \\frac{dV}{dt} = -(V(t) - V_{rest}) + RI(t)$,(25)\nwhere V(t) is the membrane potential, Vrest is the resting potential, $$\\tau_m$ is the membrane time con-stant which influences decay rate, R is the membrane resistance, and I(t) is the input current. This equation illustrates how the membrane potential responds to input currents and decays towards Vrest. When the membrane potential V(t) reaches the threshold Vth, the neuron fires and resets the poten-tial to Vreset. This resetting process can be mathematically described as:\nV(t+) = V_{reset} if V(t) \u2265 V_{th},(26)\nwhere t+ is the time immediately following the spikes. This resetting not only disrupts the continuity of V(t) but also eliminates all accumulated potential exceeding the threshold. Moreover, in the absence of input (I(t) = 0), the decay mechanism mercilessly forces the membrane potential to exponentially converge to the resting potential Vrest, following the equation:\n$V(t) = V_{rest} + (V_o - V_{rest})e^{\\frac{t}{\\tau_m}}$,(27)\nwhere $V_o$ is the initial potential. This decay process gradually diminishes the stored information in the neuron, causing the accumulated potential to disappear quickly over time. It severely limits the neuron's ability to maintain historical information. To address this issue, we specifically designed saccadic spiking neurons without decay and reset mechanisms. The training and inference processes are described as follows."}, {"title": "D SACCADIC NEURONS", "content": "Training Phase: In the training process of SNN-ViT", "follows": "n$H = M"}]}