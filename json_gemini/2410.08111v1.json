{"title": "Active Fourier Auditor for Estimating\nDistributional Properties of ML Models", "authors": ["Ayoub Ajarra", "Bishwamittra Ghosh", "Debabrota Basu"], "abstract": "With the pervasive deployment of Machine Learning (ML) models\nin real-world applications, verifying and auditing properties of ML models\nhave become a central concern. In this work, we focus on three properties: ro-\nbustness, individual fairness, and group fairness. We discuss two approaches\nfor auditing ML model properties: estimation with and without reconstruc-\ntion of the target model under audit. Though the first approach is studied\nin the literature, the second approach remains unexplored. For this purpose,\nwe develop a new framework that quantifies different properties in terms of\nthe Fourier coefficients of the ML model under audit but does not paramet-\nrically reconstruct it. We propose the Active Fourier Auditor (AFA), which\nqueries sample points according to the Fourier coefficients of the ML model,\nand further estimates the properties. We derive high probability error bounds\non AFA's estimates, along with the worst-case lower bounds on the sample\ncomplexity to audit them. Numerically we demonstrate on multiple datasets\nand models that AFA is more accurate and sample-efficient to estimate the\nproperties of interest than the baselines.", "sections": [{"title": "INTRODUCTION", "content": "As Machine Learning (ML) systems are pervasively being deployed in high-stake applications,\nmitigating discrimination and guaranteeing reliability are critical to ensure the safe pre and post-\ndeployment of ML (Madiega, 2021). These issues are addressed in the growing subfield of ML, i.e.\ntrustworthy or responsible ML (Rasheed et al., 2022; Li et al., 2023), in terms of robustness and fair-\nness of ML models. Robustness quantifies how stable are a model's predictions under perturbation\nof its inputs (Xu and Shie, 2011; Kumar et al., 2020). Fairness (Dwork et al., 2012; Barocas et al.,\n2023) seeks to address discrimination in predictions both at the individual level and across groups.\nThus, AI regulations, such as the European Union AI Act (Madiega, 2021), increasingly suggest\ncertifying different model properties, such as robustness, fairness, and privacy, for a safe integration\nof of ML in high-risk applications. Thus, estimating these model properties under minimum inter-\nactions with the models has become a central question in algorithmic auditing (Raji et al., 2020;\nWilson et al., 2021; Metaxa et al., 2021; Yan and Zhang, 2022).\nEXAMPLE 1. Following (Ghosh et al., 2021, Example 1), let us consider an ML model that\npredicts who is eligible to get medical insurance given a sensitive feature 'age', and two non-sensitive\nfeatures 'income' and 'health'. Owing to historical bias in the training data, the model, i.e. an\nexplainable decision tree, discriminates against the 'elderly' population by denying their health\ninsurance and favors the \u2018young' population. Hence, an auditor would realize that the model does\nnot satisfy group fairness since the difference in the probability of approving health insurance\nbetween the elderly and the young is large. In addition, the model violates individual fairness,\nwhere perturbing the feature 'age' from elderly to young increases the probability of insurance.\nFurther, the model lacks robustness if perturbing any feature by an infinitesimal quantity flips the\nprediction.\nRelated Work: ML Auditing. Towards trustworthy ML, several methods have been proposed\nto ally audit an ML model by estimating different distributional properties of it, such as fairness\nand robustness, where the model hyper-property has to be assessed against the distribution of\ninputs. A stream of work focuses on property verification that verifies whether these properties are\nviolated above a pre-determined threshold (Goldwasser et al., 2021; John et al., 2020; Mutreja and\nShafer, 2023; Herman and Rothblum, 2022; Kearns et al., 2018). Thus, we focus on estimating these\nproperties instead of a 'yes/no' answer, which is a harder problem than verification (Goldwasser\net al., 2021). On estimating distributional properties, Neiswanger et al. (2021) proposed a Bayesian\napproach for estimating properties of black-box optimizers and required a prior distribution of\nmodels. Wang et al. (2022) studies simpler distributional properties, e.g. the mean, the median, and\nthe trimmed mean defined as a conditional expectation, using offline and interactive algorithms.\nYan and Zhang (2022) considered a frequentist approach for estimating group fairness but assumed\nthe knowledge of the model class and a finite hypothesis class under audit. These assumptions\nare violated if we do not know the model type and can be challenging for complex models, e.g.\ndeep neural networks. Albarghouthi et al. (2017); Ghosh et al. (2021) considered finite models for\nestimating group fairness w.r.t. the features distribution, and Ghosh et al. (2022) further narrowed\ndown to linear models. Therefore, we identify the following limitations of the existing methods in ML\nauditing. (1) Property-specific auditing: most methods considered a property-specific tailored\napproach to audit ML systems, for example either robustness (Cohen et al., 2019; Salman et al.,\n2019), group fairness (Albarghouthi et al., 2017; Ghosh et al., 2021), or individual fairness (John"}, {"title": "BACKGROUND", "content": "Before proceeding to the contributions, we discuss the three statistical properties of ML models\nthat we study, i.e. robustness, individual fairness, and group fairness. We also discuss basics of\nFourier analysis that we leverage to design AFA.\nNotations. Here, a represents a scalar, and x represents a vector. X is a set. We denote [1, n] as\nthe set {1, ..., n}. We denote the power set of X by P(X).\nProperties of ML Models. A Machine Learning (ML) model h is a deterministic or probabilis-\ntic mapping from an n-dimensional input domain of features (or covariates) X to set of labels\n(or response variables or outcomes) Y. For example, for Boolean features $X \\backsimeq {-1,1}^n$, and for\ncategorical features, $X \\backsimeq [K]^n$. For binary classifiers, $y \\backsimeq {0,1}$.\nWe assume to have only black-box access to h, i.e. we send queries from a data-generating distri-\nbution and collect only the labels predicted by h. The dataset on which h is tested is sampled from\na data-generating distribution Dx,y over X \u00d7 Y, which has a marginal distribution D over X.\nWe aim to audit a distributional (aka global) property $\u03bc : H \u00d7 D_{x,y} \u2192 R$ of an ML model\n$h : X \u2192 Y$ belonging to an unknown model class H while having only black-box access to h.\nHereafter, we develop the methodology for binary classifiers and Boolean features. Later, we\ndiscuss approaches to extend the proposed methodology to categorical features and multi-class\nclassifiers, and corresponding experimental results. In this paper, we study three properties of ML\nmodels, i.e. robustness (\u00b5Rob), individual fairness (\u00b5IFair), and group fairness (\u00b5GFair), which are\ndefined below.\nRobustness is the ability of a model h to generate same output against a given input and its\nperturbed (or noisy) version. Robustness has been central to sub-fields of AI, e.g. safe RL (Garcia\nand Fern\u00e1ndez, 2015), adversarial ML (Kurakin et al., 2016; Biggio and Roli, 2018), and gained\nattention for safety-critical deployment of AI.\nDEFINITION 1 (Robustness). Given a model h and a perturbation mechanism F of input $x \u2208 X$,\nrobustness of h is $\u00b5_{Rob}(h) = P_{x\\sim D, y\\sim F(x)} [h(x) \\neq h(y)]$.\nExamples of perturbation mechanisms include Binary feature flipping $N_p(x) = {x' | \\forall i \u2208\n[n], x'_i = x_i \\times Bernoulli(p)}$ (O'Donnell, 2014), Gaussian perturbation $N_p(x) = {x' | x' = x +\ne where e ~ Normal(0, p^2I)}$, among others.\nIn trustworthy and responsible AI, another prevalent concern about deploying ML models is\nbias in their predictions. This has led to the study of different fairness metrics, their auditing\nalgorithms, and algorithms to enhance fairness (Mehrabi et al., 2021; Barocas et al., 2023). There\nare two categories of fairness measures (Barocas et al., 2023). The first is the individual fairness\nthat aims to ensure that individuals with similar features should obtain similar predictions (Dwork\net al., 2012).\nDEFINITION 2 (Individual Fairness). For a model h and a neighbourhood F(x) of a $x \u2208 X$, the\nindividual fairness discrepancy of h is $\u00b5_{IFair}(h) = P_{x\\sim D, y\\sim F(x)} [P[h(x) \\neq h(y)]$.\nThe neighborhood F(x) is commonly defined as the points around a which are at a distance less\nthan $\u03c1 \u2265 0$ w.r.t. a pre-defined metric. The metric depends on the application of choice and the"}, {"title": "ACTIVE FOURIER AUDITOR", "content": "input data (Mehrabi et al., 2021). IF of a model measures its capacity to yield similar predictions\nfor similar input features of individuals (Dwork et al., 2012; A. Friedler et al., 2016). The similarity\nbetween individuals are measured with different metrics. Let dx and dy be the metrics for the\nmetric spaces of input (X) and predictions (V), respectively.\nA model h satisfies (\u20ac,e')-IF if $d_X(x,x') < \u03b5$ implies $d_Y(h(x), h(x')) \u2264 \u03b5'$ for all (x,x') \u0454\n$X^2$ (A. Friedler et al., 2016). For Boolean features and binary classifiers, the natural candidate\nfor dx and dy is the Hamming distance. This measures the difference between vectors x and x'\nby counting the number of differing elements. Thus, $d_X(x, x') < l$ means that x' has l different\nbits than x. As auditors, we are interested in measuring how much the Hamming distance between\noutcomes of x and x', i.e. e'. However, since the data-generation process and the models might be\nstochastic, we take a stochastic view and use a perturbation mechanism that defines a neighborhood\naround each input sample.\nGroup fairness is the other category of fairness measures that considers the input to be gener-\nated from multiple protected groups (or sub-populations), and we want to remove discrimination in\npredictions across these protected groups (Mehrabi et al., 2021). Specifically, we focus on Statistical\nParity (SP) (Feldman et al., 2015; Dwork et al., 2012) as our measure of deviation from group\nfairness. For simplicity, we discuss SP for two groups, but we can also generalize it to multiple\ngroups.\nDEFINITION 3 (Statistical Parity). The statistical parity of h is $\u00b5_{GFair}(h) = |P_{x\\sim D}[h(x) =\n1|x_A = 1] - P_{x\\sim D}[h(x) = 1|x_A = \u22121]|$, where xA is the binary sensitive attribute.\nIn AFA, we use techniques of Fourier analysis to design one computational scheme for simulta-\nneously estimating these three properties of an ML model.\nA Primer on Fourier Analysis. Designing AFA is motivated by the Fourier expansion of Boolean\nfunctions. Fourier coefficients are distribution-dependent components that capture key information\nabout the distribution's properties. This study was initially addressed by (O'Donnell, 2014), who\nfocused on the uniform distribution. Later, (Heidari et al., 2021) generalized this result to arbitrary\ndistributions, which we leverage further.\nPROPOSITION 1 (Heidari et al. (2021)). There exists a set of orthonormal parity functions\n{$\u03c8_S\\}_{S\u2286[n]}$ such that any function h : {\u22121,1}n \u2192 {\u22121,1} is decomposed as\n$h(x) = \\sum_{S\u2286[n]} h(S)\u03c8_S(x)$ for any $x \\sim D$.\n(2.1)\nThe Fourier coefficients $\u0125(S) = E_{x\\sim D}[h(X)\u03c8_S(x)]$ are unique for all $S \u2286 [n]$.\nEXAMPLE 2. Let us consider h to be the XOR function on $x \u2208 {\u22121,1}^2$. This means that\n$h(-1,-1) = h(1,1) = 0$ and $h(1,-1) = h(-1,1) = 1$. The Fourier representation of $h(x) =$\n$0.5 + 0.5x_1 + 0.5x_2 \u2013 0.5x_1x_2$, when x is sampled from a uniform distribution on {\u22121,1}2.\nEXAMPLE 3. Suppose random variables X1 and X2 are drawn i.i.d. from the standard normal\ndistribution N(0,1) (Heidari et al., 2021). Define another random variable X3_as X3 = X1X2. It\ncan be verified that the Gram-Schmidt basis of XOR of X1, X2, X3 has four zero coefficients, i.e. the\nsets including X3 do not influence the outcomes. This is because X3's information is encoded in X1\nand X2 jointly."}, {"title": "ACTIVE FOURIER AUDITOR", "content": "Influence functions. To estimate the properties of interest, we use a tool from Fourier analy-\nsis, i.e. influence functions (O'Donnell, 2014). They measure how changing an input changes the\noutput of a model. Different influence functions are widely used in statistics, e.g. to design robust\nestimators (Mathieu et al., 2022), and ML, e.g. to find important features (Heidari et al., 2021),\nto evaluate how features induce bias (Ghosh et al., 2021), to explain contribution of datapoints on\npredictions (Ilyas et al., 2022). Here, we use them to estimate model properties.\nDEFINITION 4 (Influence functions). If \u0393 is a transformation of an input $x \u2208 X$, the influ-\nence function is defined as $Inf_\u0393(h) \u2261 P_{x\\sim D}[h(x) \\neq h(\u0393(x))]$. $Inf_\u0393(h)$ is called deterministic if the\ntransformation I is deterministic, and randomized if I randomized.\nIn general, deterministic influence functions are used in Boolean function analysis (O'Donnell,\n2014). In contrast, in Section 3, we express robustness, individual fairness, and group fairness with\nrandomized influence functions. We also show that the influence functions can be computed using\nthe Fourier coefficients of the model under audit (Equation (2.1)).\nIn the black-box setting, the access to the model h is limited by the query oracle, accessible\nto the auditor. The auditor's objective is to estimate the property \u03bc through interaction with this\noracle. The definition of the property estimator relies on the information made available to the\nauditor during this interaction. In the context of auditing with model reconstruction (Yan and\nZhang, 2022), the auditor is denoted as $\u00fb : H \u00d7 B \u2192 R$. Here, the auditor has access to an unlabeled\npool and applies active learning techniques (e.g. CAL algorithm) to query samples. This process\nuses the additional information given by the hypothesis class where the model h lives. Following the\nreconstruction phase, the auditor has an approximate model h of true model h, enabling estimation\nof the property via plug-in estimator (h).\nNow, we present a novel non-parametric black-box auditor AFA that assumes no knowledge of\nthe model class and the data-generating distribution. Unlike the full model-reconstruction-based\nauditors, AFA uses Fourier expansion and adaptive queries to estimate the robustness, Individual\nFairness (IF), and Group Fairness (GF) properties of a model h. In this setting, the auditor is\ndefined as $\u00fb: F_\u00b5 \u00d7 B \u2192 R$, where $F_\u00b5$ represents the set of Fourier coefficients upon which the\nproperty \u03bc depends. First, we show that property estimation with model reconstruction always\nincurs higher error. Then, we show that robustness, IF, and GF for binary classifiers can be computed\nusing Fourier coefficients of h. Finally, we compute the Fourier coefficients and thus, estimate the\nproperties at once (Algorithm 1). We begin by defining a PAC-agnostic auditor that we realise with\nAFA.\nDEFINITION 5 (PAC-agnostic auditor). Let \u00b5 be a computable distributional property of model\nh. An algorithm A is a PAC-agnostic auditor if for any $\u03b5, \u03b4\u2208 (0,1)$, there exists a function m(\u0454, \u0431)"}, {"title": "The Cost of Reconstruction", "content": "such that \u2200m \u2265 m(e,d) samples drawn from D, it outputs an estimate \u00eem satisfying P(|\u00fbm - \u03bc| \u2264\n\u03b5) \u2265 1 \u2013 \u03b4.\nRemark. \u03bc(h) is a computable property if there exists a (randomized) algorithm, such that when\ngiven access to (black-box) queries, it outputs a PAC estimate of the property \u03bc(h) (Kearns et al.,\n2018). Any distributional property, including robustness, individual fairness and group fairness, is\ncomputable given the existence of the uniform estimator.\nThe naive way to estimate a model property is to reconstruct the model and then use a plug-in\nestimator (Yan and Zhang, 2022). However, this requires an exact knowledge of the model class and\ncomes with an additional cost of reconstructing the model before property estimation. For group\nfairness, we show that the reconstruct-then-estimate approach induces significantly higher error\nthan the reconstruction error, while the exact model reconstruction itself is NP-hard (Jagielski\net al., 2020).\nPROPOSITION 2. If h is the reconstructed model from h, then\n$|\u00b5_{GFair} (h) \u2013 \u00b5_{GFair} (h)| \u2264 min \\{\\begin{array}{c} {1, \\frac{P_{x\\sim D}[h(x) \\neq h(x)]}{\\min(P_{x\\sim D}[X_A = 1], P_{x\\sim D}[X_A = -1])}} \\end{array} \\}$.\nProposition 2 connects the estimation error and the reconstruction error before plugging in the\nestimator. It also shows that to have a sensible estimation the reconstruction algorithm needs to\nachieve an error below the proportion of minority group, which can be significantly small requiring\nhigh sample complexity. The proof is deferred to Appendix A. This motivates an approach that\navoids model reconstruction by computing only the right components of the model expansion. To\ncapture the information relevant to estimating our properties of interest, we will represent them in\nterms of Fourier coefficients given in the model decomposition. Then we aim to adaptively estimate\nlarger Fourier coefficients in contrast to model reconstruction method requiring to recovering all the\nFourier coefficients."}, {"title": "Model Properties with Fourier Expansion", "content": "Throughout the rest of this paper, we denote by {$\u03c8_S\\}_{S\u2286[n]}$ the basis derived from Proposition\n1. In this section, we express the model properties of h using its Fourier coefficients. The detailed\nproofs are deferred to Appendix B.\na. Robustness. Robustness of a model h measures its ability to maintain its performance when new\ndata is corrupted. Auditing robustness requires a generative model to imitate the corruptions, which\nis modelled by the perturbation mechanism (Definition 1). As we focus on the Boolean case, the worst\ncase perturbation \u0393\u03c1 is the protocol of flipping vector coordinates with a probability p. Specifically,\na corrupted sample y is generated from x such that for every component, we independently set\n$y_i = x_i$ with probability $\\frac{1}{2} + \\frac{p}{2}$ and $y_i = -x_i$ with probability $\\frac{1}{2} - \\frac{p}{2}$. This perturbation mechanism\nleads us to the p-flipping influence function.\nDEFINITION 6 (p-flipping Influence Function). The p-flipping influence function of any model\nh is defined as $Inf_p(h) = P_{x\\sim D,y\\sim \u0393_p(x)} [h(x) \\neq h(y)]$"}, {"title": "Individual Fairness (IF)", "content": "For a Boolean classifier, we further observe that $Inf(h) = [E_{x\\sim D, y\\sim N_p(x)} [h(x)h(y)]$. This allows\nus to show that the robustness of h under \u0393, perturbation is measured by p-flipping influence\nfunction, and thus, can be computed using Fourier coefficients of h.\nPROPOSITION 3. Robustness of h under the $\u0393_p$ flipping perturbation is equivalent to the p-\nflipping influence function, and thus, can be expressed as\n$Rob (h) = Inf_p(h) = \\sum_{S\u2286[n]} \u03c1^{|S|}\u0125(S)^2$.\n(3.1)\nb. Individual Fairness (IF). To demonstrate the universality of our approach, we express IF\nwith the model's Fourier coefficients. We consider the perturbation mechanism \u0393 = $\u0393_{\u03c1,l}(\u00b7)$ that\nindependently flips uniformly I vector coordinates with a probability $\\frac{p}{2}$. Thus, we consider a\nneighbourhood with $E_{x'\\sim \u0393_{\u03c1,l}(x)} [d_X(x, x')] \u2264 (1 + \u03c1)l$ around each sample x as the similar set of\nindividuals. This perturbation mechanism leads us to the (p, l)-flipping influence function.\nDEFINITION 7 ((p, l)-flipping influence function). The (p,l)-flipping influence function of any\nmodel h is defined as $Inf_{\u03c1,l}(h) = P_{x\\sim D,y\\sim N_{\u03c1,l}(x)} [h(x) \\neq h(y)]$.\nWe leverage (p, l)-flipping influence function to express IF of h in terms of its Fourier coefficients\n(Proposition 4).\nPROPOSITION 4. Individual fairness defined with respect to the $\u0393_{\u03c1,l}$ perturbation is equivalent\nto the (p,l)-flipping influence function, and thus, can be expressed as\n$\u00b5_{IFair} (h) = Inf_{\u03c1,l}(h) = \\sum_{S\u2286[n]} \u03c1^{|S_l|}\u0125(S)^2$,\n(3.2)\nwhere $S_l$ denotes the power sets for which I features change.\nUnifying robustness and IF: The Characteristic Function. It is worth noting that IF\nis similar to robustness, differing only by a single degree of freedom, i.e. the number of flipped\ndirections 1. Specifically, from Equation (3.1) and (3.2), we observe that both the properties as\n$\u03bc(h) = \\sum_{S\u2286[n]} char(S, \u00b5.)h(S)^2$, such that char(S, prob) = $\u03c1^{|S|}$, and char(S, $IF_{air}$) = $\u03c1^{|S_l|}$. We call\nchar as the characteristic function of the property.\nc. Group Fairness (GF). Now, we focus on Group Fairness which aims to ensure similar pre-\ndictions for different subgroups of population (Barocas et al., 2023). We focus on Statistical Parity\n(SP) as the measure of deviation from GF (Feldman et al., 2015). To quantify SP, we propose a\nnovel membership influence function.\nDEFINITION 8 (Membership influence function). If A denotes a sensitive feature, we define the\nmembership influence function w.r.t. A as the conditional probability $Inf_A(h) = P_{x,y\\sim D}[h(x) \u2260\nh(y) |x_A = 1, y_A = \u22121]$.\n$Inf_A(h)$ is the conditional probability of the change in the outcome of h due to change in group\nmembership of samples from D. In other words, it expresses the amount of independence between\nthe outcome and group membership."}, {"title": "NP-hardness of Exact Computation", "content": "Note that the membership influence function is a randomised version of the deterministic in-\nfluence function in (O'Donnell, 2014). If we denote the transformation of flipping membership, i.e.\nsensitive attribute of x, f(x), the classical influence function is $Inf_{det} = P_{x\\sim D}[h(x) \u2260 h(f(x))]$.\nThe limitation of this deterministic function is that given x ~ D the transformed vector f(x) may\nnot represent a sample from D. Thus, it fails to encode the information relevant to SP, whereas the\nproposed membership influence function does it correctly as shown below.\nPROPOSITION 5. Statistical parity of h w.r.t a sensitive attribute A and distribution D is the root\nof the second order polynomial P\u2081(X), i.e. $\u03b1(1-\u03b1)X^2-\u0125(0)(1-2\u03b1)X-\\sum_{S\u2286[n],S\u2202A}\u0125(S)^2 - (1 \u2013 2\u0125(0))$,\nwhere $\u03b1 = P_{x\\sim D}[x_A = 1]$ and h(\u00d8) is the coefficient of empty set.\nSummary of the Fourier Representation of Model Properties. Robustness and individual\nfairness have the same Fourier pattern. They depend on all the Fourier coefficients of the model but\ndiffer only on their characteristic functions. In contrast, statistical parity of a sensitive feature A\ndepends only on the Fourier coefficient of that sensitive feature h({A}) and the Fourier coefficient\nof the empty set \u0125(\u00d8).\nWe have shown that the exact computation of robustness and individual fairness depends on all\nFourier coefficients of the model. Since each Fourier coefficient of h is given by $h(S) = E[h(x)\u03c8_s(x)]$,\nexactly computing a single Fourier coefficient takes O(|X|) time. Additionally, the number of Fourier\ncoefficients to compute to estimate robustness and individual fairness is exponential in the dimen-\nsion of the input domain (2n). Thus, exactly computing robustness and individual fairness requires\nO(2|X|) time. This gives us an idea about the computational hardness of the exact estimation\nproblem. Now, we prove estimating large Fourier coefficients to be NP-complete.\nTHEOREM 1. Let Q = {x, h(x)} be the set of input samples sent to h and the predictions\nobtained. Given $\u03c4 \u2208 R_{\u22650}$, exactly computing all the T-significant Fourier coefficients of h is NP-\ncomplete.\nProof Sketch. For a set of queries Q and for each power set S, Fourier coefficient is given by\n$h(S) = \\sum_{(x,h(x))\u2208Q}h(x)\u03c8_s(x)$. Maximizing the Fourier coefficient |\u0125(S)| is equivalent to max-\nimizing the agreement or disagreement between h and the sign of us for each truth assignment.\nAlternatively, maximizing |h(S)| is equivalent to finding a truth assignment that maximizes the\nnumber of true clauses in a CNF, where each clause is a disjunction of h(x) and the sign of \u03c8s(x),\nand the CNF includes all such clauses for all x \u2208 Q. This is known as the Max2Sat (maximum\ntwo satisfiability) problem, which is known to be NP-complete. Hence, we conclude that finding\nlarge Fourier coefficients is also NP-complete. This result shows that the exact computation of the\nFourier coefficients for our properties is NP-hard. This has motivated us to design AFA, which we\nlater proved to be an (\u20ac, \u03b4)-PAC agnostic auditor."}, {"title": "Algorithm: Active Fourier Auditor (AFA)", "content": "We have shown that finding significant Fourier coefficients can be an NP-hard problem. In this\nsection, we propose AFA (Algorithm 1) that takes as input a restricted access of q > 0 queries\nfrom the data-generating distribution and requests labels from the black-box oracle of h (Line 2)."}, {"title": "THEORETICAL ANALYSIS", "content": "Upper Bounds on Sample Complexity."}, {"title": "EMPIRICAL PERFORMANCE ANALYSIS", "content": "In this section, we evaluate the performance of AFA in estimating multiple models' group fairness,\nrobustness, and individual fairness. Below, we provide a detailed discussion of the experimental\nsetup, objectives, and results.\nExperimental Setup. We conduct experiments on COMPAS (Angwin et al., 2016), student\nperformance (Student) (Cortez and Silva, 2008), and drug consumption (Drug) (Fehrman et al.,\n2019) datasets. The datasets contain a mix of binary, categorical, and continuous features for binary\nand multi-class classification. We evaluate AFA on three ML models: Logistic Regression (LR), Multi-\nlayer Perceptron (MLP), and Random Forest (RF). The ground truth of group fairness, individual\nfairness, and robustness is computed using the entire dataset as in (Yan and Zhang, 2022). For\ngroup fairness, we compare AFA with uniform sampling method, namely Uniform, and the active\nfairness auditing algorithms (Yan and Zhang, 2022, Algorithm 3), i.e. CAL and its variants \u00b5CAL and\nrandomized \u00b5CAL, which requires more information about the model class than black-box access.\nWe report the best variant of CAL with the lowest error. For robustness and individual fairness, we\ncompare AFA with Uniform. Each experiment is run 10 times and we report the averages. We refer\nto Appendix E.1 for details.\nOur empirical studies have the following objectives:\n1. How accurate AFA is with respect to the baselines to audit robustness, individual fairness, and\ngroup fairness for different models and datasets?"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We propose AFA, a Fourier-based model-agnostic and black-box approach for universally audit-\ning an ML model's distributional properties. We focus on three properties: robustness, individual\nfairness, and group fairness. We show that the significant Fourier coefficients of the black-box model\nyield a PAC approximation of all properties, establishing AFA as a universal auditor of ML. Em-\npirically, AFA is more accurate, and sample efficient, while being competitive in running time than\nexisting methods across datasets. In the future, we aim to extend AFA to estimate distributional\nproperties other than the three studied in this paper."}, {"title": "APPENDIX A: THE COST OF AUDITING WITH RECONSTRUCTION: PROOF OF\nPROPOSITION 2", "content": "Proposition 2. If his the reconstructed model from h, then\n$|\u00b5_{GFair} (h) \u2013 \u00b5_{GFair} (h)| \u2264 min \\{\\begin{array}{c} {1, \\frac{P_{x\\sim D}[h(x) \\neq h(x)", "1": "P_{x\\sim D}[X_A = -1", "h(x)": "P_{x\\sim D}[h(x) = h(x)|x_A = 0", "0": "P_{x\\sim D}[h(x) = h(x)|x_A = 1"}, {"1": "n\u2265 p(P_{x\\sim D}[h(x) = h(x)|x_A = 0"}, {"1": "."}, {"1": "P_{x\\sim D}[x_A = 0", "h(x)": "P_{x\\sim D}[h(x) = h(x)|x_A = 0"}, {"1": "."}, {"1": "P_{x\\sim D}[\u0125(x) = 1, h(x) = \u22121|x_A = 1"}, {"1": "n$< P_{x\\sim D}[h(x) \\neq h(x)|x_A = 1"}, {"1": "nThe last inequality is true due to the fact that $\u0125(x) = 1,h(x) = \u22121$ is a sub-event of the event\n$h(x) \u2260 h(x)$.\nNow, by symmetry of h and \u0125, we get\n$P_{x\\sim D}[h(x) = 1|x_A = 1"}, {"1": "P_{x\\sim D}[h(x) \u2260 h(x)|x_A = 1", "0": "P_{x\\sim D}[h(x) = 1 | x = 0"}]}