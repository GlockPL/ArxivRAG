{"title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization", "authors": ["Shiyue Zhang", "David Wan", "Arie Cattan", "Ayal Klein", "Ido Dagan", "Mohit Bansal"], "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their presence in the system summary, has been widely adopted. However, it suffers from a lack of systematicity in the definition and granularity of the sub-units. We address these problems by proposing QAPyramid, which decomposes each reference summary into finer-grained question-answer (QA) pairs according to the QA-SRL framework. We collect QA-SRL annotations for reference summaries from CNN/DM and evaluate 10 summarization systems, resulting in 8.9K QA-level annotations. We show that, compared to Pyramid, QAPyramid provides more systematic and fine-grained content selection evaluation while maintaining high inter-annotator agreement without needing expert annotations. Furthermore, we propose metrics that automate the evaluation pipeline and achieve higher correlations with QAPyramid than other widely adopted metrics, allowing future work to accurately and efficiently benchmark summarization systems.", "sections": [{"title": "1 Introduction", "content": "Human evaluation is considered the gold standard for benchmarking progress in text summarization and provides the necessary training or evaluation signals for developing automatic metrics. However, there is no consensus on how human evaluation should be conducted. Flawed human evaluation protocol can undermine the reliability of any subsequent automatic evaluations or their outcomes. A key indicator of an unreliable human evaluation is the low inter-annotator agreement, which makes the evaluation results difficult to reproduce.\nTo make human evaluation more reliable, the Pyramid method was introduced as a reference-based and decomposition-based human evaluation protocol, which is proved to be more reproducible compared to direct assessment. This method involves decomposing the reference summary into Semantic Content Units (SCUs), defined as \u201csemantically motivated, subsentential units,\" and then assessing whether each unit is present (semantically entailed) in the system-generated summary. The protocol has been continuously refined for efficiency and reproducibility. Despite the widespread recognition, we identify three significant issues with the underlying sub-unit decomposition step, on which the method is based.\nFirst, the lack of a systematic definition for SCUs leads to ambiguity and inconsistency. Although Liu et al. (2023b) attempt to clarify the definition through the concept of Atomic Content Units (ACUs), they acknowledge that \u201cit can be impossible to provide a practical definition.\u201d Consequently, the content included in each unit varies among annotators, and the granularity of units is inconsistent, ultimately compromising its reproducibility. Second, the minimal SCU/ACU typically encompasses one predicate along with two or more arguments. If a system summary incorrectly represents even one of these arguments, it receives zero credit despite partial correctness. For instance, as illustrated in Figure 1, a system summary may state that \u201cthe vaccine took 30 years to develop\u201d without mentioning who developed it. While this summary does not fully entail the semantics of \u201cBritish firm developed the vaccine,\u201d it should still receive credit for correctly indicating that \u201cthe vaccine was developed.\u201d Therefore, a finer-grained representation is necessary to capture each predicate and its individual arguments separately, allowing for partial credit in evaluation. Third, due to the lack of systematicity, the Pyramid method relies on experts to extract and formulate units to ensure quality, making the protocol more costly and less scalable.\nTo address these problems, we introduce QAPyramid. Our method replaces the SCU generation step with a QA generation step that decomposes the reference summary into Question-Answer pairs (QAs) following the"}, {"title": "2 Preliminaries", "content": "2.1 Pyramid Protocol\nNenkova and Passonneau (2004) proposed the Pyramid protocol to reduce subjectivity and make the human evaluation of content selection more reliable and reproducible. To do so, the protocol is split into two steps. The first step is SCU Generation, in which experts extract fine-grained units, Semantic Content Units (SCUs), from the reference summary. We note that this step only needs to be performed once for each dataset and SCUs can be re-used for evaluating different models' generations. The second step is Presence Detection (or System Evaluation) which requires annotators to judge whether the system summary contains each SCU as a binary judgment.\nRecently, Liu et al. (2023b) revisited the protocol and refined the definition of the sub-unit into atomic content units (ACU): \u201cElementary information units in the reference summaries, which no longer need to be further split for the purpose of reducing ambiguity in human evaluation.\" However, as previously noted, this definition remains ambiguous. In response, we propose a new formalization using QA-SRL.\n2.2 QA-SRL\nSemantic role labeling (SRL) is to discover the predicate-argument structure of a sentence, i.e., to determine \"who does what to whom, when, and where,\" etc. Classic SRLs, e.g., FrameNet and PropBank , have rather complicated task definitions and require linguistic expertise to conduct annotations. He et al. (2015) introduced QA-SRL to present predicate-argument structure by question-answer (QA) pairs. Without predefining frames or semantic roles, the questions themselves define the set of possible roles. They showed that QA-SRL leads to high-quality SRL annotations and makes scalable data collection possible for annotators with little training and no linguistic expertise, which was later proved by the crowdsourced QA-SRL Bank 2.0 dataset.\nThe QA-SRL representation has since been utilized in various NLP tasks, demonstrating its versatility and effectiveness. It has been successfully employed for aligning information across different texts , detecting analogies between procedural texts , enhancing language modeling pretraining"}, {"title": "3 QAPyramid: Method and Dataset", "content": "Similar to Pyramid, our QAPyramid protocol is composed of two steps: (1) QA generation: write QA pairs for each predicate in the reference summary; (2) QA presence detection: judge the presence of each QA pair in the system summary.\n3.1 QA Generation\nFor any given dataset, QA generation only needs to be done once for the reference summaries in its evaluation set. We choose CNN/DM , one of the most popularly used text summarization datasets, as our test bed. We use a subset of 500 examples of the CNN/DM test set, the same set used by Liu et al. (2023b). For each reference summary, we first extract predicates automatically via AllenNLP SRL API which uses spaCy under the hood. On average, each reference contains 7.6 predicates.\nThen, for each predicate, we ask one human annotator to write QA pairs. Figure 6 in the Appendix is our annotation UI on Amazon Mechanical Turk for writing QA pairs. Note that the previous QA-SRL annotation  requires annotators to follow predefined automata, which sometimes results in non-natural questions. Here we opt for a less confined approach to allow annotators to write questions following our instructions (instructions can be seen in Figure 6). We show one sentence at a time and highlight one predicate in the sentence. The annotator is asked to write at most 5 questions and, for each question, fill in at most 3 answers. To recruit high-quality annotators for this task, we initially picked 4 examples as qualification tasks. Workers were qualified only if they correctly wrote QA pairs for all 4 tasks. Eventually, we recruited 31 annotators.\nFor each of the collected QA pairs, following FitzGerald et al. (2018), we ask two other annotators to verify it. This step validates if QAs are correctly written based on our instructions. Figure 7 in the Appendix is the UI for verifying QA pairs. The annotator is asked to first judge whether the question is valid; if valid, they need to write the answer to the question. Similar to QA generation, we used 4 qualification tasks to recruit 30 annotators. In addition, we conduct cross-checks between QA generation and verification. Annotators for writing QA pairs need to get more than 85% accuracy in the verification step to maintain being qualified. Annotators for verifying QA pairs need to agree with their peer annotators for more than 85% to maintain being qualified. Annotators who write QA pairs are compensated at a rate of $0.32 per HIT, and annotators who verify QA pairs are paid at a rate of $0.17 per HIT, which makes an hourly payment of around $10. We find a high inter-annotator agreement: in 90.7% of cases, two annotators agree with each other, and in 89.7% of cases, the question is labeled as valid by both annotators. These high agreement and approval rates indicate that QA generation, following the QA-SRL formalization, is systematically standardized, making it verifiable and reproducible. In the end, we only keep QA pairs that are verified to be valid by both annotators. If a predicate has fewer than 2 QA pairs, we redo QA generation and verification.\nAfter completing the data collection, we identified two issues. First, approximately 10% of the data contained duplicated QA pairs\u2014that is, questions and answers with the same meaning but different wording for the same predicate within a single reference. We (the authors) manually deduplicated these instances; this deduplication task can be easily integrated into the QA verification step in future work. Second, about 10 summaries had fewer than five QA pairs. This was mainly because no predicates were extracted from sentences lacking verbs, such as \u201cLewis Hamilton over half-a-second clear of team-mate Nico Rosberg.\u201d To address this, we manually write the QA pairs for these examples. Ultimately, we compiled a total of 8,652 QA pairs, averaging 17 QAs per reference summary.On the same dataset, there are on average 11 ACUs per reference summary, which confirms the finer granularity of our evaluation protocol.\n3.2 QA Presence Detection\nAfter we obtained all the QA pairs, the next step is to conduct system evaluations. For any system summary, we ask humans to judge whether each QA is present (\u2714) or not present (X) in the sys-"}, {"title": "3.3 Summary Scoring", "content": "Given human annotations, we now can calculate the QAPyramid scores. For any given system summary $s_i$, assume its corresponding reference summary $r_i$ has $K_i$ QA pairs $\\{QA_{ij}\\}_{j=1}^{K_i}$, then QAPyramid is defined as the number of present QA pairs in $s_i$ divided by $K_i$:\n$QAPyramid_i = \\frac{\\sum_{j=1}^{K_i} Presence(QA_{ij}, s_i)}{K_i}$\nwhere $Presence(QA_{ij}, s_i) = 1$ if $QA_{ij}$ is present in $s_i$, and 0 otherwise.\nThis metric is essentially a recall, i.e., a longer summary with more information usually gets a higher score. In an extreme case, when the summary is the same as the source document, it receives a perfect score. To combat this issue, Liu et al. (2023b) introduced a normalized ACU score, $nACU$, by multiplying the original ACU score by a length penalty, i.e., $nACU = p*ACU$, where\n$p = e^{min(0, \\frac{ \\mid s_i \\mid - \\alpha \\mid r_i\\mid }{\\mid r_i \\mid})}$ and $\\mid s_i\\mid$, $\\mid r_i \\mid$ are the length (i.e., number of words) of the system and reference summary. Essentially, system summaries that are longer than the reference summary get discounted scores. In practice, \u03b1 is set by de-correlating $nACU$ with summary length.\nHowever, this length penalty, especially how \u03b1 is being set, assumes the unnormalized score is positively correlated with summary length, i.e., longer summaries tend to get higher scores. This is usually true for fine-tuned models. But for non-finetuned LLMs, they sometimes suffer from the degeneration problem getting stuck into a repetition loop and generating a sentence or a sub-sentence repeatedly, see a degenerated summary produced by Llama-3-8B-Instruct in Figure 2. In this case, the extending length does not provide more information and thus does not lead to a higher score. Though this extending length also needs to be penalized, it has unique behavior and needs to be dealt with separately. Therefore, we introduce a novel repetition penalty $p = 1 \u2013 rp_i$, where $rp_i$ is the repetition rate of $s_i$ meaning what percentage of the summary is repetition (please refer to Figure 2 in Appendix A.1). Then we change the length penalty to\n$p = e^{min(0, \\frac{ \\mid s_i*p \\mid - \\alpha \\mid r_i\\mid }{\\mid r_i \\mid})}$ , where $s_i * p$ is the effective summary length \u2013 the length of non-repetitive text. We set \u03b1 by de-correlating $p * QAPyramid$ with effective summary length. Using our collected data, the Pearson correlation between effective summary length and QAPyramid is 0.27. After setting \u03b1 = 6, the correlation between effective summary length and $p * QAPyramid$ reduced to -0.01 (see details in Table 5 in Appendix A.1). The normalized QAPyramid is defined as:\n$nQAPyramid = p * p * QAPyramid.$\nIntuitively, $p$ penalizes long summary that has a lot of repetitions, and $p$ penalizes long summary that includes a lot of information from source."}, {"title": "3.4 Result Analysis", "content": "Table 1 shows the metric scores of 10 summarization systems on the 50-example subset from the CNN/DM test set (and Table 6 in the Appendix contains systems scores of many other automatic or semi-automatic metrics). The ACU and nACU scores are computed based on the raw annotation data released by Liu et al. (2023b).\nFirst, a consistent trend across different metrics is that BRIO and BRIO-Ext are the two best-performing models, likely because they are carefully fine-tuned on the CNN/DM training set.\nSecond, 1-shot LLMs consistently obtain worse ROUGE-2 recall or F1 scores than fine-tuned models. However, their QAPyramid and nQAPyramid scores are comparable to those of fine-tuned models. This demonstrates that, compared to metrics based on lexical overlaps, our QAPyramid method captures more semantic similarities and thus alleviates the rigidity caused by reference-based evaluations with a single reference. This finding is also consistent with previous observations that zero-shot or few-shot LLMs are preferred by humans despite their low ROUGE scores.\nThird, we observe that QAPyramid scores are overall higher than ACU scores. One hypothesis is that QAs are finer-grained than ACUs, so they give credit to finer-grained alignments between the system and reference summaries. To support this hypothesis, we find that in 21% of cases, a predicate only has a subset of its QA pairs present in the system summary, i.e., if the judgment is at the ACU level, it would miss some partial correctness.\nNext, we randomly sampled examples from summaries generated by fine-tuned models and manually compared QAPyramid with ACU. We find that, besides QA pairs being finer-grained, two other factors also contribute to higher QAPyramid scores: (1) our annotation guideline led to more lenient judgments of \"being present\" based more on semantics than lexicon, and (2) the predicate-centered nature of QA-SRL may cause one piece of information to be credited multiple times."}, {"title": "4 QAPyramid Automation", "content": "Here we explore how to automate QAPyramid, where we break it down into automation of QA generation and QA presence detection.\n4.1 QA Generation Automation\nFor the QA generation task, the input is one reference summary and one predicate (verb) within the reference, and the gold output is the human-written QA pairs for this predicate. Using our collected data, we get a set of 3,782 examples. Since we do not plan to train a QA generation model, we use all of them as our test set.\nWe explore two types of models to conduct this task automatically. First, we consider prompting large language models (LLMs). We use open-"}, {"title": "4.2 QA Presence Detection Automation", "content": "For the QA presence detection task, the input is one system summary and one QA pair from the reference summary, and the output is a binary judgment of whether the QA can be inferred from the system summary."}, {"title": "4.3 Meta Evaluation of Automated Metrics", "content": "Finally, equipped with the best automation methods, i.e., QASem with flan-t5-xl for QA generation and GPT-40 with 5-shot prompting for QA presence detection, we can assemble both a semi-automatic and a fully automatic metric."}, {"title": "5 Related Works", "content": "Human Evaluation for Text Summarization.\nHuman evaluation is the ultimate way to assess the quality of model-generated summaries. However, there is no consensus on how human evaluation should be conducted. In many cases, people resort to direct rating, i.e., humans directly rate the quality of the summary (or rate for certain aspects, e.g., relevance (Fabbri et al., 2021)). However, direct rating often suffers from subjectivity, low agreement, and thus non-reproducibility . Two factors contribute to this issue: (1) the summary is usually more than one sentence and different annotations may put their focus on different parts of the summary; (2) when reference is not provided, what content is considered important and should be selected into the summary may vary from person to person. The canonical Pyramid method was introduced to improve reproducibility. First, it is reference-based, which resolves the question of what content needs to be selected. Second, it decomposes the summary into smaller units, which reduces the ambiguity when evaluating long text. The original Pyramid method requires multiple references and full expert annotations. Shapira et al. (2019) simplified Pyramid, making it feasible for single reference, and made the judgment of unit presence crowdsourceable.\nLiu et al. (2023b) introduced the ACU (Atomic Content Unit) protocol that further clarified the concept of atomic units. However, there are still no standard guidelines for ACU or SCU. How to write each unit and what content to include in each unit are purely dependent on the annotator. Getting different sets of units from different annotators compromises the reproducibility of this protocol. In contrast, the units (QAs) of QAPyramid are systematically defined by QA-SRL. Furthermore, QA-SRL-based QAs are usually finer-grained than ACUs or SCUs. Another human evaluation approachis best-worst scaling or pairwise comparison , which has become a standard way to benchmark modern LLMS. However, it also suffers from the low-agreement issue when text is long and it scales quadratically with the number of systems.\nAutomatic Evaluation for Text Summarization.\nCompared to human evaluations, automatic metrics are fast, cheap, and reproducible. However, how well they correlate with human judgment is always a concern. Over the years, many automatic evaluation metrics have been introduced. Some early metrics measure the n-gram overlap between system and reference summaries, out of which ROUGE is the most widely adopted metric till today. To alleviate the rigidity of exact lexical match, metrics based on the similarity between embeddings were introduced. There are also works automating the Pyramid protocol . They use Open IE, SRL, AMR, fine-tuned models, or LLMs to automate unit extraction and use NLI models to automate unit presence detection. Compared to these works, AutoQAPyramid is advantageous because it automates a more systematical, reproducible, and fine-grained human evaluation protocol, QAPyramid. Besides evaluating content selection, a separate line of research has been focused on faithfulness or factuality evaluation, i.e., checking if the source document(s) entail the summary. The Pyramid type of methods have also been applied in this scenario  and the units they extract are ACU or SCU style units. There are some methods based on question generation and answering."}, {"title": "6 Conclusion", "content": "To summarize, the contributions of our work are three-fold. First, we introduce QAPyramid, an enhanced human evaluation protocol for text summarization that improves the systematicity, granularity, reproducibility, and scalability of reference-based and decomposition-based human evaluations. Second, we extensively collect 8.9K annotations following our QAPyramid protocol, which can be used to benchmark automatic metrics and summarization systems in the future. Third, we introduce semi-automatic and fully automatic metrics that partially or entirely automate QAPyramid, and they show the highest correlations with QAPyramid compared to other widely adopted summarization metrics. We hope that QAPyramid and its automation can be adopted by future work for benchmarking text summarization and possibly other language generation tasks."}, {"title": "Limitation", "content": "One limitation of QAPyramid is that we considered only QA-SRL when generating QA pairs. As mentioned in Section 2.2, QA pairs can also be generated for deverbal nominalizations , discourse relations , and adjectival semantics. Future work can focus on how to more thoroughly decompose the meaning of the reference summary.\nAnother limitation of QAPyramid is that it is only applicable to reference-based evaluation, a constraint shared by all Pyramid-style evaluations. Such evaluation requires human-written references, which may not always be available. Moreover, when only a single reference is provided, it must accurately capture all the content that should be included. If multiple different ways of content selection are all valid, an evaluation based on a single reference may be biased. Despite this rigidity, reference-based evaluation is usually more reproducible and controllable compared to reference-free evaluation ."}, {"title": "A Complementary Results", "content": "A.1 Length and Repetition Penalty\nTo determine the repetition rate $rp_i$ of a summary, we first identify the span that is repeated consecutively for more than 3 times. In the example of Figure 2, \u201cGwyneth Paltrow and Chris Martin are seen together with their children in a family photo.\u201d is the span. Then we identify the text $t_i$ where the span is being repeated; in this case, it is the length of text in red. The repetition rate $rp_i$ is then defined by $\\frac{t_i}{s_i}$, i.e., the length of repetitive text over the summary length."}]}