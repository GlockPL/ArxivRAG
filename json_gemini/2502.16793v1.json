{"title": "VGFL-SA: VERTICAL GRAPH FEDERATED LEARNING\nSTRUCTURE ATTACK BASED ON CONTRASTIVE LEARNING", "authors": ["Yang Chen", "Bin Zhou"], "abstract": "Graph Neural Networks (GNNs) have gained attention for their ability to learn representations from\ngraph data. Due to privacy concerns and conflicts of interest that prevent clients from directly sharing\ngraph data with one another, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial attacks that degrade\nperformance. However, it is a common problem that client nodes are often unlabeled in the realm\nof VGFL. Consequently, the existing attacks, which rely on the availability of labeling information\nto obtain gradients, are inherently constrained in their applicability. This limitation precludes their\ndeployment in practical, real-world environments. To address the above problems, we propose a\nnovel graph adversarial attack against VGFL, referred to as VGFL-SA, to degrade the performance\nof VGFL by modifying the local clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are trained. VGFL-SA first\naccesses the graph structure and node feature information of the poisoned clients, and generates the\ncontrastive views by node-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and the gradients of the\nadjacency matrices are obtained by the contrastive function. Finally, perturbed edges are generated\nusing gradient modification rules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA achieves good attack\neffectiveness and transferability.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning models based on graph data (Graph Neural Networks, GNNs) have been widely used in\nvarious real-world domains, for example, medical information [1, 2], bioinformation [3, 4] and social networks [5, 6, 7].\nGNNs apply deep learning-based methods on graph data to learn the structural and feature information by aggregating\nthe neighboring information of the nodes, and achieve superior performance in node classification [8, 9], link prediction\n[10, 3], graph embedding [11, 12].\nGNNs have achieved substantial advancements in the domain of graph data learning [13, 14]. Nonetheless, a predomi-\nnant characteristic among these methodologies is the collective storage of graph data. In real life, real graph data is\nusually decentralized, i.e., the data is usually owned by different clients (also known as data owners) [15]. Due to\nprivacy concerns, clients tend to protect the original data (especially sensitive and private data), leading to the data\nsilo problem [16]. For example, GNNs are expected to comprehensively assess a patient's health status and help the\npatient to predict potential diseases or find out the causes of the diseases. In the healthcare domain, different hospitals\nor healthcare organizations usually have a large amount of patient data, which is not only private but also often stored\nin the form of graph structures. Hospitals can use medical images to diagnose diseases, where each patient's medical\nrecords and images can be viewed as nodes in the graph, and collaboration between hospitals can be considered as\nedges of the graph. Hospitals usually cannot share patient-specific information due to data privacy protection and legal\nand regulatory restrictions. Therefore, cross-hospital collaboration for federated training to develop more powerful\ndisease prediction models is still a great need."}, {"title": "2 Related Work", "content": "In this section, we briefly summarize existing works on graph federation learning, graph contrastive learning, and graph\nadversarial attack."}, {"title": "2.1 Graph Federation Learning", "content": "GFL trains models across multiple data owners without exchanging the original data and can effectively protect user\nprivacy [30]. Ni et al. [31] proposed a federated privacy-preserving node GCN learning framework (FedVGCN), which\nis suitable for the case where data is vertically distributed. FedVGCN splits the computational graph data into two parts.\nFor each iteration of training, both sides pass intermediate results under homomorphic encryption. The literature [32]\nproposed vertical federated graph neural network (VFGNN), VFGNN keeps the private data (i.e., edges, node features,\nand labels) on the clients, and the rest of the information is given to be uploaded to a semi-honest server for training.\nLiu et al. [33] proposed a federated learning of subgraphs with global graph reconstruction (FedGGR). For the data silo\nproblem, Zhang et al. [34] proposed FedEgo, FedEgo uses GraphSAGE on ego-graphs to fully exploit the structural\ninformation and Mixup to address the privacy issues. Xue et al. [35] proposed a new framework for federated learning\nof personality graphs based on variational graph self-encoders (FedVGAE). Du et al. [36] proposed a new efficient GFL\nframework (FedHGCN), FedHGCN is able to be co-trained in high-dimensional space to obtain graph-rich hierarchical\nfeatures. In addition, FedHGCN uses a node selection strategy to remove nodes with redundant information from the\ngraph representation to improve efficiency. Huang et al. [37] proposed a federated learning cross-domain knowledge\ngraph embedding model (FedCKE) in which entity/relationship embeddings between different domains can interact\nsecurely without data sharing. Zheng et al. [38] proposed a cross-firm recommendation GNNs training framework\n(FL-GMT), which no longer uses traditional federation learning training methods (e.g., averaged federation), and\ndesigns a loss-based federation aggregation algorithm to improve the sample quality. The literature [39] proposed a\nfederated multi-task graph learning (FMTGL) framework to address issues in privacy preserving and scalable schemes."}, {"title": "2.2 Graph Contrastive Learning", "content": "Graph contrastive learning is an unsupervised learning method that aims to learn effective representations from graph\ndata [40]. Meng et al. [41] proposed an informative contrastive learning (IMCL) which uses a graph augmentation\ngenerator for distinguishing the augmented view from the original view. Besides, IMCL uses a pseudo-label generator\nto generate pseudo-labels as a supervisory signal to ensure that the results of the augmented view classification are\nconsistent with the original view. Feng et al. [42] proposed the ArieL method, which introduces an adversarial graph\nview for data augmentation and also uses information regularization methods for stable training. In addition, ArieL\nuses subgraph sampling to extend to different graphs. Jiang et al. [43] proposed probabilistic graph complementary\ncontrastive learning (PGCCL) for adaptive construction of complementary graphs, which employs a Beta mixture\nmodel to distinguish intraclass similarity and interclass similarity, and solves the problem of inconsistent similarity\ndistributions of data. Yang et al. [44] proposed a graph knowledge contrastive learning (GKCL), which uses exploits\nmultilevel graph knowledge to create noise-free contrastive views that can alleviate the problem of introducing noise\nand generating samples that require additional storage space during graph augmentation. The literature [45] proposed\nan implicit graph contrastive learning (IGCL), which avoids the situation where changing certain edges or nodes may\naccidentally change the graph features by reconfiguring the topology of the graph. Li et al. [46] proposed a line graph\ncontrastive learning (Linegcl), the core of which is to transform the original graph into the corresponding line graph,\nsolving the deficiencies of the existing methods in understanding the overall features and topology of the graph. Since\nthe similarity-based methods are defective in terms of node information loss and similarity metric generalization ability.\nThe literature [47] proposed a linear graph contrastive learning (LGCL), which obtains subgraph views by sampling\nh-hop subgraphs of target node pairs, and then maximizes mutual information after transforming the sampled subgraphs\ninto linear graphs. The literature [48] proposed a dyadic contrastive learning network (DCLN), which is based on\na self-supervised learning approach to enhance the model performance through the pairwise reduction of redundant\ninformation about the learned latent variables."}, {"title": "2.3 Graph Adversarial Attack", "content": "GNNs have achieved significant success in many domains and are vulnerable to adversarial attacks due to their high\ndependence on graph structure and node features. Graph Adversarial Attacks are defined as small modifications to\nthe input graph that can cause GNNs to output incorrect predictions or classification results [49]. The literature [50]\nproposed a generalized attack framework (CAMA) that generates the importance of nodes through graph activation\nmapping and its variants. Zhang et al. [51] proposed the first framework for training adversarial attacks on distributed\nGNNs (Disttack), which centers on disrupting the gradient synchronization between computational nodes by injecting"}, {"title": "3 Preliminaries", "content": "This section first introduces the knowledge related to graphs. Then, definitions related to GNNs and VGFL are\nelaborated and formalized. Finally, the attack environment of VGFL-SA is introduced.\nFor convenience, Table 1 gives the frequently used notations."}, {"title": "3.1 Graph Definition", "content": "Given a attribute graph G =(V, E, X), where V = {V1, V2, ..., Vn } represents the set of nodes, n denotes the number\nof nodes, E = {e1, e2, ..., em } is the set of edges, m is the number of edges, and X \u2208 Rn\u00d7d represents the set of node\nfeatures, where d denotes the dimension of the features. We use the adjacency matrix A \u2208 {0, 1}n\u00d7n to represent the\nlink relationship between nodes. When there is a link between node i and node j, there Aij = 1. Conversely, Aij = 0\nindicates that no link exists between node i and node j."}, {"title": "3.2 Graph Neural Networks", "content": "In recent years, more and more graph deep models have been proposed [58, 59]. Among them, the state-of-the-art\nGraph Convolutional Network (GCN) [60], Graph Attention Network (GAT) [61] have achieved excellent performance\nin node classification, link prediction and graph classification tasks.\nGCN: GCN is a graph neural network based on the idea of convolutional neural network, which aims to learn the\nrepresentation of graph nodes. The core is to perform convolutional operations on node features through the adjacency"}, {"title": "3.3 Vertical Graph Federated Learning", "content": "VGFL is an approach that combines graph data and vertical federation learning. In this framework, multiple clients\nlearn collaboratively based on parts of the graph data held by each of them without sharing the original data. Each\nclient holds different node features or a part of the graph, and the goal is to learn a global graph model by cross-party\ncollaborative training. Suppose that in VGFL, there is a collection of clients and a server S, where K is the number\nof clients. Each client holds in each client a part of the data of the graph G, i.e., Gi = (V, Ei, Xi). Where nodes are\nshared in each client, but node features and node adjacencies are different, i.e., in any client i and client j, Ei \u2229 Ej = \u00d8,\nX\u00bf \u2229 Xj = (\u00d8). The sum of the data of all clients is \u03a3Ei = E, \u03a3Xi = X.\nIn VGFL, each client trains local GNNs with its private data and updates the embedding for server aggregation.\n$h_{global} \\leftarrow CONCAT(h_1, ..., h_i, ...,h_k), s.t. h_i = f_{\\theta}(A_i, X_i).$\nwhere A is the adjacency matrix of the data in the i-th client. hi is the output embedding of the i-th client trained in\nthe local GNNs $f_{\\theta}$ (A, X). Then, the server S returns the global embeddings after propagating the embeddings of the\nclients by forward propagation. The server can use hglobal to train models for the main task, while the adversary can\nuse them to infer private data.\nTaking the node classification task as an example, the prediction of server S can be expressed as:\n$\\hat{Y} = softmax(W_l \\cdot \\rho(...\\rho(W_0 \\cdot h_{global}))).$\nwhere \u0176 is the set of node prediction labels, W is the set of training weights, and p is an aggregation function, usually\nReLU."}, {"title": "3.4 Threat Model", "content": "We consider a commonly used scenario where the central server model S is trained by multiple clients C collaboratively\nto accomplish downstream tasks. The clients do not share any information with each other. There are cases where nodes\nare unlabeled in any client, which is common in real-world scenarios. The attacker does not understand the structure of\nVGFL and does not need to know any parameter and gradient information of the server S. The attacker aims to generate\nmalicious structural perturbations from the node structure and feature information in poisoned clients. When the clients\nare trained locally with poisoned data to generate poisoned embeddings, which are then uploaded into the central server\nS to affect the final embeddings, which in turn leads to the output of incorrect results in the downstream tasks.\nThe aim of our proposed model is to accomplish structural attacks in the clients before VGFL training. Then the clients\nare trained in local GNNs to generate embeddings with malicious information to be uploaded into the central server to\nmaximize the training loss. It can be described by a mathematical formula as:\n$Max \\frac{1}{\\left|\\mathcal{V}_{test}\\right|} \\sum_{i \\in \\mathcal{V}_{test}} l_{train}(Y_i, \\hat{Y}_i),$\ns.t. $\\hat{Y} = softmax(W_l \\cdot \\rho(...\\rho(W_0 \\cdot h_{global})))$,\n$h_{global} \\leftarrow CONCAT(h_1,..., h_i,...,h_{K'}), \\hat{h}_i = f_{\\theta}(A_i, X_i) .$\nwhere $h_{global}$ and $h_i$ denote the global embedding and client embedding of poisoning, respectively."}, {"title": "4 VGFL-SA Model", "content": "We consider a common case of VGFL where there are many clients working together to complete the training process\nfor a global server. Since only the node ID are shared among clients, for the graph structure is not shared. In other\nwords, the structure is different, which creates favorable conditions for the attack. In VGFL, some nodes in the local\nclient are not labeled. In the real world, using semi-supervised learning to implement the attack in the local client is\ndifficult because the labels are hard to obtain. We propose a structure attack against VGFL, referred to as VGFL-SA,\nwhich is an unsupervised attack model. VGFL-SA aims to generate a perturbed graph by modifying the graph structure\nat a local client, then local GNNs are trained to get poisoned node embeddings, which are uploaded by the local client\nto the global server. The global server relies on embeddings uploaded by multiple clients, so embeddings containing\npoisoning information can lead to model performance in downstream tasks. The overall framework of VGFL-SA is\nshown in Fig. 2. First, the poisoned client is selected, two comparative views (Augmentation 1 and Augmentation\n2) are obtained using data augmentation. The embedding of each augmentation view is obtained using the shared\ngraph encoder, and the difference between them is obtained through the comparative function, while the gradient of the\nneighbor matrix of the poisoned client is obtained by backpropagation. Finally, VGFL-SA flip the edge with the largest\ngradient to get the poisoned graph."}, {"title": "4.1 Graph Augmentation", "content": "VGFL-SA uses graph contrastive learning to accomplish the unsupervised task, graph augmentation is essential for\ncreating graph views, graph augmentation preserves and expands the topological and semantic information of the\ngraph. VGFL-SA considers edge augmentation and node feature augmentation when augmenting the view, the specific\naugmentation methods are as follows.\nEdge Augmentation. Consider an augmentation consisting of edge removal and edge addition. Specifically, given\na client information Gi = (Ai, Xi), where Ai and Xi denote the adjacency matrix and the feature matrix of the i-th\nclient graph, respectively. Edge rewriting augments the structure by masking without considering node features."}, {"title": "4.2 Gradient Attack", "content": "Our goal is to perform graph contrastive learning using augmented views, using the differences between the augmented\nviews as supervised signals, and then obtain the gradient of the adjacency matrix by contrastive learning. The\nperformance of embedding is reduced by modifying the client links using gradient modification rules.\nThe above process can be described as:\n$\\max_{A^i, \\theta} \\mathcal{L}_{gp} \\left(f_{\\theta^*}\\left(A^1, X^1\\right), f_{\\theta^*}\\left(A^2, X^2\\right)\\right),$\ns.t. $\\theta^* = \\arg \\min_{\\theta} l \\left(f_{\\theta}\\left(A^1, X^1\\right), f_{\\theta}\\left(A^2, X^2\\right)\\right),$\n$\\left|\\left| A - A' \\right|\\right| \\leq \\Delta, \\quad i \\in [1, 2].$\nwhere f denotes a shared encoder, @ is a learnable parameter, and i is the number of augmented views, set to 2. To\nensure invisible rows for the attack, we set the attack budget \u0394.\nBackpropagation of the Eq. 13 to obtain the gradient of the adjacency matrix. In GNNs, the value of the gradient\nreflects the sensitivity of the model to each node or edge in the input graph. Specifically, edges with larger absolute\nvalues of the gradient usually have a greater impact on the model's output. Deleting these edges effectively interferes\nwith the model's learning process, leading to more significant performance degradation.\nThe gradient modification rule can be defined as:\n$\\begin{cases}\\text { Add } e_{i, x y}, s . t ., \\nabla_{i, x y}>0, A_{i, x y}=0,\\left|\\nabla_{i, x y}\\right|=\\max \\left|\\nabla_{i,}\\right|, i \\in[1, n] . \\\\ \\text { Delete } e_{i, x y}, s . t ., \\nabla_{i, x y}<0, A_{i, x y}=1,\\left|\\nabla_{i, x y}\\right|=\\min \\left|\\nabla_{i,}\\right|, i \\in[1, n] .\\end{cases}$\nwhere $\\nabla_{i, x y}$ denotes the gradient value of the adjacency matrix of node x and node y in the i-th client.\nThe core idea of Eq. 14 is to add and delete edges based on the gradient, adding edges with the largest positive gradient\nand deleting edges with the smallest negative gradient. Deleting significant edges (i.e. edges with large gradient values)\nthis can lead to maximizing the loss function of the model and can directly affect the propagation of information in the\nmodel, causing significant changes in the output.\nIn graph contrastive learning, GCN is often used in differentiable encoders, where the gradient of the augmented graph\nadjacency matrix can be computed by the following equation:"}, {"title": "4.3 Algorithm and Time Complexity", "content": "The pseudo-code of VGFL-SA is given in Algorithm 1.\nComplexity Analysis. VGFL-SA completes the attack before VGFL training and only needs to compute the client\ngradient information, so the time complexity is low. Specifically, VGFL-SA first needs to perform the augmentation\nand generalization operation to get the contrast views, and the time complexity can be denoted as O(T(n + d)), T is\nthe number of iteration. Then, the contrast views shared encoder is trained and then the adjacency matrix gradient\nis obtained by backpropagation, and the time complexity can be denoted as O(Tnd). When the graph is large, the"}, {"title": "5 Experiments", "content": "For our experiments, we chose three commonly used benchmark citation datasets (Cora, Citeseer, PubMed). The overall\ninformation is listed in Table 2, where nodes represent papers and relationships between papers are described using\nedges. Specifically, the Cora dataset, created by Cornell University researchers, contains 2708 machine learning papers\nfrom the literature database, with each paper represented by a bag-of-words model of 1433 dimensions divided into\nseven domains, i.e., there are a total of 7 Labels [63]. The Citeseer dataset, created by Cornell University researchers\nfrom the Citeseer Digital Library randomly selected papers, containing 3327 scientific papers, each represented by a\nbag-of-words model with 3703 dimensions, with 6 types of Label [63]. The PubMed dataset, created by the National\nInstitutes of Health, contains 19717 biomedical papers, with 500 dimensions per node, with 3 labels [64]."}, {"title": "5.2 Baselines", "content": "There are fewer current unsupervised model-based attacks, and we also selected some representative semi-supervised\nattack models. Specifically, unsupervised attacks are Random, UNEAttack [65]. Semi-supervised attacks are DICE\n[66], PGD [67], MinMax [68], Metattack [69], and Graph-Fraudster [27]. Semi-supervised attacks use node labels\nas key information, unsupervised attacks do not use node labels. Therefore, semi-supervised attack models tend to\nperform better than unsupervised attack models. Fortunately, experimental results demonstrate that under some metrics,\nVGFL-SA outperforms some unsupervised attacks.\nUnsupervised Attacks:\nRandom: Random attack does not rely on a deep analysis of the graph, nor does it focus on the weights of the edges. It\nsimply perturbs the edges by randomly modifying.\nUNEAttack [65]: UNEAttack is the first unsupervised attack against graph adversarial attacks, which uses the theory\nof feature optima to disrupt node embeddings to degrade the performance of GNNs.\nSemi-supervised Attacks:\nPGD [67]: PGD proposes a projected gradient descent based attack against a predefined model of GNNs (poisoning\nattack) using node labeling information.\nMinMax [68]: MinMax uses node labeling information to propose a gradient-based attack against retrainable GNNs\nmodels (evasion attacks).\nMetattack [69]: Metattack uses meta-learning algorithms for hyper-parameter optimization, with the core idea of\noptimizing input graph data as hyper-parameters for poisoning attacks in a black-box setting. To mitigate the high cost\nof meta-gradient computation and storage, first-order approximation and heuristic algorithms can be used to simplify\nthe computation of meta-gradient.\nDICE [66]: DICE solves the network centrality metrics through heuristics and modifies the graph structure to accomplish\nthe attack.\nGraph-Fraudster [27]: Graph-Fraudster is the first attack proposed for graph federated learning, which uses the\ngradient information of sum nodes to produce adversarial perturbations by adding noise to the global node embedding.\nGraph-Fraudster is an algorithm based on a targeting attack, i.e., it focuses on the classification of certain nodes. In this"}, {"title": "5.3 Local GNNS", "content": "We use three commonly used GNNs to evaluate the effectiveness and transferability of VGFL-SA as detailed below:\nGraph Neural Network (GCN) [60]: GCN is a deep learning model for graph-structured data. GCN achieves the\naggregation and updating of node features by performing convolutional operations on node features using the adjacency\nmatrix of the graph.\nGraph Attention Neural Network (GAT) [61]: GAT introduces a self-attention mechanism that enables different\nweights to be dynamically assigned to neighboring nodes during feature aggregation.\nRobust GCN [70]: Robust GCN aims to improve the robustness of graph neural networks to noise and perturbations.\nRobust GCN enhances the model's resistance to input perturbations by introducing regularization techniques and\nrobustness mechanisms."}, {"title": "5.4 Parameter Settings", "content": "In the local GNNs, the number of layers are set to 2 and the hidden layer output is 32 dimensions. In GAT, the number\nof attention heads is set to 2. The GNNs activation function is ReLU. VGFL is optimized using Adam with a learning\nrate of 0.001. During training, we randomly divide the nodes into 10%/10%/80% of the train/test/validation sets. We\nrun each experiment 10 times and report the average. For client data division, we use 2 clients by default, where the data\nratio of client 1 and client 2 is 0.5:0.5. The number of poisoned clients is 1 by default. In graph contrastive learning, the\nnumber of contrastive views is set to 2, and 2 layers of GCN are used as encoders. To ensure the stealthiness of the\nattack, we set the attack budget to A = am, where m is the number of edges and a is the budget factor, which is set to\n0.1 by default.\nOur experimental environment consists of Xeon(R) Gold 6130 (CPU), Tesla V100 32 GiB (GPU), and 25 GiB memory."}, {"title": "5.5 Assessment of Metrics", "content": "In order to evaluate the performance of VGFL-SA in detail, we use Accuracy, Precision, Recall, F1-Score, MAE and\nLog Loss metrics in our experiments. The details are as follows:\n$Accuracy = \\frac{T P+T N}{T P+F N+F P+T N}$\n$Precision = \\frac{T P}{T P+F P}$\n$Recall = \\frac{T P}{T P+F N}$\n$F1 = 2 \\times \\frac{\\text { Precision } \\times \\text { Recall }}{\\text { Precision+Recall }}$\n$M A E = \\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right|$\n$LogLoss = - \\frac{1}{n L} \\sum_{i=1}^{n} \\sum_{j=1}^{L} Y_{i j} \\log \\hat{Y}_{i j}$\nWhere, TP is the accurate optimistic predictions, FN is the false negative predictions, FP is the false positive predictions,\nand TN is the true negative predictions [18]. \u0177r is the predicted value of node i and yi is the true value. n represents\nthe number of samples, L is the number of labels, the actual label of sample i is (taking the value 0 or 1, indicating\nwhether sample i belongs to the j-th label), and the probability predicted by the model is \u0177ij (0 \u2264 \u0177ij \u2264 1 indicating\nthe probability that sample i belongs to the j-th label)."}, {"title": "5.6 VGFL-SA Attack Performance", "content": "In this section, we examine the node classification results for several types of attacks on three common datasets, the\nresults as shown in Table 3 and Table 4.\nAll the attacks shown are able to harm the performance of VGFL as shown in Table 3, i.e., the attacks are able to cause\na decrease in the accuracy of node classification. Specifically, the attack performance of semi-supervised learning\ntends to be better than the performance of unsupervised attacks. For example, using GCN as the victim model, the\nsemi-supervised learning-based Metattack achieves the best performance of 58.9% in PubMed, and our proposed\nunsupervised learning-based VGFL-SA achieves a performance of 60.8%. This result is within our expectation because\nsemi-supervised models usually outperform unsupervised models in graph deep learning. Semi-supervised learning\nmakes use of partially labeled data, and this labeling information can help the model better learn the structure and\nfeatures of the data. Attacks based on semi-supervised models are good at capturing the relationships between nodes,\nand can more accurately identify the key perturbations affecting GNNs through the information of labeled nodes. In\nVGFL, when many client nodes exist unlabeled, semi-supervised learning-based attacks cannot proceed successfully,\nand the attacker can only use unsupervised model-based attacks to explore the robustness. In particular, in PubMed,\nVGFL-SA outperforms PGD, MinMax, DICE, and Graph-Fraudster. Due to the fact that PubMed has the highest\naverage degree. A large amount of structural information can be retained in the client, and the more useful information\nthat VGFL-SA can get. The quality of the generated structural perturbations will be high, and thus the performance is\nexcellent.\nIn unsupervised model-based attacks, our proposed VGFL-SA achieves optimal performance in various datasets. For\nexample, in Citeseer, Random, UNEAttack and VGFL-SA classification accuracies are 61.5%, 56.0% and 54.6% when\nGNNs are modeled as GCNs, respectively. Since Random modifies edges randomly and does not rely on any graph\ninformation, it is not easy to identify key edges in the graph and will not be used often in real attacks. UNEAttack uses\nDeepWalk to generate node embeddings and then obtains structural perturbations through singular value decomposition.\nSince DeepWalk can only use the node index information, it cannot use the feature information of the nodes, which\nmeans that the quality of the generated perturbations is poor. From the above results, it can be seen that in unsupervised\nlearning based attacks, our proposed model can generate structures with stronger perturbations in the same budget.\nIn addition to the Accuracy metric, we also conducted experiments in Precision, Recall, F1-Score, MAE and Log\nLoss metrics. In most cases, the results are the same as in the Accuracy metric, i.e., Metattack performs best in semi-\nsupervised learning-based attacks and VGFL-SA performs best in unsupervised learning-based attacks. However, in a\nfew cases, our proposed model can outperform semi-supervised learning based attacks in some metrics. For example, in\nPubMed, when the victim model is GCN, the F1-Score of VGFL-SA is 59.6%, and the Recall of PGD, MinMax, DICE,\nand Graph-Fraudster are 59.9%, 60.7%, 61.0%, and 60.5%, respectively. This indicates that VGFL-SA recognizes\nirrelevant noise in the graph and achieves the desired results without using node labeling generated perturbations.\nIn addition, we also verify the transferability of the attack. We transfer the victim model to GAT and RobustGCN\nand the results are shown in Table 3 and Table 4. Observations show that VGFL-SA achieves good performance with\ndifferent GNNs. For example, the classification accuracy and Recall of VGFL-SA are { 56.7%, 55.9%} and { 55.4%,\n55.5%} in GAT and RGCN when the dataset is Cora, respectively. In conclusion, our proposed model also has good\ntransferability."}, {"title": "5.7 Attack Budget", "content": "The experiments in this section investigate the effect of attack performance on attack budget, and the results are shown\nin Fig. 3. We use the accuracy metric to measure the attack performance. For the validity of the experiment, the\nperturbation factors a are set to 0.06, 0.08, 0.1, 0.12 and 0.14.\nFrom the results in Fig. 3, it can be seen that as the perturbation factor a increases, the node classification performance\ndecreases, i.e., the attack performance is positively proportional to the attack budget. When the perturbation factor a\nincreases, the higher the number of modified links, the more harmful information VGFL learns, which eventually leads\nto incorrect decision making in downstream tasks. When the perturbation factor a is small, there is little difference in\nthe performance of all attacks. As the perturbation factor a increases, the difference in attack performance starts to\nincrease. For example, in PubMed, when the perturbation factor a are 0.06 and 0.12, the performance of Metattack\nand VGFL-SA are {62.5%, 56.2%} and {63.2%, 57.6%}, respectively. Comparing several types of attacks, it can be\nfound that our proposed model achieves the best performance in the unsupervised learning model regardless of the\nperturbation rate. In particular, VGFL-SA (57.6%) can outperform some semi-supervised models (MinMax: 57.8%,\nDICE: 58.1%, Graph-Fraudster: 59.4%) when the perturbation factor a is 1.2 in PubMed."}, {"title": "5.8 Comparative Learning Components", "content": "The experiments in this section investigate the effect of the comparative learning components on the performance of\nthe attack, and the results are shown in Table 5. We find that the higher the degree of randomization, the worse the\nperformance of the model in comparative learning. For example, VGFL-SA-V3 sets both structure and features to be\nrandomly augmented, and it has the lowest classification accuracy in all datasets, with an average performance 2%"}, {"title": "5.9 Number of clients", "content": "In this section, the effect of the number of clients on the attack is investigated for the cases of multiple clients-one\npoisoned client and multiple clients-multiple poisoned clients, and the results are shown in Fig. 4 and Fig. 5.\nFig. 4 shows the classification performance of various attacks in the multiple client-one poisoned client case. In all\ndatasets, as the number of multiple clients increases, the classification accuracy of the attacks also rises, i.e., the attack\nperformance decreases. For example, in PubMed, the classification accuracy of VGFL-SA is 60.8%, 61.8% and 62.5%\nwhen the number of clients K is 2, 3 and 4, respectively. This is because in VGFL, clients usually do not share raw data\namong themselves, but share model information. As the number of clients K increases, the data is dispersed more, and\nit is more difficult for the attacker to obtain enough information to construct an effective counter sample. In addition,\nwhen the number of clients is high, malicious information from poisoned clients may be canceled out, leading to more\nstable updates to the global model."}, {"title": "5.10 Proportion of poisoned clients' data", "content": "In this section, the impact of participant data percentage on VGFL-SA performance is investigated and the results are\nshown in Table 5.10. Proportion of poisoned clients' data are set to 0.3, 0.4, 0.5, 0.6, 0.7, 0.8.\nIt is observed that as the"}]}