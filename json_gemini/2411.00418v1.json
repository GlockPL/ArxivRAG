{"title": "SELF-EVOLVED REWARD LEARNING FOR LLMS", "authors": ["Chenghua Huang", "Zhizhen Fan", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Zeqi Lin", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a crucial technique\nfor aligning language models with human preferences, playing a pivotal role in\nthe success of conversational models like GPT-4, ChatGPT, and Llama 2. A core\nchallenge in employing RLHF lies in training a reliable reward model (RM), which\nrelies on high-quality labels typically provided by human experts or advanced\nAI system. These methods can be costly and may introduce biases that affect\nthe language model's responses. As language models improve, human input may\nbecome less effective in further enhancing their performance. In this paper, we\npropose Self-Evolved Reward Learning (SER), a novel approach where the RM\ngenerates additional training data to iteratively improve itself. We conducted\nextensive experiments on multiple datasets such as HH-RLHF and UltraFeedback,\nusing models like Mistral and Llama 3, and compare SER against various baselines.\nOur results demonstrate that even with limited human-annotated data, learning\nfrom self-feedback can robustly enhance RM performance, thereby boosting the\ncapabilities of large language models (LLMs).", "sections": [{"title": "INTRODUCTION", "content": "Reinforcement Learning from Human Feedback (RLHF) is a well-established approach that aligns\nLarge Language Models (LLMs) with human preference data Ouyang et al. (2022); Bai et al. (2022b).\nThe standard approach involves learning a reward model (RM) from human preferences and the\nlearned RM is then frozen to train LLMs via Reinforcement Learning (RL) such as Proximal Policy\nOptimization (PPO) Schulman et al. (2017a). Another common approach directly trains LLMs\nfrom the human preference data without learning an RM such as Direct Preference Optimiztion\n(DPO) Rafailov et al. (2024). Both approaches rely heavily on the size and quality of human-annotated\npreference data. However, the availability of such data is often limited and expensive to acquire,\nposing a significant bottleneck in the development and performance of RL approaches Yuan et al.\n(2024b). This dependency on human-annotated data hinders the scalability of strong LLMs that\nrequire vast amounts of labeled data to achieve greater performance Kaplan et al. (2020); Muennighoff\net al. (2024). To mitigate the dependency, recent works leverage the AI feedback to train RMs, referred\nto as Reinforcement Learning from AI Feedback (RLAIF) Bai et al. (2022b); Lee et al. (2023), which\nreduces the reliance on human-annotated data. However, they hold heuristic assumptions that LLMs\ncan provide high-quality feedback and they often requires stronger LLMs to provide feedback Pang\net al. (2023).\nRecent advancements suggest that LLMs have the potential to serve as world models to a certain\ndegree, capable of understanding world knowledge and complex patterns independently of explicit\nhuman input Hao et al. (2023); Guan et al. (2023); Zhao et al. (2024). Leveraging this ability, LLMs\ncan evaluate and provide feedback. In the context of RLHF and RLAIF, this capability of LLMs\ncan be extended as the role of RMs, and RL approaches rely heavily on the RMs Dewey (2014); Li\n(2017). Focusing on training a better RM with limited human-annotated data, we propose a novel"}, {"title": "RELATED WORK", "content": ""}, {"title": "REINFORCEMENT LEARNING FROM EXTERNAL FEEDBACK", "content": "Preference learning or now commonly referred to as reinforcement learning from human feedback\n(RLHF) Christiano et al. (2017); Ziegler et al. (2019); Stiennon et al. (2020b); Ouyang et al. (2022);"}, {"title": "SELF-LEARNING IN LLMS", "content": "As the LLMs are developing towards superhuman-level, which may be bottlenecked by human\nperformance level. Similar to the self-improvement in human reflection, self-learning is a new approch\nin improving LLM performance recently. Self-learning in LLMs focuses on enhancing capabilities\nwithout external supervision. SELF-ALIGN Sun et al. (2024) demonstrates self-alignment through\nprinciple-driven reasoning, allowing models to adjust their outputs based on internal guidelines.\nReSTEM Singh et al. (2023) employs self-training to enhance problem-solving abilities. RLC Pang\net al. (2023) and SCoRe Kumar et al. (2024) showcase methods for self-correction and improvement\nusing self-generated data. Additionally, Huang et al. (2022) illustrates how LLMs can refine reasoning\nthrough self-generated rationale-augmented answers, enhancing their explanatory depth. Math-\nShepherd Wang et al. (2024) and Self-Rewarding Language Models Yuan et al. (2024b) demonstrate\nself-rewarding mechanisms, where the model has the ability to provide high-quality rewards to itself.\nOur proposed approach falls in this self-learning paradigm by innovatively using the RM to generate\nfeedback for itself, fostering robust RM training and improvement."}, {"title": "SELF-EVOLVED REWARD LEARNING FOR LARGE LANGUAGE MODELS", "content": "In this section, we present our proposed Self-Evolved Reward Learning (SER) for LLMs. This\napproach enables the RM to iteratively improve itself by learning from its own high-confidence\npredictions, thereby reducing the need for extensive human-annotated data. Initially, the RM is\ntrained with a small set of human-annotated data to provide a basic understanding of good and bad\nanswers. From there, the RM evolves through self-labeling and iterative retraining. The enhanced\nRM is then employed to guide the LLM training via RL approaches. We detail each component of\nour method in the below section, including self-labeling, identifying learning status, data filtering,\nRM retraining and the LLM training via RL with improved and converged RM."}, {"title": "OVERVIEW", "content": "Figure 1 illustrates the overall pipeline of our SER method. This iterative process ensures that both\nthe reward model and the LLM are continuously refined throughout the training cycle. Our method\nfor Reward Model training consists of the following three iterative steps:\n1. Self-Label with Reward Model: The RM is initially trained with a small set of human-annotated\ndata as a warm-up stage, then the RM performs self-labeling on the unlabeled data.\n2. Identify the Learning Status of the Reward Model and Select High-Confidence Data: Evaluate\nthe RM's current ability to differentiate between good and bad answers or to amplify differences\nbetween similar answers. This status assessment guides the selection of high-confidence data."}, {"title": "Retrain the Reward Model with Pairwise Loss", "content": "3. Retrain the Reward Model with Pairwise Loss: After filtering, the selected high-confidence\ndata are used to retrain the RM with pairwise loss, iteratively enhancing its understanding of\nanswer quality.\nWith a few iterations of self-evolved reward learning, the RM training converges or meets the stopping\ncriteria, such as when no further data can be filtered, the RM is then used to guide the training of the\nLLM via RL approaches. The modified PPO algorithm incorporates the evolved reward signals to\noptimize the LLM's policy.\nOur method relies on two distinct learning statuses for the RM: (1) the ability to distinguish between\nclearly good and bad answers, and (2) the ability to refine differences between answers of similar\nquality. These statuses are separated for the following reasons: (a) Targeted Skill Development: by\nrecognizing different learning statuses, the RM can focus on specific skill sets. Initially, the model\nfocuses on clear distinctions (e.g., good vs. bad answers), and as training progresses, it refines its\ncomparative abilities with more subtle distinctions. (b) Adaptive Data Filtering: the data filtering\nprocess is driven by the current learning status, allowing the model to train on the most relevant data.\nThis adaptive approach ensures the model always works on improving the appropriate aspect of its\nperformance. (c) Improved Self-Evaluation: by continuously monitoring its learning status, the\nRM can determine when to shift from one learning focus to another. This dynamic approach fosters\nself-driven, curriculum-like learning.\nFurthermore, by allowing the LLM to generate two answers for each question, the RM is provided\nwith paired examples that are key to both learning statuses. This setup enriches the training data with\ndiverse answer qualities, enabling the RM to improve its discrimination and comparative abilities.\nOnce the RM becomes proficient at handling both tasks, it is well-equipped to guide the LLM during\nreinforcement learning."}, {"title": "STEP 1: SELF-LABEL WITH REWARD MODEL", "content": "As shown in Figure 1, we first predict a reward score for all unlabeled data based on the current\nreward model (RM). This is formally expressed as follows:\n$r_i = \\text{RM}(Q_i, A_i)$\nThis reward score may contain substantial noise, depending on the performance of the current state\nof the RM. We use these reward scores to determine the current training status and data selection\nstrategy. Initially, we employ a small amount of human-annotated data to obtain a seed RM. In this\nstudy, the seed RM is trained using 15% of the entire dataset."}, {"title": "STEP 2: IDENTIFY THE LEARNING STATUS OF THE REWARD MODEL AND SELECT\nHIGH-CONFIDENCE DATA", "content": "Each question $Q_i$ in our self-labeled dataset has two possible answers, $A^1_i$ and $A^2_i$, which can exhibit\nvarious relationships. The RM must differentiate between the following scenarios: One answer is\nclearly better than the other (e.g., $A^1_i$ is good, $A^2_i$ is bad, or vice versa), or both answers are good,\nbut one is better (or both are bad, but one is worse). The RM assigns probabilities $p^1_i$ and $p^2_i$ that\nrepresent the likelihood that $A^1_i$ and $A^2_i$ are \u201cgood\u201d. The goal is to distinguish the relative quality of\nthe answers across these different cases.\nLet $D_{\\text{train}} = \\{(Q_i, A^1_i, A^2_i)\\}_{i=1}^N$ be the training dataset. The learning status $S$ is determined by the\npredicted probability differences between $A^1_i$ and $A^2_i$:\n$\\Delta p_i = \\frac{1}{N} \\sum_{i=1}^N |p^1_i - p^2_i|$\nWe define the learning status S using thresholds $\\tau_{\\text{low}}, \\tau_{\\text{high}}$, and $\\tau_{\\Delta}$:"}, {"title": "STEP 3: RETRAIN THE REWARD MODEL WITH FILTERED DATA USING PAIRWISE LOSS", "content": "Based on the state of the RM determined in Step 2, we select different data filtering strategies as\noutlined below:\nHere, $D_{\\text{unlabeled}}$ refers to the unlabeled data. In Status 1, the filter selects high-confidence data where\n$(\\text{RM}(Q_j, A^1_j) > \\tau_{\\text{high}} \\text{ and } \\text{RM}(Q_j, A^2_j) < \\tau_{\\text{low}}) \\text{ or } (\\text{RM}(Q_j, A^1_j) < \\tau_{\\text{low}} \\text{ and } \\text{RM}(Q_j, A^2_j) >\\text{Thigh})$, ensuring the model trains on reliable examples. In Status 2, the filter selects pairs where\nthe reward difference $|\\text{RM}(Q_j, A^1_j) - \\text{RM}(Q_j, A^2_j)|$ exceeds a threshold $\\delta$, focusing on refining\ncomparative judgments. After filtering, the model is retrained using pairwise loss, allowing the model\nto compare answers relatively rather than relying on absolute labels. which consistently improves\nperformance by focusing on relative comparisons rather than absolute classifications. The pairwise\nloss function is:\n$L_{\\text{pair}} = \\frac{1}{|D_{\\text{filtered}}|} \\sum_{(Q_j, A^1_j, A^2_j) \\in D_{\\text{filtered}}} \\text{max}(0, \\Delta - (\\text{RM}(Q_j, A^1_j) - \\text{RM}(Q_j, A^2_j)))$\nwhere $\\Delta$ is the desired margin between reward scores. This iterative process, i.e., filtering data and\nretraining with pairwise loss, enables the RM to progressively refine its judgment until it converges."}, {"title": "STEP 4: TRAIN THE LLM VIA RL WITH SELF-EVOLVED REWARD MODEL", "content": "After self-evolving the RM, we use it to guide the training of the LLM via RL. To accommodate\nthe refined reward signals from RM, we modify the PPO framework. LLM training is framed as a\npolicy optimization problem. The policy $\\pi_\\phi$ generates responses A for inputs Q, and the objective\nis to optimize $\\pi_\\phi$ to maximize the rewards generated by the self-evolved RM: $r = \\text{RM}(Q, A)$. We\nmaximize the expected reward from the self-evolved RM: $\\text{max}_\\phi \\mathbb{E}_{Q \\sim D_{\\text{train}}, A \\sim \\pi_\\phi(\\cdot|Q)} [\\text{RM}(Q, A)]$"}, {"title": "THEORETICAL ANALYSIS", "content": ""}, {"title": "MAIN FINDINGS", "content": "SER consistently and effectively enhances model performance. As shown in Table 1, compared to\nthe baseline that uses only 15% of the data, SER improves the model's performance by incorporating\nself-labeled data for training. Through multiple iterations, the model achieves significant performance\ngains, resulting in an average 7.88% increase in accuracy. Furthermore, we find that as the model's\nparameter size increases, the foundational capability strengthens, and the potential for SER's self-\nimprovement further enhances. Larger parameter models can achieve higher self-improvement\nbenefits. Mistral 7B attains an average performance improvement of 7.74% during iterations, Llama\n8B achieves an average improvement of 8.94%, and Llama 13B sees an average improvement of\n6.76%. Across four datasets, Llama 8B surpasses Mistral 7B.\nIn data-rich scenarios (Stack Overflow), the performance gains from SER become smaller, averaging\nonly 2.4%. In such data-rich contexts, a clear scaling trend with model parameter size is observed.\nThe larger the model parameters, the greater the benefits of SER's self-improvement. Mistral 7B"}, {"title": "FINE-GRAINED ANALYSIS", "content": "In order to conduct a more detailed analysis of what occurs during the model's iterations, we present\nthe changes in the model's accuracy on the validation set, as shown in Figure 2. Additionally, we\nillustrate the variations in the amount of training data across different iterations, as depicted in\nFigure 3. Our main conclusions are as follows:\nThe model can iteratively enhance its performance on self-labeled data, even if the self-labeled\ndata contains noise. As shown in Figure 2, Loop1 consistently enhances the model's performance\nin every experimental setup. During the Loop1 phase, the model's performance is relatively weak,\nand there may be significant noise in the model's self-feedback; therefore, it is essential to select\nhigh-confidence samples to improve the model's performance(Status1). Generally, the performance\nimprovement in Loop1 is the most significant among all loops, and it can filter out the largest number\nof training examples. As illustrated in Figure 2, on average, Loop1 provides a 4.54% enhancement\nto the model's performance. This also verifies our Theory 1, which posits that when the model's"}, {"title": "PPO RESULTS", "content": "To validate the effectiveness of SER, we use the previously mentioned RM to guide PPO training,\nthereby optimizing the LLM. We conduct experiments on the Anthropic HH RLHF dataset and the\nStackoverflow dataset, as shown in Figure 4. In the hh-rlhf dataset, all self RLHF models exceed the\nSFT baseline in terms of win rate, indicating that the self RLHF approach enhances the capabilities\nof the LLM. Compared to RMs trained with the full human-annotated data, the win rate in PPO\nexperiments demonstrates a consistent trend with the performance of the RMs. For Mistral 7B, the\naccuracy of the self RLHF RM surpasses that of the RM trained with the full dataset, and in PPO\nexperiments, the win rate also slightly exceeds that of the full model. Additionally, to verify the\ngeneralizability of our method, we conduct the same experiment on the StackOverflow dataset. As\nshown in Figure 4(b), the SER models outperform the full models to a certain extent, demonstrating a\ntrend consistent with the accuracy of the RMs. In summary, our main findings are as follows:\nSER enhances the capabilities of LLMs, and the degree of enhancement is positively correlated\nwith the performance of the RMs. Through the self-evolved approach, we improve the performance\nof RMs using a limited amount of human-annotated data. Leveraging RMs to guide the learning of\nLLMs results in stronger LLMs. Theoretically, this process can be iterative, whereby stronger LLMs\ngenerate higher-quality responses, further enhancing the performance of RMs. However, due to the\ncomputational cost of PPO, we do not conduct related experiments. Additionally, we find that the\nperformance gains in the PPO process are positively correlated with the performance of the RMs;\nstronger RMs generally guide the training of stronger LLMs."}, {"title": "DISCUSSION", "content": "Our paper demonstrates empirical performance improvements through a self-evolved RM driven by\nintuitive motivations, though a rigorous theoretical analysis of its effectiveness is still needed. The\ndata filtering strategies are empirical, yet it's interesting that different datasets exhibit similar learning\nstatuses in each iteration loop. Future work includes developing a more robust and autonomous\nmethod to identify learning statuses and filter self-labeled data. On the other hand, our method\nprovides a feasible pathway to enhance reward modeling capabilities. An avenue worth exploring is\ngenerating more diverse responses through LLMs. By applying our method, a robust and general\nreward model can be developed to assist all existing feedback-based training methods. Additionally,\nintegrating LLMs into the entire self-evolved reward learning loop is another future work, specifically\nby incorporating step 4 in each iteration and using LLMs to generate responses for the RM to perform\nself-labeling. Our work presents a potential solution to break through the performance ceiling of\nthose strongest LLMs."}, {"title": "CONCLUSION", "content": "In this work, we introduce SER, a simple yet effective method of self-evolution that enhances model\nperformance across various datasets and models. By allowing the model to generate its own labeled\ndata and controlling the model's learning state to select appropriate data, we achieve iterative evolution\nthat ultimately converges to, or even exceeds, the performance ceiling. Extensive experiments indicate\nthat our key design (consideration of different learning states) is essential, and we analyze the effects\nthroughout the iterative process of SER, providing valuable insights for the self-improvement of\nLLMs."}, {"title": "THEORETICAL ANALYSIS", "content": ""}, {"title": "CONVERGENCE OF THE REWARD MODEL DURING SELF-TRAINING", "content": "In our SER framework, the RM iteratively improves itself by filtering self-labeled data based on\npredicted probabilities and retraining using pairwise loss. The theoretical analysis focuses on the\nconvergence of the RM's predicted probabilities over time."}, {"title": "ASSUMPTIONS", "content": "Assumption 1 (Initial Quality of Predicted Probabilities). The initial RM $R^{(0)}$ provides predictions\nwhere, for a pair of answers $(A^1_i, A^2_i)$, the probability difference $\\Delta p_i^{(0)} = |p^1_i - p^2_i|$ is meaningful, i.e.,\nthe probability difference reflects the relative quality of answers with reasonable accuracy.\nAssumption 2 (High-Confidence Data Filtering). During self-training, only pairs of answers $(A^1_i, A^2_i)$\nwhere the predicted probability difference exceeds a confidence threshold $\\tau_p$ (e.g., $|p^1_i - p^2_i| > \\tau_p$)\nare used for further training. This ensures that the model uses reliable self-labeled data."}, {"title": "THEORETICAL RESULT", "content": "Theorem 1 (Convergence of Reward Model during Self-Training). Under Assumptions 1 and 2,\nand with appropriate choices of confidence threshold $\\tau_p$ and pairwise comparison margin $\\delta$, the\nself-training process of the RM converges, and the predicted probabilities become increasingly\nreliable with each iteration, i.e., the RM improves over time."}, {"title": "PROOF SKETCH", "content": "The key steps of the proof are as follows:\n1. Initial Quality of Predictions: From Assumption 1, the RM begins with predictions that, while\nnot perfect, provide meaningful probability differences between answer pairs.\n2. High-Confidence Data Selection: By filtering the data based on the confidence threshold $\\tau_p$\n(Assumption 2), we ensure that only reliable, high-quality data points are used for retraining. This\navoids the risk of propagating errors from uncertain self-labeled data.\n3. Iterative Improvement: Each iteration of the self-training process uses pairwise loss to refine the\nRM's ability to distinguish between answer pairs, resulting in improved predicted probabilities\nwith each training step.\n4. Convergence: As the self-training process continues, the RM's predicted probabilities become\nmore accurate, converging to a stable state where further self-training leads to diminishing returns\nin performance improvement."}, {"title": "DISCUSSION", "content": "Theorem 1 shows that, under reasonable conditions, the self-training process for the RM leads to\nimproved performance over time. The key is to carefully filter the data based on predicted probabilities\nto ensure that only reliable self-labeled examples are used for training. This filtering mechanism\nprevents error propagation and ensures that the RM improves iteratively."}, {"title": "CONVERGENCE PROPERTIES OF PPO WITH A LEARNED REWARD MODEL", "content": "Proximal Policy Optimization (PPO) is used to train the LLM with reward signals generated by the\nself-evolved RM. The theoretical challenge lies in ensuring that PPO can converge effectively when\nusing an RM that has been trained through self-labeling, which may introduce some inaccuracies in\nthe reward estimates."}, {"title": "ASSUMPTIONS", "content": "Assumption 3 (Bounded Reward Estimation Error). The RM provides reward signals where the\nerror relative to the true reward function $R^*(Q, A)$ is bounded, i.e., $|\\text{RM}(Q, A) \u2013 R^*(Q, A)| \\leq \\epsilon_r$,\nwhere $\\epsilon_r$ is small."}, {"title": "THEORETICAL RESULT", "content": "Theorem 2 (Policy Improvement with Approximate Rewards). Under Assumption 3 and using PPO\nwith appropriately chosen hyperparameters, the LLM's policy $\\pi_\\phi$ converges to a near-optimal policy\nwith respect to the true reward function $R^*$, with the error bounded by $\\epsilon_r$."}, {"title": "PROOF SKETCH", "content": "The proof proceeds as follows:\n1. Consistency of Policy Gradient: The policy gradient computed using the learned RM is close to\nthe true policy gradient computed using $R^*$, with the error bounded by $\\epsilon_r$.\n2. Effect of Reward Estimation Error: The bias introduced by the reward estimation error affects\nthe gradient update but remains small, as long as $\\epsilon_r$ is small.\n3. PPO Stability: PPO's clipping mechanism prevents large updates, ensuring that policy updates\nremain stable even in the presence of small reward estimation errors.\n4. Convergence: By iteratively applying PPO updates, the policy converges to a near-optimal policy\nrelative to the true reward function, with the error in the final policy bounded by $\\epsilon_r$."}, {"title": "DISCUSSION", "content": "Theorem 2 shows that PPO remains effective even when using a learned RM, provided the reward\nestimation errors are small. Our self-evolved RM minimizes $\\epsilon_r$ over time by iteratively refining its\npredicted probabilities, ensuring that the reward signals used for policy optimization become more\naccurate. As a result, PPO can still guide the LLM toward a near-optimal policy."}, {"title": "ALGORITHM SUMMARY", "content": "Algorithm 1 summarizes the entire SSRL-LLM method, including iterative self-evolved RM training\nand reinforcement learning for LLM policy optimization."}, {"title": "EXPERIMENTAL SETUP", "content": "SFT training: For each Base Model, we perform instruction fine-tuning using preference data.\nSimilar to the setting by Rafailov et al. (2024), we sample higher quality responses from the preference\ndata based on human annotations to use as training data (for instance, in the HH-RLHF dataset, we\nsample responses labeled as 'chosen' for instruction fine-tuning). We conduct standard instruction"}, {"title": "DATASET STATISTICS", "content": "Our experiments explore four different preference datasets as show in Table 2. StackOverflow\ncontains over 3,000K QA pairs collected from StackOverflow. Each question receives a score based\non the number of upvotes, resulting in a comparison pair. HH-RLHF: we use human preference data,\nwhich consists of 118K helpful and 42K harmless instances as the training set. Similar to previous\nwork, we select the last round of dialogues to construct the data into a single-turn dialogue format.\nUltraFeedback is constructed by large language models (LLMs). It collects 64K instructions from\nvarious sources, generates 256K responses using LLMs such as LLaMA, and has these responses\nannotated and scored by GPT4. From this process, we create a preference dataset containing 100K\nentries. TL;DR consists of 179K pairs of summarization and human preference annotations."}, {"title": "EVALUATION METRICS AND BASELINE", "content": "Reward Modeling. The standard process of RLHF involves training an RM based on preference\ndata to predict the preferences between human and model responses. Subsequently, reinforcement\nlearning methods are used to optimize the language model based on the RM. Accuracy: We use\naccuracy to measure the performance of reward modeling. Specifically, for a given preference data,\nif the reward value assigned by the RM to the chosen response is higher than that to the rejected\nresponse, the prediction is considered correct. Baseline: considering that our method uses only a"}, {"title": "TRAINING DETAILS", "content": "SFT training. We use the following hyperparameters for instruction fine-tuning training. We employ\na learning rate of 2e-5 with cosine decay, 2 warmup steps, and a batch size of 16. We calculate the\nloss only for the target tokens rather than the full input sequence, and we train for 3 epochs on the\ntraining data. For smaller parameter models (e.g., llama 8B, Mistral 7B, llama 13B), we conduct the\ntraining on 8 NVIDIA A100 80G GPUs. For the llama 70B model, we perform the training on 16\nNVIDIA A100 80G GPUs.\nReward training. To enable the model to learn the relative ranking among different responses, we\nuse a pair-wise loss. We employ the sigmoid function to normalize the reward scores to a range of\n0-1. We utilize the LORA method to train the RM on the SFT baseline, with a rank of 8, a LORA\nalpha of 32, and a LoRA dropout of 0.1. The task type is sequence classification. We use a learning\nrate of 2e-5 with linear decay and the AdamW optimizer for training over 2 epochs, with a batch size\nof 4 (batch size of 2 for the LLaMA 70B model). We conduct the training on 8 NVIDIA A100 80G\nGPUs (32 NVIDIA A100 GPUs for the LLaMA 70B model).\nPPO training. For PPO training, we use a learning rate of 1.4e-5 and set the generate sample length\nto 256. We employ a batch size of 8 and a mini-batch size of 1, with 4 PPO epochs and 1 gradient\naccumulation step. The target KL divergence is set to 0.1 and initial KL coefficient is set to 0.2. To\nensure a more robust training process, we normalize the range of reward values to -0.5 to 0.5."}, {"title": "IMPLEMENTATION DETAILS", "content": ""}, {"title": "THE SPLIT OF THE DATASET", "content": "For the preference dataset, we split the training and testing sets according to the ratio of SFT:RM:PPO\n= 0.3:0.65:0.05. In this paper, SFT utilizes the chosen responses from the preference data for\ninstruction fine-tuning. For the training of Reward Modeling, our approach randomly samples 15%\nof the RM data for training, while comprehensive comparison experiments train on the entire RM\ndataset. For the HH-RLHF dataset, it is divided into harmful and helpful subsets, and we only select\nthe helpful subset."}, {"title": "STATISTICAL DATA OF THE ITERATIVE PROCESS", "content": "We quantify the amount of data filtered out during the iterative process, as shown in Figure 3. Loop 0\nrepresents a fixed value, accounting for 15% of the overall dataset. We use this portion of the data to\ntrain the seed model, upon which all subsequent iterations are based for further evolution."}, {"title": "PPO LEARNING CURVE", "content": "As shown in Figure 5, we present the reward curves of Mistral 7B and LLaMA 8B on the HH-RLHF\ndataset. Both models reach convergence at around 1200 steps. We scale the reward scores to the\nrange of -1 to 1 using the following formula:"}]}