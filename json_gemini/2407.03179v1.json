{"title": "Motion meets Attention: Video Motion Prompts", "authors": ["Qixiang Chen", "Lei Wang", "Piotr Koniusz", "Tom Gedeon"], "abstract": "Videos contain rich spatio-temporal information. Traditional methods for extracting motion, used in tasks such as action recognition, often rely on visual contents rather than precise motion features. This phenomenon is referred to as 'blind motion extraction' behavior, which proves inefficient in capturing motions of interest due to a lack of motion-guided cues. Recently, attention mechanisms have enhanced many computer vision tasks by effectively highlighting salient visual areas. Inspired by this, we propose using a modified Sigmoid function with learnable slope and shift parameters as an attention mechanism to activate and modulate motion signals derived from frame differencing maps. This approach generates a sequence of attention maps that enhance the processing of motion-related video content. To ensure temporally continuity and smoothness of the attention maps, we apply pair-wise temporal attention variation regularization to remove unwanted motions (e.g., noise) while preserving important ones. We then perform Hadamard product between each pair of attention maps and the original video frames to highlight the evolving motions of interest over time. These highlighted motions, termed video motion prompts, are subsequently used as inputs to the model instead of the original video frames. We formalize this process as a motion prompt layer and incorporate the regularization term into the loss function to learn better motion prompts. This layer serves as an adapter between the model and the video data, bridging the gap between traditional 'blind motion extraction' and the extraction of relevant motions of interest. Experimentally, we demonstrate that our plug-and-play motion prompt layer, despite its simplicity and lightweight nature, can be seamlessly integrated into existing architectures such as SlowFast, X3D and TimeSformer, achieving state-of-the-art performance on various benchmarks, including the large-scale FineGym and MPII Cooking 2 for fine-grained action recognition.", "sections": [{"title": "1. Introduction", "content": "Video-based research has gained popularity over the past several years due to its extensive applications in human-computer interaction, smart video surveillance, sports, and health-care Wang et al. (2020); Tang et al. (2023). Videos contain rich information: spatially, they include visual contents such as objects, human subjects, and scene layouts; temporally, they show the dynamics of how these objects and humans interact and evolve over time. Early works focused on using expert-designed, handcrafted descriptors to extract spatio-temporal information from videos Dalal et al. (2006); Scovanner et al. (2007); Kl\u00e4ser et al. (2008); Wang and Schmid (2013). However, while they were carefully designed, they could only handle simple contexts and were unable to generalize to other datasets even within the same domain. The major issue is that most descriptors do not focus on motions and heavily rely on visual contents. Compared to human vision system in extracting information, they are now considered outdated, even though some are still in use Wang and Schmid (2013).\nDeep learning has significantly advanced video-based research due to its end-to-end learnable nature Simonyan and Zisserman (2014); Tran et al. (2015); Feichtenhofer et al. (2016); Carreira and Zisserman (2018). New architectures, such as CNNs, RNNs, and transformers O\u2019shea and Nash (2015); Sherstinsky (2020); Vaswani et al. (2017), along with components like normalization layers, skip connections, and dropout Ba et al. (2016); He et al. (2016); Srivastava et al. (2014), and strategies like large-scale pretraining and transfer learning Carreira and Zisserman (2018); Zhuang et al. (2020), have greatly supported the evolution of modern video processing techniques. The attention mechanism has recently joined convolutional layers, MLPs, and RNNs as a fundamental building block Vaswani et al. (2017). Initially used within transformers in natural language processing, the attention mechanism is now effectively applied to image and video processing tasks Dosovitskiy et al. (2021); Arnab et al. (2021). It helps models focus on the most relevant parts of an image, relevant frames, spatial regions within frames, or significant scenes, thereby understanding both the visual contents and temporal dynamics. However, there are several challenges with attention mechanisms in video processing Guo et al. (2022); Brauwers and Frasincar (2021); Niu et al. (2021): (i) they are computationally intensive due to the calculation of spatio-temporal attention weights, (ii) scalability issues when handling videos of varying lengths, and (iii) capturing temporal dependencies is challenging, as the model must focus not only on spatial features within frames but also on the temporal relationships between frames. Moreover, while attentions provide some level of interpretability by highlighting important regions or frames, interpreting why certain regions or frames receive higher attention can be difficult in complex video tasks. Additionally, attention mechanisms must be robust to variations in video quality, lighting, occlusions, and other environmental factors. Despite large-scale datasets allowing attention-driven models to capture a wide range of spatio-temporal patterns, their generalization to unseen video data remains challenging.\nUnlike the attention mechanism, prompt engineering involves designing prompts to guide language models in producing desired responses Brown et al. (2020); Radford et al. (2021); Kim et al. (2021); Zhu et al. (2021). A well-designed prompt provides instructions or contextual clues that direct the model's output Chen et al. (2023). This allows the attention mechanism to effectively allocate focus to the relevant parts of the input, especially in tasks where there may be many frames and objects, not all of which are relevant. This highlights the potential of using prompts in video processing tasks, where identifying the relevant motions can be challenging. In this paper, we establish a strong connection among video motion, attention and prompt engineering. We introduce the concept of motion prompts to address challenges related to efficiency, interpretability, and generalizability. We use a modified Sigmoid function with learnable slope and shift parameters as a power normalization function to activate the motions: the slope controls the strength of modulation, determining whether to enhance or dampen the motions, and the shift acts as a threshold. The motions to be activated and modulated are represented as a sequence of frame differencing maps computed between consecutive frames. The activated per-frame motions can be viewed as an attention map, due to the enhancement of motion regions both spatially and temporally, as these regions evolve over time. To ensure that the generated attention maps are spatio-temporally smooth and continuous, we introduce a pair-wise temporal attention variation regularization to remove unwanted motions such as noise. We then perform the Hadamard product between each pair of attention map and the corresponding original video frame, resulting in a sequence of highlighted video sequences, which we refer to as video motion prompts. Thus, our motion prompts are motion-dependent, rather than being dependent solely on the dataset. We formalize this process as a motion prompt layer, a plug-and-play component added between the model and the input data, bridging the gap between traditional, \u2018blind motion extraction' (see Fig. 1) and the extraction of relevant motions. We show that, with only two additional learnable parameters, our motion prompt layer significantly enhances action recognition. Our contributions are summarized as follows:"}, {"title": "2. Approach", "content": "First, we present our notation. An overview of our motion prompt layer is provided in Fig. 2. In Appendix B, we provide preliminary information on activation functions, a mathematical view of attention mechanisms, and the power normalization family.\nNotation. Let $I_\\mathcal{T}$ stand for the index set {1,2,\u2026\u2026,T}. Scalars are in regular fonts; vectors are denoted by lowercase boldface letters, e.g., $\\mathbf{x}$; matrices by uppercase boldface, e.\u0434., $\\mathbf{X}$; tensors by calligraphic letters, e.g., $\\mathcal{X}$. Let $\\mathcal{X} \\in \\mathbb{R}^{d_1 \\times d_2 \\times d_3}$ denote a third-order tensor, using the Matlab convention, we refer to its t-th slice as $\\mathcal{X}_{:,:,t}$, which is a $d_1 \\times d_2$ matrix."}, {"title": "2.1. Learnable Power Normalization", "content": "Frame differencing maps. For a T-frame video $\\mathbf{X} = [\\mathbf{F}_1, \\mathbf{F}_2,\\ldots,\\mathbf{F}_T] \\in \\mathbb{R}^{H\\times W \\times 3\\times T}$ where $\\mathbf{F}_t \\in \\mathbb{R}^{H\\times W \\times 3}$ (t \u2208 $I_\\mathcal{T}$), H and W denote the frame height and width, respectively, we first convert it into a grayscale video sequence $\\mathbf{X}' = [\\mathbf{F}'_1, \\mathbf{F}'_2, \\ldots, \\mathbf{F}'_T] \\in \\mathbb{R}^{H\\times W \\times T}$. After normalizing the pixel values between 0 and 1, we compute the frame differencing maps between consecutive frames, resulting in $\\mathbf{D} = [\\mathbf{D}_1, \\mathbf{D}_2, \\ldots, \\mathbf{D}_{T-1}] \\in \\mathbb{R}^{H\\times W \\times (T-1)}$, where $\\mathbf{D}_t = \\mathbf{F}'_{t+1} - \\mathbf{F}'_{t}$ (t \u2208 $I_\\mathcal{T-1}$). Note that the pixel values in $\\mathbf{D}_t$ can be either positive or negative. Positive values indicate areas where the pixel intensity has increased from frame t to frame t + 1, while negative values indicate areas where the pixel intensity has decreased. Note that the pixel values in frame differencing maps are in the range of [-1,1].\nThe frame differencing maps $\\mathbf{D}$, record a sequence of motions between consecutive video frames, capturing both foreground and background motions, as well as noisy patterns. Depending on the task, we aim to enhance the motions of interest while suppressing the rest. For example, in human action recognition, we want to amplify the motions associated with human actions and reduce irrelevant motions. Below, we introduce our learnable Sigmoid activation function as a Power Normalization (PN) function for motion modulation. Fig. 3"}, {"title": "Learnable slope and shift parameters.", "content": "We opt for a modified Sigmoid function with learnable slope a and shift b as a PN function (see Fig. 3 (a)) on frame differencing maps. For a given frame differencing map D, we define:\n$f(D) = \\frac{1}{1+ e^{-a(D-b)}}$ (1)\nThis element-wise function maps the pixel values of frame differencing maps from [-1,1] to [0,1]. For simplicity, we set a > 0 so that $f(\\cdot)$ is a monotone increasing function. The parameter a controls the slope of the Sigmoid function\u00b9, and it influences how sharply the function transitions from its minimum to its maximum value (see Fig. 3(b)). The parameter b acts as the threshold or the point of inflection of the Sigmoid function\u00b2, and it determines the position of the Sigmoid curve along the D axis. We allow b to shift either left or right, hence it can be positive or negative (see Fig. 3(c)). However, directly learning the parameters a and b presents challenges, such as initialization sensitivity and uncertainty in the parameter search space. To address these issues, we design the following two mapping functions to learn m and n instead:\n$a(m) = \\frac{\\alpha}{\\beta|tanh(m)| + \\epsilon}$ \n$b(n) = \\gamma tanh(n)$ (2)\nHere, tanh(\u00b7) is the hyperbolic tangent function, which maps any given m and n to values between -1 and 1. This alignment with the pixel value range in frame differencing maps"}, {"title": "Upper and lower bound.", "content": "Consider the term inside the exponential: $A = a(m)(D - b(n))$, we first examine the derivative of A with respect to D: $\\frac{\\partial A}{\\partial D} = a(m) = \\frac{\\alpha}{\\beta|tanh(m)| + \\epsilon}$. Since this derivative is always positive, A is monotonically increasing with respect to D.\nThe scaling factor a(m) ranges from $\\frac{\\alpha}{\\beta + \\epsilon}$ to $\\frac{\\alpha}{\\epsilon}$ and $D - b(n)$ ranges from -1 - \u03b3 to 1+ \u03b3; hence, the lower and upper bounds of A are $\\frac{\\alpha(-1-\\gamma)}{\\beta + \\epsilon}$ and $\\frac{\\alpha(1+\\gamma)}{\\epsilon}$, respectively.\nThe Sigmoid function $\\sigma(A) = \\frac{1}{1+e^{-A}}$ approaches 0 as value in A becomes large and negative (e.g., $\\sigma(-5(1+0.6)) \\approx 0.0$), and 1 as value in A becomes large and positive (e.g., $\\sigma(5(1+0.6)) \\approx 1.0$). Consequently, for values in D in the interval [-1, 1], the function f(D) is always bounded within [0, 1].\nWell-behaved power normalization. Eq. (3) is continuous and smooth for all real values of D, m, and n. The scaling factor a(m) is always positive, and the exponential term is well-defined, producing a valid, finite value for all input values. The Sigmoid function $\\sigma(A) = \\frac{1}{1+e^{-A}}$ maps any real number to the interval [0, 1], ensuring that f(D) produces values within this range and thereby maintaining proper normalization.\nTherefore, Eq. (3) is a well-behaved PN function given: (i) It is continuous and smooth. (ii) The output values are properly normalized within the range [0,1]. (iii) The Sigmoid function ensures that the function maps real numbers to a bounded interval, maintaining normalization."}, {"title": "2.2. Motion Prompt Layer: An Adapter", "content": "Video motion prompts. Eq. (3) modulates the motions in frame differencing maps via the learnable m and n, resulting in a sequence of normalized frame differencing maps with pixel values ranging from 0 to 1. This element-wise PN process can also be viewed as activating the motions of interest, guided by the generic loss function, e.g., cross-entropy, hence we call this the attention map. The PN process produces a sequence of attention maps: $\\mathcal{A}' = [f(\\mathbf{D}_1), f(\\mathbf{D}_2),\\ldots, f(\\mathbf{D}_{T-1})] \\in \\mathbb{R}^{H\\times W \\times (T-1)}$, spatially highlighting regions where motions are of interest (e.g., 1) and dampening motions that are not of interest (e.g., 0); temporally showing the evolution of attention maps over time. We then duplicate each attention map three times, resulting in $\\mathbf{A}'^{(3)} = [f^{(3)}(\\mathbf{D}_1), f^{(3)}(\\mathbf{D}_2),\\cdots, f^{(3)}(\\mathbf{D}_{T-1})] \\in \\mathbb{R}^{H\\times W \\times 3\\times (T-1)}$, where $f^{(3)}(\\mathbf{D}_t) \\in \\mathbb{R}^{H\\times W \\times 3}$ and $t\\in I_\\mathcal{T-1}$. We perform a channel-wise Hadamard product between each duplicated attention map $f^{(3)}(\\mathbf{D}_t)$ and the original video frame $\\mathbf{F}_{(t+1)}$ so that attention attends to each channel of the original video frame, resulting in a sequence of highlighted video frames:\n$\\mathbf{Z} = \\mathcal{A}'^{(3)} \\circledcirc \\mathbf{X}_{:,:,2:}$ \n$= [f^{(3)}(\\mathbf{D}_1), f^{(3)}(\\mathbf{D}_2), ..., f^{(3)}(\\mathbf{D}_{T-1})] \\circledcirc [\\mathbf{F}_2,\\ldots, \\mathbf{F}_T]$\n$= [f^{(3)}(\\mathbf{D}_1) \\odot \\mathbf{F}_2, f^{(3)}(\\mathbf{D}_2) \\odot \\mathbf{F}_3, ..., f^{(3)}(\\mathbf{D}_{T-1}) \\odot \\mathbf{F}_T],$ (4)\nwhere $\\odot$ denotes the Hadamard (element-wise) product. $\\mathbf{Z} \\in \\mathbb{R}^{H\\times W \\times 3\\times (T-1)}$ denotes the newly generated video, referred to as Video Motion Prompts (VMPs), which are motion-dependent, attention-driven, and provide rich motion cues. $f^{(3)}(\\mathbf{D}_t) \\odot \\mathbf{F}_{(t+1)} \\in \\mathbb{R}^{H\\times W \\times 3}$ shows the motion prompt for the t-th frame. Below, we show that our video motion prompt generation process is, in fact, connected to the attention mechanism."}, {"title": "Connecting to attention mechanism.", "content": "We rewrite $f^{(3)}(\\mathbf{D}_t)$ using the Sigmoid function $\\sigma^{(3)}(.)$, replace the frame differencing map $\\mathbf{D}_t$ with the grayscale conversion function $h(.)$ as $h(\\mathbf{F}_{t+1}) - h(\\mathbf{F}_{t})$, and rewrite Eq (4) in the form of per-frame motion prompt (we use Eq. (2) for the scaling factor and shift parameter and omit m and n for simplicity):\n$\\mathbf{Z}_t = f^{(3)}(\\mathbf{D}_t) \\odot \\mathbf{F}_{(t+1)} $\n$= \\sigma^{(3)}(a[h(\\mathbf{F}_{t+1}) \u2013 h(\\mathbf{F}_t) - b]) \\odot \\mathbf{F}_{(t+1)}$ (5)\nwhere A = a[h($\\mathbf{F}_{t+1}) \u2013 h(\\mathbf{F}_t) \u2013 b] can be viewed as an attention matrix with the shifting parameter b modulating the pixel intensity changes between each pair of grayscaled consecutive frames. The Sigmoid (denoted as $\\sigma^{(3)}$) outputs are similar to the Softmax function outputs in the standard attention mechanism (as in Eq. (9) of Appendix B), which transforms raw attention scores to highlight the most important parts of the h-transformed frame differencing maps. Since we operate on each pixel in frame differencing maps to highlight all relevant motion pixels, there is no need to satisfy the criterion that attention weights sum up to 1, as required by the Softmax function. $\\mathbf{F}_{(t+1)}$ is analogous to the value matrix V in Eq. (10) of Appendix B. This shows that our motion prompt generation process is analogous to the standard attention mechanism.\nUnlike the traditional attention mechanism, our attention mechanism offers (i) lightweight computation with only two learnable parameters, (ii) interpretability, as the learnable scaling factor and shift parameter have well-explainable functionalities in the motion modulation process, and (iii) generalizability, as A is motion-dependent, relying on frame differencing maps rather than being dataset-dependent."}, {"title": "Temporal attention variation regularization.", "content": "To ensure temporally the smoothness and continuity of generated attention maps, we introduce temporal attention variation regularization on pair-wise attention maps:\n$V = \\frac{1}{T-2} \\sum_{t=1}^{T-2} ||f(\\mathbf{D}_{t+1}) \u2013 f(\\mathbf{D}_{t})||_F,$ (6)\nwhere $||\\cdot||_F$ denotes the Frobenius norm. This regularization ensures that each pair of consecutive attention maps has fewer variations in pixel values, promoting the generated attention maps to be temporally smooth while maintaining spatially the regions of motion that are of interest.\nWe design the video motion prompt generation process as a single layer with two learn-able parameters that amplify relevant motions while attenuating irrelevant movements. Eq. (6) is incorporated into the original loss function $L_{ori}$, such as cross-entropy loss for action recognition, used in models like SlowFast and TimeSformer backbones:\n$L = L_{ori} + \\lambda V,$ (7)\nwhere $\\lambda$ is a penalty parameter that controls the strength of this regularization, balancing the trade-off between temporal smoothness and the maintenance of spatially significant motion regions. We simply insert our motion prompt layer between the video input and the model architecture, using Eq. (7) as the loss function to learn the VMPs as new inputs. The entire model can be learned in an end-to-end manner or fine-tuned on specific layers, including the learning of our motion prompt layer."}, {"title": "3. Experiment", "content": "3.1. Setup\nDataset. For generic action recognition, we choose the popular and challenging HMDB-51 Kuehne et al. (2011), which features significant camera and background motion. For fine-grained action recognition, we select the large-scale MPII Cooking 2 (2,881,616 frames,"}, {"title": "3.2. Evaluation", "content": "Analysis of learnable slope and shift. We present the learning process of a and b versus the number of fine-tuning epochs in Fig. 4 with varying regularization penalty parameter $\\lambda$. We use FineGym with TimeSformer pretrained on Kinetics-400 for the fine-tuning and learning of our motion prompts. As shown in the figure, choosing bigger A results in both slope and shift parameters quickly approaching their lower bounds. Using smaller A or set \u03bb"}, {"title": "4. Conclusion", "content": "We introduce video motion prompts to enhance action recognition. We use a modified Sigmoid activation function with learnable slope and shift as a power normalization function on frame differencing maps to activate motions as attention maps. Additionally, we introduce the temporal attention variation regularization term to generate more accurate and smooth motion prompts. We formalize the entire process as a single motion prompt layer acting as an adapter, resulting in improved performance across various benchmarks and backbones."}, {"title": "Appendix A. Related Work", "content": "Below, we review closely related work on motion extraction, attention mechanisms, prompts, and adapter layers for video processing. We also highlight the significant differences between our work and these studies.\nMotion. Optical flow computed between consecutive video frames is a widely used secondary input for video processing Simonyan and Zisserman (2014); Carreira and Zisserman (2018); Wang and Koniusz (2024), complementing the use of conventional videos Wang et al. (2018, 2021). Dynamic images, which record spatio-temporal information in a single frame, were introduced in Bilen et al. (2016). A channel sampling method that combines the R, G, or B channels of consecutive frames into a single frame for better motion capture was proposed in Kim et al. (2022). Recently, Taylor videos have been introduced to capture dominant motions in videos for action recognition Wang et al. (2024).\nUnlike these approaches, our motion prompts are (i) lightweight, with only two extra learnable parameters, (ii) dependent on motions guided by frame differencing maps, and (iii) driven by attention mechanisms that highlight the spatio-temporal motion regions over time.\nAttention. To improve feature representations, attention mechanisms capture the relationships between tokens. Attention mechanisms have been efficiently incorporated into transformers, including self-attention Vaswani et al. (2017); Dosovitskiy et al. (2021) and cross-attention Lin et al. (2021); Hashiguchi and Tamaki (2022); Chen et al. (2021); Wei et al. (2020); Wang and Koniusz (2023), among others.\nOur attention mechanism, however, differs from traditional approaches. It is lightweight and does not require the learning of attention matrices. We use a sequence of frame differencing maps to record video dynamics, along with a newly introduced regularization term, to learn spatio-temporally smooth attention maps. Consequently, our attention mechanism is motion-dependent rather than dataset-dependent. Furthermore, compared to existing works, we use an activation function as power normalization to modulate motions, with learnable slope and shift parameters for controlling motion strengths and thresholding. This makes our attention mechanism more transparent and interpretable.\nPrompt. Prompt engineering has gained significant interest with advancements in image and video processing. Representative works include the use of prompt templates Radford et al. (2021), textual prompts for video content description Hu et al. (2022), incorporating video features into language models as prompts Yang et al. (2022), and learnable continuous prompt vectors as virtual tokens Ju et al. (2022). Visual prompts include methods such as visual prompt tuning Jia et al. (2022), fine-grained visual prompting Yang et al. (2024), among others.\nTo the best of our knowledge, none of these works consider using a sequence of refined motions as motion prompts to enhance video processing tasks. Our motion prompts are defined as a sequence of video frames with highlighted spatio-temporally smooth motion regions per frame. Our motion prompts are learnable and form a plug-and-play motion prompt layer. They are optimized using the original loss function.\nAdapter. There are several layers and mechanisms in neural networks that can be considered as adapters between the data input and the model itself. Embedding layers, commonly used in NLP and recommendation systems, convert categorical data or tokens into dense"}, {"title": "Appendix B. Preliminary", "content": "Below, we refer to the preliminary works used in the paper.\nActivation function. Activation functions such as Sigmoid, Tanh, ReLU, and Softmax are among the most popular and commonly used in neural networks. These functions can be either linear or non-linear, depending on their formulation and the context in which they are applied. A review of activation functions in deep learning can be found in Nwankpa et al. (2018); Dubey et al. (2022). A logistic function is a common S-shaped curve (sigmoid curve) with the equation:\n$f(x) = \\frac{L}{1+ e^{-a(x-b)}}$ (8)\nwhere L is the carrying capacity, a is the logistic growth rate (the steepness of the curve), and b is the point at which the output transitions from below L/2 to above L/2. For values of a much less than b, f(x) is close to 0 (or the lower bound), and for values of x much greater than b, f(x) is close to L (or the upper bound). This characteristic makes b act as a kind of threshold in the Sigmoid function. The standard logistic function is when L = 1, a = 1, and b = 0.\nBoth the Sigmoid and Softmax functions introduce non-linearity; however, the Softmax function additionally provides a means to interpret the neural network's output as probabilities. In attention mechanisms, the Softmax function is applied to a set of scores (often called attention scores or logits) to produce a probability distribution over the elements in the sequence. This ensures that the importance (attention) weights sum to 1.\nA mathematical view of attention. The Vision Transformer (ViT) applies the attention mechanism to image processing by first dividing an image into n patches and then treating these patches as input tokens $X \\in \\mathbb{R}^{nxd}$. The self-attention mechanism involves three main components: query Q, key K and value V matrices, which are computed as linear projections of the input matrix X. The self-attention scores are then computed as the scaled dot-product of the query and key matrices, and the resulting attention matrix is then used to weight the value matrix. The mechanism can be described as follows:\n$A = softmax(\\frac{Q K^T}{\\sqrt{d_k}}),$ (9)\n$Z = AV,$ (10)\nwhere A \u2208 $\\mathbb{R}^{n\u00d7n}$ is the attention matrix representing the attention weights, and Z\u2208 $\\mathbb{R}^{n\u00d7dk}$ is the output of the self-attention layer. Q = $XW^Q$, K = $XW^K$, and V = $XW^V$. $W^Q, W^K, W^V \\in \\mathbb{R}^{d\u00d7dk}$ are learned projection matrices. In practice, multi-head attention is used to allow the model to jointly attend to information from different representation subspaces at different positions. This mechanism enables the ViT to effectively capture the relationships between different parts of an image Dosovitskiy et al. (2021). In our work, we design a lightweight attention mechanism to highlight the motions of interest from a sequence of frame differencing maps.\nPower normalization family. Power Normalization (PN) is used to adjust the power or amplitude of signals to a standard or desired level. It is commonly used in signal and image processing, as well as in statistical methods such as non-linear pooling of features J\u00e9gou et al. (2009); Koniusz and Zhang (2021) and optical flow correction Wang and Koniusz (2024). We apply power normalizing functions to enhance or reduce motions in a sequence of frame differencing maps. A review of well-behaved PN functions such as Gamma, MaxExp, AsinhE (Arcsin hyperbolic function) and SigmE (Logistic a.k.a. Sigmoid functions) can be found in Koniusz and Zhang (2021)."}, {"title": "Appendix C. Boundedness and Differentiability", "content": "Below we proof the boundedness and differentiability for both a(m) and b(n).\nBoundedness. The hyperbolic tangent function tanh(m) has a range of (-1,1). Therefore, \u03b2|tanh(m)| has a range of (0,\u03b2). Adding a positive constant \u03f5 > 0, the range of \u03b2|tanh(m)| + \u03f5 is (\u03f5,\u03b2 + \u03f5). Thus, a(m), which is $\\frac{\\alpha}{\\beta|tanh(m)| + \\epsilon}$, ranges from $\\frac{\\alpha}{\\beta + \\epsilon}$ to $\\frac{\\alpha}{\\epsilon}$. This means a(m) is bounded between $\\frac{\\alpha}{\\beta + \\epsilon}$ and $\\frac{\\alpha}{\\epsilon}$.\nSimilarly, tanh(n) has a range of (-1,1) and b(n) = \u03b3tanh(n) has a range of (-\u03b3,\u03b3). This means b(n) is bounded.\nDifferentiability. To show differentiability, we compute the derivative of a(m) with respect to m: a'(m) = $\\frac{ag'(m)}{[g(m)]^2}$, where g(m) = \u03b2|tanh(m)| + \u03f5. The derivative of the hyperbolic tangent function is tanh'(m) = 1 - tanh\u00b2(m). The derivative of | tanh(m)| is generally given by $\\frac{d|tanh(m)|}{dm}$ = sgn(tanh(m)) \u00b7 tanh'(m) = sgn(tanh(m)) \u00b7 (1 \u2013 tanh\u00b2(m)). Therefore, g'(m) is: g'(m) = \u03b2\u00b7 sgn(tanh(m)) \u00b7 (1 \u2013 tanh\u00b2(m)). Thus, the derivative of a(m) is: a'(m) = $\\frac{-\\alpha\\cdot \\beta \\cdot sgn(tanh(m)) \\cdot (1 - tanh^2(m))}{[g(m)]^2}$, where sgn(tanh(m)) is the sign function, which is 1 for tanh(m) > 0 and \u22121 for tanh(m) < 0.\nSimilarly, we compute the derivative of b(n) with respect to n: b'(n) = \u03b3\u00b7 tanh'(n) = \u03b3. (1 \u2013 tanh\u00b2(n)). Since tanh(n) is differentiable, b(n) is also differentiable, and its derivative is given by: b'(n) = \u03b3 \u00b7 (1 \u2013 tanh\u00b2(n))."}, {"title": "Appendix D. Parameter Constraints and Sensitivity Analysis", "content": "In this section, we explore the relationships among \u03b1, \u03b2 and \u03b3. We then present a sensitivity analysis of how these parameters affect our power normalization function, and illustrate the selection process for these parameters.\nConstraints on \u03b1, \u03b2, and y. Given the function $f(D) = \\frac{1}{1+e^{-\\frac{\\alpha}{(\\beta|tanh(m)|+\\epsilon)}(D-\\gamma tanh(n))}}$, we need to ensure that f(D) has a minimum value of 0 and a maximum value of 1 over the interval [-1,1]. For simplicity, assume 0 < y < 1, a > 0 and \u03b2 > 0.\nTo satisfy f(1) \u2248 1:\n$\\begin{cases}\n\\frac{\\alpha}{(\\beta|tanh(m)| + \\epsilon)}(1 - \\gamma tanh(n)) \\to +\\infty\\\\\n\\frac{\\alpha}{(\\beta|tanh(m)| + \\epsilon)}(1-\\gamma tanh(n)) \\gg 1\n\\end{cases}$ (11)\nIn the worst-case scenario where | tanh(m)| = 1 and tanh(n) = 1, it follows:\n$\\frac{\\alpha}{(\\beta + \\epsilon)}(1-\\gamma) \\gg 1$ \n$\\alpha(1 \u2013 \\gamma) \\gg \\beta + \\epsilon$ \n$\\alpha \\gg \\frac{\\beta + \\epsilon}{1-\\gamma}$ (12)\nTo ensure f(-1) \u2248 0:\n$\\begin{cases}\n\\frac{\\alpha}{(\\beta|tanh(m)| + \\epsilon)}(-1 - \\gamma tanh(n)) \\to -\\infty\\\\\n\\frac{\\alpha}{(\\beta|tanh(m)| + \\epsilon)}(-1-\\gamma tanh(n)) \\ll -1\n\\end{cases}$ (13)\nIn the worst-case scenario where | tanh(m)| = 0 and tanh(n) = \u22121, it follows:\n$\\frac{\\alpha}{\\epsilon}(-1 + \\gamma) \\ll -1$\n$\\alpha(-1 + \\gamma) \\ll -\\epsilon$\n$\\alpha \\gg \\frac{-\\epsilon}{-1+\\gamma}$ (14)\nFrom Eq. (12) and (14), to ensure a practical and feasible relationship between \u03b1, \u03b2, and y, we derive:\n$\\alpha \\gg max \\Big{ \\frac{\\beta + \\epsilon}{1-\\gamma}, \\frac{-\\epsilon}{-1+\\gamma} \\Big}$\n$\\alpha = k \\cdot max \\Big{ \\frac{\\beta + \\epsilon}{1-\\gamma}, \\frac{-\\epsilon}{-1+\\gamma} \\Big}$ (15)\nwhere k\u226b 1 is a constant ensuring that \u03b1 is sufficiently large."}, {"title": "Sensitivity analysis on \u03b1, \u03b2, and \u03b3.", "content": "In this section", "a": "n$\\frac{\\partial f(D)}{\\partial \\alpha} = \\frac{e^{-g(D)}}{(1 + e^{-g(D)})^2"}]}